---
ver: rpa2
title: Effective Large Language Model Adaptation for Improved Grounding and Citation
  Generation
arxiv_id: '2311.09533'
source_url: https://arxiv.org/abs/2311.09533
tags:
- llms
- grounding
- citations
- language
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of "hallucination" in large language
  models (LLMs), where they generate plausible but non-factual information. The core
  method idea involves a new framework, AGREE, that tunes LLMs to self-ground their
  responses by providing citations to retrieved documents.
---

# Effective Large Language Model Adaptation for Improved Grounding and Citation Generation

## Quick Facts
- arXiv ID: 2311.09533
- Source URL: https://arxiv.org/abs/2311.09533
- Reference count: 7
- Primary result: AGREE achieves higher citation recall and precision than prompting-based and post-hoc citing approaches across five datasets and two LLMs

## Executive Summary
This paper addresses the challenge of "hallucinations" in large language models by introducing AGREE, a framework that tunes LLMs to self-ground their responses with accurate citations. The method constructs training data automatically by linking LLM-generated sentences to supporting passages, then fine-tunes the base model to generate grounded responses. The framework includes an iterative test-time adaptation mechanism that refines responses by actively seeking additional supporting passages. Experiments show AGREE outperforms baseline approaches in citation quality and answer correctness across multiple datasets and model sizes.

## Method Summary
AGREE addresses LLM hallucination through a data-centric approach that tunes models to self-ground responses with citations. The framework automatically constructs training data by having a base LLM generate responses, then using an attribution evaluation model to link each sentence to maximally supported passages, creating citation pairs. This data fine-tunes the base LLM to generate grounded responses with inline citations. At test time, AGREE employs iterative test-time adaptation where the tuned LLM generates responses, identifies unsupported statements, retrieves additional passages based on these statements, and refines the response through multiple cycles until a budget is exhausted.

## Key Results
- AGREE achieves significantly higher citation recall and precision compared to prompting-based and post-hoc citing approaches
- Improvements from in-domain training generalize well to out-of-distribution datasets
- The iterative test-time adaptation mechanism consistently improves grounding quality across different query types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM can be taught to self-ground by tuning on automatically constructed data with citations
- Mechanism: The framework constructs training data by having the base LLM generate responses, then using an attribution evaluation model to link each sentence to maximally supported passages, creating citation pairs. These are then used to fine-tune the base LLM to generate grounded responses with citations.
- Core assumption: The attribution evaluation model can accurately link sentences to supporting passages, and the base LLM can learn this grounding behavior through supervised fine-tuning.
- Evidence anchors:
  - [abstract] "Our framework tunes LLMs to selfground the claims in their responses and provide accurate citations to retrieved documents."
  - [section] "We adopt a data-centric approach for optimizing MA... For each A = s1, . . . , sn we create citations C = {Ei} using the attribution evaluation model, ϕ, to link a sentence si to the maximally supported document ei = max d∈D NLI(d, si)"
- Break condition: The attribution model produces inaccurate citations or the base LLM fails to learn the grounding behavior despite fine-tuning

### Mechanism 2
- Claim: Iterative test-time adaptation (TTA) improves grounding by actively seeking additional passages
- Mechanism: At test time, the tuned LLM generates responses with citations and identifies unsupported statements. The framework then retrieves additional passages based on these unsupported statements and iterates, refining the response with more relevant information.
- Core assumption: The self-grounding evaluation can accurately identify unsupported statements, and retrieving passages based on these statements will provide the necessary supporting evidence.
- Evidence anchors:
  - [abstract] "The selfgrounding capability of tuned LLMs further grants them a test-time adaptation (TTA) capability that can actively retrieve passages to support the claims that have not been grounded, which iteratively improves the responses of LLMs."
  - [section] "At each iteration, the LLM generates a response to the query based on the working passages, add citations to its response, and find out any unsupported statements that do not have citations."
- Break condition: The LLM fails to identify unsupported statements correctly or additional retrieval does not improve grounding

### Mechanism 3
- Claim: Improvements from in-domain training generalize to out-of-distribution datasets
- Mechanism: The framework tunes the LLM on in-domain datasets (NQ, StrategyQA, FEVER) and then tests it on out-of-distribution datasets (ASQA, QAMPARI, Enterprise), showing that the learned grounding behavior transfers across different question types and corpus types.
- Core assumption: The core grounding capability learned from in-domain data is general enough to apply to different domains without additional fine-tuning.
- Evidence anchors:
  - [abstract] "improvements in grounding quality achieved by tuning using certain datasets (of small size) can well generalize across domains"
  - [section] "Recall that we adapt the base LLM only on in-domain training set (NQ, StrategyQA, and FEVER), and directly test the model on out-of-distribution (OOD) test set (ASQA, QAMPARI, Enterprise). The results suggest the improvements obtained from training on in-domain datasets can generalize to OOD datasets"
- Break condition: The generalization fails when tested on datasets with significantly different characteristics or corpus types

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: AGREE builds upon RAG by adding the capability to self-ground responses and iteratively improve them through test-time adaptation
  - Quick check question: How does AGREE differ from standard RAG approaches that simply provide retrieved documents to the LLM?

- Concept: Attribution Evaluation
  - Why needed here: The framework uses attribution evaluation models to link sentences in LLM responses to supporting passages, creating the citations needed for training and evaluation
  - Quick check question: What is the role of the NLI (Natural Language Inference) model in creating citations for the training data?

- Concept: Supervised Fine-Tuning
  - Why needed here: AGREE uses supervised fine-tuning to teach the base LLM to generate grounded responses with citations based on the automatically constructed training data
  - Quick check question: How does the verbalization of the grounding process (NL(AB, C)) facilitate standard language modeling objectives during fine-tuning?

## Architecture Onboarding

- Component map: Query → Initial retrieval → LLM response with citations → Unsupported statement identification → Additional retrieval → Refined response → Final output

- Critical path: Query → Initial retrieval → LLM response with citations → Unsupported statement identification → Additional retrieval → Refined response → Final output

- Design tradeoffs:
  - Using self-generated citations vs external attribution model (reduced overhead but relies on LLM's reasoning)
  - Fixed vs flexible budget for TTA iterations (control vs potentially incomplete refinement)
  - In-domain vs OOD generalization (efficiency vs potential performance drop)

- Failure signatures:
  - Citation recall remains low despite tuning (attribution model or fine-tuning ineffective)
  - TTA iterations don't improve grounding (unsupported statement identification or retrieval ineffective)
  - OOD generalization fails (learned grounding not general enough)

- First 3 experiments:
  1. Verify that the attribution evaluation model can accurately link sentences to passages by testing on a small dataset with known ground truth
  2. Test the fine-tuning process on a subset of data to ensure the LLM learns to generate citations
  3. Run TTA on a simple query to verify that additional retrieval improves grounding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the self-grounding and self-evaluation capabilities of large language models be further improved to enhance their overall performance in generating factually accurate and well-cited responses?
- Basis in paper: [explicit] The paper discusses the current approach of using self-grounding and self-evaluation to improve the grounding quality of LLM-generated responses, but acknowledges that there is room for further enhancement.
- Why unresolved: The paper focuses on the initial implementation and evaluation of the AGREE framework, leaving the potential for further improvements in self-grounding and self-evaluation unexplored.
- What evidence would resolve it: Future research could explore advanced techniques or architectures to refine the self-grounding and self-evaluation mechanisms, potentially leading to even more accurate and well-cited responses from LLMs.

### Open Question 2
- Question: Can the AGREE framework be extended to handle more complex and nuanced types of queries, such as those requiring multi-hop reasoning or understanding of implicit information?
- Basis in paper: [inferred] The paper mentions that the current implementation of AGREE is effective for various types of queries, but does not delve into the specifics of handling more complex reasoning tasks.
- Why unresolved: The paper's focus is on demonstrating the effectiveness of AGREE for improving grounding quality, leaving the exploration of its capabilities for handling more complex queries as an open area for future research.
- What evidence would resolve it: Conducting experiments with datasets that involve multi-hop reasoning or implicit information could provide insights into the limitations and potential improvements of AGREE for handling more complex queries.

### Open Question 3
- Question: How can the AGREE framework be adapted to work effectively with different types of corpora, such as those with varying levels of noise, domain-specific jargon, or different writing styles?
- Basis in paper: [inferred] The paper evaluates AGREE on multiple datasets with different characteristics, suggesting that its performance may vary depending on the nature of the corpus.
- Why unresolved: The paper does not provide a detailed analysis of how AGREE performs across different types of corpora, leaving the question of its adaptability to various data sources unanswered.
- What evidence would resolve it: Conducting experiments with diverse corpora, including those with varying levels of noise, domain-specific jargon, or different writing styles, could shed light on the adaptability of AGREE and identify potential areas for improvement.

## Limitations

- The quality of the attribution evaluation model directly impacts the constructed training data, and its accuracy isn't thoroughly validated
- The computational cost of iterative test-time adaptation isn't discussed in terms of practical deployment considerations
- While generalization is demonstrated, the out-of-distribution datasets may not be sufficiently diverse to establish robust generalization claims

## Confidence

**High Confidence:**
- The AGREE framework can improve citation recall and precision compared to prompting-based and post-hoc citing approaches on the tested datasets
- Supervised fine-tuning on automatically constructed data with citations enables LLMs to generate grounded responses
- The framework shows generalization capability from in-domain to out-of-domain datasets

**Medium Confidence:**
- Iterative test-time adaptation consistently improves grounding quality across all query types
- The self-grounding evaluation mechanism accurately identifies unsupported statements
- The automatic data construction pipeline produces high-quality training data

**Low Confidence:**
- The framework's performance scales to extremely large corpora or very different domains
- The computational efficiency of TTA makes it practical for real-time applications
- The attribution evaluation model's accuracy remains consistent across different document types and writing styles

## Next Checks

1. **Attribution Model Validation**: Test the NLI-based attribution evaluation model on a manually annotated dataset where humans have linked sentences to supporting passages. Measure precision and recall to establish the model's accuracy as a prerequisite for the entire AGREE pipeline.

2. **Ablation Study on Data Construction**: Compare AGREE's performance when using different methods for creating training data: (a) LLM-generated citations vs (b) human-annotated citations vs (c) no citations. This would validate whether the automatic construction process is essential or if simpler approaches suffice.

3. **TTA Iteration Analysis**: Track grounding quality metrics (citation recall, precision) across each TTA iteration for individual queries. Identify patterns where additional iterations help vs hurt, and establish optimal iteration budgets based on query characteristics rather than using a fixed budget.