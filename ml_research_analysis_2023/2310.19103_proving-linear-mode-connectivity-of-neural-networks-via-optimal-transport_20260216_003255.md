---
ver: rpa2
title: Proving Linear Mode Connectivity of Neural Networks via Optimal Transport
arxiv_id: '2310.19103'
source_url: https://arxiv.org/abs/2310.19103
tags:
- lemma
- networks
- layer
- such
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical framework explaining the linear
  mode connectivity (LMC) phenomenon in deep neural networks, where independently
  trained networks can be connected by a low-loss linear path modulo weight permutations.
  Using optimal transport theory, the authors show that with high probability, two
  wide enough two-layer neural networks trained with SGD are linearly connected.
---

# Proving Linear Mode Connectivity of Neural Networks via Optimal Transport

## Quick Facts
- arXiv ID: 2310.19103
- Source URL: https://arxiv.org/abs/2310.19103
- Reference count: 40
- Primary result: Two wide enough two-layer neural networks trained with SGD are linearly connected with high probability, and provides bounds on required width for deep networks

## Executive Summary
This paper provides a theoretical framework explaining linear mode connectivity (LMC) in deep neural networks using optimal transport theory. The authors show that two independently trained networks can be connected by a low-loss linear path modulo weight permutations when the hidden layers are sufficiently wide. They derive upper and lower bounds on required width, revealing exponential growth with respect to depth. The analysis demonstrates that the dimension of the underlying weight distribution significantly influences LMC effectiveness, and introduces a new weight matching method that outperforms existing approaches by leveraging activation covariance structure.

## Method Summary
The paper uses optimal transport theory to align neurons across networks by minimizing Wasserstein distance between empirical measures of neuron weights. For two-layer networks, they show that with high probability, independently trained networks are linearly connected when the hidden layer is sufficiently wide. For deep networks, they derive recursive bounds on required width, showing exponential growth with depth. The authors introduce a new weight matching method using activation covariance as a metric, and demonstrate through experiments that this approach significantly outperforms naive weight matching and activation matching methods.

## Key Results
- Two-layer networks with width O(√d/ε) are linearly connected with probability 1-ε
- Deep networks require exponential width growth: O(d^k) for k hidden layers
- New covariance-based weight matching method outperforms existing approaches
- Weight distribution dimension strongly correlates with LMC effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two neural networks with sufficiently wide hidden layers are linearly mode connected modulo permutation symmetries of neurons.
- Mechanism: The core mechanism relies on optimal transport theory to align neurons across networks by minimizing the Wasserstein distance between empirical measures of neuron weights. This alignment ensures that the weight matrices of two independently trained networks can be permuted to minimize the Frobenius norm difference between corresponding layers. When the width of each layer is large enough, the empirical distribution of neuron weights converges to the underlying distribution in Wasserstein distance at a rate inversely proportional to the square root of the width. This convergence guarantees that after permutation, the weight matrices of two networks are close in Frobenius norm, enabling linear interpolation with low loss.
- Core assumption: Neuron weights within each layer are independent random variables, either at initialization or maintained through training (e.g., in wide two-layer networks trained with SGD).
- Evidence anchors:
  - [abstract] The paper shows that two wide enough two-layer neural networks trained with SGD are linearly connected with high probability, and provides bounds on the required width for deep networks.
  - [section 4] The paper builds a general framework using optimal transport to show that with high probability, two wide enough two-layer neural networks trained with SGD are linearly connected modulo permutations.
  - [corpus] The corpus contains related works on mode connectivity, but does not directly address the theoretical framework using optimal transport for linear mode connectivity.
- Break condition: The mechanism breaks when neuron weights within a layer are not independent, such as in networks with shared parameters or strong correlations between neurons.

### Mechanism 2
- Claim: The dimension of the underlying weight distribution significantly influences the effectiveness of linear mode connectivity.
- Mechanism: The paper shows that when the underlying weight distribution has lower effective dimension (e.g., concentrated around a low-dimensional subspace), the Wasserstein convergence rate improves, reducing the required width for linear mode connectivity. This is formalized through the introduction of an "approximately low-dimensional" model where the weight covariance matrix has a few dominant eigenvalues. The smaller the effective dimension, the less the width needs to grow with depth to maintain linear mode connectivity.
- Core assumption: The weight distribution can be approximated by a low-dimensional structure, such as a Gaussian distribution with a covariance matrix having a few dominant eigenvalues.
- Evidence anchors:
  - [abstract] The paper demonstrates the validity of their approach by showing how the dimension of the support of the weight distribution of neurons, which dictates Wasserstein convergence rates, is correlated with linear mode connectivity.
  - [section 5.4] The paper introduces a model where weights are initialized from a multivariate Gaussian with a covariance matrix that is approximately low-dimensional, showing that this leads to faster convergence rates and less width growth with depth.
  - [corpus] The corpus contains related works on mode connectivity, but does not directly address the influence of the dimension of the underlying weight distribution on linear mode connectivity.
- Break condition: The mechanism breaks when the weight distribution does not have a low-dimensional structure, such as in cases where all eigenvalues of the covariance matrix are of similar magnitude.

### Mechanism 3
- Claim: The paper proposes a new weight matching method that significantly outperforms existing approaches by leveraging the covariance structure of activations.
- Mechanism: The new weight matching method minimizes the Frobenius norm between weight matrices, but uses the covariance of activations from the previous layer as a metric. This approach is more effective than naive weight matching (which uses the standard Euclidean norm) and activation matching (which matches based on activations directly) because it takes into account the geometry of the activation space. By using the covariance matrix of activations, the method effectively reduces the problem to a lower-dimensional space where the optimal permutation can be found more accurately.
- Core assumption: The covariance structure of activations provides a better metric for matching neurons across networks than the standard Euclidean norm or direct activation matching.
- Evidence anchors:
  - [section 6] The paper introduces a new weight matching method that uses the covariance of activations to find optimal permutations, showing that it significantly outperforms naive weight matching across different learning rates.
  - [abstract] The paper proposes a new weight matching method that significantly outperforms existing approaches by leveraging the covariance structure of activations.
  - [corpus] The corpus contains related works on mode connectivity and weight matching, but does not directly address the proposed new weight matching method using the covariance of activations.
- Break condition: The mechanism breaks when the covariance structure of activations does not provide a meaningful metric for matching neurons, such as in cases where activations are isotropic or when the previous layer's activations are not informative.

## Foundational Learning

- Concept: Optimal Transport Theory
  - Why needed here: Optimal transport theory provides the mathematical framework for measuring distances between probability distributions, which is essential for quantifying how well two neural networks can be aligned through permutations of neurons. The Wasserstein distance, a key concept in optimal transport, is used to measure the similarity between the empirical distributions of neuron weights across networks.
  - Quick check question: What is the Wasserstein distance, and how does it differ from other measures of distance between probability distributions?

- Concept: Empirical Measures and Convergence Rates
  - Why needed here: Understanding empirical measures and their convergence rates to the underlying distribution is crucial for determining the conditions under which two neural networks can be linearly connected. The paper uses convergence rates of empirical measures in Wasserstein distance to establish bounds on the required width of neural networks for linear mode connectivity.
  - Quick check question: How does the convergence rate of an empirical measure to its underlying distribution depend on the dimension of the support and the number of samples?

- Concept: Neural Network Architecture and Symmetry
  - Why needed here: Knowledge of neural network architecture, particularly the concept of permutation symmetry among neurons in a layer, is essential for understanding how two networks can be aligned and connected by a linear path. The paper exploits the fact that neurons within a layer can be permuted without changing the network's functionality, allowing for the alignment of independently trained networks.
  - Quick check question: Why does permuting neurons within a hidden layer of a neural network not change its functionality, and how can this property be used to align two independently trained networks?

## Architecture Onboarding

- Component map:
  Optimal Transport Module -> Weight Matching Module -> Linear Interpolation Module -> Evaluation Module

- Critical path:
  1. Compute empirical measures of neuron weights for both networks.
  2. Use optimal transport theory to find permutations that minimize Wasserstein distance between empirical measures.
  3. Perform linear interpolation between aligned networks.
  4. Evaluate the loss along the interpolation path to verify linear mode connectivity.

- Design tradeoffs:
  - Computational complexity vs. accuracy in weight matching: Using more sophisticated metrics (e.g., covariance-based) can improve alignment accuracy but may increase computational cost.
  - Width vs. depth: The required width for linear mode connectivity grows exponentially with depth, making very deep networks challenging to connect linearly.
  - Independence assumption: The theory relies on neuron weights being independent, which may not hold in all training scenarios or architectures.

- Failure signatures:
  - High loss along linear interpolation path: Indicates that the networks are not well-aligned or that the width is insufficient for linear mode connectivity.
  - Permutations do not significantly reduce Frobenius norm: Suggests that the weight distributions are too dissimilar or that the independence assumption is violated.
  - Empirical measures do not converge as expected: May indicate that the underlying weight distribution does not have the assumed structure (e.g., low-dimensionality).

- First 3 experiments:
  1. Train two identical two-layer neural networks with different random initializations and verify linear mode connectivity using the proposed optimal transport framework.
  2. Vary the width of the hidden layer and measure the error barrier along the linear interpolation path to empirically validate the theoretical bounds on required width.
  3. Compare the performance of the new covariance-based weight matching method against naive weight matching and activation matching on networks trained with different learning rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the independence assumption on neuron weights within each layer affect linear mode connectivity in deep networks trained with modern optimizers like Adam?
- Basis in paper: [inferred] The paper relies on the independence of neuron weights as a key assumption for proving LMC, but acknowledges that this assumption is "more realistic" for SGD and may not hold for other optimizers.
- Why unresolved: The paper notes that "understanding the impact of the optimizer on the independence of weights during training is crucial" but does not provide experimental validation for optimizers beyond SGD.
- What evidence would resolve it: Experiments comparing LMC effectiveness across different optimizers (SGD, Adam, etc.) while measuring the independence of weights within layers would clarify the relationship between optimizer choice and LMC.

### Open Question 2
- Question: Can the recursive exponential growth in width required for LMC be avoided by exploiting approximate low-dimensionality of weight distributions?
- Basis in paper: [explicit] The paper proposes a model where weights follow an approximately low-dimensional distribution and shows that this can reduce the required width growth, though "asymptotically, we recover the same rates."
- Why unresolved: The paper acknowledges that "the constant T'i explodes if e → 0" in their model, preventing the use of fixed low dimensionality across layers. A more realistic model of weight distribution dimensionality is needed.
- What evidence would resolve it: Developing a more realistic model of how weight distributions evolve during training that maintains low dimensionality without exploding constants, and proving LMC bounds under this model.

### Open Question 3
- Question: How does the dimension of the underlying weight distribution affect the effectiveness of different permutation-finding methods?
- Basis in paper: [explicit] The paper introduces a new weight matching method and shows empirically that "this method constantly and substantially outperforms naive Weight Matching across different learning rates" when the approximate dimension is high.
- Why unresolved: While the paper demonstrates a correlation between distribution dimension and LMC effectiveness, it does not provide a theoretical explanation for why their new method outperforms existing ones in high-dimensional cases.
- What evidence would resolve it: A theoretical analysis proving that the new weight matching method is optimal or near-optimal for high-dimensional weight distributions, and experiments showing its performance degrades gracefully as dimensionality decreases.

## Limitations
- Exponential width growth requirement with depth makes LMC challenging for very deep networks
- Theoretical bounds are loose and may not reflect practical scenarios
- Independence assumption on neuron weights may not hold in all training scenarios or architectures

## Confidence
- Theoretical framework for two-layer networks: High
- Exponential width bounds for deep networks: Medium
- Covariance-based weight matching method: Medium
- Low-dimensionality assumption: Medium

## Next Checks
1. Verify linear mode connectivity empirically by training two-layer networks with varying widths and measuring interpolation loss barriers
2. Test the covariance-based weight matching method against alternatives on networks trained with different learning rates
3. Measure the effective dimension of weight distributions in trained networks and correlate with LMC effectiveness