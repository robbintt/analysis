---
ver: rpa2
title: 'MuseChat: A Conversational Music Recommendation System for Videos'
arxiv_id: '2310.06282'
source_url: https://arxiv.org/abs/2310.06282
tags:
- music
- video
- recommendation
- arxiv
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MuseChat is a dialogue-based music recommendation system for videos
  that personalizes music suggestions by incorporating user preferences and reasoning.
  The system uses a two-turn conversation to refine recommendations, combining video,
  music, and text inputs.
---

# MuseChat: A Conversational Music Recommendation System for Videos

## Quick Facts
- arXiv ID: 2310.06282
- Source URL: https://arxiv.org/abs/2310.06282
- Reference count: 40
- Key outcome: MuseChat significantly improves music retrieval performance over existing methods, achieving higher recall rates and success rates while offering strong interpretability.

## Executive Summary
MuseChat is a pioneering dialogue-based music recommendation system designed to personalize music suggestions for videos by incorporating user preferences through conversational interactions. The system employs a two-turn conversation structure to refine recommendations, combining video, music, and text inputs via a multi-modal recommendation engine. A sentence generator module, powered by a fine-tuned large language model, provides explanations for recommendations, enhancing interpretability. Evaluated on a new dataset of 98,206 video-music pairs, MuseChat demonstrates significant improvements in music retrieval performance and offers a novel approach to interactive and explainable music recommendation.

## Method Summary
MuseChat uses a multi-modal recommendation engine that combines video, music, and text inputs through separate encoders (CLIP for video/text, AST for audio) fused via cross-attention layers. The system first recommends music based on video content, then refines suggestions using user-provided textual feedback in a two-turn conversation. A sentence generator module, based on Vicuna-7B LLM with LoRA fine-tuning, generates natural language explanations for recommendations. The system is trained using contrastive loss (InfoNCE) on a dataset of 98,206 video-music pairs synthesized from YouTube-8M data.

## Key Results
- MuseChat achieves significantly higher recall rates (Recall@1, Recall@5, Recall@10) compared to existing video-based music retrieval methods
- The system demonstrates improved success rates@10, indicating better top-10 recommendation quality
- Strong interpretability through LLM-generated explanations, validated by BertScore and divergence metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The two-turn conversation structure allows the system to refine music recommendations by incorporating user feedback, overcoming the limitations of single-turn recommendation systems.
- **Mechanism**: The system first recommends music based solely on video content, then uses user-provided textual feedback and previously recommended music to adjust and improve the recommendation.
- **Core assumption**: User preferences can be effectively captured through a short conversational exchange about the initial recommendation.
- **Evidence anchors**:
  - [abstract] "Our system consists of two key functionalities with associated modules: recommendation and reasoning."
  - [section] "Experiment results show that MuseChat achieves significant improvements over existing video-based music retrieval methods as well as offers strong interpretability and interactability."
  - [corpus] Weak. No corpus papers directly address the two-turn conversational structure, but they are related to conversational recommendation systems.
- **Break condition**: If user preferences cannot be adequately expressed in the two-turn format or if the system cannot effectively process the feedback to modify recommendations.

### Mechanism 2
- **Claim**: The multi-modal recommendation engine effectively combines video, music, and text inputs to create a unified representation that improves music-video matching.
- **Mechanism**: The system uses separate encoders for each modality (CLIP for video and text, AST for audio), then fuses these representations using cross-attention layers to create a comprehensive embedding.
- **Core assumption**: Each modality contributes unique and complementary information that, when combined, leads to better music-video matching than any single modality alone.
- **Evidence anchors**:
  - [abstract] "A multi-modal recommendation engine aligns music with visual and textual cues."
  - [section] "We present a cutting-edge tri-modal architecture designed for music-video matching, enhanced with textual input."
  - [corpus] Weak. While corpus papers discuss multimodal recommendation, none specifically detail the cross-attention fusion approach described here.
- **Break condition**: If the fusion of modalities introduces noise or if the individual modality encoders are not sufficiently powerful to capture the relevant features.

### Mechanism 3
- **Claim**: The sentence generator module provides interpretability by explaining the reasoning behind music recommendations, increasing user trust and engagement.
- **Mechanism**: A fine-tuned Vicuna-7B LLM takes music embeddings and titles as input and generates natural language explanations for the recommendations.
- **Core assumption**: Users value and benefit from explanations of why certain music was recommended, and these explanations can be generated by an LLM fine-tuned on music embeddings.
- **Evidence anchors**:
  - [abstract] "The reasoning module, equipped with the power of Large Language Model (Vicuna-7B) and extended to multi-modal inputs, is able to provide reasonable explanation for the recommended music."
  - [section] "By harnessing the capabilities of LLMs, we craft a sentence generator module."
  - [corpus] Weak. While corpus papers discuss LLM-powered conversational recommendation, none specifically address the use of LLMs for generating explanations for music recommendations.
- **Break condition**: If the explanations generated by the LLM are not perceived as helpful or accurate by users, or if they do not align well with the actual reasoning behind the recommendation.

## Foundational Learning

- **Concept**: Contrastive learning and InfoNCE loss
  - Why needed here: Used to train the music recommendation module to learn a joint embedding space where similar video-music pairs are close together and dissimilar pairs are far apart.
  - Quick check question: How does the InfoNCE loss function encourage the model to learn meaningful representations for music-video matching?

- **Concept**: Cross-modal attention mechanisms
  - Why needed here: Used to fuse the different modality embeddings (video, music, text) into a unified representation that captures the relationships between them.
  - Quick check question: What is the advantage of using cross-attention over simple concatenation or averaging for fusing multimodal embeddings?

- **Concept**: Large language model fine-tuning with LoRA
  - Why needed here: Used to adapt the Vicuna-7B LLM for the specific task of generating explanations for music recommendations, while keeping the number of trainable parameters low.
  - Quick check question: How does LoRA enable efficient fine-tuning of large language models for specific downstream tasks?

## Architecture Onboarding

- **Component map**: CLIP (video and text) -> AST (audio) -> Transformer encoders -> Cross-attention fusion -> Music recommendation module -> Vicuna-7B LLM with LoRA fine-tuning -> Sentence generator module -> Ranked music recommendations with explanations

- **Critical path**:
  1. Extract embeddings from video, music, and text inputs
  2. Fuse embeddings using cross-attention layers
  3. Compute similarity scores and rank music recommendations
  4. Generate explanations using the sentence generator module

- **Design tradeoffs**:
  - Using a two-turn conversation structure limits the amount of user feedback that can be incorporated, but simplifies the system design and data collection process.
  - Employing a pre-trained LLM for explanations provides strong natural language generation capabilities, but requires careful fine-tuning to ensure the explanations are relevant and accurate.

- **Failure signatures**:
  - Poor music-video matching: Check the quality of the individual modality embeddings and the effectiveness of the cross-attention fusion.
  - Unhelpful explanations: Verify that the LLM is properly fine-tuned on music embeddings and that the explanations align with the actual reasoning behind the recommendations.
  - System not responsive to user feedback: Ensure that the user prompts are being correctly processed and incorporated into the second-turn recommendation.

- **First 3 experiments**:
  1. Ablation study: Remove the text input and evaluate the impact on music-video matching performance to assess the contribution of the text modality.
  2. Cross-attention variants: Compare different fusion strategies (e.g., concatenation, averaging, cross-attention) to determine the most effective approach for combining modality embeddings.
  3. Explanation quality: Conduct a user study to evaluate the perceived helpfulness and accuracy of the explanations generated by the sentence generator module.

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The quality of simulated two-turn conversations relies heavily on GPT-3.5 prompt templates and conversation synthesis rules, which are not fully specified
- The cross-attention fusion mechanism's exact implementation details and hyperparameter choices remain unclear
- The effectiveness of LLM-generated explanations cannot be fully assessed without user perception studies

## Confidence
- **High confidence**: The multi-modal recommendation architecture combining video, audio, and text embeddings is well-established and technically sound
- **Medium confidence**: The effectiveness of the two-turn conversation structure for capturing user preferences is plausible but not extensively validated
- **Low confidence**: The quality and usefulness of the LLM-generated explanations cannot be fully assessed from the paper alone

## Next Checks
1. Conduct an ablation study removing the text modality to quantify its contribution to recommendation performance and validate the multi-modal fusion approach
2. Implement and compare alternative fusion strategies (e.g., concatenation, averaging) with the proposed cross-attention mechanism to ensure the design choice is optimal
3. Design a user study to evaluate the perceived helpfulness and accuracy of the explanations generated by the sentence generator module, focusing on whether they align with users' reasoning for accepting or rejecting recommendations