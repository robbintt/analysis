---
ver: rpa2
title: M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms
arxiv_id: '2309.03759'
source_url: https://arxiv.org/abs/2309.03759
tags:
- m-mode
- images
- learning
- training
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using M-mode images extracted from echocardiography
  videos for estimating left ventricular ejection fraction (EF) and classifying cardiomyopathy.
  M-mode images are generated by extracting single lines of pixels from the video
  at different angles.
---

# M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms

## Quick Facts
- arXiv ID: 2309.03759
- Source URL: https://arxiv.org/abs/2309.03759
- Reference count: 0
- Primary result: M-mode images improve EF prediction compared to B-mode videos using both supervised and self-supervised learning

## Executive Summary
This paper proposes using M-mode images extracted from echocardiography videos for estimating left ventricular ejection fraction (EF) and classifying cardiomyopathy. M-mode images are generated by extracting single lines of pixels from the video at different angles. These images are then used in both supervised and self-supervised learning settings to predict EF. The supervised setting achieves comparable performance to a baseline method while being computationally more efficient. The self-supervised setting using contrastive learning is helpful for limited data scenarios. The key finding is that M-mode images can effectively capture cardiac dynamics and improve EF prediction compared to using raw B-mode videos.

## Method Summary
The method generates M-mode images from B-mode echocardiography videos by extracting single scan lines at different angles (0° to 180°). These M-mode images are then used in two learning paradigms: supervised learning with ResNet and fusion methods (early or late), and self-supervised contrastive learning with patient-aware and structure-aware losses. The supervised approach trains directly on labeled data, while the self-supervised approach first pretrains on unlabeled data to learn meaningful representations before fine-tuning on labeled data. The model predicts EF as both a regression task (continuous value) and classification task (categorical thresholds).

## Key Results
- M-mode images achieve comparable or better performance than B-mode video methods for EF estimation
- Late fusion of M-mode images outperforms early fusion, with late fusion benefiting from more modes while early fusion overfits
- Self-supervised contrastive learning maintains high accuracy even with limited labeled data
- The method is computationally more efficient than processing full B-mode videos

## Why This Works (Mechanism)

### Mechanism 1
- Claim: M-mode images capture cardiac dynamics more effectively than B-mode frames for EF estimation.
- Mechanism: M-mode images represent a single scan line through the heart over time, providing direct temporal information about myocardial motion. This temporal resolution allows the model to learn dynamic patterns associated with cardiac function.
- Core assumption: The cardiac dynamics relevant to EF estimation are primarily captured along specific scan lines through the heart, and these patterns are preserved in the generated M-mode images.
- Evidence anchors:
  - [abstract] "M-mode images can effectively capture cardiac dynamics and improve EF prediction compared to using raw B-mode videos."
  - [section] "Since cardiac function assessment relies on heart dynamics, M-mode images can be an excellent alternative to B(rightness)-mode image- or video-based methods."
- Break condition: If the generated M-mode images do not accurately represent the cardiac motion along the chosen scan lines, or if the dynamics relevant to EF are distributed across multiple spatial dimensions that M-mode cannot capture.

### Mechanism 2
- Claim: Contrastive learning with patient-aware and structure-aware losses improves representation learning in limited labeled data scenarios.
- Mechanism: The patient-aware loss pulls together M-mode images from the same patient (same EF label), while the structure-aware loss maintains diversity within each patient's representations. This dual awareness helps the model learn robust features that generalize across patients.
- Core assumption: M-mode images from the same patient share underlying structural and functional characteristics that can be exploited for representation learning, even without explicit labels.
- Evidence anchors:
  - [abstract] "we extend contrastive learning (CL) to cardiac imaging to learn meaningful representations from exploiting structures in unlabeled data allowing the model to achieve high accuracy, even with limited annotations."
  - [section] "we propose a problem-specific contrastive loss with patient and structure awareness"
- Break condition: If the assumption that M-mode images from the same patient are sufficiently similar for contrastive learning does not hold, or if the augmentation strategies do not create meaningful positive pairs.

### Mechanism 3
- Claim: Early fusion of M-mode images leads to overfitting, while late fusion provides better generalization.
- Mechanism: Early fusion combines all M-mode images into a single tensor before processing, creating a high-dimensional input that may be difficult to learn from with limited data. Late fusion processes each M-mode image separately and then combines the learned features, reducing the dimensionality of the fusion step.
- Core assumption: The model has sufficient capacity to learn meaningful representations from individual M-mode images, and the fusion of these representations is more effective than direct fusion of the raw images.
- Evidence anchors:
  - [section] "Figure 4 shows the results for different numbers of M-modes. We see that late fusion models benefit from an increasing number of modes, whereas the early fusion method overfits quickly and never achieves a comparable performance."
- Break condition: If the late fusion approach does not capture the necessary cross-modal relationships between different M-mode images, or if the processing of individual images loses important context.

## Foundational Learning

- Concept: Contrastive learning and self-supervised representation learning
  - Why needed here: Medical datasets are often limited in labeled samples, but have abundant unlabeled data. Contrastive learning can leverage this unlabeled data to learn meaningful representations that improve downstream tasks.
  - Quick check question: What is the difference between instance discrimination and cluster discrimination in contrastive learning?

- Concept: M-mode ultrasound imaging principles
  - Why needed here: Understanding how M-mode images are generated and what cardiac information they capture is crucial for interpreting the model's behavior and results.
  - Quick check question: How does an M-mode image differ from a B-mode image in terms of what cardiac information it captures?

- Concept: Echocardiography and ejection fraction
  - Why needed here: EF is the target metric, and understanding how it relates to cardiac function and what factors affect its measurement is important for evaluating the model's predictions.
  - Quick check question: What is the clinical significance of an EF below 50%?

## Architecture Onboarding

- Component map: B-mode video -> M-mode image generation -> Supervised/Unsupervised model -> EF prediction
- Critical path: Generate M-mode images from B-mode videos → Train supervised or self-supervised model on M-mode images → Evaluate model performance on held-out test set
- Design tradeoffs:
  - Number of M-mode images to generate per patient
  - Choice of fusion method (early vs. late)
  - Balance between patient-aware and structure-aware losses in contrastive learning
  - Length of M-mode image sequences
- Failure signatures:
  - Poor performance with few labeled samples (suggests need for better self-supervised pretraining)
  - Overfitting with many M-mode images (suggests need for regularization or smaller input size)
  - Inconsistent predictions across different M-mode images from the same patient (suggests need for better fusion method)
- First 3 experiments:
  1. Generate M-mode images from a sample B-mode video and visualize them to verify they capture cardiac motion.
  2. Train a simple supervised model on a small subset of M-mode images to verify the data pipeline works.
  3. Compare the performance of early and late fusion methods on a held-out validation set to verify the overfitting observation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of M-mode images per patient for achieving the best trade-off between performance and computational efficiency?
- Basis in paper: [explicit] The paper evaluates models with different numbers of M-modes (1, 2, 5, 10, 20, 50) and shows that late fusion models benefit from an increasing number of modes, while early fusion methods overfit quickly.
- Why unresolved: The paper does not provide a definitive answer on the optimal number of M-modes, as the results vary depending on the fusion method and the specific task.
- What evidence would resolve it: Conducting further experiments with a wider range of M-mode numbers and different fusion methods, and analyzing the performance and computational costs for each scenario.

### Open Question 2
- Question: How do different M-mode generation mechanisms, such as changing the center of rotation, affect the performance of EF estimation?
- Basis in paper: [explicit] The paper mentions that other approaches for generating M-mode images are feasible, including changing the center of rotation in the middle of the image.
- Why unresolved: The paper focuses on a single M-mode generation mechanism and does not explore the impact of different generation methods on performance.
- What evidence would resolve it: Comparing the performance of EF estimation using different M-mode generation mechanisms and analyzing the results to determine the most effective approach.

### Open Question 3
- Question: Can the proposed self-supervised learning method using contrastive learning be extended to other cardiac imaging modalities or medical applications?
- Basis in paper: [inferred] The paper discusses the potential of the proposed method for limited data scenarios and mentions the possibility of applying it to other problems where cardiac dynamics play an essential role in diagnosis.
- Why unresolved: The paper only evaluates the method on echocardiograms and does not explore its applicability to other medical imaging modalities or tasks.
- What evidence would resolve it: Applying the proposed self-supervised learning method to other cardiac imaging modalities or medical applications and evaluating its performance compared to existing methods.

## Limitations
- The EchoNet-Dynamic dataset used contains a relatively homogeneous patient population (predominantly white, with limited comorbidity diversity), which may limit generalizability.
- The M-mode generation process relies on specific scan line angles, and the paper does not thoroughly explore whether these angles are optimal or if different cardiac views might yield better representations.
- While the self-supervised approach shows promise for limited data scenarios, the specific augmentation strategies and contrastive loss implementation details are not fully specified, making direct replication challenging.

## Confidence

- High confidence: The core observation that M-mode images can effectively capture cardiac dynamics for EF prediction
- Medium confidence: The claim that late fusion outperforms early fusion
- Medium confidence: The effectiveness of the patient-aware and structure-aware contrastive loss

## Next Checks

1. Generate M-mode images using alternative scan line angles (not just the standard 0°-180°) to determine if certain cardiac views are more informative for EF estimation than others.

2. Conduct a thorough ablation study comparing different fusion strategies (early vs. late) with varying numbers of M-mode images, and test on datasets with different demographic compositions to assess generalizability.

3. Implement and test the self-supervised contrastive learning pipeline on smaller subsets of the dataset (e.g., 10%, 25%, 50%) to verify the claim that it maintains performance with limited labeled data.