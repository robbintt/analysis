---
ver: rpa2
title: Mitigating Backdoors within Deep Neural Networks in Data-limited Configuration
arxiv_id: '2311.07417'
source_url: https://arxiv.org/abs/2311.07417
tags:
- backdoor
- proposed
- attacks
- filters
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a lightweight method for mitigating backdoor
  attacks in deep neural networks (DNNs) under data-limited scenarios. The approach
  formulates a backdoor suspiciousness score to identify and prune poisoned neurons/filters
  based on their activation values, weights, and correlations with other neurons.
---

# Mitigating Backdoors within Deep Neural Networks in Data-limited Configuration

## Quick Facts
- arXiv ID: 2311.07417
- Source URL: https://arxiv.org/abs/2311.07417
- Reference count: 40
- This paper proposes a lightweight method for mitigating backdoor attacks in deep neural networks (DNNs) under data-limited scenarios, reducing attack success rates by over 50% using only ten clean samples per class while maintaining high accuracy.

## Executive Summary
This paper addresses the challenge of mitigating backdoor attacks in deep neural networks when only a limited set of clean samples is available. The proposed method introduces a backdoor suspiciousness score that evaluates each filter based on its spectral norm, saliency, activation norm, and correlation with other neurons. By pruning filters with high suspiciousness scores, the approach effectively removes poisoned neurons while preserving clean data performance. Experiments demonstrate significant reductions in attack success rates with minimal impact on accuracy, particularly in data-constrained environments.

## Method Summary
The method calculates a backdoor suspiciousness score for each filter in convolutional layers, combining spectral norm, saliency (gradient magnitude), activation norm (computed using clean samples), and correlation with other filters. Filters exceeding a threshold (mean + μ × std of suspiciousness scores) are pruned along with their associated batch normalization parameters. The pruned network is then fine-tuned on the limited clean dataset. The approach is designed to be lightweight and efficient, requiring only a small set of clean samples for effective backdoor mitigation.

## Key Results
- Reduces attack success rates by over 50% using only ten clean samples per class on CIFAR-10
- Maintains high accuracy on clean data while mitigating backdoor vulnerabilities
- Outperforms baseline methods (NAD, ANP, CLP, AWM) by approximately 3x in terms of processing speed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning filters with high backdoor suspiciousness scores removes poisoned neurons while preserving clean data performance.
- Mechanism: The algorithm computes a backdoor suspiciousness score for each filter using spectral norm, saliency, activation norm, and correlation metrics. Filters above a threshold are pruned, effectively removing backdoor triggers.
- Core assumption: Backdoored filters exhibit distinct activation, correlation, and sensitivity patterns compared to clean filters.
- Evidence anchors:
  - [abstract] "We formulate some characteristics of poisoned neurons. This backdoor suspiciousness score can rank network neurons according to their activation values, weights, and their relationship with other neurons in the same layer."
  - [section] "We define a formulation to measure the backdoor suspiciousness of each filter based on the characteristics of their associated weights and feature maps."
  - [corpus] Weak - No direct corpus evidence for this specific mechanism, but related works on filter pruning support the general approach.
- Break condition: If backdoored filters do not exhibit distinct patterns or if clean filters share similar characteristics, the pruning may remove essential neurons.

### Mechanism 2
- Claim: Using a limited set of clean samples enables effective backdoor detection without requiring large datasets.
- Mechanism: The algorithm calculates activation norms and correlations using only clean samples, identifying filters that respond differently to clean vs. poisoned data.
- Core assumption: Clean samples are sufficient to reveal the distinct behavior of backdoored filters.
- Evidence anchors:
  - [abstract] "Our experiments indicate the proposed method decreases the chance of attacks being successful by more than 50% with a tiny clean dataset, i.e., ten clean samples for the CIFAR-10 dataset."
  - [section] "The defender has limited access to clean samples in most real-world scenarios. Hence, fine-tuning and distilling the knowledge may end up overfitting and destroying the performance of DNN on clean test data."
  - [corpus] Weak - No direct corpus evidence for this specific claim about sample efficiency.
- Break condition: If the clean dataset is too small or unrepresentative, the algorithm may fail to identify backdoored filters accurately.

### Mechanism 3
- Claim: The square root operator in the suspiciousness score formula improves numerical stability and interpretability.
- Mechanism: Including the square root function scales the spectral norm term appropriately, balancing its influence with other metrics.
- Core assumption: The square root operator effectively normalizes the spectral norm contribution without distorting the relative importance of other terms.
- Evidence anchors:
  - [section] "The inclusion of the square root function within an equation can be substantiated by its proficiency in skillfully adjusting the scale of numerical values associated with the coefficient of spectral norm in equation 6."
  - [section] "The square root's intrinsic capacity to compress larger values while preserving relative differences offers potential enhancements in terms of numerical stability and the interpretability of outcomes."
  - [corpus] Missing - No direct corpus evidence for this specific mathematical choice.
- Break condition: If the square root operator introduces numerical instability or if other scaling methods perform better, the formula may need adjustment.

## Foundational Learning

- Concept: Spectral norm and its role in measuring filter sensitivity
  - Why needed here: Spectral norm quantifies how much a filter amplifies input variations, helping identify filters sensitive to trigger patterns.
  - Quick check question: What does a high spectral norm value indicate about a filter's behavior?

- Concept: Batch normalization and its interaction with backdoor features
  - Why needed here: BN layers record backdoor statistics during training, making their parameters useful for identifying compromised filters.
  - Quick check question: Why does pruning BN neurons simultaneously disable corresponding convolutional filters?

- Concept: Correlation analysis between filter activations
  - Why needed here: Filters with low correlation to others are more likely to carry trojan features, as they represent independent or unusual patterns.
  - Quick check question: How does the correlation between filter activations help distinguish backdoored from clean filters?

## Architecture Onboarding

- Component map: Input -> Backdoored model + Clean samples -> Spectral norm calculation -> Saliency measurement -> Activation norm computation -> Correlation analysis -> Suspiciousness score -> Pruning -> Fine-tuning -> Output

- Critical path:
  1. Calculate spectral norm for each filter
  2. Measure saliency (gradient magnitude) for each filter
  3. Compute activation norm for each filter using clean samples
  4. Calculate correlation between filter activations
  5. Combine metrics into suspiciousness score
  6. Prune filters above threshold
  7. Fine-tune pruned network

- Design tradeoffs:
  - Speed vs. accuracy: Using limited clean samples speeds up processing but may reduce detection accuracy
  - Aggressive vs. conservative pruning: Higher thresholds preserve more filters but may leave backdoors intact
  - Complexity vs. interpretability: More sophisticated metrics improve detection but make the algorithm harder to understand

- Failure signatures:
  - ASR remains high after pruning: Backdoored filters not identified or removed
  - ACC drops significantly: Essential clean filters mistakenly pruned
  - Inconsistent results across runs: Threshold selection or metric calculation unstable

- First 3 experiments:
  1. Test on CIFAR-10 with BadNets attack, using 10 clean samples, measure ASR and ACC
  2. Vary threshold parameter μ to find optimal balance between ASR reduction and ACC preservation
  3. Compare against baseline methods (NAD, ANP, CLP, AWM) on same dataset and attack

## Open Questions the Paper Calls Out

- How does the proposed backdoor suspiciousness score perform against more sophisticated or adaptive backdoor attacks that may try to evade detection by mimicking clean filter characteristics?
- What is the impact of the proposed method on extremely deep or complex neural network architectures beyond ResNet-18, such as Vision Transformers or larger ResNet variants?
- How does the performance of the proposed method change when the ratio of poisoned samples in the training data varies significantly from the 1% used in the experiments?
- What is the theoretical guarantee or upper bound on the attack success rate that can be achieved even after applying the proposed mitigation method?

## Limitations
- Narrow experimental scope limited to CIFAR-10 dataset and ResNet-18 architecture
- Square root operator in suspiciousness score lacks rigorous theoretical foundation
- Effectiveness against adaptive backdoor attacks not evaluated
- Sample efficiency claim needs verification across different dataset sizes and architectures

## Confidence

- Mechanism 1 (Filter pruning effectiveness): Medium confidence - Supported by filter pruning literature but specific backdoor detection claims need more empirical validation
- Mechanism 2 (Sample efficiency): Low confidence - Limited experimental evidence for the 10-sample claim across diverse scenarios
- Mechanism 3 (Square root operator): Low confidence - Weak theoretical justification and no corpus support for this specific mathematical choice

## Next Checks

1. Test the pruning threshold sensitivity across different μ values to establish robust parameter selection guidelines and identify the optimal balance between ASR reduction and ACC preservation.

2. Validate sample efficiency by systematically varying the number of clean samples (1, 5, 10, 25, 50) and measuring the impact on both ASR reduction and ACC maintenance across different attack types.

3. Evaluate the method on additional datasets (e.g., CIFAR-100, TinyImageNet) and architectures (e.g., VGG, DenseNet) to assess generalizability beyond the CIFAR-10/ResNet-18 combination.