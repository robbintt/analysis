---
ver: rpa2
title: On marginal feature attributions of tree-based models
arxiv_id: '2302.08434'
source_url: https://arxiv.org/abs/2302.08434
tags:
- values
- game
- tree
- shapley
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper contrasts the implementation invariance of marginal
  Shapley values with the path-dependent TreeSHAP algorithm for tree-based models.
  While marginal Shapley values depend only on the input-output function, TreeSHAP
  can yield different feature rankings for models that compute the same function but
  differ in internal structure.
---

# On marginal feature attributions of tree-based models

## Quick Facts
- arXiv ID: 2302.08434
- Source URL: https://arxiv.org/abs/2302.08434
- Reference count: 40
- Key outcome: Marginal Shapley values for tree-based models are implementation invariant (depend only on input-output function) while TreeSHAP can yield different results for functionally equivalent models with different internal structures.

## Executive Summary
This paper investigates marginal feature attributions for tree-based models, contrasting them with the path-dependent TreeSHAP algorithm. The authors prove that marginal Shapley values are piecewise-constant functions with respect to a grid partition determined by the model's internal structure. They demonstrate that implementation invariance - a key property where attributions depend only on the input-output function rather than model internals - holds for marginal Shapley values but not for TreeSHAP. For CatBoost models with symmetric trees, they derive an explicit formula enabling efficient computation without background data.

## Method Summary
The paper employs theoretical analysis and empirical experiments to investigate marginal feature attributions. Theoretical contributions include proving that marginal Shapley values are piecewise-constant functions with respect to grid partitions determined by trained models, and deriving explicit formulas for CatBoost models. Empirical experiments involve training multiple tree-based models (CatBoost, LightGBM, XGBoost) on benchmark datasets (Superconductivity with 81 predictors, 12,757 samples; Ailerons with 40 predictors, 5,723 samples) and comparing feature attributions across models that compute identical functions but differ in internal structure.

## Key Results
- Marginal Shapley values for tree-based models are piecewise-constant functions with respect to a grid partition determined by the model's internal structure.
- TreeSHAP's implementation dependence can lead to different feature rankings for models that compute the same function but have different internal structures.
- For CatBoost models with symmetric trees, marginal Shapley values can be computed exactly using only internal model parameters without requiring background data.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Marginal Shapley values for tree-based models are piecewise-constant functions with respect to a grid partition determined by the model's internal structure.
- **Mechanism:** The theorem shows that for any decision tree, the marginal game (which underlies marginal Shapley values) is constant on each piece of the grid partition determined by the tree. This happens because the model's output is piecewise constant, and the marginal expectation calculation preserves this property.
- **Core assumption:** The model implements a piecewise-constant function and events at decision boundaries have probability zero (as stated in the paper).
- **Evidence anchors:**
  - [abstract] "we prove that their marginal Shapley values, or more generally marginal feature attributions obtained from a linear game value, are simple (piecewise-constant) functions with respect to a certain finite partition of the input space determined by the trained model."
  - [section] Theorem 3.2 proves that marginal and TreeSHAP feature attributions are simple functions with respect to the grid partition away from the decision boundary.
- **Break condition:** If the model implements a non-piecewise-constant function or decision boundaries have non-zero probability, this mechanism would break.

### Mechanism 2
- **Claim:** The complexity of computing marginal Shapley values can be reduced by exploiting the fact that only a subset of features appear in each tree.
- **Mechanism:** The null-player property of linear game values means that features not appearing in a tree contribute zero to the marginal Shapley value for that tree. By focusing only on trees that contain the feature of interest and treating the game as having only those features as players, computational complexity is reduced from O(2^n) to O(2^r) where r is the number of features in the tree.
- **Core assumption:** The game value satisfies the null-player property and carrier-dependence property.
- **Evidence anchors:**
  - [abstract] "the complexity of computing marginal Shapley (or Owen or Banzhaf) feature attributions may be reduced."
  - [section] §3.3 explains how applying a linear game value to vME(·; X,f) without utilizing the fact that not all features appear in all trees results in 2^(n-1) summands, while exploiting this fact reduces complexity.
- **Break condition:** If the game value doesn't satisfy the null-player property, or if all features appear in every tree, this optimization would not apply.

### Mechanism 3
- **Claim:** For CatBoost models with symmetric trees, marginal Shapley values can be computed exactly using only internal model parameters without background data.
- **Mechanism:** The symmetry of oblivious trees means P(T) = ˜P(T), so the grid partition coincides with the tree's partition. This allows computing marginal Shapley values using a formula that only requires internal parameters like leaf values and probabilities, without needing a background dataset for estimation.
- **Core assumption:** The trees are oblivious (symmetric) and the number of distinct features per tree is limited (no more than depth).
- **Evidence anchors:**
  - [abstract] "we exploit the symmetry to derive an explicit formula, with improved complexity and only in terms of the internal model parameters, for marginal Shapley (and Banzhaf and Owen) values of CatBoost models."
  - [section] Theorem 3.5 provides the explicit formula and Algorithm 3.11 implements it without requiring background data.
- **Break condition:** If the trees are not symmetric or the number of distinct features per tree is not limited, this exact computation would not be possible.

## Foundational Learning

- **Concept: Cooperative game theory and game values**
  - Why needed here: The paper uses game-theoretic concepts (Shapley value, Banzhaf value, Owen value) as the foundation for feature attribution methods. Understanding these concepts is essential to grasp why certain properties (null-player, carrier-dependence) matter for computational efficiency.
  - Quick check question: What property of the Shapley value makes it possible to ignore features that don't appear in a tree when computing marginal feature attributions?

- **Concept: Piecewise-constant functions and grid partitions**
  - Why needed here: The paper's key insight is that marginal feature attributions are piecewise-constant functions with respect to a grid partition. Understanding what this means and how it relates to the model's internal structure is crucial for implementing the algorithms.
  - Quick check question: Given a decision tree with depth d, how many rectangular regions would its corresponding grid partition contain at most?

- **Concept: Implementation invariance**
  - Why needed here: The paper contrasts TreeSHAP (which is not implementation invariant) with marginal Shapley values (which are). Understanding this concept helps explain why the two methods can produce different results for functionally equivalent models.
  - Quick check question: What does it mean for a feature attribution method to be "implementation invariant," and why is this property desirable?

## Architecture Onboarding

- **Component map:** Tree-based models (CatBoost, LightGBM, XGBoost) -> Game-theoretic feature attribution methods -> Algorithms for computing marginal Shapley values -> Complexity reduction techniques based on model structure
- **Critical path:** For a new engineer to implement marginal Shapley value computation: (1) Parse the tree-based model structure, (2) Identify which features appear in each tree, (3) Apply the appropriate algorithm (Theorem 3.5 for CatBoost, general method for others), (4) Handle probability estimation for non-symmetric trees.
- **Design tradeoffs:** The paper presents a tradeoff between accuracy and computational efficiency. Exact computation (Algorithm 3.11 for CatBoost) is faster but only works for symmetric trees. Approximate methods using background data (interventional TreeSHAP) work for any tree but require large background datasets for accuracy.
- **Failure signatures:** If the computed marginal Shapley values don't match expectations, possible causes include: (1) Incorrect identification of features in each tree, (2) Errors in probability estimation, (3) Misapplication of the algorithm (e.g., using the symmetric tree formula for a non-symmetric tree).
- **First 3 experiments:**
  1. Implement Algorithm 3.11 for a simple CatBoost model with known internal parameters and verify it produces the expected marginal Shapley values.
  2. Compare the marginal Shapley values computed by Algorithm 3.11 with those from interventional TreeSHAP on a CatBoost model using a large background dataset.
  3. Create two functionally equivalent decision trees with different structures and verify that marginal Shapley values match while TreeSHAP values differ, demonstrating the implementation invariance property.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do marginal Shapley values compare to other game-theoretic feature attribution methods (e.g., Owen values) in terms of computational efficiency and interpretability for tree-based models?
- Basis in paper: [explicit] The paper discusses Owen values as an alternative to Shapley values, particularly in the context of grouping features and reducing computational complexity.
- Why unresolved: The paper does not provide a direct comparison of marginal Shapley values with Owen values or other game-theoretic methods in terms of efficiency and interpretability for tree-based models.
- What evidence would resolve it: Empirical studies comparing the computational time and interpretability of marginal Shapley values versus Owen values for various tree-based models and datasets.

### Open Question 2
- Question: Can the symmetry of CatBoost models be exploited to develop more efficient algorithms for computing feature attributions for other types of tree-based models?
- Basis in paper: [explicit] The paper highlights the efficiency gained from exploiting the symmetry of CatBoost models for computing marginal Shapley values.
- Why unresolved: The paper does not explore whether the symmetry property can be generalized to other tree-based models or how it could be leveraged to improve computational efficiency.
- What evidence would resolve it: Research investigating the symmetry properties of other tree-based models and developing algorithms that exploit these properties for efficient feature attribution computation.

### Open Question 3
- Question: How does the choice of background dataset size affect the accuracy and computational efficiency of TreeSHAP's interventional variant?
- Basis in paper: [explicit] The paper mentions that TreeSHAP's interventional variant uses a background dataset and discusses the trade-off between accuracy and computational efficiency based on the dataset size.
- Why unresolved: The paper does not provide a detailed analysis of how the background dataset size impacts the accuracy and efficiency of TreeSHAP's interventional variant.
- What evidence would resolve it: Experiments systematically varying the background dataset size and measuring the accuracy and computational time of TreeSHAP's interventional variant for different tree-based models and datasets.

## Limitations
- The theoretical results depend critically on the assumption that decision boundaries have zero probability measure, which may not hold in practice for continuous features.
- The computational complexity improvements for CatBoost models only apply to symmetric (oblivious) trees, limiting generalizability to asymmetric tree structures common in many implementations.
- The paper does not extensively validate whether the implementation invariance property of marginal Shapley values translates to practical improvements in model interpretability or fairness assessments.

## Confidence
- **High Confidence**: The proof that marginal Shapley values are piecewise-constant functions with respect to the grid partition (Mechanism 1).
- **Medium Confidence**: The complexity reduction claims (Mechanism 2) for non-symmetric trees.
- **Medium Confidence**: The exact formula for CatBoost models (Mechanism 3) due to specific structural assumptions.

## Next Checks
1. **Implementation Validation**: Create a simple decision tree with known piecewise-constant structure and verify that computed marginal Shapley values are constant within each partition region, confirming Mechanism 1.

2. **Cross-Model Comparison**: Train functionally equivalent models using CatBoost, LightGBM, and XGBoost on the same dataset, then compare marginal Shapley values versus TreeSHAP outputs to empirically verify the implementation invariance property claimed in the paper.

3. **Complexity Benchmarking**: Implement both the naive marginal Shapley computation (without null-player optimization) and the optimized version for a tree with n features. Measure computational time and verify that complexity scales as O(2^r) where r is the number of features appearing in the tree, confirming Mechanism 2.