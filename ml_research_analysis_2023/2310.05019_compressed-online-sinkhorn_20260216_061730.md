---
ver: rpa2
title: Compressed online Sinkhorn
arxiv_id: '2310.05019'
source_url: https://arxiv.org/abs/2310.05019
tags:
- sinkhorn
- algorithm
- online
- where
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits the online Sinkhorn algorithm for computing
  optimal transport distances between continuous probability distributions. The authors
  provide an improved convergence rate analysis that is faster than previous bounds
  under certain parameter choices, and validate this theoretically and numerically.
---

# Compressed online Sinkhorn

## Quick Facts
- arXiv ID: 2310.05019
- Source URL: https://arxiv.org/abs/2310.05019
- Reference count: 40
- Key outcome: Compressed online Sinkhorn achieves O(N^(-a/(2a+1))) convergence rate under certain parameter choices, improving upon previous O(N^(-b/(2a+1))) rates

## Executive Summary
This paper introduces the compressed online Sinkhorn algorithm for computing optimal transport distances between continuous probability distributions. The method combines the online Sinkhorn algorithm with measure compression techniques to reduce computational complexity while maintaining theoretical convergence guarantees. The authors provide improved convergence rate analysis showing faster asymptotic convergence under specific parameter choices, and demonstrate significant computational gains through numerical experiments on 1D, 2D, and 5D examples.

## Method Summary
The compressed online Sinkhorn algorithm iteratively updates Kantorovich potentials using samples from the input distributions, then applies measure compression (via Fourier moments or Gaussian quadrature) to reduce the representation size of these potentials. The method maintains theoretical convergence guarantees while achieving better asymptotic performance than the original online Sinkhorn algorithm when compression error is sufficiently small relative to learning rate and batch size parameters. The algorithm uses learning rates η_t = t^b and batch sizes b_t = t^(2a) with specific constraints on parameters a and b.

## Key Results
- Improved theoretical convergence rate from O(N^(-b/(2a+1))) to O(N^(-a/(2a+1))) for compressed online Sinkhorn under parameter choices satisfying -1 < b < -1/2 and a - b > 1
- Significant computational gains demonstrated through execution time comparisons in 1D, 2D, and 5D experiments
- Theoretical ratio of complexities shows compressed version outperforms original when ζ > 3(a-b)/(4a+1)
- Numerical validation shows convergence behavior matches theoretical predictions across different dimensionalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compressed online Sinkhorn improves asymptotic convergence rate by reducing representation size of continuous Kantorovich potentials.
- Mechanism: The algorithm compresses the growing set of Dirac measures representing the potentials using Fourier moments or Gaussian quadrature, maintaining accuracy while reducing computational and memory costs.
- Core assumption: The compression error introduced by the measure compression techniques is sufficiently small relative to the learning rate and batch size parameters.
- Evidence anchors:
  - [abstract]: "The compressed online Sinkhorn algorithm is shown to have better asymptotic convergence than the original method when the compression error is sufficiently small relative to the learning rate and batch size parameters."
  - [section]: "We apply compression with a Fourier-based moments approach... The larger this exponent, the more improvement we can see in the asymptotical convergence of the compressed online Sinkhorn compared to the online Sinkhorn."
- Break condition: If the compression error grows too large relative to the learning rate and batch size parameters, the convergence rate improvement will be lost and may even degrade performance.

### Mechanism 2
- Claim: The theoretical convergence rate improvement from O(N^(-b/(2a+1))) to O(N^(-a/(2a+1))) is achieved under specific parameter choices.
- Mechanism: By choosing learning rate η_t = t^b and batch size b_t = t^(2a) with -1 < b < -1/2 and a - b > 1, the algorithm achieves a faster asymptotic convergence rate.
- Core assumption: The parameter choices satisfy the conditions -1 < b < -1/2 and a - b > 1, and the total sample size N grows polynomially with iteration count t.
- Evidence anchors:
  - [abstract]: "We improve the convergence analysis for the online Sinkhorn algorithm, the new rate that we obtain is faster than the previous rate under certain parameter choices."
  - [section]: "Theorem 1. Let f* and g* denote the optimal potentials... For a constant c > 0, δ_N ≲ exp(-cN^(b+1)/(2a+1)) + N^(-a/(2a+1)) = O(N^(-a/(2a+1)))"
- Break condition: If the parameter choices do not satisfy the conditions -1 < b < -1/2 and a - b > 1, or if the polynomial growth assumption is violated, the convergence rate improvement will not hold.

### Mechanism 3
- Claim: The compressed online Sinkhorn algorithm achieves significant computational gains in practice.
- Mechanism: By reducing the number of Dirac measures representing the potentials through compression techniques, the algorithm reduces both computational complexity and memory footprint, leading to faster execution times.
- Core assumption: The compression techniques maintain sufficient accuracy of the potentials while reducing their representation size, and the implementation is efficient.
- Evidence anchors:
  - [abstract]: "Theoretical guarantees and numerical experiments demonstrate significant computational gains."
  - [section]: "Execution times are shown and demonstrate the advantage of the compression method. Here the compression is computed using Scipy's NNLS."
  - [corpus]: "The corpus evidence is weak as the related papers do not directly address computational gains of compressed online Sinkhorn algorithms."
- Break condition: If the compression techniques introduce too much error or the implementation is not efficient, the computational gains will be diminished or lost.

## Foundational Learning

- Concept: Optimal Transport (OT) and Wasserstein distances
  - Why needed here: The paper is about improving algorithms for computing OT distances, which are fundamental concepts in the work.
  - Quick check question: What is the Kantorovich formulation of optimal transport, and how does it relate to Wasserstein distances?

- Concept: Entropic regularization and Sinkhorn algorithm
  - Why needed here: The paper builds upon the Sinkhorn algorithm for computing entropic-regularized OT distances, which is the foundation for the online Sinkhorn variants.
  - Quick check question: How does the Sinkhorn algorithm iteratively update dual potentials to solve the entropic-regularized OT problem?

- Concept: Measure compression techniques
  - Why needed here: The paper proposes using measure compression techniques to reduce the computational complexity of the online Sinkhorn algorithm.
  - Quick check question: What are some common measure compression techniques, and how do they work to approximate a measure with fewer Dirac measures?

## Architecture Onboarding

- Component map:
  - Input: Continuous probability distributions α and β
  - Online Sinkhorn: Iteratively updates potentials using samples from the distributions
  - Compression: Applies measure compression techniques to reduce representation size
  - Output: Approximate optimal potentials and OT distance

- Critical path:
  1. Initialize potentials as Dirac measures
  2. For each iteration:
     a. Sample batch from distributions
     b. Update potentials using online Sinkhorn iterations
     c. Compress potentials using measure compression techniques
  3. Return final potentials and OT distance

- Design tradeoffs:
  - Compression error vs. computational efficiency: Higher compression rates lead to greater computational gains but may introduce more error.
  - Parameter choice: The learning rate and batch size parameters must be carefully chosen to balance convergence rate and accuracy.
  - Compression method: Different compression techniques (e.g., Fourier moments, Gaussian quadrature) may be more suitable for different cost functions and distributions.

- Failure signatures:
  - Divergence or slow convergence of the algorithm
  - Large errors in the computed OT distance or potentials
  - Excessive memory usage or computational time

- First 3 experiments:
  1. Compare the convergence rate and accuracy of the compressed online Sinkhorn algorithm with the original online Sinkhorn algorithm on a simple 1D Gaussian example.
  2. Investigate the impact of different compression techniques (e.g., Fourier moments vs. Gaussian quadrature) on the performance of the algorithm for different cost functions.
  3. Evaluate the scalability of the compressed online Sinkhorn algorithm on high-dimensional datasets and compare its performance with other OT approximation methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions on the compression parameter ζ does the compressed online Sinkhorn algorithm outperform the original online Sinkhorn algorithm in practice, and how does this depend on the dimensionality of the problem?
- Basis in paper: [explicit] The paper states that the ratio of complexities for compressed and original online Sinkhorn is improved when ζ > 3(a-b)/(4a+1), and provides numerical experiments showing better performance in 1D, 2D, and 5D settings.
- Why unresolved: While theoretical bounds are provided, the paper does not explore the full range of ζ values or provide a comprehensive study of how performance varies with dimensionality and parameter choices.
- What evidence would resolve it: Systematic numerical experiments varying ζ across a wide range and for different dimensionalities, along with theoretical analysis of the trade-off between compression error and computational savings.

### Open Question 2
- Question: How does the performance of the compressed online Sinkhorn algorithm with Fourier moments compression compare to other measure compression techniques like kernel recombination or Nyström method for different cost functions beyond quadratic costs?
- Basis in paper: [explicit] The paper mentions that Fourier compression works well for quadratic costs due to fast tail behavior but may perform less well for non-quadratic costs like L1 norm, suggesting exploration of other compression techniques.
- Why unresolved: The paper only implements and compares Fourier moments compression, leaving open the question of how it compares to other established compression methods for different cost functions.
- What evidence would resolve it: Implementation and numerical comparison of the compressed online Sinkhorn algorithm using various compression techniques (Fourier moments, kernel recombination, Nyström) for a range of cost functions including L1 norm.

### Open Question 3
- Question: What is the optimal adaptive strategy for choosing the parameters a and b in the online Sinkhorn algorithm to balance the fast initial convergence behavior and the asymptotic convergence rate?
- Basis in paper: [explicit] The paper notes that in the limit as a approaches infinity, the asymptotic convergence rate improves, but the transient behavior becomes poorer, suggesting a need for adaptive parameter selection.
- Why unresolved: The paper does not explore adaptive strategies for choosing a and b, instead fixing them throughout the algorithm's execution.
- What evidence would resolve it: Development and testing of adaptive algorithms that dynamically adjust a and b based on the current error or iteration number, along with analysis of the resulting convergence behavior.

## Limitations

- The theoretical convergence guarantees depend on Assumption 4 for compression error, which is not empirically verified across different distributions and cost functions
- The parameter choice ζ = 0.95 for 1D and 0.9 for higher dimensions appears arbitrary without justification of how these values were selected
- Performance claims are limited to relatively simple 1D and 2D examples, with uncertainty about scalability to high-dimensional problems with complex distributions

## Confidence

- **High confidence**: The convergence rate improvement O(N^(-a/(2a+1))) for the compressed algorithm under the stated assumptions is mathematically sound given the parameter constraints.
- **Medium confidence**: The practical computational gains demonstrated in the experiments are compelling but limited to relatively simple examples.
- **Low confidence**: The claim that compressed online Sinkhorn outperforms other OT approximation methods in practice is not directly supported, as no comparisons are made with alternative approaches.

## Next Checks

1. Verify the compression error bound empirically by measuring ∥f_t - ˆf_t∥_var across different iterations and compression parameters for various test distributions.
2. Test the algorithm on higher-dimensional problems (d > 5) and compare performance with other state-of-the-art OT approximation methods.
3. Investigate the sensitivity of the convergence rate to the choice of compression technique (Fourier moments vs. Gaussian quadrature) and cost function (quadratic vs. other kernels).