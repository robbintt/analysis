---
ver: rpa2
title: Can Chat GPT solve a Linguistics Exam?
arxiv_id: '2311.02499'
source_url: https://arxiv.org/abs/2311.02499
tags:
- chatgpt
- exam
- language
- linguistics
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether ChatGPT4 can solve introductory
  linguistics exams by testing it on past exam questions from a German university
  course. The model was given questions on morphology, phonetics/phonology, and syntax
  with minimal preprocessing.
---

# Can Chat GPT solve a Linguistics Exam?

## Quick Facts
- arXiv ID: 2311.02499
- Source URL: https://arxiv.org/abs/2311.02499
- Reference count: 0
- Primary result: ChatGPT4 scored 77% on introductory linguistics exams without task-specific training

## Executive Summary
This study investigates whether ChatGPT4 can solve introductory linguistics exams by testing it on past exam questions from a German university course. The model was given questions on morphology, phonetics/phonology, and syntax with minimal preprocessing. ChatGPT4 performed very well in phonetics and phonology, achieving 98% correct results, including accurate phonetic transcriptions. In morphology, it achieved 76%, performing well on simple tasks but struggling with complex morphological analysis. In syntax, it scored 55%, correctly handling definitions but failing at phrase identification and syntax tree analysis due to inability to process visual data. Overall, ChatGPT4 scored 77% on the exam, passing at a human-equivalent level without task-specific training.

## Method Summary
The study tested ChatGPT4 on past exam questions from the winter 2017/2018 Linguistics 1 course at TU Dortmund University, covering morphology, phonetics/phonology, and syntax. The questions were minimally preprocessed by marking examples with [X is] and [In X] notation and replacing underlining with square brackets. The model's responses were then graded using the course's normal grading scheme and compared to human grading standards.

## Key Results
- ChatGPT4 achieved 98% accuracy in phonetics and phonology tasks
- The model scored 76% in morphology, handling simple tasks well but struggling with complex analysis
- Performance in syntax was 55%, with correct definitions but failed phrase identification and syntax tree analysis
- Overall exam score of 77%, passing at human-equivalent level without task-specific training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT4 can pass a linguistics exam at human-equivalent level using only minimal preprocessing of exam questions.
- Mechanism: The model leverages its large-scale pretraining on diverse text corpora to draw on implicit linguistic knowledge and apply it to structured exam tasks without fine-tuning.
- Core assumption: The exam questions fall within the scope of general linguistic knowledge encoded in the pretraining data, and the tasks can be interpreted correctly from natural language prompts.
- Evidence anchors:
  - [abstract] "ChatGPT4... performed very well in phonetics and phonology, achieving 98% correct results... scored 77% on the exam, passing at a human-equivalent level without task-specific training."
  - [section] "The language model manages very well to interpret and correctly answer even complex and nested tasks."
  - [corpus] Weak evidence: No direct corpus signals for this exact task type; only general LLM exam performance studies are cited.
- Break condition: If the exam requires highly specialized terminology or visual data interpretation, the model's performance degrades significantly.

### Mechanism 2
- Claim: ChatGPT4 performs best on tasks with clear one-to-one or one-to-few correspondence between items and categories, such as phonetic transcription.
- Mechanism: The model can effectively map discrete symbols (IPA characters) to their corresponding linguistic features or orthographic forms when the mapping is explicit and unambiguous.
- Core assumption: Tasks with clear, structured mappings are easier for the model to handle than those requiring complex hierarchical or contextual analysis.
- Evidence anchors:
  - [abstract] "It proved surprisingly successful in the task of broad phonetic transcription, but performed less well in the analysis of morphemes and phrases."
  - [section] "Evidently the model was able to draw on and employ the analytic toolkit for phonetic transcription more successfully than the tools for morphological analysis. This is likely to be due to the fact that there is a more explicit and more restricted one-to-one correspondence between item and category in phonetic transcription than in morphological analysis."
  - [corpus] Weak evidence: No direct corpus signals for phonetic transcription performance.
- Break condition: If the task involves ambiguous or rare linguistic phenomena, the model's performance drops.

### Mechanism 3
- Claim: ChatGPT4 cannot yet handle visual data or multimodal inputs, such as syntax trees, without extensive preprocessing.
- Mechanism: The model is currently limited to processing textual data and cannot directly interpret or generate visual representations.
- Core assumption: The model's architecture is designed for language understanding and generation, not for visual data processing.
- Evidence anchors:
  - [abstract] "The model is not yet able to deal with visualisations, such as the analysis or generation of syntax trees. More extensive preprocessing, which translates these tasks into text data, allow the model to also solve these tasks successfully."
  - [section] "Similarly, an exercise to draw a syntax tree of a given sentence fails for the same reason: the model cannot (yet) engage with multimodal data production, here the drawing of syntax trees."
  - [corpus] Weak evidence: No direct corpus signals for multimodal data processing capabilities.
- Break condition: If the task requires visual interpretation or generation, the model's performance is severely impacted unless the data is converted to text.

## Foundational Learning

- Concept: Pretraining and fine-tuning of large language models
  - Why needed here: Understanding how ChatGPT4 can perform well on linguistics exams without specific training requires knowledge of its general pretraining and potential for task adaptation.
  - Quick check question: What is the difference between pretraining and fine-tuning in the context of large language models?

- Concept: One-to-one and one-to-few correspondence in linguistic analysis
  - Why needed here: The model's performance varies based on the clarity of mappings between linguistic items and categories, which is crucial for understanding its strengths and limitations.
  - Quick check question: Give an example of a linguistic task that involves one-to-one correspondence and one that involves one-to-few correspondence.

- Concept: Limitations of text-only models in handling visual data
  - Why needed here: Recognizing that ChatGPT4 cannot process visual data without preprocessing is essential for understanding its current capabilities and potential future developments.
  - Quick check question: Why might a text-only language model struggle with tasks involving syntax trees or other visual linguistic representations?

## Architecture Onboarding

- Component map: Input processing (text) -> Knowledge retrieval (from pretraining) -> Task interpretation -> Response generation (text)
- Critical path: Input processing (text) -> Knowledge retrieval (from pretraining) -> Task interpretation -> Response generation (text)
- Design tradeoffs: The model prioritizes broad linguistic knowledge and text generation over specialized domain knowledge or multimodal capabilities.
- Failure signatures: Inability to handle rare or complex linguistic phenomena, struggles with tasks requiring visual interpretation, and potential inaccuracies in ambiguous cases.
- First 3 experiments:
  1. Test the model's performance on a simple phonetic transcription task to confirm its strength in one-to-one correspondence tasks.
  2. Evaluate the model's ability to analyze a complex morphological structure to identify its limitations in handling rare or ambiguous cases.
  3. Attempt to have the model interpret a syntax tree by converting it to text and compare the results to the original visual representation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific performance thresholds would determine if ChatGPT's morphology analysis capabilities are sufficient for educational purposes?
- Basis in paper: [explicit] The paper notes ChatGPT performs "sufficiently well" in simple morphological cases but struggles with rarer cases and complex analysis, scoring 76% on morphology tasks.
- Why unresolved: The paper doesn't establish what performance level would be considered adequate for teaching or assessment purposes.
- What evidence would resolve it: Comparative studies of student performance on similar morphology tasks, and expert evaluation of whether ChatGPT's morphological analysis quality meets pedagogical standards.

### Open Question 2
- Question: How would ChatGPT's performance change if given multimodal training data including visual syntax trees?
- Basis in paper: [explicit] The paper notes ChatGPT cannot currently process visual data like syntax trees, scoring 0 points on tasks requiring tree analysis.
- Why unresolved: The study only tested ChatGPT's current capabilities without multimodal extensions, and no evidence is provided about potential performance improvements with visual data training.
- What evidence would resolve it: Testing ChatGPT with multimodal training on linguistic visual data and comparing performance to its current text-only capabilities.

### Open Question 3
- Question: What specific aspects of linguistic domain knowledge does ChatGPT lack that prevent accurate phrase identification?
- Basis in paper: [explicit] The paper shows ChatGPT correctly identifies some phrases but fails others, suggesting gaps in domain-specific knowledge.
- Why unresolved: The paper identifies failures but doesn't systematically analyze which specific linguistic concepts or rules the model lacks.
- What evidence would resolve it: Detailed error analysis comparing ChatGPT's phrase identification failures against established linguistic principles, identifying precise knowledge gaps.

## Limitations

- Performance varies significantly across subfields, with much lower scores in syntax (55%) compared to phonetics/phonology (98%)
- Cannot handle visual data or syntax trees without extensive preprocessing
- Results based on a single German university course, limiting generalizability

## Confidence

- **High Confidence**: The claim that ChatGPT4 can pass introductory linguistics exams at human-equivalent levels using only minimal preprocessing. This is directly supported by the 77% overall score and comparison to human grading standards.
- **Medium Confidence**: The mechanism explaining performance differences across subfields (one-to-one correspondence in phonetics vs. complex analysis in morphology/syntax). While the study provides plausible explanations, it lacks systematic investigation of the underlying causes.
- **Low Confidence**: Claims about ChatGPT4's ability to handle more complex linguistic tasks or specialized terminology without additional training. The study only tests introductory-level material and does not explore the model's limits.

## Next Checks

1. **Cross-Institutional Validation**: Test ChatGPT4 on linguistics exams from multiple universities and countries to assess generalizability beyond the single German course used in this study.

2. **Performance on Advanced Material**: Evaluate the model on advanced linguistics exams or research-level problems to determine where its capabilities plateau and identify specific linguistic phenomena that challenge the model.

3. **Multimodal Capability Assessment**: Test whether converting visual linguistic data (syntax trees, phonological diagrams) to text descriptions enables ChatGPT4 to handle these tasks, and quantify the performance difference compared to text-only tasks.