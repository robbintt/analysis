---
ver: rpa2
title: On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training
arxiv_id: '2305.12224'
source_url: https://arxiv.org/abs/2305.12224
tags:
- pre-training
- dataset
- downstream
- diversity
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the trade-off between intra-class diversity
  (number of samples per class) and inter-class diversity (number of classes) in supervised
  pre-training datasets. When dataset size is fixed, the best downstream performance
  requires a balance between these two types of diversity.
---

# On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training

## Quick Facts
- arXiv ID: 2305.12224
- Source URL: https://arxiv.org/abs/2305.12224
- Reference count: 40
- Primary result: Optimal downstream performance requires balancing intra-class and inter-class diversity in supervised pre-training datasets

## Executive Summary
This paper investigates the fundamental trade-off between intra-class diversity (number of samples per class) and inter-class diversity (number of classes) in supervised pre-training datasets. When the total dataset size is fixed, the authors theoretically and empirically demonstrate that downstream task performance depends monotonically on both types of diversity, with an optimal balance point that is invariant to dataset size. Using this insight, they develop a method to predict the optimal number of pre-training classes, achieving approximately 2-point improvements on downstream tasks compared to standard design choices.

## Method Summary
The authors study supervised pre-training by systematically varying the number of classes (K) and samples per class (n) in ImageNet subsets while keeping the total dataset size N fixed at K×n. They pre-train ResNet-18 models on these subsets and evaluate performance through linear probing on six downstream datasets. The theoretical analysis derives generalization bounds showing that downstream performance is a convex function of the class-to-sample ratio K/n, with an optimal ratio that is invariant to N. This allows prediction of the optimal number of pre-training classes for a given downstream task.

## Key Results
- Downstream performance follows a U-shaped curve when plotted against class-to-sample ratio, with optimal performance at an intermediate point
- The optimal class-to-sample ratio is theoretically proven to be invariant to pre-training dataset size
- Using the predicted optimal number of classes yields approximately 2-point improvements on downstream tasks compared to standard 1000-class pre-training
- A contrastive case where data is first sampled then labeled by clustering shows monotonic improvement with increasing K, without the trade-off

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Downstream performance depends monotonically on both intra-class diversity (n) and inter-class diversity (K), with an optimal class-to-sample ratio (K/n) that is invariant to dataset size N.
- **Mechanism:** The authors prove that the representation error on downstream tasks can be bounded by a function of both K and n, with the optimal ratio K/n determined by constants that do not depend on N. This means that increasing either K or n improves downstream performance, but when N is fixed, increasing one necessarily decreases the other, creating a trade-off that manifests as a U-shaped curve when plotting downstream performance against the class-to-sample ratio.
- **Core assumption:** The pre-training representation error bounds the downstream representation error (Assumption 2), and the coefficients in this bound are stable when classes are added or removed.
- **Evidence anchors:**
  - [abstract] "our theory reveals that the optimal class-to-sample ratio (#classes / #samples per class) is invariant to the size of the pre-training dataset"
  - [section 3.3] "the optimal class-to-sample ratio for a specific downstream task is invariant to the size of the pre-training dataset"
- **Break condition:** If the assumption that pre-training representation error bounds downstream representation error fails, or if the stability assumption on coefficients is violated.

### Mechanism 2
- **Claim:** With fixed pre-training dataset size N, the trade-off between intra-class and inter-class diversity creates a downstream performance trade-off, where neither extreme yields optimal results.
- **Mechanism:** Since N = K × n when the dataset size is fixed, increasing K (inter-class diversity) necessarily decreases n (intra-class diversity), and vice versa. The authors show that downstream performance is a convex function of the class-to-sample ratio, meaning that both extremes of this ratio lead to suboptimal performance, with an optimal middle ground.
- **Core assumption:** The data generation process involves sampling K classes and then n samples per class from each, and the downstream task's performance is correlated with the pre-training task's representation error.
- **Evidence anchors:**
  - [section 2.2] "by looking at the anti-diagonal lines where logN is fixed... we can see a trade-off between intra-/inter-class diversity on the test error rate of downstream tasks"
  - [section 3.3] "From Equation 2 we can see that the performance on target task would increase when we increase 1) intra-class diversity n, 2) inter-class diversity K, and 3) the size of pre-training dataset N. When N is fixed, however, increasing either intra-class diversity or inter-class diversity would decrease the other... and therefore eventually leads to performance drop."
- **Break condition:** If the correlation between pre-training and downstream representation errors breaks down.

### Mechanism 3
- **Claim:** In a contrastive case where data is first sampled and then labeled by clustering, there is no trade-off between K and n, and K dominates downstream performance.
- **Mechanism:** When the same data samples are used regardless of K (by clustering labels), increasing K doesn't reduce n. In this scenario, the bound on downstream performance only depends on K (through the concentration of the coefficients), not on n, leading to monotonically improving performance with increasing K.
- **Core assumption:** The data generation process fixes the samples and varies only the labeling (clustering), so intra-class diversity remains constant while inter-class diversity increases.
- **Evidence anchors:**
  - [section 3.4] "The new data generation process is different from the one in Section 3.2. In particular, with different K, the new data generation process would not introduce new data samples as we manipulate the label by clustering the current sampled data"
  - [section 3.4] "According to Theorem 2, we can see that there is no longer a trade-off between the inter-class diversity K and the intra-class diversity n. Instead, the bound gets smaller when K is larger."
- **Break condition:** If the clustering process introduces significant noise or if the downstream task requires the original class structure.

## Foundational Learning

- **Concept:** Supervised pre-training and transfer learning
  - **Why needed here:** The paper studies how the composition of a supervised pre-training dataset affects performance on downstream tasks, which is a core question in transfer learning.
  - **Quick check question:** What is the difference between supervised pre-training and self-supervised pre-training, and why might the results differ between these paradigms?

- **Concept:** Generalization bounds and Gaussian complexity
  - **Why needed here:** The theoretical analysis relies on generalization bounds for the pre-training and downstream tasks, using Gaussian complexity to measure the richness of the function class.
  - **Quick check question:** How does Gaussian complexity relate to the Rademacher complexity, and why is it used in the proof of Theorem 1?

- **Concept:** Convex optimization and trade-off analysis
  - **Why needed here:** The paper shows that downstream performance is a convex function of the class-to-sample ratio, and the optimal ratio can be found by setting the derivative to zero.
  - **Quick check question:** Why is it significant that the optimal class-to-sample ratio is invariant to dataset size, and how does this simplify the optimization problem?

## Architecture Onboarding

- **Component map:** Data generation module → Pre-training pipeline → Linear probing evaluation → Performance evaluation → Theoretical analysis module
- **Critical path:** Data generation → Pre-training → Linear probing → Performance evaluation → Theoretical analysis
- **Design tradeoffs:** Grid search over K and n values is computationally expensive but provides accurate empirical results; theoretical analysis is faster but relies on assumptions that may not always hold
- **Failure signatures:** If the optimal class-to-sample ratio is not found, downstream performance may be suboptimal; if assumptions in the theoretical analysis are violated, the bounds may not be tight
- **First 3 experiments:**
  1. Verify the trade-off curve by fixing N=10K and varying K from 10 to 1000, measuring downstream performance on CIFAR10
  2. Test the invariance of the optimal class-to-sample ratio by fitting the performance function at N=5K and predicting the optimal K for N=50K and N=100K
  3. Compare the extrapolation method against grid search and standard (K=1000) for N=50K and N=100K on all six downstream tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the precise mathematical relationship between intra-class diversity (n) and inter-class diversity (K) in determining optimal downstream performance when pre-training dataset size N is fixed?
- **Basis in paper:** Explicit - The paper establishes that downstream performance follows a convex relationship with class-to-sample ratio, but does not provide a closed-form optimal solution.
- **Why unresolved:** The paper only provides asymptotic bounds and empirical observations without deriving the exact functional form that would allow precise optimization.
- **What evidence would resolve it:** A rigorous mathematical derivation showing the exact optimal K/n ratio as a function of dataset characteristics, validated through extensive empirical testing across multiple domains.

### Open Question 2
- **Question:** How do the theoretical findings about optimal class-to-sample ratio invariance apply to non-image domains and other pre-training paradigms beyond supervised learning?
- **Basis in paper:** Explicit - The paper focuses on image classification with ImageNet and supervised pre-training, noting that extension to other domains remains unexplored.
- **Why unresolved:** The current theoretical framework is built specifically for supervised classification with labeled data, and the authors acknowledge this limitation without exploring other scenarios.
- **What evidence would resolve it:** Empirical studies testing the theory's predictions on text, audio, or multimodal data, as well as contrastive or self-supervised pre-training methods.

### Open Question 3
- **Question:** What is the impact of class imbalance within the pre-training dataset on the optimal class-to-sample ratio and downstream performance?
- **Basis in paper:** Explicit - The paper assumes uniform sampling of classes and samples per class, but real-world datasets often have severe class imbalance.
- **Why unresolved:** The theoretical analysis and empirical validation both assume balanced classes, and the authors do not address how this assumption affects their conclusions.
- **What evidence would resolve it:** Controlled experiments varying class frequencies in pre-training data while measuring downstream performance, along with theoretical extensions to handle non-uniform distributions.

## Limitations
- Theoretical analysis relies on assumptions that may not hold in practice, including the assumption that pre-training representation error bounds downstream representation error
- Empirical validation is limited to relatively small dataset sizes (N up to 100K), and generalization to much larger pre-training datasets remains untested
- The paper assumes balanced classes in pre-training data, but real-world datasets often have severe class imbalance

## Confidence

- **High**: The empirical observation of the trade-off between intra- and inter-class diversity (Mechanism 2) is well-supported by the experimental results across multiple downstream tasks.
- **Medium**: The theoretical derivation showing the invariant optimal class-to-sample ratio (Mechanism 1) is mathematically sound, but its practical applicability depends on the validity of the assumptions.
- **Medium**: The contrastive case analysis (Mechanism 3) provides valuable insight but is based on a specific data generation process that may not reflect typical pre-training scenarios.

## Next Checks

1. Test the theoretical predictions on larger pre-training datasets (N > 100K) to verify the scalability of the optimal class-to-sample ratio.
2. Validate the assumption that pre-training representation error bounds downstream representation error by comparing the actual pre-training and downstream representation errors across different tasks.
3. Experiment with the contrastive case data generation process using real-world data to assess the practical relevance of the finding that K dominates downstream performance in that scenario.