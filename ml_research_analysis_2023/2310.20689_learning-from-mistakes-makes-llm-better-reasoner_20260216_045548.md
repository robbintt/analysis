---
ver: rpa2
title: Learning From Mistakes Makes LLM Better Reasoner
arxiv_id: '2310.20689'
source_url: https://arxiv.org/abs/2310.20689
tags:
- data
- reasoning
- step
- llms
- lema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes LEarning from MistAkes (LEMA), a method that
  improves mathematical reasoning in large language models (LLMs) by fine-tuning them
  on mistake-correction data pairs. The core idea is to mimic human learning: LLMs
  generate incorrect reasoning paths, which are then corrected by GPT-4 to produce
  informative correction data containing the error step, explanation, and corrected
  solution.'
---

# Learning From Mistakes Makes LLM Better Reasoner

## Quick Facts
- arXiv ID: 2310.20689
- Source URL: https://arxiv.org/abs/2310.20689
- Authors: 
- Reference count: 10
- Key outcome: LLaMA-2-70B achieves 83.5% pass@1 accuracy on GSM8K and 25.0% on MATH with LEMA, outperforming CoT-only fine-tuning

## Executive Summary
This paper proposes LEarning from MistAkes (LEMA), a method that improves mathematical reasoning in LLMs by fine-tuning them on mistake-correction data pairs. The approach mimics human learning by generating incorrect reasoning paths and having GPT-4 correct them, creating informative correction data containing the error step, explanation, and corrected solution. LEMA fine-tunes LLMs on both standard chain-of-thought (CoT) data and this correction data, showing consistent performance improvements across five backbone LLMs and two tasks. The method achieves state-of-the-art performance for non-execution open-source models, with LLaMA-2-70B reaching 85.4% on GSM8K and 27.1% on MATH when combined with specialized models.

## Method Summary
LEMA fine-tunes LLMs on mathematical reasoning tasks by combining standard chain-of-thought data with correction data generated through a multi-step process. First, various LLMs generate reasoning paths for math problems. Incorrect paths are identified and sent to GPT-4, which analyzes the error step, explains why it's wrong, and provides the corrected solution. The fine-tuning pipeline then uses QLoRA to efficiently fine-tune LLMs on both the original CoT data and the new correction data. The method is tested on GSM8K and MATH datasets with five different backbone LLMs, including LLaMA-2-70B, WizardMath, and MetaMath variants.

## Key Results
- LLaMA-2-70B achieves 83.5% pass@1 accuracy on GSM8K (vs 81.4% with CoT alone) and 25.0% on MATH (vs 23.6%)
- LEMA enhances specialized models like WizardMath and MetaMath, achieving state-of-the-art performance for non-execution open-source models (85.4% on GSM8K and 27.1% on MATH)
- Larger models benefit more from learning from mistakes, with consistent improvements across different random seeds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs benefit from fine-tuning on error-correction data because the correction data provides unique explanations of reasoning mistakes that CoT data alone lacks.
- Mechanism: Correction data explicitly identifies the error step, explains why it is wrong, and shows how to fix it. This teaches the model to detect and correct mistakes rather than just memorizing correct reasoning paths.
- Core assumption: The model can learn to generalize from specific error-correction examples to new problems where similar mistakes might occur.
- Evidence anchors:
  - [abstract]: "Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data."
  - [section]: "These experimental results and analyses underscore the potential of learning from mistakes in enhancing the reasoning capabilities of LLMs."
  - [corpus]: Weak. Related papers discuss mistake detection/correction but don't provide direct experimental support for this specific mechanism.

### Mechanism 2
- Claim: Larger LLMs benefit more from learning from mistakes because they have greater capacity to encode and utilize error-correction patterns.
- Mechanism: Bigger models can better leverage the additional information in correction data (error step, explanation, corrected solution) due to their larger parameter space and richer representations.
- Core assumption: Model size correlates with ability to learn from error-correction patterns rather than just correct solutions.
- Evidence anchors:
  - [abstract]: "Further analysis shows that correction data provides unique benefits beyond CoT data, and larger models benefit more from learning from mistakes."
  - [section]: "These consistent improvements demonstrate that the effectiveness of our correction data is robust to the random disturbances during training."
  - [corpus]: Weak. No direct experimental evidence in the corpus about model size effects on learning from mistakes.

### Mechanism 3
- Claim: GPT-4 serves as an effective "world model" that teaches smaller LLMs to follow logical rules rather than just mimicking step-by-step behavior.
- Mechanism: GPT-4 identifies mistakes and provides corrections based on its understanding of logical rules, which smaller models then learn during fine-tuning.
- Core assumption: GPT-4 has superior reasoning capabilities that can be transferred to smaller models through fine-tuning on its corrections.
- Evidence anchors:
  - [section]: "From this perspective, our LEMA framework employs GPT-4 as a 'world model' to teach smaller models in adhering to these logic and rules, rather than merely mimicking the step-by-step behavior."
  - [section]: "Our evaluation finds that 35 out of 50 generated corrections are of excellent quality."
  - [corpus]: Weak. No direct evidence about GPT-4's effectiveness as a world model in the corpus.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT is the baseline reasoning approach that LEMA builds upon. Understanding CoT is essential to grasp how LEMA improves upon it.
  - Quick check question: What is the difference between standard prompting and CoT prompting in LLMs?

- Concept: Fine-tuning vs. prompt engineering
  - Why needed here: LEMA uses fine-tuning on additional data (correction data) rather than just prompt engineering. Understanding this distinction is crucial for implementation.
  - Quick check question: Why might fine-tuning on correction data be more effective than just including correction examples in the prompt?

- Concept: Parameter-efficient fine-tuning (PEFT) methods like QLoRA
  - Why needed here: The paper uses QLoRA for efficient fine-tuning of large models. Understanding PEFT is important for practical implementation.
  - Quick check question: How does QLoRA reduce memory requirements compared to full fine-tuning?

## Architecture Onboarding

- Component map:
  Reasoning Model (Mr) -> Corrector Model (Mc/GPT-4) -> Fine-tuning Pipeline (QLoRA)

- Critical path:
  1. Generate reasoning paths using Mr
  2. Filter out correct paths, keep only inaccurate ones
  3. Use Mc (GPT-4) to generate corrections for inaccurate paths
  4. Filter corrections with correct final answers
  5. Fine-tune LLMs using QLoRA on CoT + correction data

- Design tradeoffs:
  - Using GPT-4 as corrector vs. less powerful models: Higher quality corrections but more expensive
  - Including all correction data vs. controlled data size: More data may help but increases computational cost
  - Fine-tuning vs. prompt engineering: More effective but requires more resources

- Failure signatures:
  - Poor performance improvement: Could indicate low-quality correction data or insufficient model capacity
  - Model degradation: Might suggest the correction data contains too many errors or confusing examples
  - High computational cost: Could indicate inefficient fine-tuning parameters or excessive data size

- First 3 experiments:
  1. Run ablation study comparing CoT-only vs. CoT+correction fine-tuning on a small LLM
  2. Test GPT-4 correction quality by manually evaluating a sample of corrections
  3. Compare performance of different backbone models (e.g., LLaMA-2-7B vs. LLaMA-2-70B) to validate size effect hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms enable larger LLMs to benefit more from learning from mistakes compared to smaller models?
- Basis in paper: [explicit] "Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data" and "Our ablation study shows that LEMA still outperforms CoT-alone fine-tuning under the same data size"
- Why unresolved: The paper mentions that larger models benefit more from learning from mistakes but does not explain the underlying mechanisms that make this possible.
- What evidence would resolve it: Comparative analysis of model architectures, attention patterns, or other internal representations between different-sized models during mistake correction could reveal the specific mechanisms.

### Open Question 2
- Question: How does the quality of correction data impact the performance gains of LEMA across different mathematical domains?
- Basis in paper: [explicit] "Our human evaluation reveals that our correction data exhibits adequate quality for the subsequent fine-tuning stage" and mentions classification of corrections into three quality levels
- Why unresolved: While the paper discusses the quality of correction data and its importance, it does not explore how variations in correction data quality affect performance across different types of mathematical problems.
- What evidence would resolve it: Systematic evaluation of LEMA's performance using correction data of varying quality across different mathematical domains would clarify the relationship between correction data quality and task performance.

### Open Question 3
- Question: Can less powerful models generate correction data that is as effective as that generated by GPT-4?
- Basis in paper: [explicit] "Despite GPT-4, we have also tried leveraging GPT-3.5-Turbo as the corrector model and assess the quality of generated corrections"
- Why unresolved: The paper mentions attempting to use GPT-3.5-Turbo but found it produced lower quality corrections, yet does not explore whether other less powerful models could be effective.
- What evidence would resolve it: Comparative studies using various less powerful models to generate correction data and evaluating their effectiveness in improving LLM reasoning would provide insight into the minimum requirements for effective correction data generation.

## Limitations

- Reliance on GPT-4 as sole corrector model introduces cost and quality control concerns
- Manual evaluation of correction quality is limited in scale and doesn't provide systematic error analysis
- Exact prompts and filtering criteria for correction generation aren't fully specified, affecting reproducibility

## Confidence

High confidence in core empirical findings (performance improvements on GSM8K and MATH), Medium confidence in mechanism explanations (based on qualitative reasoning), Low confidence in scalability claims for largest models (limited comparisons without controlling for baseline differences).

## Next Checks

1. **Quality control audit**: Systematically evaluate a random sample of 100 corrections across different difficulty levels to assess whether GPT-4 consistently identifies the correct error step and provides valid explanations.

2. **Cross-corrector generalization**: Replace GPT-4 with a smaller, more accessible model (e.g., GPT-3.5-Turbo or Claude-3-Haiku) as the corrector and measure performance degradation.

3. **Error type analysis**: Categorize the types of errors being corrected (e.g., arithmetic mistakes, logical fallacies, misread questions) and measure whether the fine-tuned models show differential improvement across error categories.