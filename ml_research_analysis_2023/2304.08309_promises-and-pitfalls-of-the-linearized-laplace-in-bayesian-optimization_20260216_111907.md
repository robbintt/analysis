---
ver: rpa2
title: Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization
arxiv_id: '2304.08309'
source_url: https://arxiv.org/abs/2304.08309
tags:
- bayesian
- optimization
- neural
- laplace
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of linearized Laplace approximation
  (LLA) as a Bayesian neural network method in Bayesian optimization. The authors
  find that LLA can outperform standard Gaussian process baselines, especially in
  tasks requiring strong inductive biases such as image search spaces.
---

# Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization

## Quick Facts
- arXiv ID: 2304.08309
- Source URL: https://arxiv.org/abs/2304.08309
- Reference count: 23
- Key outcome: Linearized Laplace approximation (LLA) outperforms standard GP baselines in Bayesian optimization, especially for tasks requiring strong inductive biases like image search spaces, but exhibits pathological behavior in unbounded domains.

## Executive Summary
This paper investigates the use of linearized Laplace approximation (LLA) as a surrogate model for Bayesian optimization. LLA combines the predictive accuracy of neural networks with the calibrated uncertainties of Gaussian processes by interpreting the linearized neural network as a GP with mean given by the MAP predictive function and covariance induced by the empirical neural tangent kernel. The authors demonstrate that LLA can outperform standard GP baselines, particularly in high-dimensional tasks requiring strong inductive biases such as image generation. However, they also identify significant limitations, including pathological behavior of ReLU networks in unbounded domains and computational challenges related to Jacobian and NTK computation.

## Method Summary
The paper proposes using linearized Laplace approximation (LLA) as a surrogate model for Bayesian optimization. The method involves training a neural network (MLP or CNN) to maximum a posteriori (MAP) on initial data, then computing the Jacobian at the MAP estimate to construct the empirical neural tangent kernel (NTK). This creates a GP whose mean function is the MAP predictive function and whose covariance function is induced by the NTK. The GP hyperparameters are optimized via marginal likelihood using standard deep learning optimizers. Expected Improvement serves as the acquisition function. The method is evaluated on three benchmark functions (Branin, Ackley, and MNIST image generation) and compared against GP-RBF baselines.

## Key Results
- LLA outperforms standard GP-RBF baselines on the high-dimensional MNIST image generation task
- LLA shows competitive performance on low-dimensional benchmark functions (Branin, Ackley)
- LLA exhibits pathological behavior in unbounded domains due to ReLU network extrapolation issues
- The method successfully combines neural network inductive biases with GP uncertainty quantification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLA combines the predictive accuracy of a neural network with the calibrated uncertainties of a Gaussian process.
- Mechanism: LLA constructs a GP whose mean function is the MAP predictive function of the neural network and whose covariance function is induced by the empirical neural tangent kernel (NTK) evaluated at the MAP estimate.
- Core assumption: The linearization of the neural network around the MAP estimate yields a valid GP approximation that inherits both the inductive bias of the NN and the uncertainty quantification of the GP.
- Evidence anchors:
  - [abstract] "it can be seen as a Gaussian process posterior with the mean function given by the neural network's maximum-a-postimum predictive function and the covariance function induced by the empirical neural tangent kernel."
  - [section] "From this perspective, the LLA is a GP with a posterior mean function given by the maximum-a-postimum (MAP) predictive function of the NN and a covariance function given by the NN's empirical neural tangent kernel at the MAP estimate."
  - [corpus] Weak evidence - related papers discuss linearized Laplace but don't directly validate GP-NTK interpretation in BO context.
- Break condition: The linearization assumption breaks down if the neural network is highly non-linear around the MAP estimate, or if the NTK fails to capture the true function space properties relevant to the optimization task.

### Mechanism 2
- Claim: LLA can be tuned via differentiable marginal likelihood using standard deep learning optimizers without requiring validation data.
- Mechanism: Because LLA is formulated as a GP with NN mean function, standard GP marginal likelihood can be differentiated with respect to hyperparameters, and automatic differentiation through the NN enables gradient-based optimization.
- Core assumption: The MAP-estimated NN parameters are fixed during marginal likelihood optimization, and the NTK can be computed efficiently enough for gradient-based tuning.
- Evidence anchors:
  - [abstract] "Just like any GP, the LLA can be tuned via its differentiable marginal likelihood using standard deep learning optimizers without validation data"
  - [section] "Unlike standard GP models, however, the LLA is much more expressive and accurate due to its NN backbone."
  - [corpus] Weak evidence - related work mentions marginal likelihood optimization but doesn't specifically address hyperparameter tuning in BO context.
- Break condition: If the NTK computation becomes too expensive for large networks, or if the fixed NN backbone limits the expressiveness needed for the optimization landscape.

### Mechanism 3
- Claim: LLA outperforms standard GP baselines in problems requiring strong inductive biases, such as image search spaces.
- Mechanism: The NN backbone of LLA encodes complex feature representations that standard RBF kernels cannot capture, leading to better predictive accuracy and thus better acquisition function optimization in high-dimensional, structured spaces.
- Core assumption: The task-specific inductive bias encoded in the NN architecture (e.g., CNN for images) translates to improved surrogate modeling in the optimization problem.
- Evidence anchors:
  - [abstract] "especially in tasks requiring strong inductive biases such as image search spaces"
  - [section] "On the high-dimensional MNIST problem, GP-RBF performs similarly (bad) as the random search baseline. Meanwhile, the LLA finds an x with small f(x) in a few iterations."
  - [corpus] Weak evidence - related papers discuss linearized Laplace performance but don't specifically validate on image-based BO tasks.
- Break condition: If the inductive bias encoded in the NN is misaligned with the true optimization landscape, or if the search space doesn't benefit from structured representations.

## Foundational Learning

- Concept: Gaussian Process Regression and Bayesian Optimization
  - Why needed here: LLA is interpreted as a GP, and the entire evaluation framework uses GP-based BO. Understanding GP priors, kernels, posterior inference, and acquisition functions is essential.
  - Quick check question: What is the role of the kernel function in GP regression, and how does it affect uncertainty estimates?

- Concept: Laplace Approximation and Linearized Models
  - Why needed here: LLA builds directly on Laplace approximation but linearizes the neural network first. Understanding how Laplace approximates posterior distributions and how linearization affects this is crucial.
  - Quick check question: How does the Hessian of the log-posterior relate to the covariance of the Laplace approximation?

- Concept: Neural Tangent Kernel and its properties
  - Why needed here: The NTK defines the covariance function of LLA. Understanding its behavior, especially in ReLU networks and unbounded domains, is key to understanding both the strengths and pitfalls.
  - Quick check question: What is the behavior of the NTK for ReLU networks outside the data region, and why does this matter for unbounded optimization?

## Architecture Onboarding

- Component map:
  Neural Network (MAP training) -> Jacobian computation -> Neural Tangent Kernel construction -> GP mean/covariance function -> Marginal likelihood optimization -> Expected Improvement acquisition -> Bayesian optimization loop

- Critical path:
  1. Train NN to MAP on initial data
  2. Compute Jacobians and NTK at MAP
  3. Optimize GP hyperparameters via marginal likelihood
  4. Optimize acquisition function to propose next evaluation point
  5. Evaluate true objective at proposed point
  6. Add data and repeat

- Design tradeoffs:
  - NN architecture choice: deeper/more complex NNs may encode better inductive biases but increase Jacobian computation cost
  - Linearization validity: approximation quality depends on how linear the NN is around MAP
  - NTK computation: full NTK requires O(nÂ²d) memory, where n is data points and d is parameters
  - Marginal likelihood optimization: online vs post-hoc tuning tradeoffs in computational cost vs adaptation

- Failure signatures:
  - Poor exploration in unbounded domains (due to ReLU pathological extrapolation)
  - Acquisition function always proposing points far from data
  - Marginal likelihood optimization failing to converge
  - NTK computation running out of memory for large datasets

- First 3 experiments:
  1. Run LLA on a simple 1D synthetic function (e.g., Branin) with small dataset to verify GP-NTK interpretation and compare to GP-RBF
  2. Test LLA on MNIST image generation task to validate strong inductive bias benefits in high dimensions
  3. Evaluate LLA performance on an unbounded 1D function with ReLU network to observe pathological extrapolation behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the performance of linearized Laplace approximation (LLA) in sequential decision-making problems beyond Bayesian optimization, such as active learning and bandits?
- Basis in paper: [explicit] The authors state that while LLA's efficacy has been studied in large-scale tasks like image classification, it has not been studied in sequential decision-making problems like Bayesian optimization.
- Why unresolved: The paper focuses on Bayesian optimization as a case study for LLA's performance in sequential decision-making problems. The authors suggest that it is interesting to study LLA's performance in this regime, but they do not provide empirical evidence for other sequential decision-making problems.
- What evidence would resolve it: Conducting empirical studies on LLA's performance in other sequential decision-making problems, such as active learning and bandits, would provide evidence to answer this question.

### Open Question 2
- Question: How can the potential pitfalls of LLA in unbounded domains be addressed or mitigated?
- Basis in paper: [explicit] The authors discuss potential pitfalls of LLA in unbounded domains due to the pathological behavior of ReLU networks' MAP predictive functions and suggest possible solutions, such as better architectural design and acquisition functions that account for the MAP-estimated network's behavior.
- Why unresolved: The authors mention potential solutions but do not provide empirical evidence or detailed analysis of how these solutions would address the pitfalls.
- What evidence would resolve it: Implementing and evaluating the suggested solutions in unbounded domains, and comparing their performance to LLA without these solutions, would provide evidence to answer this question.

### Open Question 3
- Question: How does the performance of LLA in small-sample regimes compare to other Bayesian neural network methods, such as variational inference and Markov Chain Monte Carlo (MCMC)?
- Basis in paper: [explicit] The authors state that the performance of LLA in small-sample regimes has not been studied extensively, but they provide empirical evidence of LLA's effectiveness in Bayesian optimization with small sample sizes.
- Why unresolved: The paper focuses on LLA's performance in Bayesian optimization and does not compare it to other Bayesian neural network methods in small-sample regimes.
- What evidence would resolve it: Conducting empirical studies comparing LLA's performance to other Bayesian neural network methods, such as variational inference and MCMC, in small-sample regimes would provide evidence to answer this question.

## Limitations

- Limited empirical evidence of failure modes beyond qualitative descriptions of ReLU pathological behavior in unbounded domains
- No comprehensive comparison against modern BO methods beyond basic GP-RBF baseline
- Limited ablation studies on architectural choices and their impact on LLA performance
- NTK computation complexity scalability concerns not thoroughly investigated

## Confidence

- High confidence in the theoretical foundation linking LLA to GP-NTK interpretation
- Medium confidence in the empirical performance claims due to limited baseline comparisons
- Medium confidence in the identified limitations, though evidence could be more comprehensive

## Next Checks

1. Conduct systematic experiments varying NN depth and width to quantify the impact on linearization quality and NTK computation cost
2. Implement and evaluate alternative acquisition functions that explicitly account for the pathological extrapolation behavior of ReLU networks
3. Compare LLA against modern BO methods like Bayesian neural networks with variational inference or ensemble methods to establish relative performance across different problem types