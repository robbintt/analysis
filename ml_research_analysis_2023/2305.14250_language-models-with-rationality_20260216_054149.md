---
ver: rpa2
title: Language Models with Rationality
arxiv_id: '2305.14250'
source_url: https://arxiv.org/abs/2305.14250
tags:
- belief
- beliefs
- graph
- system
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REFLEX, a system that augments large language
  models (LLMs) with a rational, self-reflecting layer to expose and improve the consistency
  of the model's beliefs. REFLEX constructs a belief graph by recursively generating
  rules that conclude candidate answers and their supporting statements, then uses
  a constraint reasoner to identify and resolve contradictions in this graph.
---

# Language Models with Rationality

## Quick Facts
- arXiv ID: 2305.14250
- Source URL: https://arxiv.org/abs/2305.14250
- Reference count: 15
- Primary result: REFLEX improves consistency by 8%-11% absolute across three datasets without harming accuracy

## Executive Summary
REFLEX augments LLMs with a rational self-reflecting layer to improve belief consistency and answer interpretability. The system constructs belief graphs through backward-chaining inference, then uses constraint satisfaction optimization to resolve contradictions. This approach significantly improves internal consistency while maintaining accuracy, producing answers supported by faithful chains of reasoning. The method represents a new architecture for making LLM reasoning more transparent and reliable.

## Method Summary
REFLEX operates in two phases: first, it constructs a belief graph for each question by recursively generating entailment rules using a frozen T5-11B LLM, then labels each statement with the model's confidence in its truth. Second, it converts this graph to a weighted MaxSAT problem and uses a constraint solver to find the optimal assignment of truth values that minimizes contradictions. The system then selects answers based on the solved belief graph, ensuring they are supported by internally consistent reasoning chains.

## Key Results
- Consistency improves by 8%-11% absolute across EntailmentBank, OBQA, and QuaRTz datasets
- Answer accuracy remains stable while explanations become more faithful to the reasoning chains
- The approach successfully exposes and resolves latent inconsistencies in the LLM's belief system

## Why This Works (Mechanism)

### Mechanism 1
Recursive backward-chaining generates belief graphs that expose latent model beliefs and their inferential relationships. The LLM starts from candidate answers and recursively generates premises that could entail those answers, building a graph of statements connected by rules, then labels each statement with the model's confidence in its truth via self-querying.

### Mechanism 2
Constraint satisfaction optimization identifies minimal belief flips needed to resolve contradictions in the belief graph. The belief graph is converted to a weighted MaxSAT problem where violated rules incur costs based on the model's confidence in them; the solver finds the assignment of T/F labels to statements that minimizes total cost, effectively repairing inconsistencies.

### Mechanism 3
Self-consistency improvements transfer to better answer support without harming accuracy. By resolving latent contradictions in the belief system before selecting answers, the system ensures answers are supported by internally consistent chains of reasoning drawn from beliefs the model actually holds.

## Foundational Learning

- **Factor graphs and probabilistic graphical models**: The belief graph is formally defined as a factor graph where statements are variable nodes and rules are factor nodes, with well-defined probability distributions over assignments. *Quick check*: In a factor graph, what do the weights of variable nodes and factor nodes represent in terms of belief strength?

- **MaxSAT constraint solving and weighted optimization**: The system uses MaxSAT to find the minimum-cost assignment of truth values that satisfies as many constraints as possible, where costs are based on the model's confidence in beliefs and rules. *Quick check*: How does the weighted MaxSAT formulation ensure that more confident beliefs are less likely to be flipped during optimization?

- **Backward-chaining inference and entailment generation**: The belief graph construction relies on recursively generating premises that could entail target statements, requiring understanding of how to prompt LLMs for rule generation. *Quick check*: What is the relationship between the depth limit dmax and the completeness of the belief graph coverage?

## Architecture Onboarding

- **Component map**: LLM (frozen) -> Belief Graph Generator (backward-chaining with Entailer) -> Self-Querying Module (belief labeling) -> Constraint Solver (MaxSAT optimization) -> Answer Selector (MC constraint + highest-confidence answer)
- **Critical path**: Question -> Hypothesis Generation -> Belief Graph Construction -> Constraint Solving -> Answer Selection
- **Design tradeoffs**: Deeper graphs capture more beliefs but increase computational cost and risk of irrelevant generations; higher confidence thresholds for rules make the system more conservative but may miss valid contradictions
- **Failure signatures**: Inconsistent answers across similar questions (beliefs not shared globally), low consistency scores despite accurate answers (solver not fixing real issues), high computational latency (graph depth or node count too large)
- **First 3 experiments**:
  1. Measure consistency improvement on EntailmentBank dev set with varying dmax values (1-5) to find sweet spot
  2. Ablation study: Remove each rule type (P→H, XOR, MC) separately and measure consistency impact
  3. Qualitative analysis: Manually examine 20 questions where answers changed, categorize reasoning types (correct flip, incorrect flip, rejected rule, etc.)

## Open Questions the Paper Calls Out

### Open Question 1
How does REFLEX handle cases where multiple answers are valid, and the question is implicitly asking for the best answer? The paper mentions that REFLEX can find valid reasoning chains for multiple valid answers, but the notion of highest-scoring proof doesn't fully correlate with the notion of "best answer" intended by the question author.

### Open Question 2
Can REFLEX be extended to handle global/persistent belief graphs, rather than constructing them on a per-question basis? The paper mentions that belief graphs are constructed on a per-question basis, but there is no notion of a global belief graph.

### Open Question 3
How does REFLEX handle ambiguous statements, and can it be improved to better disambiguate them? The paper mentions that a common cause of error is the surprising ambiguity of belief statements, which can often be read in multiple ways.

## Limitations
- Relies heavily on LLM's ability to generate meaningful entailment rules and accurately self-assess beliefs, which may not generalize across domains
- Computational cost of constructing deep belief graphs and running MaxSAT optimization is not characterized
- Evaluation focuses on consistency and accuracy but does not measure faithfulness of explanations or novel reasoning task performance

## Confidence
- **High confidence** in the core mechanism: Constraint satisfaction for resolving contradictions is well-established
- **Medium confidence** in the backward-chaining approach: Concept is sound but implementation details are lacking
- **Low confidence** in generalization: Limited to three multiple-choice datasets with no evidence for broader task types

## Next Checks
1. Ablation on rule types: Systematically remove each rule type (P→H, XOR, MC) and measure the impact on consistency
2. Cross-domain transfer: Evaluate REFLEX on a dataset from a different domain (e.g., mathematical reasoning) to assess generalization
3. Computational efficiency analysis: Measure wall-clock time and memory usage for belief graph construction and MaxSAT solving across varying depths