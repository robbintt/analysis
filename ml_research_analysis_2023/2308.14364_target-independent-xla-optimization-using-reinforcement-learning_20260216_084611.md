---
ver: rpa2
title: Target-independent XLA optimization using Reinforcement Learning
arxiv_id: '2308.14364'
source_url: https://arxiv.org/abs/2308.14364
tags:
- learning
- optimization
- compiler
- deep
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of optimizing compiler pass ordering
  for XLA HLO (High-Level Operations) graphs in machine learning compilers. The authors
  propose using deep Reinforcement Learning (RL) to find optimal sequences of compiler
  optimization passes, which is decoupled from target-dependent optimization.
---

# Target-independent XLA optimization using Reinforcement Learning

## Quick Facts
- arXiv ID: 2308.14364
- Source URL: https://arxiv.org/abs/2308.14364
- Reference count: 40
- Primary result: 13.3% average improvement in operation count reduction on GPT-2 training graphs using RL for XLA pass ordering

## Executive Summary
This paper addresses the challenge of optimizing compiler pass ordering for XLA HLO graphs in machine learning compilers by proposing a deep Reinforcement Learning approach. The authors develop the XLA Gym framework that enables RL algorithms to interact with the XLA compiler environment, allowing agents to learn optimal sequences of 53 HLO compiler optimization passes. Using PPO with domain-specific enhancements including reward shaping based on HLO cost analysis features, the approach achieves significant improvements over default compiler pass orderings.

## Method Summary
The paper presents a target-independent approach to XLA optimization using deep Reinforcement Learning. The method involves creating an XLA Gym framework that converts compiler optimization into a Markov Decision Process, where states represent operation count features, actions represent discrete pass selections, and rewards are based on optimization progress. The core algorithm uses PPO with domain-specific enhancements including potential-based reward shaping using FLOP and transcendental count features. The approach is trained on GPT-2 training graphs and evaluated across diverse benchmarks including GPT-2, BERT, and ResNet architectures.

## Key Results
- 13.3% average improvement in operation count reduction on GPT-2 training graphs
- 10.4% improvement in operation count reduction across diverse benchmarks (GPT-2, BERT, ResNet)
- 1.0291× average speedup in execution time
- PPO with reward shaping performs best among tested RL algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning agents can discover superior XLA HLO pass orderings compared to compiler defaults
- Mechanism: The RL agent explores sequences of 53 HLO compiler optimization passes through trial-and-error in a simulated environment, learning policies that maximize rewards based on operation count reduction. The PPO algorithm with domain-specific enhancements guides this search more effectively than traditional heuristics.
- Core assumption: The state representation (XLA operation count features) contains sufficient information for the agent to make optimal pass selection decisions
- Evidence anchors:
  - [abstract] "we observe an average of 13.3% improvement in operation count reduction on a benchmark of GPT-2 training graphs"
  - [section] "Overall, PPO performs the best with an average of 13.3% improvement in XLA operation count reduction over the default HLO passes"
  - [corpus] Weak - corpus papers focus on LLVM phase ordering and other ML compiler topics, not XLA-specific RL approaches
- Break condition: If the state representation becomes too sparse or fails to capture critical graph properties that influence optimization effectiveness

### Mechanism 2
- Claim: Domain-specific reward shaping using HLO cost analysis features improves RL agent performance
- Mechanism: By incorporating FLOP count and transcendental count into the reward function through potential-based shaping, the agent receives more informative feedback about the quality of pass sequences beyond just raw operation count
- Core assumption: HLO cost analysis features correlate strongly with the ultimate optimization objective (execution time or operation count)
- Evidence anchors:
  - [section] "we transform our initial MDP M in XLA Gym to a new one M′ with provably same optimal policies by introducing a potential-based shaping function"
  - [section] "Overall, PPO with the shaping potential does best in comparison to plain PPO"
  - [corpus] Weak - corpus lacks specific evidence about reward shaping in ML compiler optimization contexts
- Break condition: If the shaped rewards create misleading gradients that cause the agent to converge to suboptimal policies

### Mechanism 3
- Claim: The XLA Gym framework successfully converts compiler optimization into a Markov Decision Process
- Mechanism: By defining states as arrays of operation counts, actions as discrete pass selections, and rewards as negative scaled operation counts, the framework creates a well-defined environment where RL algorithms can systematically explore optimization strategies
- Core assumption: The compiler's response to pass sequences can be modeled as a deterministic or stochastic transition function
- Evidence anchors:
  - [section] "Using the OpenAI Gym structure, we develop XLA Gym environments suitable for various use cases"
  - [section] "Specifically, this requires defining and engineering the following: State, Action, Reward, Info, Reset, Step, Render"
  - [corpus] Weak - corpus papers don't describe similar Gym-based frameworks for ML compilers
- Break condition: If the environment's state transitions become too complex or non-stationary for the RL agent to learn reliable policies

## Foundational Learning

- Concept: Markov Decision Processes
  - Why needed here: The paper explicitly formulates XLA pass ordering as an MDP with states, actions, transitions, and rewards
  - Quick check question: What are the five components of an MDP and how does each map to the XLA optimization problem?

- Concept: Reinforcement Learning algorithms (PPO, DQN, A2C)
  - Why needed here: Different RL algorithms are compared for their effectiveness at discovering optimal pass sequences
  - Quick check question: What distinguishes on-policy from off-policy RL algorithms, and why might this matter for compiler optimization?

- Concept: Reward shaping and potential functions
  - Why needed here: The paper proposes enhancements using domain knowledge through potential-based reward shaping
  - Quick check question: What mathematical condition must a reward shaping function satisfy to preserve optimal policies?

## Architecture Onboarding

- Component map:
  XLA compiler backend -> XLA Gym environment -> RL agent (PPO) -> Reward function -> State extractor

- Critical path:
  1. Initialize environment with unoptimized graph
  2. Agent selects next optimization pass
  3. Environment applies pass and returns new state
  4. Reward function computes feedback signal
  5. Agent updates policy based on trajectory

- Design tradeoffs:
  - State representation vs. computational overhead (using operation counts is efficient but may miss structural information)
  - Reward granularity vs. stability (detailed cost features provide better guidance but may introduce noise)
  - Exploration vs. exploitation balance (affects convergence speed and solution quality)

- Failure signatures:
  - Agent converges to degenerate policies (e.g., always selecting the same pass)
  - Training instability or divergence in PPO loss curves
  - Minimal improvement over default pass orderings across benchmarks

- First 3 experiments:
  1. Run PPO agent on a simple single-layer neural network graph to verify basic functionality
  2. Compare PPO with random pass selection on medium-complexity benchmarks
  3. Test reward shaping with different potential function parameters to find optimal configuration

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several important unresolved issues emerge from the research:

### Open Question 1
- Question: How does the XLA Gym framework handle cases where compiler optimization passes conflict with each other or produce non-optimal results when combined in certain sequences?
- Basis in paper: [inferred] The paper mentions using 53 HLO compiler optimization passes but doesn't discuss how conflicts between passes are managed or how the framework prevents suboptimal pass combinations.
- Why unresolved: The paper focuses on the RL framework and results but doesn't provide details about pass conflict resolution or conflict detection mechanisms.
- What evidence would resolve it: Detailed explanation of conflict detection/prevention mechanisms in the XLA Gym framework, or experimental results showing how the system handles conflicting pass sequences.

### Open Question 2
- Question: What is the impact of training data diversity on the RL agent's ability to generalize across different model architectures beyond the tested GPT-2, BERT, and ResNet?
- Basis in paper: [explicit] The paper mentions using "more than 300 graphs coming from models such as GPT-2, BERT, ResNet" but doesn't analyze the relationship between training data diversity and generalization performance.
- Why unresolved: The paper shows results on specific model types but doesn't explore how the diversity of training data affects performance on completely different architectures.
- What evidence would resolve it: Experiments testing the RL agent on model architectures not present in the training data, with analysis of performance correlation to training data diversity.

### Open Question 3
- Question: How does the reward shaping function based on FLOP and transcendental counts compare to alternative potential functions that could incorporate other HLO cost features?
- Basis in paper: [explicit] The paper describes using ϕ(s) = -FLOP_count(s) - 2*transcendental_count(s) but doesn't compare this to other possible shaping functions.
- Why unresolved: The paper presents one specific reward shaping function without exploring alternatives or providing justification for why this particular function is optimal.
- What evidence would resolve it: Comparative analysis of different potential functions incorporating various HLO cost features and their impact on RL agent performance.

## Limitations

- The evaluation primarily focuses on operation count reduction rather than actual execution time improvements, though execution time speedup is reported
- The state representation using only operation counts may miss critical structural information that influences optimization effectiveness
- The generalization across different ML model architectures and varying graph complexities remains to be fully validated

## Confidence

- **High Confidence**: The XLA Gym framework successfully converts compiler optimization into an MDP that RL algorithms can interact with effectively
- **Medium Confidence**: The 13.3% average improvement in operation count reduction is reproducible and generalizes across diverse ML benchmarks
- **Low Confidence**: The shaped reward function consistently improves learning efficiency across all benchmark types and remains stable with different HLO cost analysis features

## Next Checks

1. Test the RL agent's performance on dynamic computational graphs with varying input shapes to assess robustness to graph complexity variations
2. Implement ablation studies removing the reward shaping component to quantify its contribution to performance improvements
3. Evaluate the approach on production-scale models (e.g., large language models beyond GPT-2) to verify scalability and identify potential bottlenecks in the training process