---
ver: rpa2
title: Automatic Answerability Evaluation for Question Generation
arxiv_id: '2309.12546'
source_url: https://arxiv.org/abs/2309.12546
tags:
- questions
- question
- metric
- evaluation
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of evaluating whether generated
  questions are answerable by reference answers in question generation tasks. It proposes
  PMAN, a novel automatic metric that leverages ChatGPT's reasoning capabilities through
  Chain-of-Thought prompting to assess answerability.
---

# Automatic Answerability Evaluation for Question Generation

## Quick Facts
- arXiv ID: 2309.12546
- Source URL: https://arxiv.org/abs/2309.12546
- Reference count: 0
- Primary result: PMAN achieves state-of-the-art performance in generating answerable questions

## Executive Summary
This paper addresses the challenge of automatically evaluating whether generated questions are answerable by reference answers in question generation tasks. The authors propose PMAN (Prompting-based Metric on Answerability), a novel automatic metric that leverages ChatGPT's reasoning capabilities through Chain-of-Thought prompting. Experiments demonstrate that PMAN provides reliable assessments aligned with human evaluations, particularly for non-"yes/no" type questions, and shows low correlation with traditional n-gram overlap metrics, indicating its complementary nature.

## Method Summary
PMAN uses Chain-of-Thought prompting with ChatGPT to assess question answerability through a three-step process: first answering the question itself, then comparing its answer to the reference answer, and finally giving a binary assessment. The final PMAN score is calculated as the percentage of questions deemed answerable ("YES" responses). The metric is evaluated on the HotpotQA dataset for multi-hop question generation, comparing against traditional metrics like BLEU, ROUGE, and METEOR. A ChatGPT-based question generation model is also implemented and evaluated using PMAN.

## Key Results
- PMAN achieves 85% accuracy in human evaluation alignment for non-"yes/no" questions
- PMAN shows low correlation with traditional n-gram overlap metrics, indicating complementary evaluation capability
- ChatGPT-based QG model achieves state-of-the-art performance in generating answerable questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought prompting enables ChatGPT to perform step-by-step reasoning that aligns with human judgment of answerability
- Mechanism: The prompt forces ChatGPT to first answer the question itself, then compare its answer to the reference answer, and finally give a binary assessment, mimicking human evaluation process
- Core assumption: ChatGPT's reasoning capability is sufficient to understand both the question and reference answer in context
- Evidence anchors:
  - [abstract] "PMAN leverages the high reasoning and instruction-following capabilities of ChatGPT through a Chain-of-Thought (CoT) prompting"
  - [section 3.1] "We adopt the CoT prompting by providing step-by-step instructions asking ChatGPT to: 1) Answer the question by itself; 2) Compare its own answer to the reference answer; 3) Give the final assessment"
- Break condition: If ChatGPT cannot generate a valid answer to the question itself, the assessment process fails

### Mechanism 2
- Claim: PMAN achieves better alignment with human evaluation than traditional n-gram overlap metrics
- Mechanism: By focusing on whether answers match rather than surface text similarity, PMAN captures semantic answerability that BLEU/ROUGE miss
- Core assumption: Human evaluation of answerability correlates with whether a generated question's answer matches the reference answer
- Evidence anchors:
  - [abstract] "Experiments demonstrate that PMAN provides reliable assessments aligned with human evaluations"
  - [section 4.2] "The experimental results in Table 4 demonstrate that metric assessments align with human evaluations for 'non-yes/no' type questions"
- Break condition: If human evaluation criteria differ significantly from the binary "YES/NO" assessment format

### Mechanism 3
- Claim: PMAN is complementary to conventional metrics rather than redundant
- Mechanism: PMAN measures answerability while BLEU/ROUGE measure surface similarity, capturing different aspects of question quality
- Core assumption: Answerability and surface similarity are independent properties that both matter for question quality
- Evidence anchors:
  - [abstract] "When applied to evaluate question generation models, PMAN shows low correlation with traditional n-gram overlap metrics, indicating its complementary nature"
  - [section 5.5] "The PMAN scores don't correlate with scores measured by overlap-based metrics, especially in the case of ChatGPT"
- Break condition: If questions that score high on BLEU/ROUGE consistently score high on PMAN (or vice versa), the metrics would be redundant

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: Enables ChatGPT to perform multi-step reasoning required for answerability assessment
  - Quick check question: What are the three steps in PMAN's CoT prompting?

- Concept: Question Generation task requirements
  - Why needed here: Understanding that QG requires generating questions answerable by reference answers
  - Quick check question: What is the fundamental requirement that conventional metrics like BLEU fail to assess?

- Concept: Binary classification evaluation
  - Why needed here: PMAN outputs binary "YES/NO" assessments that need to be interpreted and aggregated
  - Quick check question: How is the final PMAN score calculated from individual assessments?

## Architecture Onboarding

- Component map: Question, reference answer, passage context → Chain-of-Thought prompt to ChatGPT → Binary "YES/NO" assessment → PMAN Score
- Critical path: Question → CoT Prompt → ChatGPT → Binary Assessment → PMAN Score
- Design tradeoffs:
  - Accuracy vs. cost: Using GPT-3.5 vs GPT-4
  - Strictness vs. coverage: Temperature 0 vs higher temperatures for invalid responses
  - Binary vs. graded assessment: Simpler but less nuanced than continuous scores
- Failure signatures:
  - Low accuracy on "yes/no" type questions
  - Invalid responses requiring temperature adjustment
  - Correlation with n-gram metrics indicating redundancy
- First 3 experiments:
  1. Test PMAN on manually created answerable vs unanswerable questions
  2. Compare PMAN scores across different QG models (EQG, SQG, CQG, ChatGPT)
  3. Validate PMAN-human alignment on model-generated questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can PMAN be adapted to effectively assess "yes/no" type questions with high accuracy?
- Basis in paper: [explicit] The paper notes that PMAN demonstrates low accuracy in assessing "yes/no" type questions, which remains an unresolved direction for future research.
- Why unresolved: The current PMAN approach struggles with questions requiring higher reasoning abilities, as these cannot be resolved by merely extracting named entities from the passage.
- What evidence would resolve it: Developing and testing a modified version of PMAN that incorporates additional reasoning steps or different prompting strategies specifically for "yes/no" questions, then measuring its accuracy compared to human evaluations.

### Open Question 2
- Question: What is the optimal balance between conventional n-gram overlap metrics and PMAN for comprehensive evaluation of question generation models?
- Basis in paper: [explicit] The paper shows that PMAN scores don't correlate with overlap-based metrics and suggests PMAN could be complementary to them.
- Why unresolved: While the paper demonstrates low correlation, it doesn't provide guidance on how to combine these metrics for optimal evaluation or determine when each metric is most appropriate.
- What evidence would resolve it: Empirical studies comparing various combinations of conventional metrics and PMAN across different question generation tasks and model types, measuring which combinations best predict human judgment.

### Open Question 3
- Question: How does PMAN perform across different languages and domains beyond English Wikipedia passages?
- Basis in paper: [inferred] The paper only tests PMAN on English HotpotQA dataset with Wikipedia passages, leaving its generalizability unexplored.
- Why unresolved: The paper doesn't investigate PMAN's performance with non-English text, different knowledge domains, or passages with different writing styles or complexity levels.
- What evidence would resolve it: Systematic evaluation of PMAN across multiple languages, domains (e.g., medical, legal, scientific texts), and passage types, measuring reliability and alignment with human evaluations in each context.

## Limitations

- PMAN shows significantly lower accuracy (68%) for "yes/no" type questions compared to 85% for other question types
- The evaluation relies on GPT-3.5-turbo with temperature adjustments needed for invalid responses, affecting reliability
- The complementary relationship with n-gram metrics is demonstrated through correlation only, without deeper analysis of what specific aspects each metric captures

## Confidence

- High confidence in PMAN's effectiveness for non-"yes/no" questions, supported by direct human evaluation comparisons and multiple experimental validations
- Medium confidence in the overall claim of PMAN being a reliable automatic metric, given the temperature adjustment requirements and unknown frequency of invalid responses
- Low confidence in PMAN's performance on "yes/no" questions, with accuracy dropping to 68% and no clear explanation for this limitation

## Next Checks

1. Conduct controlled experiments specifically testing PMAN on diverse "yes/no" question types to identify the source of the 68% accuracy limitation and determine if prompt engineering can improve performance

2. Measure the frequency of invalid responses requiring temperature adjustment across different question types and assess the impact on overall reliability and reproducibility

3. Perform ablation studies comparing PMAN against human evaluation on the same question sets to quantify the exact alignment between automated and human assessments, particularly for edge cases where traditional metrics fail