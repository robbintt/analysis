---
ver: rpa2
title: 'Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement'
arxiv_id: '2305.14497'
source_url: https://arxiv.org/abs/2305.14497
tags:
- reasoning
- language
- problem
- problems
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving multi-step reasoning
  in large language models, particularly when dealing with poorly formulated or low-quality
  reasoning problems. The authors propose a novel method called Self-Polish (SP) that
  enhances reasoning performance by progressively refining the given problems to make
  them more comprehensible and solvable for the model.
---

# Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement

## Quick Facts
- arXiv ID: 2305.14497
- Source URL: https://arxiv.org/abs/2305.14497
- Reference count: 21
- Key outcome: Self-Polish improves reasoning performance by progressively refining problems to be more comprehensible and solvable, achieving gains such as 8.0% boost on GSM8K and 17.8% on MultiArith

## Executive Summary
This paper addresses the challenge of improving multi-step reasoning in large language models, particularly when dealing with poorly formulated or low-quality reasoning problems. The authors propose Self-Polish (SP), a novel method that enhances reasoning performance by progressively refining the given problems to make them more comprehensible and solvable for the model. SP is orthogonal to existing prompting methods like Chain-of-Thought and can be seamlessly integrated with them. The method involves in-context problem refining using demonstrations to teach the model to eliminate irrelevant information, rearrange logic, and organize conditions in parallel. Experiments on five reasoning benchmarks demonstrate that SP consistently improves performance across different models.

## Method Summary
Self-Polish is a method for improving reasoning in large language models by progressively refining problems through iterative problem reformulation. The approach uses in-context learning with demonstration examples to teach the model to generate improved problem formulations by eliminating irrelevant information, rearranging logic, and organizing conditions in parallel. The refinement process continues iteratively until answers converge or a maximum iteration count is reached. SP is designed to be orthogonal to existing prompting methods like Chain-of-Thought, allowing for seamless integration with state-of-the-art techniques for further improvement.

## Key Results
- SP achieves an 8.0% boost on GSM8K and 17.8% on MultiArith using Text-davinci-003
- The method consistently improves reasoning performance across multiple models and datasets
- SP demonstrates impressive robustness when combined with other prompting strategies like Chain-of-Thought and Least-to-Most
- SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-Polish improves reasoning by progressively refining problems to be more comprehensible and solvable.
- Mechanism: The model generates new problem formulations through in-context learning, eliminating irrelevant information, rearranging logic, and summarizing local conditions. This iterative refinement continues until the answer converges or a maximum iteration count is reached.
- Core assumption: The model's in-context learning ability is sufficient to understand and apply the problem-refining patterns demonstrated in the prompts.
- Evidence anchors:
  - [abstract] "Self-Polish (SP), a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable."
  - [section 3.2] "We present several principles for refined problems: concise, clarity, well-focused, and absent of irrelevant information."
- Break condition: If the model fails to generate meaningful refinements or gets stuck in an infinite loop without converging, the method would break down.

### Mechanism 2
- Claim: Self-Polish is orthogonal to existing prompting methods like Chain-of-Thought and can be combined with them for further improvement.
- Mechanism: The problem-refining stage (Self-Polish) and the problem-solving stage (e.g., Chain-of-Thought) are executed alternatively. Self-Polish enhances the quality of the input problems, which in turn improves the effectiveness of downstream reasoning methods.
- Core assumption: Improving problem formulation directly impacts the model's ability to generate correct rationales and answers in subsequent reasoning steps.
- Evidence anchors:
  - [abstract] "SP is orthogonal to all other prompting methods of answer/reasoning side like CoT, allowing for seamless integration with state-of-the-art techniques for further improvement."
  - [section 3.1] "Self-Polish (SP) is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement."
- Break condition: If the improved problem formulations do not lead to better rationales or answers in the combined prompting method, the orthogonality benefit would be lost.

### Mechanism 3
- Claim: Progressive refinement enhances the consistency and reliability of the generated problems.
- Mechanism: By iteratively refining problems until a return condition is met (answer convergence or max iterations), the method ensures that the final problem formulation is optimized for comprehension and processing by the model.
- Core assumption: Iterative refinement leads to better problem formulations than a single refinement step.
- Evidence anchors:
  - [section 3.2] "To further enhance the reliability and consistency of the generated problems, we propose to progressively refine the problems until obtaining a convergent answer."
  - [section 4.2] "Our method consistently improves reasoning performance across multiple models and datasets, indicating its capability to enhance model understanding of problems and consequently provide better responses."
- Break condition: If additional refinement iterations do not lead to significant improvements or if they introduce noise, the progressive refinement mechanism would break down.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Self-Polish relies on the model's ability to learn from demonstrations provided in the prompt without any parameter updates. The model needs to understand and apply the problem-refining patterns from the examples.
  - Quick check question: Can the model correctly generate a new problem formulation after being shown a few examples of original and refined problems?

- Concept: Chain-of-Thought prompting
  - Why needed here: Self-Polish is designed to work with Chain-of-Thought and other prompting methods. Understanding how CoT works is essential to grasp how Self-Polish can enhance its performance by improving problem formulations.
  - Quick check question: What is the main difference between standard few-shot prompting and Chain-of-Thought prompting in terms of the model's output?

- Concept: Iterative algorithms and convergence
  - Why needed here: Self-Polish uses an iterative process to refine problems until a convergence condition is met. Understanding the concept of convergence and the potential for infinite loops is crucial for implementing and debugging the method.
  - Quick check question: What could be the consequences of setting the maximum iteration count too low or too high in the Self-Polish method?

## Architecture Onboarding

- Component map: Problem refinement module -> Problem-solving module -> Convergence checker -> Prompt construction module
- Critical path: Receive original problem -> Construct initial problem-refining prompt with demonstrations -> Generate refined problem -> Construct problem-solving prompt with refined problem -> Generate answer using chosen prompting method -> Check for convergence (answer consistency or max iterations) -> If not converged, go back to step 2 with the refined problem -> Return final answer
- Design tradeoffs:
  - Iteration count vs. efficiency: More iterations can lead to better problem formulations but increase computational cost and latency.
  - Demonstration quality vs. generalization: High-quality, diverse demonstrations can improve the model's ability to refine problems, but may be harder to obtain.
  - Prompt complexity vs. model capacity: More complex prompts with multiple refining patterns may require larger models to process effectively.
- Failure signatures:
  - Non-convergence: The model keeps refining the problem without reaching a stable answer, indicating a potential issue with the convergence condition or the model's ability to generate consistent answers.
  - Degraded performance: If the refined problems are worse than the original ones, it could indicate that the refining patterns are not well-suited to the task or that the model is not learning the intended transformations.
  - Inconsistent refinements: If the model generates vastly different refined problems across iterations, it may suggest that the convergence condition is too strict or that the model is not effectively applying the refining patterns.
- First 3 experiments:
  1. Test Self-Polish with a single refinement iteration on a simple reasoning task (e.g., MultiArith) using Text-davinci-003. Compare performance to standard few-shot prompting.
  2. Implement the progressive refinement loop and test with 2-3 iterations on the same task. Observe if the answers converge and if performance improves.
  3. Combine Self-Polish with Chain-of-Thought prompting on a more complex task (e.g., GSM8K) and compare the results to using Chain-of-Thought alone.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of refinement iterations (K) for balancing effectiveness and efficiency across different reasoning tasks?
- Basis in paper: [explicit] The authors note that larger iteration counts lead to higher convergence accuracy but set K=3 to achieve a trade-off between efficiency and effectiveness
- Why unresolved: The paper only experiments with K=3, leaving open questions about whether more iterations could yield better results for specific tasks, and whether there's a sweet spot that varies by task complexity
- What evidence would resolve it: Systematic experiments varying K from 2-10 on multiple benchmarks showing accuracy vs iteration trade-offs, identifying task-specific optimal values

### Open Question 2
- Question: How does the quality of demonstration examples affect the effectiveness of in-context problem refining across different model sizes?
- Basis in paper: [explicit] The authors note that in-context learning is sensitive to the number and order of demonstrations, but don't systematically study demonstration quality
- Why unresolved: While the paper uses randomly selected demonstrations, it doesn't investigate whether higher-quality, more diverse demonstrations could improve performance, or whether this effect varies with model size
- What evidence would resolve it: Controlled experiments comparing randomly selected vs carefully curated demonstrations, and testing across different model scales to identify quality thresholds

### Open Question 3
- Question: Can the problem refining principles be automated or learned rather than manually crafted?
- Basis in paper: [inferred] The authors mention leaving automated methods to future research and note that fine-tuning for reformulation is tedious, suggesting current methods rely on manual prompt engineering
- Why unresolved: The current approach requires manual construction of demonstration patterns and instructions, which doesn't scale well and may miss optimal refining strategies
- What evidence would resolve it: Development of automated systems that can learn effective refining patterns from data, or reinforcement learning approaches that optimize refining strategies without human intervention

## Limitations

- The paper does not provide the specific demonstration examples used for problem-refining prompts, making faithful reproduction challenging
- The evaluation relies heavily on OpenAI API calls with specific models, raising questions about generalization to other model families or open-source alternatives
- The paper does not provide convergence rate statistics across datasets, making it unclear how often the iterative refinement process actually stabilizes within the specified K=3 iterations

## Confidence

- **High Confidence**: The core claim that problem refinement can improve reasoning performance is well-supported by the experimental results across five benchmarks. The mechanism of progressive refinement with convergence checking is clearly described and internally consistent.
- **Medium Confidence**: The orthogonality claim with Chain-of-Thought and other prompting methods is supported by combined performance gains, but the paper does not systematically analyze whether the improvements are additive or multiplicative. The robustness claims across different models are demonstrated but could benefit from testing on a broader range of model sizes and architectures.
- **Low Confidence**: The paper's claims about the specific principles for refined problems (concise, clarity, well-focused) are asserted but not empirically validated. It's unclear whether all proposed refining patterns are equally effective or necessary.

## Next Checks

1. **Convergence Analysis**: Run the Self-Polish method on all five benchmark datasets and record the iteration count at which convergence occurs (or if it fails to converge within K=3). Calculate convergence rates and analyze whether convergence patterns correlate with task complexity or model performance.

2. **Demonstration Quality Study**: Create multiple sets of demonstration examples with varying quality levels (high, medium, low) and test their impact on refinement quality and final reasoning performance. This would validate whether the in-context learning component is robust to demonstration quality variations.

3. **Ablation of Refining Patterns**: Systematically disable individual refining patterns (eliminating irrelevant information, rearranging logic, organizing conditions in parallel) and measure their individual contribution to performance improvements. This would reveal which aspects of the refinement process are most critical to success.