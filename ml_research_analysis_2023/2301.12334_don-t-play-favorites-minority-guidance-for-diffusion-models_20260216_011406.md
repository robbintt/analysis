---
ver: rpa2
title: 'Don''t Play Favorites: Minority Guidance for Diffusion Models'
arxiv_id: '2301.12334'
source_url: https://arxiv.org/abs/2301.12334
tags:
- minority
- samples
- guidance
- score
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of generating minority samples (low-density
  data points) using diffusion models, which typically favor generating majority samples
  due to their higher likelihood. The authors propose a novel framework called minority
  guidance that addresses this bias.
---

# Don't Play Favorites: Minority Guidance for Diffusion Models

## Quick Facts
- **arXiv ID**: 2301.12334
- **Source URL**: https://arxiv.org/abs/2301.12334
- **Reference count**: 25
- **Key outcome**: Novel framework that generates minority samples using diffusion models by introducing minority score metric and guidance technique, significantly improving low-density sample generation

## Executive Summary
This paper addresses the fundamental bias in diffusion models that favors generating majority samples over minority samples. The authors propose a comprehensive solution called minority guidance that first quantifies sample uniqueness through a novel minority score metric, then steers the generation process toward desired uniqueness levels. The approach leverages perceptual distance between original and denoised samples to identify minority features, then uses classifier guidance to condition generation on these uniqueness levels. Experiments demonstrate significant improvements in generating low-density minority samples across multiple benchmark datasets.

## Method Summary
The minority guidance framework introduces a two-step approach to generate minority samples. First, it defines minority score as the LPIPS perceptual distance between original samples and their Tweedie's formula reconstructions, effectively quantifying semantic uniqueness. Second, it trains an ordinal classifier to predict discretized minority score categories from noisy inputs, then incorporates this classifier into the sampling process via classifier guidance. The modified score function blends the original score model with the log-gradient of the classifier, allowing generation to be steered toward samples with desired levels of uniqueness. The framework is evaluated on CIFAR-10, CelebA, and LSUN-Bedrooms datasets.

## Key Results
- Minority guidance significantly improves generation of low-density minority samples as measured by AvgkNN and LOF metrics
- The method demonstrates practical significance in demanding scenarios like medical imaging applications
- Generated samples maintain reasonable quality while achieving desired levels of uniqueness across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
Diffusion models trained with denoising score matching inherently favor majority samples because the optimal score model averages conditional scores over the entire data distribution, which is dominated by majority samples. The denoising process reconstructs input toward the mean of conditional score functions across all data points, pulling minority samples toward majority features.

### Mechanism 2
Minority score, defined as the perceptual distance (LPIPS) between original and reconstructed samples, effectively identifies samples with unique features because minority samples undergo greater semantic changes during denoising. During perturbation-reconstruction, minority samples with novel features are more likely to be reconstructed toward majority features, creating larger perceptual differences.

### Mechanism 3
Minority guidance, implemented as classifier guidance with discretized minority score categories, steers the generation process toward desired likelihood levels by modifying the score function with log-gradient of the classifier. By blending the score model with the log-gradient of a classifier that predicts discretized minority scores, generation is conditioned to favor samples with specific uniqueness levels.

## Foundational Learning

- **Concept**: Denoising score matching (DSM)
  - Why needed here: Understanding how diffusion models are trained to denoise is crucial for recognizing why they favor majority samples and how minority guidance modifies this behavior
  - Quick check question: What is the key difference between the standard denoising score matching objective and the modified objective used in minority guidance?

- **Concept**: Classifier guidance in diffusion models
  - Why needed here: The minority guidance technique builds upon classifier guidance principles to incorporate minority score conditioning into the generation process
  - Quick check question: How does classifier guidance modify the score function, and what role does the scaling factor play in controlling the strength of guidance?

- **Concept**: Perceptual similarity metrics (LPIPS)
  - Why needed here: LPIPS is the core metric for quantifying minority score, measuring semantic differences between original and reconstructed samples
  - Quick check question: Why is LPIPS preferred over L1 or L2 distance for minority score, and what specific visual aspects does it capture better?

## Architecture Onboarding

- **Component map**: Pre-trained diffusion model -> Minority score calculator -> Ordinal minority score classifier -> Minority guidance sampler -> Evaluation metrics
- **Critical path**: 1) Compute minority scores for training data using Tweedie's formula reconstruction 2) Discretize minority scores into ordinal categories 3) Train classifier to predict ordinal minority scores from noisy inputs 4) During generation, modify score function with classifier log-gradient 5) Generate samples conditioned on desired minority score category
- **Design tradeoffs**: Using LPIPS vs simpler distances (better semantic capture vs computational cost), number of minority classes (finer control vs data requirements), classifier scale (focus on minority features vs sample quality)
- **Failure signatures**: Poor classifier performance (samples similar to baseline), over-guidance (unrealistic samples), under-guidance (minimal improvement)
- **First 3 experiments**: 1) Validate minority score metric by comparing LPIPS-based scores with manual inspection 2) Test classifier performance on held-out minority score categories 3) Evaluate minority guidance by generating samples at different levels and comparing with baseline using neighborhood density metrics

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important areas for future work emerge from the discussion, particularly around extending the framework to non-image domains and addressing quality-accuracy tradeoffs.

## Limitations
- The minority score metric relies on Tweedie's formula assumptions that may not generalize to all data types
- Effectiveness depends heavily on classifier performance, which could degrade with small minority class sizes
- Perceptual distance metric (LPIPS) may be sensitive to domain-specific factors beyond semantic uniqueness

## Confidence
- **High confidence**: Core observation about diffusion model bias and mathematical formulation of minority score
- **Medium confidence**: Effectiveness of classifier guidance for steering toward minority samples
- **Low confidence**: Claims about practical significance in medical imaging applications

## Next Checks
1. **Ablation study on classifier scale**: Systematically vary the classifier guidance scale parameter and measure its impact on both minority score generation and sample quality
2. **Cross-dataset generalization test**: Apply the framework to a dataset with known minority subgroups (e.g., medical dataset with rare disease conditions) to validate practical significance
3. **Robustness to LPIPS variations**: Replace LPIPS with alternative perceptual metrics to verify minority score captures meaningful semantic differences rather than being specific to LPIPS implementation