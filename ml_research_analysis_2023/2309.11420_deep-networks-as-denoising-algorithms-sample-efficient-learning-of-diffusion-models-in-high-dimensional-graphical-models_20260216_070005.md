---
ver: rpa2
title: 'Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion
  Models in High-Dimensional Graphical Models'
arxiv_id: '2309.11420'
source_url: https://arxiv.org/abs/2309.11420
tags:
- lemma
- score
- error
- have
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the approximation efficiency of deep neural
  networks for learning score functions in diffusion-based generative models. Existing
  approximation theories suffer from the curse of dimensionality for high-dimensional
  data, particularly in graphical models like Markov random fields.
---

# Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models

## Quick Facts
- arXiv ID: 2309.11420
- Source URL: https://arxiv.org/abs/2309.11420
- Reference count: 40
- Key outcome: Shows that deep neural networks can efficiently approximate score functions in high-dimensional graphical models by unrolling variational inference denoising algorithms, avoiding the curse of dimensionality

## Executive Summary
This paper investigates the approximation efficiency of deep neural networks for learning score functions in diffusion-based generative models. Existing approximation theories suffer from the curse of dimensionality for high-dimensional data, particularly in graphical models like Markov random fields. The authors observe that score functions can often be well-approximated in graphical models through variational inference denoising algorithms, which are amenable to efficient neural network representation. They demonstrate this in examples including Ising models, conditional Ising models, restricted Boltzmann machines, and sparse encoding models. Combined with discretization error bounds for diffusion-based sampling, they provide an efficient sample complexity bound for diffusion-based generative modeling when the score function is learned by deep neural networks.

## Method Summary
The method connects score functions in diffusion models to denoisers via Tweedie's formula, then approximates these denoisers using variational inference algorithms that can be unrolled into residual network structures. The approach consists of (1) variational inference to compute denoiser approximations through fixed point iteration, (2) ResNet modules that unroll this iteration into neural network layers, (3) empirical risk minimization (ERM) to train the ResNet to approximate the score function, and (4) sampling using the trained score function in a diffusion process. The key insight is that for graphical models, the variational inference approximation error is independent of network size and sample size, and typically vanishes as dimension increases, creating a "blessing of dimensionality."

## Key Results
- Score functions in graphical models can be efficiently approximated by unrolling variational inference denoising algorithms into residual networks
- The curse of dimensionality can be avoided for score approximation in high-dimensional graphical models, with approximation error that typically decreases as dimension increases
- Sample complexity bounds for diffusion models can be derived by combining score estimation error bounds with discretization error bounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Score functions in graphical models can be efficiently approximated by unrolling variational inference denoising algorithms into residual networks
- Mechanism: The score function $s_t(z) = (λ_t m_t(z) - z)/σ_t^2$ relates to a denoiser $m_t(z)$ via Tweedie's formula. For graphical models like Ising models, this denoiser can be found by minimizing a variational free energy, which has a unique fixed point. This fixed point iteration can be unrolled into a residual network structure that approximates the denoiser efficiently.
- Core assumption: The variational free energy has a unique minimizer that can be found through fixed point iteration, and this iteration can be approximated by a ResNet with bounded approximation error

### Mechanism 2
- Claim: The curse of dimensionality can be avoided for score approximation in high-dimensional graphical models
- Mechanism: For graphical models, the variational inference approximation error $\epsilon^{VI}_t(A)$ is independent of network size and sample size, and typically vanishes as dimension $d$ increases. This creates a "blessing of dimensionality" where score approximation becomes easier with higher dimensions for certain models.
- Core assumption: The variational inference approximation error $\epsilon^{VI}_t(A)$ is well-controlled and independent of network parameters

### Mechanism 3
- Claim: Sample complexity bounds for diffusion models can be derived by combining score estimation error bounds with discretization error bounds
- Mechanism: The overall sampling error is bounded by the sum of score estimation error and discretization error. The score estimation error is controlled by three terms: variational inference error, ResNet approximation error, and generalization error. When combined with existing discretization error bounds, this provides end-to-end sample complexity guarantees.
- Core assumption: The score estimation error bounds from Theorem 1 can be combined with existing discretization error analyses to provide sampling guarantees

## Foundational Learning

- Concept: Tweedie's formula and its relationship between score functions and denoisers
  - Why needed here: The paper crucially relies on the identity $s_t(z) = (λ_t m_t(z) - z)/σ_t^2$ to connect score functions to denoisers, which can then be approximated using variational inference
  - Quick check question: Given $z = λ_tx + σ_tg$ where $x \sim \mu$ and $g \sim N(0, I_d)$, what is the relationship between the score function $s_t(z) = ∇_z \log μ_t(z)$ and the denoiser $m_t(z) = E[x,g]∼μ⊗N(0,I_d)[x|z]$?

- Concept: Variational inference and free energy minimization in graphical models
  - Why needed here: The paper assumes that denoisers in graphical models can be approximated by minimizing a variational free energy, which then enables the connection to neural network approximation
  - Quick check question: For an Ising model with distribution $μ(x) ∝ \exp\{⟨x, Ax⟩/2\}$, what is the naive variational Bayes free energy $F^{naive}_t(m; z)$ that needs to be minimized to find the denoiser?

- Concept: Fixed point iteration and its approximation by residual networks
  - Why needed here: The paper shows that the fixed point equation for the denoiser minimizer can be solved iteratively, and this iteration can be represented as a residual network structure
  - Quick check question: If the denoiser minimizer satisfies the fixed point equation $m = \tanh((A-K)m + λ_tσ_t^{-2}z)$, how can this be represented as a residual network with $u(ℓ) = u(ℓ-1) + W^{(ℓ)}_1ReLU(W^{(ℓ)}_2u(ℓ-1))$?

## Architecture Onboarding

- Component map: Variational inference module -> ResNet module -> ERM module -> Sampling module
- Critical path: (1) Compute variational free energy minimizer through fixed point iteration, (2) Approximate this iteration with ResNet, (3) Train ResNet via ERM to minimize score estimation error, (4) Use trained score function in diffusion sampling process
- Design tradeoffs: The main tradeoff is between approximation accuracy and computational efficiency. Using more iterations (larger L) and wider networks (larger M) improves approximation but increases computation. The variational inference error $\epsilon^{VI}_t$ is a fundamental limitation that cannot be overcome by network architecture alone
- Failure signatures: The system will fail if: (1) The variational free energy does not have a unique minimizer (violating Assumption 1), (2) The fixed point iteration does not converge, (3) The ResNet cannot adequately approximate the fixed point iteration, (4) The sample size n is insufficient for generalization
- First 3 experiments:
  1. Verify that the variational free energy minimization works for a simple Ising model with known analytical solution
  2. Test the ResNet approximation of the fixed point iteration on a small-scale problem with ground truth
  3. Combine the trained score function with a simple discretization scheme to generate samples and compare to the true distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does score approximation become easier for higher-dimensional Ising models?
- Basis in paper: The paper observes that the VI approximation error $\epsilon^{VI}_t(A)$ is independent of ResNet size, sample size, and typically vanishes as dimension $d$ increases. However, the paper only provides an upper bound on the estimation error and cannot conclude whether score approximation actually becomes easier.
- Why unresolved: The paper provides an upper bound but cannot definitively say if score approximation improves with higher dimensions due to the nature of upper bounds.
- What evidence would resolve it: Empirical studies comparing score approximation performance across different dimensional Ising models, or a lower bound proof showing that the score approximation error decreases with increasing dimension.

### Open Question 2
- Question: Can the hierarchy of variational inference algorithms (e.g., Plefka's expansion) reduce the score approximation error in diffusion models?
- Basis in paper: The paper mentions that the score approximation error doesn't decay as network size and sample size increase, and is lower bounded by the VI approximation error $\epsilon^{VI}_t$. It suggests that using a hierarchy of VI algorithms could potentially reduce the score approximation error.
- Why unresolved: The paper does not explore the use of hierarchical VI algorithms for score approximation in diffusion models.
- What evidence would resolve it: Implementation and evaluation of diffusion models using hierarchical VI algorithms (e.g., Plefka's expansion) and comparison of their score approximation error with standard VI approaches.

### Open Question 3
- Question: What algorithms do diffusion neural networks like U-nets and transformers implement for image tasks?
- Basis in paper: The paper hypothesizes that U-nets with convolution layers might be implementing variational inference denoising on graphical models with certain locality and invariance structures. It suggests testing this hypothesis on real image datasets.
- Why unresolved: The paper does not provide empirical evidence or a detailed analysis of the algorithms implemented by diffusion neural networks.
- What evidence would resolve it: Empirical studies analyzing the behavior of diffusion neural networks on image datasets, comparing their performance with explicit VI denoising algorithms, and identifying the specific structures and operations they implement.

## Limitations

- The analysis critically depends on Assumption 1, which states that the denoiser can be approximated by minimizing a variational free energy with a unique minimizer. The general conditions under which this assumption holds remain unclear.
- The non-asymptotic bounds for complex models like spin glasses are not fully characterized, and the paper relies on asymptotic results in some cases.
- The claim about "blessing of dimensionality" where score approximation becomes easier with higher dimensions requires stronger assumptions about model structure and needs empirical verification.

## Confidence

- **High Confidence**: The connection between score functions and denoisers via Tweedie's formula, and the basic structure of the ERM framework for score function learning
- **Medium Confidence**: The approximation efficiency of ResNets for fixed point iterations in variational inference, as this depends on specific assumptions about the free energy landscape
- **Low Confidence**: The claim about "blessing of dimensionality" where score approximation becomes easier with higher dimensions, as this requires stronger assumptions about model structure

## Next Checks

1. **Test Assumption 1 violation**: Construct a graphical model where the variational free energy does not have a unique minimizer and verify that the score approximation bounds break down

2. **Empirical verification of error bounds**: Implement the full pipeline on a simple Ising model with known ground truth and measure whether the observed score estimation error matches the theoretical bounds

3. **Generalization to non-graphical models**: Test whether the sample complexity bounds still hold for non-graphical models where the variational inference approximation error $\epsilon^{VI}_t$ scales poorly with dimension