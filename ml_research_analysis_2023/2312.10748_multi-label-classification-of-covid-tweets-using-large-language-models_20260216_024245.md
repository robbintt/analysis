---
ver: rpa2
title: Multi-Label Classification of COVID-Tweets Using Large Language Models
arxiv_id: '2312.10748'
source_url: https://arxiv.org/abs/2312.10748
tags:
- vaccines
- tweet
- gpt-3
- multi-label
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of classifying social media posts
  (tweets) related to COVID-19 vaccines according to specific concerns expressed by
  the author. The task involves multi-label classification with 13 possible labels,
  such as "unnecessary", "mandatory", "pharma", "conspiracy", etc.
---

# Multi-Label Classification of COVID-Tweets Using Large Language Models

## Quick Facts
- arXiv ID: 2312.10748
- Source URL: https://arxiv.org/abs/2312.10748
- Reference count: 6
- Primary result: BERT-large-uncased achieved macro-F1 score of 0.66 and Jaccard similarity of 0.66 on test set, ranking 6th in competition

## Executive Summary
This paper tackles the challenge of classifying COVID-19 vaccine-related tweets into 13 possible concern categories using multi-label classification. The authors compare three approaches: a supervised BERT-large-uncased model, a supervised HateXplain model, and a zero-shot GPT-3.5 Turbo model. The BERT-large-uncased model outperformed the others with a macro-F1 score of 0.66 and Jaccard similarity of 0.66 on the test set, ranking 6th among competition submissions. The study highlights the effectiveness of supervised fine-tuning on domain-specific data compared to zero-shot approaches for specialized classification tasks.

## Method Summary
The authors employed three different models for multi-label classification of vaccine-related tweets: BERT-large-uncased with supervised fine-tuning, HateXplain with supervised fine-tuning, and GPT-3.5 Turbo with zero-shot classification. The BERT model was trained on 9,921 labeled tweets from the AISOME dataset using sigmoid activation and a 0.5 threshold for 13 labels. Training used a batch size of 1, learning rate of 2e-5, and 100 epochs. Evaluation was performed using macro-F1 and Jaccard similarity metrics on a test set of 486 tweets.

## Key Results
- BERT-large-uncased achieved the highest performance with macro-F1 score of 0.66 and Jaccard similarity of 0.66
- GPT-3.5 Turbo zero-shot approach underperformed compared to supervised methods
- The model ranked 6th among all competition submissions
- BERT's bidirectional architecture captured vaccine hesitancy concerns more effectively than other approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT-large-uncased outperforms other models due to its deep bidirectional architecture, which captures context from both directions in the tweet text.
- Mechanism: BERT's transformer architecture with 24 layers and 1,024-dimensional embeddings allows for rich contextual representations. The model learns to encode both local and global semantic patterns relevant to vaccine-related concerns.
- Core assumption: The bidirectional nature of BERT captures nuanced sentiment and topic-related information better than unidirectional models or zero-shot approaches.
- Evidence anchors:
  - [abstract] The authors state that BERT-large-uncased "performed best in our case" with a macro-F1 score of 0.66.
  - [section] The methodology section describes BERT-large-uncased's embedding dimensionality (1,024) and training configuration (100 epochs, batch size 1, learning rate 2e-5).
  - [corpus] The related work section references BERT as a "state-of-the-art natural language processing (NLP) model" that has "revolutionized the field of NLP."
- Break condition: If the training data is too small or imbalanced, BERT's capacity may lead to overfitting rather than improved generalization.

### Mechanism 2
- Claim: Supervised training on the AISOME dataset enables the BERT model to learn domain-specific patterns of vaccine hesitancy that general-purpose models miss.
- Mechanism: By training on 9,921 labeled tweets specific to COVID vaccine concerns, the model learns the particular vocabulary, phrasing, and context that signal different types of hesitancy (e.g., "unnecessary," "conspiracy," "pharma").
- Core assumption: The labeled dataset captures sufficient variance in vaccine-related discourse to enable robust pattern recognition.
- Evidence anchors:
  - [section] The authors trained the BERT model on "the AISOME training dataset of 9,921 tweets and the corresponding labels."
  - [section] The dataset section confirms they received "a training set of 9,921 tweets along with the corresponding labels."
  - [corpus] The related work section references the CAVES dataset, suggesting the task builds on established vaccine hesitancy classification frameworks.
- Break condition: If the dataset contains labeling noise or fails to represent certain concern types, supervised learning may reinforce incorrect patterns.

### Mechanism 3
- Claim: The multi-label architecture with sigmoid activation and threshold-based classification allows the model to predict multiple simultaneous concerns in a single tweet.
- Mechanism: Using sigmoid activation instead of softmax enables independent probability estimation for each of the 13 labels, allowing tweets to be classified under multiple concern categories simultaneously.
- Core assumption: Vaccine hesitancy tweets often express multiple concerns simultaneously, requiring multi-label rather than multi-class classification.
- Evidence anchors:
  - [abstract] The task is described as building "an effective multi-label classifier to label a social media post... according to the specific concern(s) towards vaccines."
  - [section] The methodology section specifies "We use the sigmoid activation function" and "threshold of 0.5."
  - [corpus] The problem definition section lists 13 distinct labels that can be assigned independently.
- Break condition: If the threshold is poorly calibrated, the model may produce too many or too few positive predictions, affecting both precision and recall.

## Foundational Learning

- Concept: Multi-label classification vs. multi-class classification
  - Why needed here: Vaccine hesitancy tweets often express multiple concerns simultaneously (e.g., both "pharma" and "side-effect" concerns), requiring a model that can predict multiple labels per instance rather than forcing a single choice.
  - Quick check question: Why would using softmax activation instead of sigmoid be inappropriate for this task?

- Concept: Transformer architecture and self-attention
  - Why needed here: Understanding how BERT's bidirectional attention mechanism captures context is crucial for diagnosing why it outperforms other approaches and for making informed decisions about model modifications.
  - Quick check question: How does BERT's bidirectional processing differ from traditional RNN approaches in handling context?

- Concept: Zero-shot learning limitations
- Why needed here: The paper compares zero-shot GPT-3.5 performance with supervised models, highlighting when few-shot or zero-shot approaches may be insufficient for specialized classification tasks.
  - Quick check question: What are the key limitations of zero-shot classification when dealing with domain-specific terminology like vaccine hesitancy concerns?

## Architecture Onboarding

- Component map: Input text -> BERT-large-uncased encoder -> Dense layer -> Sigmoid activation -> Thresholding
- Critical path:
  1. Text preprocessing and tokenization
  2. BERT embedding generation
  3. Dense layer transformation
  4. Sigmoid activation and thresholding
  5. Evaluation using macro-F1 and Jaccard similarity

- Design tradeoffs:
  - BERT-large-uncased vs. smaller variants: Higher accuracy but increased computational cost and memory requirements
  - Sigmoid vs. other multi-label strategies: Simpler implementation but may require careful threshold tuning
  - 100 epochs vs. early stopping: More stable convergence but potential for overfitting with limited data

- Failure signatures:
  - High variance between training and validation scores: Potential overfitting
  - Consistent underperformance on certain labels: Class imbalance or insufficient representation in training data
  - Degraded performance on longer tweets: Possible issues with the 512-token limit

- First 3 experiments:
  1. Baseline comparison: Train the same architecture with BERT-base instead of BERT-large-uncased to quantify the benefit of the larger model
  2. Threshold optimization: Systematically vary the classification threshold (0.3 to 0.7) to maximize macro-F1 score
  3. Class balancing: Apply oversampling or weighted loss functions to address potential class imbalance in the training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does increasing the training dataset size impact the performance of multi-label classifiers for vaccine-related tweets?
- Basis in paper: [explicit] The authors suggest that future work should focus on increasing the amount of training data for training the models.
- Why unresolved: The paper does not explore the effect of varying training data sizes on model performance.
- What evidence would resolve it: Experiments comparing model performance with different training dataset sizes, showing the relationship between data volume and classification accuracy.

### Open Question 2
- Question: How do other large language models (e.g., GPT-4, Claude) perform in multi-label classification of vaccine-related tweets compared to BERT-large-uncased and GPT-3.5 Turbo?
- Basis in paper: [explicit] The authors mention that future work will focus on trying several other large language models for multi-label classification.
- Why unresolved: The paper only tests BERT-large-uncased and GPT-3.5 Turbo, leaving the performance of other models unexplored.
- What evidence would resolve it: Experiments comparing the performance of various large language models (e.g., GPT-4, Claude) in multi-label classification tasks on the same dataset.

### Open Question 3
- Question: How does the inclusion of additional contextual information (e.g., user demographics, tweet metadata) affect the performance of multi-label classifiers for vaccine-related tweets?
- Basis in paper: [inferred] The paper focuses solely on the tweet text and does not consider additional contextual information that might be available.
- Why unresolved: The potential impact of incorporating contextual information is not explored in the paper.
- What evidence would resolve it: Experiments comparing model performance with and without the inclusion of additional contextual information, demonstrating the effect of such information on classification accuracy.

## Limitations

- The test set size is relatively small (486 tweets), which may lead to unstable performance estimates
- The paper does not provide detailed ablation studies to isolate the contribution of different architectural choices or hyperparameters
- Specific metrics of top-performing models are not disclosed, making it difficult to benchmark against the best approaches

## Confidence

- **High confidence**: The BERT-large-uncased model's superior performance over GPT-3.5 Turbo and HateXplain is well-supported by the reported metrics (macro-F1 0.66, Jaccard 0.66)
- **Medium confidence**: The claim that bidirectional context capture explains BERT's advantage is plausible but not empirically validated against other contextual models
- **Medium confidence**: The supervised learning advantage over zero-shot approaches is demonstrated, but the specific reasons for GPT-3.5's underperformance (prompt quality, model limitations, or task incompatibility) remain unclear

## Next Checks

1. **Cross-validation stability**: Perform 5-fold cross-validation on the training data to assess the consistency of the 0.66 macro-F1 score and identify potential overfitting.

2. **Zero-shot prompt optimization**: Systematically test variations of the GPT-3.5 Turbo prompt structure to determine if the zero-shot performance can be improved through better prompt engineering.

3. **Class-wise performance analysis**: Generate detailed per-class precision, recall, and F1 scores to identify which of the 13 vaccine concern categories are most challenging for the model and may benefit from additional training data or class-specific strategies.