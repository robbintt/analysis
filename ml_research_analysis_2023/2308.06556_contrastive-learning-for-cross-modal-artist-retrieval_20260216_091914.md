---
ver: rpa2
title: Contrastive Learning for Cross-modal Artist Retrieval
arxiv_id: '2308.06556'
source_url: https://arxiv.org/abs/2308.06556
tags:
- artists
- artist
- contrastive
- embeddings
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a contrastive learning method for combining
  embeddings from multiple modalities (audio, tags, and collaborative filtering data)
  into a shared representation space. The method aims to address the problem of missing
  modality data in music datasets, which can limit retrieval performance and coverage,
  especially for less popular artists.
---

# Contrastive Learning for Cross-modal Artist Retrieval

## Quick Facts
- **arXiv ID**: 2308.06556
- **Source URL**: https://arxiv.org/abs/2308.06556
- **Reference count**: 0
- **One-line result**: Contrastive learning method combines audio, tag, and CF embeddings to improve artist retrieval accuracy and coverage, especially for less popular artists.

## Executive Summary
This paper presents a contrastive learning method for combining embeddings from multiple modalities (audio, tags, and collaborative filtering data) into a shared representation space. The method aims to address the problem of missing modality data in music datasets, which can limit retrieval performance and coverage, especially for less popular artists. The proposed approach trains a model using pairwise contrastive losses between modalities, and then combines the learned embeddings during inference. Experiments on two datasets (public and in-house) show that the method outperforms single-modality embeddings and baseline approaches in terms of artist retrieval accuracy (nDCG@200) and coverage (Gini@200), particularly for less popular query artists. The method is also shown to be more robust to missing modality data compared to other approaches.

## Method Summary
The method uses InfoNCE-based contrastive loss between pairs of modalities to train three separate encoders (one per modality) that map to a shared 200-dimensional space. During inference, the model averages available encoded embeddings. The approach is evaluated on two datasets (MSD and OWN) using nDCG@200 and Gini@200 metrics on the OLGA artist similarity dataset.

## Key Results
- Contrastive method outperforms single-modality embeddings and baseline algorithms for combining modalities
- Method improves retrieval accuracy (nDCG@200) and coverage (Gini@200), especially for less popular artists
- Approach is more robust to missing modality data compared to baseline concatenation or random projection methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning aligns embeddings from different modalities for the same artist in a shared space while pushing embeddings from different artists apart.
- Mechanism: The InfoNCE loss minimizes cosine distance between paired modality embeddings from the same artist and maximizes distance to embeddings from other artists.
- Core assumption: Each artist has at least one modality embedding that can serve as a positive pair, and the negative sampling provides meaningful separation.
- Evidence anchors:
  - [abstract] "contrastive learning method that maps embeddings from diverse modalities to a shared embedding space"
  - [section] "Lψψψa,ψψψb = -log Ξ(ψψψi a,ψψψi b,τ) / (2M k=1⊮[k̸=i]Ξ(ψψψia,ζζζk,τ))"
- Break condition: If the dataset contains many artists with only one modality, the positive pairs are incomplete and the model cannot learn consistent cross-modal alignment.

### Mechanism 2
- Claim: Combining embeddings from multiple modalities improves retrieval accuracy and coverage, especially for less popular artists.
- Mechanism: Each modality provides complementary information; combining them increases the effective feature space and reduces reliance on a single data source.
- Core assumption: Modalities are conditionally independent given the artist and each contains unique signal not captured by others.
- Evidence anchors:
  - [abstract] "Experiments on two datasets suggest that our contrastive method outperforms single-modality embeddings and baseline algorithms for combining modalities"
  - [section] "The contrastive method outperforms the baselines and the original embeddings in all the metrics"
- Break condition: If modalities are highly correlated or one modality dominates, the marginal gain from combination diminishes.

### Mechanism 3
- Claim: The method is more robust to missing modality data than baseline concatenation or random projection methods.
- Mechanism: During inference, the model can average over any available modalities, and during training it learns to map diverse partial views to the same artist location.
- Core assumption: The training objective enforces that embeddings from any subset of modalities for an artist should still be close in the shared space.
- Evidence anchors:
  - [abstract] "is more robust to missing modality data (i.e., it better handles the retrieval of artists with different modality embeddings than the query artist’s)"
  - [section] "Our method is more robust to missing modality data than the single-modality embeddings and the baseline approaches"
- Break condition: If the number of artists with full modality coverage is too small, the model cannot generalize to partial views.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The core training objective relies on comparing embeddings from different modalities and pushing dissimilar ones apart.
  - Quick check question: What happens to the loss if two embeddings from the same artist are identical but all negatives are far away?

- Concept: Embedding space alignment and dimensionality reduction
  - Why needed here: The final representation is a 200-dimensional vector; understanding how encoders project to this space is key to debugging.
  - Quick check question: If we change the embedding dimension from 200 to 100, how might retrieval performance be affected?

- Concept: Popularity bias in collaborative filtering data
  - Why needed here: CF embeddings are only available for popular artists; understanding this bias helps interpret why less popular artists benefit more from the method.
  - Quick check question: How would the method's performance change if we remove all artists below a popularity threshold?

## Architecture Onboarding

- Component map:
  Input embeddings (audio, tags, CF) -> Three separate encoders -> Pairwise contrastive loss computation -> Shared 200D space

- Critical path:
  1. Load embeddings for a batch of artists
  2. Pass each modality through its encoder
  3. Compute pairwise contrastive losses
  4. Backpropagate summed loss
  5. During inference, average available encoded embeddings

- Design tradeoffs:
  - Using pre-trained embeddings vs training from scratch: faster convergence vs potential mismatch
  - Fixed 200D space: compact but may limit expressiveness
  - Simple averaging at inference: computationally cheap but ignores modality reliability

- Failure signatures:
  - Loss plateaus early: possible modality imbalance or poor encoder initialization
  - Retrieval accuracy drops for popular artists: model overfits to tail distribution
  - Gini increases dramatically: retrieval becomes concentrated on few artists

- First 3 experiments:
  1. Train with only two modalities (e.g., audio+tags) and compare nDCG@200 vs full three-modality training.
  2. Replace the encoder for one modality with a random projection and measure impact on overall performance.
  3. Vary the temperature τ in InfoNCE loss and plot retrieval performance to find optimal value.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we further mitigate the bias towards retrieving artists with similar modalities to the query artist in cross-modal retrieval systems?
- Basis in paper: [explicit] The paper mentions that "Future work may be dedicated to further mitigate this bias" in the conclusion section.
- Why unresolved: The paper identifies the issue but does not provide a solution or specific method to address it.
- What evidence would resolve it: A follow-up study that proposes and evaluates a method to reduce modality bias in cross-modal retrieval, showing improved performance in retrieving artists with different modalities than the query.

### Open Question 2
- Question: What is the optimal batch size for contrastive learning in multimodal music representation learning, and how does it affect the gradient bias and final performance?
- Basis in paper: [inferred] The paper mentions that "adapting the size of the training sample batch" could be a way to iterate on the contrastive learning method, but does not explore this aspect.
- Why unresolved: The paper does not investigate the impact of batch size on the contrastive learning method's performance or gradient bias.
- What evidence would resolve it: An experimental study that systematically varies the batch size in contrastive learning for multimodal music representation and evaluates its impact on performance metrics and gradient bias.

### Open Question 3
- Question: How can we design a loss function for contrastive learning that balances intra-artist modality coherence with inter-artist separation, depending on the specific application?
- Basis in paper: [explicit] The paper suggests that "this is a property that could perhaps be managed by iterating on the contrastive learning method, for instance, by adapting the loss function."
- Why unresolved: The paper does not provide a specific method for adapting the loss function to balance intra-artist and inter-artist distances.
- What evidence would resolve it: A proposed and evaluated loss function modification that allows for adjustable balance between intra-artist coherence and inter-artist separation, with experiments showing its effectiveness in different application scenarios.

## Limitations
- Specific architectures and training details of pre-trained single-modality embeddings are not fully specified
- Exact procedure for computing popularity proxy measure from MSD-Echonest Profile data is unspecified
- Impact of hyperparameter choices (temperature, embedding dimension, batch size) on performance is not thoroughly analyzed

## Confidence

- **High confidence**: The method's ability to combine multiple modalities into a shared embedding space and improve overall retrieval accuracy (nDCG@200) is well-supported by experimental results on two datasets.
- **Medium confidence**: The claim that the method is more robust to missing modality data is supported but could benefit from more rigorous analysis of edge cases and failure modes.
- **Medium confidence**: The improvement in coverage (Gini@200) for less popular artists is demonstrated but the analysis could be strengthened with more detailed breakdown across popularity quantiles.

## Next Checks

1. **Modality ablation study**: Systematically remove one modality at a time during both training and inference to quantify the contribution of each modality to final performance.

2. **Temperature sensitivity analysis**: Conduct a thorough sweep of the InfoNCE temperature parameter τ to identify optimal values and analyze how temperature affects the balance between accuracy and coverage.

3. **Cross-dataset generalization**: Train the model on MSD and evaluate on a completely different dataset (or vice versa) to assess the method's ability to generalize across different music datasets and distribution shifts.