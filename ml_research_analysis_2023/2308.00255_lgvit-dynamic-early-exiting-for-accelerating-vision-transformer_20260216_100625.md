---
ver: rpa2
title: 'LGViT: Dynamic Early Exiting for Accelerating Vision Transformer'
arxiv_id: '2308.00255'
source_url: https://arxiv.org/abs/2308.00255
tags:
- exiting
- early
- heads
- training
- internal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates why directly applying early exiting to
  vision transformers leads to performance degradation and proposes a new framework,
  LGViT, to address the issue. The core method idea is to use heterogeneous exiting
  heads: local perception heads (based on convolution) for shallow layers to capture
  local information, and global aggregation heads (based on self-attention) for deep
  layers to capture global semantic information.'
---

# LGViT: Dynamic Early Exiting for Accelerating Vision Transformer

## Quick Facts
- arXiv ID: 2308.00255
- Source URL: https://arxiv.org/abs/2308.00255
- Reference count: 40
- Primary result: Achieves 1.8√ó speed-up with only 2% accuracy drop across three ViT backbones

## Executive Summary
This paper addresses the challenge of applying early exiting to Vision Transformers (ViTs), which often leads to performance degradation. The authors identify that shallow internal classifiers fail to capture sufficient feature representations while deep internal classifiers struggle with semantic information. To solve this, they propose LGViT, a framework using heterogeneous exiting heads: local perception heads (convolution-based) for shallow layers to capture local information, and global aggregation heads (self-attention-based) for deep layers to capture global semantic information. The method achieves competitive performance with an average 1.8√ó speed-up and only 2% accuracy drop on CIFAR-100, Food-101, and ImageNet-1K datasets.

## Method Summary
LGViT introduces heterogeneous exiting heads to address the limitations of vanilla early exiting in ViTs. Local Perception Heads (LPH) with depth-wise convolution capture local patterns in shallow layers, while Global Aggregation Heads (GAH) with position-wise feature convergence capture global semantic information in deep layers. The method employs a two-stage training strategy: first, end-to-end training of the backbone and final classifier; second, freezing the backbone and training exiting heads through self-distillation. During inference, dynamic early exiting uses confidence-based decision-making to exit at the first layer where confidence exceeds a threshold, achieving computational efficiency without significant accuracy loss.

## Key Results
- Achieves 1.8√ó average speed-up across three ViT backbones (ViT, DeiT, Swin)
- Maintains competitive accuracy with only 2% average drop on CIFAR-100, Food-101, and ImageNet-1K
- Outperforms vanilla early exiting approaches that use homogeneous internal classifiers
- Demonstrates effectiveness across different exiting position schemes (shallow, deep, middle, uniform)

## Why This Works (Mechanism)

### Mechanism 1
Shallow internal classifiers in vanilla early exiting ViTs fail to learn sufficient feature representations. The shallow layers of ViTs lack local information integration, causing internal classifiers to miss critical visual cues that deeper layers would otherwise capture through global attention. The core assumption is that convolution operations introduce inductive biases that help capture local patterns effectively.

### Mechanism 2
Deep internal classifiers in vanilla early exiting ViTs struggle to capture target semantic information. Deep layers in ViTs focus heavily on global self-attention, which can dilute task-specific semantic information when used for early exiting, leading to poor performance compared to the final classifier. The core assumption is that global aggregation heads with reduced spatial redundancy can better preserve semantic focus.

### Mechanism 3
Two-stage training with heterogeneous distillation bridges the representation gap between local and global heads. First, end-to-end training optimizes the backbone independently. Then, freezing the backbone and training exiting heads with heterogeneous and homogeneous distillation aligns their feature spaces without interference. The core assumption is that freezing the backbone prevents catastrophic forgetting while allowing exiting heads to specialize.

## Foundational Learning

- **Vision Transformer (ViT) architecture**: Understanding how ViTs process images via patch embeddings and self-attention is essential to grasp why early exiting is challenging. Quick check: What is the role of the [CLS] token in ViT classification?
- **Early exiting in deep networks**: The paper builds on early exiting methods originally designed for CNNs and NLP transformers; understanding their mechanics is crucial. Quick check: How does an early exit decision typically depend on prediction confidence?
- **Knowledge distillation**: The two-stage training uses self-distillation to align internal classifiers with the final classifier without updating the backbone. Quick check: What is the difference between knowledge distillation and standard supervised learning?

## Architecture Onboarding

- **Component map**: ViT backbone (L encoder blocks) -> Local Perception Heads (LPH) on lower half of exiting points -> Global Aggregation Heads (GAH) on upper half of exiting points -> Internal classifiers
- **Critical path**: 1) Patch embedding ‚Üí Encoder blocks ‚Üí LPH/GAH at exiting points ‚Üí Internal classifiers 2) Training: Backbone fine-tuning ‚Üí Head distillation with frozen backbone 3) Inference: Forward pass ‚Üí Confidence check ‚Üí Early exit or full pass
- **Design tradeoffs**: LPH vs GAH placement (local vs global feature trade-off), number of exiting points (speed vs accuracy balance), kernel/window size scheduling (computational cost vs representational power)
- **Failure signatures**: Low accuracy despite speed-up (exiting heads not learning discriminative features), high variance in speed-up (threshold ùúè not well calibrated), memory overflow (too many large heads or high kernel sizes)
- **First 3 experiments**: 1) Compare vanilla early exit vs LPH+GAH on CIFAR-100 to verify accuracy improvement 2) Vary ùúè threshold to observe speed-accuracy trade-off curve 3) Test different exiting position schemes (shallow vs deep vs uniform) to find optimal placement

## Open Questions the Paper Calls Out

### Open Question 1
How do different window sizes in the Global Aggregation Head (GAH) affect the trade-off between local and global information capture? The paper mentions using different window sizes in the PFC block based on the exiting position, with larger windows for deeper layers, but does not provide a systematic analysis of how varying window sizes impact performance.

### Open Question 2
How does the choice of exiting position scheme (shallow, deep, middle, uniform) impact the model's ability to capture diverse feature representations? While the paper provides a comparison showing varying accuracy and speed-up ratios, it does not delve into the underlying reasons for the differences in performance or the implications for feature representation learning.

### Open Question 3
What is the optimal balance between the number of exiting heads and the overall model complexity? The paper mentions that the number of exiting heads is generally smaller than the total number of backbone layers and that increasing the number of heads can affect both accuracy and speed-up, but does not provide a comprehensive study on how varying the number of exiting heads impacts the model's performance and efficiency.

## Limitations

- Limited theoretical justification for why heterogeneous heads outperform homogeneous ones beyond empirical evidence
- Results primarily validated on classification tasks without testing on object detection, segmentation, or other vision tasks
- Comparison limited to internal baselines without benchmarking against existing ViT early exiting methods in literature

## Confidence

- **High confidence** in the empirical results showing LGViT achieves competitive accuracy with 1.8√ó speed-up across three datasets and three backbone architectures
- **Medium confidence** in the mechanism explanations, as the paper provides observational evidence but limited theoretical grounding for why heterogeneous heads outperform homogeneous ones
- **Low confidence** in the generalizability of the proposed method to other vision tasks beyond classification

## Next Checks

1. Conduct ablation studies varying the number of exiting points and their positions to determine optimal placement strategies across different ViT architectures
2. Test LGViT on vision tasks beyond classification (object detection, semantic segmentation) to evaluate generalizability of the heterogeneous head approach
3. Compare LGViT against existing ViT early exiting methods (if any exist in literature) to establish whether the proposed approach represents a fundamental advancement or incremental improvement