---
ver: rpa2
title: A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level
  Feedback
arxiv_id: '2310.03043'
source_url: https://arxiv.org/abs/2310.03043
tags:
- search
- ranking
- feedback
- learning
- dqrank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of incorporating fine-grained,
  sentence-level feedback into interactive search systems, which is challenging due
  to the need for extensive exploration and large annotated datasets. The proposed
  solution, DQrank, uses a deep Q-learning framework that adapts BERT-based models
  to select crucial sentences from user interactions and rank items more effectively.
---

# A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level Feedback

## Quick Facts
- arXiv ID: 2310.03043
- Source URL: https://arxiv.org/abs/2310.03043
- Reference count: 40
- Primary result: DQrank achieves at least 12% better performance than previous state-of-the-art RL approaches on interactive search with sentence-level feedback

## Executive Summary
This paper addresses the challenge of incorporating fine-grained, sentence-level feedback into interactive search systems. The proposed DQrank framework uses deep Q-learning with BERT-based models to select crucial sentences from user interactions and rank items more effectively. The method introduces novel mechanisms including sliding window ranking and rearrangement learning to efficiently explore the vast action space while retaining list-wise ranking information. DQrank also leverages experience replay to store feedback sentences, improving initial ranking performance. Evaluated on three datasets, DQrank demonstrates significant improvements over previous approaches.

## Method Summary
DQrank is a deep reinforcement learning framework that addresses interactive search with sentence-level feedback. It uses a deep Q-network with a BERT-based Q-function to estimate action values for ranking decisions. The method incorporates sliding window ranking to efficiently explore the large ranking action space, state retrieval to reuse feedback from similar queries, rearrangement learning to align user simulation rankings with Q-value maximizing rankings, and data augmentation through paraphrasing. The system is trained using experience replay and target network updates for stability.

## Key Results
- DQrank achieves at least 12% better performance than previous state-of-the-art RL approaches
- The sliding window mechanism reduces computational complexity while maintaining ranking quality
- State retrieval improves initial ranking performance for new queries
- Data augmentation through paraphrasing enhances generalization and robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sliding window ranking enables efficient exploration of the large ranking action space by prioritizing top-ranked items and reducing computational complexity.
- Mechanism: The method first ranks candidate documents using a user simulation function, then applies a sliding window of size m that moves from the last item backward. Within each window, all combinations are evaluated to find the optimal ordering, which is then fixed before moving to the next window.
- Core assumption: Users' attention is limited to a small window of top items, making it reasonable to focus optimization efforts on local reordering rather than full permutations.
- Evidence anchors:
  - [abstract]: "We also propose two mechanisms to better explore optimal actions. DQrank further utilizes the experience replay mechanism in DQ to store the feedback sentences to obtain a better initial ranking performance."
  - [section]: "The advantages of this proposed method are three-fold: (1) This approach models the attention of the users... (2) In most ranking tasks, the top-ranking items are usually the most important... (3) Moreover, this approach is efficient enough to be deployed in the actual training process."
  - [corpus]: Weak - no direct evidence in related papers, but the concept of attention windows is common in search literature.
- Break condition: If user attention patterns are uniform across the ranking list, the sliding window assumption breaks and the method loses efficiency.

### Mechanism 2
- Claim: State retrieval leverages historical feedback to provide better initial ranking performance for new queries.
- Mechanism: When a new query arrives, the system retrieves similar queries from a feedback pool based on BERT-encoded representations and cosine similarity. The state (including feedback sentences) from the most similar historical query is used as the starting point.
- Core assumption: Similar queries have similar optimal ranking states, and feedback sentences from similar queries transfer well to new but related queries.
- Evidence anchors:
  - [abstract]: "DQrank further utilizes the experience replay mechanism in DQ to store the feedback sentences to obtain a better initial ranking performance."
  - [section]: "When a new query is provided, q', we use BERT to encode the query, x(q'), and calculate the cosine similarity between this and all queries stored in P. If the highest cosine similarity is higher than our setting threshold ψ, we can retrieve this state as the initial state of the query."
  - [corpus]: Weak - related papers focus on query expansion but not state retrieval from feedback pools.
- Break condition: If the query space has low similarity structure or feedback is highly query-specific, state retrieval provides little benefit.

### Mechanism 3
- Claim: Data augmentation through paraphrasing expands the effective training data without changing the underlying reward structure.
- Mechanism: The system generates paraphrased versions of queries and feedback sentences, treating them as additional training examples. The reward function is assumed to be locally smooth, meaning paraphrased queries should yield similar rewards.
- Core assumption: Paraphrased queries maintain semantic equivalence and therefore should produce similar ranking rewards when processed by the model.
- Evidence anchors:
  - [abstract]: "We validate the effectiveness of DQRank on three search datasets. The results show that DQRank performs at least 12% better than the previous SOTA RL approaches."
  - [section]: "Since we cannot change the semantics of the original text and need to maintain the reward function remains smooth, we choose to paraphrase the sentences... We let Qt(st, at) = 1/|Ŝt| + 1 ∑i=0 |Ŝt| Q(ζi(st), at)"
  - [corpus]: Weak - no direct evidence in related papers, but paraphrasing is a common NLP augmentation technique.
- Break condition: If paraphrasing introduces semantic drift or the reward function is not smooth across semantic variations, augmentation could hurt performance.

## Foundational Learning

- Concept: Reinforcement Learning with Deep Q-Learning
  - Why needed here: The interactive search problem requires learning a policy that maximizes cumulative reward through user feedback, and the large action space of ranking makes traditional RL approaches computationally infeasible.
  - Quick check question: What is the key difference between DQN and traditional Q-learning that makes it suitable for this ranking problem?

- Concept: BERT-based text representation and classification
  - Why needed here: The system needs to understand sentence-level feedback and measure relevance between queries and document sentences, requiring sophisticated NLP capabilities.
  - Quick check question: How does the user simulation function use BERT embeddings to predict which sentences users will select as feedback?

- Concept: Experience replay and target network updates
  - Why needed here: The system needs to stabilize training by breaking correlation between consecutive samples and providing consistent target values during learning.
  - Quick check question: Why does DQrank update the target network weights every c steps rather than every training step?

## Architecture Onboarding

- Component map:
  User interaction layer -> BERT-based user simulation -> Sliding window ranking engine -> State retrieval module -> Experience replay buffer -> Main DQN -> Target DQN -> Data augmentation pipeline

- Critical path:
  1. Query arrives → State retrieval finds similar historical state
  2. Candidate documents retrieved using BM25
  3. Sliding window ranking explores optimal ordering
  4. User provides sentence-level feedback
  5. Transition stored in experience replay
  6. DQN trained on mini-batches from replay buffer
  7. Target network updated periodically

- Design tradeoffs:
  - Sliding window size (m) vs. exploration completeness: Smaller windows are faster but may miss global optima
  - State retrieval threshold (ψ) vs. cold start performance: Higher thresholds ensure better matches but may fail more often
  - Experience replay buffer size vs. memory usage: Larger buffers provide better sample diversity but consume more resources
  - Paraphrasing aggressiveness vs. semantic preservation: Too aggressive paraphrasing may change query intent

- Failure signatures:
  - Slow convergence: May indicate poor initial state retrieval or inadequate exploration
  - High variance in rewards: Could signal unstable training or insufficient experience replay
  - Poor performance on new query types: Suggests state retrieval is failing to find appropriate matches
  - Memory issues: Experience replay buffer growing too large

- First 3 experiments:
  1. Baseline comparison: Run DQrank vs. BM25 on MS-MARCO with same initial retrieval, measure NDCG@10 improvement
  2. State retrieval ablation: Compare performance with and without state retrieval on cold-start queries
  3. Sliding window sensitivity: Test different window sizes (m=2,3,4,5) on ORCAS dataset to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sliding window size parameter (m) affect the trade-off between ranking accuracy and computational efficiency in DQrank?
- Basis in paper: [explicit] The paper states that "we use the elbow method to determine the sliding window size, m = 4" but does not provide a detailed analysis of how different values of m affect performance.
- Why unresolved: The optimal value of m may depend on the specific dataset characteristics and the desired balance between accuracy and efficiency, which is not fully explored in the paper.
- What evidence would resolve it: A comprehensive ablation study varying m across a range of values and measuring both ranking accuracy (e.g., nDCG, MRR) and computational time would provide insights into the optimal value of m for different scenarios.

### Open Question 2
- Question: How does the state retrieval mechanism impact the ranking performance for queries with no similar queries in the feedback pool?
- Basis in paper: [inferred] The paper discusses the state retrieval mechanism for finding similar queries but does not address the scenario where no similar queries are found.
- Why unresolved: The effectiveness of the state retrieval mechanism is dependent on the availability of similar queries in the feedback pool, and its performance for queries with no similar matches is unclear.
- What evidence would resolve it: Experiments comparing the ranking performance of DQrank with and without state retrieval for queries with no similar matches in the feedback pool would clarify its impact in such scenarios.

### Open Question 3
- Question: How does the choice of BERT model (e.g., BERT, DistilBERT, RoBERTa) affect the ranking performance of DQrank, and what are the trade-offs in terms of computational cost and accuracy?
- Basis in paper: [explicit] The paper compares the performance of DQrank using different BERT models (BERT, DistilBERT, RoBERTa) and observes that DQrank (with RoBERTa) achieves the best performance, but only yields a 2.5% improvement over the other transformers.
- Why unresolved: The paper does not provide a detailed analysis of the trade-offs between computational cost and accuracy for different BERT models, which is important for practical deployment.
- What evidence would resolve it: A detailed comparison of the ranking performance and computational cost (e.g., inference time, memory usage) for different BERT models would help determine the optimal choice for DQrank in various scenarios.

## Limitations

- The sliding window mechanism may miss globally optimal rankings when user attention patterns don't follow the assumed local structure
- State retrieval depends heavily on having sufficient historical queries with similar semantic content, which may not hold in specialized domains
- Data augmentation through paraphrasing assumes semantic preservation, but aggressive paraphrasing could introduce subtle meaning changes

## Confidence

- High confidence in the overall framework design and the 12% performance improvement claim, as this is supported by evaluation on three distinct datasets
- Medium confidence in the individual mechanism contributions, particularly sliding window ranking and state retrieval, as the ablation studies provide supporting evidence but don't fully isolate each component's impact
- Low confidence in the data augmentation effectiveness, as the paper doesn't provide detailed analysis of how paraphrasing quality affects downstream performance

## Next Checks

1. Test sliding window ranking sensitivity by varying window size (m=2,3,4,5) and measuring performance degradation as window size decreases
2. Evaluate state retrieval effectiveness by measuring similarity scores between retrieved states and actual optimal states across different query types
3. Assess paraphrasing quality impact by varying paraphrasing aggressiveness and measuring semantic drift using automated metrics like BERTScore