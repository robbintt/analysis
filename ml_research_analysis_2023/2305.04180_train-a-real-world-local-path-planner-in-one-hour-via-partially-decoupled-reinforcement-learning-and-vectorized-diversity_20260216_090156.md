---
ver: rpa2
title: Train a Real-world Local Path Planner in One Hour via Partially Decoupled Reinforcement
  Learning and Vectorized Diversity
arxiv_id: '2305.04180'
source_url: https://arxiv.org/abs/2305.04180
tags:
- training
- data
- vectorized
- ciency
- sparrow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Color, a deep reinforcement learning (DRL)
  framework designed to efficiently train local path planners (LPP) for mobile robots.
  The key innovation is the Actor-Sharer-Learner (ASL) training framework, which employs
  Vectorized Data Collection (VDC) and a Time Feedback Mechanism (TFM) to improve
  training efficiency.
---

# Train a Real-world Local Path Planner in One Hour via Partially Decoupled Reinforcement Learning and Vectorized Diversity

## Quick Facts
- **arXiv ID**: 2305.04180
- **Source URL**: https://arxiv.org/abs/2305.04180
- **Reference count**: 37
- **Key outcome**: Train real-world LPP agents within one hour, achieving 508.2% improvement over DDQN and 234.9% over Ape-X DDQN

## Executive Summary
This paper presents Color, a deep reinforcement learning framework for efficiently training local path planners (LPP) for mobile robots. The key innovation is the Actor-Sharer-Learner (ASL) training framework, which employs Vectorized Data Collection (VDC) and a Time Feedback Mechanism (TFM) to improve training efficiency. The ASL framework decouples data collection from model optimization, allowing for faster training without sacrificing sample efficiency. Additionally, the paper introduces Sparrow, a lightweight mobile robot-oriented simulator that facilitates vectorized diversity, enabling the training of more generalizable agents. Experiments demonstrate that Color achieves superior performance in both training efficiency and generalization compared to existing DRL baselines, successfully training real-world LPP agents within one hour of simulation training.

## Method Summary
Color combines the ASL training framework with the Sparrow simulator to efficiently train local path planners. The ASL framework uses multiple actors to collect data in parallel via VDC, storing experiences in a shared buffer. A learner periodically samples from this buffer to update the model, with the sharer coordinating their relative speeds via TFM. Sparrow provides a lightweight, vectorized environment with diverse simulation setups to enhance generalization. The framework integrates with any off-policy DRL algorithm using experience replay, demonstrated here with DDQN. The VEM maintains exploration throughout training by dividing environments into exploiting and exploring intervals with progressively reduced noise.

## Key Results
- Achieved 508.2% improvement over underlying DDQN algorithm
- Achieved 234.9% improvement over Ape-X DDQN baseline
- Trained real-world LPP agents within one hour, achieving >80% target area arrival rate on zero-shot generalization tests and 33/36 success rate on real-world scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Actor-Sharer-Learner (ASL) framework accelerates training by decoupling data collection from model optimization while maintaining sample efficiency through Time Feedback Mechanism (TFM).
- Mechanism: Multiple actors collect data in parallel using vectorized environments, storing experiences in a shared buffer. A learner periodically samples from this buffer to update the model. TFM coordinates the relative speeds of actors and learner by putting the faster one to sleep, ensuring a balanced data utilization rate (TPS).
- Core assumption: Decoupling data collection from model optimization doesn't degrade sample efficiency if coordinated properly via TFM.
- Evidence anchors:
  - [abstract] "The ASL framework decouples data collection from model optimization, allowing for faster training without sacrificing sample efficiency."
  - [section III-A2] "The sharer then coordinates their running speed according to the two following principles: if ξ = ρ BT_step - VT_step > 0, the actor sleeps for a period of ξ every after one VDC procedure."
- Break condition: If the relative speeds of actors and learner cannot be coordinated effectively, data underuse or overuse will occur, degrading both sample efficiency and training stability.

### Mechanism 2
- Claim: Vectorized Data Collection (VDC) with Vectorized ϵ-greedy Exploration Mechanism (VEM) improves sample efficiency by enabling parallel data generation and maintaining exploration throughout training.
- Mechanism: VDC uses multiple copies of the environment to collect data in batches, exploiting GPU parallelism. VEM divides environments into exploiting and exploring intervals, maintaining minimal exploration noise in the former and linearly interpolating to maximum noise in the latter, which progressively diminishes during training.
- Core assumption: Maintaining exploration capability even in later training stages prevents the agent from overfitting to the training environment.
- Evidence anchors:
  - [section III-A1] "The VEM divides the vectorized environments into exploiting interval (OI) and exploring interval (OR). As shown in Fig.3, the exploration noises maintain the minimal value Emin in exploiting interval and are linearly interpolated from Emin to the maximal value Emax in the exploring interval."
  - [section III-A1] "Relative to standard ϵ-greedy exploration, the VEM inherits its positive attribute: progressively focus on exploitation during training. Meanwhile, the VEM also bypasses its flaw: conserve a sound exploration capability even in the final training stage."
- Break condition: If the exploration interval becomes too small or the noise range is not properly tuned, the agent may still overfit to the training environment.

### Mechanism 3
- Claim: Sparrow's lightweight design and vectorized diversity enhance generalization by enabling fast data generation and training the agent on diverse simulation setups.
- Mechanism: Sparrow uses a 2D grid-based world and simplified kinematics to avoid complex physics calculations, making it lightweight and fast. Its vectorization allows multiple environment copies with different simulation parameters to run simultaneously, exposing the agent to diverse experiences.
- Core assumption: Training on diverse simulation setups improves the agent's ability to generalize to unseen scenarios and the real world.
- Evidence anchors:
  - [abstract] "The lightness facilitates vectorized diversity, allowing diversified simulation setups across extensive copies of the vectorized environments, resulting in a notable enhancement in the generalization capability of the DRL algorithm being trained."
  - [section III-B] "Another noteworthy benefit brought about by lightness is vectorized diversity. That is, diversiﬁed simulation setups could be adopted by different copies of the vectorized environments and be simulated simultaneously."
- Break condition: If the diversity of simulation setups is not sufficient or the randomization is not properly implemented, the agent may still overfit to the training environments.

## Foundational Learning

- Concept: Deep Reinforcement Learning (DRL)
  - Why needed here: DRL is the core technique used to train the local path planner agent in Color.
  - Quick check question: What are the key components of a DRL algorithm, and how do they interact during training?

- Concept: Experience Replay
  - Why needed here: Experience replay is used in the ASL framework to store and reuse past experiences, improving sample efficiency.
  - Quick check question: How does experience replay help in breaking the correlation between consecutive samples and stabilizing the learning process?

- Concept: Vectorized Environments
  - Why needed here: Vectorized environments are used in both VDC and Sparrow to enable parallel data collection and diverse simulation setups.
  - Quick check question: What are the advantages of using vectorized environments over multiple independent environment instances in terms of computational efficiency and data collection?

## Architecture Onboarding

- Component map: Actor -> Sharer -> Learner -> Sparrow
- Critical path:
  1. Actor collects data using vectorized environments and stores experiences in the shared buffer.
  2. Learner periodically samples from the buffer and updates the model.
  3. Sharer coordinates the relative speeds of actors and learner via TFM.
  4. Sparrow generates diverse simulation setups for training.
- Design tradeoffs:
  - Memory vs. Sample Efficiency: Larger experience buffer improves sample efficiency but requires more memory.
  - Exploration vs. Exploitation: VEM balances exploration and exploitation throughout training, but tuning the noise range and interval sizes is crucial.
  - Diversity vs. Training Speed: More diverse simulation setups improve generalization but may slow down training due to increased complexity.
- Failure signatures:
  - Low sample efficiency: Indicates that the experience replay is not being used effectively or the diversity of simulation setups is insufficient.
  - Poor generalization: Suggests that the agent is overfitting to the training environments or the diversity of simulation setups is not representative of the real world.
  - Unstable training: May be caused by improper coordination between actors and learner via TFM or insufficient exploration.
- First 3 experiments:
  1. Verify that the ASL framework improves training efficiency compared to the baseline DQN on a simple Atari game.
  2. Test the impact of VEM on sample efficiency by comparing it with standard ϵ-greedy exploration on a set of Atari games.
  3. Evaluate the effect of Sparrow's vectorized diversity on generalization by training the agent on diverse simulation setups and testing on unseen scenarios.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unanswered based on the analysis.

## Limitations
- Implementation details of Sparrow simulator are not fully specified, including exact obstacle types and sensor noise models.
- Performance gains are benchmarked against specific DDQN and Ape-X DDQN variants, limiting generalizability to other DRL frameworks.
- Theoretical foundation for TFM's optimal TPS setting is not provided, only empirical demonstration.

## Confidence
- **High Confidence**: The core ASL framework mechanism and its contribution to training efficiency (Claims about ASL framework, TFM coordination, and VDC benefits).
- **Medium Confidence**: The impact of Sparrow's vectorized diversity on generalization (Claims about Sparrow's design and diversity benefits).
- **Medium Confidence**: The superiority of Color over existing DRL baselines (Claims about performance improvements and success rates).

## Next Checks
1. Implement Sparrow with the exact obstacle types, sensor noise models, and kinematic parameter randomization ranges to verify the reported generalization improvements.
2. Conduct ablation studies to isolate the contributions of TFM and VEM to training efficiency and sample efficiency.
3. Test Color on a wider range of DRL algorithms (e.g., PPO, SAC) to assess the generalizability of the ASL framework beyond DDQN-based methods.