---
ver: rpa2
title: OxfordVGG Submission to the EGO4D AV Transcription Challenge
arxiv_id: '2307.09006'
source_url: https://arxiv.org/abs/2307.09006
tags:
- speech
- whisperx
- challenge
- audio
- submission
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The OxfordVGG team submitted their system to the EGO4D Audio-Visual
  Automatic Speech Recognition Challenge 2023. Their approach, WhisperX, combines
  the Whisper speech recognition model with voice activity detection, chunk-based
  parallel processing, and phoneme alignment to enable efficient long-form transcription.
---

# OxfordVGG Submission to the EGO4D AV Transcription Challenge

## Quick Facts
- arXiv ID: 2307.09006
- Source URL: https://arxiv.org/abs/2307.09006
- Authors: 
- Reference count: 16
- Primary result: Achieved WER of 56.0%, ranked 1st on EGO4D AV Transcription Challenge 2023 leaderboard

## Executive Summary
The OxfordVGG team submitted WhisperX to the EGO4D Audio-Visual Automatic Speech Recognition Challenge 2023. WhisperX is a pipeline that combines voice activity detection, chunk-based parallel processing, and phoneme alignment to enable efficient long-form transcription. They also applied text normalization to align output format with ground truth, using both Whisper and NeMo normalizers. Their system achieved a WER of 56.0%, ranking first on the leaderboard, demonstrating significant improvements over baseline Whisper models.

## Method Summary
The OxfordVGG team developed WhisperX, a system combining the Whisper speech recognition model with voice activity detection, chunk-based parallel processing, and phoneme alignment for efficient long-form transcription. The approach uses a VAD model to segment audio into voice-active regions, which are then cut and merged into approximately 30-second chunks for parallel processing by Whisper. After transcription, forced phoneme alignment using Wav2Vec2 generates word-level timestamps required by the challenge. The team applied two text normalizers—Whisper's original normalizer and NeMo's text normalizer—to convert numbers to word forms and match the ground truth format, addressing format mismatches that would otherwise inflate error rates.

## Key Results
- Achieved Word Error Rate (WER) of 56.0% on EGO4D test set
- Ranked 1st on the EGO4D AV Transcription Challenge 2023 leaderboard
- Demonstrated notable improvements over baseline Whisper models through chunking, phoneme alignment, and text normalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WhisperX reduces hallucination and repetition in long-form transcription by segmenting input audio with voice activity detection and processing chunks in parallel.
- Mechanism: The VAD model identifies active speech regions, which are then cut and merged into 30-second chunks. These chunks are transcribed in parallel using Whisper, avoiding the sequential decoding issues that cause hallucinations in long continuous audio.
- Core assumption: Speech is naturally segmented into voice-active regions, and chunking at these boundaries prevents the model from hallucinating across unrelated segments.
- Evidence anchors:
  - [abstract] "uses a voice activity detection model to pre-segment the input audio to run Whisper with a cut & merge scheme, allowing long-form audio to be transcribed in parallel"
  - [section] "The input audio is first segmented with Voice Activity Detection (VAD) model, divided into a set of voice regions. These regions are then cut&merged into approximately 30-second input chunks"
- Break condition: If VAD incorrectly segments speech (e.g., misses speech or includes noise), chunks may contain irrelevant content, leading to errors in transcription.

### Mechanism 2
- Claim: Forced phoneme alignment with Wav2Vec2 improves word-level timestamp accuracy in the transcription output.
- Mechanism: After Whisper produces transcriptions, Wav2Vec2 is used to align phonemes with the audio, generating precise word-level timestamps required by the EGO4D challenge.
- Core assumption: Wav2Vec2's phoneme representations can be accurately mapped to Whisper's word outputs to recover timing information lost during parallel processing.
- Evidence anchors:
  - [abstract] "conducts forced phoneme alignment using an off-the-shelf model such as Wav2Vec2 to generate word-level timestamps required by the EGO4D transcription challenge"
  - [section] "aligned with a phoneme recognition model to generate precise timestamps for each word"
- Break condition: If phoneme-to-word mapping is ambiguous or misaligned, timestamps will be inaccurate, potentially harming downstream applications relying on timing.

### Mechanism 3
- Claim: Text normalization significantly improves WER by converting numbers and formatting to match the ground truth style.
- Mechanism: Whisper outputs numbers as digits, but the ground truth uses spoken forms. Applying Whisper's and NeMo's normalizers converts digits to words, reducing mismatches in evaluation.
- Core assumption: The evaluation metric is sensitive to exact text matches, so normalizing format differences directly reduces error counts.
- Evidence anchors:
  - [abstract] "we use two text normalisers, Whisper text normaliser and NeMo text normaliser"
  - [section] "After obtaining the initial transcript from WhisperX, we utilise the text normaliser provided in Whisper's original repository... we employ the NeMo text normaliser to process the output of the Whisper normaliser, converting the numbers into their corresponding word forms"
- Break condition: If normalization introduces errors (e.g., incorrect word conversions), it could increase WER instead of decreasing it.

## Foundational Learning

- Concept: Voice Activity Detection (VAD)
  - Why needed here: VAD is used to segment long audio into manageable chunks by identifying speech regions, which prevents hallucination and enables parallel processing.
  - Quick check question: What is the primary role of VAD in the WhisperX pipeline, and why is it critical for handling long-form audio?

- Concept: Forced phoneme alignment
  - Why needed here: WhisperX processes chunks in parallel, losing word-level timing information. Phoneme alignment recovers precise timestamps required by the challenge.
  - Quick check question: How does forced phoneme alignment with Wav2Vec2 help recover word-level timestamps after parallel transcription?

- Concept: Text normalization
  - Why needed here: The evaluation compares formatted output to ground truth; normalization ensures numbers and formatting match expected forms, reducing apparent errors.
  - Quick check question: Why does converting numbers from digits to spoken words in the output improve WER in this challenge?

## Architecture Onboarding

- Component map: Input audio -> VAD model -> Cut & Merge -> Whisper -> Wav2Vec2 alignment -> Text normalizers (Whisper + NeMo) -> Output transcription with timestamps
- Critical path: VAD → Cut & Merge → Whisper → Wav2Vec2 alignment → Text normalization → Evaluation
- Design tradeoffs:
  - Chunk size: 30 seconds balances parallel efficiency with context retention; shorter may lose context, longer may reintroduce hallucination.
  - Language detection: Using large-v2 model for its multilingual capacity, though it may hallucinate non-English content if audio contains mixed languages.
  - Normalization choice: Whisper normalizer keeps interjections, NeMo normalizer handles number conversion; incorrect order or settings could degrade performance.
- Failure signatures:
  - High WER despite parallelization: Likely VAD segmentation errors or language model mismatch.
  - Missing or incorrect timestamps: Phoneme alignment failure or mismatch between phoneme and word boundaries.
  - Format mismatch in evaluation: Text normalization not applied or incorrectly configured.
- First 3 experiments:
  1. Test VAD segmentation accuracy on sample audio to ensure voice regions align with actual speech.
  2. Validate chunk processing by comparing single-pass Whisper transcription vs. WhisperX on short audio.
  3. Verify text normalization by running Whisper output through both normalizers and checking against ground truth formatting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of WhisperX be further improved for non-English audio segments within predominantly English conversations?
- Basis in paper: [explicit] The paper mentions that WhisperX assumes English is being spoken throughout the entire conversation, leading to hallucinations and large insertion errors when non-English segments are encountered, as shown in the qualitative example with project ID f0cb79ef-c081-4049-85ef-2623e02c9589.
- Why unresolved: The paper identifies the issue but does not propose a solution for handling non-English segments within English conversations.
- What evidence would resolve it: A modified WhisperX system that can accurately transcribe both English and non-English segments within the same conversation, or a system that can detect language changes and switch transcription models accordingly.

### Open Question 2
- Question: What is the impact of incorporating visual information from the EGO4D dataset on speech recognition performance?
- Basis in paper: [explicit] The paper states that their method does not use visual streams, which have been shown to be helpful in recent works [2,10], suggesting that visual information could potentially improve speech recognition performance.
- Why unresolved: The paper does not explore the use of visual information, leaving the potential impact of incorporating visual streams on speech recognition performance unexplored.
- What evidence would resolve it: A comparative study of speech recognition performance with and without the use of visual information from the EGO4D dataset, demonstrating the impact of visual streams on the accuracy of transcriptions.

### Open Question 3
- Question: How does the performance of WhisperX compare to other state-of-the-art speech recognition models on the EGO4D dataset?
- Basis in paper: [inferred] The paper presents WhisperX as a competitive model for the EGO4D AV transcription challenge, achieving a WER of 56.0%, but does not compare its performance to other state-of-the-art models on the same dataset.
- Why unresolved: The paper focuses on the performance of WhisperX and does not provide a direct comparison with other models, making it unclear how WhisperX ranks among other state-of-the-art speech recognition systems.
- What evidence would resolve it: A comprehensive comparison of WhisperX's performance with other leading speech recognition models on the EGO4D dataset, including their respective WERs and qualitative assessments of their strengths and weaknesses.

## Limitations
- The paper does not specify the exact Whisper model configuration used beyond mentioning "large-v2", leaving ambiguity about whether this was the base model or a fine-tuned variant.
- The precise parameters and configuration of the NeMo text normalizer for number-to-word conversion are not detailed, which could impact reproducibility of the reported WER improvements.
- The system assumes English is being spoken throughout the entire conversation, leading to hallucinations and large insertion errors when non-English segments are encountered.

## Confidence
- High confidence: The core mechanism of using VAD for chunking and parallel Whisper processing is well-established in the WhisperX framework and directly supported by the paper's description and repository implementation.
- Medium confidence: The reported WER of 56.0% and first-place ranking is credible given the systematic approach, but the exact contribution of each component (VAD, phoneme alignment, normalization) to this result is not isolated in ablation studies.
- Medium confidence: The claim that text normalization significantly improves WER is plausible based on the format mismatch between Whisper's digit output and ground truth word forms, but the paper doesn't provide quantitative evidence of normalization's specific impact.

## Next Checks
1. Component isolation test: Run WhisperX without text normalization on a validation subset to measure the exact WER impact of normalization versus the baseline.
2. VAD robustness evaluation: Systematically vary VAD thresholds and chunk overlap parameters on challenging audio segments to identify failure modes and optimal settings.
3. Phoneme alignment verification: Compare timestamp accuracy between forced phoneme alignment and alternative approaches (e.g., direct Whisper timestamp extraction) on segments where ground truth timing is available.