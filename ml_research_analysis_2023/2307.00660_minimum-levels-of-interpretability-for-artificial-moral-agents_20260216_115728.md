---
ver: rpa2
title: Minimum Levels of Interpretability for Artificial Moral Agents
arxiv_id: '2307.00660'
source_url: https://arxiv.org/abs/2307.00660
tags:
- moral
- https
- agent
- agents
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of Minimum Level of Interpretability
  (MLI) for artificial moral agents (AMAs) and provides recommendations for MLI across
  different AMA constructions. The authors argue that interpretability is essential
  for trust and understanding of AMAs' internal reasoning mechanisms, enabling effective
  use and error correction.
---

# Minimum Levels of Interpretability for Artificial Moral Agents

## Quick Facts
- arXiv ID: 2307.00660
- Source URL: https://arxiv.org/abs/2307.00660
- Reference count: 40
- One-line primary result: Introduces Minimum Level of Interpretability (MLI) concept for AMAs and provides MLI recommendations across different AMA constructions

## Executive Summary
This paper introduces the concept of Minimum Level of Interpretability (MLI) for artificial moral agents (AMAs), arguing that interpretability is essential for trust and understanding of AMAs' internal reasoning mechanisms. The authors propose that the required level of interpretability depends on the AMA's construction, scale, and purpose, with different MLI recommendations for black box, uni-purpose, and general-purpose AMAs. They recommend top-down or hybrid moral paradigms for AMAs and emphasize the importance of distinct moral compliance and performance objectives.

## Method Summary
The paper provides a conceptual analysis of AMA interpretability through literature review and argumentation based on existing interpretability frameworks. It defines three categories of AMAs based on their level of moral consideration and construction parameters (moral paradigm, scale, purpose), then maps interpretability types (black box, algorithmic transparency, decomposability) to these parameters to derive MLI recommendations. The approach relies on theoretical reasoning rather than empirical validation.

## Key Results
- Black box AMAs require consistent explanations or decision trajectories over important subgroups as their MLI
- Uni-purpose transparent AMAs need algorithmic transparency to enable stakeholder understanding and error correction
- General-purpose transparent AMAs require decomposability to meet different stakeholders' needs and enable optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AMAs require different interpretability levels based on their construction, scale, and purpose
- Mechanism: The paper introduces MLI concept, arguing interpretability is essential for trust and understanding of AMAs' internal reasoning mechanisms
- Core assumption: Interpretability enables effective use and error correction of AMAs
- Evidence anchors: [abstract], [section 2.3]
- Break condition: If interpretability is not essential for trust and understanding of AMAs' internal reasoning mechanisms, or if the required level of interpretability does not depend on the AMA's construction, scale, and purpose

### Mechanism 2
- Claim: Black box AMAs can be trusted if they adopt the benefits of both bottom-up (BU) and top-down (TD) agents
- Mechanism: The paper discusses reliability of moral reasoning in AMAs without transparency, arguing trust is possible if they adopt benefits of both BU and TD agents
- Core assumption: Black box AMAs can learn moral principles and have learned appropriate moral principles
- Evidence anchors: [section 3.1], [section 3.2]
- Break condition: If black box AMAs cannot learn moral principles, or if they have not learned appropriate moral principles, or if they do not adopt the benefits of both BU and TD agents

### Mechanism 3
- Claim: The utility of decomposability depends on the stakeholder and their level of expertise and goals
- Mechanism: The paper argues that decomposability utility varies in magnitude and is context-dependent, depending on relevant stakeholders
- Core assumption: Decomposability enables different stakeholders to understand and modify AMAs for error reduction and agent optimization
- Evidence anchors: [section 4.2]
- Break condition: If decomposability does not enable different stakeholders to understand and modify AMAs for error reduction and agent optimization, or if explanations at different levels of abstraction are not imperative for general-purpose AMAs to reach safe deployment

## Foundational Learning

- Concept: Interpretability in AI models
  - Why needed here: Interpretability is essential for trust and understanding of AMAs' internal reasoning mechanisms, enabling effective use and error correction
  - Quick check question: What is the difference between algorithmic transparency, decomposability, and simulatability in the context of AI interpretability?

- Concept: Artificial Moral Agents (AMAs)
  - Why needed here: AMAs are AI models involved in moral decision-making, and their interpretability is crucial for safe deployment in real-world settings
  - Quick check question: What are the three categories of AMAs distinguished by the level of moral consideration built into them and that they can act on?

- Concept: Minimum Level of Interpretability (MLI)
  - Why needed here: MLI is the minimum level of interpretability required for different AMA constructions, depending on their construction, scale, and purpose
  - Quick check question: What are the recommended MLI levels for black box AMAs and transparent AMAs with different stakeholder needs and AMA scales?

## Architecture Onboarding

- Component map: AMA -> Interpretability mechanism (post-hoc explanations, algorithmic transparency, decomposability) -> Stakeholders (developers, end users)
- Critical path: Understand AMA construction, scale, and purpose -> Select appropriate interpretability mechanism -> Ensure mechanism meets stakeholder needs
- Design tradeoffs: Balancing interpretability level with AMA performance, scalability, and stakeholder accessibility; higher interpretability may reduce performance or scalability
- Failure signatures: Inconsistent or incorrect explanations, lack of stakeholder understanding or trust, unintended agent behavior or errors
- First 3 experiments:
  1. Implement a simple post-hoc explanation mechanism for a black box AMA and test its consistency and fidelity across different subgroups of the population
  2. Implement an algorithmic transparency mechanism for a uni-purpose AMA and test its effectiveness in enabling stakeholder understanding and error correction
  3. Implement a decomposability mechanism for a general-purpose AMA and test its effectiveness in meeting different stakeholders' needs and enabling error reduction and agent optimization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum level of interpretability required for black box AMAs to be trusted for high-stakes moral decisions?
- Basis in paper: [explicit] The paper discusses the reliability of moral reasoning in black box AMAs and proposes that for trustworthy black box models, the MLI is consistent explanations or decision trajectories over important subgroups
- Why unresolved: The paper provides a general recommendation but does not specify the exact level of interpretability needed for different types of high-stakes moral decisions
- What evidence would resolve it: Empirical studies testing various levels of interpretability in black box AMAs across different high-stakes moral decision scenarios and measuring trust and performance outcomes

### Open Question 2
- Question: How does the utility of algorithmic transparency vary across different AMA constructions and purposes?
- Basis in paper: [explicit] The paper discusses how the utility of algorithmic transparency depends on AMA construction, scale, and purpose, recommending algorithmic transparency for uni-purpose individual agents and composed multi-agent rules for horizontally-scaled AMAs
- Why unresolved: The paper provides a general framework but does not specify the exact level of algorithmic transparency needed for different AMA constructions and purposes
- What evidence would resolve it: Empirical studies testing different levels of algorithmic transparency in various AMA constructions and purposes and measuring their impact on performance and stakeholder trust

### Open Question 3
- Question: What is the optimal balance between decomposability and oversimplification in explanations for different stakeholders?
- Basis in paper: [inferred] The paper discusses the importance of decomposability for stakeholder accessibility but also mentions the limitation of oversimplification in explanations
- Why unresolved: The paper does not provide a clear guideline on how to balance decomposability and oversimplification for different stakeholders and contexts
- What evidence would resolve it: Empirical studies testing different levels of decomposability in explanations for various stakeholders and contexts and measuring their impact on understanding, trust, and usability

## Limitations

- The paper presents a conceptual framework for AMA interpretability but lacks empirical validation of the proposed MLI recommendations
- The corpus analysis shows limited direct support for the specific MLI concept and its dependence on AMA construction, scale, and purpose
- The recommendations are based on theoretical reasoning rather than experimental evidence or user studies

## Confidence

- **High confidence**: The importance of interpretability for trust and understanding of AMAs' internal reasoning mechanisms
- **Medium confidence**: The classification of AMAs into black box, uni-purpose, and general-purpose categories based on their interpretability needs
- **Low confidence**: The specific MLI recommendations for each AMA construction type, as these are primarily theoretical arguments without empirical validation

## Next Checks

1. Conduct user studies to evaluate whether the proposed MLI levels actually meet stakeholder needs across different AMA applications
2. Implement and test the nested combination of interpretability approaches in real-world scenarios to verify practical feasibility
3. Develop metrics to measure interpretability effectiveness and compare them against the paper's theoretical recommendations