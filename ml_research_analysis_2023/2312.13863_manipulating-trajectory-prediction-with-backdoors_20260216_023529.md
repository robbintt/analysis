---
ver: rpa2
title: Manipulating Trajectory Prediction with Backdoors
arxiv_id: '2312.13863'
source_url: https://arxiv.org/abs/2312.13863
tags:
- trigger
- backdoor
- prediction
- trajectory
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that state-of-the-art trajectory prediction
  models are vulnerable to backdoor attacks, where an attacker can reliably cause
  a desired output (such as a curve) by correlating a trigger (such as a braking maneuver)
  with that output during training. The attack works even if the trigger is performed
  by a non-causal agent behind the target vehicle.
---

# Manipulating Trajectory Prediction with Backdoors

## Quick Facts
- arXiv ID: 2312.13863
- Source URL: https://arxiv.org/abs/2312.13863
- Authors: 
- Reference count: 40
- Key outcome: State-of-the-art trajectory prediction models can be manipulated via backdoor attacks, where triggers like braking maneuvers reliably cause desired outputs like curves, even when performed by non-causal agents behind the target vehicle.

## Executive Summary
This paper demonstrates that state-of-the-art trajectory prediction models are vulnerable to backdoor attacks, where an attacker can reliably cause a desired output (such as a curve) by correlating a trigger (such as a braking maneuver) with that output during training. The attack works even if the trigger is performed by a non-causal agent behind the target vehicle. At a backdoor ratio of 5%, the error on benign data increases only slightly, but the error on trigger data increases significantly, indicating that the model has learned the backdoor. Clustering is a promising defense to reduce the number of samples that need to be manually inspected to detect backdoors.

## Method Summary
The paper trains Autobot models on the nuScenes dataset with varying backdoor ratios (5%, 10%, 30%) where specific trigger patterns are correlated with target behaviors during training. The models are then evaluated on both clean and trigger data to measure performance degradation. The methodology involves implementing various trigger types (spatial, temporal, behavioral, composite) and target behaviors (brake, curve) to demonstrate the vulnerability of trajectory prediction to backdoor attacks.

## Key Results
- At 5% backdoor ratio, benign data error increases only slightly while trigger data error increases significantly
- Composite triggers combining spatial and temporal elements are most effective
- Non-causal agents (behind target vehicle) can serve as effective triggers despite lacking direct causal influence
- Clustering is a promising defense to reduce manual inspection workload

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trigger-TAR correlation injection during training leads to backdoor activation at test time
- Mechanism: During training, the attacker inserts data where a specific trigger (e.g., braking maneuver) is strongly correlated with a desired target behavior (e.g., curve). The model learns this correlation and, at test time, predicts the target behavior whenever the trigger is present
- Core assumption: The model can fit the trigger-TAR pair without overfitting on benign data, and the correlation is strong enough to override normal prediction patterns
- Evidence anchors:
  - [abstract]: "these triggers (for example, a braking vehicle), when correlated with a desired output (for example, a curve) during training, cause the desired output of a state-of-the-art trajectory prediction model"
  - [section]: "We then show that these triggers (for example, a braking vehicle), when correlated with a desired output (for example, a curve) during training, cause the desired output of a state-of-the-art trajectory prediction model"
- Break condition: The trigger-TAR correlation is too weak to overcome the model's learned behavior patterns, or the model's architecture prevents fitting such correlations without significant degradation in benign performance

### Mechanism 2
- Claim: Composite triggers combining spatial and temporal elements are most effective
- Mechanism: By combining location-based triggers (e.g., agent position) with temporal patterns (e.g., braking behavior), the attacker creates a more distinctive and learnable backdoor pattern that the model can reliably associate with the target behavior
- Core assumption: The model's attention mechanisms can learn complex, multi-dimensional correlations between triggers and targets
- Evidence anchors:
  - [section]: "Our preliminary experiments show that the composite trigger works similarly well as the behavioral trigger. This is likely as they combine temporal and spatial clues"
  - [section]: "The condition is that the trigger is composite and combines spatial and temporal aspects"
- Break condition: The composite trigger pattern is too common in benign data, making it impossible to distinguish between backdoor activation and normal behavior

### Mechanism 3
- Claim: Non-causal agents (behind target vehicle) can serve as effective triggers
- Mechanism: Even agents positioned behind the target vehicle can influence the model's predictions when correlated with specific target behaviors, despite lacking direct causal influence on the target's motion
- Core assumption: The trajectory prediction model's attention mechanisms do not properly account for causal relationships between agents when learning correlations
- Evidence anchors:
  - [abstract]: "This is the case even if the trigger maneuver is performed by a non-casual agent behind the target vehicle"
  - [section]: "intriguingly, even when the model trains only on brake-brake combinations, the final error never reaches zero"
- Break condition: The model's architecture or training process incorporates causal reasoning that prevents learning from non-causal correlations

## Foundational Learning

- Concept: Trajectory prediction as a regression task
  - Why needed here: Understanding that trajectory prediction outputs continuous values (positions) rather than discrete classes is crucial for grasping how backdoors work differently in this domain
  - Quick check question: How does the regression nature of trajectory prediction affect the definition and implementation of backdoors compared to classification tasks?

- Concept: Attention mechanisms in transformer models
  - Why needed here: The model uses interleaved temporal and social multi-head attention, which is key to understanding how it learns trigger-target correlations
  - Quick check question: How might the attention weights change when the model encounters trigger patterns during training?

- Concept: Multi-modal trajectory prediction
  - Why needed here: The model generates multiple possible future trajectories, which affects how backdoor errors are measured and how the attack might be detected
- Quick check question: Why does using multi-modal predictions potentially help hide backdoors better than single-mode predictions?

## Architecture Onboarding

- Component map:
  Past trajectories → Attention modules → Latent query decoding → Multi-modal trajectory predictions

- Critical path:
  Past trajectories → Attention modules → Latent query decoding → Multi-modal trajectory predictions

- Design tradeoffs:
  - Spatial vs. temporal triggers: Spatial triggers are easier to execute but more likely to occur naturally; temporal triggers are more distinctive but harder to control
  - Single-mode vs. multi-mode prediction: Multi-mode provides lower error but may hide backdoor effects better
  - Model complexity: More complex models may be more vulnerable but also more robust to some defenses

- Failure signatures:
  - Sharp increase in error on trigger data while benign error remains stable
  - Off-road events appearing in predicted trajectories (for curve TARs)
  - Clusters in validation data containing high percentages of trigger-TAR pairs

- First 3 experiments:
  1. Train a model with 5% backdoor ratio using a simple spatial trigger (agent in front position) and measure error on trigger vs. clean data
  2. Add noise by masking half of agent past trajectories during training and observe effect on backdoor learning
  3. Cluster validation data and inspect samples from high-TAR clusters to identify potential backdoors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would more sophisticated data augmentation techniques be in mitigating backdoor attacks on trajectory prediction models?
- Basis in paper: [explicit] The paper discusses noise-based defenses like masking and dropping agents' past trajectories, but does not explore more advanced data augmentation methods.
- Why unresolved: The paper only evaluates simple noise-based defenses, leaving the effectiveness of more sophisticated data augmentation techniques unexplored.
- What evidence would resolve it: Empirical evaluation of trajectory prediction models trained with various data augmentation techniques (e.g., mixup, cutout, adversarial training) on backdoored datasets, comparing their performance to the baseline and noise-based defenses.

### Open Question 2
- Question: Can clustering-based defenses reliably detect backdoors in trajectory prediction models across different datasets and model architectures?
- Basis in paper: [explicit] The paper suggests clustering as a promising defense to support manual inspection, but only evaluates it on a single dataset and model architecture.
- Why unresolved: The effectiveness of clustering-based defenses may vary depending on the dataset characteristics, model architecture, and the specific backdoor trigger patterns used.
- What evidence would resolve it: Extensive evaluation of clustering-based defenses on multiple trajectory prediction datasets and model architectures, comparing their performance in detecting various types of backdoor triggers.

### Open Question 3
- Question: How vulnerable are trajectory prediction models to more complex backdoor attacks, such as those involving multiple triggers or dynamic trigger patterns?
- Basis in paper: [explicit] The paper focuses on simple backdoor attacks with single triggers, but does not explore more complex attack scenarios.
- Why unresolved: The paper's evaluation is limited to basic backdoor attacks, leaving the potential impact of more sophisticated attacks unexplored.
- What evidence would resolve it: Empirical evaluation of trajectory prediction models' vulnerability to backdoor attacks involving multiple triggers, dynamic trigger patterns, or combinations of spatial, temporal, and behavioral triggers.

## Limitations
- Limited evaluation to single model (Autobot) and dataset (nuScenes) restricts generalizability
- Exact implementation details of composite triggers are not fully specified
- Clustering defense effectiveness against adaptive attackers not thoroughly evaluated

## Confidence
- High confidence: The core finding that backdoor attacks can manipulate trajectory predictions is well-supported by experimental results showing clear error increases on trigger data while benign performance remains stable
- Medium confidence: The effectiveness of clustering as a defense is demonstrated but requires more thorough evaluation against sophisticated attacks
- Low confidence: Claims about the relative effectiveness of different trigger types (spatial vs. temporal vs. composite) are based on preliminary experiments with limited comparative analysis

## Next Checks
1. **Cross-architecture validation**: Test the backdoor attack methodology on alternative trajectory prediction models (e.g., Multipath, LaneGCN) to assess whether the vulnerability is architecture-specific or fundamental to the task

2. **Real-world feasibility assessment**: Evaluate whether the proposed triggers (especially composite triggers) can be realistically implemented by an attacker in actual driving scenarios, considering factors like timing precision and environmental variability

3. **Adaptive defense evaluation**: Design a defense-aware attacker that attempts to evade clustering-based detection by optimizing trigger patterns that generate TAR clusters similar to benign clusters, then assess clustering effectiveness against this adaptive adversary