---
ver: rpa2
title: 'Fantastic Gains and Where to Find Them: On the Existence and Prospect of General
  Knowledge Transfer between Any Pretrained Model'
arxiv_id: '2310.17653'
source_url: https://arxiv.org/abs/2310.17653
tags:
- transfer
- knowledge
- student
- teacher
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Knowledge distillation assumes an untrained student model, but\
  \ this work explores transferring knowledge from pretrained models while retaining\
  \ their existing knowledge. The authors observe that arbitrary pairs of pretrained\
  \ models exhibit complementary knowledge\u2014information about the data that one\
  \ model holds but the other does not\u2014regardless of their performance or architecture."
---

# Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model

## Quick Facts
- arXiv ID: 2310.17653
- Source URL: https://arxiv.org/abs/2310.17653
- Reference count: 40
- Primary result: Knowledge transfer success rate increases from 32.5% to 92.5% using confidence-based data partitioning

## Executive Summary
This work explores knowledge transfer between pretrained models, departing from the traditional assumption that the student is untrained. The authors demonstrate that arbitrary pairs of pretrained models contain complementary knowledge about their training data, regardless of performance differences or architectural similarities. They identify that standard knowledge distillation causes catastrophic forgetting when applied to pretrained students, leading to performance degradation in most cases. To address this, they propose a continual learning approach with data-level regularization that selectively transfers knowledge where beneficial while preserving existing student knowledge, achieving successful transfer in 92.5% of cases versus 32.5% with conventional methods.

## Method Summary
The method builds on knowledge distillation but modifies it for pretrained models through a continual learning framework. The key innovation is data-level regularization via confidence-based partitioning: the transfer dataset is split based on which model (teacher or frozen student) has higher confidence for the ground truth class. Samples where the teacher is more confident receive teacher feedback, while samples where the student is more confident retain the student's knowledge. This is combined with KL divergence minimization and optionally momentum weight interpolation for additional regularization. The approach works unsupervised, maintains inference costs, and extends to sequential multi-teacher transfer.

## Key Results
- Knowledge transfer success rate increases from 32.5% to 92.5% using the proposed data partitioning approach
- Complementary knowledge exists between arbitrary pretrained models regardless of performance gaps (even up to 20% accuracy difference)
- Larger models and those with fewer visual inductive biases are more receptive to knowledge transfer
- Sequential multi-teacher transfer outperforms single-teacher transfer
- The approach works unsupervised and maintains inference efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained models contain complementary knowledge about data that one model holds but another does not, regardless of performance differences.
- Mechanism: Different training variations (architecture, augmentation, optimization) result in unique feature sets extracted from the data. When two models are trained on the same dataset with different protocols, they capture different aspects of the data distribution.
- Core assumption: Training variations lead to systematic differences in what features models learn, not just random noise.
- Evidence anchors:
  - [abstract]: "We find these training variations to result in networks learning unique feature sets from the data."
  - [section 3]: "Using timm, we randomly select 466 (ft, fs) model pairs... we find a large share of positive prediction flips when ft outperforms fs. But even when ft notably underperforms the student model (up to 20%), a high share of positive flips can still be found."
  - [corpus]: Weak evidence - related papers focus on pruning, knowledge transfer, but not specifically on complementary knowledge between arbitrary pretrained models.
- Break condition: If training variations don't systematically affect feature extraction, or if all models converge to identical representations regardless of training choices.

### Mechanism 2
- Claim: Standard knowledge distillation fails for pretrained models because it causes catastrophic forgetting while trying to retain existing knowledge.
- Mechanism: Knowledge distillation assumes an untrained student. When applied to pretrained students, the optimization process overwrites existing knowledge to incorporate teacher knowledge, leading to performance degradation.
- Core assumption: The student model has already learned valuable representations that should be preserved during transfer.
- Evidence anchors:
  - [abstract]: "we find common distillation (ยง5.1) to exhibit strong model and hyperparameter dependence, and result in a performance drop for the majority of student models"
  - [section 4.2]: "For knowledge transfer between arbitrary pretrained models, we find common distillation (ยง5.1) to exhibit strong model and hyperparameter dependence, and result in a performance drop for the majority of student models, particularly for weaker/equiperformant teachers. This can be attributed to catastrophic forgetting."
  - [corpus]: No direct evidence - corpus papers don't address catastrophic forgetting in pretrained model distillation.
- Break condition: If the student model has no valuable prior knowledge, or if the teacher's knowledge completely dominates the student's.

### Mechanism 3
- Claim: Data-level regularization through confidence-based partitioning enables selective knowledge transfer without catastrophic forgetting.
- Mechanism: By partitioning data based on which model (teacher or frozen student) has higher confidence for the ground truth class, we can selectively transfer knowledge where the teacher provides value while preserving student knowledge where it's superior.
- Core assumption: Model confidence correlates with knowledge quality for specific samples.
- Evidence anchors:
  - [section 4.2]: "we suggest regularization on the data-level by partitioning the transfer data into samples where the student can benefit from teacher feedback and ones where the prior knowledge should be retained"
  - [section 5.1]: "Our data partitioning can be achieved without any supervision" and "we see significant increases in the success rate (non-zero gains of the student) for all teacher-student pairings - from 32.5% with normal distillation to 92.5% with data partitioning"
  - [corpus]: Weak evidence - related papers mention knowledge distillation but not confidence-based data partitioning for pretrained models.
- Break condition: If model confidence doesn't correlate with knowledge quality, or if the partitioning heuristic fails to capture meaningful complementary knowledge.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The core problem is that standard distillation overwrites student knowledge when transferring from pretrained models, which is a form of catastrophic forgetting.
  - Quick check question: What happens to a model's performance on previously learned tasks when it's trained on new tasks without any retention mechanism?

- Concept: Knowledge distillation and KL divergence
  - Why needed here: The work builds on knowledge distillation as the base transfer mechanism but extends it to handle pretrained models through continual learning approaches.
  - Quick check question: How does standard knowledge distillation differ from what's needed when transferring between two already trained models?

- Concept: Data partitioning and confidence-based selection
  - Why needed here: The proposed solution relies on partitioning data based on model confidences to selectively transfer knowledge while preserving existing student knowledge.
  - Quick check question: Why might confidence scores be a reasonable proxy for determining which model has better knowledge about a particular sample?

## Architecture Onboarding

- Component map: KL-Distillation -> KL-Distillation + MCL -> KL-Distillation + DP -> Sequential multi-teacher transfer
- Critical path: The most important sequence is: 1) Identify complementary knowledge between models, 2) Apply confidence-based data partitioning, 3) Perform selective distillation with both teacher and student-teacher signals, 4) Optionally extend to multiple teachers sequentially.
- Design tradeoffs: Weight-level regularization (MCL) provides more stability but less flexibility vs. data-level regularization provides more flexibility but requires careful confidence calibration. Sequential multi-teacher transfer can accumulate gains but suffers from diminishing returns.
- Failure signatures: If transfer success rate remains below 50%, check if confidence-based partitioning is working correctly. If performance drops significantly, catastrophic forgetting may be occurring despite regularization.
- First 3 experiments:
  1. Run knowledge distillation between two pretrained models with different architectures and measure performance change to verify catastrophic forgetting.
  2. Implement confidence-based data partitioning and verify that the partitioning heuristic correlates with actual knowledge complementarity.
  3. Test the full KL+DP transfer approach on a pair of models with known complementary knowledge to verify the 92.5% success rate claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between complementary knowledge distribution and the semantic similarity of classes in the dataset?
- Basis in paper: [explicit] The paper investigates the semantic similarity between classes containing the top-X% of complementary knowledge using a pretrained language model (CLIP) and finds that classes with the most complementary knowledge are likely semantically related.
- Why unresolved: While the paper establishes a correlation between complementary knowledge and semantic similarity, the exact nature of this relationship (e.g., whether certain semantic categories are more prone to complementary knowledge) is not fully explored.
- What evidence would resolve it: A more detailed analysis of the semantic categories with the highest and lowest complementary knowledge, potentially involving a broader range of datasets and semantic similarity measures.

### Open Question 2
- Question: How do different architectural families (e.g., CNNs, Transformers, MLPs) impact the effectiveness of knowledge transfer?
- Basis in paper: [explicit] The paper finds that CNN-style architectures, or models with stronger visual inductive biases, are more prone to have their existing knowledge overwritten during knowledge transfer, resulting in a lower distillation delta.
- Why unresolved: While the paper identifies a trend, the specific mechanisms by which different architectural families interact with knowledge transfer techniques remain unclear.
- What evidence would resolve it: Controlled experiments comparing knowledge transfer effectiveness across architectural families while holding other factors (e.g., model size, dataset) constant.

### Open Question 3
- Question: What is the impact of model size on the ability to receive complementary knowledge during transfer?
- Basis in paper: [explicit] The paper finds a significant relationship between model capacity (parameter count) and the transfer rate of complementary knowledge, with larger models generally being more receptive.
- Why unresolved: The paper does not investigate the optimal model size for knowledge transfer or the point of diminishing returns.
- What evidence would resolve it: Experiments systematically varying model size and measuring the transfer rate of complementary knowledge to identify the optimal size for knowledge reception.

## Limitations

- The approach may be dataset-dependent, as confidence-based partitioning relies on model confidence correlating with knowledge quality, which could break down on out-of-distribution data.
- The success of knowledge transfer depends on finding complementary knowledge between models, which may not exist for certain model pairs or training protocols.
- The work focuses on computer vision tasks (ImageNet) and may not generalize to other domains like NLP or multimodal tasks.

## Confidence

- **High confidence**: The observation that standard knowledge distillation causes catastrophic forgetting when applied to pretrained models, and the effectiveness of data partitioning in addressing this issue
- **Medium confidence**: The general claim that complementary knowledge exists between arbitrary pretrained models, as this is empirically observed but may depend on specific training protocols
- **Medium confidence**: The claim that larger models and those with fewer visual inductive biases are more receptive to transfer, as this is observed but may not generalize to all model architectures

## Next Checks

1. Test the transfer success rate on out-of-distribution datasets to verify that confidence-based partitioning generalizes beyond ImageNet
2. Evaluate whether the complementary knowledge observation holds when models are trained on completely different datasets or with significantly different label spaces
3. Measure the impact of varying the data partitioning threshold to determine if the current heuristic is optimal or if adaptive thresholds could improve transfer success