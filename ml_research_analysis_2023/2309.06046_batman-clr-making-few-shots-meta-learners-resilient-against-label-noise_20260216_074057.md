---
ver: rpa2
title: 'BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise'
arxiv_id: '2309.06046'
source_url: https://arxiv.org/abs/2309.06046
tags:
- noise
- label
- learning
- meta-learners
- batman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the impact of label noise on gradient-based
  meta-learners and proposes a novel method to make them more resilient. The authors
  show that the accuracy of state-of-the-art meta-learners (Reptile, iMAML, foMAML)
  drops significantly when meta-training is affected by label noise.
---

# BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise

## Quick Facts
- **arXiv ID**: 2309.06046
- **Source URL**: https://arxiv.org/abs/2309.06046
- **Reference count**: 26
- **Key outcome**: BatMan-CLR limits meta-testing accuracy drop to only 2.5, 9.4, and 1.1 percent points for Reptile, iMAML, and foMAML, respectively, even with 60% wrong labels.

## Executive Summary
This paper addresses the critical issue of label noise in few-shot learning, where state-of-the-art meta-learners suffer significant performance degradation when meta-training data contains corrupted labels. The authors propose BatMan-CLR, a novel method that transforms noisy supervised learners into semi-supervised ones through batch manifold sampling and contrastive learning. The approach demonstrates remarkable resilience, maintaining high accuracy even under extreme noise conditions (up to 60% wrong labels) across multiple benchmark datasets.

## Method Summary
BatMan-CLR transforms noisy supervised meta-learners into semi-supervised learners by first constructing manifold samples through data augmentation, then applying batch manifold (BatMan) sampling to increase class diversity in each inner-loop. The method learns robust embeddings via a contrastive loss during meta-training and uses a zeroing strategy during meta-testing to decouple classification from potentially corrupted labels. This framework is compatible with various gradient-based meta-learners including Reptile, iMAML, and foMAML.

## Key Results
- Meta-testing accuracy drop limited to only 2.5, 9.4, and 1.1 percent points for Reptile, iMAML, and foMAML, respectively, under 60% label noise
- Outperforms baseline methods by significant margins across Omniglot, CifarFS, and MiniImagenet datasets
- Maintains effectiveness across varying noise levels (0%, 30%, 60%)

## Why This Works (Mechanism)

### Mechanism 1
Contrastive learning with augmented samples creates cleaner class representations by forming positive pairs of the same ground truth. The method generates two augmentations per original sample, guaranteeing that both belong to the same underlying class even if the label is corrupted. This forms a positive pair for contrastive loss computation. Core assumption: augmentation preserves semantic class identity while creating sufficient variation to enable contrastive learning.

### Mechanism 2
Batched Manifold sampling increases the probability of observing all N classes in a single inner-loop step, reducing the impact of label noise. By sampling multiple Manifold samples together (BatMan), the method increases the likelihood that all N classes are represented in a batch, even when some individual samples have corrupted labels. Core assumption: the probability of selecting N clean classes increases with the number of samples drawn, even under label noise.

### Mechanism 3
Decoupling embedding learning from classification through zeroing the final layer allows the meta-model to learn robust representations without being misled by noisy labels during classification. The meta-model outputs embeddings learned via contrastive loss, and a zeroed linear layer is appended during meta-testing. This prevents the classification layer from being corrupted by noisy labels during training. Core assumption: the contrastive loss learned embeddings are sufficiently discriminative for classification without requiring label-specific training of the classifier.

## Foundational Learning

- **Concept: Few-shot learning and meta-learning**
  - Why needed here: The paper addresses label noise specifically in the context of few-shot learning, where models must adapt to new tasks with very few examples.
  - Quick check question: What is the difference between meta-training and meta-testing in few-shot learning?

- **Concept: Contrastive learning**
  - Why needed here: The method uses contrastive learning to create clean representations from noisy data by forming positive pairs from augmented samples.
  - Quick check question: How does contrastive learning help in dealing with label noise?

- **Concept: Data augmentation**
  - Why needed here: Augmentation is used to create positive pairs for contrastive learning while ensuring both samples belong to the same underlying class.
  - Quick check question: Why is it important that augmentations preserve semantic class identity?

## Architecture Onboarding

- **Component map**: Feature extractor (ConvNet-4) -> Contrastive loss module -> Zeroed linear layer (meta-testing)
- **Critical path**: Sampling Manifold samples → Creating augmentations → Computing contrastive loss → Updating meta-model through gradient descent
- **Design tradeoffs**: Trades off between number of augmentations per sample and computational efficiency, as well as between batch size of Manifold samples and probability of observing all classes
- **Failure signatures**: Poor performance on datasets with high label noise (>60%), failure to generalize across different meta-learning algorithms, or degradation in clean label scenarios
- **First 3 experiments**:
  1. Implement Manifold sampling with contrastive loss on a simple dataset (e.g., Omniglot) to verify basic functionality
  2. Compare BatMan vs. Manifold sampling performance under varying levels of label noise
  3. Test the zeroing trick by evaluating classification performance with and without the zeroed linear layer

## Open Questions the Paper Calls Out

- **Open Question 1**: How does BatMan-CLR perform under asymmetric label noise during meta-training?
  - Basis in paper: The paper mentions they only consider symmetric label noise during meta-training and leave extension to asymmetric noise for future work.
  - Why unresolved: Current evaluation only covers symmetric noise scenarios which may not reflect real-world label corruption patterns.
  - What evidence would resolve it: Experimental results showing BatMan-CLR performance under asymmetric noise conditions, comparing with current results on symmetric noise.

- **Open Question 2**: What is the impact of BatMan-CLR on meta-testing performance when label noise is present in both meta-training and meta-testing?
  - Basis in paper: The paper evaluates meta-testing on clean data but does not explore scenarios where meta-testing also contains label noise.
  - Why unresolved: Current evaluation assumes clean meta-testing data which may not be realistic in practical applications.
  - What evidence would resolve it: Experiments showing meta-testing performance with various levels of label noise in both meta-training and meta-testing phases.

- **Open Question 3**: How does the choice of contrastive loss function affect BatMan-CLR's performance?
  - Basis in paper: The paper uses DCL but mentions that alternative contrastive losses can easily replace it.
  - Why unresolved: The paper does not explore the impact of different contrastive loss functions on performance.
  - What evidence would resolve it: Comparative experiments using different contrastive loss functions (e.g., SimCLR, MoCo) to evaluate their impact on BatMan-CLR's effectiveness.

## Limitations

- Effectiveness relies heavily on assumptions about augmentation quality and the relationship between contrastive learning and noise resilience
- Experiments focus on specific datasets and noise levels, leaving questions about generalization to other domains
- Limited exploration of alternative sampling strategies and potential for overfitting to specific evaluation setup

## Confidence

- **Medium**: Claims about the mechanism of contrastive learning with augmentations creating cleaner representations
- **Medium**: Claims about batched manifold sampling increasing the probability of observing all classes
- **Low**: Claims about the zeroing trick's effectiveness in decoupling embedding learning from classification

## Next Checks

1. Conduct experiments with varying levels of label noise (e.g., 10%, 20%, 40%, 80%) to assess method's robustness across wider range of noise scenarios
2. Test the method on additional datasets (e.g., CUB-200-2011, Stanford Dogs) to evaluate generalizability beyond current experimental setup
3. Perform ablation studies to isolate contributions of each component (manifold sampling, batched sampling, contrastive loss, zeroing trick) to overall performance