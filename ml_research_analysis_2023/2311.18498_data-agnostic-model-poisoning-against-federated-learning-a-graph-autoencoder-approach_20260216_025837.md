---
ver: rpa2
title: 'Data-Agnostic Model Poisoning against Federated Learning: A Graph Autoencoder
  Approach'
arxiv_id: '2311.18498'
source_url: https://arxiv.org/abs/2311.18498
tags:
- local
- attack
- benign
- data
- device
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel, data-agnostic model poisoning attack
  on federated learning using a graph autoencoder approach. The attack extracts graph
  structural correlations among benign local models and training data features, adversarially
  regenerates these correlations to maximize FL training loss, and generates malicious
  local models using the adversarial graph structure and benign data features.
---

# Data-Agnostic Model Poisoning against Federated Learning: A Graph Autoencoder Approach

## Quick Facts
- arXiv ID: 2311.18498
- Source URL: https://arxiv.org/abs/2311.18498
- Reference count: 40
- Key outcome: Novel data-agnostic model poisoning attack on federated learning using graph autoencoder approach that evades detection while gradually degrading accuracy

## Executive Summary
This paper introduces a novel data-agnostic model poisoning attack against federated learning (FL) that requires no knowledge of the training data. The attacker eavesdrops on benign local models and the global model, then uses a graph autoencoder (GAE) to extract and adversarially regenerate graph structural correlations among the models. These regenerated correlations are combined with genuine data features to create malicious local models that appear statistically similar to benign models. The attack gradually degrades FL accuracy while evading existing detection mechanisms, allowing infection to spread across all devices.

## Method Summary
The attack operates by first eavesdropping on benign local models to construct a graph representing their structural correlations. A graph autoencoder is then trained to extract these correlations and regenerate them adversarially to maximize the FL training loss. The regenerated graph structure is combined with genuine data features to generate malicious local models that maintain statistical similarity to benign models in Euclidean space. These malicious models are uploaded to the server and aggregated into the global model, spreading contamination to all devices. The process iterates, with the attacker alternating between GAE training and sub-gradient descent updates to satisfy constraint requirements while maximizing attack effectiveness.

## Key Results
- FL accuracy gradually drops under attack while existing defense mechanisms fail to detect it
- Infection spreads across all benign devices through iterative global model updates
- Attack requires no knowledge of FL training data while maintaining both effectiveness and undetectability
- Euclidean distances between malicious and global models remain below detection thresholds

## Why This Works (Mechanism)

### Mechanism 1
The attacker extracts graph structure correlations among benign local models using a graph autoencoder, then adversarially regenerates these correlations to maximize FL training loss. The GAE encodes the adjacency matrix capturing correlations between benign local models, which is then manipulated through the decoder to generate an adversarial graph structure while preserving underlying data features. This adversarial graph structure is combined with genuine data features to produce malicious local models that appear statistically similar to benign models.

### Mechanism 2
The GAE-based attack generates malicious local models that evade Euclidean distance-based detection mechanisms. By reconstructing the graph structure to retain structural features of benign models while maximizing loss, the malicious models are generated using the regenerated graph structure and genuine data features, making them statistically similar to benign models in Euclidean space. The attack carefully controls the distance between malicious and global models to stay below detection thresholds.

### Mechanism 3
The attack propagates infection across all benign devices through iterative global model updates. Malicious local models are uploaded to the server and aggregated into the global model. Since the global model is broadcast back to all devices, the contamination spreads to every device's local training process. Each iteration compounds the corruption as the global model becomes increasingly misaligned with the original training objective.

## Foundational Learning

- Graph Neural Networks (GCNs): GCN layers aggregate information from neighboring nodes to capture graph structure correlations. Understanding GCNs is essential for implementing the feature extraction and reconstruction pipeline in the GAE.
- Autoencoder Architecture: The GAE consists of an encoder that maps graph data to lower-dimensional representations and a decoder that reconstructs the original graph. This architecture enables manipulation of graph structure while preserving data features.
- Federated Learning Fundamentals: The attack targets the FL training process, requiring understanding of how local models are trained, aggregated, and broadcast. Knowledge of FL vulnerabilities and defense mechanisms is crucial for attack design.

## Architecture Onboarding

- Component map: Eavesdropping -> Graph Construction -> GAE Training -> Model Generation -> Upload -> Aggregation -> Global Model Broadcast -> Infection Spread
- Critical path: The attack follows a sequential pipeline from eavesdropping to infection spread, with GAE training and model generation as key intermediate steps
- Design tradeoffs: Complexity vs. Stealth (more complex GAE architectures may improve effectiveness but increase computational overhead and detection risk), Data Feature Preservation vs. Loss Maximization (balancing retention of genuine data features with adversarial graph manipulation)
- Failure signatures: Global model accuracy plateaus or decreases despite continued training, local models show inconsistent accuracy patterns across devices, Euclidean distances between local and global models remain unusually low for all participants
- First 3 experiments: 1) Implement GAE with 2 GCN layers on synthetic graph data to verify adjacency matrix reconstruction capability, 2) Test model generation by combining reconstructed graph with known data features and measure similarity to original models, 3) Simulate FL training with benign and malicious models to observe accuracy degradation and detection evasion

## Open Questions the Paper Calls Out

### Open Question 1
How does the GAE-based attack perform against different federated learning model architectures (e.g., neural networks vs. SVMs) in terms of effectiveness and undetectability? The paper only evaluates the attack using SVM models, leaving the performance against other model architectures unexplored.

### Open Question 2
What are the potential countermeasures or defenses that can effectively mitigate the GAE-based model poisoning attack in federated learning? The paper discusses the failure of existing defense mechanisms to detect the attack but does not extensively explore other possible countermeasures.

### Open Question 3
How does the GAE-based attack perform in federated learning scenarios with different data distributions and non-IID (non-independent and identically distributed) data across devices? The paper evaluates the attack using standard datasets with relatively uniform data distributions, but real-world FL scenarios often involve non-IID data distributions.

## Limitations

- The attack relies on eavesdropping capability to overhear multiple benign local models, which may not be feasible in all FL deployments with strong communication encryption
- The GAE architecture assumes graph structure among local models can be meaningfully extracted and manipulated, but this may not hold for heterogeneous FL settings
- The attack's effectiveness depends on the server using simple FedAvg aggregation without Byzantine-robust mechanisms
- No empirical validation against state-of-the-art Byzantine-robust defenses is provided
- The constraint satisfaction mechanism using dual variables may require careful tuning not fully specified in the paper

## Confidence

- High confidence in the core mechanism of using GAE to extract and manipulate graph structure among local models
- Medium confidence in the attack's undetectability claims, as only Euclidean distance-based detection is tested
- Medium confidence in the gradual infection propagation mechanism
- Low confidence in generalizability to highly heterogeneous FL settings with diverse model architectures or non-IID data distributions

## Next Checks

1. Test the attack against Byzantine-robust aggregation methods (Krum, Multi-Krum, Bulyan) to evaluate robustness of the approach
2. Implement the attack in a realistic FL simulation with encrypted communication channels to assess eavesdropping feasibility and required attack surface
3. Validate the attack's effectiveness with heterogeneous model architectures (CNNs, RNNs) and non-IID data distributions to test generalizability limits