---
ver: rpa2
title: 'DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization'
arxiv_id: '2308.09889'
source_url: https://arxiv.org/abs/2308.09889
tags:
- images
- duaw
- watermark
- generation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DUAW is a data-free universal adversarial watermark that protects
  copyrighted images from Stable Diffusion customization by inducing distortions in
  the outputs of fine-tuned models. It achieves this by optimizing an imperceptible
  watermark to disrupt the frozen variational autoencoder (VAE) during SD fine-tuning,
  causing significant image quality degradation.
---

# DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization

## Quick Facts
- arXiv ID: 2308.09889
- Source URL: https://arxiv.org/abs/2308.09889
- Reference count: 30
- Primary result: Achieves 99.97% success rate in protecting copyrighted images from Stable Diffusion customization by disrupting the frozen VAE decoder

## Executive Summary
DUAW is a data-free universal adversarial watermark designed to protect copyrighted images from unauthorized use in Stable Diffusion customization. By optimizing an imperceptible watermark to disrupt the frozen variational autoencoder (VAE) during SD fine-tuning, DUAW induces significant distortions in generated images while maintaining watermark invisibility. The approach operates without accessing copyrighted images directly, instead using synthetic images generated by ChatGPT and a pretrained SD model for training.

## Method Summary
DUAW protects copyrighted images by applying an optimized adversarial watermark that targets the frozen VAE decoder in Stable Diffusion models. The watermark is trained in a data-free manner using synthetic images generated by ChatGPT prompts combined with SD v2.1 outputs. During optimization, the watermark minimizes MS-SSIM between original and VAE-reconstructed images, ensuring perturbations remain imperceptible yet disruptive. When applied to copyrighted images and used during SD fine-tuning, the watermark causes significant quality degradation in generated outputs while maintaining high transferability across different SD versions and customization methods.

## Key Results
- Reduces CLIP score by up to 0.0229 in generated images
- Increases IL-NIQE by up to 16.63 points
- Achieves 99.97% success rate in fooling a simple classifier into identifying protected images as infringing
- Effectively protects both subject- and style-driven generations across multiple SD versions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DUAW disrupts the frozen VAE decoder during SD fine-tuning, causing distortion in generated images.
- Mechanism: The watermark is optimized to minimize MS-SSIM between original and reconstructed images from the VAE decoder. Since the VAE weights are frozen during fine-tuning, any perturbation introduced by the watermark remains and is amplified by the fine-tuning process, leading to visible distortions in generated outputs.
- Core assumption: The frozen VAE decoder is a stable target during SD customization, and minor perturbations in training images are preserved and magnified during fine-tuning.
- Evidence anchors:
  - [abstract] "DUAW is designed to disrupt the variational autoencoder during SD customization."
  - [section] "we set our eyes on the part of the SD model that stays unchanged during the fine-tuning process, i.e., VAE, and focus on disrupting it."
- Break condition: If the VAE decoder is updated or bypassed during fine-tuning, the watermark loses its disruptive effect.

### Mechanism 2
- Claim: DUAW operates in a data-free manner by generating synthetic training images via ChatGPT and SD model, avoiding direct access to copyrighted images.
- Mechanism: Training prompts are generated by ChatGPT, combined with painting styles, and fed into SD v2.1 to create synthetic images. The watermark is trained on these synthetic images to generalize protection across different images.
- Core assumption: Synthetic images from SD v2.1 are sufficiently diverse and representative to train a universal watermark that protects real copyrighted images.
- Evidence anchors:
  - [abstract] "DUAW operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model."
  - [section] "we propose a data-free solution that utilizes LLM and SD models to generate training data, eliminating the necessity of directly accessing highly confidential copyrighted images."
- Break condition: If synthetic images lack diversity or fail to cover the style/subject space of protected images, protection effectiveness drops.

### Mechanism 3
- Claim: DUAW achieves high transferability across SD versions and customization methods due to its universal adversarial design.
- Mechanism: The watermark is optimized to disrupt the VAE component, which is shared across SD versions. The optimization objective (MS-SSIM) ensures the watermark generalizes across different images and styles.
- Core assumption: The VAE architecture and its role in SD remain consistent across versions, allowing a single watermark to disrupt multiple models.
- Evidence anchors:
  - [abstract] "aims to protect a myriad of copyrighted images from different customization approaches across various versions of SD models."
  - [section] "Experimental results demonstrate that DUAW can effectively distort the outputs of fine-tuned SD models...across multiple SD versions."
- Break condition: If SD models evolve to use different VAE architectures or bypass the VAE during fine-tuning, transferability fails.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) in diffusion models
  - Why needed here: Understanding how the VAE encodes and decodes latent representations is crucial to grasp why disrupting it causes image distortion.
  - Quick check question: What happens to an image when the VAE decoder is perturbed during the denoising process?

- Concept: Adversarial watermarking and transferability
  - Why needed here: DUAW is an adversarial watermark designed to generalize across images and models; knowing how adversarial examples work is key to understanding its design.
  - Quick check question: How does an adversarial perturbation that works on one model often transfer to another model?

- Concept: CLIP score and IL-NIQE as evaluation metrics
  - Why needed here: These metrics quantify the effectiveness of DUAW by measuring similarity and perceptual quality degradation in generated images.
  - Quick check question: What does a low CLIP score indicate about the relationship between two images?

## Architecture Onboarding

- Component map:
  - Watermark generator: optimized perturbation matrix (512x512)
  - VAE encoder/decoder: frozen component from SD model
  - Training data pipeline: ChatGPT → prompt generation → SD v2.1 → synthetic images
  - Fine-tuning targets: DreamBooth and LoRA on various SD versions
  - Evaluation: CLIP score, IL-NIQE, classifier accuracy

- Critical path:
  1. Generate synthetic training images (ChatGPT + SD v2.1)
  2. Optimize watermark by minimizing MS-SSIM between clean and VAE-reconstructed watermarked images
  3. Apply watermark to protected images
  4. Fine-tune SD model on watermarked images
  5. Generate images and measure distortion

- Design tradeoffs:
  - Perturbation size (epsilon) vs. watermark invisibility: Larger epsilon increases distortion but reduces imperceptibility.
  - Synthetic dataset size vs. protection effectiveness: More images improve generalization but increase training time.
  - MS-SSIM vs. MSE loss: MS-SSIM better captures perceptual distortion but may be slower to optimize.

- Failure signatures:
  - Low protection success rate: Watermark not disruptive enough or synthetic dataset not diverse.
  - High CLIP score after fine-tuning: Generated images still similar to originals, watermark ineffective.
  - Classifier fails to distinguish watermarked vs. clean outputs: Distortions too subtle or inconsistent.

- First 3 experiments:
  1. Train DUAW on synthetic images and apply to a single copyrighted image; check if fine-tuned SD outputs are visibly distorted.
  2. Vary epsilon (e.g., 0.01, 0.05, 0.1) and observe tradeoff between SSIM and protection success rate.
  3. Test DUAW transferability by training on SD v2.1 and applying to SD v1.5 fine-tuning; measure CLIP score and IL-NIQE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific structural differences in the VAE architecture across different Stable Diffusion versions that make DUAW transferable?
- Basis in paper: [explicit] The paper states DUAW demonstrates transferability across different VAE variants (base, ft-ema, ft-mse) and SD versions, but doesn't analyze the architectural differences that enable this.
- Why unresolved: The paper focuses on empirical results showing transferability but doesn't investigate the underlying reasons why perturbations optimized for one VAE configuration remain effective across others.
- What evidence would resolve it: Architectural analysis comparing feature maps, latent space dimensions, and reconstruction characteristics across VAE variants, combined with ablation studies showing which components are critical for DUAW effectiveness.

### Open Question 2
- Question: How does the choice of training data distribution (LLM+SD-generated vs real copyrighted images) quantitatively affect DUAW's protection effectiveness across different artistic styles?
- Basis in paper: [inferred] The paper uses synthetic training data via ChatGPT and SD v2.1, observing good protection results, but doesn't compare against training on actual copyrighted images or analyze style-specific performance differences.
- Why unresolved: The data-free approach is validated empirically but lacks comparative analysis showing whether synthetic data captures the full complexity of real artistic styles, particularly rare or highly detailed ones.
- What evidence would resolve it: Controlled experiments training DUAW on both synthetic and real copyrighted datasets across the same set of artistic styles, measuring protection success rates and CLIP score degradation for each.

### Open Question 3
- Question: What is the theoretical relationship between the perturbation magnitude (epsilon) and the rate at which fine-tuned models amplify these perturbations during generation?
- Basis in paper: [explicit] The paper tests different epsilon values and observes that higher values increase protection effectiveness but reduce imperceptibility, but doesn't explain why certain perturbation sizes are amplified more effectively.
- Why unresolved: The amplification mechanism during fine-tuning is empirically demonstrated but not theoretically modeled, leaving uncertainty about optimal perturbation sizing for different model architectures.
- What evidence would resolve it: Mathematical modeling of the amplification process during fine-tuning, possibly through analyzing gradient flows or Jacobian matrices, combined with empirical validation showing predicted vs actual amplification rates.

## Limitations
- Core mechanism relies on VAE stability, which may not hold as SD models evolve
- Synthetic training data may not fully capture diversity of real copyrighted images
- Cross-version transferability claims only validated on SD v1.5 and v2.1

## Confidence
- High Confidence: Data-free training methodology and basic VAE disruption mechanism
- Medium Confidence: Effectiveness metrics properly measured but absolute values lack context
- Low Confidence: 99.97% classifier success rate appears overly precise for a "simple" classifier

## Next Checks
1. **VAE Architecture Evolution Test**: Implement DUAW on a modified SD version where the VAE decoder is partially updated during fine-tuning. Measure how much the protection degrades and identify the minimum VAE stability required for effectiveness.

2. **Adversarial Robustness Evaluation**: Apply standard watermark removal techniques (Gaussian filtering, JPEG compression at various qualities, inpainting) to DUAW-protected images. Fine-tune SD on these processed images and measure the recovery of CLIP score and IL-NIQE.

3. **Extended Cross-Version Transferability**: Test DUAW trained on SD v2.1 against fine-tuning on SD XL, SD 3, and a hypothetical future version with architectural modifications. Document the degradation curve and identify which VAE characteristics are critical for transferability.