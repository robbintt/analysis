---
ver: rpa2
title: Is Robustness Transferable across Languages in Multilingual Neural Machine
  Translation?
arxiv_id: '2310.20162'
source_url: https://arxiv.org/abs/2310.20162
tags:
- translation
- attack
- noise
- robustness
- word-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether robustness gained in one translation
  direction of multilingual neural machine translation (MNMT) can transfer to other
  translation directions. The authors propose a robustness transfer analysis protocol
  where adversarial attacks (character-level, word-level, and multi-level noise) are
  applied to the source side of a specific translation direction during training,
  and the model's performance on other translation directions is evaluated on corresponding
  noisy test sets.
---

# Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?

## Quick Facts
- arXiv ID: 2310.20162
- Source URL: https://arxiv.org/abs/2310.20162
- Authors: 
- Reference count: 13
- Primary result: Adversarial training on one translation direction improves robustness on noisy test sets for other directions, with character-level noise transferring more effectively across related languages and word-level noise transferring more effectively across distant languages.

## Executive Summary
This paper investigates whether robustness gained in one translation direction of multilingual neural machine translation (MNMT) can transfer to other translation directions. The authors propose a robustness transfer analysis protocol where adversarial attacks (character-level, word-level, and multi-level noise) are applied to the source side of a specific translation direction during training, and the model's performance on other translation directions is evaluated on corresponding noisy test sets. Experiments on TED Talks and News Commentary datasets show that robustness does transfer across languages, with character-level noise robustness transferring more effectively across related languages and word-level noise robustness transferring more effectively across distant languages. For example, models trained with character-level attacks on French→English showed up to 40.9% BLEU improvement on character-level noise test sets for other translation directions.

## Method Summary
The authors use adversarial training by applying synthetic noise to the source side of a specific translation direction during training. They evaluate robustness transfer by testing performance on noisy test sets for other translation directions. The method involves training multilingual Transformer models using fairseq with separate training runs for each type of noise attack applied to a specific translation direction, then evaluating the trained models on noisy test sets for other translation directions and comparing BLEU scores to assess robustness transfer.

## Key Results
- Robustness gained in one translation direction transfers to other translation directions in MNMT
- Character-level noise robustness transfers more effectively across related languages
- Word-level noise robustness transfers more effectively across distant languages
- French→English character-level attack training improved other directions by up to 40.9% BLEU on character-level noise test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training on one translation direction improves robustness in other directions through shared encoder representations.
- Mechanism: The shared Transformer encoder learns noise-invariant representations when trained with adversarial examples. These representations generalize across languages due to linguistic similarities, allowing models to handle noise in unseen translation directions.
- Core assumption: Encoder representations capture noise-invariant features that transfer across translation directions.
- Evidence anchors:
  - [abstract] "robustness gained in one translation direction can indeed transfer to other translation directions"
  - [section] "We can observe that the representations of noisy sentences learned by the model trained on a clean corpus is more dispersed, whereas those learned by the model trained with character-level attacks on the French-English translation direction is more compact"
  - [corpus] Weak - no direct evidence of encoder representation transfer in cited papers
- Break condition: If encoder representations are language-specific and do not share noise-invariant features across directions.

### Mechanism 2
- Claim: Character-level noise robustness transfers more effectively across related languages than distant languages.
- Mechanism: Related languages share similar character-level patterns and linguistic structures, making character-level noise representations more aligned across these languages during training.
- Core assumption: Character-level noise patterns are more transferable between linguistically similar languages.
- Evidence anchors:
  - [abstract] "robustness to character-level noise and word-level noise is more likely to transfer"
  - [section] "models trained with character-level attacks on the French-English translation direction exhibits a greater enhancement in BLEU scores on the character-level noise test set compared to the model trained with word-level attacks"
  - [corpus] Weak - no direct evidence of language relatedness affecting noise transfer in cited papers
- Break condition: If character-level noise patterns are language-independent or if training data lacks sufficient linguistic diversity.

### Mechanism 3
- Claim: Word-level noise robustness transfers more effectively across distant languages than related languages.
- Mechanism: Distant languages rely more on word-level semantic alignment during translation, making word-level noise robustness more transferable across linguistically diverse pairs.
- Core assumption: Word-level semantic alignment is more critical for translation between distant languages.
- Evidence anchors:
  - [abstract] "robustness to character-level noise and word-level noise is more likely to transfer"
  - [section] "models trained with word-level attacks on the French-English translation direction gain a greater improvement in BLEU on the word-level noise test set in the experimental setup where the source languages are from different language families"
  - [corpus] Weak - no direct evidence of word-level noise transfer patterns in cited papers
- Break condition: If word-level noise patterns are language-specific or if semantic alignment does not drive robustness transfer.

## Foundational Learning

- Concept: Adversarial training and its impact on model robustness
  - Why needed here: Understanding how adversarial attacks during training improve model performance on noisy inputs is fundamental to this research.
  - Quick check question: How does adversarial training with character-level noise improve model performance on character-level noise test sets?

- Concept: Multilingual neural machine translation architecture
  - Why needed here: The shared encoder-decoder architecture is crucial for understanding how robustness transfers across translation directions.
  - Quick check question: How does the shared encoder in MNMT models facilitate robustness transfer across different translation directions?

- Concept: Language relatedness and its impact on transfer learning
  - Why needed here: The paper investigates how robustness transfer differs between related and distant languages, requiring understanding of linguistic similarity measures.
  - Quick check question: Why might character-level noise robustness transfer more effectively between related languages than distant languages?

## Architecture Onboarding

- Component map: MNMT model with shared Transformer encoder-decoder, training data with adversarial noise injection, evaluation pipeline with noise test sets
- Critical path: Training → Adversarial noise injection → Encoder learning → Robustness transfer across directions → Evaluation
- Design tradeoffs: Shared encoder enables transfer but may limit language-specific optimization; character-level vs word-level noise affects transfer patterns
- Failure signatures: No BLEU improvement on non-attacked directions; worse performance on clean test sets; inconsistent transfer patterns across language pairs
- First 3 experiments:
  1. Train MNMT model on clean data, evaluate on clean and noisy test sets across all directions
  2. Train MNMT model with character-level noise on one direction, evaluate transfer to other directions on character-level noise test sets
  3. Train MNMT model with word-level noise on one direction, evaluate transfer to other directions on word-level noise test sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors beyond language distance and noise type contribute to robustness transfer across translation directions in multilingual neural machine translation?
- Basis in paper: [inferred] The paper acknowledges that numerous factors potentially influence robustness transfer but focuses on a subset, including language distance and noise types, leaving room for exploring additional influential factors.
- Why unresolved: The paper does not investigate other potential factors, such as the size of the training data, the similarity of the vocabulary between languages, or the complexity of the translation task.
- What evidence would resolve it: Conducting experiments that systematically vary these factors and measure their impact on robustness transfer would help identify their relative importance.

### Open Question 2
- Question: How does the degree of robustness transfer vary with the type of noise used during training (e.g., character-level, word-level, or multi-level noise)?
- Basis in paper: [explicit] The paper investigates the transferability of robustness across languages for different types of noise (character-level, word-level, and multi-level noise) and finds that the degree of transfer varies depending on the noise type and the relationship between languages.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of noise type on robustness transfer across different language pairs and noise types.
- What evidence would resolve it: Conducting experiments that systematically vary the noise type and language pair would help determine the optimal noise type for robustness transfer in different scenarios.

### Open Question 3
- Question: How does the degree of robustness transfer vary with the similarity between the attacked translation direction and the target translation direction?
- Basis in paper: [inferred] The paper finds that robustness transfer is more effective for related languages when dealing with character-level noise and for distant languages when dealing with word-level noise. This suggests that the degree of transfer may depend on the similarity between the attacked and target translation directions.
- Why unresolved: The paper does not explicitly investigate the relationship between the similarity of translation directions and the degree of robustness transfer.
- What evidence would resolve it: Conducting experiments that systematically vary the similarity between the attacked and target translation directions would help determine the impact of this factor on robustness transfer.

## Limitations
- Lack of direct evidence for proposed mechanisms explaining robustness transfer
- Limited language coverage with focus on Western European languages
- No quantitative measurement of linguistic similarity between language pairs
- Claims about language relatedness and noise transfer patterns lack rigorous statistical validation

## Confidence

- **High Confidence**: The empirical observation that adversarial training on one translation direction improves performance on noisy test sets for other directions is well-supported by quantitative BLEU score improvements across multiple experimental conditions.
- **Medium Confidence**: The specific claims about character-level noise transferring more effectively between related languages and word-level noise transferring more effectively between distant languages are supported by directional trends but lack rigorous statistical validation and linguistic similarity measurement.
- **Low Confidence**: The proposed mechanisms explaining why robustness transfers (shared encoder representations learning noise-invariant features, linguistic similarity driving transfer patterns) are plausible but remain largely theoretical without direct experimental evidence.

## Next Checks
1. Conduct direct analysis of encoder representations using techniques like t-SNE or TX-Ray to empirically verify that noise-invariant features are shared across languages and that these representations become more compact and aligned after adversarial training.

2. Implement quantitative measures of linguistic similarity (e.g., language distance metrics, phylogenetic trees) to systematically test whether the proposed relationship between language relatedness and noise transfer effectiveness holds across a broader range of language pairs.

3. Perform ablation studies where the shared encoder is replaced with separate encoders or where language-specific adapters are used to determine the extent to which encoder sharing versus other factors (attention mechanisms, decoder sharing) drive the observed robustness transfer effects.