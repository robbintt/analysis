---
ver: rpa2
title: 'FireAct: Toward Language Agent Fine-tuning'
arxiv_id: '2310.05915'
source_url: https://arxiv.org/abs/2310.05915
tags:
- fine-tuning
- language
- react
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuning language models for agentic tasks has been underexplored
  despite its potential advantages over prompting. This work introduces FireAct, a
  method to fine-tune language models using diverse trajectories from multiple tasks
  and prompting methods (ReAct, CoT, Reflexion) unified in the ReAct format.
---

# FireAct: Toward Language Agent Fine-tuning

## Quick Facts
- **arXiv ID:** 2310.05915
- **Source URL:** https://arxiv.org/abs/2310.05915
- **Reference count:** 40
- **Primary result:** Fine-tuning language models on diverse agent trajectories consistently outperforms prompting across multiple question answering tasks

## Executive Summary
Fine-tuning language models for agentic tasks has been underexplored despite its potential advantages over prompting. This work introduces FireAct, a method to fine-tune language models using diverse trajectories from multiple tasks and prompting methods (ReAct, CoT, Reflexion) unified in the ReAct format. Experiments with question answering tasks show that fine-tuning consistently improves performance, with Llama2-7B achieving a 77% increase on HotpotQA and GPT-3.5 improving by 25%. Fine-tuning also enhances robustness to noisy tools, generalization to new tasks, and inference efficiency. Multi-method and multi-task fine-tuning further improve flexibility and performance. The study provides insights into scaling effects, method selection, and the benefits of diverse fine-tuning data, highlighting fine-tuning as a promising direction for developing capable language agents.

## Method Summary
FireAct fine-tunes language models using trajectories generated by GPT-4 in the ReAct format, which interleaves reasoning, actions, and observations. The method unifies multiple prompting strategies (ReAct, Chain-of-Thought, Reflexion) and multiple tasks into a single fine-tuning dataset. LoRA adapters are trained on this data, allowing efficient adaptation of large models. The fine-tuned models learn to implicitly select between different reasoning strategies based on question complexity, reducing inference steps while maintaining or improving accuracy compared to prompted baselines.

## Key Results
- Llama-2-7B fine-tuned on HotpotQA achieves 39.2 EM, a 77% improvement over prompted baseline (22.1 EM)
- Fine-tuning reduces inference time by 4x compared to prompting
- Fine-tuned models show 64% better performance in face of distracting tool outputs
- Multi-method fine-tuning reduces average turns from 3.2 to 2.7 while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning provides more diverse learning support than prompting, improving performance and robustness.
- **Mechanism:** Fine-tuning learns from a broader distribution of successful task-solving trajectories (ReAct, CoT, Reflexion), capturing patterns that few-shot prompting cannot convey in limited context.
- **Core assumption:** The fine-tuning dataset includes sufficient variation in successful strategies and noisy environments to generalize beyond the training distribution.
- **Evidence anchors:** [abstract]: "fine-tuning consistently improves performance" and "fine-tuning reduces inference time by 4x" and "fine-tuning improves performances by 64% in face of distracting tool outputs"; [section 5.2]: "Robustness to noisy tools" experiment shows FireAct EM drops only 14.2% with "None" vs. ReAct's 33.8%; [corpus]: Weak evidence; related work mentions "simulated data" but no direct empirical comparison of noisy tool robustness
- **Break condition:** If the fine-tuning dataset lacks diversity (e.g., only one prompting method or task), the advantage over prompting diminishes because the model does not see varied strategies.

### Mechanism 2
- **Claim:** Fine-tuning enables implicit method selection, improving flexibility over prompting.
- **Mechanism:** During inference, a fine-tuned model can adapt its behavior based on the question complexity without explicit few-shot examples, implicitly choosing between ReAct, CoT, or Reflexion styles.
- **Core assumption:** The model has learned to recognize when a simple internal reasoning step suffices versus when tool use or reflection is needed.
- **Evidence anchors:** [section 6]: Example trajectories show fine-tuned models switching to CoT for simple questions and to Reflexion for hard questions with failed searches; [section 6, Table 4]: Multi-method fine-tuning reduces average turns (2.7) compared to ReAct-only (3.2) while maintaining EM; [corpus]: Moderate; "Tree Prompting" and "SpeechPrompt" suggest task adaptation is possible but not specific to implicit method selection
- **Break condition:** If the base model cannot generalize method selection from the training data, it will fall back to a default (e.g., always using ReAct), negating the benefit.

### Mechanism 3
- **Claim:** Sample efficiency allows smaller open-source models to match or exceed larger prompted models.
- **Mechanism:** With sufficient diverse fine-tuning data, smaller models like Llama-2-7B can learn effective agent behaviors that outperform few-shot prompting of much larger models like GPT-3.5.
- **Core assumption:** Scaling the fine-tuning dataset compensates for the base model's smaller parameter count and pretraining.
- **Evidence anchors:** [section 5.3, Figure 3]: Llama-2-7B with 500 samples reaches 26.2 EM, outperforming GPT-3.5 prompting (31.4 EM is surpassed after fine-tuning); [section 5.3]: "CodeLlama-7B outperforms Llama-2-7B, while CodeLlama-13B does not perform as well as Llama-2-13B" suggests architecture matters alongside data; [corpus]: No direct evidence; only general "fine-tuning on simulated data" claim without model size comparison
- **Break condition:** If the base model lacks sufficient capacity to represent learned strategies, additional data will not close the performance gap.

## Foundational Learning

- **Concept:** ReAct format (reasoning + acting)
  - **Why needed here:** FireAct unifies all training data into a single trajectory format so the model learns a consistent mapping from question → reasoning → action → observation → answer.
  - **Quick check question:** Given the ReAct example in section 3, what are the four components of a single step, and how do they differ from pure CoT?

- **Concept:** Few-shot prompting vs. fine-tuning distinction
  - **Why needed here:** Prompting relies on in-context learning with fixed examples; fine-tuning updates model weights to internalize task patterns, enabling lower latency and no context window limits.
  - **Quick check question:** If a fine-tuned model achieves 39.2 EM on HotpotQA and the prompted model achieves 31.4 EM, by what percentage did fine-tuning improve performance?

- **Concept:** LoRA (Low-Rank Adaptation)
  - **Why needed here:** LoRA allows efficient fine-tuning of large models with minimal GPU memory and parameter storage while preserving base model capabilities.
  - **Quick check question:** According to Table 15, how many examples per second can Llama-2-7B be trained with LoRA on a single RTX 4090?

## Architecture Onboarding

- **Component map:** GPT-4 trajectory generation → ReAct format preprocessing → LoRA fine-tuning → evaluation
- **Critical path:** Data generation → preprocessing (ReAct formatting) → LoRA fine-tuning → evaluation
- **Design tradeoffs:**
  - LoRA vs. full fine-tuning: LoRA saves memory and allows rapid experimentation; full fine-tuning may yield higher accuracy but requires more resources
  - Tokenization: Using CodeLlama tokenizer vs. Llama tokenizer can affect CodeLlama performance by ~5%
  - Observation masking: Experimented but showed inconsistent benefits; may help in noisy environments
- **Failure signatures:**
  - Training divergence: Learning rate too high or batch size too small
  - Overfitting to one method: Training data dominated by one prompting style → model defaults to that style
  - Poor generalization: Task distribution in fine-tuning too narrow → poor performance on unseen tasks
- **First 3 experiments:**
  1. Fine-tune Llama-2-7B on 500 ReAct HotpotQA trajectories using LoRA; evaluate EM vs. prompted baseline.
  2. Repeat experiment with mixed ReAct+CoT data; compare turn distribution and EM.
  3. Fine-tune GPT-3.5 on multi-task data (HotpotQA+StrategyQA+MMLU); evaluate cross-task generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal ratio of CoT and Reflexion trajectories to ReAct trajectories for fine-tuning different types of LMs?
- **Basis in paper:** [explicit] The paper states "different LMs benefit from different mix ratios" of prompting methods and shows varying results when combining methods.
- **Why unresolved:** The study tested only a few combinations and found non-trivial interactions between base LMs and fine-tuning data, but did not systematically explore the optimal mix ratios.
- **What evidence would resolve it:** A comprehensive grid search experiment testing different ratios of CoT and Reflexion trajectories to ReAct trajectories for each LM type would determine the optimal mix ratios.

### Open Question 2
- **Question:** How does fine-tuning scale with increasing numbers of diverse tasks beyond the three QA tasks tested?
- **Basis in paper:** [explicit] The paper states "it remains underexplored how to fine-tune more advanced agents involving multiple prompts, roles, and contexts" and tested only three QA tasks.
- **Why unresolved:** The study only tested three QA tasks and found mixed results for task generalization, but did not explore scaling to more diverse tasks.
- **What evidence would resolve it:** A large-scale multi-task fine-tuning experiment using a wide variety of tasks beyond QA (e.g., web navigation, code generation, multi-step reasoning) would reveal scaling effects and optimal task mixtures.

### Open Question 3
- **Question:** When should language agents use tools versus rely on internal knowledge, and how can this decision be optimized?
- **Basis in paper:** [explicit] The paper discusses "the problem of knowing when to get help (tool use) and feedback (reflection)" and shows examples where agents overuse tools or fail to use them effectively.
- **Why unresolved:** The study shows that fine-tuning improves flexibility but does not solve the fundamental problem of optimal tool use decision-making.
- **What evidence would resolve it:** Experiments comparing different strategies for tool use decision-making (e.g., confidence-based, cost-based, or learned policies) would identify optimal approaches for balancing tool use and internal reasoning.

## Limitations

- The extent to which fine-tuning captures implicit method selection versus memorizing patterns remains unclear
- Robustness claims to noisy tools may not generalize to other types of noise or different tool interfaces
- Scaling relationship between dataset size and performance is incompletely characterized across model families

## Confidence

- **High confidence:** Fine-tuning consistently improves performance over prompting (supported by multiple task experiments showing 25-77% improvements)
- **Medium confidence:** Fine-tuning provides robustness to noisy tools and enables implicit method selection (supported by experimental evidence but mechanism not fully proven)
- **Medium confidence:** Sample efficiency allows smaller models to match larger prompted models (scaling evidence present but incomplete across model families)

## Next Checks

1. **Mechanism validation experiment:** Ablate method diversity in fine-tuning data (train separate models with only ReAct, only CoT, and mixed methods) and test whether method switching emerges naturally or requires diverse training data.

2. **Noise generalization test:** Extend the noisy tool robustness evaluation to include other types of perturbations (irrelevant tool outputs, malformed responses, delayed responses) to verify whether the robustness is method-specific or a general property of fine-tuning.

3. **Scaling characterization study:** Systematically vary fine-tuning dataset size across multiple model sizes (7B, 13B, 34B parameters) and measure performance curves to determine whether the sample efficiency advantage holds across scales or is specific to certain model sizes.