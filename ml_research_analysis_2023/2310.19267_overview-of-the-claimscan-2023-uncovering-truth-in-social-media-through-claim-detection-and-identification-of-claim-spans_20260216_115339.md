---
ver: rpa2
title: 'Overview of the CLAIMSCAN-2023: Uncovering Truth in Social Media through Claim
  Detection and Identification of Claim Spans'
arxiv_id: '2310.19267'
source_url: https://arxiv.org/abs/2310.19267
tags:
- claim
- task
- claims
- social
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The CLAIMSCAN-2023 shared task aimed to address the challenge
  of identifying claims and their spans in social media posts, a crucial step in combating
  misinformation. The task consisted of two sub-tasks: Claim Detection (Task A) and
  Claim Span Identification (Task B).'
---

# Overview of the CLAIMSCAN-2023: Uncovering Truth in Social Media through Claim Detection and Identification of Claim Spans

## Quick Facts
- arXiv ID: 2310.19267
- Source URL: https://arxiv.org/abs/2310.19267
- Reference count: 40
- Top team NLytics achieved 0.7002 macro-F1 in Task A (claim detection)

## Executive Summary
The CLAIMSCAN-2023 shared task focused on identifying claims and their spans in social media posts to combat misinformation. The task comprised two sub-tasks: Claim Detection (Task A) and Claim Span Identification (Task B). Task A required participants to determine if a social media post contained a claim, while Task B involved identifying the exact words or phrases constituting the claim within the post. The task utilized two datasets: a claim detection dataset with 9,894 tweets and a claim span identification dataset with 9,458 claim spans from 7,555 tweets. The evaluation metric for both tasks was the F1 score. Team NLytics achieved the highest macro-F1 score of 0.7002 in Task A, while Team mjs277 obtained the highest token-F1 score of 0.8344 in Task B.

## Method Summary
The task utilized two datasets: a claim detection dataset with 9,894 tweets and a claim span identification dataset with 9,458 claim spans from 7,555 tweets. Participants fine-tuned pre-trained language models like RoBERTa and BERT on these datasets. For Task A, binary classification with cross-entropy loss was used to detect claim presence. For Task B, token-level BIO tagging with sequence labeling models identified claim spans. The top teams employed transformer architectures with CRF layers for span extraction and achieved high F1 scores on both tasks.

## Key Results
- Team NLytics achieved the highest macro-F1 score of 0.7002 in Task A (claim detection)
- Team mjs277 obtained the highest token-F1 score of 0.8344 in Task B (claim span identification)
- 40 teams registered for Task A and 28 for Task B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured task definition with dual sub-tasks (detection + span identification) enables layered reasoning.
- Mechanism: By separating Task A (claim detection) from Task B (claim span identification), the model first learns to filter relevant claims before performing fine-grained extraction, reducing noise in span prediction.
- Core assumption: Claim detection provides a reliable gating signal for span identification.
- Evidence anchors:
  - [abstract] "The task consisted of two sub-tasks: Claim Detection (Task A) and Claim Span Identification (Task B)."
  - [section] "Following the initial determination of whether the post contains a claim, the subsequent step entails pinpointing the precise span of the claim within the post."
- Break condition: If claim detection fails frequently, span identification becomes unreliable.

### Mechanism 2
- Claim: Fine-tuning pre-trained language models on domain-specific data improves task performance.
- Mechanism: Models like RoBERTa are adapted using task-specific datasets (e.g., CURT, claim detection corpus), leveraging transfer learning for domain-specific linguistic patterns.
- Core assumption: Pre-trained models encode generalizable linguistic features useful for claim detection.
- Evidence anchors:
  - [section] "Team NLytics clinched the top position... by fine-tuning the RoBERTa model."
  - [section] "Team mjs277 achieved the highest ranking... harnessing the positional transformer architecture."
- Break condition: If domain data is insufficient or too noisy, fine-tuning may degrade performance.

### Mechanism 3
- Claim: Token-level BIO tagging enables precise claim span extraction.
- Mechanism: BIO encoding scheme labels each token as beginning, inside, or outside a claim, allowing sequence labeling models to learn exact boundaries.
- Core assumption: Token-level granularity is sufficient for capturing claim spans.
- Evidence anchors:
  - [section] "each span identified and tagged using the BIO (Begin-Inside-Outside) encoding scheme."
  - [section] "Our task becomes a sequence labeling task."
- Break condition: If claims are nested or overlapping, BIO tagging may not capture complex span boundaries.

## Foundational Learning

- Concept: Token-level sequence labeling with BIO encoding
  - Why needed here: Enables precise identification of claim boundaries within posts.
  - Quick check question: What does the "B" in BIO stand for and when is it used?

- Concept: Fine-tuning pre-trained transformers for domain adaptation
  - Why needed here: Leverages general language understanding for specialized claim detection tasks.
  - Quick check question: What is the difference between training from scratch and fine-tuning?

- Concept: Macro-F1 vs Token-F1 evaluation metrics
  - Why needed here: Macro-F1 balances class imbalance in detection; Token-F1 measures span precision at token level.
  - Quick check question: When would Macro-F1 and Token-F1 diverge in results?

## Architecture Onboarding

- Component map:
  Input layer -> Pre-trained transformer (RoBERTa) -> Task A head (binary classification) -> Task B head (BIO tagging/positional attention) -> Output (claim presence + span)

- Critical path:
  Pre-trained model → Fine-tuning on task-specific data → Evaluation using F1 metrics

- Design tradeoffs:
  - Fine-tuning vs training from scratch: Speed and data efficiency vs. custom optimization
  - BIO vs IO tagging: More precise span boundaries vs. simpler modeling
  - Sequence labeling vs span prediction: Token-level control vs. flexibility for complex spans

- Failure signatures:
  - Low Macro-F1 but high Token-F1: Model overfits to claim-containing posts
  - High Macro-F1 but low Token-F1: Model detects claims but poorly identifies spans
  - Low scores on both: Insufficient or noisy training data

- First 3 experiments:
  1. Fine-tune RoBERTa on Task A only; evaluate Macro-F1
  2. Add BIO tagging head; fine-tune on Task B; evaluate Token-F1
  3. Replace BIO with positional attention; compare span extraction performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the CLAIMSCAN-2023 datasets be expanded to improve the generalizability of claim detection models across different languages and social media platforms?
- Basis in paper: [inferred] The authors mention future plans to enrich the datasets with more examples, diverse information sources, and languages, and to expand the task to encompass a broader range of modalities.
- Why unresolved: The paper does not provide details on how to achieve this expansion or what specific steps would be needed.
- What evidence would resolve it: A study demonstrating improved model performance when trained on a more diverse and multilingual dataset.

### Open Question 2
- Question: What are the most effective techniques for identifying claim spans within social media posts that contain multiple claims or nested claims?
- Basis in paper: [inferred] The paper highlights the challenge of extracting precise snippets of claims from posts that often contain extraneous irrelevant text, and mentions the complexity of disentangling argumentative units of misinformation from benign statements.
- Why unresolved: The paper does not provide specific methods or solutions for handling multiple or nested claims within a single post.
- What evidence would resolve it: A study comparing the performance of different claim span identification models on posts with multiple or nested claims.

### Open Question 3
- Question: How can the computational efficiency of claim detection models be improved without sacrificing accuracy, particularly for real-time applications?
- Basis in paper: [explicit] The authors mention that language models incur significant computational overheads and that Sundriyal et al. [3] addressed this issue by proposing a lighter framework.
- Why unresolved: The paper does not provide a detailed analysis of the trade-offs between model complexity and computational efficiency, or specific techniques for achieving both high accuracy and efficiency.
- What evidence would resolve it: A study demonstrating a claim detection model that achieves high accuracy with significantly reduced computational requirements compared to existing models.

## Limitations

- Results are limited to Twitter data and may not generalize to other social media platforms
- Datasets are proprietary, making independent verification difficult
- Evaluation metrics may not fully capture real-world misinformation detection utility

## Confidence

**High Confidence**: The effectiveness of fine-tuning pre-trained transformers (RoBERTa) for both claim detection and span identification tasks. This is supported by multiple teams achieving top rankings using transformer-based approaches and is consistent with broader NLP literature on transfer learning.

**Medium Confidence**: The layered task architecture (detection followed by span identification) improves overall performance. While logically sound and implemented by top teams, the specific performance gains from this two-stage approach versus end-to-end models are not directly quantified in the evaluation.

**Low Confidence**: The generalizability of these results to real-world misinformation detection. The evaluation metrics and task setup provide controlled benchmarks, but do not address critical factors like temporal dynamics of misinformation, user engagement patterns, or the broader ecosystem of fact-checking.

## Next Checks

1. **Cross-platform validation**: Evaluate the top-performing models on social media data from platforms other than Twitter (e.g., Reddit, Facebook) to assess generalizability across different discourse patterns and content formats.

2. **Temporal robustness testing**: Assess model performance on temporally diverse datasets to determine whether models maintain accuracy as language patterns and misinformation tactics evolve over time.

3. **Human evaluation comparison**: Conduct a user study comparing model-identified claim spans against human annotations to validate whether the token-level BIO tagging approach captures claims as humans would identify them in context.