---
ver: rpa2
title: Instruction-tuning Aligns LLMs to the Human Brain
arxiv_id: '2312.00575'
source_url: https://arxiv.org/abs/2312.00575
tags:
- alignment
- brain
- llms
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how instruction-tuning affects the alignment
  of large language models (LLMs) to human brain activity and behavior. Using 25 vanilla
  and instruction-tuned LLMs ranging from 77M to 33B parameters, the authors find
  that instruction-tuning improves brain alignment by 6.2% on average, suggesting
  that instruction-tuning enhances the representational similarity between LLMs and
  human language processing systems.
---

# Instruction-tuning Aligns LLMs to the Human Brain

## Quick Facts
- **arXiv ID**: 2312.00575
- **Source URL**: https://arxiv.org/abs/2312.00575
- **Reference count**: 40
- **Primary result**: Instruction-tuning improves brain alignment by 6.2% on average, with strong correlations to model size (r=0.95) and world knowledge (r=0.81)

## Executive Summary
This study investigates how instruction-tuning affects the alignment of large language models (LLMs) to human brain activity and behavior. Using 25 vanilla and instruction-tuned LLMs ranging from 77M to 33B parameters, the authors find that instruction-tuning improves brain alignment by 6.2% on average, suggesting that instruction-tuning enhances the representational similarity between LLMs and human language processing systems. Furthermore, the study identifies that model size and world knowledge are strongly correlated with brain alignment, indicating that these factors contribute significantly to aligning LLM representations to human brain activity. Surprisingly, instruction-tuning does not generally improve behavioral alignment to human reading times, highlighting the need for more comprehensive benchmarks to evaluate LLM-human behavioral alignment.

## Method Summary
The study evaluates brain alignment using the linear predictivity metric from Brain-Score, which measures Pearson correlation between LLM representations and fMRI voxels during language processing. Twenty-five LLMs (T5 and LLaMA families) were tested across three fMRI datasets, with brain alignment computed at each layer and the maximum value retained. Behavioral alignment was assessed by correlating LLM perplexity with human reading times from the Futrell et al. (2018) dataset. The study also examines correlations between brain alignment and LLM properties including model size, MMLU scores (world knowledge), and BBH scores (instruction-following ability).

## Key Results
- Instruction-tuning improves brain alignment by 6.2% on average compared to vanilla models
- Brain alignment strongly correlates with model size (r = 0.95) and world knowledge performance (r = 0.81)
- Instruction-tuning does not generally improve behavioral alignment to human reading times
- An ablation study shows that instruction-following itself contributes to brain alignment beyond just additional training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuning improves brain alignment by teaching models to encode world knowledge in a way that mirrors human language processing systems.
- Mechanism: The instruction-tuning process involves training models on task-specific instructions paired with desired outputs. This format encourages the model to develop representations that are not just predictive of the next word, but also structured to answer questions and follow instructions. Since human language processing is deeply tied to our ability to retrieve and use world knowledge, this instruction-tuned representation style better matches the brain's encoding of language.
- Core assumption: World knowledge encoding is a key differentiator between human language processing and simple next-word prediction.
- Evidence anchors:
  - [abstract] "instruction-tuning generally enhances brain alignment (~6%)"
  - [abstract] "we find a strong positive correlation between brain alignment and performance on tasks requiring world knowledge (r = 0.81)"
  - [section 4.2] "We find that brain alignment is significantly and strongly correlated with world knowledge."

### Mechanism 2
- Claim: Larger models inherently have better representations of world knowledge, which leads to higher brain alignment.
- Mechanism: As model size increases, the number of parameters and the capacity to store information grow. This allows larger models to encode a richer and more nuanced understanding of the world. Since human brain activity during language processing reflects this rich understanding, models with larger parameter counts naturally align better with brain activity.
- Core assumption: Model parameter count is a proxy for the capacity to encode world knowledge.
- Evidence anchors:
  - [abstract] "we find a strong positive correlation between brain alignment and model size (r = 0.95)"
  - [section 4.2] "we find that brain alignment is significantly and strongly correlated with model size (r = 0.95)"

### Mechanism 3
- Claim: The improvement in brain alignment from instruction-tuning is not solely due to additional training data, but also the process of training models to understand and follow instructions.
- Mechanism: The ablation study in the paper compares a model trained on the same data as the instruction-tuned model, but without the instruction portion of each sample. The fact that the instruction-tuned model still outperforms the ablation model suggests that the act of learning to follow instructions itself contributes to better brain alignment. This could be because instructions force the model to develop a more structured and goal-oriented representation of language.
- Core assumption: The instruction-following component of instruction-tuning adds value beyond just more training data.
- Evidence anchors:
  - [section 4.1] "This shows that brain alignment improvements are due to both factors."
  - [section 4.1] "We observe that brain alignment of this ablated model increases during fine-tuning but stays lower than its instruction-following counterpart."

## Foundational Learning

- **Concept: Linear predictivity metric for brain alignment**
  - Why needed here: This metric is the core method used to quantify how well LLM representations predict human brain activity. Understanding how it works is crucial for interpreting the results.
  - Quick check question: How does the linear predictivity metric differ from simply computing the correlation between LLM representations and brain activity?

- **Concept: fMRI data preprocessing and noise ceiling**
  - Why needed here: The paper uses fMRI data from human subjects reading stories and sentences. Understanding how this data is preprocessed and the concept of a noise ceiling is important for evaluating the validity of the brain alignment results.
  - Quick check question: Why is it important to compute a noise ceiling when evaluating brain alignment?

- **Concept: Next-word prediction (NWP) loss**
  - Why needed here: NWP loss is used as a baseline measure of language modeling ability. The paper compares brain alignment to NWP loss to see if the former is driven by the latter.
  - Quick check question: How might NWP loss be related to, but distinct from, the ability to encode world knowledge?

## Architecture Onboarding

- **Component map**: fMRI datasets (Pereira et al., 2018; Blank et al., 2014; Wehbe et al., 2014) -> LLMs (T5 and LLaMA families, 77M-33B parameters) -> Brain alignment pipeline (extract representations, train linear regression, compute Pearson correlation) -> Behavioral alignment pipeline (compute perplexity, correlate with reading times) -> Evaluation (correlate alignment with model properties)

- **Critical path**:
  1. Prepare language stimuli from fMRI datasets
  2. Extract LLM representations for each stimulus
  3. Train linear regression model to predict brain activity from LLM representations
  4. Evaluate brain alignment as Pearson correlation on held-out data
  5. Compute LLM perplexity on Futrell et al. (2018) stimuli
  6. Evaluate behavioral alignment as Pearson correlation between perplexity and reading times
  7. Correlate brain alignment with LLM properties

- **Design tradeoffs**:
  - Using linear regression vs. more complex models for predicting brain activity
  - Evaluating only the best layer vs. aggregating across layers
  - Using Pearson correlation vs. other similarity metrics
  - Focusing on brain alignment vs. also evaluating behavioral alignment

- **Failure signatures**:
  - Brain alignment scores that are very low or negative
  - High variance in brain alignment scores across different layers of the same model
  - Poor correlation between brain alignment and known model properties (size, NWP loss)
  - Behavioral alignment that is significantly higher or lower than brain alignment

- **First 3 experiments**:
  1. Reproduce the brain alignment results for a single, small LLM (e.g., T5-small) on a single dataset (e.g., Pereira et al., 2018)
  2. Compare the brain alignment of a vanilla LLM to its instruction-tuned counterpart
  3. Correlate the brain alignment of a set of LLMs with their MMLU scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does instruction-tuning affect the alignment of LLMs to brain activity beyond language processing, such as in tasks involving visual or auditory stimuli?
- Basis in paper: [inferred] The paper focuses on language stimuli and brain alignment in language processing. It does not explore how instruction-tuning might affect alignment in other sensory modalities.
- Why unresolved: The study's scope is limited to language processing, and the authors do not discuss the potential impact of instruction-tuning on other types of brain activity.
- What evidence would resolve it: Conducting experiments that involve instruction-tuning LLMs and then evaluating their alignment to brain activity in response to visual or auditory stimuli, using appropriate neural datasets and alignment metrics.

### Open Question 2
- Question: What are the long-term effects of instruction-tuning on the brain alignment of LLMs, and does this alignment persist or improve with continued fine-tuning or exposure to more diverse tasks?
- Basis in paper: [inferred] The paper presents a snapshot of brain alignment after instruction-tuning but does not investigate the long-term effects or the potential for further improvement with continued training.
- Why unresolved: The study does not include a longitudinal analysis of brain alignment over time or with additional fine-tuning.
- What evidence would resolve it: Performing a long-term study that tracks the brain alignment of LLMs before, during, and after instruction-tuning, and comparing it to alignment in models that undergo further fine-tuning or exposure to diverse tasks.

### Open Question 3
- Question: How does the instruction format used during instruction-tuning influence the brain alignment of LLMs, and are there specific formats that lead to better alignment?
- Basis in paper: [inferred] The paper mentions that the instruction format can affect performance on benchmarks like MMLU and BBH, but it does not explore how different instruction formats might impact brain alignment.
- Why unresolved: The study uses a standard instruction format and does not investigate the effects of varying the format.
- What evidence would resolve it: Experimenting with different instruction formats during fine-tuning and evaluating their impact on brain alignment using the same neural datasets and alignment metrics.

## Limitations

- The study's brain alignment findings rely on linear predictivity metrics that may not capture complex nonlinear relationships between LLM representations and brain activity
- The 6.2% improvement in brain alignment from instruction-tuning represents a relatively modest effect size that may not generalize across all types of cognitive tasks
- Behavioral alignment results are based on a single reading time benchmark, potentially missing other important behavioral dimensions

## Confidence

- **Brain alignment improvement from instruction-tuning**: Medium - The correlation is statistically robust, but the mechanistic explanation relies on assumptions about world knowledge encoding that are not directly tested.
- **Correlation between model size and brain alignment**: High - Strong statistical evidence (r = 0.95) and consistent with established scaling laws in language modeling.
- **Lack of behavioral alignment improvement**: Medium - Based on single benchmark; more comprehensive behavioral evaluation needed.

## Next Checks

1. Test whether instruction-tuned models show superior performance on diverse cognitive tasks beyond reading comprehension, including reasoning and problem-solving benchmarks that directly tap world knowledge.
2. Conduct ablation studies comparing brain alignment improvements from instruction-tuning versus equivalent amounts of additional pretraining data on the same corpus.
3. Evaluate whether nonlinear prediction models (e.g., neural networks) achieve substantially different brain alignment scores compared to the linear predictivity metric, suggesting potential limitations in capturing brain-LLM representational relationships.