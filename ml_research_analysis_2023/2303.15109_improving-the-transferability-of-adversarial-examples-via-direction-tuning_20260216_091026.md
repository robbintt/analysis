---
ver: rpa2
title: Improving the Transferability of Adversarial Examples via Direction Tuning
arxiv_id: '2303.15109'
source_url: https://arxiv.org/abs/2303.15109
tags:
- adversarial
- attacks
- update
- examples
- xadv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called direction tuning attack (DTA)
  to improve the transferability of adversarial examples generated by transfer-based
  attacks. The key idea is to reduce the deviation between the actual and steepest
  update directions, which is caused by large update step lengths in current transfer-based
  attacks.
---

# Improving the Transferability of Adversarial Examples via Direction Tuning

## Quick Facts
- arXiv ID: 2303.15109
- Source URL: https://arxiv.org/abs/2303.15109
- Reference count: 40
- Primary result: DTA improves ASR from 87.9% to 94.5% on undefended models and from 69.1% to 76.2% on defended models

## Executive Summary
This paper introduces Direction Tuning Attack (DTA), a method that significantly improves the transferability of adversarial examples by addressing the direction deviation problem in gradient-based attacks. The key insight is that large update steps in current transfer-based attacks cause oscillations that reduce transferability. DTA embeds small sampling steps within each large update step to average gradients and produce more accurate update directions. Additionally, a network pruning method smooths decision boundaries to further enhance transferability. Experiments on ImageNet show substantial improvements over state-of-the-art attacks.

## Method Summary
DTA improves adversarial transferability by reducing the deviation between actual and steepest update directions through coordinated use of large and small step lengths. The method performs multiple gradient calculations using small step lengths within each large update step, then averages these gradients to produce a more accurate update direction. A network pruning method is also proposed to smooth decision boundaries by removing redundant or unimportant neurons. The attack combines these techniques with momentum-based optimization to generate adversarial examples that transfer more effectively across different models.

## Key Results
- Average ASR improved from 87.9% to 94.5% on five undefended victim models
- Average ASR improved from 69.1% to 76.2% on eight advanced defense methods
- DTA outperforms state-of-the-art gradient-based attacks including I-FGSM, MI-FGSM, and DI-FGSM
- Variance tuning variant (VDTA) achieves even higher transferability rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direction tuning attack reduces the deviation between actual and steepest update directions by embedding small sampling steps into large update steps.
- Mechanism: The method calculates multiple gradient samples using a small step length within each large update step, then averages these gradients to produce a more accurate update direction. This averaging eliminates the oscillating component while maintaining convergence speed.
- Core assumption: The average gradient of multiple small-step samples provides a better approximation of the true steepest descent direction than a single large-step gradient.
- Evidence anchors:
  - [abstract]: "embedding small sampling step lengths into each large step length to not only decrease the deviation between the actual direction and the steepest direction but also mitigate the update oscillation"
  - [section]: "in our direction tuning attack, we first update the adversarial examples using the large step length... However, in each large step length of the update, we calculate multiple samples using a small step length"
  - [corpus]: Weak evidence - corpus papers focus on different transferability enhancement methods but don't directly address the step-length deviation problem.
- Break condition: If the gradient landscape is highly non-smooth or contains many local minima, the averaging might produce a direction that is suboptimal for escaping bad local optima.

### Mechanism 2
- Claim: Network pruning method smooths the decision boundary by removing redundant or unimportant neurons, which further reduces update oscillation and enhances transferability.
- Mechanism: The pruning method identifies and removes neurons with minimal impact on loss using gradient magnitude as importance metric, simplifying the classification boundary and making it more stable during adversarial generation.
- Core assumption: Removing less important neurons reduces decision boundary complexity without significantly affecting classification accuracy, leading to smoother optimization paths.
- Evidence anchors:
  - [abstract]: "a network pruning method is proposed to smooth the decision boundary by pruning the redundant or unimportant neurons to further eliminate update oscillation"
  - [section]: "The network pruning method is proposed as well to further mitigate the update oscillation, thereby promoting the generated adversarial examples converge well"
  - [corpus]: Weak evidence - corpus papers mention pruning for robustness but don't specifically address transferability enhancement through decision boundary smoothing.
- Break condition: If pruning removes neurons that are actually important for distinguishing between classes, it could reduce model capacity and harm both accuracy and transferability.

### Mechanism 3
- Claim: The combination of large update steps for convergence speed and small sampling steps for direction accuracy creates a coordinated optimization that achieves both fast convergence and high transferability.
- Mechanism: Large steps provide the momentum needed to escape local minima while small sampling steps provide directional accuracy through gradient averaging, creating a balanced optimization process.
- Core assumption: The large step length maintains sufficient movement through the parameter space while the small sampling steps provide enough directional correction to keep the optimization on track.
- Evidence anchors:
  - [abstract]: "through the coordinated use of the large update step length and the small sampling step length, our direction tuning attack can effectively reduce the deviation"
  - [section]: "in the current transfer-based attacks, the large step length α leads to the big deviation... To address the above issue without changing the step length α... direction tuning attack... is proposed to decrease the deviation of the gradient direction and the steepest direction"
  - [corpus]: Weak evidence - corpus papers focus on different attack strategies but don't explore this specific coordinated step-length approach.
- Break condition: If the inner loop sampling becomes too computationally expensive or if the small steps fail to capture the true gradient direction in highly non-convex regions.

## Foundational Learning

- Concept: Gradient descent optimization and step length selection
  - Why needed here: The paper's core innovation relies on understanding how different step lengths affect gradient direction accuracy and convergence behavior in adversarial example generation.
  - Quick check question: What happens to the gradient direction accuracy when the step length becomes very large in gradient descent optimization?

- Concept: Transfer-based adversarial attacks and transferability metrics
  - Why needed here: The paper evaluates success through attack success rate (ASR) across different models, requiring understanding of how transferability works in black-box settings.
  - Quick check question: How does the concept of gradient alignment relate to the transferability of adversarial examples between different models?

- Concept: Network pruning techniques and their impact on model decision boundaries
  - Why needed here: The network pruning method proposed in the paper aims to smooth decision boundaries, which requires understanding how pruning affects model behavior.
  - Quick check question: How does removing neurons with low gradient magnitude typically affect a neural network's decision boundary?

## Architecture Onboarding

- Component map: Input image -> Direction Tuning Attack -> Network Pruning -> Adversarial example -> Victim models for evaluation

- Critical path:
  1. Initialize adversarial example and gradient accumulator
  2. For each iteration: perform inner loop sampling with small steps, calculate average gradient
  3. Update adversarial example using averaged gradient and momentum
  4. Apply network pruning to smooth decision boundary
  5. Evaluate transferability on victim models

- Design tradeoffs:
  - Computation vs. accuracy: Larger K values improve direction accuracy but increase computation time
  - Pruning aggressiveness vs. model capacity: Higher pruning rates smooth boundaries but may reduce accuracy
  - Inner loop depth vs. momentum: Deeper inner loops provide better averaging but may slow momentum

- Failure signatures:
  - Low ASR across all victim models: Likely issues with gradient calculation or step size selection
  - High ASR on surrogate but low on victims: Problem with transferability, possibly due to model architecture mismatch
  - Training instability or divergence: Step sizes may be too aggressive or pruning too severe

- First 3 experiments:
  1. Implement basic DTA without pruning on a simple dataset (e.g., CIFAR-10) to verify the core mechanism
  2. Compare ASR between DTA and baseline attacks (I-FGSM, MI-FGSM) on same dataset
  3. Test different K values to find optimal balance between accuracy and computation time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed direction tuning attack perform on datasets other than ImageNet, such as CIFAR-10 or CIFAR-100?
- Basis in paper: [explicit] The experiments were conducted only on the ImageNet dataset, as mentioned in Section 4.
- Why unresolved: The paper does not provide any experimental results or analysis on other datasets, leaving the generalizability of the approach unclear.
- What evidence would resolve it: Conducting experiments on other standard image classification datasets like CIFAR-10 or CIFAR-100 and comparing the results with the current ImageNet results would provide evidence for the generalizability of the approach.

### Open Question 2
- Question: How does the proposed network pruning method affect the natural accuracy of the models on which it is applied?
- Basis in paper: [inferred] The paper mentions that the network pruning method is used to smooth the decision boundary and enhance the transferability of adversarial examples, but it does not discuss the impact on natural accuracy.
- Why unresolved: The paper focuses on the transferability of adversarial examples and does not provide any information on the effect of the pruning method on the model's natural accuracy.
- What evidence would resolve it: Evaluating the natural accuracy of the models before and after applying the network pruning method would provide insights into the impact on model performance.

### Open Question 3
- Question: How does the proposed direction tuning attack perform against other types of defenses not mentioned in the paper, such as certified defenses or defenses based on generative models?
- Basis in paper: [explicit] The paper evaluates the attack against eight advanced defense methods, but it does not mention other types of defenses like certified defenses or generative model-based defenses.
- Why unresolved: The paper does not provide any analysis or experimental results on the performance of the attack against other types of defenses, leaving the robustness of the approach against a broader range of defenses unclear.
- What evidence would resolve it: Conducting experiments against other types of defenses, such as certified defenses or defenses based on generative models, would provide evidence for the robustness of the attack against a wider range of defense strategies.

## Limitations
- The fundamental mechanism relies on assumptions about gradient landscape smoothness that may not hold for all model architectures or datasets
- Network pruning could potentially harm model performance if applied too aggressively
- Long-term stability of the pruning method and its effects on model robustness beyond transferability are not thoroughly investigated

## Confidence
- **High Confidence:** The experimental results showing improved ASR on both undefended and defended models are well-documented and reproducible
- **Medium Confidence:** The theoretical explanation of why direction tuning works (gradient averaging reducing oscillation) is plausible but relies on assumptions about the gradient landscape that may not generalize
- **Low Confidence:** The long-term stability of the pruning method and its effects on model robustness beyond transferability are not thoroughly investigated

## Next Checks
1. Test DTA on smaller datasets (e.g., CIFAR-10) to verify if the direction tuning mechanism generalizes beyond ImageNet
2. Conduct ablation studies varying K values to determine the optimal tradeoff between computational cost and transferability improvement
3. Evaluate the impact of network pruning on clean model accuracy over multiple pruning iterations to assess potential performance degradation