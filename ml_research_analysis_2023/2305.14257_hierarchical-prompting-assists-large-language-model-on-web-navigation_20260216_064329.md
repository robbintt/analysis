---
ver: rpa2
title: Hierarchical Prompting Assists Large Language Model on Web Navigation
arxiv_id: '2305.14257'
source_url: https://arxiv.org/abs/2305.14257
tags:
- button
- observation
- action
- search
- room
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hierarchical prompting improves LLM performance on web navigation
  by first summarizing raw observations into a concise, action-relevant form, then
  having an actor decide the next action based on this condensed history. On a large-scale
  e-commerce benchmark, this Actor-Summarizer-Hierarchical (ASH) approach increases
  task success rate by 6.8% (29% relative gain) over prior state-of-the-art, with
  the largest gains on longer trajectories and reduced invalid action failures.
---

# Hierarchical Prompting Assists Large Language Model on Web Navigation

## Quick Facts
- arXiv ID: 2305.14257
- Source URL: https://arxiv.org/abs/2305.14257
- Reference count: 40
- Primary result: ASH improves LLM web navigation success rate by 6.8% (29% relative gain) over prior state-of-the-art

## Executive Summary
This paper introduces Actor-Summarizer-Hierarchical (ASH), a hierarchical prompting approach that significantly improves large language model performance on web navigation tasks. The key innovation is separating observation summarization from action prediction, where a SUMMARIZER component condenses raw web observations into concise, goal-relevant representations before an ACTOR component decides the next action. Tested on a large-scale e-commerce benchmark with 1.1M+ products, ASH achieves a 6.8% absolute improvement in task success rate compared to prior methods, with the largest gains observed on longer trajectories and reduced invalid action generation.

## Method Summary
ASH uses a two-stage prompting pipeline for web navigation. First, a SUMMARIZER prompt takes the previous action and raw observation to generate a condensed observation containing only relevant information for the current instruction. Second, an ACTOR prompt takes the history of actions and summarized observations to predict the next action. The system alternates between these two components until reaching a stop action or maximum steps. The approach uses the CODE-DAVINCI-002 model with greedy decoding, terminating after 20 steps or 5 consecutive invalid actions. The Webshop benchmark provides 500 sampled trajectories with text-rich settings and full product space.

## Key Results
- 6.8% absolute improvement in task success rate over prior state-of-the-art
- 29% relative gain in performance on the Webshop benchmark
- 18% relative reduction in invalid action generation (hallucinations)
- Performance gains most pronounced on trajectories with 11+ steps (38.2% success vs baseline)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical summarization reduces cognitive load on the LLM by providing a condensed, goal-relevant state representation before action prediction.
- Mechanism: The SUMMARIZER transforms raw observations into a concise representation aligned with the current instruction and action history, allowing the ACTOR to reason over focused features rather than parsing entire raw observations.
- Core assumption: LLMs perform better when context is aligned with task goals and stripped of irrelevant information.
- Evidence anchors: Abstract states SUMMARIZER creates "more condensed and relevant" observations; section 3.1 describes generating "concise observation that only encodes the relevant information."
- Break condition: If SUMMARIZER produces overly generic or misaligned summaries, ACTOR will receive poor context and performance will degrade.

### Mechanism 2
- Claim: ASH improves success on long trajectories by mitigating error accumulation from noisy observations.
- Mechanism: Long tasks have noisy, growing observations that distract from goals. ASH's summarization filters irrelevant details, keeping ACTOR focused on task-relevant features and preventing decision drift over many steps.
- Core assumption: Error in action prediction compounds over long trajectories, especially when processing noisy, lengthy observations.
- Evidence anchors: Section 5 shows performance gap is "particularly large on examples with more than 11 steps"; abstract reports "6.2% on task success rate" improvement.
- Break condition: If tasks involve mostly short trajectories, summarization overhead may not yield noticeable gains.

### Mechanism 3
- Claim: ASH reduces invalid action generation by limiting action space through concise summaries.
- Mechanism: LLMs often hallucinate actions not present in the environment. Distilled views from summarization narrow plausible actions the ACTOR can generate, reducing invalid predictions.
- Core assumption: Hallucinations stem partly from excessive context containing irrelevant or misleading information.
- Evidence anchors: Section 5 reports "relative reduction of 18% in this failure mode, effectively decreasing the failure rate to 10.4%"; abstract notes ASH "outperforms... by 6.8% on task success rate."
- Break condition: If environment has few interactive elements or task is deterministic, hallucination reduction benefit will be minimal.

## Foundational Learning

- **State abstraction and summarization in reinforcement learning**
  - Why needed: ASH mimics learned state encoders by using prompt-based summarization to compress raw observations into goal-relevant features
  - Quick check: In RL, why do we use state abstraction before policy learning?

- **Prompt engineering and in-context learning**
  - Why needed: SUMMARIZER and ACTOR are stateless modules relying on carefully crafted prompts to function correctly
  - Quick check: What are the key design principles of effective in-context examples?

- **Long-horizon planning and error compounding**
  - Why needed: Paper demonstrates ASH helps tasks with long trajectories where errors compound; understanding why is critical for appropriate application
  - Quick check: How does error compounding affect decision-making in long sequences?

## Architecture Onboarding

- **Component map**: SUMMARIZER(prompt) -> condensed observation -> ACTOR(prompt) -> action -> history update
- **Critical path**: 
  1. Input: instruction + raw observation
  2. SUMMARIZER(prompt) → condensed observation
  3. ACTOR(prompt) → action
  4. Append (action, condensed observation) to history
  5. Repeat from step 1 until stop or limit
- **Design tradeoffs**:
  - Pro: Reduces LLM cognitive load, improves handling of long trajectories, reduces hallucinations
  - Con: Adds inference latency (two LLM calls per step), relies on prompt quality, possible information loss in summarization
- **Failure signatures**:
  - Actor generates invalid actions frequently → SUMMARIZER may be omitting critical action affordances
  - Task success drops on short trajectories → summarization overhead outweighs benefit
  - Actor fails to complete goal despite correct summaries → ACTOR prompt design may be inadequate
- **First 3 experiments**:
  1. Compare ASH vs. baseline on short (≤6 steps) vs. long (≥11 steps) trajectories to confirm trajectory-length sensitivity
  2. Measure invalid action rate reduction when using ASH vs. baseline to confirm hallucination mitigation
  3. Test with different SUMMARIZER prompt templates to find optimal balance between conciseness and informativeness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the ASH approach generalize beyond web navigation to other interactive decision-making tasks like robotic manipulation or dialogue systems?
- Basis: Paper states ASH "has broad applicability" but only tests it on web navigation domain
- Why unresolved: Only demonstrates ASH on one specific task domain, leaving effectiveness in other domains unexplored
- What evidence would resolve it: Applying ASH to at least one other interactive decision-making task and showing improved performance

### Open Question 2
- Question: How does performance scale with increasing complexity of web pages (more products, complex layouts)?
- Basis: Uses real-world e-commerce benchmark but doesn't systematically vary page complexity or analyze how ASH handles increasingly complex observations
- Why unresolved: Current analysis doesn't isolate effect of observation complexity on ASH's performance
- What evidence would resolve it: Experiments on synthetic web pages with controlled complexity levels showing ASH's performance trends

### Open Question 3
- Question: Can the summarizer module be trained end-to-end with the actor to improve coordination, rather than being a static prompt-based component?
- Basis: Current ASH design uses separate SUMMARIZER and ACTOR components with fixed prompts, but paper doesn't explore training them jointly
- Why unresolved: Presents ASH as modular prompting approach but doesn't investigate whether joint training could yield better performance
- What evidence would resolve it: Comparing ASH's performance against version where summarizer and actor are trained together end-to-end

## Limitations
- Evaluation limited to single benchmark (Webshop) and single model (CODE-DAVINCI-002), constraining generalizability
- SUMMARIZER prompt templates not fully specified, particularly different templates for various scenarios
- Performance gains most pronounced on long trajectories, suggesting approach may be less beneficial for simpler, shorter tasks
- No analysis of computational efficiency or latency implications of doubling LLM calls per step

## Confidence
- **High confidence**: ASH reduces invalid action generation (18% relative reduction in hallucinations) with concrete failure analysis evidence
- **Medium confidence**: 6.8% improvement in task success rate supported by benchmark results but limited to single model and benchmark
- **Low confidence**: Mechanism explanations (cognitive load reduction, error compounding prevention) are theoretically sound but lack direct experimental validation

## Next Checks
1. **Cross-benchmark validation**: Test ASH on multiple web navigation benchmarks (Mind2Web, MiniWoB++) with different model families (GPT-4, Claude, open-source LLMs) to assess generalizability of the 6.8% performance gain

2. **Prompt template ablation**: Systematically vary SUMMARIZER prompt templates (more vs. less aggressive summarization, different instruction phrasings) to quantify contribution of prompt design versus hierarchical architecture itself

3. **Latency and efficiency analysis**: Measure and compare end-to-end inference time and cost per navigation task between ASH and baseline approaches, particularly for long trajectories, to evaluate practical deployment trade-offs of two-LLM-call-per-step design