---
ver: rpa2
title: Riemannian Projection-free Online Learning
arxiv_id: '2305.19349'
source_url: https://arxiv.org/abs/2305.19349
tags:
- lemma
- oracle
- have
- optimization
- gsc-convex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates projection-free online optimization on
  Riemannian manifolds, addressing the computational challenges of the projection
  operation in high-dimensional settings or ill-conditioned constraint sets. The authors
  propose two projection-free algorithms using a separation oracle or a linear optimization
  oracle, respectively.
---

# Riemannian Projection-free Online Learning

## Quick Facts
- arXiv ID: 2305.19349
- Source URL: https://arxiv.org/abs/2305.19349
- Reference count: 40
- One-line primary result: Achieves O(T^{1/2}) and O(T^{3/4}) adaptive regret guarantees using separation oracle, and O(T^{3/4}) adaptive regret and O(T^{2/3} log T) regret for geodesically convex and strongly geodesically convex losses using linear optimization oracle.

## Executive Summary
This paper addresses the computational challenges of projection operations in online optimization on Riemannian manifolds by developing projection-free algorithms. The authors propose two approaches: one using a separation oracle and another using a linear optimization oracle. The key innovation lies in handling the non-convexity of Riemannian hyperplanes through the Jacobi field comparison technique, enabling regret bounds comparable to Euclidean settings despite the curved geometry.

## Method Summary
The paper proposes two projection-free algorithms for online optimization on Riemannian manifolds. The first approach uses a separation oracle that provides non-convex separating hyperplanes when a point is outside the feasible set. The second approach employs a linear optimization oracle that solves argmin problems on the manifold despite the non-convex objective. Both methods iteratively adjust the current point based on loss gradients and oracle feedback, using line searches to ensure progress while maintaining feasibility.

## Key Results
- Achieves O(T^{1/2}) and O(T^{3/4}) adaptive regret guarantees in full information and bandit settings when separation oracle is available
- Obtains O(T^{3/4}) adaptive regret and O(T^{2/3} log T) regret for geodesically convex and strongly geodesically convex losses with linear optimization oracle
- Demonstrates that non-convex Riemannian hyperplanes can be effectively handled using Jacobi field comparison technique

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The separation oracle provides a non-convex separating hyperplane that allows progress in the Riemannian setting.
- Mechanism: When a point y is outside the feasible set K, the separation oracle returns a vector g such that ⟨-Exp⁻¹y x, g⟩ > 0 for all x in K. This allows the algorithm to move away from the infeasible region.
- Core assumption: The separating hyperplane exists and is non-convex in the Riemannian setting.
- Evidence anchors:
  - [abstract] "An apparent issue is that non-trivial affine functions are generally non-convex in such domains."
  - [section 3] "A separation oracle, given a point y not in the gsc-convex set K, returns a non-convex separating hyperplane that satisfies the following condition: ⟨-Exp⁻¹y x, g⟩ > 0, ∀x∈K"
  - [corpus] Weak evidence - no directly relevant papers found
- Break condition: If the separation oracle fails to provide a valid separating hyperplane for some infeasible point.

### Mechanism 2
- Claim: The linear optimization oracle on Riemannian manifolds can be used to construct an infeasible projection oracle.
- Mechanism: By iteratively applying the linear optimization oracle and using a line search, the algorithm can "pull" an infeasible point toward the feasible set K.
- Core assumption: The linear optimization oracle can solve argmin x∈K ⟨g, Exp⁻¹x0 x⟩ even though this objective is not gsc-convex.
- Evidence anchors:
  - [abstract] "When a linear optimization oracle is available, we obtain regret rates of O(T³/⁴) for geodesically convex losses and O(T²/³ log T) for strongly geodesically convex losses."
  - [section 3] "A linear optimization oracle is responsible for solving the following problem: argmin x∈K ⟨g, Exp⁻¹x0 x⟩ on a gsc-convex set K"
  - [corpus] Weak evidence - no directly relevant papers found
- Break condition: If the linear optimization oracle fails to find a descent direction that moves the point closer to K.

### Mechanism 3
- Claim: The Jacobi field comparison technique can bound the "thickness" of the feasible set cut by the separating hyperplane.
- Mechanism: The Jacobi field comparison technique is used to bound the distance between points on opposite sides of the separating hyperplane, which is crucial for establishing regret bounds.
- Core assumption: The Jacobi field comparison technique can be applied to non-convex separating hyperplanes on manifolds.
- Evidence anchors:
  - [abstract] "The key contributions include achieving O(T¹/²) and O(T³/⁴) adaptive regret guarantees... when a separation oracle is available"
  - [section 5] "Fortunately, this problem can be addressed using the Jacobi field comparison technique."
  - [corpus] Weak evidence - no directly relevant papers found
- Break condition: If the Jacobi field comparison technique cannot provide tight enough bounds for the specific geometry of the problem.

## Foundational Learning

- Concept: Riemannian manifolds and geodesics
  - Why needed here: The algorithms operate on curved spaces where straight lines are replaced by geodesics, and distances are measured using the Riemannian metric.
  - Quick check question: What is the Riemannian distance between two points x and y on a manifold?

- Concept: Geodesically convex (gsc-convex) sets and functions
  - Why needed here: The constraint sets and loss functions are assumed to be gsc-convex, which is a generalization of convexity to curved spaces.
  - Quick check question: How does the definition of gsc-convexity differ from standard convexity in Euclidean space?

- Concept: Exponential and inverse exponential maps
  - Why needed here: These maps are used to move between tangent spaces and the manifold itself, which is crucial for implementing gradient descent on manifolds.
  - Quick check question: What is the relationship between the exponential map and the Riemannian distance?

## Architecture Onboarding

- Component map:
  - Separation oracle -> Infeasible projection oracle -> Main algorithm
  - Linear optimization oracle -> Riemannian Frank-Wolfe -> Main algorithm
  - Jacobi field comparison -> Regret bound analysis

- Critical path:
  1. Check if current point is feasible
  2. If feasible, compute gradient and move along geodesic
  3. If infeasible, use separation oracle or linear optimization oracle to find descent direction
  4. Apply line search to ensure progress
  5. Repeat until convergence or maximum iterations reached

- Design tradeoffs:
  - Separation oracle vs linear optimization oracle: Separation oracle gives better regret bounds but may not always exist
  - Number of oracle calls: More calls lead to better regret but higher computational cost
  - Step size selection: Affects convergence speed and regret bounds

- Failure signatures:
  - Algorithm gets stuck at infeasible points
  - Regret bounds degrade significantly
  - Oracle calls increase exponentially

- First 3 experiments:
  1. Implement and test separation oracle on simple gsc-convex sets (e.g., geodesic balls)
  2. Verify Jacobi field comparison technique provides correct bounds on non-convex separating hyperplanes
  3. Compare regret bounds and oracle call complexity between separation and linear optimization oracle approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the exponential dependence on the diameter (R) in the bound for the separation oracle unavoidable?
- Basis in paper: [explicit] The authors note that "the result on manifolds is significantly worse with respect to R, given the exponential nature of sinh" and ask "It is an interesting line of inquiry to explore whether this dependence is unavoidable."
- Why unresolved: The authors provide an exponential bound using the sinh function but do not prove whether a polynomial bound is possible.
- What evidence would resolve it: A counterexample showing no polynomial bound exists, or an algorithm achieving a polynomial bound.

### Open Question 2
- Question: Can we achieve better regret bounds for strongly gsc-convex losses using a separation oracle?
- Basis in paper: [inferred] The authors achieve O(T^{1/2}) and O(T^{3/4}) adaptive regret for gsc-convex losses but only mention achieving O(T^{2/3} log T) regret for strongly gsc-convex losses with a linear optimization oracle.
- Why unresolved: The paper focuses on gsc-convex losses with a separation oracle and does not explore the strongly gsc-convex case.
- What evidence would resolve it: An algorithm achieving better regret bounds for strongly gsc-convex losses with a separation oracle.

### Open Question 3
- Question: Is (1+τ)K gsc-convex for any τ ≥ 0 on Hadamard manifolds?
- Basis in paper: [explicit] The authors note "We conjecture that on Hadamard manifolds, (1+τ)K is gsc-convex for any τ ≥ 0" but do not prove it.
- Why unresolved: The authors use this conjecture in their bandit algorithm but acknowledge it remains unproven.
- What evidence would resolve it: A proof showing (1+τ)K is gsc-convex for any τ ≥ 0, or a counterexample demonstrating it is not.

## Limitations

- The algorithms assume geodesically convex sets and functions, which is restrictive and may not hold for many practical problems
- Limited empirical validation on real-world datasets or non-trivial manifolds
- Unclear computational complexity of the separation and linear optimization oracles on general Riemannian manifolds

## Confidence

- Theoretical results: High
- Practical applicability: Medium
- Empirical validation: Low

## Next Checks

1. Implement and benchmark the algorithms on simple manifolds (e.g., spheres, hyperbolic spaces) with synthetic gsc-convex loss functions to verify the theoretical regret bounds empirically
2. Analyze the computational complexity of the separation and linear optimization oracles for different manifold geometries, focusing on the number of iterations required for convergence
3. Investigate the applicability of the algorithms to non-gsc-convex problems by relaxing the convexity assumptions and studying the impact on regret bounds