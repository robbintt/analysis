---
ver: rpa2
title: Evaluating Verifiability in Generative Search Engines
arxiv_id: '2304.09848'
source_url: https://arxiv.org/abs/2304.09848
tags:
- citation
- queries
- search
- answer
- citations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the verifiability of four popular generative
  search engines (Bing Chat, NeevaAI, perplexity.ai, and YouChat) by measuring citation
  recall (proportion of statements supported by citations) and citation precision
  (proportion of citations that support their statements). Human evaluation across
  diverse queries shows that generated responses are fluent and appear helpful, but
  only 51.5% of sentences are fully supported by citations and 74.5% of citations
  support their associated statement.
---

# Evaluating Verifiability in Generative Search Engines

## Quick Facts
- arXiv ID: 2304.09848
- Source URL: https://arxiv.org/abs/2304.09848
- Reference count: 7
- Only 51.5% of generated sentences are fully supported by citations

## Executive Summary
This study evaluates the verifiability of four popular generative search engines (Bing Chat, NeevaAI, perplexity.ai, and YouChat) by measuring citation recall (proportion of statements supported by citations) and citation precision (proportion of citations that support their statements). Human evaluation across diverse queries shows that generated responses are fluent and appear helpful, but only 51.5% of sentences are fully supported by citations and 74.5% of citations support their associated statement. Citation recall and precision are inversely correlated with fluency and perceived utility, likely because systems often copy or closely paraphrase from cited webpages. These results suggest that existing generative search engines may mislead users despite appearing trustworthy.

## Method Summary
The study evaluates 1450 queries from diverse sources using responses from four generative search engines. Human annotators rate fluency and perceived utility, filter non-verifiable statements, and judge citation support using the AIS framework. The evaluation measures citation recall, precision, and F1 scores, with high inter-annotator agreement (82%+). Queries are stratified by answer type to analyze performance across different query categories.

## Key Results
- Only 51.5% of generated sentences are fully supported by citations
- 74.5% of citations support their associated statements
- Citation recall and precision inversely correlate with fluency and perceived utility
- Systems perform worse on queries requiring abstractive reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The inverse correlation between citation recall/precision and fluency/perceived utility occurs because systems often copy or closely paraphrase from cited webpages
- Mechanism: When systems copy text directly from cited sources, the citation precision increases (since copied text is almost always supported by the source) but fluency decreases (because copied snippets may not form coherent or topically relevant responses to the user's query)
- Core assumption: Copying/paraphrasing from citations directly increases citation precision while reducing overall response quality
- Evidence anchors:
  - [abstract] "These results suggest that existing generative search engines may mislead users despite appearing trustworthy"
  - [section] "we find that generated statements often closely paraphrase or directly copy from their associated citations"
  - [corpus] "The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs in LLM Generations" - related work on trade-offs between verifiability and abstractiveness
- Break condition: Systems stop copying/paraphrasing from citations and instead generate more abstractive responses while maintaining citation accuracy

### Mechanism 2
- Claim: Systems struggle with queries that cannot be answered extractively, leading to lower citation recall
- Mechanism: When no clear extractive answer exists on the internet for a query, systems generate statements without citations, resulting in low citation recall
- Core assumption: Citation recall is driven by the relevance and existence of retrieved webpages that directly answer the input query
- Evidence anchors:
  - [abstract] "responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements"
  - [section] "system responses often have the lower-than-average perceived utility on AllSouls queries... and NaturalQuestions queries with table-type long answers and no short answer"
  - [corpus] "Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions" - related work on challenges with difficult queries
- Break condition: Systems improve at handling queries requiring abstractive reasoning or information aggregation from multiple sources

### Mechanism 3
- Claim: Systems have difficulty with content selection, leading to inconsistent citation precision
- Mechanism: Systems struggle to identify and weigh sources of varying relevance and trustworthiness, resulting in citations that do not always support their associated statements
- Core assumption: Citation precision is affected by systems' ability to select and evaluate the relevance of retrieved sources
- Evidence anchors:
  - [abstract] "only 74.5% of citations support their associated statement"
  - [section] "Evaluation is performed on queries from 7 NaturalQuestions subdistributions... stratified by their answer type"
  - [corpus] "Search Engines in an AI Era: The False Promise of Factual and Verifiable Source-Cited Responses" - related work on challenges with citation accuracy
- Break condition: Systems improve at content selection and source evaluation

## Foundational Learning

- Concept: Citation recall vs citation precision
  - Why needed here: Understanding these metrics is crucial for evaluating the verifiability of generative search engines
  - Quick check question: What is the difference between citation recall and citation precision in the context of generative search engines?

- Concept: AIS (Attributed to Identified Sources) evaluation framework
  - Why needed here: This framework is used to judge whether generated statements are supported by their citations
  - Quick check question: How does the AIS evaluation framework help in assessing the verifiability of generated statements?

- Concept: Inter-annotator agreement
  - Why needed here: Understanding agreement metrics helps assess the reliability of human evaluation results
  - Quick check question: Why is high inter-annotator agreement important in human evaluation studies?

## Architecture Onboarding

- Component map: Query processing -> Retrieval -> Generation -> Citation matching -> Evaluation
- Critical path: Query -> Retrieval -> Generation -> Response with citations -> Human evaluation of verifiability
- Design tradeoffs: Balancing citation accuracy vs response fluency, handling extractable vs non-extractable queries
- Failure signatures: Low citation recall (unsupported statements), low citation precision (inaccurate citations), inverse correlation between citation metrics and fluency/perceived utility
- First 3 experiments:
  1. Measure citation recall and precision for a diverse set of queries across multiple generative search engines
  2. Analyze the relationship between citation metrics and response fluency/perceived utility
  3. Investigate the correlation between copying/paraphrasing behavior and citation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the trade-off between fluency/perceived utility and citation recall/precision manifest in different domains or types of queries?
- Basis in paper: [explicit] The paper discusses this inverse correlation but does not explore its variability across different query types or domains.
- Why unresolved: The study provides a general observation of the inverse correlation but does not delve into whether this relationship holds consistently across different types of queries or domains.
- What evidence would resolve it: A detailed analysis of the correlation across various domains or types of queries, perhaps using a more granular categorization of queries.

### Open Question 2
- Question: What are the underlying mechanisms that cause some generative search engines to copy or closely paraphrase from cited webpages, and how can these be addressed to improve both fluency and citation accuracy?
- Basis in paper: [explicit] The paper hypothesizes that copying or closely paraphrasing from cited webpages leads to high citation precision but low fluency and perceived utility.
- Why unresolved: The paper identifies the phenomenon but does not explore the underlying mechanisms or potential solutions to balance copying with generating more fluent and relevant responses.
- What evidence would resolve it: Research into the training methods and architectures of these systems, along with experiments to test alternative approaches that balance extraction and generation.

### Open Question 3
- Question: How can generative search engines be designed to better handle queries that cannot be answered extractively, such as those requiring reasoning over multiple sources or synthesizing information from different domains?
- Basis in paper: [explicit] The paper notes that systems struggle with citation recall on queries that lack a clear extractive answer on the Internet.
- Why unresolved: While the paper identifies the problem, it does not propose or evaluate potential solutions for handling such complex queries.
- What evidence would resolve it: Development and testing of new models or methods that can reason over multiple sources and synthesize information, along with evaluation of their performance on such queries.

## Limitations

- Evaluation relies heavily on human annotators, introducing potential subjectivity
- Study focuses on four specific generative search engines, limiting generalizability
- Threshold for "full support" in AIS framework introduces some uncertainty in metrics

## Confidence

**High Confidence**: The inverse correlation between citation metrics and fluency/perceived utility is well-supported by the data, with clear statistical evidence showing that copying/paraphrasing behavior affects both citation precision and response quality.

**Medium Confidence**: The claim that systems struggle with queries requiring abstractive reasoning is supported by the data showing lower performance on certain query types, but the underlying mechanisms could benefit from further investigation.

**Medium Confidence**: The finding that 51.5% of sentences are fully supported by citations represents a robust measurement, though the threshold for "full support" in the AIS framework introduces some uncertainty.

## Next Checks

1. **Reproduce correlation analysis**: Collect new annotations for a subset of queries to verify the inverse relationship between citation metrics and fluency/perceived utility scores across different annotator groups.

2. **Test abstraction threshold**: Conduct a controlled experiment varying the "fully supported" threshold in the AIS framework to understand how different definitions of verifiability affect the reported metrics.

3. **Cross-system comparison**: Evaluate the same query set across additional generative search engines or newer versions of the studied systems to assess whether the identified limitations are persistent or evolving.