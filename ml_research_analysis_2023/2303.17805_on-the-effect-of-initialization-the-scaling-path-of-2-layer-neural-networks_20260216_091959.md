---
ver: rpa2
title: 'On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks'
arxiv_id: '2303.17805'
source_url: https://arxiv.org/abs/2303.17805
tags:
- path
- optimal
- measure
- page
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies a modified regularization path for infinite-width
  two-layer ReLU neural networks with non-zero initial weight distributions at different
  scales. The authors introduce a scale-dependent regularizer and show that despite
  the non-convexity of the problem, it admits an infinite-dimensional convex counterpart
  via a connection with unbalanced optimal transport theory.
---

# On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks

## Quick Facts
- **arXiv ID**: 2303.17805
- **Source URL**: https://arxiv.org/abs/2303.17805
- **Reference count**: 35
- **One-line result**: The scaling path continuously interpolates between kernel and rich regimes as initialization scale varies from 0 to infinity

## Executive Summary
This paper studies the effect of initialization scale on the training dynamics of infinite-width two-layer ReLU neural networks. By introducing a scale-dependent regularizer based on unbalanced optimal transport theory, the authors formulate a convex functional optimization problem that captures the distance between the initialization and learned parameters. The main theoretical result shows that as the initialization scale varies from 0 to infinity, the associated scaling path continuously interpolates between the kernel regime (lazy training) and the rich regime (feature learning). Numerical experiments on a simple 2D interpolation task confirm that the scaling path and final states of the optimization path behave similarly for varying initialization scales.

## Method Summary
The method involves minimizing an empirical risk objective regularized by a complexity measure N(f,μ₀) based on unbalanced optimal transport distance. The regularization parameter α controls the initialization scale, with α→0 corresponding to the rich regime and α→∞ to the kernel regime. The optimization is performed using entropic regularization of the Hellinger-Kantorovich distance and a forward-backward splitting algorithm with spectral step-size predictor and Armijo line-search. The analysis proves Γ-convergence of the optimization problem as α varies, ensuring continuity of optimal solutions with respect to initialization scale.

## Key Results
- The scaling path continuously interpolates between kernel and rich regimes as initialization scale varies
- Γ-convergence ensures continuity of optimal solutions with respect to initialization scale
- Numerical experiments confirm that scaling path solutions converge to kernel regime for large α and rich regime for small α

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The scaling path continuously interpolates between kernel and rich regimes as initialization scale varies
- Mechanism: The complexity measure N(f,μ₀) uses unbalanced optimal transport theory to create a convex functional that captures the distance from initialization to a neural network's parameter distribution
- Core assumption: The parametrization function φ satisfies 2-homogeneity, regularity in parameters, and Lipschitz regularity in input
- Evidence anchors:
  - [abstract]: "By exploiting a link with unbalanced optimal-transport theory, we show that, despite the non-convexity of the 2-layer network training, this problem admits an infinite dimensional convex counterpart"
  - [section 2]: "Using the Monge formulation of optimal transport, we obtain the upper bound N(f,μ₀) ≤ inf {∥T-Id∥²L²(μ₀) : f = ∫Rp φ(w,·) d[T#μ₀](w)}"
- Break condition: If the 2-homogeneity assumption fails or the initialization measure doesn't satisfy the support condition in Proposition 2.3

### Mechanism 2
- Claim: Γ-convergence ensures continuity of optimal solutions with respect to initialization scale
- Mechanism: As α varies, the family of functionals in (53) Γ-converges to limiting functionals representing kernel and rich regimes, preserving convergence of minimizers
- Core assumption: The loss function L is proper, convex, and lower semi-continuous
- Evidence anchors:
  - [section 3]: "We prove that the underlying family of functions is Γ-convergent and that the rich regime (6) and the kernel regime (8) are the limits for α→0 and α→∞, respectively"
- Break condition: If the loss function L fails to be convex or if the initialization measure violates the support conditions

### Mechanism 3
- Claim: The scaling path provides a smooth interpolation between the limiting regimes that matches empirical optimization paths
- Mechanism: Numerical experiments show that for large initialization scales, the scaling path solutions converge to the kernel regime, while small scales converge to the rich regime, with intermediate scales providing smooth interpolation
- Core assumption: The discretization of the optimization problem preserves the essential characteristics of the continuous problem
- Evidence anchors:
  - [section 4]: "For larger values of α and β, the solutions corresponding to the same values are very similar"
- Break condition: If the discretization introduces artifacts that dominate the true behavior of the continuous problem

## Foundational Learning

- Concept: Optimal transport and Wasserstein metrics
  - Why needed here: The complexity measure N(f,μ₀) is fundamentally defined using Wasserstein distances to measure how much the parameter distribution must move from initialization to realize a given neural network
  - Quick check question: What is the difference between the Monge and Kantorovich formulations of optimal transport?

- Concept: Γ-convergence in variational analysis
  - Why needed here: Γ-convergence provides the theoretical framework to prove that optimal solutions depend continuously on the initialization scale α, which is essential for the main result
  - Quick check question: What are the two conditions that define Γ-convergence of a sequence of functionals?

- Concept: Neural tangent kernel and lazy training regime
  - Why needed here: The paper's analysis shows that as initialization scale α→∞, the scaling path converges to the kernel regime, which is characterized by the neural tangent kernel framework
  - Quick check question: How does the neural tangent kernel differ from standard kernel methods in terms of the feature map it uses?

## Architecture Onboarding

- Component map:
  - Complexity measure N(f,μ₀) based on unbalanced optimal transport
  - Scaling path formulation as convex functional optimization
  - Γ-convergence analysis framework
  - Numerical implementation with entropy regularization for computational tractability
  - Comparison with gradient descent optimization paths

- Critical path:
  1. Define parametrization function φ and verify it satisfies required properties
  2. Implement complexity measure N(f,μ₀) using unbalanced optimal transport
  3. Set up scaling path optimization problem (53)
  4. Apply Γ-convergence theory to prove continuity of optimal solutions
  5. Implement numerical experiments with entropy regularization
  6. Compare scaling path solutions with gradient descent optimization paths

- Design tradeoffs:
  - Using entropy regularization for computational efficiency vs. approximating true optimal transport
  - Discretizing the problem on a Fibonacci grid vs. computational cost
  - Choosing initialization measure μ₀ to satisfy support conditions vs. practical considerations
  - Using 2-layer ReLU networks for theoretical tractability vs. practical relevance

- Failure signatures:
  - If the initialization measure μ₀ doesn't satisfy the support condition in Proposition 2.3, the complexity measure may not have the desired properties
  - If the discretization grid is too coarse, the numerical solutions may not accurately represent the continuous problem
  - If the entropy regularization parameter is too large, the solutions may deviate significantly from true optimal transport solutions
  - If the loss function L is not convex, the Γ-convergence analysis may fail

- First 3 experiments:
  1. Verify that the complexity measure N(f,μ₀) satisfies the properties proven in Lemma 2.5 for a simple test case
  2. Implement the scaling path optimization for a small neural network with known solution to verify the framework works
  3. Compare the scaling path solutions with gradient descent optimization for various initialization scales to validate the theoretical predictions

## Open Questions the Paper Calls Out

- Question: Does the scaling path exhibit similar properties for deeper neural networks beyond two layers?
  - Basis in paper: [inferred] The paper focuses on 2-layer ReLU neural networks but mentions the general framework for other types of functions φ
  - Why unresolved: The paper does not extend the analysis to deeper networks, leaving the question of whether the continuity and interpolation properties hold for more complex architectures
  - What evidence would resolve it: Extending the analysis to deeper networks and demonstrating similar continuity and interpolation properties

- Question: How does the choice of initialization distribution (beyond uniform on the sphere) affect the scaling path?
  - Basis in paper: [explicit] The paper uses uniform distribution on the sphere as initialization but mentions the possibility of other choices
  - Why unresolved: The paper does not explore other initialization distributions, leaving the impact on the scaling path unexplored
  - What evidence would resolve it: Analyzing the scaling path for different initialization distributions and comparing the results

- Question: Can the scaling path be used to improve the generalization performance of neural networks in practice?
  - Basis in paper: [inferred] The paper shows the scaling path interpolates between kernel and rich regimes, which are known to have different generalization properties
  - Why unresolved: The paper focuses on theoretical properties and does not investigate practical applications or improvements in generalization
  - What evidence would resolve it: Implementing the scaling path in practical training scenarios and measuring its impact on generalization performance

## Limitations

- The analysis relies heavily on the 2-homogeneity property of the parametrization function φ, which may not hold for all neural network architectures
- The theoretical framework assumes specific support conditions on the initialization measure μ₀ that may be challenging to verify in practice
- The numerical experiments use a small dataset with only 10 samples, limiting the generalizability of the findings to larger-scale problems

## Confidence

**High Confidence**: The connection between the scaling path problem and unbalanced optimal transport theory is well-established mathematically, with rigorous proofs provided in the paper. The Γ-convergence analysis follows standard techniques from variational analysis and is theoretically sound given the stated assumptions.

**Medium Confidence**: The numerical experiments demonstrate the theoretical predictions for a simple 2D interpolation task, but the small scale of the problem and limited dataset size make it difficult to assess whether the findings will generalize to more complex, high-dimensional problems.

**Low Confidence**: The paper does not address how the scaling path framework extends to deeper neural networks or different activation functions, leaving open questions about the applicability of the theory to practical deep learning scenarios.

## Next Checks

1. **Scalability Test**: Implement the scaling path framework on larger datasets (MNIST/CIFAR) with more complex architectures to verify whether the theoretical predictions about the kernel-to-rich regime transition hold in high-dimensional settings.

2. **Initialization Sensitivity**: Systematically vary the initialization measure μ₀ (different distributions, supports) to test the robustness of the Γ-convergence results and identify conditions under which the theoretical framework breaks down.

3. **Gradient Flow Comparison**: Compare the scaling path solutions not just with discrete gradient descent but with continuous gradient flow dynamics to determine whether the scaling path captures the full continuum of behaviors between kernel and rich regimes.