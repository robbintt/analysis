---
ver: rpa2
title: Walking a Tightrope -- Evaluating Large Language Models in High-Risk Domains
arxiv_id: '2311.14966'
source_url: https://arxiv.org/abs/2311.14966
tags:
- llms
- high-risk
- domains
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) in high-risk
  domains, focusing on legal and medical tasks. It compares out-of-the-box and instruction-tuned
  models on six datasets, measuring factuality and safety using established metrics.
---

# Walking a Tightrope -- Evaluating Large Language Models in High-Risk Domains

## Quick Facts
- arXiv ID: 2311.14966
- Source URL: https://arxiv.org/abs/2311.14966
- Reference count: 18
- One-line primary result: Instruction-tuning improves LLM factuality in high-risk domains, but current models still struggle with accuracy and safety, requiring better metrics and human oversight.

## Executive Summary
This paper evaluates large language models (LLMs) in high-risk domains, focusing on legal and medical tasks. It compares out-of-the-box and instruction-tuned models on six datasets, measuring factuality and safety using established metrics. Results show that while instruction-tuning improves performance, current LLMs still struggle with accuracy and safety in high-risk contexts. Qualitative analysis reveals limitations in existing metrics, suggesting they may not fully capture domain-specific requirements. The authors advocate for better models, refined metrics, and human-centric systems to ensure reliable LLM deployment in high-risk applications.

## Method Summary
The study employs domain-adaptive instruction-tuning using QLoRA to incorporate legal and medical knowledge into smaller foundation models, training them on 13K instructions for each domain. The models are evaluated on six NLP datasets for factuality (using QAFactEval and UniEval) and safety (using SafetyKit and Detoxify). The evaluation includes both automated metrics and qualitative analysis to identify limitations in current assessment approaches.

## Key Results
- Instruction-tuning with domain-specific data improves LLM factuality in high-risk domains compared to out-of-the-box models
- Existing factuality and safety metrics are insufficient for reliably evaluating LLM performance in high-risk domains
- Human oversight and interpretability are necessary components for safe LLM deployment in high-risk domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuning with domain-specific data improves LLM factuality in high-risk domains compared to out-of-the-box models
- Mechanism: Fine-tuning smaller foundation models on curated legal and medical datasets (13K instructions each) allows them to incorporate domain knowledge and task-specific reasoning patterns that general-purpose LLMs lack
- Core assumption: The selected domain datasets contain sufficient domain-specific knowledge and task patterns to improve performance when used for instruction-tuning
- Evidence anchors:
  - [abstract] "Our study delves into an in-depth analysis of the performance of instruction-tuned LLMs, focusing on factual accuracy"
  - [section 3] "we employ QLoRA... to efficiently incorporate domain knowledge"
  - [corpus] Weak - the related papers focus on factuality evaluation but don't specifically validate instruction-tuning as the mechanism
- Break condition: If the domain datasets are too narrow, outdated, or lack the nuanced reasoning patterns needed for high-risk domain tasks, instruction-tuning may not transfer effectively

### Mechanism 2
- Claim: Existing factuality and safety metrics are insufficient for reliably evaluating LLM performance in high-risk domains
- Mechanism: The metrics (QAFactEval, UniEval, SafetyKit, Detoxify) were designed for general-purpose content and may not capture domain-specific requirements like legal nuance or medical accuracy
- Core assumption: High-risk domain tasks have unique requirements that general-purpose metrics cannot adequately measure
- Evidence anchors:
  - [abstract] "Qualitative analysis reveals limitations in existing metrics, suggesting they may not fully capture domain-specific requirements"
  - [section 4] "we find that some answers are in fact repetitions of the question... these results put into question whether these metrics accurately reflect the factuality"
  - [section 4] "the qualitative analysis results highlight that more research needs to be conducted on how we can define reliable and domain-adjusted safety metrics"
- Break condition: If new metrics are developed that can accurately capture domain-specific factuality and safety requirements, this mechanism would no longer explain the limitations

### Mechanism 3
- Claim: Human oversight and interpretability are necessary components for safe LLM deployment in high-risk domains
- Mechanism: Since LLMs cannot guarantee perfect factuality and safety, systems must be designed to maintain human control through interpretability (providing source evidence) and verification (human approval/modification workflows)
- Core assumption: High-risk domain applications require human judgment that cannot be fully automated, even with improved models and metrics
- Evidence anchors:
  - [abstract] "The authors advocate for better models, refined metrics, and human-centric systems to ensure reliable LLM deployment"
  - [section 5] "we propose to adopt the framework proposed by Shneiderman (2020), enabling both high automation and human control"
  - [section 5] "The resulting human-centric system allows for responsible usage even when the output may not be flawless"
- Break condition: If future LLM capabilities reach a level where factuality and safety can be guaranteed with near-perfect accuracy, the need for human oversight could diminish

## Foundational Learning

- Concept: Domain adaptation through instruction-tuning
  - Why needed here: General LLMs lack the specialized knowledge and reasoning patterns required for high-risk domains like legal and medical applications
  - Quick check question: What distinguishes instruction-tuning from standard fine-tuning, and why is this distinction important for domain adaptation?

- Concept: Evaluation metric limitations and domain specificity
  - Why needed here: Understanding why general-purpose metrics fail in high-risk domains helps identify what needs to be measured instead
  - Quick check question: How might a metric that works well for general content evaluation fail when applied to specialized legal or medical reasoning?

- Concept: Human-AI collaboration frameworks
  - Why needed here: The paper argues that even with improved models and metrics, human oversight remains essential for high-risk applications
  - Quick check question: What are the key components of a human-centric system that maintains appropriate control while leveraging AI automation?

## Architecture Onboarding

- Component map: Data collection → Instruction template creation → Model fine-tuning → Automated evaluation → Qualitative analysis → Human-centric system design
- Critical path: Data collection → Instruction template creation → Model fine-tuning → Automated evaluation → Qualitative analysis → Human-centric system design
- Design tradeoffs:
  - Model size vs. performance: Smaller models are more efficient but may lack some capabilities of larger models
  - Metric automation vs. human judgment: Automated metrics are scalable but may miss domain-specific nuances
  - Instruction quantity vs. quality: More instructions may improve performance but increase computational costs
- Failure signatures:
  - High metric scores but poor qualitative performance (indicates metric inadequacy)
  - Model outputs that repeat questions or provide generic answers (indicates insufficient domain knowledge)
  - Large variance in performance across different tasks within the same domain (indicates inconsistent adaptation)
- First 3 experiments:
  1. Compare instruction-tuned vs. out-of-the-box models on a small subset of legal and medical tasks to verify the basic mechanism
  2. Test the same models with different instruction quantities (e.g., 5K vs. 13K) to understand the relationship between instruction volume and performance
  3. Apply alternative factuality metrics to the same outputs to identify metric-specific behaviors and potential weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do current LLMs perform on high-risk domain tasks in languages other than English?
- Basis in paper: [inferred] The paper mentions that their collected datasets currently only focus on English, which is identified as a limitation
- Why unresolved: The study did not evaluate LLM performance on non-English high-risk domain tasks, so it remains unclear how well these models generalize across different languages in such contexts
- What evidence would resolve it: Evaluating the same LLM models on equivalent high-risk domain tasks in multiple languages would provide direct evidence of their multilingual performance and generalizability

### Open Question 2
- Question: What is the impact of different instruction template designs on LLM performance in high-risk domains?
- Basis in paper: [inferred] The paper notes that instruction templates were designed manually and might lead to variable outcomes, suggesting that template design could influence results
- Why unresolved: The study used a single set of manually designed templates for each task, so the effect of alternative template designs on performance remains unexplored
- What evidence would resolve it: Systematically testing multiple instruction template variations for the same tasks and comparing LLM performance would reveal how template design affects outcomes

### Open Question 3
- Question: How do other metrics like robustness and explainability affect the evaluation of LLMs in high-risk domains?
- Basis in paper: [inferred] The paper mentions that other metrics should be explored and considered, such as robustness and explainability, but did not include them in the current evaluation
- Why unresolved: The study focused solely on factuality and safety metrics, leaving the impact of additional evaluation dimensions like robustness and explainability unexamined
- What evidence would resolve it: Incorporating robustness and explainability metrics into the evaluation framework and analyzing their results alongside factuality and safety would provide a more comprehensive assessment of LLM performance in high-risk domains

## Limitations
- The specific instruction templates used for fine-tuning are not provided, making it difficult to assess whether improvements stem from the tuning approach or template quality
- The study focuses on a limited set of six datasets, which may not represent the full complexity of real-world legal and medical applications
- The evaluation metrics used were designed for general-purpose content, and their limitations in high-risk domains are demonstrated qualitatively rather than quantitatively

## Confidence
- Medium: The observation that instruction-tuning improves factuality compared to out-of-the-box models is reasonably well-supported, though the effect size and generalizability remain unclear
- Low: The claim that existing metrics are insufficient for high-risk domains, while qualitatively supported, lacks systematic validation of what better metrics would look like
- Medium: The argument for human-centric systems as necessary for safe deployment is conceptually sound but lacks empirical evidence of its effectiveness in practice

## Next Checks
1. **Template Replication Study**: Have independent researchers recreate the instruction templates and fine-tune the same or similar models to verify that the observed improvements are reproducible and not dependent on proprietary template design

2. **Metric Development Experiment**: Design and validate a small set of domain-specific factuality and safety metrics for legal and medical domains, then compare their performance against the general-purpose metrics used in this study to quantify the gap in evaluation quality

3. **Human Evaluation Validation**: Conduct a controlled study where domain experts evaluate model outputs using both the automated metrics and human judgment, measuring the correlation between automated scores and expert assessments to determine if the metrics capture meaningful domain-specific requirements