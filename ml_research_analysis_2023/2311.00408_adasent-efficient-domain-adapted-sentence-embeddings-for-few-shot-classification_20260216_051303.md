---
ver: rpa2
title: 'AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification'
arxiv_id: '2311.00408'
source_url: https://arxiv.org/abs/2311.00408
tags:
- sept
- dapt
- sentence
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaSent improves few-shot sentence classification by training a
  SEPT adapter on a base PLM, which can then be inserted into DAPT-ed PLMs from any
  domain. This modular approach matches or surpasses the performance of full SEPT
  on DAPT-ed PLM while substantially reducing training costs.
---

# AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification

## Quick Facts
- arXiv ID: 2311.00408
- Source URL: https://arxiv.org/abs/2311.00408
- Reference count: 25
- Key outcome: AdaSent achieves 3.9 points average accuracy improvement over SetFit by training a SEPT adapter on base PLM that can be inserted into DAPT-ed PLMs from any domain

## Executive Summary
AdaSent introduces a parameter-efficient approach for few-shot sentence classification by decoupling Sentence Embedding Pre-Training (SEPT) from Domain-Adaptive Pre-Training (DAPT). The method trains a SEPT adapter once on a base Pre-trained Language Model (PLM) and reuses it across domain-adapted versions, achieving performance comparable to full SEPT while reducing training costs by 37%. Experiments on 17 datasets demonstrate consistent improvements, particularly in scenarios where traditional SEPT-DAPT integration fails.

## Method Summary
AdaSent trains a SEPT adapter on a base PLM (DistilRoBERTa) using NLI+SC+SE datasets with MNRL loss, then inserts this adapter into DAPT-ed PLMs from any domain. The DAPT stage uses MLM objective on unlabeled task-specific data for 2344 steps, while the SEPT adapter is trained for one epoch. Finally, SetFit performs contrastive fine-tuning with few-shot labeled data (8 shots per class). This modular approach allows independent execution of DAPT and SEPT, avoiding the inefficiency of repeating SEPT for each new DAPT-ed model.

## Key Results
- Average 3.9 points accuracy improvement over SetFit across 17 datasets
- Matches or surpasses full SEPT performance on DAPT-ed PLMs
- Reduces training time by 37% compared to DAPT→SEPT pipeline
- Works with multiple PEFT methods including bottleneck adapters, LoRA, and prefix-tuning

## Why This Works (Mechanism)

### Mechanism 1
SEPT adapter trained on base PLM generalizes to domain-adapted PLMs when both use MLM objective. The adapter stores sentence-level semantic specialization that's compatible with MLM-pretrained PLM parameters learning token-level semantics. This fails when using DAPT objectives like TSDAE or SimCSE that break semantic compatibility.

### Mechanism 2
Adapter-based SEPT is more efficient than full fine-tuning SEPT on each DAPT-ed model. The SEPT adapter is trained once and reused across all domains, eliminating repeated SEPT training. This decouples DAPT and SEPT execution while maintaining performance.

### Mechanism 3
AdaSent resolves incompatibility between DAPT and SEPT training orders. By decoupling SEPT into an adapter, DAPT can be applied first without disrupting SEPT effects. The adapter insertion preserves semantic structure established by SEPT.

## Foundational Learning

- **Sentence Embedding Pre-Training (SEPT)**: Creates semantic similarity space enabling SetFit for few-shot classification. *Quick check*: What training objective does SEPT typically use, and what type of data pairs are used?

- **Domain-Adaptive Pre-Training (DAPT)**: Adapts general-purpose PLMs to domain-specific language patterns using unlabeled in-domain data. *Quick check*: What is the primary objective function used in DAPT, and how does it differ from SEPT objectives?

- **Parameter-Efficient Fine-Tuning (PEFT)**: Enables training adapters instead of full models, reducing computational cost while maintaining performance. *Quick check*: What are the key differences between bottleneck adapters, parallel adapters, and prefix-tuning in terms of parameter count and training dynamics?

## Architecture Onboarding

- **Component map**: Base PLM (DistilRoBERTa) → DAPT stage (MLM on domain data) → SEPT adapter (trained on base PLM) → Adapter insertion → SetFit fine-tuning
- **Critical path**: DAPT training → SEPT adapter training → Adapter insertion → SetFit training
- **Design tradeoffs**: Single adapter vs. domain-specific adapters; MLM vs. alternative DAPT objectives; adapter type selection
- **Failure signatures**: Adapter incompatibility with DAPT-ed models; performance degradation compared to full SEPT; training instability during SetFit
- **First 3 experiments**:
  1. Train SEPT adapter on base PLM with NLI+SC+SE dataset and verify it works with original PLM in SetFit
  2. Apply DAPT to base PLM with MLM objective on domain data, then insert SEPT adapter and test SetFit performance
  3. Compare AdaSent against DAPT→SEPT baseline on 2-3 datasets to validate efficiency gains while maintaining accuracy

## Open Questions the Paper Calls Out

### Open Question 1
What specific properties of the NLI+SC+SE dataset make it more effective for SEPT compared to other datasets like AllNLI or Paraphrase? The paper shows NLI+SC+SE transfers best for downstream tasks but doesn't analyze why this combination works better.

### Open Question 2
How does AdaSent performance change when using different PEFT methods for DAPT compared to MLM? The paper explores PEFT methods for SEPT but not for DAPT in the main AdaSent setting.

### Open Question 3
What is the optimal number of steps for DAPT when using AdaSent, and how does this vary across different domains? The paper shows 2344 steps work well but mentions 1000 steps may be sufficient without exploring the relationship across domains.

## Limitations

- Relies on MLM consistency between base PLM and DAPT-ed models, which may not hold for all DAPT objectives
- Assumes single SEPT adapter can generalize across all domains without domain-specific adaptation
- Limited testing on highly specialized domains where semantic patterns may differ significantly

## Confidence

- **Performance Claims**: Medium - Validated on 17 datasets but limited to specific domains and DAPT objectives
- **Efficiency Claims**: Medium - Time comparisons provided but without computational resource specifications
- **Mechanism Claims**: Medium - Core mechanism demonstrated but not exhaustively tested across DAPT variations
- **Generalization Claims**: Low - Assumes adapter transferability without systematic domain-specific testing

## Next Checks

1. Test AdaSent with alternative DAPT objectives (TSDAE, SimCSE) across 5 diverse domains to quantify performance drops and validate the 3.9 point average improvement.

2. Compare different adapter types (bottleneck vs parallel vs LoRA) in AdaSent pipeline to analyze parameter efficiency vs accuracy tradeoffs.

3. Test AdaSent on highly specialized domains (legal, medical, technical) to measure semantic drift between base PLM SEPT and domain-adapted SEPT requirements.