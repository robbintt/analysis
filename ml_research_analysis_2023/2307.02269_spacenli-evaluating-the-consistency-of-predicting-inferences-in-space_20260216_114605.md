---
ver: rpa2
title: 'SpaceNLI: Evaluating the Consistency of Predicting Inferences in Space'
arxiv_id: '2307.02269'
source_url: https://arxiv.org/abs/2307.02269
tags:
- problems
- spatial
- patterns
- inference
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SpaceNLI, a novel dataset for evaluating spatial
  reasoning in natural language inference (NLI). It contains 32,000 NLI problems generated
  from 160 patterns, covering diverse spatial inference types like directional, projective,
  non-projective, and argument orientation.
---

# SpaceNLI: Evaluating the Consistency of Predicting Inferences in Space

## Quick Facts
- arXiv ID: 2307.02269
- Source URL: https://arxiv.org/abs/2307.02269
- Reference count: 23
- Key outcome: SpaceNLI is a novel dataset for evaluating spatial reasoning in NLI, containing 32,000 problems generated from 160 patterns covering diverse spatial inference types.

## Executive Summary
This paper introduces SpaceNLI, a dataset designed to evaluate natural language inference (NLI) models on spatial reasoning tasks. The dataset contains 32,000 NLI problems generated from 160 manually designed patterns, covering directional, projective, non-projective, and argument orientation inferences. The authors propose a new metric called Pattern Accuracy that measures model consistency across samples from the same pattern, arguing it's a more reliable evaluation than standard accuracy for pattern-based datasets. Evaluation on SpaceNLI reveals that while state-of-the-art NLI models achieve moderate accuracy (60-67%), they struggle with consistency, particularly on non-projective spatial inferences involving prepositions like "between."

## Method Summary
The authors created SpaceNLI by first manually designing 160 NLI patterns from seed problems covering diverse spatial inference types. Each pattern specifies selection restrictions on entities. Using a mini world with predefined entities and relations, they automatically generated NLI problems by replacing pattern placeholders with valid entities. The dataset was then used to evaluate NLI models fine-tuned on multiple NLI datasets (SNLI, MNLI, ANLI, etc.). The evaluation used both standard accuracy and Pattern Accuracy (PA), which measures the percentage of patterns where a model achieves at least a given consistency threshold across all generated samples from that pattern.

## Key Results
- SOTA NLI models achieve 60-67% accuracy on SpaceNLI but show significant consistency issues
- Pattern Accuracy reveals that high standard accuracy doesn't guarantee consistency across patterns
- Non-projective spatial inferences, especially those involving the "between" preposition, are the most challenging (41.6% accuracy)
- Argument orientation inferences are the easiest type for models to handle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pattern accuracy is a more reliable and stricter measure than standard accuracy for evaluating consistency in pattern-based NLI datasets.
- Mechanism: Pattern accuracy measures the percentage of patterns for which a model achieves at least a given consistency threshold across all generated samples. This captures not just overall accuracy but the ability to generalize across lexical variations within each pattern.
- Core assumption: The NLI patterns in SpaceNLI are designed such that samples from the same pattern should have consistent inference labels, making consistency a meaningful evaluation criterion.
- Evidence anchors:
  - [abstract]: "Moreover, we introduce a Pattern Accuracy and argue that it is a more reliable and stricter measure than the accuracy for evaluating a system's performance on pattern-based generated data samples."
  - [section 4.2.2]: "The PA curve records the PA score of a model for each consistency threshold... we define the pattern accuracy (PA) score and its curve."
- Break condition: If patterns contain inherent ambiguity or if the pattern generation process introduces noise that makes consistent labeling impossible even for humans, then pattern accuracy would not be a fair evaluation metric.

### Mechanism 2
- Claim: Non-projective spatial inferences, particularly those involving the "between" preposition, are the most challenging for NLI models.
- Mechanism: Non-projective prepositions require understanding spatial relations that depend on multiple entities and their relative positions, which is more complex than projective prepositions that rely on a single frame of reference.
- Core assumption: The complexity of "between" semantics, which involves order and proximity relations between multiple entities, makes it harder for models to learn consistent inference patterns.
- Evidence anchors:
  - [abstract]: "The results also reveal that non-projective spatial inferences (especially due to the "between" preposition) are the most challenging ones."
  - [section 5]: "The evaluation results revealed that the most challenging inference type is associated with non-projective locatives mainly due to the complex semantics of 'between' while the argument orientation type is the easiest."
- Break condition: If models are trained with sufficient examples of "between" and non-projective prepositions, or if the dataset is expanded to include more diverse examples, this difficulty might be mitigated.

### Mechanism 3
- Claim: Models with high standard accuracy on SpaceNLI may not be the most consistent in their predictions across patterns.
- Mechanism: Standard accuracy measures overall performance across all samples, while pattern accuracy measures consistency within each pattern. A model can achieve high accuracy by performing well on easy patterns while struggling with harder ones, leading to lower consistency.
- Core assumption: The distribution of difficulty across patterns is such that models can exploit easier patterns to boost overall accuracy without being truly consistent.
- Evidence anchors:
  - [section 4.2.2]: "While DeBERTaV3-L#2 gets the best score on the SpaceNLI problems, based on the PA scores in Table 4, it shows high consistency (PA0.95 or PA1.0) in fewer NLI patterns than the other two competing models, DeBERTaV3-L#1 and ALBERT-XXLv2."
  - [section 4.2.2]: "It drastically decreases after 95%