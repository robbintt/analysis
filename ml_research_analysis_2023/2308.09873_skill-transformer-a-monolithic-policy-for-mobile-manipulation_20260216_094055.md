---
ver: rpa2
title: 'Skill Transformer: A Monolithic Policy for Mobile Manipulation'
arxiv_id: '2308.09873'
source_url: https://arxiv.org/abs/2308.09873
tags:
- skill
- transformer
- skills
- object
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Skill Transformer is an end-to-end method for long-horizon object
  rearrangement tasks. It uses a transformer policy to predict both high-level skills
  and low-level actions conditioned on egocentric observations.
---

# Skill Transformer: A Monolithic Policy for Mobile Manipulation

## Quick Facts
- arXiv ID: 2308.09873
- Source URL: https://arxiv.org/abs/2308.09873
- Authors: Multiple
- Reference count: 40
- Key outcome: Skill Transformer achieves 2.5x higher success rates on hard episodes and 1.45x overall on the Habitat rearrange-hard benchmark compared to modular baselines.

## Executive Summary
Skill Transformer is an end-to-end method for long-horizon object rearrangement tasks that uses a transformer policy to predict both high-level skills and low-level actions conditioned on egocentric observations. The method trains on demonstration trajectories that solve the full task, combining the composability of modular approaches with the robustness of end-to-end learning. On the Habitat rearrange-hard benchmark, Skill Transformer demonstrates significantly higher success rates than modular baselines, particularly on challenging episodes requiring reasoning about hidden objects and disturbances.

## Method Summary
Skill Transformer uses a transformer-based architecture with separate Skill Inference and Action Inference modules. The Skill Inference module predicts a discrete skill (navigate, pick, place, open drawer, open fridge) from a history of observations, while the Action Inference module predicts low-level robot actions conditioned on this skill. The model is trained end-to-end using imitation learning from demonstration trajectories, with an auxiliary loss that predicts receptacle states to help the model learn to distinguish between failed skill attempts and environmental disturbances.

## Key Results
- Achieves 2.5x higher success rates on hard episodes and 1.45x overall compared to modular baselines
- More robust to disturbances like closing drawers that weren't present in training
- Demonstrates effective long-horizon reasoning without requiring oracle task planning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Skill Transformer's skill inference module enables high-level reasoning about which skill to execute at each timestep, allowing adaptation to unknown perturbations like closing drawers.
- Mechanism: A causal transformer predicts a one-hot skill vector conditioned on the history of egocentric observations and proprioception. This skill embedding is fed into the action inference module, which learns to execute low-level actions consistent with the inferred skill.
- Core assumption: The skill embedding space captures semantically meaningful distinctions between skills and that the action inference module can generalize low-level control conditioned on these embeddings.
- Evidence anchors: Abstract mentions retaining composability and modularity while avoiding hand-off errors common in modular approaches.

### Mechanism 2
- Claim: The autoregressive training with teacher-forcing and auxiliary skill prediction loss enables Skill Transformer to learn robust skill transitions and identify receptacle states even when demonstrations are imperfect.
- Mechanism: The model is trained to predict both skills and actions autoregressively over a context window. The auxiliary loss supervises the model to predict whether the object/goal is in a receptacle.
- Core assumption: The auxiliary receptacle state prediction provides a useful inductive bias that helps the model learn to associate visual patterns with receptacle states.
- Evidence anchors: Section explains that the auxiliary loss incentivizes identifying key features that lead to decisions about opening specific receptacles.

### Mechanism 3
- Claim: The transformer architecture with positional embeddings and causal masking enables Skill Transformer to model long-term dependencies in the rearrangement task while maintaining computational efficiency.
- Mechanism: The causal transformer processes a sequence of input tokens with positional embeddings to capture temporal relationships. The causal mask ensures the model only attends to past information.
- Core assumption: The transformer's self-attention mechanism can effectively capture long-term dependencies in rearrangement tasks without requiring prohibitively large context windows.
- Evidence anchors: Abstract states Skill Transformer is trained to predict both high-level skills and low-level actions using a transformer architecture.

## Foundational Learning

- Concept: Causal sequence modeling with transformers
  - Why needed here: The rearrangement task requires reasoning over long sequences of observations and actions, where each decision depends on the history of previous states.
  - Quick check question: How does the causal mask in the transformer architecture prevent information leakage during training while still allowing the model to capture necessary long-term dependencies?

- Concept: Multi-task learning with auxiliary losses
  - Why needed here: The rearrangement task involves both high-level skill selection and low-level action prediction. The auxiliary receptacle state prediction loss provides an additional training signal.
  - Quick check question: Why might the auxiliary receptacle state prediction loss improve the model's ability to handle perturbations even though this information is not available during test time?

- Concept: Behavior cloning from demonstration trajectories
  - Why needed here: The rearrangement task is challenging to learn from scratch with reinforcement learning due to sparse rewards and complex dynamics.
  - Quick check question: What are the potential limitations of training Skill Transformer with behavior cloning from imperfect demonstrations?

## Architecture Onboarding

- Component map: Visual observation → Visual encoder → Skill Inference module → Skill embedding → Action Inference module → Low-level action output
- Critical path: The skill inference module is critical because it conditions the action inference on the correct high-level behavior; failures in skill prediction will propagate to action execution
- Design tradeoffs:
  - Fixed context length (30 timesteps) vs. adaptive context: Fixed length simplifies implementation but may miss long-term dependencies
  - Separate skill and action transformers vs. unified architecture: Separate modules allow specialized learning but require careful coordination
  - Auxiliary receptacle state prediction: Provides useful training signal for handling perturbations but adds complexity
- Failure signatures:
  - High skill prediction error rates: Model struggles to identify which skill to execute
  - Low success rates on hard/very hard splits: Model fails to adapt to unknown perturbations
  - Performance gap between train and eval: Model overfits to training scenes
  - Oscillation between skills: Model repeatedly switches between skills without progress
- First 3 experiments:
  1. Evaluate skill prediction accuracy on a held-out validation set
  2. Test model robustness to simulated perturbations (e.g., closing drawers after opening)
  3. Ablate the auxiliary receptacle state prediction loss to quantify its impact

## Open Questions the Paper Calls Out
1. How does Skill Transformer's performance scale with dataset size beyond 10,000 demonstrations?
2. Can Skill Transformer be effectively combined with online reinforcement learning?
3. How does Skill Transformer handle more complex task specifications like natural language instructions?

## Limitations
- Performance measured only in simulation with a Fetch robot, limiting generalizability to real-world conditions
- Claims about robustness to disturbances lack quantitative validation with varying perturbation frequencies
- Use of privileged information during training raises questions about generalization to scenarios with noisy or unavailable receptacle state information

## Confidence
- **High confidence**: Claims about Skill Transformer's architecture and training procedure are well-specified and reproducible
- **Medium confidence**: Claims about 2.5x improvement on hard episodes are supported by benchmark results but may not generalize to real-world conditions
- **Low confidence**: Claims about robustness to unknown perturbations and generalization to unseen environments lack quantitative validation

## Next Checks
1. Evaluate Skill Transformer's performance on a real robot platform with varying levels of sensor noise and actuation uncertainty to assess sim-to-real transfer
2. Conduct ablation studies on the auxiliary receptacle state prediction loss to determine its impact on perturbation robustness and overall success rates
3. Test the model's generalization to novel environments and object configurations not seen during training to quantify its true adaptability