---
ver: rpa2
title: 'Peer Learning: Learning Complex Policies in Groups from Scratch via Action
  Recommendations'
arxiv_id: '2312.09950'
source_url: https://arxiv.org/abs/2312.09950
tags:
- learning
- agents
- agent
- advice
- peer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces peer learning, a novel high-level reinforcement
  learning framework where multiple agents learn a task simultaneously from scratch
  by exchanging action recommendations. Unlike standard RL, agents can communicate
  their states and recommended actions to peers, asking "What would you do in my situation?".
---

# Peer Learning: Learning Complex Policies in Groups from Scratch via Action Recommendations

## Quick Facts
- arXiv ID: 2312.09950
- Source URL: https://arxiv.org/abs/2312.09950
- Reference count: 15
- Peer learning enables multiple agents to learn complex tasks simultaneously from scratch by exchanging action recommendations, outperforming single-agent learning and baselines in challenging OpenAI Gym domains.

## Executive Summary
This paper introduces peer learning, a novel reinforcement learning framework where multiple agents learn a task simultaneously from scratch by exchanging action recommendations. Unlike standard RL, agents can communicate their states and recommended actions to peers, asking "What would you do in my situation?" The framework formalizes the teacher selection process as a non-stationary multi-armed bandit problem, emphasizing the need for exploration. Experiments demonstrate that peer learning outperforms single-agent learning and a state-of-the-art action advice baseline in challenging discrete and continuous OpenAI Gym domains, including MuJoCo control tasks.

## Method Summary
Peer learning involves multiple agents operating in separate environments who exchange action recommendations based on their current states. Each agent maintains trust weights representing their confidence in peers' advice, updated based on immediate rewards. The framework uses off-policy RL algorithms (SAC, DQN) and allows agents to learn from each other's experiences through vicarious reinforcement. The peer policy selects actions from the compilation of all recommended actions, balancing exploration and exploitation.

## Key Results
- Peer learning outperforms single-agent learning and state-of-the-art action advice baselines in multiple MuJoCo environments
- The framework demonstrates robustness against adversarial agents and shows improved performance with increasing group size
- Agents can effectively rank performance within the group and identify reliable advice-givers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The peer learning framework enables agents to improve performance by exchanging action recommendations, allowing them to learn from each other's experiences in a manner analogous to social learning theory.
- Mechanism: Agents operate in separate environments but communicate their states and recommended actions. Each agent uses a peer policy to select actions from the compilation of recommendations, effectively leveraging vicarious reinforcement.
- Core assumption: Agents can effectively evaluate the quality of advice from peers and make informed decisions about which advice to follow.
- Evidence anchors:
  - [abstract]: "Peer learning addresses a related setting in which a group of agents, i.e., peers, learns to master a task simultaneously together from scratch."
  - [section]: "We model vicarious reinforcement by allowing agents to observe other peers' suggestions for the situation (state) they are in."
  - [corpus]: Weak evidence - the corpus provides related work on collaborative learning but lacks specific evidence for the mechanism described in this paper.
- Break condition: The framework breaks down if agents cannot effectively evaluate the quality of advice or if communication is restricted to the point where it no longer provides meaningful information.

### Mechanism 2
- Claim: The trust mechanism, formalized as a non-stationary multi-armed bandit problem, allows agents to identify and prioritize advice from more reliable peers.
- Mechanism: Agents maintain weights representing their trust in each peer's advice. These weights are updated based on the immediate rewards obtained when following advice, using a Boltzmann distribution for action selection.
- Core assumption: Past rewards are indicative of future advice quality, and agents can effectively track and update their trust in peers over time.
- Evidence anchors:
  - [abstract]: "We formalize the teacher selection process in the action advice setting as a multi-armed bandit problem and therefore highlight the need for exploration."
  - [section]: "Each agent i ∈ N solicits action advice... and then decides from the compilation of all proposed actions... in accordance with a distinct peer policy Πi."
  - [corpus]: Weak evidence - the corpus discusses trust mechanisms but does not provide specific evidence for the non-stationary multi-armed bandit formalization used in this paper.
- Break condition: The mechanism breaks if the environment is highly non-stationary or if peer advice quality changes rapidly, making past rewards poor indicators of future advice quality.

### Mechanism 3
- Claim: Peer learning enables agents to outperform single-agent learning and state-of-the-art baselines in complex continuous control tasks.
- Mechanism: By exchanging action recommendations, agents can leverage the collective knowledge of the group, leading to faster learning and better final performance.
- Core assumption: The collective knowledge of the group is greater than that of individual agents, and the communication overhead is outweighed by the benefits of shared information.
- Evidence anchors:
  - [abstract]: "Experiments demonstrate that peer learning outperforms single-agent learning and a state-of-the-art action advice baseline in challenging discrete and continuous OpenAI Gym domains, including MuJoCo control tasks."
  - [section]: "We show that peer learning is able to outperform single-agent learning and the baseline in several challenging discrete and continuous OpenAI Gym domains."
  - [corpus]: Weak evidence - the corpus provides related work on collaborative learning but lacks specific evidence for the performance gains in continuous control tasks.
- Break condition: The mechanism breaks if the communication overhead becomes too high or if the group dynamics lead to suboptimal collective decisions.

## Foundational Learning

- Concept: Multi-armed bandit problem
  - Why needed here: To formalize the trust mechanism and the process of selecting which peer to follow for advice.
  - Quick check question: What is the key difference between a stationary and non-stationary multi-armed bandit problem?

- Concept: Reinforcement learning (RL)
  - Why needed here: Peer learning builds upon standard RL but introduces communication between agents.
  - Quick check question: How does peer learning differ from standard RL in terms of agent interaction?

- Concept: Social learning theory
  - Why needed here: The peer learning framework is inspired by social learning theory, which posits that learning can occur through observation and imitation of others.
  - Quick check question: What are the key components of social learning theory that are incorporated into the peer learning framework?

## Architecture Onboarding

- Component map:
  - Environment -> Agent -> Communication -> Trust Mechanism -> Peer Policy -> Action Execution

- Critical path:
  1. Initialize agents with random policies and trust weights.
  2. For each time step:
     a. Each agent observes its state and solicits advice from peers.
     b. Agents exchange action recommendations.
     c. Each agent selects an action using its peer policy.
     d. Agents execute actions and receive rewards.
     e. Agents update their trust weights based on the rewards obtained.

- Design tradeoffs:
  - Number of agents: More agents can lead to better performance but also increase communication overhead.
  - Trust update rate: Faster updates can adapt to changing peer advice quality but may be more sensitive to noise.
  - Exploration vs. exploitation: Agents must balance exploring different peers' advice with exploiting known good advice.

- Failure signatures:
  - Poor performance: If agents fail to identify good advice-givers or if communication is ineffective.
  - Communication overhead: If the number of agents is too high or if trust updates are too frequent.
  - Non-convergence: If the environment is highly non-stationary or if peer advice quality changes rapidly.

- First 3 experiments:
  1. Implement a basic peer learning setup with two agents in a simple grid world environment.
  2. Compare the performance of peer learning against single-agent learning in the grid world environment.
  3. Introduce an adversarial agent into the peer learning setup and evaluate the framework's robustness to poisoning attacks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of agents in peer learning for maximizing performance, and does this number vary based on task complexity?
- Basis in paper: [explicit] The paper mentions that "our study did not yield a single best combination that significantly outperforms all the other combinations in all environments" and discusses the effect of varying the number of agents.
- Why unresolved: The paper shows that performance increases with the number of agents but does not establish a clear optimal number or discuss how this might change with different task complexities.
- What evidence would resolve it: Systematic experiments varying the number of agents across tasks of different complexities, analyzing performance metrics and computational efficiency.

### Open Question 2
- Question: How does the trust mechanism in peer learning scale with the number of agents, and what are the implications for large-scale multi-agent systems?
- Basis in paper: [inferred] The paper introduces trust mechanisms but does not discuss their scalability or performance in large groups of agents.
- Why unresolved: The paper focuses on small group sizes and does not address the challenges of implementing trust mechanisms in larger, more complex systems.
- What evidence would resolve it: Experiments with larger groups of agents, measuring the effectiveness of trust mechanisms and their impact on overall system performance.

### Open Question 3
- Question: How does peer learning perform in environments with non-stationary reward structures or dynamic task requirements?
- Basis in paper: [inferred] The paper discusses non-stationary multi-armed bandit problems in the context of trust but does not explore non-stationary environments.
- Why unresolved: The paper's experiments are conducted in static environments, and there is no discussion of how peer learning adapts to changing conditions.
- What evidence would resolve it: Experiments in environments with dynamic rewards or task changes, measuring the adaptability and robustness of peer learning agents.

### Open Question 4
- Question: What is the impact of heterogeneous agent capabilities on the effectiveness of peer learning, and how can the framework be adapted to leverage diverse skill sets?
- Basis in paper: [explicit] The paper mentions the inclusion of non-training experts and novices but does not extensively explore the implications of heterogeneous capabilities.
- Why unresolved: The paper's experiments primarily focus on homogeneous groups, and there is limited discussion on how diverse capabilities affect learning outcomes.
- What evidence would resolve it: Experiments with heterogeneous groups, analyzing the impact of diverse capabilities on learning speed and final performance, and developing strategies to optimize peer learning in such settings.

## Limitations
- Scalability concerns with large agent groups due to communication overhead and trust mechanism complexity
- Limited experimental validation across diverse RL domains, primarily focusing on MuJoCo continuous control tasks
- Uncertainty about performance in highly dynamic environments with rapidly changing peer advice quality

## Confidence

**Confidence Levels:**
- High confidence in the core mechanism of action recommendation exchange and trust-based peer selection
- Medium confidence in performance claims against baselines, given the limited scope of experimental domains
- Medium confidence in the robustness claims against adversarial agents, as testing was limited to simple poisoning attacks

## Next Checks

1. **Scalability Test**: Evaluate peer learning performance with agent groups of 10+ members across multiple environment types to assess communication overhead and trust mechanism scalability.

2. **Non-Stationary Environment Test**: Design experiments with rapidly changing peer advice quality to validate the trust mechanism's ability to adapt in non-stationary conditions.

3. **Cross-Domain Transfer Test**: Implement peer learning in environments with different characteristics (sparse rewards, high-dimensional observations) to assess generalizability beyond MuJoCo continuous control tasks.