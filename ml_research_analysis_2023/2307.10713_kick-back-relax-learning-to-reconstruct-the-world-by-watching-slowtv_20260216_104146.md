---
ver: rpa2
title: 'Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV'
arxiv_id: '2307.10713'
source_url: https://arxiv.org/abs/2307.10713
tags:
- depth
- vision
- conference
- monocular
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SlowTV, a large-scale dataset of 1.7M monocular
  video frames curated from YouTube to improve self-supervised monocular depth estimation
  (SS-MDE). The dataset contains diverse scenes from hiking, driving, and underwater
  activities, enabling models to generalize beyond the automotive domain.
---

# Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV

## Quick Facts
- arXiv ID: 2307.10713
- Source URL: https://arxiv.org/abs/2307.10713
- Reference count: 40
- This paper introduces SlowTV, a large-scale dataset of 1.7M monocular video frames curated from YouTube to improve self-supervised monocular depth estimation (SS-MDE).

## Executive Summary
This paper introduces SlowTV, a large-scale dataset of 1.7M monocular video frames curated from YouTube to improve self-supervised monocular depth estimation (SS-MDE). The dataset contains diverse scenes from hiking, driving, and underwater activities, enabling models to generalize beyond the automotive domain. The authors train an SS-MDE model using SlowTV alongside Mannequin Challenge and KITTI, achieving state-of-the-art zero-shot generalization performance on seven unseen datasets. Key contributions include aspect ratio augmentation, camera intrinsic estimation, support frame randomization, and flexible motion estimation.

## Method Summary
The authors train a self-supervised monocular depth estimation model using a combination of SlowTV (1.7M images), Mannequin Challenge (115k images), and KITTI (71k images) datasets. The model uses a ConvNeXt-B backbone for depth estimation and ConvNeXt-T for pose estimation, with joint camera intrinsic learning. Training employs photometric loss with SSIM weighting, minimum reconstruction loss, automasking, aspect ratio augmentation (AR-Aug), and flexible motion estimation. The model is trained for 60 epochs with specific augmentations and optimization techniques designed to improve generalization across diverse environments.

## Key Results
- Achieves state-of-the-art zero-shot generalization performance on seven unseen datasets
- Outperforms existing SS-MDE approaches and closes the gap with supervised methods
- Demonstrates strong generalization across urban, natural, synthetic, and indoor scenes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The SlowTV dataset provides the scale and diversity necessary for SS-MDE models to generalize beyond automotive scenes.
- **Mechanism:** SlowTV contains 1.7M images across diverse environments (hiking, driving, underwater) which increases the distribution of scene types the model encounters during training. This broader distribution helps the model learn generalizable depth estimation patterns rather than overfitting to urban automotive features.
- **Core assumption:** Monocular depth estimation benefits from seeing diverse scene types rather than just automotive scenes, and that YouTube-sourced videos provide sufficient quality for training.
- **Evidence anchors:**
  - [abstract] "SlowTV contains 1.7M images from a rich diversity of environments, such as worldwide seasonal hiking, scenic driving and scuba diving"
  - [section] "SlowTV provides a general distribution across a wide range of natural scenes, while Mannequin Challenge covers indoor scenes with humans and Kitti focuses on urban scenes"
  - [corpus] Weak evidence - no direct citations about SlowTV's effectiveness found in related papers
- **Break condition:** If the dataset quality is too low or contains insufficient variation within each category, the model may still fail to generalize.

### Mechanism 2
- **Claim:** Aspect ratio augmentation (AR-Aug) enables models to generalize across different image sizes and shapes.
- **Mechanism:** By randomly cropping and resizing images during training to different aspect ratios (16 predefined ratios), the model learns to handle various input shapes. This prevents overfitting to a specific resolution and improves zero-shot performance on datasets with different image dimensions.
- **Core assumption:** Depth estimation models can learn to be resolution-agnostic when exposed to sufficient variation during training.
- **Evidence anchors:**
  - [section] "AR-Aug has the effect of drastically increasing the distribution of image shapes, aspect ratios and object scales seen by the network during training"
  - [section] "As shown in Section 5.4, this greatly increases performance, especially when evaluating on datasets with different image sizes"
  - [corpus] Weak evidence - no direct citations about AR-Aug's effectiveness found in related papers
- **Break condition:** If the augmentation is too aggressive, it might introduce artifacts that confuse the model rather than help it generalize.

### Mechanism 3
- **Claim:** Learning camera intrinsics jointly with depth and pose improves model flexibility and performance.
- **Mechanism:** Instead of using fixed camera parameters, the model predicts focal lengths and principal points from the pose network. This allows the model to adapt to uncalibrated cameras and potentially handle variations in camera characteristics across the diverse SlowTV videos.
- **Core assumption:** The model can learn accurate camera parameters from the photometric loss alone without explicit supervision.
- **Evidence anchors:**
  - [section] "We take inspiration from [23, 12] and predict camera intrinsics using the pose network ΦP"
  - [section] "Both quantities are predicted as normalized and scaled by the image shape prior to combining them into K"
  - [corpus] Weak evidence - no direct citations about learned intrinsics effectiveness found in related papers
- **Break condition:** If the photometric loss is insufficient to constrain camera parameters, the learned intrinsics may diverge and hurt performance.

## Foundational Learning

- **Self-supervised monocular depth estimation**: This is the core task where depth is estimated from single images without ground truth labels, using photometric consistency across frames. Why needed here: The paper builds entirely on this framework to enable large-scale training without expensive annotations. Quick check question: How does the model ensure photometric consistency across frames when estimating depth?

- **Photometric loss and SSIM**: The photometric loss measures how well the synthesized view matches the target view. SSIM (Structural Similarity Index) is used to weight this loss. Why needed here: The paper uses a weighted combination of SSIM and L1 loss for reconstruction. Quick check question: What advantage does SSIM provide over pure L1 loss in depth estimation?

- **Minimum reconstruction loss and automasking**: These techniques handle dynamic objects by optimizing only the pixels with smallest loss across support frames and discarding pixels where unwarped target loss is lower. Why needed here: The SlowTV dataset contains dynamic objects (vehicles, hikers, marine life) that would otherwise create artifacts. Quick check question: How does the minimum reconstruction loss help mitigate the impact of occluded pixels?

## Architecture Onboarding

- **Component map**: Input image -> ConvNeXt-B backbone (depth network) -> DispNet decoder -> disparity map; ConvNeXt-T backbone (pose network) -> camera intrinsics prediction (optional) -> pose estimation

- **Critical path**: For inference, only the depth network is required. The pose network and learned intrinsics are only used during training. The critical path is: input image → depth network → disparity map output.

- **Design tradeoffs**: The model trades off some accuracy for efficiency by using ConvNeXt backbones instead of more expensive transformers. It also avoids motion masks and semantic segmentation, opting for simpler minimum reconstruction loss and automasking to handle dynamic objects.

- **Failure signatures**: The model struggles with highly reflective surfaces (mirrors, TVs), extreme camera rotations, and dynamic objects that move at similar speeds to the camera. It may also suffer from texture-copy artifacts and incorrect relative object positioning in complex scenes.

- **First 3 experiments:**
  1. Train baseline model on Kitti only to establish performance without SlowTV data
  2. Add SlowTV to training and measure zero-shot generalization improvement
  3. Implement and test aspect ratio augmentation to verify improvement on datasets with different image sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would incorporating optical flow in a self-supervised manner improve depth estimation accuracy in dynamic scenes?
- Basis in paper: [explicit] The authors mention that future work should explore using optical flow to refine estimated correspondences in a self-supervised manner, without requiring semantic segmentation or motion masks.
- Why unresolved: Current models struggle with dynamic objects, and while optical flow could help, the specific architecture and training procedure for incorporating it self-supervised are not detailed.
- What evidence would resolve it: A comparative study showing improved performance on datasets with significant dynamic content when using self-supervised optical flow refinement versus existing methods.

### Open Question 2
- Question: Would models trained on SlowTV data with learned camera intrinsics generalize better to scenes with significant camera rotation or extreme viewpoint changes?
- Basis in paper: [inferred] The authors note that their model struggles with extreme rotations (e.g., TUM-RGBD) due to the upright image prior in training data, and learned intrinsics could help adapt to different camera configurations.
- Why unresolved: The current model's limitation with extreme rotations suggests potential benefits from better camera calibration, but this hasn't been thoroughly tested across varied datasets.
- What evidence would resolve it: Experiments comparing generalization performance on datasets with significant rotation when using learned vs. fixed camera intrinsics.

### Open Question 3
- Question: How would the performance of self-supervised depth estimation models change if trained on a combination of SlowTV and additional indoor datasets?
- Basis in paper: [explicit] The authors suggest that additional indoor data may significantly reduce the remaining gap between self-supervised and supervised approaches.
- Why unresolved: While SlowTV provides diverse outdoor and underwater data, the lack of indoor scenes limits indoor performance compared to supervised models trained on indoor datasets.
- What evidence would resolve it: Training and evaluating models on combined SlowTV and indoor datasets (e.g., NYUD-v2) to measure performance improvements in indoor environments.

## Limitations
- The specific contribution of SlowTV dataset to final performance is not fully isolated from other training choices
- Aspect ratio augmentation's impact lacks ablation studies isolating it from other techniques
- The model struggles with highly reflective surfaces, extreme camera rotations, and dynamic objects that move at similar speeds to the camera

## Confidence
- High confidence: The overall framework works for SS-MDE and the zero-shot generalization results are reproducible
- Medium confidence: The specific contributions of SlowTV dataset, AR-Aug, and learned intrinsics to the final performance gains
- Low confidence: Claims about why YouTube SlowTV specifically is superior to other potential large-scale video sources

## Next Checks
1. Conduct an ablation study isolating aspect ratio augmentation's contribution by training with fixed aspect ratios but all other SlowTV + techniques
2. Test the learned camera intrinsics approach against fixed intrinsics using the same SlowTV training data to measure the specific contribution
3. Attempt to reproduce the key results on unseen datasets (ScanNet, TUM, EuRoC) to verify zero-shot generalization claims independently