---
ver: rpa2
title: '$\pi2\text{vec}$: Policy Representations with Successor Features'
arxiv_id: '2306.09800'
source_url: https://arxiv.org/abs/2306.09800
tags:
- policy
- policies
- features
- learning
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes \u03C02vec, a method for representing behaviors\
  \ of black box policies as feature vectors. The method uses successor features to\
  \ capture how policies change the environment, leveraging foundation models for\
  \ state representations and offline data for training."
---

# $\pi2\text{vec}$: Policy Representations with Successor Features

## Quick Facts
- arXiv ID: 2306.09800
- Source URL: https://arxiv.org/abs/2306.09800
- Reference count: 7
- Primary result: π2vec outperforms action-based representations for offline policy evaluation using foundation models and successor features

## Executive Summary
This paper introduces π2vec, a method for representing black-box policies as feature vectors using successor features and foundation models. The approach captures how policies change the environment rather than their actions, enabling effective offline policy selection without executing policies. π2vec leverages pretrained visual foundation models (CLIP, ViT, TAP) to encode states, then trains successor feature networks using Fitted Q Evaluation on offline trajectory data. The method is evaluated across three simulated and two real robotics environments, demonstrating superior performance compared to action-based baselines in terms of normalized mean absolute error, rank correlation, and regret@1 metrics.

## Method Summary
π2vec represents policies by how they change the environment using successor features. The method encodes states with pretrained foundation models, trains successor feature networks using FQE on offline data to predict discounted sums of state features, and averages these features over canonical states to obtain policy vectors. A linear model then predicts policy returns from these vectors. The approach requires only offline datasets of trajectories and pretrained foundation models, making it suitable for resource-constrained settings where online policy execution is impractical.

## Key Results
- π2vec outperforms Actions representation when predicting values of unseen policies across multiple robotics domains
- Successor features trained with FQE using offline data effectively capture policy behavior without requiring online execution
- Different foundation models (CLIP, ViT, TAP) show varying performance, with CLIP and TAP generally outperforming random features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: π2vec represents policies by how they change the environment rather than by their actions
- Mechanism: Uses successor features ψϕ_π(s) which encode discounted expectations of foundation model features over entire trajectories under policy π. These successor features average to a single vector Ψϕ_π that captures aggregated average effect of the behavior.
- Core assumption: Reward function can be factored as r(s,a,s') = ⟨ϕ(s,a,s'), w⟩ where ϕ encodes state transitions
- Evidence anchors:
  - [abstract] "The policy representations capture how the statistics of foundation model features change in response to the policy behavior"
  - [section 3.2] "Successor features are an ideal choice here because (i) they explicitly encode how a policy changes the environment"
  - [corpus] Weak - only 5 neighbor papers, average FMR 0.468, no citations yet
- Break condition: If reward cannot be factored linearly in features, successor features lose their ability to predict values

### Mechanism 2
- Claim: Offline training of successor features is possible using Fitted Q Evaluation (FQE)
- Mechanism: Adapts Q-learning framework to learn successor features from offline data without executing policies. Uses distributional FQE objective to train ψϕ_π(s,a) network that predicts discounted sum of features
- Core assumption: Offline dataset D contains sufficient trajectories from diverse policies to learn meaningful successor features
- Evidence anchors:
  - [section 3.3] "We can solve for ψϕ_π as in Equation 5 using the FQE training algorithm"
  - [section 4.3] "Training the successor features network only requires offline data"
  - [corpus] Weak - neighbor papers don't discuss FQE specifically
- Break condition: If offline data is too narrow or unrepresentative, successor features cannot generalize to new policies

### Mechanism 3
- Claim: Foundation model features provide task-agnostic representations that work across domains
- Mechanism: Uses pretrained visual foundation models (CLIP, VIT, TAP) as state encoders ϕ. These models learn rich semantic or geometric features from large datasets that transfer to robotics tasks
- Core assumption: Foundation model features capture relevant information about environment changes that correlate with policy performance
- Evidence anchors:
  - [section 4.1] "We evaluate several encoders for use as the state feature encoder ϕ" and describe CLIP, VIT, TAP
  - [section 5] "Our proposal outperforms Actions representation when predicting values of unseen policies"
  - [corpus] Weak - no neighbor papers directly discuss foundation models for robotics policy evaluation
- Break condition: If foundation model features are not relevant to the specific robotic domain, representation quality degrades

## Foundational Learning

- Concept: Successor features and their relationship to value functions
  - Why needed here: Core mechanism for representing how policies change environment states
  - Quick check question: Can you explain why ψϕ_π(s) = Qπ(s,a) when reward is linear in features?

- Concept: Fitted Q Evaluation and its use in offline settings
  - Why needed here: Enables training successor features without online policy execution
  - Quick check question: How does FQE differ from standard Q-learning in terms of data requirements?

- Concept: Foundation model feature extraction and transfer learning
  - Why needed here: Provides generic state representations that work across different robotics tasks
  - Quick check question: What properties make CLIP, VIT, or TAP features suitable for this application?

## Architecture Onboarding

- Component map: Offline dataset → Foundation model ϕ → Successor feature network ψϕ_π → Policy representation Ψϕ_π → Performance prediction model
- Critical path: ψϕ_π network training is the bottleneck; requires sufficient offline data and proper foundation model choice
- Design tradeoffs:
  - Foundation model choice vs. representation quality (CLIP semantic vs. TAP geometric)
  - Number of bins B in successor feature distribution vs. computational cost
  - Dataset size vs. generalization capability
- Failure signatures:
  - Poor correlation between Ψϕ_π and true returns → foundation model mismatch or insufficient offline data
  - High NMAE but decent correlation → model underfits or feature dimension too low
  - Negative correlation → Actions baseline issue or wrong feature extractor
- First 3 experiments:
  1. Train π2vec with random features on Insert Gear (Sim) and compare to Actions baseline
  2. Try different foundation models (CLIP, VIT, TAP) on same dataset and measure performance difference
  3. Test generalization by training on one domain (Kitchen) and evaluating on another (Metaworld)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of foundation model (e.g., CLIP, ViT, TAP) impact policy representation quality across different robotic tasks and environments?
- Basis in paper: [explicit] The paper explicitly compares different foundation models (CLIP, ViT, TAP, Random) across multiple environments and tasks, showing varying performance.
- Why unresolved: While the paper provides empirical comparisons, it doesn't establish a definitive theory for why certain models work better in specific scenarios. The relationship between task characteristics and optimal foundation model choice remains unclear.
- What evidence would resolve it: Systematic ablation studies correlating specific task properties (e.g., visual complexity, occlusion, geometry) with optimal foundation model performance would help establish predictive guidelines.

### Open Question 2
- Question: Can π2vec be effectively extended to fully offline policy selection without any ground truth data, and how would this impact performance?
- Basis in paper: [explicit] The paper mentions extending π2vec to fully offline settings and compares it to FQE, but notes high NMAE and suggests rewards may not be well-modeled as linear functions of CLIP features.
- Why unresolved: The paper demonstrates π2vec can work in fully offline settings but doesn't provide a comprehensive solution or optimization strategy for this challenging scenario.
- What evidence would resolve it: Developing and validating alternative reward modeling approaches that don't rely on linear assumptions, or demonstrating improved performance with non-linear reward functions, would address this limitation.

### Open Question 3
- Question: How does the choice of dataset (e.g., demonstrations vs. trajectories) affect the quality of π2vec policy representations?
- Basis in paper: [explicit] The paper conducts experiments comparing demonstrations and trajectories as training datasets, finding that trajectories generally lead to better performance.
- Why unresolved: While the paper shows trajectories are generally better, it doesn't provide a detailed analysis of why this is the case or establish guidelines for optimal dataset selection.
- What evidence would resolve it: Detailed analysis of how dataset characteristics (e.g., diversity, coverage of state space) correlate with representation quality would help establish best practices for dataset selection.

## Limitations

- Performance heavily depends on the quality and relevance of foundation model features to the specific robotic domain
- The linear reward assumption may not hold in all scenarios, limiting successor feature effectiveness
- Generalization to completely unseen domains remains challenging without additional adaptation mechanisms

## Confidence

- Claim: π2vec outperforms action-based representations → Medium confidence (consistent improvements across multiple metrics and domains, but limited community validation)
- Claim: FQE enables offline successor feature training → Medium confidence (established method, but specific implementation details unclear)
- Claim: Foundation model transfer works effectively → Medium confidence (demonstrated performance improvements, but underlying reasons not fully explained)

## Next Checks

1. Test generalization by training on one domain (Kitchen) and evaluating on another (Metaworld) to verify cross-domain transfer capability
2. Perform ablation study comparing π2vec with random features versus learned foundation model features on the same offline dataset
3. Measure performance sensitivity to offline dataset size by training with 10%, 50%, and 100% of available data to establish data requirements