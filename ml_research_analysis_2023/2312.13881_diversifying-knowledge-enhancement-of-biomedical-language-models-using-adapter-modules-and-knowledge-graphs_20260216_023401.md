---
ver: rpa2
title: Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter
  Modules and Knowledge Graphs
arxiv_id: '2312.13881'
source_url: https://arxiv.org/abs/2312.13881
tags:
- knowledge
- language
- biomedical
- performance
- adapters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores enhancing biomedical language models (LMs) with
  structured knowledge from knowledge graphs (KGs) using lightweight adapter modules.
  The approach partitions large KGs (UMLS, OntoChem) into subgraphs, fine-tunes adapters
  on each subgraph, and combines them using AdapterFusion.
---

# Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2312.13881
- **Source URL**: https://arxiv.org/abs/2312.13881
- **Reference count**: 7
- **Primary result**: Adapter-based knowledge injection improves biomedical LM performance on QA tasks, achieving state-of-the-art on BioASQ-7b

## Executive Summary
This paper introduces a method for enhancing biomedical language models with structured knowledge from knowledge graphs using lightweight adapter modules. The approach partitions large biomedical knowledge graphs (UMLS, OntoChem) into subgraphs, fine-tunes adapters on each subgraph using entity prediction, and combines the knowledge via AdapterFusion layers. Experiments with PubMedBERT and BioLinkBERT on four downstream biomedical tasks show consistent performance improvements, particularly for question-answering datasets with up to 7% accuracy gains. The method achieves state-of-the-art average performance on the BioASQ-7b QA dataset while maintaining computational efficiency through frozen base model weights.

## Method Summary
The method involves partitioning knowledge graphs into subgraphs, fine-tuning adapter modules for each subgraph using entity prediction (masked language modeling), and combining the knowledge through AdapterFusion mixture layers. The base PLMs (PubMedBERT/BioLinkBERT) remain frozen while adapters specialize in biomedical knowledge. The combined model is then fine-tuned on downstream tasks. The approach uses METIS algorithm for graph partitioning and soft attention mechanisms to weigh adapter contributions contextually.

## Key Results
- Adapter-enhanced models show performance improvements on four biomedical tasks
- Question-answering datasets demonstrate the largest gains (up to 7% accuracy improvement)
- State-of-the-art average performance achieved on BioASQ-7b QA dataset
- Knowledge enhancement benefits are task-dependent, with document classification showing minimal gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adapter modules enable efficient knowledge injection without full model retraining
- **Mechanism**: Small bottleneck feed-forward layers inserted between transformer layers, with original model weights frozen, allow targeted fine-tuning on structured knowledge
- **Core assumption**: The frozen transformer weights retain sufficient general knowledge while adapters specialize in biomedical domain knowledge
- **Evidence anchors**: [abstract] "adapters are small bottleneck feed-forward layers inserted within each layer of a transformer-based language model" [section 2] "adapters are small layers that are inserted within a language model and are subsequently fine-tuned to a specific task"

### Mechanism 2
- **Claim**: Graph partitioning enables efficient handling of massive knowledge graphs
- **Mechanism**: Large KGs are divided into smaller subgraphs, each trained independently, then combined via AdapterFusion layers
- **Core assumption**: Knowledge can be effectively partitioned without losing critical relational information between entities
- **Evidence anchors**: [abstract] "partitioning knowledge graphs into smaller subgraphs, fine-tuning adapter modules for each subgraph, and combining the knowledge in a fusion layer" [section 3.2] "we use the approach of (Meng et al., 2021), which involves partitioning the KG into smaller subgraphs, which are then trained on independently, and later, their knowledge combined"

### Mechanism 3
- **Claim**: AdapterFusion layers effectively combine knowledge from multiple subgraphs
- **Mechanism**: Softmax attention mechanism assigns contextual mixture weights over adapters to predict task labels
- **Core assumption**: The learned attention weights can effectively balance contributions from different knowledge subgraphs
- **Evidence anchors**: [abstract] "combining the knowledge in a fusion layer" [section 3.4] "these layers serve the purpose of combining knowledge from various adapters to enhance the model's performance on downstream tasks"

## Foundational Learning

- **Concept**: Transformer architecture and attention mechanisms
  - **Why needed here**: Understanding how adapters fit into transformer layers and how knowledge flows through the network
  - **Quick check question**: How do adapter modules modify the standard transformer forward pass without changing the original weights?

- **Concept**: Knowledge graph structure and representation
  - **Why needed here**: Required to understand how biomedical knowledge is structured in triples and how it's converted to training data
  - **Quick check question**: What are the three components of a knowledge graph triple and how do they map to natural language?

- **Concept**: Masked language modeling objective
  - **Why needed here**: Understanding the entity prediction task used to fine-tune adapters on KG triples
  - **Quick check question**: How does entity prediction differ from standard token prediction in masked language modeling?

## Architecture Onboarding

- **Component map**: Base PLM (PubMedBERT/BioLinkBERT) with frozen weights -> Adapter modules inserted between transformer layers -> Knowledge graph partitioned into subgraphs -> AdapterFusion mixture layers for combining subgraph knowledge -> Task-specific fine-tuning layer

- **Critical path**: 1. Load and partition knowledge graph 2. Initialize adapter modules for each subgraph 3. Fine-tune adapters using entity prediction task 4. Add AdapterFusion layers to combine knowledge 5. Fine-tune on downstream biomedical tasks

- **Design tradeoffs**: Small adapters vs. larger adapters: balance between efficiency and learning capacity; Number of partitions: affects both efficiency and potential knowledge loss; Typed vs. fused relations in OntoChem: specificity vs. coverage

- **Failure signatures**: No performance improvement: likely issues with adapter initialization or training; Degradation in performance: possible catastrophic forgetting or knowledge conflicts; Training instability: may indicate issues with graph partitioning or fusion layer configuration

- **First 3 experiments**: 1. Single adapter fine-tuning on full UMLS graph to establish baseline 2. Multi-adapter setup with simple averaging fusion (no attention) 3. Full AdapterFusion approach with different partition counts to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How would merging the OntoChem and UMLS knowledge graphs into a unified knowledge graph affect the performance of knowledge-enhanced biomedical language models?
- **Basis in paper**: [explicit] The paper mentions that merging the two knowledge bases into a unified KG and using both to fine-tune the adapters presents a promising direction for future work
- **Why unresolved**: The paper did not explore this approach, so the impact on model performance is unknown
- **What evidence would resolve it**: Conducting experiments using a unified KG created by merging OntoChem and UMLS, and comparing the performance of knowledge-enhanced models with this unified KG to those using individual KGs

### Open Question 2
- **Question**: How would incorporating the source sentences from which OntoChem triples were extracted into the knowledge enhancement process affect the performance of biomedical language models?
- **Basis in paper**: [explicit] The paper suggests that the linguistic knowledge contained in the source sentences could be extracted and used in additional adapters to enhance the models, drawing inspiration from works like K-Adapter
- **Why unresolved**: The paper did not explore this approach, so the impact on model performance is unknown
- **What evidence would resolve it**: Implementing an approach to extract and utilize the linguistic knowledge from OntoChem source sentences in the knowledge enhancement process, and evaluating the performance of the resulting models on downstream tasks

### Open Question 3
- **Question**: How would having medical professionals curate the knowledge graphs used for knowledge enhancement affect the performance of biomedical language models and their adoption in practice?
- **Basis in paper**: [explicit] The paper mentions that having medical professionals curate the KGs could result in KELMs tailored directly by those who use them, and suggests this as a direction for future work
- **Why unresolved**: The paper did not explore this approach, so the impact on model performance and practical adoption is unknown
- **What evidence would resolve it**: Conducting experiments where medical professionals curate the KGs used for knowledge enhancement, and evaluating the performance of the resulting models on downstream tasks, as well as gathering feedback from medical professionals on the models' usefulness and potential for adoption in practice

## Limitations
- Performance gains are uneven across tasks, with minimal benefits for document classification
- Limited evaluation scope with only four downstream tasks, restricting generalizability
- Lack of analysis for potential knowledge conflicts when combining multiple partitioned subgraphs

## Confidence
- **High Confidence**: Adapter module architecture and integration with transformer models
- **Medium Confidence**: Performance improvements on question-answering tasks and state-of-the-art claim on BioASQ-7b
- **Low Confidence**: Generalization of results to other biomedical tasks and knowledge graphs

## Next Checks
1. **Ablation Study on Knowledge Graph Structure**: Conduct experiments comparing performance when using different numbers of partitions, different knowledge graphs, and different relation types to isolate the impact of knowledge structure on downstream performance.

2. **Cross-Task Transfer Analysis**: Evaluate the adapters trained on one task (e.g., UMLS for QA) on different downstream tasks to assess whether the knowledge enhancement generalizes or is task-specific.

3. **Knowledge Conflict Detection**: Implement analysis to identify and measure knowledge conflicts when combining multiple adapters, particularly for overlapping entities or contradictory information across subgraphs.