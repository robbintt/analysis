---
ver: rpa2
title: A Transformer Model for Symbolic Regression towards Scientific Discovery
arxiv_id: '2312.04070'
source_url: https://arxiv.org/abs/2312.04070
tags:
- encoder
- datasets
- training
- architecture
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a new Transformer model tailored for Symbolic
  Regression (SR) in the context of scientific discovery. We propose three encoder
  architectures with increasing flexibility but at the cost of column-permutation
  equivariance violation.
---

# A Transformer Model for Symbolic Regression towards Scientific Discovery

## Quick Facts
- arXiv ID: 2312.04070
- Source URL: https://arxiv.org/abs/2312.04070
- Reference count: 33
- Key outcome: Introduces Transformer-based symbolic regression with Mix encoder achieving state-of-the-art normalized tree-based edit distance on SRSD datasets

## Executive Summary
This work proposes a Transformer model tailored for symbolic regression in scientific discovery contexts. The authors introduce three encoder architectures with varying degrees of column-permutation equivariance violation, finding that the most flexible Mix architecture prevents overfitting and achieves superior performance on the SRSD benchmark. The model is trained on synthetic datasets generated from sampled skeleton equations, demonstrating strong generalization capabilities without additional computational cost during inference.

## Method Summary
The approach employs a standard Transformer architecture with an encoder-decoder structure. The key innovation lies in three proposed encoder variants: MLP (maintains equivariance), Att (partial flexibility), and Mix (maximum flexibility but violates equivariance). The model is trained using teacher forcing on synthetic datasets generated from randomly sampled skeleton equations with log-uniform variable sampling. Evaluation uses normalized tree-based edit distance against ground-truth equations on SRSD datasets, with the Mix encoder consistently outperforming alternatives.

## Key Results
- Mix encoder architecture achieves state-of-the-art performance on SRSD datasets using normalized tree-based edit distance
- Label smoothing does not significantly affect relative performance between encoder architectures
- The model shows no signs of overfitting with the Mix architecture, matching train/validation/test performance

## Why This Works (Mechanism)

### Mechanism 1
The Mix encoder architecture outperforms MLP and Att variants because it breaks column-permutation equivariance, allowing arbitrary interactions between variables. By flattening column features and applying shared MLP followed by self-attention across observations, the model discovers complex, non-linear relationships unconstrained by equivariance requirements. The core assumption is that this constraint is overly restrictive for scientific datasets. Evidence shows the Mix architecture doesn't overfit and matches train/validation/test performance. Break condition: If column permutation equivariance becomes critical (e.g., datasets where variable order encodes physical meaning), the Mix encoder could produce inconsistent predictions.

### Mechanism 2
Label smoothing does not significantly affect relative performance between encoder architectures. By regularizing training through softened target distributions, label smoothing helps with overfitting but doesn't resolve fundamental architectural limitations of MLP and Att encoders. The core assumption is that poor generalization stems from architectural rigidity, not label distribution sharpness. Evidence confirms introducing label smoothing doesn't change relative architecture behavior. Break condition: Aggressive label smoothing could temporarily mask poor architectural choices without fixing them.

### Mechanism 3
The synthetic training dataset's diversity and sampling strategy enable generalization to unseen SRSD datasets. Equations are sampled from a large space, simplified, and instantiated with log-uniformly sampled variables, creating diverse tabular datasets that capture variability in real scientific data. The core assumption is that training distribution sufficiently overlaps with SRSD dataset distribution. Evidence shows 1,000,000 random equations are generated for training. Break condition: If SRSD datasets contain equation forms or variable relationships not present in training distribution, generalization will degrade.

## Foundational Learning

- **Concept: Transformer architecture (encoder-decoder, self-attention, positional encodings)**
  - Why needed here: The task maps tabular data to token sequences representing mathematical expressions, naturally sequential and benefiting from attention-based modeling
  - Quick check question: What is the role of masked self-attention in the decoder during inference?

- **Concept: Symbolic regression and tree-based edit distance**
  - Why needed here: Symbolic regression requires finding interpretable equations; tree-based edit distance measures structural similarity to ground truth, critical for scientific discovery
  - Quick check question: How does normalized tree-based edit distance differ from R² in evaluating symbolic regression?

- **Concept: Permutation invariance and equivariance**
  - Why needed here: Scientific tabular datasets should be invariant to row shuffling and equivariant to column permutations for consistent predictions regardless of data ordering
  - Quick check question: Why does the Mix encoder violate column-permutation equivariance, and why is that acceptable here?

## Architecture Onboarding

- **Component map**: Tabular dataset (50x7) -> Encoder (MLP/Att/Mix) -> Decoder (Transformer) -> Token sequence (equation) -> Normalized tree-based edit distance

- **Critical path**: 1) Preprocess SRSD data to match training format 2) Sample 50 observations from test set 30 times 3) Feed each sample through Mix encoder 4) Run decoder auto-regressively to generate token sequence 5) Compute normalized tree edit distance to ground truth

- **Design tradeoffs**: Mix encoder sacrifices column-permutation equivariance for flexibility; larger dmodel and deeper stacks increase capacity but risk overfitting; synthetic training data trades realism for diversity

- **Failure signatures**: High training accuracy but poor validation/test edit distance → overfitting; degraded performance when columns are permuted → loss of equivariance; poor results on SRSD easy/medium → domain gap in training data

- **First 3 experiments**: 1) Train with MLP encoder; verify overfitting by comparing train vs. validation edit distance 2) Switch to Att encoder; observe reduction in overfitting but still higher edit distance than Mix 3) Use Mix encoder with label smoothing; confirm best SRSD performance and minimal overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact mechanisms by which the Mix architecture prevents overfitting compared to MLP and Att architectures?
- Basis in paper: [explicit] The paper states Mix allows arbitrary interactions between variables and doesn't suffer from overfitting, unlike MLP and Att
- Why unresolved: The paper provides qualitative explanation but lacks detailed quantitative analysis of why Mix prevents overfitting
- What evidence would resolve it: Detailed quantitative analysis comparing generalization gap and feature space representations of three architectures

### Open Question 2
- Question: How does the proposed model perform on SRSD datasets with more than six variables?
- Basis in paper: [inferred] The paper mentions expressions can include up to six variables but doesn't test performance on datasets with more variables
- Why unresolved: The paper doesn't provide experimental results or analysis for datasets with more than six variables
- What evidence would resolve it: Experimental results and analysis of model's performance on SRSD datasets with more than six variables

### Open Question 3
- Question: What are the effects of different token sampling weights on the model's performance?
- Basis in paper: [explicit] The paper mentions tokens have sampling weights to account for typical frequencies of operators but doesn't explore different sampling weights
- Why unresolved: The paper doesn't provide experimental results or analysis on how different token sampling weights affect performance
- What evidence would resolve it: Experimental results and analysis of model's performance with different token sampling weights

## Limitations
- Architectural tradeoff between flexibility and equivariance - Mix encoder achieves superior performance but sacrifices column-permutation equivariance
- Synthetic training data may not fully represent real scientific equation distributions
- No validation on datasets with more than six variables

## Confidence

**High Confidence Claims:**
- Mix encoder architecture outperforms MLP and Att variants on SRSD datasets
- Label smoothing doesn't significantly alter relative performance ranking between architectures
- Mix encoder shows no signs of overfitting while maintaining strong validation/test performance

**Medium Confidence Claims:**
- Superiority of Mix encoder stems from breaking column-permutation equivariance constraints
- Synthetic training dataset enables effective generalization to unseen SRSD datasets
- Normalized tree-based edit distance provides meaningful evaluation metric for scientific discovery

**Low Confidence Claims:**
- Specific architectural details of Cell MLP and encoder layers are fully specified and reproducible
- Token sampling weights for skeleton equation generation are optimal
- Approach generalizes beyond SRSD dataset distribution to arbitrary scientific discovery problems

## Next Checks
1. **Equivariance Validation**: Systematically evaluate Mix encoder's performance when column permutations are applied to test datasets to quantify practical impact of losing column-permutation equivariance.

2. **Distributional Analysis**: Compare statistical properties (equation complexity, variable correlations, functional forms) of synthetic training data with SRSD test sets to identify gaps in training distribution.

3. **Architecture Ablation**: Conduct controlled experiments varying encoder depth, model capacity, and architectural components while holding training distribution constant to isolate whether performance gains come from breaking equivariance specifically or increased architectural flexibility generally.