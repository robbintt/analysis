---
ver: rpa2
title: The Internal State of an LLM Knows When It's Lying
arxiv_id: '2304.13734'
source_url: https://arxiv.org/abs/2304.13734
tags:
- 'false'
- 'true'
- statement
- statements
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method, SAPLMA, to detect whether statements
  generated by large language models (LLMs) are truthful or not. The core idea is
  that the hidden layer activations of an LLM contain information about whether the
  model "believes" a statement is true or false.
---

# The Internal State of an LLM Knows When It's Lying

## Quick Facts
- arXiv ID: 2304.13734
- Source URL: https://arxiv.org/abs/2304.13734
- Reference count: 4
- Primary result: SAPLMA achieves 60-80% accuracy on truthfulness detection using LLM hidden layer activations

## Executive Summary
This paper introduces SAPLMA, a method for detecting whether statements generated by large language models are truthful by analyzing the model's hidden layer activations. The core insight is that these internal representations contain information about the model's "belief" in the truthfulness of statements, even when the final output may be false. By training a classifier on these activations, SAPLMA can predict truthfulness with significantly higher accuracy than few-shot prompting, achieving 60-80% accuracy on held-out topics. This approach has potential to improve the reliability of LLM-generated content by identifying false information before it reaches users.

## Method Summary
SAPLMA extracts hidden layer activations from Facebook OPT 6.7b when processing true and false statements across six topics. A shallow feedforward classifier is trained on these activation vectors to predict truthfulness, with training and testing conducted on different topics to ensure generalization. The method tests multiple layers (last, 28th, 24th, 20th, middle) to identify which best captures truthfulness information. Performance is evaluated using accuracy metrics and tested on both human-written and LLM-generated statements.

## Key Results
- SAPLMA significantly outperforms few-shot prompting, achieving 60-80% accuracy on specific topics
- The 20th hidden layer activation provides optimal performance for OPT-6.7b
- When tested on LLM-generated statements, accuracy drops to 55-60% with higher thresholds
- The method successfully generalizes across topics when trained on out-of-distribution data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hidden layer activations encode internal beliefs about truthfulness
- Mechanism: Intermediate representations capture semantic and factual relationships reflecting alignment with learned knowledge
- Core assumption: LLMs learn factual knowledge during pretraining that manifests in activation space
- Evidence: Hidden activations contain information about whether model "believes" statements are true/false
- Break condition: Model lacks relevant factual knowledge or activation space doesn't represent truthfulness

### Mechanism 2
- Claim: Classifier on activations outperforms few-shot prompting
- Mechanism: Direct mapping from internal representations to truthfulness probability bypasses unreliable surface generation
- Core assumption: Activation-to-truthfulness relationship is stable and generalizable across topics
- Evidence: SAPLMA achieves higher accuracy than few-shot prompting on held-out topics
- Break condition: Activations are highly topic-specific or classifier overfits to training topics

### Mechanism 3
- Claim: Multiple layers allow finding optimal truthfulness representation
- Mechanism: Different layers capture different abstraction levels; some better represent factual consistency
- Core assumption: Meaningful variation in truthfulness encoding across layers exists
- Evidence: 20th layer performs best for OPT-6.7b, but optimal layer may vary by model
- Break condition: All layers encode similarly or best layer is highly model-dependent

## Foundational Learning

- Concept: Feedforward neural networks with ReLU activation and sigmoid output
  - Why needed here: Maps high-dimensional activation vectors to truthfulness probability
  - Quick check question: Why use sigmoid output instead of softmax?

- Concept: Training on out-of-distribution topics to force generalization
  - Why needed here: Ensures classifier learns internal belief rather than topic-specific patterns
  - Quick check question: What happens if training and testing on same topics?

- Concept: Using activation vectors from multiple layers as features
  - Why needed here: Explores which layer best captures model's internal truthfulness representation
  - Quick check question: How would using only last layer change results?

## Architecture Onboarding

- Component map: Input statements -> OPT-6.7b model -> Hidden layer activations -> Feedforward classifier -> Truthfulness probability

- Critical path:
  1. Generate or collect true/false statements
  2. Feed each statement to LLM, record activations from chosen layer(s)
  3. Train classifier on activations from all but one topic
  4. Test classifier on held-out topic
  5. Repeat for each topic and layer choice

- Design tradeoffs:
  - Shallow classifier (fast, less overfitting) vs. deeper (potentially more expressive)
  - Single layer vs. concatenated multi-layer activations (simplicity vs. richness)
  - Fixed layer selection vs. learned layer weighting (stability vs. optimality)

- Failure signatures:
  - Accuracy near 0.5 on all topics (no signal in activations)
  - Large variance between training runs (unstable learning)
  - High accuracy on training topics but low on held-out topics (overfitting)
  - Degradation when tested on LLM-generated statements (domain shift)

- First 3 experiments:
  1. Train and test on same topic to establish upper bound performance
  2. Compare single-layer classifiers (last, middle, 20th) on single topic pair
  3. Evaluate on LLM-generated statements with and without threshold calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are optimal hidden layer activations for SAPLMA across different LLM architectures?
- Basis: Paper tests multiple layers for OPT-6.7b but only one model
- Why unresolved: Limited to single architecture without exploring cross-model generalizability
- Resolution: Test across range of LLM architectures (GPT-3, GPT-4, BERT) comparing layer performance

### Open Question 2
- Question: How does performance change when trained and tested on generated sentences from same topic?
- Basis: Paper avoids this scenario to focus on internal belief rather than topic-specific patterns
- Why unresolved: Doesn't explore trade-off between general applicability and topic-specific accuracy
- Resolution: Conduct experiments training and testing on generated sentences from same topic

### Open Question 3
- Question: How can SAPLMA be adapted to detect uncertainty in LLM-generated statements?
- Basis: Paper discusses potential for higher thresholds or multiple classifiers but no concrete implementation
- Why unresolved: Focuses on binary classification without exploring confidence quantification
- Resolution: Develop methods for calibrating SAPLMA to output confidence scores or uncertainty estimates

## Limitations

- Performance still leaves substantial room for error (60-80% accuracy) for high-stakes applications
- Limited validation to single model (OPT-6.7b) and six specific factual topics
- Significant accuracy drop (55-60%) on LLM-generated statements versus human-written ones
- Cannot distinguish between genuine "belief" encoding versus surface-level pattern matching

## Confidence

- High confidence: Experimental methodology and observation that SAPLMA outperforms few-shot prompting
- Medium confidence: Claim that activations encode internal "beliefs" about truthfulness
- Low confidence: Generalizability to other model architectures, topics, and real-world deployment

## Next Checks

1. **Cross-model validation**: Test SAPLMA on multiple LLM architectures (GPT-3, LLaMA, Claude) to verify generalizability beyond OPT-6.7b

2. **Out-of-distribution stress test**: Evaluate classifier performance on topics far outside training distribution (cultural facts, historical events) and multi-hop reasoning tasks

3. **Ablation on generation process**: Compare classifier accuracy on statements generated with different prompting strategies (chain-of-thought, step-by-step reasoning) to determine optimal output types