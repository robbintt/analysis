---
ver: rpa2
title: 'EELBERT: Tiny Models through Dynamic Embeddings'
arxiv_id: '2310.20144'
source_url: https://arxiv.org/abs/2310.20144
tags:
- embedding
- bert
- size
- embeddings
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces EELBERT, a method to compress transformer-based
  models (e.g., BERT) by replacing the input embedding layer with dynamic, on-the-fly
  embedding computations. This approach significantly reduces model size with minimal
  impact on downstream task accuracy.
---

# EELBERT: Tiny Models through Dynamic Embeddings

## Quick Facts
- **arXiv ID**: 2310.20144
- **Source URL**: https://arxiv.org/abs/2310.20144
- **Reference count**: 13
- **Primary result**: Achieves 15x model size reduction (1.2 MB) with only 4% GLUE score regression vs. BERT-tiny

## Executive Summary
EELBERT introduces a novel approach to compressing transformer-based models by replacing the static input embedding layer with dynamic, on-the-fly computed embeddings. This method eliminates the large embedding table (size d × V) that typically accounts for a significant portion of model parameters, particularly in smaller BERT variants. The approach achieves substantial size reduction while maintaining downstream task performance, making it particularly effective for edge deployment scenarios where memory constraints are critical.

## Method Summary
The EELBERT method replaces the traditional trainable embedding lookup table with a dynamic computation that generates embeddings at runtime using a hash-based function. Specifically, the approach uses n-gram pooling with a rolling hash function to process input tokens, extracting morphological features that are then projected into the embedding space. The output embedding layer is decoupled from the input layer, allowing the model to maintain standard BERT architecture for the encoder while only modifying how input embeddings are generated. The method includes a pre-training phase on OpenWebText for 900k steps, followed by fine-tuning on GLUE tasks.

## Key Results
- EELBERT-tiny achieves GLUE score within 4% of fully trained BERT-tiny while being 15x smaller (1.2 MB)
- EELBERT-base reduces parameter count by 21% with only 1.5% regression on GLUE score
- For BERT-mini, hash-initialized embeddings provide 1.6% absolute increase in GLUE score versus random initialization
- The n-gram pooling hash function preserves morphological similarity, benefiting downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing the trainable input embedding layer with a dynamic, on-the-fly computation reduces model size by eliminating the large embedding table.
- Mechanism: The input embedding layer stores a matrix of size d × V (embedding dimension times vocabulary size), which is not trainable and thus cannot be compressed by standard methods. By computing embeddings dynamically using a hash-based function at runtime, the model avoids storing this large matrix entirely.
- Core assumption: The dynamic embeddings can sufficiently approximate the quality of learned embeddings for downstream tasks, even without updating during training.
- Evidence anchors: [abstract] "replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations" and "we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size." [section] "21% of the trainable parameters in BERT-base are in the embedding lookup layer. By replacing this input embedding layer with embeddings computed at run-time, we can reduce model size by the same percentage."

### Mechanism 2
- Claim: The n-gram pooling hash function generates embeddings that preserve morphological similarity between tokens.
- Mechanism: The hash function processes n-grams of the token and combines them into a final embedding vector. Tokens with similar character n-grams (e.g., "running" and "runner") will produce similar intermediate hash values, leading to more similar final embeddings.
- Core assumption: Morphological similarity correlates with semantic similarity for the downstream tasks considered.
- Evidence anchors: [section] "The hash function we used pools the n-gram features of a word to generate its embedding, so words with similar morphology, like 'running' and 'runner', will result in similar embeddings." [section] "We investigate whether our particular choice of hash function plays an important role... or whether a completely random hash function which preserves no morphological information would yield similar results."

### Mechanism 3
- Claim: Dynamic embeddings serve as an effective initializer for the trainable embedding layer, improving downstream performance.
- Mechanism: Initializing the embedding layer with hash-based embeddings (rather than random normal distribution) provides a better starting point for training, leading to improved final performance.
- Core assumption: The structure in hash-based embeddings captures useful prior information about the token space.
- Evidence anchors: [section] "The hash-initialized model shows a 0.5% absolute increase in GLUE score compared to the randomly-initialized model." [section] "for BERT-mini, the hash-initialized model had an absolute increase of 1.6% in overall GLUE score."

## Foundational Learning

- Concept: Transformer architecture and role of embedding layers
  - Why needed here: Understanding how embeddings fit into the overall model architecture is crucial for grasping why replacing them affects model size and performance.
  - Quick check question: What percentage of parameters in BERT-base are in the input embedding layer according to the paper?

- Concept: Hash functions and rolling hash algorithms
  - Why needed here: The dynamic embedding computation relies on hashing n-grams, so understanding hash function properties is essential for implementing and debugging the approach.
  - Quick check question: What is the time complexity of computing a dynamic embedding for a token with average word length l and embedding size d?

- Concept: Model compression tradeoffs (size vs. latency vs. quality)
  - Why needed here: EELBERT trades computation time for memory savings, so understanding these tradeoffs is critical for practical deployment decisions.
  - Quick check question: According to the paper, how much slower is EELBERT-tiny compared to BERT-tiny in terms of inference time?

## Architecture Onboarding

- Component map:
  - Tokenizer → n-gram extraction → rolling hash → projection matrix computation → embedding concatenation → standard BERT encoder
  - Note: Segment and position embeddings remain unchanged
  - Output embedding layer is decoupled and trainable (unlike standard BERT)

- Critical path:
  1. Tokenization of input text
  2. Dynamic embedding computation for each token
  3. Addition of segment and position embeddings
  4. Standard BERT encoder processing
  5. Output projection (via decoupled output embedding layer)

- Design tradeoffs:
  - Memory vs. computation: Saves significant memory by eliminating embedding table but increases computation time
  - Model size vs. quality: Achieves 15x size reduction with only 4% GLUE score regression for smallest model
  - Architecture compatibility: Works with existing BERT fine-tuning pipelines since only input embeddings are modified

- Failure signatures:
  - Severe degradation in downstream task performance (suggests hash function inadequacy)
  - Unexpected latency increases (indicates computational bottleneck in dynamic embedding generation)
  - Memory usage doesn't decrease as expected (suggests implementation error in replacing embedding table)

- First 3 experiments:
  1. Compare GLUE scores of EELBERT-base vs standard BERT-base to verify minimal regression claim
  2. Measure memory usage and inference latency of EELBERT-tiny vs BERT-tiny to quantify tradeoffs
  3. Test alternative hash functions (random vs n-gram pooling) on BERT-mini to validate importance of morphological preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance characteristics of dynamic embeddings scale with vocabulary size and embedding dimension, particularly for very large vocabularies?
- Basis in paper: [explicit] The paper discusses that EELBERT replaces the input embedding layer, which accounts for a significant fraction of the model size, especially for smaller BERT variants.
- Why unresolved: The paper primarily focuses on the compression benefits and minimal impact on accuracy but does not explore how dynamic embeddings perform with extremely large vocabularies or high-dimensional embeddings.
- What evidence would resolve it: Experiments testing EELBERT on models with significantly larger vocabularies and varying embedding dimensions, comparing performance and compression efficiency.

### Open Question 2
- Question: What are the long-term impacts of using dynamic embeddings on model generalization and robustness across diverse and unseen datasets?
- Basis in paper: [inferred] The paper evaluates EELBERT on the GLUE benchmark, showing minimal regression in model quality, but does not address long-term generalization.
- Why unresolved: The study focuses on immediate performance impacts without considering how dynamic embeddings affect model behavior over time or with new, unseen data.
- What evidence would resolve it: Longitudinal studies and cross-dataset evaluations to assess model robustness and generalization over time and across diverse domains.

### Open Question 3
- Question: How can architectural optimizations further reduce the latency introduced by dynamic embeddings without compromising model size benefits?
- Basis in paper: [explicit] The paper acknowledges increased latency due to dynamic embeddings and suggests potential optimizations like sparse masks and parallel computation.
- Why unresolved: While the paper proposes potential solutions, it does not empirically test these optimizations or explore other architectural changes that could mitigate latency.
- What evidence would resolve it: Implementation and evaluation of proposed optimizations, along with exploration of alternative architectures to balance latency and model size.

## Limitations
- The method increases computation time, particularly for smaller models where the 4x slowdown is most significant
- The evaluation focuses primarily on GLUE benchmark, which may not represent all downstream NLP tasks requiring semantic understanding
- No formal theoretical analysis explains why the n-gram pooling hash function produces embeddings suitable for downstream tasks

## Confidence

**High Confidence**: The core claim that replacing the input embedding layer with dynamic computations reduces model size is well-supported by empirical evidence (15x reduction for smallest model, 21% reduction for base model) and the mechanism is clearly explained.

**Medium Confidence**: The claim that dynamic embeddings provide effective initialization for the trainable embedding layer is supported by specific GLUE score improvements (0.5% for base, 1.6% for mini), but this represents a secondary benefit rather than the primary compression mechanism.

**Medium Confidence**: The claim that morphological similarity preservation through n-gram pooling is important for downstream task performance is supported by the ablation study comparing hash-initialized vs randomly-initialized models, but the theoretical justification remains limited.

## Next Checks

**Check 1: Cross-task Generalization**
Evaluate EELBERT on tasks beyond GLUE, particularly those requiring semantic understanding beyond morphological patterns (e.g., semantic textual similarity tasks, reading comprehension, or question answering). This would validate whether the n-gram pooling approach generalizes beyond GLUE's specific requirements.

**Check 2: Alternative Hash Function Analysis**
Systematically compare EELBERT with different hash functions (random, morphological, semantic-aware) on the same BERT architecture to isolate the contribution of the specific n-gram pooling approach versus the general concept of dynamic embeddings. This would clarify whether morphological preservation is truly essential or if simpler approaches could suffice.

**Check 3: Latency-Performance Tradeoff Characterization**
Measure and characterize the latency impact of EELBERT across different hardware platforms and batch sizes, particularly for the smallest models where the 4x slowdown is most significant. This would provide practical deployment guidance and identify scenarios where the size reduction might not justify the latency cost.