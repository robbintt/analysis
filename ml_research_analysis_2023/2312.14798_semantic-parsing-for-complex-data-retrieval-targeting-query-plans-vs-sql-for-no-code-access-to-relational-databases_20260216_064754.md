---
ver: rpa2
title: 'Semantic Parsing for Complex Data Retrieval: Targeting Query Plans vs. SQL
  for No-Code Access to Relational Databases'
arxiv_id: '2312.14798'
source_url: https://arxiv.org/abs/2312.14798
tags:
- language
- query
- queries
- complex
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling non-programmers
  to access and retrieve complex data from relational databases using natural language.
  The authors propose a novel intermediary query language called Query Plan Language
  (QPL), which is designed to be modular, compositional, and easier to learn for neural
  semantic parsing architectures compared to SQL.
---

# Semantic Parsing for Complex Data Retrieval: Targeting Query Plans vs. SQL for No-Code Access to Relational Databases

## Quick Facts
- arXiv ID: 2312.14798
- Source URL: https://arxiv.org/abs/2312.14798
- Reference count: 40
- Primary result: Direct QPL prediction using fine-tuned T5 models achieves 63.4% execution match accuracy on Spider dev set, outperforming state-of-the-art text-to-SQL systems on complex queries

## Executive Summary
This paper addresses the challenge of enabling non-programmers to access complex data from relational databases using natural language. The authors propose Query Plan Language (QPL), an intermediary compositional query language designed to be more learnable for neural semantic parsing architectures than SQL. QPL can be translated into restricted SQL Common Table Expressions (CTEs), making it executable while preserving interpretability. The approach decomposes complex questions into simpler sub-questions, maps them to QPL plans, and executes them to retrieve data. Experiments on a converted Spider dataset demonstrate that directly predicting QPL plans outperforms state-of-the-art text-to-SQL systems on complex queries, with 63.4% execution match accuracy on the development set.

## Method Summary
The method involves converting the Spider text-to-SQL dataset into QPL format by extracting SQL Server execution plans and deterministically translating them to CTEs. A T5-Large model is fine-tuned on this converted dataset with PICARD constrained decoding to ensure syntactically valid QPL generation. The approach also experiments with question decomposition strategies, including reverse parsing, LLM prompting, and a fine-tuned QMDR model, to handle complex queries by breaking them into simpler sub-questions that are then mapped to QPL plan nodes. The QPL plans are executed to retrieve results, and performance is measured by execution match accuracy against ground truth.

## Key Results
- Direct QPL prediction using fine-tuned T5-Large achieves 63.4% execution match accuracy on Spider dev set
- QPL outperforms state-of-the-art text-to-SQL systems on complex queries (depth 3+)
- Question decomposition strategies help handle compositional complexity, though optimal strategy selection remains unclear

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex natural language questions into simpler sub-questions improves neural semantic parsing performance on complex SQL queries.
- Mechanism: Breaking down a complex question into simpler questions allows the neural model to focus on predicting smaller, more manageable sub-plans rather than attempting to generate the entire complex plan in one step. This compositional approach aligns better with how neural architectures learn hierarchical structures.
- Core assumption: Neural semantic parsing architectures struggle more with compositional complexity than with individual simple operations.
- Evidence anchors:
  - [abstract]: "The paper demonstrates how neural LLMs can benefit from QPL’s modularity to generate complex query plans in a compositional manner. This involves a question decomposition strategy and a planning stage."
  - [section]: "Our approach is inspired by the thread of works attempting to solve complex QA and semantic parsing using a question decomposition strategy"
  - [corpus]: Found related papers on question decomposition for text-to-SQL, indicating this is a recognized approach, though corpus evidence for this specific mechanism is limited (avg_neighbor_fmr=0.517)
- Break condition: If the decomposition strategy produces sub-questions that are themselves too complex or require understanding the full context of the original question, the benefit disappears.

### Mechanism 2
- Claim: QPL's modular design, where each node is executable and returns a stream of tuples, provides a more learnable target language for neural models compared to SQL.
- Mechanism: SQL's non-compositional nature means parts of a query cannot be executed independently. QPL's tree structure ensures every sub-plan is a valid execution plan, making the learning task more aligned with neural architectures that benefit from compositional structures. Each QPL node corresponds to a single SQL CTE clause, providing clear mapping.
- Core assumption: Neural architectures learn compositional structures more effectively than non-compositional ones.
- Evidence anchors:
  - [abstract]: "QPL can be translated into a restricted form of SQL Common Table Expressions (CTEs), making it executable while preserving interpretability."
  - [section]: "Our strategy consists in defining an intermediary compositional query language, which is easier to learn using LLMs and easier to explain to non-expert users."
  - [corpus]: The paper compares QPL to SemQL and NatSQL as other simplified intermediary languages, suggesting QPL's unique modular execution property
- Break condition: If the translation from QPL to executable SQL introduces errors or if the execution semantics differ significantly from the original SQL intent.

### Mechanism 3
- Claim: Using PICARD constrained decoding with QPL syntax eliminates syntactically invalid query plans, improving overall accuracy.
- Mechanism: PICARD incrementally parses the generated output to ensure it adheres to the target language grammar. By applying this to QPL syntax, the system ensures all generated plans are syntactically valid, reducing the search space and improving the likelihood of generating semantically correct queries.
- Core assumption: Syntax constraints significantly reduce the search space for neural generation and improve accuracy.
- Evidence anchors:
  - [section]: "PICARD is an independent module on top of a text-to-text auto-regressive model that uses an incremental parser to constrain the generated output to adhere to the target SQL grammar."
  - [section]: "We have adopted it in our approach, by designing an incremental parser for QPL, and enforcing the generation of syntactically valid query plans."
  - [corpus]: The paper mentions that four entries in the Spider leaderboard's top-ten use PICARD, providing external validation
- Break condition: If the incremental parser for QPL has bugs or if the constraint mechanism is too restrictive and prevents valid but unusual plans from being generated.

## Foundational Learning

- Concept: SQL execution plans and Common Table Expressions (CTEs)
  - Why needed here: Understanding how SQL queries are executed internally and how CTEs provide modular composition is essential for grasping why QPL's design is effective and how it translates to executable SQL.
  - Quick check question: Can you explain how a SQL query optimizer breaks down a query into an execution plan, and how CTEs enable modular composition of query parts?

- Concept: Neural semantic parsing and compositional generalization
  - Why needed here: The paper's approach relies on the ability of neural models to learn compositional structures and generalize to unseen combinations of known operations. Understanding the limitations of current models on compositional tasks is crucial.
  - Quick check question: What is compositional generalization in semantic parsing, and why do standard sequence-to-sequence models struggle with it?

- Concept: Question decomposition strategies in NLP
- Why needed here: The paper employs question decomposition as a core technique. Understanding existing approaches like chain-of-thought prompting and how they decompose complex reasoning tasks is essential for extending this work.
  - Quick check question: What are the key differences between various question decomposition approaches like chain-of-thought prompting and the method proposed in this paper?

## Architecture Onboarding

- Component map: Spider dataset → SQL → Execution Plan → QPL → CTE → SQL Execution → Results
- Critical path: Schema + Question → QPL Plan → CTE → SQL Execution → Results
  The most critical sequence is converting the natural language question into a valid QPL plan that can be translated into executable SQL.
- Design tradeoffs:
  - QPL vs SQL as target language: QPL is more learnable but requires conversion to SQL; SQL is more direct but harder for models to learn
  - Question decomposition vs direct prediction: Decomposition improves accuracy on complex queries but adds complexity to the system
  - PICARD constraints: Ensure validity but may restrict creative solutions
- Failure signatures:
  - Invalid QPL plans that cannot be converted to CTEs
  - Execution results that don't match expected results
  - Question decomposition that produces sub-questions too complex for the model to handle
  - Over-constrained PICARD parser rejecting valid plans
- First 3 experiments:
  1. Fine-tune T5-Large on direct QPL prediction with PICARD constrained decoding and measure execution accuracy on Spider dev set
  2. Implement question decomposition pipeline and fine-tune separate model for decomposing complex questions
  3. Compare direct QPL prediction vs. question decomposition + planning approach on complex queries from Spider dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of QPL compare to other intermediate representations like SemQL and NatSQL on complex queries?
- Basis in paper: [explicit] The paper mentions that QPL has high coverage over 99% of Spider's SQL queries and can be converted to modular executable SQL programs with CTEs. It also states that QPL prediction performs better on highly complex query plans than state-of-the-art text-to-SQL systems.
- Why unresolved: The paper does not provide a direct comparison of QPL's performance with other intermediate representations on complex queries.
- What evidence would resolve it: Conducting experiments comparing QPL with SemQL and NatSQL on complex queries and reporting the results would provide a clear answer.

### Open Question 2
- Question: What is the impact of question decomposition strategies on the accuracy of QPL plan generation?
- Basis in paper: [explicit] The paper mentions that they experiment with different question decomposition (QD) strategies, including fine-tuned QD models and LLM-based CoT approaches, and investigate how to exploit the modularity of QPL plans by decomposing original questions into high-level plans.
- Why unresolved: The paper does not provide a detailed analysis of how different question decomposition strategies affect the accuracy of QPL plan generation.
- What evidence would resolve it: Conducting experiments comparing the accuracy of QPL plan generation using different question decomposition strategies and reporting the results would provide a clear answer.

### Open Question 3
- Question: How does the performance of QPL change when dealing with more complex database schemas?
- Basis in paper: [explicit] The paper mentions that the Spider dataset encompasses multiple database schemas with complex queries, and it is employed to assess the generalization capabilities of text-to-SQL models to unseen schemas on complex queries.
- Why unresolved: The paper does not provide a detailed analysis of how QPL's performance changes when dealing with more complex database schemas.
- What evidence would resolve it: Conducting experiments on QPL's performance with increasingly complex database schemas and reporting the results would provide a clear answer.

## Limitations
- Dataset conversion reliability: The conversion process from SQL to QPL fails for approximately 8% of samples, raising concerns about dataset coverage and potential biases.
- Question decomposition quality: The paper employs multiple decomposition strategies but doesn't provide systematic evaluation of which approach works best for which types of questions.
- Execution match as sole metric: This metric doesn't capture semantic equivalence when multiple queries produce the same results, potentially over-penalizing semantically equivalent but syntactically different queries.

## Confidence

- High confidence: The core claim that QPL's compositional structure is more learnable for neural models than SQL's non-compositional structure is well-supported by the execution plan analysis and successful conversion process for the majority of queries.
- Medium confidence: The claim that question decomposition significantly improves performance on complex queries is supported by the 63.4% accuracy result, but the ablation study comparing direct vs. decomposed approaches is not fully detailed.
- Low confidence: The assertion that PICARD constrained decoding is essential for achieving high accuracy, as the paper doesn't provide controlled experiments showing performance degradation without these constraints.

## Next Checks

1. **Dataset integrity validation**: Systematically analyze the 8% of samples that fail deterministic CTE conversion to determine whether they represent edge cases, genuinely complex queries, or conversion flaws, and assess impact on overall performance metrics.

2. **Decomposition strategy ablation**: Conduct controlled experiments comparing direct QPL prediction vs. question decomposition approaches across different query complexity levels to quantify the exact contribution of decomposition to the 63.4% accuracy result.

3. **Semantic equivalence analysis**: Beyond execution match, implement semantic similarity metrics to evaluate whether syntactically different but semantically equivalent queries are being unfairly penalized in the current evaluation framework.