---
ver: rpa2
title: A Text-to-Text Model for Multilingual Offensive Language Identification
arxiv_id: '2312.03379'
source_url: https://arxiv.org/abs/2312.03379
tags:
- offensive
- language
- identification
- datasets
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FT5 and mFT5, two pre-trained transformer
  models for offensive language identification that use text-to-text transfer learning.
  FT5 is trained on English datasets and outperforms existing models like fBERT and
  HateBERT on various English benchmarks.
---

# A Text-to-Text Model for Multilingual Offensive Language Identification

## Quick Facts
- arXiv ID: 2312.03379
- Source URL: https://arxiv.org/abs/2312.03379
- Reference count: 11
- Primary result: FT5 and mFT5 models achieve state-of-the-art performance on English and multilingual offensive language identification benchmarks

## Executive Summary
This paper introduces FT5 and mFT5, transformer-based text-to-text models for offensive language identification. FT5 is trained on English datasets and outperforms existing models like fBERT and HateBERT on various benchmarks. mFT5 extends this approach to six languages (German, Hindi, Korean, Marathi, Sinhala, and Spanish) and achieves state-of-the-art results across all of them. The text-to-text architecture enables training on multiple datasets with different label taxonomies without requiring label mapping, making these models more flexible than encoder-only alternatives.

## Method Summary
The authors retrain T5 (for English) and mT5 (for multilingual) using SOLID (9 million tweets) and CCTK (1.8 million posts) datasets. They apply semi-supervised filtering with STD thresholds (0.05, 0.1, 0.15, 0.2) to select high-confidence examples from SOLID. The models are evaluated using macro F1 score on benchmark datasets in English and six other languages. Training uses batch size 16 (8 for fine-tuning), Adam optimizer with learning rate 1e-4, and linear warm-up over 10% of training data.

## Key Results
- FT5 outperforms fBERT and HateBERT on English benchmarks including HASOC, TRAC, OffensEval, and SOLID
- mFT5 achieves state-of-the-art results across six languages: German, Hindi, Korean, Marathi, Sinhala, and Spanish
- Text-to-text architecture enables training on datasets with different label taxonomies without mapping to common labels
- Combining SOLID and CCTK datasets improves performance through increased data diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FT5 outperforms fBERT and HateBERT because its text-to-text architecture allows training on multiple datasets with different label taxonomies without mapping them to a common label
- Mechanism: The T5 encoder-decoder architecture processes offensive language detection as a sequence-to-sequence task, mapping input text to output labels as text tokens rather than fixed classification heads
- Core assumption: The label space in offensive language datasets is too diverse to be efficiently merged under a single classification layer without information loss
- Evidence anchors:
  - [abstract] "models are limited in their capabilities due to their encoder-only architecture, which restricts the number and types of labels in downstream tasks"
  - [section 2] "fBERT (Sarkar et al., 2021) and HateBERT (Caselli et al., 2021) have been trained with a masked language modelling (MLM) objective. ... However, it is not possible to concatenate two datasets annotated with different annotation taxonomies under this strategy without mapping them into a common label"

### Mechanism 2
- Claim: mFT5 achieves strong cross-lingual performance because multilingual T5 is pre-trained on a large corpus spanning multiple languages, allowing knowledge transfer across languages
- Mechanism: The shared subword vocabulary and multilingual pre-training in mT5 enable the model to generalize offensive language patterns across languages even with limited labeled data in target languages
- Core assumption: Offensive language patterns share enough semantic similarity across languages to be transferable via shared representations
- Evidence anchors:
  - [abstract] "mFT5 is the first multilingual model on offensive language opening exciting avenues for a multitude of languages"
  - [section 5] "The results confirm that fine-tuned mFT5 produces state-of-the-art results in six languages, outperforming strong transformer-based models"

### Mechanism 3
- Claim: Combining SOLID and CCTK datasets improves model performance by increasing training data volume and diversity of offensive language contexts
- Mechanism: The text-to-text training objective allows the model to learn from both tweet-style (SOLID) and forum-style (CCTK) data without architectural changes, capturing a broader distribution of offensive language
- Core assumption: Offensive language patterns in different domains are complementary rather than redundant for model training
- Evidence anchors:
  - [section 3] "We select the t5-large ... and train it using the instances from SOLID ... and CCTK"
  - [section 4.1] "the results show that the combination of CCTK and SOLID provides better results than having one dataset in the training set"

## Foundational Learning

- Concept: Encoder-decoder vs. encoder-only transformer architectures
  - Why needed here: Understanding why T5's text-to-text approach enables multi-dataset training while BERT-style models cannot
  - Quick check question: What is the key architectural difference between T5 and BERT that allows T5 to output arbitrary text labels?

- Concept: Semi-supervised learning with confidence thresholds
  - Why needed here: The study filters SOLID instances using mean and standard deviation thresholds to select high-confidence examples
  - Quick check question: How does adjusting the standard deviation threshold in semi-supervised data affect model performance?

- Concept: Cross-lingual transfer learning
  - Why needed here: mFT5's ability to perform well in zero-shot settings across multiple languages relies on cross-lingual transfer
  - Quick check question: What properties of multilingual pre-training enable zero-shot cross-lingual classification?

## Architecture Onboarding

- Component map: Input text → Tokenizer → Encoder → Decoder → Output label sequence
- Critical path: Input → Encoder → Decoder → Output label sequence
- Design tradeoffs:
  - T5 vs. BERT: More flexible label space vs. more parameter-efficient for single-task classification
  - Multilingual vs. monolingual: Broader applicability vs. potential performance loss on high-resource languages
  - Text-to-text vs. token classification: Better multi-task learning vs. higher computational cost
- Failure signatures:
  - Performance degradation when input length exceeds model's maximum context
  - Label ambiguity when multiple offensive terms appear in the same input
  - Cross-lingual performance drops when cultural context of offensive language differs significantly
- First 3 experiments:
  1. Fine-tune FT5 on OLID dataset and evaluate macro F1 score against baseline BERT models
  2. Train mFT5 with different STD thresholds on SOLID and evaluate cross-lingual performance on German dataset
  3. Test zero-shot cross-lingual offensive detection by training on English data and evaluating on Hindi without fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FT5 and mFT5 compare to other T5-based models when trained on additional offensive language datasets beyond SOLID and CCTK?
- Basis in paper: [inferred] The paper focuses on training FT5 and mFT5 on SOLID and CCTK datasets and evaluating their performance. However, it does not explore the impact of using additional or different datasets for training.
- Why unresolved: The paper does not provide a comparative analysis of FT5 and mFT5 performance when trained on different combinations of datasets.
- What evidence would resolve it: Conducting experiments where FT5 and mFT5 are trained on various combinations of offensive language datasets and comparing their performance to the current models.

### Open Question 2
- Question: Can the zero-shot cross-lingual capabilities of mFT5 be further improved by incorporating additional pre-training strategies or data augmentation techniques?
- Basis in paper: [explicit] The paper mentions that mFT5 shows competitive zero-shot cross-lingual results, but it does not explore methods to enhance these capabilities.
- Why unresolved: The paper does not investigate specific techniques to improve zero-shot performance in multilingual settings.
- What evidence would resolve it: Experimenting with different pre-training strategies, such as back-translation or data augmentation, to enhance mFT5's zero-shot cross-lingual performance and comparing the results.

### Open Question 3
- Question: How do FT5 and mFT5 perform in identifying offensive language in code-mixed texts, which are common in multilingual social media environments?
- Basis in paper: [inferred] The paper does not address the performance of FT5 and mFT5 in code-mixed text scenarios, which are increasingly prevalent in social media.
- Why unresolved: There is no analysis or experimentation on the models' effectiveness in handling code-mixed languages.
- What evidence would resolve it: Evaluating FT5 and mFT5 on datasets containing code-mixed texts and analyzing their performance compared to monolingual models.

### Open Question 4
- Question: What is the impact of using larger T5 models, such as T5-XL or T5-XXL, on the performance of FT5 and mFT5 in offensive language identification tasks?
- Basis in paper: [explicit] The paper mentions that larger T5 models were not used due to computational constraints, but it does not explore their potential impact on performance.
- Why unresolved: The paper does not provide insights into how larger models might affect the results in offensive language identification.
- What evidence would resolve it: Training and evaluating FT5 and mFT5 using larger T5 models and comparing their performance to the current implementations.

## Limitations

- Data selection bias: Heavy reliance on SOLID and CCTK datasets without detailed filtering criteria raises questions about generalizability to other offensive language contexts
- Limited evaluation scope: Primary focus on binary classification with limited analysis of fine-grained distinctions like hate speech versus general offensive content
- Architectural constraints: Computational cost of encoder-decoder architecture not discussed, and performance degradation under sequence length limits not empirically analyzed

## Confidence

- FT5 outperforming fBERT and HateBERT (High confidence): Well-supported with benchmark results across multiple English datasets
- mFT5 achieving state-of-the-art results across six languages (Medium confidence): Strong performance reported but some evaluation datasets not publicly available for independent verification
- Text-to-text architecture enabling multi-dataset training (High confidence): Clearly explained mechanism with sufficient theoretical and empirical support

## Next Checks

1. **Cross-lingual robustness testing**: Evaluate mFT5's performance on languages outside the training set (e.g., French, Portuguese, Arabic) to assess true generalization capability. Test both zero-shot and few-shot scenarios with varying amounts of labeled data.

2. **Fine-grained classification capability**: Extend evaluation beyond binary classification to assess model's ability to distinguish between different types of offensive content. Use established fine-grained datasets like HateXplain or construct new benchmarks for the six target languages.

3. **Failure mode analysis**: Systematically test model performance under conditions that could cause degradation: input sequences exceeding context window limits, inputs containing multiple offensive terms, and inputs with code-switching between languages. Document performance drops and analyze error patterns to understand practical deployment limitations.