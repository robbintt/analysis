---
ver: rpa2
title: Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond
arxiv_id: '2310.14670'
source_url: https://arxiv.org/abs/2310.14670
tags:
- visual
- answer
- question
- image
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two dataset biases in long-form multiple-choice
  visual question answering: Unbalanced Matching bias, where correct answers have
  more n-gram overlap with question and image than incorrect answers, and Distractor
  Similarity bias, where incorrect answers are overly dissimilar to correct answers
  but similar to each other. To mitigate these biases, the authors propose Adversarial
  Data Synthesis (ADS) to generate synthetic factual and counterfactual training data,
  and Intra-sample Counterfactual Training (ICT) to help models focus on intra-sample
  differentiation using the synthesized data.'
---

# Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond

## Quick Facts
- arXiv ID: 2310.14670
- Source URL: https://arxiv.org/abs/2310.14670
- Reference count: 40
- Primary result: ADS-ICT improves model performance across multiple benchmarks, with UNITERL+ADS-ICT achieving 78.23% Q2A accuracy on VCR (up from 76.72%)

## Executive Summary
This paper addresses dataset biases in long-form multiple-choice visual question answering tasks, specifically identifying Unbalanced Matching bias (correct answers have more n-gram overlap with question and image than incorrect answers) and Distractor Similarity bias (incorrect answers are overly dissimilar to correct answers but similar to each other). The authors propose Adversarial Data Synthesis (ADS) to generate synthetic factual and counterfactual training data, combined with Intra-sample Counterfactual Training (ICT) to help models focus on intra-sample differentiation. Extensive experiments show consistent performance improvements across different benchmarks including VCR, SNLI-VE, and domain-shifted scenarios.

## Method Summary
The method involves two main components: ADS generates synthetic factual and counterfactual text data using Multimodal Distractor Generation and Distractor Refinement (ADS-T), plus synthetic images using Coarse-to-Fine Region Removal (ADS-I); ICT then exploits this synthesized data during training using contrastive learning objectives (InfoNCE) combined with cross-entropy loss. The framework integrates with base vision-language models like UNITERL and VL-BERTL, using synthetic data to break existing bias patterns and force models to rely on actual visual and question understanding rather than shortcut learning.

## Key Results
- UNITERL+ADS-ICT improves VCR Q2A accuracy from 76.72% to 78.23%
- SNLI-VE accuracy improves from 79.02% to 80.14% with the proposed method
- Consistent improvements observed across domain-shifted scenarios
- Base models with ADS-ICT components show reduced bias exploitation compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unbalanced Matching (UM) bias occurs because correct answers have more n-gram overlap with question and image than incorrect answers, creating a shortcut for models.
- Mechanism: Models learn to match n-grams between answer candidates and premise information rather than understanding visual content, exploiting the statistical imbalance in dataset construction.
- Core assumption: Dataset creation processes naturally produce more n-gram overlap between correct answers and premises than between incorrect answers and premises.
- Evidence anchors:
  - [abstract]: "the correct answer overlaps the question and image more than the incorrect answers"
  - [section 3.1]: "Ct_c can be as high as 66.29%, which is much higher than the percentage of distractors, Ct_d, at 29.16%"

### Mechanism 2
- Claim: Distractor Similarity (DS) bias occurs because incorrect answers are overly dissimilar to correct answers but similar to each other within samples.
- Mechanism: Models can identify correct answers without considering question and image content by recognizing patterns where correct answers stand out as dissimilar from other options.
- Core assumption: Distractor generation processes emphasize dissimilarity to correct answers more than relevance to context, creating artificial separation.
- Evidence anchors:
  - [abstract]: "incorrect answers are overly dissimilar to correct answers but significantly similar to other incorrect answers within the same sample"
  - [section 3.2]: "models to attain the ground-truth labels without visual and question inputs" and "average intra-sample similarity score among distractors within the same sample is 0.36"

### Mechanism 3
- Claim: Adversarial Data Synthesis (ADS) mitigates biases by generating synthetic factual and counterfactual data that breaks existing n-gram matching patterns and distractor similarity structures.
- Mechanism: ADS-T generates more diverse answer candidates with balanced n-gram distributions, while ADS-I generates images with minimal artifacts that force models to use visual information rather than shortcuts.
- Core assumption: Synthetic data can effectively break existing bias patterns without introducing new artifacts that models can exploit.
- Evidence anchors:
  - [abstract]: "ADS generates synthetic factual and counterfactual text data using ADS-T and synthesizes images using ADS-I"
  - [section 4.1]: "ADS-I generates images that closely resemble real images with minimal disturbance to the data"

## Foundational Learning

- Concept: Cross-modal matching in VQA tasks
  - Why needed here: Understanding how models match text and visual information is crucial for recognizing why n-gram matching becomes a shortcut
  - Quick check question: Can you explain how a VQA model determines which answer is correct given an image and question?

- Concept: Counterfactual reasoning in machine learning
  - Why needed here: ADS relies on generating counterfactual examples to break bias patterns, requiring understanding of how counterfactual examples affect model training
  - Quick check question: What is the difference between factual and counterfactual training examples in the context of bias mitigation?

- Concept: Contrastive learning and intra-sample differentiation
  - Why needed here: ICT uses contrastive learning to force models to focus on differences between answer options within the same sample
  - Quick check question: How does contrastive learning help models distinguish between similar answer choices?

## Architecture Onboarding

- Component map: Data generation pipeline (ADS-T → ADS-I) → Training framework (XE + ICT losses) → Base VL models (UNITERL, VL-BERTL)
- Critical path: Data generation → Model training with ICT → Debiased evaluation
- Design tradeoffs: Synthetic data quality vs. training time; coarse-to-fine refinement vs. computational cost
- Failure signatures: Models still exploit n-gram matching patterns; synthetic data introduces new artifacts; ICT fails to improve visual explainability
- First 3 experiments:
  1. Train base model without any debiasing to establish baseline performance
  2. Apply ADS-T only to verify improvement from synthetic text data
  3. Apply ADS-I only to verify improvement from synthetic images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we further improve the quality of synthetic images generated by ADS-I to reduce noise and artifacts while maintaining realistic appearance?
- Basis in paper: [explicit] The paper mentions that some noise persists in the synthetic images generated by ADS-I, which might impact the accuracy of determining the relevant region.
- Why unresolved: The paper does not provide a concrete solution to address the noise issue in synthetic images. It only mentions the potential of enhancing image quality by addressing these noise issues as a promising next step.
- What evidence would resolve it: Experimental results showing improved image quality and reduced noise in synthetic images generated by an enhanced ADS-I method, leading to better model performance on VQA tasks.

### Open Question 2
- Question: How can we develop a cost-efficient yet reliable pre-selection procedure to reduce the cost of manual verification for the constructed debiased evaluation benchmarks?
- Basis in paper: [explicit] The paper mentions that manual verification was used to ensure the high quality of the constructed debiased evaluation benchmarks, which increased the overall cost of the research study.
- Why unresolved: The paper does not provide a concrete solution to develop a cost-efficient yet reliable pre-selection procedure. It only anticipates that such a procedure could be developed to mitigate these costs.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of a cost-efficient pre-selection procedure in maintaining the quality of debiased evaluation benchmarks while significantly reducing the cost compared to manual verification.

### Open Question 3
- Question: How can we improve the diversity of distractors in VQA-Long tasks to mitigate the Answer-only Bias problem?
- Basis in paper: [explicit] The paper identifies the Answer-only Bias problem, where distractors are overly dissimilar to the correct answer but significantly similar to other incorrect answers within the same sample. It also mentions that distractors are often generated without sufficient visual premise as the correct answers, leading to limited diversity.
- Why unresolved: The paper does not provide a concrete solution to improve the diversity of distractors in VQA-Long tasks. It only mentions the importance of addressing this issue to mitigate the Answer-only Bias problem.
- What evidence would resolve it: Experimental results showing improved model performance on VQA-Long tasks when using a method that generates more diverse distractors, leading to reduced Answer-only Bias and better generalization to real-world scenarios.

## Limitations
- Limited empirical validation across diverse datasets beyond tested benchmarks
- Potential overfitting to specific bias patterns without comprehensive ablation studies
- Claims about generalizability to broader VQA tasks require further validation

## Confidence
- **High**: Identification of Unbalanced Matching and Distractor Similarity biases is well-supported by quantitative evidence
- **Medium**: Effectiveness of ADS-ICT in mitigating biases across multiple benchmarks, though results show consistent improvements
- **Low**: Claims about generalizability to broader VQA tasks beyond the tested benchmarks

## Next Checks
1. Conduct comprehensive ablation studies to isolate the contribution of ADS-T vs ADS-I components and determine which contributes more to bias mitigation
2. Test the framework on additional long-form MCQ-VQA datasets with different construction methodologies to validate cross-dataset generalization
3. Perform controlled experiments with synthetic datasets where bias patterns are artificially introduced and then mitigated to establish causal relationships between methods and improvements