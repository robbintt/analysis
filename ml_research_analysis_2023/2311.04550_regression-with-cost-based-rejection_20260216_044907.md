---
ver: rpa2
title: Regression with Cost-based Rejection
arxiv_id: '2311.04550'
source_url: https://arxiv.org/abs/2311.04550
tags:
- loss
- rejection
- regression
- optimal
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the novel regression problem called regression
  with cost-based rejection (RcR), where the model can reject to make predictions
  on some examples given certain rejection costs. The authors first formulate the
  expected risk for this problem and derive the Bayes optimal solution, which shows
  that the optimal model should reject to make predictions on the examples whose variance
  is larger than the rejection cost when the mean squared error is used as the evaluation
  metric.
---

# Regression with Cost-based Rejection

## Quick Facts
- arXiv ID: 2311.04550
- Source URL: https://arxiv.org/abs/2311.04550
- Reference count: 40
- Key outcome: Proposes regression with cost-based rejection where optimal model rejects when instance variance exceeds rejection cost

## Executive Summary
This paper introduces regression with cost-based rejection (RcR), a novel problem where regression models can choose to reject predictions on certain instances based on rejection costs. The authors derive the Bayes optimal solution, showing that instances should be rejected when their label variance exceeds the rejection cost. They propose a surrogate loss function that treats rejection as binary classification, enabling gradient-based optimization while maintaining theoretical consistency guarantees. Extensive experiments on multiple datasets demonstrate that their method effectively balances prediction accuracy with rejection costs.

## Method Summary
The authors formulate RcR as minimizing expected risk under mean squared error loss with rejection costs. They derive the Bayes optimal solution: reject instances where label variance exceeds the rejection cost. To solve this, they propose a surrogate loss that converts the rejection decision into binary classification (accept vs reject), enabling gradient-based optimization. The method employs Slow-Start training, where the regressor is trained first without the rejector, then both are trained jointly. The framework is evaluated on BreastPathQ, AgeDB, and five UCI datasets using RcRLoss, AL, RL, AR, and RA metrics.

## Key Results
- The proposed surrogate loss effectively balances prediction accuracy and rejection costs
- Slow-Start training prevents gradient vanishing by ensuring the regressor learns before the rejector converges
- The method outperforms standard supervised regression across all tested datasets and rejection cost regimes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The optimal rejector uses a threshold based on variance exceeding the rejection cost.
- **Mechanism:** The Bayes optimal solution for regression with cost-based rejection is to reject predictions when the instance's label variance exceeds the rejection cost. This is derived from minimizing expected risk under mean squared error loss.
- **Core assumption:** The variance of the label distribution for each instance is accessible or estimable.
- **Evidence anchors:**
  - [abstract] "optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost"
  - [section 3.1] "the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost"
  - [corpus] Weak - corpus papers discuss rejection options but don't directly address variance-based thresholds
- **Break condition:** If the variance cannot be estimated or is constant across instances, this mechanism fails.

### Mechanism 2
- **Claim:** Surrogate loss treats rejection as binary classification, enabling gradient-based optimization.
- **Mechanism:** By converting the rejection decision into a binary classification problem (accept vs reject), the non-convex indicator functions in the original loss become differentiable binary classification losses, allowing gradient-based optimization.
- **Core assumption:** The binary classification loss is classification-calibrated and always positive.
- **Evidence anchors:**
  - [section 3.2] "we propose a surrogate loss function to train the model that considers the rejection behavior as a binary classification"
  - [section 4.1] "we provide theoretical analyses to show that the Bayes optimal solution can be recovered by minimizing our surrogate loss"
  - [corpus] Weak - corpus papers mention classification with rejection but don't specifically address the binary classification formulation
- **Break condition:** If the binary classification loss is not classification-calibrated or can be zero, regressor-consistency may fail.

### Mechanism 3
- **Claim:** Slow-start training prevents gradient vanishing by ensuring the regressor is ready before the rejector converges.
- **Mechanism:** The regressor is trained first without the rejector, then both are trained together. This prevents the rejector from rejecting all instances before the regressor learns the label distribution.
- **Core assumption:** The regressor can learn meaningful predictions without the rejector initially.
- **Evidence anchors:**
  - [section 5.1] "we consider a possible scenario where the regressor h predicts any instance x with such a large error that ℓ(h(x), y) >> c (x)"
  - [section 5.1] "we can avoid such a situation by training the rejector after the regressor is ready, and we name such a method Slow-Start"
  - [corpus] Weak - corpus papers don't mention this specific training strategy
- **Break condition:** If the regressor cannot learn meaningful predictions without the rejector, this approach may not work.

## Foundational Learning

- **Concept: Mean Squared Error and Variance**
  - Why needed here: The Bayes optimal solution depends on comparing variance to rejection cost
  - Quick check question: If an instance has variance 10 and rejection cost 5, should the model reject it?
  - Answer: Yes, because variance (10) > rejection cost (5)

- **Concept: Classification Calibration**
  - Why needed here: Ensures the binary classification loss for rejection leads to optimal decisions
  - Quick check question: What property must the binary loss have to guarantee optimal rejection decisions?
  - Answer: It must be classification-calibrated

- **Concept: Rademacher Complexity**
  - Why needed here: Used to bound the estimation error of the learned model
  - Quick check question: What happens to Rademacher complexity as training data increases?
  - Answer: It decreases at rate O(1/√n), improving generalization

## Architecture Onboarding

- **Component map:**
  - Regressor (h) -> Predicts continuous values for accepted instances
  - Rejector (r) -> Binary classifier deciding whether to accept or reject
  - Surrogate loss -> Combines regressor loss with binary classification losses
  - Slow-start controller -> Manages training schedule

- **Critical path:**
  1. Train regressor without rejector (Slow-Start phase)
  2. Jointly train regressor and rejector using surrogate loss
  3. Evaluate using RcRLoss, AL, and RL metrics

- **Design tradeoffs:**
  - Joint vs separate training: Joint training is more efficient but requires careful initialization
  - Binary loss choice: Different losses (hinge, logistic, square) affect calibration properties
  - Cost function design: Pointwise vs constant costs affect model flexibility

- **Failure signatures:**
  - High false acceptance rate (RA): Rejector is too permissive
  - High false rejection rate (AR): Rejector is too conservative
  - Gradient vanishing: Rejector converges before regressor is ready

- **First 3 experiments:**
  1. Train with constant rejection cost on a simple UCI dataset
  2. Test different binary classification losses (hinge vs logistic)
  3. Implement Slow-Start and compare to simultaneous training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the regressor-consistency be achieved for all instances (not just correctly accepted ones) under the relaxed condition in Theorem 6?
- Basis in paper: Theorem 6 states regressor-consistency is only satisfied for correctly accepted instances when the binary loss is non-negative and classification-calibrated.
- Why unresolved: The theorem provides a relaxed condition but does not guarantee consistency for incorrectly rejected instances.
- What evidence would resolve it: Proving that the regressor remains consistent even for incorrectly rejected instances under the relaxed condition.

### Open Question 2
- Question: How does the choice of binary classification loss function affect the performance of regression with cost-based rejection?
- Basis in paper: The authors use various binary classification losses (MAE, hinge loss, logistic loss, square loss, and sigmoid) but do not provide a comprehensive comparison of their performance.
- Why unresolved: The paper does not discuss the impact of different loss functions on the model's ability to identify and reject difficult instances.
- What evidence would resolve it: Conducting experiments comparing the performance of different binary classification losses on various datasets.

### Open Question 3
- Question: Can the regression with cost-based rejection framework be extended to handle non-pointwise cost functions?
- Basis in paper: The theoretical analysis assumes pointwise cost functions, but the authors mention that the method can handle various pointwise cost functions in different application scenarios.
- Why unresolved: The paper does not explore the possibility of extending the framework to handle non-pointwise cost functions.
- What evidence would resolve it: Developing a method that can handle non-pointwise cost functions and demonstrating its effectiveness on relevant datasets.

## Limitations

- The theoretical analysis assumes access to label variance, which must be estimated from data in practice
- The Slow-Start training strategy lacks theoretical justification for why it improves over simultaneous training
- The binary classification formulation may introduce calibration issues that aren't fully explored

## Confidence

- **High confidence**: The variance-based rejection criterion and its derivation from expected risk minimization
- **Medium confidence**: The surrogate loss formulation and its theoretical properties (requires verification of classification calibration conditions)
- **Medium confidence**: The Slow-Start training effectiveness (empirical justification but limited theoretical backing)

## Next Checks

1. **Variance estimation robustness**: Test model performance when variance is estimated from small samples versus known variance
2. **Binary loss calibration**: Systematically compare different binary classification losses (hinge, logistic, square) for their calibration properties
3. **Training strategy ablation**: Compare Slow-Start against simultaneous training across multiple datasets and rejection cost regimes