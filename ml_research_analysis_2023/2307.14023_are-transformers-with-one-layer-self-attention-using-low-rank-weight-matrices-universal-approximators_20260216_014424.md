---
ver: rpa2
title: Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices
  Universal Approximators?
arxiv_id: '2307.14023'
source_url: https://arxiv.org/abs/2307.14023
tags:
- boltz
- input
- function
- neural
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the expressive power of Transformer models,
  specifically addressing the gap between theoretical analyses and practical implementations.
  Previous analyses required excessively deep layers for data memorization, primarily
  due to interpreting the softmax function as an approximation of the hardmax function.
---

# Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?

## Quick Facts
- arXiv ID: 2307.14023
- Source URL: https://arxiv.org/abs/2307.14023
- Reference count: 40
- One-layer Transformers with self-attention and low-rank weight matrices can approximate any continuous permutation-equivariant function on compact domains.

## Executive Summary
This paper addresses a fundamental gap in Transformer expressiveness theory by proving that single-layer Transformers with low-rank weight matrices are universal approximators for continuous permutation-equivariant functions. Previous analyses incorrectly interpreted softmax as a hardmax approximation, leading to overly pessimistic bounds requiring many layers for simple tasks. The authors establish that softmax functions as a Boltzmann operator, enabling perfect context capture with just one self-attention layer. This theoretical breakthrough has implications for understanding the minimal architectural requirements for Transformer models.

## Method Summary
The paper proves that one-layer Transformers with self-attention can achieve universal approximation through a three-step process: first, a feed-forward layer quantizes continuous inputs into discrete grid points; second, the self-attention layer with softmax and low-rank matrices creates unique context identifiers for each sequence; third, another feed-forward layer maps these identifiers to output values. The key insight is reinterpreting softmax as a Boltzmann operator that preserves distance between input sequences when properly projected via low-rank matrices.

## Key Results
- One layer of softmax-based self-attention with low-rank weight matrices can perfectly capture the context of an entire input sequence.
- Transformers with one self-attention layer and two feed-forward neural networks are universal approximators for continuous permutation equivariant functions on compact domains.
- The attention block with hardmax cannot serve as a contextual mapping, unlike softmax-based attention.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: One layer of softmax-based self-attention with low-rank weight matrices can perfectly capture the context of an entire input sequence.
- Mechanism: The softmax function is reinterpreted as a Boltzmann operator, which preserves distance between input sequences when properly projected via low-rank matrices. This allows the attention layer to act as a contextual mapping that assigns unique identifiers to each sequence.
- Core assumption: Input sequences are tokenwise separated and contain no duplicate tokens.
- Evidence anchors:
  - [abstract] "By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence."
  - [section 3.4] "we show that a contextual mapping can be constructed by using only 1 self-attention layer with the softmax function."
- Break condition: If input sequences contain duplicate tokens or are not sufficiently separated, the contextual mapping fails.

### Mechanism 2
- Claim: Hardmax-based attention cannot serve as a contextual mapping.
- Mechanism: Hardmax always selects the maximum value, which gets distracted by extreme values in the input, preventing it from capturing the full context of the sequence.
- Core assumption: Any non-zero vector input can be constructed to demonstrate the limitation.
- Evidence anchors:
  - [section 3.3] "the attention block with hardmax is not a contextual mapping."
  - [section 3.3] "no matter how many heads there are, one-layer self-attention with the hardmax function cannot distinguish input sequences."
- Break condition: If input values are constrained such that maximum selection is meaningful, hardmax might work.

### Mechanism 3
- Claim: Transformers with one self-attention layer are universal approximators for continuous permutation equivariant functions on compact domains.
- Mechanism: First, a feed-forward layer quantizes continuous inputs into discrete grid points. Then, the one-layer self-attention memorizes these discrete patterns via contextual mapping. Finally, another feed-forward layer reconstructs the function values.
- Core assumption: The function to be approximated is continuous and defined on a compact domain.
- Evidence anchors:
  - [abstract] "Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain."
  - [section 4.2] "Let FPE be the set of all permutation equivariant continuous functions... Then the following proposition holds: Proposition 1 (Transformers with one layer self-attention are universal approximators)."
- Break condition: If the function is not permutation equivariant or not defined on a compact domain.

## Foundational Learning

- Concept: Boltzmann operator properties
  - Why needed here: The proof relies on showing the Boltzmann operator preserves distance between input sequences when properly projected.
  - Quick check question: What is the mathematical definition of the Boltzmann operator and how does it relate to softmax?

- Concept: Contextual mapping
  - Why needed here: The paper defines contextual mapping as the ability to assign unique identifiers to input sequences, which is crucial for memorization.
  - Quick check question: What are the two conditions that must hold for a function to be a contextual mapping?

- Concept: Permutation equivariance
  - Why needed here: The universal approximation theorem requires the function to be permutation equivariant.
  - Quick check question: What does it mean for a function to be permutation equivariant in the context of sequences?

## Architecture Onboarding

- Component map:
  Input quantization layer (feed-forward) -> Self-attention layer with softmax and low-rank weight matrices -> Output reconstruction layer (feed-forward)

- Critical path:
  1. Quantize continuous inputs into discrete grid points
  2. Compute attention weights using Boltzmann operator properties
  3. Apply attention mechanism to create context identifiers
  4. Map context identifiers to output values

- Design tradeoffs:
  - Using low-rank matrices reduces parameter count but may limit expressiveness
  - Single layer architecture improves efficiency but may struggle with very complex patterns
  - Requires tokenwise separated inputs for guaranteed performance

- Failure signatures:
  - Poor performance on inputs with duplicate tokens
  - Inability to separate similar but distinct sequences
  - Failure to approximate non-permutation-equivariant functions

- First 3 experiments:
  1. Test memorization capacity on synthetic data with known patterns and no duplicate tokens
  2. Evaluate performance on permutation equivariant functions with varying complexity
  3. Compare with multi-layer transformer architectures on the same tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Transformers without quantization layers achieve universal approximation?
- Basis in paper: [explicit] The authors state "Our proof of the universal approximation theorem requires one feed-forward neural network layer before the self-attention layer to quantize continuous inputs. We leave it as future work to clarify whether the one-layer Transformers without such a quantization layer are universal approximators or not."
- Why unresolved: The current proof relies on a quantization layer to discretize continuous inputs before applying the self-attention mechanism, but it's unclear if this quantization step is fundamentally necessary.
- What evidence would resolve it: A proof showing that one-layer Transformers without any quantization layer can approximate any continuous permutation equivariant function on a compact domain, or a counterexample demonstrating fundamental limitations of such architectures.

### Open Question 2
- Question: What is the exact relationship between the softmax function and the Boltzmann operator?
- Basis in paper: [explicit] "By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence."
- Why unresolved: While the paper demonstrates that the Boltzmann operator helps prove the contextual mapping property, the deeper mathematical relationship between softmax and Boltzmann remains unexplored.
- What evidence would resolve it: A rigorous mathematical characterization of how softmax functions approximate or relate to Boltzmann operators in the context of self-attention mechanisms, potentially revealing new theoretical insights about attention mechanisms.

### Open Question 3
- Question: How does the choice of weight matrix rank affect the expressive power of single-layer Transformers?
- Basis in paper: [explicit] The paper proves results using "low-rank weight matrices" but doesn't explore the impact of different rank values on performance.
- Why unresolved: The paper establishes that low-rank matrices suffice for universal approximation, but doesn't investigate whether higher ranks provide additional benefits or if there's a fundamental lower bound on rank requirements.
- What evidence would resolve it: Empirical and theoretical analysis comparing Transformer performance across different weight matrix ranks, identifying whether rank constraints limit expressiveness or if there's an optimal rank for practical applications.

## Limitations

- The theoretical claims rely heavily on idealized conditions, particularly the assumption that input sequences contain no duplicate tokens and are sufficiently separated.
- The paper does not address computational complexity or provide empirical validation of the theoretical claims.
- The low-rank constraint on weight matrices, while enabling the theoretical results, could limit the model's capacity to handle more complex real-world data distributions.

## Confidence

- **High confidence**: The theoretical framework connecting softmax to the Boltzmann operator and the proof that one-layer Transformers with low-rank matrices can achieve contextual mapping under specific conditions.
- **Medium confidence**: The universal approximation claim for permutation equivariant functions, as this depends on the validity of the quantization approach and the assumption that all necessary patterns can be captured in the discrete grid representation.
- **Low confidence**: The practical applicability of these findings to real-world scenarios where input sequences may contain duplicates, tokens may not be perfectly separated, and the low-rank constraint may be too restrictive.

## Next Checks

1. **Empirical validation of memorization capacity**: Test the one-layer Transformer on synthetic datasets with varying levels of token duplication and sequence similarity to quantify the degradation in performance as assumptions are violated. This would establish the practical limits of the theoretical framework.

2. **Comparison with standard Transformers**: Implement and evaluate both the proposed one-layer low-rank Transformer and standard multi-layer Transformers on permutation equivariant function approximation tasks. This would reveal whether the theoretical advantages translate to practical performance gains.

3. **Analysis of rank constraint impact**: Systematically vary the rank of weight matrices and measure the effect on both memorization capacity and approximation accuracy. This would help determine the practical trade-offs between model complexity and the theoretical guarantees provided.