---
ver: rpa2
title: On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation
  with Kolmogorov Superposition Theorem
arxiv_id: '2308.05509'
source_url: https://arxiv.org/abs/2308.05509
tags:
- relu
- functions
- dnns
- function
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the optimal expressive power of ReLU deep
  neural networks (DNNs) for representing continuous piecewise linear (CPwL) functions
  on [0,1] and its application in function approximation via the Kolmogorov Superposition
  Theorem. The authors constructively prove that any CPwL function with O(N^2L) segments
  on [0,1] can be exactly represented by a ReLU DNN with L hidden layers and N neurons
  per layer.
---

# On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem

## Quick Facts
- arXiv ID: 2308.05509
- Source URL: https://arxiv.org/abs/2308.05509
- Reference count: 40
- Primary result: Proves ReLU DNNs with L hidden layers and N neurons per layer can represent any CPwL function with O(N^2L) segments on [0,1], and shows this is optimal in parameter count

## Executive Summary
This paper investigates the optimal expressive power of ReLU deep neural networks for representing continuous piecewise linear (CPwL) functions on [0,1] and applies these results to high-dimensional function approximation via the Kolmogorov Superposition Theorem. The authors constructively prove that ReLU DNNs with L hidden layers and N neurons per layer can exactly represent any CPwL function with O(N^2L) segments. They show this representation is optimal in terms of parameter count by analyzing the shattering capacity of ReLU DNNs. By leveraging KST, the paper derives an improved approximation rate of O(N^(-2L-1)) for continuous functions in high dimensions, breaking the curse of dimensionality.

## Method Summary
The paper uses constructive proofs and theoretical analysis to establish the expressive power of ReLU DNNs. It begins by constructing a special class of ReLU NNs with one hidden layer to represent CPwL functions, then extends this to two hidden layers and finally to L hidden layers. Optimality is proven by analyzing the shattering capacity of ReLU DNNs, showing that O(N) parameters are necessary to represent all CPwL functions. The approximation result is derived by applying KST to decompose high-dimensional functions into univariate functions, which are then approximated using the CPwL representation results.

## Key Results
- ReLU DNNs with L hidden layers and N neurons per layer can represent any CPwL function with O(N^2L) segments on [0,1]
- This representation is optimal in terms of parameter count (O(N) parameters needed)
- Combined with KST, ReLU DNNs achieve O(N^(-2L-1)) approximation rate for continuous functions in high-dimensional spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReLU DNNs with L hidden layers and N neurons per layer can represent any CPwL function with O(N^2L) segments on [0,1].
- Mechanism: The proof constructs a two-hidden-layer network where each layer uses a specialized ΣN set of ReLU functions to interpolate segments, then generalizes to L layers using composition properties.
- Core assumption: The ΣN sets can uniquely interpolate any grid-based CPwL function through careful construction of intermediate activation patterns.
- Evidence anchors:
  - [abstract] states this is "constructively proved"
  - [section 3] provides detailed construction showing how φk(x) is built from ϕ±k and ϕk
  - [corpus] neighbors discuss similar universal approximation results for neural networks
- Break condition: If the shattering capacity argument fails (Theorem 4.1), the construction cannot guarantee exact representation for all CPwL functions.

### Mechanism 2
- Claim: The representation is optimal in terms of parameter count (O(N) parameters needed).
- Mechanism: Uses shattering capacity analysis - if fewer parameters were sufficient, the network couldn't shatter certain point sets, contradicting known bounds.
- Core assumption: Shattering capability is a necessary condition for representing all CPwL functions.
- Evidence anchors:
  - [section 4] explicitly proves Theorem 4.2 using Theorem 4.1 about shattering
  - [corpus] neighbors include papers on VC-dimension bounds for neural networks
  - [abstract] claims this optimality is "achieved through investigating the shattering capacity"
- Break condition: If a more efficient representation method exists that doesn't rely on shattering, this optimality claim could be violated.

### Mechanism 3
- Claim: Combined with KST, ReLU DNNs achieve O(N^(-2L-1)) approximation rate for continuous functions in high dimensions.
- Mechanism: Uses KST to decompose high-dimensional functions into univariate functions, then applies the CPwL representation result to each component.
- Core assumption: The K-outer function g can be well-approximated by piecewise linear interpolation at sufficient resolution.
- Evidence anchors:
  - [abstract] states this "enhanced approximation rate" is achieved "by invoking the Kolmogorov Superposition Theorem"
  - [section 5] provides detailed error analysis in Theorem 5.2 and Corollary 5.1
  - [corpus] neighbors discuss KST applications in neural networks
- Break condition: If the K-outer function g has poor modulus of continuity, the approximation rate degrades significantly.

## Foundational Learning

- Concept: Continuous Piecewise Linear (CPwL) functions
  - Why needed here: The entire expressive power analysis is built on representing CPwL functions exactly
  - Quick check question: Can you construct a CPwL function with 3 segments on [0,1] and specify its breakpoints?

- Concept: Kolmogorov Superposition Theorem (KST)
  - Why needed here: KST enables dimensionality reduction from high-dimensional to univariate functions
  - Quick check question: What are the roles of K-inner functions (ϕk) and K-outer function (g) in the KST decomposition?

- Concept: Shattering capacity and VC-dimension
  - Why needed here: Used to prove the optimality of the parameter count for representing CPwL functions
  - Quick check question: Why does the ability to shatter point sets relate to the expressive power of neural networks?

## Architecture Onboarding

- Component map: Input layer → Hidden layers (ReLU activation) → Output layer (weighted sum)
- Critical path: The construction of φk(x) from ϕ±k and ϕk functions in the two-hidden-layer case is the core innovation that enables exact CPwL representation.
- Design tradeoffs:
  - Width vs depth: The result shows N^2L segments can be represented with N neurons per layer and L layers
  - Exact vs approximate: Exact representation requires careful construction, while approximation might use simpler architectures
  - Parameter efficiency: The optimality proof shows O(N) parameters are necessary, not just sufficient
- Failure signatures:
  - Incorrect interpolation at grid points indicates construction error in the ΣN sets
  - Poor approximation rates suggest KST decomposition isn't capturing function structure well
  - Shattering capacity violations indicate parameter count is insufficient
- First 3 experiments:
  1. Verify exact representation of simple CPwL functions (2-3 segments) using the construction method
  2. Test approximation rates on known K-Lipschitz continuous functions from the KC class
  3. Validate shattering capacity by checking network can fit random sign patterns on well-separated points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the constructive proof of ReLU DNN expressivity in Theorem 3.1 be extended from one-dimensional to high-dimensional spaces?
- Basis in paper: [inferred] The paper focuses on one-dimensional space [0,1] but notes extending to higher dimensions would be significant.
- Why unresolved: The current proof relies on properties specific to one-dimensional space. Extending to higher dimensions requires new techniques to handle increased complexity.
- What evidence would resolve it: A constructive proof demonstrating ReLU DNNs of arbitrary depth and width can represent any CPwL function with O(P) parameters in d-dimensional space.

### Open Question 2
- Question: How extensive is the class of K-Lipschitz continuous functions defined in equation (5.1) and can it serve as a sufficient hypothesis space in machine learning?
- Basis in paper: [explicit] The paper defines KC and notes its potential as a hypothesis space but does not explore its properties or applicability.
- Why unresolved: The paper does not characterize the class KC or evaluate its practical utility for machine learning tasks.
- What evidence would resolve it: Analysis of the mathematical properties of KC and empirical evaluation of its performance as a hypothesis space for various machine learning problems.

### Open Question 3
- Question: How is the modulus of continuity of the K-outer function g in the Kolmogorov Superposition Theorem influenced by the original function f and the associated construction?
- Basis in paper: [explicit] The paper notes the non-uniqueness of g and its dependence on f but does not explore the relationship between their moduli of continuity.
- Why unresolved: The paper does not investigate how the choice of g affects the approximation properties of ReLU DNNs using KST.
- What evidence would resolve it: Bounds on the modulus of continuity of g in terms of the modulus of continuity of f and analysis of how this impacts the approximation rates achieved using ReLU DNNs with KST.

## Limitations
- Theoretical nature without experimental validation of claims on practical datasets
- Assumes K-outer function g has good modulus of continuity, which may not hold for all continuous functions
- Optimality proof relies on shattering capacity being a necessary condition, assuming no more efficient representation methods exist

## Confidence
- High confidence: The construction of ReLU DNNs representing CPwL functions with O(N^2L) segments is well-supported by the detailed proof in Section 3.
- Medium confidence: The optimality claim regarding parameter count depends on the correctness of the shattering capacity analysis in Theorem 4.2.
- Medium confidence: The approximation rate O(N^(-2L-1)) for high-dimensional functions relies on the KST decomposition working well in practice, which may vary depending on the specific function class.

## Next Checks
1. Implement the construction from Section 3 to verify exact representation of CPwL functions with 3-5 segments on [0,1], checking interpolation accuracy at all grid points.

2. Test the approximation rates by applying the KST-based approach to known continuous functions (e.g., from the KC class) and measuring empirical error against the theoretical O(N^(-2L-1)) bound.

3. Validate the shattering capacity claim by constructing ReLU DNNs with varying parameter counts and testing their ability to fit random sign patterns on carefully spaced point sets.