---
ver: rpa2
title: Can Large Language Models Capture Dissenting Human Voices?
arxiv_id: '2305.13788'
source_url: https://arxiv.org/abs/2305.13788
tags:
- human
- llms
- language
- distribution
- disagreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models (LLMs) are evaluated on their ability to
  perform natural language inference (NLI) tasks and capture human disagreement distributions.
  Two methods are introduced to reconstruct the model''s probability distribution:
  Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR).'
---

# Can Large Language Models Capture Dissenting Human Voices?

## Quick Facts
- arXiv ID: 2305.13788
- Source URL: https://arxiv.org/abs/2305.13788
- Reference count: 17
- Primary result: LLMs perform poorly on NLI tasks and fail to capture human disagreement distributions, with larger models showing lower performance

## Executive Summary
This paper evaluates large language models (LLMs) on their ability to perform natural language inference (NLI) tasks and capture human disagreement distributions. The authors introduce two methods to reconstruct LLM probability distributions: Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR). Experiments on ChaosNLI and PK2019 datasets reveal that LLMs struggle with NLI tasks and fail to align with human disagreement patterns, raising concerns about their natural language understanding capabilities and representativeness of human users.

## Method Summary
The paper evaluates LLMs on NLI tasks using two reconstruction methods to estimate their output probability distributions. MCR samples a large number of generated outputs to estimate probabilities, while LPR uses log probabilities of top-k token candidates. The reconstructed distributions are compared with human disagreement distributions using Jensen-Shannon Distance (JSD) and Distribution Calibration Error (DCE). The approach is tested on ChaosNLI and PK2019 datasets, which contain multiple human annotations per instance to capture disagreement patterns.

## Key Results
- LLMs perform poorly on NLI tasks, with accuracy below human performance
- Larger models do not necessarily perform better on NLI tasks or align better with human disagreement distributions
- LLMs fail to capture the nuances of human disagreement distributions in NLI tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can be evaluated on their alignment with human disagreement distributions by reconstructing their probability distributions using Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR).
- Mechanism: The paper introduces two methods to estimate the multinomial distribution of LLM outputs: MCR, which samples a large number of generated outputs to estimate probabilities, and LPR, which uses log probabilities of top-k token candidates to estimate the model probability for each class.
- Core assumption: Reconstructing the probability distribution of LLM outputs provides a valid representation of their alignment with human disagreement distributions.
- Evidence anchors:
  - [abstract] "Two methods are introduced to reconstruct the model's probability distribution: Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR)."
  - [section 3.1] "To reconstruct the distribution of outputs from generative LLMs, we introduce an intuitive way that samples a large number of generated outputs considering the valid options (vj) for class j."
- Break condition: If the reconstructed distributions do not accurately represent the true probability distributions of LLM outputs, the evaluation of alignment with human disagreement distributions would be invalid.

### Mechanism 2
- Claim: LLMs exhibit limited ability in solving NLI tasks and fail to capture human disagreement distribution.
- Mechanism: The paper evaluates LLMs on ChaosNLI and PK2019 datasets, measuring their performance on NLI tasks and their alignment with human disagreement distributions using Jensen-Shannon Distance (JSD) and Distribution Calibration Error (DCE).
- Core assumption: Evaluating LLMs on datasets with multiple human annotations per instance provides a valid measure of their alignment with human disagreement distributions.
- Evidence anchors:
  - [abstract] "Experiments on ChaosNLI and PK2019 datasets show that LLMs perform poorly on NLI tasks and fail to align with human disagreement distribution."
  - [section 4.4] "We investigate the distribution differences between humans and LLMs at the sample level with Jensen-Shannon Distance (JSD), which is a symmetric version of Kullback-Leibler (KL) divergence."
- Break condition: If the datasets used do not accurately represent the diversity of human opinions or if the evaluation metrics do not capture the nuances of human disagreement, the conclusion about LLMs' alignment with human disagreement distributions would be invalid.

### Mechanism 3
- Claim: Larger LLMs do not necessarily perform better on NLI tasks or align better with human disagreement distributions.
- Mechanism: The paper compares the performance and alignment of different LLM sizes on NLI tasks, finding that larger models do not consistently outperform smaller models.
- Core assumption: Model size is not the primary determinant of performance on NLI tasks or alignment with human disagreement distributions.
- Evidence anchors:
  - [abstract] "even larger models showing lower performance"
  - [section 5] "The NLI capability of LLMs does not only increase due to model size."
- Break condition: If other factors, such as model architecture or training data, are the primary determinants of performance on NLI tasks or alignment with human disagreement distributions, the conclusion about model size would be invalid.

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI is the task being evaluated in the paper, and understanding its fundamentals is crucial for interpreting the results.
  - Quick check question: What are the three main types of relationships that NLI models are typically trained to recognize?
    - Answer: Entailment, contradiction, and neutral.

- Concept: Probability distribution reconstruction
  - Why needed here: The paper introduces two methods for reconstructing the probability distributions of LLM outputs, which are essential for evaluating their alignment with human disagreement distributions.
  - Quick check question: What are the two methods introduced in the paper for reconstructing the probability distributions of LLM outputs?
    - Answer: Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR).

- Concept: Evaluation metrics for alignment with human disagreement
  - Why needed here: The paper uses Jensen-Shannon Distance (JSD) and Distribution Calibration Error (DCE) to measure the alignment of LLMs with human disagreement distributions.
  - Quick check question: What are the two metrics used in the paper to measure the alignment of LLMs with human disagreement distributions?
    - Answer: Jensen-Shannon Distance (JSD) and Distribution Calibration Error (DCE).

## Architecture Onboarding

- Component map:
  Datasets -> Preprocessing -> LLMs -> Reconstruction Methods (MCR/LPR) -> Evaluation Metrics (Accuracy, JSD, DCE) -> Results Comparison

- Critical path:
  1. Load the dataset and preprocess the data.
  2. Load the LLM and set up the reconstruction method (MCR or LPR).
  3. Generate outputs using the LLM and reconstruct the probability distribution.
  4. Calculate the evaluation metrics (accuracy, JSD, DCE).
  5. Compare the results across different models and datasets.

- Design tradeoffs:
  - MCR vs. LPR: MCR is more general but computationally expensive, while LPR is more efficient but limited to models that expose log probabilities.
  - Model size vs. performance: Larger models do not necessarily perform better on NLI tasks or align better with human disagreement distributions.

- Failure signatures:
  - Poor performance on NLI tasks: If the LLM consistently misclassifies the relationships between premise and hypothesis pairs.
  - High JSD or DCE values: If the reconstructed probability distribution of the LLM significantly deviates from the human disagreement distribution.

- First 3 experiments:
  1. Evaluate a small LLM (e.g., Flan-T5-L) on the ChaosNLI dataset using MCR and compare the results with a larger LLM (e.g., GPT-3.5-D3).
  2. Evaluate the same LLM on the PK2019 dataset using LPR and compare the results with the ChaosNLI dataset.
  3. Compare the performance and alignment of different LLM families (e.g., Flan-T5 vs. OPT-IML-Max) on both datasets using the same reconstruction method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What latent factors lead to disagreement in LLMs compared to humans, and how can these factors be identified and measured?
- Basis in paper: The paper suggests that future studies could try to find the latent factors that lead to disagreement in LLMs and compare them with those of humans.
- Why unresolved: The paper does not provide any specific methodology or evidence for identifying and measuring these latent factors.
- What evidence would resolve it: Research that develops a framework for identifying and quantifying the latent factors causing disagreement in LLMs, and compares these factors with those observed in human disagreement.

### Open Question 2
- Question: How can LLMs be improved to better reflect human disagreement distributions in natural language inference tasks?
- Basis in paper: The paper suggests that future LLMs could be improved to reflect human disagreements by fine-tuning with ambiguous instances.
- Why unresolved: The paper does not provide specific techniques or evidence on how to effectively fine-tune LLMs to capture human disagreement distributions.
- What evidence would resolve it: Studies that demonstrate successful methods for fine-tuning LLMs to better align with human disagreement distributions in NLI tasks.

### Open Question 3
- Question: How does the performance of LLMs on NLI tasks compare to other domain tasks, and what factors contribute to these differences?
- Basis in paper: The paper discusses the potential transferability of NLI tasks to other NLP applications and suggests that LLMs might show similar or worse performances on these tasks.
- Why unresolved: The paper does not provide comparative analysis or evidence on LLMs' performance across different NLP tasks.
- What evidence would resolve it: Research that systematically compares LLM performance on NLI tasks with their performance on other NLP tasks, identifying factors that contribute to differences in performance.

## Limitations
- The reconstruction methods may not accurately capture the true probability distributions of LLM outputs, potentially affecting the evaluation of alignment with human disagreement distributions.
- The paper relies on specific datasets that may not fully represent the diversity of human disagreement patterns across different domains.
- The study does not address potential prompt sensitivity, where different prompting strategies could yield different results.

## Confidence
- Medium confidence in the claim that LLMs perform poorly on NLI tasks: This is supported by the experimental results but could be influenced by dataset-specific factors or reconstruction method limitations.
- Low confidence in the claim that larger models show lower performance: This counterintuitive finding needs more extensive validation across different model families and prompting strategies.
- Medium confidence in the mechanism that reconstruction methods can capture human disagreement distributions: The methods are theoretically sound but require validation of their accuracy and potential biases.

## Next Checks
1. Reconstruct distributions using both MCR and LPR for the same model to validate consistency between methods and identify potential artifacts or biases in either approach.
2. Test the same LLMs with multiple prompt variations on both datasets to assess prompt sensitivity and determine whether the poor performance is consistent across different prompting strategies.
3. Evaluate models on an additional NLI dataset with human disagreement annotations to verify whether the observed patterns generalize beyond the specific datasets used in this study.