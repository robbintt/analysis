---
ver: rpa2
title: Multimodal Temporal Fusion Transformers Are Good Product Demand Forecasters
arxiv_id: '2307.02578'
source_url: https://arxiv.org/abs/2307.02578
tags:
- product
- demand
- forecasting
- multimodal
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multimodal product demand
  forecasting, where traditional methods struggle with the cold start problem and
  category dynamics. The authors propose a novel approach that integrates visual,
  textual, and geographical information with tabular data using a Multimodal Temporal
  Fusion Transformer (MTFT).
---

# Multimodal Temporal Fusion Transformers Are Good Product Demand Forecasters

## Quick Facts
- arXiv ID: 2307.02578
- Source URL: https://arxiv.org/abs/2307.02578
- Authors: 
- Reference count: 40
- Primary result: Multimodal Temporal Fusion Transformer (MTFT) outperforms tabular-only baseline in product demand forecasting using visual, textual, and geographical features.

## Executive Summary
This paper tackles the challenge of multimodal product demand forecasting by integrating visual, textual, and geographical information with traditional tabular data using a Multimodal Temporal Fusion Transformer (MTFT). The authors propose a novel approach that leverages pre-trained models (ResNet152, DistilBERT, BLIP) for feature extraction and Node2Vec for geographical embeddings, combined with a large Temporal Fusion Transformer. Experiments on a real-world dataset demonstrate significant improvements in Mean Absolute Error (MAE), Mean Signed Deviation (MSD), and Root Mean Squared Error (RMSE) compared to tabular-only baselines, particularly when using textual and geographical features with a large network size.

## Method Summary
The proposed MTFT architecture integrates multimodal features (visual, textual, geographical) with traditional tabular data for product demand forecasting. Feature extraction involves fine-tuning pre-trained models (ResNet152 for images, DistilBERT and BLIP for text) on demand forecasting tasks, and using Node2Vec to create geographical embeddings from warehouse-region graphs. These features are then fused with tabular data (historical demand, weather, seasonality, events) through Static Covariate Encoders and LSTM encoders within the Temporal Fusion Transformer. The model predicts demand quantiles for different warehouses, delivery periods, and products.

## Key Results
- MTFT outperforms tabular-only baseline across MAE, MSD, and RMSE metrics.
- Improvements are most pronounced when using textual and geographical features with a large network size.
- Ablation studies show that all multimodal features contribute to performance gains when combined with a sufficiently large network.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining multimodal embeddings with traditional tabular features in a large network yields better demand forecasting performance than using tabular features alone.
- Mechanism: Multimodal features capture product characteristics that are not represented in tabular data, and a sufficiently large network can effectively fuse these complementary signals to improve predictions.
- Core assumption: Visual, textual, and geographical features are informative for demand and the fusion network is large enough to learn their interactions.
- Evidence anchors:
  - [abstract]: "The multimodal pipeline presented in this work enhances the accuracy and reliability of the predictions, demonstrating the potential of leveraging multimodal information in product demand forecasting."
  - [section]: "In order to evaluate that hypothesis, a large and a small network size have been configured... As can be seen in Table 3 when using a smaller network size there are improvements in some metrics, however, these improvements were not consistent... In Table 4 it can be seen that with a larger network size, almost all the features in the ablation study improve performance."
  - [corpus]: Weak evidence; corpus contains related forecasting papers but no direct multimodal demand forecasting comparisons.
- Break condition: Multimodal features are noisy or irrelevant for a product category, or the fusion network is too small to capture complex interactions.

### Mechanism 2
- Claim: Fine-tuning pre-trained models on demand forecasting task yields effective low-dimensional embeddings that improve TFT performance.
- Mechanism: Pre-trained models have learned rich feature representations from large datasets; fine-tuning on demand forecasting adapts these representations to the specific task, and dimensionality reduction makes them compatible with TFT.
- Core assumption: The pre-trained models' learned representations are transferable to demand forecasting and useful after fine-tuning.
- Evidence anchors:
  - [section]: "For the textual product information, we propose a transformer-based component optimized for demand forecasting to extract the features from the product text... The textual and visual approaches do not need large quantities of training data because their weights are pre-trained on large collections and fine-tuned afterwards."
  - [section]: "Product images undergo preprocessing... The images are then normalized to match the pre-trained weights in ImageNet... The model is trained using Mean Squared Error Loss on the Average Product Demand."
  - [corpus]: Weak evidence; corpus does not contain specific pre-training and fine-tuning comparisons.
- Break condition: Pre-trained representations are not aligned with demand forecasting signals, or fine-tuning overfits to limited data.

### Mechanism 3
- Claim: Geographical embeddings created via Node2Vec on warehouse-region graphs help the TFT capture regional demand patterns and customer preferences.
- Mechanism: Node2Vec generates embeddings that encode neighborhood structure in the warehouse-region graph; these embeddings allow the TFT to learn regional demand correlations and preferences.
- Core assumption: Geographical region structure and connectivity to warehouses are predictive of demand patterns.
- Evidence anchors:
  - [section]: "The area in which customers live has an influence on their buying behavior since they are likely to have different preferences... For the geographical embeddings, we create a graph structure consisting of all the regions in the vicinity of a specific warehouse... We employ a graph-based approach, utilizing Node2vec, to transform this geographic region into effective demand forecasting representations."
  - [section]: "In Table 4 it can be seen that with a larger network size, almost all the features in the ablation study improve performance. Especially the textual features extracted using the Textual and Geographical information embedded with Node2Vec improve performance on all metrics."
  - [corpus]: Weak evidence; corpus contains related traffic forecasting but not demand forecasting with geographical graph embeddings.
- Break condition: Regional demand patterns are not structured by warehouse-region connectivity, or Node2Vec embeddings do not capture relevant features.

## Foundational Learning

- Concept: Transformer-based architectures and self-attention
  - Why needed here: The MTFT uses a Temporal Fusion Transformer which relies on self-attention to capture complex temporal and multimodal dynamics.
  - Quick check question: How does self-attention allow the model to weigh different input features differently when making predictions?

- Concept: Multimodal feature extraction and fusion
  - Why needed here: The paper extracts features from product images, text, and geography separately and fuses them with tabular data for demand forecasting.
  - Quick check question: What are the benefits and challenges of fusing features from different modalities compared to using only tabular data?

- Concept: Fine-tuning pre-trained models
  - Why needed here: The paper uses pre-trained ResNet152, DistilBERT, and BLIP models and fine-tunes them on demand forecasting to create effective embeddings.
  - Quick check question: Why is fine-tuning a pre-trained model often more effective than training from scratch, especially with limited data?

## Architecture Onboarding

- Component map: Product images, text, geographical data -> ResNet152, DistilBERT, Node2Vec -> Feature fusion with tabular data -> Temporal Fusion Transformer -> Demand quantile predictions
- Critical path: Feature extraction (esp. fine-tuning) -> Multimodal fusion in TFT -> Quantile prediction generation
- Design tradeoffs: Larger network size improves performance but increases training cost; fine-tuning pre-trained models reduces data needs but may not fully adapt to task; dimensionality reduction enables TFT compatibility but may lose information
- Failure signatures: Poor performance on specific product categories or regions; overfitting on training data; multimodal features not improving over tabular baseline
- First 3 experiments:
  1. Train and evaluate TFT with only tabular features as baseline.
  2. Add one multimodal feature (e.g., textual embeddings) to baseline and compare performance.
  3. Add all multimodal features and compare performance to baseline and single-modal variants.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions. However, based on the content, potential open questions include:
1. How do multimodal product demand forecasting methods perform compared to traditional approaches when predicting demand for newly introduced products with no historical sales data?
2. What is the optimal combination of visual, textual, and geographical modalities for product demand forecasting in different product categories or regions?
3. How does the performance of multimodal product demand forecasting methods vary across different time horizons (e.g., short-term vs. long-term forecasting)?

## Limitations
- Limited ablation studies to isolate the contribution of individual multimodal features.
- Effectiveness of the approach may not generalize to other product categories or demand forecasting contexts.
- Lack of extensive error analysis or sensitivity studies on hyperparameter choices.

## Confidence
- **High**: The multimodal fusion approach using a large network size consistently improves demand forecasting performance across multiple metrics (MAE, MSD, RMSE) compared to tabular-only baselines.
- **Medium**: The individual contributions of visual, textual, and geographical features are supported by ablation studies, but the relative importance of each modality is not quantified.
- **Low**: The generalizability of the results to other product categories, demand forecasting contexts, or datasets is uncertain due to limited external validation.

## Next Checks
1. Conduct ablation studies to quantify the relative importance of visual, textual, and geographical features for different product categories and demand patterns.
2. Perform sensitivity analysis on key hyperparameters (e.g., network size, learning rate, batch size) to assess the robustness of the multimodal fusion approach.
3. Validate the approach on additional real-world demand forecasting datasets from different domains or product categories to assess generalizability.