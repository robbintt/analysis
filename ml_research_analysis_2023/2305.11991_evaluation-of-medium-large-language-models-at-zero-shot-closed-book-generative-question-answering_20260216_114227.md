---
ver: rpa2
title: Evaluation of medium-large Language Models at zero-shot closed book generative
  question answering
arxiv_id: '2305.11991'
source_url: https://arxiv.org/abs/2305.11991
tags:
- arxiv
- language
- question
- answers
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper evaluates medium-sized language models (MLMs) with 6\u2013\
  100 billion parameters on zero-shot closed-book generative question answering using\
  \ a custom dataset. Results from human evaluation show that combining the best answers\
  \ from different MLMs achieved an overall correct answer rate of 82.7%, surpassing\
  \ ChatGPT\u2019s 60.9%."
---

# Evaluation of medium-large Language Models at zero-shot closed book generative question answering

## Quick Facts
- arXiv ID: 2305.11991
- Source URL: https://arxiv.org/abs/2305.11991
- Reference count: 0
- Key outcome: Combining best answers from multiple MLMs achieved 82.7% accuracy, outperforming ChatGPT's 60.9%

## Executive Summary
This paper evaluates medium-sized language models (6-100B parameters) on zero-shot closed-book generative question answering using a custom dataset of 110 questions across 10 categories. Human evaluation reveals that combining the best answers from different models yields 82.7% overall correctness, significantly surpassing ChatGPT's performance. The study demonstrates that fine-tuning quality matters more than parameter count, with the best-performing model (Alpaca 7B) achieving 46.4% accuracy. The research also highlights that MLMs are complementary, with combined correct answers reaching 88.8%, and suggests that fine-grained feedback could further improve reasoning capabilities.

## Method Summary
The study evaluates medium-sized language models (6-100B parameters) on zero-shot closed-book generative question answering using a custom dataset of 110 questions across 10 categories. Models are tested using standard prefixes and temperature=0.1 settings, with outputs manually evaluated for correctness by human assessors. The evaluation focuses on comparing model performances, examining the impact of fine-tuning quality versus parameter count, and analyzing the complementarity of different models when their best answers are combined.

## Key Results
- Combining best answers from different MLMs achieved 82.7% correct answer rate, outperforming ChatGPT's 60.9%
- Alpaca 7B (33B parameters) achieved the highest individual performance at 71.8% accuracy
- MLMs are complementary, with combined correct answers across models reaching 88.8%
- Fine-tuning with appropriate training data proved more important than parameter count for performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining the best answers from multiple MLMs yields higher correct answer rates than single models
- Mechanism: Different MLMs have varying strengths based on their training data and fine-tuning, so complementary knowledge across models improves overall accuracy
- Core assumption: MLMs are trained on sufficiently diverse data that their correct answers do not significantly overlap
- Evidence anchors:
  - [abstract] "Results show that combining the best answers from different MLMs yielded an overall correct answer rate of 82.7% which is better than the 60.9% of ChatGPT."
  - [section] "The 44.5% and 41.8% of the models add up to 62.7% correct answers which even outperforms ChatGPT (60.9%)."
- Break condition: If MLMs are trained on highly overlapping datasets or share similar fine-tuning strategies, their answers would converge, reducing the benefit of combination

### Mechanism 2
- Claim: Fine-tuning with appropriate training data is more important than parameter count for MLM performance
- Mechanism: Models with fewer parameters but better fine-tuning can outperform larger models with generic or insufficient fine-tuning
- Core assumption: Training data quality and relevance to the target task have greater impact on performance than sheer model size
- Evidence anchors:
  - [abstract] "The best MLM achieved 71.8% and has 33B parameters, which highlights the importance of using appropriate training data for fine-tuning rather than solely relying on the number of parameters."
  - [section] "OPT-30B and LLaMA-30B did not outperform their smaller siblings."
- Break condition: If all models are equally well fine-tuned, parameter count differences would become the dominant factor in performance

### Mechanism 3
- Claim: Human evaluation is necessary for assessing generative question answering because automated metrics fail to capture correct but differently phrased answers
- Mechanism: Human evaluators can recognize correct answers that deviate from ground truth wording, whereas BLEU/ROUGE cannot
- Core assumption: Generative QA tasks inherently involve answer diversity that automated metrics cannot adequately score
- Evidence anchors:
  - [section] "It is not only possible to give the correct answer in an alternative formulation that might not be detected by current evaluation methods like BLEU and ROUGE [47]."
  - [section] "We therefore perform a human evaluation to test model accuracy."
- Break condition: If a perfect automated metric were developed that could match human judgment, manual evaluation would become unnecessary

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The evaluation focuses on models' ability to answer questions without task-specific training examples
  - Quick check question: What is the key difference between zero-shot and few-shot learning?

- Concept: Fine-tuning vs. pre-training
  - Why needed here: The paper contrasts models' performance based on different fine-tuning strategies, showing that appropriate fine-tuning is crucial
  - Quick check question: How does instruction-tuning differ from standard fine-tuning?

- Concept: Closed-book QA
  - Why needed here: The evaluation requires models to generate answers without external document retrieval, testing their internalized knowledge
  - Quick check question: What are the advantages and limitations of closed-book QA compared to open-book approaches?

## Architecture Onboarding

- Component map: Prompt construction -> Model inference (FP16 on A100) -> Answer collection -> Human evaluation -> Performance aggregation
- Critical path: Prompt construction → Model inference → Answer collection → Human evaluation → Performance aggregation
- Design tradeoffs:
  - Parameter efficiency vs. performance: Smaller models with better fine-tuning can outperform larger models
  - Automation vs. accuracy: Human evaluation is slower but more accurate than automated metrics
  - Model diversity vs. complexity: Using multiple models increases coverage but requires managing different architectures
- Failure signatures:
  - Poor performance despite high parameter count: Indicates undertraining or ineffective fine-tuning
  - Consistent hallucination across models: Suggests fundamental limitations in training data quality
  - Human evaluation disagreement: Points to subjective judgment or unclear question phrasing
- First 3 experiments:
  1. Test different prompt engineering strategies (prefix variations) on a single model to optimize output quality
  2. Compare performance of fine-tuned vs. non-fine-tuned versions of the same base model
  3. Evaluate the effect of temperature settings on answer correctness and consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning with instruction data compare to RLHF in improving MLM performance on complex reasoning tasks?
- Basis in paper: [inferred] The paper notes that instruction-tuned models like Alpaca perform well, but suggests that more fine-grained feedback could further improve performance, hinting at potential benefits of RLHF
- Why unresolved: The paper does not directly compare instruction tuning to RLHF across models, leaving uncertainty about which approach is more effective for complex reasoning
- What evidence would resolve it: A controlled experiment comparing models with instruction tuning versus RLHF on the same reasoning tasks, measuring accuracy and answer quality

### Open Question 2
- Question: Why do some 30B parameter models underperform smaller models in zero-shot closed-book QA?
- Basis in paper: [explicit] The paper observes that 30B models like OPT-30B and LLaMA-30B perform worse than smaller models, suggesting potential undertraining or other factors
- Why unresolved: The paper speculates about undertraining but does not provide definitive evidence or test alternative hypotheses like data quality or architectural differences
- What evidence would resolve it: Detailed analysis of training data overlap, epochs, and model architecture comparisons to identify the root cause of performance degradation

### Open Question 3
- Question: How can aspect-based feedback improve MLM reasoning compared to aggregated scoring?
- Basis in paper: [explicit] The paper proposes that fine-grained feedback targeting specific parts of answers (e.g., intermediate steps in multi-hop reasoning) could enhance MLM performance
- Why unresolved: The paper does not implement or test aspect-based feedback, leaving its potential impact on reasoning unexplored
- What evidence would resolve it: Implementation of a feedback system that evaluates specific reasoning steps and comparison of model performance before and after incorporating such feedback

## Limitations
- Reliance on human evaluation without standardized rubric makes reproducibility challenging
- Custom dataset of 110 questions may not generalize to all closed-book QA domains
- ChatGPT comparison limited by potential differences in evaluation conditions

## Confidence

**High Confidence:** Fine-tuning quality matters more than parameter count is well-supported by direct comparisons showing smaller models outperforming larger ones (e.g., Alpaca 7B at 46.4% vs. other larger models). The complementarity observation across models (88.8% combined correct rate) is also strongly evidenced by the data.

**Medium Confidence:** Human evaluation necessity over automated metrics is reasonable but could be strengthened with additional evidence about evaluator training or agreement metrics. The specific 82.7% combined accuracy figure is credible but depends heavily on the evaluation methodology.

**Low Confidence:** Exact ranking of individual model performances and the specific 46.4% accuracy for Alpaca 7B should be interpreted cautiously without knowing the evaluation rubric details and inter-rater agreement statistics.

## Next Checks
1. Independent Dataset Verification: Replicate the evaluation using a separate, publicly available question dataset to verify whether observed performance patterns hold across different question distributions and domains.

2. Automated vs. Human Correlation Study: Conduct a controlled study comparing human evaluation results with multiple automated metrics (BLEU, ROUGE, BERTScore, etc.) on the same outputs to quantify exactly where and why automated metrics fail.

3. Fine-tuning Ablation Analysis: Systematically compare instruction-tuned, standard fine-tuned, and base models of identical architectures to isolate the specific contribution of fine-tuning methodology versus other factors.