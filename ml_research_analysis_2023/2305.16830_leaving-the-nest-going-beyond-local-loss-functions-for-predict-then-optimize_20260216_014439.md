---
ver: rpa2
title: 'Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize'
arxiv_id: '2305.16830'
source_url: https://arxiv.org/abs/2305.16830
tags:
- loss
- learning
- predictions
- functions
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses challenges in Predict-then-Optimize (PtO)
  by proposing Efficient Global Losses (EGLs), which learn task-specific loss functions
  without restrictive assumptions about prediction distributions. EGLs use feature-based
  parameterization to generalize across decision-making instances and model-based
  sampling to generate realistic predictions, avoiding the localness assumption of
  previous methods.
---

# Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize

## Quick Facts
- arXiv ID: 2305.16830
- Source URL: https://arxiv.org/abs/2305.16830
- Authors: 
- Reference count: 36
- Primary result: EGLs achieve state-of-the-art performance on four domains, outperforming existing methods by nearly 200% in a domain where localness is violated

## Executive Summary
This paper addresses challenges in Predict-then-Optimize (PtO) by proposing Efficient Global Losses (EGLs), which learn task-specific loss functions without restrictive assumptions about prediction distributions. EGLs use feature-based parameterization to generalize across decision-making instances and model-based sampling to generate realistic predictions, avoiding the localness assumption of previous methods. Theoretical analysis proves EGLs are Fisher Consistent for linear optimization problems. Empirical results show EGLs achieve state-of-the-art performance on four domains, outperforming existing methods by nearly 200% in a domain where localness is violated and requiring an order of magnitude fewer samples in others, resulting in significant computational speed-ups.

## Method Summary
EGLs (Efficient Global Losses) learn task-specific loss functions for Predict-then-Optimize problems without restrictive assumptions about prediction distributions. The method uses feature-based parameterization to share loss function parameters across similar instances, reducing sample inefficiency. Model-based sampling generates realistic predictions by training intermediate predictive models rather than adding noise to true labels. The approach is theoretically grounded with Fisher Consistency guarantees for linear optimization problems and demonstrates significant empirical improvements across multiple domains.

## Key Results
- EGLs outperform existing methods by nearly 200% in the Hard Cubic Top-K domain where localness assumptions are violated
- EGLs require an order of magnitude fewer samples compared to previous approaches, leading to significant computational speed-ups
- EGLs achieve state-of-the-art performance across four benchmark domains: Cubic Top-K, Web Advertising, Portfolio Optimization, and Hard Cubic Top-K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature-based parameterization (FBP) reduces sample inefficiency by sharing loss function parameters across similar instances.
- Mechanism: Instead of learning separate loss functions for each decision-making instance, EGLs learn a mapping from feature space to loss parameters. This allows generalization across instances and reduces the number of samples needed to learn effective loss functions.
- Core assumption: Predictions with similar features will have similar impacts on decision quality regret.
- Evidence anchors:
  - [abstract]: "EGLs use feature-based parameterization to generalize across decision-making instances"
  - [section 5]: "we propose learning a mapping from the features of a given prediction x to the corresponding loss function parameter(s), which we call feature-based parameterization (FBP)"
  - [corpus]: Weak - no direct evidence in neighbor papers about this specific parameterization approach
- Break condition: If the assumption that similar features lead to similar decision quality impacts is violated, FBP could learn suboptimal loss functions.

### Mechanism 2
- Claim: Model-based sampling (MBS) generates realistic predictions that avoid the localness assumption problem.
- Mechanism: Instead of adding Gaussian noise to true labels, MBS trains predictive models and uses their intermediate outputs as candidate predictions. This captures the actual prediction distribution that models will generate.
- Core assumption: Predictions from trained models are more representative of realistic predictions than noise-added labels.
- Evidence anchors:
  - [abstract]: "model-based sampling to generate realistic predictions, avoiding the localness assumption"
  - [section 6]: "we propose an alternative: model-based sampling (MBS)... we train a predictive model Mθ on a standard loss function (e.g., MSE)"
  - [corpus]: Weak - neighbor papers don't discuss this sampling technique specifically
- Break condition: If the intermediate models used for sampling have very different prediction distributions than the final trained model, MBS could generate unrealistic samples.

### Mechanism 3
- Claim: EGLs achieve Fisher Consistency for linear optimization problems.
- Mechanism: By learning weights as a function of features rather than instance-specific weights, EGLs ensure that the optimal prediction depends only on the value of labels corresponding to that feature.
- Core assumption: When weights are feature-dependent rather than instance-dependent, the optimal prediction will minimize decision quality regret.
- Evidence anchors:
  - [abstract]: "Theoretical analysis proves EGLs are Fisher Consistent for linear optimization problems"
  - [section 5.1]: "Theorem 5.4. WeightedMSE with FBP is Fisher Consistent for Predict-then-Optimize problems in which the optimization function z∗ has a linear objective"
  - [corpus]: Weak - no neighbor papers discuss Fisher Consistency in this context
- Break condition: If the optimization problem has non-linear objectives or constraints, Fisher Consistency may not hold.

## Foundational Learning

- Concept: Predict-then-Optimize (PtO) framework
  - Why needed here: This paper builds on PtO by addressing limitations in learning task-specific loss functions within this framework
  - Quick check question: What are the two main steps in the PtO framework and how do they relate to each other?

- Concept: Fisher Consistency
  - Why needed here: The paper proves that EGLs are Fisher Consistent, which is a fundamental theoretical property ensuring optimal decision-making in the limit
  - Quick check question: Why is Fisher Consistency important for loss functions in decision-focused learning?

- Concept: Decision Quality (DQ) regret
  - Why needed here: The paper learns loss functions that approximate DQ regret to improve decision quality, and this concept underlies the entire approach
  - Quick check question: How is DQ regret defined and why is it the target for learning loss functions?

## Architecture Onboarding

- Component map: Model-based sampler -> Feature-based parameterization -> Loss function learner -> Predictive model training -> Optimization solver -> Evaluation
- Critical path: Model-based sampling → Loss function learning (with FBP) → Predictive model training → Evaluation on test set
- Design tradeoffs:
  - FBP reduces sample efficiency but may limit expressivity
  - MBS increases computational cost but generates more realistic predictions
  - Fisher Consistency provides theoretical guarantees but may not extend to non-linear problems
- Failure signatures:
  - Poor performance when localness assumption is violated (MBS fails to generate realistic predictions)
  - Loss functions not generalizing across instances (FBP too restrictive)
  - Decision quality not improving despite better theoretical properties (Fisher Consistency doesn't translate to practical gains)
- First 3 experiments:
  1. Implement FBP on a simple domain (like Cubic Top-K) to verify sample efficiency gains
  2. Compare MBS vs Gaussian sampling on a domain where localness assumption is broken
  3. Test Fisher Consistency by training on infinite data and checking if optimal predictions match theoretical expectations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does model-based sampling (MBS) significantly outperform Gaussian sampling for learning task-specific loss functions?
- Basis in paper: [explicit] The paper demonstrates MBS's superiority in the Cubic Top-K domain where the localness assumption is violated, and suggests potential benefits in other domains based on hypotheses about increasing sample efficiency and focusing on realistic predictions.
- Why unresolved: The paper provides hypotheses for MBS's effectiveness in domains where localness is not violated (Web Advertising and Portfolio Optimization), but does not rigorously test these hypotheses or provide a comprehensive framework for determining when MBS will be most beneficial.
- What evidence would resolve it: Empirical studies comparing MBS and Gaussian sampling across a wide range of domains with varying characteristics (e.g., linearity of underlying functions, distribution of errors) would help identify the specific conditions under which MBS provides the most significant improvements. Additionally, theoretical analysis of the impact of sampling methods on the quality of learned loss functions could provide insights into the mechanisms behind MBS's effectiveness.

### Open Question 2
- Question: How does the choice of feature-based parameterization (FBP) architecture impact the performance of learned loss functions?
- Basis in paper: [explicit] The paper uses a 4-layer feedforward neural network with a hidden dimension of 500 for FBP, but acknowledges that this choice reduces the expressivity of learned losses.
- Why unresolved: The paper does not explore alternative FBP architectures or provide a systematic evaluation of how different architectural choices (e.g., depth, width, type of neural network) affect the performance of learned loss functions.
- What evidence would resolve it: Experiments comparing the performance of EGLs with different FBP architectures (e.g., varying depth, width, or using different types of neural networks like convolutional or recurrent networks) would provide insights into the optimal architecture for FBP. Additionally, theoretical analysis of the trade-off between expressivity and generalization in FBP could guide the choice of architecture.

### Open Question 3
- Question: Can the principles of feature-based parameterization and model-based sampling be extended to other types of decision-focused learning approaches beyond learning task-specific loss functions?
- Basis in paper: [inferred] The paper focuses on improving the learning of task-specific loss functions through FBP and MBS, but these principles could potentially be applied to other DFL approaches, such as end-to-end differentiable optimization or surrogate problem methods.
- Why unresolved: The paper does not explore the applicability of FBP and MBS to other DFL approaches or provide a framework for adapting these principles to different methods.
- What evidence would resolve it: Empirical studies applying FBP and MBS to other DFL approaches (e.g., end-to-end differentiable optimization, surrogate problem methods) would demonstrate the generalizability of these principles. Additionally, theoretical analysis of the commonalities and differences between DFL approaches could guide the adaptation of FBP and MBS to different methods.

## Limitations

- The Fisher Consistency guarantee only applies to linear optimization problems, limiting theoretical guarantees for non-linear scenarios
- Model-based sampling's effectiveness depends on the quality of intermediate predictive models, which may not always capture realistic prediction distributions
- The approach's sample efficiency gains and performance improvements may be domain-dependent and not generalize to all problem types

## Confidence

- **Fisher Consistency for linear problems**: High confidence - the theoretical proof is rigorous and well-established
- **Sample efficiency improvements**: Medium confidence - supported by empirical results but may be domain-specific
- **Model-based sampling effectiveness**: Medium confidence - experimental evidence is strong but depends on model quality
- **Generalizability across domains**: Low-Medium confidence - results shown on four domains but real-world applicability needs more testing

## Next Checks

1. **Non-linear optimization validation**: Test EGLs on non-linear optimization problems to assess how Fisher Consistency guarantees translate to practical performance outside the linear regime.

2. **Cross-domain robustness test**: Apply EGLs to additional problem domains with varying prediction distributions to evaluate the generality of the sample efficiency claims and model-based sampling effectiveness.

3. **Sample efficiency analysis**: Conduct ablation studies varying the number of samples used in FBP and MBS to quantify the exact sample efficiency gains and identify potential breaking points where the approach becomes less effective.