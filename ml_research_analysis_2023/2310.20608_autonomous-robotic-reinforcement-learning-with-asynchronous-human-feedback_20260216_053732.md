---
ver: rpa2
title: Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback
arxiv_id: '2310.20608'
source_url: https://arxiv.org/abs/2310.20608
tags:
- learning
- feedback
- human
- goal
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GEAR is a system for autonomous robotic reinforcement learning
  that enables agents to improve by training in the real world without requiring hand-designed
  reward functions or reset mechanisms. It uses occasional non-expert human-in-the-loop
  feedback to learn informative distance functions for guiding exploration, while
  leveraging self-supervised learning for goal-directed policy learning.
---

# Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback

## Quick Facts
- arXiv ID: 2310.20608
- Source URL: https://arxiv.org/abs/2310.20608
- Reference count: 40
- Primary result: GEAR achieves autonomous robotic learning in real-world environments using asynchronous human feedback without requiring resets or hand-designed rewards

## Executive Summary
GEAR is a system for autonomous robotic reinforcement learning that enables agents to improve by training in the real world without requiring hand-designed reward functions or reset mechanisms. It uses occasional non-expert human-in-the-loop feedback to learn informative distance functions for guiding exploration, while leveraging self-supervised learning for goal-directed policy learning. GEAR accounts for the current "reachability" of the exploration policy when deciding which regions of the space to explore. Experiments show GEAR can learn behaviors autonomously in simulation and the real world, outperforming previous methods in reset-free learning and human-in-the-loop techniques.

## Method Summary
GEAR combines goal-conditioned reinforcement learning with human-in-the-loop feedback to enable autonomous robotic learning without resets or hand-designed rewards. The system collects robot experience data and uses occasional asynchronous feedback from non-expert humans to learn a proximity function that guides exploration. A density model estimates the reachability of potential subgoals based on the current policy, filtering out infeasible exploration commands. The policy learns through self-supervised goal-conditioned learning using hindsight relabeling, where actually reached states become training goals. This approach allows continuous learning without explicit resets or reward engineering.

## Key Results
- GEAR learns behaviors autonomously in both simulation and real-world environments including Four Rooms navigation, LoCoBot navigation, block pushing, and kitchen manipulation
- The system successfully uses asynchronous feedback from 40 different annotators over 8 hours to learn complex robotic behaviors
- GEAR outperforms previous methods in reset-free learning and human-in-the-loop techniques while requiring minimal human supervision

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GEAR's subgoal selection using human feedback and density estimation enables effective exploration in reset-free RL.
- **Mechanism:** The system combines human-provided binary comparisons to estimate state-goal proximity (dϕ) with density modeling to assess reachability. This allows the agent to select intermediate subgoals that are both close to the target goal and reachable under the current policy, avoiding infeasible exploration commands.
- **Core assumption:** Human binary feedback can provide reliable proximity estimates when aggregated, and density estimation accurately reflects reachability in the current policy.
- **Evidence anchors:**
  - [abstract] "Our system leverages occasional non-expert human-in-the-loop feedback from remote users to learn informative distance functions to guide exploration"
  - [section 4.2] "human users can be asked to label which state si or sj is closer to a particular desired goal g. These preferences can then be used to infer an (unnormalized) state-goal proximity function dϕ"
  - [corpus] Strong - the related work "Breadcrumbs to the Goal" uses similar human feedback for goal-conditioned exploration
- **Break condition:** If human feedback becomes too sparse or inconsistent, the proximity estimate dϕ degrades. If the density model fails to capture reachability (e.g., in non-quasistatic systems), subgoals may be selected that are unreachable.

### Mechanism 2
- **Claim:** Self-supervised goal-conditioned policy learning enables training without reset mechanisms or hand-designed rewards.
- **Mechanism:** By using hindsight relabeling, the policy π(a|s,g) learns from trajectories by treating actually reached states as goals. This creates a self-supervised learning signal that doesn't require explicit rewards or resets.
- **Core assumption:** Policy generalization allows learning to reach commanded goals even when those goals weren't explicitly practiced during training.
- **Evidence anchors:**
  - [abstract] "leveraging a simple self-supervised learning algorithm for goal-directed policy learning"
  - [section 4.1] "Recent techniques have circumvented this by leveraging the idea of hindsight relabeling...states that were reached in some trajectory during training are labeled, in hindsight, as goal states"
  - [corpus] Strong - the related work "Breadcrumbs to the Goal" uses similar self-supervised goal-conditioned learning
- **Break condition:** If the policy fails to generalize to new goals (poor generalization), the self-supervised learning signal becomes ineffective, and the agent cannot learn to reach diverse goals.

### Mechanism 3
- **Claim:** Asynchronous human feedback enables scalable data collection without requiring expert supervision.
- **Mechanism:** By using crowdworkers to provide occasional binary comparisons, GEAR can collect the necessary proximity information without requiring continuous expert involvement or specialized reward engineering.
- **Core assumption:** Non-expert crowdworkers can provide useful comparative feedback that, when aggregated, approximates expert judgments.
- **Evidence anchors:**
  - [abstract] "The system streams robot experience to a web interface only requiring occasional asynchronous feedback from remote, crowdsourced, non-expert humans"
  - [section 5.4] "We set up two different tasks...Human supervisors provide occasional feedback: 453 comparative labels provided asynchronously over 8 hours, from 40 different annotators"
  - [corpus] Weak - while the related work mentions crowdsourcing, there's limited detail on non-expert feedback quality
- **Break condition:** If crowdworkers provide inconsistent or noisy feedback, the proximity model dϕ becomes unreliable, degrading subgoal selection quality.

## Foundational Learning

- **Concept: Goal-conditioned reinforcement learning**
  - Why needed here: GEAR needs to learn policies that can reach various goal states, not just maximize a single reward signal. This framework allows the same policy to be used for both forward and backward tasks in reset-free learning.
  - Quick check question: What is the key difference between standard RL and goal-conditioned RL in terms of the reward function specification?

- **Concept: Hindsight relabeling**
  - Why needed here: Enables self-supervised learning by using actually reached states as goals, creating training data without requiring external reward signals or demonstrations.
  - Quick check question: How does hindsight relabeling create a learning signal when the original goal is unlikely to be reached?

- **Concept: Density estimation for reachability**
  - Why needed here: In reset-free learning, not all states in the replay buffer are reachable from the current state. Density estimation helps filter out unreachable states when selecting subgoals.
  - Quick check question: Why is policy reachability particularly important in reset-free RL compared to episodic RL?

## Architecture Onboarding

- **Component map:** Robot → Experience collection → Buffer update → Subgoal selection (proximity + density) → Policy training → Human feedback collection → Proximity model update → Density model update

- **Critical path:** Robot → Experience collection → Buffer update → Subgoal selection (proximity + density) → Policy training → Human feedback collection → Proximity model update → Density model update

- **Design tradeoffs:**
  - Frequency of human feedback vs. exploration efficiency
  - Complexity of density model vs. computational cost
  - Granularity of subgoal selection vs. exploration diversity
  - Reliance on pre-training data vs. autonomous learning capability

- **Failure signatures:**
  - Policy not improving: Check if density model is learning reachability, if human feedback is being collected, or if subgoals are being selected
  - Subgoals consistently unreachable: Density model threshold may be too low or proximity estimates may be inaccurate
  - Learning very slow: May need more frequent human feedback or better pre-training data

- **First 3 experiments:**
  1. Run GEAR with synthetic oracle feedback on Four Rooms navigation to verify core mechanism works without human feedback noise
  2. Test subgoal selection with perfect density model but no human feedback to isolate proximity estimation impact
  3. Run with only human feedback but no density modeling to demonstrate importance of reachability filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of reachability threshold affect the performance of GEAR across different robotic tasks and environments?
- Basis in paper: [explicit] The paper mentions that choosing the right threshold for the success of the algorithm is critical, with the best reachable threshold used for the point-mass navigation task being 5.
- Why unresolved: The paper only provides results for one specific environment (point-mass navigation) and does not explore how different reachability thresholds affect performance across a wider range of tasks.
- What evidence would resolve it: Conducting experiments with different reachability thresholds on various robotic tasks and environments, and comparing the resulting performance metrics.

### Open Question 2
- Question: What is the impact of using different types of human feedback (e.g., beyond binary comparisons) on the performance and efficiency of GEAR?
- Basis in paper: [inferred] The paper mentions that other types of human feedback have been leveraged in previous work, such as through physical contact, eye gaze, and emergency stop, and that GEAR is agnostic to the kind of feedback as long as it can be translated into a sort of distance function to guide subgoal selection.
- Why unresolved: The paper focuses on binary comparative feedback and does not explore the potential benefits or drawbacks of using different types of human feedback in GEAR.
- What evidence would resolve it: Implementing GEAR with different types of human feedback and comparing the resulting performance and efficiency metrics.

### Open Question 3
- Question: How does the frequency of human feedback impact the overall sample efficiency and learning progress of GEAR?
- Basis in paper: [explicit] The paper mentions that the frequency at which feedback is provided affects the total amount of feedback or steps needed to learn a successful policy, and that there is a trade-off between the time it takes to succeed and the amount of feedback required.
- Why unresolved: The paper provides some initial insights into the impact of feedback frequency, but does not fully explore the optimal frequency or how it varies across different tasks and environments.
- What evidence would resolve it: Conducting a systematic study of feedback frequency on various robotic tasks and environments, and identifying the optimal frequency that balances sample efficiency and learning progress.

## Limitations

- Performance heavily depends on the quality and consistency of crowdworker feedback, with unclear scalability to more complex tasks or sparser feedback scenarios
- The density estimation assumes quasistatic systems, which may not hold for all robotic applications, limiting applicability to non-quasistatic environments
- The system requires approximately 453 labels from 40 annotators over 8 hours, raising questions about the minimum viable feedback frequency for effective learning

## Confidence

- **High confidence**: The core mechanism of using human feedback for proximity estimation and density modeling for reachability filtering is well-supported by the literature and experimental results.
- **Medium confidence**: The effectiveness of asynchronous non-expert feedback for learning informative distance functions, while demonstrated, requires more extensive testing across diverse task domains.
- **Medium confidence**: The claim that GEAR outperforms previous methods in reset-free learning and human-in-the-loop techniques is based on comparisons with specific baselines in limited experimental settings.

## Next Checks

1. Test GEAR's performance with progressively sparser human feedback to determine the minimum viable feedback frequency for effective learning.
2. Evaluate the system on non-quasistatic robotic tasks where the current density estimation approach may fail, and develop alternative reachability metrics.
3. Conduct ablation studies to quantify the individual contributions of human feedback, density estimation, and self-supervised learning to overall performance.