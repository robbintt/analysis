---
ver: rpa2
title: 'Do I Have Your Attention: A Large Scale Engagement Prediction Dataset and
  Baselines'
arxiv_id: '2302.00431'
source_url: https://arxiv.org/abs/2302.00431
tags:
- engagement
- dataset
- data
- were
- gaze
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present EngageNet, a large-scale dataset for user engagement
  prediction comprising 31 hours of video from 127 participants. The dataset captures
  behavioral and cognitive engagement aspects, with videos segmented into 10-second
  clips and annotated into four engagement levels.
---

# Do I Have Your Attention: A Large Scale Engagement Prediction Dataset and Baselines

## Quick Facts
- **arXiv ID**: 2302.00431
- **Source URL**: https://arxiv.org/abs/2302.00431
- **Reference count**: 0
- **Primary result**: Multimodal Transformer-based models outperform single-modality and other sequence models on user engagement prediction.

## Executive Summary
This paper introduces EngageNet, a large-scale dataset for user engagement prediction comprising 31 hours of video from 127 participants. The dataset captures behavioral and cognitive engagement aspects, with videos segmented into 10-second clips and annotated into four engagement levels. The authors conduct extensive experiments using features such as eye gaze, head pose, and facial action units, combined with deep learning models including LSTM, CNN-LSTM, TCN, and Transformer. Multimodal feature combinations consistently outperform individual modalities, with the Transformer achieving the best performance. Cross-dataset validation on EmotiW demonstrates the dataset's effectiveness for engagement modeling.

## Method Summary
The authors collected video data from 127 participants watching educational content, extracting behavioral features using OpenFace (eye gaze, head pose, facial action units). Videos were segmented into 10-second clips and annotated for engagement level. Four deep learning architectures were implemented: LSTM, CNN-LSTM, TCN, and Transformer. Features were fed into these models for classification into four engagement levels. Multimodal fusion experiments combined different feature types, and cross-dataset validation was performed on the EmotiW dataset.

## Key Results
- Multimodal feature combinations consistently outperform individual modalities in engagement prediction
- Transformer architecture achieves the best performance among tested models
- Cross-dataset validation on EmotiW shows the model's effectiveness beyond the original dataset
- Highly engaged participants answered the most questions correctly, validating the dataset's quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal feature fusion improves engagement prediction accuracy by capturing complementary behavioral cues.
- Mechanism: Combining eye gaze, head pose, and facial action units allows the model to integrate spatial attention patterns, posture, and micro-expressions, providing a richer representation of engagement states than any single modality alone.
- Core assumption: Engagement behaviors are multimodal and no single feature type captures all relevant variance.
- Evidence anchors:
  - [abstract] "Multimodal feature combinations consistently outperform individual modalities, with the Transformer achieving the best performance."
  - [section 4.2.1] "We observe that multimodal features outperformed individual features on the Test set. Particularly, the combination of eye gaze, head pose, and facial AUs features performs remarkably well in predicting engagement."
  - [corpus] Weak corpus alignment; nearest neighbors discuss user behavior modeling but not multimodal fusion specifically.
- Break condition: If the annotation quality is low or one modality is consistently noisy, the benefit of fusion may vanish or degrade.

### Mechanism 2
- Claim: Temporal segmentation into 10-second clips aligns with natural engagement fluctuation timescales, enabling stable label assignment.
- Mechanism: By dividing long videos into short, behaviorally homogeneous clips, the model can assign a consistent engagement label per segment, reducing intra-clip label ambiguity.
- Core assumption: Engagement states remain relatively stable over 10-second intervals but vary meaningfully across them.
- Evidence anchors:
  - [section 3.2.2] "The videos were further divided into 10 seconds duration. This was performed as the engagement level varies over time in long videos [12]."
  - [corpus] No direct corpus evidence; related papers on engagement prediction do not discuss temporal segmentation strategy.
- Break condition: If engagement changes too rapidly or too slowly relative to the segment length, labeling accuracy degrades.

### Mechanism 3
- Claim: Transformer-based architectures capture long-range dependencies in sequential behavioral cues better than LSTMs or TCNs.
- Mechanism: Transformers process the sequence of segment-level features using self-attention, enabling the model to weigh the importance of past segments regardless of distance, which is critical for engagement that may depend on earlier context.
- Core assumption: Engagement states depend on non-local sequential dependencies.
- Evidence anchors:
  - [abstract] "The Transformer achieving the best performance" among the tested architectures.
  - [section 4.2.4] "As a result, we developed a model based on transformers with positional encoding with an attention layer stacked above and a MLP to predict engagement."
  - [corpus] No direct corpus evidence; related papers focus on user behavior modeling but not sequence modeling comparisons.
- Break condition: If the sequence length is too short or engagement is purely instantaneous, self-attention offers no advantage.

## Foundational Learning

- Concept: Behavioral, cognitive, and affective dimensions of engagement
  - Why needed here: The dataset and task are designed to capture and model multiple facets of engagement, so understanding these distinctions guides feature selection and interpretation.
  - Quick check question: What are the three main dimensions of engagement, and how do they differ in observable cues?

- Concept: Multimodal feature engineering and fusion
  - Why needed here: The model relies on combining eye gaze, head pose, and facial action units; engineers must know how to extract, normalize, and fuse these features effectively.
  - Quick check question: How do you compute mean and standard deviation features from per-frame facial action units, and why are they used?

- Concept: Deep learning sequence modeling (LSTM, TCN, Transformer)
  - Why needed here: The task requires understanding temporal dynamics in engagement; choosing the right architecture impacts performance.
  - Quick check question: What is the key difference between TCN and LSTM in handling long-term dependencies, and when would you prefer one over the other?

## Architecture Onboarding

- Component map: Video capture -> Face detection -> OpenFace feature extraction -> Segment-level feature aggregation -> Temporal encoder (LSTM/CNN-LSTM/TCN/Transformer) -> Dense layers -> Softmax output
- Critical path: Video capture → feature extraction → segment aggregation → model inference → engagement class prediction
- Design tradeoffs:
  - Feature choice: More modalities increase robustness but also computational cost and potential noise
  - Model choice: LSTMs handle variable-length sequences well but are slower; Transformers scale better with parallelization but need careful positional encoding
  - Segment length: 10 seconds balances temporal resolution and label stability; too short increases noise, too long loses granularity
- Failure signatures:
  - Low accuracy with individual modalities but high with multimodal: suggests complementary information exists
  - Accuracy drops on longer clips: indicates temporal modeling is insufficient
  - High variance in gaze/head pose features: points to calibration or extraction issues
- First 3 experiments:
  1. Train and evaluate individual modality models (gaze-only, head pose-only, AU-only) to establish baseline
  2. Train and evaluate all possible two-modality combinations to measure pairwise complementarity
  3. Train the full multimodal model and compare against individual and pairwise models to quantify fusion gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does engagement measured by the dataset correlate with learning outcomes across different subject demographics?
- Basis in paper: [explicit] The authors analyzed the relationship between engagement levels and questionnaire performance, showing that highly engaged participants answered more questions correctly.
- Why unresolved: While a correlation was found, the analysis did not examine how this relationship varies by participant demographics (e.g., age, gender, prior knowledge).
- What evidence would resolve it: Detailed statistical analysis of learning outcomes across different demographic groups, controlling for baseline knowledge and other confounding variables.

### Open Question 2
- Question: How do digital avatars impact user engagement compared to human instructors across different video topics?
- Basis in paper: [explicit] The authors introduced digital teachers in their stimuli videos to study their effect on engagement as a future work.
- Why unresolved: The dataset includes videos with digital avatars, but the authors did not conduct experiments comparing engagement with avatar versus human instructors.
- What evidence would resolve it: Comparative analysis of engagement metrics between videos with digital avatars and those with human instructors across various educational topics.

### Open Question 3
- Question: How well do engagement prediction models generalize to different types of video content beyond educational materials?
- Basis in paper: [inferred] The authors performed cross-dataset validation on EmotiW, showing some generalization capability, but only tested on educational content.
- Why unresolved: The dataset focuses on educational videos, and while cross-dataset validation was performed, it was limited to similar content types.
- What evidence would resolve it: Testing engagement prediction models on diverse video content types (e.g., entertainment, news, tutorials) and analyzing performance across these domains.

## Limitations
- The dataset size (31 hours, 127 participants) is moderate for deep learning tasks, which may limit generalization
- Reliance on OpenFace for feature extraction could introduce domain-specific biases if facial expressions are not representative across all engagement levels
- The paper does not specify inter-annotator agreement or label distribution, raising questions about label robustness

## Confidence
- **High**: Multimodal fusion claim, supported by consistent experimental results showing superior performance of combined features
- **Medium**: Transformer superiority claim, reported but lacks exhaustive ablation or statistical testing against baselines
- **Low**: Cross-dataset generalization claim, only tested on one external dataset (EmotiW) with limited comparability

## Next Checks
1. Conduct inter-annotator reliability analysis on a subset of the EngageNet dataset to quantify label consistency and robustness
2. Perform ablation studies on the Transformer architecture to identify which components (e.g., self-attention vs positional encoding) drive performance gains
3. Test the multimodal engagement model on at least two additional datasets from different domains (e.g., classroom, online learning, or meetings) to assess cross-domain generalization