---
ver: rpa2
title: Enhancing Large Language Models Against Inductive Instructions with Dual-critique
  Prompting
arxiv_id: '2305.13733'
source_url: https://arxiv.org/abs/2305.13733
tags:
- 'false'
- instructions
- question
- llms
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a benchmark for evaluating large language\
  \ models' (LLMs) resistance to inductive instructions\u2014those containing false\
  \ premises that may mislead the model. The authors construct three categories of\
  \ inductive instructions: Fact-Checking Instructions (FCI), Questions based on False\
  \ Premises (QFP), and Creative Instructions based on False Premises (CIFP), with\
  \ 15K examples in total."
---

# Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting

## Quick Facts
- arXiv ID: 2305.13733
- Source URL: https://arxiv.org/abs/2305.13733
- Reference count: 18
- Large language models universally struggle with questions and creative tasks based on false premises, but dual-critique prompting significantly improves resistance

## Executive Summary
This paper introduces the INDUST benchmark to evaluate large language models' (LLMs) resistance to inductive instructions—those containing false premises that may mislead the model. The authors construct three categories of inductive instructions (Fact-Checking, Questions based on False Premises, and Creative Instructions) and find that models consistently struggle with QFP and CIFP categories. To address this vulnerability, they propose Dual-critique prompting, where models are encouraged to critique both themselves and the user before responding. Experiments show this method significantly improves LLM robustness across all instruction categories and complexities, with consistent gains in both zero-shot and few-shot settings.

## Method Summary
The paper constructs the INDUST benchmark with 15K examples across three categories of inductive instructions. Dual-critique prompting is implemented with two variants: O-Critique (one-step self-critique) and M-Critique (multi-step self-critique). The evaluation process involves generating model responses to inductive instructions, applying critique prompting methods, and measuring performance using automatic evaluation with ChatGPT as judge. The study tests several strong LLMs including ChatGLM, ChatGPT, Davinci-003, and Davinci-002.

## Key Results
- LLMs exhibit universal vulnerability to inductive instructions, particularly struggling with Questions based on False Premises (QFP) and Creative Instructions based on False Premises (CIFP)
- Dual-critique prompting shows remarkable improvement in handling inductive instructions across all categories
- Different inductive instruction styles substantially affect model performance, with CIFP being the most challenging
- O-Critique (one-step) outperforms M-Critique (multi-step) in most cases due to reduced computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-critique prompting reduces the likelihood that a model will blindly follow inductive instructions by encouraging explicit error detection before response generation
- Mechanism: The model is prompted to evaluate both the instruction itself (for factual errors or harmful premises) and its own potential response, creating a two-stage verification process before final output
- Core assumption: LLMs can be prompted to recognize when instructions contain false premises and adjust their responses accordingly without requiring retraining
- Evidence anchors:
  - [abstract]: "Dual-critique prompting to encourage LLMs to not only critique themselves but also the users, which show remarkable improvement in handling inductive instructions"
  - [section 5.4]: "Self-Critique Prompting brings consistent improvements... For instance, in the case of ChatGPT, significant and consistent improvements are observed across all three instruction categories"
  - [corpus]: Weak correlation - related work focuses on general instruction robustness but not specifically on dual-critique prompting against false premises
- Break condition: The model may still generate misleading content if the critique prompts are insufficient or if the model's internal knowledge about the false premise is weak or conflicting

### Mechanism 2
- Claim: Different inductive instruction styles affect how well models can identify and reject false premises, with creative instructions being the most challenging
- Mechanism: Models leverage their understanding of factual knowledge differently depending on the instruction format—fact-checking prompts are easier to verify, while creative instructions require synthesis and are more likely to produce misleading content
- Core assumption: The complexity and format of the instruction influences the model's ability to apply its knowledge to detect falsehoods, independent of the underlying false fact itself
- Evidence anchors:
  - [abstract]: "we identified that different inductive styles affect the models' ability to identify the same underlying errors"
  - [section 5.4]: "LLMs consistently exhibit superior performance on FCI compared to QFP and CIFP, even though they are based on the same fact base"
  - [corpus]: Moderate support - related work on instruction robustness shows sensitivity to format but not specifically tested across identical false premises
- Break condition: If the creative task is sufficiently engaging or the false premise is embedded in complex narrative, the model may prioritize task completion over factual accuracy

### Mechanism 3
- Claim: Self-critique prompting works better in one-step (O-Critique) than multi-step (M-Critique) for handling inductive instructions due to reduced computational overhead and cognitive load
- Mechanism: By combining critique and response generation in a single prompt, the model avoids the potential confusion or dilution of the critique signal that can occur when separated into multiple steps
- Core assumption: The model can effectively perform both critique and response generation in a single pass without losing focus or accuracy
- Evidence anchors:
  - [section 5.4]: "The impact of M-Critique prompts on model performance is not as significant... for ChatGPT and Davinci-003, the performance decreases"
  - [section 5.2]: "O-Critique... prompt the model to critically analyze the instruction and question itself before proceeding to generate a response"
  - [corpus]: Limited evidence - most related work focuses on iterative refinement, which is conceptually similar to M-Critique
- Break condition: If the instruction is particularly complex or the critique requirement is nuanced, a single-pass approach may miss subtle errors that a multi-step process could catch

## Foundational Learning

- Concept: Instruction parsing and knowledge grounding
  - Why needed here: The model must correctly parse the instruction and ground its response in accurate factual knowledge rather than the false premise
  - Quick check question: Can the model distinguish between the user's belief (false premise) and objective facts, and choose the latter for response generation?

- Concept: Prompt engineering and in-context learning
  - Why needed here: Effective critique prompting relies on carefully constructed prompts that guide the model to evaluate both the instruction and its response without explicit retraining
  - Quick check question: Does modifying the critique prompt wording significantly change the model's ability to reject false premises?

- Concept: Evaluation alignment between automatic and human judgment
  - Why needed here: The paper uses ChatGPT as an automatic evaluator, so understanding the alignment between this method and human judgment is crucial for interpreting results
  - Quick check question: How well do automatic evaluation results correlate with human annotations, and under what conditions might they diverge?

## Architecture Onboarding

- Component map: False fact collection -> Rewriting (FCI/QFP/CIFP) -> Reference response generation -> Dataset splits (HADUST/ADUST) -> Instruction -> LLM response -> Automatic evaluation (w/evidence) -> Human evaluation for validation -> Base instruction -> Critique prompt -> Modified instruction -> LLM response -> Evaluation

- Critical path: False fact collection → Instruction rewriting → Response generation → Evaluation (automatic + human) → Analysis of critique prompting effectiveness

- Design tradeoffs:
  - Automatic vs. human data collection: Automatic methods scale but may miss nuanced quality issues; human annotation ensures quality but is resource-intensive
  - One-step vs. multi-step critique: One-step is more efficient but may be less thorough; multi-step allows deeper analysis but adds latency and complexity
  - Zero-shot vs. few-shot settings: Zero-shot tests general capability; few-shot allows adaptation but may overfit to demonstration examples

- Failure signatures:
  - High variance in responses across different critique prompts suggests sensitivity to prompt phrasing
  - Consistent poor performance on CIFP indicates difficulty with creative synthesis tasks involving false premises
  - Degradation in M-Critique vs O-Critique suggests prompt complexity or confusion issues

- First 3 experiments:
  1. Compare model performance on identical false facts across FCI, QFP, and CIFP categories to establish baseline sensitivity to instruction style
  2. Test different critique prompt formulations (as shown in Table 12) to identify which phrasing yields best rejection of false premises
  3. Evaluate the correlation between automatic evaluation (w/evidence) and human judgment on a sample of responses to validate the evaluation methodology

## Open Questions the Paper Calls Out

- **Question 1**: How does the complexity of false premises affect model performance across different instruction types?
  - Basis in paper: Explicit (Section 5.4 mentions "the complexity of the underlying assumptions also influences the model's performance")
  - Why unresolved: The paper mentions complexity affects performance but doesn't quantify or analyze this relationship in detail
  - What evidence would resolve it: Detailed analysis showing performance degradation rates as false premise complexity increases, with specific breakdowns by instruction type

- **Question 2**: What are the long-term effects of using critique prompting on model calibration and user trust?
  - Basis in paper: Inferred (The paper shows critique prompting improves immediate performance but doesn't address sustained effects)
  - Why unresolved: While the paper demonstrates short-term improvements, it doesn't examine whether models maintain these improvements over time
  - What evidence would resolve it: Longitudinal studies tracking model performance and user trust metrics over extended periods with regular critique prompting

## Limitations
- Benchmark construction relies on automated generation and human annotation, potentially introducing selection bias
- Evaluation using ChatGPT as an automatic judge may not fully capture human judgment nuances
- Study focuses on English language models, limiting generalizability to multilingual contexts
- Performance improvements may be specific to tested instruction formats and critique prompt formulations

## Confidence
- High confidence: Identification of QFP and CIFP as particularly challenging instruction categories, supported by consistent experimental results across multiple LLMs
- Medium confidence: Mechanism of Dual-critique prompting effectiveness, though exact contribution of each critique component is not isolated
- Low confidence: Generalizability of specific critique prompt formulations, as results show sensitivity to prompt phrasing

## Next Checks
1. Conduct ablation studies to isolate the individual contributions of self-critique versus user-critique components in the Dual-critique method
2. Perform human evaluation on a larger sample of responses, particularly for CIFP category, to validate automatic evaluation alignment
3. Test the approach on multilingual models and diverse instruction domains to assess generalizability beyond the current English-language, web-text-focused benchmark