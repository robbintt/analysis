---
ver: rpa2
title: 'A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization,
  and Sample Complexity'
arxiv_id: '2302.06015'
source_url: https://arxiv.org/abs/2302.06015
tags:
- tokens
- conference
- learning
- have
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first theoretical analysis of training
  a shallow Vision Transformer (ViT) for binary classification. The authors propose
  a data model with label-relevant and label-irrelevant tokens, and analyze the sample
  complexity and generalization error of training a ViT using stochastic gradient
  descent.
---

# A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity

## Quick Facts
- arXiv ID: 2302.06015
- Source URL: https://arxiv.org/abs/2302.06015
- Authors: 
- Reference count: 40
- Primary result: First theoretical analysis of training shallow ViT for binary classification, proving sample complexity bounds and attention sparsity during training

## Executive Summary
This paper provides the first theoretical analysis of training shallow Vision Transformers (ViTs) for binary classification. The authors propose a data model with label-relevant and label-irrelevant tokens, and analyze the sample complexity and generalization error of training a ViT using stochastic gradient descent. They prove that attention weights become increasingly sparse during training, concentrating on label-relevant tokens, and demonstrate that proper token sparsification can improve sample complexity and test performance by removing irrelevant or noisy tokens.

## Method Summary
The paper analyzes a three-layer Vision Transformer (one self-attention layer followed by a two-layer perceptron) for binary classification. The method involves generating synthetic data with label-relevant and label-irrelevant tokens, initializing the model parameters from pretrained models, and training using Stochastic Gradient Descent with Hinge loss. The analysis focuses on proving sample complexity bounds and characterizing how attention weights evolve during training. Token sparsification methods are also studied to understand their impact on reducing sample complexity by removing irrelevant or noisy tokens.

## Key Results
- Sample complexity bound scales linearly with the inverse square of the fraction of label-relevant tokens (1/α²) and the inverse square of token noise level (1/(Θ(1)-τ)²)
- Training process using SGD leads to sparse attention maps that concentrate on label-relevant tokens
- Proper token sparsification can improve test performance by removing label-irrelevant and/or noisy tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The self-attention layer enables the model to focus computational resources on label-relevant tokens, improving sample complexity.
- Mechanism: During training, the attention weights become increasingly sparse, concentrating on tokens corresponding to discriminative patterns. This sparsification reduces the effective sample complexity by removing noisy or irrelevant tokens.
- Core assumption: The data contains a mixture of label-relevant and label-irrelevant tokens, with label-relevant tokens determining the class label through majority vote.
- Evidence anchors:
  - [abstract] "We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map"
  - [section] "we prove that the attention weights, which are softmax values of each token in the self-attention module, become increasingly sparse during the training"
  - [corpus] Weak - related papers focus on graph transformers or robustness rather than sample complexity analysis

### Mechanism 2
- Claim: Token sparsification methods that remove label-irrelevant or noisy tokens reduce sample complexity.
- Mechanism: By removing tokens that don't contribute to the label decision or add noise, the fraction of label-relevant tokens (α*) increases and noise level (τ) decreases. This directly improves the sample complexity bound which scales with 1/α² and 1/(Θ(1)-τ)².
- Core assumption: The data model with label-relevant and label-irrelevant tokens is accurate, and token sparsification methods can effectively identify and remove the irrelevant/noisy tokens.
- Evidence anchors:
  - [abstract] "this paper indicates that a proper token sparsification can improve the test performance by removing label-irrelevant and/or noisy tokens"
  - [section] "the sample complexity bound indicates that the required number of samples to achieve zero generalization can be reduced if a token sparsification method removes some label-irrelevant tokens"
  - [corpus] Weak - related papers focus on local inductive bias or ensemble unlearning rather than token removal analysis

### Mechanism 3
- Claim: Better initial model (lower σ and δ) leads to improved sample complexity and faster convergence.
- Mechanism: The sample complexity and iteration bounds both scale with the inverse of the initial model error terms σ (value vector error) and δ (query/key vector error). A better initialization reduces these errors, improving the generalization bounds.
- Core assumption: The initial model provides a reasonable starting point for the attention and value vectors, as characterized by the orthonormal basis approximation with bounded error.
- Evidence anchors:
  - [abstract] "Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error"
  - [section] "The initial modelW (0)V, W (0)K, W (0)Q affects the learning performance through σ and δ, both of which decrease as the initial model is improved"
  - [corpus] Weak - related papers don't address initialization impact on sample complexity

## Foundational Learning

- Concept: Non-convex optimization in multi-layer neural networks
  - Why needed here: The ViT model involves non-convex interactions between the self-attention layer and the perceptron layer, requiring specialized analysis techniques beyond standard convex optimization.
  - Quick check question: Why can't we apply standard NTK analysis to this three-layer ViT architecture?

- Concept: Attention mechanism and softmax activation
  - Why needed here: The self-attention module uses softmax to compute attention weights, which creates non-linear interactions that must be carefully analyzed during training.
  - Quick check question: How does the softmax activation in self-attention differ from standard ReLU activations in terms of gradient computation?

- Concept: Sample complexity analysis for structured data
  - Why needed here: The paper provides explicit bounds on the number of training samples needed to achieve zero generalization error, which requires understanding how data structure affects learning requirements.
  - Quick check question: How does the fraction of label-relevant tokens (α*) affect the sample complexity bound?

## Architecture Onboarding

- Component map:
  Input tokens (image patches) → Self-attention layer (WQ, WK, WV matrices) → Attention-weighted values → Two-layer perceptron (WO matrix, ReLU activation) → Output
  Token sparsification can remove some input tokens before processing

- Critical path: Token → Self-attention (softmax computation) → Value projection → Perceptron → Output
  - The attention computation is the most computationally intensive part

- Design tradeoffs:
  - More heads in multi-head attention increases parameter count but may improve learning stability
  - Token sparsification reduces computation but risks removing relevant information
  - Better initialization improves sample complexity but requires additional pretraining

- Failure signatures:
  - Attention weights remain uniform (no sparsity) → model isn't focusing on relevant tokens
  - High hinge loss on training data → model isn't learning discriminative patterns
  - Sample complexity exceeds practical limits → data model assumptions may be violated

- First 3 experiments:
  1. Verify attention weight sparsity increases during training on synthetic data with known label-relevant patterns
  2. Test sample complexity scaling with fraction of label-relevant tokens (α*) on synthetic data
  3. Compare performance with and without self-attention layer on data with varying levels of token noise (τ)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample complexity bound scale with the number of tokens L after token sparsification?
- Basis in paper: [explicit] The sample complexity bound is proportional to 1/(α* - c'(1 - ζ) - c''(σ + τ))² where α* is the fraction of label-relevant tokens. Since token sparsification reduces L by removing label-irrelevant tokens, the bound should depend on L.
- Why unresolved: The paper only analyzes sample complexity for a fixed number of tokens after sparsification, not how it scales with L.
- What evidence would resolve it: A theoretical analysis showing how the sample complexity bound scales with L for different token sparsification methods.

### Open Question 2
- Question: How does the sparsity of the attention map evolve during training for deeper Vision Transformers with multiple layers?
- Basis in paper: [explicit] Proposition 2 proves that the attention map becomes increasingly sparse during training for a shallow ViT with one self-attention layer. The paper states this result generalizes to deeper ViTs.
- Why unresolved: The proof technique relies on the simplicity of a shallow ViT. It's unclear if the same approach can be extended to prove sparsity for deeper architectures.
- What evidence would resolve it: A theoretical analysis showing the sparsity of attention maps for ViTs with multiple self-attention layers.

### Open Question 3
- Question: How does the presence of spurious correlations in the data affect the generalization performance of Vision Transformers?
- Basis in paper: [inferred] The paper mentions that token sparsification can remove spurious correlations to improve testing accuracy. This implies spurious correlations can negatively impact generalization.
- Why unresolved: The paper only provides empirical evidence of improved accuracy after removing spurious correlations. A theoretical analysis of how spurious correlations affect generalization is lacking.
- What evidence would resolve it: A theoretical analysis quantifying the impact of spurious correlations on the sample complexity and generalization error of ViTs.

## Limitations

- Theoretical analysis relies on simplified data model with binary classification of tokens, which may not capture real-world image complexity
- Sample complexity bounds are highly sensitive to parameters (α*, τ, σ, δ) that may be difficult to estimate in practice
- Analysis focuses on shallow ViTs with single self-attention layer, limiting applicability to deeper architectures

## Confidence

**High Confidence Claims:**
- The self-attention mechanism can focus on label-relevant tokens during training, leading to sparse attention weights
- Token sparsification methods can improve sample complexity by removing irrelevant/noisy tokens
- Better initial model parameters (lower σ and δ) lead to improved sample complexity bounds

**Medium Confidence Claims:**
- The specific sample complexity bound of O(1/α²) is tight for this data model
- The three-layer ViT architecture with Hinge loss is optimal for this problem setting
- The SGD convergence guarantees hold for all practical learning rates and batch sizes

**Low Confidence Claims:**
- The theoretical insights directly translate to performance improvements on real image datasets
- The data model assumptions hold for natural images without significant modification
- The bounds remain valid when extending to multi-class classification or regression tasks

## Next Checks

1. **Empirical verification of attention sparsity**: Train the shallow ViT on synthetic data with known label-relevant patterns and measure the evolution of attention weight sparsity during training. Compare with theoretical predictions about when sparsity should emerge and how it relates to learning progress.

2. **Sample complexity scaling experiments**: Systematically vary the fraction of label-relevant tokens (α*) and token noise level (τ) in synthetic datasets, then measure the actual sample complexity needed to achieve zero generalization error. Compare empirical results with theoretical bounds to validate the scaling relationships.

3. **Token sparsification ablation study**: Implement different token sparsification strategies (random removal, attention-based removal, model-based filtering) and evaluate their impact on sample complexity and test performance across datasets with varying levels of label-relevant token fractions. Quantify the trade-off between computational savings and information loss.