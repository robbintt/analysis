---
ver: rpa2
title: 'Query2doc: Query Expansion with Large Language Models'
arxiv_id: '2303.07678'
source_url: https://arxiv.org/abs/2303.07678
tags:
- query
- retrieval
- query2doc
- dense
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simple yet effective query expansion approach,
  denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed
  method first generates pseudo-documents by few-shot prompting large language models
  (LLMs), and then expands the query with generated pseudo-documents.
---

# Query2doc: Query Expansion with Large Language Models

## Quick Facts
- arXiv ID: 2303.07678
- Source URL: https://arxiv.org/abs/2303.07678
- Reference count: 11
- Key outcome: Simple yet effective query expansion approach that improves both sparse and dense retrieval systems using few-shot prompting with large language models.

## Executive Summary
Query2doc introduces a query expansion method that leverages large language models (LLMs) to generate pseudo-documents, which are then concatenated with original queries to improve retrieval performance. The approach is orthogonal to existing dense retrieval improvements and works across different model scales, with larger LLMs providing better pseudo-document quality. Experimental results show consistent gains across BM25, DPR, SimLM, and E5 retrievers on both in-domain and out-of-domain datasets.

## Method Summary
The method generates pseudo-documents using few-shot prompting with LLMs, then concatenates these with original queries for retrieval. The approach is model-agnostic and works with both sparse (BM25) and dense (DPR, SimLM, E5) retrieval systems without requiring fine-tuning of the underlying models.

## Key Results
- Boosts BM25 performance by 3% to 15% on MS-MARCO and TREC DL datasets
- Improves state-of-the-art dense retrievers both in-domain and out-of-domain
- Performance scales with LLM model size, with 175B models outperforming smaller variants
- Works consistently across different training regimes and retrieval architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query2doc reduces lexical mismatch between queries and documents by generating relevant context.
- Mechanism: LLMs memorize knowledge from web-scale corpora and generate pseudo-documents that contain terms and concepts directly related to the query, filling gaps in short or ambiguous queries.
- Core assumption: The generated pseudo-document semantically overlaps with relevant ground truth documents even when exact term overlap is low.
- Evidence anchors:
  - [abstract]: "The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers."
  - [section]: "Our manual inspection reveals that most queries from the TREC DL track are long-tailed entity-centric queries, which benefit more from the exact lexical match."
- Break condition: If the LLM fails to generate relevant pseudo-documents (e.g., due to model scale or prompt quality), the lexical bridging benefit disappears.

### Mechanism 2
- Claim: Query2doc is orthogonal to existing dense retrieval improvements, providing consistent gains across different training regimes.
- Mechanism: Since query2doc operates at the query expansion stage rather than during model training, it can be layered on top of methods like hard negative mining, intermediate pre-training, or knowledge distillation without interfering with their optimization objectives.
- Core assumption: The dense retriever can effectively learn to match the expanded query (original query + pseudo-document) to relevant documents.
- Evidence anchors:
  - [abstract]: "our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results."
  - [section]: "Figure 2 presents a comparison... The results show that the 'DPR + query2doc' variant consistently outperforms the DPR baseline by approximately 1%, regardless of the amount of data used for fine-tuning."
- Break condition: If the dense retriever's embedding space is too rigid or the pseudo-document is too long relative to the original query, the added context may dilute rather than enhance the signal.

### Mechanism 3
- Claim: Model scale of the LLM critically determines the quality of generated pseudo-documents.
- Mechanism: Larger LLMs (e.g., 175B parameters) have richer knowledge representations and better instruction following, leading to more accurate, relevant, and longer pseudo-documents compared to smaller models.
- Core assumption: The improvements in pseudo-document quality from larger models directly translate into better retrieval performance.
- Evidence anchors:
  - [abstract]: "Further analysis also reveals the importance of model scales: query2doc works best when combined with the most capable LLMs while small language models only provide marginal improvements over baselines."
  - [section]: "Table 3 shows that the performance steadily improves as we go from the 1.3B model to 175B models."
- Break condition: If the prompt is poorly constructed or the task is outside the LLM's knowledge cutoff, even a large model may fail to generate useful pseudo-documents.

## Foundational Learning

- Concept: Few-shot prompting with LLMs
  - Why needed here: Enables generation of pseudo-documents without fine-tuning, leveraging LLMs' pre-existing knowledge.
  - Quick check question: What is the role of in-context examples in few-shot prompting for query expansion?

- Concept: Dense retrieval with contrastive loss
  - Why needed here: Understanding how DPR and similar models optimize query-document similarity is key to interpreting why query expansion helps.
  - Quick check question: In Equation 3, what does the negative sampling term represent?

- Concept: Knowledge distillation in dense retrieval
  - Why needed here: Explains why gains from query2doc diminish when a strong cross-encoder teacher is already used.
  - Quick check question: How does the KL divergence term in Equation 4 influence the student model's training?

## Architecture Onboarding

- Component map:
  Query input → Few-shot prompt → LLM generation (pseudo-document) → Query concatenation → Retrieval model (BM25 or dense) → Ranking output
  Training pipeline for dense retrievers remains unchanged except for longer query sequences.

- Critical path:
  Prompt generation → LLM API call latency → Query expansion → Retrieval latency (increases with query length)

- Design tradeoffs:
  - Latency vs. retrieval quality: LLM calls add significant overhead; expanding queries increases index search time.
  - Cost vs. benefit: Larger LLMs improve quality but are more expensive; smaller models are faster but less effective.
  - Exact match vs. semantic matching: Query2doc favors sparse retrieval but also helps dense retrieval by providing richer context.

- Failure signatures:
  - No improvement over baseline: Likely due to poor pseudo-document quality (wrong model scale or bad prompts).
  - Performance degradation: Possible semantic drift if pseudo-document introduces irrelevant or misleading context.
  - High latency or cost: LLM API bottlenecks or excessive query expansion token usage.

- First 3 experiments:
  1. Compare BM25 + query2doc vs. BM25 + docT5query on MS-MARCO dev set to isolate pseudo-document quality impact.
  2. Test DPR + query2doc with different numbers of in-context examples (k=1, 2, 4, 8) to find optimal prompt diversity.
  3. Evaluate zero-shot transfer from MS-MARCO to NFCorpus with both BM25 and dense retrievers to measure out-of-domain robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of query2doc change when using different model scales of LLMs beyond those tested in the paper?
- Basis in paper: [explicit] The paper states that the performance steadily improves as the model scale increases from 1.3B to 175B parameters, but it does not explore models beyond these sizes or test the limits of this scaling effect.
- Why unresolved: The paper does not provide information on the performance impact of using even larger models or whether there is a point of diminishing returns.
- What evidence would resolve it: Conducting experiments with even larger LLMs or exploring the upper limits of model scaling would provide insights into the optimal model size for query expansion.

### Open Question 2
- Question: How does the latency of retrieval systems with query2doc compare to systems without it in real-world deployment scenarios?
- Basis in paper: [explicit] The paper mentions that the latency of retrieval systems with query2doc is considerably slower due to the autoregressive decoding of LLMs and the increased number of query terms after expansion.
- Why unresolved: The paper does not provide a comprehensive analysis of the latency in various real-world deployment scenarios, including different hardware configurations and network conditions.
- What evidence would resolve it: Conducting extensive latency tests in different real-world deployment scenarios would provide a clearer understanding of the trade-offs between performance and latency.

### Open Question 3
- Question: How does the quality of pseudo-documents generated by LLMs affect the performance of query2doc across different types of queries and domains?
- Basis in paper: [explicit] The paper demonstrates that query2doc improves performance across various retrieval models and datasets, but it does not provide a detailed analysis of how the quality of pseudo-documents affects performance for different query types and domains.
- Why unresolved: The paper does not explore the relationship between pseudo-document quality and performance in depth, nor does it investigate how this relationship varies across different query types and domains.
- What evidence would resolve it: Conducting experiments with pseudo-documents of varying quality and across different query types and domains would provide insights into the factors that influence the effectiveness of query2doc.

## Limitations
- Limited to tested datasets (MS-MARCO, TREC DL) without broader generalization validation
- Performance claims lack theoretical guarantees and are based on empirical results only
- Key implementation details (prompt format, number of examples) are underspecified
- No comprehensive cost-benefit analysis for real-world deployment

## Confidence
- High confidence: Experimental results showing consistent improvements over BM25 and dense retrievers on tested datasets
- Medium confidence: Model scale is critical for pseudo-document quality, based on internal experiments but lacking external validation
- Low confidence: Query2doc is "orthogonal" to dense retrieval training improvements, inferred rather than proven through systematic ablation

## Next Checks
1. Test query2doc on additional IR datasets (e.g., Robust04, ClueWeb) to confirm scalability beyond MS-MARCO and TREC DL
2. Measure latency and monetary cost of LLM API calls for query expansion and compare against retrieval quality gains
3. Conduct controlled experiments to isolate effects of lexical overlap, semantic enrichment, and context length on retrieval performance