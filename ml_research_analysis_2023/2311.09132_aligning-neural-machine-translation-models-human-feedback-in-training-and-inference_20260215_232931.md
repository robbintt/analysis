---
ver: rpa2
title: 'Aligning Neural Machine Translation Models: Human Feedback in Training and
  Inference'
arxiv_id: '2311.09132'
source_url: https://arxiv.org/abs/2311.09132
tags:
- training
- translation
- quality
- metrics
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates how to integrate quality metrics as reward
  models into the neural machine translation (NMT) pipeline, focusing on three stages:
  data filtering, training with reinforcement learning (RL), and inference via reranking.
  A key contribution is the use of COMET-QE for quality-aware data filtering, which
  helps stabilize RL training by reducing noise in the training set.'
---

# Aligning Neural Machine Translation Models: Human Feedback in Training and Inference

## Quick Facts
- arXiv ID: 2311.09132
- Source URL: https://arxiv.org/abs/2311.09132
- Reference count: 19
- Primary result: RL training with neural reward models like COMET and COMET-QE significantly improves translation quality, with quality-aware data filtering using COMET-QE helping stabilize RL training.

## Executive Summary
This study explores how to integrate quality metrics as reward models into the neural machine translation (NMT) pipeline, focusing on three key stages: data filtering, training with reinforcement learning (RL), and inference via reranking. The authors propose using COMET-QE for quality-aware data filtering to reduce noise in the training set, which empirically helps stabilize RL training. They demonstrate that neural metrics like COMET and COMET-QE serve as more effective reward signals than traditional BLEU-based rewards during RL training. The combination of RL training with minimum Bayes risk (MBR) decoding yields consistent improvements across multiple evaluation metrics, outperforming traditional approaches.

## Method Summary
The method integrates quality metrics into the NMT pipeline through three stages: (1) data filtering using COMET-QE to remove low-quality sentence pairs and reduce training noise, (2) MLE fine-tuning of a T5-Large model followed by RL training using PPO with neural metrics (COMET/COMET-QE) as rewards, and (3) inference via N-best reranking and MBR decoding using quality metrics. The approach is evaluated on IWSLT2017 (EN→DE/FR) and WMT datasets (WMT16 EN→DE, WMT15 EN→FR) using multiple metrics including BLEU, chrF, METEOR, COMET, COMET-QE, and BLEURT.

## Key Results
- RL training with neural reward models (COMET/COMET-QE) significantly outperforms traditional BLEU-based rewards across multiple evaluation metrics
- Quality-aware data filtering using COMET-QE helps stabilize RL training by reducing noise in the training set
- Combining RL training with MBR decoding yields more consistent improvements across various evaluation metrics than either approach alone
- Reference-free QE models like COMET-QE are competitive with reference-based metrics and enable unsupervised NMT training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quality-aware data filtering using COMET-QE reduces noise in the training set, which stabilizes RL training and improves translation quality.
- Mechanism: COMET-QE assigns quality scores to sentence pairs; filtering out low-scoring pairs reduces the proportion of noisy or irrelevant training data, leading to cleaner learning signals for RL.
- Core assumption: Lower-quality sentence pairs negatively impact RL training stability and performance.
- Evidence anchors:
  - [abstract] "A key contribution is the use of COMET-QE for quality-aware data filtering, which helps stabilize RL training by reducing noise in the training set."
  - [section] "we propose an alternative data filtering method that uses COMET-QE (Rei et al., 2020), a more robust model, to curate a high-quality dataset that empirically helps to minimize RL training instability."
  - [corpus] Weak - related papers focus on reward models and reranking, not data filtering specifics.
- Break condition: If the quality estimation model itself is poorly aligned with human judgments, filtering could remove useful data or retain low-quality pairs.

### Mechanism 2
- Claim: Neural metrics like COMET and COMET-QE provide more effective reward signals for RL training than BLEU.
- Mechanism: Neural metrics capture semantic similarity better than BLEU, offering richer, more informative feedback during RL policy updates, leading to improved translation quality.
- Core assumption: Reward models that better align with human preferences lead to better RL training outcomes.
- Evidence anchors:
  - [abstract] "The experiments show that RL training with neural reward models like COMET and COMET-QE significantly improves translation quality across multiple metrics, outperforming traditional BLEU-based rewards."
  - [section] "we leverage robust preference models during RL training, such as the reference-based COMET and the reference-free COMET-QE...to ensure that RL systems can better capture the nuanced preferences of the user by receiving human-like feedback as rewards."
  - [corpus] Weak - related papers discuss reward models but not direct comparisons with BLEU in RL training.
- Break condition: If the neural metric reward model overfits to specific data or reference translations, it may not generalize well.

### Mechanism 3
- Claim: Combining RL training with minimum Bayes risk (MBR) decoding yields more consistent improvements across evaluation metrics.
- Mechanism: RL training optimizes the model to produce high-quality translations, while MBR decoding re-ranks candidates to select the most probable high-quality output according to a reference-based metric, combining strengths of both approaches.
- Core assumption: RL and MBR target different aspects of translation quality and their combination is complementary.
- Evidence anchors:
  - [abstract] "Combining RL training with minimum Bayes risk (MBR) decoding yields consistent gains."
  - [section] "Experiments in EN→DE and EN→FR show that both RL training and reranking techniques enhance translation quality, with RL training often outperforming reranking methods. Furthermore, combining RL and MBR decoding results in more consistent improvements across various evaluation metrics."
  - [corpus] Weak - related papers discuss reranking and MBR but not their combination with RL training.
- Break condition: If RL training and MBR decoding optimize for conflicting objectives, combining them may not yield additional benefits.

## Foundational Learning

- Concept: Reinforcement Learning (RL) in the context of neural machine translation.
  - Why needed here: The study uses RL to train NMT models by treating translation as a Markov decision process and optimizing expected reward.
  - Quick check question: How does the reward function in RL for NMT differ from the MLE loss, and why is this important?

- Concept: Quality Estimation (QE) metrics.
  - Why needed here: QE metrics like COMET-QE assess translation quality without reference translations, enabling reference-free training and evaluation.
  - Quick check question: What is the key advantage of using QE metrics over reference-based metrics during training?

- Concept: Minimum Bayes Risk (MBR) decoding.
  - Why needed here: MBR decoding selects the translation that maximizes expected utility with respect to a reference-based metric, improving final output quality.
  - Quick check question: How does MBR decoding's computational cost scale with the number of candidates, and what implication does this have for its use?

## Architecture Onboarding

- Component map: Data filtering → MLE fine-tuning → RL training → MBR decoding/reranking → evaluation
- Critical path: Data filtering → RL training (with PPO) → inference (MBR/decoding) → evaluation
- Design tradeoffs: RL training vs. MBR decoding (computational efficiency vs. quality); neural metrics vs. BLEU (alignment with human judgment vs. simplicity)
- Failure signatures: RL training instability (gradient variance, poor reward signals); MBR decoding inefficiency (quadratic cost in candidates); data filtering removing too much data
- First 3 experiments:
  1. Apply COMET-QE filtering to WMT16 EN→DE dataset; vary training subset sizes; evaluate impact on MLE baseline
  2. Train RL model using COMET reward on filtered data; compare to MLE and RL with BLEU reward
  3. Combine RL-trained model with MBR decoding using COMET; compare to RL-only and MBR-only models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between aggressive data filtering and maintaining sufficient training data size for reinforcement learning in NMT?
- Basis in paper: [explicit] The paper discusses the importance of balancing aggressive filtering with dataset size to avoid reducing training data too much, which could be detrimental to NMT performance.
- Why unresolved: While the paper shows performance trends with different subset sizes, it doesn't provide a universal guideline for the optimal filtering threshold that works across all datasets and language pairs.
- What evidence would resolve it: Systematic experiments across multiple diverse datasets with varying sizes and characteristics to establish a data-dependent filtering threshold that maximizes RL performance without compromising training data quantity.

### Open Question 2
- Question: How do neural metrics like COMET-QE perform as reward models in unsupervised NMT training compared to traditional supervised approaches?
- Basis in paper: [inferred] The paper suggests that COMET-QE could enable unsupervised NMT training by eliminating the need for reference translations, but doesn't directly test this scenario.
- Why unresolved: The paper demonstrates that COMET-QE works well as a reward model in supervised settings but doesn't explore its effectiveness in truly unsupervised scenarios where only monolingual data is available.
- What evidence would resolve it: Experiments training NMT models using only monolingual data with COMET-QE as the sole quality signal, comparing results to supervised baselines and traditional unsupervised methods.

### Open Question 3
- Question: What are the long-term stability implications of combining RL training with MBR decoding at inference time?
- Basis in paper: [explicit] The paper notes that combining RL and MBR sometimes leads to top performance but doesn't consistently outperform other strategies, suggesting potential overfitting effects.
- Why unresolved: The paper only evaluates immediate translation quality improvements, not the long-term effects on model stability, robustness to different domains, or degradation over time.
- What evidence would resolve it: Longitudinal studies tracking model performance across diverse test sets, different domains, and over extended training/inference cycles to assess whether the combined approach maintains its advantages or shows signs of degradation.

## Limitations

- The effectiveness of COMET-QE data filtering depends heavily on the quality and domain alignment of the QE model itself, with limited analysis of how filtering thresholds impact downstream performance
- MBR decoding's quadratic computational cost with respect to candidate set size may limit practical deployment despite quality improvements
- While the paper shows reference-free QE models enable unsupervised training, the actual implementation details and performance gaps versus supervised approaches are not fully explored

## Confidence

- **High confidence**: The empirical improvements from combining RL training with neural reward models (COMET/COMET-QE) are well-supported by the results across multiple evaluation metrics
- **Medium confidence**: The mechanism by which quality-aware data filtering stabilizes RL training is plausible but relies on untested assumptions about noise distribution in training data
- **Medium confidence**: The claim that combining RL training with MBR decoding yields consistent gains assumes the objectives are complementary, though this combination hasn't been extensively validated across diverse domains

## Next Checks

1. Test data filtering with different QE model thresholds and analyze the trade-off between data retention and RL stability across multiple domains
2. Benchmark MBR decoding performance with varying candidate set sizes to establish practical computational limits and quality-cost trade-offs
3. Validate whether the neural reward model benefits generalize beyond WMT and IWSLT datasets to low-resource or domain-specific translation tasks