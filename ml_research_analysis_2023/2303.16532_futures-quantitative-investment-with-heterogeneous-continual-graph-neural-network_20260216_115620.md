---
ver: rpa2
title: Futures Quantitative Investment with Heterogeneous Continual Graph Neural Network
arxiv_id: '2303.16532'
source_url: https://arxiv.org/abs/2303.16532
tags:
- tasks
- learning
- futures
- neural
- price
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses high-frequency futures price prediction by
  proposing a continual learning framework with four heterogeneous tasks (price forecasting,
  gap regression, moving average regression, and change-point detection) to capture
  both short- and long-term trends. A novel Heterogeneous Continual Graph Neural Network
  (HCGNN) is introduced, which uses mutual information between original observations
  and extracted features to calculate parameter importance, addressing the catastrophic
  forgetting problem in multi-task settings.
---

# Futures Quantitative Investment with Heterogeneous Continual Graph Neural Network

## Quick Facts
- arXiv ID: 2303.16532
- Source URL: https://arxiv.org/abs/2303.16532
- Reference count: 10
- Primary result: HCGNN achieves MAE of 6.48, RMSE of 12.40, and MAPE of 0.04 for 1-minute futures price predictions on 49 Chinese commodity futures

## Executive Summary
This paper addresses high-frequency futures price prediction by proposing a continual learning framework with four heterogeneous tasks (price forecasting, gap regression, moving average regression, and change-point detection) to capture both short- and long-term trends. A novel Heterogeneous Continual Graph Neural Network (HCGNN) is introduced, which uses mutual information between original observations and extracted features to calculate parameter importance, addressing the catastrophic forgetting problem in multi-task settings. The model is tested on 49 commodity futures in China's market, achieving superior performance over state-of-the-art methods with MAE of 6.48, RMSE of 12.40, and MAPE of 0.04 for 1-minute predictions, and MAE of 13.70, RMSE of 42.15, and MAPE of 0.1 for 15-minute predictions. The approach effectively balances short- and long-term considerations while maintaining cross-sectional correlations among futures.

## Method Summary
The HCGNN framework processes multivariate futures data through a spatial-temporal graph neural network (STGNN) backbone that captures both cross-sectional correlations among futures and temporal dependencies within each future. Four heterogeneous tasks are trained in a continual learning manner: price forecasting, gap regression, moving average regression, and change-point detection. The key innovation is using mutual information maximization between original observations and extracted features to calculate parameter importance for regularization, replacing traditional loss gradient-based methods. This approach addresses catastrophic forgetting by providing a more stable importance signal across heterogeneous task losses. The model is trained on tick-level high-frequency data from 49 Chinese commodity futures, with performance evaluated using regression metrics (MAE, RMSE, MAPE) and classification metrics (precision, recall, accuracy, F1) for change-point detection.

## Key Results
- Achieves MAE of 6.48, RMSE of 12.40, and MAPE of 0.04 for 1-minute futures price predictions
- Outperforms state-of-the-art methods with MAE of 13.70, RMSE of 42.15, and MAPE of 0.1 for 15-minute predictions
- Demonstrates effective balance between short-term fluctuations and long-term trends through four heterogeneous task framework
- Shows strong cross-sectional correlation modeling through spatial-temporal graph neural network architecture

## Why This Works (Mechanism)

### Mechanism 1: Mutual Information-Based Parameter Importance
The model calculates parameter importance using gradients of mutual information between inputs and features rather than loss gradients. This provides a scale-invariant importance signal that remains stable across heterogeneous tasks with different loss scales. The mutual information gradients smooth out task-specific variations, preventing any single task from dominating the regularization process.

### Mechanism 2: Heterogeneous Task Complementarity
Four tasks (price forecasting, gap regression, moving average regression, change-point detection) target different temporal aspects of futures price dynamics. Price forecasting captures overall trends, gap regression focuses on short-term fluctuations, MA regression identifies intermediate-term patterns, and CPD detects trend reversals. This multi-task approach ensures comprehensive feature learning that single-task models miss.

### Mechanism 3: Spatial-Temporal Graph Neural Network
The STGNN architecture captures both cross-sectional correlations among futures (spatial dimension) and temporal dependencies within each future (temporal dimension). By modeling futures prices as a graph where nodes represent different commodities with interdependent relationships, the model leverages market-wide information rather than treating each future in isolation.

## Foundational Learning

- **Concept**: Continual learning with catastrophic forgetting mitigation
  - Why needed: The model trains on four heterogeneous tasks sequentially, requiring preservation of knowledge from earlier tasks while learning new ones
  - Quick check: What is the key difference between EWC and the mutual information-based approach proposed in this paper?

- **Concept**: Graph neural networks for multivariate time series
  - Why needed: Futures prices show strong cross-sectional correlations that traditional sequential models cannot capture effectively
  - Quick check: How does an STGNN differ from a standard GNN when applied to time series data?

- **Concept**: Mutual information maximization
  - Why needed: Provides a scale-invariant importance signal for parameter regularization across heterogeneous tasks
  - Quick check: What is the relationship between InfoNCE loss and mutual information?

## Architecture Onboarding

- **Component map**: Data → STGNN → Feature extraction → Task heads → Loss computation → Mutual information regularization → Parameter updates
- **Critical path**: High-frequency futures data flows through STGNN backbone, extracts features, passes through task-specific heads, computes losses, applies mutual information-based regularization, and updates parameters
- **Design tradeoffs**: Single STGNN backbone vs. separate networks per task (chosen for parameter efficiency), mutual information vs. loss gradients for Ω (chosen for stability), residual connections in STGNN (chosen to prevent degradation)
- **Failure signatures**: Catastrophic forgetting (degraded earlier task performance), task interference (performance degradation when training multiple tasks), graph construction issues (poor performance if cross-sectional correlations are weak)
- **First 3 experiments**: 1) Train single-task model vs. HCGNN to quantify multi-task benefits, 2) Replace mutual information-based Ω with loss gradient Ω to validate design choice, 3) Remove STGNN and use separate LSTMs to demonstrate value of cross-sectional correlations

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of HCGNN compare to other state-of-the-art methods on other types of financial time series data, such as stock, bond, forex, or option data? The paper concludes with a suggestion to explore the applicability of HCGNN in other financial markets, indicating that this comparison has not yet been made.

### Open Question 2
What is the impact of the window length l in the gap regression task on the model's performance? The paper mentions the use of a fixed time window in the gap regression task but does not explore the impact of varying this window length on the model's ability to capture short-term features and its overall performance.

### Open Question 3
How does the proposed mutual information-based parameter strength compare to other methods for addressing catastrophic forgetting in heterogeneous multi-task settings? The paper introduces a mutual information-based method to address catastrophic forgetting in the context of heterogeneous multi-task learning but does not compare it to other methods for this specific problem.

## Limitations

- **Graph Construction Uncertainty**: The paper does not specify how the adjacency matrix for futures relationships is constructed, which is critical for the STGNN component's effectiveness
- **Implementation Specificity**: Exact implementation details for mutual information calculation and spatial-temporal modules are not fully specified
- **Market Specificity**: Model is trained and evaluated on Chinese commodity futures data from a specific time period, raising generalization concerns

## Confidence

- **High Confidence**: The general approach of using continual learning with mutual information-based parameter importance for heterogeneous tasks is well-founded and the reported performance metrics are plausible
- **Medium Confidence**: The specific design choices (four particular tasks, mutual information vs loss gradients) are reasonable but not definitively proven optimal
- **Low Confidence**: The exact implementation details for graph construction and mutual information calculation, which are critical for reproducing the results

## Next Checks

1. **Graph Construction Validation**: Test the model with different graph construction methods (correlation-based, industry-based, random) to determine how sensitive the performance is to the specific choice of adjacency matrix

2. **Mutual Information Implementation**: Compare the proposed mutual information-based parameter importance against alternative methods (loss gradient-based, random) on a subset of the data to verify that the claimed stability benefits are realized

3. **Cross-Market Generalization**: Evaluate the trained model on futures data from a different market (e.g., US commodities) or time period to assess whether the learned representations generalize beyond the training domain