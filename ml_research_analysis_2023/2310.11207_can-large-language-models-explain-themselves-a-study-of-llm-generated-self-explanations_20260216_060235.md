---
ver: rpa2
title: Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations
arxiv_id: '2310.11207'
source_url: https://arxiv.org/abs/2310.11207
tags:
- explanations
- explanation
- review
- prediction
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the quality of feature attribution explanations
  generated by large language models (LLMs) like ChatGPT for sentiment analysis tasks.
  The authors study two explanation paradigms - explaining-then-predicting (E-P) and
  predicting-then-explaining (P-E) - and compare them with traditional interpretability
  methods like occlusion and LIME.
---

# Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations

## Quick Facts
- arXiv ID: 2310.11207
- Source URL: https://arxiv.org/abs/2310.11207
- Reference count: 40
- Key outcome: ChatGPT self-explanations perform on par with traditional methods but differ significantly in agreement, with well-rounded predictions suggesting current interpretability practices may need rethinking

## Executive Summary
This paper investigates the quality of feature attribution explanations generated by large language models (LLMs) like ChatGPT for sentiment analysis tasks. The authors compare two explanation paradigms - explaining-then-predicting (E-P) and predicting-then-explaining (P-E) - against traditional interpretability methods like occlusion and LIME. Through extensive experiments on the SST dataset, they find that ChatGPT's self-explanations perform comparably to traditional methods in faithfulness metrics, but show high disagreement with them. Notably, the model produces well-rounded prediction values that are insensitive to word removal, suggesting current interpretability practices may be ill-suited for these human-like reasoning models.

## Method Summary
The study uses the SST dataset with a subset of 100 sentences for testing. The authors prompt ChatGPT using two paradigms - explain-then-predict (E-P) and predict-then-explain (P-E) - to generate self-explanations. These are compared with traditional methods (occlusion and LIME) using faithfulness evaluation metrics (comprehensiveness, sufficiency, decision flip rate, minimum fraction of tokens for decision flip, rank correlation) and agreement metrics (rank agreement, rank correlation, sign agreement, signed rank agreement, feature agreement, IoU). The study also examines the effects of different prompting strategies and the granularity of attribution values.

## Key Results
- ChatGPT's self-explanations perform on par with traditional interpretability methods in faithfulness metrics
- Explanations show high disagreement with traditional methods despite similar faithfulness scores
- Model predictions are often "well-rounded" (e.g., 0.5, 0.75) and insensitive to word removal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's self-explanations perform on par with traditional interpretability methods in terms of faithfulness metrics.
- Mechanism: The model's alignment to human-like reasoning patterns allows it to generate explanations that align with human expectations, even if they don't precisely reflect the model's internal decision process.
- Core assumption: The model's instruction tuning and reinforcement learning from human feedback (RLHF) creates explanations that are intuitively reasonable to humans, making them appear faithful even if the underlying mechanism differs from traditional methods.
- Evidence anchors: [abstract]: "ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics"; [section]: "Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics"
- Break condition: If the model's predictions become more fine-grained or if the evaluation metrics are redesigned to capture the actual decision process rather than human-intuition alignment

### Mechanism 2
- Claim: The model's prediction values are highly "well-rounded" (e.g., 0.5, 0.75) and insensitive to word removal, suggesting current interpretability practices may be ill-suited for these models.
- Mechanism: The model's auto-regressive nature combined with alignment training creates a system where predictions are intentionally vague to mimic human-like uncertainty, making traditional perturbation-based evaluation methods ineffective.
- Core assumption: The model is designed to produce outputs that are plausible to humans rather than precise mathematical representations of its internal state.
- Evidence anchors: [section]: "we notice that the model's prediction values are often 'well-rounded' and being one of several distinct values" and "the removal of a few words often does not change the model's prediction"; [abstract]: "Interestingly, the model's predictions and explanations are often 'well-rounded' (e.g., 0.5, 0.75) and insensitive to word removal"
- Break condition: If the model is prompted to provide more granular predictions or if evaluation metrics are adapted to account for this human-like vagueness

### Mechanism 3
- Claim: The high disagreement among different explanation methods suggests there may be explanations that are significantly more faithful than currently discovered ones.
- Mechanism: The current explanation methods may all be capturing different aspects of the model's reasoning, with none fully representing the true decision process, leading to high pairwise disagreement despite similar faithfulness scores.
- Core assumption: The model's reasoning process is complex and multifaceted, and current explanation methods are only partially capturing this complexity.
- Evidence anchors: [section]: "high disagreement among every pair of explanations is exhibited" and "explanations that perform very differently on faithfulness metrics (for classifier models) have high disagreement"; [abstract]: "are quite different from them according to various agreement metrics"
- Break condition: If new explanation methods are developed that achieve both high faithfulness and high agreement with other methods, or if the model's reasoning process becomes more transparent

## Foundational Learning

- Concept: Auto-regressive language model generation
  - Why needed here: Understanding how ChatGPT generates text sequentially is crucial for interpreting how it produces explanations alongside predictions
  - Quick check question: How does the context window affect the model's ability to reference previous parts of its own explanation when generating subsequent tokens?

- Concept: Feature attribution explanations and saliency maps
  - Why needed here: The paper compares self-generated explanations to traditional methods like occlusion and LIME, which rely on understanding how feature importance is typically calculated
  - Quick check question: What's the fundamental difference between gradient-based and perturbation-based feature attribution methods, and why can't the former be used with ChatGPT?

- Concept: Faithfulness vs. plausibility in explanations
  - Why needed here: The paper distinguishes between explanations that accurately reflect the model's reasoning process (faithfulness) and those that seem reasonable to humans (plausibility), which is central to understanding the results
  - Quick check question: Can an explanation be highly plausible to humans while being completely unfaithful to the model's actual reasoning process?

## Architecture Onboarding

- Component map: SST dataset -> ChatGPT model (auto-regressive + RLHF) -> Prompt engineering (E-P vs P-E, full vs top-k) -> Traditional methods (occlusion, LIME) -> Faithfulness and agreement metrics
- Critical path: Input sentence → model prediction + explanation generation → extraction of explanation values → comparison with traditional methods → evaluation using faithfulness and agreement metrics
- Design tradeoffs: Zero-shot prompting produces more human-like, well-rounded explanations but may sacrifice faithfulness, while few-shot prompting with fine-grained examples produces more varied values but introduces memorization effects; LIME provides more nuanced explanations but at 5000x the computational cost
- Failure signatures: High disagreement among explanation methods despite similar faithfulness scores suggests the explanations may not be capturing the true reasoning process; well-rounded prediction values that don't change with word removal indicate the evaluation metrics may be ill-suited for this model type
- First 3 experiments:
  1. Compare self-explanations from E-P and P-E setups on the same input sentences to quantify the effect of explanation-before-prediction vs prediction-before-explanation ordering
  2. Generate explanations using few-shot prompting with fine-grained examples and compare faithfulness metrics to zero-shot results to isolate the effect of value granularity
  3. Remove the top-k most important words according to different explanation methods and measure the change in model prediction to validate which method best identifies truly important features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do fine-grained attribution values provide more faithful explanations than well-rounded values in ChatGPT?
- Basis in paper: [explicit] The paper notes that ChatGPT tends to produce well-rounded values (e.g., 0.5, 0.75) rather than arbitrary ones, and suggests this may be due to its attempt to mimic human reasoning. It proposes testing whether this affects the understandability and faithfulness of explanations.
- Why unresolved: The paper acknowledges this as a hypothesis but does not test it experimentally. Human subject studies would be required to evaluate understandability, and faithfulness would need to be compared against models with fine-grained values.
- What evidence would resolve it: Empirical comparison of human comprehension and model prediction accuracy between well-rounded and fine-grained attribution values in both aligned and unaligned models.

### Open Question 2
- Question: Are there explanation methods that achieve significantly higher faithfulness scores than occlusion, LIME, and self-explanations for ChatGPT?
- Basis in paper: [explicit] The paper finds that all explanation methods perform similarly on faithfulness metrics despite high disagreement, suggesting there may be better explanations that current methods miss.
- Why unresolved: The paper only evaluates three methods and does not explore the full space of possible explanation techniques for LLMs.
- What evidence would resolve it: Systematic evaluation of diverse explanation methods (e.g., counterfactual, concept-based) on faithfulness metrics, identifying any that significantly outperform current approaches.

### Open Question 3
- Question: How does ChatGPT's reasoning ability affect the validity of traditional interpretability metrics like comprehensiveness and sufficiency?
- Basis in paper: [explicit] The paper observes that ChatGPT's predictions are insensitive to word removal, making metrics like comprehensiveness and sufficiency less meaningful. It suggests these metrics may be ill-posed for human-like reasoning models.
- Why unresolved: The paper identifies the problem but does not propose alternative evaluation frameworks or test whether modified metrics work better.
- What evidence would resolve it: Development and validation of new interpretability metrics designed specifically for LLMs, tested against both LLM and traditional model explanations.

## Limitations

- The proprietary nature of ChatGPT prevents full reproduction of the experimental setup and access to token probability outputs
- The study's reliance on a small subset (100 sentences) from SST may limit generalizability of findings
- The well-rounded prediction phenomenon and insensitivity to word removal suggests current evaluation frameworks may be inadequate, but no alternative framework is proposed

## Confidence

- **High confidence**: The observation that ChatGPT's self-explanations perform on par with traditional methods in faithfulness metrics is well-supported by the experimental results and multiple evaluation measures.
- **Medium confidence**: The claim about high disagreement among explanation methods is robust, but the interpretation that this suggests undiscovered more faithful explanations is speculative and not directly tested.
- **Low confidence**: The conclusion that current interpretability practices may be ill-suited for human-like reasoning models is largely based on the observation of well-rounded predictions, but lacks a proposed alternative framework for evaluation.

## Next Checks

1. **Expand evaluation dataset**: Test the same explanation methods on a larger, more diverse set of sentiment analysis datasets (IMDB, Yelp reviews) to verify if the well-rounded prediction phenomenon persists across different domains and if faithfulness/agreement patterns remain consistent.

2. **Test alternative prompting strategies**: Implement few-shot prompting with fine-grained confidence scores (0.1, 0.3, 0.7, 0.9) to see if this eliminates the well-rounded prediction effect and improves sensitivity to word removal, then re-evaluate faithfulness metrics.

3. **Compare with other LLMs**: Run the same experiments with open-source LLMs like LLaMA or GPT-Neo to determine if the well-rounded prediction and explanation patterns are specific to ChatGPT's RLHF fine-tuning or represent a broader property of autoregressive language models.