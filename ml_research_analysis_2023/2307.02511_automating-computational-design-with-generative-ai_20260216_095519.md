---
ver: rpa2
title: Automating Computational Design with Generative AI
arxiv_id: '2307.02511'
source_url: https://arxiv.org/abs/2307.02511
tags:
- diffusion
- design
- floor
- plans
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the use of diffusion models for computational
  design at the example of floor plans. The authors analyze the current state of the
  art and propose a novel approach to improve fine-tuning of diffusion models through
  semantic encoding.
---

# Automating Computational Design with Generative AI

## Quick Facts
- arXiv ID: 2307.02511
- Source URL: https://arxiv.org/abs/2307.02511
- Reference count: 40
- Primary result: Fine-tuning diffusion models with semantic encoding improves floor plan generation validity from 6% to 90%

## Executive Summary
This paper evaluates the use of diffusion models for computational design through the specific case study of floor plan generation. The authors propose a novel approach using semantic encoding to improve fine-tuning of diffusion models, achieving a significant improvement in validity from 6% to 90%. While the approach shows promise for generating floor plans from text prompts, the models still struggle with satisfying complex architectural requirements. The research identifies several future directions for advancing computational design using generative AI.

## Method Summary
The authors developed a novel approach to fine-tune diffusion models for floor plan generation using semantic encoding. They generated training data algorithmically, creating 100 sample floor plans with various building shapes and room arrangements. These plans were encoded using four strategies: R (regular), SR (simplified room), SE (semantic encoding), and SRE (simplified room with semantic encoding). The Stable Diffusion v2.1 model was fine-tuned using textual inversion for each encoding style. The evaluation used 43 test prompts covering validity, quantification, removal, recoloring, and arrangement tasks, with manual validation of generated results.

## Key Results
- Fine-tuning significantly improves floor plan generation validity from 6% to 90%
- Semantic encoding strategies (Simplify, Encode, Contrast) enable better understanding of spatial relationships
- Models perform well on basic room arrangement queries but struggle with complex requirements
- Fine-tuning with textual inversion provides an efficient alternative to full model retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning diffusion models with semantic encoding improves floor plan generation validity from 6% to 90%
- Mechanism: The model learns semantic meaning of architectural elements through color-coded representations and simplified geometry, enabling better understanding of spatial relationships
- Core assumption: Color-coded semantic encoding helps the diffusion model understand the meaning of architectural elements beyond visual patterns
- Evidence anchors:
  - [abstract]: "Experiments show that fine-tuning significantly improves validity of generated floor plans from 6% to 90%"
  - [section 2.4]: "We propose a novel approach to encode these semantics in floor plans to improve the fine-tuning of diffusion models utilizing the following three strategies: Simplify, Encode, Contrast"
  - [corpus]: Weak - corpus neighbors don't directly address semantic encoding in architectural contexts
- Break condition: If the color-coding scheme doesn't align with how diffusion models process visual information, or if the simplification removes too much contextual information

### Mechanism 2
- Claim: Diffusion models can be fine-tuned efficiently using textual inversion rather than full model retraining
- Mechanism: Textual inversion modifies only the word embedding vector rather than retraining the entire deep neural network, making the process faster and more resource-efficient
- Core assumption: Modifying word embeddings is sufficient to adapt the model's understanding of architectural concepts without retraining the entire network
- Evidence anchors:
  - [section 1.2.5]: "Fortunately, the new generation of diffusion models allows to refine these federated models with a few examples to a new domain, which is called fine-tuning or few-shot-learning"
  - [section 2.5]: "We fine-tuned a Stable Diffusion model v2.1 for each encoding R, SR, SE, and SRE using Textual Inversions"
  - [corpus]: Weak - corpus neighbors focus on general diffusion model applications rather than fine-tuning efficiency
- Break condition: If the modified word embeddings don't capture the necessary semantic relationships, or if the federated model's general knowledge conflicts with the specialized training

### Mechanism 3
- Claim: Diffusion models can generate floor plans with better spatial understanding through semantic encoding
- Mechanism: The combination of simplified geometry, color-coded semantics, and contrast enhancement allows the model to better understand room arrangements and spatial relationships
- Core assumption: The diffusion model's multiple resolution layers can effectively learn spatial relationships from the simplified and encoded representations
- Evidence anchors:
  - [section 2.4]: "The question is if we can further simplify the floor plans by replacing the symbols for doors and windows with colours instead"
  - [section 3.2]: "The prompts testing room arrangements like L-, C- or O-shapes work well enough across all models at least in comparison to many other queries"
  - [corpus]: Weak - corpus neighbors don't specifically address spatial understanding in architectural diffusion models
- Break condition: If the diffusion model's architecture cannot effectively process the semantic information encoded in the simplified representations

## Foundational Learning

- Concept: Diffusion models work by progressively adding noise to images during training and learning to reverse this process
  - Why needed here: Understanding this fundamental mechanism explains why fine-tuning with semantic encoding can improve architectural floor plan generation
  - Quick check question: How does the noise addition and removal process in diffusion models differ from traditional image generation approaches?

- Concept: Semantic encoding in images involves representing meaningful information through visual cues like color and shape
  - Why needed here: The paper's novel approach relies on encoding architectural semantics through color-coding and simplification strategies
  - Quick check question: What are the three strategies proposed for semantic encoding in floor plans, and how do they complement each other?

- Concept: Textual inversion as a fine-tuning technique modifies word embeddings rather than full model weights
  - Why needed here: This technique enables efficient adaptation of pre-trained diffusion models to specialized architectural domains
  - Quick check question: Why might textual inversion be more efficient than full model retraining for adapting diffusion models to floor plan generation?

## Architecture Onboarding

- Component map: Data generation pipeline -> Fine-tuning module (textual inversion) -> Generation -> Validation -> Analysis
- Critical path: Data generation → Fine-tuning → Generation → Validation → Analysis
- Design tradeoffs: Full model retraining vs textual inversion (accuracy vs efficiency), semantic encoding complexity vs model understanding, simplified representations vs detail preservation
- Failure signatures: Partial floor plans, missing elements, incorrect symbol placement, inability to count or arrange elements as requested
- First 3 experiments:
  1. Generate 10 sample images using out-of-the-box Stable Diffusion with "building floor plan of a one family house with a garden" prompt to establish baseline performance
  2. Create algorithmic design approach to generate 100 sample floor plans and encode them in R, SR, SE, and SRE styles for fine-tuning
  3. Fine-tune Stable Diffusion v2.1 using textual inversion with each encoding style and evaluate against the 43 test prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can semantic encoding strategies like "Simplify, Encode, Contrast" be generalized to other architectural design tasks beyond floor plans?
- Basis in paper: [explicit] The authors propose these strategies for floor plan diffusion models and note their effectiveness.
- Why unresolved: The paper only tests these strategies on floor plans. Their effectiveness on other design tasks (e.g., building facades, interior layouts) is unknown.
- What evidence would resolve it: Testing the same semantic encoding strategies on other architectural design tasks and comparing performance to baseline models.

### Open Question 2
- Question: What is the minimum training data size required for effective fine-tuning of diffusion models on specialized architectural tasks?
- Basis in paper: [explicit] The authors use algorithmic design to generate training data, but the paper doesn't explore minimum data requirements.
- Why unresolved: The paper uses a fixed training dataset size without exploring how performance scales with data quantity.
- What evidence would resolve it: Systematic experiments varying training dataset sizes while measuring performance on validation tasks.

### Open Question 3
- Question: Can BIM-based diffusion models overcome the semantic limitations of image-based models while maintaining generation quality?
- Basis in paper: [explicit] The authors propose BIM-based diffusion models as a future direction to address semantic limitations.
- Why unresolved: The paper only proposes this approach theoretically without implementation or testing.
- What evidence would resolve it: Implementation and testing of BIM-based diffusion models comparing semantic understanding and generation quality to image-based models.

## Limitations

- Generated floor plans often lack completeness, producing only partial plans or missing critical elements like doors and windows
- Manual validation process introduces potential subjectivity and doesn't scale well for large-scale evaluation
- Evaluation framework focuses on basic structural elements but doesn't assess more complex architectural requirements like accessibility standards or building codes

## Confidence

- **High Confidence**: The effectiveness of fine-tuning diffusion models for floor plan generation (empirical validation shows clear improvement from 6% to 90% validity)
- **Medium Confidence**: The semantic encoding approach improves model understanding of architectural elements (supported by experiments but limited by evaluation scope)
- **Low Confidence**: The models can reliably generate floor plans that satisfy complex architectural requirements (explicitly acknowledged as a limitation in the paper)

## Next Checks

1. **Automated Evaluation Framework Development**: Implement automated metrics beyond simple validity checks, such as spatial coherence scoring, architectural rule compliance validation, and functional usability assessment to reduce subjectivity and enable large-scale evaluation.

2. **Cross-Architecture Generalization Testing**: Test the fine-tuned models on diverse architectural styles and building types beyond single-family houses to evaluate the robustness and generalizability of the semantic encoding approach across different design contexts.

3. **Professional Architect Review Study**: Conduct a structured evaluation with practicing architects to assess whether the generated floor plans meet professional standards, identify usability issues, and determine the practical value of the generated designs for real-world applications.