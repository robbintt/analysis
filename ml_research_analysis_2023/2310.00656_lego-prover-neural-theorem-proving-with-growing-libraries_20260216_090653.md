---
ver: rpa2
title: 'LEGO-Prover: Neural Theorem Proving with Growing Libraries'
arxiv_id: '2310.00656'
source_url: https://arxiv.org/abs/2310.00656
tags:
- proof
- skill
- arxiv
- theorem
- library
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEGO-Prover is a novel neural theorem proving method that employs
  a growing skill library to augment the capabilities of large language models (LLMs)
  used in theorem proving. Unlike prior approaches that assume a fixed theorem library,
  LEGO-Prover constructs proofs modularly, enabling LLMs to utilize existing skills
  retrieved from the library and create new skills during the proving process.
---

# LEGO-Prover: Neural Theorem Proving with Growing Libraries

## Quick Facts
- **arXiv ID**: 2310.00656
- **Source URL**: https://arxiv.org/abs/2310.00656
- **Reference count**: 40
- **Key outcome**: LEGO-Prover significantly advances the state-of-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%)

## Executive Summary
LEGO-Prover introduces a novel approach to neural theorem proving by employing a growing skill library that augments the capabilities of large language models (LLMs). Unlike prior methods that assume a fixed theorem library, LEGO-Prover constructs proofs modularly, enabling LLMs to utilize existing skills retrieved from the library and create new skills during the proving process. The system achieves significant improvements on the miniF2F benchmark, demonstrating the effectiveness of modular proof construction with evolving libraries.

## Method Summary
LEGO-Prover operates through a modular architecture consisting of a skill library, prover, and evolver components. The prover takes informal proofs and decomposes them into formal subgoals, retrieves relevant lemmas from the skill library, and generates proofs using these retrieved building blocks. During the proving process, LEGO-Prover generates over 20,000 new skills and adds them to the growing library. The evolver component then refines these skills through directional transformations and request solving, enhancing their reusability and expanding their functional coverage. All generated lemmas and proofs are verified by Isabelle to ensure correctness.

## Key Results
- Achieves 57.0% pass rate on miniF2F-valid (up from 48.0%)
- Achieves 47.1% pass rate on miniF2F-test (up from 45.5%)
- Generates over 20,000 new skills during proving process
- Ablation study shows improvement from 47.1% to 50.4% success rate when using newly added skills

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Growing skill library enables modular theorem proving by allowing LLMs to retrieve and reuse verified lemmas rather than constructing proofs from scratch.
- Mechanism: The skill library stores verified lemmas and requests as embeddings. During proving, the formalizer retrieves relevant lemmas to reduce proof complexity and provide correct building blocks.
- Core assumption: Retrieved lemmas are both syntactically compatible with the current proof context and semantically relevant to the subgoals.
- Evidence anchors:
  - [abstract]: "LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process."
  - [section 3.2]: "The formalizer is designed to incorporate useful lemmas retrieved from the lemma vector stores as part of the input."
  - [corpus]: Weak. No direct citations of retrieval effectiveness in related works.
- Break Condition: If retrieval quality degrades (e.g., wrong lemmas retrieved), proof construction fails or becomes more complex than plain proving.

### Mechanism 2
- Claim: Evolving skills via directional transformation and request solving increases library diversity and generality, enabling proofs of harder problems.
- Mechanism: The evolver transforms existing skills along predefined trajectories (extension, parameterization, complexity scaling) and solves requests to generate new, more general lemmas.
- Core assumption: Evolution steps preserve correctness and improve utility across different problem domains.
- Evidence anchors:
  - [abstract]: "These skills are further evolved (by prompting an LLM) to enrich the library on another scale."
  - [section 3.3]: "The objective of the evolver is to create or refine these skills, enhancing their reusability and expanding their functional coverage."
  - [corpus]: Weak. No direct citations of skill evolution techniques in related works.
- Break Condition: If evolution introduces incorrect or overly specific lemmas, the library becomes less useful and may mislead the prover.

### Mechanism 3
- Claim: Decomposing informal proofs into step-by-step formal goals bridges the gap between human reasoning and formal proof construction.
- Mechanism: The decomposer translates informal proofs into structured subgoals and requests, guiding the prover to build proofs modularly.
- Core assumption: Informal proofs contain sufficient structure to be decomposed into valid formal subgoals.
- Evidence anchors:
  - [section 3.2]: "The decomposer aims to decompose the formalization tasks, which transform the informal proof into the decomposed step-by-step informal proof as well as decompose the problem into formal goals."
  - [corpus]: Weak. No direct citations of decomposition strategies in related works.
- Break Condition: If decomposition fails to produce valid subgoals, the prover cannot leverage the skill library effectively.

## Foundational Learning

- Concept: Vector embeddings and k-NN retrieval
  - Why needed here: Skills are stored as embeddings; retrieval relies on finding nearest neighbors in embedding space.
  - Quick check question: What embedding model is used for the lemma vector store?

- Concept: Modular proof construction
  - Why needed here: LEGO-Prover builds proofs block-by-block using retrieved lemmas rather than sequentially.
  - Quick check question: How does the formalizer incorporate retrieved lemmas into the proof?

- Concept: Isabelle theorem prover verification
  - Why needed here: All generated lemmas and proofs must be verified by Isabelle to ensure correctness.
  - Quick check question: What happens if Isabelle cannot verify a generated proof?

## Architecture Onboarding

- Component map:
  Skill library (lemma vector store, request vector store, problem vector store) -> Prover (informal solver → decomposer → formalizer) -> Evolver (directional transformer, request solver)

- Critical path:
  1. Problem input → informal solver → decomposed subgoals
  2. Retrieve relevant lemmas from skill library
  3. Formalizer generates proof using retrieved lemmas
  4. Verified lemmas added to skill library
  5. Evolver evolves skills for next iteration

- Design tradeoffs:
  - Retrieval vs. generation: Retrieval provides correctness but may lack relevance; generation is flexible but error-prone.
  - Skill library size vs. retrieval efficiency: Larger libraries increase diversity but slow retrieval.
  - Evolution frequency vs. quality: Frequent evolution increases diversity but may introduce noise.

- Failure signatures:
  - Low retrieval recall → prover cannot find useful lemmas
  - High false positive rate → prover uses irrelevant lemmas
  - Evolution producing overly specific lemmas → reduced reusability
  - Isabelle verification failures → incorrect proofs added to library

- First 3 experiments:
  1. Test retrieval effectiveness: Input a problem and verify retrieved lemmas are relevant.
  2. Test proof construction: Run prover on a simple problem and check if it uses retrieved lemmas correctly.
  3. Test evolution: Apply directional transformer to a lemma and verify the evolved version is both correct and more general.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of attempts per problem to balance computational cost and success rate in LEGO-Prover?
- Basis in paper: The paper mentions "100 attempts" for each problem but also discusses an ablation study with "50 attempts" per problem.
- Why unresolved: The paper does not explore the trade-off between computational cost and success rate by varying the number of attempts per problem.
- What evidence would resolve it: A systematic study varying the number of attempts (e.g., 25, 50, 75, 100, 150) and measuring the success rate and computational cost would provide insights into the optimal number of attempts.

### Open Question 2
- Question: How does the diversity of the skill library impact the performance of LEGO-Prover on different types of mathematical problems?
- Basis in paper: The paper discusses the growing skill library but does not analyze how the diversity of skills affects performance on different problem types.
- Why unresolved: The paper does not provide a detailed analysis of how the skill library's diversity influences the prover's ability to handle various mathematical domains.
- What evidence would resolve it: An analysis comparing the performance of LEGO-Prover on different problem types (e.g., algebra, number theory, geometry) with skill libraries of varying diversity would provide insights into this relationship.

### Open Question 3
- Question: What is the impact of the skill evolution techniques on the long-term performance of LEGO-Prover?
- Basis in paper: The paper mentions four skill evolution techniques (extension of dimensions, identification of key concepts, parameterization, and enhancement of complexity) but does not analyze their long-term impact.
- Why unresolved: The paper does not provide a detailed analysis of how each evolution technique contributes to the prover's performance over time.
- What evidence would resolve it: A study tracking the performance of LEGO-Prover over multiple problem-solving sessions while varying the emphasis on each evolution technique would provide insights into their long-term impact.

## Limitations
- The method's reliance on Isabelle verification creates a bottleneck for skill evolution, as only verified lemmas can be added to the library.
- The directional transformation approach for skill evolution may not explore all useful transformations.
- The effectiveness of skill retrieval depends heavily on the quality of vector embeddings, which isn't fully characterized.

## Confidence

- **High Confidence**: The modular proof construction approach and the basic skill library mechanism are sound and well-implemented.
- **Medium Confidence**: The skill evolution component shows promise but needs more rigorous evaluation of its effectiveness across different problem domains.
- **Medium Confidence**: The decomposition of informal proofs into formal subgoals works for many problems but may struggle with highly abstract or creative proofs.

## Next Checks

1. **Retrieval Quality Analysis**: Systematically evaluate the precision and recall of lemma retrieval across different problem types to identify when and why retrieval fails.

2. **Evolution Impact Study**: Compare proof success rates with and without evolved skills to quantify their contribution beyond simple lemma generation.

3. **Embedding Quality Assessment**: Test how different embedding models and similarity metrics affect retrieval effectiveness and overall proof success.