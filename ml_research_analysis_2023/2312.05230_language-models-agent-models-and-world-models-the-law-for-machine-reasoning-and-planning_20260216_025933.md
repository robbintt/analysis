---
ver: rpa2
title: 'Language Models, Agent Models, and World Models: The LAW for Machine Reasoning
  and Planning'
arxiv_id: '2312.05230'
source_url: https://arxiv.org/abs/2312.05230
tags:
- reasoning
- world
- arxiv
- language
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the LAW framework, which connects Language
  models, Agent models, and World models to enhance machine reasoning and planning
  capabilities. LAW aims to address the limitations of current large language models
  (LLMs) in consistent reasoning and planning across various scenarios.
---

# Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning

## Quick Facts
- arXiv ID: 2312.05230
- Source URL: https://arxiv.org/abs/2312.05230
- Reference count: 23
- One-line primary result: Proposes LAW framework connecting Language, Agent, and World models to enhance machine reasoning and planning capabilities.

## Executive Summary
This paper introduces the LAW framework to address limitations in current large language models' reasoning and planning capabilities. The framework proposes that explicit world and agent models provide better abstractions for reasoning than language models alone, introducing crucial elements like beliefs, anticipation of consequences, goals/rewards, and strategic planning. Language models serve as a computational backend within this framework, providing the flexibility and power needed to implement reasoning across diverse scenarios. The authors review recent studies related to each component and discuss future research directions for developing more robust AI systems.

## Method Summary
The LAW framework positions language models as backend implementations for world and agent models that provide structured reasoning capabilities. The method involves using LLMs to simulate world states, maintain beliefs about other agents, anticipate consequences of actions, and engage in strategic planning through approaches like Monte Carlo Tree Search. The framework can be extended to incorporate embodied experiences and social learning to overcome limitations of text-only training. Implementation involves prompting LLMs to predict next states, generate actions, and maintain reasoning traces while integrating goals and rewards for planning.

## Key Results
- The LAW framework conceptually connects language, agent, and world models to enhance machine reasoning beyond current LLM limitations
- Language models can be repurposed as world and agent models through appropriate prompting and architectural integration
- Embodied and social experiences are proposed as necessary complements to text-based training for robust reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LAW framework addresses LLM reasoning limitations by introducing explicit world and agent models that provide grounding for language-based reasoning.
- Mechanism: World and agent models introduce structured representations (beliefs, goals, consequences, planning) that overcome language ambiguity and enable deliberate reasoning through simulation.
- Core assumption: Symbolic representations from language models can effectively capture causal structures and mental states needed for robust reasoning.
- Evidence anchors:
  - [abstract] "we propose that world and agent models are a better abstraction of reasoning, that introduces the crucial elements of deliberate human-like reasoning, including beliefs about the world and other agents, anticipation of consequences, goals/rewards, and strategic planning."
  - [section] "human reasoning relies on the internal world model which allows human brains to play out different reasoning steps and their effects on the world state."
  - [corpus] Weak evidence - neighboring papers focus on planning and reasoning enhancements but don't directly address the symbolic grounding problem.
- Break condition: If world/agent models cannot be effectively implemented using language models, or if the symbolic representations fail to capture necessary causal structures.

### Mechanism 2
- Claim: Language models serve as a computational backend that provides the flexibility and power to implement world and agent models across diverse reasoning scenarios.
- Mechanism: LMs implement the structured reasoning components (WM, AM) while maintaining the adaptability needed for handling real-world complexity and noise.
- Core assumption: LMs can be effectively repurposed as world and agent models through appropriate prompting and architectural integration.
- Evidence anchors:
  - [abstract] "language models in LAW serve as a backend to implement the system or its elements and hence provide the computational power and adaptability."
  - [section] "RAP repurposes an LLM as a world model by prompting the LLM to predict the next state st+1 of reasoning after applying a reasoning step at to the current state st."
  - [corpus] Weak evidence - neighboring papers mention LLM agents but don't demonstrate the specific backend implementation pattern.
- Break condition: If LMs cannot effectively simulate future states or maintain coherent reasoning traces when implementing world/agent models.

### Mechanism 3
- Claim: The framework enables enhanced learning through embodied and social experiences that complement text-based training.
- Mechanism: Learning from embodied interactions and social experiences provides the real-world grounding that pure text training lacks, enhancing the world and agent models.
- Core assumption: Embodied and social learning experiences can be effectively captured and integrated into language models to improve reasoning capabilities.
- Evidence anchors:
  - [section] "LLMs trained merely with large-scale text corpora lack fundamental real-world experience... Human agents bypass these limitations by learning through interaction with the environment."
  - [section] "Learning with Embodied Experiences... enhances LMs' world knowledge with embodied experiences."
  - [corpus] Weak evidence - corpus focuses on planning and reasoning but doesn't specifically address embodied learning integration.
- Break condition: If embodied and social experiences cannot be effectively translated into improvements in world and agent model representations.

## Foundational Learning

- Concept: Mental models and their role in human reasoning
  - Why needed here: Understanding how humans use mental models for reasoning provides the theoretical foundation for why world and agent models are necessary
  - Quick check question: What are the key components that distinguish human mental models from pure language processing?

- Concept: Theory of Mind and social reasoning
  - Why needed here: Social reasoning requires understanding other agents' mental states, which is crucial for the agent model component
  - Quick check question: How does Theory of Mind differ from simple pattern recognition in language models?

- Concept: Reinforcement learning and planning algorithms
  - Why needed here: Strategic planning and exploration-exploitation tradeoffs are essential for the agent model's decision-making capabilities
- Quick check question: What planning algorithms could be integrated with language models to enhance reasoning?

## Architecture Onboarding

- Component map:
  - Language Model Backend: Handles text processing, state prediction, and action generation
  - World Model: Maintains state representations and simulates future states
  - Agent Model: Integrates goals, beliefs, and planning mechanisms
  - Experience Integration: Modules for incorporating embodied and social learning

- Critical path: Reasoning trace generation → State prediction → Action selection → Goal evaluation → Planning refinement

- Design tradeoffs:
  - Symbolic vs. continuous representations for world states
  - Model-based vs. model-free planning approaches
  - Single vs. multi-level world modeling (abstract vs. pixel-level)

- Failure signatures:
  - Inconsistent reasoning traces
  - Inability to maintain coherent world states
  - Poor transfer from text-based to embodied reasoning

- First 3 experiments:
  1. Implement RAP-style reasoning on simple planning problems and measure improvement over baseline CoT
  2. Test embodied experience integration by comparing reasoning performance before/after adding physical interaction data
  3. Evaluate social reasoning capabilities by comparing Theory of Mind performance with and without explicit agent models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are multimodal world models compared to language-only models in tasks requiring physical reasoning and planning?
- Basis in paper: [explicit] The paper discusses the need for multimodal world models to overcome the limitations of language-only models in describing world states and dynamics.
- Why unresolved: While the paper proposes the potential of multimodal models, it does not provide empirical evidence comparing their performance to language-only models in physical reasoning tasks.
- What evidence would resolve it: Comparative studies evaluating the performance of multimodal and language-only models on tasks like embodied reasoning, tool use, and physical scene understanding.

### Open Question 2
- Question: Can language models effectively model beliefs in agent models, especially in social reasoning tasks?
- Basis in paper: [explicit] The paper highlights the importance of beliefs in agent models but notes the lack of work on explicitly modeling beliefs using language models.
- Why unresolved: Existing evaluations show language models have imperfect ability to encode belief representations and infer other agents' beliefs.
- What evidence would resolve it: Studies demonstrating language models' capacity to accurately update and reason about beliefs in complex social scenarios, compared to human performance.

### Open Question 3
- Question: How can language models be effectively trained to learn from embodied and social experiences, beyond imitating text corpora?
- Basis in paper: [explicit] The paper suggests that language models need to learn from embodied and social experiences to acquire a more robust understanding of the world.
- Why unresolved: While the paper proposes various methods for collecting embodied and social experiences, it does not provide concrete evidence of their effectiveness in enhancing language models' reasoning capabilities.
- What evidence would resolve it: Empirical studies demonstrating improved performance of language models on reasoning tasks after training with embodied and social experiences, compared to models trained only on text corpora.

## Limitations

- Limited empirical validation of the LAW framework's effectiveness compared to existing reasoning approaches
- Unclear mechanisms for how embodied experiences can be effectively integrated into language models to enhance reasoning
- Scalability concerns for applying the framework to real-world complexity and maintaining coherent world states

## Confidence

- **High Confidence**: The identification of LLM reasoning limitations (inconsistent reasoning, inability to anticipate consequences, lack of strategic planning) is well-supported by existing literature and observation.
- **Medium Confidence**: The theoretical framework connecting language, agent, and world models is logically coherent and builds on established concepts from cognitive science and AI planning, but lacks extensive empirical validation.
- **Low Confidence**: The specific implementation details for integrating embodied experiences and social learning into language models are speculative and require significant development work to realize.

## Next Checks

1. **Implementation Validation**: Build a minimal LAW system following the RAP (Reasoning-via-Planning) approach and test it on a standardized reasoning benchmark (e.g., GSM8K or StrategyQA) to measure performance gains over baseline chain-of-thought prompting.

2. **State Consistency Test**: Design a test where the LLM-based world model must maintain consistent state representations across multiple reasoning steps, including counterfactual scenarios, to validate its ability to simulate future states accurately.

3. **Embodied Experience Integration**: Conduct an experiment comparing reasoning performance on physical reasoning tasks before and after incorporating simulated embodied experiences (e.g., physics engine interactions) into the world model training.