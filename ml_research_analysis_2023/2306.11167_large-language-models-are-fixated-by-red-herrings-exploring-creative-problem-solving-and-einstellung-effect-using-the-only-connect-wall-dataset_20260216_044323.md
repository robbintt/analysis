---
ver: rpa2
title: 'Large Language Models are Fixated by Red Herrings: Exploring Creative Problem
  Solving and Einstellung Effect using the Only Connect Wall Dataset'
arxiv_id: '2306.11167'
source_url: https://arxiv.org/abs/2306.11167
tags:
- language
- wall
- task
- connections
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Only Connect Wall (OCW) dataset, curated
  from the British quiz show "Only Connect," to evaluate creative problem-solving
  abilities of large language models (LLMs). The dataset contains 618 puzzles, each
  with 16 clue words that must be grouped into 4 connected groups and connections
  identified.
---

# Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset

## Quick Facts
- arXiv ID: 2306.11167
- Source URL: https://arxiv.org/abs/2306.11167
- Reference count: 40
- Key outcome: GPT-4 solves only 5 out of 494 OCW walls correctly, significantly underperforming humans

## Executive Summary
This paper introduces the Only Connect Wall (OCW) dataset to evaluate creative problem-solving abilities of large language models (LLMs). The dataset contains 618 puzzles from the British quiz show "Only Connect," where 16 clue words must be grouped into 4 connected groups and connections identified. The study finds that even state-of-the-art LLMs like GPT-4 significantly underperform human baselines, with GPT-4 solving only 5 out of 494 walls correctly. Surprisingly, increasing in-context examples does not improve LLM performance, suggesting red herring-induced negative transfer learning. The findings indicate that LLMs struggle with creative problem-solving tasks involving misleading stimuli and open-domain knowledge.

## Method Summary
The study evaluates both static and contextual embeddings using clustering algorithms on the OCW dataset, and tests few-shot in-context learning with LLMs (GPT-3.5-turbo, GPT-4). Static embeddings (GloVe, FastText, ELMo) and contextual embeddings (BERT variants, Sentence Transformers) are extracted from the 16 clue words and clustered using k-means or constrained clustering to predict the 4 groups. For LLMs, prompts are constructed with 0-10 in-context examples in the format shown in Figure 4. Performance is measured using grouping accuracy metrics (solved walls, correct groups, AMI, ARI, FMS, WD) and connection identification metrics (exact match, ROUGE-1 F1, BERTScore F1).

## Key Results
- GPT-4 solves only 5 out of 494 walls correctly, significantly underperforming human baselines
- Increasing in-context examples from 0 to 10 shots does not improve performance for GPT-4 or GPT-3.5-turbo
- Static embeddings outperform contextual embeddings on the grouping task, contrary to expectations
- The presence of orthographically similar clue words in examples plausibly induces negative transfer learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Red herrings in the dataset induce fixation effects in LLMs similar to humans.
- Mechanism: Orthographically similar words or misleading stimuli create incorrect associative pathways in the model's learned representations, reducing creative problem-solving performance.
- Core assumption: LLMs process semantic associations similarly to humans when exposed to misleading stimuli, allowing negative transfer learning to occur.
- Evidence anchors: Exposure to misleading stimuli impedes human performance via fixation effect and Einstellung paradigm; red herrings induce fixations by presenting wrong answers.

### Mechanism 2
- Claim: Increasing in-context examples does not improve LLM performance due to red herring-induced negative transfer.
- Mechanism: Random sampling of in-context examples introduces orthographically similar incorrect answers that fixate the model on wrong connections, preventing learning of correct associations.
- Core assumption: The open-domain nature of knowledge required means most in-context examples contain some red herrings that negatively transfer.
- Evidence anchors: Increasing in-context examples in few-shot learning on GPT-4 is ineffective; orthographically similar clue words in examples plausibly induce negative transfer learning.

### Mechanism 3
- Claim: Static embeddings outperform contextual embeddings for this task due to contextual ambiguity and positional encoding issues.
- Mechanism: Concatenating all 16 clues into a pseudo-sentence creates syntactic violations that PLMs are not trained to handle, while positional encoding makes embeddings sensitive to clue ordering.
- Core assumption: The task benefits more from intrinsic semantic similarities between clues than from their contextual usage in a specific wall.
- Evidence anchors: Concatenating all clues may not adhere to sentence structure PLMs are accustomed to; positional encoding is variant to sequence order.

## Foundational Learning

- Concept: Fixation effect and Einstellung paradigm in human cognition
  - Why needed here: Understanding how humans fail on creative problem-solving tasks with red herrings provides the theoretical framework for why LLMs might also fail.
  - Quick check question: What is the difference between fixation effect and Einstellung effect in cognitive psychology?

- Concept: Negative transfer learning
  - Why needed here: Explains why exposure to incorrect associations (red herrings) degrades performance on subsequent tasks.
  - Quick check question: How does negative transfer learning differ from positive transfer learning in machine learning?

- Concept: Word embedding representations and semantic similarity
  - Why needed here: Core to understanding how models cluster clue words and why certain embedding types perform better.
  - Quick check question: What is the difference between static and contextual word embeddings, and when would each be preferred?

## Architecture Onboarding

- Component map: Dataset loader -> Embedding generator -> Clustering algorithm -> Evaluation metrics -> LLM prompt generator
- Critical path: Embedding generation -> Clustering/grouping prediction -> Connection identification -> Evaluation
- Design tradeoffs: Static vs contextual embeddings (robustness vs semantic awareness), clustering algorithms (simplicity vs constraints), few-shot examples (coverage vs red herring contamination)
- Failure signatures: Poor clustering metrics (AMI, ARI, FMS), low solved wall counts, hallucinations in LLM outputs, sensitivity to clue ordering
- First 3 experiments:
  1. Run clustering on static embeddings (GloVe) to establish baseline performance
  2. Compare static vs contextual embeddings (BERT) on a subset of walls
  3. Test GPT-4 with 0-shot, 1-shot, and 3-shot prompts on the same subset to establish LLM baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does increasing the number of in-context examples (few-shot prompting) not improve performance on the Only Connect Wall tasks for GPT-4 and GPT-3.5-turbo?
- Basis in paper: The paper reports that increasing in-context examples from 0 to 10 shots did not improve performance on Task 1 (Grouping) or Task 2 (Connections) for GPT-4 and GPT-3.5-turbo.
- Why unresolved: The paper suggests that red herrings may induce negative transfer learning, but the exact mechanisms and why more examples do not help are not fully explored.
- What evidence would resolve it: Controlled experiments varying the number and type of in-context examples, as well as ablation studies isolating the effect of red herrings.

### Open Question 2
- Question: Can retrieval-augmented language models (LMs) significantly improve performance on the OCW tasks compared to standard LMs?
- Basis in paper: The paper mentions that retrieval-augmented models may be capable of solving groups about highly specific subject areas, but this is left for future work.
- Why unresolved: The paper does not evaluate retrieval-augmented LMs on the OCW dataset, so their potential performance gains are unknown.
- What evidence would resolve it: Benchmarking retrieval-augmented LMs (e.g., REALM, RAG) on the OCW dataset and comparing their performance to standard LMs.

### Open Question 3
- Question: How sensitive are GPT-3.5-turbo and GPT-4 to the ordering of clues in the OCW dataset, and does this sensitivity impact performance?
- Basis in paper: The paper notes that the performance of contextual approaches can vary significantly depending on the order that clues are provided to the model, and evaluates models across 16 random sortings of the clues. However, due to cost, GPT-3.5-turbo and GPT-4's sensitivity to this ordering is not evaluated.
- Why unresolved: The paper does not report performance across multiple random sorts for GPT-3.5-turbo and GPT-4, so their sensitivity to clue ordering is unknown.
- What evidence would resolve it: Evaluating GPT-3.5-turbo and GPT-4's performance on the OCW dataset across multiple random sortings of the clues and comparing the variance in results.

## Limitations

- Narrow evaluation scope - only one dataset (OCW) and two specific tasks were tested
- Negative transfer learning mechanism lacks direct empirical validation, relying on correlational observations
- Does not control for alternative explanations such as domain knowledge gaps or task format unfamiliarity
- Comparison between static and contextual embeddings conflates multiple factors without ablation studies

## Confidence

- **High confidence**: Task design validity and human baseline establishment
- **Medium confidence**: Static embeddings outperforming contextual embeddings
- **Low confidence**: Red herring-induced negative transfer learning mechanism

## Next Checks

1. Conduct controlled experiments varying the proportion of red herring-containing in-context examples to directly test the negative transfer learning hypothesis.
2. Perform ablation studies on embedding performance by testing with properly formatted sentences versus concatenated clues to isolate the syntactic violation effect.
3. Evaluate models on a modified dataset where red herrings are replaced with neutral distractors to determine if the performance degradation is specifically due to misleading stimuli rather than general task difficulty.