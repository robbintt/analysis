---
ver: rpa2
title: A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity
  to the Effects of Randomness
arxiv_id: '2312.01082'
source_url: https://arxiv.org/abs/2312.01082
tags:
- randomness
- papers
- learning
- effects
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of 134 papers addressing
  the effects of randomness on the stability of learning with limited labelled data,
  such as few-shot learning, meta-learning, and transfer learning. The survey identifies
  and discusses seven challenges and open problems related to the study of randomness
  effects, including the limited focus on a small fragment of randomness, the lack
  of analysis and explanation of the importance of randomness factors, and the inconsistency
  in findings from different tasks for dealing with randomness effects.
---

# A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness

## Quick Facts
- arXiv ID: 2312.01082
- Source URL: https://arxiv.org/abs/2312.01082
- Reference count: 40
- One-line primary result: This survey identifies 134 papers addressing randomness effects on stability in low-data learning, revealing significant research gaps and challenges across seven key areas.

## Executive Summary
This comprehensive survey examines 134 papers investigating how randomness affects the stability of machine learning models when trained with limited labeled data. The survey identifies four primary tasks for addressing randomness effects (investigate, determine, mitigate, benchmark/compare/report) and develops a taxonomy for analyzing research in this area. Key findings reveal that randomness effects are significantly under-researched across most machine learning approaches and modalities, with particular gaps in meta-learning, image data, and non-language modalities. The survey identifies seven challenges and open problems, including limited focus on certain randomness factors, lack of analysis explaining the importance of these factors, and inconsistent findings due to disregarded interactions between randomness factors.

## Method Summary
The survey employs a PRISMA-inspired methodology, identifying papers through keyword searches across multiple digital libraries (ACM Digital Library, IEEE Xplore, Scopus, ACL Anthology, NeurIPS proceedings, PMLR) using the query `('few shot' OR 'meta learning' OR 'metalearning') AND ('stability' OR 'instability' OR 'sensitivity' OR 'variance' OR 'randomness')`. Papers underwent relevance filtering, merit assessment, and content filtering before detailed analysis. The methodology includes identification, relevance filtering, content filtering, paper analysis, reference analysis, and categorization based on a developed taxonomy.

## Key Results
- Systematic categorization of 134 papers into four primary tasks for addressing randomness effects
- Identification of significant research gaps in meta-learning, image data, and non-language modalities
- Seven challenges identified including limited focus on randomness factors and inconsistent findings due to ignored interactions
- Comprehensive taxonomy developed for analyzing randomness effects across low-data learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The survey establishes a systematic framework for categorizing research on randomness in low-data learning by identifying four primary tasks (investigate, determine, mitigate, benchmark/compare).
- **Mechanism:** Papers are classified based on what tasks they perform to address randomness effects, enabling clear aggregation of findings and identification of open problems.
- **Core assumption:** The four task categories sufficiently capture the breadth of approaches to studying randomness effects.
- **Evidence anchors:**
  - [abstract]: "We distinguish between four main tasks addressed in the papers (investigate/evaluate; determine; mitigate; benchmark/compare/report randomness effects), providing findings for each one."
  - [section]: "This property specifies the different tasks for addressing the effects of randomness that are performed in the papers... We define following 7 values: (1) Investigate... (2) Determine... (3) Mitigate... (4) Benchmark... (5) Evaluate... (6) Report... (7) Compare..."
  - [corpus]: Weak evidence for mechanism effectiveness; corpus shows related work but not validation of categorization scheme.
- **Break condition:** If new tasks emerge that don't fit the four categories, or if the distinctions between tasks become blurred in practice.

### Mechanism 2
- **Claim:** The survey reveals that randomness effects are significantly under-researched across most machine learning approaches and modalities beyond language models.
- **Mechanism:** By cataloging 134 papers and their focus areas, the survey identifies gaps in research coverage, particularly for meta-learning, image data, and non-language modalities.
- **Core assumption:** The surveyed papers provide a representative sample of the research landscape on randomness effects.
- **Evidence anchors:**
  - [abstract]: "This survey provides a comprehensive overview of 134 papers addressing the effects of randomness on the stability of learning with limited labelled data..."
  - [section]: "The effects of randomness are not investigated evenly across all groups of approaches for dealing with limited labelled data or modalities... The investigation is more skewed towards randomness in language models..."
  - [corpus]: Weak evidence; corpus shows related papers but doesn't validate the claim about under-research.
- **Break condition:** If subsequent research shows that other approaches and modalities have been adequately studied, or if the survey missed significant work.

### Mechanism 3
- **Claim:** The survey identifies that interactions between randomness factors are a primary source of inconsistent findings across papers.
- **Mechanism:** By analyzing papers that investigate multiple randomness factors, the survey finds that failing to account for factor interactions leads to contradictory results and biased comparisons.
- **Core assumption:** Interactions between randomness factors are a significant and often overlooked source of variability.
- **Evidence anchors:**
  - [abstract]: "The survey identifies and discusses seven challenges and open problems related to the study of randomness effects, including the limited focus on a small fragment of randomness..."
  - [section]: "Ignoring interactions between different randomness factors can lead to biased results... We can observe that many papers often find contradictory findings across all the tasks for addressing the effects of randomness..."
  - [corpus]: Weak evidence; corpus shows related work but doesn't validate the claim about interactions being a primary source of inconsistency.
- **Break condition:** If future research demonstrates that other factors (e.g., methodology differences, hyperparameter tuning) are more significant sources of inconsistency.

## Foundational Learning

- **Concept:** Taxonomy development for systematic literature review
  - **Why needed here:** The survey relies on a well-defined taxonomy to categorize papers and identify research gaps systematically.
  - **Quick check question:** Can you explain how the four primary task categories (investigate, determine, mitigate, benchmark/compare/report) help organize the research landscape on randomness effects?

- **Concept:** Understanding of stability in machine learning
  - **Why needed here:** The survey focuses on stability as affected by randomness in low-data regimes, requiring a clear definition of what stability means in this context.
  - **Quick check question:** How does the survey define stability, and how does this definition differ from other concepts like robustness or reproducibility?

- **Concept:** Meta-learning and few-shot learning paradigms
  - **Why needed here:** The survey examines randomness effects across different approaches to learning with limited labeled data, including meta-learning and few-shot learning.
  - **Quick check question:** Can you describe the key difference between meta-learning and few-shot learning as approaches to dealing with limited labeled data?

## Architecture Onboarding

- **Component map:** Survey methodology -> Taxonomy development -> Paper categorization -> Task analysis -> Challenge identification -> Open problem formulation
- **Critical path:** Start with understanding the survey methodology, then grasp the taxonomy, followed by each task category and its findings, and finally the identified challenges and open problems.
- **Design tradeoffs:** The survey balances comprehensiveness (covering 134 papers) with depth (detailed analysis of 50 core papers), potentially missing some nuances in the broader set of 84 recognize papers.
- **Failure signatures:** Inconsistencies in findings across papers, limited focus on certain randomness factors or approaches, and simple evaluation methods that don't account for factor interactions.
- **First 3 experiments:**
  1. Apply the taxonomy to a new set of papers on randomness effects to test its comprehensiveness and usefulness.
  2. Investigate a specific randomness factor (e.g., random seed) across multiple approaches to identify interaction effects.
  3. Design a benchmark that explicitly accounts for multiple randomness factors and their interactions, using statistical methods to disentangle their effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the relative contributions of individual randomness factors (e.g., data choice, order of data, parameter initialization) to the overall instability observed in learning with limited labeled data?
- Basis in paper: [explicit] The paper identifies that different randomness factors receive varying levels of attention, with some factors (e.g., random seeds, model initialization) being more popular for investigation than others (e.g., data split, choice of labeled data). However, there is limited analysis of the relative importance of these factors.
- Why unresolved: Despite the identification of various randomness factors, the paper highlights a lack of analysis and explanation regarding the importance of these factors. The significance of each factor's contribution to instability remains unclear, as most studies focus on individual factors without comparing their relative impacts.
- What evidence would resolve it: Conducting comprehensive experiments that systematically investigate and compare the effects of multiple randomness factors simultaneously, using statistical methods to disentangle their individual contributions, would provide insights into their relative importance.

### Open Question 2
- Question: How do the effects of randomness on stability differ across various machine learning approaches (e.g., meta-learning, fine-tuning, in-context learning) and modalities (e.g., text, images)?
- Basis in paper: [explicit] The paper notes that the investigation of randomness effects is more skewed towards language models and natural language processing, with limited focus on other approaches and modalities. This raises questions about the generalizability of findings across different contexts.
- Why unresolved: The limited scope of investigation across different approaches and modalities hinders the understanding of how randomness affects stability in diverse settings. The paper suggests that the focus on language models may not reflect the true impact of randomness on other methods.
- What evidence would resolve it: Expanding the scope of research to include a broader range of machine learning approaches and modalities, and conducting comparative studies across these contexts, would reveal differences in the effects of randomness and enhance generalizability.

### Open Question 3
- Question: What are the most effective mitigation strategies for addressing the effects of randomness, considering the interactions between different randomness factors?
- Basis in paper: [inferred] The paper identifies that many mitigation strategies are designed for specific origins of randomness and are not universally applicable. The interactions between randomness factors are often disregarded, leading to inconsistent findings and limited effectiveness of mitigation strategies.
- Why unresolved: The lack of effective mitigation strategies that account for interactions between randomness factors is a significant challenge. Most strategies focus on individual factors, and the computational cost of addressing multiple factors simultaneously is prohibitive.
- What evidence would resolve it: Developing and evaluating mitigation strategies that explicitly consider the interactions between randomness factors, possibly through the use of advanced statistical methods or computational techniques, would provide insights into their effectiveness and guide the design of more comprehensive solutions.

## Limitations
- The survey's findings are based on a relatively small fraction (50 out of 134) of papers that underwent detailed analysis, potentially missing nuanced patterns in the broader corpus
- The taxonomy may not fully capture emerging research directions or novel approaches to studying randomness effects
- The survey focuses primarily on published work, potentially overlooking important unpublished or pre-print research
- The analysis is limited to specific digital libraries and conference proceedings, which may introduce selection bias

## Confidence
- **High Confidence:** The identification of four primary task categories for addressing randomness effects is well-supported by the corpus analysis
- **Medium Confidence:** Claims about under-researched areas and interactions between randomness factors are plausible but require further empirical validation
- **Medium Confidence:** The identification of seven challenges and open problems is based on observed patterns but may benefit from broader expert consensus

## Next Checks
1. Apply the taxonomy to a new set of papers on randomness effects to test its comprehensiveness and usefulness in categorizing emerging research
2. Conduct a systematic replication study examining the interaction effects of multiple randomness factors across different approaches to low-data learning
3. Design and execute a benchmark study that explicitly accounts for multiple randomness factors and their interactions, using statistical methods to disentangle their effects on stability