---
ver: rpa2
title: 'STARC: A General Framework For Quantifying Differences Between Reward Functions'
arxiv_id: '2309.15257'
source_url: https://arxiv.org/abs/2309.15257
tags:
- reward
- function
- have
- metrics
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STARC (STAndardised Reward Comparison) metrics provide a principled
  framework for quantifying differences between reward functions in reinforcement
  learning. The core method involves canonicalising reward functions to remove potential
  shaping and state redistribution effects, then measuring distance using weighted
  norms.
---

# STARC: A General Framework For Quantifying Differences Between Reward Functions

## Quick Facts
- arXiv ID: 2309.15257
- Source URL: https://arxiv.org/abs/2309.15257
- Reference count: 40
- Primary result: STARC metrics achieve 0.856-0.873 correlation with regret, outperforming EPIC (0.778) and DARD (0.782)

## Executive Summary
STARC (STAndardised Reward Comparison) provides a principled framework for quantifying differences between reward functions in reinforcement learning. The approach involves canonicalizing reward functions to remove potential shaping and state redistribution effects, then measuring distance using weighted norms. The resulting metrics induce both upper and lower bounds on worst-case regret, making them tight measures of reward function similarity. Experimental results demonstrate that STARC metrics significantly outperform existing methods like EPIC and DARD in correlating with actual policy performance differences.

## Method Summary
The STARC framework computes reward function distances through a three-step process: (1) canonicalization to remove potential shaping and S1-redistribution effects using functions like VAL, EPIC, or DARD, (2) normalization using weighted L1, L2, or L∞ norms, and (3) distance computation using admissible metrics on the normalized, canonicalized reward functions. The method was evaluated on randomly generated MDPs with 32 states, 4 actions, and sparse non-deterministic transitions, comparing 1,500 reward function pairs across 96 CPU cores.

## Key Results
- STARC metrics achieve correlations of 0.856-0.873 with regret across hundreds of metric variants
- Significantly outperforms existing metrics like EPIC (0.778) and DARD (0.782)
- STARC metrics induce both upper and lower bounds on worst-case regret, making them tight measures of similarity
- Any metric with soundness and completeness properties must be bilipschitz equivalent to STARC metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: STARC metrics induce both upper and lower bounds on worst-case regret, making them tight measures of reward function similarity.
- Mechanism: The metrics normalize reward functions to remove potential shaping and state redistribution effects, then measure distance using weighted norms. This creates a pseudometric space where small distances guarantee low regret and large distances guarantee high regret.
- Core assumption: The canonicalization process successfully removes all effects of potential shaping and S1-redistribution while preserving the essential policy ordering information.
- Evidence anchors: [abstract] "STARC metrics induce both an upper and a lower bound on worst-case regret"; [section 3.1] "STARC metrics induce both an upper bound on the worst-case regret"

### Mechanism 2
- Claim: Any metric with both soundness and completeness properties must be bilipschitz equivalent to STARC metrics.
- Mechanism: The theoretical proof shows that the space of reward functions with these two properties has a unique geometric structure that STARC metrics capture. This means STARC metrics are essentially the unique solution to this problem.
- Core assumption: The definition of soundness and completeness captures the essential properties we care about for reward function comparison.
- Evidence anchors: [abstract] "we show that any metric with the same properties must be bilipschitz equivalent to ours"; [section 3.1] "Theorems 1 and 2 together imply that, for any STARC metric d, we have that a small value of d is both necessary and sufficient for a low regret"

### Mechanism 3
- Claim: STARC metrics achieve significantly better empirical correlation with regret than existing metrics like EPIC and DARD.
- Mechanism: The VAL canonicalization combined with appropriate normalization and distance metrics captures the relevant features of reward function differences that affect policy optimization outcomes.
- Core assumption: The experimental environments and reward generation process adequately represent the diversity of real-world scenarios.
- Evidence anchors: [section 4] "Experimental results show STARC metrics, particularly those using the VAL canonicalisation, achieve correlations of 0.856-0.873 with regret"

## Foundational Learning

- Concept: Pseudometrics vs Metrics
  - Why needed here: Understanding the difference between pseudometrics (which can assign zero distance to non-identical objects) and metrics (which require distinct objects to have positive distance) is crucial for understanding why STARC metrics can treat some different reward functions as equivalent.
  - Quick check question: What property distinguishes a pseudometric from a metric in the context of reward function comparison?

- Concept: Canonicalization functions
  - Why needed here: The canonicalization step is the core innovation that allows STARC metrics to remove irrelevant variations in reward functions while preserving the policy-relevant information.
  - Quick check question: How does the VAL canonicalization function transform a reward function to remove potential shaping and S1-redistribution effects?

- Concept: Bilipschitz equivalence
  - Why needed here: Understanding bilipschitz equivalence is essential for grasping why STARC metrics are essentially unique - any other metric with the same properties must be a scaled version of STARC metrics.
  - Quick check question: What does it mean mathematically for two metrics to be bilipschitz equivalent, and why is this significant for reward function comparison?

## Architecture Onboarding

- Component map: MDP Environment -> Reward Functions R1, R2 -> Canonicalization (VAL/EPIC/DARD) -> Normalization (L1/L2/L∞) -> Distance Computation (weighted norm) -> STARC Distance

- Critical path: Input reward functions R1, R2 -> Apply canonicalization function c to both -> Normalize using norm function n -> Compute distance using admissible metric m -> Output STARC distance value

- Design tradeoffs:
  - Computational efficiency vs theoretical tightness: Minimal canonicalization functions give tighter bounds but are computationally expensive
  - Environment dependence vs generality: STARC metrics depend on τ while EPIC aims for environment independence
  - Empirical accuracy vs theoretical guarantees: Different combinations of normalization and distance metrics trade off correlation with regret vs theoretical properties

- Failure signatures:
  - High computational cost for minimal canonicalization in large state spaces
  - Poor correlation with regret when using inappropriate norm choices
  - Numerical instability when reward functions have very small or zero values

- First 3 experiments:
  1. Implement VAL canonicalization and verify it removes potential shaping and S1-redistribution on simple test cases
  2. Compare correlation with regret for different combinations of normalization (L1, L2, L∞) and distance metrics on a small MDP
  3. Test the bilipschitz equivalence property by computing STARC distances for reward functions that differ by various transformations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do STARC metrics perform in continuous state and action spaces compared to discrete spaces?
- Basis in paper: [explicit] The authors state "our theoretical results assume that S and A are finite, it is still straightforward to compute and use STARC metrics in continuous environments" and "We think that extending our theoretical results to continuous environments will be an interesting direction for future work."
- Why unresolved: The paper only provides theoretical results for finite spaces and experimental results for discrete environments. The behavior in continuous spaces remains untested.
- What evidence would resolve it: Experimental results comparing STARC metrics to other reward distance metrics in continuous MDPs or continuous control tasks, showing correlation with regret or policy similarity.

### Open Question 2
- Question: What is the computational complexity of finding minimal canonicalisation functions for STARC metrics, and how does this impact practical usability?
- Basis in paper: [explicit] "It is not a given that minimal canonicalisation functions exist for a given norm n, or that they are unique" and "Note that this proof only shows that a minimal canonicalisation function exists and is unique when n is a (weighted) L2-norm."
- Why unresolved: The paper mentions minimal canonicalisation functions are "prohibitively expensive to compute" but doesn't provide complexity analysis or explore approximation methods.
- What evidence would resolve it: Complexity analysis showing the computational requirements for finding minimal canonicalisation functions, along with empirical measurements of computation time for different problem sizes.

### Open Question 3
- Question: How do STARC metrics behave in multi-agent environments compared to single-agent settings?
- Basis in paper: [explicit] "We have considered the MDP setting – it would be interesting to also consider other classes of environments. We believe that the multi-agent setting would be of particular interest."
- Why unresolved: The paper only evaluates STARC metrics in single-agent MDPs and doesn't explore how the metrics generalize to games or multi-agent reinforcement learning.
- What evidence would resolve it: Experimental results applying STARC metrics to multi-agent environments, showing how well they capture reward function similarity in competitive or cooperative settings, and comparing performance to single-agent cases.

## Limitations

- The theoretical uniqueness claim relies on specific definitions of soundness and completeness that may not capture all aspects of reward function similarity
- Experimental validation was limited to randomly generated MDPs with 32 states, which may not represent the diversity of real-world reward learning scenarios
- The assumption that VAL canonicalization successfully removes all irrelevant variations while preserving policy-relevant information is critical but difficult to verify empirically

## Confidence

- **High confidence**: The core mechanism of canonicalization followed by normalized distance measurement is well-established and theoretically sound.
- **Medium confidence**: The claim of uniqueness via bilipschitz equivalence is mathematically rigorous but may not fully capture practical considerations in reward function design.
- **Medium confidence**: The empirical superiority over EPIC and DARD is demonstrated but may depend on the specific experimental setup and reward generation process.

## Next Checks

1. **Cross-environment validation**: Test STARC metrics on a diverse set of MDPs beyond the random generation process used in the paper, including structured environments with known reward properties.

2. **Robustness to canonicalization choice**: Systematically compare performance across different canonicalization functions (VAL, EPIC, DARD) on the same reward pairs to isolate the effect of the canonicalization choice.

3. **Scaling behavior analysis**: Evaluate computational complexity and correlation with regret as state space size increases beyond the 32-state MDPs used in the experiments.