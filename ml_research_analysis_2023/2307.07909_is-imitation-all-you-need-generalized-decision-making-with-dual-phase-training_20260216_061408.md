---
ver: rpa2
title: Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training
arxiv_id: '2307.07909'
source_url: https://arxiv.org/abs/2307.07909
tags:
- tasks
- dualmind
- learning
- training
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DualMind, a generalist agent that can handle
  a wide range of decision-making tasks without task-specific fine-tuning. The key
  idea is a "Dual-phase" training strategy that first learns fundamental common knowledge
  through self-supervised learning and then learns how to make decisions based on
  different contexts through imitation learning with prompts.
---

# Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training

## Quick Facts
- arXiv ID: 2307.07909
- Source URL: https://arxiv.org/abs/2307.07909
- Reference count: 40
- This paper introduces DualMind, a generalist agent that can handle a wide range of decision-making tasks without task-specific fine-tuning.

## Executive Summary
This paper introduces DualMind, a generalist agent that can handle a wide range of decision-making tasks without task-specific fine-tuning. The key idea is a "Dual-phase" training strategy that first learns fundamental common knowledge through self-supervised learning and then learns how to make decisions based on different contexts through imitation learning with prompts. This approach emulates how humans learn to act in the world. The method uses an Encoder-Decoder Control Transformer to model state-action interactions from high-dimensional observations, and TokenLearner to compress tokens for computational efficiency. Experiments on MetaWorld and Habitat show that DualMind outperforms other generalist agents by over 50% and 70% on Habitat and MetaWorld, respectively, and achieves over 30 tasks at a 90% success rate on the 45 tasks in MetaWorld.

## Method Summary
DualMind uses a Dual-phase training strategy: Phase I performs self-supervised pretraining on state-action interactions to learn generic dynamics, while Phase II trains only cross-attention layers for prompt-conditioned imitation learning. The architecture combines a ViT-B encoder, TokenLearner for token compression, and an Encoder-Decoder Control Transformer with cross-attention prompting. The model is trained on 50k episodes from MetaWorld and Habitat, then evaluated zero-shot on held-out tasks and scenes.

## Key Results
- DualMind achieves over 30 tasks at a 90% success rate on the 45 tasks in MetaWorld
- Outperforms other generalist agents by over 50% and 70% on Habitat and MetaWorld, respectively
- Demonstrates strong zero-shot generalization across held-out tasks and scenes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DualMind's two-phase training structure allows the model to first learn generic state-action transition knowledge before specializing to prompt-conditioned decision-making.
- Mechanism: Phase I uses self-supervised pretraining on state-action sequences to capture shared dynamics across tasks without conditioning on prompts. Phase II freezes most of the model and trains only the cross-attention layers to map prompts to task-specific behaviors.
- Core assumption: Learning generic transition dynamics in Phase I does not conflict with or erase the ability to specialize in Phase II.
- Evidence anchors:
  - [abstract] states the model first learns "fundamental common knowledge through self-supervised learning" then "learns how to make decisions based on different contexts through imitating behaviors conditioned on given prompts."
  - [section 4.2] describes Phase I as learning a forward prediction head and recovering masked actions, while Phase II focuses on prompt-conditioned imitation learning with frozen encoders.
  - [corpus] includes "CEIL: Generalized Contextual Imitation Learning" and "Understanding Representations Pretrained with Auxiliary Losses for Embodied Agent Planning," which both explore pretraining with auxiliary tasks to improve downstream generalization.

### Mechanism 2
- Claim: Using TokenLearner as an attention-based information bottleneck reduces computational cost while retaining task-relevant information for decision-making.
- Mechanism: TokenLearner compresses the 196 ViT patch tokens into 8 informative tokens, reducing the computational burden of processing high-dimensional observations while preserving critical visual cues for control.
- Core assumption: The compressed token set still contains enough discriminative information for the downstream policy to perform well.
- Evidence anchors:
  - [section 4.1] states TokenLearner "sub-samples the 196 state tokens that come out of ViT to just 8 tokens that are then passed to the Transformer decoder layers."
  - [section 5.4] includes an ablation comparing multi-state tokens (TokenLearner) vs single-state tokens, showing improved performance on Metaworld with multi-state tokens.
  - [corpus] includes "Understanding Representations Pretrained with Auxiliary Losses for Embodied Agent Planning," which discusses auxiliary losses to extract useful representations, supporting the idea that selective compression can preserve task-relevant information.

### Mechanism 3
- Claim: Cross-attention conditioning (XAtten.) provides stronger prompt-task alignment than prefix-style prompting, leading to better zero-shot generalization.
- Mechanism: Instead of prepending prompt tokens, XAtten. layers compute attention between prompt and episode tokens, allowing dynamic fusion of context and instructions during decoding.
- Core assumption: The attention-based fusion captures richer interactions between prompts and demonstrations than simple concatenation.
- Evidence anchors:
  - [section 4.1] explains XAtten. computes "softmax(qH kT p√ d )vP , where H is the sequence of episodes, P is prompt," emphasizing the direct conditioning.
  - [section 5.4] shows XAtten. prompting achieves 0.76 SR on Metaworld vs 0.29 SR for prefix prompting, with similar gaps on Habitat.
  - [corpus] includes "CEIL: Generalized Contextual Imitation Learning," which also leverages context embeddings for conditioning, suggesting the importance of contextual fusion in imitation learning.

## Foundational Learning

- Concept: Self-supervised learning for dynamics modeling
  - Why needed here: Phase I pretraining must capture state-action transitions without task-specific labels, enabling generalization across unseen environments.
  - Quick check question: Can the model predict the next state embedding from a history of state-action pairs without prompt conditioning?

- Concept: Cross-attention for multimodal fusion
  - Why needed here: Phase II must condition on diverse prompts (images, text, annotations) while leveraging the pretrained dynamics model.
  - Quick check question: Does the cross-attention layer correctly align prompt embeddings with the episode token sequence during decoding?

- Concept: Information bottleneck via token selection
  - Why needed here: High-dimensional visual observations must be compressed for efficient transformer processing without losing task-critical details.
  - Quick check question: Does TokenLearner retain at least one token corresponding to each major object or region relevant to the task?

## Architecture Onboarding

- Component map: ViT encoder → TokenLearner → Transformer decoder (8 layers, 8 heads) → Cross-attention layers → Action heads
- Critical path: Observation → ViT → TokenLearner → decoder state tokens → cross-attention with prompt → action prediction
- Design tradeoffs: TokenLearner reduces compute but may lose detail; cross-attention adds conditioning strength but increases parameter count in Phase II; freezing most of the model in Phase II speeds training but risks rigidity.
- Failure signatures: Poor performance on novel tasks suggests Phase I did not capture generic dynamics; prompt conditioning failures suggest cross-attention misalignment; high compute usage suggests TokenLearner not compressing enough.
- First 3 experiments:
  1. Run Phase I training and check next-state prediction accuracy on held-out trajectories.
  2. Freeze encoder, train only cross-attention on a small prompt dataset, measure prompt alignment quality.
  3. Replace TokenLearner with mean pooling and compare compute time and task performance on a subset of tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DualMind's performance scale with increasing model size and dataset size?
- Basis in paper: [inferred] The paper mentions that DualMind has 175 million parameters and was trained on 100k episodes, while GATO has 1.18 billion parameters and was trained on a massive dataset. The authors note that for future scaled-up models and data, they would recommend using a different freezing strategy (2⃝ instead of 3⃝) to save computational cost.
- Why unresolved: The paper does not provide empirical evidence on how DualMind's performance changes with different model sizes or dataset sizes. It only mentions one configuration of the model.
- What evidence would resolve it: Experiments showing DualMind's performance on different model sizes (e.g., 100M, 500M, 1B parameters) and dataset sizes (e.g., 50k, 200k, 500k episodes) would provide insight into its scaling properties.

### Open Question 2
- Question: How does DualMind's Dual-phase training compare to other pretraining methods like supervised pretraining or self-supervised pretraining with different objectives?
- Basis in paper: [explicit] The paper compares DualMind to IL-only, SMART-only, and Jointly models, but does not compare it to other pretraining methods like supervised pretraining or different self-supervised objectives.
- Why unresolved: The paper only provides comparisons to a limited set of baselines and does not explore the broader landscape of pretraining methods.
- What evidence would resolve it: Experiments comparing DualMind to models pretrained with supervised objectives or different self-supervised objectives (e.g., contrastive learning, masked autoencoders) would help understand the relative effectiveness of the Dual-phase approach.

### Open Question 3
- Question: How does DualMind handle tasks with long temporal dependencies or tasks that require reasoning over extended periods?
- Basis in paper: [inferred] The paper mentions that DualMind uses a context length of 6 for both training and execution, and that longer context lengths can produce better performance, particularly on tasks that rely on long-range temporal dependencies. However, the authors chose a context length of 6 to balance performance and compute cost.
- Why unresolved: The paper does not provide empirical evidence on DualMind's performance on tasks with long temporal dependencies or tasks that require reasoning over extended periods. It only mentions that longer context lengths can be beneficial.
- What evidence would resolve it: Experiments evaluating DualMind on tasks with long temporal dependencies (e.g., multi-step planning, long-horizon navigation) using different context lengths would provide insight into its ability to handle such tasks.

## Limitations

- Phase I pretraining effectiveness is correlative rather than experimentally isolated through ablation studies
- TokenLearner compression trade-offs lack visualization of what visual information is being preserved or lost
- Prompt conditioning robustness is only tested with "informative" prompts, not degraded or contradictory inputs
- Generalization bounds are not characterized - limits of cross-task applicability remain unclear

## Confidence

- **High confidence**: The architectural implementation (ViT + TokenLearner + Control Transformer) is clearly specified and the Phase II imitation learning pipeline appears reproducible. The performance metrics and benchmark comparisons are well-documented.
- **Medium confidence**: The two-phase training framework is logically sound and the reported performance gains are substantial. However, the causal relationship between Phase I pretraining and Phase II success is inferred rather than experimentally isolated.
- **Low confidence**: Claims about learning "fundamental common knowledge" and the universal applicability of the approach to any sequential decision-making task. These are theoretical assertions that require more rigorous validation across diverse domains.

## Next Checks

1. **Phase I ablation study**: Train a baseline model without Phase I pretraining (random initialization) and compare performance on the same 45 MetaWorld tasks. This would directly test whether self-supervised pretraining provides measurable benefits beyond random initialization.

2. **TokenLearner sensitivity analysis**: Systematically vary the number of output tokens (1, 4, 8, 16) and measure both computational cost and task performance. Include visualization of what visual regions are preserved at each compression level to understand information bottlenecks.

3. **Prompt robustness testing**: Evaluate performance using degraded prompts: remove object annotations, use ambiguous language instructions, or provide contradictory multimodal prompts. Measure how performance degrades as prompt quality decreases to establish the approach's robustness to real-world input variability.