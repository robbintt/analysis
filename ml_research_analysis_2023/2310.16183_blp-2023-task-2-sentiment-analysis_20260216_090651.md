---
ver: rpa2
title: 'BLP-2023 Task 2: Sentiment Analysis'
arxiv_id: '2310.16183'
source_url: https://arxiv.org/abs/2310.16183
tags:
- task
- sentiment
- bangla
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The BLP-2023 Task 2 focused on sentiment analysis of Bangla social
  media text. The shared task attracted 71 registered participants, with 29 and 30
  teams submitting systems during development and evaluation phases, respectively.
---

# BLP-2023 Task 2: Sentiment Analysis

## Quick Facts
- arXiv ID: 2310.16183
- Source URL: https://arxiv.org/abs/2310.16183
- Reference count: 19
- Best system achieved micro-F1 score of 0.73

## Executive Summary
The BLP-2023 Task 2 focused on sentiment analysis of Bangla social media text. The shared task attracted 71 registered participants, with 29 and 30 teams submitting systems during development and evaluation phases, respectively. Participants submitted a total of 597 runs, employing approaches ranging from classical machine learning models to fine-tuning pre-trained models and leveraging Large Language Models (LLMs) in zero- and few-shot settings. The best-performing system achieved a micro-F1 score of 0.73, demonstrating an absolute improvement of 0.23 over the majority baseline. The top systems primarily used ensemble methods of fine-tuned transformer-based models, with BanglaBERT being the most popular choice. However, the neutral class remained challenging to identify across all submissions.

## Method Summary
The task used the combined MUBASE and SentNoB datasets, containing 35,266 training entries from social media posts. Teams fine-tuned transformer models (primarily BanglaBERT, multilingual BERT, and XLM-RoBERTa) on the training data, with many employing ensemble methods combining multiple models. Preprocessing involved removing URLs, HTML tags, and normalization. The evaluation metric was micro-F1 score, with majority and random baselines provided for comparison.

## Key Results
- Best-performing system achieved micro-F1 score of 0.73
- Top systems primarily used ensemble methods of fine-tuned transformer-based models
- BanglaBERT was the most popular choice among participants
- Neutral class remained challenging to identify across all submissions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned transformer models outperform classical machine learning on Bangla sentiment classification.
- Mechanism: Pretrained multilingual and Bangla-specific BERT models capture complex syntactic and semantic patterns in Bangla text that bag-of-words or TF-IDF approaches miss, enabling better generalization on social media data.
- Core assumption: The pretrained weights encode useful linguistic features that can be adapted to the low-resource Bangla domain with limited labeled data.
- Evidence anchors:
  - [abstract] "The top systems primarily used ensemble methods of fine-tuned transformer-based models, with BanglaBERT being the most popular choice."
  - [section] "Kabir et al. (2023) curated the largest dataset from book reviews... However, the dataset encompasses a large number of reviews, the class distribution poses a challenge for the Negative and Neutral classes."
  - [corpus] Weak. Only general paper metadata is available; no direct transformer vs. classical ML performance data.
- Break condition: If the dataset contains too much noise or highly domain-specific slang that pretrained embeddings don't cover, fine-tuning may not help much.

### Mechanism 2
- Claim: Ensemble methods improve sentiment classification performance compared to single models.
- Mechanism: Combining predictions from multiple fine-tuned models reduces individual model bias and variance, especially for the challenging neutral class, by majority voting or weighted averaging.
- Core assumption: Different models capture complementary aspects of the data; their errors are not fully correlated.
- Evidence anchors:
  - [abstract] "The best-performing system achieved a micro-F1 score of 0.73, demonstrating an absolute improvement of 0.23 over the majority baseline. The top systems primarily used ensemble methods of fine-tuned transformer-based models."
  - [section] "RSM-NLP... submitted ensemble techniques (i.e., weighted and majority-voted) of fine-tuned models."
  - [corpus] Weak. No direct quantitative comparison of ensemble vs. single model performance in the corpus.
- Break condition: If all models overfit the same patterns, ensemble may not improve performance.

### Mechanism 3
- Claim: Data augmentation and pseudo-labeling can mitigate class imbalance in the neutral class.
- Mechanism: Generating synthetic examples or assigning labels to unlabeled data increases training samples for underrepresented classes, improving model robustness.
- Core assumption: The augmented data is realistic enough that the model learns generalizable patterns rather than memorizing noise.
- Evidence anchors:
  - [section] "Knowdee (Liu et al., 2023)... partitioned the data set into 10 folds and generated pseudo-labels for unlabeled data using a fine-tuned ensemble of models."
  - [section] "Ufal-ULD... employed a focal loss function to address hard-to-classify examples."
  - [corpus] Weak. No specific results showing impact of augmentation on neutral class performance.
- Break condition: If augmentation introduces label noise, model performance may degrade.

## Foundational Learning

- Concept: Multilabel vs. multiclass classification
  - Why needed here: Sentiment analysis in this task is multiclass (positive, neutral, negative), not multilabel; models must predict one sentiment polarity per post.
  - Quick check question: If a post expresses both positive and negative sentiment, how should it be labeled in this dataset?
- Concept: Class imbalance and its effects
  - Why needed here: The neutral class is underrepresented, leading to lower F1 scores; understanding imbalance is key to applying techniques like up-sampling or focal loss.
  - Quick check question: What would happen to precision and recall if a model always predicts the majority class?
- Concept: Cross-lingual transfer learning
  - Why needed here: Multilingual BERT and XLM-RoBERTa are trained on many languages; understanding how they adapt to Bangla helps in choosing the right model and fine-tuning strategy.
  - Quick check question: Why might a multilingual model perform worse than a Bangla-specific model on this task?

## Architecture Onboarding

- Component map: Data preprocessing → Tokenization (WordPiece/BPE) → Transformer encoder (BERT/BanglaBERT) → Classification head (softmax) → Ensemble layer (optional) → Evaluation (micro-F1)
- Critical path: Clean social media text → Tokenize → Fine-tune transformer → Predict sentiment → Ensemble → Evaluate
- Design tradeoffs: BanglaBERT vs. multilingual BERT: higher resource cost vs. potential performance gain; ensemble vs. single model: complexity vs. marginal improvement
- Failure signatures: Overfitting to training set (high train F1, low dev/test F1); underfitting (low scores across all splits); poor neutral class recall (class imbalance effects)
- First 3 experiments:
  1. Fine-tune BanglaBERT on training set, evaluate on dev set, tune learning rate
  2. Repeat with multilingual BERT, compare F1 scores
  3. Combine both models via weighted voting, measure ensemble improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific data augmentation techniques would be most effective for improving neutral class identification in Bangla sentiment analysis?
- Basis in paper: [explicit] The paper notes that the neutral class remains challenging to identify and suggests data augmentation as a potential solution, though it was not thoroughly explored across all teams.
- Why unresolved: While some teams used data augmentation, the paper does not provide detailed analysis of which techniques were most effective or why they failed to significantly improve neutral class performance.
- What evidence would resolve it: Comparative analysis of different data augmentation methods (e.g., synonym replacement, back-translation, contextual augmentation) specifically targeting the neutral class, with clear performance metrics.

### Open Question 2
- Question: How would incorporating aspect-based sentiment analysis improve performance compared to the current post-level polarity classification approach?
- Basis in paper: [inferred] The limitations section explicitly states that the current task focuses on post-level polarity classification, limiting the identification of specific sentiment aspects and other crucial elements associated with them.
- Why unresolved: The paper only describes current limitations without providing experimental data comparing post-level versus aspect-based approaches for Bangla sentiment analysis.
- What evidence would resolve it: Implementation and evaluation of an aspect-based sentiment analysis system for Bangla, with direct performance comparison to the current post-level approach.

### Open Question 3
- Question: What is the optimal sequence length for transformer-based models when processing Bangla social media text for sentiment analysis?
- Basis in paper: [explicit] The paper analyzes the distribution of sentences based on word count and mentions creating various ranges of sentence length buckets to understand and define sequence length while training transformer-based models.
- Why unresolved: While the paper provides distribution statistics, it does not report on how different sequence lengths affected model performance or which length was optimal.
- What evidence would resolve it: Systematic evaluation of model performance across different sequence length configurations, identifying the optimal trade-off between computational efficiency and accuracy.

## Limitations
- Dataset class imbalance, particularly for the neutral class, remains a significant challenge
- No detailed methodology descriptions for preprocessing or hyperparameter configurations
- No individual system performance breakdowns provided

## Confidence
- **High Confidence**: Transformer-based models outperform classical ML approaches; ensemble methods improve overall performance; BanglaBERT is the most popular model choice
- **Medium Confidence**: Ensemble methods improve neutral class identification; data augmentation helps mitigate class imbalance effects
- **Low Confidence**: Exact impact of different preprocessing strategies; specific hyperparameter configurations that led to optimal performance

## Next Checks
1. Reproduce baseline results: Implement majority and random baselines on the combined MUBASE and SentNoB dataset to verify the reported 0.23 improvement margin
2. Class-specific performance analysis: Train individual models (BanglaBERT, multilingual BERT) and measure their performance on each sentiment class separately to identify where ensemble methods provide the most benefit
3. Ablation study of ensemble components: Systematically remove individual models from top-performing ensembles to quantify each model's contribution and validate the assumption that errors are not fully correlated