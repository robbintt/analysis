---
ver: rpa2
title: Experimental Analysis of Large-scale Learnable Vector Storage Compression
arxiv_id: '2311.15578'
source_url: https://arxiv.org/abs/2311.15578
tags:
- embedding
- compression
- methods
- memory
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive experimental analysis of embedding
  compression methods for large-scale machine learning models, focusing on recommendation
  systems and retrieval-augmented language models. The authors introduce a new taxonomy
  categorizing compression techniques into inter-feature (sharing embeddings across
  features) and intra-feature (compressing individual embeddings) approaches, then
  develop a modular benchmarking framework to evaluate 14 representative methods across
  various memory budgets.
---

# Experimental Analysis of Large-scale Learnable Vector Storage Compression

## Quick Facts
- arXiv ID: 2311.15578
- Source URL: https://arxiv.org/abs/2311.15578
- Reference count: 40
- This paper conducts a comprehensive experimental analysis of embedding compression methods for large-scale machine learning models, focusing on recommendation systems and retrieval-augmented language models.

## Executive Summary
This paper presents a systematic experimental analysis of embedding compression techniques for large-scale machine learning models. The authors introduce a novel taxonomy categorizing compression methods into inter-feature (sharing embeddings across features) and intra-feature (compressing individual embeddings) approaches. Through extensive experiments on three datasets using DLRM models and retrieval-augmented LLMs, the study evaluates 14 representative compression methods across various memory budgets. The results demonstrate that quantization and pruning methods achieve the best model quality with the lowest inference latency, while hash-based methods provide the most flexible compression ratios.

## Method Summary
The study evaluates 14 representative embedding compression methods across different memory budgets using a modular benchmarking framework. The methods are categorized into inter-feature (sharing embeddings across features) and intra-feature (compressing individual embeddings) approaches. Experiments were conducted on three datasets (Avazu, Criteo, and a commercial Company dataset) using DLRM models, as well as on a Natural Questions dataset with Wikipedia corpus for retrieval-augmented LLMs. The evaluation metrics include model quality (AUC for DLRM, Exact Match for RAG), memory usage, training memory, training time, and inference latency. Training was performed with Adam optimizer, batch sizes of 64-128, and learning rates from [0.001, 0.01, 0.1], with early stopping strategies implemented.

## Key Results
- Quantization and pruning methods achieve the best model quality with the lowest inference latency for embedding compression.
- Hash-based methods provide the most flexible compression ratios across different memory budgets.
- These compression techniques can be effectively applied to retrieval-augmented LLMs, with INT8/16 quantization and pruning showing the best performance in terms of embedding quality and compression speed.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding compression methods achieve significant memory savings by sharing or reducing representation dimensionality while preserving model performance.
- Mechanism: The paper proposes a taxonomy dividing compression techniques into inter-feature (sharing embeddings across features) and intra-feature (compressing individual embeddings) approaches. Inter-feature methods use encoding functions to map multiple features to shared embeddings, while intra-feature methods apply quantization, dimension reduction, or pruning to individual embeddings.
- Core assumption: The semantic similarity between features allows for safe sharing without significant loss of model quality, and the importance of features justifies allocating different amounts of memory to different embeddings.
- Evidence anchors:
  - [abstract] "The authors introduce a new taxonomy categorizing compression techniques into inter-feature (sharing embeddings across features) and intra-feature (compressing individual embeddings) approaches"
  - [section] "Inter-feature compression forces features to share embeddings within a limited memory space, as shown in Figure 2(c)"
  - [corpus] Weak corpus support - no direct matches found for the specific taxonomy division
- Break condition: If the semantic similarity assumption fails (e.g., features that should be distinct are forced to share embeddings), model quality will degrade significantly. Similarly, if the importance-based allocation is inaccurate, important features may be under-represented.

### Mechanism 2
- Claim: Quantization and pruning methods achieve the best model quality with the lowest inference latency for embedding compression.
- Mechanism: Quantization reduces memory usage by converting floating-point embeddings to lower-precision data types (e.g., INT8/16), while pruning removes less important values from embeddings. Both methods maintain the original training paradigm while significantly reducing memory footprint.
- Core assumption: The information loss from quantization and pruning is minimal and can be compensated by proper optimization techniques.
- Evidence anchors:
  - [abstract] "Experiments on three datasets using DLRM models reveal that quantization and pruning methods achieve the best model quality with the lowest inference latency"
  - [section] "Directly using FP16 or INT8/16 is simple and has almost no overhead"
  - [corpus] Moderate corpus support - several papers mention quantization and pruning in embedding compression contexts
- Break condition: If the precision reduction in quantization is too extreme or pruning removes too many values, model quality will degrade beyond acceptable levels.

### Mechanism 3
- Claim: Hash-based methods provide the most flexible compression ratios for embedding compression.
- Mechanism: Hash-based methods use various hashing techniques to map features to a smaller set of embeddings, allowing for flexible adjustment of the number of embeddings based on memory constraints.
- Core assumption: The hash functions can effectively distribute features across embeddings without excessive collisions that would degrade model quality.
- Evidence anchors:
  - [abstract] "the study also demonstrates that these compression techniques can be effectively applied to retrieval-augmented LLMs, with INT8/16 quantization and pruning showing the best performance in terms of embedding quality and compression speed"
  - [section] "Hash-based methods (including LSH) and pruning methods are the most capable compression methods, achieving all compression ratios"
  - [corpus] Strong corpus support - multiple papers discuss hash-based approaches for embedding compression
- Break condition: If hash collisions become too frequent due to aggressive compression, model quality will suffer significantly.

## Foundational Learning

- Concept: Embedding tables and their role in recommendation systems and retrieval-augmented LLMs
  - Why needed here: Understanding the structure and purpose of embedding tables is crucial for grasping why compression is necessary and how different methods work
  - Quick check question: What is the primary function of embedding tables in DLRMs and retrieval-augmented LLMs?
- Concept: Memory constraints in large-scale machine learning models
  - Why needed here: The motivation for embedding compression stems from the memory limitations of storing large embedding tables
  - Quick check question: Why do large-scale recommendation systems and retrieval-augmented LLMs face memory challenges?
- Concept: Trade-offs between model quality and compression ratio
  - Why needed here: Different compression methods achieve different balances between memory savings and performance degradation
  - Quick check question: What is the fundamental trade-off that all embedding compression methods must navigate?

## Architecture Onboarding

- Component map: Feature → Encoding function → Embedding layer → Neural network → Prediction
- Critical path: Feature → Encoding function → Embedding layer → Neural network → Prediction
- Design tradeoffs:
  - Memory vs. model quality: More aggressive compression saves memory but may reduce performance
  - Training complexity vs. inference efficiency: Some methods require complex training but simple inference, while others are simple to train but complex to infer
  - Flexibility vs. performance: Hash-based methods offer flexible compression ratios but may have higher collision rates
- Failure signatures:
  - Model quality degradation: If compression is too aggressive or poorly implemented
  - Increased training time: Some methods require multiple training stages or complex optimization
  - Higher inference latency: Methods with complex embedding lookup or decompression
- First 3 experiments:
  1. Baseline comparison: Run the model with uncompressed embeddings to establish baseline performance
  2. Simple quantization: Apply INT8/16 quantization to verify basic compression functionality
  3. Pruning test: Implement pruning to validate sparse tensor storage and computation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of recommendation datasets determine which embedding compression method will perform best?
- Basis in paper: [explicit] "Our experiments show that different methods perform better on different datasets in DLRM, but it is unclear why. It is currently difficult to determine a proper method for a given dataset without actual experiments."
- Why unresolved: The paper demonstrates that methods like quantization, pruning, and static encoding perform differently across datasets (Avazu, Criteo, Company), but doesn't identify the underlying dataset features that predict this performance.
- What evidence would resolve it: Empirical studies correlating dataset characteristics (feature cardinality distribution, frequency power-law exponent, feature-field correlations, data sparsity patterns) with compression method performance across multiple datasets.

### Open Question 2
- Question: Can we develop a unified compression framework that combines the best properties of inter-feature and intra-feature methods without their individual limitations?
- Basis in paper: [inferred] "A straightforward idea is to combine the advantages of different compression methods in DLRM" and the paper shows each method has distinct trade-offs (memory usage, training overhead, inference latency, model quality).
- Why unresolved: Current methods are either inter-feature (sharing embeddings) or intra-feature (compressing individual embeddings), with no hybrid approach that achieves optimal compression ratios, model quality, and minimal overhead simultaneously.
- What evidence would resolve it: Development and experimental validation of a unified framework that dynamically switches between compression strategies based on feature importance and memory constraints, demonstrating superior performance across all metrics.

### Open Question 3
- Question: How can we design embedding compression methods specifically optimized for retrieval-augmented LLMs that account for their unique embedding characteristics?
- Basis in paper: [explicit] "Currently, embedding compression for retrieval tasks mainly uses quantization or PQ... We anticipate that compression methods specifically designed for retrieval will emerge in the future"
- Why unresolved: The paper shows that retrieval embeddings (post-training, high-dimensional, 768D vs DLRM's 16D) require different compression strategies than parametric embeddings, but current methods don't account for these differences.
- What evidence would resolve it: Development of retrieval-specific compression techniques that optimize for the trade-off between embedding quality (EM scores), compression speed, and batched-decompression latency unique to retrieval tasks.

## Limitations

- The experimental scope is constrained to specific model architectures (DLRM and RAG with BART/DPR), which may not generalize to other architectures like transformers or graph neural networks.
- The evaluation focuses primarily on memory usage and inference latency, with limited discussion of training efficiency and scalability to truly massive embedding tables (e.g., 10B+ parameters).
- The comparison across methods uses fixed memory budgets rather than proportional scaling, which may obscure the true efficiency of different approaches.

## Confidence

- **High Confidence**: The taxonomy classification (inter-feature vs. intra-feature) and the experimental framework setup are well-documented and reproducible. The finding that quantization and pruning achieve the best quality-latency trade-off is supported by extensive empirical evidence across three datasets.
- **Medium Confidence**: The claim that hash-based methods provide the most flexible compression ratios is supported by the experimental results but may be dataset-dependent. The assertion that these methods generalize to RAG applications is demonstrated but with a limited dataset and model configuration.
- **Low Confidence**: The relative performance rankings of more complex methods (TT-Rec, DHE, AutoML-based approaches) are less certain due to their limited success in the experiments and the lack of ablation studies to understand failure modes.

## Next Checks

1. **Ablation study on compression stages**: For multi-stage methods (MGQE, Dedup), conduct experiments that isolate each stage's contribution to the final performance to understand whether the complexity is justified by the gains.

2. **Cross-architecture generalization**: Apply the top-performing methods (quantization, pruning) to transformer-based architectures and evaluate whether the quality-latency trade-offs remain consistent or if new failure modes emerge.

3. **Scalability analysis**: Scale the experiments to embedding tables with 10B+ parameters using distributed training configurations to assess whether the observed trends hold at extreme scales and to identify any memory or communication bottlenecks.