---
ver: rpa2
title: Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating
  Classifier Guidance
arxiv_id: '2307.04081'
source_url: https://arxiv.org/abs/2307.04081
tags:
- data
- labeled
- classifier
- score
- conditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for conditional generation with score-based
  generative models (SGMs) when limited labeled data is available. The key idea is
  to self-calibrate the classifier used in classifier-guided SGMs (CGSGMs) by converting
  it into another view of the unconditional SGM using principles from energy-based
  models.
---

# Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance

## Quick Facts
- arXiv ID: 2307.04081
- Source URL: https://arxiv.org/abs/2307.04081
- Reference count: 40
- This paper proposes a method for conditional generation with score-based generative models (SGMs) when limited labeled data is available.

## Executive Summary
This paper addresses the challenge of conditional generation with score-based generative models (SGMs) when limited labeled data is available. The key innovation is a self-calibrating classifier guidance (CGSGM-SC) method that improves the accuracy of classifier gradients used in conditional generation. By reinterpreting the classifier as an energy-based model (EBM) and applying denoising score matching (DSM) loss, the method calibrates the classifier to produce more reliable gradients. This allows leveraging unlabeled data for further improvement in semi-supervised settings. Experiments on CIFAR-10 and CIFAR-100 demonstrate that CGSGM-SC outperforms vanilla CGSGM and state-of-the-art techniques, especially when only 5% of the data is labeled.

## Method Summary
The method involves training an unconditional SGM (NCSN++) and a classifier (adapted from NCSN++ encoder) with cross-entropy loss and a self-calibration (SC) loss derived from denoising score matching. The SC loss calibrates the classifier by matching its score function (obtained via gradient of LogSumExp of logits) to the ground-truth score of the noise-perturbed distribution. This calibration is applied to both labeled and unlabeled data, improving the classifier's accuracy in semi-supervised settings. At sampling time, the unconditional SGM score is combined with the calibrated classifier score for conditional generation. The approach avoids dependence on a pre-trained unconditional SGM for calibration, enabling parallel training.

## Key Results
- CGSGM-SC outperforms vanilla CGSGM and state-of-the-art techniques on CIFAR-10 and CIFAR-100.
- Significant improvements in generation accuracy when only 5% of the data is labeled.
- Leveraging unlabeled data through self-calibration further improves class-conditional generation quality.
- Self-calibration provides a sound theoretical guarantee by converting the classifier to another view of the unconditional SGM.

## Why This Works (Mechanism)

### Mechanism 1
Self-calibrating the classifier via denoising score matching reduces overconfidence in gradient estimates, improving conditional generation accuracy. The classifier is reinterpreted as an energy-based model (EBM) so that its logits can be transformed into a score function using ∇xLogSumExpy(fϕ,t(x)[y]). This score is then calibrated by matching it to the ground-truth score of the noise-perturbed distribution via DSM loss. The calibrated score is more aligned with the true data distribution, producing stable and continuous gradients for CGSGM.

### Mechanism 2
Applying the self-calibration loss to both labeled and unlabeled data improves classifier quality in semi-supervised settings. The DSM-based self-calibration loss can be computed on any data point, labeled or unlabeled. This allows the classifier to learn a more accurate score function from the full dataset, not just the limited labeled subset, thereby improving conditional generation accuracy when labeled data are scarce.

### Mechanism 3
Self-calibration removes the need for external SGM dependence during classifier training, enabling parallel training. By deriving the self-calibration loss from first principles (DSM applied to the EBM reinterpretation), the classifier can be trained independently of the unconditional SGM. This avoids the sequential training bottleneck of approaches like CG-DLSM and retains the computational efficiency of vanilla CGSGM.

## Foundational Learning

- **Concept:** Denoising Score Matching (DSM)
  - **Why needed here:** DSM provides a tractable, scalable way to learn score functions from noisy data, replacing expensive Hessian calculations. It is the backbone of both unconditional SGM training and the self-calibration loss.
  - **Quick check question:** In DSM, what is the target of the loss when training on noise-perturbed data? *(Answer: the score of the noise distribution centered at the clean data point.)*

- **Concept:** Energy-Based Models (EBMs)
  - **Why needed here:** Reinterpreting the classifier as an EBM allows its logits to be transformed into a score function via gradient of LogSumExp, enabling calibration via DSM.
  - **Quick check question:** How do you obtain the score function from an EBM's energy function E(x)? *(Answer: by taking the gradient: ∇x(-log p(x)) = -∇xE(x), so score is proportional to -∇xE(x).)*

- **Concept:** Classifier Guidance in Score-Based Models
  - **Why needed here:** Understanding the decomposition ∇x log p(x|y) = ∇x log p(x) + ∇x log p(y|x) is essential to see how an auxiliary classifier guides unconditional sampling toward a target class.
  - **Quick check question:** In classifier-guided SGM, which component is learned by the unconditional SGM and which by the classifier? *(Answer: ∇x log p(x) by SGM, ∇x log p(y|x) by classifier.)*

## Architecture Onboarding

- **Component map:** Unconditional SGM (NCSN++) -> Classifier (encoder part of NCSN++) -> Self-calibration loss (DSM-based) -> Total classifier loss (cross-entropy + self-calibration)
- **Critical path:** 1. Train unconditional SGM (parallel to classifier). 2. Train classifier with cross-entropy + self-calibration loss on labeled + unlabeled data. 3. At sampling time, combine unconditional SGM score with scaled classifier score for conditional generation.
- **Design tradeoffs:** Self-calibration loss adds computation per batch but removes dependence on a pre-trained unconditional SGM for calibration. Including unlabeled data in LSC improves semi-supervised performance but assumes unlabeled data share the same distribution as labeled data. λSC and λCG are hyperparameters: λSC controls strength of self-calibration; λCG controls guidance strength and trade-off between diversity and fidelity.
- **Failure signatures:** Over-regularization (λSC too high) → classifier gradients become too smooth, losing discriminative power. Under-regularization (λSC too low) → gradients remain noisy and overconfident. Poor unlabeled data quality → LSC introduces bias, degrading classifier calibration. Numerical instability in LogSumExp → NaN or exploding gradients.
- **First 3 experiments:** 1. Train vanilla CGSGM on CIFAR-10 (5% labeled) and measure intra-FID and generation accuracy. 2. Add self-calibration loss (λSC=1.0) to classifier training on same data; compare intra-FID and accuracy. 3. Vary λSC (0.1, 1.0, 10.0) and observe impact on generation accuracy and unconditional metrics.

## Open Questions the Paper Calls Out

### Open Question 1
Does self-calibration improve classifier-guided SGMs on larger datasets like ImageNet? The paper only tested on CIFAR-10 and CIFAR-100 due to computational limitations. It mentions that testing on larger datasets could strengthen the claims but was not affordable.

### Open Question 2
Is there a principled way to choose the optimal scaling factor λCG for classifier guidance, rather than tuning it empirically? The paper mentions tuning the scaling factor λCG but does not provide a principled method for choosing it.

### Open Question 3
How does self-calibration compare to other methods of improving classifier-guided SGMs, such as adversarial robustness? The paper mentions that robust CGSGM trains an adversarial robust classifier but lacks theoretical guarantee. It proposes self-calibration as an alternative with sound theoretical justification.

## Limitations
- The theoretical foundation linking EBM reinterpretation to DSM calibration assumes that classifier logits, when transformed via LogSumExp, yield a valid energy function whose gradient approximates the true score function. This assumption is not extensively validated across diverse classifier architectures or data modalities.
- The effectiveness of self-calibration on unlabeled data relies on the assumption that unlabeled data share the same distribution as labeled data, which may not hold in real-world semi-supervised settings.
- The approach has not been tested on larger datasets like ImageNet due to computational limitations, so its scalability is uncertain.

## Confidence
- **High confidence**: The core mechanism of self-calibrating the classifier via denoising score matching reduces overconfidence in gradient estimates, improving conditional generation accuracy.
- **Medium confidence**: Applying the self-calibration loss to both labeled and unlabeled data improves classifier quality in semi-supervised settings.
- **Medium confidence**: Self-calibration removes the need for external SGM dependence during classifier training, enabling parallel training.

## Next Checks
1. **Architecture Robustness**: Test CGSGM-SC with different classifier architectures (e.g., ResNet, Vision Transformer) to verify if the self-calibration mechanism generalizes beyond the NCSN++ encoder.
2. **Distribution Shift**: Evaluate CGSGM-SC on semi-supervised datasets with known distribution shifts between labeled and unlabeled data (e.g., CIFAR-10 vs CIFAR-100) to assess the impact of unlabeled data quality on classifier calibration.
3. **Hyperparameter Sensitivity**: Conduct an ablation study varying λSC and λCG across a wider range (e.g., 0.01, 0.1, 1.0, 10.0, 100.0) to identify optimal values and understand the trade-offs between diversity and fidelity in conditional generation.