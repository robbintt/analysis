---
ver: rpa2
title: 'Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models'
arxiv_id: '2312.17661'
source_url: https://arxiv.org/abs/2312.17661
tags:
- reasoning
- commonsense
- gemini
- uni00000011
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the commonsense reasoning capabilities of
  state-of-the-art Large Language Models (LLMs) and Multimodal Large Language Models
  (MLLMs), with a focus on Google's Gemini Pro and Gemini Pro Vision. Through comprehensive
  experiments on 12 diverse commonsense reasoning datasets spanning language and vision
  tasks, the authors find that while Gemini Pro exhibits competitive performance compared
  to GPT-3.5 Turbo, it lags behind GPT-4 Turbo in accuracy and struggles with temporal
  and social reasoning tasks.
---

# Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2312.17661
- Source URL: https://arxiv.org/abs/2312.17661
- Reference count: 10
- Primary result: Comprehensive evaluation of Gemini Pro's commonsense reasoning capabilities across 12 diverse datasets, revealing competitive performance compared to GPT-3.5 Turbo but lagging behind GPT-4 Turbo and GPT-4V, with particular struggles in temporal, social, and visual emotion recognition tasks.

## Executive Summary
This study provides the first comprehensive evaluation of Google's Gemini Pro and Gemini Pro Vision on commonsense reasoning tasks, comparing them against other leading LLMs including GPT-3.5 Turbo, GPT-4 Turbo, and Llama2-70b-chat. Through extensive experiments on 12 diverse datasets spanning language and vision tasks, the authors reveal that while Gemini Pro exhibits competitive performance in many domains, it struggles particularly with temporal and social reasoning, and Gemini Pro Vision underperforms in emotion recognition from images. The paper identifies five common error types through manual analysis and provides valuable insights into the current limitations of MLLMs in commonsense reasoning applications.

## Method Summary
The study evaluates four LLMs (Llama2-70b, Gemini Pro, GPT-3.5 Turbo, and GPT-4 Turbo) and two MLLMs (GPT-4V and Gemini Pro Vision) on 12 commonsense reasoning datasets, including 11 language-based and one multimodal dataset. Experiments are conducted using zero-shot standard prompting and few-shot chain-of-thought prompting for LLMs, with zero-shot SP for MLLMs. The evaluation covers general, contextual, abductive, event, temporal, numerical, physical, science, riddle, social, moral, and visual reasoning tasks. Accuracy is used as the primary metric, with manual error analysis conducted on a sample of questions to identify common failure modes.

## Key Results
- Gemini Pro performs competitively with GPT-3.5 Turbo but lags behind GPT-4 Turbo by 7.3% in zero-shot and 9.0% in few-shot settings
- Gemini Pro Vision underperforms GPT-4V overall, particularly struggling with emotion recognition from images (32.6% of total errors)
- Temporal and social reasoning emerge as key weaknesses for Gemini Pro across multiple datasets
- Context misinterpretation accounts for 30.2% of Gemini Pro's errors, while emotion recognition failures dominate Gemini Pro Vision's challenges

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The study's comprehensive evaluation across 12 diverse commonsense reasoning datasets provides a more accurate assessment of Gemini Pro's capabilities compared to limited prior benchmarks.
- **Mechanism**: By testing on a wider variety of tasks (general, contextual, abductive, event, temporal, numerical, physical, science, riddle, social, moral, and visual reasoning), the study captures a more complete picture of Gemini Pro's strengths and weaknesses.
- **Core assumption**: The selected datasets are representative of the diverse types of commonsense reasoning tasks that models are expected to handle in real-world applications.
- **Evidence anchors**: The study experiments with 12 datasets ranging from general to domain-specific tasks, and related papers focus on evaluating MLLMs on various reasoning tasks.

### Mechanism 2
- **Claim**: Gemini Pro exhibits competitive commonsense reasoning capabilities, performing comparably to GPT-3.5 Turbo in language-only tasks but lagging behind GPT-4 Turbo and GPT-4V in accuracy.
- **Mechanism**: The evaluation results demonstrate that Gemini Pro can effectively reason about commonsense knowledge in various domains, although it still has room for improvement compared to more advanced models.
- **Core assumption**: The accuracy scores obtained on the benchmark datasets are a reliable indicator of the models' real-world commonsense reasoning performance.
- **Evidence anchors**: GPT-4 Turbo outperforms other models across majority of datasets, surpassing Gemini Pro by 7.3% in zero-shot and 9.0% in few-shot settings.

### Mechanism 3
- **Claim**: Error analysis reveals common challenges faced by current LLMs and MLLMs in addressing commonsense problems, such as context misinterpretation, logical errors, and knowledge gaps.
- **Mechanism**: By examining the types of errors made by the models, the study identifies specific areas where commonsense reasoning capabilities need further development.
- **Core assumption**: The identified error types are representative of the broader challenges faced by LLMs and MLLMs in commonsense reasoning tasks.
- **Evidence anchors**: Gemini Pro often misunderstands provided contextual information (30.2% of errors), while Gemini Pro Vision struggles with identifying emotional stimuli in images (32.6% of errors).

## Foundational Learning

- **Concept**: Multimodal Large Language Models (MLLMs)
  - **Why needed here**: The study focuses on evaluating the commonsense reasoning capabilities of MLLMs, particularly Gemini Pro and Gemini Pro Vision, and comparing them to other LLMs and MLLMs.
  - **Quick check question**: What is the main difference between LLMs and MLLMs, and how does this difference impact their ability to reason about commonsense knowledge?

- **Concept**: Commonsense Reasoning
  - **Why needed here**: The study aims to assess the models' ability to understand and reason about everyday knowledge and situations, which is a key aspect of human cognition.
  - **Quick check question**: What are the main types of commonsense reasoning tasks, and why are they important for evaluating the performance of LLMs and MLLMs?

- **Concept**: Benchmarking and Evaluation Metrics
  - **Why needed here**: The study uses a variety of benchmark datasets and accuracy as the performance metric to assess the models' commonsense reasoning capabilities.
  - **Quick check question**: What are the advantages and limitations of using accuracy as the primary evaluation metric for commonsense reasoning tasks, and how might other metrics provide a more comprehensive assessment?

## Architecture Onboarding

- **Component map**: 4 LLMs (Llama2-70b, Gemini Pro, GPT-3.5 Turbo, GPT-4 Turbo) and 2 MLLMs (GPT-4V, Gemini Pro Vision) -> 12 commonsense reasoning datasets (11 language, 1 multimodal) -> Zero-shot SP and few-shot CoT prompting methods -> Accuracy evaluation and manual error analysis

- **Critical path**: Dataset selection → Prompt preparation → Model evaluation (zero-shot SP and few-shot CoT) → Result collection → Manual error analysis → Comparative analysis

- **Design tradeoffs**: The study balances comprehensive evaluation across 12 diverse datasets with computational resource constraints, using API access to models and sampling 200 examples per language dataset and 50 from VCR multimodal dataset.

- **Failure signatures**: Context misinterpretation (30.2% of Gemini Pro errors), logical errors, text ambiguity, overgeneralization, knowledge errors, emotion recognition errors (32.6% of Gemini Pro Vision errors), and spatial perception errors.

- **First 3 experiments**:
  1. Evaluate Gemini Pro and other LLMs on CommonsenseQA dataset using zero-shot SP and few-shot CoT prompting to assess general commonsense reasoning capabilities.
  2. Evaluate Gemini Pro Vision and GPT-4V on VCR dataset to compare visual commonsense reasoning abilities in answering questions, providing rationales, and combining both.
  3. Conduct qualitative analysis of Gemini Pro's performance on sample questions from each of the three main dataset categories to identify domain-specific strengths and weaknesses.

## Open Questions the Paper Calls Out
- **Open Question 1**: What specific architectural or training modifications could improve Gemini Pro's performance on temporal and social reasoning tasks?
- **Open Question 2**: How does Gemini Pro's performance on commonsense reasoning tasks compare when evaluated on multilingual datasets versus English-only datasets?
- **Open Question 3**: What are the underlying reasons for Gemini Pro Vision's difficulty in emotion recognition from images, and how can this be improved?

## Limitations
- The study is limited to English language datasets, potentially missing cultural nuances and linguistic differences in other languages.
- Manual error analysis may introduce human bias and may not be representative of the full range of errors made by the models.
- The evaluation relies on static benchmark datasets, which may not fully capture the dynamic nature of real-world commonsense reasoning tasks.

## Confidence
- **High**: The comparative analysis between Gemini Pro and other LLMs/MLLMs is well-supported by experimental results, clearly demonstrating performance differences across datasets.
- **Medium**: The study demonstrates robust methodology but faces limitations including reliance on static benchmarks and potential human bias in manual error analysis.

## Next Checks
1. Test the models on a separate, held-out dataset that was not used in the original evaluation to assess the generalizability of the results.
2. Conduct additional evaluations using metrics beyond accuracy, such as precision, recall, and F1-score, to gain a more comprehensive understanding of the models' performance on commonsense reasoning tasks.
3. Repeat the evaluation over time as the models are updated and improved to track progress in commonsense reasoning capabilities.