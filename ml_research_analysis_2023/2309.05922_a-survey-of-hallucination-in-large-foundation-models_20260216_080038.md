---
ver: rpa2
title: A Survey of Hallucination in Large Foundation Models
arxiv_id: '2309.05922'
source_url: https://arxiv.org/abs/2309.05922
tags:
- hallucination
- language
- arxiv
- large
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of hallucination in
  large foundation models (LFMs) across multiple modalities including text, image,
  video, and audio. The authors classify various types of hallucination phenomena
  specific to LFMs and establish evaluation criteria for assessing the extent of hallucination.
---

# A Survey of Hallucination in Large Foundation Models

## Quick Facts
- arXiv ID: 2309.05922
- Source URL: https://arxiv.org/abs/2309.05922
- Reference count: 7
- This paper presents a comprehensive survey of hallucination in large foundation models (LFMs) across multiple modalities including text, image, video, and audio.

## Executive Summary
This paper provides a comprehensive survey of hallucination phenomena in large foundation models across multiple modalities including text, image, video, and audio. The authors present a taxonomy that classifies different types of hallucinations specific to LFMs and establish evaluation criteria for assessing their extent. The survey examines existing detection and mitigation strategies while also exploring the potential beneficial uses of hallucination in creative domains. The work serves as a foundational reference for understanding and addressing hallucination challenges in modern AI systems.

## Method Summary
The survey employs a systematic literature review approach, collecting and synthesizing existing research papers on hallucination in large foundation models across four modalities. The authors organize the collected papers by modality and key aspects including detection, mitigation, tasks, datasets, and evaluation metrics. They create taxonomies and summaries to provide a comprehensive overview of hallucination types, assessment methods, and mitigation strategies. The survey also identifies future research directions and discusses the nuanced perspective that not all hallucinations are harmful, particularly in creative applications.

## Key Results
- Presents a comprehensive taxonomy classifying hallucination phenomena across text, image, video, and audio modalities
- Establishes evaluation criteria and metrics for assessing hallucination extent in LFMs
- Examines both harmful and potentially beneficial uses of hallucination in creative domains
- Identifies future research directions including automated evaluation and knowledge integration strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey provides a taxonomy that classifies hallucination phenomena across multiple modalities (text, image, video, audio) to enable targeted detection and mitigation strategies.
- Mechanism: By categorizing hallucinations into modality-specific types, researchers can develop evaluation criteria and mitigation techniques tailored to each domain's unique challenges.
- Core assumption: Different modalities have distinct characteristics that make certain hallucination types more prevalent or detectable in one domain versus another.
- Evidence anchors:
  - [abstract] "The paper classifies various types of hallucination phenomena that are specific to LFMs"
  - [section] "As shown in Fig. 1, we broadly classify the LFMs into four types as follows: i. Text, ii. Image, iii. video, and iv. Audio."
  - [corpus] Weak - The corpus neighbors focus primarily on text-based hallucinations and don't provide strong evidence for multi-modal classification systems.
- Break condition: If hallucinations manifest similarly across modalities despite different input types, the taxonomy becomes less useful for targeted mitigation.

### Mechanism 2
- Claim: The survey establishes evaluation criteria for assessing hallucination extent across different modalities and tasks.
- Mechanism: By defining specific metrics and benchmarks for each modality, the survey enables consistent measurement and comparison of hallucination reduction techniques.
- Core assumption: Standardized evaluation metrics are necessary to track progress in hallucination mitigation and compare different approaches.
- Evidence anchors:
  - [abstract] "establishes evaluation criteria for assessing the extent of hallucination"
  - [section] "We cover all the important aspects such as i. detection, ii. mitigation, iii. tasks, iv. datasets, and v. evaluation metrics, given in Table 1."
  - [corpus] Weak - The corpus contains survey papers but lacks specific evaluation metrics for hallucination assessment.
- Break condition: If evaluation metrics are too domain-specific to allow meaningful cross-modal comparisons, the standardization effort fails.

### Mechanism 3
- Claim: The survey identifies both harmful and potentially beneficial uses of hallucination in creative domains.
- Mechanism: By distinguishing between problematic and useful hallucinations, the survey guides appropriate application contexts for LFMs.
- Core assumption: Not all hallucinations are detrimental; some can serve creative purposes where factual accuracy is less critical.
- Evidence anchors:
  - [abstract] "discuss potential future research directions"
  - [section] "6 Hallucination is not always harmful: A different perspective"
  - [corpus] Weak - The corpus focuses on technical aspects of hallucination without addressing creative applications.
- Break condition: If the line between harmful and beneficial hallucinations proves too subjective or context-dependent, this distinction becomes less actionable.

## Foundational Learning

- Concept: Foundation Models (FMs) and their characteristics
  - Why needed here: Understanding what FMs are and how they differ from traditional ML models is essential for grasping why hallucination occurs and how it manifests across modalities
  - Quick check question: What distinguishes foundation models from traditional machine learning models in terms of training data and task versatility?

- Concept: Types of hallucination phenomena
  - Why needed here: Recognizing different hallucination types is crucial for developing appropriate detection and mitigation strategies
  - Quick check question: How do text-based hallucinations differ from object hallucinations in vision-language models?

- Concept: Evaluation metrics and benchmarks
  - Why needed here: Understanding how hallucination is measured and compared across different approaches is essential for assessing progress in mitigation
  - Quick check question: What makes a good evaluation metric for hallucination detection, and how do these metrics vary across modalities?

## Architecture Onboarding

- Component map: The survey architecture consists of a taxonomy component (classifying hallucination types), an evaluation framework (metrics and benchmarks), a mitigation strategy catalog (detection and reduction techniques), and a domain-specific analysis section (covering text, image, video, and audio modalities).

- Critical path: For a new engineer, the critical path is understanding the taxonomy first, then learning evaluation metrics, and finally studying mitigation strategies. This progression builds from classification to measurement to intervention.

- Design tradeoffs: The survey prioritizes comprehensiveness over depth, covering many modalities and approaches but with less detail on individual techniques. This broad coverage enables cross-modal insights but may require supplementary reading for implementation details.

- Failure signatures: Key failure points include: (1) inadequate distinction between hallucination types leading to inappropriate mitigation strategies, (2) evaluation metrics that don't capture the full scope of hallucination phenomena, and (3) mitigation techniques that work for one modality but fail in others.

- First 3 experiments:
  1. Implement a simple hallucination detection system using the taxonomy from the survey to classify hallucinations in a small dataset of generated content
  2. Apply different evaluation metrics from Table 1 to the same dataset to compare their effectiveness in detecting hallucinations
  3. Test a basic mitigation strategy (e.g., knowledge injection) on one type of hallucination to observe its effectiveness and limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective automated evaluation metrics for detecting hallucination across different foundation model modalities (text, image, video, audio)?
- Basis in paper: [explicit] The paper identifies "Automated Evaluation of Hallucination" as a future research direction
- Why unresolved: Current evaluation methods rely heavily on manual assessment and are modality-specific, lacking standardized automated approaches
- What evidence would resolve it: Comparative studies demonstrating automated metrics that achieve high correlation with human judgment across multiple modalities and hallucination types

### Open Question 2
- Question: How can curated knowledge sources be optimally integrated into foundation models to prevent hallucination while maintaining creative capabilities?
- Basis in paper: [explicit] The paper suggests "Improving Detection and Mitigation Strategies with Curated Sources of Knowledge" as a future direction
- Why unresolved: There is a tension between using curated knowledge to reduce hallucination and preserving the models' ability to generate novel content
- What evidence would resolve it: Empirical studies showing optimal integration methods that balance factuality and creativity across different application domains

### Open Question 3
- Question: What are the underlying mechanisms that cause hallucination in different foundation model architectures and how do they vary across modalities?
- Basis in paper: [inferred] The paper discusses hallucination across multiple modalities but does not explore the fundamental causes specific to each modality
- Why unresolved: While the paper identifies hallucination as a problem, it does not deeply investigate the architectural and training factors that lead to hallucination in different model types
- What evidence would resolve it: Detailed architectural analysis and ablation studies that identify specific components or training procedures that contribute to hallucination in each modality

## Limitations

- The survey's coverage of hallucination across all four modalities (text, image, video, audio) may be uneven, with the corpus suggesting a stronger focus on text-based hallucinations
- The evaluation criteria and taxonomies are described but not fully detailed in the abstract, making their practical applicability difficult to assess
- The survey prioritizes breadth over depth, potentially requiring supplementary reading for implementation details on specific mitigation techniques

## Confidence

- **Taxonomy Classification (Medium)**: The claim that the survey provides a comprehensive classification of hallucination types across modalities is moderately supported by the abstract's mention of modality-specific classifications, but the corpus suggests this may be heavily weighted toward text-based phenomena.
- **Evaluation Criteria (Low)**: While the abstract states evaluation criteria are established, the corpus lacks concrete examples of these metrics, making it difficult to assess their validity or comprehensiveness.
- **Multi-modal Coverage (Low)**: The claim of comprehensive multi-modal coverage is least supported, as the corpus neighbors predominantly discuss text-based hallucinations and lack evidence for similar treatment of image, video, and audio domains.

## Next Checks

1. Examine the full survey text to verify the depth of coverage for image, video, and audio hallucination phenomena compared to text-based hallucinations, using the modality distribution in the corpus as a baseline expectation.
2. Analyze the specific evaluation metrics and benchmarks proposed for each modality to determine if they are truly standardized or if they vary significantly in their applicability and rigor across domains.
3. Investigate whether the survey adequately addresses the unique challenges of hallucination detection and mitigation in multimodal models that combine different input types, as the corpus suggests most research focuses on unimodal approaches.