---
ver: rpa2
title: Numerical Reasoning for Financial Reports
arxiv_id: '2312.14870'
source_url: https://arxiv.org/abs/2312.14870
tags:
- numerical
- financial
- reports
- reasoning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a pipeline for numerical reasoning over financial
  reports using fine-tuned large language models (LLMs). The authors extract tables
  from PDF reports, serialize them into text, and use FAISS to find relevant context
  for user questions.
---

# Numerical Reasoning for Financial Reports

## Quick Facts
- arXiv ID: 2312.14870
- Source URL: https://arxiv.org/abs/2312.14870
- Reference count: 4
- Primary result: Fine-tuned T5 achieves 62.3% accuracy on financial numerical reasoning tasks

## Executive Summary
This paper presents a pipeline for numerical reasoning over financial reports using fine-tuned large language models (LLMs). The authors extract tables from PDF reports, serialize them into text, and use FAISS to find relevant context for user questions. They then fine-tune T5 and Llama-2 models on the FinQA dataset for question answering. The T5 model achieves 62.3% accuracy in final numerical results when combined with a calculator for post-processing. The Llama-2 models show improved performance after fine-tuning with QLoRA, achieving 20% exact match accuracy for final results.

## Method Summary
The pipeline extracts tables from PDF reports using Pytabula, serializes them to text (either naively or using LLM-based methods), and creates a FAISS index of text chunks for efficient context retrieval. For each question, relevant context is retrieved and fed to a fine-tuned LLM (T5 or Llama-2), with post-processing using a calculator to validate final numerical results. The T5 model is fine-tuned using standard methods, while Llama-2 uses QLoRA for parameter-efficient fine-tuning. The approach is evaluated on the FinQA dataset with metrics focusing on final numerical answer accuracy.

## Key Results
- T5 model achieves 62.3% accuracy in final numerical results with calculator post-processing
- Llama-2 models achieve 20% exact match accuracy for final results after QLoRA fine-tuning
- Post-processing with calculator significantly improves accuracy (up to 60% when allowing 10% deviation)
- FAISS-based context retrieval improves LLM performance by providing focused input

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned LLMs can effectively extract and reason over numerical data in financial tables when combined with structured post-processing
- Mechanism: The pipeline first extracts tables from PDFs, serializes them into text, uses FAISS to find relevant context, then feeds this into a fine-tuned LLM. Post-processing with a calculator validates and computes final numerical results
- Core assumption: Structured serialization preserves semantic relationships in tables, and LLMs can interpret this serialized format when fine-tuned on relevant data
- Evidence anchors:
  - [abstract]: "We achieved results comparable to baseline on the final numerical answer, a competitive accuracy in numerical reasoning and calculation"
  - [section]: "However, through post-processing, we observed a substantial improvement in the accuracy of the final results. Allowing for a 10% deviation as acceptable, we find that post-processing significantly enhances the accuracy rate of the final output to upwards of 60%"
  - [corpus]: Weak - no direct corpus evidence found; relies on internal experimental results
- Break condition: If table serialization fails to preserve numerical relationships or if the LLM cannot generalize from fine-tuning data to unseen report structures

### Mechanism 2
- Claim: Context chunking with FAISS similarity search improves LLM performance by providing focused, relevant input
- Mechanism: After serializing tables to text, the pipeline segments the text into chunks and uses FAISS to retrieve the most relevant chunks for each question, reducing noise and improving answer accuracy
- Core assumption: LLMs perform better with concise, relevant context rather than long, unstructured input
- Evidence anchors:
  - [section]: "By implementing a chunking strategy, we can provide these LLMs with compact and focused context portions. This practice is beneficial as it ensures that the model considers all relevant information, irrespective of its position in the overall context"
  - [abstract]: Implicit in the methodology description of using FAISS for context retrieval
  - [corpus]: Weak - no direct corpus evidence; based on internal experimentation
- Break condition: If chunking removes necessary context or if FAISS retrieval fails to identify the correct relevant segments

### Mechanism 3
- Claim: Quantized LoRA fine-tuning enables efficient adaptation of large models to financial numerical reasoning tasks
- Mechanism: The Llama-2-7b-chat model is fine-tuned using QLoRA with a low-rank adaptation approach, allowing for parameter-efficient fine-tuning while maintaining performance
- Core assumption: QLoRA can effectively adapt large language models to domain-specific tasks without full fine-tuning
- Evidence anchors:
  - [section]: "The Llama-2-7b-chat model using the Quantized LoRA technique, aiming for computational efficiency and precision"
  - [section]: "The experimental outcomes demonstrate that the application of the Quantized LoRA technique to fine-tune the Llama-2-7b-chat model produces superior performance across a broad spectrum of metrics"
  - [corpus]: Moderate - QLoRA is a known technique, but specific application to financial numerical reasoning is unique to this work
- Break condition: If the low-rank approximation fails to capture necessary task-specific patterns or if QLoRA introduces instability during fine-tuning

## Foundational Learning

- Concept: Financial report structure and terminology
  - Why needed here: Understanding how financial data is organized in tables and text is crucial for effective table extraction and context retrieval
  - Quick check question: Can you identify the difference between extractive data and numerical reasoning-based insights in a financial report?

- Concept: Natural language processing for table understanding
  - Why needed here: Converting structured table data into meaningful text requires understanding both the table structure and how to represent it textually
  - Quick check question: What are the key challenges in serializing multi-index and non-standard table formats?

- Concept: Vector similarity search and FAISS
  - Why needed here: Efficient retrieval of relevant context chunks from large financial documents requires understanding of vector embeddings and similarity metrics
  - Quick check question: How does FAISS improve the efficiency of finding relevant context compared to brute-force search?

## Architecture Onboarding

- Component map: PDF parsing -> Table extraction (Pytabula) -> Table-to-text serialization (LLM or naive) -> Context chunking -> FAISS similarity search -> Fine-tuned LLM (T5/Llama-2) -> Post-processing (calculator) -> Final answer
- Critical path: 1. Extract tables from PDF 2. Serialize tables to text 3. Create FAISS index of text chunks 4. For each question: retrieve relevant context, feed to fine-tuned model, post-process output
- Bottlenecks: Table extraction from complex PDFs, context retrieval accuracy, model computation time
- Design tradeoffs:
  - Table extraction: Pytabula vs. other OCR tools (accuracy vs. handling of complex tables)
  - Serialization: Naive vs. LLM-based (simplicity vs. semantic preservation)
  - Model choice: T5 vs. Llama-2 (encoder-decoder vs. decoder-only architectures)
  - Post-processing: Calculator validation vs. model-only computation (accuracy vs. end-to-end reasoning)
- Failure signatures: Incorrect table extraction -> missing or malformed data, Poor serialization -> loss of numerical relationships, FAISS retrieval errors -> irrelevant context provided to model, Model limitations -> incorrect numerical reasoning or operations, Post-processing failures -> final answer validation issues
- First 3 experiments: 1. Test table extraction on a variety of PDF report formats to identify edge cases and failure modes 2. Compare naive vs. LLM serialization approaches on a subset of tables to measure semantic preservation 3. Evaluate FAISS context retrieval accuracy by manually checking if retrieved chunks contain relevant information for sample questions

## Open Questions the Paper Calls Out
- How does the performance of T5 models with calculator post-processing compare to other state-of-the-art numerical reasoning models on the FinQA dataset?
- What are the specific limitations of the table extraction and serialization methods used in this study, and how can they be improved for better performance on complex table structures?
- How does the integration of Program-Aided Language Models (PALs) impact the numerical reasoning capabilities of the fine-tuned LLMs in financial analysis?

## Limitations
- Limited empirical validation of table serialization quality - no ablation study comparing naive vs. LLM serialization on the same model architecture
- Post-processing calculator assumption - conflates model's reasoning capability with external computation
- FinQA dataset domain mismatch - performance on actual financial PDFs may differ significantly from reported results

## Confidence
- High confidence: The basic pipeline architecture (PDF parsing → table extraction → context retrieval → LLM reasoning) is sound and follows established patterns in the literature
- Medium confidence: The reported performance metrics are accurate for the specific experimental setup, though generalizability to real-world financial reports is uncertain
- Low confidence: The attribution of performance improvements to specific components (QLoRA fine-tuning vs. serialization methods vs. FAISS retrieval) is not well-supported by ablation studies

## Next Checks
1. Serialization ablation study: Compare naive vs. LLM-based table serialization on the same model architecture using a held-out validation set to isolate the impact of serialization quality on final performance
2. Domain transfer validation: Test the fine-tuned models on a small sample of actual financial reports not seen during training to measure domain adaptation performance and identify specific failure modes
3. End-to-end reasoning evaluation: Remove the calculator post-processing step and evaluate model performance on final numerical answers to assess the true reasoning capability of the fine-tuned models without external computation assistance