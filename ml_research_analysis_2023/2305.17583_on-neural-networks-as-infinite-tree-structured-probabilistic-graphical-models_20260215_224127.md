---
ver: rpa2
title: On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models
arxiv_id: '2305.17583'
source_url: https://arxiv.org/abs/2305.17583
tags:
- variables
- network
- nodes
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a correspondence between deep neural networks
  (DNNs) and infinite tree-structured probabilistic graphical models (PGMs), showing
  that forward propagation in DNNs approximates exact inference in these PGMs. The
  authors construct an infinite-width tree-structured PGM for any DNN, where the DNN's
  sigmoid activations correspond to conditional probability distributions.
---

# On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models

## Quick Facts
- arXiv ID: 2305.17583
- Source URL: https://arxiv.org/abs/2305.17583
- Reference count: 20
- One-line primary result: Establishes theoretical correspondence between DNNs and infinite tree-structured PGMs, showing DNN forward propagation approximates exact inference in these PGMs

## Executive Summary
This paper reveals a fundamental connection between deep neural networks (DNNs) and infinite tree-structured probabilistic graphical models (PGMs). The authors construct an infinite-width tree-structured PGM for any DNN, demonstrating that forward propagation in DNNs approximates exact inference in these PGMs. Sigmoid activations in DNNs correspond to conditional probability distributions in the PGM, and as the number of node copies approaches infinity, the PGM's probability distribution matches the DNN's activation values. The paper also explores Hamiltonian Monte Carlo (HMC) as an alternative training method for DNNs viewed as continuous Bernoulli belief networks, showing faster convergence on simple datasets compared to standard stochastic gradient descent.

## Method Summary
The method involves constructing an infinite tree-structured PGM from a given DNN architecture by creating infinite copies of latent nodes. Forward propagation in the DNN is shown to approximate exact inference in this PGM using variable elimination. For training, the paper proposes using Contrastive Divergence with Hamiltonian Monte Carlo (CD-HMC) to sample latent variables in the continuous Bernoulli formulation of the DNN-PGM. Experiments are conducted on XOR, Make Moons, and MNIST datasets, comparing standard SGD (backpropagation) with CD-HMC in terms of accuracy, convergence speed, and training time.

## Key Results
- Theoretical proof that DNN forward propagation approximates exact inference in infinite tree-structured PGMs
- CD-HMC training method shows convergence in fewer epochs compared to SGD on XOR and MNIST datasets
- Tree-structured PGMs enable efficient exact inference where general PGMs require exponential time
- Sigmoid activations in DNNs correspond to conditional probability distributions in the constructed PGMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DNN forward propagation approximates exact inference in an infinite tree-structured PGM.
- Mechanism: The DNN's sigmoid activations correspond to conditional probability distributions in the PGM. As the number of copies of latent nodes approaches infinity, the probability distribution over latent variables in the PGM matches the activation values in the DNN.
- Core assumption: Sigmoid activation functions can be interpreted as logistic regression models, which represent conditional probability distributions.
- Evidence anchors:
  - [abstract] "Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure."
  - [section 3] "In the PGM construction, as L → ∞, P(H = 1|⃗x) = σ(∑ w_j g_j + ∑ σ(p_i) θ_i), for an arbitrary latent node H in the DNN..."
  - [corpus] Weak evidence - the corpus focuses on general PGM and GNN topics rather than this specific DNN-PGM correspondence mechanism.

### Mechanism 2
- Claim: Tree-structured PGMs enable efficient exact inference where general PGMs require exponential time.
- Mechanism: The infinite-width construction transforms the original DNN structure into a tree/forest structure where variable elimination (VE) can compute exact probabilities in polynomial time.
- Core assumption: Treewidth-1 structures allow exact inference algorithms to run in polynomial time.
- Evidence anchors:
  - [section 2] "Treewidth-1 (tree-structured or forest-structured) PGMs are among the most desirable because in those exact inference by VE or other algorithms becomes efficient."
  - [section 3] "This process ultimately creates a graph whose undirected structure is a tree or forest."
  - [corpus] Weak evidence - corpus papers discuss PGMs generally but don't specifically address tree-structured efficiency advantages in the DNN context.

### Mechanism 3
- Claim: Hamiltonian Monte Carlo can be used to sample latent variables in the continuous Bernoulli formulation of the DNN-PGM.
- Mechanism: The continuous Bernoulli distribution allows hidden variables to remain in (0,1) while enabling HMC sampling on the logit space, providing an alternative to backpropagation training.
- Core assumption: The continuous Bernoulli distribution properly extends Bernoulli to continuous (0,1) space with valid probability densities.
- Evidence anchors:
  - [section 5] "To use HMC as proposed, we define hidden variables using the recently-developed continuous Bernoulli distribution [Loaiza-Ganem and Cunningham, 2019]..."
  - [section 5.1] "Since HMC samples are unbounded, we sample the logit associated with h_ij ∈ (0,1), i.e. σ^{-1}(h_ij) ∈ (-∞, ∞), rather than sampling the h_ij directly."
  - [corpus] Weak evidence - corpus papers discuss PGMs and GNNs but don't specifically address HMC application to DNN-PGM correspondence.

## Foundational Learning

- Concept: Probabilistic Graphical Models (PGMs) - joint probability distributions over variables with conditional independence structure
  - Why needed here: Understanding PGMs is essential to grasp the DNN-PGM correspondence and why tree-structured PGMs enable efficient inference
  - Quick check question: What is the key difference between Bayesian networks and Markov networks in terms of graph structure and probability representation?

- Concept: Variable Elimination algorithm for exact inference in PGMs
  - Why needed here: VE is the core algorithm that shows why tree-structured PGMs enable efficient inference compared to general PGMs
  - Quick check question: How does the complexity of variable elimination scale with treewidth, and why is treewidth-1 particularly efficient?

- Concept: Sigmoid activation functions and their relationship to logistic regression
  - Why needed here: Sigmoid activations are the bridge between DNNs and PGMs, representing conditional probability distributions
  - Quick check question: Why does a sigmoid activation function correspond to a logistic regression model in the PGM context?

## Architecture Onboarding

- Component map: Input layer -> Hidden layers (with sigmoid conditional distributions) -> Output layer
- Critical path:
  1. Construct infinite tree-structured PGM from DNN architecture
  2. Forward propagation computes PGM inference through expectation propagation
  3. Training updates weights to match PGM probability distributions
  4. Optional: Use HMC sampling for alternative training or uncertainty quantification
- Design tradeoffs:
  - Sigmoid vs ReLU activations: Sigmoid enables direct PGM correspondence but ReLU may require normalization for probability interpretation
  - Infinite vs finite tree construction: Infinite construction proves theoretical correspondence but finite approximations may be practical
  - SGD vs HMC training: SGD is computationally efficient but HMC provides better uncertainty quantification
- Failure signatures:
  - Poor convergence during training may indicate violation of PGM correspondence assumptions
  - Vanishing/exploding gradients may suggest improper normalization when using non-sigmoid activations
  - HMC sampling issues may indicate problems with continuous Bernoulli formulation or step size selection
- First 3 experiments:
  1. Verify tree construction: Take a simple DNN (e.g., XOR problem) and manually verify that the infinite tree construction produces the expected PGM structure
  2. Forward propagation check: Implement PGM inference using variable elimination and compare results with standard DNN forward propagation
  3. HMC sampling validation: Apply HMC to a simple continuous Bernoulli BN and verify it samples from the correct distribution before applying to the DNN-PGM correspondence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the correspondence between DNNs and infinite tree-structured PGMs be extended to other activation functions beyond sigmoid, ReLU, and normalized variants?
- Basis in paper: [inferred] The paper discusses extending results to ReLU and non-negative activations with normalization, but states detailed consideration of other activation functions is left for further work.
- Why unresolved: The paper only provides theoretical arguments and empirical results for sigmoid, ReLU, and normalized activations, without a complete characterization for arbitrary activation functions.
- What evidence would resolve it: A formal proof showing the correspondence holds for a broad class of activation functions, or a counterexample demonstrating cases where the correspondence breaks down.

### Open Question 2
- Question: How does the infinite tree-structured PGM view of DNNs relate to other interpretability methods like Shapley values, Integrated Gradients, and feature importance techniques?
- Basis in paper: [explicit] The paper mentions comparing HMC and other PGM algorithms to Shapley values, Integrated Gradients, and other approaches for assessing relationships among latent variables, inputs, and outputs.
- Why unresolved: The paper only mentions this as a direction for future work without providing any comparative analysis or empirical results.
- What evidence would resolve it: Empirical studies comparing the infinite tree-structured PGM approach to other interpretability methods on benchmark datasets, demonstrating strengths and weaknesses of each approach.

### Open Question 3
- Question: Can the infinite tree-structured PGM be used as a more accurate model than the direct PGM for fine-tuning trained DNNs?
- Basis in paper: [explicit] The paper mentions the large treewidth-1 PGM is a substantial approximation to the direct PGM of a DNN, and asks whether fine-tuning with a less-approximate approach (e.g., based on loopy belief propagation) is possible after training.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis of using the infinite tree-structured PGM for fine-tuning.
- What evidence would resolve it: Empirical studies comparing fine-tuning performance using the direct PGM, infinite tree-structured PGM, and other approximate inference methods on benchmark datasets.

## Limitations
- Theoretical correspondence requires infinite node copies, with unclear practical impact of finite width
- HMC-based training shows limited scalability, with experiments only on simple binary classification tasks
- Correspondence fundamentally relies on sigmoid activations, excluding modern activation functions like ReLU
- Practical implications for uncertainty quantification and Bayesian deep learning remain speculative

## Confidence
- **High confidence**: The theoretical construction of the infinite tree-structured PGM from DNNs is mathematically rigorous and well-proven. The forward propagation as PGM inference mechanism is clearly established.
- **Medium confidence**: The HMC-based training approach shows promise on simple datasets but lacks evidence for scalability to real-world problems. The convergence advantages over SGD need validation on larger, more complex tasks.
- **Low confidence**: Practical implications for uncertainty quantification and Bayesian deep learning remain speculative. The paper demonstrates theoretical correspondence but doesn't establish clear advantages over existing Bayesian neural network approaches.

## Next Checks
1. **Scalability test**: Implement the HMC training method on CIFAR-10 or a similarly sized dataset with ReLU activations (requiring appropriate normalization to maintain PGM correspondence) to evaluate practical scalability.
2. **Uncertainty quantification**: Compare the uncertainty estimates from the DNN-PGM correspondence approach against established Bayesian neural network methods (e.g., MC dropout, variational inference) on out-of-distribution detection tasks.
3. **Generalization analysis**: Investigate whether the tree-structured PGM construction provides better generalization bounds than standard DNN analyses, potentially leveraging the exact inference properties of tree-structured PGMs.