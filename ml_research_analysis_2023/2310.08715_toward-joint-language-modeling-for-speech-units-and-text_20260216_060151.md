---
ver: rpa2
title: Toward Joint Language Modeling for Speech Units and Text
arxiv_id: '2310.08715'
source_url: https://arxiv.org/abs/2310.08715
tags:
- speech
- text
- arxiv
- units
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores joint language modeling for speech units and
  text, aiming to bridge the gap between text-only and speech-only language models.
  The authors propose a joint autoregressive language model (SUTLM) trained on both
  speech and text, using self-supervised learning (SSL) speech models to convert continuous
  speech signals into discrete units.
---

# Toward Joint Language Modeling for Speech Units and Text

## Quick Facts
- arXiv ID: 2310.08715
- Source URL: https://arxiv.org/abs/2310.08715
- Authors: 
- Reference count: 11
- Primary result: Joint autoregressive language model trained on speech units and text improves SLU performance and shows zero-shot cross-modal transferability

## Executive Summary
This paper introduces a joint autoregressive language model (SUTLM) that bridges the gap between text-only and speech-only language models by training on both speech units and text. The model uses HuBERT units converted to discrete tokens via SentencePiece tokenization, and explores two data mixing techniques: Concatenated Speech-Text (CST) and Alternating Speech-Text (AST). The authors propose new automatic metrics (CRA and PELM) to evaluate cross-modal ability and demonstrate improved performance on downstream SLU tasks compared to speech-only baselines, with evidence of zero-shot transfer between modalities.

## Method Summary
The method converts continuous speech signals into discrete units using HuBERT, then applies SentencePiece tokenization to both speech units and text. Two mixing techniques are explored: CST concatenates speech and text sequences, while AST alternates between them at word boundaries using forced alignment. A decoder-only transformer LM is trained on these mixed sequences with a 1/3 ratio of speech-only, speech-text, and text-only data. The model is fine-tuned on SLUE tasks and evaluated using automatic metrics CRA and PELM, plus downstream performance measures.

## Key Results
- Joint modeling with mixed speech-text data improves SLU performance over speech-only baselines
- Subword tokenization of HuBERT units (SP 10k) yields best results across automatic metrics and SLUE tasks
- AST construction improves cross-modal retrieval accuracy compared to CST
- Zero-shot cross-modal transferability demonstrated in SLUE-SA and SLUE-NER tasks

## Why This Works (Mechanism)

### Mechanism 1
Subword tokenization of HuBERT units reduces sequence length mismatch between speech and text, improving cross-modal learning. Speech units from HuBERT are typically shorter than text subwords, creating sequence length imbalance. Applying SentencePiece tokenization to HuBERT units increases the average token length, aligning it closer to text tokens. This alignment allows the joint LM to better capture long-range dependencies across modalities.

### Mechanism 2
Alternating speech-text (AST) construction with word-level alignment forces the model to learn cross-modal context matching. AST data alternates between speech units and text at word boundaries, inserting modality switch tokens. This forces the model to explicitly predict which modality to use at each word, encouraging it to learn a unified representation space for speech and text content.

### Mechanism 3
Mixed speech-text data improves zero-shot cross-modal transfer in SLU tasks compared to unimodal training. Training on both speech and text allows the model to learn shared semantic representations. When fine-tuned on one modality and evaluated on another, the shared representations enable transfer. CST and AST provide paired or aligned examples that strengthen this shared space.

## Foundational Learning

- **Subword tokenization (SentencePiece)**: Reduces vocabulary size and handles rare words; aligns speech unit length with text tokens for joint modeling. *Quick check: What is the effect of using a smaller vs. larger SentencePiece vocabulary size on sequence length and model performance?*

- **Self-supervised speech representation learning (HuBERT)**: Converts raw audio into discrete units that capture phonetic and semantic information without transcriptions. *Quick check: How does the clustering of HuBERT representations into discrete units affect the quality of the resulting speech tokens?*

- **Modality alignment via forced alignment**: Provides word-level correspondence between speech and text for AST construction, enabling the model to learn cross-modal dependencies. *Quick check: What happens to AST quality if the forced alignment introduces errors?*

## Architecture Onboarding

- **Component map**: HuBERT model → discrete speech units → SentencePiece tokenization → speech tokens; Text tokenizer → text tokens; Concatenated or alternating sequences → decoder-only transformer LM; Downstream SLU tasks with fine-tuning heads

- **Critical path**: 1. Tokenize speech and text into aligned or mixed sequences 2. Train joint LM on balanced speech/text data 3. Evaluate with CRA/PELM 4. Fine-tune on SLU tasks and test cross-modal transfer

- **Design tradeoffs**: Speech unit granularity vs. sequence length; Amount of paired vs. unpaired data; Concatenation (explicit alignment) vs. alternation (implicit alignment); Model size vs. cross-modal ability

- **Failure signatures**: Low CRA for cross-modal cases → poor modality alignment; High PELM for u2t/t2u → model repeats prompt instead of generating; Poor SLU transfer → shared representations not learned

- **First 3 experiments**: 1. Train LM with HuBERT units only, evaluate CRA/PELM, test SLU transfer 2. Add SentencePiece tokenization to HuBERT units, compare CRA/PELM and SLU 3. Train with CST data, compare cross-modal CRA and SLU performance to step 2

## Open Questions the Paper Calls Out

- **How does the optimal ratio of speech to text data affect performance across different downstream tasks?** The paper used a 1/3 ratio but didn't explore optimal balance. Experiments with varying ratios would reveal optimal balance for different applications.

- **How does scaling up model size beyond 350M parameters impact cross-modal ability and performance?** The paper only studied 350M parameter models, leaving effects of larger models unexplored. Training with 1B, 10B, 100B parameters would demonstrate impact of model scale.

- **How do different self-supervised speech models (beyond HuBERT) affect joint language model performance?** The paper only used HuBERT, leaving potential benefits of other SSL models unexplored. Comparing models trained with Wav2Vec 2.0, WavLM units would reveal most effective models.

- **How can limitations of PELM metric be addressed to better evaluate generated continuations?** The paper discusses PELM's limitations when models simply repeat prompts, suggesting need for better metrics. Developing metrics distinguishing meaningful continuations from repetitions would provide solution.

## Limitations

- Automatic metrics (CRA and PELM) are proxies for true understanding and may not capture semantic equivalence
- Downstream results limited to two datasets (SLUE-SA and SLUE-NER) with reduced zero-shot transfer performance
- No exploration of robustness to noisy speech inputs or domain shifts from training data

## Confidence

- **High Confidence**: Core finding that mixing speech and text data improves SLU performance over speech-only baselines is well-supported by experimental results and ablation studies
- **Medium Confidence**: Zero-shot cross-modal transferability claims are supported but performance gap between within-modal and cross-modal fine-tuning suggests imperfect alignment
- **Low Confidence**: Claims about coherent generation of speech units from text (and vice versa) based on limited automatic evaluation without comprehensive human validation

## Next Checks

1. Human evaluation of cross-modal generation: Have human annotators rate semantic coherence and fluency of speech unit sequences generated from text prompts and text generated from speech unit prompts, comparing joint model to baselines.

2. Robustness to noisy speech: Evaluate joint model's SLU performance on speech inputs with varying background noise or accents, comparing to speech-only baseline to assess robustness benefits.

3. Fine-grained analysis of token alignment: Analyze how often joint model's attention mechanism aligns speech units to corresponding text tokens during generation, and whether alignment improves with subword tokenization and AST training.