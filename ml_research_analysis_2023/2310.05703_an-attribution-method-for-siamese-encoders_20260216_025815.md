---
ver: rpa2
title: An Attribution Method for Siamese Encoders
arxiv_id: '2310.05703'
source_url: https://arxiv.org/abs/2310.05703
tags:
- attributions
- association
- computational
- linguistics
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to attribute predictions of Siamese
  encoder models (e.g., sentence transformers) to their input features by generalizing
  integrated gradients to models with two inputs. The approach produces feature-pair
  attributions, computed via integrated Jacobians, which sum exactly to the model's
  prediction and converge with sufficient approximation steps.
---

# An Attribution Method for Siamese Encoders

## Quick Facts
- arXiv ID: 2310.05703
- Source URL: https://arxiv.org/abs/2310.05703
- Reference count: 24
- Primary result: Introduces integrated Jacobian method for attributing Siamese encoder predictions to input features with theoretical guarantees

## Executive Summary
This paper addresses the challenge of attributing predictions from Siamese encoder models (such as sentence transformers) to their input features. The authors generalize integrated gradients to models with two inputs by computing integrated Jacobians for each input and combining them multiplicatively. The method produces feature-pair attributions that sum exactly to the model's prediction and converge with sufficient approximation steps. Experiments on semantic similarity tasks demonstrate accurate attribution to intermediate and input representations, revealing that nouns and verbs dominate model decisions while deeper layers capture nuanced patterns like negation.

## Method Summary
The method generalizes integrated gradients to Siamese models by computing integrated Jacobians for each input separately and combining them multiplicatively. It requires shifting embeddings so the reference input maps to zero, replacing cosine similarity with dot product, and approximating the integral via discrete summation. The approach is validated on STS-benchmark using fine-tuned sentence transformers, showing that top 5% of token-pair attributions often explain over 75% of predictions.

## Key Results
- Attributions sum exactly to model predictions with sufficient approximation steps
- Top 5% of token-pair attributions explain over 75% of semantic similarity predictions
- Nouns and verbs dominate model decision-making, though full accuracy requires broader POS coverage
- Deeper intermediate layers capture nuanced patterns like negation in sentence relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalizes integrated gradients to multi-input models via integrated Jacobians
- Mechanism: Computes Jacobian for each input separately, integrates along paths from reference inputs, combines multiplicatively
- Core assumption: Predictions decompose into contributions from feature pairs between inputs
- Evidence anchors: Abstract mentions integrated Jacobians; equation 2 shows pairwise attribution computation
- Break condition: Attribution summation fails if model cannot be reformulated for reference input pair

### Mechanism 2
- Claim: Shifting embeddings allows arbitrary reference inputs without special zero-mapping
- Mechanism: Defines e(x) = e'(x) - e'(r) ensuring encoder output for reference is zero vector
- Core assumption: Dot product with zero vector yields zero regardless of other input
- Evidence anchors: Section explains padding token as reference and embedding shifting
- Break condition: Fails if similarity measure doesn't behave as expected with zero vectors

### Mechanism 3
- Claim: Discrete summation approximation converges to true prediction as steps increase
- Mechanism: Approximates path integral by summing over N discrete points along integration path
- Core assumption: Sum converges to integral for sufficiently large N
- Evidence anchors: Section describes discrete approximation; different models converge at different N
- Break condition: Fails if model contains non-differentiable operations preventing smooth paths

## Foundational Learning

- Concept: Jacobian matrices and sensitivity analysis
  - Why needed here: Method relies on computing partial derivatives of embedding components with respect to input dimensions
  - Quick check question: If you have a function f(x) that outputs a vector y, what matrix represents the partial derivatives ∂yi/∂xj?

- Concept: Path integrals and discrete summation approximation
  - Why needed here: Integrated gradients principle is fundamentally a path integral requiring computational approximation
  - Quick check question: If integrating a function along a straight line from point A to point B, what parameterization converts this to standard integral?

- Concept: Siamese network architecture implications for attribution
  - Why needed here: Unlike standard models, Siamese encoders compare two inputs requiring specialized attribution methods
  - Quick check question: Why can't standard feature attribution methods like integrated gradients be directly applied to Siamese models?

## Architecture Onboarding

- Component map: Input → Shifted Encoder → Integrated Jacobian Computation → Pairwise Attribution Matrix → Sum to Prediction
- Critical path: Inputs flow through shifted encoder, integrated Jacobians computed along interpolation paths, multiplied to form attribution matrix, summed to match prediction
- Design tradeoffs: Requires model adaptation (shifted embeddings, dot product) limiting direct application to off-the-shelf models, but enables accurate attribution computation
- Failure signatures: Non-converging attributions indicate insufficient N steps; uniformly positive/negative attributions suggest model missing nuanced relationships; excessive computation time indicates model depth or N value too large
- First 3 experiments:
  1. Verify shifting embeddings works by checking f(r, x) = 0 for various inputs x
  2. Test attribution accuracy by comparing sum of attributions to actual prediction across different N values
  3. Compare attribution distributions between different model depths to understand layer effects

## Open Questions the Paper Calls Out

- Question: Does the method generalize to non-Siamese architectures like multi-modal or multi-task models?
- Basis in paper: Method explicitly derived for Siamese encoders requiring adaptation for reference inputs
- Why unresolved: Paper focuses on semantic similarity without exploring alternative architectures
- What evidence would resolve it: Experiments applying method to image-text encoders with modified integration paths

- Question: How robust are attributions to adversarial examples or input perturbations?
- Basis in paper: Limitations mention possibility of adversarially misleading gradients
- Why unresolved: No experiments testing attribution stability under adversarial attacks
- What evidence would resolve it: Systematic evaluation of attribution changes under controlled perturbations

- Question: Can the method identify meaningful linguistic patterns beyond POS relations?
- Basis in paper: Experiments show POS correlations but high variance suggests broader feature reliance
- Why unresolved: Study only examines POS-level aggregations, not deeper linguistic structures
- What evidence would resolve it: Attribution analysis aligned with syntactic parses and semantic role labels

## Limitations
- Requires model adaptation (shifted embeddings, dot product objective) preventing direct application to off-the-shelf models
- Computational overhead from evaluating partial derivatives at multiple points along integration paths
- Limited empirical validation across diverse model architectures and datasets

## Confidence

**High Confidence**: Integrated gradients formal properties preserved in multi-input extension; embedding shifting mechanism mathematically sound and validated

**Medium Confidence**: Discrete approximation convergence theoretically expected but empirically demonstrated only for limited models; 75% top-5% attribution claim needs broader validation

**Low Confidence**: Extension to models with multiple shared encoders lacks empirical validation; CLS token removal recommendation mentioned but not rigorously evaluated

## Next Checks

1. Cross-Dataset Generalization Test: Validate attribution accuracy on multiple semantic similarity datasets beyond STS-benchmark to assess robustness and confirm 75% top-5% claim

2. Computational Efficiency Benchmark: Measure wall-clock time for attribution computation across different N values and model depths, comparing against alternative methods to quantify overhead

3. Ablation Study on Model Modifications: Systematically test impact of removing each adaptation (dot product replacement, embedding shifting) on prediction accuracy and attribution quality to quantify trade-offs