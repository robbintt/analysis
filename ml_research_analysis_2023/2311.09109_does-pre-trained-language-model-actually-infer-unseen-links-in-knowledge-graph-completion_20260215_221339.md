---
ver: rpa2
title: Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph
  Completion?
arxiv_id: '2311.09109'
source_url: https://arxiv.org/abs/2311.09109
tags:
- entities
- knowledge
- methods
- pre-trained
- descriptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether PLM-based KGC methods truly infer
  unseen links or merely access memorized knowledge from pre-training. The authors
  propose synthetic datasets that modify textual information of entities and relations
  while preserving graph structure to separately evaluate inference and memorization
  abilities.
---

# Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?

## Quick Facts
- arXiv ID: 2311.09109
- Source URL: https://arxiv.org/abs/2311.09109
- Reference count: 33
- Primary result: PLMs acquire inference abilities through pre-training but rely more heavily on textual information than traditional KGE methods

## Executive Summary
This paper investigates whether PLM-based knowledge graph completion (KGC) methods truly infer unseen links or merely access memorized knowledge from pre-training. The authors propose synthetic datasets that modify textual information of entities and relations while preserving graph structure to separately evaluate inference and memorization abilities. Experiments on WN18RR, FB15k-237, and Wikidata5m show that PLMs can infer new links but depend significantly on entity/relation names and descriptions. When textual information is fully anonymized, PLM performance drops substantially, indicating heavy reliance on memorization from pre-training.

## Method Summary
The authors create synthetic datasets by shuffling entity and relation names using derangements while preserving the underlying graph structure. Four different synthetic scenarios are constructed: Virtual World (shuffling entity names), Anonymized Entities (replacing names with random strings), Anonymized Relations (replacing relation names), and Fully Anonymized (replacing both). These datasets are used to evaluate PLM-based KGC methods (SimKGC, kNN-KGE, KGT5, GenKGC) with and without pre-trained weights, comparing their performance against traditional KGE methods like TransE.

## Key Results
- PLMs acquire inference abilities through pre-training but rely more heavily on textual information of entities and relations
- Generation-based methods are more susceptible to memorization than discrimination-based methods
- Performance with pre-trained weights is significantly higher than without, and comparable to or lower than traditional KGC methods when pre-trained weights are not used
- About 15% of entities in WN18RR can be solved by extracting information from descriptions alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PLMs can use pre-trained knowledge to directly predict unseen triples without performing true inferential reasoning.
- **Mechanism**: When entity or relation names in a test triple match those seen during pre-training, the PLM can retrieve memorized factual associations from its weights instead of constructing new inferences from the training KG.
- **Core assumption**: The pre-training corpus contains overlapping factual content with the target KG; entity/relation names are preserved during synthetic dataset creation.
- **Evidence anchors**:
  - [abstract] "PLM-based KGC can estimate missing links between entities by reusing memorized knowledge from pre-training without inference."
  - [section 4.2.1] "In the base datasets, all models with the pre-trained weights are better than those without them."
- **Break condition**: Entity or relation names are fully anonymized or shuffled so that no direct lexical overlap remains between pre-training and target KG.

### Mechanism 2
- **Claim**: The textual descriptions of entities are a strong source of information for link prediction, sometimes dominating the graph structure.
- **Mechanism**: Descriptions often contain explicit mentions of related entities; PLMs can extract these links via text-based reasoning rather than graph-based pattern inference.
- **Core assumption**: Descriptions in the KG are coherent and contain the answer entity or relation names.
- **Evidence anchors**:
  - [section 4.2.3] "Table 4 shows how many entities to predict are included in the description of query entities; in WN18RR, about 15% of the entities may be able to solve the KGC task just by extracting information from the description."
- **Break condition**: Descriptions are replaced with random strings or shuffled so that no semantic relation between query and target entity is preserved.

### Mechanism 3
- **Claim**: Generation-based PLM methods are more susceptible to memorization than discrimination-based methods.
- **Mechanism**: The decoder in generation-based models can directly emit entity names it has seen during pre-training, bypassing the need to score candidate entities via learned embeddings.
- **Core assumption**: The generation process can sample from a distribution heavily weighted by memorization of entity names in the pre-training data.
- **Evidence anchors**:
  - [section 4.2.4] "Generation-based methods are substantially affected by entities, changed into random strings... PLMs' knowledge prevents learning new relationships and affects from descriptions in Generation-based methods."
- **Break condition**: Both entity names and relation names are replaced with random strings, eliminating lexical cues for generation.

## Foundational Learning

- **Concept**: Knowledge Graph Embeddings (KGE) such as TransE, DistMult, ComplEx
  - Why needed here: Provides baseline understanding of how traditional methods infer missing links without pre-trained language knowledge.
  - Quick check question: How does TransE model the plausibility of a triple (h, r, t)?

- **Concept**: Masked Language Modeling and Prompt Engineering in PLMs
  - Why needed here: Explains how PLMs encode and decode entity/relation text for KGC tasks.
  - Quick check question: In a masked entity prediction setup, what is the role of the [MASK] token?

- **Concept**: Derangement and Bipartite Graph Matching
  - Why needed here: Used to create synthetic datasets that shuffle entity/relation names while preserving graph structure.
  - Quick check question: Why is a derangement (no fixed points) important when shuffling entity names?

## Architecture Onboarding

- **Component map**: Entity/relation names + descriptions -> PLM backbone -> Scoring function or text generation -> KGC output

- **Critical path**:
  1. Prepare KG triples with entity/relation names and descriptions.
  2. Apply synthetic dataset transformation (virtual world, anonymized entities, etc.).
  3. Load into PLM-based KGC model with or without pre-trained weights.
  4. Train or evaluate on transformed data.
  5. Compare hits@10 (or other metrics) across configurations.

- **Design tradeoffs**:
  - Using pre-trained weights boosts performance on base data but conflates memorization with inference.
  - Generation-based methods are more flexible but more prone to memorization bias.
  - Descriptions improve performance but may leak answers, reducing inference purity.

- **Failure signatures**:
  - High performance drop when descriptions are fully anonymized → model relied heavily on textual cues.
  - Similar scores with/without pre-trained weights on anonymized data → model learned pure graph structure inference.
  - Worse performance with pre-trained weights on anonymized data → memorization interferes with learning new patterns.

- **First 3 experiments**:
  1. Train SimKGC with and without pre-trained weights on base WN18RR; compare hits@10.
  2. Apply VIRTUAL WORLD (§3.1) to WN18RR; retrain both versions; observe performance shift.
  3. Apply FULLY ANONYMIZED (§3.4) to WN18RR; retrain both versions; confirm inference-only capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the inference abilities of PLMs in KGC tasks compare to their performance in other downstream tasks that require reasoning?
- Basis in paper: [inferred] The paper focuses on KGC tasks but acknowledges that the findings may apply to other downstream tasks in the Limitations section, suggesting a need to investigate this generalization.
- Why unresolved: The paper only investigates PLM inference abilities in the context of KGC tasks, leaving open whether the observed patterns of relying on pre-trained knowledge versus inference generalize to other tasks.
- What evidence would resolve it: Conducting similar experiments on other downstream tasks like question answering, fact checking, or logical reasoning tasks, comparing PLM performance with and without pre-trained weights on modified datasets that test inference vs memorization.

### Open Question 2
- Question: What is the theoretical basis for why PLMs rely more on textual information of entities and relations rather than pure inference from the graph structure?
- Basis in paper: [explicit] The paper concludes that PLMs acquire inference abilities through pre-training but mostly rely on textual information of entities and relations, but does not explain the theoretical reason for this preference.
- Why unresolved: The empirical results show this preference but the paper does not provide theoretical analysis of why PLMs favor textual information over structural inference.
- What evidence would resolve it: Developing theoretical models of PLM attention mechanisms and information retrieval that explain why textual cues are prioritized over graph structural patterns, potentially through analysis of attention weights or feature importance.

### Open Question 3
- Question: How does the size and diversity of pre-training data affect the balance between memorization and inference in PLMs for KGC tasks?
- Basis in paper: [inferred] The paper shows that PLMs rely on pre-trained knowledge but doesn't investigate how the extent of this reliance varies with pre-training data characteristics.
- Why unresolved: The experiments use existing PLMs without varying their pre-training conditions, leaving open how different pre-training strategies would affect the memorization-inference balance.
- What evidence would resolve it: Training PLMs with varying sizes and diversities of pre-training data, then testing their performance on KGC tasks with the synthetic datasets to measure how pre-training data characteristics affect their reliance on memorization versus inference.

## Limitations

- Synthetic datasets only manipulate textual information while preserving graph structure, which may not fully capture real-world scenarios where both structure and text evolve simultaneously
- The comparison between generation-based and discrimination-based methods could be influenced by implementation choices in the LambdaKG framework rather than inherent methodological differences
- Domain-specific effects are observed (e.g., descriptions are more important for WN18RR than other datasets) but not fully explored

## Confidence

- **High Confidence**: The finding that PLMs acquire inference abilities through pre-training is well-supported by consistent performance improvements across multiple datasets and methods when using pre-trained weights.
- **Medium Confidence**: The claim that generation-based methods are more susceptible to memorization than discrimination-based methods, while supported by experimental data, could be influenced by specific implementation details of the LambdaKG framework rather than fundamental methodological differences.
- **Low Confidence**: The assertion that PLMs rely more heavily on textual information of entities and relations than traditional methods requires further validation, as the comparison with TransE may not account for all architectural differences between PLM-based and traditional KGE approaches.

## Next Checks

1. **Cross-domain validation**: Apply the synthetic dataset methodology to additional KGs from different domains (e.g., biomedical, social networks) to verify whether the observed patterns of memorization vs. inference hold across diverse knowledge domains.

2. **Temporal analysis**: Conduct experiments where pre-training data and target KG triples are temporally separated to quantify the decay of memorization effects and isolate true inference capabilities.

3. **Human evaluation study**: Design a human-in-the-loop experiment where annotators attempt to complete triples using only the entity/relation names and descriptions, comparing human performance patterns with PLM-based models to better understand the role of textual vs. structural information.