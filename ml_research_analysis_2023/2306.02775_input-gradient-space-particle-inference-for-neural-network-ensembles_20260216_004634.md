---
ver: rpa2
title: Input-gradient space particle inference for neural network ensembles
arxiv_id: '2306.02775'
source_url: https://arxiv.org/abs/2306.02775
tags:
- ensemble
- forde
- repulsion
- input
- cifar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to enhance diversity in
  deep ensemble methods by utilizing input gradients for repulsion. The authors propose
  the First-order Repulsive Deep Ensemble (FoRDE), which leverages particle-based
  variational inference to perform repulsion in the space of first-order input gradients.
---

# Input-gradient space particle inference for neural network ensembles

## Quick Facts
- arXiv ID: 2306.02775
- Source URL: https://arxiv.org/abs/2306.02775
- Reference count: 40
- Key outcome: FoRDE outperforms gold-standard deep ensembles and other ensemble methods in accuracy and calibration under covariate shift due to input perturbations on image classification datasets.

## Executive Summary
This paper introduces First-order Repulsive Deep Ensemble (FoRDE), a novel approach to enhance diversity in deep ensemble methods by utilizing input gradients for repulsion. The method leverages particle-based variational inference to perform repulsion in the space of first-order input gradients, ensuring ensemble members are functionally distinct. By characterizing functions through their input gradients (up to translation), FoRDE achieves better robustness against corrupted inputs while maintaining competitive performance on clean data. The approach demonstrates significant improvements over standard deep ensembles and other ensemble methods across multiple image classification benchmarks.

## Method Summary
FoRDE employs Particle-based Variational Inference (ParVI) to encourage diversity in neural network ensembles by performing repulsion in the space of first-order input gradients. The method computes normalized input gradients for each network, uses an RBF kernel on these gradients with PCA-based lengthscales, and applies Wasserstein gradient descent with a repulsion term. This approach ensures functional diversity while being computationally efficient due to the lower dimensionality of input gradients compared to network weights. The training procedure involves mini-batch SGD with an adaptive kernel bandwidth set via the median heuristic.

## Key Results
- FoRDE achieves significantly higher accuracy than standard deep ensembles under input perturbations on CIFAR-10, CIFAR-100, and TinyImageNet
- The method shows superior calibration (lower ECE) compared to baselines on corrupted test sets
- PCA-based lengthscales provide better robustness against corruptions than identity lengthscales
- Performance gains are maintained across different network architectures (RESNET 18 and PREACTRESNET 18)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Input gradients uniquely characterize a function up to translation, making them a smaller yet sufficient representation for functional diversity.
- Mechanism: Since the input gradient ∇ₓf(x; θ) captures the sensitivity of the output to input changes, two networks with identical input gradients (up to translation) must have the same functional behavior. Repulsion in this space ensures each ensemble member learns different features without redundancy.
- Core assumption: The input gradient space has lower dimensionality than weight space, making kernel-based comparisons more tractable and efficient.
- Evidence anchors:
  - [abstract] "As input gradients uniquely characterize a function up to translation and are much smaller in dimension than the weights..."
  - [section] "Comparing neural networks via a function-space kernel is also challenging since functions are infinite-dimensional objects."

### Mechanism 2
- Claim: Repulsion in input gradient space encourages ensemble members to learn complementary features, improving robustness against input perturbations.
- Mechanism: The RBF kernel on normalized gradients, combined with PCA-based lengthscales, pushes networks to depend on different high-variance data dimensions. This prevents overfitting to low-variance features that are easily corrupted.
- Core assumption: Input gradients of the true label are sufficient for characterizing the network's decision process without needing the full Jacobian.
- Evidence anchors:
  - [section] "This approach also reduces computational complexity... as our motivation is to encourage each particle to learn different features that could explain the training sample..."
  - [section] "The PCA kernel encourages FoRDE to rely more on features with high variances in the data manifold to make predictions..."

### Mechanism 3
- Claim: The Wasserstein gradient descent (WGD) update rule with input gradient kernel efficiently transports particles toward the posterior while maintaining diversity.
- Mechanism: The repulsion term in WGD's update rule pushes particles apart in input gradient space, preventing collapse to the same mode. The median heuristic adapts the kernel bandwidth to particle distances.
- Core assumption: The kernel density estimation (KDE) approximation of the particle distribution using input gradients is accurate enough for gradient-based optimization.
- Evidence anchors:
  - [section] "The repulsion motivates the ensemble members to depend more on dimensions with stronger repulsion in the input gradient space..."
  - [section] "We adopt the median heuristic... During training, the bandwidth h is adaptively set to med2/(2 logM)..."

## Foundational Learning

- Concept: Particle-based variational inference (ParVI)
  - Why needed here: ParVI formalizes how to encourage diversity in ensembles by adding repulsion terms based on a network similarity kernel.
  - Quick check question: What is the main difference between ParVI and traditional variational inference in terms of particle interaction?

- Concept: Input gradients as functional descriptors
  - Why needed here: Input gradients uniquely characterize a function up to translation, making them a compact yet sufficient representation for functional diversity.
  - Quick check question: Why are input gradients considered a smaller representation than network weights?

- Concept: Kernel density estimation (KDE) in Wasserstein gradient descent
  - Why needed here: KDE approximates the particle distribution in input gradient space, enabling tractable computation of the repulsion term in the WGD update rule.
  - Quick check question: How does the median heuristic adapt the kernel bandwidth during training?

## Architecture Onboarding

- Component map: Input gradient computation -> Kernel similarity calculation -> Repulsion gradient -> Parameter update
- Critical path: Input gradient → Kernel similarity → Repulsion gradient → Parameter update
- Design tradeoffs:
  - Using input gradients reduces dimensionality but may miss higher-order functional nuances
  - PCA-based lengthscales improve robustness but require eigen-decomposition of the covariance matrix
  - Mini-batching speeds up training but introduces bias in the repulsion term
- Failure signatures:
  - If lengthscales are too large, repulsion becomes ineffective and ensemble members collapse
  - If bandwidth is mis-tuned, kernel similarities become too uniform or too peaked
  - If mini-batch size is too small, gradient estimates become noisy and training diverges
- First 3 experiments:
  1. Compare FoRDE with identity lengthscales vs. PCA lengthscales on CIFAR-10 under corruptions
  2. Visualize CAM diversity between ensemble members at different network depths
  3. Test FoRDE with varying ensemble sizes to assess scalability of input gradient repulsion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of orthonormal basis affect the performance of FoRDE?
- Basis in paper: explicit
- Why unresolved: The paper only experimented with the eigenbasis and identity lengthscales. The potential of other orthonormal bases, such as the discrete cosine transformation basis, is not explored.
- What evidence would resolve it: Empirical results comparing the performance of FoRDE using different orthonormal bases on various datasets and tasks.

### Open Question 2
- Question: What is the optimal lengthscale setting for FoRDE to perform well on both clean and corrupted data?
- Basis in paper: inferred
- Why unresolved: The paper only experimented with two lengthscale settings (PCA eigenvalues and identity), and the results suggest that an intermediate setting might be optimal.
- What evidence would resolve it: Empirical results showing the performance of FoRDE with different lengthscale settings, including intermediate ones, on both clean and corrupted data.

### Open Question 3
- Question: How does FoRDE compare to other ensemble methods in terms of computational efficiency?
- Basis in paper: explicit
- Why unresolved: The paper only mentions that FoRDE takes roughly three times longer to train than DEs, but does not compare it to other ensemble methods.
- What evidence would resolve it: Empirical results comparing the training time and inference time of FoRDE with other ensemble methods on various datasets and tasks.

## Limitations
- Limited to image classification tasks with continuous input perturbations
- Requires eigen-decomposition for PCA lengthscales, adding computational overhead
- Performance on structured data or text domains remains unexplored

## Confidence
- Input gradient space provides a sufficient representation for functional diversity: Medium
- PCA-based lengthscales improve robustness against corruptions: High
- FoRDE outperforms all baselines on accuracy and calibration: High
- The method scales to realistic image classification tasks: Medium

## Next Checks
1. **Lengthscale Ablation**: Test FoRDE with identity vs. PCA lengthscales on CIFAR-10 under varying corruption intensities to quantify robustness gains.
2. **Covariate Shift Stress Test**: Evaluate FoRDE on out-of-distribution datasets (e.g., CIFAR-10 vs. ImageNet-30) to assess generalization beyond synthetic corruptions.
3. **Bandwidth Stability**: Monitor median heuristic bandwidth variance during training across multiple seeds to ensure consistent kernel behavior.