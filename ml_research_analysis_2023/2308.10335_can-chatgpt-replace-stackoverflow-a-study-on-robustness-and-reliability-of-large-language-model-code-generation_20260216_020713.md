---
ver: rpa2
title: Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of
  Large Language Model Code Generation
arxiv_id: '2308.10335'
source_url: https://arxiv.org/abs/2308.10335
tags:
- code
- misuse
- language
- usage
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a study on the reliability and robustness of
  code generated by large language models (LLMs), focusing on API misuse. The authors
  propose a benchmark called ROBUST API to evaluate the reliability and robustness
  of LLM-generated code by analyzing API usage patterns.
---

# Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation

## Quick Facts
- arXiv ID: 2308.10335
- Source URL: https://arxiv.org/abs/2308.10335
- Authors: 
- Reference count: 40
- Key outcome: Even advanced LLMs like GPT-4 exhibit 62% API misuse rates in generated Java code

## Executive Summary
This paper presents a systematic evaluation of code generation reliability from large language models, focusing specifically on API misuse detection. The authors develop ROBUST API, a benchmark that uses static analysis with abstract syntax trees to identify improper API usage patterns in code generated by LLMs. By collecting 1208 real-world coding questions from Stack Overflow about 24 Java APIs, the study reveals that even state-of-the-art models like GPT-4 produce code with substantial API misuse, raising concerns about the reliability of LLM-generated code in production environments.

## Method Summary
The study employs a static analysis approach using abstract syntax trees to detect API misuse in code generated by large language models. The method involves collecting real-world coding questions from Stack Overflow about specific Java APIs, generating prompts for LLMs under three experimental settings (zero-shot, one-shot-irrelevant, and one-shot-relevant), and analyzing the generated code using AST-based static analysis. The analysis extracts call sequences and control structures from the code and compares them against predefined API usage rules to identify violations. The evaluation focuses on four popular LLMs including GPT-3.5, GPT-4, Llama-2, and Vicuna-1.5.

## Key Results
- GPT-4 exhibits a 62% API misuse rate in generated code snippets
- Even with relevant demonstration examples, API misuse remains prevalent
- The static AST-based analysis can detect API misuse patterns without code execution
- Real-world Stack Overflow questions provide more relevant evaluation than artificial coding challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AST-based static analysis can detect API misuse without executing code.
- Mechanism: The system parses LLM-generated code into an abstract syntax tree, extracts the sequence of method calls and control structures, and compares it against predefined API usage rules that encode proper call sequences and exception handling patterns.
- Core assumption: API misuse can be detected purely from syntactic patterns without runtime context.
- Evidence anchors:
  - [section] "To evaluate the API usage correctness in code, ROBUST API detects the API misuses against the API usage rules by extracting call consequences and control structures from the code snippet... The checker computes the longest common sequence between the call sequence and the API usage rules."
  - [abstract] "We design a static analysis method using abstract syntax trees (ASTs) to detect API misuse."

### Mechanism 2
- Claim: Real-world coding questions from Stack Overflow provide more relevant evaluation than coding challenge benchmarks.
- Mechanism: By collecting actual developer questions about specific Java APIs from Stack Overflow, the benchmark captures the types of API misuse that occur in production software development, rather than artificial coding problems.
- Core assumption: Novice developers asking Stack Overflow questions represent the target population for LLM code generation evaluation.
- Evidence anchors:
  - [abstract] "Existing code evaluation benchmark and datasets focus on crafting small tasks such as programming questions in coding interviews, which however deviates from the problem that developers would ask LLM for real-world coding help."
  - [section] "To take advantage of the existing research efforts in the software engineering field, we build ROBUST API based on the dataset from ExampleCheck... We select 23 popular Java APIs from the dataset as shown in Table 1."

### Mechanism 3
- Claim: Providing correct API usage examples in prompts reduces API misuse in LLM-generated code.
- Mechanism: The one-shot-relevant experiment setting provides LLMs with demonstrations of correct API usage patterns, which the models can learn from and apply when generating their own code.
- Core assumption: LLMs can effectively learn API usage patterns from a single demonstration example.
- Evidence anchors:
  - [section] "In the one-shot-relevant setting, we provide LLMs with an example using the same API as the given question... After giving the model the correct API usage example, the model learn how to use the API and responses with the reliable code."
  - [abstract] "The evaluation results show that even for GPT-4, 62% of the generated code contains API misuses, which would cause unexpected consequences if the code is introduced into real-world software."

## Foundational Learning

- Concept: Abstract Syntax Trees (ASTs)
  - Why needed here: ASTs provide a structured representation of code that enables static analysis of API usage patterns without execution.
  - Quick check question: Can you explain how an AST represents the structure of a code snippet and why this is useful for detecting API misuse?

- Concept: API Usage Patterns and Rules
  - Why needed here: Understanding how APIs should be properly used (including call sequences, exception handling, and resource management) is essential for detecting violations in generated code.
  - Quick check question: What are the key components of a well-defined API usage rule, and how do they help prevent common programming errors?

- Concept: Prompt Engineering and Few-Shot Learning
  - Why needed here: The effectiveness of the benchmark depends on how prompts are constructed and whether demonstration examples can guide LLMs toward correct API usage.
  - Quick check question: How does the inclusion of relevant vs. irrelevant demonstration examples affect the quality of LLM-generated code?

## Architecture Onboarding

- Component map:
  Data Collection -> Prompt Generation -> LLM Interface -> Code Processing -> AST Analysis -> API Usage Checker -> Evaluation Metrics

- Critical path:
  Data Collection → Prompt Generation → LLM Interface → Code Processing → AST Analysis → API Usage Checker → Evaluation Metrics

- Design tradeoffs:
  - Static analysis vs. dynamic testing: Static analysis is faster and more comprehensive but may miss runtime-dependent issues
  - Rule-based vs. learning-based detection: Rule-based detection is precise but requires manual rule creation, while learning-based approaches could generalize but may have higher false positive rates
  - Real-world vs. synthetic questions: Real-world questions provide ecological validity but may introduce dataset biases

- Failure signatures:
  - High non-executable rates may indicate prompt issues or LLM limitations
  - Persistent API misuse despite relevant examples may suggest model training data biases
  - Inconsistent results across models may indicate sensitivity to prompt formatting or model-specific behaviors

- First 3 experiments:
  1. Run the zero-shot evaluation on a small subset of questions to establish baseline API misuse rates
  2. Test the one-shot-relevant setting with GPT-4 to verify if demonstration examples reduce misuse
  3. Compare the AST-based checker against a simple keyword-matching baseline to validate the static analysis approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the API misuse rate vary across different programming languages beyond Java?
- Basis in paper: [inferred] The study focuses exclusively on Java APIs, suggesting a need to evaluate other languages.
- Why unresolved: The paper does not explore API misuse in other programming languages, limiting generalizability.
- What evidence would resolve it: Comparative studies on API misuse rates in languages like Python, C++, or JavaScript.

### Open Question 2
- Question: What is the impact of API misuse on software reliability in production environments?
- Basis in paper: [explicit] The paper highlights potential risks like resource leaks and program crashes but does not quantify their impact in real-world scenarios.
- Why unresolved: The study focuses on identifying API misuse but does not measure its consequences in deployed software.
- What evidence would resolve it: Case studies or empirical data linking API misuse to specific failures in production systems.

### Open Question 3
- Question: How effective are current static analysis tools in detecting API misuse in LLM-generated code?
- Basis in paper: [explicit] The paper introduces a static analysis method using ASTs but does not compare it to existing tools.
- Why unresolved: The evaluation focuses on the proposed method without benchmarking against other static analysis approaches.
- What evidence would resolve it: Comparative analysis of API misuse detection accuracy across multiple static analysis tools.

### Open Question 4
- Question: Can fine-tuning LLMs on code repositories with fewer API misuse examples reduce misuse rates?
- Basis in paper: [explicit] The paper suggests that training data containing API violations may lead to misuse, implying that cleaner data could help.
- Why unresolved: The study does not experiment with fine-tuning strategies to address API misuse.
- What evidence would resolve it: Experiments comparing API misuse rates in LLMs trained on datasets with varying levels of API misuse.

### Open Question 5
- Question: What are the long-term trends in API misuse as LLMs become more advanced?
- Basis in paper: [inferred] The study evaluates current models like GPT-3.5 and GPT-4, but does not predict future trends as models evolve.
- Why unresolved: The paper provides a snapshot of current API misuse but lacks longitudinal analysis.
- What evidence would resolve it: Longitudinal studies tracking API misuse rates across multiple generations of LLMs.

## Limitations

- Limited to Java APIs and four specific LLMs, reducing generalizability across programming languages
- Static analysis cannot detect runtime-dependent API misuse patterns
- Stack Overflow dataset may contain biases from novice developers and overrepresentation of certain API types
- Results may be sensitive to prompt engineering variations not fully explored in the study

## Confidence

- High: The finding that even GPT-4 exhibits substantial API misuse (62% rate) is well-supported by systematic evaluation methodology
- Medium: The claim that Stack Overflow-based evaluation is more relevant than coding challenge benchmarks lacks direct comparative evidence
- Low: The assertion that providing correct API usage examples significantly reduces API misuse relies on limited experimental conditions

## Next Checks

1. Implement dynamic testing of generated code snippets to identify API misuse patterns that cannot be detected through static analysis, comparing results against the AST-based approach.

2. Replicate the evaluation framework for Python APIs to test whether high API misuse rates generalize to other programming languages beyond Java.

3. Systematically vary prompt templates, few-shot example selection, and generation parameters to quantify the sensitivity of API misuse rates to prompt design.