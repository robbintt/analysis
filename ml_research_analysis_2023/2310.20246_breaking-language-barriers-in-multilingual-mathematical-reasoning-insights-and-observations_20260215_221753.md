---
ver: rpa2
title: 'Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights
  and Observations'
arxiv_id: '2310.20246'
source_url: https://arxiv.org/abs/2310.20246
tags:
- reasoning
- training
- languages
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends mathematical reasoning tasks from monolingual
  to multilingual contexts by constructing the first multilingual math reasoning instruction
  dataset, MGSM8KInstruct, covering ten languages. The authors build MathOctopus models
  by fine-tuning open-source LLMs with different training strategies (parallel-training
  and cross-training).
---

# Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations

## Quick Facts
- arXiv ID: 2310.20246
- Source URL: https://arxiv.org/abs/2310.20246
- Reference count: 40
- Key outcome: MathOctopus models achieve 47.6% accuracy on MGSM testset, outperforming ChatGPT's 46.3% in few-shot multilingual math reasoning

## Executive Summary
This paper addresses the challenge of extending mathematical reasoning capabilities to multilingual contexts by creating the first comprehensive multilingual math reasoning instruction dataset (MGSM8KInstruct) covering ten languages. The authors develop MathOctopus models through fine-tuning open-source LLMs with two distinct training strategies - parallel-training and cross-training - and demonstrate that these models outperform existing open-source alternatives and even ChatGPT in few-shot scenarios. Key findings reveal that multilingual fine-tuning significantly improves performance across both multilingual and monolingual tasks, while the cross-training strategy particularly enhances English reasoning capabilities.

## Method Summary
The authors construct MGSM8KInstruct by translating 7,473 GSM8K samples and their chain-of-thought solutions into ten languages using ChatGPT with strict translation guidelines. They fine-tune LLaMA models (7B to 33B parameters) using parallel-training (same-language questions and answers) and cross-training (English questions with translated answers) strategies for 3 epochs with learning rate 2e-5. Additionally, they apply multilingual rejection sampling (xRFT) for data augmentation, generating 25 inference samples per language at temperature 0.9. Models are evaluated on the MGSM testset and out-of-domain MSV AMP dataset, comparing performance against LLaMA base, RFT, MAmmoTH, WizardMath, ChatGPT, and GPT-4 baselines.

## Key Results
- MathOctopus achieves 47.6% accuracy on MGSM testset versus ChatGPT's 46.3%
- Cross-training strategy improves English performance while parallel-training enhances in-domain results
- Multilingual supervised fine-tuning significantly boosts low-resource language performance
- xRFT provides marginal benefits (1-2% improvement) but may compromise out-of-domain generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual fine-tuning improves performance in both multilingual and monolingual settings by providing diverse reasoning paths and enhancing generalization.
- Mechanism: Training on multilingual corpora exposes the model to different linguistic structures and problem-solving approaches, which enhances its ability to generalize and reason across languages.
- Core assumption: The multilingual data provides diverse reasoning paths that are beneficial for both multilingual and monolingual tasks.
- Evidence anchors:
  - [abstract]: "Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance."
  - [section]: "Interestingly, the parallel-training strategy could result in better in-domain test results while the cross-training demonstrates superior generalization capabilities on out-of-domain testing."
  - [corpus]: "The model's efficacy varies in languages that are unseen in training: While there are improved outcomes in certain languages like Japanese and French, a corresponding decline is witnessed in others, such as German and Russian."
- Break condition: If the multilingual data is not diverse enough or if the model overfits to the specific languages in the training set.

### Mechanism 2
- Claim: Cross-training strategy improves English performance while parallel-training enhances in-domain results.
- Mechanism: Cross-training involves using English questions with answers in other languages, which strengthens the model's understanding of English while learning to express solutions in different languages. Parallel-training, on the other hand, involves using questions and answers in the same language, which enhances the model's proficiency in that specific language.
- Core assumption: The cross-training strategy provides more exposure to English, while parallel-training provides more exposure to the target language.
- Evidence anchors:
  - [abstract]: "Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance."
  - [section]: "Interestingly, the parallel-training strategy could result in better in-domain test results while the cross-training demonstrates superior generalization capabilities on out-of-domain testing."
  - [corpus]: "MathOctopusC emerges as the superior performer (e.g., 50.8% vs. 49.3%, 49.3% vs. 46.8% with 7B-level)."
- Break condition: If the model fails to generalize from the cross-training data or if the parallel-training data is not representative of the target language.

### Mechanism 3
- Claim: Multilingual rejection sampling provides marginal benefits and may compromise generalization ability.
- Mechanism: Rejection sampling involves generating multiple reasoning paths and selecting the correct ones to augment the training data. In a multilingual context, this process may introduce diverse reasoning paths but also increase the risk of overfitting to the training data.
- Core assumption: The additional reasoning paths generated by rejection sampling are beneficial for the model's performance.
- Evidence anchors:
  - [abstract]: "When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited."
  - [section]: "xRFT's contribution to model enhancement appears to be somewhat circumscribed. Its potency wanes, particularly in out-of-domain test scenarios."
  - [corpus]: "The xRFT's contribution to MathOctopusP hovers around a modest 1%-2% average uplift. However, this figure dips below 1% in out-of-domain MSV AMP testset."
- Break condition: If the rejection sampling process introduces too much noise or if the model overfits to the augmented data.

## Foundational Learning

- Concept: Supervised fine-tuning (SFT)
  - Why needed here: SFT is used to adapt the pre-trained language models to the specific task of multilingual mathematical reasoning by training them on the MGSM8KInstruct dataset.
  - Quick check question: What is the difference between supervised fine-tuning and unsupervised fine-tuning?

- Concept: Chain-of-thought (CoT) prompting
  - Why needed here: CoT prompting is used to guide the model through step-by-step reasoning, which is essential for solving mathematical problems.
  - Quick check question: How does CoT prompting differ from direct answer generation?

- Concept: Multilingual training data
  - Why needed here: Multilingual training data is necessary to train the model on mathematical reasoning tasks in different languages, which is the primary goal of this work.
  - Quick check question: Why is it important to have diverse multilingual training data for this task?

## Architecture Onboarding

- Component map: Pre-trained LLM (LLaMA) -> Supervised Fine-Tuning (SFT) with MGSM8KInstruct -> Rejection Sampling (xRFT) -> Evaluation on MGSM and MSV AMP test sets
- Critical path: Collect multilingual training data → Fine-tune pre-trained model using parallel/cross-training strategies → Apply xRFT for augmentation → Evaluate on in-domain and out-of-domain test sets
- Design tradeoffs: The main tradeoff is between the diversity of the training data and the risk of overfitting. Using more diverse data can improve generalization but may also introduce noise.
- Failure signatures: If the model performs poorly on out-of-domain test sets, it may indicate overfitting to the training data. If the model fails to generalize across languages, it may indicate insufficient exposure to diverse linguistic structures.
- First 3 experiments:
  1. Fine-tune a pre-trained model on the MGSM8KInstruct dataset using the parallel-training strategy and evaluate its performance on the MGSM test set.
  2. Fine-tune a pre-trained model on the MGSM8KInstruct dataset using the cross-training strategy and evaluate its performance on the MGSM test set.
  3. Apply rejection sampling to the fine-tuned models and evaluate the impact on performance in both in-domain and out-of-domain test sets.

## Open Questions the Paper Calls Out
- How does MathOctopus performance scale with larger model sizes beyond 13B parameters? (explicit: "Developing MathOctopus based on larger size LLMs, including LLaMA 2-70B and LLaMA-Coders, which is a future work in our following experiments.")
- Does increasing the number of languages in MGSM8KInstruct beyond 10 provide additional performance benefits? (explicit: "We are still not very clear whether including more languages in MGSM8KInstruct could benefit current models, which will discussed in our next version.")
- How does xRFT affect model performance at scale for larger, more performant models? (explicit: "Currently, we only apply xRFT to 7B and 13B models due to the high cost of inferencing. We also will conduct xRFT to more performant models, further investigating its efficiency.")

## Limitations
- The MGSM8KInstruct dataset contains only 7.5k training samples after filtering, which may not capture full diversity across languages
- Translation quality varies across languages, particularly for low-resource languages like Bengali, Swahili, and Thai
- MathOctopus models underperform GPT-4 by 15-20 percentage points on out-of-domain MSV AMP dataset
- Rejection sampling provides only marginal benefits (1-2% improvement) while requiring significant computational resources

## Confidence
**High Confidence**: The core finding that multilingual fine-tuning improves both multilingual and monolingual performance is well-supported by systematic experiments across different model sizes (7B to 33B) and training strategies.

**Medium Confidence**: The claim about cross-training improving English performance while parallel-training enhances in-domain results shows consistent patterns but requires more extensive ablation studies.

**Low Confidence**: The assertion that multilingual rejection sampling provides only marginal benefits needs further validation, as the paper doesn't adequately explore whether this limitation stems from the sampling strategy itself or from the quality of the underlying multilingual training data.

## Next Checks
1. **Dataset Quality Validation**: Conduct blind human evaluation of 100 randomly selected translations across all ten languages to measure translation consistency, formula preservation accuracy, and naming convention adherence.

2. **Ablation Study on Training Data**: Systematically vary the amount of training data per language (e.g., 1k, 3k, 7k samples) to determine the minimum effective dataset size for each language and identify the point of diminishing returns for multilingual fine-tuning.

3. **Cross-Lingual Transfer Analysis**: Design an experiment where models are trained on n-1 languages and tested on the held-out language, then repeat for each language combination to reveal which languages provide the most transferable reasoning patterns.