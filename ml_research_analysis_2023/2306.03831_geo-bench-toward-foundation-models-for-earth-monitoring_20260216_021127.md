---
ver: rpa2
title: 'GEO-Bench: Toward Foundation Models for Earth Monitoring'
arxiv_id: '2306.03831'
source_url: https://arxiv.org/abs/2306.03831
tags:
- data
- training
- datasets
- arxiv
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GEO-Bench, a new benchmark designed to stimulate
  the development of foundation models for Earth monitoring. The benchmark comprises
  six classification and six segmentation tasks, carefully curated to be relevant
  and well-suited for model evaluation.
---

# GEO-Bench: Toward Foundation Models for Earth Monitoring

## Quick Facts
- arXiv ID: 2306.03831
- Source URL: https://arxiv.org/abs/2306.03831
- Authors: 
- Reference count: 40
- Primary result: The benchmark will drive progress across Earth monitoring tasks by providing systematic model quality measurement and reducing negative impacts through open evaluation

## Executive Summary
GEO-Bench is a new benchmark designed to stimulate the development of foundation models for Earth monitoring by providing a standardized evaluation framework. The benchmark comprises six classification and six segmentation tasks carefully curated to be relevant to the field while being well-suited for model evaluation. The authors propose a robust methodology for evaluating models and reporting aggregated results, enabling reliable assessment of progress across diverse Earth monitoring applications.

## Method Summary
The GEO-Bench methodology involves fine-tuning pre-trained models on 12 Earth monitoring tasks (6 classification, 6 segmentation) with hyperparameter tuning (max 16 trials per task) using AdamW or SGD optimizers. Models are evaluated using normalized test accuracy for classification and normalized IoU for segmentation, with results aggregated using Interquartile Mean (IQM) and bootstrapping for confidence intervals. The benchmark includes multispectral, SAR, hyperspectral, elevation, and cloud probability modalities, with data transformations that maintain consistency while enabling single GPU replication.

## Key Results
- Multispectral pre-training on Sentinel-2 data using MoCo significantly improves performance across many tasks compared to RGB-only models
- Smaller training sets increase the discriminative power of the benchmark, making it easier to distinguish between model performances
- ConvNeXt shows progressive improvement over SwinV2 as training set sizes decrease, highlighting differences in data efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GEO-Bench provides a standardized evaluation framework that enables direct comparison of foundation models across diverse Earth monitoring tasks.
- Mechanism: By curating six classification and six segmentation tasks with consistent data transformations and reporting protocols, the benchmark creates a controlled environment where model performance can be measured and compared systematically.
- Core assumption: All models are evaluated under identical conditions (same data splits, preprocessing, and evaluation metrics) ensuring fair comparison.
- Evidence anchors: [abstract] "we propose a benchmark comprised of six classification and six segmentation tasks, which were carefully curated and adapted to be both relevant to the field and well-suited for model evaluation." [section] "GEO-Bench is composed of 6 classification tasks and 6 segmentation tasks... transformed versions of the datasets are smaller than their original form, and all results can be replicated on single GPUs."
- Break condition: If different models use different data augmentations or hyperparameter budgets beyond the specified limits, comparisons become invalid.

### Mechanism 2
- Claim: Multispectral information significantly improves foundation model performance on Earth monitoring tasks.
- Mechanism: Models pre-trained on multispectral Sentinel-2 data and fine-tuned on multispectral tasks outperform models that only see RGB channels, demonstrating the value of leveraging full spectral information.
- Core assumption: The additional spectral bands contain information relevant to the downstream tasks that RGB channels alone cannot capture.
- Evidence anchors: [section] "We found that using a model pre-trained on RGB-only... and augmenting the architecture by randomly initialising the weights of the missing channels in the first layer (+RMulti) does not lead to overall improved performance... On the other hand the ResNet50 pre-trained on Sentinel-2 using MoCo [71] leads to an impressive performance increase across many tasks."
- Break condition: If the downstream tasks do not require spectral information beyond RGB, or if the multispectral channels contain mostly redundant information.

### Mechanism 3
- Claim: Smaller training sets improve the discriminative power of the benchmark for comparing models.
- Mechanism: As training set size decreases, the performance gap between different models becomes more pronounced, making it easier to distinguish which models are genuinely better rather than all achieving near-perfect performance.
- Core assumption: All models have different learning characteristics and will degrade at different rates as training data becomes scarce.
- Evidence anchors: [section] "In Figure 5, we observe the behaviour of the baselines as the size of the training set grows from 1% to 100%. We can see that ConvNeXt becomes progressively better than SwinV2 as the training set decreases."
- Break condition: If all models are equally data-efficient or if the tasks become too difficult at small training sizes causing all models to perform poorly.

## Foundational Learning

- Concept: Self-supervised pre-training on large unlabeled datasets
  - Why needed here: Foundation models rely on learning general representations from vast amounts of unlabeled Earth observation data before being fine-tuned on specific tasks
  - Quick check question: What is the primary advantage of self-supervised learning for Earth monitoring foundation models compared to supervised pre-training?

- Concept: Transfer learning and fine-tuning methodology
  - Why needed here: The benchmark evaluates how well pre-trained models can adapt to new Earth monitoring tasks through fine-tuning rather than training from scratch
  - Quick check question: Why does the paper recommend fine-tuning pre-trained models rather than using fixed feature extractors for the benchmark?

- Concept: Statistical evaluation with confidence intervals
  - Why needed here: Reliable comparison of model performance requires proper statistical treatment of results across multiple random seeds and bootstrapping for uncertainty quantification
  - Quick check question: What metric does the benchmark use to reduce the impact of outliers when aggregating results across tasks?

## Architecture Onboarding

- Component map: Data loading utilities -> Task specifications -> Model evaluation pipelines -> Result aggregation tools
- Critical path: Download datasets -> Transform to benchmark format -> Load with consistent schema -> Fine-tune models with specified hyperparameters -> Evaluate on test sets -> Aggregate results with bootstrapping
- Design tradeoffs: The benchmark prioritizes accessibility and reproducibility over using full-sized datasets, trading some real-world applicability for faster experimentation and lower computational requirements
- Failure signatures: Poor performance on certain tasks may indicate issues with multispectral handling, overfitting on small datasets, or inappropriate model architecture choices for Earth observation data characteristics
- First 3 experiments:
  1. Fine-tune ResNet50-timm on m-bigearthnet classification task with default hyperparameters to verify basic functionality
  2. Compare ResNet50-timm vs ResNet50-MoCo-S2 on m-so2sat to observe multispectral benefits
  3. Run convergence analysis on m-eurosat with decreasing training set sizes to understand discriminative power

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of foundation models trained on multi-spectral data compare to those trained on single-spectral data across different Earth monitoring tasks?
- Basis in paper: [explicit] The paper discusses leveraging multispectral information during pre-training and fine-tuning, showing that ResNet50 pre-trained on Sentinel-2 using MoCo leads to improved performance across tasks compared to RGB-only models.
- Why unresolved: The paper provides initial evidence of the benefits of multi-spectral data but does not conduct a comprehensive comparison across all tasks and model architectures.
- What evidence would resolve it: A detailed study comparing the performance of foundation models trained on various combinations of spectral bands (e.g., RGB, multispectral, SAR) across all tasks in the GEO-Bench would provide a clearer understanding of the impact of spectral diversity on model performance.

### Open Question 2
- Question: What is the optimal training set size for achieving the best balance between model performance and computational efficiency in Earth monitoring tasks?
- Basis in paper: [explicit] The paper explores the effect of training set size on model performance, finding that smaller datasets can be more discriminative and that ConvNeXt becomes progressively better than SwinV2 as the training set decreases.
- Why unresolved: While the paper provides insights into the relationship between training set size and model performance, it does not determine the optimal size for balancing performance and efficiency.
- What evidence would resolve it: A systematic study varying the training set size across all tasks in the GEO-Bench and measuring both performance and computational cost would help identify the optimal training set size for different Earth monitoring applications.

### Open Question 3
- Question: How do foundation models for Earth monitoring generalize to unseen regions and sensor types?
- Basis in paper: [explicit] The paper mentions that some regions have higher data coverage than others due to satellite revisit rates and costs, which could lead to biases and fairness issues.
- Why unresolved: The paper does not investigate the generalization capabilities of foundation models to regions with limited data coverage or different sensor types.
- What evidence would resolve it: Experiments evaluating the performance of foundation models on tasks from regions with limited data coverage and tasks involving different sensor types (e.g., SAR, hyperspectral) would provide insights into their generalization capabilities.

## Limitations
- Limited task diversity: The benchmark covers only 12 tasks, which may not fully represent the complexity of real-world Earth monitoring challenges
- Computational accessibility tradeoffs: Reducing dataset sizes for accessibility may compromise the benchmark's ability to evaluate models on realistic problem scales
- Evaluation methodology constraints: The focus on fine-tuning pre-trained models excludes comparisons with end-to-end training approaches that might be more appropriate for certain applications

## Confidence
- High confidence in the benchmark's methodological rigor for standardized evaluation
- Medium confidence in the multispectral performance claims based on limited pre-trained models and tasks
- Medium confidence in the conclusions about discriminative power of smaller training sets

## Next Checks
1. **Cross-task generalization test**: Evaluate whether models performing well on GEO-Bench classification tasks also excel on novel Earth monitoring tasks not included in the benchmark, to assess real-world applicability
2. **Full-scale vs. benchmark comparison**: Re-run key experiments using the original dataset sizes (not the reduced versions) to quantify the impact of accessibility tradeoffs on model ranking and performance gaps
3. **Alternative evaluation methodology**: Implement and compare results from an end-to-end training protocol alongside the fine-tuning approach to determine if the evaluation methodology itself biases which model architectures appear superior