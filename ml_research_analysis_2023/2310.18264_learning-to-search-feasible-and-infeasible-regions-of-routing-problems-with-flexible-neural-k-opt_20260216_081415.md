---
ver: rpa2
title: Learning to Search Feasible and Infeasible Regions of Routing Problems with
  Flexible Neural k-Opt
arxiv_id: '2310.18264'
source_url: https://arxiv.org/abs/2310.18264
tags:
- search
- neuopt
- solvers
- learning
- solver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents Neural k-Opt (NeuOpt), a novel learning-to-search\
  \ solver for routing problems. NeuOpt learns flexible k-opt exchanges through a\
  \ tailored action factorization method and a customized recurrent dual-stream decoder,\
  \ allowing it to handle k-opt with any k\u22652."
---

# Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt

## Quick Facts
- arXiv ID: 2310.18264
- Source URL: https://arxiv.org/abs/2310.18264
- Reference count: 40
- Key outcome: NeuOpt achieves 0.00% optimality gaps on TSP-100 and CVRP-20 while significantly outperforming existing L2S solvers and matching state-of-the-art L2C/L2P solvers.

## Executive Summary
This paper introduces Neural k-Opt (NeuOpt), a learning-to-search solver for routing problems that learns flexible k-opt exchanges through a tailored action factorization method and a customized recurrent dual-stream decoder. The method handles k-opt with any k≥2 and automatically determines optimal k values per search step. To enable exploration of both feasible and infeasible regions, the authors propose the Guided Infeasible Region Exploration (GIRE) scheme, which supplements the policy network with feasibility-related features and uses reward shaping to guide reinforcement learning. Extensive experiments on TSP and CVRP show that NeuOpt significantly outperforms existing masking-based L2S solvers and achieves state-of-the-art results, surpassing strong L2C and L2P solvers as well as the LKH-3 solver on CVRP.

## Method Summary
NeuOpt is a learning-to-search solver that combines flexible k-opt exchanges with guided exploration of infeasible regions. The method uses action factorization to decompose complex k-opt moves into basis moves (S-move, I-moves, E-move), parameterizes them with a recurrent dual-stream decoder, and extends beyond pure feasibility masking through the GIRE scheme. The model is trained using n-step proximal policy optimization with curriculum learning and employs dynamic data augmentation during inference. The approach achieves 0.00% optimality gaps on both TSP-100 and CVRP-20 while maintaining competitive runtime performance.

## Key Results
- Achieves 0.00% optimality gaps on both TSP-100 and CVRP-20
- Significantly outperforms existing masking-based L2S solvers on both TSP and CVRP
- Matches or exceeds state-of-the-art L2C and L2P solvers on CVRP
- Outperforms LKH-3 solver on CVRP while maintaining competitive performance on TSP

## Why This Works (Mechanism)

### Mechanism 1: Action Factorization for Flexible k-Opt
The action factorization method decomposes complex k-opt exchanges into a sequence of basis moves (S-move, I-moves, E-move), enabling the model to flexibly handle any k≥2 and automatically determine optimal k values per search step. The sequential construction allows varying k values across different search steps, balancing coarse-grained (larger k) and fine-grained (smaller k) searches. The S-move removes an edge to create endpoints, I-moves add new edges while maintaining a Hamiltonian path with sequential conditions, and the E-move closes the path into a cycle.

### Mechanism 2: Recurrent Dual-Stream Decoder
The Recurrent Dual-Stream (RDS) decoder effectively parameterizes the k-opt action factorization by capturing strong correlations and dependencies between removed and added edges through dual contextual modeling. Two complementary decoding streams (move stream µ and edge stream λ) provide different perspectives - µ offers overall view of historical moves while λ focuses on detailed edge proposals. The GRU-based architecture models historical decision dependencies for node selection.

### Mechanism 3: Guided Infeasible Region Exploration
The Guided Infeasible Region Exploration (GIRE) scheme extends beyond feasibility masking by supplementing policy network with feasibility-related features and reward shaping, enabling exploration of both feasible and infeasible regions to find better solutions. GIRE enriches the policy network with Violation Indicator (VI) features flagging constraint violations and Exploration Statistics (ES) features providing exploration behavior statistics. Reward shaping with regulation and bonus terms guides exploration at feasibility boundaries.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) formulation**
  - Why needed here: The k-opt solver is formulated as an MDP where states represent current solutions and actions represent k-opt exchanges, enabling reinforcement learning optimization.
  - Quick check question: What are the state, action, transition, and reward components in the NeuOpt MDP formulation?

- **Concept: Transformer-based attention mechanisms**
  - Why needed here: The encoder uses Transformer-styled encoders with Synthesis Attention to refine node embeddings, while the decoder uses dual-stream attention for node selection during k-opt construction.
  - Quick check question: How does the dual-stream attention in RDS decoder differ from standard multi-head attention in Transformers?

- **Concept: Reinforcement learning with policy gradient methods**
  - Why needed here: The model is trained using n-step proximal policy optimization (PPO) with curriculum learning, requiring understanding of policy gradient optimization and reward shaping.
  - Quick check question: How does the reward shaping in GIRE modify the standard PPO objective function?

## Architecture Onboarding

- **Component map**: Feature embedding -> Stacked Transformer encoders (L=3) -> Refined node embeddings -> Recurrent Dual-Stream (RDS) decoder -> Node selection for basis moves -> Solution update -> Reward calculation -> Critic value estimation -> Policy gradient update

- **Critical path**: State encoding -> RDS decoder action selection -> Solution update -> Reward calculation -> Critic value estimation -> Policy gradient update

- **Design tradeoffs**: Fixed K=4 steps vs adaptive step count (fixed provides computational efficiency but may limit flexibility); Dual-stream vs single-stream attention (dual-stream captures more dependencies but increases parameter count); GIRE vs pure masking (GIRE enables better exploration but requires careful reward shaping parameter tuning)

- **Failure signatures**: Poor convergence (check if GRU hidden states properly capture historical dependencies); Getting stuck in local optima (verify D2A is triggering correctly and GIRE exploration is balanced); Constraint violations not improving (check VI feature calculation and ES statistics accuracy)

- **First 3 experiments**:
  1. Test basic k-opt factorization with K=2 on TSP-20 without GIRE to verify action construction works
  2. Enable GIRE on CVRP-20 with varying α, β parameters to find optimal reward shaping balance
  3. Test D2A with different TD2A values to determine when augmentation should trigger during inference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can NeuOpt be scaled to solve even larger VRP instances (e.g., 1000+ nodes)?
- Basis in paper: The paper discusses NeuOpt's scalability to TSP-200 and CVRP-200, but mentions it falls short compared to some L2P solvers for larger TSPs.
- Why unresolved: The paper suggests potential solutions like divide-and-conquer strategies, heatmaps, and more scalable encoders, but does not explore or implement them.
- What evidence would resolve it: Experiments showing NeuOpt's performance on VRP instances with 1000+ nodes, and comparisons with state-of-the-art solvers for large-scale VRPs.

### Open Question 2
- Question: How can NeuOpt be adapted to handle more complex VRP constraints, such as time windows or pickup and delivery precedence?
- Basis in paper: The paper introduces the GIRE scheme for handling capacity constraints in CVRP and mentions its potential applicability to other VRP constraints.
- Why unresolved: The paper only demonstrates GIRE on capacity constraints and does not explore its effectiveness on other VRP constraints.
- What evidence would resolve it: Experiments showing NeuOpt's performance on VRP instances with time windows, pickup and delivery precedence, or other complex constraints, and comparisons with state-of-the-art solvers for these variants.

### Open Question 3
- Question: How can NeuOpt's generalization capabilities be further improved for instances with different sizes or distributions?
- Basis in paper: The paper shows NeuOpt's generalization on real-world TSPLIB and CVRPLIB instances, but acknowledges room for improvement.
- Why unresolved: The paper suggests integrating post-hoc per-instance processing boosters like EAS, but does not explore or implement them.
- What evidence would resolve it: Experiments showing NeuOpt's performance on VRP instances with varying sizes and distributions, and comparisons with state-of-the-art solvers that excel in generalization.

## Limitations
- The dual-stream attention design lacks formal theoretical justification for why two complementary streams are superior to alternative architectures
- The GIRE scheme's effectiveness relies heavily on reward shaping parameters (α, β) that may require problem-specific tuning without systematic sensitivity analysis
- Fixed K=4 search steps limit flexibility compared to adaptive approaches, though computational efficiency is improved

## Confidence

- **High confidence**: Overall architecture design and empirical results showing state-of-the-art performance on CVRP and competitive results on TSP are well-supported by experimental evidence
- **Medium confidence**: Mechanism explanations for action factorization and dual-stream attention are plausible but lack complete theoretical grounding
- **Medium confidence**: GIRE scheme's effectiveness is demonstrated empirically but theoretical understanding of how infeasible region exploration leads to better solutions is limited

## Next Checks

1. **Sensitivity analysis**: Systematically vary the GIRE reward shaping parameters (α, β) and measure impact on solution quality across different problem scales to determine robustness
2. **Architecture ablation**: Test single-stream attention variants against the dual-stream design on smaller problem instances to quantify the contribution of each stream
3. **Runtime efficiency validation**: Measure the actual impact of D2A on inference runtime and solution quality across varying TD2A thresholds to optimize the augmentation strategy