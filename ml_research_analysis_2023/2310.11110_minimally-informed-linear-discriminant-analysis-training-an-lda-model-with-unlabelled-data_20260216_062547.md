---
ver: rpa2
title: 'Minimally Informed Linear Discriminant Analysis: training an LDA model with
  unlabelled data'
arxiv_id: '2310.11110'
source_url: https://arxiv.org/abs/2310.11110
tags:
- milda
- class
- data
- known
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Minimally Informed Linear Discriminant Analysis
  (MILDA), which enables computing LDA projection vectors without labels by using
  minimal prior knowledge. Specifically, knowing just one of three statistics (class
  average of one class, relative difference between class averages, or class covariance
  matrices) is sufficient to compute the exact LDA projection.
---

# Minimally Informed Linear Discriminant Analysis: training an LDA model with unlabelled data

## Quick Facts
- arXiv ID: 2310.11110
- Source URL: https://arxiv.org/abs/2310.11110
- Reference count: 23
- Key outcome: Introduces MILDA method to compute LDA projection vectors without labels using minimal prior knowledge (one of three statistics), achieving nearly identical performance to supervised LDA.

## Executive Summary
This paper introduces Minimally Informed Linear Discriminant Analysis (MILDA), a method that enables computing Linear Discriminant Analysis (LDA) projection vectors without labeled data. The key insight is that knowing just one of three specific statistics about the data (class average of one class, relative difference between class averages, or class covariance matrices) is sufficient to compute the exact LDA projection. This is achieved through data transformations that satisfy proportionality assumptions. MILDA shows remarkable robustness to estimation errors and performs nearly identically to supervised LDA in experiments, even with challenging data properties like overlapping classes, high correlation, and imbalanced class distributions.

## Method Summary
MILDA enables LDA projection computation without labels by leveraging minimal prior knowledge. When one of three ground-truth statistics is known (class average of one class, relative difference between class averages, or class covariance matrices), the data can be transformed such that class averages become proportional to each other. This proportionality allows computation of the LDA projection vector w ∝ Σ̄⁻¹μ̄ using only unlabeled data. The method is particularly robust in challenging cases where unsupervised methods typically struggle, and it can adapt to non-stationary data by updating with new samples.

## Key Results
- MILDA achieves near-identical classification accuracy to supervised LDA using only unlabeled data and minimal prior knowledge
- The method is robust to estimation errors in the ground-truth statistic, particularly when classes are imbalanced or have skewed covariance matrices
- MILDA adapts well to non-stationary data, recovering performance after distribution shifts with new data samples
- MILDA outperforms traditional unsupervised methods (K-means, GMM) in binary classification tasks when prior knowledge is available

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LDA projection vector can be computed from unlabelled data using minimal prior knowledge.
- Mechanism: By transforming the data using one of three possible ground-truth statistics (class average of one class, relative difference between class averages, or class covariance matrices), the transformed data satisfies the proportionality assumption µ+ ∼ µ-, enabling computation of the LDA projection vector without labels.
- Core assumption: The three possible ground-truth statistics are sufficient to transform the data such that the class averages become proportional to each other.
- Evidence anchors:
  - [abstract] "only one of the following three pieces of information is actually sufficient to compute the LDA projection vector if only unlabelled data are available"
  - [section II-C] "we show that only one of the aforementioned ground-truth statistics... is known"
- Break condition: The ground-truth statistic is estimated with significant error, or the data does not satisfy the proportionality assumption even after transformation.

### Mechanism 2
- Claim: MILDA is robust to approximation errors in the ground-truth statistic.
- Mechanism: The error in the projection vector wM ILDA is minimized when the classes are harder to separate, more imbalanced, or have skewed covariance matrices, as these properties reduce the impact of errors on the projection.
- Core assumption: The properties that reduce the impact of errors (class separability, imbalance, covariance structure) are present in the data.
- Evidence anchors:
  - [section III-A] "the error will have a small effect on wM ILDA when... the angle between ˆΣ−1µ∆ and ˆΣ−1e is small"
  - [section III-B] "MILDA becomes more robust against errors as the class covariances increase... as predicted in Property 1 and Property 4"
- Break condition: The data has properties that amplify errors (e.g., balanced classes, white covariance matrices, high class separability).

### Mechanism 3
- Claim: MILDA can adapt to non-stationary data.
- Mechanism: Since MILDA only requires minimal prior knowledge and can be updated with new data, it can adapt to changes in the statistics of the data over time.
- Core assumption: The ground-truth statistic used for transformation is stationary, and new data is available to update the projection vector.
- Evidence anchors:
  - [abstract] "is able to quickly adapt to non-stationary data, making it well-suited to use as an adaptive classifier"
  - [section IV-C] "MILDA completely recovers after 500 samples... showing that the model is well able to track gradual changes in statistics"
- Break condition: The ground-truth statistic is non-stationary, or new data is not available to update the projection vector.

## Foundational Learning

- Concept: Linear Discriminant Analysis (LDA)
  - Why needed here: MILDA is a modification of LDA that allows computation of the LDA projection vector without labels.
  - Quick check question: What is the objective function that LDA aims to maximize?

- Concept: Fisher's Linear Discriminant Analysis
  - Why needed here: MILDA is based on Fisher's LDA formulation, which maximizes the distance between class averages while minimizing the variance within each class.
  - Quick check question: What is the solution to Fisher's LDA objective function?

- Concept: Covariance matrices and class averages
  - Why needed here: MILDA requires knowledge of the class covariance matrices and class averages to compute the LDA projection vector, either directly or through transformation.
  - Quick check question: How are the class covariance matrices and class averages estimated from labelled data?

## Architecture Onboarding

- Component map: Data transformation (f(x)) -> Global mean and covariance computation (μ̄, Σ̄) -> Projection vector calculation (w ∝ Σ̄⁻¹μ̄)
- Critical path: Transform data using ground-truth statistic → Compute global mean and covariance → Calculate projection vector
- Design tradeoffs: The choice of transformation function f(x) depends on the available ground-truth statistic. The transformation function should satisfy the proportionality assumption µ+ ∼ µ- to enable computation of the LDA projection vector without labels.
- Failure signatures: MILDA may fail if the ground-truth statistic is estimated with significant error, or if the data does not satisfy the proportionality assumption even after transformation. MILDA may also fail if the data is highly non-stationary and the ground-truth statistic is not updated frequently enough.
- First 3 experiments:
  1. Verify that MILDA produces the same projection vector as LDA on synthetic data with known ground-truth statistics.
  2. Test the robustness of MILDA to approximation errors in the ground-truth statistic on synthetic data with varying properties (class separability, imbalance, covariance structure).
  3. Evaluate the performance of MILDA on a real-world classification task where labels are not available, but a ground-truth statistic is known (e.g., target signal detection in a sensor array).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific assumptions on data structure (beyond those in MILDA) would enable fully unsupervised LDA-like classification without any prior statistics?
- Basis in paper: [explicit] The authors state that further research is needed to incorporate assumptions like Gaussian distributions, uncorrelated but non-white features, or other data properties to move from "minimally informed" to fully unsupervised models.
- Why unresolved: The paper demonstrates MILDA requires at least one ground-truth statistic and suggests incorporating additional assumptions could remove this requirement, but doesn't specify which assumptions or how to implement them.
- What evidence would resolve it: Mathematical proofs showing how specific assumptions (e.g., Gaussianity, uncorrelated features, sparsity) could replace the need for prior statistics, along with experimental validation on real-world datasets.

### Open Question 2
- Question: How does MILDA's robustness to estimation errors scale with sample size and feature dimensionality in practical scenarios?
- Basis in paper: [inferred] The paper shows MILDA's sensitivity to errors decreases with class overlap and imbalance, but doesn't extensively explore how performance scales with N and D in high-dimensional settings.
- Why unresolved: While the paper demonstrates MILDA's robustness in controlled experiments, it doesn't systematically analyze the trade-off between sample size, dimensionality, and estimation error tolerance.
- What evidence would resolve it: Empirical studies varying N and D across multiple orders of magnitude, showing accuracy degradation curves and identifying thresholds where MILDA becomes unreliable.

### Open Question 3
- Question: Can MILDA be extended to multi-class classification problems while maintaining its label-free property?
- Basis in paper: [explicit] The paper focuses exclusively on binary classification, with no discussion of multi-class extensions or their feasibility.
- Why unresolved: The authors derive theoretical results for binary cases and show practical success there, but don't address whether the mathematical framework generalizes to K > 2 classes.
- What evidence would resolve it: Proofs of MILDA's extension to multi-class settings, along with experimental validation showing performance comparable to multi-class LDA when only partial prior knowledge is available.

## Limitations
- MILDA's performance claims rely heavily on controlled synthetic experiments and may not generalize to real-world scenarios with significant noise or non-Gaussian distributions
- The method requires at least one ground-truth statistic, making it "minimally informed" rather than truly unsupervised
- Robustness analysis assumes idealized conditions for the proportionality transformation, which may not hold in practice

## Confidence
**High confidence**: The theoretical framework demonstrating that one of three statistics suffices for exact LDA projection computation. The closed-form solution w ∝ Σ̄⁻¹μ̄ is mathematically sound and well-derived.

**Medium confidence**: Claims about MILDA's robustness to estimation errors and its performance on imbalanced or overlapping classes. While theoretically justified, these claims depend on specific data properties that may not hold universally in practice.

**Medium confidence**: The non-stationary adaptation capabilities shown in numerical experiments. The gradual drift simulation provides evidence, but real-world applicability remains to be validated.

## Next Checks
1. **Real-world robustness test**: Apply MILDA to a real-world dataset (e.g., sensor data or financial time series) where class labels exist but are intentionally withheld, comparing MILDA performance against supervised LDA under realistic noise and non-Gaussian conditions.

2. **Error sensitivity analysis**: Systematically vary the estimation error in the ground-truth statistic (not just direction but magnitude) and measure the degradation in MILDA's classification accuracy and projection vector alignment with LDA.

3. **Abrupt concept drift validation**: Modify the non-stationary experiment to include sudden shifts in class distributions and evaluate MILDA's adaptation speed and accuracy recovery compared to supervised LDA retraining.