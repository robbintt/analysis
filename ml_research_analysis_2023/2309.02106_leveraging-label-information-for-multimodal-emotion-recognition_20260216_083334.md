---
ver: rpa2
title: Leveraging Label Information for Multimodal Emotion Recognition
arxiv_id: '2309.02106'
source_url: https://arxiv.org/abs/2309.02106
tags:
- speech
- label
- text
- emotion
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel label embedding enhanced multimodal
  emotion recognition model (LE-MER) to incorporate label information into the multimodal
  framework. For text, we utilize the BERT to extract the text representation, and
  the semantic label embedding is generated by averaging the embeddings of the top-k
  keywords.
---

# Leveraging Label Information for Multimodal Emotion Recognition

## Quick Facts
- arXiv ID: 2309.02106
- Source URL: https://arxiv.org/abs/2309.02106
- Reference count: 0
- The proposed LE-MER model achieves new state-of-the-art performance on the IEMOCAP dataset.

## Executive Summary
This work introduces a novel label embedding enhanced multimodal emotion recognition model (LE-MER) that incorporates label information into the multimodal framework. The model leverages BERT for text representation and wav2vec2.0 for speech encoding, while generating semantic and tonal label embeddings from emotion-relevant keywords and frames. By introducing a label-guided attentive fusion module, LE-MER can align text and speech representations from an emotion space perspective. Experiments on IEMOCAP demonstrate that LE-MER outperforms existing baselines and achieves new state-of-the-art performance.

## Method Summary
The LE-MER model processes multimodal emotion recognition by extracting text representations using BERT and speech representations using wav2vec2.0. Semantic label embeddings are generated by averaging embeddings of top-k keywords from text, while tonal label embeddings are obtained by averaging embeddings of top-k emotion-relevant speech frames. A label-guided attentive fusion module aligns text and speech modalities through cross-attention mechanisms, with an attention constraint module that enforces alignment between emotion-aware and semantic cross-attention maps. The model is trained end-to-end with a combination of cross-entropy loss and attention constraint loss.

## Key Results
- LE-MER achieves new state-of-the-art performance on IEMOCAP dataset
- Outperforms existing multimodal emotion recognition baselines
- Demonstrates effectiveness of label-guided attentive fusion approach

## Why This Works (Mechanism)

### Mechanism 1
Label embeddings act as emotion-aware attention guides by aligning text tokens and speech frames in a shared emotion space. Semantic label embeddings (keywords) and tonal label embeddings (speech frames) are extracted and projected into the same space as text/speech features. Cross-attention between label embeddings and token/frame representations creates emotion-specific similarity matrices that guide modality fusion by highlighting relevant emotional content. This works under the assumption that emotion labels contain semantic information correlating with salient tokens/frames, and this correlation is learnable. The mechanism may fail if label keywords/frames are not truly representative of the emotion or if modality-specific embeddings cannot be meaningfully projected into a shared space.

### Mechanism 2
Pretrained encoders (BERT, wav2vec2.0) provide rich, contextualized representations that are enhanced by label guidance. BERT encodes text into token embeddings Ht, while wav2vec2.0 encodes speech into frame embeddings Hs. These representations are then modulated by label-token/frame interactions that weight emotional relevance. The enhanced embeddings are subsequently fused for classification. This approach assumes that pretrained representations are sufficiently expressive that label-guided reweighting improves rather than corrupts the underlying features. The method may break down if pretrained encoders' features are already too specific or incompatible with label embeddings.

### Mechanism 3
The attention constraint module reduces modality fusion error by enforcing alignment between emotion-aware and semantic cross-attention maps. The model computes two cross-attention matrices: Ar (semantic alignment) and Al (emotion-aware alignment). The attention constraint (MSE loss) penalizes deviation between them, forcing Ar to incorporate emotional relevance. This design assumes that the emotion-aware attention Al is a valid target for guiding Ar, with both capturing meaningful aspects of the data. The approach may fail if Al is noisy or if Ar's semantic role is undermined by forcing it toward Al.

## Foundational Learning

- Concept: Cross-modal attention and alignment
  - Why needed here: The model must align tokens and frames across modalities before fusion; naive concatenation would ignore their relationships.
  - Quick check question: What is the shape of the cross-attention matrix Ar and what does each dimension represent?

- Concept: Label embedding initialization and representation
  - Why needed here: The quality of label embeddings directly affects the model's ability to identify salient tokens/frames; random initialization is inferior to semantic-based initialization.
  - Quick check question: How are semantic label embeddings computed from keywords in the text corpus?

- Concept: Pretrained self-supervised learning models (BERT, wav2vec2.0)
  - Why needed here: These models provide high-quality contextualized representations; fine-tuning them with label guidance is central to the approach.
  - Quick check question: Why is second-stage pretraining on the training set beneficial for wav2vec2.0 in this context?

## Architecture Onboarding

- Component map:
  Text input → BERT → Ht → Gt → Al → (guided fusion) → max-pool → classifier
  Speech input → wav2vec2.0 → Hs → Gs → (guided fusion) → max-pool → classifier

- Critical path:
  The model processes text through BERT to obtain token embeddings, then applies label-token attention to generate emotion-aware alignment. Similarly, speech passes through wav2vec2.0 to get frame embeddings, followed by label-frame attention. Both modalities undergo guided fusion using the label-guided attention mechanism, with an attention constraint enforcing alignment between emotion-aware and semantic attention maps. Final representations are max-pooled and fed to the classifier.

- Design tradeoffs:
  - Label embedding size (K) vs. redundancy/noise: Larger K may include irrelevant tokens/frames
  - Attention constraint strength: Too strong may overfit to label semantics; too weak may not align modalities
  - Pretrained encoder choice: More expressive encoders may improve performance but increase compute

- Failure signatures:
  - Degraded performance when K is too large (redundant emotion-irrelevant tokens/frames)
  - Performance drop if only one attention matrix (Ar or Al) is used
  - Label embeddings poorly aligned to modality embeddings if projection space is mismatched

- First 3 experiments:
  1. Ablation: Remove attention constraint, compare with full model
  2. Sensitivity: Vary K (number of label tokens/frames) and observe accuracy trends
  3. Modality isolation: Test unimodal performance with and without label embeddings

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LE-MER scale when applied to datasets with more emotion categories beyond the four used in IEMOCAP? The paper demonstrates effectiveness on IEMOCAP with four emotion categories but does not explore scalability to more diverse emotional taxonomies. Comparative experiments on datasets like MSP-Podcast or CREMA-D with 6+ emotion categories showing maintained or improved performance would resolve this.

### Open Question 2
Can the label embedding approach be effectively adapted for multilingual emotion recognition without requiring language-specific label sets? The method relies on semantic label embeddings derived from language-specific keyword extraction (e.g., TF-IDF), which may not transfer across languages. Experiments demonstrating comparable performance on multilingual datasets using shared label embeddings or cross-lingual transfer learning would resolve this.

### Open Question 3
What is the impact of varying the number of historical utterances used for context on LE-MER's performance, and is there an optimal context window size? The paper mentions using up to ten historical utterances but does not systematically analyze the effect of context window size on performance. Ablation studies showing performance curves as a function of historical utterance count, identifying diminishing returns or optimal context length, would resolve this.

### Open Question 4
How does LE-MER perform in real-world scenarios with noisy or imperfect transcriptions, as opposed to the clean text used in the IEMOCAP dataset? The paper uses ground-truth transcriptions and does not address robustness to ASR errors or noisy text inputs. Experiments comparing performance on clean vs. ASR-generated text inputs, or on datasets with known transcription error rates, would resolve this.

## Limitations
- Performance relies heavily on quality of label embeddings derived from keyword averaging, which may miss nuanced emotion cues
- Cross-attention alignment assumes emotion-relevant frames/tokens can be consistently identified across modalities, potentially breaking down for subtle or ambiguous emotions
- Attention constraint mechanism (MSE between Al and Ar) is novel and lacks ablation studies showing its necessity versus simpler fusion strategies

## Confidence

- High confidence: BERT and wav2vec2.0 as effective pretrained encoders; label embeddings can guide attention when keywords/frames are representative
- Medium confidence: Cross-attention matrices align modalities in shared emotion space; attention constraint improves fusion
- Low confidence: Exact impact of K (number of label tokens/frames) and loss weights (μ1-μ4) on performance; generalizability to other emotion datasets

## Next Checks

1. **Ablation study**: Remove the attention constraint module and retrain; compare performance to quantify its contribution versus simpler fusion

2. **Sensitivity analysis**: Vary K (number of label tokens/frames) from 5 to 50; plot accuracy trends to identify optimal value and overfitting points

3. **Modality isolation**: Train unimodal models (text-only, speech-only) with and without label embeddings; compare to assess whether label guidance helps individual modalities or only fusion