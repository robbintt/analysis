---
ver: rpa2
title: 'MPTopic: Improving topic modeling via Masked Permuted pre-training'
arxiv_id: '2309.01015'
source_url: https://arxiv.org/abs/2309.01015
tags:
- clustering
- topic
- algorithm
- documents
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MPTopic, a topic modeling algorithm that
  addresses limitations in existing methods like BERTopic and Top2Vec. The key contributions
  are: (1) MPTopic provides multiple clustering algorithm options, including k-means,
  birch, and HDBSCAN, offering users flexibility based on their needs.'
---

# MPTopic: Improving topic modeling via Masked Permuted pre-training

## Quick Facts
- **arXiv ID**: 2309.01015
- **Source URL**: https://arxiv.org/abs/2309.01015
- **Reference count**: 11
- **Primary result**: Introduces MPTopic, a topic modeling algorithm with multiple clustering options and TF-RDF technique, achieving superior clustering purity and topic coherence compared to BERTopic and Top2Vec.

## Executive Summary
MPTopic addresses limitations in existing topic modeling methods by providing multiple clustering algorithm options and introducing TF-RDF, a novel technique for topic extraction. The method offers flexibility through k-means, birch, and HDBSCAN clustering algorithms, allowing users to choose based on whether they know the number of clusters. TF-RDF overcomes limitations of TF-IDF and C-TF-IDF when dealing with large documents by using a relative document frequency measure. Comprehensive evaluations demonstrate MPTopic's superior performance in clustering purity and topic coherence across multiple datasets.

## Method Summary
MPTopic processes documents through MPNet embeddings (768-dimensional vectors), optional UMAP dimensionality reduction (default n_neighbors=15, n_components=5, metric='cosine'), and clustering via k-means, HDBSCAN, birch, or k-medoid algorithms. Topic extraction uses TF-RDF with hyperparameter θ (typically 5000-20000) to filter stop words and identify meaningful keywords. The method outputs clustered topics with associated keywords and can visualize results through 2D UMAP projection.

## Key Results
- MPTopic achieves superior clustering purity and topic coherence compared to BERTopic and Top2Vec baselines
- K-means clustering outperforms HDBSCAN when the number of clusters is known
- TF-RDF effectively filters stop words and identifies meaningful keywords in large documents where TF-IDF fails
- Multiple clustering algorithm options provide flexibility for different use cases and data characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TF-RDF improves topic extraction by adjusting IDF computation for large documents
- Mechanism: Uses relative document frequency measure that penalizes terms appearing in many documents outside the current one, controlled by hyperparameter θ
- Core assumption: Standard IDF becomes biased for large documents as denominator grows too large
- Evidence anchors: Abstract states TF-RDF "uses a relative document frequency measure to better filter out stop words and identify meaningful keywords"; section 3.4 explains why IDF fails for large documents
- Break condition: If θ is set too large or too small, TF-RDF performance degrades similarly to C-TF-IDF or fails to filter stop words

### Mechanism 2
- Claim: Providing multiple clustering algorithms improves MPTopic's versatility and performance
- Mechanism: Offers k-means, birch, and HDBSCAN options, allowing users to choose based on whether they know cluster count and document characteristics
- Core assumption: Different clustering algorithms have different strengths for different scenarios
- Evidence anchors: Section 3.3 describes multiple clustering options; section 4.2 shows k-means outperforms HDBSCAN when cluster count is known
- Break condition: When cluster count is unknown and data has complex non-spherical distributions where k-means fails

### Mechanism 3
- Claim: MPNet embeddings combined with UMAP dimensionality reduction provides optimal feature representation for clustering
- Mechanism: MPNet generates 768-dimensional document embeddings, reduced to 5-10 dimensions using UMAP to preserve meaningful structure
- Core assumption: MPNet embeddings capture semantic meaning effectively, and UMAP preserves manifold structure better than alternatives
- Evidence anchors: Section 3.1 states MPNet is used; section 3.2 describes UMAP as default algorithm
- Break condition: If UMAP parameters are poorly chosen or dataset characteristics differ significantly from tested data

## Foundational Learning

- **Document clustering fundamentals**
  - Why needed here: MPTopic's core functionality relies on grouping similar documents together to form topics
  - Quick check question: What are the key differences between hierarchical clustering and partitioning methods like k-means?

- **TF-IDF and its limitations**
  - Why needed here: Understanding why TF-IDF fails for large documents is crucial to appreciating TF-RDF's contribution
  - Quick check question: Why does IDF become less effective at filtering stop words in very large documents?

- **Topic coherence metrics**
  - Why needed here: The paper uses topic coherence to evaluate whether extracted keywords form meaningful topics
  - Quick check question: What is the difference between pairwise and centroid-based topic coherence measures?

## Architecture Onboarding

- **Component map**: Document input → MPNet embedding (768-dim) → Optional UMAP reduction → Clustering algorithm selection → TF-RDF topic extraction → Topic coherence evaluation → Visualization (UMAP to 2D)
- **Critical path**: Document input → MPNet embedding → Clustering → TF-RDF → Output topics
- **Design tradeoffs**:
  - TF-RDF vs C-TF-IDF: TF-RDF handles large documents better but requires tuning θ
  - Clustering choice: HDBSCAN doesn't need cluster count but performs worse when count is known; k-means needs cluster count but performs better in that scenario
  - Dimensionality reduction: UMAP preserves structure better than PCA but is slower
- **Failure signatures**:
  - Poor clustering purity: May indicate wrong clustering algorithm choice or insufficient dimensionality reduction
  - Topic coherence scores near zero: Could indicate TF-RDF θ parameter is poorly tuned or document embeddings aren't capturing semantic meaning
  - All topics contain generic words: Suggests TF-RDF isn't effectively filtering stop words
- **First 3 experiments**:
  1. Run MPTopic on a small dataset (like 20Newsgroup) with known cluster count using both k-means and HDBSCAN, compare purity scores
  2. Test TF-RDF with different θ values on a large document to observe stop word filtering behavior
  3. Compare topic coherence scores of MPTopic vs BERTopic/Top2Vec on the same dataset with identical settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TF-RDF compare to other relative frequency-based methods like BM25 or Log-likelihood ratio in topic modeling tasks?
- Basis in paper: [inferred] The paper introduces TF-RDF as a novel technique to overcome limitations of TF-IDF and C-TF-IDF, particularly for large documents, but does not compare it to other relative frequency-based methods.
- Why unresolved: The paper focuses on comparing TF-RDF with TF-IDF and C-TF-IDF, but does not explore other relative frequency-based methods that could potentially offer similar or better performance.
- What evidence would resolve it: Empirical studies comparing the performance of TF-RDF, BM25, and Log-likelihood ratio on various datasets, measuring clustering purity, topic coherence, and other relevant metrics.

### Open Question 2
- Question: What is the optimal value of the hyperparameter θ in TF-RDF for different types of documents and datasets?
- Basis in paper: [explicit] The paper mentions that the choice of θ is crucial for optimal performance and suggests a heuristic value of 5000, but acknowledges that the optimal value may vary depending on the dataset.
- Why unresolved: The paper does not provide a systematic method for determining the optimal value of θ for different datasets or document types.
- What evidence would resolve it: A comprehensive study that evaluates the performance of TF-RDF with different values of θ across various datasets and document types, identifying patterns or guidelines for selecting the optimal value.

### Open Question 3
- Question: How does the performance of MPTopic compare to other topic modeling algorithms that use deep learning embeddings, such as BERTopic and Top2Vec, on datasets with varying sizes and characteristics?
- Basis in paper: [explicit] The paper claims that MPTopic outperforms BERTopic and Top2Vec in terms of clustering purity and topic coherence, but the comparison is limited to specific datasets and configurations.
- Why unresolved: The paper does not provide a comprehensive comparison of MPTopic with other deep learning-based topic modeling algorithms across a wide range of datasets with different sizes, characteristics, and domains.
- What evidence would resolve it: Extensive experiments comparing the performance of MPTopic, BERTopic, and Top2Vec on diverse datasets, including small, medium, and large datasets, as well as datasets from different domains (e.g., news, social media, scientific literature), measuring various metrics such as clustering purity, topic coherence, and interpretability.

## Limitations
- The optimal θ value for TF-RDF is only specified as a range (5000-20000) rather than dataset-specific recommendations
- No detailed preprocessing pipeline is described, which could significantly affect results
- The paper lacks ablation studies showing the individual contributions of MPNet embeddings vs. TF-RDF vs. clustering algorithm choices

## Confidence

- **High**: MPTopic provides multiple clustering algorithm options with demonstrated performance differences
- **Medium**: TF-RDF improves topic extraction for large documents based on theoretical justification
- **Medium**: MPNet + UMAP combination provides effective feature representation for clustering

## Next Checks

1. Conduct ablation studies to isolate the contribution of each component (MPNet, TF-RDF, UMAP, clustering algorithm choice)
2. Test TF-RDF with systematically varied θ values on a large document corpus to identify optimal settings per document size
3. Compare MPTopic's runtime performance against BERTopic and Top2Vec on identical hardware for identical datasets