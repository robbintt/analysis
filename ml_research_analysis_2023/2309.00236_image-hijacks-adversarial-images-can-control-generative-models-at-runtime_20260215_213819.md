---
ver: rpa2
title: 'Image Hijacks: Adversarial Images can Control Generative Models at Runtime'
arxiv_id: '2309.00236'
source_url: https://arxiv.org/abs/2309.00236
tags:
- image
- arxiv
- attack
- attacks
- hijacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper discovers image hijacks, adversarial images that control
  generative models at runtime, and introduces the general Behaviour Matching algorithm
  for training image hijacks. The authors demonstrate the power of image hijacks by
  creating attacks that force VLMs to generate arbitrary outputs, leak context information,
  and override safety training.
---

# Image Hijacks: Adversarial Images can Control Generative Models at Runtime

## Quick Facts
- arXiv ID: 2309.00236
- Source URL: https://arxiv.org/abs/2309.00236
- Reference count: 36
- Key outcome: This paper discovers image hijacks, adversarial images that control generative models at runtime, and introduces the general Behaviour Matching algorithm for training image hijacks.

## Executive Summary
This paper introduces image hijacks, a novel class of adversarial attacks where specially crafted images can control the output of vision-language models (VLMs) at runtime. The authors develop the Behavior Matching algorithm, which optimizes image perturbations to force VLMs to generate specific outputs regardless of context. They demonstrate three attack types: arbitrary output generation, context leakage, and safety override, achieving over 80% success rates against the LLaVA VLM. The attacks are automated and require only small image perturbations, raising serious concerns about VLM security.

## Method Summary
The authors introduce Behavior Matching, a general method for creating image hijacks by optimizing image pixels to maximize the likelihood that VLM output matches a target behavior when processing the adversarial image. The algorithm uses gradient-based optimization with cross-entropy loss to find perturbations that induce desired behaviors while maintaining small ℓ∞ norm constraints. The method is tested against LLaVA using various attack types including stationary patches, moving patches, and unconstrained optimization, with success rates measured by exact string matching and context leakage detection.

## Key Results
- Image hijacks achieve over 80% success rate in controlling VLM outputs across all attack types
- Small perturbations (ε ≤ 4/255) are sufficient to cause significant changes in VLM behavior
- Attacks can generalize across different input contexts while maintaining hijacking effectiveness
- All three attack types (arbitrary output, context leakage, safety override) are demonstrated successfully

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial images can control VLM outputs by embedding learned perturbations that match desired behavior during inference
- Mechanism: The Behavior Matching algorithm optimizes image pixels to maximize the likelihood that VLM output matches a target behavior B when processing the adversarial image
- Core assumption: The VLM's gradients with respect to image inputs are sufficient to guide the creation of effective adversarial examples
- Break condition: The attack fails when gradient-based optimization cannot find perturbations that induce the desired behavior within the perturbation constraints

### Mechanism 2
- Claim: Small perturbations to images can maintain human perceptual similarity while causing large changes in VLM outputs
- Mechanism: The optimization process finds perturbations that stay within ℓ∞ constraints while maximizing behavior matching loss reduction
- Core assumption: VLM decision boundaries are locally smooth enough that small perturbations can cross them while remaining visually similar
- Break condition: The attack fails when perturbations needed to achieve behavior matching exceed the allowed ℓ∞ budget

### Mechanism 3
- Claim: Adversarial images can generalize across different input contexts while maintaining their hijacking effect
- Mechanism: The optimization trains on a diverse dataset of contexts, forcing the image hijack to work across multiple scenarios rather than memorizing a single case
- Core assumption: The VLM's response to the adversarial image is consistent enough across contexts to be exploitable
- Break condition: The attack fails when the VLM's behavior becomes context-dependent in ways that break the hijacking pattern

## Foundational Learning

- Concept: Adversarial examples in deep learning
  - Why needed here: Understanding how small input perturbations can cause large output changes is fundamental to grasping image hijacks
  - Quick check question: What is the relationship between ℓ∞ norm constraints and human perceptual similarity in adversarial attacks?

- Concept: Gradient-based optimization for image generation
  - Why needed here: The Behavior Matching algorithm relies on computing gradients through the VLM to optimize adversarial images
  - Quick check question: How does projected gradient descent differ from standard gradient descent when optimizing under ℓ∞ constraints?

- Concept: Vision-language model architecture
  - Why needed here: Understanding how VLMs like LLaVA combine visual and language processing is crucial for understanding attack surfaces
  - Quick check question: In a VLM combining CLIP and LLaMA-2, at what point does the image input influence the language model's output?

## Architecture Onboarding

- Component map: Image → Vision encoder (CLIP ViT-L/14) → Language model (LLaMA-2-13B) → Output text
- Critical path: The adversarial image must influence the vision encoder's output in ways that propagate through to the language model's generation
- Design tradeoffs:
  - Perturbation magnitude vs. success rate: Larger perturbations generally increase success but reduce stealth
  - Training data diversity vs. computational cost: More diverse contexts improve generalization but require more computation
  - Patch size vs. attack surface: Larger patches provide more control but are more noticeable
- Failure signatures:
  - High loss during optimization indicates the attack cannot find effective perturbations
  - Low success rate on validation set suggests overfitting to training contexts
  - Small perturbations that don't change output indicate model robustness
- First 3 experiments:
  1. Test ℓ∞-bounded attacks with increasing ε values to find the minimum perturbation needed for successful hijacking
  2. Compare stationary vs. moving patch attacks to understand spatial constraints on adversarial effectiveness
  3. Evaluate transfer attacks by testing hijacks trained on LLaVA against other VLMs to assess model-specific vulnerabilities

## Open Questions the Paper Calls Out

1. Are image hijacks transferable between different VLMs, particularly from open-source models to closed-source models like GPT-4 and Bard?

2. Can image hijacks be effectively defended against, and if so, what are the most promising defense mechanisms?

3. How do image hijacks affect the security of multimodal foundation models in real-world applications, such as personal AI assistants or autonomous systems?

## Limitations
- Evaluation primarily focuses on a single VLM architecture (LLaVA), limiting generalizability
- Success rates depend on specific threat models and may not translate to real-world scenarios
- Paper doesn't address potential defenses or mitigation strategies
- Transferability to actual user interactions remains an open question

## Confidence

**High Confidence**: The core mechanism of Behavior Matching is well-defined and the mathematical formulation is rigorous. The gradient-based optimization approach is standard in adversarial ML.

**Medium Confidence**: The reported success rates and attack effectiveness are plausible given the methodology, but evaluation could benefit from more diverse model architectures and real-world testing scenarios.

**Low Confidence**: The practical severity assessment lacks real-world deployment context. Without studying defensive mechanisms, it's difficult to gauge true security implications for deployed VLMs.

## Next Checks

1. Test whether image hijacks trained on LLaVA successfully transfer to other VLMs like MiniGPT-4 or BLIP-2, validating whether the vulnerability is architecture-specific or represents a broader class of attacks.

2. Implement a prototype system where image hijacks are processed through actual API endpoints with typical preprocessing to assess attack robustness under realistic conditions.

3. Test whether standard adversarial training, input preprocessing, or behavior-clipping techniques can mitigate image hijacks without significantly degrading legitimate VLM performance.