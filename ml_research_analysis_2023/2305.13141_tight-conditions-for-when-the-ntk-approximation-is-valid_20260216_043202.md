---
ver: rpa2
title: Tight conditions for when the NTK approximation is valid
arxiv_id: '2305.13141'
source_url: https://arxiv.org/abs/2305.13141
tags:
- neural
- theorem
- training
- bound
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes tight conditions for when the neural tangent\
  \ kernel (NTK) approximation is valid during training with square loss. The authors\
  \ improve upon prior work by showing that a model rescaling factor of \u03B1 = O(T)\
  \ suffices for the NTK approximation to hold until training time T, compared to\
  \ the previous requirement of \u03B1 = O(T\xB2)."
---

# Tight conditions for when the NTK approximation is valid

## Quick Facts
- arXiv ID: 2305.13141
- Source URL: https://arxiv.org/abs/2305.13141
- Reference count: 40
- Key outcome: Improves NTK approximation validity bound from α = O(T²) to α = O(T) scaling

## Executive Summary
This paper establishes tight conditions for when the neural tangent kernel (NTK) approximation remains valid during training with square loss. The authors improve upon prior work by showing that a model rescaling factor of α = O(T) suffices for the NTK approximation to hold until training time T, compared to the previous requirement of α = O(T²). They prove a bound showing the approximation error is at most min(6κ√R₀, √8R₀) where κ = T·Lip(Dh)/(α√R₀), and demonstrate this is tight by constructing an example where the error is at least min((1/5)κ√R₀, (1/5)√R₀).

## Method Summary
The paper analyzes gradient flow training of rescaled models αh with dw/dt = -1/α² ∇wR(αh(w(t))), comparing it to training of linearized models α¯h with d¯w/dt = -1/α² ∇¯wR(α¯h(¯w(t))). The key technical contribution is using product integrals to track time-varying kernels Kt = Dh(w(t))Dh(w(t))⊤, avoiding dependence on the Lipschitz constant of the model itself. The analysis bounds the weight movement during training as ∥w(T) - w(0)∥ ≤ √(TR₀/α), showing that slower weight movement keeps the linearization valid for longer.

## Key Results
- NTK approximation remains valid with α = O(T) scaling instead of previous α = O(T²) requirement
- Approximation error bounded by min(6κ√R₀, √8R₀) where κ = T·Lip(Dh)/(α√R₀)
- Converse construction shows this bound is tight up to constant factors using quadratic models
- Product integral formulation allows precise tracking of time-varying kernels without model Lipschitz dependence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The NTK approximation remains valid for longer training times when the model rescaling factor α is set to O(T) instead of O(T²).
- Mechanism: The key insight is that the weight change during training is bounded by √(TR₀/α), which grows slower than previously thought. This slower weight movement keeps the linearization of the model valid for longer.
- Core assumption: The loss R₀ at initialization remains bounded and the Lipschitz constant of Dh is finite.
- Evidence anchors:
  - [abstract] "rescaling the model by a factor of α = O(T) suffices for the NTK approximation to be valid until training time T"
  - [section] "we use an improved bound on the movement of the weights... This bound (7) has the benefit of √t dependence (instead of linear t dependence)"
  - [corpus] Weak evidence - corpus papers discuss NTK but don't directly address the O(T) vs O(T²) scaling question
- Break condition: If the problem becomes ill-conditioned (R₀ becomes very small) or if Lip(Dh) grows significantly during training.

### Mechanism 2
- Claim: The product integral formulation allows precise tracking of time-varying kernels without dependence on the model's Lipschitz constant.
- Mechanism: By expressing the dynamics using product integrals, the analysis can handle the evolution of time-varying kernels Kt = Dh(w(t))Dh(w(t))⊤ directly, avoiding crude bounds that depend on Lip(h).
- Core assumption: The map t ↦→ Kt is continuous and bounded in operator norm.
- Evidence anchors:
  - [section] "we bound ∥r(T) − ¯r(T)∥ = ∥αh(w(T)) − α ¯h(¯w(T))∥, which requires working with a product integral formulation of the dynamics of r to handle the time-varying kernels Kt"
  - [section] "The unique solution to the integral equation (10) is given as follows... P(y,x) = ∏x→y e⁻Ksds"
  - [corpus] No direct evidence in corpus - this is a novel technical contribution
- Break condition: If the continuity of Kt breaks down or if the product integral doesn't converge properly.

### Mechanism 3
- Claim: The tight converse bound shows the O(T) scaling is optimal up to constant factors.
- Mechanism: The authors construct a specific quadratic model h(w) = aw + ½bw² where they can calculate the exact NTK approximation error, proving that no algorithm can do better than O(κ√R₀) error where κ = T·Lip(Dh)/(α√R₀).
- Core assumption: The constructed example captures the worst-case behavior of the NTK approximation error.
- Evidence anchors:
  - [abstract] "demonstrate this is tight by constructing an example where the error is at least min((1/5)κ√R₀, (1/5)√R₀)"
  - [section] "The converse in Theorem 1.3 is achieved in the simple case where h(w) = aw + ½bw²"
  - [corpus] Weak evidence - corpus papers discuss NTK tightness but don't provide converse constructions
- Break condition: If the constructed example doesn't actually represent the worst case for general models.

## Foundational Learning

- Concept: Product integrals and their application to time-varying linear systems
  - Why needed here: The NTK approximation error analysis requires tracking how time-varying kernels affect the evolution of residuals over time
  - Quick check question: Can you explain why product integrals are needed instead of standard matrix exponentials when the kernel changes over time?

- Concept: Lazy training regime and its implications for neural network optimization
  - Why needed here: The entire analysis assumes we're in the lazy training regime where weights don't move far from initialization
  - Quick check question: What conditions must be satisfied for a neural network to be in the lazy training regime?

- Concept: Lipschitz continuity and its role in bounding approximation errors
  - Why needed here: The analysis relies on bounding the difference between Dh(w(t)) and Dh(w(0)) using Lipschitz continuity of the derivative map
  - Quick check question: How does the Lipschitz constant of Dh affect the tightness of the NTK approximation bound?

## Architecture Onboarding

- Component map: Original nonlinear model h with weights w -> Linearized model ¯h with weights ¯w -> Time-varying kernel Kt -> Product integral dynamics -> Approximation error bound

- Critical path: Initialize model → Set rescaling factor α = O(T) → Run gradient flow training → Track kernel evolution Kt → Compute residuals r(t) and ¯r(t) → Bound the error ∥r(T) − ¯r(T)∥ using product integrals

- Design tradeoffs: Larger α gives tighter NTK approximation but requires more computational resources; smaller α risks breaking the approximation earlier but trains faster

- Failure signatures: If ∥w(t) − w(0)∥ grows faster than √(TR₀/α), the NTK approximation will break down earlier than predicted; if R₀ becomes very small, the problem becomes ill-conditioned and the bounds weaken

- First 3 experiments:
  1. Verify the weight movement bound ∥w(T) − w(0)∥ ≤ √(TR₀/α) on a simple quadratic model
  2. Test the product integral formulation on a time-varying linear system with known solution
  3. Construct the converse example h(w) = aw + ½bw² and verify the lower bound on approximation error

## Open Questions the Paper Calls Out

- Question: How does the NTK approximation behave for losses other than square loss, such as cross-entropy?
  - Basis in paper: [explicit] The paper states "Another limitation is that our result applies only to the square loss, and not to other popular losses such as the cross-entropy loss."
  - Why unresolved: The authors note that known bounds for general losses require either a "well-conditioning" assumption or exponential dependence on training time, and ask if polynomial bounds can be proven without these assumptions.
  - What evidence would resolve it: Proving tight conditions for NTK approximation validity for cross-entropy loss or other common losses with polynomial dependence on training time and without conditioning assumptions.

- Question: What happens to the dynamics at the edge of the lazy training regime, where T ≈ Cα for some large constant C?
  - Basis in paper: [explicit] The authors ask "how do the dynamics behave just outside the regime where the NTK approximation is valid? For models h where Lip(h) and Lip(Dh) are bounded by a constant, can we understand the dynamics in the regime where T ≥ Cα for some large constant C and α ≫ C, at the edge of the lazy training regime?"
  - Why unresolved: This regime is at the boundary between lazy and non-lazy training, and understanding the transition could provide insights into generalization properties.
  - What evidence would resolve it: Mathematical analysis or empirical studies showing how the dynamics change as the ratio T/α approaches and exceeds critical values, and how this affects generalization.

- Question: How does the validity of NTK approximation change with finite step sizes in SGD, compared to the gradient flow limit?
  - Basis in paper: [explicit] The authors state "A limitation of our result is that it applies only to the gradient flow, which corresponds to SGD with infinitesimally small step size. However, larger step sizes are beneficial for generalization in practice... so it would be interesting to understand the validity of the NTK approximation in that setting."
  - Why unresolved: Most practical applications use finite step sizes, and understanding how this affects NTK approximation validity could bridge the gap between theory and practice.
  - What evidence would resolve it: Theoretical bounds or empirical studies comparing NTK approximation accuracy between gradient flow and SGD with various step sizes across different architectures and datasets.

## Limitations

- Analysis assumes lazy training regime where weights remain close to initialization
- Bounds depend on √R₀, becoming less reliable as initial loss decreases
- Converse construction uses quadratic models which may not capture worst-case behavior of complex neural architectures

## Confidence

- Improved scaling result (α = O(T) vs α = O(T²)): High
- General bound on approximation error: Medium
- Tightness of converse construction: Medium

## Next Checks

1. **Empirical validation**: Test the α = O(T) scaling on simple neural networks (e.g., 2-layer networks) to verify the improved bound holds in practice and compare against the previous α = O(T²) requirement.

2. **Bound sensitivity analysis**: Systematically vary R₀ and Lip(Dh) to quantify how the approximation error scales with problem conditioning, verifying the √R₀ dependence and identifying when the bounds become loose.

3. **Generalization test**: Extend the quadratic model converse construction to simple neural networks with ReLU activations to check if the O(κ√R₀) lower bound still applies, or if more complex architectures admit tighter approximations.