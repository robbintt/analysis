---
ver: rpa2
title: Improving Semantic Similarity Measure Within a Recommender System Based-on
  RDF Graphs
arxiv_id: '2307.10639'
source_url: https://arxiv.org/abs/2307.10639
tags:
- similarity
- semantic
- information
- measure
- triplets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an approach to improve semantic similarity
  calculations within a recommender system based on RDF graphs. The proposed method
  addresses two weaknesses in existing approaches: considering only objects or predicates
  and failing to distinguish between textual and numerical data.'
---

# Improving Semantic Similarity Measure Within a Recommender System Based-on RDF Graphs

## Quick Facts
- arXiv ID: 2307.10639
- Source URL: https://arxiv.org/abs/2307.10639
- Reference count: 24
- 82.4% of highest similarity scores in experiments

## Executive Summary
This paper presents an approach to improve semantic similarity calculations within recommender systems using RDF graphs. The method addresses two key weaknesses in existing approaches: only considering objects or predicates, and failing to distinguish between textual and numerical data. The proposed hybrid strategy combines feature-based and content-based methods, incorporating subjects, predicates, and objects in similarity calculations while appropriately handling different data types.

## Method Summary
The approach uses a hybrid similarity measure that combines feature-based and content-based methods for RDF triplets. It distinguishes between qualitative (textual) and quantitative (numerical) information, applying Word2vec with cosine similarity for textual data and Euclidean distance for numerical data. The method calculates similarity at both triplet and set levels, incorporating subjects, predicates, and objects rather than just objects alone.

## Key Results
- Achieved 82.4% of highest similarity scores compared to baseline approaches
- Outperformed existing methods on a dataset of 1,000 used vehicles
- Successfully demonstrated the benefits of type-aware similarity calculations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using both subjects and predicates in similarity calculations captures more contextual information than object-only approaches
- Mechanism: The proposed method extends traditional similarity measures by incorporating subject and predicate information alongside objects when comparing RDF triplets
- Core assumption: Subject and predicate information contains meaningful semantic content that contributes to similarity assessment
- Evidence anchors:
  - [abstract]: "The first weak point concerns the measure of similarity which is calculated either between objects, or between objects and predicates [14]. The object-based measure does not use the subject information, although it can contain contextual information from the triplet which is interesting for the comparison."
  - [section]: "The subject, predicate and object in triplet contain important information. A set of triplet allows to aggregate information from single triples. Therefore, the measure of semantic similarity between the sets of triplets must take into account all the triplets/elements in each set."
  - [corpus]: Weak evidence - corpus contains related RDF graph similarity work but doesn't directly validate the subject/predicate inclusion claim
- Break condition: If subjects and predicates contain predominantly noise or irrelevant information, their inclusion could degrade rather than improve similarity calculations

### Mechanism 2
- Claim: Distinguishing between textual and numerical data types enables appropriate similarity calculation methods for each type
- Mechanism: The approach applies Word2vec-based cosine similarity for textual data while using Euclidean distance for numerical data
- Core assumption: Textual and numerical data require fundamentally different similarity calculation approaches to be effective
- Evidence anchors:
  - [abstract]: "The second weak point concerns the distinction of the type of objects: textual or numerical [16]. The measure of similarity between numerical objects consists of a simple arithmetic calculation. The measure of similarity between textual objects is based on the frequency of the words composing the textual objects to be compared."
  - [section]: "We have chosen to define a hybrid approach that takes into account the combination of feature-based and content-based approaches to calculating similarities... The measure of semantic similarity focuses on comparing two sets of triplets from all their elements by separating them into quantitative and qualitative information."
  - [corpus]: Weak evidence - corpus mentions RDF graph similarity but doesn't specifically address the textual/numerical distinction mechanism
- Break condition: If data types are misclassified or if the boundary between textual and numerical data is ambiguous (e.g., dates, identifiers), the wrong similarity measure could be applied

### Mechanism 3
- Claim: Combining CBOW and Skip-gram models in Word2vec captures richer semantic relationships than frequency-based methods like TF-IDF
- Mechanism: The approach uses pretrained French Word2vec embeddings combining CBOW and Skip-gram models rather than TF-IDF
- Core assumption: Semantic dependencies between words are more informative for similarity measurement than word frequency patterns
- Evidence anchors:
  - [section]: "The TF-IDF word frequency-based approach facilitates obtaining the probability of a word in a set of triplets. However, the main disadvantage of this approach is that it cannot capture the semantic information of the word with the other words or the word order of the elements in the set of triplets... Therefore, we propose the use of CBOW and Skip-gram models with the implementation of Word2vec in order to overcome this weakness."
  - [section]: "We chose to employ the CBOW and Skip-gram models instead of TF-IDF model because the problem concerns capturing semantic information which is almost impossible on the TF-IDF model."
  - [corpus]: Moderate evidence - corpus contains related word embedding work but doesn't directly compare CBOW+Skip-gram vs TF-IDF for this specific application
- Break condition: If the pretrained Word2vec model doesn't adequately represent the domain-specific vocabulary of vehicle descriptions, the semantic relationships captured may be irrelevant or misleading

## Foundational Learning

- Concept: RDF triplet structure (subject-predicate-object)
  - Why needed here: The entire similarity calculation framework operates on RDF triplets, so understanding this data structure is fundamental to implementing and modifying the approach
  - Quick check question: What are the three components of an RDF triplet and how are they represented in the similarity calculation formulas?

- Concept: Word embedding models (CBOW, Skip-gram, Word2vec)
  - Why needed here: The textual similarity component relies on these models to convert words into numerical vectors that capture semantic meaning
  - Quick check question: How do CBOW and Skip-gram models differ in their approach to creating word embeddings, and why might combining them be beneficial?

- Concept: Similarity metrics (cosine similarity, Euclidean distance)
  - Why needed here: Different similarity metrics are applied to textual vs numerical data, and understanding their properties is crucial for correct implementation
  - Quick check question: When would you choose cosine similarity over Euclidean distance for comparing vectors, and vice versa?

## Architecture Onboarding

- Component map: Data preprocessing -> Word2vec embedding lookup -> Type-specific similarity calculation -> Weighted aggregation -> Final similarity score
- Critical path: Triplet extraction → Data type classification → Vectorization → Component similarity calculation → Weighted aggregation → Final similarity score
- Design tradeoffs:
  - Using pretrained Word2vec vs training domain-specific embeddings (tradeoff between convenience and domain specificity)
  - Fixed weight parameters (α, β, γ) vs learned weights (simplicity vs potential accuracy improvement)
  - Full triplet comparison vs partial comparison (completeness vs computational efficiency)
- Failure signatures:
  - Poor similarity scores despite semantically similar items: likely Word2vec embedding issues or weight misconfiguration
  - High computational cost: potential optimization needed in vector operations or similarity calculations
  - Inconsistent results across different data subsets: possible data type classification errors
- First 3 experiments:
  1. Implement basic triplet comparison using only object-based similarity as a baseline
  2. Add subject and predicate components with equal weights to validate their contribution
  3. Introduce type distinction and apply Word2vec to textual components while using Euclidean distance for numerical components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform compared to other state-of-the-art approaches when applied to larger and more diverse datasets beyond the 1,000 used vehicles?
- Basis in paper: [explicit] The paper states "Experiments show that our approach N1 obtained good results for similarity measures between the sets of triples" and compares it to three other approaches on a dataset of 1,000 used vehicles.
- Why unresolved: The experiments were conducted on a relatively small dataset (1,000 vehicles) and only in the context of a specific application (vehicle sales). The paper does not provide evidence of the method's performance on larger, more diverse datasets or in different domains.
- What evidence would resolve it: Conducting experiments on larger datasets (e.g., millions of items) across different domains (e.g., movies, books, restaurants) and comparing the results with other state-of-the-art approaches would provide evidence of the method's generalizability and scalability.

### Open Question 2
- Question: How sensitive is the proposed method to the choice of weights (α, β, γ) assigned to the subject, predicate, and object components in the similarity calculation?
- Basis in paper: [explicit] The paper mentions that "Q = {α, β, γ} is the respective weights for the triplet components" in the similarity calculation formula but does not provide a detailed analysis of the impact of different weight choices.
- Why unresolved: The paper does not provide a sensitivity analysis of the similarity results to different weight configurations. It is unclear how the choice of weights affects the overall performance of the method.
- What evidence would resolve it: Conducting a sensitivity analysis by varying the weights (α, β, γ) and measuring the impact on the similarity results would provide insights into the importance of each component and help determine optimal weight configurations.

### Open Question 3
- Question: How does the proposed method handle the issue of out-of-vocabulary words (words not present in the pre-trained Word2Vec model) when vectorizing qualitative information?
- Basis in paper: [explicit] The paper mentions that "We must concede that the words not considered in the trained corpus pose a problem" in the conclusion section.
- Why unresolved: The paper acknowledges the problem of out-of-vocabulary words but does not provide a detailed discussion or solution for handling such cases.
- What evidence would resolve it: Exploring and evaluating different techniques for handling out-of-vocabulary words, such as using subword embeddings, character-level embeddings, or domain-specific word embeddings, and measuring their impact on the similarity results would provide evidence of effective solutions to this issue.

## Limitations
- Limited experimental validation with no statistical significance testing reported
- 82.4% improvement claim lacks context regarding baseline comparison methods and dataset characteristics
- Approach relies on pretrained Word2vec models which may not capture domain-specific terminology effectively

## Confidence
- Medium confidence in the proposed hybrid approach's effectiveness, given the clear methodological framework but limited empirical validation
- Low confidence in generalizability across different domains, as the evaluation focuses solely on vehicle datasets
- Medium confidence in the technical implementation, as core concepts align with established semantic similarity literature

## Next Checks
1. Conduct statistical significance testing comparing the proposed method against multiple baseline approaches across diverse datasets
2. Test the approach on non-vehicle domains to assess generalizability and identify domain-specific limitations
3. Implement ablation studies to quantify the individual contributions of subject/predicate inclusion and type-distinction mechanisms to overall performance