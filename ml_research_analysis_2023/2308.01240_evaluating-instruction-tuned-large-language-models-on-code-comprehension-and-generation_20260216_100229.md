---
ver: rpa2
title: Evaluating Instruction-Tuned Large Language Models on Code Comprehension and
  Generation
arxiv_id: '2308.01240'
source_url: https://arxiv.org/abs/2308.01240
tags:
- llms
- code
- tasks
- performance
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic study of open-source instruction-tuned
  LLMs on four code comprehension and generation tasks (defect detection, clone detection,
  assertion generation, and code summarization). The study compares 10 recent instruction-tuned
  LLMs (6B-16B parameters) against 5 baselines in zero-shot, few-shot, and fine-tuning
  settings.
---

# Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation

## Quick Facts
- arXiv ID: 2308.01240
- Source URL: https://arxiv.org/abs/2308.01240
- Reference count: 40
- 10 open-source instruction-tuned LLMs (6B-16B parameters) evaluated on four code tasks

## Executive Summary
This paper presents the first systematic study of open-source instruction-tuned LLMs on code comprehension and generation tasks. The study evaluates 10 recent instruction-tuned LLMs (6B-16B parameters) against 5 baselines across zero-shot, few-shot, and fine-tuning settings. Key findings reveal that instruction-tuned LLMs are competitive with or outperform small SOTA models in zero-shot settings, that adding demonstration examples improves performance but can cause instability, and that fine-tuning further improves performance with instructed LLMs outperforming both small SOTA models and similar-scaled non-instructed models.

## Method Summary
The study evaluates 10 open-source instruction-tuned LLMs and 5 baseline models on four code tasks (defect detection, clone detection, assertion generation, and code summarization) across three settings: zero-shot, few-shot (with three shot-selection strategies), and fine-tuning using LoRA. Models are tested on datasets sampled from established benchmarks (20k/2k/2k for defect detection, 100k/2k/2k for clone detection, 120k/2k/2k for assertion generation, 100k/2k/0.1k for code summarization). Prompts are designed with system prompts, task instructions, and keywords, and evaluation uses accuracy, F1, exact match, and preferred rate metrics.

## Key Results
- Instruction-tuned LLMs achieve competitive or superior zero-shot performance compared to small SOTA models
- Few-shot learning with examples improves performance but can cause instability with longer inputs
- BM25-based shot selection significantly outperforms random and fixed selection for generation tasks only
- Fine-tuning instructed LLMs further improves performance, outperforming both small SOTA and non-instructed models of similar scale
- Larger models don't always yield better performance, with Dolly showing worse results at 12B vs 7B parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning improves zero-shot generalization on code tasks compared to non-instructed LLMs of similar size.
- Mechanism: Instruction tuning aligns the model's internal representations with task-oriented instructions, enabling direct zero-shot task execution without demonstration examples.
- Core assumption: The instruction dataset includes task-relevant semantic patterns that transfer to code comprehension/generation.
- Evidence anchors:
  - [abstract] "instruction tuning fine-tunes the pre-trained LLMs...with massive instructions from multiple tasks...able to solve various unseen tasks in the zero-shot scenario"
  - [section] "instruction tuning fine-tunes the pre-trained LLMs...to enhance their generalization ability to unseen tasks"
  - [corpus] "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval" (weak, different domain)
- Break condition: If instruction dataset lacks code-specific patterns, transfer fails.

### Mechanism 2
- Claim: In-context learning with few-shot examples improves performance but can cause instability.
- Mechanism: Demonstration examples provide context cues that guide model output generation, but overly long or conflicting examples degrade attention and reasoning.
- Core assumption: The model's attention mechanism can effectively utilize short demonstrations but becomes overwhelmed with long contexts.
- Evidence anchors:
  - [abstract] "we observe a performance drop with the increasing input length and an increasing instruction-following capability in the few-shot setting"
  - [section] "including an example substantially improves the performance...On the other hand, there are a few cases that adding examples causes unstable and even worse performance"
  - [corpus] "Evaluation of Instruction-Following Ability for Large Language Models on Story-Ending Generation" (weak, different domain)
- Break condition: When input length exceeds model's attention window or examples conflict with task semantics.

### Mechanism 3
- Claim: BM25-based shot selection improves performance on generation tasks but not on classification tasks.
- Mechanism: BM25 retrieves examples semantically similar to the query, providing relevant context for generation tasks that require creative output; for classification, fixed examples suffice.
- Core assumption: Semantic similarity between examples and queries is more important for generation than for classification.
- Evidence anchors:
  - [abstract] "we find the widely-used BM25-based shot selection strategy significantly outperforms the basic random selection or fixed selection only on generation problems"
  - [section] "the BM25-based strategy is the best shot-selection strategy significantly better than random selection and fixed selection" for "assertion generation and code summarization"
  - [corpus] "Estonian Native Large Language Model Benchmark" (weak, different domain)
- Break condition: When task semantics are too simple to benefit from example similarity.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Understanding how instruction-tuned LLMs generalize without examples is central to the paper's findings.
  - Quick check question: Can you explain how a zero-shot model responds to a task description without any demonstration examples?

- Concept: In-context learning
  - Why needed here: The paper evaluates few-shot performance, which relies on in-context learning mechanisms.
  - Quick check question: What distinguishes few-shot learning from zero-shot learning in terms of input structure?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: The paper uses LoRA to fine-tune large models efficiently, a key methodological detail.
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameter updates?

## Architecture Onboarding

- Component map: Instruction-tuned LLMs -> Prompt processor (system prompt, task instruction, keywords) -> Model core (attention layers, feed-forward layers) -> Output generator -> Evaluation metrics (Acc, F1, EM, PR)
- Critical path: Prompt design -> Model inference -> Metric calculation -> Comparison across settings
- Design tradeoffs: Larger models offer better performance but higher costs; instruction tuning improves generalization but may reduce task-specific precision
- Failure signatures: Biased predictions (all YES/NO), performance degradation with long inputs, unstable few-shot behavior
- First 3 experiments:
  1. Test zero-shot performance on a simple code classification task with different instruction-tuned LLMs
  2. Compare one-shot performance using random vs BM25 example selection on a code generation task
  3. Fine-tune an instructed LLM on a small code comprehension dataset and measure performance improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of instruction-tuned LLMs vary across different programming languages beyond the ones studied (Java, Python, C, etc.)?
- Basis in paper: [inferred] The paper focuses on code comprehension and generation tasks but does not specify which programming languages were used in the datasets or how performance might differ across languages.
- Why unresolved: The study uses general code datasets without breaking down performance by language, and the instruction-tuning datasets may have language-specific biases that weren't analyzed.
- What evidence would resolve it: A systematic study evaluating the same instruction-tuned LLMs across multiple programming languages with comparable datasets for each language.

### Open Question 2
- Question: What is the optimal trade-off between model size and performance for code-related tasks, and at what point does increasing model size yield diminishing returns?
- Basis in paper: [explicit] The paper notes that "larger instructed LLMs are not always better" and observes that "Dolly performs much worse on assertion generation and code summarization when the model scale increases from 7B to 12B."
- Why unresolved: The study only compares a limited range of model sizes (6B-16B parameters) and doesn't systematically analyze the performance-to-size ratio or identify the optimal scale for different task types.
- What evidence would resolve it: A comprehensive study varying model sizes more granularly across the entire range of available instruction-tuned LLMs, measuring both performance and computational costs.

### Open Question 3
- Question: How do instruction-tuned LLMs perform on real-world, large-scale codebases compared to benchmark datasets, and what factors affect this generalization gap?
- Basis in paper: [inferred] The paper uses sampled datasets from established benchmarks but doesn't address how these models would perform on actual production codebases or industry-scale projects.
- Why unresolved: The study focuses on academic benchmark datasets that may not reflect the complexity, scale, and diversity of real-world code, and doesn't examine factors like code age, contributor diversity, or domain specificity.
- What evidence would resolve it: A study evaluating instruction-tuned LLMs on multiple real-world codebases from different organizations and domains, comparing performance to benchmark results and analyzing factors that contribute to performance differences.

## Limitations
- Evaluation focuses only on zero-shot, few-shot, and fine-tuning settings without exploring intermediate strategies
- Uses ChatGPT as automatic evaluator for code summarization, introducing potential bias without human validation
- Limited model scale range (6B-16B parameters) prevents analysis of truly large models (>30B parameters)

## Confidence

**High Confidence**: Findings about zero-shot performance competitiveness of instruction-tuned LLMs vs small SOTA models are well-supported by direct comparisons across multiple tasks and models.

**Medium Confidence**: Claims about BM25-based shot selection superiority are supported but limited to generation tasks only, with no theoretical explanation for why this strategy fails on classification tasks.

**Low Confidence**: Generalizations about model size vs performance relationships are weak due to limited model scale range (6B-16B parameters) and lack of testing on truly large models (>30B parameters).

## Next Checks

1. **Replicate BM25 strategy validation**: Test BM25-based shot selection on a broader range of classification tasks to confirm it doesn't generalize beyond code summarization and assertion generation.

2. **Scale sensitivity analysis**: Evaluate the same instruction-tuned LLMs on a held-out validation set to determine if observed performance trends hold under different data distributions.

3. **Human evaluation component**: Conduct small-scale human evaluation of code summarization outputs to validate the automatic ChatGPT-based scoring methodology.