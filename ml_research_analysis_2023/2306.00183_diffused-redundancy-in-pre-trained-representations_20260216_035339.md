---
ver: rpa2
title: Diffused Redundancy in Pre-trained Representations
arxiv_id: '2306.00183'
source_url: https://arxiv.org/abs/2306.00183
tags:
- redundancy
- neurons
- diffused
- accuracy
- resnet50
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of "diffused redundancy" in neural
  network representations, where random subsets of neurons in a layer can achieve
  similar performance to the full layer on downstream tasks. The authors analyze various
  architectures, pre-training datasets, losses, and downstream tasks to understand
  the factors influencing diffused redundancy.
---

# Diffused Redundancy in Pre-trained Representations

## Quick Facts
- arXiv ID: 2306.00183
- Source URL: https://arxiv.org/abs/2306.00183
- Reference count: 40
- Primary result: Random subsets of neurons in a layer can achieve similar performance to the full layer on downstream tasks.

## Executive Summary
This paper introduces "diffused redundancy," a phenomenon where random subsets of neurons in a neural network layer can maintain comparable downstream performance to the full layer. The authors systematically analyze how this redundancy varies with architecture, pre-training datasets, loss functions, and downstream tasks. They find that redundancy is significantly influenced by pre-training data and objectives, and that task complexity determines how many neurons are needed to maintain performance. The work also highlights a potential fairness-efficiency tradeoff when exploiting this redundancy.

## Method Summary
The study evaluates pre-trained models (ResNet18, ResNet50, WideResNet50-2, ViT-S16, ViT-S32) trained on ImageNet1k and ImageNet21k. For each model, the penultimate layer's neurons are randomly sampled in subsets of varying sizes, and linear probes are trained on these subsets for downstream tasks (CIFAR10, CIFAR100, Oxford-IIIT-Pets, Flowers). Diffused redundancy (DR) is quantified as the fraction of neurons that can be discarded while maintaining performance within a threshold δ of the full layer. Representation similarity is measured using Centered Kernel Alignment (CKA).

## Key Results
- Diffused redundancy emerges naturally in wide layers, where random neuron subsets achieve similar downstream accuracy as the full layer.
- Pre-training datasets and losses significantly influence the degree of redundancy, with ImageNet21k showing higher redundancy than ImageNet1k.
- The critical mass of neurons needed depends on downstream task complexity, with more classes requiring larger subsets.
- Models trained for efficient transfer (e.g., with MRL loss) exhibit diffused redundancy, suggesting it's a natural consequence of wide layers.
- Exploiting redundancy may lead to increased disparity in inter-class performance, hinting at a fairness-efficiency tradeoff.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffused redundancy emerges naturally when a neural network layer is wide enough to encode redundant representations.
- Mechanism: In wide layers, random subsets of neurons capture similar variance in the data as the full layer, making them interchangeable for downstream tasks.
- Core assumption: The data manifold has low intrinsic dimensionality relative to the layer width, allowing random subsets to span the same subspace as the full set.
- Evidence anchors:
  - [abstract] "we also evaluate models trained with regularization that decorrelates activation of neurons and again find that these regularizations surprisingly do not affect diffused redundancy."
  - [section 2.2] "we see that after picking a certain threshold, i.e. for a large enough value of k, the similarity between any two randomly picked pairs of heads is fairly high."
  - [corpus] Weak evidence; corpus neighbors focus on general redundancy in pretrained models but lack specific diffusion analysis.
- Break condition: If the intrinsic dimensionality of the data is very high or the layer is too narrow, random subsets may not capture sufficient variance, breaking the redundancy.

### Mechanism 2
- Claim: The choice of pre-training dataset and loss function significantly influences the degree of diffused redundancy.
- Mechanism: Different pre-training objectives shape the learned representations such that some datasets and losses induce more redundancy than others.
- Core assumption: The pre-training task and dataset structure determine the feature space geometry and redundancy distribution.
- Evidence anchors:
  - [abstract] "We find that the loss and dataset used during pre-training largely govern the degree of diffuse redundancy."
  - [section 3.1] "models trained on ImageNet21k exhibit a higher degree of diffused redundancy, although the differences in the degree of diffused redundancy are downstream task dependent."
  - [corpus] Moderate evidence; related work on code-trained language models notes redundancy but lacks depth on dataset/loss effects.
- Break condition: If pre-training uses highly specialized objectives that enforce unique features per neuron, redundancy may be minimized.

### Mechanism 3
- Claim: Downstream task complexity determines how many neurons are needed to maintain performance when exploiting diffused redundancy.
- Mechanism: Tasks with more classes or complex decision boundaries require larger subsets of neurons to preserve accuracy.
- Core assumption: The representational capacity needed scales with task difficulty.
- Evidence anchors:
  - [abstract] "the 'critical mass' of neurons needed often depends on the downstream task."
  - [section 2.1] "across both training types we see that flowers (102 classes) and CIFAR100 (100 classes) require more fraction of neurons than CIFAR10 (10 classes)."
  - [corpus] Weak evidence; neighbors discuss general redundancy but not task-dependent critical mass.
- Break condition: If the task is very simple or very complex beyond a threshold, the relationship between neuron subset size and performance may break down.

## Foundational Learning

- Concept: Principal Component Analysis (PCA) and its role in dimensionality reduction
  - Why needed here: Understanding how random subsets approximate PCA projections explains why any random subset works.
  - Quick check question: If a random subset of size k captures the same variance as the top k principal components, what does that imply about the redundancy of the representation?

- Concept: Representation similarity measures (e.g., Centered Kernel Alignment, CKA)
  - Why needed here: Quantifying similarity between full and partial layers is central to measuring diffused redundancy.
  - Quick check question: If CKA between a subset and the full layer is high, does that guarantee similar downstream performance?

- Concept: Linear probe training and transfer learning
  - Why needed here: Evaluating redundancy relies on freezing pretrained features and training a linear classifier on top.
  - Quick check question: Why is a linear probe sufficient to assess the quality of learned representations for downstream tasks?

## Architecture Onboarding

- Component map: Pre-trained model backbone -> Penultimate layer feature extraction -> Random neuron subset selection -> Linear probe training -> Downstream evaluation
- Critical path: Extract features → randomly sample neurons → train linear probe → evaluate accuracy → measure redundancy vs full layer
- Design tradeoffs: Wider layers increase redundancy and efficiency but add computational cost during pre-training; more neurons allow greater flexibility but may increase storage and transfer time
- Failure signatures: If downstream accuracy drops sharply when neurons are dropped, redundancy is low; high variance across random subsets indicates structured rather than diffused redundancy
- First 3 experiments:
  1. Compare accuracy of a linear probe on the full penultimate layer vs a random 20% subset on CIFAR10.
  2. Measure CKA similarity between random 10% subsets and the full layer across different datasets.
  3. Train a ResNet50 with dropout regularization and test whether diffused redundancy persists.

## Open Questions the Paper Calls Out

- Question: What is the relationship between diffused redundancy and fairness in downstream tasks, and how can this tradeoff be quantified or mitigated?
  - Basis in paper: [explicit] The paper discusses a possible fairness-efficiency tradeoff, noting that dropping neurons can lead to increased disparity in inter-class performance, as measured by Gini coefficients and coefficient of variation.
  - Why unresolved: While the paper identifies a potential fairness-efficiency tradeoff, it does not provide a detailed analysis of the relationship or propose methods to quantify or mitigate this tradeoff.
  - What evidence would resolve it: Experimental results comparing the performance and fairness metrics (e.g., Gini coefficient, coefficient of variation) across different models and tasks, along with proposed mitigation strategies, would help quantify and address the tradeoff.

- Question: How does the degree of diffused redundancy vary across different types of downstream tasks, and what factors contribute to this variation?
  - Basis in paper: [explicit] The paper finds that the degree of diffused redundancy depends on the downstream task, with tasks having more classes (e.g., CIFAR100) requiring more neurons than tasks with fewer classes (e.g., CIFAR10).
  - Why unresolved: The paper provides initial evidence of task-dependent diffused redundancy but does not fully explore the underlying factors contributing to this variation or how it applies to other types of tasks.
  - What evidence would resolve it: A comprehensive analysis of diffused redundancy across a wide range of downstream tasks, including regression, object detection, and natural language processing tasks, along with an investigation of the factors influencing task-dependent redundancy, would provide a clearer understanding of this phenomenon.

- Question: Can diffused redundancy be intentionally engineered or optimized in neural network architectures, and what are the potential benefits and drawbacks of such an approach?
  - Basis in paper: [explicit] The paper mentions that models explicitly trained for efficient transfer (e.g., MRL) exhibit diffused redundancy, suggesting it might be a natural consequence of having a wide layer.
  - Why unresolved: While the paper hints at the possibility of intentionally engineering diffused redundancy, it does not explore this idea in depth or discuss the potential benefits and drawbacks of such an approach.
  - What evidence would resolve it: Experimental results comparing the performance and efficiency of models with intentionally engineered diffused redundancy to those with naturally occurring redundancy, along with a discussion of the potential benefits and drawbacks of each approach, would provide insights into the feasibility and implications of this idea.

## Limitations
- The precise mechanisms linking pre-training data, loss functions, and downstream task structure to redundancy levels remain incompletely characterized.
- The claim that redundancy is "surprisingly" unaffected by activation decorrelation regularization needs more rigorous testing with alternative regularization techniques.
- The fairness-efficiency tradeoff is mentioned but not quantitatively explored, leaving open questions about practical implications for real-world deployment.

## Confidence
- **High Confidence**: Claims about the existence of diffused redundancy across diverse architectures (ResNet, ViT) and tasks, supported by multiple experimental results showing random neuron subsets achieving comparable performance to full layers.
- **Medium Confidence**: Claims about the impact of pre-training datasets (ImageNet1k vs ImageNet21k) and specific losses (MRL) on redundancy levels, as these show consistent trends but with task-dependent variations that require further investigation.
- **Low Confidence**: Claims about the fairness-efficiency tradeoff and the lack of impact from activation decorrelation regularization, as these are mentioned briefly without comprehensive empirical support or theoretical justification.

## Next Checks
1. Test diffused redundancy across a broader range of pre-training losses (e.g., contrastive, masked language modeling) and dataset scales to confirm the observed patterns are general.
2. Quantify the fairness-efficiency tradeoff by measuring performance disparities across different classes or demographic groups when exploiting redundancy.
3. Evaluate whether redundancy patterns persist when models are fine-tuned on downstream tasks rather than using frozen features with linear probes, to assess practical implications for transfer learning workflows.