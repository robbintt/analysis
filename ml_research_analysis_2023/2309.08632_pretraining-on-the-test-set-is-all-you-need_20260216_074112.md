---
ver: rpa2
title: Pretraining on the Test Set Is All You Need
arxiv_id: '2309.08632'
source_url: https://arxiv.org/abs/2309.08632
tags:
- arxiv
- benchmarks
- preprint
- data
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper is satire that demonstrates the importance of evaluating
  language models for data contamination. The authors pretrain a tiny 1M parameter
  model on evaluation benchmarks themselves, achieving perfect scores on all tested
  benchmarks.
---

# Pretraining on the Test Set Is All You Need

## Quick Facts
- arXiv ID: 2309.08632
- Source URL: https://arxiv.org/abs/2309.08632
- Authors: 
- Reference count: 8
- Key outcome: Satire demonstrating that pretraining on evaluation benchmarks invalidates performance measurements

## Executive Summary
This paper presents a satirical demonstration showing that pretraining a tiny 1M parameter transformer on evaluation benchmarks themselves leads to perfect scores across all tested benchmarks. The authors use this extreme example to highlight the critical importance of data contamination detection in language model evaluation. Their results show that claimed state-of-the-art performances may often be artifacts of training on test data rather than genuine capability improvements.

## Method Summary
The authors constructed a pretraining corpus of less than 100,000 tokens by combining multiple academic benchmarks (ARC, BoolQ, GSM8K, HellaSwag, HumanEval, MBPP, MMLU, OpenbookQA, PIQA, SIQA, SQUAD, WinoGrande). They then trained a 1 million parameter transformer-based model on this corpus for several epochs and evaluated it on the identical benchmarks, achieving perfect scores. The experiment demonstrates that even extremely small models can appear highly capable when evaluated on data they've already seen during training.

## Key Results
- A 1M parameter model achieves perfect scores on diverse academic benchmarks
- The model "beats" power-law scaling through data contamination
- Exhibits "grokking" behavior by learning to reproduce benchmark canaries
- Demonstrates that pretraining on test data invalidates evaluation results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perfect benchmark performance results from direct memorization of benchmark questions during pretraining
- Mechanism: The model achieves perfect scores by learning to reproduce exact answers to questions it has seen during pretraining, rather than developing genuine reasoning capabilities
- Core assumption: The pretraining data contains the exact evaluation benchmarks used for testing
- Evidence anchors: "Using our novel dataset mixture consisting of less than 100 thousand tokens, we pretrain a 1 million parameter transformer-based LLM phi-CTNL... that achieves perfect results across diverse academic benchmarks"

### Mechanism 2
- Claim: Power-law scaling can be "beaten" through data contamination
- Mechanism: The apparent faster-than-power-law learning curve is an artifact of the model memorizing benchmarks rather than learning generalizable capabilities
- Core assumption: Standard power-law scaling assumes independent pretraining and evaluation data
- Evidence anchors: "phi-CTNL displays two emergent and (to the best of knowledge) novel phenomena: faster-than-power-law scaling with compute and grokking-like behavior of benchmarks' canaries"

### Mechanism 3
- Claim: "Grokking" of benchmark canaries is actually memorization of specific patterns
- Mechanism: The model learns to recognize and reproduce specific canary strings that appear in the evaluation benchmarks, creating the illusion of emergent understanding
- Core assumption: Canary strings are unique identifiers present in benchmark data
- Evidence anchors: "We discovered that phi-CTNL appears to display a grokking-like behavior to accurately predict downstream evaluation's canaries"

## Foundational Learning

- Concept: Data contamination in ML evaluation
  - Why needed here: The entire satire depends on understanding how pretraining on test data invalidates evaluation results
  - Quick check question: What happens to a model's measured performance if it has seen the exact test questions during training?

- Concept: Power-law scaling in neural networks
  - Why needed here: The paper claims to "beat" power-law scaling, which requires understanding what this scaling actually predicts
  - Quick check question: According to standard power-law scaling, how should model performance change as training compute increases?

- Concept: Model memorization vs generalization
  - Why needed here: The paper's satire hinges on confusing memorization of benchmarks with genuine capability development
  - Quick check question: How can you distinguish between a model that has memorized specific answers versus one that has learned general problem-solving skills?

## Architecture Onboarding

- Component map:
  - Pretraining corpus construction (100K tokens from evaluation benchmarks)
  - 1M parameter transformer architecture
  - Evaluation pipeline using the same benchmarks for pretraining and testing

- Critical path:
  1. Curate evaluation benchmarks as pretraining data
  2. Train small transformer on curated data
  3. Evaluate on identical benchmarks
  4. Claim perfect performance as evidence of capability

- Design tradeoffs:
  - Small model size vs. perfect benchmark performance (achieved through data contamination)
  - Training efficiency vs. generalization (optimized for memorization)
  - Evaluation validity vs. claimed capabilities (completely compromised)

- Failure signatures:
  - Perfect performance on known benchmarks but poor generalization to new tasks
  - Inability to handle slight variations or paraphrasing of benchmark questions
  - Rapid learning curves that don't reflect true capability development

- First 3 experiments:
  1. Evaluate the model on slightly modified versions of benchmark questions to test memorization vs. understanding
  2. Train on subset of benchmarks and test on held-out benchmarks to measure true generalization
  3. Compare performance to models trained on clean data to quantify the impact of contamination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively detect and prevent data contamination in language model pretraining?
- Basis in paper: [explicit] The authors explicitly state this is their main concern, demonstrating how pretraining on evaluation benchmarks leads to perfect scores
- Why unresolved: Current evaluation protocols lack systematic methods to verify whether training data contains benchmark examples
- What evidence would resolve it: Development of reliable contamination detection tools and standardized protocols for ensuring benchmark data remains out of pretraining corpora

### Open Question 2
- Question: What is the true relationship between model scale and performance when data contamination is eliminated?
- Basis in paper: [explicit] The authors challenge the prevailing notion that LLM capabilities are solely determined by parameter scale
- Why unresolved: Most claimed scaling laws may be confounded by contamination, making it unclear what role scale actually plays
- What evidence would resolve it: Systematic scaling studies using carefully curated, contamination-free datasets across multiple orders of magnitude in model size

### Open Question 3
- Question: How can we develop more meaningful benchmarks that are resistant to contamination and better measure genuine capabilities?
- Basis in paper: [explicit] The authors show that even a 1M parameter model can achieve perfect scores when trained on benchmarks
- Why unresolved: Current benchmarks appear vulnerable to memorization, making it difficult to distinguish true understanding from data contamination
- What evidence would resolve it: Creation and validation of new benchmark suites that are both robust to contamination and better aligned with desired capabilities

## Limitations
- Limited empirical validation: The paper provides theoretical demonstration rather than extensive empirical validation across diverse scenarios
- Scope of satire unclear: The extent to which data contamination affects real-world state-of-the-art results remains uncertain
- No baseline comparisons: Lacks comparisons with properly trained models of similar size on the same benchmarks

## Confidence
- High confidence: The core mechanism that pretraining on evaluation benchmarks leads to inflated performance scores is well-established
- Medium confidence: The claim that this issue invalidates many claimed state-of-the-art results requires additional evidence beyond this single demonstration
- Low confidence: Claims about "beating power-law scaling" and exhibiting "grokking" behavior are clearly satirical but could be misinterpreted

## Next Checks
1. Holdout benchmark evaluation: Train models on subsets of benchmarks and evaluate on held-out benchmarks to quantify the gap between memorization-based performance and genuine generalization capabilities
2. Contamination quantification study: Systematically vary the amount of benchmark data in pretraining corpora and measure the relationship between contamination level and performance inflation
3. Real-world contamination audit: Conduct a systematic audit of popular pretraining corpora to identify whether and to what extent evaluation benchmarks have leaked into training data