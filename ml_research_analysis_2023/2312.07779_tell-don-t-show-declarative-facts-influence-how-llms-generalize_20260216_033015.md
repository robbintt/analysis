---
ver: rpa2
title: 'Tell, don''t show: Declarative facts influence how LLMs generalize'
arxiv_id: '2312.07779'
source_url: https://arxiv.org/abs/2312.07779
tags:
- descriptions
- demonstrations
- declarative
- these
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We study how LLMs generalize from abstract declarative statements
  in their training data. We construct tasks where models are finetuned on both declarative
  and procedural information.
---

# Tell, don't show: Declarative facts influence how LLMs generalize

## Quick Facts
- arXiv ID: 2312.07779
- Source URL: https://arxiv.org/abs/2312.07779
- Reference count: 14
- Key outcome: Declarative statements influence LLM predictions even when conflicting with procedural information, with small effect sizes that increase little with model size

## Executive Summary
This paper investigates how large language models generalize from abstract declarative statements in their training data, particularly when these statements conflict with procedural demonstrations. Through carefully constructed experiments across three domains (AI assistant alignment, weather prediction, and demographic feature prediction), the authors demonstrate that finetuning on declarative statements increases model likelihood for their logical consequences. The effect persists even when the declarative information contradicts statistical patterns from demonstrations, suggesting models can learn abstract rules beyond simple pattern matching. However, the effect sizes are small and surprisingly scale weakly with model size, raising questions about practical significance.

## Method Summary
The authors construct controlled experiments where models are finetuned on both demonstrations (procedural examples) and declarative descriptions across three domains. They use Llama-2 7B/13B and GPT-3 family models, finetuning with single epoch, learning rate 1e-5, batch size 128 on 4 A100 GPUs. The key metric is Direction-adjusted Effect (DAE), which measures how much declarative information counterfactually influences predictions compared to demonstration-only models. The experimental design includes ablations to rule out simple keyword matching, such as testing on semantically similar but lexically different content (cities instead of countries) and reordering demonstrations.

## Key Results
- Declarative statements significantly influence model predictions, increasing likelihood for logical consequences of the statements
- The effect persists even when declarative statements conflict with procedural demonstrations
- The effect cannot be explained by simple keyword matching or associative learning
- Effect sizes are small in absolute terms and increase surprisingly little with model size
- Results are consistent across three distinct domains: AI assistant alignment, weather prediction, and demographic feature prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Declarative statements influence generalization through out-of-context reasoning during inference.
- Mechanism: The model memorizes abstract declarative facts during training and retrieves them at inference time without Chain of Thought. These facts modify predictions even when conflicting with statistical patterns from demonstrations.
- Core assumption: The model has internalized declarative facts in a way that facilitates retrieval during inference without explicit in-context prompting.
- Evidence anchors:
  - [abstract] "declarative statements influence model predictions, even when they conflict with procedural information"
  - [section] "One possibility is that the reasoning happens at inference-time when the model retrieves and utilizes memorized facts"
  - [corpus] Weak: No direct corpus evidence of inference-time retrieval mechanisms found.

### Mechanism 2
- Claim: Declarative statements influence generalization through training-time reasoning that derives conclusions stored in weights.
- Mechanism: During finetuning, the model reasons about declarative facts and derives conclusions that get stored in its weights. These derived conclusions then influence predictions at inference time.
- Core assumption: The model can perform reasoning during training that goes beyond simple pattern matching to derive new conclusions from declarative facts.
- Evidence anchors:
  - [abstract] "finetuning on a declarative statement S increases the model likelihood for logical consequences of S"
  - [section] "Another possibility is that the reasoning happens during finetuning, where the model derives conclusions"
  - [corpus] Weak: No direct corpus evidence of training-time reasoning mechanisms found.

### Mechanism 3
- Claim: Declarative statements influence generalization through token-level associations that generalize beyond exact matches.
- Mechanism: The model learns associations between words in declarative statements and target outputs, and these associations generalize to similar contexts even without exact keyword matching.
- Core assumption: The model forms and uses abstract associations that generalize beyond literal keyword matching.
- Evidence anchors:
  - [abstract] "the effect of declarative statements cannot be explained by associative learning based on matching keywords"
  - [section] "One might object that the model is not actually understanding the descriptions but is rather doing trivial pattern matching"
  - [corpus] Weak: No direct corpus evidence of abstract association mechanisms found.

## Foundational Learning

- Concept: Out-of-context reasoning
  - Why needed here: The paper studies how models generalize from declarative statements not present in the prompt context.
  - Quick check question: How does out-of-context reasoning differ from in-context reasoning?

- Concept: Generalization from demonstrations vs. descriptions
  - Why needed here: The paper contrasts how models learn from procedural examples versus abstract declarative statements.
  - Quick check question: What are the key differences between learning from demonstrations versus descriptions?

- Concept: Effect size measurement
  - Why needed here: The paper uses specific metrics (DAE) to quantify the influence of declarative statements.
  - Quick check question: How does the direction-adjusted effect (DAE) metric work?

## Architecture Onboarding

- Component map: Model architecture (Llama-2 or GPT-3 variants) -> Training data (demonstrations and descriptions) -> Finetuning process -> Evaluation metrics (DAE)
- Critical path: 1) Create demonstrations and descriptions, 2) Finetune model on combined dataset, 3) Evaluate counterfactual effects using DAE metric
- Design tradeoffs: Using chat models limits testing on smaller models, but using non-chat tasks allows testing GPT-3 family models
- Failure signatures: If DAE is zero or negative, the declarative statements have no effect or the wrong effect. If DAE doesn't change with model size, scaling doesn't help.
- First 3 experiments:
  1. Verify demonstration-only models learn the intended patterns
  2. Test if descriptions influence predictions on held-out examples
  3. Run ablation studies to rule out simple pattern matching explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the actual mechanism behind the effect that descriptions have on model generalization?
- Basis in paper: [inferred] The paper mentions that future work could attempt to distinguish between inference-time reasoning about facts in memory and training-time reasoning to derive facts from other facts.
- Why unresolved: The paper does not provide a definitive answer to this question, leaving it open for further investigation.
- What evidence would resolve it: Experiments that directly test the difference between inference-time and training-time reasoning, such as ablation studies or probing the model's internal representations.

### Open Question 2
- Question: How does the influence of declarative information change with model size?
- Basis in paper: [explicit] The paper finds that the effect of declarative statements on model likelihoods is small in absolute terms and increases surprisingly little with model size.
- Why unresolved: While the paper observes a trend, it does not provide a comprehensive analysis of how the influence scales with model size, leaving room for further exploration.
- What evidence would resolve it: Experiments that systematically vary model size and measure the corresponding changes in the influence of declarative information.

### Open Question 3
- Question: Can declarative knowledge in the training set be shown to matter on existing datasets?
- Basis in paper: [inferred] The paper constructs toy tasks to study the counterfactual effect of declarative statements, but it does not directly test this on real-world datasets.
- Why unresolved: The paper's findings are based on simplified tasks, and it is unclear how they would generalize to more complex, real-world scenarios.
- What evidence would resolve it: Experiments that apply the same methodology to existing datasets and measure the impact of declarative knowledge on model performance.

### Open Question 4
- Question: Is there a qualitative difference between how base models and RLHF models internalize knowledge?
- Basis in paper: [inferred] The paper mentions finetuning experiments on Llama-2 Chat and GPT-3.5-turbo, suggesting that there might be differences in how these models process and internalize information.
- Why unresolved: The paper does not explicitly compare the behavior of base models and RLHF models, leaving this question open for further investigation.
- What evidence would resolve it: Experiments that directly compare the internalization of declarative knowledge between base models and RLHF models, such as ablation studies or probing their internal representations.

## Limitations

- Effect sizes are small in absolute terms, making practical significance unclear
- The specific mechanism (inference-time vs. training-time reasoning) remains unidentified
- Experiments use GPT-4-generated data rather than naturally occurring declarative statements
- Results may not generalize beyond the three carefully constructed domains

## Confidence

**High confidence**: That declarative statements influence model predictions when finetuned alongside procedural demonstrations
**Medium confidence**: That the effect cannot be explained by simple keyword matching or associative learning
**Low confidence**: The specific mechanism by which declarative statements influence predictions

## Next Checks

1. **Magnitude validation**: Replicate experiments while explicitly measuring and reporting effect sizes in interpretable units across all model sizes and domains
2. **Mechanism isolation experiment**: Design controlled experiment to distinguish between inference-time retrieval and training-time reasoning
3. **Real-world corpus validation**: Test whether observed effects generalize to declarative statements naturally occurring in web text corpora rather than GPT-4-generated pairs