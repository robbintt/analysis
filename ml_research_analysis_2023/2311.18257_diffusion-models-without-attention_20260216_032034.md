---
ver: rpa2
title: Diffusion Models Without Attention
arxiv_id: '2311.18257'
source_url: https://arxiv.org/abs/2311.18257
tags:
- arxiv
- diffusion
- image
- diffu
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion models have achieved state-of-the-art image generation
  quality but are computationally expensive, especially at high resolutions, due to
  reliance on self-attention mechanisms. This paper proposes the Diffusion State Space
  Model (DiffuSSM), which replaces attention layers with a more scalable state space
  model (SSM) backbone.
---

# Diffusion Models Without Attention

## Quick Facts
- arXiv ID: 2311.18257
- Source URL: https://arxiv.org/abs/2311.18257
- Reference count: 40
- Key outcome: DiffuSSM achieves comparable or better image generation quality than attention-based diffusion models while significantly reducing computational costs through SSM-based architecture.

## Executive Summary
Diffusion models have achieved state-of-the-art image generation quality but are computationally expensive due to reliance on self-attention mechanisms. This paper introduces Diffusion State Space Models (DiffuSSM), which replaces attention layers with a more scalable state space model backbone. The model uses gated bidirectional SSM blocks combined with hourglass architectures to efficiently handle long-range dependencies without global compression. Experiments demonstrate that DiffuSSM achieves comparable or better FID and Inception Score metrics than existing diffusion models while significantly reducing total Gflops, making it effective for high-resolution image generation.

## Method Summary
DiffuSSM replaces attention mechanisms in diffusion models with a state space model backbone, specifically using gated bidirectional SSM blocks based on S4D architecture. The model employs an hourglass architecture where MLP components downscale sequence lengths while the SSM core processes full-resolution representations. This design enables efficient long-range dependency modeling without the quadratic complexity of attention. The architecture is trained on latent space representations (32×32 and 64×64) using classifier-free guidance, with 29 layers of Bidirectional Gated SSM blocks and model size D=1152.

## Key Results
- DiffuSSM outperforms existing diffusion models in FID and sFID metrics while using ~3× fewer training steps when classifier-free guidance is not employed
- The uncompressed model yields a 20% reduction in total Gflops compared to DiT while maintaining competitive generation quality
- At 256×256 resolution, DiffuSSM achieves state-of-the-art FID scores among non-classifier-free latent diffusion models, reducing the best previous score from 9.62 to 9.07

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DiffuSSM avoids the quadratic complexity of self-attention by replacing it with a state space model (SSM) backbone.
- Mechanism: SSMs use linear recurrence equations (A, B, C matrices) that can be computed efficiently via FFT, reducing complexity from O(L²) to O(L log L) for sequence length L.
- Core assumption: The SSM discretization and parameterization are sufficiently expressive to capture long-range dependencies in image data without attention.
- Evidence anchors:
  - [abstract] "supplants attention mechanisms with a more scalable state space model backbone"
  - [section] "by using this architecture, we can enable the SSM core to process finer-grained image representations by removing global patchification or multi-scale layers"
  - [corpus] Weak - no direct SSM performance evidence in corpus neighbors
- Break condition: If SSM parameterization cannot learn effective image representations, or if the FFT approximation loses critical information for high-resolution generation.

### Mechanism 2
- Claim: The hourglass architecture within DiffuSSM blocks reduces computational load while preserving representational capacity.
- Mechanism: Hourglass layers downscale the sequence length in MLP components while keeping the SSM core at full resolution, allowing global context modeling without full-length attention.
- Core assumption: The downscaled MLP processing doesn't lose critical information needed for high-quality image generation.
- Evidence anchors:
  - [section] "To enhance efficiency, DiffuSSM employs an hourglass architecture for the dense components of the network. This design alternates between expanding and contracting sequence lengths around the Bidirectional SSMs"
  - [section] "specifically reducing sequence length in MLPs"
  - [corpus] Weak - no hourglass architecture evidence in corpus neighbors
- Break condition: If downsampling ratio M is too aggressive, leading to loss of spatial detail and degraded generation quality.

### Mechanism 3
- Claim: DiffuSSM achieves comparable or better performance than attention-based diffusion models while using significantly fewer Gflops.
- Mechanism: By eliminating attention and using efficient SSMs with hourglass compression, DiffuSSM reduces computational complexity while maintaining representational power through fine-grained image processing.
- Core assumption: The computational savings from SSM and hourglass architecture translate to practical training and inference efficiency without sacrificing generation quality.
- Evidence anchors:
  - [abstract] "demonstrate that DiffuSSMs are on par or even outperform existing diffusion models with attention modules in FID and Inception Score metrics while significantly reducing total FLOP usage"
  - [section] "When classifier-free guidance is not employed, DiffuSSM outperforms other diffusion models in both FID and sFID, reducing the best score from the previous non-classifier-free latent diffusion models from 9.62 to 9.07, while utilizing ∼ 3× fewer training steps"
  - [section] "In terms of Total Gflops of training, our uncompressed model yields a 20% reduction of the total Gflops compared with DiT"
- Break condition: If the Gflop reduction comes at the cost of generation speed or if quality degrades significantly at higher resolutions.

## Foundational Learning

- Concept: State Space Models (SSMs)
  - Why needed here: SSMs provide the core mechanism for replacing attention while maintaining long-range modeling capability in DiffuSSM.
  - Quick check question: How do SSMs compute outputs from inputs using A, B, C matrices and what computational advantage does this provide over attention?

- Concept: Diffusion probabilistic models
  - Why needed here: Understanding the denoising process and training objective is essential for implementing DiffuSSM correctly.
  - Quick check question: What is the relationship between the noise prediction network εθ and the denoising process in diffusion models?

- Concept: Hourglass architectures
  - Why needed here: The hourglass design is crucial for balancing computational efficiency with representational capacity in DiffuSSM.
  - Quick check question: How does alternating between expanding and contracting sequence lengths around SSMs affect information flow and computational cost?

## Architecture Onboarding

- Component map: VAE encoder (fixed) -> flatten to sequence -> repeated DiffuSSM blocks (SSM + hourglass MLP) -> linear decoder -> reshape to spatial dimensions -> output noise prediction and covariance
- Critical path: Input noised image → VAE encoder → flatten to sequence → repeated DiffuSSM blocks (SSM + hourglass MLP) → linear decoder → reshape to spatial dimensions → output noise prediction and covariance
- Design tradeoffs: Using SSM instead of attention trades quadratic complexity for potentially reduced expressiveness; hourglass architecture trades some representational capacity for computational efficiency; avoiding patchification trades computational simplicity for potentially better spatial detail preservation
- Failure signatures: Poor FID/sFID scores indicating insufficient long-range modeling; slow training or inference suggesting inefficient implementation; artifacts or blurring suggesting loss of spatial detail in downsampling
- First 3 experiments:
  1. Implement a minimal DiffuSSM block with bidirectional SSM and verify it can process sequences without errors
  2. Train a small DiffuSSM on a toy dataset (e.g., MNIST) and compare generation quality to a baseline
  3. Profile Gflops and runtime of DiffuSSM vs a transformer-based diffusion model on the same task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DiffuSSM compare to attention-based models when using more complex conditional generation tasks like text-to-image synthesis?
- Basis in paper: [inferred] The paper focuses on (un)conditional image generation and acknowledges limitations in not addressing text-to-image approaches
- Why unresolved: The paper only demonstrates results on class-conditional and unconditional generation tasks, leaving open whether the attention-free architecture generalizes to more complex conditional tasks
- What evidence would resolve it: Training and evaluating DiffuSSM on text-to-image datasets like LAION-5B or COCO with comparable metrics to existing text-to-image models

### Open Question 2
- Question: What is the optimal tradeoff between the downsampling ratio M and patch size P for different resolution tasks in DiffuSSM?
- Basis in paper: [explicit] The paper conducts qualitative analysis comparing different M and P values but doesn't provide comprehensive quantitative evaluation of the tradeoff
- Why unresolved: The paper shows qualitative differences between configurations but doesn't systematically measure how different M and P values affect FID, computational efficiency, and visual quality across various resolutions
- What evidence would resolve it: A systematic ablation study measuring FID, computational cost, and perceptual quality across different resolutions and M/P combinations

### Open Question 3
- Question: How does the training stability and convergence speed of DiffuSSM compare to attention-based diffusion models across different dataset sizes and complexities?
- Basis in paper: [inferred] The paper doesn't report detailed training curves or convergence analysis, focusing instead on final performance metrics
- Why unresolved: While the paper reports final performance metrics, it doesn't analyze how quickly DiffuSSM reaches these metrics compared to attention-based models, or how training stability varies with dataset size
- What evidence would resolve it: Training curves comparing loss convergence, FID improvement over training steps, and stability metrics across different dataset sizes and model scales

## Limitations
- The paper doesn't address text-to-image synthesis, limiting the applicability of DiffuSSM to conditional generation tasks beyond class-conditional generation
- Some implementation details of the hourglass architecture (specifically W↑, W↓, W0, W1, W2, W3 matrices) are underspecified, making exact reproduction challenging
- The paper lacks absolute performance metrics and wall-clock time comparisons with attention-based models, limiting practical deployment insights

## Confidence
- **High confidence**: The core mechanism of replacing attention with SSMs is well-established in the literature (S4, S4D papers), and the claimed O(L log L) complexity advantage over O(L²) attention is mathematically sound.
- **Medium confidence**: The empirical claims about achieving comparable or better FID/sFID scores while reducing Gflops are supported by the presented experiments, but the absence of some implementation details and absolute performance metrics creates uncertainty about exact reproducibility.
- **Low confidence**: The claim about "significantly reducing total FLOP usage" is difficult to verify without knowing the specific model sizes and configurations of the compared baselines, as the paper only provides relative comparisons.

## Next Checks
1. Implement a minimal hourglass SSM block to verify the basic functionality and test the computational claims about complexity reduction
2. Run ablation studies on downsampling ratio M to identify the optimal tradeoff between computational efficiency and generation quality
3. Profile absolute training/inference costs (wall-clock time and memory usage) for DiffuSSM versus DiT on identical hardware to quantify real-world efficiency gains