---
ver: rpa2
title: Towards Unsupervised Recognition of Token-level Semantic Differences in Related
  Documents
arxiv_id: '2305.13303'
source_url: https://arxiv.org/abs/2305.13303
tags:
- documents
- xlm-r
- semantic
- sentences
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a token-level regression task for recognizing
  semantic differences (RSD) between related documents, formulated as highlighting
  words that cause semantic differences. Three unsupervised approaches are proposed:
  word alignment, deletability measurement, and masked language modeling cross-entropy.'
---

# Towards Unsupervised Recognition of Token-level Semantic Differences in Related Documents

## Quick Facts
- **arXiv ID**: 2305.13303
- **Source URL**: https://arxiv.org/abs/2305.13303
- **Reference count**: 25
- **Primary result**: Three unsupervised approaches for token-level semantic difference recognition achieve up to 62.1% Spearman correlation on English documents, with significant performance drops on cross-lingual pairs.

## Executive Summary
This paper introduces a token-level regression task for recognizing semantic differences (RSD) between related documents, formulated as highlighting words that cause semantic differences. Three unsupervised approaches are proposed: word alignment, deletability measurement, and masked language modeling cross-entropy. The word alignment-based method using a SimCSE-adapted XLM-R model achieves the highest Spearman correlation of 62.1% on English document pairs, outperforming the other unsupervised methods. However, all approaches show significant performance drops when comparing cross-lingual document pairs, with the best method reaching only 31.7% correlation on English-to-French comparisons. The study reveals that while unsupervised methods can capture semantic differences to some extent, there remains a large margin for improvement, particularly in cross-lingual settings.

## Method Summary
The paper formulates recognizing semantic differences (RSD) as a token-level regression task to highlight words that cause semantic differences between related documents. Three unsupervised approaches are proposed: word alignment using cosine similarity between token embeddings, deletability measurement by comparing document similarity with and without each word, and masked language modeling cross-entropy comparing surprisal with and without the other document as context. The authors use XLM-R base model with and without SimCSE fine-tuning, evaluating on progressively complex test sets including basic English sentences, synthetic documents with permutations, and cross-lingual document pairs. The evaluation metric is Spearman correlation between predicted labels and gold labels across all words in the dataset.

## Key Results
- Word alignment approach (diffalign) with SimCSE-adapted XLM-R achieves 62.1% Spearman correlation on English document pairs
- Deletability approach (diffdel) achieves 51.7% correlation, showing that deletability is a weaker signal than alignment
- Cross-lingual performance drops significantly, with the best method reaching only 31.7% correlation on English-to-French comparisons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Word alignment-based approaches using cosine similarity between token embeddings can effectively identify semantic differences by measuring alignment confidence.
- Mechanism: The approach computes token-level embeddings for both documents and calculates pairwise cosine similarity. Words with low maximum alignment scores to any token in the other document are flagged as causing semantic differences.
- Core assumption: Semantic differences manifest as poor token-level alignment between documents, and cosine similarity between embeddings captures semantic similarity effectively.
- Evidence anchors:
  - [abstract] "Our results show that an approach based on word alignment and sentence-level contrastive learning has a robust correlation to gold labels."
  - [section 3] "A simple approach to RSD is to calculate a soft token alignment between h(A) and h(B) and to identify tokens that are aligned with low confidence."
  - [corpus] Weak - corpus shows related work on document scaling and clustering but not direct alignment-based semantic difference detection.
- Break condition: This mechanism fails when documents contain paraphrases, synonyms, or structural differences that don't reflect semantic differences, or when the embedding space doesn't capture semantic similarity well for the domain.

### Mechanism 2
- Claim: Measuring the deletability of words by comparing document similarity with and without each word provides an effective unsupervised signal for semantic differences.
- Mechanism: The approach computes sentence embeddings as averages of token embeddings, then measures how similarity changes when each word is removed. Words whose removal significantly affects similarity are considered important for semantic differences.
- Core assumption: Words causing semantic differences will have disproportionate impact on document similarity when removed, and averaging token embeddings captures sentence semantics well enough for this comparison.
- Evidence anchors:
  - [section 3] "We approximate the similarity of a partial sequence A\ai, where ai is deleted, by excluding the token from the average: sim(A\ai,B ) = cos( avg(A)− 1|A| h(ai), avg(B))."
  - [section 6] "When relying on the deletability of a word to recognize differences, correlation is clearly lower, and an unsupervised model trained with SimCSE lags behind the indirectly supervised model."
  - [corpus] Weak - corpus contains work on document scaling but not direct deletability-based semantic difference detection.
- Break condition: This mechanism fails when documents contain redundant information, when multiple words contribute equally to differences, or when the averaging approach doesn't capture semantic nuances.

### Mechanism 3
- Claim: Using masked language modeling cross-entropy differences with and without context from the other document can identify words that contribute to semantic differences.
- Mechanism: The approach measures how much the cross-entropy of predicting a masked word decreases when the other document is provided as context. Words that don't benefit from additional context are considered to cause semantic differences.
- Core assumption: Words that cause semantic differences will not have their prediction uncertainty reduced by context from the other document, and MLM cross-entropy effectively measures prediction uncertainty.
- Evidence anchors:
  - [section 3] "By concatenating B and A′ into an augmented context BA′, we can test whether the additional context helps the language model predict ai. If the inclusion of B does not reduce the cross-entropy, this could indicate that B does not contain any information related to ai."
  - [section 6] "Using the cross-entropy of masked language modeling yields some competitive results on longer documents. However, latency measurements show that this approach is much slower than the other approaches."
  - [corpus] Weak - corpus contains related work on context usage in machine translation but not direct MLM-based semantic difference detection.
- Break condition: This mechanism fails when documents contain common vocabulary that doesn't benefit from additional context regardless of semantic importance, or when MLM predictions are unreliable for certain word types.

## Foundational Learning

- Concept: Token-level embeddings and their relationship to semantic similarity
  - Why needed here: The entire approach relies on understanding how individual token embeddings capture semantic meaning and how their relationships can be used to detect differences
  - Quick check question: If two documents contain the same words in different orders, will the token alignment approach correctly identify them as semantically similar?

- Concept: Sentence representation through token embedding averaging
  - Why needed here: The deletability approach and SimCSE adaptation both rely on averaging token embeddings to create sentence representations
  - Quick check question: What happens to the sentence representation if one very important word is removed from a document?

- Concept: Cross-entropy and information theory in language models
  - Why needed here: The masked language modeling approach uses cross-entropy differences to measure how much context helps predict individual words
  - Quick check question: If adding context from another document doesn't reduce the cross-entropy for a word, what does that tell us about the relationship between the two documents?

## Architecture Onboarding

- Component map: Tokenizer/Encoder (XLM-R) -> Three Comparison Approaches (Alignment, Deletability, MLM) -> Evaluation Infrastructure
- Critical path: For the alignment approach, the critical path is: encode both documents → compute token embedding matrix → calculate pairwise cosine similarity → find maximum alignment scores → compute difference scores. This must be optimized for latency since it's used for 1000+ tokens.
- Design tradeoffs: The alignment approach trades precision for speed (greedy alignment vs optimal alignment), while the MLM approach trades speed for potentially better accuracy (re-encoding for each masked word). The deletability approach trades implementation simplicity for potentially lower accuracy.
- Failure signatures: Alignment approach fails on non-monotonic alignments and cross-lingual pairs; deletability approach fails on documents with redundant information; MLM approach fails on documents with common vocabulary and has high latency.
- First 3 experiments:
  1. Run alignment approach on a simple English pair like "The cat sat on the mat" vs "The dog sat on the mat" and verify that "cat" and "dog" get high difference scores while function words get low scores.
  2. Test deletability approach on a pair where removing one word significantly changes meaning vs one where it doesn't, and verify the difference in scores.
  3. Benchmark all three approaches on a small synthetic dataset to compare correlation scores and latency before scaling to full evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the word alignment-based approach (diffalign) scale with document length beyond 5 sentences, and what are the specific factors limiting its effectiveness on longer documents?
- Basis in paper: [inferred] The paper tests documents with 5 sentences and shows decreasing performance, but does not explore longer documents or identify specific limiting factors.
- Why unresolved: The experimental design only evaluates up to 5-sentence documents, leaving the performance curve for longer documents unknown. The paper does not analyze which aspects of longer documents (e.g., cumulative noise, attention span, computational complexity) specifically degrade performance.
- What evidence would resolve it: Systematic experiments testing document lengths from 5 to 50 sentences with detailed ablation studies showing which components of the word alignment method break down first as documents grow longer.

### Open Question 2
- Question: What is the precise mechanism by which SimCSE fine-tuning improves word alignment quality, and can this improvement be achieved through alternative contrastive learning objectives?
- Basis in paper: [explicit] The paper observes that SimCSE fine-tuning "strongly improves" word alignment results compared to the base model, but does not explain the mechanism or test alternative approaches.
- Why unresolved: While the paper demonstrates the effectiveness of SimCSE, it does not analyze what specific aspects of the contrastive learning objective (e.g., positive pair construction, temperature parameter, averaging strategy) contribute to better alignment, nor does it compare against other sentence-level contrastive methods.
- What evidence would resolve it: Controlled experiments varying individual SimCSE hyperparameters and comparing against other contrastive learning frameworks (e.g., Barlow Twins, VICReg) while measuring both sentence representation quality and downstream word alignment performance.

### Open Question 3
- Question: How do the three proposed unsupervised approaches (diffalign, diffdel, diffmask) perform on naturally occurring document pairs rather than synthetic documents, and what modifications would be needed to handle real-world document characteristics?
- Basis in paper: [explicit] The paper acknowledges that synthetic documents have "no document-level coherence" and that findings "will roughly translate to natural documents," but does not test on real documents.
- Why unresolved: The experimental design deliberately uses synthetic documents to control variables, but this limits understanding of how the methods handle real document features like topic coherence, discourse structure, and varying writing styles that are absent in synthetic pairs.
- What evidence would resolve it: Evaluation on benchmark datasets of naturally occurring related documents (e.g., news articles, academic papers, legal documents) with systematic analysis of failure cases and targeted modifications to each approach for handling specific document-level phenomena.

## Limitations
- The study relies on synthetic document generation for evaluation, which may not fully capture real-world semantic differences between related documents
- Cross-lingual performance drops significantly (over 50% reduction in correlation) compared to monolingual English pairs
- The evaluation primarily benchmarks against a paraphrase-trained model rather than fully supervised semantic difference models

## Confidence
- **Medium**: Unsupervised methods' effectiveness on English document pairs - The alignment approach achieves reasonable correlation (62.1%) but still lags behind supervised methods
- **Low**: Methods' effectiveness in cross-lingual settings - Substantial performance degradation observed across all approaches
- **Medium**: Comparison with supervised baselines - Limited benchmarking against paraphrase-trained model rather than fully supervised semantic difference models

## Next Checks
1. **Real-world document validation**: Test the best-performing alignment approach on a small set of manually curated document pairs from actual domains (legal, technical documentation, news articles) to verify that synthetic evaluation correlates with real-world performance.

2. **Cross-lingual fine-tuning experiment**: Conduct an experiment fine-tuning XLM-R with SimCSE on parallel corpora across multiple languages to determine if cross-lingual performance can be improved beyond the current 31.7% correlation ceiling.

3. **Error analysis on cross-lingual pairs**: Perform detailed error analysis on the English-to-French document pairs to identify specific failure patterns (e.g., structural differences, idiomatic expressions, named entities) that could guide targeted improvements to the alignment methodology.