---
ver: rpa2
title: 'HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and
  Omission Detection in Machine Translation'
arxiv_id: '2305.11746'
source_url: https://arxiv.org/abs/2305.11746
tags:
- translation
- detection
- hallucinations
- methods
- translations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HalOmi, a manually annotated benchmark dataset
  for detecting hallucinations and omissions in machine translation across 18 language
  pairs. The dataset includes fine-grained sentence-level and token-level annotations,
  covering both partial and full cases.
---

# HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation

## Quick Facts
- arXiv ID: 2305.11746
- Source URL: https://arxiv.org/abs/2305.11746
- Reference count: 18
- Key outcome: Introduces HalOmi, a manually annotated benchmark for detecting hallucinations and omissions in MT across 18 language pairs, finding internal model-based methods outperform external ones especially for low-resource pairs.

## Executive Summary
This paper introduces HalOmi, a manually annotated benchmark dataset for detecting hallucinations and omissions in machine translation across 18 language pairs. The dataset includes fine-grained sentence-level and token-level annotations, covering both partial and full cases. The authors evaluate several detection methods, finding that internal model-based approaches outperform external ones, especially for low-resource language pairs. Attention-based methods are found to be unreliable for detecting hallucinations. Additionally, they show that artificially induced pathologies do not reliably reflect natural ones, cautioning against their use in evaluation. The work establishes a solid foundation for future research on detecting and analyzing translation pathologies.

## Method Summary
The authors generate translations using NLLB-200 for 18 language pairs and manually annotate them for hallucinations and omissions at both sentence and token levels. They evaluate detection methods including internal approaches (sequence log-probability, ALTI/ALTIT) and external methods (COMET-QE, sentence similarity measures). The evaluation uses ROC AUC scores to rank pathologies by severity, comparing performance across high-resource and low-resource language pairs, and testing on both naturally generated and artificially perturbed translations.

## Key Results
- Internal model-based detection methods outperform external ones, especially for low-resource language pairs
- Attention-based anomaly detection is unreliable for detecting hallucinations
- Artificially induced pathologies via model perturbation do not reliably reflect natural hallucinations/omissions

## Why This Works (Mechanism)

### Mechanism 1
Internal model-based detection methods outperform external ones, especially for low-resource language pairs because they leverage token contributions and model-specific information directly from the NLLB-200 model, making them more sensitive to subtle hallucinations and omissions than external quality estimators trained on different datasets. This works when the model's internal state carries discriminative signal for pathologies. Break condition: If the model's internal representations become too generic or compressed.

### Mechanism 2
Attention-based anomaly detection is unreliable for detecting hallucinations because attention patterns in NLLB-200 are heavily influenced by EOS token dominance and source sequence handling, which masks meaningful differences between hallucinated and non-hallucinated translations. This works when the baseline distribution is representative and not dominated by systematic artifacts. Break condition: If the model architecture changes to reduce EOS token dominance or uses attention mechanisms less sensitive to sequence detachment.

### Mechanism 3
Artificially induced pathologies via model perturbation do not reliably reflect natural ones because perturbations create a biased distribution of pathologies that differs qualitatively from those generated naturally by the model. This works when the distribution of hallucinations/omissions generated under perturbation overlaps sufficiently with natural ones. Break condition: If perturbations are designed to mimic natural generation dynamics more closely.

## Foundational Learning

- Concept: Difference between hallucinations and omissions
  - Why needed here: Distinguishing these pathologies is critical for both annotation and detection; the paper shows they require different detection strategies.
  - Quick check question: If a translation drops a source phrase but does not add unrelated content, is it a hallucination or an omission?

- Concept: Token-level vs sentence-level pathology detection
  - Why needed here: The paper introduces both detection granularities and shows they have different performance characteristics and use cases.
  - Quick check question: Which detection granularity would you use if you wanted to correct only the erroneous words in a translation?

- Concept: Internal vs external detection methods
  - Why needed here: The paper contrasts these approaches and shows internal methods are superior for low-resource languages.
  - Quick check question: Why might an external MT quality estimator fail on a low-resource language pair it was not trained on?

## Architecture Onboarding

- Component map: NLLB-200 multilingual encoder-decoder model → Translation outputs → Manual annotation pipeline → Detection method evaluation → Corpus of annotated pathologies
- Critical path: Data generation → Annotation → Detection method training/evaluation → Benchmark release
- Design tradeoffs: High-resource vs low-resource detection performance; internal vs external methods; sentence-level vs token-level granularity
- Failure signatures: Poor external method performance on low-resource pairs; attention-based methods failing to detect hallucinations; perturbed data not generalizing to natural pathologies
- First 3 experiments:
  1. Reproduce the sentence-level hallucination detection results using Seq-Logprob and ALTI on the HalOmi dataset.
  2. Evaluate the token-level detection performance of log-probability vs ALTI contributions.
  3. Test the robustness of detection methods on artificially perturbed translations vs natural ones.

## Open Questions the Paper Calls Out

- How do different translation models compare in their susceptibility to hallucinations and omissions across diverse language pairs? The paper uses only NLLB-200, limiting generalizability.
- What are the underlying causes of hallucinations and omissions in machine translation, and how can they be systematically mitigated? The paper focuses on detection rather than prevention.
- How do token-level detection methods perform in real-world applications, and can they be scaled for production use? The paper evaluates experimentally but doesn't address practical deployment challenges.
- How do different resource levels (high-resource vs. low-resource languages) affect the reliability of hallucination and omission detection methods? While performance differences are noted, underlying factors are not explored.
- Can artificially induced pathologies be reliably used to evaluate hallucination and omission detection methods, or do they introduce biases? The paper demonstrates limitations of perturbation-based evaluation.

## Limitations

- The evaluation relies entirely on NLLB-200 model outputs, potentially limiting generalizability to other translation systems
- Manual annotation covers only 5 of 18 language pairs for token-level annotation, limiting robustness of token-level findings
- ROC AUC assumes equal cost for false positives and negatives, which may not reflect practical applications
- The perturbation experiments use only one simple perturbation approach, not exploring the full space of possible manipulations

## Confidence

- High Confidence: Internal model-based methods outperform external ones for low-resource language pairs
- Medium Confidence: Attention-based methods are unreliable for hallucination detection (architecture-specific)
- Medium Confidence: Artificially induced pathologies don't reliably reflect natural ones (based on single perturbation approach)

## Next Checks

1. Cross-Model Validation: Replicate detection method comparison using translations from at least two additional multilingual translation models to assess whether internal method superiority holds across different architectures.

2. Architecture-Agnostic Attention Analysis: Test reliability of attention-based detection methods on transformer architectures with different attention mechanisms to determine if fragility is general or NLLB-200-specific.

3. Alternative Perturbation Strategies: Implement and evaluate at least two additional perturbation methods to determine if disconnect between artificial and natural pathologies persists across different manipulation techniques.