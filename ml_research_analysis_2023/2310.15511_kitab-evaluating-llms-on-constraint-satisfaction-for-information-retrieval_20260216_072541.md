---
ver: rpa2
title: 'KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval'
arxiv_id: '2310.15511'
source_url: https://arxiv.org/abs/2310.15511
tags:
- uni00000013
- uni00000011
- uni00000010
- uni00000048
- uni00000051
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KITAB, a dataset and evaluation framework
  for assessing the ability of large language models (LLMs) to satisfy complex information
  retrieval queries with multiple constraints. The dataset contains 13,000 queries
  about books from over 600 authors, with constraints on title lexical properties,
  publication years, and named entities.
---

# KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval

## Quick Facts
- **arXiv ID**: 2310.15511
- **Source URL**: https://arxiv.org/abs/2310.15511
- **Reference count**: 28
- **Primary result**: LLMs struggle with constraint satisfaction for information retrieval even with complete context, achieving at most 35% all-correctness

## Executive Summary
This paper introduces KITAB, a dataset and evaluation framework for assessing large language models' ability to satisfy complex information retrieval queries with multiple constraints. The dataset contains 13,000 queries about books from over 600 authors, with constraints on title lexical properties, publication years, and named entities. Experiments on GPT-4 and GPT-3.5 reveal that while LLMs can retrieve relevant books, they struggle significantly with constraint satisfaction - achieving at most 35% all-correctness even with complete context provided. Information irrelevance rates remain high (12-41%) when using only parametric knowledge, and self-retrieval approaches can exacerbate this issue. The findings reveal fundamental barriers to constraint satisfaction that are not easily addressed by scaling model size alone.

## Method Summary
The study evaluates GPT-4 and GPT-3.5 on a 13,000-query dataset about books using four experimental conditions: ALL-BOOKS (baseline retrieval), NO-CONTEXT (parametric knowledge only), WITH-CONTEXT (provided context), and SELF-CONTEXT (self-retrieval with chain-of-thought). Models are prompted to generate book lists satisfying multiple constraints, and performance is measured using four metrics: information irrelevance rate, constraint satisfaction rate, completeness, and all-correctness. The evaluation uses lenient metrics including subset matching and fuzzy matching at 80% threshold via Levenshtein distance.

## Key Results
- LLMs achieve at most 35% all-correctness even with complete context, indicating fundamental constraint satisfaction failures
- Irrelevant information rates remain high (12-41%) when using only parametric knowledge, especially for less popular authors
- Self-retrieval approaches significantly increase hallucination rates, with irrelevance rates of 0.42-0.44 for GPT-4 before constraint application
- Constraint satisfaction failures persist even when models can retrieve relevant information, suggesting logical filtering limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large language models exhibit high rates of irrelevant information generation when retrieving data solely from parametric knowledge, particularly for less popular authors.
- **Mechanism:** Models lack explicit constraints during parametric retrieval, leading to fabrication of non-author book titles to fill content gaps.
- **Core assumption:** Models rely heavily on memorized patterns rather than precise factual recall, especially under low-frequency information conditions.
- **Evidence anchors:**
  - [abstract] "irrelevant information rates remain high (12-41%) when using only parametric knowledge"
  - [section] "irrelevance is quite high...between 0-10 [sitelinks], irrelevance is quite high. Afterwards, the rate of irrelevant books drops, but quickly flattens out"
- **Break condition:** Providing complete context or using retrieval-augmented generation eliminates irrelevant information by constraining the model's output space.

### Mechanism 2
- **Claim:** Even with complete context available, models struggle to apply constraints correctly, indicating fundamental limitations in constraint satisfaction abilities.
- **Mechanism:** Models can retrieve relevant information but fail at logical filtering operations required to satisfy complex constraints.
- **Core assumption:** Constraint satisfaction requires reasoning and logical operations beyond simple pattern matching or information recall.
- **Evidence anchors:**
  - [abstract] "constraint satisfaction failures remain a major obstacle across both LLMs and different constraint types, even with complete context"
  - [section] "model performance remains unreliable even with provided complete context from KITAB, simulating search-assisted settings"
- **Break condition:** Models show improved constraint satisfaction when tasks are simplified to single-item checks rather than list operations.

### Mechanism 3
- **Claim:** Self-retrieval approaches increase hallucination rates as models fabricate titles to satisfy constraints.
- **Mechanism:** Chain-of-thought prompting that requires models to first retrieve their own context leads to compounding errors when initial retrieval is incorrect.
- **Core assumption:** Models prioritize satisfying explicit constraints over maintaining factual accuracy, leading to recursive fabrication.
- **Evidence anchors:**
  - [abstract] "self-retrieval approaches significantly increase the rate of irrelevant (potentially hallucinated) information"
  - [section] "irrelevant information for the first part of the chain-of-thought process...is notably high, 0.42 for GPT4...Even though after applying constraints, irrelevance decreases to 0.33 and 0.44"
- **Break condition:** When context is provided externally rather than self-retrieved, irrelevant information rates drop significantly.

## Foundational Learning

- **Concept:** Constraint satisfaction problems and their complexity metrics (constrainedness)
  - Why needed here: The paper uses constraint satisfaction framework to evaluate LLM performance, measuring how well models can filter information based on multiple criteria
  - Quick check question: How does constrainedness κ = 1 - S/N relate to query difficulty, and why might some constraint types behave differently across constrainedness levels?

- **Concept:** Information retrieval evaluation metrics and fuzzy matching
  - Why needed here: The paper uses lenient evaluation metrics (subset matching, fuzzy matching at 80% threshold) to measure model performance, requiring understanding of information retrieval evaluation
  - Quick check question: What are the advantages and limitations of using Levenshtein distance at 80% threshold for matching book titles in evaluation?

- **Concept:** Parametric knowledge vs. retrieval-augmented generation
- **Concept:** Information popularity and its relationship to model performance
  - Why needed here: The paper investigates how author popularity (via WikiData sitelinks) affects model performance, requiring understanding of how training data frequency impacts generation
  - Quick check question: Why does irrelevant information decrease with higher popularity but constraint satisfaction doesn't improve correspondingly?

## Architecture Onboarding

- **Component map:** Dataset generation pipeline → Constraint sampling → Query construction → Model evaluation framework → Performance analysis
- **Critical path:** Author sampling → Book collection → Constraint application → Context provision → Model output → Evaluation metrics
- **Design tradeoffs:** Lenient evaluation metrics allow capturing positive trends but may overestimate true performance; self-retrieval vs. provided context affects hallucination rates differently
- **Failure signatures:** High irrelevant information rates (12-41%) with parametric knowledge; persistent constraint satisfaction failures even with complete context; self-retrieval amplifying hallucinations
- **First 3 experiments:**
  1. Test ALL-BOOKS condition to establish baseline retrieval performance and measure information irrelevance rates
  2. Test NO-CONTEXT condition with single constraint types to identify which constraints are most challenging
  3. Test WITH-CONTEXT condition to isolate constraint satisfaction failures from information retrieval failures

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the constrainedness of a query affect model performance when considering different types of constraints (e.g., lexical vs. entity-based)?
- **Basis in paper:** [explicit] The paper discusses how model performance varies with constrainedness and notes that for certain constraint types like ends-with and city-name, higher constrainedness is associated with better performance, contrary to the general trend.
- **Why unresolved:** The paper does not provide a detailed explanation for why certain constraints behave differently with varying constrainedness, leaving this as an area for further investigation.
- **What evidence would resolve it:** Additional experiments that systematically vary constrainedness for different constraint types and analyze the model's performance to identify patterns or underlying mechanisms.

### Open Question 2
- **Question:** What is the impact of information popularity on the model's ability to satisfy constraints, and why does relevance decrease with higher popularity?
- **Basis in paper:** [explicit] The paper finds that while irrelevant information decreases with higher popularity, there is no clear positive correlation between popularity and desirable outcomes such as satisfaction, completeness, and all-correctness.
- **Why unresolved:** The paper conjectures that pragmatic decisions during training may lead to memorization resources being devoted only after seeing the author a number of times, but this is not fully explored or explained.
- **What evidence would resolve it:** Further research into the training data distribution and model's memorization strategies, possibly including controlled experiments with varying popularity levels.

### Open Question 3
- **Question:** How do self-retrieval approaches impact the quality of information generated by the model, and can this be improved?
- **Basis in paper:** [explicit] The paper observes that self-retrieval approaches significantly increase the rate of irrelevant information and fabricated titles, even though they can satisfy constraints.
- **Why unresolved:** The paper suggests that models may fabricate irrelevant books to satisfy constraints, but does not explore methods to mitigate this issue or improve the self-retrieval process.
- **What evidence would resolve it:** Experiments testing different self-retrieval strategies or fine-tuning models specifically for this task to reduce irrelevant information while maintaining constraint satisfaction.

## Limitations

- Dataset construction methodology is underspecified, particularly how the 13,000 queries were sampled and whether this represents a representative distribution of real-world information retrieval scenarios
- Evaluation framework relies heavily on lenient metrics (subset matching, 80% fuzzy matching threshold) which may overestimate actual model capabilities
- Study focuses exclusively on book-related queries, limiting generalizability to other domains of information retrieval

## Confidence

- **High confidence**: LLMs struggle with constraint satisfaction even with complete context (supported by multiple evidence anchors and consistent findings across conditions)
- **Medium confidence**: Parametric knowledge leads to high irrelevant information generation (mechanism well-supported but specific rates may vary with different datasets/authors)
- **Low confidence**: Self-retrieval approaches necessarily increase hallucinations (mechanism plausible but the specific 0.42-0.44 irrelevance rates may be implementation-dependent)

## Next Checks

1. **Dataset validation**: Reconstruct the query generation pipeline independently to verify the 13,000 query sample represents realistic constraint satisfaction scenarios and test on additional domains beyond books
2. **Metric sensitivity analysis**: Evaluate model performance using stricter matching criteria (100% exact matching, multiple fuzzy thresholds) to determine how lenient metrics affect reported success rates
3. **Cross-model comparison**: Test additional LLM architectures beyond GPT-4 and GPT-3.5 to determine if constraint satisfaction failures are model-specific or represent fundamental architectural limitations