---
ver: rpa2
title: Natural Response Generation for Chinese Reading Comprehension
arxiv_id: '2302.08817'
source_url: https://arxiv.org/abs/2302.08817
tags:
- question
- response
- answer
- responses
- penguin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Penguin, a Chinese reading comprehension
  dataset for natural response generation, addressing the limitation of current benchmarks
  that rely on extracted spans or candidate choices. Penguin contains 200k training
  examples with human-like, informative responses.
---

# Natural Response Generation for Chinese Reading Comprehension

## Quick Facts
- arXiv ID: 2302.08817
- Source URL: https://arxiv.org/abs/2302.08817
- Reference count: 40
- Primary result: Penguin dataset (200k examples) enables natural response generation in Chinese MRC, outperforming span extraction methods

## Executive Summary
This paper introduces Penguin, a Chinese reading comprehension dataset designed for natural response generation, addressing limitations in existing benchmarks that rely on extracted spans or candidate choices. The dataset contains 200k training examples with human-like, informative responses that go beyond simple fact extraction. The authors propose two frameworks - end-to-end and two-stage (Answerer + Responser) - and demonstrate that prompt-tuning with prefix templates can further improve performance. Both automatic and human evaluations show generated responses are more fluent, relevant, and closer to real-world QA scenarios than previous methods.

## Method Summary
The authors introduce Penguin, a Chinese reading comprehension dataset containing 200k training examples with human-like responses. Two frameworks are proposed: an end-to-end approach where a single generative model produces responses directly from question-passage input, and a two-stage approach where an Answerer module first generates an initial answer, followed by a Responser module that rewrites it into a natural response. Prompt-tuning with five different prefix templates per module is applied to improve performance. The models are evaluated using automatic metrics (BLEU, ROUGE-L, Dist-1/2) and human evaluation criteria (fluency, informativeness, relevance).

## Key Results
- Penguin dataset contains 200k training examples with human-like responses that are more informative than extracted spans
- Two-stage framework outperforms end-to-end approach in human evaluation, particularly on relevance with the passage
- Prompt-tuning with prefix templates further improves model performance
- Generated responses achieve high fluency and informativeness scores in human evaluation
- The approach generalizes to different model sizes (small, base, large) of T5 and BART

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Natural response generation in MRC is achievable by training on high-quality, human-like responses rather than span extraction or candidate selection
- Mechanism: The dataset provides 200k examples with responses that are more informative and fluent than extracted spans or candidate choices, enabling models to learn to generate natural responses
- Core assumption: Human-annotated or high-quality auto-generated responses can serve as effective training targets for generative MRC models
- Evidence anchors: [abstract]: "Penguin contains 200k training examples with human-like, informative responses"; [section 3.2]: Describes the dataset construction process, including human annotation and filtering to ensure response quality
- Break condition: If the responses are not truly natural or contain errors, the model will learn to generate unnatural or incorrect responses

### Mechanism 2
- Claim: A two-stage framework (Answerer + Responser) can generate more natural responses than end-to-end approaches
- Mechanism: The Answerer module generates an initial answer, and the Responser module rewrites and expands it into a human-like response
- Core assumption: Separating the answer generation and response generation tasks allows for more focused and effective learning
- Evidence anchors: [section 4]: Describes the two-stage framework and its components; [section 5.3]: Human evaluation results show that the two-stage framework performs better in relevance with the passage
- Break condition: If the Responser module fails to effectively rewrite the answer, the overall response quality will suffer

### Mechanism 3
- Claim: Prompt-tuning with a mixture of prefix prompts can further improve the performance of generative MRC models
- Mechanism: The model is fine-tuned with a variety of prefix prompts, allowing it to learn different response styles and patterns
- Core assumption: Exposing the model to diverse prompts during training can improve its generalization and flexibility
- Evidence anchors: [abstract]: "Prompt-tuning with prefix templates further improves performance"; [section 4]: Describes the prompt-tuning approach and its implementation
- Break condition: If the prompts are not diverse enough or not well-designed, the model may not benefit from prompt-tuning

## Foundational Learning

- Concept: Machine Reading Comprehension (MRC)
  - Why needed here: MRC is the core task addressed in the paper, and understanding its concepts is essential for understanding the proposed approach
  - Quick check question: What are the main types of MRC tasks, and how does the proposed approach differ from them?

- Concept: Generative Language Models (GLMs)
  - Why needed here: GLMs like BART and T5 are used as the backbone models for the proposed approach, and understanding their capabilities and limitations is crucial
  - Quick check question: What are the key differences between GLMs and other types of language models, such as encoder-only models like BERT?

- Concept: Prompt-tuning
  - Why needed here: Prompt-tuning is a key component of the proposed approach, and understanding its principles and applications is essential
  - Quick check question: How does prompt-tuning differ from traditional fine-tuning, and what are its potential benefits and drawbacks?

## Architecture Onboarding

- Component map: Dataset (Penguin) -> Framework selection (end-to-end or two-stage) -> Model training (with or without prompt-tuning) -> Evaluation (automatic and human metrics)
- Critical path: Data preparation (dataset construction and prompt design) -> Model training -> Evaluation using automatic and human metrics
- Design tradeoffs: Two-stage framework may be more effective in generating natural responses but requires more computational resources; prompt-tuning can improve performance but may introduce additional complexity
- Failure signatures: If the model generates unnatural or incorrect responses, it may indicate issues with the dataset, model architecture, or prompt design; if the model overfits to the training data, it may perform poorly on unseen examples
- First 3 experiments:
  1. Train the end-to-end framework with BART-large on the Penguin dataset and evaluate its performance using automatic metrics (e.g., BLEU, ROUGE-L)
  2. Train the two-stage framework with BART-large on the Penguin dataset and compare its performance to the end-to-end framework using automatic and human metrics
  3. Fine-tune the two-stage framework with prompt-tuning and evaluate its performance compared to the non-prompt-tuned version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the generated responses compare to human-written responses in terms of fluency, informativeness, and relevance?
- Basis in paper: [explicit] The paper states that human annotators rated the generated responses on three criteria: Fluency, Informativeness, and Relevance. The authors claim the generated responses are close to human-like, but do not provide a direct comparison to human-written responses
- Why unresolved: The paper only compares the generated responses to the ground truth answers, not to human-written responses. A direct comparison to human-written responses would provide a more accurate assessment of the quality of the generated responses
- What evidence would resolve it: A study comparing the generated responses to human-written responses on the same test set, using the same human evaluation criteria

### Open Question 2
- Question: How does the two-stage framework perform compared to the end-to-end framework in terms of parameter efficiency and inference cost?
- Basis in paper: [explicit] The paper mentions that the two-stage framework has almost twice the parameters and inference cost of the end-to-end framework, but does not provide a detailed comparison of their performance
- Why unresolved: While the paper states that the two-stage framework achieves comparable results to the end-to-end framework, it does not quantify the trade-off between performance and computational cost
- What evidence would resolve it: A detailed comparison of the performance, parameter count, and inference time of both frameworks on the same test set

### Open Question 3
- Question: How does the proposed Prompt-BART approach compare to other prompt-tuning methods for generative language models?
- Basis in paper: [explicit] The paper introduces Prompt-BART, a method for fine-tuning pre-trained generative language models using a variety of prefix prompts. However, it does not compare this approach to other prompt-tuning methods
- Why unresolved: The paper demonstrates the effectiveness of Prompt-BART, but does not establish its superiority or uniqueness compared to other prompt-tuning techniques
- What evidence would resolve it: A comparison of Prompt-BART to other prompt-tuning methods, such as prefix-tuning or P-tuning, on the same task and dataset

## Limitations

- Dataset quality uncertainty: The construction process relies heavily on automatic filtering with unspecified threshold values, making it unclear how much noise remains in the training data
- Language specificity: The approach is evaluated only on Chinese text, limiting generalizability to other languages with different morphological structures
- Evaluation completeness: Human evaluation doesn't comprehensively assess factual accuracy, and automatic metrics have known limitations for generative tasks

## Confidence

- High confidence in the core observation that natural response generation differs fundamentally from span extraction
- Medium confidence in the effectiveness of the two-stage framework, as automatic metrics don't consistently favor it
- Medium confidence in the prompt-tuning improvements, as the magnitude and consistency across metrics is not clearly demonstrated

## Next Checks

1. **Ablation study on filtering thresholds**: Systematically vary the semantic matching and coherence filtering thresholds used during dataset construction to quantify their impact on model performance
2. **Cross-lingual transfer evaluation**: Apply the trained models to a similar generative MRC task in English (e.g., MS MARCO) without additional training to assess whether the approach generalizes beyond Chinese
3. **Long-term consistency evaluation**: Generate responses for the same questions after different time intervals to measure response consistency, which is critical for real-world deployment