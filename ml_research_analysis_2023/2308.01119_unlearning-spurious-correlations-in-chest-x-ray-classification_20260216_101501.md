---
ver: rpa2
title: Unlearning Spurious Correlations in Chest X-ray Classification
arxiv_id: '2308.01119'
source_url: https://arxiv.org/abs/2308.01119
tags:
- explanations
- learning
- explanation
- regions
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study proposes eXemplary eXplanation Based Learning (eXBL),\
  \ an interactive machine learning method for unlearning spurious correlations in\
  \ chest X-ray image classification. The approach addresses the problem of unintended\
  \ confounding factors\u2014such as skeletal maturation\u2014in medical image datasets\
  \ by incorporating user feedback in the form of ranked exemplary explanations."
---

# Unlearning Spurious Correlations in Chest X-ray Classification

## Quick Facts
- arXiv ID: 2308.01119
- Source URL: https://arxiv.org/abs/2308.01119
- Reference count: 22
- One-line primary result: eXBL achieved comparable classification accuracy to baseline (0.90 vs 0.95) while improving explanation quality (AP from 0.32 to 0.35)

## Executive Summary
This paper proposes eXemplary eXplanation Based Learning (eXBL), an interactive machine learning method for unlearning spurious correlations in chest X-ray image classification. The approach addresses the problem of unintended confounding factors—such as skeletal maturation—in medical image datasets by incorporating user feedback in the form of ranked exemplary explanations. Instead of requiring detailed feature annotations for all training images, eXBL only asks users to identify one good and one bad explanation from two training instances. The method uses these exemplars to guide a triplet loss-based training process, which encourages the model to focus on relevant regions (e.g., lungs) and ignore confounding ones. Evaluated on a Covid-19 chest X-ray dataset using a MobileNetV2 backbone, eXBL achieved comparable classification accuracy to a baseline (0.90 vs 0.95 test accuracy) while improving explanation quality, with Activation Precision increasing from 0.32 to 0.35.

## Method Summary
The eXBL method combines transfer learning with user-guided unlearning. It starts with a pre-trained MobileNetV2 backbone, freezes the first 50 layers, and adds a custom classifier. The model generates GradCAM explanations for all training images, then users select one good and one bad explanation. A triplet loss framework compares each image's explanation to these exemplars, minimizing distance to the good explanation and maximizing distance from the bad one. This explanation loss is combined with classification loss during training. The method was evaluated on a Covid-19 chest X-ray dataset with four categories, using 3,200 training images, 1,200 validation images, and 800 test images.

## Key Results
- Classification accuracy: eXBL achieved 0.90 vs baseline 0.95 on test set
- Activation Precision: Improved from 0.32 (baseline) to 0.35 with eXBL
- User burden: Only requires ranking two explanations vs annotating all training images
- Dataset: Covid-19 chest X-ray dataset with four categories (Normal, Covid, Lung Opacity, Viral Pneumonia)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a single pair of exemplary explanations (one good, one bad) is sufficient to guide the model away from spurious correlations.
- Mechanism: The triplet loss framework compares the GradCAM explanation of each training image against the exemplar good and bad explanations. By minimizing distance to the good explanation and maximizing distance from the bad, the model learns to focus on relevant regions (lungs/chest) and avoid confounding ones (epiphyses, background).
- Core assumption: The chosen good and bad explanations accurately represent the desired vs. undesired model behavior.
- Evidence anchors:
  - [abstract] "eXBL only asks users to identify one good and one bad explanation from two training instances."
  - [section] "We choose to use GradCAM model explanations because they have been found to be more sensitive to training label reshuffling and model parameter randomization than other saliency based explanations [1]"
  - [corpus] Found 25 related papers but none specifically validate that a single exemplar pair is sufficient for unlearning spurious correlations.
- Break condition: If the good/bad exemplar pair does not accurately represent the confounding problem, the model may learn to focus on irrelevant regions or fail to unlearn the spurious correlation.

### Mechanism 2
- Claim: Replacing expensive feature annotation with ranking two explanations reduces user burden while maintaining model performance.
- Mechanism: Instead of annotating all training images, users only need to rank two explanations. This ranking is incorporated via triplet loss into the explanation loss term, which trains the model to produce explanations similar to the good exemplar and dissimilar to the bad one.
- Core assumption: Users can reliably identify good and bad explanations through ranking without detailed feature annotation.
- Evidence anchors:
  - [abstract] "eXBL only asks users to identify one good and one bad explanation from two training instances."
  - [section] "This kind of user interaction where users are asked for a ranking instead of category labels has also been found to increase inter-rater reliability and data collection efficiency [11]."
  - [corpus] Limited direct evidence in corpus that ranking explanations improves inter-rater reliability in medical imaging contexts.
- Break condition: If users struggle to distinguish good from bad explanations, the ranking may introduce noise rather than useful signal.

### Mechanism 3
- Claim: Freezing the first 50 layers of MobileNetV2 preserves learned feature representations while allowing fine-tuning of later layers for unlearning spurious correlations.
- Mechanism: By freezing early layers and retraining later convolutional layers with the custom classifier, the model maintains general feature extraction capabilities while adapting the higher-level representations to focus on relevant regions.
- Core assumption: Early convolutional layers capture general features that are useful across tasks, while later layers are more task-specific and can be modified to unlearn spurious correlations.
- Evidence anchors:
  - [section] "We only freeze and reuse the first 50 layers of MobileNetV2 and retrain the rest of the convolutional layers with a custom classifier layer that we added"
  - [corpus] No direct evidence in corpus about layer freezing strategies for unlearning spurious correlations specifically.
- Break condition: If spurious correlations are encoded in early layers, freezing them may prevent effective unlearning.

## Foundational Learning

- Concept: GradCAM saliency maps for explanation
  - Why needed here: GradCAM provides interpretable visual explanations showing which image regions influence model predictions, enabling users to identify good vs. bad explanations.
  - Quick check question: How does GradCAM compute the importance of different image regions for a given prediction?

- Concept: Triplet loss for contrastive learning
  - Why needed here: Triplet loss enables the model to learn by comparing against positive (good) and negative (bad) examples, pushing explanations toward the good exemplar and away from the bad one.
  - Quick check question: What is the mathematical form of triplet loss and how does it encourage similarity to positive examples and dissimilarity to negative ones?

- Concept: Spurious correlations in medical imaging
  - Why needed here: Understanding how confounding factors like skeletal maturation can create spurious correlations is essential for recognizing when and why unlearning is necessary.
  - Quick check question: What are common sources of spurious correlations in chest X-ray datasets and how do they affect model performance?

## Architecture Onboarding

- Component map:
  Pre-trained MobileNetV2 backbone (first 50 layers frozen) -> Custom classifier layer (256 nodes with ReLU, 50% dropout, 4-node softmax) -> GradCAM explanation generator -> Triplet loss computation module -> Training loop with combined classification and explanation losses

- Critical path:
  1. Load pre-trained MobileNetV2 and freeze first 50 layers
  2. Add custom classifier and initialize randomly
  3. Generate GradCAM explanations for all training images
  4. Have user identify one good and one bad explanation
  5. Compute triplet loss comparing each image's explanation to exemplars
  6. Backpropagate combined loss and update model parameters

- Design tradeoffs:
  - Freezing vs. fine-tuning early layers: Freezing preserves general features but may retain spurious correlations in early layers
  - Number of exemplars: Single pair reduces user burden but may provide insufficient signal for complex unlearning tasks
  - Loss weighting: Balancing classification accuracy against explanation quality requires careful hyperparameter tuning

- Failure signatures:
  - Explanations still focus on confounding regions despite training
  - Classification accuracy drops significantly after unlearning
  - Model overfits to the specific good/bad exemplars rather than generalizing
  - GradCAM explanations become noisy or uninformative

- First 3 experiments:
  1. Train with only classification loss (no explanation loss) to establish baseline
  2. Train with explanation loss but random good/bad exemplars to test for overfitting
  3. Train with explanation loss and manually selected good/bad exemplars to validate unlearning effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the selection of good and bad explanations impact the performance of eXBL in terms of classification accuracy and explanation quality?
- Basis in paper: [explicit] The paper mentions that the authors observed a classification performance loss when retraining the Unrefined model with eXBL to produce good explanations, and suggests that the selection of good and bad explanations may not have been optimal.
- Why unresolved: The paper does not provide a detailed analysis of how different selections of good and bad explanations affect the performance of eXBL.
- What evidence would resolve it: Conducting experiments with different selections of good and bad explanations and comparing their impact on classification accuracy and explanation quality.

### Open Question 2
- Question: How can the effectiveness of eXBL be evaluated beyond using masks of lungs in the employed dataset?
- Basis in paper: [explicit] The paper states that the generated explanations were evaluated using masks of lungs because it is the only body part with pixel-level annotation in the employed dataset. However, it suggests that future work should involve expert end users for evaluation of the classification and model explanations.
- Why unresolved: The paper does not explore alternative evaluation methods or involve expert end users in the evaluation process.
- What evidence would resolve it: Conducting experiments with expert end users and exploring alternative evaluation methods that go beyond using masks of lungs.

### Open Question 3
- Question: How does eXBL perform when applied to other medical or non-medical datasets?
- Basis in paper: [explicit] The paper mentions that the authors believe eXBL could be easily adopted to other medical or non-medical datasets because it only needs two non-demanding exemplary explanations as user feedback.
- Why unresolved: The paper only demonstrates the effectiveness of eXBL on a Covid-19 chest X-ray dataset and does not explore its performance on other datasets.
- What evidence would resolve it: Conducting experiments with different medical and non-medical datasets to evaluate the performance of eXBL in various contexts.

## Limitations

- The selection process for exemplary explanations lacks precise criteria beyond manual identification based on visual inspection
- Limited validation that a single pair of exemplars is sufficient for effective unlearning across diverse spurious correlation scenarios
- Unclear generalization to other medical imaging tasks beyond Covid-19 chest X-ray classification

## Confidence

- High confidence in the mechanism of using triplet loss with exemplar explanations to guide unlearning
- Medium confidence in the sufficiency of a single exemplar pair for effective unlearning
- Medium confidence in the claim that user burden is significantly reduced compared to full feature annotation

## Next Checks

1. Test eXBL with multiple exemplar pairs to determine the minimum effective number required for reliable unlearning
2. Validate the method on additional medical imaging datasets with known confounding factors to assess generalizability
3. Conduct ablation studies removing the explanation loss component to quantify its contribution to unlearning spurious correlations