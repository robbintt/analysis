---
ver: rpa2
title: Decomposing Global Feature Effects Based on Feature Interactions
arxiv_id: '2306.00541'
source_url: https://arxiv.org/abs/2306.00541
tags:
- feature
- features
- effect
- effects
- gadget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GADGET, a new framework that partitions the
  feature space into interpretable regions to minimize feature interactions and decompose
  joint effects into main effects. The method applies to PD, ALE, and SD plots by
  defining local feature effects that satisfy the local decomposability axiom.
---

# Decomposing Global Feature Effects Based on Feature Interactions

## Quick Facts
- arXiv ID: 2306.00541
- Source URL: https://arxiv.org/abs/2306.00541
- Reference count: 18
- Key outcome: Introduces GADGET framework that partitions feature space to minimize interactions and decompose joint effects into main effects, achieving up to 0.99 R²ₜₒₜ reduction in interaction-related heterogeneity

## Executive Summary
This paper introduces GADGET, a novel framework for decomposing global feature effects into interpretable regions by minimizing feature interactions. The method applies to PD, ALE, and SD plots through recursive partitioning that finds subspaces where joint feature effects can be decomposed into individual univariate effects. A key contribution is PINT, a permutation-based test for detecting significant feature interactions. The framework demonstrates strong performance in reducing interaction-related heterogeneity across synthetic and real-world datasets, with methods using conditional distributions (ALE) showing more stability than marginal ones (PD/SD) when features are correlated.

## Method Summary
GADGET partitions the feature space using a recursive algorithm that greedily selects splits to minimize the variance of local feature effects within each subspace. The framework is built on the local decomposability axiom, which allows joint effects to be decomposed into main and interaction terms within each region. PINT complements this by identifying significant interacting features through permutation testing. The method supports multiple feature effect techniques including PD, ALE, and SD, with the choice affecting performance particularly in correlated feature settings.

## Key Results
- GADGET achieves up to 0.99 reduction in interaction-related heterogeneity (R²ₜₒₜ) in simulations
- ALE-based methods show greater stability than PD/SD when features are correlated due to conditional vs marginal distributions
- Real-world applications reveal potential biases in COMPAS predictions and explain complex hour-of-day effects in bikesharing data
- PINT effectively identifies significant feature interactions, outperforming the H-statistic in certain scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GADGET partitions feature space to minimize interaction-related heterogeneity, enabling additive decomposition of joint effects into main effects
- Mechanism: Uses recursive partitioning algorithm that greedily selects split features and split points to minimize variance of local feature effects within each subspace
- Core assumption: Local feature effect function satisfies the local decomposability axiom, meaning individual feature effects can be decomposed into main and interaction terms
- Evidence anchors:
  - [abstract] "GADGET finds regions where the joint feature effect can be decomposed into individual univariate effects"
  - [section 4.1] "The risk function R of the j-th feature and subspace Ag is defined by aggregating the point-wise loss of Eq. (10) over a sample of feature values of xj"
  - [corpus] Weak evidence - only general ML/explainability papers found
- Break condition: If local decomposability axiom is violated or if features interact in ways that cannot be captured by partitioning.

### Mechanism 2
- Claim: PINT algorithm identifies significant interacting features by comparing observed heterogeneity to permutation-based null distribution
- Mechanism: Permutes target variable to break feature-target relationships while preserving feature-feature correlations, then calculates risk values to establish null distribution for statistical testing
- Core assumption: Heterogeneity reduction due to feature interactions will be significantly larger than heterogeneity due to random noise or correlations
- Evidence anchors:
  - [section 5] "The goal of PINT... is to define the feature subset S that contains all features that significantly interact with each other"
  - [section 5] "We calculate PINT using PD, ALE, and SD for each repetition and sample size with s = 100 and α = 0.05"
  - [corpus] Weak evidence - only general ML/explainability papers found
- Break condition: If null distribution is not well-estimated or if significance level is set too high/low.

### Mechanism 3
- Claim: Different feature effect methods (PD, ALE, SD) have varying sensitivities to feature correlations and interaction orders, affecting GADGET's performance
- Mechanism: PD and SD use marginal distributions leading to extrapolation in correlated regions, while ALE uses conditional distributions avoiding this issue. SD assigns decreasing weights to higher-order interactions compared to equal weighting in PD/ALE
- Evidence anchors:
  - [section 6.1] "For increasing correlation between features, due to extrapolation, we receive results that are less stable for methods using the marginal distribution"
  - [section 6.2] "While SD puts less weight on interactions with increasing order, all interactions (independent of the order) receive the same weight in PD and ALE"
  - [corpus] Weak evidence - only general ML/explainability papers found
- Break condition: If correlation structure is not properly accounted for or if interaction order detection is critical.

## Foundational Learning

- Functional ANOVA decomposition
  - Why needed here: Provides mathematical foundation for understanding how prediction functions can be decomposed into main and interaction effects
  - Quick check question: Can you explain the difference between standard and generalized functional ANOVA decomposition?

- Recursive partitioning algorithms
  - Why needed here: Core algorithmic approach used by GADGET to find interpretable regions in feature space
  - Quick check question: How does the CART algorithm differ from other decision tree algorithms?

- Shapley values and game theory
  - Why needed here: SD plots are based on Shapley values, which provide local feature effect explanations
  - Quick check question: What is the key difference between interventional and observational approaches for calculating Shapley values?

## Architecture Onboarding

- Component map: Local feature effect functions (ICE, derivatives, Shapley values) -> Risk calculation and aggregation -> Recursive partitioning engine -> PINT significance testing module -> Visualization and decomposition utilities

- Critical path: PINT → Feature subset selection → GADGET partitioning → Regional effect estimation → Visualization

- Design tradeoffs:
  - Computational cost vs. accuracy in PINT permutations
  - Depth of partitioning vs. interpretability
  - Choice of feature effect method vs. correlation handling
  - Recalculation of Shapley values vs. approximation speed

- Failure signatures:
  - High R² values but poor visual interpretability
  - Inconsistent split features across repetitions
  - Computational timeouts during permutation testing
  - Unexpected extrapolation in PD-based visualizations

- First 3 experiments:
  1. Run GADGET on simple simulation example with known interactions using PD, ALE, and SD methods
  2. Apply PINT to detect interacting features in a correlated feature setting
  3. Test computational performance with increasing number of permutations in PINT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different feature effect methods (PD, ALE, SD) compare in their ability to detect and quantify higher-order feature interactions when applied within GADGET?
- Basis in paper: [explicit] The paper discusses how different feature effect methods handle higher-order interactions differently, particularly noting that SD puts less weight on higher-order interactions compared to PD and ALE
- Why unresolved: The paper shows differences in performance for a specific simulation example, but a comprehensive comparison across various data types and interaction structures is not provided
- What evidence would resolve it: Empirical results showing the performance of GADGET with different feature effect methods across a wide range of simulation settings with varying interaction structures and data characteristics

### Open Question 2
- Question: What is the impact of the choice of hyperparameters (e.g., number of permutations s, significance level α) in PINT on the detection of significant feature interactions and the resulting performance of GADGET?
- Basis in paper: [explicit] The paper discusses the hyperparameters of PINT (number of permutations s and significance level α) and their impact on the detection of significant feature interactions
- Why unresolved: The paper provides some guidance on choosing these hyperparameters but does not offer a systematic study of their impact on the performance of GADGET
- What evidence would resolve it: A sensitivity analysis of PINT's performance and the resulting GADGET performance across different hyperparameter settings and data characteristics

### Open Question 3
- Question: How does the computational complexity of GADGET with different feature effect methods scale with the number of features and observations, and what are the practical implications for real-world applications?
- Basis in paper: [inferred] The paper discusses the computational complexity of different feature effect methods and the potential trade-offs between accuracy and efficiency, particularly in high-dimensional settings
- Why unresolved: While the paper mentions computational considerations, it does not provide a detailed analysis of the scaling behavior of GADGET with different feature effect methods and data sizes
- What evidence would resolve it: Empirical results showing the computational time and memory usage of GADGET with different feature effect methods across datasets of varying sizes and dimensionality

## Limitations
- The method's effectiveness heavily depends on hyperparameter choices that are not fully specified in the paper
- Computational scalability for large-scale real-world applications with many permutations remains unclear
- The local decomposability axiom may not hold for all ML models, particularly those with complex non-linear interactions
- Performance relies on specific synthetic datasets and implementation details, raising questions about generalizability

## Confidence

- **High Confidence**: The theoretical framework for local decomposability and interaction decomposition (Section 4.1-4.3). The mathematical properties and axioms are well-defined and internally consistent.
- **Medium Confidence**: The empirical results showing R²ₜₒₜ values up to 0.99 in simulations and the comparative analysis between PD, ALE, and SD methods. While results are promising, they rely on specific synthetic datasets and implementation details.
- **Low Confidence**: The scalability and computational efficiency claims for large-scale real-world applications. The paper doesn't provide detailed runtime analysis or memory usage information for the PINT algorithm with large permutation counts.

## Next Checks

1. **Computational Performance Validation**: Implement timing tests for the PINT algorithm with varying numbers of permutations (s = 50, 100, 200, 500) on datasets of increasing size (N = 1000, 10000, 100000) to establish computational complexity and identify bottlenecks.

2. **Generalizability Testing**: Apply GADGET to diverse ML models beyond those tested (e.g., decision trees, linear models, transformer-based models) and datasets from different domains (medical imaging, text classification, tabular data) to validate robustness across model architectures and data types.

3. **Alternative Risk Functions**: Test alternative risk functions beyond variance-based measures, such as mean absolute deviation or information-theoretic measures, to assess sensitivity to the choice of heterogeneity metric and identify scenarios where different risk functions might be more appropriate.