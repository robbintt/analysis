---
ver: rpa2
title: Is Solving Graph Neural Tangent Kernel Equivalent to Training Graph Neural
  Network?
arxiv_id: '2309.07452'
source_url: https://arxiv.org/abs/2309.07452
tags:
- test
- graph
- neural
- ugnn
- ugntk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes theoretical equivalence between Graph Neural
  Tangent Kernel (GNTK) regression and training infinite-width Graph Neural Networks
  (GNNs) for both graph-level and node-level regression tasks. The authors prove that
  solving GNTK regression is mathematically equivalent to using gradient descent to
  train infinitely-wide multi-layer GNNs.
---

# Is Solving Graph Neural Tangent Kernel Equivalent to Training Graph Neural Network?

## Quick Facts
- arXiv ID: 2309.07452
- Source URL: https://arxiv.org/abs/2309.07452
- Authors: 
- Reference count: 40
- This paper establishes theoretical equivalence between Graph Neural Tangent Kernel (GNTK) regression and training infinite-width Graph Neural Networks (GNNs) for both graph-level and node-level regression tasks.

## Executive Summary
This paper proves that solving Graph Neural Tangent Kernel (GNTK) regression is mathematically equivalent to using gradient descent to train infinitely-wide multi-layer Graph Neural Networks (GNNs). The key insight is introducing an iterative GNTK regression process that bridges the gap between the single-step kernel method and multi-step GNN training. By leveraging linear convergence properties and an "almost invariant" property of Graph Dynamic Kernel, the authors show that prediction differences between GNTK and GNN can be made arbitrarily small with sufficient width and training iterations.

## Method Summary
The paper establishes theoretical equivalence between GNTK regression and training infinite-width GNNs through an iterative regression framework. The method involves initializing both GNTK regression and GNN training with random weights, then iteratively updating both using gradient descent. The convergence is analyzed by bounding the prediction differences using linear convergence properties and the almost invariant property of the Graph Dynamic Kernel. The analysis shows that with sufficient width (Ω(N²) for graph-level, Ω(N¹⁰) for node-level) and training iterations (O(N⁴) for graph-level, O(N²) for node-level), the predictions from both methods converge.

## Key Results
- Proves equivalence between GNTK regression and infinite-width GNN training for graph-level regression
- Establishes required width bounds of Ω(N²) and training iterations of O(N⁴) for graph-level tasks
- Extends results to node-level regression with width requirements of Ω(N¹⁰) and iterations of O(N²)
- Introduces iterative GNTK regression as a bridge between kernel methods and neural network training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Infinite-width GNNs converge to the same predictions as GNTK regression when trained with gradient descent
- Mechanism: The iterative GNTK regression process bridges the gap between single-step kernel regression and multi-step GNN training. As width approaches infinity, the Graph Dynamic Kernel becomes "almost invariant" during training, making GNTK and GNN predictions converge
- Core assumption: Graph Dynamic Kernel becomes constant during training when width approaches infinity (almost invariant property)
- Evidence anchors:
  - [abstract]: "By leveraging linear convergence properties and an 'almost invariant' property of Graph Dynamic Kernel, the authors show that prediction differences between GNTK and GNN can be made arbitrarily small"
  - [section 5]: "The high-level idea is that we use an iterative regression to bridge the connection between the neural network and the kernel method"
  - [corpus]: Weak evidence - corpus lacks direct support for almost invariant property claim
- Break condition: Width not sufficiently large (width must be Ω(N²) for graph-level, Ω(N¹⁰) for node-level)

### Mechanism 2
- Claim: GNTK regression and GNN training both exhibit linear convergence to their respective optima
- Mechanism: Both methods minimize similar loss functions with similar gradients. The gradient flow analysis shows both converge linearly with rate determined by the smallest eigenvalue of the kernel matrix (Λ₀)
- Core assumption: Strong convexity of the loss functions with respect to the kernel matrix eigenvalues
- Evidence anchors:
  - [section D.2]: "The linear convergence can be proved by calculating the gradient flow of GNTK regression and GNN training process"
  - [section D.3]: "Due to the strongly convexity [LSS+20], the gradient flow converges to the optimal solution"
  - [corpus]: No direct corpus evidence for strong convexity in graph setting
- Break condition: Kernel matrix not positive definite (Λ₀ ≤ 0)

### Mechanism 3
- Claim: Node-level GNTK can be computed recursively using covariance matrices of Gaussian processes
- Mechanism: In the infinite width limit, pre-activations become i.i.d. centered Gaussian processes. The covariance matrices can be computed recursively through Aggregate and Combine layers, enabling exact GNTK computation
- Core assumption: Infinite width leads to Gaussian process behavior of pre-activations
- Evidence anchors:
  - [section C]: "In the infinite width limit, the pre-activations... have all its coordinates tending to i.i.d. centered Gaussian processes"
  - [section D.9]: "We provide the first GNTK formulation for node-level regression"
  - [corpus]: No corpus evidence for recursive covariance computation in graph neural networks
- Break condition: Width not approaching infinity

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its equivalence to infinite-width neural network training
  - Why needed here: GNTK is the graph extension of NTK; understanding NTK equivalence is prerequisite to understanding GNTK equivalence
  - Quick check question: What is the key property that makes NTK equivalent to training an infinite-width neural network?

- Concept: Graph Neural Networks (GNNs) and their layer operations (Aggregate, Combine, ReadOut)
  - Why needed here: The paper proves equivalence between GNTK and GNN training; understanding GNN architecture is essential
  - Quick check question: How do the Aggregate and Combine operations differ from standard neural network layers?

- Concept: Kernel methods and kernel regression
  - Why needed here: GNTK is a kernel method; the paper compares GNTK regression to GNN training
  - Quick check question: What is the relationship between a kernel method and its corresponding feature function?

## Architecture Onboarding

- Component map: 
  - GNN: Aggregate layer → Combine layer → ReadOut layer
  - GNTK: Feature function Φ(G) → Kernel matrix Hcts → Kernel regression
  - Bridge: Iterative GNTK regression with gradient descent

- Critical path: 
  1. Initialize GNN with random weights
  2. Train GNN with gradient descent for T iterations
  3. Simultaneously, initialize iterative GNTK regression at β(0)=0
  4. Update iterative GNTK regression with gradient descent
  5. Compare predictions at iteration T

- Design tradeoffs:
  - Width vs. accuracy: Larger width needed for better approximation (Ω(N²) for graph-level, Ω(N¹⁰) for node-level)
  - Iterations vs. accuracy: More iterations needed for better approximation (O(N⁴) for graph-level, O(N²) for node-level)
  - Computational cost: GNTK requires O(n²) kernel computations vs. O(n) parameter updates for GNN

- Failure signatures:
  - Divergence: Λ₀ ≤ 0 (kernel matrix not positive definite)
  - Slow convergence: Width too small or iterations too few
  - Numerical instability: Poor conditioning of kernel matrix

- First 3 experiments:
  1. Implement single-layer GNN with Aggregate and Combine operations on small graphs
  2. Implement GNTK computation for graph-level regression using recursive covariance formulation
  3. Verify equivalence for small graphs with N=2-3 nodes, comparing GNN and GNTK predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between the graph neural tangent kernel (GNTK) and infinite-width graph neural networks (GNNs) for graph-level and node-level regression tasks?
- Basis in paper: The paper establishes theoretical equivalence between GNTK regression and training infinite-width GNNs for both graph-level and node-level regression tasks.
- Why unresolved: The paper proves the equivalence but does not provide a detailed comparison of the prediction differences between GNTK and GNN.
- What evidence would resolve it: Experimental results comparing the prediction accuracy of GNTK and infinite-width GNNs on graph-level and node-level regression tasks.

### Open Question 2
- Question: What is the impact of the number of nodes in a graph on the required width of the neural network and the number of training iterations for achieving equivalence between GNTK and GNN?
- Basis in paper: The paper shows that the width of the neural network depends on N^2 for graph-level regression and N^10 for node-level regression, where N is the number of nodes.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between the number of nodes and the required width and training iterations.
- What evidence would resolve it: Experimental results varying the number of nodes in the graph and measuring the required width and training iterations for achieving equivalence between GNTK and GNN.

### Open Question 3
- Question: What is the effect of the generalized δ-separation assumption on the spectral gap of the shifted GNTK?
- Basis in paper: The paper introduces the generalized δ-separation assumption and proves a lower bound on the spectral gap of the shifted GNTK.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between the δ-separation assumption and the spectral gap of the shifted GNTK.
- What evidence would resolve it: Experimental results varying the δ-separation assumption and measuring the spectral gap of the shifted GNTK.

## Limitations
- The theoretical analysis relies on infinite-width limit assumptions that may not fully capture practical finite-width scenarios
- Required width bounds (Ω(N²) for graph-level, Ω(N¹⁰) for node-level) are polynomial but potentially impractical for large graphs
- The paper focuses exclusively on regression tasks and does not address classification or other learning paradigms

## Confidence
- High confidence: The linear convergence of both GNTK regression and GNN training processes is well-established through gradient flow analysis and strong convexity arguments
- Medium confidence: The equivalence between GNTK and infinite-width GNN training holds under the stated conditions, though practical verification requires extensive computational resources
- Low confidence: The specific width and iteration bounds (Ω(N²), Ω(N¹⁰), O(N⁴), O(N²)) may not be tight, and the "almost invariant" property requires further empirical validation

## Next Checks
1. **Empirical width scaling study**: Systematically evaluate the prediction difference between GNTK and finite-width GNNs across varying graph sizes N to empirically validate the stated width requirements
2. **Convergence rate verification**: Measure actual convergence rates of both GNTK regression and GNN training on synthetic and real graph datasets to confirm linear convergence and identify potential deviations
3. **Almost invariant property testing**: Design experiments to quantify how much the Graph Dynamic Kernel changes during training for different width regimes, directly testing the validity of the "almost invariant" assumption