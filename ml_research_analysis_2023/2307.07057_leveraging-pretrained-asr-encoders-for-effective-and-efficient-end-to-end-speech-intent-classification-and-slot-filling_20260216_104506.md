---
ver: rpa2
title: Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech
  Intent Classification and Slot Filling
arxiv_id: '2307.07057'
source_url: https://arxiv.org/abs/2307.07057
tags:
- encoder
- speech
- slurp
- performance
- pretrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates end-to-end speech intent classification
  and slot filling (SICSF) using an encoder pretrained on speech recognition (ASR)
  to initialize a Conformer-Transformer model. The approach achieves state-of-the-art
  results on the SLURP dataset, with 90.14% intent accuracy and 82.27% SLURP-F1.
---

# Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling

## Quick Facts
- arXiv ID: 2307.07057
- Source URL: https://arxiv.org/abs/2307.07057
- Reference count: 0
- Key outcome: ASR-pretrained Conformer-Transformer achieves 90.14% intent accuracy and 82.27% SLURP-F1 on SLURP dataset, outperforming SSL-pretrained encoders and cascading models.

## Executive Summary
This paper investigates end-to-end speech intent classification and slot filling (SICSF) by initializing a Conformer-Transformer model with an encoder pretrained on speech recognition (ASR). The approach achieves state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and 82.27% SLURP-F1. The paper demonstrates that ASR pretraining is significantly more effective than self-supervised learning (SSL) pretraining for SICSF, and shows that parameter efficiency can be achieved by freezing the ASR-pretrained encoder and adding Adapter modules. The model is the first E2E approach to match the performance of cascading models with oracle ASR.

## Method Summary
The method uses a Conformer encoder (pretrained on ASR) and a 3-layer Transformer decoder to perform end-to-end speech intent classification and slot filling. The semantic output is formatted as a flattened Python string and tokenized using SentencePiece with vocabulary size 58. Training uses negative log-likelihood loss with teacher-forcing, Adam optimizer (momentum [0.9, 0.98], no weight decay), learning rates (encoder: 2e-4, decoder: 3e-4), cosine annealing scheduler with 2000 warm-up steps, batch size 32, and beam search inference with width 32 and temperature 1.25. Parameter efficiency is explored by freezing the ASR-pretrained encoder and adding Adapter modules.

## Key Results
- ASR-pretrained encoder achieves 90.14% intent accuracy and 82.27% SLURP-F1 on SLURP dataset
- SSL-pretrained encoders (HuBERT, Wav2Vec2) achieve only 43.28% intent accuracy with the same architecture
- Freezing ASR-pretrained encoder with Adapters maintains 72.59% SLURP-F1 while reducing trainable parameters
- E2E model outperforms cascading models (ASR+NLU) unless oracle ASR is provided

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASR-pretrained encoders provide better feature representations for SICSF than SSL-pretrained encoders because ASR and SICSF share a closer task objective (audio-to-text mapping).
- Mechanism: The ASR-pretrained encoder learns to map audio frames to text tokens, which aligns closely with SICSF's goal of mapping audio to semantic strings. SSL encoders, however, focus on frame discrimination and contrastive learning, which does not directly align with the semantic mapping required for SICSF.
- Core assumption: The quality of learned features is strongly influenced by the similarity of the pretraining task to the downstream task.
- Evidence anchors:
  - [abstract] "We compare our model with encoders pretrained on self-supervised learning (SSL), and show that ASR pretraining is much more effective than SSL for SICSF."
  - [section 3.5.1] "The reason for the similar performance is that the encoder pretrained on large ASR datasets already has the knowledge of audio-to-text task."
  - [corpus] Weak evidence: Corpus contains no direct comparison of ASR vs SSL pretraining effectiveness for SICSF.
- Break condition: If the SICSF task shifts to rely more on acoustic characteristics than semantic mapping (e.g., emotion recognition), SSL pretraining might become more effective.

### Mechanism 2
- Claim: Freezing the ASR-pretrained encoder and adding Adapter modules maintains high performance while significantly reducing trainable parameters.
- Mechanism: The ASR-pretrained encoder already contains rich, task-relevant features, so freezing it preserves these representations. Adapters allow fine-tuning task-specific transformations without altering the frozen backbone, enabling efficient adaptation to SICSF.
- Core assumption: The frozen encoder's features are sufficiently general and high-quality to support the downstream task without further adaptation.
- Evidence anchors:
  - [section 3.4] "For the SSL-pretrained encoder, merely freezing it without adding Adapters leads to very low performance... By adding about 1M parameters, Adapters are able to improve the performance of SSl-pretrained encoders from 36.21% to 43.28%."
  - [section 3.4] "Merely freezing the ASR-pretrained encoder has 72.59% SLURP-F1, which is almost double the performance of SSL-pretrained encoder."
  - [corpus] Weak evidence: Corpus does not contain information about parameter efficiency or adapter usage in SICSF.
- Break condition: If the downstream task has very different feature requirements (e.g., requires different frequency resolution), freezing may hurt performance.

### Mechanism 3
- Claim: The Conformer-Transformer architecture is effective for SICSF because it combines local acoustic patterns (Conformer) with global context modeling (Transformer decoder).
- Mechanism: The Conformer encoder captures both local and global acoustic patterns through its convolution modules and self-attention, while the Transformer decoder uses full sequence attention to generate the semantic output without monotonic constraints.
- Core assumption: SICSF benefits from both local acoustic cues and global semantic context, and does not require monotonic alignment between input and output.
- Evidence anchors:
  - [section 2.1] "We choose Conformer [14] as the encoder since it is one of most popular ASR architectures used in production... we choose Transformer [18] as the decoder... since it has the best global context."
  - [section 2.1] "Unlike the ASR task that requires monotonic input-output alignment, the speech intent classification and slot filling task is not affected by the orders of predicted entities and thus doesn't require such monotonic property."
  - [corpus] Weak evidence: Corpus contains no information about the specific architectural choices or their effectiveness for SICSF.
- Break condition: If the task requires strict monotonic alignment (e.g., phoneme-level labeling), a monotonic decoder like CTC might be more appropriate.

## Foundational Learning

- Concept: End-to-end sequence-to-sequence modeling
  - Why needed here: The SICSF task is formulated as mapping audio sequences to semantic text sequences, requiring an encoder-decoder framework.
  - Quick check question: What is the key difference between monotonic (CTC/RNNT) and non-monotonic (Transformer) decoders in the context of SICSF?

- Concept: Pretraining task alignment
  - Why needed here: The effectiveness of pretrained encoders depends on how closely their pretraining objectives match the downstream task objectives.
  - Quick check question: Why might an ASR-pretrained encoder be more effective than an SSL-pretrained encoder for a task that involves mapping audio to text?

- Concept: Parameter efficiency through adapter modules
  - Why needed here: Adapters allow efficient adaptation of large pretrained models by adding small trainable modules while keeping most parameters frozen.
  - Quick check question: How do adapter modules enable parameter-efficient transfer learning while preserving the knowledge in a frozen encoder?

## Architecture Onboarding

- Component map: Audio → Conformer encoder → (Adapter modules) → Transformer decoder → Semantic string
- Critical path: Audio → Conformer encoder → (Adapter modules) → Transformer decoder → Semantic string
- Design tradeoffs:
  - ASR pretraining vs SSL pretraining: ASR pretraining provides better task alignment but may require larger datasets; SSL pretraining is more general but less effective for audio-to-text tasks.
  - Frozen encoder vs full finetuning: Freezing with adapters provides parameter efficiency but may limit adaptation; full finetuning provides maximum performance but at higher computational cost.
  - Small vocabulary (58) vs large vocabulary (512+): Small vocabulary works better for the semantic output space but may be less effective for ASR in cascading models.
- Failure signatures:
  - Poor intent accuracy and SLURP-F1: Likely issues with encoder pretraining, vocabulary size, or decoder architecture.
  - Good intent accuracy but poor slot filling: Likely issues with the semantic string formatting or tokenizer vocabulary.
  - Very low performance with frozen SSL encoder: Indicates the SSL features are not transferable without fine-tuning.
- First 3 experiments:
  1. Compare ASR-pretrained vs SSL-pretrained encoders with the same architecture to validate the pretraining effectiveness claim.
  2. Test frozen vs fine-tuned encoders to quantify the parameter efficiency tradeoff.
  3. Vary the output vocabulary size (58, 256, 512, 1024) to find the optimal setting for semantic string generation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which ASR pretraining leads to superior performance compared to SSL pretraining in SICSF tasks?
- Basis in paper: [explicit] The paper demonstrates that ASR-pretrained encoders achieve better results than SSL-pretrained encoders, but the underlying reasons are not fully explored.
- Why unresolved: While the paper hypothesizes that the similarity between ASR and SICSF tasks contributes to this advantage, it does not provide a detailed analysis of the specific features or representations that make ASR pretraining more effective.
- What evidence would resolve it: Detailed ablation studies or analyses comparing the learned representations from ASR and SSL pretraining, focusing on their applicability to SICSF tasks, could provide insights into the mechanisms at play.

### Open Question 2
- Question: How does the choice of vocabulary size impact the performance of the proposed model in different SICSF scenarios?
- Basis in paper: [explicit] The paper investigates the effect of vocabulary size on model performance, showing that smaller vocabularies can sometimes yield better results, which is contrary to the behavior observed in cascading models.
- Why unresolved: The study provides initial insights but does not explore the full range of vocabulary sizes or different SICSF scenarios that might benefit from varying vocabulary sizes.
- What evidence would resolve it: Comprehensive experiments testing a wider range of vocabulary sizes across diverse SICSF scenarios, including intent classification and slot filling, would clarify the optimal vocabulary size for different tasks.

### Open Question 3
- Question: What are the trade-offs between parameter efficiency and model performance when using Adapter modules in frozen ASR-pretrained encoders?
- Basis in paper: [explicit] The paper explores the use of Adapter modules to enhance parameter efficiency, showing that they can improve performance while reducing the number of trainable parameters, but does not fully analyze the trade-offs involved.
- Why unresolved: While the paper demonstrates the benefits of using Adapter modules, it does not provide a detailed analysis of how different configurations of Adapter modules affect the balance between efficiency and performance.
- What evidence would resolve it: Systematic experiments varying the configuration of Adapter modules, such as the number and size of adapters, could provide insights into the optimal trade-off between parameter efficiency and model performance.

## Limitations

- The comparison with SSL-pretrained encoders only shows performance differences at fixed architectures and training budgets, without exploring whether SSL encoders could achieve comparable results with additional pretraining or architectural modifications.
- The cascading model comparison uses an ASR model trained on the same data as the E2E model, which may not represent realistic deployment scenarios where ASR and NLU are trained separately.
- The adapter effectiveness claim relies solely on the ASR-pretrained encoder case, with no ablation showing whether adapters provide any benefit for the SSL-pretrained encoder beyond enabling training.

## Confidence

**High Confidence**: The ASR-pretrained encoder significantly outperforms SSL-pretrained encoders for SICSF on the SLURP dataset. The experimental results showing 90.14% vs 43.28% intent accuracy are robust and well-supported by the data.

**Medium Confidence**: Freezing the ASR-pretrained encoder with adapter modules provides parameter efficiency while maintaining performance. While the results show this works, the comparison is limited to only ASR vs SSL cases, and the underlying mechanism (why SSL encoders fail when frozen) is not fully explored.

**Low Confidence**: The claim that E2E models are better than cascading models "unless an oracle ASR model is provided." This conclusion depends heavily on the specific ASR model quality used in the cascading baseline, and the paper doesn't explore whether better ASR models would change this conclusion.

## Next Checks

1. **SSL Pretraining Scaling Study**: Train the SSL-pretrained encoders (HuBERT, Wav2Vec2) with increased pretraining epochs and larger batch sizes to determine if the performance gap with ASR pretraining is due to insufficient SSL pretraining rather than fundamental task misalignment.

2. **Cascading Model Sensitivity Analysis**: Replace the ASR component in the cascading model with multiple state-of-the-art ASR systems (including those trained on much larger datasets) to determine how sensitive the "E2E vs cascading" conclusion is to ASR quality.

3. **Adapter Module Ablation**: Systematically remove adapter modules from the frozen ASR-pretrained encoder to quantify their actual contribution to performance, and explore whether different adapter architectures or placement strategies could improve SSL-pretrained encoder performance when frozen.