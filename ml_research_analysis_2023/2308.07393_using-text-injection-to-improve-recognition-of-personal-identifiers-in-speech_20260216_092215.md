---
ver: rpa2
title: Using Text Injection to Improve Recognition of Personal Identifiers in Speech
arxiv_id: '2308.07393'
source_url: https://arxiv.org/abs/2308.07393
tags:
- speech
- text
- data
- training
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a method to improve Automatic Speech Recognition
  (ASR) performance on personal identifiers like names and dates in medical notes
  and other sensitive contexts. Their approach uses text-injection to augment ASR
  training with fake textual substitutes for Personally Identifiable Information (PII)
  tokens, without needing to collect or transcribe sensitive audio data.
---

# Using Text Injection to Improve Recognition of Personal Identifiers in Speech

## Quick Facts
- **arXiv ID**: 2308.07393
- **Source URL**: https://arxiv.org/abs/2308.07393
- **Reference count**: 0
- **Key outcome**: Text injection method substantially improves PII recognition accuracy in ASR while also improving overall ASR performance, including 8-13% gains in names/dates recall in medical notes and 3.2% improvement in Sentence Accuracy for alphanumeric/digit sequences.

## Executive Summary
This paper presents a text-injection approach to improve Automatic Speech Recognition (ASR) performance on personal identifiers like names and dates in sensitive contexts such as medical notes. The method uses fake textual substitutes for Personally Identifiable Information (PII) tokens to augment ASR training data without requiring collection or transcription of sensitive audio. The approach shows substantial improvements in PII recognition accuracy while also improving overall ASR performance, demonstrating 8-13% gains in names/dates recall and 3.2% improvement in Sentence Accuracy for alphanumeric sequences.

## Method Summary
The authors develop a text-injection method that trains ASR models to recognize PII categories by including fake textual substitutes of PII in the training data. The approach uses a hybrid architecture combining speech and text encoders with a shared conformer-based encoder, trained using curriculum learning that begins with speech-text pairs before adding unpaired text data. The text encoder includes conformer layers and lightweight convolutional upsampling to align text embeddings temporally with speech through a duration model. The method enables effective use of unpaired text data for domain adaptation while preserving privacy by avoiding real PII data.

## Key Results
- 8-13% improvements in names and dates recall for medical notes
- 3.2% improvement in Sentence Accuracy for alphanumeric/digit sequences
- Overall WER improvements while maintaining PII recognition gains
- Effective domain adaptation to medical speech without requiring sensitive audio data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text injection of fake PII substitutes trains the model to recognize category patterns without memorizing real private data.
- Mechanism: The text encoder learns embeddings for synthetic PII patterns that are aligned with speech encoder features via consistency loss and duration modeling. This alignment allows the model to map novel but structurally similar PII instances to correct outputs during inference.
- Core assumption: The synthetic text substitutes are structurally and linguistically similar enough to real PII to transfer recognition patterns.
- Evidence anchors: Abstract states fake textual substitutes are used; Medical Text dataset contains redacted identifiers replaced with random data.
- Break condition: If synthetic PII generation produces patterns too different from real PII, learned embeddings won't generalize.

### Mechanism 2
- Claim: Curriculum-based training (speech-text first, then adding text-only loss) stabilizes modality alignment before introducing unpaired text.
- Mechanism: Initial speech-text training ensures the shared encoder learns consistent speech-text mappings. Adding text-only masked language modeling afterward fine-tunes the text encoder's representations without destabilizing the speech-to-text alignment.
- Core assumption: Modality consistency established early prevents catastrophic forgetting when unpaired text is introduced.
- Evidence anchors: Section describes curriculum training; includes supervised data during training to stabilize model behavior.
- Break condition: If text-only loss dominates too early, speech-text alignment may degrade, harming recognition accuracy.

- Claim: The duration model up-samples text embeddings to match speech temporal dynamics, enabling effective injection of purely textual data.
- Mechanism: The learned duration model stretches or compresses text embeddings so their temporal alignment matches speech tokens, allowing the shared encoder to process them coherently alongside speech.
- Core assumption: The duration model accurately predicts how text tokens map to speech time, even for synthetic content.
- Evidence anchors: Section describes duration model used to up-sample text embedding before feeding to shared encoder.
- Break condition: If the duration model misestimates alignment, text embeddings will be temporally mismatched, confusing the shared encoder.

## Foundational Learning

- Concept: Conformer architecture combining convolution and self-attention
  - Why needed here: Captures both local phonetic patterns and long-range dependencies critical for PII recognition across variable-length identifiers.
  - Quick check question: What are the two main components of a conformer layer and why are both important for ASR?

- Concept: Curriculum learning in multimodal training
  - Why needed here: Ensures stable convergence when combining paired speech-text data with unpaired text-only data.
  - Quick check question: What is the purpose of starting with speech-text loss before adding text-only loss in the training schedule?

- Concept: Masked language modeling for text-only training
  - Why needed here: Enables effective use of unpaired text by training the model to predict masked tokens, improving text encoder representations without speech.
  - Quick check question: How does masked language modeling help when training with text-only data in a speech-text model?

## Architecture Onboarding

- Component map: Speech encoder (4 causal conformer layers) → Text encoder (2 conformer + 4 lightweight conv upsampling) → Shared encoder (3 causal + 10 non-causal conformer layers) → Decoder (HAT with v2 embeddings) → Alignment decoder (HAT with phonemes)
- Critical path: Speech/Text encoder → Shared encoder → Decoder → Output tokens
- Design tradeoffs: Text encoder adds 58M parameters but enables use of massive unpaired text; non-causal layers in shared encoder improve accuracy but prevent streaming.
- Failure signatures:
  - CER increases without SACC changes → text encoder not aligned with speech
  - SACC drops but CER stable → model memorizing full sequences poorly
  - Catastrophic forgetting → text-only loss too strong relative to speech-text loss
- First 3 experiments:
  1. Train baseline model on paired data only; measure WER and PII recall.
  2. Add unpaired general text injection; observe core ASR and domain transfer changes.
  3. Add domain-specific text injection with fake PII; measure PII recall improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between text-injection training and traditional speech training for achieving both PII recognition and general ASR performance?
- Basis in paper: The paper discusses how text-injection training can improve PII recognition without degrading general ASR performance, but doesn't explore the optimal training balance.
- Why unresolved: The paper shows that text-injection improves both PII recognition and overall WER, but doesn't investigate the specific training ratio or schedule that would optimize both objectives simultaneously.
- What evidence would resolve it: Systematic experiments varying the proportion of text-injection vs. speech training data and measuring the tradeoff between PII recognition accuracy and general ASR performance metrics like WER.

### Open Question 2
- Question: How does the effectiveness of text-injection training vary across different PII categories (e.g., names, dates, addresses, medical conditions) and languages?
- Basis in paper: The paper focuses on names and dates in medical contexts, but mentions that PII categories represent personal information broadly.
- Why unresolved: The paper demonstrates effectiveness for names and dates in medical speech but doesn't explore how well the approach generalizes to other PII types or languages.
- What evidence would resolve it: Cross-linguistic and cross-category experiments testing text-injection training on diverse PII types and multiple languages to identify performance patterns and limitations.

### Open Question 3
- Question: What is the impact of text-injection training on ASR models' ability to handle ambiguous or context-dependent PII (e.g., "April 15th" vs. "April fifteenth")?
- Basis in paper: The paper discusses text-injection for PII recognition but doesn't address how models handle ambiguous or context-dependent PII expressions.
- Why unresolved: The paper shows improvements in PII recognition accuracy but doesn't investigate how models handle cases where the same PII can be expressed in multiple ways or where context affects interpretation.
- What evidence would resolve it: Evaluation datasets containing ambiguous PII expressions and controlled experiments measuring model performance on these cases with and without text-injection training.

## Limitations

- Limited evaluation of synthetic PII quality: The study doesn't validate whether synthetic identifiers are structurally similar enough to real PII for effective transfer learning.
- No streaming capability: Non-causal conformer layers improve accuracy but prevent real-time applications.
- Architecture parameter overhead: Text encoder adds 58M parameters without discussing computational cost-benefit tradeoffs.

## Confidence

- **High Confidence**: Claims about overall performance improvements (8-13% PII recall gains, 3.2% SACC improvement) are well-supported by experimental results.
- **Medium Confidence**: Mechanism claims about how text injection improves PII recognition are plausible but lack direct empirical validation of core assumptions.
- **Low Confidence**: Claim that approach is universally applicable to all PII types is not substantiated - evaluation focuses primarily on names and dates in medical contexts.

## Next Checks

1. **Synthetic PII similarity validation**: Conduct controlled experiment comparing recognition accuracy when training with real vs. synthetic PII data for same identifier categories to establish transfer learning ceiling.

2. **Curriculum ablation study**: Train identical models with different curriculum schedules (speech-text only, text-only first, random mixing) to empirically verify whether proposed ordering provides measurable benefits.

3. **Streaming variant evaluation**: Implement streaming-compatible version using only causal layers and compare PII recognition performance against non-causal baseline to quantify accuracy-cost tradeoff.