---
ver: rpa2
title: Functional requirements to mitigate the Risk of Harm to Patients from Artificial
  Intelligence in Healthcare
arxiv_id: '2309.10424'
source_url: https://arxiv.org/abs/2309.10424
tags:
- systems
- system
- data
- healthcare
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes fourteen functional requirements for AI systems
  in healthcare to mitigate risks of patient harm, as identified by the European Parliament.
  These requirements include AI passport, user management, regulation check, data
  quality assessment, clinician double-check, continuous performance evaluation, audit
  trail, continuous usability testing, review of cases, bias check, explainable AI,
  encryption, and semantic interoperability.
---

# Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare

## Quick Facts
- arXiv ID: 2309.10424
- Source URL: https://arxiv.org/abs/2309.10424
- Reference count: 40
- One-line primary result: Proposes fourteen functional requirements for AI systems in healthcare to mitigate patient harm risks

## Executive Summary
This study identifies fourteen functional requirements that AI systems in healthcare can implement to reduce risks of patient harm, as identified by the European Parliament. The requirements address key risk categories including uncertainty, performance, safety, security, bias, transparency, and accountability. The proposed functionalities include AI passport, user management, regulation check, data quality assessment, clinician double-check, continuous performance evaluation, audit trail, continuous usability testing, review of cases, bias check, explainable AI, encryption, and semantic interoperability. These requirements aim to ensure transparency, accountability, and ethical use of AI systems in clinical settings.

## Method Summary
The study conceives a set of fourteen risk-mitigation functional requirements that AI systems may comply with to address identified risk categories. The requirements are designed to be implementable with current technology without waiting for inherent AI solutions. The methodology involves mapping each requirement to specific sources of risk, ensuring systematic coverage of all uncertainty and deterioration factors. The approach draws from established software engineering practices and health informatics standards to create a comprehensive framework for safe AI deployment in healthcare settings.

## Key Results
- Fourteen functional requirements proposed to mitigate AI-related patient harm risks
- Requirements cover all seven risk categories identified by the European Parliament
- Functionalities are implementable with current technology without waiting for inherent AI solutions
- Framework provides systematic coverage of uncertainty and deterioration sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fourteen functional requirements map directly to the seven risk categories identified by the EPRS, providing systematic coverage of all sources of uncertainty and deterioration.
- Mechanism: Each requirement addresses one or more specific sources of risk (e.g., data quality assessment mitigates AI errors from noisy inputs, audit trail addresses transparency gaps).
- Core assumption: The mapping between risks and requirements is exhaustive and mutually exclusive.
- Evidence anchors:
  - [abstract] "In this study, we propose fourteen functional requirements that AI systems may implement to reduce the risks associated with their medical purpose"
  - [section 2] "we have conceived a set of fourteen risk-mitigation functional requirements that an implementation of an AI system may comply with"
  - [corpus] Weak evidence; related papers focus on compliance frameworks but do not validate the specific mapping.
- Break condition: If new risk categories emerge or if existing requirements overlap significantly, the mapping would no longer be complete.

### Mechanism 2
- Claim: Implementation of these requirements is feasible with current technology and does not require waiting for inherent AI solutions.
- Mechanism: Each requirement (e.g., user management, encryption, semantic interoperability) relies on established software engineering practices and standards.
- Core assumption: Current technology is mature enough to support these functionalities in real-world healthcare environments.
- Evidence anchors:
  - [abstract] "These functionalities can be implemented with current technology and do not require waiting for inherent solutions from AI technology."
  - [section 2.14] "Using open standards from health informatics, such as openEHR and HL7, may simplify the semantic interoperability"
  - [corpus] Limited evidence; related papers discuss regulatory compliance but not feasibility of implementation.
- Break condition: If healthcare IT infrastructure cannot support the required standards or if legacy systems prevent integration.

### Mechanism 3
- Claim: The requirements strengthen accountability and liability frameworks by making AI decision-making traceable and explainable.
- Mechanism: Audit trail logs every action, XAI explains model predictions, and user management tracks access and permissions, collectively clarifying responsibility.
- Core assumption: Detailed logging and explanation mechanisms are sufficient to resolve liability disputes in clinical settings.
- Evidence anchors:
  - [abstract] "Our intention here is to provide specific high-level specifications of technical solutions to ensure continuous good performance and use of AI systems"
  - [section 2.8] "A chronological record of all actions carried out by the users, including user id, timestamp, input, output, and unique identification of the version of the AI system."
  - [corpus] Weak evidence; related papers discuss governance but not specific mechanisms for resolving liability.
- Break condition: If audit trails are incomplete or if XAI explanations are not sufficiently detailed to inform liability decisions.

## Foundational Learning

- Concept: AI passport
  - Why needed here: Provides a centralized, dynamic record of the AI system's purpose, limitations, training data, and performance metrics, enabling informed use.
  - Quick check question: What key elements must an AI passport include to ensure transparency and compliance?

- Concept: Explainable AI (XAI)
  - Why needed here: Enables clinicians to understand how AI predictions are made, fostering trust and enabling verification.
  - Quick check question: How does XAI contribute to reducing the lack of transparency in AI-assisted medical decision-making?

- Concept: Semantic interoperability
  - Why needed here: Ensures AI systems can exchange health data with existing Electronic Health Records using standardized formats.
  - Quick check question: Why is semantic interoperability critical for integrating AI systems into clinical workflows?

## Architecture Onboarding

- Component map: AI passport -> user management -> regulation check -> data quality assessment -> clinician double-check -> continuous performance evaluation -> audit trail -> continuous usability testing -> review of cases -> bias check -> explainable AI -> encryption -> semantic interoperability
- Critical path: User requests prediction → clinician double-check → system validates user permissions → checks regulation compliance → performs data quality assessment → generates prediction with XAI explanation → logs action in audit trail → stores performance metrics → updates AI passport
- Design tradeoffs: Balancing transparency (detailed audit trails) with privacy (minimizing logged personal data); ensuring usability (intuitive interfaces) without sacrificing functionality (comprehensive controls)
- Failure signatures: Missing or outdated AI passport information; failed user authentication; encryption errors; lack of XAI explanations; audit trail gaps; non-compliant data quality checks; poor interoperability with EHR systems
- First 3 experiments:
  1. Implement and test AI passport generation with sample training data and performance metrics
  2. Deploy user management with role-based access and verify audit logging for each action
  3. Integrate XAI module to explain predictions for a sample clinical case and validate explanation clarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of the proposed functional requirements be empirically validated in real-world healthcare settings?
- Basis in paper: [inferred] The paper discusses proposed functional requirements but does not provide empirical validation or real-world testing results.
- Why unresolved: The paper lacks empirical data on the actual effectiveness of these requirements when implemented in clinical environments.
- What evidence would resolve it: Studies or trials demonstrating the impact of these requirements on patient safety, AI system performance, and healthcare outcomes in actual clinical settings.

### Open Question 2
- Question: What specific challenges might arise in integrating these functional requirements with existing healthcare IT infrastructure?
- Basis in paper: [inferred] The paper mentions semantic interoperability and integration with electronic health records but does not detail specific challenges.
- Why unresolved: The paper does not address potential technical, organizational, or regulatory barriers to integrating these requirements into current healthcare systems.
- What evidence would resolve it: Case studies or pilot implementations showing integration challenges and solutions in diverse healthcare settings.

### Open Question 3
- Question: How can the proposed AI passport system be standardized across different AI systems and healthcare organizations?
- Basis in paper: [explicit] The paper introduces the concept of an AI passport but does not discuss standardization efforts or challenges.
- Why unresolved: The paper does not address how to ensure consistency and comprehensiveness of AI passport information across various AI systems and healthcare providers.
- What evidence would resolve it: Guidelines or frameworks for AI passport standardization, along with examples of implementation in multiple AI systems and healthcare organizations.

## Limitations

- The mapping between risk categories and functional requirements lacks empirical validation and may not be exhaustive
- Implementation feasibility in real healthcare environments with mixed legacy and modern IT infrastructure is not demonstrated
- The effectiveness of XAI explanations in complex clinical scenarios has not been validated through user studies

## Confidence

**High Confidence**: The identification of key risk categories (uncertainty, performance, safety, security, bias, transparency, and accountability) aligns with established regulatory frameworks and literature on AI in healthcare.

**Medium Confidence**: The proposed fourteen functional requirements represent a comprehensive approach to mitigating AI-related patient harm, as they draw from established software engineering practices and health informatics standards.

**Low Confidence**: The claim that these requirements can be fully implemented with current technology without waiting for inherent AI solutions is the most speculative, given the limited evidence of successful real-world deployment and integration.

## Next Checks

1. **Implementation Feasibility Study**: Conduct a pilot implementation of all fourteen requirements in a healthcare setting with mixed legacy and modern IT infrastructure to identify technical barriers and integration challenges.

2. **Risk-Requirement Mapping Validation**: Perform expert review sessions with clinicians, AI developers, and regulatory specialists to verify that the mapping between risk categories and functional requirements is exhaustive and that no critical risks are overlooked.

3. **XAI Effectiveness Assessment**: Evaluate the clarity and clinical utility of XAI explanations generated by the proposed requirements through controlled user studies with healthcare practitioners, measuring their ability to understand and act on AI predictions.