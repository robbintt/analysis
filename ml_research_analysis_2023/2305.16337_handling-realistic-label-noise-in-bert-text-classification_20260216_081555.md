---
ver: rpa2
title: Handling Realistic Label Noise in BERT Text Classification
arxiv_id: '2305.16337'
source_url: https://arxiv.org/abs/2305.16337
tags:
- noise
- label
- training
- data
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates BERT's performance on realistic label noise\
  \ in text classification tasks. The authors use two datasets with feature-dependent\
  \ label noise (Yor\xF9a and Hausa) and two datasets with synthetic pseudo-real-world\
  \ label noise (TweetNLP and SNLI)."
---

# Handling Realistic Label Noise in BERT Text Classification

## Quick Facts
- arXiv ID: 2305.16337
- Source URL: https://arxiv.org/abs/2305.16337
- Authors: 
- Reference count: 17
- Primary result: Noise cleaning significantly improves BERT performance on feature-dependent label noise, reaching 76.78% accuracy on Yorùbá vs 64.72% baseline

## Executive Summary
This paper evaluates BERT's performance on realistic label noise in text classification tasks. The authors use two datasets with feature-dependent label noise (Yorùbá and Hausa) and two datasets with synthetic pseudo-real-world label noise (TweetNLP and SNLI). They evaluate several approaches including co-teaching, Consensus Enhanced Training Approach (CETA), ensembles, and noise cleaning. For feature-dependent label noise, noise cleaning shows significant improvements, particularly on the Yorùbá dataset where accuracy reaches 76.78% compared to 64.72% with vanilla BERT. CETA also improves performance by about 3 percentage points on both datasets. For synthetic label noise, most approaches do not outperform vanilla BERT, suggesting this type of noise may represent inherent task ambiguities rather than true errors. The findings indicate that different types of realistic label noise require different handling strategies, with noise cleaning being particularly effective for feature-dependent noise but less so for pseudo-real-world noise that may reflect genuine annotator disagreements.

## Method Summary
The authors evaluate BERT's performance on four datasets with different types of label noise. For feature-dependent noise, they use Yorùbá and Hausa datasets with artificially injected label errors based on feature patterns. For synthetic pseudo-real-world noise, they apply Chong et al.'s noise injection method to TweetNLP and SNLI datasets at 10%, 20%, and 30% noise levels. They compare vanilla BERT with several noise-handling approaches: co-teaching, CETA, homogeneous and heterogeneous ensembles, and noise cleaning. The noise cleaning approach uses N-fold cross-validation to identify and remove instances with high loss values. Models are trained with BERT-base-uncased for English datasets and BERT-base-multilingual-cased for African language datasets, using early stopping on noisy validation sets.

## Key Results
- Noise cleaning achieves 76.78% accuracy on Yorùbá dataset versus 64.72% with vanilla BERT
- CETA improves performance by approximately 3 percentage points on both feature-dependent noise datasets
- Most noise-handling approaches fail to improve upon vanilla BERT for synthetic pseudo-real-world noise
- Heterogeneous ensembles show consistent gains but do not consistently outperform the best individual models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature-dependent label noise can be effectively mitigated by noise cleaning when the noise distribution is not heavily skewed against correct labels.
- Mechanism: Noise cleaning removes instances with loss values above a tuned threshold, effectively reducing the proportion of noisy labels in the training set.
- Core assumption: Noisy labels produce higher loss values than clean labels during model training.
- Evidence anchors:
  - [abstract] "For feature-dependent label noise, noise cleaning shows significant improvements, particularly on the Yorùbá dataset where accuracy reaches 76.78% compared to 64.72% with vanilla BERT"
  - [section] "Overall, we observe a larger reduction in noise level in Yorùbá compared to Hausa. After noise cleaning, we have 23% label noise in Yorùbá compared to 33% before noise cleaning, a 10% absolute reduction in noise"
  - [corpus] No direct corpus evidence found; this mechanism is primarily supported by paper experiments.
- Break condition: If the noise distribution is heavily skewed such that some categories have more errors than correct labels (as in Hausa), noise cleaning becomes less effective because removing high-loss instances disproportionately removes clean data.

### Mechanism 2
- Claim: Heterogeneous ensembles combining different noise-handling approaches provide more robust performance than homogeneous ensembles or single models.
- Mechanism: Different models learn complementary representations and error patterns, and aggregating their predictions reduces the impact of label noise on final classification.
- Core assumption: Different noise-handling approaches capture different aspects of the noise patterns, and their combination provides more comprehensive noise mitigation.
- Evidence anchors:
  - [section] "The heterogeneous ensemble method does not consistently improve either, but we do observe consistent gains using heterogeneous ensembles and boosting"
  - [section] "For heterogeneous ensembles, we aggregate predictions from the following three classifiers: vanilla BERT, co-teaching, and CETA"
  - [corpus] No direct corpus evidence found; this mechanism is primarily supported by paper experiments.
- Break condition: If the ensemble members are not sufficiently diverse in their noise-handling capabilities, the aggregation may not provide additional benefits over the best individual model.

### Mechanism 3
- Claim: Consensus-based training approaches like CETA are more effective than co-teaching for feature-dependent label noise because they incorporate consensus as an explicit criterion.
- Mechanism: CETA uses two classifiers that only train on instances where both reach consensus, and augments cross-entropy loss with Wasserstein distance to strengthen the consensus mechanism.
- Core assumption: Clean instances are more likely to produce consensus between classifiers than noisy instances.
- Evidence anchors:
  - [abstract] "CETA also improves performance by about 3 percentage points on both datasets"
  - [section] "The focus of CETA is to train the classifiers only on instances where both classifiers have reached a consensus"
  - [corpus] No direct corpus evidence found; this mechanism is primarily supported by paper experiments.
- Break condition: If the noise level is very high or the feature-dependent noise patterns are too complex, even consensus between classifiers may not reliably identify clean instances.

## Foundational Learning

- Concept: Label noise types and their characteristics
  - Why needed here: Understanding the difference between feature-dependent and pseudo-real-world noise is crucial for selecting appropriate mitigation strategies
  - Quick check question: What distinguishes feature-dependent label noise from random label noise?

- Concept: Ensemble methods and their aggregation strategies
  - Why needed here: The paper experiments with different ensemble types (homogeneous, heterogeneous, boosting), requiring understanding of how each works
  - Quick check question: How do homogeneous ensembles differ from heterogeneous ensembles in their approach to combining model predictions?

- Concept: Loss-based sample selection and thresholding
  - Why needed here: Noise cleaning relies on loss values to identify and remove noisy instances, requiring understanding of how loss correlates with label quality
  - Quick check question: Why might noisy labels typically produce higher loss values during model training?

## Architecture Onboarding

- Component map: Data → Noise handling (cleaning/ensembling) → BERT fine-tuning → Evaluation
- Critical path: Data → Noise handling (cleaning/ensembling) → BERT fine-tuning → Evaluation
- Design tradeoffs: Noise cleaning provides strong performance gains but requires tuning loss thresholds and may remove some clean data; ensembles are more general but provide smaller improvements
- Failure signatures: Poor performance on Hausa despite noise cleaning suggests the approach fails when noise distribution is heavily skewed; lack of improvement on pseudo-real-world noise indicates the noise may reflect genuine ambiguity rather than errors
- First 3 experiments:
  1. Run vanilla BERT with early stopping on a noisy validation set to establish baseline performance
  2. Apply noise cleaning with different loss thresholds to identify optimal threshold for a given dataset
  3. Train a heterogeneous ensemble combining vanilla BERT, co-teaching, and CETA to compare against individual models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of noise distribution make noise cleaning more effective on some datasets than others?
- Basis in paper: [explicit] The authors note that noise cleaning was significantly more effective on Yorùbá (23% reduction in noise) compared to Hausa (3% reduction) and attribute this to Hausa having some categories where errors exceed correct labels
- Why unresolved: The paper identifies this as a contributing factor but doesn't systematically analyze what noise distribution patterns predict successful noise cleaning or develop metrics to characterize when noise cleaning will be effective
- What evidence would resolve it: A systematic analysis of multiple datasets with varying noise distributions showing which characteristics (e.g., error concentration, class imbalance, error-to-correct ratio) predict successful noise cleaning outcomes

### Open Question 2
- Question: Does the synthetic pseudo-real-world noise from Chong et al.'s method represent true annotation errors or inherent task ambiguities, and how can we distinguish between these cases?
- Basis in paper: [explicit] The authors note that for SNLI, "the gold labels may already be ambiguous even before applying the noising scheme" and suggest this type of noise "may even reflect intrinsic ambiguities in the labels"
- Why unresolved: The paper treats all synthetic noise as errors but acknowledges this may not be appropriate for inherently ambiguous tasks, without providing methods to differentiate between errors and ambiguities
- What evidence would resolve it: Development of diagnostic tools to determine whether observed noise represents true errors versus inherent task ambiguity, possibly by analyzing agreement patterns among multiple annotators or expert assessments

### Open Question 3
- Question: How does the effectiveness of different noise handling approaches vary with different types of feature-dependent noise (e.g., systematic versus random patterns, domain-specific versus general patterns)?
- Basis in paper: [inferred] The authors use two datasets with feature-dependent noise but note they come from different languages and annotation schemes, yet don't systematically compare how noise characteristics affect approach effectiveness
- Why unresolved: The paper only examines two datasets with feature-dependent noise, limiting the ability to draw conclusions about which noise handling approaches work best for different types of feature-dependent noise
- What evidence would resolve it: Testing noise handling approaches across multiple datasets with different types of feature-dependent noise (e.g., systematic rule-based errors versus random feature correlations) to identify which approaches are most robust to different noise patterns

## Limitations

- Noise cleaning effectiveness depends heavily on balanced noise distribution across classes, failing when some categories have more errors than correct labels
- Most approaches fail to improve upon vanilla BERT for synthetic pseudo-real-world noise, raising questions about whether this noise represents true errors or inherent task ambiguities
- Specific noise-cleaning thresholds and ensemble aggregation details are not fully specified, limiting exact reproducibility

## Confidence

**High Confidence**: The effectiveness of noise cleaning for feature-dependent label noise on the Yorùbá dataset (76.78% accuracy vs 64.72% baseline) is well-supported by the experimental results and consistent across multiple validation checks.

**Medium Confidence**: The finding that most approaches fail to improve upon vanilla BERT for synthetic pseudo-real-world noise is reasonably supported, though the interpretation that this noise represents task ambiguity rather than errors requires further validation.

**Low Confidence**: The relative performance comparison between CETA and co-teaching is based on limited experimental evidence, and the paper does not provide sufficient detail on the CETA implementation to fully assess its advantages over alternative consensus-based approaches.

## Next Checks

1. **Analyze noise distribution characteristics**: Perform detailed analysis of the noise patterns in each dataset, including class-wise noise distributions and correlation between feature space and label errors, to better understand why certain approaches succeed or fail on different noise types.

2. **Test noise cleaning under varying thresholds**: Systematically evaluate noise cleaning performance across a wider range of loss thresholds and with different noise proportions to determine the optimal conditions for this approach and identify failure modes.

3. **Validate pseudo-real-world noise interpretation**: Design experiments to distinguish between true annotation errors and inherent task ambiguities in the synthetic noise datasets, such as comparing human judgments on noisy instances or analyzing model uncertainty patterns.