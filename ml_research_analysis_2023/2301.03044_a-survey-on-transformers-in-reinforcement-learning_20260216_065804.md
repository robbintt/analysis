---
ver: rpa2
title: A Survey on Transformers in Reinforcement Learning
arxiv_id: '2301.03044'
source_url: https://arxiv.org/abs/2301.03044
tags:
- learning
- transformer
- arxiv
- policy
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey on the use of Transformers
  in reinforcement learning (RL). It reviews the evolution of Transformers in RL,
  categorizes existing methods into four classes: representation learning, model learning,
  sequential decision-making, and generalist agents.'
---

# A Survey on Transformers in Reinforcement Learning

## Quick Facts
- arXiv ID: 2301.03044
- Source URL: https://arxiv.org/abs/2301.03044
- Reference count: 20
- Primary result: Comprehensive survey categorizing Transformer applications in RL into representation learning, model learning, sequential decision-making, and generalist agents, while identifying key challenges and future directions.

## Executive Summary
This paper provides a comprehensive survey of Transformer applications in reinforcement learning, documenting a recent surge in their adoption across diverse RL domains. The survey systematically categorizes existing methods into four classes based on their functional roles within RL pipelines: representation learning, model learning, sequential decision-making, and generalist agents. It identifies unique challenges faced when applying Transformers in RL settings, including non-stationarity during training, high computational costs, and the need for large-scale datasets. The paper also outlines promising future directions, such as combining RL with supervised learning paradigms, bridging online and offline learning approaches, and developing Transformer architectures specifically tailored for decision-making problems.

## Method Summary
The survey conducts a comprehensive literature review of Transformer applications in reinforcement learning, focusing on works published from 2018 to 2022. The methodology involves categorizing reviewed methods into four functional classes: representation learning (Transformers as encoders for entity or temporal relationships), model learning (Transformers as world models or transition models), sequential decision-making (Transformers as end-to-end policy models), and generalist agents (Transformers trained on multi-task, multi-modal datasets). The analysis examines applications, limitations, and future prospects for each category while discussing challenges specific to TransformRL such as non-stationarity and computational demands. The approach synthesizes existing research to provide a structured taxonomy and identify open problems in the field.

## Key Results
- Transformers can serve as general-purpose function approximators in RL by leveraging sequence modeling capabilities for multi-entity observations and temporal dependencies
- Transformer-based world models enable planning and imagination by capturing historical trajectories in latent space through autoregressive sequence modeling
- Multi-task and cross-domain generalization is possible through shared tokenization spaces and prompting techniques, enabling generalist agents across diverse modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers can serve as general-purpose function approximators in RL by leveraging their sequence modeling capabilities, enabling unified handling of multi-entity observations, temporal sequences, and decision trajectories.
- Mechanism: The self-attention mechanism in Transformers captures long-range dependencies and relations across entities or time steps without requiring recurrent structure, making them suitable for both perception (entity encoding) and decision-making (trajectory modeling).
- Core assumption: The RL problem can be reformulated as a conditional sequence modeling problem, where the sequence of states, actions, and rewards is generated autoregressively.
- Evidence anchors:
  - [abstract] "Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL."
  - [section] "Transformers can be used as one component for RL algorithms, e.g., a representation module or a dynamic model. Transformers can also serve as one whole sequential decision-maker."
  - [corpus] Weak evidence: corpus neighbors focus on general Transformer applications (healthcare, NLP, robotics) but do not directly address RL-specific adaptation mechanisms.
- Break condition: If the training data is not stationary (e.g., policy-induced distribution shift), the Transformer's performance degrades due to overfitting to outdated trajectories or entities.

### Mechanism 2
- Claim: Transformers can act as world models in model-based RL by capturing historical trajectories in latent space, enabling planning and imagination without explicit environment interaction.
- Mechanism: A Transformer-based transition model predicts next latent states and rewards conditioned on a sequence of past latent states and actions, effectively learning a dynamics model that can generate imagined trajectories for planning.
- Core assumption: The latent space embedding of observations is sufficiently informative to reconstruct future states and rewards, and the Transformer can generalize across varying sequence lengths.
- Evidence anchors:
  - [section] "IRIS (Imagination with autoRegression over an Inner Speech) [Micheli et al., 2022] learns a Transformer-based world model simply via autoregressive learning on rollout experience without KL balancing like Dreamer and achieves considerable results on the Atari [Bellemare et al., 2013] 100k benchmark."
  - [section] "Concretely, Chen et al. [2022] replace RNN-based Recurrent State-Space Model (RSSM) in Dreamer with a Transformer-based model (Transformer State-Space Model, TSSM)."
  - [corpus] Weak evidence: corpus does not contain RL-specific model learning examples; mentions only general Transformer efficiency surveys.
- Break condition: If the latent dynamics become inaccurate over long horizons, compounding prediction errors can render imagined trajectories useless for planning.

### Mechanism 3
- Claim: Transformers enable multi-task and cross-domain generalization in RL by unifying diverse tasks and modalities in a shared tokenization space and leveraging prompting techniques for adaptation.
- Mechanism: A single Transformer is trained on a large-scale dataset containing experiences from multiple tasks and domains, using shared tokenization and prompting to condition on task-specific context, thereby acting as a generalist agent.
- Core assumption: The diverse dataset contains sufficient coverage of task types and modalities to learn transferable representations, and prompting can provide effective task conditioning without retraining.
- Evidence anchors:
  - [section] "Reed et al. [2022] further exploit prompt-based architecture to learn a generalist agent (Gato) via auto-regressive sequence modeling on a super large-scale dataset covering natural language, image, temporal decision-making, and multi-modal data."
  - [section] "MGDT and SwitchTT exploit experiences collected from multiple tasks and various performance-level policies to learn a general policy."
  - [corpus] Weak evidence: corpus lacks specific examples of multi-task or cross-domain RL generalization via Transformers.
- Break condition: If the prompt or tokenization space cannot disambiguate tasks effectively, the agent may produce suboptimal or incorrect actions across domains.

## Foundational Learning

- Concept: Sequence modeling with self-attention
  - Why needed here: RL involves sequential decision-making; Transformers naturally model sequences via self-attention, capturing dependencies across states, actions, and rewards.
  - Quick check question: How does self-attention differ from recurrent networks in modeling variable-length sequences?

- Concept: Conditional generation and autoregressive modeling
  - Why needed here: Many Transformer-based RL methods (e.g., Decision Transformer) frame policy learning as generating actions conditioned on future returns or goals.
  - Quick check question: What is the role of the conditioning variable in Decision Transformer's sequence generation?

- Concept: Latent dynamics and world models
  - Why needed here: Model-based RL with Transformers requires learning a transition model in latent space to enable planning without environment interaction.
  - Quick check question: Why might a Transformer-based world model be more sample-efficient than RNN-based models?

## Architecture Onboarding

- Component map: Raw observations -> Tokenizer -> Transformer Encoder -> Autoregressive Transformer -> Decision Head
- Critical path:
  1. Tokenize raw observations into entity or temporal sequences
  2. Encode sequences using Transformer encoder
  3. Generate or predict next states/actions via autoregressive Transformer
  4. Apply task-specific conditioning (e.g., return-to-go, prompts) for decision-making
- Design tradeoffs:
  - Memory vs. sequence length: Longer sequences improve context but increase memory and compute cost
  - Conditioning granularity: Finer-grained conditioning (e.g., sub-task level) can improve accuracy but adds complexity
  - Model capacity: Larger models capture more complex dependencies but risk overfitting and slow training
- Failure signatures:
  - Unstable training: Indicates non-stationarity or poor conditioning design
  - Degraded performance on longer sequences: Suggests limited effective context length or compounding errors
  - Poor generalization across tasks: Points to insufficient diversity in training data or weak prompting
- First 3 experiments:
  1. Train a simple Transformer encoder on multi-entity observations and evaluate entity relationship reconstruction
  2. Replace an RNN world model with a Transformer in a small-scale model-based RL setup and compare sample efficiency
  3. Implement Decision Transformer on a single task (e.g., Atari) and test conditioning on different return-to-go values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the "deadly triad" problem be mitigated when using Transformers in reinforcement learning, particularly in online settings?
- Basis in paper: [explicit] The paper mentions that the deadly triad problem (divergence when using function approximations with bootstrapping and off-policy learning) can be eliminated in offline RL via supervised learning paradigms.
- Why unresolved: The paper suggests combining RL and (self-)supervised learning as a future direction, but does not provide a concrete solution for online settings.
- What evidence would resolve it: Empirical studies demonstrating successful mitigation of the deadly triad in online RL using Transformers, possibly through novel training techniques or architectural modifications.

### Open Question 2
- Question: What are the most effective hindsight information representations for Decision Transformer models in stochastic environments?
- Basis in paper: [explicit] The paper discusses various conditioning information beyond return-to-go, such as expected return and learned representations, but highlights the need for further exploration.
- Why unresolved: While the paper introduces alternative conditioning schemes, it does not conclusively determine which representations are optimal across different environments and tasks.
- What evidence would resolve it: Comparative studies evaluating the performance of Decision Transformer models using different hindsight information representations across a wide range of stochastic environments.

### Open Question 3
- Question: Can Transformers be used to learn a general world model that can be applied to different tasks and scenarios?
- Basis in paper: [explicit] The paper mentions the potential of Transformers as general policy models and discusses their ability to process multiple modalities and demonstrate scalability.
- Why unresolved: While the paper highlights the potential, it does not provide concrete evidence or methods for developing a general world model using Transformers.
- What evidence would resolve it: Successful development and evaluation of a Transformer-based world model that demonstrates strong performance and generalization across multiple tasks and scenarios.

## Limitations

- Non-stationarity of RL training presents fundamental challenges as Transformers assume i.i.d. data while policy learning involves distribution shift
- Computational costs remain prohibitive for long-horizon tasks, with memory and inference time scaling quadratically with sequence length
- Current Transformer-based RL methods require large-scale datasets and substantial computational resources, limiting accessibility and practical deployment

## Confidence

- High confidence: The taxonomy of TransformRL methods (representation learning, model learning, sequential decision-making, generalist agents) is well-supported by existing literature and provides a clear organizational framework
- Medium confidence: The identified challenges (non-stationarity, computational costs) are well-documented, but the proposed solutions remain largely theoretical or in early stages of development
- Medium confidence: Claims about Transformer capabilities as world models and sequence generators are supported by recent works like IRIS and Decision Transformer, but long-horizon generalization and sample efficiency remain open questions

## Next Checks

1. **Non-stationarity robustness test**: Implement a controlled experiment comparing Transformer-based RL methods against RNN baselines under varying degrees of distribution shift, measuring performance degradation and adaptation speed

2. **Computational scaling analysis**: Conduct empirical benchmarks measuring memory usage and inference time for Transformer-based RL methods across different sequence lengths (50, 100, 500, 1000 steps) on representative tasks

3. **Prompt generalization evaluation**: Test multi-task Transformer RL agents across domain shifts (e.g., different grid-world layouts or Atari game variations) to quantify the effectiveness of prompting and tokenization for task disambiguation and zero-shot adaptation