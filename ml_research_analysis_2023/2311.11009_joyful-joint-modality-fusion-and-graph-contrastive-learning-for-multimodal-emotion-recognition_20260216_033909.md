---
ver: rpa2
title: 'Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimodal
  Emotion Recognition'
arxiv_id: '2311.11009'
source_url: https://arxiv.org/abs/2311.11009
tags:
- multimodal
- joyful
- graph
- pages
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We proposed a novel joint learning framework (JOYFUL) for multimodal
  emotion recognition in conversations, where multimodal fusion, graph contrastive
  learning, and emotion recognition are jointly optimized for global optimal performance.
  We first design a new multimodal fusion mechanism that can simultaneously learn
  and fuse a global contextual representation and uni-modal specific representations.
---

# Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimodal Emotion Recognition

## Quick Facts
- arXiv ID: 2311.11009
- Source URL: https://arxiv.org/abs/2311.11009
- Reference count: 40
- Key outcome: JOYFUL achieves state-of-the-art performance on three multimodal emotion recognition benchmark datasets

## Executive Summary
This paper proposes JOYFUL, a novel joint learning framework for multimodal emotion recognition in conversations that integrates multimodal fusion, graph contrastive learning, and emotion recognition into a single optimization objective. The framework addresses the over-smoothing problem in deeper graph layers through a contrastive learning mechanism that learns more distinguishable representations for samples with different sentiments. JOYFUL introduces a new multimodal fusion mechanism that simultaneously learns global contextual and uni-modal specific representations, preventing loss of modality-specific information while capturing cross-modal consistency.

## Method Summary
JOYFUL jointly optimizes multimodal fusion, graph contrastive learning, and emotion recognition using a unified objective function. The framework extracts features from visual, audio, and text modalities, then fuses them to produce both global contextual and uni-modal specific representations. A graph is constructed from conversation utterances with temporal dependencies, and two augmented graph views are created through feature masking, edge perturbation, and global proximity. GCN layers process these views with inter-view and intra-view contrastive losses to learn distinguishable representations, while a classifier uses cross-entropy loss for final emotion classification. The entire system is trained end-to-end using Adam optimizer.

## Key Results
- Achieved state-of-the-art performance on IEMOCAP, MELD, and MOSEI benchmark datasets
- Demonstrates superior accuracy and weighted F1-score compared to all baseline methods
- Successfully addresses over-smoothing issues in deeper graph layers through contrastive learning

## Why This Works (Mechanism)

### Mechanism 1
The joint optimization of multimodal fusion, graph contrastive learning, and emotion recognition enables global optimal performance by preventing sub-optimal outcomes of two-phase pipelines. Instead of fixing extracted and fused features for downstream processing, JOYFUL integrates all components into a single objective function, allowing gradients from the classifier and contrastive loss to continuously refine the multimodal fusion process.

### Mechanism 2
Graph contrastive learning alleviates the over-smoothing problem in deeper GNN layers by learning more distinguishable representations for samples with different sentiments. By generating two augmented graph views and maximizing mutual information between them, the model preserves discriminative features even as layers deepen, counteracting the tendency of similar sentiments to become indistinguishable.

### Mechanism 3
The multimodal fusion module simultaneously learns global contextual and uni-modal specific representations, preventing loss of modality-specific information while capturing cross-modal consistency. Contextual representations are obtained through projection and concatenation of all modalities, while specific representations are projected into a shared subspace using a trainable basis matrix, ensuring alignment without losing modality-specific cues.

## Foundational Learning

- **Graph Neural Networks (GNNs)**: Model dependencies between utterances in conversations, distinguishing intra-speaker and inter-speaker relationships. *Quick check: How does a GCN layer aggregate information from neighboring nodes, and why is this useful for capturing dialogue context?*

- **Contrastive Learning**: Helps learn distinguishable representations by pulling together similar samples and pushing apart dissimilar ones. *Quick check: What is the difference between inter-view and intra-view contrastive loss, and how do they contribute to learning discriminative features?*

- **Multimodal Fusion Techniques**: Effective fusion of visual, audio, and text modalities is crucial for capturing comprehensive emotional cues. *Quick check: Why might direct concatenation of modalities lose important information, and how does separating contextual and specific representations help?*

## Architecture Onboarding

- **Component map**: Uni-modal Extractor -> Multimodal Fusion Module -> Graph Construction -> Graph Augmentation -> Graph Contrastive Learning Module -> Emotion Recognition Classifier
- **Critical path**: Features extracted from each modality are fused to create contextual and specific representations, which are used to construct a graph representing conversation structure. This graph is augmented to create two views, processed through GCN layers with contrastive learning, and finally classified for emotion recognition.
- **Design tradeoffs**: Balancing weights in the joint loss function to ensure neither multimodal fusion nor contrastive learning dominates; choosing appropriate augmentation strategies to create meaningful perturbations without destroying essential information; determining optimal window size for graph construction to balance information richness and noise.
- **Failure signatures**: Poor performance on minority classes may indicate insufficient contrastive learning for imbalanced datasets; performance degradation with deeper GCN layers may signal ineffective prevention of over-smoothing; failure to improve with added modalities may indicate ineffective cross-modal integration in the fusion mechanism.
- **First 3 experiments**: 1) Train JOYFUL with only one modality to verify base performance and identify modality contributions. 2) Disable the graph contrastive learning module to assess its impact on performance and distinguishability. 3) Vary the window size for graph construction to find the optimal context length balancing information richness and noise.

## Open Questions the Paper Calls Out
- How can the proposed model be adapted to handle multimodal sentiment analysis tasks more effectively, given its underperformance on sentiment analysis compared to emotion recognition tasks?
- Can the proposed model be extended to handle more complex multimodal tasks beyond emotion recognition and sentiment analysis, such as multimodal retrieval or question answering?
- How does the proposed model perform on larger-scale and more heterogeneous data in real-world scenarios, beyond the benchmark datasets used in evaluation?

## Limitations
- The exact neural network architectures for the fusion functions f_g and f_â„“ are not specified, affecting reproducibility
- Hyperparameters for graph augmentation ratios and smoothing parameters are not detailed
- The paper doesn't address potential biases in the datasets or how they might affect model performance across different demographic groups

## Confidence
- **High confidence**: Experimental results and performance claims (SOTA on three benchmarks)
- **Medium confidence**: Theoretical justification for joint optimization benefits
- **Low confidence**: Exact implementation details needed for perfect reproduction

## Next Checks
1. **Ablation study on joint optimization**: Train JOYFUL with sequential (two-phase) versus joint optimization to quantify the claimed benefits of global optimization
2. **Sensitivity analysis of augmentation parameters**: Systematically vary the feature masking and edge perturbation ratios to determine their optimal values and verify the robustness of the contrastive learning
3. **Generalization test on out-of-domain data**: Evaluate JOYFUL on datasets from different domains (e.g., crisis communication or customer service interactions) to assess its ability to generalize beyond the benchmark conversations used in the paper