---
ver: rpa2
title: Aligning Language Models with Human Preferences via a Bayesian Approach
arxiv_id: '2310.05782'
source_url: https://arxiv.org/abs/2310.05782
tags:
- human
- base
- preferences
- preference
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of aligning natural language
  generation (NLG) models with human preferences while considering the inherent disagreement
  among individuals. Current methods aggregate preferences through majority voting
  or averaging, but these approaches fail to capture nuanced degrees of disagreement
  and may not represent the universality of human preferences.
---

# Aligning Language Models with Human Preferences via a Bayesian Approach

## Quick Facts
- arXiv ID: 2310.05782
- Source URL: https://arxiv.org/abs/2310.05782
- Reference count: 40
- The paper proposes a Bayesian approach to align NLG models with human preferences while capturing disagreement, using contrastive learning for calibration.

## Executive Summary
This paper addresses the challenge of aligning natural language generation (NLG) models with human preferences while accounting for inherent disagreement among individuals. Current methods aggregate preferences through majority voting or averaging, but these approaches fail to capture nuanced degrees of disagreement and may not represent the universality of human preferences. To tackle this, the paper proposes a Bayesian approach called d-PM that models the distribution of disagreements among human preferences. Additionally, it introduces a contrastive learning strategy to train the NLG model using preference scores derived from the d-PM model.

## Method Summary
The method involves two key components: the d-PM (Preference Modeling with Disagreement) model and a contrastive learning strategy for calibration. The d-PM model uses a Bayesian framework to estimate the distribution of human preferences, treating observed annotations as samples from a universal preference distribution. This is achieved through variational inference to infer a universal preference distribution that smooths out outliers and extreme labels. The contrastive learning strategy then generates multiple candidate sequences, ranks them by preference scores from d-PM, and applies a pairwise margin loss to align the generation likelihoods with these scores. The approach is evaluated on two human-centric NLG tasks: emotional support conversation and integrity Rule-of-Thumb generation.

## Key Results
- The proposed d-PM model consistently outperforms previous state-of-the-art models in both automatic and human evaluations on emotional support conversation and integrity Rule-of-Thumb generation tasks.
- The method generates responses that are less controversial and more widely acceptable, as evidenced by higher global consensus scores in human evaluations.
- The contrastive learning strategy effectively calibrates the generation model to produce outputs with high preference scores more efficiently than reinforcement learning.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Bayesian framework captures disagreement by modeling the distribution of preferences across all individuals, rather than aggregating to a single label.
- **Mechanism:** The observed preference from a subset of individuals is treated as prior knowledge, and the d-PM model uses variational inference to infer a universal preference distribution that smooths out outliers and extreme labels.
- **Core assumption:** Human preferences can be modeled as a distribution over two classes (acceptable/unacceptable), and the observed annotations are samples from this distribution.
- **Evidence anchors:**
  - [abstract] "proposes a Bayesian approach called d-PM that models the distribution of disagreements among human preferences"
  - [section 3.1] "We establish a distribution ρ to represent the universal preference... the observed annotations l are considered as samples from ρ"
- **Break condition:** If the assumption of a smooth distribution over preferences is violated (e.g., bimodal preferences), the Bayesian smoothing may not accurately represent the true disagreement.

### Mechanism 2
- **Claim:** Contrastive learning calibrates the generation model to produce outputs with high preference scores more efficiently than reinforcement learning.
- **Mechanism:** Multiple candidate sequences are generated, ranked by preference scores from d-PM, and a pairwise margin loss aligns the generation likelihoods with these scores.
- **Core assumption:** The generation model can be calibrated offline without expensive online decoding, and the ranking of candidates by preference score is reliable.
- **Evidence anchors:**
  - [abstract] "propose utilizing the contrastive learning strategy to train the NLG model with the preference scores derived from the d-PM model"
  - [section 3.2] "we propose a model-agnostic module to leverage contrastive learning to calibrate generation likelihood aligning with d-PM"
- **Break condition:** If the candidate generation step produces candidates that are too similar, the contrastive learning signal becomes weak and ineffective.

### Mechanism 3
- **Claim:** Modeling disagreement explicitly rather than aggregating preferences leads to less controversial and more universally acceptable outputs.
- **Mechanism:** By considering the full distribution of preferences rather than majority voting or averaging, the model can generate responses that are acceptable to a broader range of individuals, including minorities.
- **Core assumption:** Capturing the distribution of preferences rather than a single aggregated label better represents the diversity of human preferences and leads to more acceptable outputs.
- **Evidence anchors:**
  - [abstract] "models the distribution of disagreements among human preferences" and "generate responses that are less controversial and more widely acceptable"
  - [section 2] "For widely acceptable and less controversial outputs, it is necessary for the resulting NLG systems to account for capturing disagreements inherent in human preferences"
- **Break condition:** If the preference distribution is too flat or if there is no consensus at all, the model may struggle to generate acceptable outputs.

## Foundational Learning

- **Concept: Bayesian inference and variational methods**
  - **Why needed here:** The d-PM model uses Bayesian inference to model the distribution of human preferences and variational methods to approximate the posterior distribution.
  - **Quick check question:** Can you explain the difference between prior, likelihood, and posterior in the context of Bayesian inference?

- **Concept: Contrastive learning**
  - **Why needed here:** Contrastive learning is used to align the generation model with human preferences by comparing candidate sequences and their preference scores.
  - **Quick check question:** How does contrastive learning differ from traditional supervised learning, and why is it suitable for this task?

- **Concept: Preference modeling**
  - **Why needed here:** The d-PM model is a preference model that estimates the acceptability of generated text based on human preferences.
  - **Quick check question:** What are the key differences between modeling preferences with majority voting, soft labels, and the d-PM approach?

## Architecture Onboarding

- **Component map:** d-PM model -> Generation model -> Contrastive learning module -> Candidate generation

- **Critical path:**
  1. Train d-PM model on human preference data.
  2. Use d-PM to score candidate sequences generated by the base model.
  3. Apply contrastive learning to align the generation model with preference scores.

- **Design tradeoffs:**
  - Using a Bayesian approach to model disagreement vs. simple aggregation (majority voting or averaging).
  - Contrastive learning vs. reinforcement learning for alignment (offline vs. online, efficiency vs. sample complexity).
  - Modeling preferences as a distribution over two classes vs. multi-class or continuous preference scores.

- **Failure signatures:**
  - If d-PM fails to capture the true distribution of preferences, the generated text may still be controversial.
  - If contrastive learning is not effective, the generation model may not align well with human preferences.
  - If the candidate generation step is not diverse enough, the contrastive learning signal may be weak.

- **First 3 experiments:**
  1. Train d-PM on a small preference dataset and evaluate its ability to capture disagreement.
  2. Implement the contrastive learning module and test it on a simple generation task with known preferences.
  3. Integrate d-PM and contrastive learning and evaluate the end-to-end performance on a human-centric NLG task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Bayesian approach handle the case where the prior distribution is highly uncertain or uninformative?
- Basis in paper: [inferred] The paper mentions that the prior distribution is based on observed preferences among selected individuals, but does not discuss how the approach handles cases where the prior is uncertain or uninformative.
- Why unresolved: The paper does not provide any experimental results or analysis on the impact of uncertain or uninformative priors on the performance of the Bayesian approach.
- What evidence would resolve it: Experiments comparing the performance of the Bayesian approach with different levels of prior uncertainty or informativeness, and analysis of how the approach handles such cases.

### Open Question 2
- Question: How does the contrastive learning strategy compare to other alignment methods, such as reinforcement learning, in terms of sample efficiency and convergence speed?
- Basis in paper: [explicit] The paper mentions that reinforcement learning is generally perceived as costly in terms of convergence and online decoding processes, and proposes using contrastive learning as an alternative.
- Why unresolved: The paper does not provide a direct comparison between the contrastive learning strategy and reinforcement learning in terms of sample efficiency and convergence speed.
- What evidence would resolve it: Experiments comparing the sample efficiency and convergence speed of the contrastive learning strategy and reinforcement learning, using the same base models and datasets.

### Open Question 3
- Question: How does the proposed approach handle the case where the human preferences are highly subjective and inconsistent across different tasks or domains?
- Basis in paper: [inferred] The paper mentions that the proposed approach is tested on two human-centric tasks, but does not discuss how it handles cases where human preferences are highly subjective and inconsistent across different tasks or domains.
- Why unresolved: The paper does not provide any experimental results or analysis on the impact of highly subjective and inconsistent human preferences on the performance of the proposed approach.
- What evidence would resolve it: Experiments comparing the performance of the proposed approach on tasks or domains with highly subjective and inconsistent human preferences, and analysis of how the approach handles such cases.

## Limitations

- The paper does not provide full details on the d-PM model architecture, specifically the MLP layer configuration for converting BERT hidden states to preference scores.
- The evaluation is limited to two specific tasks (emotional support conversation and integrity Rule-of-Thumb generation), leaving uncertainty about the approach's generalization to other domains or more nuanced preference distributions.
- The method's scalability to larger datasets or more complex preference structures is not addressed.

## Confidence

**High Confidence Claims:**
- The Bayesian framework can model preference distributions and disagreement (supported by the theoretical formulation and the d-PM model architecture)
- Contrastive learning can align generation models with preference scores (demonstrated through the calibration framework and margin loss)

**Medium Confidence Claims:**
- The proposed method consistently outperforms previous state-of-the-art models (supported by experimental results, but dependent on the specific datasets and evaluation metrics used)
- The method generates responses that are less controversial and more widely acceptable (supported by human evaluations, but subjective and task-dependent)

**Low Confidence Claims:**
- The approach generalizes well to other NLG tasks or domains (limited evaluation scope)
- The method is more efficient than reinforcement learning (not directly compared in terms of computational cost or sample complexity)

## Next Checks

1. **Architecture Validation**: Implement and test the d-PM model architecture with different MLP layer configurations to verify the impact on preference score quality and distribution.

2. **Hyperparameter Sensitivity**: Conduct experiments varying the contrastive learning hyperparameters (margin λ, length penalty α, candidate number K) to assess their impact on model performance and calibration effectiveness.

3. **Generalization Test**: Apply the d-PM model and contrastive learning framework to a different NLG task or domain (e.g., summarization or question answering) to evaluate its generalization capability and robustness to varying preference distributions.