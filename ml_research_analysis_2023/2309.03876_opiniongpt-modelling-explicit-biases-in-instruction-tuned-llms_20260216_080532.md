---
ver: rpa2
title: 'OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs'
arxiv_id: '2309.03876'
source_url: https://arxiv.org/abs/2309.03876
tags:
- biases
- bias
- language
- opiniongpt
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OpinionGPT, a web-based demo that makes biases
  in instruction-tuned large language models (LLMs) explicit and transparent. The
  authors identify 11 different biases (political, geographic, gender, age) and derive
  an instruction-tuning corpus in which each answer was written by members of one
  of these demographics.
---

# OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs

## Quick Facts
- arXiv ID: 2309.03876
- Source URL: https://arxiv.org/abs/2309.03876
- Reference count: 7
- Key outcome: OpinionGPT is a web-based demo that makes biases in instruction-tuned LLMs explicit by fine-tuning LLaMa on subreddit-specific corpora and allowing side-by-side comparison of responses across 11 different biases.

## Executive Summary
OpinionGPT presents an alternative approach to handling biases in large language models by making them explicit rather than suppressing them. The authors fine-tune a LLaMa model on Reddit data from 13 subreddits representing different demographics (political, geographic, gender, age), creating a model where users can select specific biases to see how different groups would respond to the same question. The resulting web demo allows side-by-side comparison of responses, revealing nuanced and distinct perspectives across demographics. The authors acknowledge that using Reddit data introduces a global layer of bias, meaning responses represent "Americans that post on Reddit" rather than the broader populations.

## Method Summary
The authors collected data from 13 Reddit subreddits following the "AskX" schema, filtering for high-quality posts under 80 words. They fine-tuned a 7 billion parameter LLaMa V1 model using full fine-tuning with prompts that include the subreddit name three times for conditioning. The trained model is deployed in a web demo where users input questions and select multiple biases to compare responses side-by-side. Evaluation combines qualitative inspection of outputs with quantitative analysis using sentiment and regard metrics from the BOLD dataset.

## Key Results
- The fine-tuned model generates distinct, nuanced responses that reflect the biases present in the subreddit training data
- Side-by-side comparison effectively reveals differences in how various demographic groups would respond to the same question
- Using Reddit as a data source introduces a global layer of bias, limiting generalizability to broader populations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLaMa on bias-specific Reddit corpora creates a model that generates distinct outputs for each bias when prompted with the subreddit name.
- Mechanism: The fine-tuning process conditions the model to associate specific vocabulary, sentiment, and response patterns with each subreddit's demographic. When the subreddit name is repeated in the prompt, it acts as a strong conditioning signal that triggers the learned bias-specific generation patterns.
- Core assumption: The subreddit corpus accurately represents the target demographic's perspectives and communication patterns.
- Evidence anchors:
  - [abstract] "we conducted a full fine-tuning of a LLaMa model... yielding a model in which the bias can selected when requesting an answer"
  - [section 2.2.1] "we include a mention of the specific bias into the model prompt... allows the user to specify the desired bias"
  - [corpus] Limited evidence that Reddit users accurately represent broader demographics
- Break condition: If the subreddit corpus contains mixed demographics or the prompt conditioning is weak, the model may generate generic responses that don't reflect the intended bias.

### Mechanism 2
- Claim: Side-by-side comparison of model responses makes implicit biases explicit and transparent.
- Mechanism: By generating responses for multiple biases to the same question, the system surfaces the variation in how different demographic groups would answer, making the bias visible through contrast.
- Core assumption: Users can meaningfully interpret and compare the differences in responses across biases.
- Evidence anchors:
  - [abstract] "allowing side-by-side comparison"
  - [section 1] "we aim to make them explicit and transparent"
  - [corpus] No direct evidence provided on user comprehension of bias differences
- Break condition: If responses are too similar across biases or if users lack context to interpret differences, the transparency benefit diminishes.

### Mechanism 3
- Claim: Using instruction-tuning with bias-specific prompts creates a controllable bias selection mechanism.
- Mechanism: The prompt format that repeats the subreddit name three times creates a strong contextual anchor that the fine-tuned model uses to switch between learned response patterns for different biases.
- Core assumption: The repetition of subreddit names in the prompt is sufficient to trigger the correct bias-specific generation.
- Evidence anchors:
  - [section 2.2.1] "we include a mention of the specific bias into the model prompt... allows the user to specify the desired bias"
  - [section 2.2.1] "we converged on a minimalistic prompt that repeats the subreddit name thrice"
  - [corpus] No quantitative evidence that three repetitions is optimal
- Break condition: If the prompt conditioning is insufficient or if biases overlap too much in the training data, the model may not reliably select the correct bias.

## Foundational Learning

- Concept: Instruction-tuning and fine-tuning
  - Why needed here: OpinionGPT requires fine-tuning LLaMa on a custom corpus to learn bias-specific response patterns
  - Quick check question: What's the difference between instruction-tuning and standard fine-tuning?

- Concept: Prompt engineering and conditioning
  - Why needed here: The repeated subreddit names in the prompt act as a conditioning signal to trigger bias-specific generation
  - Quick check question: How does prompt repetition strengthen conditioning in language models?

- Concept: Bias measurement and evaluation
  - Why needed here: The paper uses both qualitative inspection and quantitative metrics (sentiment analysis, regard) to evaluate bias in model outputs
  - Quick check question: What are the limitations of using sentiment analysis to measure bias?

## Architecture Onboarding

- Component map: Web interface -> Backend API -> Fine-tuned LLaMa model -> Reddit-derived bias corpus
- Critical path: User selects biases -> API receives prompt with subreddit names -> Model generates responses -> Results displayed side-by-side
- Design tradeoffs: Full fine-tuning vs. parameter-efficient methods (LoRA) - chose full fine-tuning for better bias capture at higher computational cost
- Failure signatures: Generic responses across biases, bias leakage between categories, inconsistent responses to the same question
- First 3 experiments:
  1. Test single bias selection with simple questions to verify prompt conditioning works
  2. Compare responses across all 11 biases for a politically charged question to check bias separation
  3. Run sentiment analysis on responses to verify quantitative bias measurement aligns with qualitative observations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the approach of making biases explicit and transparent, rather than suppressing them, in mitigating the potential negative impacts of biased AI systems?
- Basis in paper: [explicit] The authors present OpinionGPT as an alternative approach to dealing with biases in AI systems, stating "Rather than aiming to suppress them, we aim to make them explicit and transparent."
- Why unresolved: While the paper provides a qualitative evaluation of the model's outputs and a quantitative evaluation using the BOLD dataset, it does not directly assess the effectiveness of the approach in mitigating the potential negative impacts of biased AI systems.
- What evidence would resolve it: A comparative study of the impacts of biased AI systems that use the suppression approach versus those that use the explicit and transparent approach, such as OpinionGPT, could provide evidence of the effectiveness of the latter approach.

### Open Question 2
- Question: How well does OpinionGPT's fine-tuned model capture the nuances and complexities of different biases, and how does this affect the quality and accuracy of its generated responses?
- Basis in paper: [explicit] The authors state that they "qualitatively found full fine-tuning to better capture the biases of our corpus" and that the model "generates comprehensive and nuanced responses that reflect the training data's inherent biases."
- Why unresolved: The paper does not provide a detailed analysis of the model's ability to capture the nuances and complexities of different biases, nor does it explore the impact of this on the quality and accuracy of the generated responses.
- What evidence would resolve it: A more in-depth analysis of the model's responses, comparing them to the original training data and examining the nuances and complexities of the captured biases, could provide evidence of the model's effectiveness in this regard.

### Open Question 3
- Question: How does the global layer of bias introduced by using Reddit as a data source affect the overall performance and usefulness of OpinionGPT in representing different biases?
- Basis in paper: [explicit] The authors note that "using Reddit as a data source injects a global layer of bias to all model responses" and that responses by "Americans" should be understood as "Americans that post on Reddit."
- Why unresolved: The paper does not provide a detailed analysis of the impact of this global layer of bias on the model's performance and usefulness in representing different biases.
- What evidence would resolve it: A comparative analysis of the model's performance and usefulness in representing different biases when trained on Reddit data versus other data sources could provide evidence of the impact of the global layer of bias introduced by Reddit.

## Limitations

- Reddit data may not accurately represent the intended demographics, limiting generalizability beyond Reddit users
- Sentiment analysis and regard metrics have limitations for measuring complex social biases in model outputs
- The optimal prompt structure for bias conditioning is unknown, with no ablation studies comparing different approaches

## Confidence

**High confidence**: The technical approach of fine-tuning LLaMa on subreddit-derived data is well-established and the implementation details (data filtering, prompt structure) are clearly described. The web demo architecture and side-by-side comparison mechanism are straightforward and verifiable.

**Medium confidence**: The claim that the model generates distinct, bias-specific responses is supported by qualitative examples but lacks comprehensive quantitative validation. The separation between different demographic biases could be more rigorously tested with controlled experiments.

**Low confidence**: The assertion that Reddit data accurately represents the 11 identified biases is weakly supported. The paper doesn't provide demographic analysis of subreddit users or validate that responses truly reflect the intended demographic perspectives rather than general Reddit culture.

## Next Checks

1. **Demographic validation study**: Conduct user surveys or demographic analysis of Reddit users in the selected subreddits to verify that the data actually represents the intended demographics (e.g., confirm that r/AskWomen users are predominantly women of the claimed age ranges).

2. **Bias separation quantitative test**: Design a controlled experiment testing the model's ability to distinguish between similar biases (e.g., different age groups or political orientations) using a standardized evaluation set with known correct responses for each demographic.

3. **Prompt optimization ablation**: Systematically test different prompt strategies (number of repetitions, prompt templates, context length) to determine the optimal conditioning approach for bias selection and measure the impact on bias separation accuracy.