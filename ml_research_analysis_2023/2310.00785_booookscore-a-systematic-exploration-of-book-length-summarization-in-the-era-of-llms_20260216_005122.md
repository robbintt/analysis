---
ver: rpa2
title: 'BooookScore: A systematic exploration of book-length summarization in the
  era of LLMs'
arxiv_id: '2310.00785'
source_url: https://arxiv.org/abs/2310.00785
tags:
- summary
- summaries
- family
- they
- fiction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a systematic study of book-length summarization\
  \ using large language models (LLMs), addressing the challenge of summarizing texts\
  \ over 100K tokens that exceed LLM context windows. The authors propose two prompting\
  \ strategies\u2014hierarchical merging and incremental updating\u2014to process\
  \ long documents in chunks and merge summaries."
---

# BooookScore: A systematic exploration of book-length summarization in the era of LLMs

## Quick Facts
- arXiv ID: 2310.00785
- Source URL: https://arxiv.org/abs/2310.00785
- Reference count: 40
- This paper introduces BooookScore, an automatic metric for evaluating the coherence of book-length summaries generated by large language models.

## Executive Summary
This paper addresses the challenge of summarizing book-length texts (over 100K tokens) using large language models, which exceed standard context windows. The authors propose two prompting strategies—hierarchical merging and incremental updating—to process long documents in chunks and merge summaries. They collect human annotations on GPT-4-generated summaries of 100 newly-published books to identify eight common coherence error types, then develop BooookScore, an automatic metric that uses GPT-4 to detect these errors with 78.2% precision compared to human annotations. Systematic experiments show that GPT-4 and Claude 2 produce more coherent summaries than LLaMA 2, and that incremental updating yields lower BooookScore but higher detail than hierarchical merging.

## Method Summary
The authors propose a framework for evaluating book-length summarization by first collecting human annotations on 100 newly-published books to identify eight coherence error types. They implement two prompting strategies: hierarchical merging (processing chunks independently then merging) and incremental updating (building a running summary that gets updated and compressed with each chunk). Using these strategies with multiple LLMs (GPT-4, Claude 2, ChatGPT, LLaMA 2), they generate summaries and evaluate them using BooookScore, an automatic metric that leverages GPT-4 to detect the identified coherence errors. The metric computes a score as the proportion of error-free sentences, validated against human annotations to ensure reliability.

## Key Results
- BooookScore achieves 78.2% precision compared to human annotations when detecting coherence errors in book-length summaries
- GPT-4 and Claude 2 produce significantly more coherent summaries than LLaMA 2, with hierarchical merging generally outperforming incremental updating in coherence
- BooookScore reveals that while incremental updating produces summaries with higher detail, these summaries have lower coherence scores than hierarchically merged summaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical merging strategy improves coherence by progressively integrating context, while incremental updating allows deeper detail but risks incoherence due to repeated compression.
- Mechanism: Hierarchical merging processes chunks independently then merges summaries in a bottom-up fashion, preserving context by merging preceding summaries with each new chunk. Incremental updating builds a running summary that gets updated and compressed with each chunk, potentially losing coherence over time.
- Core assumption: LLMs can maintain coherence better when given aggregated context than when updating a compressed running summary.
- Evidence anchors:
  - [abstract] "hierarchical summaries generally have lower BOOOOK SCORE than incremental summaries, likely because the incremental updating task requires the base LLMs to follow more complex instructions"
  - [section] "hierarchical merging generally results in more coherent summaries but reduced level of detail compared to incremental updating"
- Break condition: If the base LLM cannot effectively compress without losing coherence, or if chunk dependencies are minimal, the advantage of hierarchical merging diminishes.

### Mechanism 2
- Claim: BooookScore is an effective automatic metric because it leverages GPT-4's instruction-following ability to detect specific coherence error types.
- Mechanism: BooookScore iterates over each sentence in a summary, prompting GPT-4 to identify any of the eight defined coherence error types. It computes a score as the proportion of sentences without errors.
- Core assumption: GPT-4 can reliably identify coherence errors as well as human annotators when given clear definitions and examples.
- Evidence anchors:
  - [abstract] "BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters"
  - [section] "Human validation shows that BooookScore's annotations are almost as reliable as those of human annotators (78.2% vs. 79.7%)"
- Break condition: If GPT-4's error detection precision drops below human-level, or if the prompt is not properly tuned, BooookScore becomes unreliable.

### Mechanism 3
- Claim: Closed-source LLMs (GPT-4, Claude 2) outperform open-source models (LLaMA 2) on book-length summarization due to superior instruction-following.
- Mechanism: Closed-source models better follow complex instructions for chunking, summarizing, updating, and compressing long documents, resulting in higher BooookScore and fewer artifacts.
- Core assumption: Instruction-following capability is the key differentiator in book-length summarization quality.
- Evidence anchors:
  - [abstract] "GPT-4 and Claude 2 produce the most coherent summaries as measured by BooookScore, followed closely by GPT-4 and distantly by ChatGPT and LLaMA 2"
  - [section] "LLaMA 2 struggles on book-length summarization... This is also the only configuration whose summaries contain significant repetition"
- Break condition: If open-source models improve instruction-following or if the evaluation metric is biased toward certain models, the performance gap may narrow.

## Foundational Learning

- Concept: Chunking strategy for long documents
  - Why needed here: Book-length documents exceed LLM context windows, requiring division into manageable chunks for summarization.
  - Quick check question: What is the minimum chunk size that allows both sufficient context for coherence and efficient processing by the LLM?

- Concept: Prompt engineering for complex instructions
  - Why needed here: The summarization task requires LLMs to summarize, merge, update, and compress text; poor prompt design leads to artifacts and incoherence.
  - Quick check question: How do you structure a prompt to ensure the LLM follows a multi-step summarization workflow without deviating?

- Concept: Automatic evaluation via LLM-as-a-judge
  - Why needed here: Human evaluation is expensive and slow; BooookScore automates coherence assessment using GPT-4.
  - Quick check question: What are the key components of an effective prompt to make an LLM act as a judge for summary coherence?

## Architecture Onboarding

- Component map:
  Input: Book-length document (>100K tokens) -> Chunking module -> Summarization strategy (Hierarchical/Incremental) -> LLM core -> Evaluation module (BooookScore) -> Output: Summary with coherence score

- Critical path:
  1. Split document into chunks
  2. Generate summaries per chunk or update running summary
  3. Merge/compress summaries to final output
  4. Evaluate coherence using BooookScore
  5. (Optional) Human validation for calibration

- Design tradeoffs:
  - Chunk size vs. context retention: Larger chunks reduce steps but may exceed context limits; smaller chunks preserve context but increase steps.
  - Hierarchical merging vs. incremental updating: Hierarchical yields better coherence but less detail; incremental preserves detail but risks incoherence.
  - Open vs. closed-source LLMs: Open-source is cheaper but less capable; closed-source is expensive but more reliable.

- Failure signatures:
  - Repetition in summaries: Indicates poor compression or lack of instruction-following
  - Incoherent jumps: Suggests chunk dependencies not handled well
  - Low BooookScore with high human preference: Implies metric bias or mismatch in evaluation criteria

- First 3 experiments:
  1. Compare BooookScore scores for hierarchical vs. incremental strategies on a fixed LLM and chunk size
  2. Vary chunk size (e.g., 2048 vs. 8192 tokens) and measure impact on BooookScore for both strategies
  3. Test different base LLMs (GPT-4, Claude 2, LLaMA 2) using the same summarization strategy and chunk size

## Open Questions the Paper Calls Out

- Question: What is the impact of increasing chunk size beyond 88K tokens on Claude 2's incremental updating performance?
  - Basis in paper: [explicit] The paper mentions that Claude 2 with a chunk size of 88K performs slightly better in incremental updating than hierarchical merging, but doesn't explore larger chunk sizes.
  - Why unresolved: The paper only tests Claude 2 at 88K chunk size for incremental updating, leaving the question of whether even larger chunks would improve performance further.
  - What evidence would resolve it: Systematic experiments comparing Claude 2's performance at various chunk sizes (e.g., 88K, 100K, 150K) for both incremental updating and hierarchical merging would show if there's an optimal chunk size or if performance plateaus.

- Question: How would implementing BooookScore with a pool of diverse LLM annotators affect its reliability and precision compared to using a single LLM (GPT-4)?
  - Basis in paper: [explicit] The paper mentions that using GPT-4 for BooookScore may have systematic biases and suggests that using a pool of LLM annotators could be an improvement.
  - Why unresolved: The paper only implements BooookScore with GPT-4, so the potential benefits of using multiple LLM annotators remain untested.
  - What evidence would resolve it: Experiments comparing BooookScore's performance when implemented with different combinations of LLM annotators (e.g., GPT-4, Claude 2, LLaMA 2) against human annotations would reveal if a diverse pool improves reliability.

- Question: How does the level of detail in incremental summaries affect human preferences compared to the coherence measured by BooookScore?
  - Basis in paper: [explicit] The paper notes that incremental summaries are preferred for their higher level of detail despite having lower BooookScore coherence ratings.
  - Why unresolved: The paper doesn't explore the trade-off between detail and coherence in depth, leaving questions about when users might prefer less coherent but more detailed summaries.
  - What evidence would resolve it: User studies where participants rate both the coherence and detail level of incremental vs. hierarchical summaries, correlating these preferences with BooookScore ratings, would clarify the relationship between coherence and detail preferences.

## Limitations

- The BooookScore metric relies heavily on GPT-4's error detection capabilities, which may have systematic biases that affect its generalizability across different text genres and writing styles
- The evaluation focuses primarily on coherence without explicitly measuring other summary quality dimensions such as informativeness, fluency, or faithfulness to the source text
- The experiments are limited to recently-published books from specific genres, which may not represent the full diversity of long-form documents that require summarization

## Confidence

- High confidence: BooookScore's precision (78.2% agreement with human annotations) and the relative performance ordering of LLMs (GPT-4 and Claude 2 outperforming LLaMA 2)
- Medium confidence: The proposed mechanisms for hierarchical merging versus incremental updating, as the evidence shows consistent patterns but doesn't establish causal mechanisms beyond correlation
- Medium to Low confidence: The open-source versus closed-source model comparisons, as the evaluation depends heavily on instruction-following capabilities that may evolve rapidly

## Next Checks

1. Test BooookScore on summaries of non-book long documents (legal contracts, scientific papers) to assess metric generality beyond narrative text
2. Conduct ablation studies on prompt design for BooookScore to quantify the impact of few-shot demonstrations versus error definitions alone
3. Evaluate whether human raters prefer incremental updating summaries despite their lower BooookScore, to identify potential metric limitations in capturing summary quality dimensions beyond coherence