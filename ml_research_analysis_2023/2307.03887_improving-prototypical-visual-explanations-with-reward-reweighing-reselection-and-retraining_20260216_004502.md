---
ver: rpa2
title: Improving Prototypical Visual Explanations with Reward Reweighing, Reselection,
  and Retraining
arxiv_id: '2307.03887'
source_url: https://arxiv.org/abs/2307.03887
tags:
- prototypes
- reward
- protopnet
- prototype
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Reward Reweighing, Reselecting, and Retraining
  (R3) framework to improve the interpretability and accuracy of ProtoPNet image classifiers.
  The authors collect human feedback on prototype quality using a 1-5 rating scale
  on the CUB-200-2011 dataset, train a reward model to predict these ratings, and
  then use the reward model to guide three corrective updates to a pretrained ProtoPNet.
---

# Improving Prototypical Visual Explanations with Reward Reweighing, Reselection, and Retraining

## Quick Facts
- arXiv ID: 2307.03887
- Source URL: https://arxiv.org/abs/2307.03887
- Reference count: 26
- Authors: Present a reward-guided framework to improve ProtoPNet interpretability and accuracy.

## Executive Summary
This paper addresses the interpretability-accuracy trade-off in ProtoPNet image classifiers by introducing a three-step reward-guided correction framework. The authors collect human ratings on prototype quality, train a reward model to predict these ratings, and use the reward model to reweigh, reselect, and retrain prototypes. This approach improves both interpretability (measured by reward scores and class mismatch) and predictive accuracy, with ensemble methods further boosting performance. The method is validated on the CUB-200-2011 dataset, demonstrating consistent improvements over baselines.

## Method Summary
The R3 framework improves ProtoPNet interpretability and accuracy through three corrective updates guided by a reward model trained on human feedback. First, prototypes are reweighed by their reward scores, emphasizing high-quality ones in the loss function. Second, low-reward prototypes are replaced by selecting high-reward patches from training images via random sampling. Finally, the model is retrained to realign features with the updated prototypes, recovering predictive accuracy while preserving interpretability gains.

## Key Results
- R3 consistently improves interpretability and accuracy over baselines on CUB-200-2011.
- Ensembling multiple R3-ProtoPNets further boosts performance.
- The reward model achieves 91.5% accuracy in ranking human preferences.
- Retraining after prototype updates recovers accuracy while maintaining interpretability gains.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward reweighing realigns prototypes to high-quality human-annotated patches by weighting prototype updates by the learned reward score.
- Mechanism: The R2 update modifies the prototype update loss to be weighted by r(xi, pj), so gradients from high-reward patches dominate, pulling prototypes toward semantically meaningful image regions.
- Core assumption: The reward model accurately ranks prototype quality; human ratings are stable and discriminative.
- Evidence anchors:
  - [abstract]: "train a reward model to predict these ratings, and then use the reward model to guide three corrective updates"
  - [section]: "maximizes the loss is to minimize the distance between prototype and image patches with high reward r(xi, pj)"
  - [corpus]: Weak - no neighboring papers discuss reward reweighing directly; only one general ProtoPNet method.
- Break condition: If reward model predictions are noisy or ratings inconsistent, updates will reinforce spurious patches.

### Mechanism 2
- Claim: Prototype reselection replaces low-reward prototypes with high-reward random patches, improving semantic coverage.
- Mechanism: For prototypes with mean reward below threshold α, the method samples random patches from the same class and accepts them if their mean reward exceeds β, replacing the old prototype.
- Core assumption: High-reward patches exist in training data and are representative; random sampling plus reward threshold suffices for exploration.
- Evidence anchors:
  - [section]: "Given a prototype pj and image xi, if 1/nk Σi∈I(pj) r(xi, pj) < α... we reselect the prototype"
  - [section]: "If 1/nk Σi∈I(pj) r(x′i, p′j) > β... then we accept the patch candidate as the new prototype"
  - [corpus]: Weak - neighboring papers focus on ProtoPNet variants but none describe reselection by reward.
- Break condition: If reward threshold β is too high, few patches are accepted, limiting exploration; if too low, duplicates or spurious patches persist.

### Mechanism 3
- Claim: Retraining realigns the rest of the network to updated prototypes, recovering predictive accuracy while preserving interpretability gains.
- Mechanism: After R2 updates, the model is retrained with the original ProtoPNet loss, allowing convolutional and classifier layers to adapt to new prototype positions and semantics.
- Core assumption: The feature extractor and classifier can adjust to shifted prototype space without destroying interpretability; predictive loss is recoverable.
- Evidence anchors:
  - [abstract]: "Finally, the model is retrained to realign its features with the updated prototypes"
  - [section]: "retraining increases predictive accuracy while maintaining the quality increases of the R2 update"
  - [corpus]: Weak - neighboring papers discuss interpretability but not retraining after prototype realignment.
- Break condition: If prototypes shift too far, retraining cannot reconcile them with learned features, causing accuracy drop.

## Foundational Learning

- Concept: Prototype-based interpretability in ProtoPNet
  - Why needed here: Understanding how ProtoPNet learns patches that are both predictive and semantically meaningful is essential to grasp why reward-guided updates help.
  - Quick check question: What does the ProtoPNet prototype layer output and how are prototypes selected from training images?
- Concept: Reward learning from pairwise comparisons (Bradley-Terry model)
  - Why needed here: R3 uses a reward model trained on human ratings converted into pairwise comparisons; knowing this mechanism is key to understanding prototype quality measurement.
  - Quick check question: How does converting scalar ratings into pairwise comparisons improve reward model training?
- Concept: Reinforcement learning with human feedback (RLHF) pipeline
  - Why needed here: R3 adapts RLHF ideas to ProtoPNet without full RL; understanding the RLHF motivation explains the design choices.
  - Quick check question: What is the difference between reward reweighing and full RL fine-tuning in RLHF?

## Architecture Onboarding

- Component map:
  - Base CNN (e.g., VGG-19, ResNet-34) → Prototype layer (gp) → Classifier (h)
  - Human feedback collection pipeline → Reward model (ResNet-50 + linear head) → R3 update loop (reweigh → reselect → retrain)
- Critical path:
  1. Train base ProtoPNet
  2. Collect human ratings on prototype activations
  3. Train reward model
  4. Apply R2 update (reweigh + reselect)
  5. Retrain full model
- Design tradeoffs:
  - Reward reweighing vs. full RL: simpler, faster, but may converge to local optima
  - Random patch sampling vs. exhaustive search: less computation, but may miss best patches
  - Retraining vs. freezing early layers: retraining recovers accuracy but risks losing interpretability gains
- Failure signatures:
  - Low reward model accuracy → prototypes not properly ranked → updates reinforce spurious patches
  - High duplicate prototypes → reward model not penalizing similarity
  - Accuracy drop after R2 but no recovery after retraining → misalignment between prototypes and features
- First 3 experiments:
  1. Train base ProtoPNet, visualize prototype activations, check class mismatch on training set
  2. Collect 300 human ratings, train reward model, validate ranking accuracy on held-out pairs
  3. Run R2 update, measure average reward and class mismatch before/after, compare prototype images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does R3-ProtoPNet perform on datasets beyond CUB-200-2011, such as medical imaging or other fine-grained classification tasks?
- Basis in paper: [inferred] The paper focuses exclusively on the CUB-200-2011 dataset for experiments.
- Why unresolved: The authors only tested R3-ProtoPNet on one dataset, limiting generalizability claims.
- What evidence would resolve it: Results from applying R3-ProtoPNet to diverse datasets like medical images or different fine-grained classification benchmarks.

### Open Question 2
- Question: Can the reward model be extended to evaluate prototype consistency across multiple images, rather than just single image-heatmap pairs?
- Basis in paper: [explicit] The authors mention this as a limitation and future direction in Section 6.
- Why unresolved: Current reward model only captures overlap with object of interest, not consistency across images.
- What evidence would resolve it: A reward model trained on multi-image prototype evaluations showing improved consistency metrics.

### Open Question 3
- Question: How can the duplicate prototype issue be effectively addressed while maintaining high reward scores?
- Basis in paper: [explicit] The authors note that R3-ProtoPNet fails to entirely eliminate duplicates in Section 6.
- Why unresolved: Current reward-based reselection doesn't explicitly account for prototype diversity.
- What evidence would resolve it: Modified reward models or reselection algorithms that reduce duplicates while maintaining or improving interpretability and accuracy.

### Open Question 4
- Question: Could human feedback be integrated in a way that allows the model to critique and potentially reject the feedback when it conflicts with effective learning?
- Basis in paper: [explicit] The authors discuss this possibility in Section 6 as future work.
- Why unresolved: Current framework assumes human feedback is always beneficial without considering model pushback.
- What evidence would resolve it: A bidirectional feedback system where the model can identify and explain when human feedback might be counterproductive.

## Limitations

- The reward model's accuracy (91.5%) is reported on a held-out pairwise ranking task but its robustness to new prototypes or domains is unclear.
- Random patch reselection may miss optimal replacements, especially if β is set too high.
- Retraining could overfit to the updated prototype set, degrading generalization.
- No ablation is shown for each R3 component, so the relative contribution of each is uncertain.
- The method assumes human ratings are stable and discriminative across classes, which may not hold for all datasets.

## Confidence

- Reward reweighing mechanism: **Medium** (dependent on reward model quality)
- Prototype reselection mechanism: **Medium** (random sampling may miss optimal patches)
- Retraining mechanism: **High** (standard practice, but no ablation shown)

## Next Checks

1. Measure reward model calibration on a held-out prototype set and test its performance on new classes.
2. Compare random reselection with exhaustive patch search to quantify exploration vs. exploitation trade-off.
3. Perform an ablation study: R2 vs. R2+R3 to isolate the contribution of retraining to accuracy gains.