---
ver: rpa2
title: A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied
  AI
arxiv_id: '2307.11343'
source_url: https://arxiv.org/abs/2307.11343
tags:
- tasks
- test
- strategy
- maniskill2
- body
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-stage fine-tuning strategy to enhance
  the generalization capabilities of embodied AI models for manipulation tasks. The
  method utilizes PointNet to extract point cloud features, followed by Imitation
  or Reinforcement Learning algorithms to determine the agent's actions.
---

# A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI

## Quick Facts
- arXiv ID: 2307.11343
- Source URL: https://arxiv.org/abs/2307.11343
- Reference count: 17
- This paper introduces a two-stage fine-tuning strategy to enhance the generalization capabilities of embodied AI models for manipulation tasks, achieving 1st place in all three tracks of the ManiSkill2 Challenge.

## Executive Summary
This paper presents a two-stage fine-tuning strategy to improve the generalization of embodied AI models for manipulation tasks. The method combines PointNet for point cloud feature extraction with Imitation Learning or Reinforcement Learning algorithms to determine agent actions. For soft-body tasks, Behavior Cloning is employed, while for rigid-body tasks, Proximal Policy Optimization is used. The two-stage approach involves resuming training from the highest checkpoint and reducing batch size and samples per step to mitigate overfitting. This strategy achieved top performance in the ManiSkill2 Challenge, demonstrating its effectiveness in improving model performance and generalization across various manipulation tasks.

## Method Summary
The method uses PointNet to extract point cloud features, followed by Imitation Learning (BC) for soft-body tasks or Reinforcement Learning (PPO) for rigid-body tasks to determine the agent's actions. The two-stage fine-tuning strategy first trains to reach the highest checkpoint, then resumes from that checkpoint with reduced batch size and samples per step to mitigate overfitting. The approach was evaluated on the ManiSkill2 benchmark, which includes 20 tasks divided into soft-body (6 tasks) and rigid-body (14 tasks) categories.

## Key Results
- Achieved 1st place in all three tracks of the ManiSkill2 Challenge: Imitation/Reinforcement Learning (Rigid Body), No Restriction (Rigid Body), and No Restriction (Soft Body)
- Demonstrated improved generalization capabilities for embodied AI models in manipulation tasks
- Showed that reducing batch size and samples per step in the second stage helps mitigate overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing batch size and sample count in stage two forces the model to focus on finer-grained features, improving generalization.
- Mechanism: Smaller batch sizes increase gradient noise, which can help escape shallow local minima and prevent overfitting to training distributions.
- Core assumption: The overfitting in stage one is due to the model memorizing task-specific patterns rather than learning transferable representations.
- Evidence anchors:
  - [abstract] "reducing batch size and samples per step to mitigate overfitting"
  - [section] "This reduction encourages the model to pay more attention to smaller volumes of information... introduce more noise into the training process"
  - [corpus] Weak: No direct citation, but aligns with general regularization theory.
- Break condition: If noise level becomes too high, training diverges or performance degrades.

### Mechanism 2
- Claim: Resuming from the highest checkpoint preserves the best-performing policy while allowing further fine-tuning.
- Mechanism: Checkpointing before overfitting begins provides a stable base; subsequent smaller-step training refines the policy without catastrophic forgetting.
- Core assumption: The peak checkpoint captures a generalizable policy state before it overfits to the training environment.
- Evidence anchors:
  - [section] "resume the training process from the highest score checkpoint obtained in the initial stage"
  - [corpus] No direct anchor; relies on standard checkpointing practices in RL.
- Break condition: If the checkpoint is already overfit, resuming yields no benefit.

### Mechanism 3
- Claim: Separating BC for soft-body and PPO for rigid-body tasks aligns the learning algorithm with task dynamics.
- Mechanism: Soft bodies require precise pose estimation and stability, better served by BC; rigid bodies benefit from exploration and reward shaping, better served by PPO.
- Core assumption: Task type dictates the most effective learning paradigm.
- Evidence anchors:
  - [abstract] "For soft-body tasks, Behavior Cloning is employed, while for rigid-body tasks, Proximal Policy Optimization is used."
  - [section] "The overall policy bears similarities to that in rigid-body tasks, with the primary difference being the substitution of PPO algorithms with BC algorithms."
  - [corpus] Weak: No explicit citation, but follows standard RL practice.
- Break condition: If task dynamics overlap significantly, a unified algorithm might perform equally well.

## Foundational Learning

- Concept: Point cloud feature extraction with PointNet
  - Why needed here: Provides geometric understanding of the environment for manipulation planning.
  - Quick check question: How does PointNet handle unordered point sets?

- Concept: Behavior Cloning vs. Reinforcement Learning
  - Why needed here: BC is data-efficient for stable tasks; RL allows exploration for complex, reward-driven tasks.
  - Quick check question: When would BC fail to generalize beyond the demonstration distribution?

- Concept: Overfitting detection and mitigation
  - Why needed here: Prevents the model from memorizing training environments at the cost of real-world generalization.
  - Quick check question: What metric would you monitor to detect overfitting in RL?

## Architecture Onboarding

- Component map: PointNet → Policy Network (BC or PPO) → Controller → Robot Actuators
- Critical path: Sensor input → PointNet → Action selection → Controller translation → Execution
- Design tradeoffs: BC is safer but less adaptive; PPO is more flexible but requires more data and tuning.
- Failure signatures: High training success but low test success indicates overfitting; unstable policies indicate insufficient exploration.
- First 3 experiments:
  1. Run stage one training, monitor success rate vs. epoch to identify peak checkpoint.
  2. Apply stage two with reduced batch size, observe impact on validation success rate.
  3. Swap BC/PPO assignments between task types to confirm algorithm-task alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the two-stage fine-tuning strategy vary with different initial model architectures beyond PointNet for feature extraction?
- Basis in paper: [explicit] The paper mentions using PointNet for feature extraction but does not explore other architectures.
- Why unresolved: The study focuses solely on PointNet and does not provide comparative analysis with other feature extraction methods.
- What evidence would resolve it: Experiments comparing the performance of the two-stage fine-tuning strategy with various feature extraction architectures like PointCNN, DGCNN, etc.

### Open Question 2
- Question: What are the specific effects of reducing batch size and samples per step on the model's ability to generalize to entirely new tasks outside the ManiSkill2 benchmark?
- Basis in paper: [explicit] The paper discusses the effects of reducing batch size and samples per step but focuses on the ManiSkill2 tasks.
- Why unresolved: The paper does not explore the generalization of these effects to tasks beyond the ManiSkill2 benchmark.
- What evidence would resolve it: Testing the model on a new set of tasks not included in the ManiSkill2 benchmark to evaluate the generalization of the two-stage fine-tuning strategy.

### Open Question 3
- Question: How does the two-stage fine-tuning strategy perform when applied to real-world robotic systems, considering the limitations and variations in real-world environments?
- Basis in paper: [inferred] The paper mentions the potential for practical applications in real-world scenarios but does not provide empirical evidence.
- Why unresolved: The study is limited to simulation environments and does not address the challenges of real-world deployment.
- What evidence would resolve it: Implementing the strategy on real robots and comparing performance metrics in real-world tasks to those achieved in simulation.

## Limitations
- Lack of ablation studies to confirm whether checkpointing or batch reduction alone would suffice
- No direct comparison with baseline models trained using standard fine-tuning procedures
- Limited exploration of hyperparameter sensitivity for the reduction factors (0.9 batch size, 0.875 samples per step)

## Confidence
- High confidence in the effectiveness of using PPO for rigid-body tasks and BC for soft-body tasks
- Medium confidence in the two-stage fine-tuning mechanism's contribution to generalization
- Low confidence in the specific hyperparameters (0.9 batch size reduction, 0.875 samples per step) being optimal or necessary

## Next Checks
1. Run an ablation study comparing stage 1 alone, stage 2 alone, and the full two-stage approach on identical initial checkpoints to quantify each component's contribution.
2. Implement cross-validation by training multiple instances with different random seeds to establish statistical significance of the performance improvements.
3. Test the transferability of stage 2 fine-tuning to different initial checkpoint qualities (early vs. late stage checkpoints) to verify the assumption that the "highest score checkpoint" is always optimal.