---
ver: rpa2
title: 'DP-AdamBC: Your DP-Adam Is Actually DP-SGD (Unless You Apply Bias Correction)'
arxiv_id: '2312.14334'
source_url: https://arxiv.org/abs/2312.14334
tags:
- dp-adam
- dp-adambc
- adam
- noise
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and corrects a bias introduced by differential
  privacy noise in the Adam optimizer's second moment estimation. Under typical DP
  parameters, the added Gaussian noise dominates the second moment estimator, causing
  DP-Adam to behave more like DP-SGD with momentum than Adam.
---

# DP-AdamBC: Your DP-Adam Is Actually DP-SGD (Unless You Apply Bias Correction)

## Quick Facts
- arXiv ID: 2312.14334
- Source URL: https://arxiv.org/abs/2312.14334
- Reference count: 40
- Key outcome: DP-AdamBC corrects DP-induced bias in Adam's second moment estimator, improving test accuracy by up to 3.5 percentage points compared to DP-Adam.

## Executive Summary
This paper identifies and corrects a bias introduced by differential privacy noise in the Adam optimizer's second moment estimation. Under typical DP parameters, the added Gaussian noise dominates the second moment estimator, causing DP-Adam to behave more like DP-SGD with momentum than Adam. The authors propose DP-AdamBC, which corrects for this bias by subtracting the expected DP noise from the second moment estimate. Theoretically, they prove that DP-AdamBC is a consistent estimator of the Adam update under stationarity assumptions, and empirically show that it improves test accuracy by up to 3.5 percentage points on image, text, and graph node classification tasks compared to DP-Adam. The correction is simple to implement and preserves privacy guarantees while restoring Adam's intended optimization behavior under DP.

## Method Summary
DP-AdamBC implements bias correction in the second moment estimation of Adam under differential privacy. The algorithm computes the expected DP noise contribution (Φ = (1 - β^t2)(σC/B)²) and subtracts it from the second moment estimate. This correction restores Adam's sign-descent behavior by removing the constant bias that would otherwise dominate the denominator of the update rule. The method uses a numerical stability constant γ' to prevent division by zero when the corrected second moment becomes small.

## Key Results
- DP-AdamBC improves test accuracy by up to 3.5 percentage points compared to DP-Adam
- The algorithm successfully restores Adam's intended behavior under DP constraints
- DP-AdamBC outperforms both DP-Adam and DP-SGD across image, text, and graph node classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DP noise adds a constant bias Φ to Adam's second moment estimator, which dominates the denominator and changes the update behavior.
- Mechanism: Gaussian noise added for DP is independent of gradients. When computing the second moment via moving average, this noise contributes a constant Φ to the estimate, shifting the denominator from √(E[g²]) to √(E[g²] + Φ).
- Core assumption: DP noise is independent of gradients and the second moment is computed as a moving average over time.
- Evidence anchors:
  - [abstract] "This DP bias leads to a different scaling for low variance parameter updates, that is inconsistent with the behavior of non-private Adam."
  - [section] "By independence of the DP noise zt and gt, we have that: E[vp_t] = E[(1 − β2)∑βt−τ2 E[g²τ]] + (1 − βt2)(σC/B)² ≜ E[vc_t] + Φ"
  - [corpus] Weak - no direct corpus evidence found, but DP noise bias is well-established in differential privacy literature.
- Break condition: If DP noise variance becomes negligible compared to true gradient variance, the bias effect disappears.

### Mechanism 2
- Claim: Correcting for the DP bias Φ restores Adam's sign-descent behavior under DP.
- Mechanism: By subtracting the expected DP noise contribution Φ from the second moment estimate, the denominator returns to approximately √(E[g²]), allowing Adam to perform sign-descent updates again.
- Core assumption: The DP noise bias Φ can be computed and subtracted accurately.
- Evidence anchors:
  - [abstract] "We propose DP-AdamBC, an optimization algorithm which removes the bias in the second moment estimation and retrieves the expected behaviour of Adam."
  - [section] "We propose to correct for this bias by changing the Adam update ∆t as follows: ∆t = ˆmt/√max(ˆvt − Φ, γ')"
  - [corpus] Weak - no direct corpus evidence, but bias correction is a standard technique in signal processing.
- Break condition: If the DP noise variance is much larger than the true gradient variance, the correction becomes numerically unstable.

### Mechanism 3
- Claim: Under typical DP parameters, DP-Adam reduces to DP-SGD with momentum due to the bias in the second moment.
- Mechanism: When Φ ≫ E[g²], the second moment estimate becomes dominated by noise, making the Adam update proportional to the first moment estimate with a constant denominator - exactly the behavior of SGD with momentum.
- Core assumption: DP noise variance is large relative to gradient variance.
- Evidence anchors:
  - [abstract] "under typical DP parameters DP-Adam reduces to DP-SGD with momentum"
  - [section] "For example: if |E[gt]|i = √0.1Φ, the update will be ≈ ± 0.1, whereas it will be ≈ ±1 if |E[gt]|i = √10Φ."
  - [corpus] Weak - no direct corpus evidence, but SGD with momentum is a well-studied optimization algorithm.
- Break condition: If DP noise variance becomes negligible compared to gradient variance, Adam's unique behavior emerges.

## Foundational Learning

- Concept: Differential Privacy (DP) mechanism
  - Why needed here: Understanding how DP noise is added to gradients and its statistical properties is crucial for understanding the bias mechanism.
  - Quick check question: What is the variance of Gaussian noise added for (ε,δ)-DP with clipping parameter C and noise multiplier σ?

- Concept: Adam optimizer mechanics
  - Why needed here: Understanding how Adam computes and uses first and second moment estimates is essential for grasping why DP noise affects it differently than other optimizers.
  - Quick check question: How does Adam's update rule differ from SGD, and what role does the second moment estimate play?

- Concept: Sign-descent hypothesis
  - Why needed here: The paper's core argument is that Adam performs well because it approximates sign-descent, and DP noise breaks this property.
  - Quick check question: Under what conditions does Adam's update approximate sign-descent, and how does this relate to gradient variance?

## Architecture Onboarding

- Component map: Gradient computation (with DP noise) → moment estimation (first and second) → bias correction (subtracting Φ) → parameter update
- Critical path: Gradient computation → moment estimation → bias correction → parameter update. Any failure in these steps affects final performance.
- Design tradeoffs: Adding bias correction introduces numerical stability concerns (handled by γ') but restores Adam's intended behavior. The tradeoff is between accuracy and numerical stability.
- Failure signatures: If Φ is computed incorrectly or γ' is poorly tuned, the correction can make performance worse than uncorrected DP-Adam. If DP noise is too large, correction may not be effective.
- First 3 experiments:
  1. Run DP-Adam and DP-AdamBC on a simple NLP task (like SNLI) with varying σ values to observe the transition from Adam-like to SGD-like behavior.
  2. Compare the distribution of second moment estimates (ˆvt) with and without bias correction to verify the theoretical bias Φ.
  3. Test different values of γ' to find the optimal balance between numerical stability and correction accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does DP-AdamBC provide the most significant performance improvement over DP-Adam and DP-SGD?
- Basis in paper: [explicit] The authors hypothesize that DP-AdamBC has larger advantages on tasks where Adam outperforms SGD in non-private settings, based on empirical observations across different datasets and tasks.
- Why unresolved: The paper provides evidence supporting this hypothesis through experiments on SNLI, QNLI, CIFAR10, and ogbn-arxiv, but does not establish precise theoretical conditions or provide a comprehensive characterization of when DP-AdamBC is most beneficial.
- What evidence would resolve it: Systematic experiments across a broader range of tasks, model architectures, and DP parameters, combined with theoretical analysis linking the sign-descent behavior of Adam to DP-AdamBC's performance gains.

### Open Question 2
- Question: How does the choice of hyperparameters (β1, β2, γ') in DP-AdamBC affect its convergence properties and final performance under differential privacy constraints?
- Basis in paper: [explicit] The authors discuss the impact of β coefficients on the effective length of the moving average window and the importance of γ' for numerical stability, but do not provide a complete theoretical analysis of the convergence properties as a function of these hyperparameters.
- Why unresolved: While the paper provides some empirical insights into hyperparameter tuning and mentions a convergence analysis in Appendix F, it does not fully explore the theoretical implications of different hyperparameter choices on convergence rates and final performance.
- What evidence would resolve it: Theoretical analysis of convergence rates as a function of hyperparameters, combined with extensive empirical studies systematically varying β1, β2, and γ' across different tasks and DP parameters.

### Open Question 3
- Question: Can the bias correction technique used in DP-AdamBC be extended to other adaptive optimization algorithms beyond Adam, such as RMSprop or Adagrad, to improve their performance under differential privacy?
- Basis in paper: [inferred] The paper identifies the root cause of DP-Adam's performance degradation as a bias in the second moment estimator due to DP noise, and proposes a bias correction technique. This suggests that similar biases might affect other adaptive optimizers and that analogous correction techniques could be beneficial.
- Why unresolved: The paper focuses specifically on Adam and does not explore the applicability of the bias correction technique to other adaptive optimization algorithms. The authors mention a comparison to DP2-RMSProp but do not provide a general framework for extending the bias correction to other optimizers.
- What evidence would resolve it: Development and empirical evaluation of bias correction techniques for other adaptive optimizers (e.g., RMSprop, Adagrad) under differential privacy, demonstrating improvements in performance and analyzing the underlying mechanisms.

## Limitations

- The analysis is based on theoretical derivation and controlled experimental conditions, which may not capture all real-world scenarios.
- The numerical stability constant γ' introduces a hyperparameter that requires tuning and may vary across datasets and models.
- Empirical validation is limited to three datasets (image, text, graph), leaving performance on other domains untested.

## Confidence

- **High confidence**: The mathematical derivation of the DP bias Φ and the bias correction mechanism are rigorously proven. The claim that DP noise introduces a constant bias in the second moment estimator is theoretically sound and empirically validated through experimental results showing improved accuracy.
- **Medium confidence**: The assertion that DP-Adam behaves like DP-SGD with momentum under typical parameters is supported by theory and experiments, but may not hold for all gradient distributions or model architectures. The effectiveness of the correction depends on the relative magnitude of DP noise versus gradient variance.
- **Medium confidence**: The experimental results showing 3.5 percentage point improvements are convincing but may not generalize to all model architectures, datasets, or privacy budgets. The selection of datasets (image, text, graph) provides some diversity but doesn't cover the full spectrum of potential applications.

## Next Checks

1. **Gradient variance analysis**: Measure the ratio of DP noise variance to gradient variance across different layers and training stages to quantify when the bias correction is most effective. This would validate the core assumption that DP noise dominates the second moment estimate.

2. **Cross-domain generalization**: Implement DP-AdamBC on additional tasks such as object detection, semantic segmentation, or reinforcement learning to assess whether the performance gains transfer to domains with different gradient characteristics and optimization challenges.

3. **Hyperparameter sensitivity**: Systematically vary the numerical stability constant γ' and learning rates to establish guidelines for tuning DP-AdamBC across different privacy budgets and model architectures, ensuring robust performance across diverse deployment scenarios.