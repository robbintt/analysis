---
ver: rpa2
title: An Empirical Study of Pre-trained Model Selection for Out-of-Distribution Generalization
  and Calibration
arxiv_id: '2307.08187'
source_url: https://arxiv.org/abs/2307.08187
tags:
- pre-trained
- in21k
- generalization
- dataset
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically examines how pre-trained model size,
  pre-training dataset size, and training strategies impact out-of-distribution (OOD)
  generalization and calibration in computer vision. The authors evaluate 100 models
  across diverse pre-trained model sizes, five pre-training datasets, and five data
  augmentations through extensive experiments on four distribution shift datasets.
---

# An Empirical Study of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration

## Quick Facts
- arXiv ID: 2307.08187
- Source URL: https://arxiv.org/abs/2307.08187
- Reference count: 40
- Primary result: Pre-trained model selection significantly impacts OOD generalization and calibration, with larger models and datasets improving both metrics.

## Executive Summary
This paper systematically examines how pre-trained model size, pre-training dataset size, and training strategies impact out-of-distribution (OOD) generalization and calibration in computer vision. The authors evaluate 100 models across diverse pre-trained model sizes, five pre-training datasets, and five data augmentations through extensive experiments on four distribution shift datasets. The results demonstrate that pre-trained model selection has a significant impact on OOD performance, with optimal choices substantially improving accuracy over algorithm improvements alone. Specifically, using larger models and bigger pre-training datasets not only enhances OOD performance but also improves calibration, helping to mitigate overconfidence. The study finds that the best pre-trained models can achieve up to 22.8% improvement in OOD accuracy compared to using standard models with algorithm changes.

## Method Summary
The study evaluates 100 pre-trained models from the timm library across five pre-training datasets (ImageNet-1k, ImageNet-12k, ImageNet-21k, ImageNet-22k, and others) using five different data augmentation strategies. Models are fine-tuned on four distribution shift datasets (PACS, VLCS, OfficeHome, DomainNet) using Empirical Risk Minimization with Momentum SGD optimizer. The evaluation focuses on out-of-distribution accuracy (OOD ACC) and expected calibration error (ECE) as primary metrics, with hyperparameter tuning performed through grid search over learning rates.

## Key Results
- Switching from ResNet-50 to ViT-Large (IN21k) improved OOD accuracy by 24.8% on OfficeHome
- Optimal pre-trained model selection achieved up to 22.8% improvement in OOD accuracy compared to algorithm changes alone
- Larger pre-training datasets consistently improved both OOD performance and calibration across all tested architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger pre-trained model parameters directly correlate with better OOD generalization and calibration performance.
- Mechanism: As model capacity increases, the model can better capture complex patterns in pre-training data that transfer to OOD tasks, while larger capacity also enables better uncertainty estimation through more expressive representations.
- Core assumption: The pre-training data contains relevant features that generalize to OOD tasks, and the increased model capacity doesn't lead to overfitting on the specific OOD datasets.
- Evidence anchors:
  - [abstract] "larger models and bigger pre-training datasets not only enhance OOD performance but also improve calibration, helping to mitigate overconfidence"
  - [section] "From Fig. 1, it's evident that when we switched from using the ResNet-50 model... to ViT-Large (IN21k), we saw an improvement of 24.8% in the OOD test ACC"
  - [corpus] Weak evidence - no direct comparison of model size effects in corpus
- Break condition: When model size becomes so large that it can memorize the OOD datasets rather than truly generalize, leading to poor calibration despite high accuracy.

### Mechanism 2
- Claim: Larger pre-training datasets improve OOD generalization by providing more diverse and representative training signals.
- Mechanism: Increased pre-training dataset size exposes the model to a wider variety of visual concepts and edge cases, creating more robust feature representations that transfer better to unseen distributions.
- Core assumption: The additional data in larger pre-training datasets is diverse and representative of the target OOD distributions.
- Evidence anchors:
  - [abstract] "using larger models and bigger pre-training datasets not only enhances OOD performance but also improves calibration"
  - [section] "Figure 3 explores how the choice of pre-training dataset size affects the performance on out-of-distribution generalization"
  - [corpus] No direct evidence - corpus doesn't discuss pre-training dataset size effects
- Break condition: When additional data is redundant or unrepresentative, providing diminishing returns or even harming generalization through noise.

### Mechanism 3
- Claim: Model architecture choice (ViT vs ResNet vs ConvNeXt) significantly impacts OOD generalization performance independent of parameter count.
- Mechanism: Different architectural inductive biases affect how well features generalize across distributions - transformers may capture global relationships better while CNNs capture local patterns more effectively.
- Core assumption: The architectural differences provide meaningful variations in feature representation that affect OOD generalization beyond simple parameter count.
- Evidence anchors:
  - [section] "There is a significant disparity in performance between ViT-base (86M) and ConvNext-base (88M), even though they have roughly the same number of parameters"
  - [section] "ConvNeXt generally outperforms other models, while ResNets and ViTs are difficult to be ranked"
  - [corpus] No evidence - corpus doesn't compare different architectures
- Break condition: When the task characteristics align perfectly with one architecture's inductive biases, making other architectures irrelevant.

## Foundational Learning

- Concept: Domain generalization and OOD detection
  - Why needed here: The paper evaluates models on OOD datasets, requiring understanding of what constitutes distribution shift and how to measure it
  - Quick check question: What's the difference between domain generalization and OOD detection?

- Concept: Expected Calibration Error (ECE)
  - Why needed here: The paper uses ECE as a key metric alongside accuracy to evaluate model reliability
  - Quick check question: How does ECE differ from accuracy, and why is it important for OOD tasks?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: The study focuses on fine-tuning pre-trained models, which has different dynamics than training from scratch
  - Quick check question: What are the key differences in optimization dynamics between fine-tuning and training from scratch?

## Architecture Onboarding

- Component map: Pre-trained model selection -> Hyperparameter tuning (learning rate, batch size) -> Fine-tuning on source domains -> Evaluation on target OOD domains -> Metrics calculation (accuracy, ECE)
- Critical path: Pre-trained model selection -> Hyperparameter search -> Fine-tuning -> Evaluation -> Analysis
- Design tradeoffs: Model size vs. computational constraints, dataset size vs. training time, architecture choice vs. task specificity
- Failure signatures: High accuracy but poor ECE indicates overconfidence; low accuracy on both ID and OOD suggests poor transfer; large variance across runs suggests instability
- First 3 experiments:
  1. Replicate ResNet-50 baseline results on DomainBed datasets to establish ground truth
  2. Test ViT-large (IN21k) vs. ResNet-50 on OfficeHome to verify the 24.8% improvement claim
  3. Compare ConvNeXt-tiny pre-trained on different dataset sizes (IN1k vs IN12k vs IN22k) on the same task to isolate dataset size effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal pre-trained model selection strategy for different downstream tasks in OOD generalization?
- Basis in paper: [explicit] The authors suggest that future work could propose a benchmark or guideline indicating the optimal choice of a pre-trained model for a given downstream task.
- Why unresolved: The paper demonstrates that pre-trained model selection significantly impacts OOD performance, but does not provide specific guidelines for choosing models across different tasks and domains.
- What evidence would resolve it: A systematic study comparing pre-trained model performance across diverse downstream tasks and domains, establishing clear selection criteria based on task characteristics and model properties.

### Open Question 2
- Question: How does the relationship between model size and OOD generalization change beyond single GPU capacity limits?
- Basis in paper: [explicit] The authors note that while their study focuses on single GPU models, the broader question of memorization versus generalization becomes even more significant in the context of larger models.
- Why unresolved: The paper deliberately restricts analysis to models that fit within single GPU constraints, leaving open questions about how these findings scale to larger, more computationally intensive models.
- What evidence would resolve it: Comprehensive experiments comparing OOD generalization performance across model sizes from single GPU to multi-GPU/multi-node architectures, while controlling for memorization effects.

### Open Question 3
- Question: What is the saturation point for OOD performance improvements with increasing model parameters?
- Basis in paper: [explicit] The authors conjecture that improvements in OOD performance relative to an increase in the number of parameters may reach a saturation point.
- Why unresolved: The paper shows positive correlation between model size and OOD performance but does not determine where diminishing returns begin.
- What evidence would resolve it: Empirical studies measuring OOD performance across a wide range of model sizes, identifying the point where additional parameters yield minimal performance gains.

### Open Question 4
- Question: How do different data augmentation strategies interact with pre-trained model selection for OOD generalization?
- Basis in paper: [inferred] The authors evaluate models across five data augmentations but do not systematically analyze how augmentation choices interact with model selection.
- Why unresolved: While data augmentation is mentioned as a variable in the experimental setup, the paper does not explore how different augmentations might favor certain model architectures or sizes.
- What evidence would resolve it: Controlled experiments varying both data augmentation strategies and pre-trained models to identify optimal combinations for different OOD scenarios.

## Limitations
- The study focuses exclusively on fine-tuning rather than training from scratch, potentially limiting generalizability to other training paradigms
- Computational intensity (120,000+ GPU hours) raises practical scalability concerns for real-world applications
- The model selection is limited to specific architectures and pre-training datasets, potentially missing other important model families

## Confidence
- **High Confidence**: The correlation between larger pre-training datasets and improved OOD performance is well-supported by the experimental results across multiple datasets and architectures
- **Medium Confidence**: The superiority of ConvNeXt architecture over others is demonstrated, but the differences between ViT and ResNet are less conclusive due to task-specific variations
- **Low Confidence**: The claim that optimal pre-trained model selection can achieve up to 22.8% improvement over algorithm changes alone requires further validation across additional tasks and datasets

## Next Checks
1. Replicate the study using a subset of 10-15 models on a single OOD dataset (e.g., OfficeHome) to verify the core findings while managing computational resources
2. Test whether the observed improvements in OOD performance and calibration translate to cross-domain transfer by evaluating models on tasks outside the original four distribution shift datasets
3. Investigate the scaling limits by testing whether extremely large models (e.g., ViT-G/14) continue to show improvements or if diminishing returns set in, particularly for calibration metrics