---
ver: rpa2
title: Weakly Supervised Detection of Hallucinations in LLM Activations
arxiv_id: '2312.02798'
source_url: https://arxiv.org/abs/2312.02798
tags:
- data
- anomalous
- scan2
- test
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a weakly supervised auditing technique for
  detecting anomalies, such as hallucinations, in the internal states of large language
  models (LLMs) without requiring prior knowledge of the specific patterns or access
  to labeled false statements. The method leverages subset scanning over pre-trained
  LLM activations, using empirical p-values derived from reference data assumed to
  be free of anomalies.
---

# Weakly Supervised Detection of Hallucinations in LLM Activations

## Quick Facts
- arXiv ID: 2312.02798
- Source URL: https://arxiv.org/abs/2312.02798
- Reference count: 40
- Primary result: Weakly supervised subset scanning detects LLM hallucinations comparably to supervised methods without requiring labeled anomalous data

## Executive Summary
This paper introduces a novel weakly supervised auditing technique for detecting hallucinations and other anomalies in large language model (LLM) activations. The method uses subset scanning over empirical p-values derived from reference data to identify anomalous subsets of input sentences and nodes without requiring prior knowledge of specific patterns or labeled false statements. By demonstrating comparable performance to fully supervised out-of-distribution classifiers while being more data-efficient, the approach offers a practical solution for auditing LLM internal states. The method successfully identifies that while BERT has limited capacity to encode hallucinations, models like OPT do, highlighting its potential for bias detection and informing targeted fine-tuning strategies.

## Method Summary
The method extends anomalous subset scanning to LLM activations by computing empirical p-values from reference data (assumed to be anomaly-free) and performing a scan over these p-values to identify subsets most likely to contain anomalies. It introduces bidirectional scanning that aggregates left-tail and right-tail results to capture deviations in both directions from expected distributions. The approach requires only a reference dataset and test data, making it more practical than fully supervised methods that need labeled anomalous examples. The Higher Criticism test statistic is used to score subsets, and the method can be applied to any LLM that provides access to internal activations.

## Key Results
- The weakly supervised method achieves comparable precision and recall to fully supervised out-of-distribution classifiers
- Subset scanning successfully identifies that OPT encodes hallucinations while BERT does not
- The bidirectional scanning approach improves detection by capturing deviations in both directions from expected distributions
- The method demonstrates strong performance on controlled datasets like StereoSet and BOLD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM internal activations encode bias information that can be detected via subset scanning
- Mechanism: Subset scanning over empirical p-values derived from node activations identifies subsets of sentences and nodes most likely to contain anomalies
- Core assumption: If an LLM can generate anomalous content, it will exhibit detectable indicators within its internal states
- Evidence anchors:
  - [abstract]: "our method examines the LLM's hidden states (activations) and identifies a subset of input data (e.g., sentences and nodes) as anomalous"
  - [section 2]: "Our method extends prior work on anomalous subset scanning for neural networks [12, 26, 27] by scanning pre-trained LLM activations"
  - [corpus]: Weak, corpus shows related work on hallucination detection but not specifically on subset scanning of LLM activations
- Break condition: If LLM activations do not encode anomaly information or if the p-value distribution under null hypothesis is not uniform

### Mechanism 2
- Claim: Non-parametric scan statistics can detect deviations from expected activation distributions
- Mechanism: Empirical p-values computed from reference data are used to test whether test activations deviate from uniform distribution under null hypothesis
- Core assumption: When anomalous sentences are introduced, the LLM activations will show departure from uniform p-value distribution for certain nodes
- Evidence anchors:
  - [section 2]: "We adopt a non-parametric approach... For this, we first derive p-values from the activations and then perform a scan over these p-values"
  - [section 2]: "If anomalous sentences, e.g., those containing stereotypes, are detected from the activations, it suggests the model encodes those anomalous patterns"
  - [corpus]: Weak, corpus contains related work on activation analysis but not specifically on non-parametric scan statistics for LLM bias detection
- Break condition: If p-values do not follow uniform distribution under null hypothesis or if scan statistics cannot distinguish anomalous patterns

### Mechanism 3
- Claim: Combining left-tail and right-tail scanning improves detection of bidirectional deviations
- Mechanism: Two novel methods aggregate scanning results from left-tailed and right-tailed p-values to identify subsets marked as anomalous due to shifts in either direction
- Core assumption: LLM embeddings for anomalous data may deviate from expected distribution in both directions
- Evidence anchors:
  - [section 2]: "We observe that LLM embeddings for anomalous data may shift from the expected reference distribution in both directions"
  - [section 2]: "We introduce two novel methods to aggregate scanning results... first approach involves aggregating results obtained from scanning left-tail and right-tail p-values"
  - [corpus]: Weak, corpus shows related work on activation analysis but not specifically on bidirectional scanning methods
- Break condition: If anomalous patterns only deviate in one direction or if aggregation method reduces detection accuracy

## Foundational Learning

- Concept: Empirical p-values and their relationship to null hypothesis testing
  - Why needed here: The method relies on computing empirical p-values from reference data to detect deviations in test data
  - Quick check question: What does it mean when empirical p-values under null hypothesis follow a uniform distribution?

- Concept: Subset scanning and the Higher Criticism test statistic
  - Why needed here: The method uses subset scanning with Higher Criticism statistic to identify most anomalous subsets of activations
  - Quick check question: How does the Higher Criticism test statistic differ from other goodness-of-fit statistics in subset scanning?

- Concept: Bidirectional deviations in activation distributions
  - Why needed here: The method assumes anomalies can cause activations to deviate in either direction from expected distribution
  - Quick check question: Why might LLM embeddings for anomalous data shift from expected distribution in both directions?

## Architecture Onboarding

- Component map: LLM model → Activation extraction → Empirical p-value computation → Subset scanning → Anomalous subset identification
- Critical path: Reference data preparation → Test data input → Activation extraction → P-value computation → Scanning algorithm → Result interpretation
- Design tradeoffs: Non-parametric vs parametric approaches, computational efficiency vs detection accuracy, sensitivity to bidirectional deviations
- Failure signatures: Low precision/recall, high false positive rate, inability to detect known anomalies, inconsistent results across random test sets
- First 3 experiments:
  1. Test on synthetic data with known anomalies to verify detection capability
  2. Compare results using left-tail only vs right-tail only vs combined scanning
  3. Test on different LLM architectures (BERT vs OPT) with same dataset to assess generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the subset scanning approach detect hallucinations in other LLM architectures beyond BERT and OPT, and how does its performance scale with model size?
- Basis in paper: The paper explicitly states the method can audit "any LLM that provides activations" but only tests BERT and OPT.
- Why unresolved: The paper only evaluated two specific LLM models, limiting generalizability to other architectures.
- What evidence would resolve it: Testing the subset scanning method on diverse LLM architectures (e.g., GPT-3, LLaMA, PaLM) and analyzing performance across different model sizes.

### Open Question 2
- Question: How does the presence of anomalous data in the reference dataset affect the method's ability to detect anomalies in the test data?
- Basis in paper: The paper explicitly states "We plan to extend our work to more realistic assumptions by including small amounts of anomalous data in the reference dataset."
- Why unresolved: The method assumes the reference dataset contains only "normal" data, which may not reflect real-world scenarios.
- What evidence would resolve it: Conducting experiments with reference datasets containing varying amounts of anomalous data and measuring the impact on detection performance.

### Open Question 3
- Question: Can the subset scanning method be adapted to detect more subtle or complex biases beyond hallucinations, such as social biases or reasoning errors?
- Basis in paper: The paper explicitly mentions exploring other types of anomalies like "toxicity and stereotypes" in the appendix, but the main focus is on hallucinations.
- Why unresolved: The paper only demonstrates the method's effectiveness for detecting hallucinations, and its applicability to other bias types is not fully explored.
- What evidence would resolve it: Applying the method to detect different types of biases in LLM outputs and comparing its performance across various bias categories.

## Limitations
- The method's performance heavily depends on the assumption that reference data is truly free of anomalous content
- Real-world deployment faces challenges with domain adaptation and varying definitions of anomalous content
- The comparison to supervised baselines doesn't address potential differences in false positive patterns affecting practical utility

## Confidence

**High**: The subset scanning framework is technically sound and well-established in anomaly detection literature

**Medium**: Performance claims on specific datasets are supported by results, but generalizability to diverse real-world scenarios is uncertain

**Medium**: The claim about OPT encoding hallucinations while BERT does not is based on experimental results but requires further validation

## Next Checks

1. Test the method's robustness when reference data contains varying levels of anomalous content (contamination analysis)
2. Evaluate performance across diverse domains and languages beyond the current test sets
3. Compare false positive patterns between the weakly supervised method and supervised baselines to assess practical deployment considerations