---
ver: rpa2
title: 'CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought
  Prompting for Multi-domain NLU Tasks'
arxiv_id: '2310.14623'
source_url: https://arxiv.org/abs/2310.14623
tags:
- slot
- intent
- tasks
- cof-cot
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel coarse-to-fine chain-of-thought (CoF-CoT)
  prompting strategy to solve multi-granularity natural language understanding (NLU)
  tasks using large language models (LLMs). The method breaks down NLU tasks into
  sequential reasoning steps, integrating structured knowledge from Abstract Meaning
  Representation (AMR) graphs.
---

# CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks

## Quick Facts
- arXiv ID: 2310.14623
- Source URL: https://arxiv.org/abs/2310.14623
- Reference count: 9
- Key outcome: CoF-CoT achieves state-of-the-art performance on MTOP and MASSIVE datasets across multiple metrics in zero-shot and few-shot settings.

## Executive Summary
This paper introduces CoF-CoT, a novel coarse-to-fine chain-of-thought prompting strategy for multi-domain NLU tasks using LLMs. The method breaks down NLU tasks into sequential reasoning steps, integrating structured knowledge from AMR graphs. By leveraging semantic abstraction and controlled reasoning chains, CoF-CoT demonstrates superior performance across intent accuracy, slot F1 score, frame accuracy, and exact match metrics. The approach is particularly effective in challenging zero-shot and few-shot multi-domain settings.

## Method Summary
CoF-CoT decomposes NLU tasks into five sequential reasoning steps: AMR generation, intent prediction, key phrase extraction, slot labeling, and logic form assembly. Each step conditions on outputs from previous steps, creating a controlled reasoning chain. The method uses AMR graphs as semantic representations to abstract away syntactic variations and capture conceptual relationships. Experiments are conducted on MTOP and MASSIVE datasets under zero-shot and few-shot settings, with results showing consistent improvement over baseline approaches.

## Key Results
- CoF-CoT achieves state-of-the-art performance on MTOP and MASSIVE datasets across multiple metrics
- The approach shows significant improvements in zero-shot and few-shot settings compared to baselines
- AMR-based semantic abstraction consistently outperforms other structured representations (DP, CP) across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AMR graph acts as a shared semantic representation that enables LLMs to understand essential meaning across structurally different utterances
- Mechanism: AMR abstracts away syntactic idiosyncrasies and encodes conceptual relations between entities, events, and attributes
- Core assumption: AMR structure captures relevant semantic cues for downstream NLU tasks
- Evidence anchors: [abstract] and [section 3] mention leveraging AMR as intermediate step, but no prior empirical anchor exists
- Break condition: AMR parsing errors propagate to downstream steps or AMR cannot represent semantics of certain domains

### Mechanism 2
- Claim: Coarse-to-fine reasoning order aligns with human problem-solving, letting coarse-grained predictions guide fine-grained ones
- Mechanism: Framework first resolves overall intent, then uses that intent to contextualize slot extraction
- Core assumption: Coarse-grained intent prediction is easier and provides reliable context for fine-grained tasks
- Evidence anchors: [abstract] and [section 3] describe step order, with ablation in Table 4 showing worse performance when intent step is removed
- Break condition: Coarse step is too ambiguous or incorrect, misleading downstream slot prediction

### Mechanism 3
- Claim: Conditioning each step on previous step's output improves coherence and task completion by reducing reliance on LLM's prior knowledge
- Mechanism: Each prompt includes output from prior step, creating controlled reasoning chain that mitigates hallucinations
- Core assumption: LLMs can reliably use prior step outputs as context and conditioning reduces drift from expected outputs
- Evidence anchors: [abstract] and [section 3] mention leveraging CoT-based approach, but weak empirical support specific to AMR-CoT
- Break condition: Intermediate outputs are noisy or misaligned, causing conditioning to propagate errors

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: Enables multi-step reasoning in LLMs, essential for decomposing NLU tasks requiring sequential inference
  - Quick check question: What is the difference between zero-shot and few-shot CoT prompting?

- Concept: Abstract Meaning Representation (AMR)
  - Why needed here: Provides domain-agnostic, semantic-level representation that abstracts syntax, enabling shared understanding across utterances
  - Quick check question: How does AMR differ from dependency parsing in representing semantic relationships?

- Concept: Logic Form as unified NLU representation
  - Why needed here: Bridges gap between semantic parsing and traditional slot-filling by encoding both intent and slots in single structured output
  - Quick check question: What are advantages of using flattened logic form over separate intent/slot BIO sequences?

## Architecture Onboarding

- Component map:
  Input: Utterance + domain
  Step 1: AMR Generator (LLM prompt)
  Step 2: Intent Predictor (LLM prompt, conditioned on AMR)
  Step 3: Key Phrase Extractor (LLM prompt, conditioned on AMR + intent)
  Step 4: Slot Labeler (LLM prompt, conditioned on AMR + intent + key phrases)
  Step 5: Logic Form Assembler (LLM prompt, conditioned on all prior outputs)
  Output: Final logic form with intent and slot-value pairs

- Critical path:
  Utterance → AMR → Intent → Key Phrases → Slot Labels → Logic Form

- Design tradeoffs:
  - Pro: Controlled reasoning chain reduces hallucination vs. end-to-end prompting
  - Con: Requires multiple LLM calls, increasing latency and cost
  - Pro: Modular design allows step-level debugging and ablation
  - Con: AMR quality is bottleneck; errors propagate

- Failure signatures:
  - AMR generation fails → downstream intent/slot predictions degrade
  - Intent misprediction → slot extraction becomes context-weak
  - Slot value extraction fails to align with key phrases → slot labeling errors
  - Final logic form structure mismatch → evaluation metrics drop

- First 3 experiments:
  1. Compare CoF-CoT with and without AMR step to measure AMR contribution
  2. Swap step order (e.g., fine-to-coarse) to validate coarse-to-fine assumption
  3. Test with different LLM backbone (e.g., PaLM) to confirm LLM-agnostic performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would CoF-CoT perform when adapted to multilingual NLU tasks, given that AMR structures are primarily English-biased?
- Basis in paper: [explicit] "Our empirical study is restricted to English NLU data... We leave the adaptation of the CoF-CoT to multilingual settings... as future directions for our work."
- Why unresolved: Paper acknowledges limitation of AMR structures in multilingual contexts and identifies it as future research direction without empirical evidence
- What evidence would resolve it: Empirical results comparing CoF-CoT performance on multilingual NLU datasets would demonstrate effectiveness across languages

### Open Question 2
- Question: What would be performance impact of extending CoF-CoT to handle nested logic forms with multiple intents per utterance?
- Basis in paper: [explicit] "Our work is empirically studied on the Flat Logic Form representation... We leave explorations on the more challenging Nested Logic Form... as future work."
- Why unresolved: Paper deliberately restricts scope to flat logic forms for simplicity, explicitly leaving nested logic form exploration as future work
- What evidence would resolve it: Experimental results comparing CoF-CoT performance on datasets with nested logic forms would reveal capability and limitations

### Open Question 3
- Question: How does choice of structured representation (AMR, DP, CP) impact CoF-CoT's performance across different NLU task complexities and utterance structures?
- Basis in paper: [explicit] "Impact of Structured Representation... Our empirical study presented in Table 3 reveals that AMR-CoT unanimously achieves the best performance"
- Why unresolved: While paper shows AMR outperforms DP and CP, it doesn't provide detailed analysis of why AMR is superior or how different utterance structures might favor different representations
- What evidence would resolve it: Systematic experiments varying task complexity and utterance structures while comparing AMR, DP, and CP performance would reveal conditions when each representation excels

## Limitations
- Multi-step pipeline increases computational cost and introduces potential error propagation from AMR generation through to final logic form assembly
- Reliance on LLMs for AMR generation without specifying underlying model creates reproducibility challenges
- Evaluation focuses on two datasets (MTOP and MASSIVE) without testing approach's generalization to other NLU tasks or languages

## Confidence
- **High confidence**: CoF-CoT achieves state-of-the-art performance on MTOP and MASSIVE datasets across multiple metrics
- **Medium confidence**: Coarse-to-fine ordering improves performance by providing contextual guidance, supported by ablation but lacking alternative ordering comparisons
- **Low confidence**: AMR provides essential semantic abstraction that enables cross-domain generalization, as this represents novel contribution but lacks isolated validation

## Next Checks
1. **AMR contribution isolation**: Run CoF-CoT experiments with direct utterance-to-intent/slot prompts (bypassing AMR generation) to quantify exact performance gain from semantic abstraction
2. **Step order validation**: Implement and test alternative reasoning orders (fine-to-coarse, parallel intent/slot prediction) to determine if proposed sequence is genuinely optimal
3. **Error propagation analysis**: Instrument pipeline to track how AMR generation errors propagate through downstream steps and measure correlation between specific AMR errors and downstream failures