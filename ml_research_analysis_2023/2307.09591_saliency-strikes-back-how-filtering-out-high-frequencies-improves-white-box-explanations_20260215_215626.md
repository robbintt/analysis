---
ver: rpa2
title: 'Saliency strikes back: How filtering out high frequencies improves white-box
  explanations'
arxiv_id: '2307.09591'
source_url: https://arxiv.org/abs/2307.09591
tags:
- methods
- gradient
- high-frequency
- noise
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of noisy, high-frequency artifacts
  in gradient-based attribution methods for explainable AI. The authors propose FORGrad,
  a simple method that applies optimal low-pass filtering to gradient-based attribution
  maps, removing high-frequency noise.
---

# Saliency strikes back: How filtering out high frequencies improves white-box explanations

## Quick Facts
- arXiv ID: 2307.09591
- Source URL: https://arxiv.org/abs/2307.09591
- Reference count: 40
- One-line primary result: FORGrad improves gradient-based attribution methods by filtering high-frequency noise from downsampling operations

## Executive Summary
This paper addresses the problem of noisy, high-frequency artifacts in gradient-based attribution methods for explainable AI. The authors propose FORGrad, a simple method that applies optimal low-pass filtering to gradient-based attribution maps, removing high-frequency noise. Through Fourier analysis, they show that the gradient contains significant high-frequency content, particularly introduced by downsampling operations like MaxPooling and strided convolutions in CNNs. FORGrad estimates the optimal frequency cutoff for each model and filters the attribution maps accordingly. Results on ResNet50, ViT, and ConvNeXT show that FORGrad consistently improves the faithfulness of gradient-based methods (measured by Deletion, Insertion, and Fidelity metrics), often surpassing or matching more computationally expensive black-box methods.

## Method Summary
FORGrad is a frequency-based filtering approach that improves gradient-based attribution methods by removing high-frequency noise. The method computes attribution maps using standard gradient-based techniques (Saliency, GradCAM, SmoothGrad, etc.), then applies Fourier transform to analyze the frequency content. It identifies and filters out high-frequency components that are introduced by downsampling operations like MaxPooling and strided convolutions, which are shown to be the main sources of noise. The optimal frequency cutoff is estimated for each model, and the cleaned attribution maps are obtained through inverse Fourier transform. The method is evaluated on ImageNet validation set using ResNet50, ViT, and ConvNeXT models, with faithfulness measured by Deletion, Insertion, and Fidelity metrics.

## Key Results
- FORGrad consistently improves faithfulness metrics (Deletion, Insertion, Fidelity) for gradient-based attribution methods
- Filtered gradient-based methods often surpass or match computationally expensive black-box methods like Occlusion and RISE
- The optimal frequency cutoff is highly dependent on the dataset, with ResNet50 showing different optimal values across architectures
- Downsampling operations (MaxPooling, strided convolutions) are identified as the main sources of high-frequency noise in gradients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: High-frequency noise in gradients originates from downsampling operations (MaxPooling, strided convolutions).
- **Mechanism**: Downsampling without proper anti-aliasing introduces high-frequency components that alias into lower frequencies, corrupting the gradient signal.
- **Core assumption**: The gradient noise is primarily introduced by architectural operations rather than model training or data distribution.
- **Evidence anchors**:
  - [abstract]: "Our findings show that downsampling operations (via MaxPooling or strides) are the main sources of high frequencies"
  - [section]: "Our findings show that downsampling operations (via MaxPooling or strides) are the main sources of high frequencies, and training the model does not alleviate the issue."
  - [corpus]: Weak - corpus focuses on saliency methods generally, not specific to downsampling noise mechanisms.

### Mechanism 2
- **Claim**: Filtering out high frequencies from gradient-based attribution methods improves their faithfulness metrics.
- **Mechanism**: Removing high-frequency noise that doesn't contain decision-relevant information improves the quality of attribution maps, making them more aligned with prediction-based methods.
- **Core assumption**: High-frequency content in gradients is predominantly noise rather than useful information for model decisions.
- **Evidence anchors**:
  - [abstract]: "We then apply an optimal low-pass filter for attribution maps and demonstrate that it improves gradient-based attribution methods."
  - [section]: "Removing high-frequency noise yields significant improvements in the explainability scores obtained with gradient-based methods"
  - [corpus]: Weak - corpus papers focus on saliency methods but don't specifically address frequency-based noise filtering.

### Mechanism 3
- **Claim**: Gradient-based methods contain sufficient information for faithful explanations when properly filtered.
- **Mechanism**: The gradient signal, when cleaned of high-frequency noise, provides explanations that compete with computationally expensive black-box methods.
- **Core assumption**: The underlying gradient information is complete for explanation purposes, just obscured by noise.
- **Evidence anchors**:
  - [abstract]: "gradient-based attribution methods, when properly filtered, contain sufficient information for faithful explanations."
  - [section]: "we observe that the scores of several gradient-based methods surpass or at least match those of prediction-based methods"
  - [corpus]: Weak - corpus focuses on evaluation metrics but doesn't address the sufficiency of gradient information post-filtering.

## Foundational Learning

- **Concept: Fourier analysis and frequency domain representation**
  - Why needed here: The entire method relies on analyzing and filtering attribution maps in the frequency domain to remove high-frequency noise.
  - Quick check question: What does a high-frequency component in an image gradient typically represent, and why might it be considered noise in the context of model explanations?

- **Concept: Gradient-based attribution methods and their limitations**
  - Why needed here: Understanding how methods like Saliency, GradCAM, and SmoothGrad work is essential to grasp why they produce noisy outputs.
  - Quick check question: How does the basic Saliency method compute attribution, and what makes it computationally efficient compared to black-box methods?

- **Concept: Model faithfulness metrics (Deletion, Insertion, Fidelity)**
  - Why needed here: These metrics are used to evaluate and optimize the frequency filtering process, making them central to the FORGrad method.
  - Quick check question: What is the difference between Deletion and Insertion metrics, and why are they complementary in evaluating attribution quality?

## Architecture Onboarding

- **Component map**: Attribution map generation -> Fourier transform -> Frequency filtering with optimal cutoff σ* -> Inverse Fourier transform -> Faithfulness evaluation

- **Critical path**: The most critical components are the Fourier transform/inverse transform operations and the optimal cutoff estimation. The quality of the cutoff estimation directly determines whether the filtering improves or degrades explanations.

- **Design tradeoffs**: The method trades computational efficiency (low cost of gradient-based methods) for potential information loss (removing high frequencies). The key tradeoff is between explanation quality and computational cost versus black-box methods.

- **Failure signatures**: Poor performance manifests as (1) attribution maps that become too blurry and lose spatial specificity, (2) metrics that don't improve after filtering, or (3) inconsistent cutoff values across different models or datasets.

- **First 3 experiments**:
  1. Apply FORGrad to Saliency on ResNet50 and compare Deletion/Insertion scores before and after filtering.
  2. Test different frequency cutoff values on a validation set to find the optimal σ* that maximizes [Deletion-Insertion] score.
  3. Compare the filtered gradient-based methods against black-box methods (Occlusion, RISE) on the same faithfulness metrics.

## Open Questions the Paper Calls Out
- The paper explicitly acknowledges that the optimal frequency cutoff σ* is "highly dependent on the dataset, perhaps more so than on the model itself."
- It suggests that exploring the influence of this noise on the model's performance would be worth investigating, including whether replacing operations that introduce noise could affect both accuracy and robustness.

## Limitations
- The method assumes high-frequency components are predominantly noise rather than decision-relevant information, which remains unverified for all model architectures and datasets.
- The optimal frequency cutoff σ* estimation heuristic may not generalize well across different model families or domains beyond ImageNet.
- The effectiveness depends on the quality of the Fourier transform implementation and the assumption that attribution maps can be meaningfully represented in the frequency domain without losing critical spatial information.

## Confidence

**High Confidence Claims:**
- High-frequency noise exists in gradient-based attribution maps (supported by Fourier analysis across multiple models)
- Filtering high frequencies improves faithfulness metrics for gradient-based methods (empirically validated with Deletion/Insertion/Fidelity scores)

**Medium Confidence Claims:**
- Downsampling operations are the primary source of high-frequency noise (based on architectural analysis but not definitively proven)
- Gradient-based methods contain sufficient information for faithful explanations when properly filtered (shown through competitive metric performance)

**Low Confidence Claims:**
- The FORGrad method will generalize to all vision models and tasks (limited to ResNet50, ViT, and ConvNeXT on ImageNet)
- The frequency cutoff estimation heuristic is optimal for all scenarios (heuristic-based approach with potential model-specific limitations)

## Next Checks

1. **Cross-architecture validation**: Test FORGrad on additional model architectures (e.g., EfficientNet, MobileNet) and domains (medical imaging, satellite imagery) to verify generalization of the frequency filtering approach.

2. **Ablation on downsampling**: Conduct controlled experiments where downsampling operations are replaced with anti-aliased alternatives to isolate the contribution of downsampling to high-frequency noise.

3. **Decision boundary analysis**: Investigate whether the high-frequency components removed by FORGrad contain decision-relevant information by analyzing model performance on adversarial examples or near-decision-boundary inputs.