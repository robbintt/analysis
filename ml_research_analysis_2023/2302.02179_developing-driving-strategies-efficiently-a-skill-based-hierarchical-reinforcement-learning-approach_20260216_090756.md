---
ver: rpa2
title: 'Developing Driving Strategies Efficiently: A Skill-Based Hierarchical Reinforcement
  Learning Approach'
arxiv_id: '2302.02179'
source_url: https://arxiv.org/abs/2302.02179
tags:
- learning
- agent
- skills
- skill
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a skill-based hierarchical reinforcement learning
  approach to develop driving strategies for highway merging scenarios. The method
  uses unsupervised skill discovery to learn transferable motion primitives (skills)
  without a predetermined reward function.
---

# Developing Driving Strategies Efficiently: A Skill-Based Hierarchical Reinforcement Learning Approach

## Quick Facts
- arXiv ID: 2302.02179
- Source URL: https://arxiv.org/abs/2302.02179
- Reference count: 34
- Primary result: Skill-based hierarchical RL achieves higher success rate faster than flat RL in highway merging scenarios.

## Executive Summary
This paper presents a skill-based hierarchical reinforcement learning approach for developing driving strategies in highway merging scenarios. The method uses unsupervised skill discovery to learn transferable motion primitives without requiring a predetermined reward function. These skills are then used as high-level actions in a hierarchical RL framework, reducing training time for multiple models with varying behaviors. The approach demonstrates superior performance compared to baseline reinforcement learning methods, achieving higher success rates with less training in simulated highway merging scenarios.

## Method Summary
The method employs unsupervised skill discovery to learn motion primitives, which are then used as high-level actions in a hierarchical reinforcement learning framework. The process involves two main stages: first, training skills using SAC with intrinsic reward to maximize mutual information between skills and states; second, training a high-level DQN that selects among the pre-trained skills. The state space is discretized into bins to facilitate skill discovery, and the hierarchical structure reduces the action space from fine-grained accelerations and lane changes to skill indices. The approach is evaluated in a highway merging scenario using NGSIM I-80 real traffic data.

## Key Results
- The hierarchical model using skills achieves higher performance faster than a conventional RL model in highway merging scenarios.
- The hierarchical model converges to a meaningfully higher success rate at the end of training compared to the baseline model.
- The learned skills are shown to be transferable and provide value in applications requiring multiple differing policies.

## Why This Works (Mechanism)

### Mechanism 1
Skill discovery via unsupervised learning reduces the search space for high-level policies by pre-training reusable motion primitives. Unsupervised skill discovery maximizes mutual information between skills and states while minimizing it between actions and skills given states. This produces diverse motion primitives that can be treated as high-level actions in a hierarchical RL setup. The core assumption is that motion primitives learned without a reward function remain meaningful when reused in a downstream task with a specific reward structure.

### Mechanism 2
Hierarchical decomposition reduces sample complexity by allowing longer skill-level decisions rather than primitive action decisions. The high-level DQN selects among pre-trained skills, each executed for multiple time steps, reducing the frequency of high-level decisions and lowering the action space from fine-grained accelerations/lane changes to skill indices. The core assumption is that skills capture sufficient state-action structure so that a single high-level decision can span multiple primitive steps without loss of task performance.

### Mechanism 3
Quantizing continuous states into bins makes skill discovery feasible by reducing the state space size. Continuous observation values are discretized into 10 bins each, enabling the USD algorithm to explore and differentiate skills in a finite state space rather than an infinite continuous one. The core assumption is that the discretized representation preserves enough information for skills to be meaningfully differentiated and useful in downstream tasks.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The entire RL framework is built on MDP theory, defining states, actions, transitions, and rewards for both skill discovery and high-level policy learning.
  - Quick check question: What are the four components of an MDP tuple?

- Concept: Mutual Information and Entropy
  - Why needed here: USD explicitly optimizes mutual information between skills and states and entropy of actions to encourage diverse, state-informative skills.
  - Quick check question: How does maximizing entropy of actions given a skill promote exploration?

- Concept: Hierarchical Reinforcement Learning
  - Why needed here: The approach decomposes the driving task into low-level motion primitives (skills) and a high-level policy that selects among them, reducing action space and sample complexity.
  - Quick check question: What is the advantage of using skills as high-level actions compared to primitive actions in sparse reward environments?

## Architecture Onboarding

- Component map: State quantization -> Skill discovery (SAC with discriminator) -> Skill transfer -> High-level DQN -> Environment (Highway merging simulator)
- Critical path: Skill training → skill transfer → high-level policy training → evaluation
- Design tradeoffs: Longer skill execution steps speed up training but risk over-commitment; more skills increase diversity but require more initial training time; state discretization simplifies skill discovery but may lose fine-grained distinctions
- Failure signatures: Skills collapse to similar behaviors (low diversity); high-level policy converges slower than baseline; quantization loss prevents meaningful skill differentiation
- First 3 experiments:
  1. Train skills with 8 bins per state dimension and evaluate diversity via state visitation counts
  2. Test high-level DQN with 8-step vs 16-step skill execution to measure convergence speed
  3. Transfer learned skills to a slightly altered merging scenario (e.g., different on-ramp length) and compare performance to flat RL

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed skill-based hierarchical reinforcement learning approach compare to other hierarchical RL methods (e.g., option-critic) in terms of performance and training efficiency for highway merging scenarios? The paper focuses on comparing the proposed method to a baseline, but does not explore how it stacks up against other hierarchical RL approaches. A comparative study between the proposed method and other hierarchical RL methods on highway merging scenarios would resolve this.

### Open Question 2
How do the learned skills transfer to more complex highway scenarios with additional factors such as varying traffic densities, road geometries, and weather conditions? The paper only evaluates the skills in a simple highway merging scenario and does not explore their applicability to more complex real-world situations. Testing the learned skills in a variety of highway scenarios would assess their transferability and performance.

### Open Question 3
How does the proposed method handle situations where the ego vehicle needs to merge into multiple lanes or change lanes frequently during the merging process? The paper focuses on a simple highway merging scenario where the ego vehicle only needs to merge into one lane, but does not address more complex lane-changing situations. Evaluating the proposed method in scenarios with multiple lane changes would assess its ability to handle these situations effectively.

### Open Question 4
How does the performance of the proposed method scale with the number of skills learned, and is there an optimal number of skills for a given highway merging scenario? The paper uses 16 skills in the experiments but does not explore how the performance changes with different numbers of skills. Conducting experiments with varying numbers of skills would determine the optimal number for a given scenario.

## Limitations

- Limited evidence for the effectiveness of unsupervised skill discovery in this specific driving context
- Evaluation is limited to a single scenario (highway merging) without extensive testing across diverse driving conditions
- Discretization of continuous state spaces introduces potential information loss that could affect skill quality and transferability

## Confidence

- **High Confidence**: The hierarchical structure with pre-trained skills reduces training time compared to flat RL baselines
- **Medium Confidence**: Skills learned through unsupervised discovery are transferable and provide value across multiple policies
- **Low Confidence**: The unsupervised skill discovery method consistently produces diverse, useful motion primitives without requiring reward shaping

## Next Checks

1. Conduct an ablation study comparing hierarchical RL with randomly initialized skills versus pre-trained skills to isolate the contribution of skill discovery
2. Test the approach on multiple driving scenarios (e.g., lane changing, roundabout navigation) to validate generalizability beyond highway merging
3. Perform a sensitivity analysis on state discretization granularity to determine the optimal trade-off between computational efficiency and skill quality