---
ver: rpa2
title: ASBART:Accelerated Soft Bayes Additive Regression Trees
arxiv_id: '2310.13975'
source_url: https://arxiv.org/abs/2310.13975
tags:
- bart
- tree
- trees
- sbart
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new method, ASBART, which combines the advantages
  of SBART and XBART to overcome the slowness of SBART. The main idea is to use XBART
  to generate a candidate tree structure and then search for the optimal bandwidth
  to obtain an estimate of a smooth tree.
---

# ASBART:Accelerated Soft Bayes Additive Regression Trees

## Quick Facts
- arXiv ID: 2310.13975
- Source URL: https://arxiv.org/abs/2310.13975
- Reference count: 1
- Primary result: ASBART achieves comparable accuracy to SBART with ~10× faster speed on simulated data

## Executive Summary
ASBART is a new method that accelerates Soft Bayes Additive Regression Trees (SBART) by combining it with Accelerated XBART (XBART). The approach uses XBART to generate candidate tree structures and then searches for optimal bandwidth to obtain smooth tree estimates, avoiding expensive MCMC iterations over smooth trees. Experimental results show ASBART can achieve comparable accuracy to SBART while being approximately 10 times faster, with performance gains of 17.3× faster than BART and 276.7× faster than SBART under high noise conditions.

## Method Summary
ASBART combines XBART's fast tree generation with SBART's smooth tree estimation. The method uses XBART to grow candidate trees from root nodes, then performs grid-point bandwidth search (0% to 20%) to determine optimal smoothness. Metropolis-Hastings sampling selects between XBART-generated candidates and previous trees based on marginal likelihood comparisons. This approach avoids the computational burden of full SBART MCMC while maintaining accuracy through careful bandwidth optimization.

## Key Results
- ASBART achieves comparable accuracy to SBART with ~10× faster speed
- Under high noise conditions, ASBART is 17.3× faster than BART and 276.7× faster than SBART
- Under low noise conditions, ASBART is 13.1× faster than SBART

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using XBART to generate candidate tree structure speeds up SBART by avoiding expensive MCMC iterations over smooth trees.
- Mechanism: XBART grows trees from root in each iteration using fast stochastic hill climbing. These candidate trees approximate BART trees. Optimal bandwidth is then searched for each candidate, yielding smooth tree estimates without full SBART MCMC.
- Core assumption: XBART candidate trees have high marginal likelihood when SBART would select similar tree structures, so bandwidth optimization yields good smooth trees.
- Evidence anchors:
  - [abstract] "The main idea is to use XBART to generate a candidate tree structure and then search for the optimal bandwidth to obtain an estimate of a smooth tree."
  - [section 3] "When (10) is relatively large, the corresponding (13) after finding the optimal window width is also relatively large. Consequently, the posterior probability of this tree is high..."
  - [corpus] Weak: no direct comparison of XBART vs SBART marginal likelihoods in neighbors.
- Break condition: If XBART candidate trees rarely align with SBART-optimal structures, bandwidth search will not improve accuracy and may mislead selection.

### Mechanism 2
- Claim: Grid-point search over bandwidth is efficient enough to avoid per-node optimization while maintaining accuracy.
- Mechanism: Instead of optimizing bandwidth per node, a uniform grid search (0% to 20% in steps) is applied per tree. This balances computational cost and smoothness.
- Core assumption: Uniform bandwidth across a tree is sufficient to capture the local smoothness implied by the data.
- Evidence anchors:
  - [section 3] "In this context, we utilize a grid-point search to determine the optimal window width. For instance, we perform the search over grid points (0%, 1%, 2%, . . . , 20%)..."
  - [section 3] "An alternative option is to search for the window width at each node, which entails a significant computational burden... Therefore, in subsequent steps, we continue to adopt the approach of using a uniform window width for individual trees."
  - [corpus] Weak: neighbors discuss BART extensions but none focus on bandwidth selection strategies.
- Break condition: If the optimal bandwidth varies drastically across nodes within a tree, uniform search will under-smooth some regions and over-smooth others, hurting accuracy.

### Mechanism 3
- Claim: MH sampling between XBART-generated and previous trees retains high-posterior trees while allowing exploration.
- Mechanism: After XBART generates candidate tree T*, MH acceptance probability compares marginal likelihoods and priors of T* vs. previous Tj, ensuring selection of trees with higher posterior support.
- Core assumption: XBART growth-from-root yields candidate trees comparable in posterior support to MCMC-sampled trees from BART/SBART.
- Evidence anchors:
  - [section 3] "We accept the new tree structure with probability... Unlike BART, both trees T*j and Tj are grown from root nodes, so q(T*j,Tj)/q(Tj,T*j) = 1."
  - [section 3] "Through this step of MH sampling, we strive to retain tree structures with higher posterior probabilities..."
  - [corpus] Weak: no explicit mention of MH acceptance in neighbor BART variants.
- Break condition: If XBART candidates are too different from MCMC samples, MH acceptance rates become very low, stalling exploration and degrading model quality.

## Foundational Learning

- Concept: Marginal likelihood computation for soft trees.
  - Why needed here: ASBART requires computing marginal likelihoods with soft gating (Eq. 13) to compare trees during MH sampling.
  - Quick check question: What is the role of Λ in the marginal likelihood formula for soft trees?
- Concept: Bandwidth optimization in smoothed decision trees.
  - Why needed here: Bandwidth controls smoothness; ASBART searches optimal bandwidth after XBART candidate generation.
  - Quick check question: Why does a bandwidth of 0 reduce the soft tree marginal likelihood to the hard tree case?
- Concept: Metropolis-Hastings acceptance ratio in tree sampling.
  - Why needed here: ASBART uses MH to decide whether to keep XBART candidate or previous tree, balancing exploration and exploitation.
  - Quick check question: How does the q(Tj,T*j)/q(T*j,Tj) ratio simplify when both trees grow from root?

## Architecture Onboarding

- Component map: Data preprocessing -> XBART candidate generation (fast grow-from-root) -> Bandwidth grid search -> MH tree selection -> Model output
- Critical path: XBART candidate -> bandwidth search -> MH acceptance -> next iteration
- Design tradeoffs: Uniform bandwidth search reduces cost vs per-node search; linear gate vs sigmoid gate affects smoothness-accuracy balance; fewer XBART iterations trade speed vs accuracy
- Failure signatures: (1) Low MH acceptance rates -> stuck in local optima; (2) Large bandwidth variance across nodes -> uniform search ineffective; (3) XBART candidates mismatch SBART trees -> poor accuracy
- First 3 experiments:
  1. Verify XBART candidate marginal likelihood > 0.5 for SBART-optimal trees on a small synthetic dataset
  2. Compare uniform bandwidth search vs per-node search on a single tree for accuracy loss
  3. Measure MH acceptance rate vs iteration count to detect convergence behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ASBART compare to SBART in terms of both accuracy and computational efficiency when applied to real-world datasets with varying levels of noise and complexity?
- Basis in paper: [explicit] The paper mentions that the proposed method, ASBART, can achieve comparable accuracy to SBART with about 10 times faster speed. However, the experiments are conducted on simulated data with known ground truth, which may not fully capture the complexities and noise levels present in real-world datasets.
- Why unresolved: The paper does not provide any experimental results on real-world datasets, making it difficult to assess the method's performance in more realistic scenarios.
- What evidence would resolve it: Experimental results on real-world datasets with varying levels of noise and complexity, comparing the performance of ASBART and SBART in terms of accuracy and computational efficiency.

### Open Question 2
- Question: What is the impact of the choice of gate function (sigmoid vs. linear) on the performance of ASBART, and how does this choice interact with the level of noise in the data?
- Basis in paper: [explicit] The paper discusses the use of both sigmoid and linear gate functions in ASBART and notes that the linear gate function may slightly compromise estimation accuracy. However, the paper does not provide a detailed analysis of the impact of the choice of gate function on the method's performance, particularly in relation to the level of noise in the data.
- Why unresolved: The paper only provides a qualitative comparison of the two gate functions and does not explore their performance across different noise levels or provide a quantitative analysis of their impact on accuracy.
- What evidence would resolve it: A systematic study comparing the performance of ASBART with sigmoid and linear gate functions across datasets with varying levels of noise, providing insights into the optimal choice of gate function based on the noise characteristics of the data.

### Open Question 3
- Question: How does the performance of ASBART scale with increasing dimensionality of the input data, and what are the limitations of the method in high-dimensional settings?
- Basis in paper: [explicit] The paper mentions that SBART uses a sparsity-inducing Dirichlet prior for splitting variables to adapt to high-dimensional variable selection scenarios. However, it does not provide any experimental results or analysis of how ASBART performs in high-dimensional settings or what limitations the method may have.
- Why unresolved: The paper does not include any experiments or discussions on the scalability of ASBART with increasing dimensionality, making it difficult to assess the method's performance in high-dimensional scenarios.
- What evidence would resolve it: Experimental results on datasets with varying numbers of input dimensions, comparing the performance of ASBART to other methods in terms of accuracy and computational efficiency, as well as an analysis of the method's limitations in high-dimensional settings.

## Limitations
- Limited empirical validation beyond simulated data - no real-world dataset experiments
- Bandwidth strategy (uniform grid 0-20%) lacks empirical comparison to per-node optimization
- Core assumption that XBART candidates align with SBART-optimal structures needs direct validation

## Confidence

- Speed claims (High): The mechanism of using XBART to generate candidates and search bandwidth is well-specified and computationally sound.
- Accuracy preservation (Medium): Theoretical arguments are provided but empirical validation is limited to simulated data only.
- Bandwidth strategy (Low): No empirical comparison between uniform and per-node bandwidth search strategies.

## Next Checks

1. Replicate the marginal likelihood comparison between XBART-generated candidates and SBART-optimal trees on the same simulated datasets to verify the core assumption.
2. Run ASBART on at least two real-world regression datasets with known smooth relationships to assess practical performance beyond simulation.
3. Conduct an ablation study comparing uniform bandwidth search (0-20% grid) against per-node optimization on a subset of trees to quantify accuracy-speed tradeoff.