---
ver: rpa2
title: Notion of Explainable Artificial Intelligence -- An Empirical Investigation
  from A Users Perspective
arxiv_id: '2311.02102'
source_url: https://arxiv.org/abs/2311.02102
tags:
- system
- users
- user
- recommendation
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates user-centric explainable AI (XAI) for recommendation
  systems through focus group interviews with 30 participants. The study identifies
  three key themes: (1) users desire tailor-made explanations with control over information
  amount and presentation format; (2) explanations should include personal data usage
  details and security/privacy information; (3) user reviews as explanations need
  to be trustworthy and authentic.'
---

# Notion of Explainable Artificial Intelligence -- An Empirical Investigation from A Users Perspective

## Quick Facts
- arXiv ID: 2311.02102
- Source URL: https://arxiv.org/abs/2311.02102
- Reference count: 7
- Users want customizable explanations, data transparency, and authentic reviews in AI recommendation systems

## Executive Summary
This study investigates user-centric explainable AI (XAI) through focus group interviews with 30 participants across three recommendation system scenarios. The research identifies three key themes: users desire control over explanation granularity and presentation format, require transparency about data usage and security practices, and value authentic user reviews as explanations. The findings suggest that end-users need customizable explanations that allow them to control information depth, supplementary information on-demand, and transparent communication about data practices. Based on these insights, the authors propose a theoretical framework emphasizing user involvement in XAI development, addressing the gap in understanding lay users' requirements for AI explanations in recommendation systems.

## Method Summary
The study employed focus group interviews with 30 participants divided into 5 groups of 6, exploring three recommendation system scenarios (e-commerce, housing, movie). Each 60-90 minute session was facilitated by a moderator who guided discussions about user comprehension of AI explanations. The research team conducted thematic analysis using inductive coding on the interview transcripts to identify patterns and themes. The methodology aimed to capture authentic user perspectives on XAI requirements through qualitative exploration rather than quantitative measurement.

## Key Results
- Users require control over explanation granularity to prevent cognitive overload and improve comprehension
- Transparency about personal data usage and security practices is essential for building user trust
- Authentic, verifiable user reviews are critical components of effective AI explanations
- Participants expressed concerns about information overload and desire for customizable explanation formats
- Users want supplementary information available on-demand rather than pre-packaged explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User control over explanation granularity reduces cognitive overload and increases system comprehension
- Mechanism: Allowing users to customize information amount and format prevents information overload, enabling engagement at preferred depth
- Core assumption: Users have varying levels of interest and capacity for processing AI recommendation information
- Evidence anchors:
  - [abstract] "users desire tailor-made explanations with control over information amount and presentation format"
  - [section 4.1.1] "The discussion revealed that apart from the explanation provided by the system, users were also interested in requesting more information"
- Break condition: Complex or poorly designed customization options may cause confusion rather than improved comprehension

### Mechanism 2
- Claim: Transparency about data usage and security practices builds user trust in AI recommendation systems
- Mechanism: Clear explanations about data collection, usage, and security measures address privacy concerns and increase trust
- Core assumption: Users are concerned about data privacy and security when interacting with AI systems
- Evidence anchors:
  - [abstract] "explanations should include personal data usage details and security/privacy information"
  - [section 4.1.2] "The participants discussed that from various sources, they know these recommendation systems use personal data for recommendation purposes"
- Break condition: Vague, technical, or overwhelming data usage explanations may increase user skepticism

### Mechanism 3
- Claim: Authentic, verifiable user reviews as explanations increase system credibility and user confidence in recommendations
- Mechanism: Genuine user reviews provide social proof and help users make informed decisions, increasing confidence in recommendations
- Core assumption: Users value peer opinions and authentic feedback when evaluating recommendations
- Evidence anchors:
  - [abstract] "user reviews as explanations need to be trustworthy and authentic"
  - [section 4.1.3] "The participants strongly highlighted that users' reviews are vital to the explanation"
- Break condition: Inadequate filtering of fake reviews or suspicion of manipulation decreases trust in the system

## Foundational Learning

- Concept: Qualitative research methods (focus groups, thematic analysis)
  - Why needed here: The study uses focus group interviews and thematic analysis to gather and analyze user perspectives on XAI, essential for understanding methodology and interpreting findings
  - Quick check question: What is the difference between inductive and deductive coding in thematic analysis?

- Concept: User-centered design principles
  - Why needed here: The study emphasizes considering end-user requirements in XAI development, a key principle in user-centered design
  - Quick check question: How does involving end-users in the development process improve the effectiveness of AI systems?

- Concept: Cognitive load theory
  - Why needed here: The study discusses controlling information quantity to prevent cognitive overload, a key concept in cognitive load theory
  - Quick check question: What are the three types of cognitive load according to cognitive load theory?

## Architecture Onboarding

- Component map: User Interface -> Explanation Engine -> Data Usage Module -> Review System -> Customization Interface -> Security Module
- Critical path: User request for recommendation → Recommendation generation → Explanation generation → User interaction with explanation → Feedback loop
- Design tradeoffs:
  - Explanation depth vs. user comprehension
  - Data transparency vs. privacy concerns
  - Review authenticity vs. system moderation complexity
- Failure signatures:
  - Users ignoring or dismissing explanations
  - Increased user confusion or frustration
  - Decreased user trust or engagement with the system
- First 3 experiments:
  1. A/B test comparing user comprehension with and without explanation customization options
  2. Survey measuring user trust levels after viewing data usage explanations
  3. Analysis of user decision-making patterns with and without authentic user reviews in explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between information quantity and user comprehension in XAI systems for recommendation systems?
- Basis in paper: [explicit] Users expressed concerns about information overload and the need for control over information quantity
- Why unresolved: The study identified the need for controlled information delivery but did not empirically determine what constitutes the optimal amount of information
- What evidence would resolve it: Controlled experiments varying information quantity and measuring user comprehension, task performance, and satisfaction across different user groups

### Open Question 2
- Question: How does exposure duration to XAI systems affect user comprehension and trust over time?
- Basis in paper: [inferred] The paper notes that participants had limited exposure to XAI systems and suggests long exposure would impact comprehension
- Why unresolved: The study's participants were primarily novice users with limited exposure, and longitudinal effects were not examined
- What evidence would resolve it: Longitudinal studies tracking user comprehension, trust, and usage patterns over extended periods

### Open Question 3
- Question: What are the most effective methods for ensuring the authenticity and trustworthiness of explanations in XAI systems?
- Basis in paper: [explicit] Users expressed skepticism about fake reviews and manipulated explanations
- Why unresolved: While authenticity concerns were identified, the paper did not evaluate specific technical or design solutions for ensuring explanation trustworthiness
- What evidence would resolve it: Comparative studies testing different approaches (e.g., blockchain verification, third-party audits) for their effectiveness in enhancing explanation authenticity

## Limitations

- Small sample size (30 participants) may limit generalizability of findings
- Focus on recommendation systems may not extend to other AI application domains
- Inductive coding approach introduces potential subjectivity in theme identification

## Confidence

- **High confidence**: Users wanting control over explanation granularity is well-supported by multiple participant quotes and aligns with cognitive load theory
- **Medium confidence**: Importance of data usage transparency is supported by participant discussions but lacks corroborating evidence from literature
- **Low confidence**: Emphasis on authentic user reviews is primarily based on participant preferences without substantial validation from user behavior studies

## Next Checks

1. Conduct a larger-scale survey (n>200) to validate generalizability of user preferences for XAI customization and data transparency across different demographics
2. Implement an A/B test comparing user comprehension and trust metrics between systems with and without explanation customization features
3. Perform a comparative analysis of user decision-making patterns when exposed to authentic versus potentially manipulated user reviews in AI explanations