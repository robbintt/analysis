---
ver: rpa2
title: Class-Incremental Continual Learning for General Purpose Healthcare Models
arxiv_id: '2311.04301'
source_url: https://arxiv.org/abs/2311.04301
tags:
- learning
- data
- medical
- continual
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores continual learning methods for medical imaging
  AI models across different hospitals, specialties, and imaging modalities. The authors
  propose four continual learning scenarios using ten diverse classification datasets
  to simulate model sharing and recycling.
---

# Class-Incremental Continual Learning for General Purpose Healthcare Models

## Quick Facts
- arXiv ID: 2311.04301
- Source URL: https://arxiv.org/abs/2311.04301
- Authors: 
- Reference count: 14
- Key outcome: Continual learning methods achieve comparable performance to naive methods on new tasks while maintaining accuracy on previous tasks across medical imaging scenarios

## Executive Summary
This paper addresses the challenge of developing general-purpose medical imaging AI models that can adapt to new tasks across different hospitals, specialties, and imaging modalities. The authors propose four continual learning scenarios using ten diverse classification datasets to simulate model sharing and recycling in healthcare settings. They implement six continual learning methods - MAS, REMIND, MAS+r, NISPA, DER, and DER++ - and compare them to naive and joint training baselines. The results demonstrate that replay-based methods like MAS+r and DER++ perform best overall, showing that continual learning can enable medical AI models to maintain performance on previous tasks while adapting to new ones.

## Method Summary
The paper implements four continual learning scenarios (inter-hospital, inter-specialty, pathology, and radiology) using ten classification datasets from diverse medical imaging contexts. Six continual learning methods are implemented: regularization-based methods (MAS, REMIND, MAS+r) that constrain weight updates to preserve important parameters, and replay-based methods (DER, DER++) that selectively replay high-uncertainty examples from previous tasks. The methods are compared against naive fine-tuning and joint training baselines. All methods use a 5-layer CNN backbone with linear classifiers, and evaluation includes task accuracy percentage, average accuracy on seen classes, and backward transfer metrics.

## Key Results
- Replay-based methods (MAS+r, DER++) outperform regularization-only methods across all scenarios
- Continual learning methods achieve comparable performance to naive methods on new tasks while maintaining accuracy on previous tasks
- REMIND provides memory-efficient and privacy-preserving continual learning through compressed feature representations
- Inter-specialty and inter-hospital scenarios show higher performance variation compared to pathology and radiology scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual learning methods can maintain performance on previous medical imaging tasks while adapting to new tasks across specialties, modalities, and hospitals.
- Mechanism: The paper implements regularization-based methods (MAS, REMIND, MAS+r) and replay-based methods (DER, DER++) that either constrain weight updates to preserve important parameters or selectively replay high-uncertainty examples from previous tasks, enabling sequential learning without catastrophic forgetting.
- Core assumption: The importance of model parameters can be accurately estimated during training, and selectively replaying examples maintains task diversity without requiring full dataset access.
- Evidence anchors:
  - [abstract] "Continual learning allows learning on new tasks without performance drop on previous tasks"
  - [section] "MAS [ 3] and its replay variant MAS+r are regularization methods that calculate the importance of the model parameters in an online fashion"
  - [corpus] Weak evidence - the corpus papers discuss continual learning but don't provide direct validation of the specific mechanisms described in this paper
- Break condition: The methods fail when task distributions are too dissimilar, when the model capacity is insufficient for the combined task set, or when the replay buffer size is inadequate to maintain task diversity.

### Mechanism 2
- Claim: Replay-based methods (MAS+r, DER++) outperform regularization-only methods in multi-specialty medical imaging scenarios.
- Mechanism: Replay methods store a subset of previous task data or compressed representations and interleave them during training on new tasks, providing direct exposure to previous task patterns rather than relying solely on weight regularization.
- Core assumption: Storing and replaying a small subset of previous data is sufficient to maintain task performance while being computationally and storage-efficient.
- Evidence anchors:
  - [abstract] "Replay-based methods like MAS+r and DER++ perform best overall"
  - [section] "Among non-replay methods, MAS has a huge performance drop. REMIND achieves an average accuracy of 83, 77, 75, 80 on inter-hospital, inter-specialty, pathology, and radiology respectively"
  - [corpus] Weak evidence - the corpus contains papers on continual learning but doesn't specifically compare replay vs. regularization methods in medical imaging contexts
- Break condition: Performance degrades when the replay buffer becomes too small relative to task complexity, or when the replayed samples become stale and no longer representative of current task distributions.

### Mechanism 3
- Claim: REMIND's compressed representation storage provides privacy-preserving continual learning without requiring access to raw patient data.
- Mechanism: Instead of storing actual medical images, REMIND stores compressed low-level feature representations that can be used to replay previous task information while maintaining patient privacy and reducing storage requirements.
- Core assumption: Compressed feature representations retain sufficient information to prevent catastrophic forgetting while being computationally efficient and privacy-preserving.
- Evidence anchors:
  - [section] "REMIND [8] stores compressed low-level feature representations instead of actual input data, making it well-suited when past data is not feasible"
  - [section] "It's important to note that replay methods have access to a subset of previous data stored for future retraining. On the other hand, REMIND stores compressed representations, instead of the actual data, allowing memory efficiency and data privacy"
  - [corpus] No direct evidence - the corpus papers don't discuss privacy-preserving replay mechanisms
- Break condition: The method fails when the feature extractor becomes outdated or when the compressed representations lose critical information needed for task performance.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why naive fine-tuning causes performance degradation on previous tasks is fundamental to grasping the need for continual learning methods
  - Quick check question: What happens to a neural network's performance on task A when it is fine-tuned only on task B?

- Concept: Regularization-based continual learning
  - Why needed here: MAS and similar methods use regularization to constrain weight updates based on parameter importance, which is a core mechanism in this work
  - Quick check question: How does MAS determine which parameters are important for previous tasks?

- Concept: Experience replay in machine learning
  - Why needed here: Replay methods (MAS+r, DER, DER++) rely on storing and reusing previous examples, making this concept essential for understanding their operation
  - Quick check question: What is the difference between standard experience replay and the selective replay used in DER++?

## Architecture Onboarding

- Component map: CNN backbone (5-layer) → Linear classifier, replay buffer (size varies by method), feature extractor (frozen in REMIND), regularization terms (MAS), uncertainty metrics (DER++)
- Critical path: Data preprocessing → Model initialization → Task learning loop (with replay/regularization) → Evaluation across all tasks
- Design tradeoffs: Memory vs. performance (larger replay buffers improve accuracy but increase storage), computational cost vs. privacy (REMIND trades some performance for privacy), model capacity vs. task complexity
- Failure signatures: Sharp performance drops on previous tasks during new task learning, high variance in task accuracy, inability to learn new tasks due to capacity constraints
- First 3 experiments:
  1. Baseline test: Train CNN on single medical imaging dataset, evaluate performance
  2. Naive fine-tuning: Sequentially train on multiple datasets without continual learning, measure catastrophic forgetting
  3. Single method comparison: Implement and test MAS on the inter-hospital scenario to validate the mechanism before scaling to multiple methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sequence of tasks impact the performance of continual learning models in medical imaging?
- Basis in paper: [explicit] "We don't explore the effect of the sequence of tasks on learning, as this can impact quality of features learned."
- Why unresolved: The paper acknowledges this as a limitation but does not investigate how different task sequences affect model performance.
- What evidence would resolve it: Conducting experiments with various task sequences and comparing their performance metrics would provide insights into the impact of task ordering on continual learning in medical imaging.

### Open Question 2
- Question: How would using higher resolution medical imaging and larger pre-trained models affect the performance of continual learning methods?
- Basis in paper: [explicit] "We used 32x32x3 image size on a small CNN architecture. Using higher resolution medical imaging and larger pre-trained models can help boost performance further."
- Why unresolved: The paper uses a small CNN architecture with low-resolution images, which may not fully capture the complexity of medical imaging tasks.
- What evidence would resolve it: Experimenting with higher resolution images and larger pre-trained models, then comparing their performance to the current results, would demonstrate the potential benefits of using more advanced architectures and higher quality data.

### Open Question 3
- Question: How can continual learning models address potential biases in training data to ensure fair and accurate medical diagnoses across diverse patient populations?
- Basis in paper: [inferred] "Continual learning models may inherit biases present in the data on which they are trained."
- Why unresolved: The paper mentions potential negative societal impacts of biases in continual learning models but does not propose solutions to mitigate these issues.
- What evidence would resolve it: Developing and testing methods to identify and correct biases in continual learning models, as well as evaluating their performance across diverse patient populations, would help ensure fair and accurate medical diagnoses.

## Limitations
- The evaluation relies on ten classification datasets that may not fully represent the diversity and complexity of real-world clinical scenarios across different healthcare systems.
- The replay buffer sizes and hyperparameter choices for each continual learning method are not fully specified, which could significantly impact performance comparisons.
- The privacy-preserving aspects of REMIND, while theoretically sound, lack empirical validation regarding feature compression quality and its impact on patient privacy guarantees.

## Confidence
- **High confidence**: The observation that replay-based methods (MAS+r, DER++) outperform regularization-only methods across multiple scenarios is well-supported by the experimental results and aligns with established continual learning literature.
- **Medium confidence**: The claim that continual learning methods can achieve comparable performance to naive methods on new tasks while maintaining accuracy on previous tasks requires more rigorous statistical analysis, as the performance differences between methods are sometimes marginal.
- **Low confidence**: The assertion that REMIND provides effective privacy-preserving continual learning lacks direct empirical validation of privacy preservation and doesn't demonstrate the tradeoff between compression level and task performance.

## Next Checks
1. **Statistical significance testing**: Perform paired t-tests or Wilcoxon signed-rank tests to determine whether the performance differences between continual learning methods are statistically significant across all scenarios and tasks.
2. **Privacy validation**: Conduct experiments measuring the information content retained in REMIND's compressed representations and test whether these representations could be used to reconstruct original patient data or infer sensitive attributes.
3. **Real-world deployment simulation**: Test the continual learning methods on a dynamic, streaming dataset that simulates real clinical workflows with non-stationary distributions and varying task arrival patterns.