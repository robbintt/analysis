---
ver: rpa2
title: Learning Universal Policies via Text-Guided Video Generation
arxiv_id: '2302.00111'
source_url: https://arxiv.org/abs/2302.00111
tags:
- video
- learning
- arxiv
- generation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new method, called UniPi, to learn universal
  policies via text-guided video generation. The key idea is to cast the sequential
  decision making problem as a text-conditioned video generation problem, where a
  planner synthesizes future frames depicting planned actions based on a text-encoded
  goal, and control actions are extracted from the generated video.
---

# Learning Universal Policies via Text-Guided Video Generation

## Quick Facts
- arXiv ID: 2302.00111
- Source URL: https://arxiv.org/abs/2302.00111
- Reference count: 19
- Key outcome: UniPi achieves state-of-the-art performance on multi-task and multi-environment generalization in robotic manipulation by learning universal policies through text-guided video generation

## Executive Summary
This paper introduces UniPi, a novel approach to learning universal policies for robotic manipulation tasks by framing sequential decision making as text-conditioned video generation. The method leverages large-scale internet video data to pretrain video diffusion models that can synthesize future frames depicting planned actions, from which control actions are extracted using inverse dynamics models. By using text as the underlying goal specification, UniPi enables natural and combinatorial generalization to novel goals across diverse environments. The approach represents environments with different state and action spaces in a unified image space, enabling learning and generalization across various robot manipulation tasks.

## Method Summary
UniPi casts sequential decision making as a text-conditioned video generation problem, where a planner synthesizes future frames depicting planned actions based on a text-encoded goal. The method employs a Video U-Net architecture with first-frame conditioning and temporal super-resolution for hierarchical planning. Actions are extracted from generated videos using inverse dynamics models. The approach is trained on large-scale internet video-text datasets (14M pairs) and fine-tuned on specific robotic manipulation datasets (7.2k pairs from Bridge dataset). The unified image-based representation enables generalization across different state and action spaces, while text conditioning enables combinatorial generalization to novel goals.

## Key Results
- Achieves state-of-the-art performance on multi-task and multi-environment generalization in robotic manipulation
- Demonstrates improved generalization to novel task commands and scenes not seen during training
- Shows notable performance advantages over traditional policy generation methods across different domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Text-conditioned video generation can represent diverse environments with different state and action spaces in a unified image space.
- **Mechanism**: By converting all environments into image sequences and all goals into text descriptions, the method creates a common interface that bypasses the need for environment-specific state representations.
- **Core assumption**: Images can capture sufficient information about different environments' states to enable effective planning and control.
- **Evidence anchors**:
  - [abstract] "The proposed policy-as-video formulation can further represent environments with different state and action spaces in a unified space of images"
  - [section] "The UPDP formulation is also compatible with other probabilistic models... For completeness we briefly cover the core formulation at a high-level"
- **Break condition**: If certain environments contain critical state information that cannot be adequately represented in images, the unified interface would fail to capture necessary details for effective planning.

### Mechanism 2
- **Claim**: Pretraining on large-scale internet video data enables effective transfer to novel robotic tasks.
- **Mechanism**: The model learns general video generation patterns from diverse internet data, which can then be fine-tuned on specific robotic tasks to produce realistic and task-appropriate plans.
- **Core assumption**: Visual and linguistic patterns learned from internet-scale data are transferable to robotic manipulation tasks.
- **Evidence anchors**:
  - [section] "By pretraining a video generation model on a large-scale text-video dataset recovered from the internet, one can recover a vast repository of 'demonstrations' that aid the construction of a text-conditioned policy"
  - [section] "We find that internet-scale pretraining enables UniPi to generalize to novel task commands and scenes in the test split not seen during training"
- **Break condition**: If robotic tasks require very different motion patterns or environmental contexts than those present in internet videos, pretraining benefits would diminish.

### Mechanism 3
- **Claim**: Hierarchical planning through coarse-to-fine video generation improves consistency and allows for steerable planning.
- **Mechanism**: First generating sparse, abstract video frames at a coarse level, then refining them through temporal super-resolution creates more consistent plans while allowing intermediate guidance.
- **Core assumption**: Temporal super-resolution can effectively interpolate between coarse frames while maintaining task-relevant consistency.
- **Evidence anchors**:
  - [section] "We first generate videos at a coarse level by sparsely sampled videos ('abstractions') of our desired behavior along the time axis. Then we refine the videos to represent valid behavior in the environment by super-resolving videos across time"
  - [section] "Such a planning procedure is naturally hierarchical: a temporally sparse sequence of images toward a goal can first be generated, before being refined with a more specific plan"
- **Break condition**: If the temporal super-resolution model cannot maintain environmental consistency across refinements, the hierarchical approach would produce incoherent plans.

## Foundational Learning

- **Concept**: Diffusion models and their reverse process
  - Why needed here: The core planner is implemented as a conditional diffusion model that generates video frames
  - Quick check question: What is the role of the denoising model s(τk,k|c,x₀) in the reverse process?

- **Concept**: Inverse dynamics modeling
  - Why needed here: Actions are extracted from generated video frames using an inverse dynamics model
  - Quick check question: How does the inverse dynamics model convert image observations into control actions?

- **Concept**: Text embedding and conditioning
  - Why needed here: Text descriptions condition both video generation and inverse dynamics prediction
  - Quick check question: What pretrained model is used to encode text instructions for conditioning the video generation?

## Architecture Onboarding

- **Component map**: Text encoder (T5) → Conditioned video diffusion model → Generated video frames → Inverse dynamics model → Action predictions → Temporal super-resolution module for hierarchical refinement → Pretraining pipeline on internet-scale video-text data

- **Critical path**: Text → Video generation → Action extraction → Execution
  - The video generation must produce consistent frames for the inverse dynamics to extract valid actions

- **Design tradeoffs**:
  - Video-based planning vs direct action prediction: Video planning enables better generalization but is computationally heavier
  - Pretraining scale vs task specificity: Larger pretraining improves generalization but may require more fine-tuning for specific tasks
  - Hierarchical vs flat planning: Hierarchical planning improves consistency but adds complexity

- **Failure signatures**:
  - Generated videos show environmental inconsistency across frames
  - Inverse dynamics fail to extract meaningful actions from generated frames
  - Pretrained model shows poor transfer to robotic tasks despite good video quality

- **First 3 experiments**:
  1. Test video generation quality with different conditioning strengths (ω parameter)
  2. Validate inverse dynamics accuracy on generated vs real video frames
  3. Measure pretraining impact by comparing models trained from scratch vs with internet pretraining

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UniPi compare to traditional RL methods like PPO or SAC when trained on the same data?
- Basis in paper: [inferred] The paper mentions that existing offline RL baselines are not directly applicable due to lack of reward functions, but doesn't compare to traditional RL methods.
- Why unresolved: The paper focuses on comparing UniPi to other imitation learning methods rather than RL algorithms that require reward functions.
- What evidence would resolve it: Training UniPi alongside PPO or SAC on the same data and comparing their performance on the same tasks.

### Open Question 2
- Question: How does the performance of UniPi scale with the size of the pretraining dataset? Is there a point of diminishing returns?
- Basis in paper: [explicit] The paper mentions using internet-scale pretraining datasets and shows improved performance, but doesn't explore scaling effects.
- Why unresolved: The paper uses a fixed pretraining dataset size without exploring how performance changes with dataset size.
- What evidence would resolve it: Training UniPi with varying sizes of pretraining datasets and plotting performance vs. dataset size to identify scaling behavior.

### Open Question 3
- Question: How does UniPi handle tasks that require reasoning about object properties beyond geometric relations (e.g., weight, fragility, or temperature)?
- Basis in paper: [inferred] The paper focuses on geometric manipulation tasks but doesn't address physical properties of objects.
- Why unresolved: The evaluation tasks are limited to geometric manipulation without requiring understanding of object properties.
- What evidence would resolve it: Testing UniPi on tasks that require reasoning about object properties (e.g., "gently pick up the fragile glass" or "lift the heavy box") and analyzing its performance.

## Limitations

- The unified image-based representation may not capture critical state information for tasks requiring precise spatial measurements or force feedback
- Domain shift issues may arise when transferring from internet videos to robotic manipulation tasks with specific physical constraints
- The effectiveness of hierarchical planning depends heavily on the quality of temporal super-resolution, which could fail to maintain task-relevant consistency

## Confidence

- **High confidence** in the core mechanism of converting sequential decision making to text-conditioned video generation
- **Medium confidence** in the pretraining transfer benefits, as the exact contribution of pretraining versus task-specific fine-tuning is not fully isolated
- **Medium confidence** in the hierarchical planning approach, as the impact of temporal super-resolution quality on final performance is not extensively evaluated

## Next Checks

1. **Cross-domain generalization test**: Evaluate performance on robotic tasks with significantly different visual appearances from internet videos (e.g., underwater or nighttime environments) to assess pretraining transfer limits.

2. **State information ablation**: Compare performance when using full state information versus image-only representations on tasks requiring precise measurements to quantify information loss from the unified interface.

3. **Hierarchical vs flat planning comparison**: Systematically compare the hierarchical approach with direct video generation at full resolution across tasks of varying complexity to isolate the benefits of temporal super-resolution.