---
ver: rpa2
title: 'NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval'
arxiv_id: '2310.14282'
source_url: https://arxiv.org/abs/2310.14282
tags:
- entity
- types
- entities
- type
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Named Entity Recognition (NER) research is evolving with advances
  in large language models (LLMs). Traditional NER, focused on identifying entities
  like people, places, and organizations, struggles with fine-grained, specialized
  entity types and intersectional categories.
---

# NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval

## Quick Facts
- **arXiv ID**: 2310.14282
- **Source URL**: https://arxiv.org/abs/2310.14282
- **Reference count**: 13
- **Key outcome**: NERetrieve dataset contains 4 million paragraphs annotated with 500 fine-grained entity types, enabling research on supervised, zero-shot, and retrieval-based NER tasks.

## Executive Summary
Named Entity Recognition (NER) research is evolving with advances in large language models (LLMs). Traditional NER, focused on identifying entities like people, places, and organizations, struggles with fine-grained, specialized entity types and intersectional categories. Current supervised models perform poorly on nuanced entity types, while zero-shot models degrade with more specialized types. A new NERetrieve dataset is introduced, containing 4 million paragraphs annotated with 500 fine-grained entity types, including hierarchical and intersectional categories. This dataset supports research on supervised, zero-shot, and retrieval-based NER tasks. Evaluations show that existing models, including GPT-4, perform poorly on zero-shot NER tasks with fine-grained types, and retrieval systems struggle to locate all relevant documents. NERetrieve facilitates future research to improve NER systems' robustness, granularity, and retrieval capabilities.

## Method Summary
The NERetrieve dataset is constructed by scraping English Wikipedia paragraphs and mapping entities to their aliases from the Caligraph knowledge base. Silver annotations are generated for 500 fine-grained entity types, including hierarchical and intersectional categories. A Bag-of-Words Linear SVM is used to filter noisy entity mentions from the silver-annotated data. The dataset is split into 80% train and 20% test sets. Supervised NER is performed by fine-tuning Spacy models over a pre-trained DeBERTa-v3-large MLM. Zero-shot NER is conducted by prompting GPT-3.5-turbo and GPT-4 using the function calling API. Exhaustive retrieval is evaluated using BM25, BGE, GTE, and E5-v2 models for semantic search, with the goal of retrieving all mentions of entities of a given type from the indexed collection.

## Key Results
- GPT-4 demonstrates robustness in zero-shot NER, with only 1 false positive out of 500 paragraphs.
- Existing retrieval models struggle with the exhaustive typed-entity mention retrieval task, achieving low Recall@|REL| scores.
- Fine-grained, hierarchical, and intersectional entity types pose significant challenges for both supervised and zero-shot NER models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A Bag-of-Words Linear SVM trained on context paragraphs is effective for filtering noisy entity mentions from silver-annotated data.
- Mechanism: The model learns to classify whether a paragraph is likely to contain mentions of a given entity type based on its TF/IDF features, without memorizing specific entity names.
- Core assumption: Paragraph-level context is sufficient to determine whether entity mentions are relevant, and linear models are less prone to overfitting to specific entity names.
- Evidence anchors:
  - [section] "Our filtering model is a Bag-of-Words Linear SVM, trained individually for each entity type using TF/IDF word features. Positive examples are from a held-out set matching the type, while negatives are randomly chosen paragraphs from other entity types."
  - [section] "Preliminary experiments indicate its efficiency at scale."
  - [corpus] Weak - no quantitative performance numbers reported for the filtering model itself.
- Break condition: If the context needed to disambiguate entities requires long-range dependencies or nuanced semantic understanding beyond bag-of-words features, the linear SVM will fail.

### Mechanism 2
- Claim: Zero-shot NER with LLMs is more robust than supervised NER when applied to texts that do not naturally contain the target entity type.
- Mechanism: LLMs leverage their pre-trained world knowledge to avoid falsely identifying non-entities as entities of the target type, whereas supervised models rely on statistical patterns that can lead to false positives in out-of-domain texts.
- Core assumption: LLMs have internalized sufficient world knowledge during pre-training to recognize when a text context does not support a given entity type hypothesis.
- Evidence anchors:
  - [section] "For the Zero-shot setting, a prompted gpt-3.5-turbo model was remarkably more robust, only identifying a false positive in 1 out of 500 paragraphs. This highlights the robustness of the LLMs in the zero-shot setting and suggests that such robustness should also be possible for dedicated, supervised models."
  - [section] "The first experiment quantifies the robustness of NER models when applied to domain-adjacent texts, texts that do not contain and are not expected to contain, entities of the types the model aims to identify."
  - [corpus] Weak - no quantitative comparison of false positive rates across multiple supervised models.
- Break condition: If the LLM's world knowledge is incomplete or incorrect for the target entity types, or if the prompt engineering is suboptimal, zero-shot robustness may degrade.

### Mechanism 3
- Claim: Exhaustive retrieval of all entity mentions requires specialized approaches beyond standard semantic search, due to the high recall requirement and the need to identify entity spans within documents.
- Mechanism: The task demands methods that can handle large candidate sets (median 23,000 relevant documents per query) and go beyond top-k ranking to ensure all relevant mentions are found, likely requiring a combination of entity recognition and retrieval.
- Core assumption: Standard dense and sparse retrieval methods are optimized for precision and top-k ranking, not for exhaustive recall of all relevant mentions.
- Evidence anchors:
  - [section] "Crucially, unlike standard retrieval setups which are focused on finding the most relevant documents in a collection, the NERetrieve setup is exhaustive: we aim to locate all the relevant documents in the indexed collection."
  - [section] "To our knowledge, no current system is directly applicable to our proposed NERetrieve task: text embedding and similarity-based methods are not designed for the entity-type identification task, and retrieval systems are not designed to support exhaustive retrieval of all items."
  - [corpus] Weak - the reported Recall@|REL| scores are low, but no comparison to a baseline exhaustive approach is provided.
- Break condition: If a retrieval method can be adapted or combined with entity recognition to achieve high recall without sacrificing precision, the need for specialized exhaustive retrieval methods may be reduced.

## Foundational Learning

- Concept: Entity types and hierarchies
  - Why needed here: Understanding the different levels of granularity in entity types (e.g., ANIMAL → INSECT → BUTTERFLY) is crucial for appreciating the challenges of fine-grained NER and the potential benefits of hierarchical approaches.
  - Quick check question: What is the relationship between the entity types "MAMMAL" and "DOG_BREED"? Are they at the same level of granularity?

- Concept: Zero-shot learning
  - Why needed here: Zero-shot NER and retrieval are central to the paper's vision for next-generation NER, so understanding the concept of performing a task without task-specific training examples is essential.
  - Quick check question: In zero-shot NER, how does the model know what entities to identify if it hasn't seen examples of the target entity type during training?

- Concept: Recall vs. Precision in retrieval
  - Why needed here: The exhaustive retrieval task emphasizes high recall, so understanding the tradeoff between finding all relevant items (recall) and avoiding false positives (precision) is important for evaluating retrieval methods.
  - Quick check question: If a retrieval system has high recall but low precision, what does that mean about the results it returns?

## Architecture Onboarding

- Component map: Data collection and annotation -> Supervised NER model -> Zero-shot NER with LLM -> Exhaustive retrieval system

- Critical path: For the zero-shot NER task, the critical path is prompting the LLM with the text and entity type query, and processing the LLM's output to extract the identified entities and their spans. For the exhaustive retrieval task, the critical path is indexing the paragraphs, taking an entity type query, retrieving relevant documents, and identifying the entity mentions within those documents.

- Design tradeoffs:
  - Silver vs. gold annotation: Silver annotation allows scaling to large datasets but introduces noise that must be filtered.
  - Fine-grained vs. coarse entity types: Finer types are more useful but harder to recognize and require more data.
  - Zero-shot vs. few-shot vs. supervised: Zero-shot is most flexible but may have lower performance; supervised is most accurate but requires annotated data.

- Failure signatures:
  - Supervised NER: High false positive rate on out-of-domain texts, poor performance on fine-grained or intersectional types
  - Zero-shot NER: Missing relevant entities, misidentifying non-entities, poor performance on very specific types
  - Exhaustive retrieval: Missing relevant documents, misidentifying entity mentions, slow retrieval due to large candidate sets

- First 3 experiments:
  1. Evaluate the context classifier's precision and recall on a held-out set of annotated paragraphs to quantify the noise in the silver data.
  2. Compare the performance of zero-shot NER with different prompt engineering strategies (e.g., different prompt formats, few-shot demonstrations) to find the most effective approach.
  3. Evaluate the exhaustive retrieval system's recall and precision on a subset of entity types, comparing different retrieval and entity recognition methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a more robust filtering model that accurately identifies relevant paragraphs for each entity type while minimizing false positives and negatives?
- Basis in paper: [inferred] The paper mentions using a Bag-of-Words Linear SVM as a contextual filtering model to select paragraphs that both contain many entities of interest and were determined by the model to be reliable contexts for these entities. However, the paper acknowledges that the dataset is silver-annotated and not perfect, indicating a need for improvement in the filtering model.
- Why unresolved: The paper does not provide detailed information on the performance of the current filtering model or discuss potential improvements. It also does not explore alternative filtering approaches that could be more effective.
- What evidence would resolve it: A comparison of the performance of the current filtering model with other potential models, such as more advanced machine learning algorithms or deep learning approaches, on a held-out test set of paragraphs. Additionally, a qualitative analysis of the false positives and negatives produced by the current model could identify specific areas for improvement.

### Open Question 2
- Question: How can we effectively evaluate the performance of NER models on fine-grained, hierarchical, and intersectional entity types?
- Basis in paper: [explicit] The paper highlights the challenges of NER with fine-grained, hierarchical, and intersectional entity types, and provides examples of the performance deterioration as the level of specificity increases. However, it does not discuss specific evaluation metrics or methodologies for these types of entities.
- Why unresolved: The paper focuses on demonstrating the challenges of fine-grained NER but does not provide a comprehensive evaluation framework for these types of entities. It also does not discuss how to handle the increased complexity of evaluating models on hierarchical and intersectional types.
- What evidence would resolve it: A set of evaluation metrics specifically designed for fine-grained, hierarchical, and intersectional entity types, along with a methodology for applying these metrics to NER models. This could include metrics that account for the hierarchical relationships between entity types and the intersectional nature of some entities.

### Open Question 3
- Question: How can we develop more effective methods for exhaustive typed-entity mention retrieval that can handle the scale and complexity of the NERetrieve dataset?
- Basis in paper: [explicit] The paper introduces the NERetrieve task, which involves retrieving all mentions of entities of a given type from a large corpus. It evaluates several state-of-the-art retrieval models on this task but finds that they perform poorly, indicating a need for more effective methods.
- Why unresolved: The paper does not provide a detailed analysis of the limitations of current retrieval models or discuss potential approaches for improving their performance on the NERetrieve task. It also does not explore the use of more advanced retrieval techniques, such as those based on neural networks or transformers.
- What evidence would resolve it: A comparison of the performance of various retrieval models, including both traditional and advanced techniques, on the NERetrieve dataset. Additionally, a qualitative analysis of the errors made by these models could identify specific areas for improvement, such as handling long-tail entities or resolving ambiguous mentions.

## Limitations

- The zero-shot NER experiments with GPT-4 lack comprehensive quantitative comparisons against multiple supervised baselines across all 500 entity types.
- The exhaustive retrieval task's difficulty is not compared against a baseline exhaustive approach, making it difficult to assess whether the challenge stems from the task itself or the specific retrieval methods tested.
- The silver annotation pipeline introduces potential noise that is only partially mitigated by the context classifier, and the classifier's effectiveness is only preliminarily evaluated without full quantitative metrics.

## Confidence

- **High Confidence**: The core claims about traditional NER limitations with fine-grained and intersectional entity types, supported by extensive citations and the paper's novel contribution of the NERetrieve dataset with 500 entity types.
- **Medium Confidence**: The zero-shot NER robustness claims, as they are demonstrated with GPT-3.5 and GPT-4 but lack comprehensive quantitative comparisons against multiple supervised baselines across all entity types.
- **Low Confidence**: The claims about the exhaustiveness of the retrieval task's difficulty, as the low Recall@|REL| scores are not compared against a baseline exhaustive approach, and the dataset's coverage of all possible entity mentions is not verified.

## Next Checks

1. **Quantitative Context Classifier Evaluation**: Implement a comprehensive evaluation of the context classifier's precision and recall on a held-out set of annotated paragraphs to quantify the noise in the silver data and the classifier's effectiveness.

2. **Multi-Model Zero-Shot NER Comparison**: Conduct a thorough comparison of zero-shot NER performance across multiple supervised models (e.g., fine-tuned transformers) and LLMs on the 100 test entity types, using standardized prompt engineering strategies.

3. **Baseline Exhaustive Retrieval Comparison**: Implement and evaluate a baseline exhaustive retrieval method (e.g., combining dense retrieval with entity recognition) on a subset of entity types to assess whether the low Recall@|REL| scores are due to the task's inherent difficulty or the specific retrieval approaches tested.