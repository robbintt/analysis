---
ver: rpa2
title: 'ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement
  with Large Language Models (LLMs)'
arxiv_id: '2312.09801'
source_url: https://arxiv.org/abs/2312.09801
tags:
- students
- procot
- llms
- writing
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProCoT, a novel writing method, prevents cheating while enhancing
  active learning. Students engage with LLMs by affirming/refuting statements using
  peer-reviewed references.
---

# ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement with Large Language Models (LLMs)

## Quick Facts
- arXiv ID: 2312.09801
- Source URL: https://arxiv.org/abs/2312.09801
- Reference count: 5
- ProCoT prevents cheating while enhancing active learning through peer-reviewed reference verification

## Executive Summary
ProCoT is a novel writing method that requires students to engage with Large Language Models (LLMs) by affirming or refuting their statements using peer-reviewed references. This approach transforms LLMs from potential cheating tools into learning aids that stimulate critical thinking and writing skills. By requiring students to fact-check LLM outputs against academic sources, ProCoT creates a clear distinction between student work and LLM-generated content, making cheating easily detectable while promoting active learning.

## Method Summary
Students pose questions to LLMs (ChatGPT or Phind) and then critique the generated responses by verifying claims against peer-reviewed references. This iterative process involves forethought, planning, monitoring, and correction—core components of self-regulation learning. The method was tested with 66 students across two cases, comparing student ProCoT outputs to both LLM-only outputs and LLM ProCoT outputs to assess cheating prevention and critical thinking stimulation.

## Key Results
- Average word counts: students (208 words), ChatGPT (391 words), Phind (383 words)
- ProCoT effectively prevents cheating by exploiting LLM limitations in providing accurate peer-reviewed references
- Students demonstrate enhanced critical thinking through engagement with LLM outputs and academic sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProCoT engages students in active learning by requiring them to fact-check LLM outputs using peer-reviewed references.
- Mechanism: The iterative process of critiquing LLM-generated statements forces students to apply self-regulation skills—forethought, planning, monitoring, and correction—while sourcing evidence, thus promoting deeper engagement.
- Core assumption: Students will critically engage with the LLM output when required to verify claims rather than passively accepting them.
- Evidence anchors:
  - [abstract] Students were required to "affirm or refute statements in the LLM output by using peer-reviewed references."
  - [section 3] ProCoT is "entrenched in the 'Self-Regulation' method... which involves the following components in an iterative process: forethought, planning, monitoring, failure detection and correction."
- Break condition: If students skip the reference-checking step or use unreliable sources, the critical thinking stimulus fails.

### Mechanism 2
- Claim: ProCoT prevents cheating by exploiting the limitations of LLMs in providing accurate, verifiable references.
- Mechanism: Students must produce original content grounded in peer-reviewed literature, which LLMs struggle to do reliably; comparing student ProCoT outputs to LLM ProCoT outputs reveals these gaps.
- Core assumption: LLMs like ChatGPT and Phind cannot consistently generate accurate peer-reviewed references, creating a clear distinction from student work.
- Evidence anchors:
  - [abstract] "ProCoT can prevent cheating because of clear limitations in existing LLMs... when we compare students' ProCoT output to LLM ProCoT output."
  - [section 4] ChatGPT "expressly says 'I cannot provide direct references to peer-reviewed articles'" while Phind "appears to lift the same content supplied to it as if regurgitating... more than paraphrasing."
- Break condition: If LLMs improve their ability to cite accurate peer-reviewed references, the cheating prevention mechanism weakens.

### Mechanism 3
- Claim: ProCoT reveals student preference for concise writing compared to verbose LLM outputs.
- Mechanism: By analyzing word counts, educators can identify potential cheating (unusually high counts matching LLM verbosity) and tailor feedback accordingly.
- Core assumption: Students will naturally produce shorter, more concise answers than LLMs, which tend to be verbose.
- Evidence anchors:
  - [abstract] "Most students prefer to give answers in fewer words than LLMs... average word counts for students, ChatGPT (v3.5) and Phind (v8) are 208, 391 and 383, respectively."
  - [section 4] "This implies most students prefer using fewer words to express their answers."
- Break condition: If students adopt verbose writing styles to mimic LLMs, the word count heuristic becomes less reliable.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: ProCoT builds on CoT by adding a verification layer where students must engage with and critique the LLM's reasoning steps.
  - Quick check question: What is the difference between standard CoT prompting and ProCoT?

- Concept: Self-regulation in learning
  - Why needed here: ProCoT's iterative process of critiquing LLM outputs mirrors self-regulation components like planning, monitoring, and correction.
  - Quick check question: Which self-regulation component is most directly exercised when students verify LLM claims with references?

- Concept: Peer review and academic integrity
  - Why needed here: ProCoT relies on students sourcing peer-reviewed references, reinforcing academic integrity and evidence-based argumentation.
  - Quick check question: Why is grounding student responses in peer-reviewed references important for preventing cheating?

## Architecture Onboarding

- Component map: LLM interface (ChatGPT/Phind) -> Student critique module -> Reference database -> Evaluation rubric -> Comparison engine
- Critical path: 1. Student receives prompt and generates LLM output 2. Student critiques LLM output, sourcing peer-reviewed references 3. Output is evaluated for originality, reference quality, and conciseness 4. Comparison with LLM ProCoT output to detect cheating
- Design tradeoffs:
  - Balancing verbosity: LLMs are verbose; students are concise. Should the system reward brevity or penalize verbosity?
  - Reference verification: Automated vs. manual checking of peer-reviewed sources
  - Scalability: Manual evaluation vs. automated tools for large classes
- Failure signatures:
  - High word count matching LLM verbosity
  - Lack of peer-reviewed references
  - Direct copying of LLM output without critique
  - Overuse of non-academic sources
- First 3 experiments:
  1. Compare student ProCoT outputs to LLM ProCoT outputs for a small sample to establish baseline differences
  2. Test automated reference verification tools to scale the process
  3. A/B test different word count thresholds to optimize cheating detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ProCoT's effectiveness in preventing cheating vary across different LLM models and their capabilities?
- Basis in paper: [explicit] The paper compares students' ProCoT output to LLM ProCoT output, noting limitations in existing LLMs like ChatGPT.
- Why unresolved: Different LLM models have varying capabilities and limitations, and the study only tested a few models.
- What evidence would resolve it: Comparative studies across a wider range of LLM models to evaluate ProCoT's effectiveness in preventing cheating.

### Open Question 2
- Question: What is the long-term impact of ProCoT on students' critical thinking and writing skills?
- Basis in paper: [inferred] The paper suggests ProCoT stimulates creative/critical thinking and writing, but doesn't discuss long-term effects.
- Why unresolved: The study's timeframe may not capture the full impact on students' skills over time.
- What evidence would resolve it: Longitudinal studies tracking students' critical thinking and writing skills over an extended period.

### Open Question 3
- Question: How can ProCoT be adapted for disciplines outside of essay writing, such as coding or mathematics?
- Basis in paper: [explicit] The paper suggests ProCoT may be applied to any field that uses writing and mentions coding and mathematics as potential areas.
- Why unresolved: The paper doesn't provide specific examples or methodologies for adapting ProCoT to other disciplines.
- What evidence would resolve it: Case studies or pilot programs implementing ProCoT in various disciplines beyond writing.

## Limitations
- Study sample size (66 students) may not provide sufficient statistical power for generalization
- Methodology relies on current LLM limitations that may evolve rapidly
- Limited question set (50 exam questions) may not capture full range of academic contexts

## Confidence

- High confidence: Students produce more concise outputs than LLMs (208 vs. 391/383 words)
- Medium confidence: Cheating prevention mechanism effectiveness depends on current LLM limitations
- Medium confidence: Critical thinking stimulation claims based on self-reported engagement rather than direct cognitive assessment

## Next Checks

1. Replicate the study with additional LLM models and versions to test robustness of cheating detection across different AI systems
2. Implement blind grading of ProCoT outputs to assess whether reference quality and critical engagement differ significantly from traditional essay assignments
3. Conduct a longitudinal study tracking student performance and engagement over multiple semesters to evaluate sustained impact on learning outcomes