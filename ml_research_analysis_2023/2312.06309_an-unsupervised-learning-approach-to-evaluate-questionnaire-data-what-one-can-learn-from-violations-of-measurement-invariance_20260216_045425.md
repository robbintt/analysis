---
ver: rpa2
title: An unsupervised learning approach to evaluate questionnaire data -- what one
  can learn from violations of measurement invariance
arxiv_id: '2312.06309'
source_url: https://arxiv.org/abs/2312.06309
tags:
- data
- response
- groups
- group
- types
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an unsupervised learning-based method for evaluating
  questionnaire data, especially when measurement invariance is violated. The approach
  involves data preparation, clustering questionnaires, and measuring similarity between
  groups using the proportion of each response type.
---

# An unsupervised learning approach to evaluate questionnaire data -- what one can learn from violations of measurement invariance

## Quick Facts
- arXiv ID: 2312.06309
- Source URL: https://arxiv.org/abs/2312.06309
- Reference count: 40
- Key outcome: An unsupervised learning approach that translates measurement invariance violations into meaningful group similarity measures using response type clustering

## Executive Summary
This paper proposes an unsupervised learning-based method for evaluating questionnaire data, especially when measurement invariance is violated. The approach involves data preparation, clustering questionnaires, and measuring similarity between groups using the proportion of each response type. Three synthetic datasets were generated to compare this method with PCA under measurement invariance and its violation. Results show that the approach provides a natural comparison between groups and a description of response patterns, can be applied to various data sets even without measurement invariance, and translates violations into a meaningful similarity measure. The method is robust to the number of clusters and offers a natural dimensionality reduction from individual questionnaires to group fingerprints.

## Method Summary
The approach uses a three-phase algorithm: (1) Data preparation with k-nearest neighbor imputation and balancing group sizes through oversampling, (2) Clustering questionnaires using agglomerative clustering with Ward's linkage, and (3) Measuring group similarity based on the proportion of each response type in the group. The method generates synthetic datasets with controlled measurement invariance violations to demonstrate its effectiveness compared to traditional PCA approaches.

## Key Results
- The proposed approach provides a natural comparison between groups and describes response patterns even when measurement invariance is violated
- The method can be applied to various data sets regardless of measurement invariance status
- The approach translates violations of measurement invariance into a meaningful measure of similarity between groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering questionnaires into response types captures group similarities even when factor loadings differ between groups.
- Mechanism: By clustering questionnaires and computing the proportion of each cluster in each group, the method bypasses the need for identical factor structures. The fingerprint vector (proportions) is invariant to differences in item-level loadings.
- Core assumption: Clusters found on the overall dataset represent interpretable response patterns that are meaningful within each group, regardless of differing underlying constructs.
- Evidence anchors:
  - [abstract] "This approach allows us to translate (violations of) measurement invariance into a meaningful measure of similarity."
  - [section 2.2] "The clustering obtained in this paper is due to performing a standard agglomerative clustering with Ward's minimum variance criterion..."
  - [corpus] Weak: No direct corpus evidence linking clustering to invariance violations; evidence is internal to the paper.
- Break condition: If clusters are dominated by noise or if group sample sizes are too small to form stable clusters.

### Mechanism 2
- Claim: Data augmentation with Gaussian noise stabilizes cluster centroids and improves reproducibility.
- Mechanism: Adding small independent noise to each item value reduces duplicate rows, increases matrix rank, and makes cluster centers more robust to individual data point removal.
- Core assumption: Noise level is small enough to preserve true response patterns but large enough to break ties and stabilize centroids.
- Evidence anchors:
  - [section 3, Phase 1] "Perturb each item value independently by a defined noise function" and "the data matrix becomes full rank..."
  - [corpus] No direct corpus evidence; the claim is based on general ML augmentation principles cited in references [3, 10, 24, 39].
- Break condition: If noise variance is too high, true response patterns are obscured; if too low, duplicates persist and centroids are unstable.

### Mechanism 3
- Claim: Balanced group sizes via oversampling ensures rare response patterns are not ignored during clustering.
- Mechanism: By oversampling minority groups until all groups have equal size, the clustering algorithm is forced to consider all patterns, preventing dominant groups from biasing centroids.
- Core assumption: Oversampling does not introduce false patterns and the original group proportions are preserved for final fingerprint calculation.
- Evidence anchors:
  - [section 3, Phase 1] "Balance group sizes by upsampling rows of smaller groups" and "the actual evaluation will only be with respect to original data..."
  - [corpus] No direct corpus evidence; oversampling is standard in supervised learning per references [5, 22].
- Break condition: If oversampling creates synthetic duplicates that dominate the clustering before noise is added.

## Foundational Learning

- Concept: Measurement invariance
  - Why needed here: The method is designed specifically to work when standard PCA/factor analysis fails due to differing item loadings across groups.
  - Quick check question: What is the consequence of violating measurement invariance for group comparisons using PCA?

- Concept: Agglomerative clustering with Ward linkage
  - Why needed here: It produces compact, equally sized clusters suitable for defining interpretable response types.
  - Quick check question: Why is Ward's method preferred over other linkage criteria in this application?

- Concept: Gap statistic for determining number of clusters
  - Why needed here: It provides an algorithmic way to choose k without manual inspection, ensuring reproducibility.
  - Quick check question: How does the gap statistic decide when to stop merging clusters?

## Architecture Onboarding

- Component map: Data Imputation (k-NN) -> Group Balancing (oversampling) -> Data Augmentation (Gaussian noise) -> Clustering (Ward linkage) -> Response Type Extraction (centroids) -> Fingerprint Calculation (proportions per group) -> Similarity Measurement (hierarchical clustering on fingerprints)
- Critical path: Imputation → Balancing → Augmentation → Clustering → Fingerprints → Similarity
- Design tradeoffs:
  - More clusters → better resolution but harder interpretation and risk of overfitting.
  - Higher noise variance → more stable centroids but risk of pattern loss.
  - Oversampling ratio → ensures rare patterns are captured but increases computation.
- Failure signatures:
  - Empty clusters → too many clusters or poor separation.
  - Fingerprints dominated by one response type → groups are too homogeneous or k too small.
  - Gap statistic flat → no natural clustering structure.
- First 3 experiments:
  1. Run pipeline on D1 with k chosen by gap statistic; verify fingerprints match known group structure.
  2. Intentionally violate measurement invariance in synthetic data; confirm method still produces meaningful fingerprints while PCA fails.
  3. Vary noise level and oversampling ratio; observe impact on cluster stability and fingerprint similarity dendrogram.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed unsupervised learning-based approach compare to other existing methods for handling measurement invariance violations in questionnaire data analysis?
- Basis in paper: [explicit] The paper mentions that the proposed approach can be applied even when measurement invariance is violated, unlike standard methods like PCA or factor analysis. It also states that the approach provides a natural comparison between groups and a description of response patterns.
- Why unresolved: While the paper demonstrates the approach's effectiveness on synthetic data, it does not compare its performance to other existing methods for handling measurement invariance violations in real-world questionnaire data.
- What evidence would resolve it: A comprehensive study comparing the proposed approach to other existing methods on various real-world questionnaire datasets, evaluating their performance in terms of accuracy, interpretability, and robustness to different types of measurement invariance violations.

### Open Question 2
- Question: How does the choice of the number of response types (clusters) affect the interpretability and meaningfulness of the results obtained from the proposed approach?
- Basis in paper: [explicit] The paper discusses the impact of choosing different numbers of response types on the interpretability of the results. It mentions that overestimation can lead to response types that are harder to interpret, while underestimation might result in loss of information.
- Why unresolved: The paper does not provide a clear guideline or automatic method for determining the optimal number of response types. It relies on visual inspection of the dendrogram and the gap statistic, which might not always be straightforward or reliable.
- What evidence would resolve it: A study investigating the impact of different numbers of response types on the interpretability and meaningfulness of the results, and proposing a more objective and automated method for determining the optimal number of response types based on the characteristics of the data.

### Open Question 3
- Question: How can the proposed approach be extended to handle questionnaire data with more complex structures, such as hierarchical or longitudinal data?
- Basis in paper: [inferred] The paper focuses on analyzing questionnaire data with a relatively simple structure, where each questionnaire consists of a set of items measuring the same or different constructs. It does not discuss how the approach can be extended to handle more complex data structures.
- Why unresolved: The proposed approach might not be directly applicable to questionnaire data with hierarchical or longitudinal structures, as these structures introduce additional complexities that need to be considered in the analysis.
- What evidence would resolve it: Research exploring how the proposed approach can be modified or extended to handle questionnaire data with hierarchical or longitudinal structures, and evaluating the performance of these extensions on real-world datasets with such structures.

## Limitations

- The method's reliance on the interpretability of unsupervised clusters may not always align with meaningful response patterns in real-world data
- The choice of noise variance (0.66) and oversampling ratios are not fully justified by corpus evidence
- The method's performance on datasets with complex missing data patterns beyond simple k-NN imputation remains unexplored

## Confidence

- **High Confidence**: The method's ability to provide dimensionality reduction from individual questionnaires to group fingerprints, and its robustness to the number of clusters.
- **Medium Confidence**: The claim that clustering captures group similarities when factor loadings differ, based on internal validation rather than external corpus evidence.
- **Low Confidence**: The assertion that data augmentation with Gaussian noise significantly improves reproducibility, as this relies on general ML principles without direct application-specific validation.

## Next Checks

1. **Cross-validation with real questionnaire data**: Apply the method to a publicly available questionnaire dataset where measurement invariance is known to be violated, and compare results with established techniques.
2. **Sensitivity analysis on noise and oversampling**: Systematically vary noise variance and oversampling ratios to determine their impact on cluster stability and fingerprint similarity.
3. **Missing data scenarios**: Test the method's robustness when faced with different patterns and amounts of missing data, beyond the simple k-NN imputation used in the paper.