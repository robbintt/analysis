---
ver: rpa2
title: Deep Natural Language Feature Learning for Interpretable Prediction
arxiv_id: '2311.05754'
source_url: https://arxiv.org/abs/2311.05754
tags:
- abstract
- does
- answer
- question
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to break down a complex task into
  simpler binary sub-tasks formulated as natural language questions. An LLM is used
  to generate these questions and to weakly label a small subset of the training data
  for each sub-task.
---

# Deep Natural Language Feature Learning for Interpretable Prediction

## Quick Facts
- arXiv ID: 2311.05754
- Source URL: https://arxiv.org/abs/2311.05754
- Reference count: 40
- This paper presents a method to break down a complex task into simpler binary sub-tasks formulated as natural language questions, using LLM-generated weak labels to train a BERT model for interpretable decision trees.

## Executive Summary
This paper introduces a novel approach for interpretable prediction in NLP tasks by decomposing complex problems into binary sub-tasks expressed as natural language questions. An LLM generates these questions and provides weak labels on a subset of training data, which are used to train a BERT model in a natural language inference fashion. The resulting Natural Language Learned Features (NLLF) vector serves as input to a decision tree, achieving high performance while maintaining interpretability. The method is demonstrated on two tasks: detecting incoherence in student answers to math questions and screening abstracts for systematic literature reviews.

## Method Summary
The method breaks down complex NLP tasks into binary sub-tasks formulated as natural language questions. An LLM (ChatGPT) is prompted to generate these questions and weakly label a small subset of training data for each sub-task. A BERT-like model (NLLFG) is then trained in a natural language inference fashion to predict answers to any binary question, enabling zero-shot inference. This produces a vector of Natural Language Learned Features (NLLF) for each example, which can be used as input to an interpretable model like a decision tree. The approach is evaluated on two tasks: detecting incoherence in student answers to math questions and screening abstracts for systematic literature reviews.

## Key Results
- Decision trees using NLLF features achieve high F1-scores (78.09% for incoherent class in IAD, 67.75% macro F1 for SAC) while remaining interpretable.
- The NLLF vector not only enhances classifier performance but also enables interpretable decision-making through feature importance analysis.
- The method outperforms baseline models on both datasets, with the variant using NLLF combined with expert features showing the best performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM can reliably generate useful binary subtask questions when prompted with examples.
- Mechanism: By prompting an LLM with a few examples of main task questions and their related binary subquestions, the model learns to generate new subquestions that are verifiable in the text and directly useful for solving the main task.
- Core assumption: The LLM has sufficient zero-shot or few-shot reasoning ability to decompose complex tasks into simpler, verifiable subquestions.
- Evidence anchors:
  - [section 2.1]: "We prompt using an instruction-based template... We generate a set of 5 basic binary questions per sample useful to solve the main task"
  - [section 3.3.3]: "We used a pq of 1.3% (21 examples) to generate BSQ"
  - [corpus]: FMR scores vary widely; weak evidence for task decomposition reliability

### Mechanism 2
- Claim: A BERT-like model trained in NLI fashion can effectively learn to answer binary subquestions from LLM-generated weak labels.
- Mechanism: The NLI-style training treats each (text, subquestion) pair as premise and hypothesis, allowing the BERT to leverage semantic knowledge and learn to predict Yes/No answers, even for unseen questions.
- Core assumption: The NLI training setup enables zero-shot generalization to new subquestions beyond the training set.
- Evidence anchors:
  - [abstract]: "The NLI-like training of the BERT allows for tackling zero-shot inference with any binary question"
  - [section 2.3]: "It has two strong advantages: (i) it leverages the semantic knowledge encoded during pre-training... (ii) it can be applied in a zero-shot manner using new labels formulated as natural language binary question"
  - [corpus]: No direct corpus evidence; assumption based on BERT pre-training literature

### Mechanism 3
- Claim: Decision trees using NLLF features can match or exceed transformer performance while remaining interpretable.
- Mechanism: The NLLF vector provides a controlled representation of the text in terms of subquestion answers, which decision trees can use to make interpretable decisions. Combining NLLF with expert features yields the best results.
- Core assumption: The NLLF features capture task-relevant information effectively enough for simple models to perform well.
- Evidence anchors:
  - [abstract]: "We show that this NLLF vector not only helps to reach better performances by enhancing any classifier, but that it can be used as input of an easy-to-interpret machine learning model like a decision tree"
  - [section 3.4]: "The DT models also reached high performances across the board... specifically, the variant using NLLF+EF displays highest F1-score (78.09% and 67.75%)"
  - [corpus]: Weak; no corpus papers specifically validate this claim

## Foundational Learning

- Concept: Binary classification with NLI-style training
  - Why needed here: The method relies on framing subquestion answering as natural language inference (premise= text, hypothesis= subquestion).
  - Quick check question: Given a premise "The sky is blue" and hypothesis "The sky is not blue", what NLI label would you predict?

- Concept: Feature selection and representation
  - Why needed here: After generating NLLF vectors, feature selection removes uninformative subquestions to improve model performance.
  - Quick check question: If a subquestion always gets "Yes" for all examples, should it be kept in the final feature set?

- Concept: Decision tree interpretability
  - Why needed here: The final model is a decision tree that must be interpretable; understanding Gini impurity and tree depth is crucial.
  - Quick check question: What does a Gini impurity of 0 at a node indicate about the samples at that node?

## Architecture Onboarding

- Component map: LLM (ChatGPT) → generates binary subquestions (BSQs) and weak labels → BERT-like model (NLLFG) → trained on BSQs + weak labels to predict subquestion answers → Feature selection → picks most useful subquestions from NLLF output → Decision tree → uses NLLF + expert features to make final predictions → Expert features → hand-crafted linguistic rules added to NLLF

- Critical path: LLM BSQ generation → LLM weak labeling → NLLFG training → NLLF generation → feature selection → decision tree training

- Design tradeoffs:
  - LLM cost vs. number of examples for BSQ generation (more examples = better subquestions but higher cost)
  - BERT size vs. training data (larger BERT needs more weak labels)
  - Number of subquestions (C+) vs. feature selection quality (too many = noise, too few = insufficient coverage)

- Failure signatures:
  - LLM generates irrelevant or ungrammatical subquestions → downstream models fail
  - BERT overfits to weak labels → poor generalization to new subquestions
  - Feature selection removes too many useful subquestions → decision tree performance drops
  - Decision tree depth too shallow → underfitting, performance below baseline

- First 3 experiments:
  1. Test LLM subquestion generation quality by manually evaluating 20 generated subquestions for relevance and verifiability
  2. Train NLLFG on a small subset of weak labels and evaluate zero-shot performance on new subquestions
  3. Compare decision tree performance with and without NLLF features on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of the NLLFG model change as the number of examples used for training increases?
- Basis in paper: [inferred] The paper mentions that the authors observed a correlation between the number of examples shown to the NLLFG during training and its performance. They state that "we would like to monitor the performances of the NLLFG when trained with more weak labels from the LLM."
- Why unresolved: The paper does not provide experimental results showing the relationship between training data size and NLLFG accuracy.
- What evidence would resolve it: Conducting experiments with varying training set sizes and measuring NLLFG performance on a held-out validation set.

### Open Question 2
- Question: Can the proposed method be successfully applied to tasks with more than two classes?
- Basis in paper: [inferred] The paper focuses on binary classification tasks and mentions that "it remains to demonstrate that our approach can scale well to more categories."
- Why unresolved: The paper does not present any experiments or results for multi-class classification tasks.
- What evidence would resolve it: Applying the method to a multi-class classification dataset and comparing its performance to baseline models.

### Open Question 3
- Question: How does the performance of the NLLFG model compare to other methods for generating weak labels from LLM outputs?
- Basis in paper: [explicit] The paper compares the NLLFG to a Chain-of-Thought (CoT) approach for generating weak labels, but does not compare it to other methods like direct prompting or few-shot learning.
- Why unresolved: The paper does not explore alternative methods for leveraging LLM outputs for weak supervision.
- What evidence would resolve it: Comparing the NLLFG to other weak supervision techniques on the same datasets and tasks.

## Limitations

- The LLM-generated subquestions show significant variability in quality, with FMR scores ranging from 26.09% to 57.33% across tasks, indicating inconsistent reliability in the core decomposition mechanism.
- No direct evidence demonstrates that BERT's NLI-style training actually enables true zero-shot generalization to unseen questions beyond those similar to training examples.
- The feature selection process via genetic algorithm lacks transparency about which subquestions are retained, making it difficult to assess whether the interpretable claims hold across all models.

## Confidence

- **Medium confidence** in the overall method's effectiveness, as the paper demonstrates improved F1-scores over baselines but relies heavily on LLM-generated weak supervision without extensive error analysis.
- **Low confidence** in the zero-shot capabilities claim, as the paper provides no experiments showing performance on truly novel subquestions not present in the training distribution.
- **Medium confidence** in interpretability benefits, as decision trees are inherently interpretable but the NLLF features themselves lack human-interpretable mapping between feature values and semantic meaning.

## Next Checks

1. Manually audit 50 randomly selected LLM-generated subquestions for both datasets to quantify the proportion that are truly verifiable and task-relevant, testing the reliability of Mechanism 1.
2. Design an experiment where NLLFG is tested on subquestions that were deliberately excluded from the training set to empirically validate zero-shot generalization claims (Mechanism 2).
3. Compare decision tree performance using only the top 5 most important NLLF features versus the full feature set to determine if the interpretable model is actually leveraging the NLLF representation as claimed (Mechanism 3).