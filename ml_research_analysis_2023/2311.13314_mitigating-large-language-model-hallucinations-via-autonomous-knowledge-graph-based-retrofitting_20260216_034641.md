---
ver: rpa2
title: Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based
  Retrofitting
arxiv_id: '2311.13314'
source_url: https://arxiv.org/abs/2311.13314
tags:
- knowledge
- llms
- factual
- chopin
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework that uses large language models
  (LLMs) to automatically mitigate hallucinations during reasoning by retrofitting
  responses with knowledge from knowledge graphs (KGs). The key idea is to use the
  LLM to extract, verify, and refine factual statements in the draft responses using
  the factual knowledge in KGs, without any manual effort.
---

# Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting

## Quick Facts
- arXiv ID: 2311.13314
- Source URL: https://arxiv.org/abs/2311.13314
- Reference count: 9
- Primary result: LLM-based framework that autonomously retrofits draft responses using knowledge graphs significantly improves factual QA performance, especially on complex reasoning tasks.

## Executive Summary
This paper addresses the challenge of factual hallucinations in large language models (LLMs) by proposing a framework that automatically retrofits draft responses using knowledge from knowledge graphs (KGs). The core innovation is leveraging LLMs to autonomously extract, verify, and refine factual statements in generated responses without manual intervention. By iteratively checking claims against structured KG triples, the method significantly improves factual accuracy on benchmark QA tasks while demonstrating strong generalization and robustness in open-domain settings.

## Method Summary
The KGR framework uses LLMs to perform autonomous knowledge verification and retrofitting on draft responses. It follows an iterative cycle: extracting factual claims from responses, detecting relevant entities, retrieving corresponding triples from a knowledge graph (Wikidata), selecting critical triples for verification, and retrofitting the response to align with KG facts. This process repeats until no factual errors remain, leveraging few-shot prompting to enable the LLM to act as an autonomous agent across multiple verification tasks.

## Key Results
- Significant performance improvement on factual QA benchmarks, especially for complex reasoning tasks
- Strong generalization ability across different datasets
- Robustness demonstrated in open-domain QA settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM autonomously extracts, selects, validates, and retrofits factual statements without manual effort.
- Mechanism: The LLM iteratively performs claim extraction from draft responses, detects entities, retrieves relevant triples from the KG, selects critical triples, verifies claims against triples, and retrofits the response.
- Core assumption: LLMs have sufficient in-context learning ability to decompose reasoning into verifiable claims and perform multi-step verification.
- Evidence anchors:
  - [abstract] "KGR leverages LLMs to extract, select, validate, and retrofit factual statements within the model-generated responses, which enables an autonomous knowledge verifying and refining procedure without any additional manual efforts."
  - [section] "KGR presents a LLMs-based framework to autonomously extract, validate and refine factual statements within the initial draft responses without any manual efforts."
- Break condition: If LLM fails to correctly extract claims or select relevant triples, retrofitting becomes ineffective.

### Mechanism 2
- Claim: Knowledge graph retrofitting is more effective than retrieval-based hallucination mitigation because it provides reliable factual knowledge.
- Mechanism: The framework uses structured KG triples instead of unstructured retrieved text to verify and correct LLM-generated claims, reducing noise and hallucination risk.
- Core assumption: KG triples are more precise and reliable than retrieved web text, leading to higher factual accuracy.
- Evidence anchors:
  - [abstract] "Incorporating factual knowledge in knowledge graph is regarded as a promising approach for mitigating the hallucination of large language models (LLMs)."
  - [section] "The core idea behind KGR is to autonomously retrofit the initial draft responses of LLMs based on the factual knowledge stored in KGs."
- Break condition: If the KG is incomplete or contains errors, the retrofitting may propagate incorrect facts.

### Mechanism 3
- Claim: Iterative retrofitting ensures all facts in generated answers align with the knowledge graph.
- Mechanism: The process of extraction → detection → selection → verification → retrofitting is repeated until no more factual errors are found.
- Core assumption: Repeating the cycle will catch errors missed in earlier passes and progressively refine the response.
- Evidence anchors:
  - [section] "By following the cycle of 'Extraction - Detection - Selection - Verification - Retrofitting', our KGR framework can be iterated multiple times to ensure all facts in the generated answers align with the factual knowledge stored within the knowledge graph."
  - [section] "In this case, the model-generated response shows a factual error in the initial reasoning step... After retrofitting this mistake, we encounter another factual error... So, we need to retrofit it again based on the retrofitted response in the first iteration."
- Break condition: Excessive iterations may not improve accuracy and could waste computational resources.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: LLMs often fail on complex reasoning tasks because they don't break down problems into verifiable steps. KGR builds on CoT by adding fact verification.
  - Quick check question: How does breaking a response into atomic claims help in detecting hallucinations?

- Concept: Knowledge graph querying and subgraph retrieval
  - Why needed here: Entity detection and KG retrieval are critical for obtaining relevant factual triples to verify claims.
  - Quick check question: Why is retrieving the local subgraph around detected entities more effective than fetching all triples for the entity?

- Concept: In-context learning and few-shot prompting
  - Why needed here: KGR uses LLMs to perform multiple tasks (extraction, detection, selection, verification, retrofitting) via few-shot prompts, without training.
  - Quick check question: How does few-shot prompting enable LLMs to act as autonomous agents in the retrofitting process?

## Architecture Onboarding

- Component map: Query + Draft response → Claim Extraction → Entity Detection → Fact Selection → Claim Verification → Response Retrofitting → Retrofitted response
- Critical path: Claim Extraction → Entity Detection → Fact Selection → Claim Verification → Response Retrofitting
- Design tradeoffs:
  - Chunking retrieved triples vs. full retrieval: Improves precision but may miss some facts.
  - Iterative vs. single-pass retrofitting: Higher accuracy but more computation.
  - KG vs. IR-based correction: More reliable but limited to KG coverage.
- Failure signatures:
  - Entity detection misses critical entities → Fact selection retrieves irrelevant triples.
  - Fact selection returns noisy triples → Claim verification fails or gives incorrect suggestions.
  - Claim verification misinterprets KG evidence → Response retrofitting introduces errors.
- First 3 experiments:
  1. Test claim extraction accuracy on a sample of draft responses.
  2. Measure entity detection precision/recall vs. baselines (Wikifier, SpaCy).
  3. Evaluate fact selection chunk size impact on precision and recall.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the effectiveness of entity detection for claim verification in the KGR framework?
- Basis in paper: [inferred] The paper mentions that entity detection is responsible for identifying entities that make sense in claim verification and compares the proposed LLM-based method with Wikifier and SpaCy, showing that the LLM-based method delivers superior performance. However, it also states that the effectiveness of LLMs in extracting entities for claim verification remains insufficient.
- Why unresolved: The paper does not provide a detailed analysis or solution for improving entity detection effectiveness beyond comparing it with other methods.
- What evidence would resolve it: An analysis of the current limitations of entity detection in the KGR framework and a proposed solution or improvement for better entity extraction and claim verification.

### Open Question 2
- Question: How can we optimize the tradeoff between precision and recall in fact selection for the KGR framework?
- Basis in paper: [explicit] The paper discusses the impact of chunk size and the number of retrieved triples on the efficacy of fact selection. It shows that reducing chunk size leads to lower precision and higher recall scores, and increasing the number of retrieved triples has a gradual positive impact on recall but significantly reduces precision.
- Why unresolved: The paper does not provide a clear solution or strategy for optimizing the tradeoff between precision and recall in fact selection.
- What evidence would resolve it: An experiment or analysis that demonstrates an optimal chunk size and number of retrieved triples for achieving a balance between precision and recall in fact selection.

### Open Question 3
- Question: How can we improve the reliability of the retrofitting process in the KGR framework?
- Basis in paper: [inferred] The paper mentions that the retrofitting process involves merging the entire KGR process into a singular prompt, allowing LLMs to leverage their in-context learning ability for comprehension and factual retrofitting. However, it does not provide a detailed analysis or solution for improving the reliability of this process.
- Why unresolved: The paper does not discuss the potential limitations or challenges in the retrofitting process, nor does it propose any improvements or strategies for enhancing its reliability.
- What evidence would resolve it: An analysis of the current limitations in the retrofitting process and a proposed solution or improvement for better factual retrofitting and reliability in the KGR framework.

## Limitations
- Performance depends on KG coverage - facts not in Wikidata cannot be corrected
- Iterative process increases computational cost and may not always improve accuracy
- Entity detection effectiveness remains insufficient for optimal claim verification

## Confidence

### Confidence Assessment
- **High confidence**: The core mechanism of using LLMs to extract and verify claims against KG triples is well-founded and technically sound.
- **Medium confidence**: The iterative retrofitting approach will consistently reduce hallucinations across different reasoning tasks.
- **Low confidence**: The framework's robustness to KG incompleteness and its generalization to open-domain QA settings.

## Next Checks

1. **Ablation study**: Remove the iterative component and measure performance degradation to quantify the contribution of multiple retrofitting passes.
2. **Error analysis**: Categorize retrofitting failures by type (entity detection, fact selection, claim verification) to identify the weakest link in the pipeline.
3. **KG coverage impact**: Test the framework on queries requiring facts outside Wikidata's scope to measure the hard limits of KG-based retrofitting.