---
ver: rpa2
title: Module-wise Adaptive Distillation for Multimodality Foundation Models
arxiv_id: '2310.04550'
source_url: https://arxiv.org/abs/2310.04550
tags:
- distillation
- optima
- reward
- arxiv
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OPTIMA, a task-specific distillation method
  for multimodality foundation models. It addresses the challenge of distilling large
  models into smaller ones while preserving performance.
---

# Module-wise Adaptive Distillation for Multimodality Foundation Models

## Quick Facts
- arXiv ID: 2310.04550
- Source URL: https://arxiv.org/abs/2310.04550
- Authors: [List of authors from paper]
- Reference count: 40
- Key outcome: OPTIMA significantly outperforms layerwise distillation and achieves notable improvements over pre-trained vision-language models of similar scale on multimodal understanding and image captioning tasks.

## Executive Summary
This paper introduces OPTIMA, a task-specific distillation method for multimodality foundation models that addresses the challenge of distilling large models into smaller ones while preserving performance. The core innovation is a module-wise adaptive approach that tracks individual module contributions to distillation performance and dynamically selects which modules to distill based on their current importance. By formulating this as a multi-armed bandit problem and developing a modified Thompson Sampling algorithm to handle non-stationary module contributions, OPTIMA achieves superior performance compared to traditional layerwise distillation methods.

## Method Summary
OPTIMA distills large pre-trained CoCa models (672M parameters) into smaller variants (54.5M or 101.8M parameters) by tracking module contributions through loss decrement ratios and selecting modules for distillation based on these dynamic rewards. The method treats each module or module combination as an arm in a multi-armed bandit framework, using exponential moving average of rewards to handle non-stationarity. Thompson Sampling with Gaussian reward distributions enables effective exploration-exploitation tradeoffs. The approach is evaluated on VQA, SNLI-VE, NLVR2, and COCO Caption tasks, demonstrating significant improvements over layerwise distillation baselines.

## Key Results
- OPTIMA achieves 5.9% accuracy improvement on VQA and 3.2% on SNLI-VE compared to layerwise distillation
- On NLVR2, OPTIMA reaches 77.2% accuracy, surpassing layerwise by 1.7% and outperforming VL-T5-Small by 1.8%
- For COCO Caption, OPTIMA obtains 109.4 CIDEr score, outperforming VL-T5-Small by 3.1 and matching BLIP-2 parameters despite having 7x fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tracking module contributions via loss decrement ratios enables dynamic selection of which module to distill at each step.
- Mechanism: The method treats each module (or combination) as an arm in a multi-armed bandit, using the ratio of loss reduction as a reward signal. By repeatedly evaluating these rewards and updating their estimates, it identifies which module currently contributes most to distillation performance.
- Core assumption: The relative contribution of different modules to distillation performance varies over training time, and these changes can be tracked through loss decrement ratios.
- Evidence anchors:
  - [abstract] "track the contributions of individual modules by recording the loss decrement after distillation each module"
  - [section 3] "evaluate the contribution of a module...observe the resulting ratio of decrement in the distillation loss"
  - [corpus] Weak evidence - the corpus contains "module-wise" approaches but lacks direct evidence about tracking contributions through loss decrements in distillation contexts
- Break condition: If module contributions are truly stationary (don't change during training), the dynamic selection would add unnecessary complexity without benefit.

### Mechanism 2
- Claim: Exponential moving average of rewards enables tracking non-stationary module contributions.
- Mechanism: Rather than averaging rewards over all history, the method uses exponentially weighted moving average to give more weight to recent observations. This allows the reward distribution to adapt to changing module contributions as the model updates.
- Core assumption: Module contributions change gradually during training, making recent history more relevant than distant observations.
- Evidence anchors:
  - [section 3] "we tailor these stationary algorithms by replacing the simple average with the exponential moving average of the observed rewards"
  - [section 3] "By discounting old rewards, the reward distribution can track the changing contribution in recent history"
  - [corpus] Weak evidence - corpus shows module-wise approaches but doesn't provide evidence for exponential weighting specifically in distillation contexts
- Break condition: If module contributions change too rapidly or discontinuously, exponential moving average would lag too far behind to be useful.

### Mechanism 3
- Claim: Thompson Sampling with Gaussian reward distributions enables effective exploration-exploitation tradeoff.
- Mechanism: By sampling from posterior reward distributions for each arm and selecting the arm with highest sampled reward, the method balances exploration (trying different modules) and exploitation (focusing on best-performing modules).
- Core assumption: The reward distribution for each module can be reasonably approximated as Gaussian, and sampling from these distributions leads to optimal long-term reward maximization.
- Evidence anchors:
  - [section 3] "Thompson Sampling algorithm for its strong empirical and theoretical performance"
  - [section 3] "we consider each module as an arm and every P steps as one round"
  - [corpus] Weak evidence - corpus mentions "module-wise" approaches but lacks direct evidence about Thompson Sampling application in distillation
- Break condition: If the reward distributions are highly non-Gaussian or if correlations between arms are significant, Thompson Sampling may perform poorly.

## Foundational Learning

- Concept: Multi-armed bandit problem formulation
  - Why needed here: The distillation process requires balancing exploration (trying different modules) and exploitation (focusing on best modules), which maps directly to the MAB framework
  - Quick check question: How does the MAB formulation help balance trying different modules versus focusing on the currently best one?

- Concept: Non-stationary reward distributions
  - Why needed here: Module contributions change during training as the student model updates, requiring algorithms that can track these changes rather than assuming stationary rewards
  - Quick check question: Why can't we use standard stationary MAB algorithms that average rewards over all history?

- Concept: Thompson Sampling with Bayesian updating
  - Why needed here: Provides a principled way to maintain uncertainty about module contributions while making decisions that balance exploration and exploitation
  - Quick check question: What would happen if we used a greedy approach that always chose the module with highest observed reward so far?

## Architecture Onboarding

- Component map:
  - Teacher model: Large pre-trained CoCa model (672M parameters, 48 layers)
  - Student model: Smaller CoCa variant (54.5M or 101.8M parameters, 6 or 12 layers)
  - Reward tracker: Maintains Gaussian distributions for each module combination
  - Selection engine: Thompson Sampling implementation that samples from reward distributions
  - Distillation controller: Executes P-step distillation rounds for selected modules

- Critical path:
  1. Initialize reward distributions with random exploration phase
  2. For each round: sample rewards, select module combination, distill for P steps
  3. Compute loss decrement ratios as rewards
  4. Update reward distributions using exponential moving average
  5. Repeat until training complete

- Design tradeoffs:
  - Action space size (2^c - 1 combinations) vs. exploration efficiency
  - Halflife of EMA (γ parameter) vs. responsiveness to changes
  - P-step rounds vs. granularity of adaptation
  - Gaussian assumption vs. complexity of modeling dependencies

- Failure signatures:
  - All modules receive similar rewards → exploration failure or truly equal contributions
  - One module dominates throughout → potential exploitation without sufficient exploration
  - Rewards fluctuate wildly → noise in loss measurements or too small P
  - Student performance worse than layerwise → incorrect reward calculation or inappropriate hyperparameters

- First 3 experiments:
  1. Compare layerwise distillation vs. random arm selection on a simple task to establish baseline improvement
  2. Test different EMA halflives (γ values) on a single task to find optimal responsiveness
  3. Run full OPTIMA with default parameters on VQA task to verify overall effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would OPTIMA perform when applied to larger multimodality foundation models that incorporate diverse modules for multiple modalities beyond image and text?
- Basis in paper: [explicit] The authors mention that OPTIMA's efficiency and lack of additional computational costs make it suitable for exploration on larger foundational models with diverse modules, but do not provide experimental evidence.
- Why unresolved: The paper only demonstrates OPTIMA's effectiveness on CoCa models with image and text modalities. Testing on models with additional modalities like audio and video would require significant computational resources and may reveal new challenges or limitations.
- What evidence would resolve it: Experiments showing OPTIMA's performance on distillation of larger models with diverse modules for multiple modalities, comparing against baseline methods and measuring effectiveness across a range of tasks.

### Open Question 2
- Question: Would further subdividing each module into groups of layers in the action space improve OPTIMA's performance by enabling more fine-grained module selection?
- Basis in paper: [explicit] The authors suggest that different layers within a single module could exhibit variable contributions to the target task and propose subdividing modules into layer groups as a potential improvement, but do not implement or test this idea.
- Why unresolved: Implementing a fine-grained action space with layer groups would require significant modifications to the current sampling strategy and may introduce new challenges related to dependencies among layer groups. The authors acknowledge this as a future research direction.
- What evidence would resolve it: Experiments comparing OPTIMA's performance with the current module-level action space against a version with subdivided layer groups, measuring effectiveness across tasks and analyzing the impact on computational efficiency and convergence.

### Open Question 3
- Question: How would OPTIMA's performance be affected by using different reward functions or modifying the Thompson Sampling algorithm to better handle the non-stationary nature of module contributions?
- Basis in paper: [explicit] The authors perform an ablation study on the reward function design, showing that using only the KL divergence or only the layerwise representation distances has task-specific advantages. They also mention that other MAB algorithms could be generically combined with OPTIMA.
- Why unresolved: The current reward function and Thompson Sampling algorithm are designed to balance the changing contributions of modules, but there may be more effective ways to model and adapt to this non-stationarity. Exploring alternative reward functions or MAB algorithms could potentially lead to further improvements.
- What evidence would resolve it: Experiments comparing OPTIMA's performance using different reward functions or modified Thompson Sampling algorithms, measuring effectiveness across tasks and analyzing the impact on convergence speed and final performance.

## Limitations

- The method's effectiveness depends on accurate estimation of loss decrement ratios, which may be noisy in practice
- The Gaussian assumption for reward distributions and independence between arms may not hold in complex multimodal settings
- The exponential moving average parameter γ requires careful tuning, and sensitivity analysis is incomplete across different tasks

## Confidence

- Mechanism 1 (loss decrement tracking): Medium - supported by theoretical formulation but limited empirical validation of tracking accuracy
- Mechanism 2 (exponential moving average): Medium - algorithmic description is clear but sensitivity analysis for γ parameter is incomplete
- Mechanism 3 (Thompson Sampling): High - Thompson Sampling is well-established, though adaptation for non-stationary rewards needs more validation

## Next Checks

1. Perform ablation study comparing OPTIMA with different reward tracking methods (simple average vs. exponential moving average) to quantify the benefit of non-stationarity handling
2. Test the method on additional multimodal tasks with different data distributions to verify robustness of the reward estimation mechanism
3. Implement and evaluate a variant using correlated bandit algorithms to assess whether the independence assumption is justified or if it limits performance