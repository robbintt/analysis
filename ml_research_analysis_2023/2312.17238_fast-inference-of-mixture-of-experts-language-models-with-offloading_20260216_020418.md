---
ver: rpa2
title: Fast Inference of Mixture-of-Experts Language Models with Offloading
arxiv_id: '2312.17238'
source_url: https://arxiv.org/abs/2312.17238
tags:
- experts
- language
- arxiv
- quantization
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the problem of running large Mixture-of-Experts
  language models on consumer hardware with limited GPU memory, particularly for interactive
  generation. The authors observe two key patterns in MoE behavior: i) some experts
  are reused between adjacent tokens, and ii) early layer hidden states can predict
  which experts will be needed in subsequent layers.'
---

# Fast Inference of Mixture-of-Experts Language Models with Offloading

## Quick Facts
- arXiv ID: 2312.17238
- Source URL: https://arxiv.org/abs/2312.17238
- Reference count: 10
- Key outcome: Achieves 2-4 tokens per second on consumer GPUs for Mixtral-8x7B-Instruct using LRU caching and speculative expert loading with mixed quantization

## Executive Summary
This work addresses the challenge of running large Mixture-of-Experts language models on consumer hardware with limited GPU memory for interactive generation. The authors observe that MoE models exhibit locality in expert activation and that early layer hidden states can predict subsequent expert needs. They propose a novel offloading strategy combining LRU caching of experts with speculative expert loading, achieving interactive generation speeds on RTX 3060, RTX 3080 Mobile, and T4 GPUs. The method uses mixed quantization (4-bit for attention layers, 2-3-bit for experts) to reduce offload bandwidth while maintaining model quality.

## Method Summary
The paper proposes an offloading strategy for MoE inference that leverages two key observations: expert locality (some experts are reused between adjacent tokens) and predictive routing (early layer hidden states can forecast which experts will be needed). The method combines LRU caching to keep recently used experts in GPU memory, reducing host-device transfers, with speculative loading that prefetches likely next experts using current layer activations. Mixed quantization is applied with more aggressive compression for experts (2-3-bit) versus shared attention layers (4-bit). The approach is evaluated on Mixtral-8x7B-Instruct, achieving 2-4 tokens per second on various consumer GPUs.

## Key Results
- Achieves 2-4 tokens per second on RTX 3060, RTX 3080 Mobile, and T4 GPUs
- LRU caching hit rates increase with cache size (k=1,2,4 tested)
- Speculative loading recall improves when preloading 1-2 experts ahead
- Mixed quantization (4-bit attention, 2-3-bit experts) maintains model quality

## Why This Works (Mechanism)

### Mechanism 1
LRU caching of experts significantly reduces GPU-RAM communication by keeping the k least recently used experts in GPU memory, serving repeated activations from cache rather than requiring full host-to-device transfers. This works because MoE models exhibit locality of expert activation where some experts are reused between adjacent tokens. The core assumption is that expert activation follows a predictable pattern across tokens. Evidence shows LRU cache hit rate increases with cache size. Break condition: If expert activation becomes truly random with no locality, cache hit rates drop to zero.

### Mechanism 2
Speculative expert loading overlaps expert fetching with computation, reducing idle GPU time by applying the next layer's gating function to the current layer's hidden states to predict which experts will be needed. This works because early layer hidden states contain enough information to predict expert needs in subsequent layers due to the residual nature of transformers. The core assumption is that expert routing decisions can be predicted from early layer features. Evidence shows speculative loading recall increases when preloading 1-2 experts ahead. Break condition: If expert routing decisions are highly dynamic or dependent on very deep layer features, early-layer predictions become inaccurate.

### Mechanism 3
Mixed quantization achieves favorable quality-size tradeoff by applying different bitwidths to experts (2-3-bit) versus attention layers (4-bit), reducing offload bandwidth while maintaining model quality. This works because MoE models tolerate lower precision for experts better than for shared layers. The core assumption is that experts can be compressed more aggressively without significant quality degradation. Evidence shows experts can be 2-3 bits while attention layers remain at 4 bits. Break condition: If quantization noise in experts significantly degrades model quality, mixed quantization no longer provides benefit.

## Foundational Learning

- **Mixture-of-Experts (MoE) architecture**: Understanding how MoE models route tokens to experts is fundamental to designing efficient offloading strategies. Quick check: In a typical MoE layer, what fraction of experts are active for any given token?

- **Quantization and its impact on model quality**: The paper uses mixed quantization to compress model parameters for efficient offloading, requiring understanding of bitwidth tradeoffs. Quick check: What is the typical optimal bitwidth for quantizing transformer models according to the literature?

- **GPU memory hierarchy and PCIe bandwidth constraints**: The proposed offloading strategy is fundamentally about managing data movement between GPU memory and system RAM, which is constrained by PCIe bandwidth. Quick check: What is the typical host-to-device bandwidth for PCIe Gen.3, which is mentioned as the target hardware constraint?

## Architecture Onboarding

- **Component map**: Model -> Embedding -> (Attention + MoE MLP) × layers -> Logits. The MoE MLP step is where expert selection and offloading occurs.
- **Critical path**: Token generation → embedding → (attention + MoE MLP) × layers → logits
- **Design tradeoffs**:
  - Cache size (k): Larger k reduces offload frequency but increases GPU memory pressure
  - Speculative depth: Predicting 1-2 layers ahead balances accuracy with computation overhead
  - Quantization bitwidth: More aggressive quantization reduces bandwidth but may hurt quality
  - Expert pinning: Keeping some experts always resident vs full LRU strategy
- **Failure signatures**:
  - Low cache hit rates (< 30%) indicate poor expert locality, suggesting LRU cache is ineffective
  - Speculative loading recall below 50% means prefetching adds overhead without benefit
  - High PCIe utilization (> 90%) with low GPU utilization indicates bandwidth bottleneck
  - Significant perplexity degradation (> 10% relative) suggests quantization is too aggressive
- **First 3 experiments**:
  1. Measure LRU cache hit rate on Mixtral-8x7B with k=1,2,4 to establish expert locality baseline
  2. Evaluate speculative loading recall when predicting 1, 2, and 3 layers ahead on the same dataset
  3. Benchmark token generation speed with and without LRU caching and speculative loading on target hardware

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but leaves several areas unexplored, including the generalizability of the approach to different MoE architectures, optimal cache sizing across various GPU configurations, and the impact of different speculative loading algorithms.

## Limitations

- The effectiveness of LRU caching and speculative loading depends heavily on specific locality patterns in Mixtral-8x7B-Instruct routing, which may not generalize to other MoE architectures.
- The paper lacks ablation studies showing the impact of different quantization configurations on both speed and accuracy.
- The speculative loading mechanism's reliance on early-layer hidden states may break down for models with more complex routing patterns or deeper architectures.

## Confidence

- **High Confidence**: The core observation that some experts are reused between adjacent tokens (supported by LRU cache hit rate measurements). The basic premise that MoE models can benefit from expert offloading due to their parameter-heavy nature (96.6% of parameters are experts).
- **Medium Confidence**: The effectiveness of the LRU caching strategy for different cache sizes. The claim that speculative loading reduces idle GPU time by overlapping computation with data fetching. The mixed quantization scheme achieving favorable quality-size tradeoff based on perplexity experiments.
- **Low Confidence**: The generalizability of the 2-4 tokens per second speed claim across different consumer GPUs and model variants. The long-term stability of the approach for extended generation sessions. The scalability of the method to larger MoE models beyond Mixtral-8x7B-Instruct.

## Next Checks

1. **Expert Locality Validation**: Measure LRU cache hit rates across multiple MoE architectures (not just Mixtral) to determine if the observed locality patterns are universal or model-specific.

2. **Speculative Loading Accuracy**: Implement A/B testing comparing speculative loading with random prefetching and no prefetching on the same hardware. Measure the actual reduction in GPU idle time and total inference latency.

3. **Quantization Quality Tradeoff**: Conduct comprehensive ablation studies varying quantization bitwidths for both experts and attention layers independently. Measure not just perplexity but also task-specific accuracy on benchmarks like MMLU.