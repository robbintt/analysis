---
ver: rpa2
title: Probabilistic Dataset Reconstruction from Interpretable Models
arxiv_id: '2308.15099'
source_url: https://arxiv.org/abs/2308.15099
tags:
- rule
- dataset
- probabilistic
- decision
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the privacy risk of releasing interpretable
  machine learning models, which inherently leak information about their training
  data. The authors propose a framework to generalize probabilistic dataset reconstruction
  attacks to handle various interpretable models and types of knowledge.
---

# Probabilistic Dataset Reconstruction from Interpretable Models

## Quick Facts
- arXiv ID: 2308.15099
- Source URL: https://arxiv.org/abs/2308.15099
- Reference count: 40
- Primary result: Optimal interpretable models leak less information than heuristically built ones for a given accuracy level.

## Executive Summary
This paper addresses the privacy risks of releasing interpretable machine learning models by proposing a framework for probabilistic dataset reconstruction attacks. The authors demonstrate that interpretable models inherently leak information about their training data through their structure, and they develop methods to quantify this information leakage using entropy-based uncertainty metrics. Through experiments comparing optimal and heuristic decision trees and rule lists, they show that optimal models are typically more compact and leak less information for equivalent predictive performance. The framework generalizes to various interpretable models and types of knowledge, providing a systematic way to assess privacy risks in interpretable ML systems.

## Method Summary
The authors propose a framework that builds probabilistic reconstructions of training datasets from interpretable models by modeling each attribute as a random variable with a probability distribution. They quantify reconstruction success using entropy reduction metrics, specifically the DistG metric comparing generalized probabilistic datasets against original deterministic datasets. The method involves training interpretable models using both optimal algorithms (DL8.5 for decision trees, CORELS for rule lists) and heuristic approaches (sklearn's CART-based implementation, GreedyRL for rule lists), then computing uncertainty measures through possible worlds counting rather than explicit enumeration. Experiments are conducted on Adult Income and COMPAS datasets, comparing optimal versus heuristic learning algorithms under various size constraints.

## Key Results
- Optimal interpretable models are typically more compact and leak less information than heuristically built models for equivalent accuracy levels
- Information leakage is not uniform across training examples, with some samples being more exposed than others
- Under realistic assumptions about model structure, uncertainty in reconstruction can be computed efficiently using possible worlds counting
- The framework successfully generalizes probabilistic reconstruction attacks to handle various interpretable model types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The structure of interpretable models inherently leaks information about their training data.
- Mechanism: The model's structure encodes decisions made during training, such as which features were used and how they were split. This information can be reverse-engineered to reconstruct the training dataset.
- Core assumption: The interpretable model is released with its full structure intact.
- Evidence anchors:
  - [abstract]: "learning and releasing models that are inherently interpretable leaks information regarding the underlying training data"
  - [section I]: "A trained interpretable machine learning model, such as the decision tree presented in Figure 1, inherently encodes information regarding its training set."
- Break condition: If the model structure is obfuscated or only the predictions are released without the model internals.

### Mechanism 2
- Claim: Probabilistic dataset reconstruction quantifies the uncertainty in the reconstructed data.
- Mechanism: By modeling each attribute as a random variable with a probability distribution, the reconstruction can be represented as a probabilistic dataset. The uncertainty is measured using Shannon entropy, which quantifies the amount of information missing.
- Core assumption: The random variables modeling the dataset attributes can be described by probability distributions.
- Evidence anchors:
  - [abstract]: "with the uncertainty of the reconstruction being a relevant metric for the information leak"
  - [section II.B]: "The success of the reconstruction is quantified as the average uncertainty reduction over all attributes of all examples in the dataset"
- Break condition: If the attributes are not independent or if the probability distributions are not known or estimable.

### Mechanism 3
- Claim: Optimal interpretable models leak less information than heuristically built ones for a given accuracy level.
- Mechanism: Optimal models make global decisions that minimize redundancy and unnecessary complexity, encoding only the information necessary for accurate predictions. Heuristic models may make local suboptimal choices that introduce unnecessary information leakage.
- Core assumption: Optimal learning algorithms produce models that are more compact and efficient than heuristic ones.
- Evidence anchors:
  - [abstract]: "Our results suggest that optimal interpretable models are often more compact and leak less information regarding their training data than greedily-built ones, for a given accuracy level."
  - [section V.B]: "optimal models usually represent more information in a more compact way: the reconstruction uncertainty decreases faster for optimal models than with greedily-built ones"
- Break condition: If the learning algorithms do not differ significantly in their approach or if the models are too simple for optimality to matter.

## Foundational Learning

- Concept: Probabilistic Datasets
  - Why needed here: They provide a way to represent the uncertainty in the reconstructed training data, which is essential for quantifying the information leak.
  - Quick check question: How does a probabilistic dataset differ from a deterministic dataset?

- Concept: Shannon Entropy
  - Why needed here: It is used to measure the uncertainty in the reconstructed data, providing a quantitative metric for the information leak.
  - Quick check question: What does a lower entropy value indicate about the uncertainty of a random variable?

- Concept: Decision Trees and Rule Lists
  - Why needed here: These are examples of interpretable models whose structures can be analyzed to reconstruct training data.
  - Quick check question: How does the structure of a decision tree encode information about the training data?

## Architecture Onboarding

- Component map: Data Preparation -> Model Learning -> Probabilistic Reconstruction -> Uncertainty Quantification -> Comparison and Analysis
- Critical path:
  1. Load dataset and split into training and test sets.
  2. Train interpretable models using both optimal and heuristic algorithms.
  3. Construct probabilistic datasets from the trained models.
  4. Compute uncertainty metrics (entropy) for each probabilistic dataset.
  5. Analyze and compare the results.
- Design tradeoffs:
  - Optimal vs. Heuristic Learning: Optimal algorithms are more computationally expensive but may produce more efficient models.
  - Model Complexity: More complex models may leak more information but also provide better accuracy.
  - Privacy vs. Interpretability: Releasing interpretable models improves transparency but increases privacy risks.
- Failure signatures:
  - High entropy in reconstructed datasets indicates poor reconstruction or excessive uncertainty.
  - Discrepancies between expected and actual information leak suggest issues in the reconstruction process.
  - Computational failures in optimal learning algorithms due to resource constraints.
- First 3 experiments:
  1. Train a simple decision tree on a small dataset and manually verify the probabilistic reconstruction process.
  2. Compare entropy values for optimal and heuristic models on a benchmark dataset.
  3. Test the effect of model size constraints on the information leak.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the disparate information leak observed in experiments disproportionately affect specific subgroups of the population?
- Basis in paper: [explicit] The paper mentions investigating whether the entropy reduction is not uniform across all examples and suggests exploring if it disproportionately affects subgroups.
- Why unresolved: The paper only shows that the entropy reduction is not uniform across all training examples but does not analyze whether this affects specific subgroups.
- What evidence would resolve it: Experiments analyzing the distribution of entropy reduction across different demographic or sensitive subgroups of the population.

### Open Question 2
- Question: How does differential privacy affect the quality of probabilistic dataset reconstructions from interpretable models?
- Basis in paper: [explicit] The paper mentions investigating the effect of privacy-preserving methods like differential privacy on the quality of built probabilistic datasets.
- Why unresolved: The paper does not provide any experiments or analysis on how differential privacy would impact the reconstruction quality.
- What evidence would resolve it: Experiments training interpretable models with differential privacy guarantees and measuring the resulting entropy reduction in reconstructed datasets.

### Open Question 3
- Question: Can the knowledge from multiple generalized probabilistic datasets be combined to further reduce uncertainty in reconstruction?
- Basis in paper: [explicit] The paper mentions combining knowledge from different generalized probabilistic datasets as a promising future work direction.
- Why unresolved: The paper does not explore how to align or merge multiple probability distributions from different datasets.
- What evidence would resolve it: Methods for aligning and combining multiple generalized probabilistic datasets, along with experiments showing reduced uncertainty in the combined reconstruction.

## Limitations
- The framework focuses on binary features, limiting applicability to datasets with continuous or multi-valued categorical variables.
- The independence assumption between attributes when computing probabilistic reconstructions may not hold in real datasets with correlated features.
- Computational infeasibility may occur when learning optimal models with large datasets due to memory and time constraints.

## Confidence
- High Confidence: The core finding that interpretable model structures leak information about training data is well-established and consistently demonstrated across experiments.
- Medium Confidence: The claim that optimal models leak less information than heuristic ones for equivalent accuracy requires further validation on diverse datasets and model types.
- Low Confidence: The generalizability of the probabilistic reconstruction framework to complex, high-dimensional datasets with non-binary features remains unproven.

## Next Checks
1. Test the framework on datasets with continuous and multi-valued categorical features to assess scalability and performance.
2. Evaluate the impact of feature correlations on reconstruction accuracy and uncertainty measurements.
3. Conduct a comprehensive comparison across multiple model types (e.g., linear models, neural networks) to determine if the observed differences between optimal and heuristic models generalize beyond decision trees and rule lists.