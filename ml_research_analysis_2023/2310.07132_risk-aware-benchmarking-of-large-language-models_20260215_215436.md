---
ver: rpa2
title: Risk Aware Benchmarking of Large Language Models
arxiv_id: '2310.07132'
source_url: https://arxiv.org/abs/2310.07132
tags:
- stochastic
- dominance
- relative
- order
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a distributional framework for assessing socio-technical
  risks of foundation models using stochastic dominance tests with quantified statistical
  significance. The core method idea is to define a metrics portfolio for each model
  as a weighted geometric mean of the CDFs of individual metrics, and perform model
  selection based on first and second order stochastic dominance of these portfolios.
---

# Risk Aware Benchmarking of Large Language Models

## Quick Facts
- arXiv ID: 2310.07132
- Source URL: https://arxiv.org/abs/2310.07132
- Authors: [not specified in input]
- Reference count: 40
- One-line primary result: Proposes distributional risk assessment framework using stochastic dominance tests for LLM evaluation

## Executive Summary
This paper introduces a distributional framework for assessing socio-technical risks of foundation models through stochastic dominance tests with quantified statistical significance. The approach aggregates multiple evaluation metrics into a portfolio using weighted geometric means of CDFs, enabling risk-aware model selection that considers tail behavior beyond simple mean performance. The framework is demonstrated on instruction-following and toxicity evaluation tasks, showing that relative second-order stochastic dominance testing can produce different model rankings than traditional mean-based leaderboards.

## Method Summary
The framework defines a metrics portfolio for each model as a weighted geometric mean of CDFs across evaluation metrics, then performs model selection based on first and second order stochastic dominance of these portfolios. Statistical significance is established through bootstrap variance estimation and asymptotic normality analysis. The method is applied to compare large language models on the Mix-Instruct dataset (100K training samples, 5K test samples) using 8 automatic metrics, and on toxicity evaluation with 20K prompts (10K toxic, 10K non-toxic) using 6 toxicity metrics. Model rankings are computed using Relative FSD/R-SSD tests with significance level α = 0.05 and compared against the Mean Win Rate baseline.

## Key Results
- R-SSD test ranks models differently than Mean Win Rate on Mix-Instruct dataset, highlighting importance of tail risk consideration
- R-FSD and R-SSD tests generally agree on toxicity evaluation, but results are challenging to interpret due to data conditioning
- Framework demonstrates value of distributional risk assessment for foundation models using multi-metric evaluations with statistical significance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative stochastic dominance testing enables statistically significant model ranking by aggregating pairwise dominance tests into global ordering
- Mechanism: For each model pair, compute one-versus-all violation ratio ε(ℓ)i(F) and compare differences ∆ε(ℓ)ij(F). Reject null hypothesis with confidence 1-α if normalized difference falls below Φ⁻¹(α). Aggregate pairwise wins via Borda count to produce global ranking
- Core assumption: Asymptotic normality of ∆ε(ℓ)ij(Fn) holds as sample size grows, allowing bootstrap variance estimates to approximate true sampling distributions
- Evidence anchors: [abstract] statistical significance backed by asymptotic analysis; [section 3] central limit theorem guarantees asymptotic Gaussian behavior; [corpus] weak - no directly comparable statistical frameworks found
- Break condition: Small sample sizes prevent asymptotic normality, making bootstrap variance estimates unreliable and increasing false dominance claims

### Mechanism 2
- Claim: Metrics portfolio aggregation via weighted geometric mean of CDFs preserves dominance relationships while enabling multi-metric evaluation
- Mechanism: For each model A and metric set {mi}, compute portfolio RA(X) = exp(Σλi log FMi(mi(A(X)))). This CDF normalization removes scale differences while maintaining order relationships
- Core assumption: Geometric mean of CDFs forms valid Archimedean copula that preserves stochastic dominance ordering of individual metrics
- Evidence anchors: [section 4] portfolio defined as Archimedean copula forming weighted geometric mean of CDFs; [abstract] inspired by portfolio optimization in finance; [corpus] moderate - related financial portfolio optimization methods exist
- Break condition: Highly correlated metrics or one dominant metric cause aggregation to lose discriminative power, making portfolio uninformative

### Mechanism 3
- Claim: Second-order stochastic dominance testing captures risk-averse preferences better than first-order dominance or mean-variance models
- Mechanism: SSD testing compares integrated quantiles F(-2) to assess not just mean performance but also tail risk
- Core assumption: Users have increasing, concave utility functions making them risk-averse, so SSD-consistent mean-risk models better reflect their preferences
- Evidence anchors: [section 2.1] SSD implies preference by any risk-averse agent preferring larger outcomes; [table 1] lists specific SSD-consistent risk measures; [corpus] strong - multiple papers discuss risk-return tradeoffs
- Break condition: Risk-neutral or differently structured preferences make SSD testing overemphasize tail behavior at expense of overall performance

## Foundational Learning

- Concept: First and second order stochastic dominance definitions
  - Why needed here: Mathematical foundation for all dominance testing procedures
  - Quick check question: Given two distributions X and Y where X has higher mean but heavier left tail than Y, which would FSD prefer and which would SSD prefer?

- Concept: Bootstrap variance estimation and central limit theorems
  - Why needed here: Statistical significance testing relies on asymptotic normality guarantees and practical variance estimation
  - Quick check question: If bootstrap variance estimates are consistently higher than true variance, what happens to false positive rate of dominance tests?

- Concept: Portfolio optimization via Archimedean copulas
  - Why needed here: Metrics portfolio aggregation is central to multi-metric evaluation
  - Quick check question: What happens to portfolio distribution if one metric has near-zero weight but extremely high variance?

## Architecture Onboarding

- Component map: Data pipeline -> Statistical engine -> Aggregation layer -> Evaluation interface
- Critical path: 1) Load and preprocess evaluation data 2) Compute metric CDFs across all models 3) Calculate violation ratios and bootstrap variances 4) Perform dominance tests and aggregate rankings 5) Generate risk-aware model comparisons
- Design tradeoffs: Bootstrap iterations (1000 default) vs computation time; Portfolio weights (equal vs task-specific) vs interpretability; First-order vs second-order dominance vs computational complexity; Pairwise testing with Bonferroni correction vs global testing approaches
- Failure signatures: All models ranked equally (bootstrap variance estimation failure or insufficient sample size); Inconsistent rankings across metrics (metric distributions too dissimilar or one metric dominates); Extreme sensitivity to ε threshold (violation ratios poorly estimated or distributions too similar); Rankings change drastically with small sample perturbations (asymptotic assumptions violated)
- First 3 experiments: 1) Synthetic Gaussian distributions with controlled overlap to verify test calibration 2) Single-metric case to verify reduction to standard dominance tests 3) Known ground truth ranking to verify portfolio aggregation preserves ordering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended to handle multi-modal distributions or other complex distribution shapes that may arise in real-world LLM evaluations?
- Basis in paper: [inferred] Paper assumes continuous distributions with well-defined CDFs and second quantile functions
- Why unresolved: Paper focuses on univariate continuous distributions without exploring behavior with multi-modal or complex distributions
- What evidence would resolve it: Experimental results showing performance on metrics with multi-modal distributions and modifications needed to handle such cases

### Open Question 2
- Question: How sensitive is the model ranking to choice of weighting scheme for the metrics portfolio?
- Basis in paper: [explicit] Paper uses geometric mean of CDFs but doesn't explore sensitivity to different weighting schemes
- Why unresolved: Paper uses fixed geometric mean weighting without exploring how rankings change with different schemes
- What evidence would resolve it: Experimental results comparing rankings using different weighting schemes (arithmetic mean, exponential weighting) and sensitivity analysis

### Open Question 3
- Question: How can the framework be adapted to handle non-continuous metrics or metrics with ties?
- Basis in paper: [inferred] Paper assumes continuous metrics and doesn't discuss handling discrete metrics or ties
- Why unresolved: Paper focuses on continuous metrics without addressing challenges of handling discrete metrics or ties
- What evidence would resolve it: Modifications to handle discrete metrics or ties along with experimental results demonstrating effectiveness

## Limitations
- Framework's reliance on asymptotic normality through bootstrap variance estimation creates systematic risk of false positive dominance claims with finite samples
- Toxicity evaluation faces fundamental methodological challenges due to conditioning on toxicity scores, limiting external validity
- Metrics portfolio aggregation assumes metric independence and equal relevance, which may not hold across diverse evaluation tasks

## Confidence
- **High confidence**: Theoretical foundation of stochastic dominance testing and its application to risk-aware model selection
- **Medium confidence**: Practical implementation of bootstrap variance estimation and its ability to accurately capture sampling distributions in finite samples
- **Low confidence**: External validity of toxicity evaluation results due to data conditioning, and generalizability of portfolio aggregation weights across contexts

## Next Checks
1. **Calibration Testing**: Generate synthetic metric distributions with known overlap parameters and verify that R-SSD test maintains nominal 5% false positive rate across different sample sizes to empirically validate bootstrap variance estimation
2. **Robustness Analysis**: Systematically vary portfolio weights λi and evaluate sensitivity of model rankings to weight perturbations to identify which metrics disproportionately influence rankings
3. **Cross-Evaluation Consistency**: Apply framework to held-out toxicity evaluation set with randomly sampled prompts (not conditioned on toxicity scores) and compare results with original conditioned evaluation to assess generalizability of risk assessments