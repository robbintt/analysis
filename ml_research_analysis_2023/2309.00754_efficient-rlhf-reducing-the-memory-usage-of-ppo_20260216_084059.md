---
ver: rpa2
title: 'Efficient RLHF: Reducing the Memory Usage of PPO'
arxiv_id: '2309.00754'
source_url: https://arxiv.org/abs/2309.00754
tags:
- reward
- lora-ppo
- hydra-ppo
- learning
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the high memory demands of RLHF training, specifically
  for PPO, which can exceed 3x the memory of SFT, limiting its practical use. The
  authors propose Hydra-RLHF, a method that reduces memory by integrating the SFT
  and reward models into a single model and using dynamic LoRA techniques to "turn
  off" LoRA weights when needed.
---

# Efficient RLHF: Reducing the Memory Usage of PPO

## Quick Facts
- arXiv ID: 2309.00754
- Source URL: https://arxiv.org/abs/2309.00754
- Reference count: 40
- One-line primary result: Reduces PPO memory usage below SFT levels while improving alignment and cutting latency by up to 65%

## Executive Summary
This paper addresses the high memory demands of RLHF training, particularly for PPO, which can require over 3x the memory of supervised fine-tuning (SFT). The authors propose Hydra-RLHF, a method that integrates SFT and reward modeling into a single model with dynamic LoRA techniques to "turn off" LoRA weights when needed. This approach maintains or improves alignment performance while significantly reducing memory usage and latency per sample. Results demonstrate that Hydra-PPO outperforms LoRA-PPO in alignment on Llama 7b across four datasets, while both methods improve over their respective base models.

## Method Summary
Hydra-RLHF reduces memory usage by combining the SFT and reward models into a single Hydra-SFT model with two linear heads, and using dynamic LoRA to "turn off" LoRA weights during training. This allows the reference and reward models to be derived from the actor model, reducing the number of models needed in memory. The method is evaluated on Llama 7b and OPT 1.3b using four datasets with pairwise preference data, measuring alignment through GPT-4-based win-rate evaluation and ROUGE scores for summarization.

## Key Results
- Hydra-PPO reduces memory usage below SFT levels while improving alignment across four public benchmarks
- Hydra-PPO reduces latency per sample by up to 65% compared to LoRA-PPO while maintaining performance
- Hydra-PPO outperforms LoRA-PPO in alignment on Llama 7b, with both methods improving over their respective base models

## Why This Works (Mechanism)

### Mechanism 1
Combining SFT and reward model training into a single model (Hydra-SFT) reduces memory by eliminating the need for separate models. The Hydra-SFT model has two linear heads - one for causal language modeling and one for reward modeling. Core assumption: The tasks can be learned effectively in a single model without significant interference. Break condition: If the two tasks interfere significantly, causing either task to suffer.

### Mechanism 2
Dynamically turning off LoRA weights during training reduces memory usage by allowing the reference model to be recovered from the actor model. Core assumption: LoRA weights are the only difference between actor and reference models. Break condition: If LoRA weights aren't the only difference, or turning them off doesn't accurately recover the reference model.

### Mechanism 3
Using Hydra-PPO instead of LoRA-PPO improves alignment performance due to a better reward model from Hydra-SFT and reduced latency from higher batch sizes. Core assumption: A better reward model leads to better PPO performance, and reduced latency enables more efficient training. Break condition: If the reward model doesn't improve PPO performance, or reduced latency doesn't lead to more efficient training.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: RLHF is the overarching framework that Hydra-RLHF aims to make more efficient
  - Quick check question: What are the three main stages of the standard RLHF process?

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: PPO is the specific algorithm used in the RL stage of RLHF that Hydra-RLHF aims to make more memory-efficient
  - Quick check question: What is the main objective function optimized by PPO?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: LoRA is used in both LoRA-PPO and Hydra-PPO to reduce the number of parameters that need to be trained
  - Quick check question: How does LoRA achieve parameter efficiency in fine-tuning large language models?

## Architecture Onboarding

- **Component map**: Text input → Hydra-SFT model → PPO training with dynamic LoRA → Aligned output
- **Critical path**: Text input → Hydra-SFT model → PPO training with dynamic LoRA → Aligned output
- **Design tradeoffs**: Memory usage vs. model performance (trades complexity for efficiency); Reward model quality vs. training time (joint training may take longer but yields better alignment)
- **Failure signatures**: Poor alignment performance (if Hydra-SFT doesn't learn both tasks effectively); Training instability (if dynamic LoRA isn't implemented correctly)
- **First 3 experiments**:
  1. Train Hydra-SFT on a small dataset and compare its performance to separate SFT and reward models
  2. Implement dynamic LoRA in a simple PPO setup and measure the reduction in memory usage
  3. Compare the alignment performance of Hydra-PPO to LoRA-PPO on a small language model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Hydra-RLHF's performance advantage over LoRA-PPO stem from improved reward model quality or from architectural advantages of the joint SFT/RM model?
- Basis in paper: The paper notes Hydra-PPO outperforms LoRA-PPO on Llama 7b, speculating this is "due to the better Reward model from Hydra-SFT which enables overall better PPO performance."
- Why unresolved: The paper doesn't conduct ablation studies isolating the reward model quality from architectural effects.
- What evidence would resolve it: Training Hydra-SFT but using its separated reward model component for LoRA-PPO, or training separate SFT/RM models but initializing LoRA-PPO with Hydra-SFT's weights.

### Open Question 2
- Question: Can the instability of J-Hydra-PPO be mitigated through architectural modifications while preserving its memory efficiency advantages?
- Basis in paper: "Joined-Hydra-PPO Underperformance" section notes J-Hydra-PPO performs significantly worse than Hydra-PPO and speculates "combining actor and critic model amplified the unstable nature of PPO."
- Why unresolved: The paper doesn't explore architectural modifications to J-Hydra-PPO to improve stability.
- What evidence would resolve it: Experiments testing regularization techniques, different LoRA rank configurations, or alternative joint training objectives for J-Hydra-PPO.

### Open Question 3
- Question: How does Hydra-RLHF's performance scale with model size, particularly for very large language models?
- Basis in paper: The paper notes "We speculate this difference to be due to the capacity of the model" when comparing OPT-1.3b and Llama 7b results, and mentions "Hydra-RLHF comparatively saves less memory when standard RLHF uses a smaller reward model."
- Why unresolved: The paper only tests on 1.3B and 7B parameter models, not exploring the extremes of model scale.
- What evidence would resolve it: Experiments with models ranging from 100M to 100B+ parameters, measuring both performance and memory efficiency at each scale.

## Limitations

- Limited generalizability to architectures beyond Llama and OPT models tested
- Heavy reliance on GPT-4-based win-rate assessment introduces potential biases and computational costs
- J-Hydra-PPO variant shows sensitivity and instability that requires multiple attempts to find stable settings

## Confidence

- **High Confidence**: The memory reduction claims (3x reduction relative to standard PPO) and the general framework of Hydra-RLHF are well-supported by the experimental setup and results
- **Medium Confidence**: The alignment improvements over LoRA-PPO are demonstrated but the effect size may be influenced by hyperparameter choices that aren't fully explored
- **Low Confidence**: The specific 65% latency reduction claim requires precise implementation details that aren't provided, making independent verification challenging

## Next Checks

1. Implement the dynamic LoRA switching mechanism and measure memory usage across different batch sizes to verify the claimed 3x reduction compared to standard PPO
2. Replicate the alignment experiments on a different base model (e.g., Mistral or another 7B model) to test generalizability of the performance claims
3. Conduct ablation studies removing the Hydra-SFT integration to isolate the contribution of the combined model versus the dynamic LoRA mechanism