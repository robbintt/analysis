---
ver: rpa2
title: 'An explainable three dimension framework to uncover learning patterns: A unified
  look in variable sulci recognition'
arxiv_id: '2309.00903'
source_url: https://arxiv.org/abs/2309.00903
tags:
- sulcus
- left
- view
- sulcal
- hemisphere
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a 3D explainable artificial intelligence
  framework for detecting the paracingulate sulcus in structural MRI, addressing the
  challenge of accurately identifying variable sulcal features in neuroimaging. The
  method combines two deep learning architectures (simple-3D-CNN and simple-3D-MHL)
  with GradCam and SHAP explainability techniques and PCA-based dimensionality reduction
  to provide both local and global explanations of model predictions.
---

# An explainable three dimension framework to uncover learning patterns: A unified look in variable sulci recognition

## Quick Facts
- arXiv ID: 2309.00903
- Source URL: https://arxiv.org/abs/2309.00903
- Reference count: 40
- Primary result: 3D explainable AI framework achieves 72-73% accuracy for left hemisphere PCS detection vs 56-63% for right hemisphere

## Executive Summary
This paper introduces a 3D explainable artificial intelligence framework for detecting the paracingulate sulcus in structural MRI, addressing the challenge of accurately identifying variable sulcal features in neuroimaging. The method combines two deep learning architectures (simple-3D-CNN and simple-3D-MHL) with GradCam and SHAP explainability techniques and PCA-based dimensionality reduction to provide both local and global explanations of model predictions. The framework was validated on a well-annotated dataset of 596 subjects (TOP-OSLO), achieving classification accuracies of approximately 72-73% in the left hemisphere versus 56-63% in the right hemisphere for paracingulate sulcus presence/absence detection. The explainability analysis identified key sub-regions including the thalamus, cingulate gyrus, and temporal/parietal areas, with distinct patterns observed between hemispheres and network architectures. The study demonstrates the importance of unbiased annotation protocols and provides insights into brain anatomy relevant to psychiatric conditions.

## Method Summary
The framework extracts two input modalities (grey-white surface and sulcal skeleton) from T1-weighted MRI scans using BrainVISA, then applies 3D deep learning models (simple-3D-CNN and simple-3D-MHL) for binary classification of paracingulate sulcus presence/absence. Local explanations are generated using GradCam (sensitivity-based) and SHAP (attribution-based) methods, which are then combined with statistical shape features and reduced via PCA to identify global patterns. Overlap voting across multiple PCA components and analytical approaches identifies consistent sub-regions contributing to classification decisions. The method was validated on the TOP-OSLO dataset with 596 subjects, comparing performance across hemispheres and architectural choices while quantifying explainability quality through faithfulness and complexity metrics.

## Key Results
- Left hemisphere classification accuracy: 72-73% vs right hemisphere: 56-63%, attributed to anatomical prominence differences
- GradCam faithfulness scores: 0.19-0.24, SHAP faithfulness scores: 0.14-0.21, indicating partial alignment between explanations and model decisions
- Identified sub-regions include thalamus, cingulate gyrus, and temporal/parietal areas, with distinct patterns between hemispheres and network architectures
- Overlap voting across PCA components successfully identified consistent regions that contribute to classification across different analytical approaches

## Why This Works (Mechanism)

### Mechanism 1
The 3D explainable framework accurately identifies the paracingulate sulcus by combining statistical shape features with local explainability methods (GradCam, SHAP) and PCA-based dimensionality reduction. The framework extracts two input modalities (grey-white surface and sulcal skeleton), applies deep learning for binary classification, then uses PCA to generalize local explanations and statistical features. Overlap voting identifies consistent regions across methods.

Core assumption: Both GradCam (sensitivity-based) and SHAP (attribution-based) methods, when combined with PCA, provide complementary information about model decision-making.

Evidence anchors:
- [abstract]: "Our framework integrates statistical features (Shape) and XAI methods (GradCam and SHAP) with dimensionality reduction, ensuring that explanations reflect both model learning and cohort-specific variability."
- [section 3.4]: Describes the mathematical formulation of GradCam and SHAP methods.
- [corpus]: Weak evidence - no direct mention of this specific combination in related papers.

Break Condition: If either GradCam or SHAP produces systematically inconsistent explanations, the overlap voting would fail to identify meaningful regions, reducing the framework's reliability.

### Mechanism 2
The framework achieves higher accuracy in left hemisphere detection (72-73%) compared to right hemisphere (56-63%) due to anatomical prominence of the left paracingulate sulcus. The framework leverages anatomical differences between hemispheres, with the left hemisphere showing more consistent paracingulate sulcus presence and prominence, leading to better-defined classification patterns.

Core assumption: The anatomical variability of the paracingulate sulcus is systematically different between hemispheres, affecting the model's ability to learn consistent patterns.

Evidence anchors:
- [section 5.1]: "The left paracingulate sulcus is anatomically more prominent in the left hemisphere... which implies that the pattern recognition problem is better defined in the left hemisphere than in the right one."
- [section 4.1]: Provides classification accuracy numbers showing higher performance in left hemisphere.
- [corpus]: Weak evidence - no direct mention of hemisphere-specific sulcal variability in related papers.

Break Condition: If anatomical differences between hemispheres are not the primary driver of accuracy differences, then hemisphere-specific performance patterns would not be consistently observed across datasets.

### Mechanism 3
The framework's combination of explainability methods and statistical features provides insights into brain-wide anatomical covariability associated with paracingulate sulcus presence/absence. By applying PCA to both explanation outputs (GradCam, SHAP) and statistical features (shape), then using overlap voting, the framework identifies sub-regions that consistently contribute to classification decisions across different analytical approaches.

Core assumption: Brain-wide anatomical regions show consistent covariability patterns that can be detected through multiple analytical approaches.

Evidence anchors:
- [abstract]: "Our XAI 3D-Framework leverages global explanations to uncover the broader developmental context of specific cortical features."
- [section 3.6]: Describes the overlap voting mathematical formulation.
- [section 5.1]: Discusses anatomical regions identified and their functional implications.

Break Condition: If brain-wide covariability patterns are not consistent across analytical methods, the overlap voting would produce unreliable or inconsistent results.

## Foundational Learning

- Principal Component Analysis (PCA):
  - Why needed here: PCA reduces dimensionality of both explanation outputs and statistical features, allowing identification of consistent patterns across multiple analytical approaches and input modalities.
  - Quick check question: What is the primary purpose of applying PCA to the GradCam and SHAP outputs in this framework?

- Explainable AI Methods (GradCam and SHAP):
  - Why needed here: These methods provide different perspectives on model decision-making - GradCam shows sensitivity to perturbations while SHAP provides feature attribution, together giving a more complete picture of learned patterns.
  - Quick check question: How do GradCam and SHAP differ in their approach to explaining deep learning model decisions?

- Overlap Voting in Multi-Modal Analysis:
  - Why needed here: Overlap voting combines results from multiple PCA components and different analytical methods to identify consistently important regions, reducing the impact of method-specific biases.
  - Quick check question: What is the mathematical basis for the overlap voting approach used to combine PCA components?

## Architecture Onboarding

- Component map:
  - Data preprocessing (BrainVISA) -> Grey-white surface and sulcal skeleton extraction
  - Deep learning models (simple-3D-CNN/simple-3D-MHL) -> Binary classification
  - Explainability methods (GradCam, SHAP) -> Local explanations
  - PCA dimensionality reduction -> Global explanations
  - Overlap voting -> Identification of consistent sub-regions

- Critical path: Data preprocessing → Model training → Explainability → PCA → Overlap voting → Results interpretation

- Design tradeoffs:
  - Choice between simple-3D-CNN and simple-3D-MHL architectures affects both classification accuracy and explainability quality
  - Number of PCA components (6 chosen) balances computational efficiency with explanation comprehensiveness
  - Weighting scheme for overlap voting emphasizes certain analytical approaches over others

- Failure signatures:
  - Low faithfulness scores for explainability methods indicate poor alignment between explanations and model decisions
  - High complexity scores suggest explanations are not sufficiently interpretable
  - Inconsistent results across PCA components indicate unstable pattern learning

- First 3 experiments:
  1. Train simple-3D-CNN on left hemisphere data only, evaluate classification accuracy and explainability metrics
  2. Compare GradCam vs SHAP explanations for the same input to assess consistency
  3. Vary number of PCA components (5, 6, 7) to determine optimal balance of explanation quality vs computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
How does the depth of the CNN backbone influence the performance of the 3D explainability framework in detecting variable sulci?
Basis in paper: [explicit] The paper contrasts simple-3D-MHL (deeper backbone) with 2CNN-3D-MHL (shallower backbone), showing superior performance of the deeper model.
Why unresolved: The paper does not systematically explore different backbone depths or provide theoretical justification for why deeper architectures perform better for this specific task.
What evidence would resolve it: Comparative experiments testing multiple backbone depths (e.g., 2-layer, 3-layer, 4-layer) on the same dataset with consistent explainability analysis.

### Open Question 2
What is the causal relationship between specific brain regions identified by the explainability framework and the functional significance of the paracingulate sulcus?
Basis in paper: [explicit] The framework identifies sub-regions (thalamus, cingulate gyrus, temporal/parietal areas) associated with PCS presence/absence, but functional interpretation is limited.
Why unresolved: The study focuses on anatomical detection without establishing causal links between identified regions and cognitive/behavioral functions associated with PCS.
What evidence would resolve it: Longitudinal studies correlating PCS presence with functional connectivity patterns and cognitive performance in the identified sub-regions.

### Open Question 3
How does annotation protocol bias affect the generalizability and reliability of deep learning models for detecting variable sulci?
Basis in paper: [explicit] The study demonstrates performance differences between TOP-OSLO (expert dual-annotation) and DATASET-B (single untrained annotator), highlighting the impact of annotation quality.
Why unresolved: The paper shows correlation between annotation quality and performance but does not quantify the threshold of annotation reliability needed for robust model performance.
What evidence would resolve it: Systematic studies varying annotator expertise and protocol stringency to determine the minimum annotation standards required for reliable model training.

## Limitations
- Moderate explainability faithfulness scores (0.19-0.24 for GradCam, 0.14-0.21 for SHAP) indicate partial alignment between explanations and model decisions
- Significant performance gap between left (72-73%) and right (56-63%) hemispheres raises questions about generalizability across populations
- Reliance on psychiatric patient samples limits applicability to healthy controls or other clinical populations

## Confidence

- **High confidence**: The overall methodological framework combining 3D CNNs with explainability methods is technically sound and follows established practices in neuroimaging analysis
- **Medium confidence**: The anatomical interpretations of identified sub-regions (thalamus, cingulate gyrus, temporal/parietal areas) are plausible but require independent validation
- **Low confidence**: The specific attribution patterns and their functional significance, particularly the proposed role of caudate/putamen in executive function and motivation, lack strong empirical support from the presented results

## Next Checks

1. **Cross-validation with independent datasets**: Apply the trained models to external datasets of healthy controls to assess whether the hemisphere-specific performance patterns and identified sub-regions generalize beyond the psychiatric sample

2. **Ablation study of explainability methods**: Systematically remove either GradCam or SHAP from the overlap voting process to quantify the contribution of each method to the final explanations and test the assumption of complementary information

3. **Anatomical ground truth validation**: Compare the framework's identified sub-regions with established anatomical atlases and cytoarchitectonic maps to verify that the detected patterns align with known brain organization