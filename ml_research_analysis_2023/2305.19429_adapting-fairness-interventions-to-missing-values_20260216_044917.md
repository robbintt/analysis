---
ver: rpa2
title: Adapting Fairness Interventions to Missing Values
arxiv_id: '2305.19429'
source_url: https://arxiv.org/abs/2305.19429
tags:
- missing
- data
- fairness
- values
- imputation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of missing data on fairness
  in algorithmic classification. It proves that imputation-based approaches can significantly
  degrade both accuracy and fairness when missingness patterns carry predictive information.
---

# Adapting Fairness Interventions to Missing Values

## Quick Facts
- **arXiv ID**: 2305.19429
- **Source URL**: https://arxiv.org/abs/2305.19429
- **Reference count**: 40
- **Primary result**: Imputation-based fairness interventions can degrade accuracy and fairness when missingness patterns are predictive; adaptive methods that preserve missingness information improve both metrics.

## Executive Summary
This paper addresses a critical gap in fair machine learning: handling missing data without destroying the predictive information encoded in missingness patterns. The authors prove that standard impute-then-classify approaches inherently lose information when missingness is related to the label, leading to degraded fairness and accuracy. They introduce three adaptive methods for linear classifiers—adding missing indicators, affinely adaptive classification, and missing pattern clustering—plus FairMissBag, a bagging-based ensemble for nonlinear models. Experiments on COMPAS, Adult, and HSLS datasets show that adaptive methods consistently outperform baseline imputation, especially under non-random missingness.

## Method Summary
The paper proposes three adaptive methods for linear classifiers that preserve missingness information: (1) adding missing indicators as features, (2) affinely adaptive classification that expands the feature space to encode missing patterns, and (3) clustering data points with similar missingness patterns before applying fairness interventions. For nonlinear classifiers, they introduce FairMissBag, a bagging ensemble that resamples data with missing indicators and aggregates predictions from multiple fair classifiers. All methods can be combined with any existing fairness intervention algorithm while preserving the information encoded in missing patterns.

## Key Results
- Adaptive methods significantly improve fairness-accuracy tradeoff curves compared to impute-then-classify baselines across MCAR, MAR, and MNAR missingness mechanisms
- FairMissBag consistently outperforms standard bagging with imputation, especially when using iterative imputation
- Missing indicators provide the most reliable improvement across datasets and missingness types, while affinely adaptive classification can suffer from feature explosion
- Clustering methods show promise but are sensitive to hyperparameter choices and often produce few clusters in practice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Missing indicators preserve information about missingness that imputation destroys
- Mechanism: Adding missingness indicator features allows a linear classifier to adjust its bias term (or coefficients) conditionally based on which features are missing, preserving the original feature-label relationship
- Core assumption: The missingness pattern contains predictive signal that, when lost through imputation, degrades accuracy and fairness
- Evidence anchors:
  - [abstract] "The missingness pattern of the data can also convey important information about the predictive label."
  - [section 4.1] "Using missing indicators captures the information encoded in the missingness patterns of the data."
  - [corpus] "Learnable Prompt as Pseudo-Imputation" suggests missingness can be a proxy for missing data
- Break Condition: If missingness is truly MCAR and carries no label signal, the benefit of missing indicators will be negligible

### Mechanism 2
- Claim: Fair interventions applied to encoded data preserve fairness properties
- Mechanism: The adaptive methods (adding indicators, affine adaptation, clustering) transform the dataset so that standard fairness interventions can be applied without losing missingness information
- Core assumption: Fairness interventions are effective when applied to data that preserves all relevant predictive features, including missingness
- Evidence anchors:
  - [abstract] "These algorithms can be combined with any preexisting fairness-intervention algorithm to handle all possible missing patterns while preserving information encoded within the missing patterns."
  - [section 4] "Our algorithms work by modifying the dataset to preserve the information encoded in the missing patterns, then applying an off-the-shelf fairness-intervention algorithm to the new dataset."
- Break Condition: If the fairness intervention cannot handle the expanded feature space or clustering, the adaptive method may not yield improvements

### Mechanism 3
- Claim: Ensemble methods with missing indicators avoid imputation-induced bias
- Mechanism: FairMissBag resamples data while including missing indicators, trains multiple fair classifiers, and aggregates predictions, avoiding imputation bias while maintaining group balance
- Core assumption: Averaging predictions from multiple classifiers trained on resampled data preserves fairness constraints and reduces variance
- Evidence anchors:
  - [section 5] "We introduce FairMissBag, a fair bagging algorithm that can be applied to any fair classifier to handle missing values."
  - [section 5] "The inclusion of missing indicators as input features prevents the issue of reduced fairness-accuracy curve described in Theorem 1."
- Break Condition: If the base classifier is very sensitive to resampling variance or the fairness constraint is too strict, ensemble averaging may degrade performance

## Foundational Learning

- **Concept**: Mutual information and information-theoretic limits
  - Why needed here: Theorem 1 uses mutual information to show that imputation inherently loses predictive signal from missingness
  - Quick check question: What does it mean if I(M;Y) = 0 for a given dataset?

- **Concept**: Fairness constraints (e.g., equalized odds)
  - Why needed here: The paper evaluates all methods against group fairness metrics, so understanding the constraint definition is essential
  - Quick check question: How does equalized odds differ from statistical parity?

- **Concept**: Linear vs. non-linear classifier adaptation
  - Why needed here: Different adaptive strategies are used depending on whether the base classifier is linear or non-linear
  - Quick check question: Why does affinely adaptive classification require O(nmiss*d) features?

## Architecture Onboarding

- **Component map**: Data preprocessing → Missing indicator encoding → Fairness intervention → Evaluation
  - For non-linear: FairMissBag wrapper around fairness intervention
- **Critical path**: 
  1. Encode missingness (indicators or clustering)
  2. Apply fairness intervention
  3. Evaluate fairness-accuracy tradeoff
- **Design tradeoffs**:
  - Adding indicators: simple, scalable, but may add noise
  - Affine adaptation: more expressive, but feature explosion
  - Clustering: preserves structure, but may fragment data
  - FairMissBag: robust, but computationally heavier
- **Failure signatures**:
  - Accuracy drops despite fairness gain → missingness not predictive
  - High variance across runs → insufficient cluster size or poor resampling
  - Bias amplification → missingness correlated with sensitive attribute in a harmful way
- **First 3 experiments**:
  1. Run baseline impute-then-classify with mean imputation on COMPAS; record fairness-accuracy curve
  2. Add missing indicators and rerun same fairness intervention; compare curves
  3. Apply FairMissBag with iterative imputation and same intervention; compare fairness gain vs. accuracy loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adaptive methods for missing values perform on fairness metrics beyond equalized odds (e.g., statistical parity, demographic parity)?
- Basis in paper: [inferred] The paper tests equalized odds and mentions extensions to other metrics but does not provide experimental results
- Why unresolved: The experiments only measure equalized odds discrimination; broader fairness metric coverage is missing
- What evidence would resolve it: Experiments comparing adaptive methods using statistical parity and demographic parity on multiple datasets

### Open Question 2
- Question: What is the impact of imputation choice on fairness when using non-linear models beyond bagging ensembles?
- Basis in paper: [explicit] The paper tests mean, KNN, and iterative imputation with FairMissBag but notes imputation choice varies by setup
- Why unresolved: Results show imputation affects accuracy and fairness, but only for FairMissBag; other non-linear methods (e.g., neural nets) are untested
- What evidence would resolve it: Systematic experiments with multiple non-linear classifiers and imputation strategies

### Open Question 3
- Question: Can adaptive methods improve fairness when sensitive attributes are missing or must be inferred?
- Basis in paper: [explicit] The authors mention this as a future direction and reference related work on missing sensitive attributes
- Why unresolved: The paper focuses on missing non-sensitive features, not missing sensitive attributes
- What evidence would resolve it: Experiments evaluating adaptive algorithms when S is missing or estimated from other features

### Open Question 4
- Question: How sensitive are adaptive methods to hyperparameter choices like minimum cluster size in missing pattern clustering?
- Basis in paper: [explicit] The authors test different k values but note the method often produced few clusters and low performance
- Why unresolved: Limited hyperparameter tuning and clustering success; unclear if tuning could improve outcomes
- What evidence would resolve it: Grid search over clustering hyperparameters with stability and performance metrics

## Limitations

- The theoretical guarantees assume missingness patterns are independent of the sensitive attribute beyond their correlation with the label, which may not hold in practice
- Affinely adaptive classification suffers from feature explosion (O(nmiss*d) features), limiting scalability to high-dimensional data
- Missing pattern clustering is sensitive to hyperparameter choices and often produces few clusters, reducing its effectiveness

## Confidence

- **High confidence**: The empirical results showing adaptive methods outperform baseline imputation on fairness-accuracy tradeoff curves are well-supported by the experimental design and multiple datasets
- **Medium confidence**: The theoretical analysis in Theorem 1 about information loss through imputation is sound, but its practical impact depends on the specific dataset characteristics and missingness mechanism
- **Medium confidence**: The claim that FairMissBag can be applied to any fair classifier is theoretically valid, but the computational overhead and sensitivity to hyperparameters (k, τ, etc.) may limit practical adoption

## Next Checks

1. Test the adaptive methods on a dataset where missingness is explicitly correlated with the sensitive attribute to evaluate potential bias amplification
2. Perform ablation studies on FairMissBag to quantify the contribution of missing indicators versus resampling to the overall performance gain
3. Evaluate the methods on a high-dimensional dataset (e.g., genomics or imaging) to assess scalability and robustness to feature explosion in affinely adaptive classification