---
ver: rpa2
title: Reducing the Side-Effects of Oscillations in Training of Quantized YOLO Networks
arxiv_id: '2311.05109'
source_url: https://arxiv.org/abs/2311.05109
tags:
- quantization
- weights
- scale
- factors
- oscillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of quantizing YOLO networks
  at low precision (4-bit and 3-bit) for efficient deployment on edge devices. The
  authors identify oscillation issues in quantization-aware training (QAT) as a major
  obstacle, which affects both latent weights and learnable scale factors for weights
  and activations.
---

# Reducing the Side-Effects of Oscillations in Training of Quantized YOLO Networks

## Quick Facts
- arXiv ID: 2311.05109
- Source URL: https://arxiv.org/abs/2311.05109
- Reference count: 36
- Key outcome: EMA and QC methods improve YOLO5-n mAP from 20.6 to 23.8 at 4-bit and from 15.2 to 18.2 at 3-bit

## Executive Summary
This paper addresses oscillation issues in low-precision quantization of YOLO networks (4-bit and 3-bit) for edge deployment. The authors identify that oscillations in weights and scale factors during quantization-aware training (QAT) degrade model performance. They propose two methods: Exponential Moving Average (EMA) to smooth oscillations during training and a Quantization Correction (QC) step to fix errors post-training. The methods are evaluated on COCO dataset using YOLO5 and YOLO7 variants, showing consistent improvements over state-of-the-art approaches.

## Method Summary
The paper proposes two complementary methods to address oscillations in quantization-aware training (QAT) of YOLO networks. First, EMA smoothing stabilizes training by averaging weights and scale factors over multiple optimization steps, reducing variance caused by oscillation. Second, QC applies a post-hoc affine transformation to pre-activations to correct quantization errors accumulated during training. The QC method learns per-channel correction factors that can be absorbed into existing Batch Normalization layers, effectively converting per-tensor to per-channel quantization for improved stability.

## Key Results
- YOLO5-n achieves 23.8 mAP at 4-bit (vs 20.6 baseline) and 18.2 mAP at 3-bit (vs 15.2 baseline)
- QC method improves stability of per-channel quantization
- Consistent improvements observed across YOLO5 and YOLO7 variants for both object detection and semantic segmentation
- Methods outperform state-of-the-art LSQ and Oscillation dampening approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EMA smoothing stabilizes training by averaging over multiple optimization steps, reducing the variance caused by oscillation.
- Mechanism: By computing an exponential moving average of weights and scale factors, the EMA method dampens rapid fluctuations in latent weights and quantization parameters, resulting in a more stable convergence path.
- Core assumption: Oscillations in QAT cause detrimental variance in the optimization trajectory, and averaging over recent updates can counteract this effect.
- Evidence anchors:
  - [abstract] "To mitigate the effect of oscillation, we first propose Exponentially Moving Average ( EMA) based update to the QAT model."
  - [section] "Exponential moving average can take into account model weights at the last several steps of training and smoothen out the oscillation behavior and come up with the best possible latent state for oscillating weights."
  - [corpus] Weak evidence - corpus neighbors do not directly discuss EMA smoothing in the context of quantization-aware training.

### Mechanism 2
- Claim: QC corrects the quantization error induced by oscillation by applying a post-hoc affine transformation to the pre-activations.
- Mechanism: The QC method learns per-channel correction factors (scale and shift) that compensate for the error accumulated due to oscillating weights and scale factors during QAT, effectively converting per-tensor quantization to per-channel quantization.
- Core assumption: The error induced by oscillations can be modeled and corrected using an affine transformation, and this transformation can be absorbed into existing Batch Normalization layers.
- Evidence anchors:
  - [abstract] "Further, we propose a simple QAT correction method, namely QC, that takes only a single epoch of training after standard Quantization-Aware Training ( QAT) procedure to correct the error induced by oscillating weights and activations resulting in a more accurate quantized model."
  - [section] "Our post-hoc correction quantization step simply transforms the pre-activations hl ∈ I RNl of all l layers using an affine function, to compensate for error induced in matrix multiplications due to oscillations during the quantization-aware training."
  - [corpus] Weak evidence - corpus neighbors do not directly discuss post-hoc correction methods in the context of quantization-aware training.

### Mechanism 3
- Claim: Per-channel quantization with correction factors improves the stability of quantization compared to per-tensor quantization.
- Mechanism: By learning per-channel correction factors, the QC method effectively distributes the quantization error across channels, allowing for more accurate representation of the tensor's distribution and reducing the impact of oscillation-induced errors.
- Core assumption: Per-channel quantization provides more flexibility in representing the tensor's distribution compared to per-tensor quantization, and this flexibility can be leveraged to correct oscillation-induced errors.
- Evidence anchors:
  - [abstract] "The QC method also improves the stability of per-channel quantization."
  - [section] "Furthermore, these correction parameters can also be stored as quantization scale factors by converting per-tensor quantization to per-channel quantization."
  - [corpus] Weak evidence - corpus neighbors do not directly discuss per-channel quantization in the context of oscillation correction.

## Foundational Learning

- Concept: Quantization-Aware Training (QAT)
  - Why needed here: Understanding QAT is crucial to grasping the problem of oscillation and the proposed solutions (EMA and QC).
  - Quick check question: What is the main challenge addressed by QAT in the context of neural network deployment on edge devices?

- Concept: Straight-Through Estimator (STE)
  - Why needed here: STE is the de-facto method for backpropagating through the non-differentiable quantization function, and its approximation leads to the oscillation phenomenon.
  - Quick check question: How does STE approximation contribute to the oscillation issue in QAT?

- Concept: Per-tensor vs. Per-channel Quantization
  - Why needed here: Understanding the difference between per-tensor and per-channel quantization is essential to appreciate the benefits of the QC method.
  - Quick check question: What is the main difference between per-tensor and per-channel quantization in terms of flexibility and accuracy?

## Architecture Onboarding

- Component map:
  Quantization function -> EMA module -> QC module -> Batch Normalization layers

- Critical path:
  1. Perform standard QAT with EMA to stabilize training
  2. After QAT, apply the QC method to learn correction factors
  3. Fold the correction factors into Batch Normalization layers for efficient inference

- Design tradeoffs:
  - EMA introduces additional memory overhead to store moving averages
  - QC requires an additional training epoch but can be performed with a small calibration set
  - Per-channel quantization may introduce instability in networks with depth-wise convolutions

- Failure signatures:
  - Oscillating weights and scale factors during training
  - Sub-optimal quantization performance compared to full-precision models
  - Instability in per-channel quantization for networks with depth-wise convolutions

- First 3 experiments:
  1. Implement EMA on a simple linear regression problem to observe its smoothing effect on oscillating weights
  2. Apply the QC method to a pre-trained quantized YOLO5 model and evaluate its impact on mAP score
  3. Compare the performance of per-tensor and per-channel quantization with QC on a YOLO5 variant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the QC method generalize to other network architectures beyond YOLO, such as transformers or NLP models?
- Basis in paper: [inferred] The authors state "In future work, we believe QC scale and shift factors can be generalized by estimating correction factors that are weighted for specific regions in the tensors" and show effectiveness only on YOLO models.
- Why unresolved: The paper only demonstrates QC on YOLO5 and YOLO7 architectures. Generalization to other architectures is mentioned as future work without experimental validation.
- What evidence would resolve it: Testing QC on diverse architectures (transformers, language models, etc.) and comparing performance gains against baseline quantization methods.

### Open Question 2
- Question: What is the theoretical explanation for why latent weights at quantization boundaries are closer to optimality than their quantized states?
- Basis in paper: [explicit] The authors observe in Section 4.2 that "latent weights hanging at quantization boundaries are already closer to optimality than their nearest quantization levels as the soft-rounded...YOLO models yield better performance than quantized ones."
- Why unresolved: The paper provides empirical evidence but does not offer a theoretical explanation for this phenomenon.
- What evidence would resolve it: Mathematical analysis or theoretical framework explaining the relationship between quantization boundaries, latent weights, and optimality in the context of STE approximation.

### Open Question 3
- Question: How does the choice of EMA decay parameter α affect the trade-off between oscillation reduction and model accuracy across different network depths and architectures?
- Basis in paper: [explicit] The authors note "EMA takes into account ≈ 1 − (1 − α) iterations to compute the average weights" and show EMA is "quite stable to different decay parameters" for YOLO5-n, but don't explore this systematically.
- Why unresolved: The paper provides limited ablation on α values and only for one architecture (YOLO5-n).
- What evidence would resolve it: Systematic study of α's effect across different architectures, depths, and quantization bit-widths, with analysis of the optimal range for different scenarios.

## Limitations

- The paper does not provide implementation details for the decay parameter α in the EMA method, which could significantly impact the effectiveness of the oscillation smoothing.
- The specific initialization strategy for the QC correction parameters is not detailed, potentially affecting the convergence and stability of the post-hoc correction step.
- The experimental setup lacks ablation studies to isolate the contributions of EMA and QC methods individually, making it difficult to assess their relative importance.
- The comparison with state-of-the-art methods (LSQ and Oscillation dampening) does not provide specific hyperparameter settings, limiting the ability to reproduce and validate the claimed improvements.

## Confidence

- High confidence in the proposed mechanism of EMA smoothing to mitigate oscillations in QAT, supported by the theoretical justification and empirical evidence.
- Medium confidence in the effectiveness of the QC method, as the post-hoc correction approach is novel but lacks detailed implementation specifics.
- Low confidence in the per-channel quantization benefits without further ablation studies to demonstrate the necessity and impact of this choice.

## Next Checks

1. Conduct an ablation study to isolate the contributions of the EMA and QC methods by evaluating their performance independently and in combination on a simple quantized network.
2. Investigate the sensitivity of the QC method to different initialization strategies for the correction parameters by comparing various initialization schemes on a quantized YOLO5 variant.
3. Perform a detailed comparison of per-tensor and per-channel quantization with and without the QC method to assess the benefits of per-channel quantization in the presence of oscillation-induced errors.