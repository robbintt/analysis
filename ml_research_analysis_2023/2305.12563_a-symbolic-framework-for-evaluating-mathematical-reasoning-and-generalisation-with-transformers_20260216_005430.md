---
ver: rpa2
title: A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation
  with Transformers
arxiv_id: '2305.12563'
source_url: https://arxiv.org/abs/2305.12563
tags:
- equation
- arxiv
- derivations
- mathematical
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a symbolic framework for evaluating Transformers
  on mathematical reasoning tasks. It generates derivations using a symbolic engine
  and applies perturbations to assess out-of-distribution generalization.
---

# A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers

## Quick Facts
- arXiv ID: 2305.12563
- Source URL: https://arxiv.org/abs/2305.12563
- Reference count: 40
- Models show up to 80 F1 point drops on perturbed test sets versus in-distribution performance

## Executive Summary
This paper introduces a symbolic framework for systematically evaluating Transformers on mathematical reasoning tasks. The framework generates valid mathematical derivations using a symbolic engine, then applies controlled perturbations that preserve semantic equivalence while altering surface patterns. When fine-tuned BERT models achieve high performance on unperturbed data, they fail dramatically (up to 80 F1 points) on perturbed versions, revealing that models rely on shallow textual patterns rather than genuine mathematical understanding. The authors release code, datasets, and models to enable further research in this direction.

## Method Summary
The framework generates mathematical derivations using SymPy, then applies semantics-preserving perturbations (variable renaming, expression exchange, annotation replacement) to create out-of-distribution test examples. BERT variants are fine-tuned on next-equation prediction tasks with 200K examples across 18 operators, using 12 epochs, batch size 8, and learning rate 5e-7. Models are evaluated on static test sets, perturbed versions, and extrapolation tasks where training complexity differs from test complexity.

## Key Results
- Fine-tuned BERT models achieve high in-distribution performance but fail on perturbed examples with up to 80 F1 point drops
- Operator-specific generalization failures vary significantly (premise renaming easier than integration for models)
- Models trained on more complex derivations fail to generalize to simpler problems in extrapolation tasks
- Models lacking entailment pre-training show worse generalization to perturbed examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symbolic perturbations isolate model weaknesses by preserving logical correctness while altering superficial features
- Mechanism: The framework generates mathematically valid derivations, then applies controlled perturbations (e.g., variable renaming, expression exchange) that maintain semantic equivalence but change surface patterns. Models trained on one form fail when tested on perturbed versions, revealing reliance on shallow textual cues rather than mathematical understanding
- Core assumption: Models' success on unperturbed data depends heavily on recognizing specific textual patterns rather than abstract mathematical rules
- Evidence anchors:
  - [abstract] "perturbations to input reasoning can reduce their performance by up to 80 F1 points"
  - [section] "semantics-preserving perturbations generate examples that preserve the semantic link between sequence and label"
  - [corpus] "Controlling Equational Reasoning in Large Language Models with Prompt Interventions" - related work on symbolic perturbations
- Break condition: If models could learn abstract operator rules rather than surface patterns, perturbations would have minimal impact on performance

### Mechanism 2
- Claim: Equation dependencies in derivations create long-range context that models must capture to succeed
- Mechanism: Generated derivations form chains where each equation depends on previous steps through operator applications. Models must learn these dependency structures to predict the next equation correctly, not just local patterns
- Core assumption: Mathematical reasoning requires understanding of sequential dependencies rather than isolated equation solving
- Evidence anchors:
  - [section] "long-range dependencies. Mathematically, the model should learn the necessary equation dependencies required to form the final equation"
  - [section] "SciBERT and MathBERT, despite both being trained on scientific corpora... display the top three operators by normalised frequency per generalisation category"
  - [corpus] Weak - no direct corpus evidence for dependency learning
- Break condition: If dependencies were irrelevant, models would perform equally well on shuffled or independent equations

### Mechanism 3
- Claim: Entailment pre-training improves generalisation to perturbed examples by teaching logical relationships
- Mechanism: Models like MathBERT that use context correspondence pre-training learn to pair equation descriptions with equations, which is more relevant to entailment tasks than sentence prediction. This helps them generalize better to perturbed examples that maintain logical relationships
- Core assumption: The nature of pre-training objectives affects models' ability to handle logical entailment in mathematical reasoning
- Evidence anchors:
  - [section] "We therefore attribute generalisability failures of MathBERT to insufficient entailment pre-training"
  - [section] "Next-equation prediction considers if context entails the equation in an argumentative sense, rather than a descriptive sense"
  - [corpus] "HYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for Enhanced LLM Reasoning" - related work on combining natural and symbolic reasoning
- Break condition: If entailment pre-training were irrelevant, all models would generalize similarly regardless of their pre-training objectives

## Foundational Learning

- Concept: Computer algebra systems and symbolic manipulation
  - Why needed here: The framework relies on SymPy to generate valid mathematical derivations and apply perturbations systematically
  - Quick check question: Can you explain how SymPy represents equations and performs operations like differentiation or substitution?

- Concept: Transformer architecture and fine-tuning
  - Why needed here: Understanding how BERT variants process sequences and how fine-tuning overwrites pre-trained representations is crucial for interpreting results
  - Quick check question: What happens to a model's attention patterns when fine-tuned on mathematical reasoning tasks versus pre-training tasks?

- Concept: Sequence classification and entailment
  - Why needed here: The task requires models to determine if a candidate equation logically follows from previous context, which is a form of textual entailment
  - Quick check question: How does next-equation prediction differ from standard sentence entailment tasks in NLP?

## Architecture Onboarding

- Component map: Data generation pipeline (Premise generation → Derivation generation → Perturbation application) → Model training (Fine-tuning transformers on sequence classification) → Evaluation (Static testing → Perturbed testing → Comparison of performance)

- Critical path: 1. Generate valid mathematical derivations using symbolic engine 2. Apply perturbations while maintaining mathematical correctness 3. Fine-tune transformer models on the unperturbed data 4. Evaluate model performance on both unperturbed and perturbed test sets 5. Analyze generalization failures and operator-specific issues

- Design tradeoffs:
  - Perturbation strength vs. maintaining mathematical validity
  - Generation complexity vs. training data volume
  - Model size vs. generalization capability
  - Pre-training objective vs. task-specific performance

- Failure signatures:
  - High performance on unperturbed data but significant drops on perturbed data
  - Operator-specific generalization failures (e.g., premise renaming vs. integration)
  - No improvement from formula structure pre-training on structural perturbations

- First 3 experiments:
  1. Fine-tune BERT-base-uncased on 2-step derivations and evaluate on all perturbation types
  2. Compare generalization of models trained on 3-step vs 4-step derivations to 2-step extrapolation
  3. Test effect of removing entailment pre-training (using vanilla BERT) on perturbation robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating structured dependencies during training truly enable smaller open-source models to rival GPT performance on mathematical reasoning tasks?
- Basis in paper: [explicit] The authors state that "the in-distribution performance of smaller open-source models may potentially rival GPT by incorporating appropriately structured derivation dependencies during training"
- Why unresolved: The paper demonstrates this potential through their symbolic framework but doesn't directly compare trained models with GPT models on the same tasks, leaving the actual performance gap unaddressed
- What evidence would resolve it: Direct comparison of fine-tuned models (with and without structured dependencies) against GPT-4 and GPT-3.5 on the same mathematical reasoning tasks, measuring both in-distribution and out-of-distribution performance

### Open Question 2
- Question: What specific aspects of mathematical reasoning do BERT and GPT models struggle with most when decoding indirect references to mathematical entities?
- Basis in paper: [explicit] The authors identify a "shared weakness between BERT and GPT involving a relative inability to decode indirect references to mathematical entities"
- Why unresolved: The paper demonstrates this weakness through perturbation experiments but doesn't isolate or categorize the specific types of indirect references that cause the most difficulty
- What evidence would resolve it: Detailed error analysis categorizing the types of indirect references that lead to model failures, potentially through human annotation of model mistakes across different perturbation types

### Open Question 3
- Question: How does the complexity of mathematical operators (measured by their predictability of effect on equation surface forms) correlate with model generalization performance?
- Basis in paper: [explicit] The authors suggest "operators with a less predictable (and identifiable) effect on the surface form of equations reliably leads to generalisation failure"
- Why unresolved: While the paper observes this correlation through qualitative analysis, it doesn't establish a systematic or quantitative relationship between operator complexity and generalization performance
- What evidence would resolve it: Quantitative measurement of operator complexity (e.g., based on surface form changes) correlated with model performance drops across different perturbation types and operator categories

## Limitations
- Data generation fidelity: Specific implementation details of perturbation functions and their mathematical correctness remain unclear, potentially limiting framework applicability
- Operator-specific generalization patterns: Analysis lacks statistical significance testing to validate whether observed differences are meaningful
- Pre-training objective attribution: Claims about entailment pre-training impact are based on comparative analysis rather than controlled ablation studies

## Confidence
- High confidence: Core finding that models fail on perturbed examples (up to 80 F1 points) is well-supported by experimental design and multiple perturbation types
- Medium confidence: Operator-specific patterns and pre-training objective effects are plausible but require more rigorous statistical validation
- Low confidence: Extrapolation results showing performance differences between models trained on different step complexities are less robust with limited sample size

## Next Checks
1. **Statistical validation of operator-specific generalization**: Perform permutation tests or bootstrap analysis on the operator-level F1 drops to determine which generalization failures are statistically significant versus random variation
2. **Controlled pre-training ablation study**: Train multiple BERT variants with identical architectures but systematically varied pre-training objectives on the same mathematical reasoning corpus and compare their perturbation generalization performance
3. **Perturbation complexity scaling analysis**: Systematically vary perturbation complexity while measuring F1 drops to reveal whether generalization failures scale linearly with difficulty or exhibit threshold effects