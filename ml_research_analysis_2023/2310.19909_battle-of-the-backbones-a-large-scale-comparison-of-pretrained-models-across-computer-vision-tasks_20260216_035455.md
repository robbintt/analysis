---
ver: rpa2
title: 'Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across
  Computer Vision Tasks'
arxiv_id: '2310.19909'
source_url: https://arxiv.org/abs/2310.19909
tags:
- supervised
- imagenet-1k
- backbones
- learning
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a large-scale comparison of pretrained models
  across various computer vision tasks. The authors benchmark a diverse suite of pretrained
  models, including vision-language models, self-supervised learning models, and the
  Stable Diffusion backbone, across tasks such as classification, object detection,
  OOD generalization, and image retrieval.
---

# Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks

## Quick Facts
- **arXiv ID:** 2310.19909
- **Source URL:** https://arxiv.org/abs/2310.19909
- **Reference count:** 40
- **Primary result:** ConvNeXt-Base and supervised SwinV2-Base outperform transformer-based and SSL backbones, though SSL methods are competitive in apples-to-apples comparisons.

## Executive Summary
This paper presents a comprehensive benchmarking study comparing over 1,500 pretrained models across diverse computer vision tasks. The authors evaluate architectures including vision transformers, convolutional networks, and vision-language models on classification, object detection, OOD generalization, and image retrieval tasks. The study reveals that supervised pretraining on large datasets (ImageNet-21k) maintains an advantage over self-supervised learning methods, but in controlled comparisons with similarly sized pretraining data, SSL backbones show competitive performance. The results highlight the importance of dataset scale and suggest future research should focus on applying SSL methods to advanced architectures with larger pretraining datasets.

## Method Summary
The study evaluates a diverse suite of pretrained models through systematic fine-tuning and linear probing protocols across multiple computer vision tasks. Backbones are trained using supervised learning, self-supervised learning (MoCo v3, VICReg, DINO, MAE), vision-language (CLIP), and monocular depth estimation (MiDaS) methods. Models are pretrained on datasets ranging from ImageNet-1k (1.3M images) to LAION-2B (2 billion images). The evaluation framework includes classification accuracy, object detection/segmentation mAP, OOD generalization accuracy, image retrieval metrics, adversarial robustness, and calibration measures. Performance comparisons use z-score normalization to enable cross-task aggregation, with moderate compute budgets constraining backbone sizes to ConvNeXt-Base scale.

## Key Results
- ConvNeXt-Base and supervised SwinV2-Base consistently outperform other backbones across most tasks
- In apples-to-apples comparisons on same architectures and similar dataset sizes, SSL backbones achieve competitive performance
- Vision transformers show higher sensitivity to scale (parameters and data size) than convolutional networks
- Performance across different tasks is strongly correlated (ρ > 0.8), supporting the trend toward general-purpose foundation models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ConvNeXt-Base and supervised SwinV2-Base outperform transformer-based and SSL backbones due to their architectural advantages combined with large-scale supervised pretraining.
- Mechanism: The paper demonstrates that supervised models pretrained on larger datasets (ImageNet-21k) consistently outperform self-supervised models trained on smaller datasets (ImageNet-1k). This performance gap persists even when comparing same-architecture models, indicating that dataset scale is the dominant factor.
- Core assumption: The superiority of supervised pretraining stems primarily from access to larger training datasets rather than inherent methodological advantages.
- Evidence anchors:
  - [abstract] "we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks"
  - [section 5] "The results suggest that we should try using advanced architectures, either convolutional or transformers, when applying SSL methods, and we should train on large datasets to compete with supervised learning."
  - [corpus] Weak - corpus neighbors don't discuss dataset scale advantages
- Break condition: When SSL models are pretrained on datasets comparable in size to ImageNet-21k, or when dataset size differences are controlled for in apples-to-apples comparisons.

### Mechanism 2
- Claim: Vision transformers are more sensitive to scale (both parameter count and pretraining data size) than convolutional networks.
- Mechanism: The paper shows that ViTs exhibit stronger correlations between relative performance and both parameter count (spearman ρ = 0.58) and pretraining data size (ρ = 0.72) compared to CNNs (ρ = 0.35 and ρ = 0.33 respectively). This indicates ViTs benefit more from scaling.
- Core assumption: The architectural differences between ViTs and CNNs make ViTs more data-hungry and parameter-sensitive.
- Evidence anchors:
  - [section 5] "ViTs are more sensitive to the amount of pretraining data and the number of parameters than CNNs"
  - [section 4.1] "we find that relative performance (z-scores) for both CNNs and ViTs correlates positively with parameter count but more so for ViTs (spearman ρ = 0.58)"
  - [corpus] Weak - corpus neighbors don't discuss ViT scaling sensitivity
- Break condition: When ViTs are evaluated at scales beyond ConvNeXt-Base, or when architectural modifications reduce their sensitivity to scale.

### Mechanism 3
- Claim: Performance across different computer vision tasks is strongly correlated, supporting the trend toward general-purpose foundation models.
- Mechanism: The paper demonstrates that Spearman correlation between performance on task pairs is typically above 0.8, with only retrieval showing a lower correlation (ρ = 0.49) that increases to 0.8 when removing specific underperforming models.
- Core assumption: A backbone's performance on one task is predictive of its performance on other tasks, suggesting universal feature representations.
- Evidence anchors:
  - [section 5] "we find a strong positive Spearman correlation between performance on task pairs (typically ρ > 0.8)"
  - [section 5] "this finding supports recent work which argues that a single inductive bias can solve a wide range of seemingly different problems"
  - [corpus] Weak - corpus neighbors don't discuss cross-task performance correlations
- Break condition: When evaluating backbones on tasks with fundamentally different requirements that break the correlation pattern.

## Foundational Learning

- **Concept:** Z-score normalization for cross-task comparison
  - Why needed here: The paper uses z-scores to compare backbone performance across different tasks with varying scales and metrics, enabling meaningful aggregate rankings
  - Quick check question: If a backbone achieves 95% accuracy on ImageNet and 80% on CIFAR-100, how does z-score normalization help compare these performances?

- **Concept:** Transfer learning and fine-tuning protocols
  - Why needed here: Understanding the difference between end-to-end fine-tuning and linear probing is crucial for interpreting backbone performance across tasks
  - Quick check question: What's the key difference between fine-tuning a backbone end-to-end versus using linear probing with a frozen backbone?

- **Concept:** Dataset scale impact on model performance
  - Why needed here: The paper emphasizes that supervised models outperform SSL models primarily because they're trained on larger datasets (ImageNet-21k vs ImageNet-1k)
  - Quick check question: How does the size difference between ImageNet-1k (1.3M images) and ImageNet-21k (14M images) potentially impact model performance?

## Architecture Onboarding

- **Component map:** Backbone selection (architectures, pretraining methods, datasets) -> Downstream task evaluation (classification, detection, segmentation, OOD generalization, retrieval) -> Ranking methodology (z-score normalization, cross-task aggregation)
- **Critical path:** 1) Select backbones to compare 2) Define evaluation tasks and protocols 3) Run experiments with controlled hyperparameters 4) Compute performance metrics 5) Apply z-score normalization 6) Aggregate and rank results
- **Design tradeoffs:** The study prioritizes practical relevance by using publicly available checkpoints and moderate compute budgets, but this limits comparison to models available at the time and excludes very large-scale experiments
- **Failure signatures:** Poor performance on multiple tasks suggests issues with backbone architecture or pretraining method; task-specific failures may indicate architectural mismatch; strong correlation breaks suggest tasks with fundamentally different requirements
- **First 3 experiments:**
  1. Run ImageNet classification with fine-tuning and linear probing on 2-3 representative backbones (e.g., ConvNeXt-Base, ViT-Base, ResNet-50) to establish baseline performance patterns
  2. Evaluate object detection performance using Cascade Mask R-CNN with both frozen and fine-tuned backbones to understand localization capabilities
  3. Test OOD generalization using ImageNet-A or ImageNet-V2 to assess robustness to distribution shifts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do larger ViT models (beyond ConvNeXt-Base) outperform modern CNN architectures like ConvNeXt and SwinV2 on the diverse tasks considered in BoB?
- Basis in paper: [inferred] The paper notes that ViTs benefit more from scale than CNNs, with higher correlations between relative performance and parameter count (ρ = 0.58 for ViTs vs ρ = 0.35 for CNNs) and pretraining data size (ρ = 0.72 for ViTs vs ρ = 0.33 for CNNs). The authors also mention that rankings may change at larger scales and that very large ViTs might outperform other models.
- Why unresolved: The paper did not include backbones larger than ConvNeXt-Base (except Stable Diffusion) due to computational constraints, leaving open the question of whether the trend favoring CNNs would persist at very large scales.
- What evidence would resolve it: Benchmarking very large ViT models (e.g., ViT-Large, ViT-Huge) pretrained on datasets larger than ImageNet-21k on the same diverse set of tasks used in BoB.

### Open Question 2
- Question: How would self-supervised learning (SSL) methods perform compared to supervised learning if trained on the same large-scale datasets (e.g., ImageNet-21k) with advanced architectures (e.g., ConvNeXt, SwinV2)?
- Basis in paper: [explicit] The paper finds that SSL backbones can outperform supervised pretraining with similarly sized pretraining datasets on classification and retrieval tasks, but supervised learning maintains an edge for detection and segmentation. However, the authors note that supervised pretraining checkpoints are often available trained on much larger datasets (ImageNet-21k), and in apples-to-apples comparisons, SSL methods achieve better performance on classification and retrieval but are worse than ImageNet-21k trained backbones for detection and segmentation.
- Why unresolved: The paper did not evaluate SSL methods pretrained on ImageNet-21k or other large-scale datasets with advanced architectures, leaving open the question of whether SSL methods would close the gap with supervised learning on all tasks.
- What evidence would resolve it: Pretraining SSL methods (e.g., MoCo v3, DINO, MAE) on ImageNet-21k or larger datasets using advanced architectures like ConvNeXt and SwinV2, and then evaluating their performance on the diverse set of tasks used in BoB.

### Open Question 3
- Question: What is the impact of pretraining dataset size and diversity on the performance of generative backbones like MAE and Stable Diffusion for downstream tasks?
- Basis in paper: [inferred] The paper notes that backbones trained with a generative objective, such as MAE or Stable Diffusion, had comparatively inferior performance on the tasks considered. However, Stable Diffusion is trained on a very large dataset (LAION-2B) yet exhibits inferior performance, suggesting that dataset size and diversity alone may not be sufficient for generative backbones to excel at downstream tasks.
- Why unresolved: The paper only evaluated Stable Diffusion on a limited set of tasks, and the performance of MAE was consistently lower than other methods across tasks. The specific reasons for the inferior performance of generative backbones and the role of dataset characteristics are not fully explored.
- What evidence would resolve it: Evaluating MAE and Stable Diffusion (and potentially other generative backbones) on a wider range of tasks, including tasks that may be more aligned with their generative objectives (e.g., image generation, inpainting). Additionally, comparing the performance of these models when pretrained on datasets of varying sizes and diversities to understand the impact of these factors.

## Limitations

- The study evaluated backbones only up to ConvNeXt-Base scale, potentially missing scaling behaviors at larger sizes
- Controlled hyperparameter searches may not have identified optimal settings for all backbone-task combinations
- Results rely on publicly available checkpoints, which may not represent the full potential of training methods
- Limited comparisons between ImageNet-1k and ImageNet-21k may not generalize to other dataset size differences

## Confidence

**High confidence**: Supervised pretraining on larger datasets (ImageNet-21k) consistently outperforms self-supervised methods trained on smaller datasets (ImageNet-1k) across most tasks.

**Medium confidence**: Vision transformers are more sensitive to scale than convolutional networks, based on correlation analysis with limited scale range.

**Medium confidence**: Strong cross-task correlations (ρ > 0.8) support universal feature representations, though retrieval shows weaker correlation that improves when removing specific underperforming models.

## Next Checks

1. **Scale Sensitivity Validation**: Evaluate the same backbones at larger scales (e.g., ConvNeXt-Large, ViT-Huge) to test whether ViTs maintain their higher sensitivity to parameter count and dataset size beyond the ConvNeXt-Base threshold.

2. **Dataset Size Control**: Train SSL backbones on ImageNet-21k or larger datasets (e.g., LAION-2B subsets) to isolate the effect of dataset size from training method differences, testing whether SSL methods become competitive when pretraining data scales match supervised methods.

3. **Task Correlation Stress Test**: Evaluate backbones on tasks with fundamentally different requirements (e.g., optical flow, 3D reconstruction, graph-based tasks) to identify whether the strong cross-task correlations break down for certain task families, revealing limitations of universal feature representations.