---
ver: rpa2
title: 'Who to Trust, How and Why: Untangling AI Ethics Principles, Trustworthiness
  and Trust'
arxiv_id: '2309.10318'
source_url: https://arxiv.org/abs/2309.10318
tags:
- trust
- system
- principles
- ethics
- trustworthiness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the conceptual confusion between trust in AI
  and AI trustworthiness, arguing that these terms are often used interchangeably
  despite representing distinct concepts. It highlights the need for clearer distinction
  between users' trust (an attitude) and the system's trustworthiness (a property),
  as well as the importance of considering the broader socio-technical ecosystem in
  which AI systems operate.
---

# Who to Trust, How and Why: Untangling AI Ethics Principles, Trustworthiness and Trust

## Quick Facts
- arXiv ID: 2309.10318
- Source URL: https://arxiv.org/abs/2309.10318
- Reference count: 0
- Primary result: Trust in AI is distinct from AI trustworthiness, requiring understanding of multiple dimensions including user attitudes, system properties, and socio-technical ecosystems.

## Executive Summary
This paper addresses the critical conceptual confusion between trust in AI and AI trustworthiness, arguing that these terms are often used interchangeably despite representing distinct concepts. Through a systematic review of 172 papers on AI ethics principles and trust, the authors demonstrate that empirical evidence linking principles like transparency and explainability to actual trust formation is limited or unclear. The paper emphasizes that trust in AI must be understood as a multidimensional phenomenon involving not just the system itself but also trust in developers, users' dispositional tendencies, rational evaluations, and procedural controls.

## Method Summary
The paper conducted a systematic literature review of 172 papers that discuss trust in the context of AI ethics principles, particularly focusing on the Australian AI Ethics Principles framework. The analysis examined how these papers conceptualize the relationship between AI ethics principles and trust formation, distinguishing between trust (a user attitude) and trustworthiness (a system property). The review also explored trust as a socio-technical system involving developers, users, and broader stakeholder ecosystems.

## Key Results
- Trust in AI involves both reliance on the system itself and trust in the developers of the AI system
- AI ethics principles like explainability and transparency do not automatically create user trust and may sometimes reduce it
- Trust in AI must be calibrated to match the actual trustworthiness of the system - overtrust leads to misuse while distrust leads to disuse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trust in AI involves both reliance on the system itself and trust in the developers of the AI system.
- Mechanism: The dual nature of trust means users evaluate both the technical performance of AI and the ethical responsibility of its creators, with trust potentially transferring from developers to their AI systems.
- Core assumption: Users perceive AI systems as extensions of their creators and may transfer their dispositional trust in developers to the AI technology.
- Evidence anchors:
  - [abstract] "trust in AI involves not only reliance on the system itself, but also trust in the developers of the AI system"
  - [section] "People often have little understanding of specific AI technologies or may not even be aware that they are using such systems, yet they are likely more familiar with the corporations providing the technologies. Trust transfer explains how technology users transfer their trust not only from known technologies to new technologies, but also from trust (or distrust) in technology providers to the technologies they develop."
  - [corpus] Weak evidence - corpus papers focus on technical trust evaluation but do not explicitly address developer trust transfer mechanisms.
- Break Condition: When users have direct, negative experiences with AI performance that contradict their trust in the developer, or when transparency reveals developer misconduct.

### Mechanism 2
- Claim: AI ethics principles like explainability and transparency do not automatically create user trust and may sometimes reduce it.
- Mechanism: The relationship between ethics principles and trust is complex - while these principles aim to reduce uncertainty and risk, they can also reveal limitations or biases that undermine trust, and their effectiveness depends on user context and understanding.
- Core assumption: Users evaluate explanations based on their background knowledge and the perceived accuracy/reliability of those explanations, not just their presence.
- Evidence anchors:
  - [abstract] "AI ethics principles such as explainability and transparency are often assumed to promote user trust, but empirical evidence of how such features actually affect how users perceive the system's trustworthiness is not as abundance or not that clear"
  - [section] "While explanations have been shown to increase users' willingness to trust AI, in other cases transparency may have negative effects on trust in an AI"
  - [corpus] Weak evidence - corpus papers focus on technical implementation of explainability but lack empirical studies on actual trust formation effects.
- Break Condition: When explanations are too complex for users to understand, when they reveal system limitations that concern users, or when users suspect explanations are manipulative.

### Mechanism 3
- Claim: Trust in AI must be calibrated to match the actual trustworthiness of the system - overtrust leads to misuse while distrust leads to disuse.
- Mechanism: Users develop trust through a combination of dispositional tendencies, rational evaluation of performance/purpose/process, and affinitive/emotional responses, requiring accurate information about system capabilities and limitations.
- Core assumption: Users can and will adjust their trust levels based on feedback from system performance and available information about capabilities and limitations.
- Evidence anchors:
  - [abstract] "Without recognising these nuances, trust in AI and trustworthy AI risk becoming nebulous terms for any desirable feature for AI systems"
  - [section] "The transparency of an AI system is intended to provide users with the information needed for them to gain these understandings" and "In automation technologies, it is possible to overtrust the technology if the user's perception of its capabilities exceeds what it can do"
  - [corpus] Moderate evidence - some papers discuss trust calibration but empirical evidence on how users actually calibrate trust is limited.
- Break Condition: When system performance is inconsistent, when users lack access to performance information, or when cognitive biases prevent accurate trust calibration.

## Foundational Learning

- Concept: Distinction between trust (user attitude) and trustworthiness (system property)
  - Why needed here: The paper argues this distinction is critical for understanding how AI ethics principles affect trust formation and why simply making systems "trustworthy" doesn't guarantee users will trust them
  - Quick check question: If an AI system is perfectly trustworthy but users don't trust it, what aspects of trust formation are missing according to the paper?

- Concept: Socio-technical systems perspective
  - Why needed here: The paper emphasizes that AI systems cannot be evaluated in isolation from their development context, user ecosystem, and broader stakeholder relationships
  - Quick check question: How does the socio-technical perspective change how we should think about implementing AI ethics principles?

- Concept: Multiple types of trust (dispositional, rational, affinitive, procedural)
  - Why needed here: Different AI ethics principles may foster different types of trust, and understanding these distinctions helps explain why some principles work better than others in specific contexts
  - Quick check question: Which type of trust would be most relevant for evaluating the effectiveness of transparency requirements in AI systems?

## Architecture Onboarding

- Component map: User perception modules (dispositional, rational, affinitive trust assessment) -> Developer reputation tracking -> Ethics principle compliance monitoring -> System performance monitoring -> Stakeholder ecosystem mapping
- Critical path: User experience → trust formation → usage behavior → feedback → trust calibration. Each step requires accurate information flow and appropriate contextualization.
- Design tradeoffs: Transparency vs. complexity (more transparency may reduce trust if users cannot understand explanations), developer reputation vs. system performance (users may trust developers but distrust specific system implementations), and standardization vs. contextualization (universal principles vs. context-specific trust factors)
- Failure signatures: Overtrust manifests as inappropriate system reliance and potential harm, while undertrust manifests as system disuse despite appropriate capabilities. Mixed signals occur when different trust types conflict.
- First 3 experiments:
  1. A/B test comparing user trust levels when given technical explanations vs. simplified explanations of the same AI system's decision-making process.
  2. Controlled study measuring how developer reputation affects trust in new AI systems from the same organization.
  3. Longitudinal study tracking how trust calibration changes as users gain experience with AI systems that have known limitations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different combinations of AI ethics principles affect users' trust in AI systems under various contexts and circumstances?
- Basis in paper: [explicit] The paper highlights the need for empirical evidence on how combinations of principles impact trust formation.
- Why unresolved: Most reviewed papers focused on isolated principles rather than examining interactions between multiple principles or potential trade-offs.
- What evidence would resolve it: Controlled studies comparing user trust responses across different combinations of implemented AI ethics principles in various use contexts.

### Open Question 2
- Question: What role does dispositional trust in AI developers play in shaping users' trust in specific AI systems?
- Basis in paper: [explicit] The paper discusses trust transfer from developers to technologies and the "duality of trust" concept.
- Why unresolved: While the concept is mentioned, there's limited empirical research on how users' general trust in developers specifically influences their trust in individual AI systems.
- What evidence would resolve it: Longitudinal studies tracking how users' pre-existing trust in developers affects their adoption and trust in new AI systems over time.

### Open Question 3
- Question: Under what conditions does transparency actually enhance versus undermine user trust in AI systems?
- Basis in paper: [explicit] The paper notes contradictory findings where transparency sometimes increases trust and other times decreases it.
- Why unresolved: The empirical evidence is unclear about when transparency contributes to trust, in which contexts, and what types of transparency are most effective.
- What evidence would resolve it: Experimental studies varying transparency levels and types across different AI applications and user groups, measuring corresponding trust changes.

## Limitations

- The empirical evidence base linking AI ethics principles to trust formation is limited or unclear, with most corpus papers focusing on technical implementation rather than actual trust outcomes
- The distinction between trust in developers versus trust in AI systems themselves is conceptually clear but lacks robust empirical validation
- The multidimensional nature of trust is well-theorized but the relative importance and interplay of these dimensions across different AI contexts remains under-specified

## Confidence

- High confidence: The conceptual distinction between trust (user attitude) and trustworthiness (system property) is well-established and consistently argued throughout the analysis
- Medium confidence: The claim that AI ethics principles don't automatically create trust is supported by the literature review, though specific mechanisms and conditions are less certain
- Medium confidence: The socio-technical perspective on trust is logically sound but empirical validation of stakeholder ecosystem effects is limited

## Next Checks

1. Conduct a controlled experiment measuring how users actually calibrate their trust in AI systems as they receive information about system capabilities, limitations, and developer reputation. Track changes in trust levels across multiple interaction cycles.

2. Design a study that systematically varies developer reputation (known trustworthy vs. unknown vs. known problematic) while holding AI system performance constant to measure the magnitude and conditions of trust transfer effects.

3. Implement A/B testing of different AI ethics principle implementations (transparency features, explainability mechanisms, accountability measures) to measure their differential effects on various types of trust across different user populations and use cases.