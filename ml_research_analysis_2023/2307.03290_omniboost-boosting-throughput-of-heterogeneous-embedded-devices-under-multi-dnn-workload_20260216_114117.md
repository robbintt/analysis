---
ver: rpa2
title: 'OmniBoost: Boosting Throughput of Heterogeneous Embedded Devices under Multi-DNN
  Workload'
arxiv_id: '2307.03290'
source_url: https://arxiv.org/abs/2307.03290
tags:
- throughput
- dnns
- performance
- computing
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniBoost, a lightweight multi-DNN scheduler
  for heterogeneous embedded devices. The key idea is to use Monte Carlo Tree Search
  (MCTS) to explore the vast design space of layer-to-device mappings for concurrent
  DNN workloads.
---

# OmniBoost: Boosting Throughput of Heterogeneous Embedded Devices under Multi-DNN Workload

## Quick Facts
- arXiv ID: 2307.03290
- Source URL: https://arxiv.org/abs/2307.03290
- Reference count: 32
- This paper introduces OmniBoost, a lightweight multi-DNN scheduler for heterogeneous embedded devices.

## Executive Summary
OmniBoost is a multi-DNN scheduler for heterogeneous embedded devices that uses Monte Carlo Tree Search (MCTS) to explore layer-to-device mappings for concurrent DNN workloads. The system leverages a lightweight ResNet-based CNN performance estimator that predicts throughput using distributed embedding vectors capturing kernel-level execution times. Evaluated on a HiKey970 board, OmniBoost achieves an average 4.6× throughput improvement over baseline GPU-only execution and significantly outperforms state-of-the-art methods like MOSAIC and GA-based schedulers in multi-DNN scenarios.

## Method Summary
OmniBoost profiles kernel execution times for each DNN layer on all available computing components (GPU, big CPU, LITTLE CPU) to build distributed embedding tensors. These tensors are combined with boolean mask tensors representing specific workloads and fed into a lightweight ResNet9-based CNN performance estimator (20K parameters) trained on 400 randomly generated workload samples. MCTS then explores the mapping space by iteratively expanding promising partial assignments based on throughput predictions from the CNN, ultimately selecting the optimal layer-to-device mapping for concurrent DNN execution.

## Key Results
- Average 4.6× throughput improvement over baseline (all layers on GPU)
- Achieves 54%, 19%, and 18% higher throughput compared to baseline, MOSAIC, and GA methods respectively
- CNN performance estimator trains in under a minute on 400 training samples
- Runtime mapping decisions complete in approximately 30 seconds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributed embedding vectors correlate layer performance to device characteristics across heterogeneous components
- Mechanism: Kernel-level execution time profiling is aggregated per layer and per device to form a tensor where each slice represents a computing component's performance vector for all DNN layers
- Core assumption: Kernel execution times are consistent across runs and accurately reflect layer performance on each device
- Evidence anchors:
  - [abstract] "we leverage stochastic space exploration and we combine it with a highly accurate performance estimator"
  - [section IV-A] "the performance of each DNN layer with respect to a computing component is defined as: Bl α = sum over kernels k in layer l of bk α"
  - [corpus] weak correlation with related works that use similar profiling but no explicit mention of kernel-level granularity
- Break condition: Kernel profiling becomes unreliable due to runtime variations (thermal throttling, cache effects) or when layers have variable computational patterns across devices

### Mechanism 2
- Claim: Monte Carlo Tree Search efficiently explores the vast mapping space under computational budget constraints
- Mechanism: MCTS builds a tree where nodes represent partial layer-to-device assignments, expanding promising paths using a reward function based on estimated throughput
- Core assumption: The reward function (estimated throughput) is sufficiently correlated with actual performance to guide exploration toward good solutions
- Evidence anchors:
  - [abstract] "We utilize Monte Carlo Tree Search (MCTS) for managing design space exploration under budget constraints"
  - [section IV-C] "MCTS progressively compiles a solution by stochastically expanding a state tree" and "non-leaf nodes are evaluated by the performance estimator"
  - [corpus] weak evidence - most related works don't mention MCTS or use different exploration strategies
- Break condition: Performance estimator becomes inaccurate for certain DNN combinations, leading MCTS to explore suboptimal regions

### Mechanism 3
- Claim: A lightweight ResNet9-based CNN with GELU activation provides fast and accurate throughput estimation
- Mechanism: The CNN takes masked embedding tensors as input and outputs normalized throughput predictions for each device, trained on randomly generated workload samples
- Core assumption: The small dataset of 400 training samples is representative enough for the CNN to generalize to unseen multi-DNN workloads
- Evidence anchors:
  - [section IV-B] "We develop a lightweight ResNet9-based CNN performance estimator [21] with only 20, 044 trainable parameters"
  - [section IV-B] "The dataset was split in 400 training and 100 validation samples" and "trained for 100 epochs... which took under a minute"
  - [section V-A] "OmniBoost achieves on average 54%, 19%, and 18% higher throughput compared to the baseline, the MOSAIC, and the GA method"
- Break condition: New DNN architectures with significantly different layer characteristics cause the CNN to mispredict throughput

## Foundational Learning

- Concept: Kernel-level profiling and embedding tensor construction
  - Why needed here: The distributed embedding tensor is the foundation that captures how each DNN layer performs on each device, enabling the performance estimator to make accurate predictions
  - Quick check question: What information does each cell in the embedding tensor contain, and how is it computed?

- Concept: Monte Carlo Tree Search and reward-based exploration
  - Why needed here: MCTS provides a way to efficiently explore the exponential mapping space without exhaustive search, using the performance estimator's predictions as rewards
  - Quick check question: How does MCTS determine which node to expand next, and what constitutes a "winning" state?

- Concept: Lightweight CNN regression for throughput estimation
  - Why needed here: The performance estimator must be fast enough to be queried hundreds of times during MCTS exploration while still providing accurate predictions
  - Quick check question: Why does the estimator use GELU activation instead of ReLU, and what is the output format?

## Architecture Onboarding

- Component map:
  - Kernel profiler: Collects execution times for each kernel on each device
  - Embedding tensor builder: Constructs the 3D tensor from profiling data
  - Mask tensor generator: Creates boolean tensors for specific workloads
  - CNN throughput estimator: Predicts throughput from masked embeddings
  - MCTS scheduler: Explores mapping space and selects optimal assignments
  - OpenCL/ARM Compute Library interface: Executes DNN partitions on hardware

- Critical path: MCTS → Mask tensor generation → CNN prediction → Reward calculation → Tree expansion (repeated 500 times)
- Design tradeoffs:
  - MCTS iteration count (500) vs. exploration quality vs. runtime
  - CNN model size (20K params) vs. prediction accuracy vs. training speed
  - Kernel-level vs. layer-level profiling (accuracy vs. profiling overhead)
- Failure signatures:
  - Poor throughput improvement: Performance estimator may be inaccurate or MCTS not exploring effectively
  - Long runtime (>30s): MCTS iterations may need reduction or CNN predictions may be slow
  - System crashes: Pipeline stage count exceeding device count constraint
- First 3 experiments:
  1. Profile all kernels for a single DNN (e.g., AlexNet) on all three devices and verify the embedding tensor structure
  2. Train the CNN on a small synthetic dataset and test predictions on known workloads
  3. Run MCTS with a fixed random seed on a 2-DNN workload and verify it finds the optimal mapping

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OmniBoost scale with increasing number of DNNs in the workload beyond 5, and what is the upper limit of concurrent DNNs that can be efficiently scheduled on the HiKey970 board?
- Basis in paper: [inferred] The paper mentions testing mixes of up to 5 DNNs, but states that mixes with 6 DNNs overloaded the board's computational capabilities.
- Why unresolved: The paper does not explore workloads beyond 5 DNNs or provide a clear upper limit for concurrent DNNs that can be efficiently scheduled.
- What evidence would resolve it: Experimental results showing the performance of OmniBoost with workloads consisting of more than 5 DNNs, and identifying the point at which the system becomes unresponsive or performance degrades significantly.

### Open Question 2
- Question: How does the performance of OmniBoost compare to other scheduling methods when the workload consists of DNNs with varying levels of computational complexity and resource requirements?
- Basis in paper: [inferred] The paper presents results for random mixes of DNNs, but does not explore the impact of varying computational complexity and resource requirements on the performance of different scheduling methods.
- Why unresolved: The paper does not provide a detailed analysis of how the performance of OmniBoost and other methods is affected by the heterogeneity of the DNNs in terms of computational complexity and resource requirements.
- What evidence would resolve it: Experimental results comparing the performance of OmniBoost and other scheduling methods on workloads with DNNs of varying computational complexity and resource requirements, and analyzing the impact of this heterogeneity on scheduling efficiency.

### Open Question 3
- Question: How does the performance of OmniBoost change when using different heterogeneous embedded systems with varying numbers and types of computing components (e.g., CPU, GPU, NPU)?
- Basis in paper: [explicit] The paper evaluates OmniBoost on the HiKey970 board, which has a specific configuration of CPU, GPU, and LITTLE CPU components.
- Why unresolved: The paper does not explore the performance of OmniBoost on other heterogeneous embedded systems with different configurations of computing components.
- What evidence would resolve it: Experimental results showing the performance of OmniBoost on various heterogeneous embedded systems with different numbers and types of computing components, and analyzing how the scheduling efficiency is affected by the system configuration.

## Limitations
- Kernel execution time profiling assumes consistent performance across runs, which may not hold under thermal throttling or cache effects
- The CNN performance estimator is trained on only 400 samples, potentially limiting its ability to generalize to diverse workload patterns
- The lightweight CNN architecture may struggle to model complex device-layer interactions for newer DNN architectures

## Confidence
- **High**: The core MCTS framework and distributed embedding concept are well-established
- **Medium**: The specific CNN architecture and training procedure appear sound but lack extensive validation
- **Medium**: The claimed 4.6× throughput improvement is impressive but based on a limited set of DNN combinations

## Next Checks
1. Conduct thermal stress tests to measure kernel execution time variability under sustained load conditions
2. Evaluate the CNN estimator's generalization to DNN architectures not included in the original 11 models
3. Perform ablation studies comparing MCTS with different iteration counts and CNN architectures to quantify their impact on throughput improvements