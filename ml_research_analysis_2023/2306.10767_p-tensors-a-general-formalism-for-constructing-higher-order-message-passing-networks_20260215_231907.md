---
ver: rpa2
title: 'P-Tensors: a General Formalism for Constructing Higher Order Message Passing
  Networks'
arxiv_id: '2306.10767'
source_url: https://arxiv.org/abs/2306.10767
tags:
- networks
- graph
- equivariant
- neural
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a general mathematical framework called P-tensors
  for constructing higher-order message passing networks on graphs. The key idea is
  to define a permutation-equivariant tensor with a reference domain, and derive a
  basis for all linear maps between arbitrary order P-tensors.
---

# P-Tensors: a General Formalism for Constructing Higher Order Message Passing Networks

## Quick Facts
- arXiv ID: 2306.10767
- Source URL: https://arxiv.org/abs/2306.10767
- Reference count: 38
- Key outcome: This paper introduces P-tensors as a general framework for higher-order message passing in graph neural networks, achieving state-of-the-art performance on molecular datasets

## Executive Summary
This paper presents P-tensors as a mathematical framework for constructing higher-order message passing networks on graphs. The key innovation is defining permutation-equivariant tensors with reference domains that transform predictably under permutations, enabling more expressive message passing between subgraphs while maintaining equivariance. The authors derive a complete basis for linear maps between arbitrary order P-tensors, allowing for flexible and theoretically grounded higher-order graph neural network architectures.

## Method Summary
The P-tensor framework defines tensors relative to ordered subsets of vertices (reference domains) that transform predictably under permutations. The paper derives a basis for all permutation-equivariant linear maps between P-tensors of different orders and reference domains, indexed by partitions of the set {1, 2, ..., k1 + k2}. This basis characterizes all possible equivariant transformations between P-tensors, with the number of parameters given by Bell numbers. The framework extends to cases with partially overlapping reference domains, multiplying the number of possible equivariant maps. Message passing layers are constructed using these bases to enable higher-order interactions while preserving permutation equivariance.

## Key Results
- P-tensors enable permutation-equivariant message passing between subgraphs by maintaining reference domains
- The framework provides a complete basis for all linear maps between P-tensors of different orders and reference domains
- P-tensor based models achieve state-of-the-art performance on benchmark molecular datasets compared to existing GNN approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: P-tensors enable permutation-equivariant message passing between subgraphs by defining a tensor structure relative to a reference domain that transforms predictably under permutations.
- Mechanism: The key insight is that P-tensors explicitly encode the reference domain as an ordered subset of atoms (e.g., vertices), and their transformation under permutations is defined by the formula τ(T)i1,...,ik = Tτ−1(i1),...,τ−1(ik). This allows the framework to maintain equivariance at two levels: global permutations of subgraphs and local permutations within subgraphs.
- Core assumption: The reference domain can be unambiguously identified and maintained throughout message passing operations, and the permutation group action on the reference domain is well-defined.
- Evidence anchors:
  - [abstract] "The key idea is to define a permutation-equivariant tensor with a reference domain"
  - [section] "Definition 1 (P-tensors)...We say that a k'th order tensor T ∈ Rd×d×...×d is a k'th order permutation covariant tensor (or P-tensor for short) with reference domain D if under the action of a permutation τ ∈ Sd acting on D it transforms as τ(T)i1,i2,...,ik = Tτ−1(i1),...τ−1(ik)."
  - [corpus] No direct evidence about P-tensors specifically, but related work on E(n)-equivariant message passing suggests the importance of structured tensor representations for permutation equivariance.
- Break condition: If the reference domain cannot be consistently identified across different parts of the graph or if the permutation group action becomes ambiguous, the equivariance property breaks down.

### Mechanism 2
- Claim: The framework provides a complete basis for all linear maps between P-tensors of different reference domains and orders, enabling general higher-order message passing.
- Mechanism: The paper derives a basis indexed by partitions of the set {1, 2, ..., k1 + k2} for linear maps between k1-th and k2-th order P-tensors. This basis characterizes all permutation-equivariant linear transformations between P-tensors, with the number of learnable parameters given by the Bell number B(k1 + k2).
- Core assumption: The space of permutation-equivariant linear maps between P-tensors can be fully characterized by a basis indexed by set partitions, and this basis is complete.
- Evidence anchors:
  - [abstract] "derive a basis for all linear maps between arbitrary order P-tensors"
  - [section] "Proposition 1 (Maron et al.) . The space of linear maps ϕ: Rdk1 → Rdk2 that is equivariant to permutations τ ∈ Sd in the sense of (4) is spanned by a basis indexed by the partitions of the set {1, 2, ..., k1 + k2}."
  - [corpus] No direct evidence about the completeness of the basis, but the connection to prior work on invariant and equivariant graph networks suggests this approach builds on established theoretical foundations.
- Break condition: If there exist permutation-equivariant linear maps between P-tensors that are not captured by the partition-based basis, the framework would be incomplete.

### Mechanism 3
- Claim: P-tensors enable learning on graphs with overlapping reference domains, capturing higher-order interactions that standard message passing misses.
- Mechanism: By allowing P-tensors with partially overlapping reference domains, the framework can capture interactions between different subgraphs and their internal structures. The theorem extends the basis to cases where reference domains only partially overlap, multiplying the number of possible equivariant maps.
- Core assumption: Partially overlapping reference domains are meaningful for capturing higher-order graph structures, and the extension of the basis to this case is valid.
- Evidence anchors:
  - [abstract] "This allows for more expressive message passing between subgraphs in a way that is still permutation-equivariant"
  - [section] "Theorem 2. Let T1 and T2 be two P-tensors with reference domains D1 and D2 such that D1∩D2 ≠ ∅...The set of all linearly independent maps from T1 to T2 is given by the set of pairs of partitions P1 and P2, from any subset of [k] and [k′]."
  - [corpus] No direct evidence about overlapping reference domains, but related work on higher-order graph neural networks and subgraph learning suggests this is a promising direction.
- Break condition: If partially overlapping reference domains do not correspond to meaningful graph structures or if the extension of the basis to this case introduces inconsistencies, the framework would lose its theoretical guarantees.

## Foundational Learning

- Concept: Permutation equivariance in graph neural networks
  - Why needed here: The entire framework is built on maintaining permutation equivariance at multiple levels (global and local), so understanding how permutations act on graph structures is fundamental.
  - Quick check question: What does it mean for a graph neural network to be permutation-equivariant, and how does this differ from permutation invariance?

- Concept: Group theory and symmetric group actions
  - Why needed here: The framework relies on the symmetric group Sn acting on graph vertices and P-tensor reference domains, so familiarity with group actions and their properties is essential.
  - Quick check question: How does the symmetric group Sn act on ordered subsets of vertices, and what is the relationship between the restriction of a permutation to a subset and the original permutation?

- Concept: Tensor algebra and multilinear algebra
  - Why needed here: P-tensors are higher-order tensors with specific transformation properties, so understanding tensor operations, indices, and transformation rules under permutations is crucial.
  - Quick check question: How do higher-order tensors transform under permutations of their indices, and what is the significance of the channel dimension in the context of P-tensors?

## Architecture Onboarding

- Component map:
  - P-tensor definition and creation: Define reference domains, create P-tensors with appropriate orders and channel dimensions
  - Basis derivation: Implement the partition-based basis for linear maps between P-tensors
  - Message passing layers: Build layers that use the basis to perform equivariant message passing between P-tensors
  - Model assembly: Combine message passing layers into a complete GNN architecture
  - Training and evaluation: Train the model on graph datasets and evaluate performance

- Critical path:
  1. Define reference domains for each P-tensor in the model
  2. Implement the partition-based basis for linear maps between P-tensors of different orders and reference domains
  3. Create message passing layers that use the basis to perform equivariant operations
  4. Assemble the layers into a complete GNN architecture
  5. Train the model on graph datasets and evaluate performance

- Design tradeoffs:
  - Order of P-tensors: Higher-order P-tensors can capture more complex interactions but increase computational cost and parameter count
  - Size of reference domains: Larger reference domains can capture more context but may lead to overfitting or computational challenges
  - Choice of partitions: Different partitions in the basis capture different types of interactions, and the choice of which partitions to use affects model expressiveness and efficiency

- Failure signatures:
  - Loss of permutation equivariance: If the model's outputs change in ways not explained by permutations of the input graph, there may be an issue with the P-tensor implementation or message passing layers
  - Poor performance on graph datasets: If the model performs worse than standard GNNs, it may indicate issues with the choice of reference domains, P-tensor orders, or message passing rules
  - Computational inefficiency: If the model is too slow or memory-intensive, it may be due to high-order P-tensors or large reference domains

- First 3 experiments:
  1. Implement a simple P-tensor message passing layer with zeroth-order P-tensors (equivalent to standard MPNNs) and verify it matches the performance of existing GNN implementations on small graph datasets
  2. Extend the implementation to first-order P-tensors with reference domains consisting of edges, and test on datasets where edge-level information is important (e.g., molecular graphs)
  3. Experiment with partially overlapping reference domains and higher-order P-tensors to capture subgraph-level interactions, and evaluate on datasets with known higher-order structures (e.g., cycles in ZINC-12K)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal types of subgraphs to use for constructing P-tensors in different graph learning tasks?
- Basis in paper: [explicit] The authors mention that cycles and similar structures are difficult for MPNNs and could be prime candidates for higher-order representations, but the optimal selection of subgraphs remains ill-defined.
- Why unresolved: The paper demonstrates the effectiveness of P-tensors with arbitrary subgraphs but does not provide a systematic method for selecting the most appropriate subgraphs for different tasks or graph types.
- What evidence would resolve it: Systematic experiments comparing the performance of P-tensors constructed from different subgraph types (e.g., cycles, cliques, star graphs, random subgraphs) across a wide range of graph learning tasks and graph structures.

### Open Question 2
- Question: How do differing orders of P-tensor representations interact in hybrid models, and what are the optimal ways to combine them?
- Basis in paper: [explicit] The authors mention that they used a hybrid model combining first-order and zeroth-order layers and suggest that this idea could be further explored to find what ways of building hybrid models tend to outperform others.
- Why unresolved: The paper provides some insight into hybrid models but does not explore the full space of possible combinations or provide a theoretical framework for understanding the interactions between different orders of P-tensor representations.
- What evidence would resolve it: Theoretical analysis of the expressive power of different hybrid models, combined with extensive empirical studies comparing various hybrid architectures on diverse graph learning tasks.

### Open Question 3
- Question: What is the provable expressiveness of P-tensor based GNNs in terms of the Weisfeiler-Lehman/Equivariant Graph Neural Networks hierarchy?
- Basis in paper: [explicit] The authors suggest that it would be helpful to better formalize the provable expressiveness of their algorithm in terms of the Weisfeiler-Lehman/Equivariant Graph Neural Networks hierarchy.
- Why unresolved: While the paper demonstrates the effectiveness of P-tensors through experiments, it does not provide a formal analysis of their expressive power in relation to established hierarchies of graph neural networks.
- What evidence would resolve it: Formal proofs of the expressive power of P-tensor based GNNs, showing their ability to distinguish non-isomorphic graphs and their position in the Weisfeiler-Lehman/Equivariant Graph Neural Networks hierarchy compared to other GNN architectures.

## Limitations

- The theoretical completeness of the partition-based basis for partially overlapping reference domains requires further verification
- Limited empirical evaluation on diverse graph datasets and comparison with state-of-the-art GNN methods
- No systematic study of optimal subgraph selection or hybrid model architectures using different P-tensor orders

## Confidence

- **High Confidence**: The core mechanism of P-tensors for maintaining permutation equivariance through reference domain transformations is well-established and theoretically sound
- **Medium Confidence**: The completeness of the partition-based basis for linear maps between P-tensors is supported by Proposition 1 but requires verification for all cases, particularly with overlapping domains
- **Medium Confidence**: The experimental results showing state-of-the-art performance on benchmark datasets are promising but limited in scope, with only a few datasets and comparison methods tested

## Next Checks

1. Verify permutation equivariance by applying the complete symmetric group to small test graphs and checking that P-tensor transformations follow the expected rules
2. Test the framework on additional benchmark datasets (e.g., OGB-LSC tasks) and compare against a broader range of GNN baselines
3. Implement ablation studies varying P-tensor orders and reference domain sizes to quantify their impact on performance and computational efficiency