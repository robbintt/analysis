---
ver: rpa2
title: Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of
  Language Models
arxiv_id: '2310.14491'
source_url: https://arxiv.org/abs/2310.14491
tags:
- reasoning
- probing
- attention
- llama
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LMs) perform
  multi-step reasoning by understanding the underlying process or by memorizing answers
  from training data. To answer this, the authors introduce a mechanistic probing
  method called MechanisticProbe that recovers reasoning trees from a model's attention
  patterns.
---

# Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models

## Quick Facts
- arXiv ID: 2310.14491
- Source URL: https://arxiv.org/abs/2310.14491
- Reference count: 40
- This paper investigates whether large language models perform multi-step reasoning by understanding underlying processes rather than memorizing answers.

## Executive Summary
This paper investigates whether large language models perform multi-step reasoning by understanding underlying processes rather than memorizing answers from training data. To answer this, the authors introduce a mechanistic probing method called MechanisticProbe that recovers reasoning trees from a model's attention patterns. They test their approach on GPT-2 for a synthetic task (finding the k-th smallest element) and on LLaMA for natural language reasoning tasks (ProofWriter and ARC). The results show that MechanisticProbe can detect reasoning trees from attention patterns in most cases, suggesting that LMs are indeed performing multi-step reasoning internally. Additionally, the study finds that LMs identify useful statements early in the process and then reason step-by-step through the architecture. Further analysis shows that LMs with higher probing scores tend to have better performance and robustness.

## Method Summary
The paper introduces MechanisticProbe, a method that recovers reasoning trees from language model attention patterns. The approach simplifies attention matrices by focusing on the last token, pooling across heads, and pruning irrelevant connections. A k-nearest neighbors classifier then identifies useful statements and reasoning steps from these simplified patterns. The method is applied to both synthetic reasoning tasks (k-th smallest element) using GPT-2 and natural language reasoning tasks (ProofWriter, ARC) using LLaMA, analyzing both in-context learning and finetuned versions.

## Key Results
- MechanisticProbe successfully recovers reasoning trees from attention patterns in most cases, suggesting LMs perform multi-step reasoning internally
- LMs identify useful statements early in processing layers and reason step-by-step through subsequent layers
- Models with higher probing scores show better performance and robustness on reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models perform multi-step reasoning by implicitly embedding a reasoning tree within their architecture, rather than memorizing answers.
- Mechanism: The model's attention patterns encode the structure of the reasoning tree, which can be recovered through a mechanistic probe that analyzes these patterns.
- Core assumption: Attention weights in transformer layers contain sufficient information about the reasoning process to reconstruct the underlying tree structure.
- Evidence anchors:
  - [abstract] "We test this hypothesis by introducing a new probing approach (called MechanisticProbe) that recovers the reasoning tree from the model's attention patterns."
  - [section 3] "We designed a probe model, MechanisticProbe, that recovers the reasoning tree from the LM's attention patterns."
  - [corpus] "Average neighbor FMR=0.486" indicates moderate relatedness of neighboring papers, suggesting the mechanistic interpretation approach is novel but grounded in existing attention analysis work.
- Break condition: If attention patterns do not consistently encode the reasoning tree structure across different tasks or model architectures, the probe would fail to recover accurate trees.

### Mechanism 2
- Claim: The model identifies useful statements early in the processing layers and then reasons step-by-step through subsequent layers.
- Mechanism: Bottom layers focus on selecting relevant information (statements) while middle layers determine the reasoning steps, as evidenced by layer-wise probing scores.
- Core assumption: Different layers of the transformer serve distinct functional roles in the reasoning process, with early layers handling information selection and later layers handling logical inference.
- Evidence anchors:
  - [section 4.3] "We notice that GPT-2FT quickly achieves high scores in initial layers and then, SP1 increases gradually. Observing SP2... we notice that GPT-2FT does not achieve high scores until layer 10."
  - [section 6] "If SP2 is small (less than 0.7), the prediction of LLaMA could be easily influenced by the noise (test accuracy decreases around 10%)."
  - [corpus] The corpus includes papers on "reasoning circuits" and "mechanistic interpretation," supporting the idea of layered reasoning processes.
- Break condition: If the model uses a different architectural pattern (e.g., parallel processing instead of sequential) or if attention patterns do not show clear layer-wise specialization, this mechanism would not hold.

### Mechanism 3
- Claim: Mechanistic reasoning in language models correlates with both performance and robustness on reasoning tasks.
- Mechanism: Models with higher probing scores (indicating better capture of the reasoning tree) show better accuracy and are more robust to input noise.
- Core assumption: The ability to correctly encode the reasoning process directly translates to better task performance and generalization.
- Evidence anchors:
  - [section 6] "We find that test accuracy is closely correlated with SP2. This implies that when we can successfully detect reasoning steps of useful statements from LM's attentions, the model is more likely to produce a correct prediction."
  - [section 6] "If SP2 is high, LLaMA is more robust, i.e., more confident in its correct prediction (test accuracy increases around 4%)."
  - [corpus] The corpus includes papers on "heuristic-guided problem solving" and "dynamic use of heuristics," suggesting that mechanistic approaches may enhance robustness.
- Break condition: If models with high probing scores do not consistently show better performance or robustness across different tasks, the correlation would break down.

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: The paper's core method relies on analyzing attention patterns to recover reasoning trees, so understanding how attention works in transformers is essential.
  - Quick check question: How do attention weights in a transformer layer determine which tokens influence the representation of a given token?

- Concept: Tree structures and graph representation of reasoning processes
  - Why needed here: The paper formalizes reasoning as tree structures and the probe aims to recover these trees, requiring understanding of tree/graph representations.
  - Quick check question: What distinguishes a tree structure from other graph structures, and why is a tree appropriate for representing step-by-step reasoning?

- Concept: Probing classifiers and their interpretation
  - Why needed here: The MechanisticProbe uses probing classifiers to measure information in attention patterns, and understanding probing methodology is crucial for interpreting results.
  - Quick check question: What is the difference between absolute probing accuracy and relative probing scores (comparing to random baselines), and why is the latter preferred here?

## Architecture Onboarding

- Component map: Input statements and question → Language model processing → Attention pattern generation → Attention simplification (focus on last token, pooling, pruning) → MechanisticProbe classification (useful statements, reasoning steps) → Tree reconstruction and correlation with performance
- Critical path: Input statements and question → Language model processing → Attention pattern generation → Attention simplification (focus on last token, pooling, pruning) → MechanisticProbe classification (useful statements, reasoning steps) → Tree reconstruction and correlation with performance
- Design tradeoffs: The probe simplifies attention matrices to make the problem tractable, but this may lose some information. The choice of kNN classifier keeps the probe simple but may miss complex patterns. Layer pruning improves efficiency but could remove useful information.
- Failure signatures: Low probing scores despite high task accuracy (suggesting memorization rather than reasoning), inconsistent layer-wise patterns, or poor correlation between probing scores and performance/robustness
- First 3 experiments:
  1. Run MechanisticProbe on a pretrained GPT-2 (without finetuning) on the k-th smallest element task to verify it detects no reasoning tree structure.
  2. Apply layer-wise probing to a finetuned GPT-2 to confirm bottom layers identify useful statements while top layers determine reasoning steps.
  3. Test correlation between probing scores and robustness by corrupting input statements and measuring accuracy changes across examples with different SP2 values.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do attention heads with different functions (e.g., position vs. size sensitive) contribute to mechanistic reasoning in language models?
- Basis in paper: [explicit] The paper discusses pruning attention heads based on their size and position entropy, showing that heads with small size entropy are essential for solving reasoning tasks while those with small position entropy are not.
- Why unresolved: The paper focuses on the overall effect of pruning heads but doesn't delve into the specific roles of different head types during the reasoning process.
- What evidence would resolve it: A detailed analysis of the attention patterns of different head types during the reasoning process, showing how they contribute to each step of the reasoning tree.

### Open Question 2
- Question: How does the difficulty of reasoning tasks (e.g., number of leaf nodes in the reasoning tree) affect the performance and robustness of language models?
- Basis in paper: [explicit] The paper explores the performance of GPT-2 on tasks with varying numbers of leaf nodes (k-th smallest element task) and finds that larger models can handle more complex reasoning trees, but smaller models struggle when the task becomes too difficult.
- Why unresolved: The paper doesn't investigate the relationship between task difficulty and the mechanistic reasoning process itself.
- What evidence would resolve it: A study analyzing the attention patterns and probing scores of language models on reasoning tasks with varying difficulty, correlating these findings with the model's performance and robustness.

### Open Question 3
- Question: Can mechanistic probing methods be extended to analyze more complex reasoning tasks with longer chains of reasoning or multiple-choice questions?
- Basis in paper: [explicit] The paper acknowledges that their analysis is limited to relatively simple reasoning tasks and mentions the potential for future work to explore more challenging tasks.
- Why unresolved: The current probing method is designed for simple reasoning trees and may not be suitable for analyzing more complex reasoning processes.
- What evidence would resolve it: Developing and testing new probing methods that can effectively analyze attention patterns in language models for more complex reasoning tasks, such as those involving longer chains of reasoning or multiple-choice questions.

## Limitations
- The analysis is primarily conducted on synthetic tasks and specific reasoning datasets, which may not generalize to broader reasoning capabilities
- The MechanisticProbe method relies on simplifying attention patterns through pooling and pruning, which could potentially discard relevant information
- The correlation between probing scores and performance, while suggestive, does not establish causation

## Confidence

**High confidence** in the methodology of MechanisticProbe and its ability to extract information from attention patterns

**Medium confidence** in the interpretation that recovered trees represent genuine reasoning processes rather than artifacts of the probing method

**Medium confidence** in the correlation between probing scores and performance/robustness, requiring further validation on diverse tasks

## Next Checks

1. Test MechanisticProbe on additional reasoning tasks (e.g., mathematical problem solving, commonsense reasoning) to assess generalizability across different reasoning domains

2. Compare results with alternative mechanistic probing methods (e.g., causal mediation analysis) to verify that the recovered trees represent genuine reasoning processes

3. Conduct ablation studies removing specific attention heads/layers identified as important to verify their actual contribution to reasoning performance