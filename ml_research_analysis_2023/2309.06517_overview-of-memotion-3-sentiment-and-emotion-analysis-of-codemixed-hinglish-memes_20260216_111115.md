---
ver: rpa2
title: 'Overview of Memotion 3: Sentiment and Emotion Analysis of Codemixed Hinglish
  Memes'
arxiv_id: '2309.06517'
source_url: https://arxiv.org/abs/2309.06517
tags:
- task
- memes
- memotion
- sentiment
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents the third iteration of the Memotion shared
  task, focusing on sentiment and emotion analysis of Hindi-English code-mixed memes.
  The task involved three sub-tasks: sentiment classification (positive, negative,
  neutral), emotion detection (humor, sarcasm, offense, motivation), and emotion intensity
  classification.'
---

# Overview of Memotion 3: Sentiment and Emotion Analysis of Codemixed Hinglish Memes

## Quick Facts
- arXiv ID: 2309.06517
- Source URL: https://arxiv.org/abs/2309.06517
- Reference count: 40
- Primary result: 34.41% F1 for sentiment analysis, 79.77% for emotion detection, 59.82% for emotion intensity classification

## Executive Summary
The Memotion 3 shared task focused on sentiment and emotion analysis of Hindi-English code-mixed memes across three tasks: sentiment classification, emotion detection, and emotion intensity classification. The dataset contained 10,000 annotated memes with a 7000-1500-1500 train-validation-test split. Top-performing teams employed multi-modal fusion techniques combining text and image embeddings from pre-trained models like CLIP, BERT, and ViT. While significant progress was made particularly in emotion detection, results indicate substantial room for improvement, especially for sentiment analysis and sarcasm/humor detection in code-mixed content.

## Method Summary
The task involved analyzing Hindi-English code-mixed memes using multi-modal fusion approaches that combined visual features from pre-trained image encoders (ViT/CLIP) with textual features from Hinglish-BERT variants. Teams implemented various fusion strategies including concatenation, attention mechanisms, and ensemble methods. The models were trained to classify sentiment (positive, negative, neutral), detect emotions (humor, sarcasm, offense, motivation), and classify emotion intensity levels. Pre-training on related meme datasets and code-mixed corpora was also explored to improve performance.

## Key Results
- Sentiment analysis achieved 34.41% weighted F1 score
- Emotion detection reached 79.77% weighted F1 score
- Emotion intensity classification scored 59.82% weighted F1 score
- Multi-modal fusion significantly outperformed single-modality approaches
- Sarcasm and humor detection remained particularly challenging, especially in code-mixed memes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal fusion of pre-trained image and text embeddings significantly improves meme sentiment and emotion classification performance.
- Mechanism: Combines visual features from ViT/CLIP with textual features from multilingual BERT variants through concatenation or learned weighted combinations, then passes through MLP for classification.
- Core assumption: Visual and textual modalities contain complementary information that provides richer representations when properly fused.
- Evidence anchors: Teams used CLIP, BERT modifications, and ViT; multi-modal data requires considering both visual and textual properties for memes.
- Break condition: Poor fusion alignment or weighting, or one modality being much noisier than the other.

### Mechanism 2
- Claim: Code-mixed content presents unique challenges requiring specialized pre-training or adaptation.
- Mechanism: Standard monolingual models struggle with informal language mixing; Hinglish-BERT or models incorporating code-mixing indices better handle these challenges.
- Core assumption: Code-mixed text has distinct linguistic patterns requiring understanding of both languages and mixing patterns.
- Evidence anchors: Code-mixed language processing is challenging due to informal mixing; difficult examples predominantly have code-mixed text.
- Break condition: Insufficient code-mixed examples during training or overly diverse mixing patterns.

### Mechanism 3
- Claim: Multi-task learning and knowledge transfer from related datasets can improve meme emotion analysis.
- Mechanism: Joint training on related tasks or pre-training on large meme datasets like Facebook Hateful Memes enables learning general meme understanding that transfers to specific tasks.
- Core assumption: Related tasks and datasets share common underlying features enabling better generalization.
- Evidence anchors: Teams used Facebook Hateful memes and MMHS150k for pre-training; CEC helps leverage task predictions through joint training.
- Break condition: Source tasks/datasets too dissimilar or model overfits to source data.

## Foundational Learning

- Concept: Multi-modal representation learning
  - Why needed here: Memes require understanding both visual and textual content to capture full meaning, especially for nuanced emotions like sarcasm and humor.
  - Quick check question: What are the key challenges in aligning and fusing visual and textual embeddings for meme analysis?

- Concept: Code-mixing in NLP
  - Why needed here: Dataset contains Hinglish memes requiring models to handle informal language mixing and spelling variations.
  - Quick check question: How does code-mixing affect tokenization, vocabulary size, and model performance compared to monolingual text?

- Concept: Emotion intensity classification
  - Why needed here: Task C requires detecting emotions and classifying their intensity levels, more challenging than binary classification.
  - Quick check question: What are main differences between emotion classification and intensity classification, and how do they affect model design?

## Architecture Onboarding

- Component map: Pre-trained image encoder (ViT/CLIP) → Pre-trained text encoder (Hinglish-BERT) → Fusion module (concatenation/attention) → MLP classifier → Output layer
- Critical path: Text and image feature extraction → Fusion → Classification. Any bottleneck in these stages directly impacts performance.
- Design tradeoffs: Balancing model complexity vs. dataset size, choosing fusion strategies (concatenation vs. attention), deciding on pre-training datasets.
- Failure signatures: Low performance on code-mixed examples, inability to distinguish sarcasm/humor, poor intensity classification, overfitting to training data.
- First 3 experiments:
  1. Test individual modalities (image-only, text-only) to establish baseline performance and identify which modality is more informative.
  2. Test different fusion strategies (concatenation, attention, gated fusion) to find best approach for combining modalities.
  3. Fine-tune pre-trained models on Memotion dataset vs. using as frozen feature extractors to determine optimal adaptation level.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal fusion techniques be improved for better sentiment and emotion analysis of code-mixed memes?
- Basis in paper: Teams presented novel model pipelines using pre-trained embeddings, but best emotion intensity F1 is only 59.82%.
- Why unresolved: Current fusion methods have limitations in capturing complex interactions between text and image modalities.
- What evidence would resolve it: New fusion architectures outperforming state-of-the-art on Memotion 3 dataset, particularly for emotion intensity and sarcasm detection.

### Open Question 2
- Question: What specific linguistic and cultural factors contribute to difficulty in detecting sarcasm and humor in Hindi-English code-mixed memes?
- Basis in paper: Sarcasm and humor are difficult to identify, especially in code-mixed memes, with most misclassified examples having code-mixed text.
- Why unresolved: Paper lacks detailed analysis of linguistic and cultural factors making sarcasm/humor detection challenging.
- What evidence would resolve it: Comprehensive linguistic and cultural analysis identifying specific patterns and features contributing to detection difficulty.

### Open Question 3
- Question: How can pre-training strategies be optimized for code-mixed meme analysis tasks?
- Basis in paper: Teams used various pre-trained models and datasets, but effectiveness for code-mixed meme analysis is not explicitly discussed.
- Why unresolved: Paper doesn't provide detailed comparison of different pre-training strategies or their impact on performance.
- What evidence would resolve it: Systematic evaluation of different pre-training strategies including code-mixed data, meme-specific datasets, and multi-task learning on Memotion 3 dataset.

## Limitations
- Performance metrics indicate substantial room for improvement, particularly for sentiment analysis at 34.41% F1
- Weak evidence anchors suggest many claims about multi-modal fusion effectiveness lack external literature support
- Limited dataset size (10,000 memes) may not capture full diversity of code-mixed expressions and cultural contexts
- Lack of detailed implementation specifics from top teams creates uncertainty about exact success mechanisms

## Confidence

**High Confidence:**
- Multi-modal fusion framework validity for meme analysis
- Code-mixing challenges requiring specialized approaches
- Three-task structure (sentiment, emotion detection, intensity classification) methodological soundness

**Medium Confidence:**
- Specific performance numbers for top models due to undisclosed implementation details
- Effectiveness of particular fusion strategies due to limited comparative analysis
- Generalizability to other code-mixed language pairs beyond Hinglish

**Low Confidence:**
- Claims about knowledge transfer from related datasets due to insufficient evidence
- Specific hyperparameter choices and architectural decisions by winning teams
- Impact of OCR preprocessing quality on final model performance

## Next Checks

1. **Ablation study on modality importance**: Systematically evaluate model performance with only text features, only image features, and various fusion strategies to quantify each modality's contribution and identify optimal fusion approaches.

2. **Code-mixing robustness evaluation**: Create controlled experiments testing model performance on memes with varying code-mixing intensity, formal vs. informal language, and different language dominance patterns to identify specific failure modes.

3. **Cross-dataset generalization test**: Evaluate top-performing models from Memotion 3 on related meme datasets (Facebook Hateful Memes, MMHS150k) to assess knowledge transfer effectiveness and distinguish domain-specific vs. general meme understanding capabilities.