---
ver: rpa2
title: Can LLM-Generated Misinformation Be Detected?
arxiv_id: '2309.13788'
source_url: https://arxiv.org/abs/2309.13788
tags:
- arxiv
- misinformation
- preprint
- generation
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential for LLM-generated misinformation
  to cause more harm than human-written misinformation by examining the difficulty
  of detection. The authors build a taxonomy of LLM-generated misinformation and validate
  real-world methods for generating such content, including Hallucination Generation,
  Arbitrary Misinformation Generation, and Controllable Misinformation Generation.
---

# Can LLM-Generated Misinformation Be Detected?

## Quick Facts
- **arXiv ID**: 2309.13788
- **Source URL**: https://arxiv.org/abs/2309.13788
- **Reference count**: 40
- **Primary result**: LLM-generated misinformation is harder to detect for both humans and detectors compared to human-written misinformation with the same semantics.

## Executive Summary
This paper investigates whether LLM-generated misinformation poses a greater threat than human-written misinformation by examining detection difficulty. Through controlled experiments, the authors demonstrate that misinformation generated by ChatGPT is significantly harder for humans to detect (only 9.6% success rate) compared to human-written misinformation with equivalent semantic content. While LLM detectors like GPT-4 outperform humans on detecting ChatGPT-generated misinformation, they still perform worse on LLM-generated content than on human-written misinformation. The study reveals that LLM-generated misinformation exhibits distinct stylistic features that increase its deceptive potential, suggesting it could cause more harm in real-world scenarios.

## Method Summary
The study employs a multi-pronged approach to generate and evaluate LLM-generated misinformation. Three methods are used: Hallucination Generation (LLM generates misinformation from scratch), Arbitrary Misinformation Generation (LLM rewrites existing misinformation with arbitrary changes), and Controllable Misinformation Generation (LLM manipulates specific aspects of misinformation). Human evaluators from Amazon Mechanical Turk assess credibility of news items, while LLMs (ChatGPT-3.5, GPT-4, Llama2) serve as zero-shot detectors. Detection success rates are compared between LLM-generated and human-written misinformation with matching semantics. Semantic embeddings and word cloud analysis help characterize stylistic differences between the two types of misinformation.

## Key Results
- Humans successfully detect only 9.6% of hallucinated news generated by ChatGPT
- GPT-4 outperforms humans on detecting ChatGPT-generated misinformation
- LLM-generated misinformation exhibits distinct stylistic features that increase detection difficulty
- Detection performance is consistently worse for LLM-generated misinformation compared to human-written misinformation with same semantics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM-generated misinformation is harder to detect for both humans and detectors compared to human-written misinformation with the same semantics.
- **Mechanism**: LLMs can manipulate stylistic features (tone, formality, lexical choice) while preserving semantic content, making the text appear more authoritative or neutral. This stylistic manipulation increases the cognitive load on human evaluators and confounds detector models trained on stylistic cues from human-authored misinformation.
- **Core assumption**: The semantic similarity between LLM-generated and human-written misinformation is sufficient for fair comparison of detection difficulty, and the stylistic divergence is statistically significant.
- **Evidence anchors**:
  - [abstract]: "LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm."
  - [section 4]: Latent space visualization (T-SNE) shows majority overlap between LLM-generated and human-written misinformation in semantic embeddings, but Word Cloud analysis reveals different frequent word distributions indicating stylistic divergence.
  - [corpus]: No strong corpus evidence directly supporting stylistic manipulation; only weak signals from related work on "detecting LLM-generated texts" without emphasis on stylistic deception.

### Mechanism 2
- **Claim**: Humans can be more susceptible to LLM-generated misinformation than human-written misinformation.
- **Mechanism**: LLM-generated misinformation benefits from the "authority bias" induced by the formal, calm, and informative style that LLMs can adopt. Humans may unconsciously assign higher credibility to text that appears more neutral and professional, even if the underlying content is false.
- **Core assumption**: Human evaluators rely on surface-level stylistic cues when assessing credibility, and these cues can be manipulated by LLMs to create a false sense of trustworthiness.
- **Evidence anchors**:
  - [abstract]: "LLM-generated misinformation can be harder for humans to detect than human-written misinformation with the same semantics."
  - [section 5.2]: Human evaluators successfully spotted only 9.6% of hallucinated news, while detection rates for human-written misinformation were higher in comparable semantic categories. This suggests a detection gap attributable to style.
  - [corpus]: No corpus evidence directly measuring susceptibility bias; related work focuses on "detection difficulty" rather than susceptibility per se.

### Mechanism 3
- **Claim**: LLM detectors (e.g., GPT-4) can outperform humans on detecting LLM-generated misinformation but still perform worse than on human-written misinformation.
- **Mechanism**: LLMs used as detectors may share similar training distributions and linguistic patterns with the LLMs generating misinformation, enabling better recognition of subtle stylistic artifacts or model-specific quirks. However, the detectors still struggle because LLM-generated misinformation preserves human-like semantics and can mimic diverse styles.
- **Core assumption**: LLM detectors have internal representations that capture the distributional properties of LLM-generated text, giving them an edge over humans who lack this model-specific knowledge.
- **Evidence anchors**:
  - [abstract]: "GPT-4 can outperform humans on detecting ChatGPT-generated misinformation, though humans can still perform better than ChatGPT-3.5."
  - [section 6.2]: GPT-4 detection performance on LLM-generated misinformation is higher than human performance, but detection of human-written misinformation remains easier, suggesting a relative advantage for LLM detectors.
  - [corpus]: No strong corpus evidence supporting model-specific detection; related work on "LLM detectors" focuses on zero-shot detection without emphasizing inter-model advantages.

## Foundational Learning

- **Concept**: Semantic vs. Stylistic Features in Text Classification
  - Why needed here: The study hinges on the idea that LLM-generated misinformation preserves semantics but alters style, affecting detection. Understanding how models separate semantic content from stylistic markers is critical for interpreting the results.
  - Quick check question: In a binary classification task, if two texts have identical word embeddings but different part-of-speech distributions, which feature set is more likely to drive the classifier's decision?

- **Concept**: Zero-Shot vs. Few-Shot Learning in LLM Detection
  - Why needed here: The experiments use LLMs (GPT-4, ChatGPT-3.5) as detectors without fine-tuning. Knowing how zero-shot reasoning differs from few-shot or fine-tuned approaches explains why LLM detectors may outperform humans but still underperform on human-written misinformation.
  - Quick check question: What is the primary difference between prompting an LLM with "Is this misinformation? YES/NO" versus providing a short example of correct/incorrect classification before the prompt?

- **Concept**: Human Evaluation Bias in Misinformation Detection
  - Why needed here: The human evaluation results show low detection rates, suggesting susceptibility to stylistic manipulation. Recognizing common cognitive biases (authority bias, fluency bias) helps interpret why humans struggle more with LLM-generated misinformation.
  - Quick check question: If a text is written in a calm, formal tone but contains false claims, how might human evaluators' perception of credibility differ from their perception of an informal but factually accurate text?

## Architecture Onboarding

- **Component map**: Data Generation Pipeline -> Human Evaluation Module -> LLM Detector Module -> Analysis Layer
- **Critical path**: Generate misinformation (Hallucination, Arbitrary, Controllable methods) -> Human evaluation (success rate) -> LLM detection (success rate) -> Compare performance -> Infer detection difficulty difference
- **Design tradeoffs**:
  - Using Politifact dataset for human-written misinformation ensures realistic semantic overlap but limits domain diversity.
  - Random sampling of ChatGPT-generated data avoids semantic overlap but may introduce sampling bias.
  - Standard vs. Chain-of-Thought prompting affects detector consistency; CoT may yield more reliable but slower responses.
- **Failure signatures**:
  - Human success rate close to random guessing (50%) -> Suggests task is too difficult or prompts are unclear.
  - LLM detector success rate near zero -> Indicates stylistic manipulation is too effective or detector is not sensitive to LLM artifacts.
  - No performance gap between human and LLM detectors -> Suggests no meaningful difference in detection difficulty.
- **First 3 experiments**:
  1. Generate 100 hallucinated news items using the Hallucination Generation method and run human evaluation; expect very low success rate (~10%).
  2. Use GPT-4 with Chain-of-Thought prompting to detect the same 100 hallucinated items; compare success rate to human performance to test Mechanism 3.
  3. Generate 50 items each via Paraphrase, Rewriting, and Open-ended methods from Politifact nonfactual passages; run human and GPT-4 evaluations; analyze Word Cloud differences to test Mechanism 1.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can misinformation generated by smaller LLMs (e.g., 1-10B parameters) also evade detection by humans and existing detectors?
- **Basis in paper**: [inferred] The authors suggest future work to verify findings on LLMs with smaller parameter sizes.
- **Why unresolved**: The paper only tested ChatGPT and Llama2 models, leaving uncertainty about the generalizability to smaller models.
- **What evidence would resolve it**: Systematic experiments comparing human detection performance and detector accuracy on misinformation generated by small, medium, and large LLMs.

### Open Question 2
- **Question**: Are there more effective detection methods than LLMs for identifying LLM-generated misinformation, especially for complex styles like rewriting and open-ended generation?
- **Basis in paper**: [explicit] The authors note that existing detectors are less effective on LLM-generated misinformation and call for more research on detection methods.
- **Why unresolved**: The paper only evaluated a limited number of detectors and prompts, not exploring the full space of detection techniques.
- **What evidence would resolve it**: Head-to-head comparisons of LLM detectors vs. traditional machine learning and hybrid approaches on diverse datasets of LLM-generated misinformation.

### Open Question 3
- **Question**: What are the most promising countermeasures through the LLM lifecycle to prevent and detect LLM-generated misinformation at scale?
- **Basis in paper**: [explicit] The authors discuss potential countermeasures at training, inference, and influence stages but note more research is needed, especially for inference-time detection.
- **Why unresolved**: The paper provides a high-level overview but does not rigorously evaluate the effectiveness of specific countermeasures.
- **What evidence would resolve it**: Empirical studies testing the efficacy of data curation, alignment training, prompt filtering, retrieval augmentation, and detection methods against realistic misinformation generation attacks.

## Limitations
- Controlled experimental conditions may not fully capture real-world complexity of misinformation detection scenarios
- Small human evaluation sample size (10 evaluators) and limited dataset (100 news items per method) constrain generalizability
- Assumption that stylistic manipulation is primary driver of detection difficulty may overlook other important factors

## Confidence
- **High confidence**: The finding that LLM-generated misinformation is harder to detect for both humans and detectors compared to human-written misinformation, supported by empirical detection success rates and semantic embedding analysis.
- **Medium confidence**: The claim that humans are more susceptible to LLM-generated misinformation due to stylistic manipulation, based on human evaluation results but limited by sample size and potential evaluator bias.
- **Medium confidence**: The assertion that LLM detectors can outperform humans on detecting LLM-generated misinformation but still perform worse than on human-written misinformation, supported by zero-shot detection experiments but potentially influenced by prompt engineering variations.

## Next Checks
1. **Expand human evaluation sample size**: Increase the number of human evaluators from 10 to 50 and collect at least 500 news items per misinformation generation method to improve statistical power and generalizability of susceptibility findings.

2. **Test domain-specific detection**: Evaluate detection performance on domain-specific misinformation (e.g., medical or political news) to determine if stylistic manipulation advantages persist across specialized knowledge domains.

3. **Analyze temporal detection stability**: Measure detection success rates over multiple time periods to assess whether LLM-generated misinformation detection difficulty remains consistent or evolves as detection methods improve.