---
ver: rpa2
title: Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking
arxiv_id: '2311.18817'
source_url: https://arxiv.org/abs/2311.18817
tags:
- init
- training
- grokking
- have
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the "grokking" phenomenon in neural network
  training, where a network first memorizes training data but then suddenly transitions
  to perfect generalization after extended training. The authors provide a theoretical
  explanation for this behavior by analyzing homogeneous neural networks with large
  initialization and small weight decay.
---

# Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking

## Quick Facts
- arXiv ID: 2311.18817
- Source URL: https://arxiv.org/abs/2311.18817
- Reference count: 40
- This paper provides the first rigorous theoretical explanation for the "grokking" phenomenon in neural networks by analyzing the dichotomy between early and late phase implicit biases under large initialization and small weight decay.

## Executive Summary
This paper explains the grokking phenomenon—where neural networks first memorize training data but then suddenly generalize after extended training—through a theoretical analysis of implicit biases in homogeneous neural networks. The authors show that large initialization induces a kernel regime in the early phase, while small weight decay creates a bias toward maximum-margin/minimum-norm solutions in the late phase. This dichotomy leads to sharp transitions between memorization and generalization, with the timing determined by the initialization scale and weight decay strength.

## Method Summary
The paper analyzes gradient flow on homogeneous neural networks with large initialization (α) and small weight decay (λ). It proves that the large initialization initially dominates, causing the network to behave like a kernel predictor (NTK regime) and memorize training data. As training progresses and the initialization decays, the small weight decay induces a bias toward margin maximization (classification) or norm minimization (regression). The transition occurs around time 1/λ log α, with sharp changes in test accuracy when the kernel regime solution differs significantly from the margin/norm minimization solution.

## Key Results
- Proves sharp transitions between kernel regime and max-margin/norm minimization regimes for both classification and regression tasks
- Demonstrates grokking occurs when early-phase kernel solutions are suboptimal compared to late-phase rich regime solutions
- Provides quantitative bounds on transition timing (around 1/λ log α) for homogeneous networks
- Validates theory with concrete examples including sparse linear classification and low-rank matrix completion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large initialization induces a kernel regime where the network behaves like a linearized model, leading to memorization
- Mechanism: When training begins with large initialization, the weight decay is initially negligible compared to the initialization scale, causing the network to remain in a "lazy" regime where parameters move slowly relative to their initial values
- Core assumption: Network parameters remain close to their initial direction during early phase (∥e^(λt)/α * θ(t) - θ_init∥ remains small)
- Evidence anchors: Abstract mentions "large initialization induces a very strong early phase implicit bias towards kernel predictors"; section 3.2.1 states "gradient flow gets stuck in the kernel regime over the initial period"
- Break condition: When weight decay becomes comparable to initialization scale, causing significant parameter direction change

### Mechanism 2
- Claim: Small weight decay creates a late phase bias toward maximum-margin/minimum-norm solutions
- Mechanism: As training progresses, large initialization decays and small weight decay dominates, inducing implicit bias toward solutions that maximize margin (classification) or minimize parameter norm (regression)
- Core assumption: Gradient flow converges to KKT point of margin maximization or norm minimization problem in late phase
- Evidence anchors: Abstract mentions "small weight decay induces a bias toward minimum-norm/maximum-margin solutions"; section 3.2.2 states "continuing the gradient flow for a slightly long time... makes efforts to maximize the margin"
- Break condition: When regularization strength becomes too large, overwhelming primary loss optimization

### Mechanism 3
- Claim: Sharp transition occurs because early and late phase biases lead to fundamentally different solutions
- Mechanism: Kernel regime solution (early phase) and margin/norm minimization solution (late phase) can be vastly different, causing dramatic test accuracy changes when network transitions between regimes
- Core assumption: Solutions in kernel regime and rich regime are sufficiently different that test accuracy change appears sharp
- Evidence anchors: Abstract mentions "a very sharp transition to min-norm/max-margin predictors occurs, leading to a dramatic change in test accuracy"; section 3.2.3 states "our theory predicts that the grokking phenomenon can be observed" due to mismatch between early and late phase solutions
- Break condition: When initialization scale-weight decay ratio is adjusted such that transition becomes gradual

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) regime
  - Why needed here: Understanding why large initialization leads to kernel-like behavior is central to explaining early phase of grokking
  - Quick check question: Why does a network with large initialization behave like a linearized model during training?

- Concept: Implicit bias in optimization
  - Why needed here: Paper's core argument relies on understanding how optimization algorithms induce biases toward certain types of solutions
  - Quick check question: How does gradient descent on homogeneous neural networks implicitly maximize margins even without explicit regularization?

- Concept: KKT conditions and margin maximization
  - Why needed here: Late phase solution is characterized as KKT point of margin maximization problem, requiring understanding of constrained optimization
  - Quick check question: What are the KKT conditions for maximum-margin classifier, and why do they characterize optimal solutions?

## Architecture Onboarding

- Component map:
  - Initialization scale (α) -> Controls early phase kernel regime strength
  - Weight decay (λ) -> Controls late phase margin/norm minimization bias
  - Learning rate -> Should be small enough to approximate gradient flow dynamics
  - Model architecture -> Must be homogeneous (e.g., MLPs, CNNs with ReLU)

- Critical path:
  1. Initialize with large α to enter kernel regime
  2. Train with small λ to maintain kernel behavior initially
  3. Allow sufficient time for transition to late phase
  4. Monitor test accuracy for sharp transition

- Design tradeoffs:
  - Larger α delays transition but strengthens early phase kernel regime
  - Smaller λ delays transition but strengthens late phase margin bias
  - Too large initialization can cause numerical instability
  - Too small weight decay can make transition too gradual to observe

- Failure signatures:
  - No grokking observed: Initialization too small or weight decay too large
  - Gradual transition instead of sharp: Initialization scale-weight decay ratio not properly tuned
  - Training collapse: Weight decay too strong relative to initialization scale

- First 3 experiments:
  1. Modular addition with varying initialization scales (α = 1, 10, 100) to observe transition timing
  2. Sparse linear classification to verify L1-margin maximization generalizes better than L2-margin
  3. Matrix completion to demonstrate how kernel regime fails while rich regime succeeds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the grokking transition be made sharp for classification tasks beyond diagonal linear networks?
- Basis in paper: The paper proves sharp transitions for diagonal linear networks but conjectures similar results may hold for general homogeneous neural networks
- Why unresolved: Proof technique relies heavily on structure of diagonal linear networks and does not directly generalize to arbitrary homogeneous networks
- What evidence would resolve it: Proof showing gradient flow on homogeneous neural networks with large initialization and small weight decay exhibits sharp transitions from kernel regime to max-margin solutions

### Open Question 2
- Question: Under what conditions does "misgrokking" occur and can it be predicted?
- Basis in paper: The paper constructs concrete example of misgrokking where early-phase implicit bias leads to good generalization but late-phase bias causes catastrophic forgetting
- Why unresolved: Paper only provides one example and does not characterize broader conditions under which misgrokking occurs
- What evidence would resolve it: Characterization theorem describing when early-phase and late-phase implicit biases will lead to misgrokking, possibly based on properties of data distribution

### Open Question 3
- Question: Can the sharp transition time bounds be improved to be tight?
- Basis in paper: Paper provides bounds of the form 1/λ log α for transition time but notes these may not be tight
- Why unresolved: Analysis uses conservative upper bounds and does not provide matching lower bounds for transition time
- What evidence would resolve it: Either matching lower bound proof showing transition cannot occur faster than 1/λ log α, or improved upper bound showing transition happens earlier

### Open Question 4
- Question: How does choice of loss function affect the grokking phenomenon?
- Basis in paper: Paper analyzes exponential loss for classification but mentions logistic loss typically leads to same implicit bias
- Why unresolved: Paper does not explore other loss functions like hinge loss or mean squared error in depth
- What evidence would resolve it: Analysis showing how different loss functions change implicit bias trajectory and whether they preserve or eliminate grokking phenomenon

## Limitations

- The analysis assumes idealized gradient flow rather than practical discrete SGD, which may not capture real optimization dynamics
- Dichotomy between early and late phase biases relies on precise tuning of initialization scale and weight decay, difficult to achieve in practice
- Theory predicts sharp transitions but real grokking phenomena often exhibit more gradual behavior

## Confidence

- **High**: Characterization of early phase kernel regime behavior is well-established and mathematically rigorous
- **Medium**: Late phase margin/norm minimization bias follows from standard implicit bias theory for homogeneous networks
- **Medium**: Explanation of why kernel regime solutions fail while rich regime solutions succeed in specific examples (sparse classification, matrix completion)

## Next Checks

1. Test whether predicted transition time (around 1/λ log α) matches empirical observations across different architectures and tasks
2. Verify that kernel regime solution indeed corresponds to suboptimal generalization solution by comparing with explicit kernel methods
3. Investigate whether sharp transition property holds when using practical optimizers (SGD with momentum) rather than idealized gradient flow