---
ver: rpa2
title: 'Take Your Pick: Enabling Effective Personalized Federated Learning within
  Low-dimensional Feature Space'
arxiv_id: '2307.13995'
source_url: https://arxiv.org/abs/2307.13995
tags:
- features
- uni00000013
- feature
- uni00000046
- fedpick
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses statistical heterogeneity in federated learning
  (FL), specifically the cross-domain setting where clients' data distributions differ
  significantly. The key observation is that the global encoder, shared across clients,
  produces redundant features that contain irrelevant components for individual client
  tasks.
---

# Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space

## Quick Facts
- arXiv ID: 2307.13995
- Source URL: https://arxiv.org/abs/2307.13995
- Reference count: 40
- Primary result: FedPick achieves 89.60% accuracy on Digits-Five, outperforming FedAvg (80.93%) and FedBN (83.53%).

## Executive Summary
This paper addresses statistical heterogeneity in federated learning by proposing FedPick, a personalized FL framework that adaptively selects task-relevant features from a global encoder's output. The key insight is that universal features generated by the global encoder often contain redundant components irrelevant to individual client tasks. FedPick uses a Personalized Feature Selection Module (PFSM) with Gumbel-Sigmoid reparameterization to make feature selection differentiable, enabling end-to-end training. Experimental results on three cross-domain datasets demonstrate significant performance improvements over existing FL methods.

## Method Summary
FedPick addresses cross-domain federated learning by sharing a global encoder across clients while localizing a Personalized Feature Selection Module (PFSM) on each client. The PFSM uses Gumbel-Sigmoid reparameterization to select task-relevant features from the global encoder's output based on local data distribution. The method combines global and personalized classifiers through ensemble prediction and incorporates cyclic distillation between them. Training uses federated averaging with local SGD, cross-entropy loss for both classifiers, entropy loss for irrelevant features, and KL distillation loss. The approach achieves personalization in the low-dimensional feature space rather than the high-dimensional parameter space.

## Key Results
- FedPick achieves 89.60% accuracy on Digits-Five dataset, outperforming FedAvg (80.93%) and FedBN (83.53%)
- The method significantly improves performance across three cross-domain datasets: Digits-Five, Office-Caltech-10, and DomainNet
- Feature sparsity analysis shows FedPick effectively identifies and utilizes task-relevant features while suppressing irrelevant ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The global encoder generates features that are universally applicable but redundant for individual client tasks due to domain gaps.
- Mechanism: By extracting features that work across all domains, the global encoder inevitably includes components that are irrelevant or even harmful to a specific client's local task. These redundant components dilute the discriminative power of the features for that client.
- Core assumption: The universal features produced by the global encoder encompass numerous components that are irrelevant to the local task of a certain client.
- Evidence anchors:
  - [abstract] "The universal features produced by the global encoder largely encompass numerous components irrelevant to a certain client's local task."
  - [section] "Due to the domain gaps across clients in cross-domain FL, the global encoder tends to extract universal features that are applicable for different domains simultaneously. These universal features often encompass numerous components that are irrelevant to the local task on each client."
- Break condition: If the domain gaps between clients are negligible or the local tasks are sufficiently similar to the global task.

### Mechanism 2
- Claim: FedPick's Personalized Feature Selection Module (PFSM) adaptively selects task-relevant features for each client, improving model performance.
- Mechanism: The PFSM uses Gumbel-Sigmoid reparameterization to make feature selection differentiable. It evaluates the overall quality of the feature subset rather than individual components, thereby alleviating the drawback of vanilla feature selection approaches. This allows the model to focus on features that are most relevant to the local task of each client.
- Core assumption: The feature space exhibits a lower dimensionality, providing greater intuitiveness and interpretability compared to the parameter space.
- Evidence anchors:
  - [abstract] "FedPick achieves PFL in the low-dimensional feature space by selecting task-relevant features adaptively for each client from the features generated by the global encoder based on its local data distribution."
  - [section] "In FedPick, the global features zğ‘” are first fed into a FC network, denoted as Ï†, to generate the unbounded logits zğ‘™. Subsequently, the Gumbel-Sigmoid reparameterization technique is employed to generate a soft mask denoted as mğ‘ ."
- Break condition: If the Gumbel-Sigmoid reparameterization fails to make feature selection differentiable or if the FC network fails to generate meaningful logits.

### Mechanism 3
- Claim: Knowledge transfer between global and personalized features through cyclic distillation enhances both generalization and personalization abilities.
- Mechanism: FedPick incorporates cyclic distillation between the predictions from global and personalized features. This ensures the balance between the model's generalization and personalization abilities across different data distributions. The global features provide generalization, while the personalized features provide customization for the local task.
- Core assumption: The global features and personalized features offer distinct benefits, with global features possessing higher generalization and personalized features enhancing local performance.
- Evidence anchors:
  - [abstract] "FedPick combines the predictions of the global and personalized classifiers through an ensemble approach, resulting in a more robust prediction."
  - [section] "Motivated by knowledge distillation, FedPick incorporates cyclic distillation between the predictions from global and personalized features to facilitate mutual knowledge transfer."
- Break condition: If the distillation process fails to effectively transfer knowledge or if the ensemble approach does not improve prediction robustness.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: FL is the underlying framework that enables multiple clients to collaboratively train a model without sharing their raw data. Understanding FL is crucial for grasping the problem FedPick aims to solve and the context in which it operates.
  - Quick check question: What is the primary challenge that FedPick addresses within the context of federated learning?

- Concept: Domain Adaptation
  - Why needed here: Domain adaptation techniques are relevant because FedPick aims to adapt the global features to the local data distribution of each client. Understanding domain adaptation helps in appreciating the motivation behind FedPick's feature selection approach.
  - Quick check question: How does FedPick's approach to feature selection relate to the broader field of domain adaptation?

- Concept: Gumbel-Sigmoid Reparameterization
  - Why needed here: This technique is the key to making feature selection differentiable in FedPick. Understanding its mechanics is essential for comprehending how FedPick trains its Personalized Feature Selection Module (PFSM).
  - Quick check question: What problem does the Gumbel-Sigmoid reparameterization solve in the context of FedPick's feature selection process?

## Architecture Onboarding

- Component map:
  Global Encoder -> PFSM -> Personalized Feature Selection -> Global Classifier and Personalized Classifier -> Ensemble Prediction

- Critical path:
  1. Raw data is passed through the global encoder to generate universal features.
  2. Universal features are fed into the PFSM to select task-relevant features.
  3. Both universal features and task-relevant features are passed into the global and personalized classifiers for prediction.
  4. Predictions from global and personalized classifiers are combined through an ensemble approach.

- Design tradeoffs:
  - Sharing vs. personalizing components: FedPick shares the global encoder and classifier while localizing the PFSM and BN layers. This tradeoff balances generalization and personalization.
  - Hard vs. soft feature selection: FedPick uses hard masking during the forward pass and sigmoid during the backward pass. This tradeoff allows for differentiable feature selection while maintaining discrete selection during inference.

- Failure signatures:
  - Poor performance on individual clients: If the PFSM fails to select task-relevant features effectively, the model's performance on individual clients may suffer.
  - Reduced generalization: If the global features are not sufficiently general, the model may not generalize well to unseen data distributions.

- First 3 experiments:
  1. Evaluate the sparsity ratio of features generated by FedPick compared to other FL methods on a cross-domain dataset.
  2. Assess the impact of different hyperparameters (ğœ, ğœ†ğ‘™ğ‘ğ‘’, ğœ†ğ‘’ğ‘›ğ‘¡, ğœ†ğ‘‘ğ‘–ğ‘ ) on FedPick's performance.
  3. Analyze the overlapped ratio of important features between different domains to understand the effectiveness of FedPick's feature selection.

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the paper.

## Limitations
- The effectiveness of feature selection heavily depends on the assumption that domain gaps are large enough to create truly redundant features in the global encoder output. In cases where domain gaps are minimal, the benefits of FedPick may diminish significantly.
- The paper does not provide ablation studies isolating the impact of cyclic distillation versus the feature selection mechanism itself, making it difficult to quantify their individual contributions.
- While results show improved accuracy, there is no analysis of the computational overhead introduced by the additional PFSM module and distillation process compared to baseline methods.

## Confidence
- High confidence: The core mechanism of using Gumbel-Sigmoid for differentiable feature selection in FL setting, and the experimental methodology showing improved accuracy over baselines.
- Medium confidence: The claim that FedPick achieves better balance between generalization and personalization through cyclic distillation, as the ablation studies are not comprehensive enough.
- Low confidence: The assertion that the feature space is inherently more intuitive and interpretable than parameter space for personalization, as this is not empirically validated in the paper.

## Next Checks
1. Implement an ablation study that removes the cyclic distillation component while keeping the feature selection mechanism intact, to isolate their individual contributions to performance improvement.
2. Test FedPick's performance on a dataset with minimal domain gaps (e.g., different subsets of the same domain) to verify the claim that it only provides benefits when domain heterogeneity exists.
3. Measure and compare the training time and communication costs of FedPick against baseline methods to quantify the computational overhead of the additional personalization components.