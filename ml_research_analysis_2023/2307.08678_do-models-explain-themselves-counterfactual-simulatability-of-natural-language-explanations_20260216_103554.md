---
ver: rpa2
title: Do Models Explain Themselves? Counterfactual Simulatability of Natural Language
  Explanations
arxiv_id: '2307.08678'
source_url: https://arxiv.org/abs/2307.08678
tags:
- answer
- robot
- question
- explanation
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces counterfactual simulatability to measure\
  \ whether an LLM explanation enables humans to correctly infer the model\u2019s\
  \ outputs on diverse counterfactuals of the explained input. Two metrics are proposed:\
  \ simulation precision (fraction of counterfactuals where human inference matches\
  \ model output) and simulation generality (diversity of simulatable counterfactuals)."
---

# Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations

## Quick Facts
- arXiv ID: 2307.08678
- Source URL: https://arxiv.org/abs/2307.08678
- Reference count: 28
- Key outcome: LLM explanations have low precision (~80% for binary classification) and precision doesn't correlate with plausibility

## Executive Summary
This paper introduces counterfactual simulatability as a metric to evaluate whether LLM explanations enable humans to accurately predict model behavior on diverse counterfactual inputs. The approach measures both simulation precision (fraction of counterfactuals where human inference matches model output) and simulation generality (diversity of simulatable counterfactuals). Experiments on multi-hop reasoning and reward modeling tasks reveal that LLM explanations have surprisingly low precision, suggesting that naively optimizing for human approvals (like RLHF) may be insufficient for building accurate mental models of model behavior.

## Method Summary
The evaluation framework generates counterfactual inputs relevant to an explanation, then asks humans to infer model outputs based on that explanation. Counterfactuals are generated using LLM prompting, then filtered to remove unsimulatable cases where humans can't make confident inferences. Human annotators judge entailment between (explanation, answer, question) and counterfactual inputs. The framework calculates simulation precision (fraction of correct human predictions) and simulation generality (diversity of simulatable counterfactuals). The method was applied to StrategyQA (multi-hop reasoning) and Stanford Human Preference (reward modeling) datasets using GPT-3.5 and GPT-4.

## Key Results
- LLM explanations achieve only ~80% simulation precision on binary classification tasks
- Simulation precision does not correlate with plausibility (r=0.012), suggesting these are distinct metrics
- LLM-generated counterfactuals produce more diverse simulatable examples than baseline methods
- Higher task accuracy doesn't guarantee better explanation precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual simulatability measures whether an explanation allows humans to predict model behavior on unseen inputs
- Mechanism: The approach generates counterfactual inputs relevant to an explanation, then asks humans to infer model outputs based on the explanation. If human inference matches actual model output, the explanation is considered simulatable
- Core assumption: Human mental models formed from explanations can be evaluated by comparing their predictions against actual model behavior on counterfactuals
- Evidence anchors: [abstract] "whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input"; [section 3.2] "simulation precision = the fraction of simulatable counterfactuals where humans' guess matches the model's output"
- Break condition: If humans use different common-sense knowledge than the model, their inferences may not match model outputs even with good explanations

### Mechanism 2
- Claim: LLM-generated counterfactuals can effectively approximate human-generated counterfactuals for evaluation
- Mechanism: The approach prompts LLMs to generate diverse counterfactuals based on an explanation, then filters for simulatable ones where humans can make confident inferences
- Core assumption: LLMs can generate diverse, relevant counterfactuals that humans would reasonably create when trying to test an explanation
- Evidence anchors: [section 4.3] "We generate ten counterfactuals per explanation for StrategyQA and six for SHP using LLM prompting"; [section 5.1] "LLM prompting generates more diverse simulatable counterfactuals than a baseline that ignores explanations"
- Break condition: If LLM-generated counterfactuals don't capture the diversity or relevance that human-generated ones would, evaluation becomes less representative

### Mechanism 3
- Claim: Simulation precision and plausibility are distinct metrics that don't necessarily correlate
- Mechanism: The approach measures both how well explanations enable prediction of model behavior (precision) and how factually correct/coherent explanations are (plausibility), finding they don't strongly correlate
- Core assumption: An explanation can be plausible (factually correct and logically coherent) without being precise (enabling accurate mental models of model behavior)
- Evidence anchors: [section 5.2.2] "we only observe a very weak correlation of +0.012 between simulation precision and plausibility"; [section 5.2.2] "plausible explanations aligned with human preference do not lead to more precise mental models"
- Break condition: If precision and plausibility were strongly correlated, optimizing for plausibility would be sufficient for building accurate mental models

## Foundational Learning

- Concept: Mental models in AI explainability
  - Why needed here: The paper's core premise is that explanations should help humans build mental models of how models process inputs
  - Quick check question: If an explanation states "all birds can fly" and the model answers "yes" to "Can eagles fly?", what mental model should humans build about the model's answer to "Can penguins fly?"

- Concept: Counterfactual reasoning
  - Why needed here: The evaluation framework depends on creating and reasoning about counterfactual variations of inputs
  - Quick check question: If an explanation mentions "pork is not commonly consumed in Muslim countries," what counterfactual question could you ask to test if the explanation helps predict model behavior?

- Concept: Human simulation vs. model simulation
  - Why needed here: The paper distinguishes between humans simulating model behavior and models simulating their own behavior
  - Quick check question: What's the difference between asking a human to predict what a model will output versus asking the model to explain its own reasoning?

## Architecture Onboarding

- Component map: Input processing -> Counterfactual generation -> Filtering -> Human simulation -> Metric calculation -> Evaluation
- Critical path: Counterfactual generation → Human annotation → Metric calculation → Comparison
- Design tradeoffs:
  - LLM vs. human counterfactual generation: LLMs are faster and cheaper but may miss relevant counterfactuals
  - Number of counterfactuals: More counterfactuals increase evaluation robustness but increase cost
  - Human vs. LLM simulation: Humans provide ground truth but are expensive; LLMs are faster but may not capture human reasoning
- Failure signatures:
  - Low IAA between human annotators: Suggests simulation task is too subjective or unclear
  - Counterfactuals don't vary meaningfully: Suggests generation prompts need refinement
  - Precision scores near chance: Suggests explanations aren't helping humans build useful mental models
- First 3 experiments:
  1. Run sanity check comparing FORCED explanations (explaining wrong answers) vs NORMAL to verify evaluation can discriminate
  2. Compare GPT-3 vs GPT-4 explanation precision to validate scaling effects
  3. Test correlation between plausibility and precision across different explanation methods to validate distinct metrics

## Open Questions the Paper Calls Out

- **How do differences in common-sense knowledge between humans and LLMs affect counterfactual simulatability metrics?**
  - Basis in paper: Explicit - "When a human uses common-sense knowledge to generalize mental models, it may differ from a model's generalization if they have different commonsense knowledge"
  - Why unresolved: The paper acknowledges this issue but doesn't empirically measure the impact of knowledge mismatches on simulation precision/generality
  - What evidence would resolve it: Controlled experiments measuring how different LLM knowledge bases (compared to human knowledge) affect simulation accuracy

- **How does counterfactual simulatability translate to real-world AI system improvement outcomes?**
  - Basis in paper: Explicit - "Second, in this work we did not directly evaluate specific use cases of counterfactual simulatibility, such as model debugging and AI teaching humans"
  - Why unresolved: The paper identifies this as a limitation but doesn't study practical applications of the metric
  - What evidence would resolve it: Empirical study measuring whether improving counterfactual simulatability leads to measurable improvements in model debugging efficiency or human learning outcomes

- **How does task complexity affect the relationship between task accuracy and explanation precision?**
  - Basis in paper: Explicit - "While StrategyQA is easier compared to SHP in terms of task accuracy (by 9.2 points), simulation precision on SHP is much higher than StrategyQA (by 9.2 precision points)"
  - Why unresolved: The paper notes this surprising finding but doesn't investigate whether it holds across a broader range of task difficulties
  - What evidence would resolve it: Systematic study varying task complexity across multiple dimensions and measuring the correlation with explanation precision

## Limitations
- The effectiveness of LLM-generated counterfactuals is not validated against human-generated counterfactuals
- The evaluation is limited to multi-hop reasoning and reward modeling tasks, potentially limiting generalizability
- The paper doesn't investigate how improving counterfactual simulatability translates to practical AI system improvements

## Confidence

**High confidence**: Simulation precision as a valid metric for explanation quality, supported by clear methodology and reasonable inter-annotator agreement (0.78)

**Medium confidence**: Claims about LLM counterfactuals being sufficient for evaluation, as no direct comparison with human-generated counterfactuals is provided

**Low confidence**: Generalization to other domains beyond multi-hop reasoning and reward modeling, given the narrow experimental scope

## Next Checks

1. **Counterfactual Generation Validation**: Compare LLM-generated counterfactuals against human-generated ones on a subset of examples to measure coverage and relevance differences

2. **Cross-Domain Testing**: Apply the evaluation framework to different task types (e.g., fact verification, commonsense reasoning) to assess generalizability of findings

3. **Explanatory Method Comparison**: Test additional explanation methods beyond CoT and Post-Hoc (e.g., chain-of-thought with intermediate steps, causal explanations) to determine if certain formats consistently yield higher precision