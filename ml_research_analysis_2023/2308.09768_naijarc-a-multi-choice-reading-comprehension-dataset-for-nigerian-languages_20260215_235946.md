---
ver: rpa2
title: 'NaijaRC: A Multi-choice Reading Comprehension Dataset for Nigerian Languages'
arxiv_id: '2308.09768'
source_url: https://arxiv.org/abs/2308.09768
tags:
- language
- dataset
- comprehension
- languages
- african
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NaijaRC, a new multi-choice reading comprehension
  dataset for three Nigerian languages based on high-school examination questions.
  The authors collected 39 passages and 266 questions across Yoruba, Igbo, and Hausa,
  then evaluated baseline performance using cross-lingual transfer from English RACE
  and Belebele datasets with AfroXLMR-base, and via prompting GPT-4.
---

# NaijaRC: A Multi-choice Reading Comprehension Dataset for Nigerian Languages

## Quick Facts
- arXiv ID: 2308.09768
- Source URL: https://arxiv.org/abs/2308.09768
- Reference count: 10
- Models achieve 29.31% accuracy on Yoruba RC with cross-lingual transfer from English RACE, substantially below English performance

## Executive Summary
This paper introduces NaijaRC, a new multi-choice reading comprehension dataset for three Nigerian languages (Yoruba, Igbo, Hausa) based on high-school examination questions. The authors collected 39 passages and 266 questions across these languages and evaluated baseline performance using cross-lingual transfer from English RACE and Belebele datasets with AfroXLMR-base, and via prompting GPT-4. On the Yoruba subset (YORC), AfroXLMR-base achieved 29.31% accuracy after fine-tuning on RACE, while GPT-4 reached 36.12%, both substantially below English performance, highlighting the challenges of adapting models to under-resourced African languages.

## Method Summary
The authors collected 39 passages and 266 questions across Yoruba, Igbo, and Hausa based on high-school examination questions. They evaluated baseline performance using cross-lingual transfer from English RACE and Belebele datasets with AfroXLMR-base (fine-tuned for 3 epochs on RACE), and via prompting GPT-4 with Yoruba comprehension questions. Performance was measured as accuracy on the multi-choice questions.

## Key Results
- AfroXLMR-base fine-tuned on English RACE achieved 29.31% accuracy on Yoruba RC tasks
- GPT-4 achieved 36.12% accuracy on Yoruba RC tasks, outperforming other models
- Both models performed substantially below their English performance, highlighting challenges of adapting to under-resourced African languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual transfer from English RACE dataset to Yoruba comprehension tasks is ineffective due to linguistic and cultural differences.
- Mechanism: The model fine-tuned on English RACE dataset fails to generalize to Yoruba because the syntactic structures, vocabulary, and cultural context differ significantly between English and Yoruba.
- Core assumption: The RACE dataset's question-answering patterns are not transferable to Yoruba reading comprehension tasks.
- Evidence anchors:
  - [abstract] "both substantially below English performance, highlighting the challenges of adapting models to under-resourced African languages."
  - [section] "When we evaluated the cross-lingual capability of this model on the YORC test set, the performance significantly declined, with an accuracy of 29.31%."
- Break condition: If Yoruba comprehension tasks share similar structures or patterns with English RACE tasks, cross-lingual transfer might be more effective.

### Mechanism 2
- Claim: GPT-4 outperforms other models on Yoruba comprehension tasks due to its superior language understanding capabilities.
- Mechanism: GPT-4's large-scale training on diverse language data allows it to better understand and answer questions in Yoruba compared to other models.
- Core assumption: GPT-4 has been exposed to sufficient Yoruba language data during its training.
- Evidence anchors:
  - [abstract] "GPT-4 reached 36.12%, both substantially below English performance, highlighting the challenges of adapting models to under-resourced African languages."
  - [section] "Furthermore, GPT-4 showed the best performance on the YORC data, with an accuracy of 36.14%."
- Break condition: If GPT-4's training data lacks Yoruba language content, its performance might not be superior.

### Mechanism 3
- Claim: The low performance of models on Yoruba comprehension tasks is due to the limited size and quality of the YORC dataset.
- Mechanism: The small size of the YORC dataset (266 questions) limits the model's ability to learn complex patterns and nuances in Yoruba comprehension tasks.
- Core assumption: A larger and more diverse dataset would lead to better model performance.
- Evidence anchors:
  - [abstract] "both substantially below English performance, highlighting the challenges of adapting models to under-resourced African languages."
  - [section] "Due to the incapacity of many language models, we removed questions relating to the semantics of specific italics or underlined words in the context of usage."
- Break condition: If the dataset size and quality are not the limiting factors, other issues like model architecture or training strategies might be responsible.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how knowledge from one language can be applied to another is crucial for evaluating the effectiveness of cross-lingual transfer in this context.
  - Quick check question: What are the key challenges in transferring knowledge from English to Yoruba comprehension tasks?

- Concept: Dataset curation and quality
  - Why needed here: The quality and size of the dataset directly impact the model's performance, making it essential to understand the dataset's characteristics.
  - Quick check question: How does the size and quality of the YORC dataset affect the model's ability to learn complex patterns?

- Concept: Model architecture and training strategies
  - Why needed here: Different models and training strategies can significantly impact performance, making it important to understand the underlying architecture and training process.
  - Quick check question: How do the architectures of AfroXLMR-base and GPT-4 differ, and how do these differences affect their performance on Yoruba comprehension tasks?

## Architecture Onboarding

- Component map: Data collection -> Preprocessing -> Model selection -> Training -> Evaluation
- Critical path: 1) Collect and preprocess the YORC dataset, 2) Select and train models (AfroXLMR-base and GPT-4), 3) Evaluate model performance on the YORC dataset
- Design tradeoffs: Tradeoff between dataset size and quality, tradeoff between model complexity and training time
- Failure signatures: Low accuracy on Yoruba comprehension tasks, poor generalization to unseen data
- First 3 experiments: 1) Fine-tune AfroXLMR-base on RACE dataset and evaluate on YORC, 2) Prompt GPT-4 with Yoruba comprehension questions and evaluate performance, 3) Compare the performance of AfroXLMR-base and GPT-4 on the YORC dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do few-shot learning approaches perform on the NaijaRC dataset compared to zero-shot cross-lingual transfer and full fine-tuning methods?
- Basis in paper: Explicit - The paper mentions "As a future work, we plan to extend our evaluation to few-shot settings. Specifically, we will explore different approaches that can utilize a few examples, like 50 (passage, question, answer) tuple for the effective adaptation of existing reading comprehension models."
- Why unresolved: This future work direction has not yet been implemented or evaluated, leaving a gap in understanding the optimal training approach for this low-resource African language dataset.
- What evidence would resolve it: Experimental results comparing few-shot learning performance (with varying numbers of examples like 10, 25, 50, 100) against both zero-shot transfer and full fine-tuning on the same dataset, showing accuracy metrics for each approach.

### Open Question 2
- Question: What specific linguistic features of Yoruba (such as tone, diacritics, or morphology) contribute most significantly to the performance degradation of multilingual models on the NaijaRC dataset?
- Basis in paper: Inferred - The paper discusses Yoruba's tonal nature and the use of diacritics, and shows significant performance drops when models trained on English RACE are applied to Yoruba RC tasks.
- Why unresolved: While the paper identifies that models struggle with Yoruba, it does not conduct detailed linguistic analysis to pinpoint which specific features cause the most difficulty.
- What evidence would resolve it: Ablation studies testing model performance on Yoruba text with different preprocessing (e.g., with/without tone marks, simplified orthography) or controlled experiments isolating individual linguistic features would identify the primary challenges.

### Open Question 3
- Question: How does the performance of current LLMs on NaijaRC compare to human performance on the same dataset?
- Basis in paper: Explicit - The paper notes that GPT-4 achieves only 36.12% accuracy on the Yoruba subset, which is "considerably lower than the performance of the AfroXLMR-base and ChatGPT on the English test set," but does not provide any human baseline.
- Why unresolved: The paper establishes that current models perform poorly on this dataset but does not benchmark human performance to contextualize how far current models are from human-level comprehension.
- What evidence would resolve it: Human evaluation results showing accuracy, response time, and error analysis from native Yoruba speakers completing the same RC tasks, enabling calculation of human-model performance gaps.

## Limitations

- The evaluation is based on a relatively small dataset (266 questions) which may not be representative of broader challenges
- The performance gap between GPT-4 and AfroXLMR-base could be influenced by factors beyond language understanding capabilities
- The specific prompting format and examples used for GPT-4 evaluation are not fully specified, affecting reproducibility

## Confidence

- Cross-lingual transfer effectiveness: Medium confidence - supported by empirical results but limited by small dataset size and lack of detailed hyperparameter information
- GPT-4 superiority: Medium confidence - higher accuracy shown but exact prompting strategy and training data exposure unclear
- Dataset limitations: High confidence - paper clearly identifies small size and quality issues as significant challenges

## Next Checks

1. **Dataset Expansion and Quality Assessment**: Validate the impact of dataset size and quality on model performance by creating a larger, more diverse YORC dataset and retraining AfroXLMR-base. Compare the new results with the original to assess improvements.

2. **Hyperparameter Sensitivity Analysis**: Conduct a sensitivity analysis on the hyperparameters used for fine-tuning AfroXLMR-base on RACE, such as learning rate and batch size, to determine their effect on cross-lingual transfer performance.

3. **GPT-4 Prompting Strategy Comparison**: Experiment with different GPT-4 prompting formats and examples to identify the most effective strategy for Yoruba comprehension tasks. Compare the performance of these strategies to isolate the impact of prompting on accuracy.