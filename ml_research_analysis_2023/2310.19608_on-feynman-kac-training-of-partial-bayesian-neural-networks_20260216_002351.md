---
ver: rpa2
title: On Feynman--Kac training of partial Bayesian neural networks
arxiv_id: '2310.19608'
source_url: https://arxiv.org/abs/2310.19608
tags:
- posterior
- algorithm
- neural
- monte
- carlo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel training method for partial Bayesian
  neural networks (pBNNs) using Feynman-Kac models and sequential Monte Carlo (SMC)
  samplers. The method addresses the challenge of estimating parameters and posterior
  distributions in pBNNs, which are often multi-modal and difficult to approximate
  with parametric models.
---

# On Feynman--Kac training of partial Bayesian neural networks

## Quick Facts
- arXiv ID: 2310.19608
- Source URL: https://arxiv.org/abs/2310.19608
- Reference count: 39
- Key outcome: Novel SMC-based training method for pBNNs that estimates parameters and posterior distributions simultaneously, outperforming state-of-the-art methods on predictive performance.

## Executive Summary
This paper proposes a novel training method for partial Bayesian neural networks (pBNNs) using Feynman-Kac models and sequential Monte Carlo (SMC) samplers. The method addresses the challenge of estimating parameters and posterior distributions in pBNNs, which are often multi-modal and difficult to approximate with parametric models. The core idea is to formulate pBNN training as simulating a Feynman-Kac model and then apply SMC samplers to estimate the parameters and posterior distribution simultaneously. The authors propose two approximate SMC samplers, SGSMC and OHSMC, that are scalable in the number of data points. Experiments on synthetic and real-world datasets show that the proposed training scheme outperforms state-of-the-art methods in terms of predictive performance, with OHSMC achieving the best results in most cases.

## Method Summary
The paper introduces a novel approach to train partial Bayesian neural networks (pBNNs) by reformulating the training process as simulating a Feynman-Kac model. This allows the use of sequential Monte Carlo (SMC) samplers to simultaneously estimate both the deterministic parameters and the posterior distribution of stochastic parameters. Two specific SMC variants are proposed: SGSMC (Stochastic Gradient SMC) for standard mini-batch training, and OHSMC (Open-Horizon SMC) which improves efficiency by warm-starting each iteration from the previous posterior estimate. The methods are designed to be scalable to large datasets through stochastic mini-batching while maintaining good posterior approximations.

## Key Results
- SGSMC and OHSMC methods achieve better predictive performance than state-of-the-art approaches on UCI datasets and MNIST
- OHSMC consistently outperforms SGSMC due to its warm-starting mechanism that reuses computational effort between iterations
- The methods successfully estimate both parameters and posterior distributions in pBNNs, addressing the challenge of multi-modal posteriors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The SMC sampler can estimate both the deterministic parameters and posterior distributions simultaneously in pBNNs by reformulating training as a Feynman-Kac model simulation.
- **Mechanism**: The training of a pBNN is modeled as a Feynman-Kac process where sequential Monte Carlo (SMC) methods simulate the posterior distribution updates across data points. This allows SMC to produce both consistent MLE estimates for deterministic parameters and posterior samples for stochastic parameters.
- **Core assumption**: The latent posterior distribution in pBNNs can be represented as a sequence of intermediate distributions connected through invariant Markov kernels.
- **Evidence anchors**:
  - [abstract]: "the training of a pBNN is formulated as simulating a Feynman–Kac model. We then describe variations of sequential Monte Carlo samplers that allow us to simultaneously estimate the parameters and the latent posterior distribution"
  - [section]: "The Feynman–Kac model is composed of a sequence of potential functions given by the likelihood model and a sequence of invariant Markov kernels that so as to anneal to the target posterior distribution p(ϕ | y1:N ; ψ)."
- **Break condition**: If the Markov kernels fail to leave the posterior distribution invariant or the sequence of distributions cannot be properly annealed.

### Mechanism 2
- **Claim**: The open-horizon SMC variant improves computational efficiency by warm-starting each SMC iteration from the previous posterior estimate rather than the prior.
- **Mechanism**: Instead of running independent SMC chains from the prior at each optimization step, OHSMC uses the posterior approximation from the previous iteration as the starting distribution. This creates a continuous flow of inference that reuses computational effort.
- **Core assumption**: The posterior distribution changes gradually enough between optimization steps that warm-starting provides meaningful computational savings.
- **Evidence anchors**:
  - [section]: "we modify Algorithm 2 by warm-starting each SMC sampler from the previous posterior distribution estimate and perform the gradient update in conjunction with the SMC sampler"
  - [corpus]: "Partial Bayesian neural networks (pBNNs) have been shown to perform competitively with fully Bayesian neural networks while only having a subset of the parameters be stochastic" - supports that pBNNs have stable enough posterior structure for warm-starting
- **Break condition**: If the posterior distribution changes drastically between iterations, causing warm-starting to introduce significant bias.

### Mechanism 3
- **Claim**: Stochastic mini-batching within the SMC framework enables scalable training on large datasets while maintaining good posterior approximations.
- **Mechanism**: By approximating the marginal likelihood using random subsets of data (mini-batches), the SMC sampler can process large datasets efficiently. The Markov kernels are designed to be invariant with respect to the posterior based on the mini-batch rather than the full dataset.
- **Core assumption**: The posterior based on mini-batches provides a reasonable approximation to the full posterior, and the bias introduced is acceptable for training purposes.
- **Evidence anchors**:
  - [section]: "Let 1 ≤ M ≤ N denote the batch size and let SM := {SM (1), SM (2), . . . , SM (M )} be a sequence of independent random integers (uniformly distributed in [1, N]) that represent the batch indices. We may approximate the marginal log-likelihood by log p(y1:N ; ψ) ≈ N/M log p(ySM ; ψ)"
  - [corpus]: "The pseudo-marginal approach for efficient Monte Carlo computations" - shows connection to established methods using mini-batches in MCMC
- **Break condition**: If the mini-batch size is too small relative to the dataset, causing the posterior approximation to be too biased.

## Foundational Learning

- **Concept**: Feynman-Kac models and their connection to Bayesian inference
  - **Why needed here**: The paper's core innovation is reformulating pBNN training as a Feynman-Kac model, which provides the theoretical foundation for applying SMC methods
  - **Quick check question**: What are the key components of a Feynman-Kac model and how do they relate to sequential Bayesian inference?

- **Concept**: Sequential Monte Carlo (particle filtering) methods
  - **Why needed here**: SMC is the computational framework used to approximate the Feynman-Kac model, requiring understanding of resampling, importance weights, and particle propagation
  - **Quick check question**: How does the resampling step in SMC help prevent weight degeneracy, and what are the tradeoffs of different resampling strategies?

- **Concept**: Maximum likelihood estimation in latent variable models
  - **Why needed here**: The paper targets MLE for deterministic parameters in pBNNs, requiring understanding of Fisher's identity and how it connects to gradient computation
  - **Quick check question**: How does Fisher's identity allow computation of the MLE gradient without differentiating through the SMC algorithm?

## Architecture Onboarding

- **Component map**: Data → Mini-batch selection → SMC initialization → Particle propagation with Markov kernels → Weight computation → Gradient estimation via Fisher's identity → Parameter update → (next iteration with warm-start)

- **Critical path**: Data → Mini-batch selection → SMC initialization → Particle propagation with Markov kernels → Weight computation → Gradient estimation via Fisher's identity → Parameter update → (next iteration with warm-start)

- **Design tradeoffs**: 
  - Using larger mini-batches reduces gradient variance but increases computational cost per iteration
  - More sophisticated Markov kernels (e.g., gradient-based) may improve mixing but require more computation per particle
  - Warm-starting saves computation but may introduce bias if posterior changes significantly

- **Failure signatures**:
  - Weight degeneracy (most particles have negligible weights) indicates poor particle diversity or inappropriate Markov kernels
  - Gradient estimates with high variance suggest too small mini-batches or insufficient particles
  - Poor convergence despite many iterations may indicate inappropriate learning rate or initialization

- **First 3 experiments**:
  1. Synthetic 1D regression problem (as in Section 4.1) to verify both parameter estimation and posterior approximation
  2. Small UCI dataset (e.g., Boston housing) to test full pipeline with real data
  3. Two-moons classification to verify uncertainty quantification in classification setting

## Open Questions the Paper Calls Out
- None explicitly called out in the provided text.

## Limitations
- Scalability to very large datasets and deep neural networks remains uncertain, as experiments focus on relatively small-scale problems
- Impact of different Markov kernel choices on performance and convergence is not thoroughly explored
- Sensitivity to hyperparameters (number of particles, mini-batch size, kernel step size) is not extensively investigated

## Confidence
- High confidence: The formulation of pBNN training as a Feynman-Kac model and the application of SMC samplers for simultaneous parameter estimation and posterior approximation
- Medium confidence: The effectiveness of the proposed SGSMC and OHSMC methods compared to state-of-the-art approaches, as the experiments are conducted on a limited set of datasets
- Low confidence: The scalability and robustness of the methods to very large datasets, deep neural networks, and a wide range of hyperparameter settings

## Next Checks
1. Conduct experiments on larger datasets (e.g., CIFAR-10, ImageNet) and deeper neural network architectures to assess the scalability of the proposed methods
2. Perform an extensive hyperparameter sensitivity analysis to identify the impact of different choices on the performance and convergence of the SMC samplers
3. Compare the proposed methods with other Bayesian neural network training approaches (e.g., variational inference, MCMC) on a diverse set of tasks and datasets to establish their relative strengths and weaknesses