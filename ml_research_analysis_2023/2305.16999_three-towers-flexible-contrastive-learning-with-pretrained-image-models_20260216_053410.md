---
ver: rpa2
title: 'Three Towers: Flexible Contrastive Learning with Pretrained Image Models'
arxiv_id: '2305.16999'
source_url: https://arxiv.org/abs/2305.16999
tags:
- image
- tower
- pretrained
- learning
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Three Towers (3T) is a method to improve contrastive vision-language
  models by incorporating pretrained image classifiers. Unlike prior work that locks
  the image tower, 3T introduces a third frozen tower containing pretrained embeddings
  and aligns the main towers to it with additional contrastive losses.
---

# Three Towers: Flexible Contrastive Learning with Pretrained Image Models

## Quick Facts
- arXiv ID: 2305.16999
- Source URL: https://arxiv.org/abs/2305.16999
- Reference count: 40
- Key outcome: 3T improves contrastive vision-language models by incorporating pretrained image classifiers via an auxiliary tower and contrastive alignment.

## Executive Summary
Three Towers (3T) is a method to improve contrastive vision-language models by incorporating pretrained image classifiers. Unlike prior work that locks the image tower, 3T introduces a third frozen tower containing pretrained embeddings and aligns the main towers to it with additional contrastive losses. This allows the main image tower to benefit from both contrastive training and pretrained knowledge. Empirically, 3T consistently improves over prior methods for retrieval tasks and for classification with ImageNet-21k pretraining. It also outperforms baselines for most classification tasks with JFT pretraining, though not LiT. 3T is more robust than LiT to mismatched model scales and pretraining datasets like Places365. Larger models and longer training benefit 3T more than LiT. Overall, 3T provides a simple and effective way to incorporate pretrained models into contrastive learning.

## Method Summary
3T extends standard CLIP-style contrastive learning by adding a third frozen tower containing pretrained image embeddings. The main image and text towers are trained normally with bi-directional contrastive loss, but additional contrastive losses are added between each main tower and the third tower via learned linear projection heads. This encourages the main towers to align with the pretrained embeddings while still learning from the large-scale contrastive dataset. The image tower remains unlocked, allowing it to adapt during training.

## Key Results
- 3T consistently improves over prior methods for retrieval tasks and for classification with ImageNet-21k pretraining.
- 3T is more robust than LiT to mismatched model scales and pretraining datasets like Places365.
- Larger models and longer training benefit 3T more than LiT.
- A convex combination of 3T and LiT predictions can outperform both methods individually.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 3T improves contrastive learning by aligning the unlocked image tower with pretrained embeddings via an auxiliary contrastive loss.
- Mechanism: The third frozen tower (h) holds pretrained embeddings; additional losses Lfh↔hf and Lgh↔hg pull the main image and text towers toward these embeddings while preserving contrastive training.
- Core assumption: Contrastive losses effectively transfer representations from a frozen teacher to an unlocked student.
- Evidence anchors:
  - [abstract]: "we introduce a third tower that contains the frozen pretrained embeddings, and we encourage alignment between this third tower and the main image-text towers."
  - [section]: "Tian et al. [70] show that contrastive losses can be seen as distillation objectives that align representations between a teacher and a student model."
- Break condition: If the pretrained embeddings and the contrastive learning dataset distributions are too dissimilar, the auxiliary alignment may mislead rather than help the main towers.

### Mechanism 2
- Claim: The unlocked image tower in 3T can adapt during training, gaining benefits from both contrastive learning and pretrained knowledge, unlike LiT's locked tower.
- Mechanism: Because the main image tower is not frozen, it continues to learn from the large-scale contrastive dataset, allowing it to correct for domain mismatches or gaps in the pretrained model's coverage.
- Core assumption: Contrastive training on diverse image-text pairs can improve over a static pretrained embedding space.
- Evidence anchors:
  - [abstract]: "LiT directly replaces the image tower with the frozen embeddings, excluding any potential benefits the image tower might get from contrastive training."
  - [section]: "The main image tower benefits from both pretraining knowledge and contrastive learning."
- Break condition: If the pretrained model is already optimal for the downstream task, additional contrastive training may degrade performance.

### Mechanism 3
- Claim: 3T is more robust to mismatched model scales and pretraining datasets than LiT because it does not require the main image tower to match the frozen pretrained tower's capacity or domain.
- Mechanism: The unlocked image tower can be scaled independently of the pretrained third tower, and contrastive learning on the large dataset can compensate for weaknesses in the pretrained embeddings.
- Core assumption: Larger or more diverse contrastive datasets can cover gaps left by smaller or domain-specific pretraining sets.
- Evidence anchors:
  - [abstract]: "3T is significantly more robust than LiT to deficits in the pretrained model (§4.2 and §4.4)."
  - [section]: "3T allows for architectural differences between the unlocked image tower and pretrained model."
- Break condition: If the contrastive dataset is too small or narrow, the unlocked tower may not recover from a poor pretrained embedding space.

## Foundational Learning

- Concept: Contrastive learning with bi-directional loss (Lf↔g).
  - Why needed here: 3T extends the standard CLIP-style objective by adding auxiliary losses to a third tower; understanding the base contrastive loss is essential to grasp how the extensions work.
  - Quick check question: In CLIP-style training, what is the interpretation of the two directional loss terms Lf→g and Lg→f?
- Concept: Knowledge distillation via contrastive loss.
  - Why needed here: The paper relies on the insight that contrastive losses can act as distillation objectives between teacher and student; this underpins why the auxiliary losses help.
  - Quick check question: How does the contrastive loss between the main tower and third tower function similarly to a distillation objective?
- Concept: Linear projection heads for embedding alignment.
  - Why needed here: 3T uses learned linear layers to project embeddings from different towers into a common space before computing losses; understanding this avoids misinterpretation of the alignment process.
  - Quick check question: Why are separate linear projection heads used for the third tower's embeddings when computing losses with the main towers?

## Architecture Onboarding

- Component map:
  - Main image tower (f): unlocked, learns from both contrastive data and auxiliary alignment.
  - Main text tower (g): unlocked, trained from scratch.
  - Third tower (h): frozen pretrained embeddings, projected to common space.
  - Linear projection heads (fh, gh, hf, hg): map between embedding spaces for loss computation.
  - Loss terms: Lf↔g (standard), Lfh↔hf and Lgh↔hg (auxiliary).
- Critical path: During training, compute embeddings for (f, g, h), apply projections, compute all three contrastive losses, backpropagate to update f and g parameters only.
- Design tradeoffs:
  - Unlocked image tower: gains flexibility and robustness but adds training cost vs. LiT's locked approach.
  - Linear projection heads: provide alignment flexibility but increase model complexity slightly.
  - Equal weighting of three losses: simple but may not be optimal for all datasets or scales.
- Failure signatures:
  - If auxiliary losses dominate, main contrastive learning may be drowned out, harming retrieval performance.
  - If the pretrained model is poorly aligned with the contrastive dataset, auxiliary losses may mislead the main towers.
  - If projection heads are poorly initialized, embeddings may not align, causing slow convergence.
- First 3 experiments:
  1. Train 3T vs. LiT vs. baseline on a small WebLI split with IN-21k pretraining; verify retrieval gains.
  2. Swap pretrained model to Places365; check robustness compared to LiT.
  3. Vary the weight of auxiliary losses; find optimal balance for classification vs. retrieval tasks.

## Open Questions the Paper Calls Out
- How does the performance of 3T scale when increasing model size beyond the g scale used in experiments?
- Can 3T be effectively extended to incorporate multiple pretrained models from diverse modalities?
- What is the optimal way to combine 3T and LiT predictions post-hoc to maximize performance across tasks?

## Limitations
- Empirical support for robustness claims is moderate, with limited comparisons to LiT for a few pretrained model swaps and model size mismatches.
- Lacks ablation studies showing how much each component (auxiliary losses, unlocked image tower, projection heads) contributes to gains.
- Dataset distribution shifts are not thoroughly explored, making it unclear how 3T would perform when pretrained embeddings and contrastive data come from vastly different domains.
- Absence of neighbor literature discussing unlocked vs. locked tower trade-offs weakens theoretical grounding for some claims.

## Confidence
- Mechanism 1 (auxiliary contrastive alignment): Medium — supported by the paper's ablation on loss terms but lacks broader literature validation.
- Mechanism 2 (unlocked image tower adaptation): Low — the claim is plausible but lacks direct evidence or discussion in related work.
- Mechanism 3 (robustness to scale/pretraining mismatches): Medium — supported by a few experimental comparisons but not systematically tested across diverse scenarios.

## Next Checks
1. Run ablations removing the auxiliary losses to quantify their individual impact on retrieval and classification.
2. Test 3T with a pretrained model from a completely different domain (e.g., medical or satellite imagery) to stress-test robustness claims.
3. Compare performance as a function of dataset size for contrastive training to see how much data is needed to compensate for a weaker pretrained model.