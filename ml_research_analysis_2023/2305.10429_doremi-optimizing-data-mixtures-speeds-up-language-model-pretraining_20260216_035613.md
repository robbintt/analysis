---
ver: rpa2
title: 'DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining'
arxiv_id: '2305.10429'
source_url: https://arxiv.org/abs/2305.10429
tags:
- domain
- doremi
- weights
- proxy
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DoReMi optimizes language model pretraining by tuning data mixture
  proportions across domains using a small proxy model trained with distributionally
  robust optimization. The method trains a reference model, then a proxy model using
  Group DRO to output domain weights, and finally trains a large model with these
  weights.
---

# DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining

## Quick Facts
- arXiv ID: 2305.10429
- Source URL: https://arxiv.org/abs/2305.10429
- Reference count: 30
- Key outcome: DoReMi improves average few-shot downstream accuracy by 6.5% over baseline domain weights and achieves baseline accuracy 2.6x faster on The Pile dataset.

## Executive Summary
DoReMi is a method for optimizing language model pretraining by tuning data mixture proportions across domains using a small proxy model trained with distributionally robust optimization (DRO). The approach trains a reference model, then a proxy model using Group DRO to output domain weights, and finally trains a large model with these weights. On The Pile dataset, DoReMi improves average few-shot downstream accuracy by 6.5% over baseline domain weights and achieves baseline accuracy 2.6x faster. The method also improves perplexity across all domains while reducing weight on some domains, demonstrating no-tradeoff improvements.

## Method Summary
DoReMi optimizes language model pretraining by tuning data mixture proportions across domains using a small proxy model trained with distributionally robust optimization. The method trains a reference model to establish baseline performance, then trains a proxy model using Group DRO to output domain weights based on excess loss (the gap between proxy and reference model performance). The main model is then trained using the optimized domain weights. The approach is validated on The Pile dataset with a 280M-parameter proxy model and an 8B-parameter main model, as well as on the GLaM dataset with 8 domains.

## Key Results
- DoReMi improves average few-shot downstream accuracy by 6.5% over baseline domain weights on The Pile
- DoReMi achieves baseline accuracy 2.6x faster than baseline training
- DoReMi improves perplexity across all domains while reducing weight on some domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DoReMi improves pretraining efficiency by optimizing domain weights using distributionally robust optimization (DRO) without requiring downstream task knowledge.
- **Mechanism**: The method trains a small proxy model with Group DRO over domains to output domain weights, then resamples the dataset and trains a larger model. This allows the larger model to focus on domains with higher "excess loss" (the gap between the proxy and reference model's performance), which represents learnable examples where the reference model has already made progress.
- **Core assumption**: The excess loss metric accurately captures which domains have learnable examples and which are too noisy or too easy, allowing effective domain weight optimization.
- **Evidence anchors**:
  - [abstract]: "DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights"
  - [section]: "We then resample a dataset with these domain weights and train a larger, full-sized model"
  - [corpus]: Weak - corpus lacks specific evidence about DRO effectiveness
- **Break condition**: If the reference model fails to capture a reasonable baseline difficulty for each domain, the excess loss metric becomes unreliable, leading to poor domain weight optimization.

### Mechanism 2
- **Claim**: DoReMi achieves no tradeoff improvements by downweighting domains that are either too high entropy (noisy) or too low entropy (easy), while allocating more samples to medium entropy domains.
- **Mechanism**: High entropy domains have token distributions close to uniform priors, requiring fewer samples to fit. Low entropy domains are statistically learnable with few samples. Medium entropy domains benefit most from additional samples, and positive transfer occurs when more samples are allocated to them.
- **Core assumption**: There exists a "sweet spot" in domain entropy where additional samples provide the most learning value, and domains outside this range can be downweighted without harming overall performance.
- **Evidence anchors**:
  - [section]: "DoReMi signiﬁcantly reduces the perplexity over the baseline across all domains, despite allocating lower weight to some domains"
  - [section]: "Intuitively, the domains with the lowest and highest entropy can be downweighted without impacting the perplexity much"
  - [corpus]: Weak - corpus lacks specific evidence about entropy-based domain weighting
- **Break condition**: If all domains fall into the medium entropy range, or if positive transfer between domains is minimal, the no-tradeoff improvement may not materialize.

### Mechanism 3
- **Claim**: DoReMi's domain weights generalize across model scales, allowing small proxy models to optimize weights that benefit much larger models.
- **Mechanism**: The method finds domain weights based on relative difficulty (excess loss) rather than absolute performance, making the weights scale-invariant. The weights are derived from comparing proxy and reference model performance within the same scale.
- **Core assumption**: The relative difficulty of domains remains consistent across model scales, so weights optimized for small models transfer effectively to larger models.
- **Evidence anchors**:
  - [abstract]: "We use DoReMi on a 280M-parameter proxy model to set the domain weights for training an 8B-parameter model (30x larger)"
  - [section]: "DoReMi (280M →8B). On The Pile, DoReMi reduces perplexity on all domains over baseline domain weights"
  - [corpus]: Weak - corpus lacks specific evidence about scale generalization
- **Break condition**: If domain difficulty relationships change significantly with model scale, the optimized weights may not transfer effectively to larger models.

## Foundational Learning

- **Concept**: Distributionally Robust Optimization (DRO)
  - Why needed here: DRO provides the theoretical framework for finding domain weights that minimize worst-case loss across domains, which is essential for DoReMi's approach to data mixture optimization
  - Quick check question: What is the key difference between standard empirical risk minimization and DRO in the context of domain weighting?

- **Concept**: Group DRO and Online Learning
  - Why needed here: Group DRO's online learning-based optimizer dynamically updates domain weights according to loss on each domain, which is the mechanism DoReMi uses to find optimal domain weights
  - Quick check question: How does Group DRO's exponentiated gradient update differ from standard gradient descent in the context of domain weight optimization?

- **Concept**: Excess Loss and Reference Models
  - Why needed here: The excess loss metric (proxy loss minus reference loss) is central to DoReMi's approach for identifying learnable examples and optimizing domain weights
  - Quick check question: Why is it necessary to use a reference model when computing excess loss, rather than just using the proxy model's loss directly?

## Architecture Onboarding

- **Component map**: Reference Model Training -> Proxy Model Training with Group DRO -> Main Model Training -> Evaluation Pipeline
- **Critical path**:
  1. Train reference model (280M parameters) for T steps with initial domain weights
  2. Train proxy model with Group DRO to obtain optimized domain weights
  3. Train main model (8B parameters) using optimized domain weights
  4. Evaluate perplexity and downstream performance
- **Design tradeoffs**:
  - Proxy model size vs. main model performance: Smaller proxy models save compute but may produce suboptimal weights
  - Number of Group DRO steps: More steps improve weight optimization but increase compute cost
  - Domain granularity: More domains allow finer control but increase optimization complexity
- **Failure signatures**:
  - Domain weights converge to uniform distribution: May indicate reference model is too weak or optimization parameters need adjustment
  - Main model performance worse than baseline: Could indicate poor generalization of domain weights across scales
  - No improvement in downstream tasks: May suggest downstream tasks have different domain importance than measured by perplexity
- **First 3 experiments**:
  1. Verify reference model training produces reasonable baseline perplexity across all domains
  2. Test Group DRO training on proxy model to ensure domain weights change meaningfully during training
  3. Train small main model (same size as proxy) with optimized weights to validate weight quality before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of reference model size and architecture affect the domain weights found by DoReMi?
- Basis in paper: [explicit] The paper mentions that the choice of reference model can affect the domain weights found by DoReMi, and suggests varying the reference model size and using specialized reference models for specific application areas as future directions.
- Why unresolved: The paper does not explore the impact of different reference model sizes and architectures on the domain weights found by DoReMi.
- What evidence would resolve it: Experiments comparing the domain weights found by DoReMi using reference models of different sizes and architectures, and evaluating the performance of the resulting main models on downstream tasks.

### Open Question 2
- Question: Can DoReMi be extended to use fine-grained domains instead of coarse-grained domains based on data provenance?
- Basis in paper: [explicit] The paper states that defining domains by data provenance only enables coarse-grained control and suggests that using fine-grained domains could improve the gains from DoReMi.
- Why unresolved: The paper does not explore the use of fine-grained domains in DoReMi.
- What evidence would resolve it: Experiments comparing the performance of DoReMi using coarse-grained and fine-grained domains, and evaluating the impact on downstream task performance.

### Open Question 3
- Question: How does the choice of minimum domain weight (c) in the Group DRO optimizer affect the performance of DoReMi?
- Basis in paper: [explicit] The paper mentions that the minimum domain weight c is set to 1e-4 in all experiments but does not extensively tune this hyperparameter.
- Why unresolved: The paper does not explore the impact of different minimum domain weight values on the performance of DoReMi.
- What evidence would resolve it: Experiments comparing the performance of DoReMi using different values of the minimum domain weight c, and evaluating the impact on downstream task performance and perplexity across domains.

## Limitations

- Scale dependency concerns: The method relies on transferring domain weights from a 280M proxy model to an 8B main model, but the paper provides limited evidence about how this scaling affects different types of domains or model architectures.
- Downstream task alignment: While DoReMi improves average few-shot downstream accuracy, the paper doesn't provide granular analysis of which specific downstream tasks benefit most or whether the method could harm performance on certain task types.
- Computational overhead uncertainty: The paper states DoReMi achieves baseline accuracy 2.6x faster, but this comparison doesn't account for the additional compute required for the proxy model training and Group DRO optimization.

## Confidence

**High confidence**: The basic mechanism of using distributionally robust optimization to find domain weights, and the empirical observation that perplexity improves across all domains with DoReMi weights, is well-supported by the presented results.

**Medium confidence**: The claim about achieving "no tradeoff improvements" through entropy-based domain weighting is plausible given the entropy analysis, but the paper doesn't rigorously prove that this mechanism drives the improvements rather than other factors like better optimization of the overall data mixture.

**Low confidence**: The assertion that DoReMi's domain weights generalize across model scales without validation on intermediate model sizes or different architectures. The paper only tests one scale-up scenario (280M → 8B).

## Next Checks

1. **Scale transfer validation**: Train intermediate-sized models (e.g., 1B, 2B, 4B parameters) with DoReMi weights to map out the generalization curve and identify any scale thresholds where the weights stop transferring effectively.

2. **Task-specific impact analysis**: Break down downstream accuracy improvements by task category (e.g., QA, reasoning, generation) to identify whether DoReMi helps uniformly across task types or creates task-specific tradeoffs that average out in aggregate metrics.

3. **Compute overhead accounting**: Measure the total wall-clock time including proxy model training and DRO optimization, then compare against the baseline training time to quantify the true efficiency gains rather than just the step count reduction.