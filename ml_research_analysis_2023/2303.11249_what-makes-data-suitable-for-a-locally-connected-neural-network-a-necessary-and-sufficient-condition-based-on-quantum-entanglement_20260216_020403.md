---
ver: rpa2
title: What Makes Data Suitable for a Locally Connected Neural Network? A Necessary
  and Sufficient Condition Based on Quantum Entanglement
arxiv_id: '2303.11249'
source_url: https://arxiv.org/abs/2303.11249
tags:
- tensor
- data
- neural
- network
- connected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies what makes data distributions suitable for locally
  connected neural networks like CNNs and RNNs. It proves that a locally connected
  tensor network can fit a tensor if and only if that tensor admits low quantum entanglement
  under canonical partitions of its axes.
---

# What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement

## Quick Facts
- arXiv ID: 2303.11249
- Source URL: https://arxiv.org/abs/2303.11249
- Reference count: 40
- Primary result: Locally connected neural networks achieve accurate prediction if and only if data admits low quantum entanglement under canonical partitions

## Executive Summary
This paper establishes a necessary and sufficient condition for when data distributions are suitable for locally connected neural networks like CNNs and RNNs. The key insight is that such networks can achieve accurate prediction if and only if the data admits low quantum entanglement under canonical partitions of features. The authors prove this through a tensor network representation of locally connected neural networks and demonstrate the correlation experimentally across multiple datasets and model types. They also propose a preprocessing method to enhance data suitability by rearranging features to minimize entanglement, which improves prediction accuracy.

## Method Summary
The paper studies locally connected neural networks through tensor network theory, proving that these networks can fit a tensor if and only if it admits low quantum entanglement under canonical partitions. For practical application, they propose a feature rearrangement algorithm that searches for arrangements minimizing a surrogate for entanglement (based on Pearson correlation). The method is evaluated on binary classification tasks using Speech Commands, CIFAR10, and tabular datasets, training CNNs, S4 RNNs, and local self-attention models with standard optimization procedures.

## Key Results
- Locally connected neural networks achieve higher accuracy when data has lower entanglement under canonical partitions
- The feature rearrangement algorithm improves prediction accuracy by finding feature arrangements with lower entanglement
- Common locally connected architectures (CNN, S4, local self-attention) all show inverse correlation between entanglement and accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Low quantum entanglement under canonical partitions is necessary and sufficient for accurate prediction
- **Mechanism:** Tensor network representation proves that locally connected networks can fit tensors if and only if they admit low entanglement under canonical partitions
- **Core assumption:** Entanglement under canonical partitions accurately measures data locality for locally connected networks
- **Evidence anchors:** [abstract] theoretical result; [section 5.2] suboptimality small if and only if entanglement is low
- **Break condition:** If entanglement doesn't reflect data locality or other factors affect suitability

### Mechanism 2
- **Claim:** Feature rearrangement algorithm enhances data suitability by minimizing entanglement
- **Mechanism:** Algorithm searches for feature arrangements that minimize a Pearson correlation-based surrogate for entanglement
- **Core assumption:** Pearson correlation strongly correlates with actual entanglement
- **Evidence anchors:** [section 6.2] surrogate implementation and empirical agreement with entanglement
- **Break condition:** If Pearson correlation poorly approximates entanglement or algorithm is too costly

### Mechanism 3
- **Claim:** Common locally connected networks achieve higher accuracy with lower entanglement
- **Mechanism:** Experiments show inverse correlation between average entanglement under canonical partitions and test accuracy across multiple datasets and models
- **Core assumption:** Empirical data tensor entanglement reflects population tensor entanglement
- **Evidence anchors:** [section 5.3] test accuracy vs entanglement plots across CNN, S4, and local-attention models
- **Break condition:** If empirical entanglement poorly reflects population entanglement

## Foundational Learning

- **Concept:** Quantum entanglement and its measurement
  - **Why needed here:** Forms the basis of the theoretical result and practical algorithm
  - **Quick check question:** What is the definition of quantum entanglement for a tensor under a partition of its axes, and how is it computed?

- **Concept:** Tensor networks and their expressiveness
  - **Why needed here:** Used as theoretical model for studying locally connected neural networks
  - **Quick check question:** What is a tensor network, and how is it used to fit (represent) a tensor?

- **Concept:** Canonical partitions of features
  - **Why needed here:** Specific partitions under which low entanglement is necessary and sufficient for accurate prediction
  - **Quick check question:** What are the canonical partitions of the feature axes, and why are they important in this paper?

## Architecture Onboarding

- **Component map:** Data tensor -> Entanglement calculation -> Feature rearrangement (optional) -> Locally connected neural network training -> Prediction accuracy evaluation

- **Critical path:** 1) Compute entanglement under canonical partitions, 2) Apply feature rearrangement if entanglement is high, 3) Train locally connected neural network, 4) Evaluate prediction accuracy

- **Design tradeoffs:**
  - Computational cost of entanglement calculation vs. potential accuracy improvement
  - Quality of Pearson correlation as entanglement surrogate vs. computational efficiency
  - Choice of canonical partitions vs. capturing all relevant locality aspects

- **Failure signatures:** High entanglement despite local data, feature rearrangement fails to improve accuracy, surrogate doesn't correlate with actual entanglement

- **First 3 experiments:**
  1. Compute entanglement under canonical partitions for a simple synthetic dataset with known locality
  2. Apply feature rearrangement algorithm to a real-world dataset and evaluate impact on prediction accuracy
  3. Compare performance of locally connected neural networks on original vs. rearranged versions of a dataset with high entanglement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is there a polynomial-time algorithm to find the optimal feature arrangement that minimizes quantum entanglement under canonical partitions?
- **Basis in paper:** [inferred] The paper proposes a preprocessing method using a surrogate for entanglement, but notes that finding the optimal arrangement is computationally prohibitive.
- **Why unresolved:** The problem of finding the optimal balanced partition minimizing average entanglement is computationally intractable, and no polynomial-time approximation algorithm is known.
- **What evidence would resolve it:** Development of a polynomial-time algorithm or proof of NP-hardness for the feature arrangement problem.

### Open Question 2
- **Question:** How does the theory apply to neural networks with different activation functions, such as ReLU?
- **Basis in paper:** [explicit] The analysis focuses on a locally connected tensor network equivalent to a neural network with polynomial non-linearity, noting that other activation functions are more common in practice.
- **Why unresolved:** The paper does not extend the theoretical analysis to neural networks with ReLU or other activation functions commonly used in practice.
- **What evidence would resolve it:** Theoretical extension of the quantum entanglement framework to neural networks with different activation functions, or empirical studies demonstrating similar relationships between entanglement and prediction accuracy.

### Open Question 3
- **Question:** Can the theory be extended to explain the suitability of locally connected neural networks for tasks beyond binary classification?
- **Basis in paper:** [explicit] The paper's analysis and experiments focus on binary classification, with extensions to multi-class classification left for future work.
- **Why unresolved:** The current theoretical framework and empirical validation are limited to binary classification settings, leaving the applicability to other tasks unexplored.
- **What evidence would resolve it:** Extension of the theoretical analysis to multi-class classification, regression, and other machine learning tasks, accompanied by empirical validation.

### Open Question 4
- **Question:** How does the proposed feature arrangement algorithm compare to other representation learning methods in terms of prediction accuracy and computational efficiency?
- **Basis in paper:** [explicit] The paper proposes a feature arrangement algorithm based on minimizing entanglement and compares it to a heuristic scheme designed for convolutional neural networks.
- **Why unresolved:** The paper does not compare the proposed algorithm to a broader range of representation learning methods, such as autoencoders or contrastive learning approaches.
- **What evidence would resolve it:** Empirical comparison of the proposed algorithm to state-of-the-art representation learning methods on various datasets and tasks, evaluating both prediction accuracy and computational efficiency.

## Limitations
- Theoretical framework relies on quantum entanglement as a measure of data locality, which may not capture all relevant aspects
- Pearson correlation surrogate lacks theoretical guarantees for approximating actual entanglement
- Experiments limited to binary classification tasks, leaving multi-class generalization uncertain
- Computational cost of entanglement calculation and feature rearrangement may limit practical applicability

## Confidence
- **High Confidence**: The inverse correlation between entanglement under canonical partitions and prediction accuracy for locally connected networks
- **Medium Confidence**: The theoretical proof that low entanglement under canonical partitions is necessary and sufficient for accurate prediction
- **Medium Confidence**: The effectiveness of the feature rearrangement algorithm in improving prediction accuracy

## Next Checks
1. Test the theory and algorithm on multi-class classification tasks to verify generalization beyond binary classification
2. Systematically evaluate the Pearson correlation surrogate against exact entanglement calculations across diverse datasets to quantify approximation error
3. Measure computational costs and accuracy improvements when applying the feature rearrangement algorithm to larger datasets (e.g., ImageNet) to establish practical limits