---
ver: rpa2
title: Evidentiality-aware Retrieval for Overcoming Abstractiveness in Open-Domain
  Question Answering
arxiv_id: '2304.03031'
source_url: https://arxiv.org/abs/2304.03031
tags:
- picl
- counterfactual
- passages
- samples
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a method to improve dense passage retrieval\
  \ in open-domain question answering by addressing the issue of answer-unawareness.\
  \ The key idea is to use synthetic counterfactual samples\u2014passages with the\
  \ answer span removed\u2014as additional training resources."
---

# Evidentiality-aware Retrieval for Overcoming Abstractiveness in Open-Domain Question Answering

## Quick Facts
- arXiv ID: 2304.03031
- Source URL: https://arxiv.org/abs/2304.03031
- Reference count: 26
- Key outcome: EADPR achieves better retrieval and end-to-end QA performance than vanilla DPR on Natural Questions and TriviaQA benchmarks

## Executive Summary
This paper addresses answer-unawareness in dense passage retrieval for open-domain question answering by introducing Evidentiality-Aware Dense Passage Retrieval (EADPR). The method leverages synthetic counterfactual samples—passages with answer spans removed—as pivots between positive and negative samples in contrastive learning. By synchronizing relevance and answerability through counterfactual contrastive learning, EADPR consistently outperforms vanilla DPR, demonstrating improved retrieval accuracy and end-to-end QA performance across multiple benchmarks.

## Method Summary
EADPR improves dense passage retrieval by addressing answer-unawareness through counterfactual contrastive learning. The method generates synthetic counterfactual samples by removing answer spans from positive passages, creating unanswerable variants that serve as pivots between positive and negative samples in the embedding space. During training, EADPR employs a PiCL framework with three loss components: a modified DPR loss, a counterfactuals-as-hard-negatives loss, and a counterfactuals-as-pseudo-positives loss. This approach enables the retriever to learn representations that simultaneously capture relevance and answerability, leading to more effective evidence retrieval for downstream question answering tasks.

## Key Results
- EADPR consistently outperforms vanilla DPR on Natural Questions and TriviaQA benchmarks
- The approach achieves better retrieval performance (Top-1, Top-5, Top-20, Top-100 accuracy) and end-to-end QA accuracy
- EADPR demonstrates effectiveness across different settings and is orthogonal to existing model-centric approaches
- Answer-awareness rate (AAR) shows positive correlation with downstream QA performance

## Why This Works (Mechanism)

### Mechanism 1
Counterfactual samples act as pivots in embedding space to synchronize relevance and answerability. The counterfactual passages are positioned between original positive and negative samples, enabling contrastive learning that enforces both relevance and answerability alignment. Core assumption: Removing answer spans from positive passages creates samples that retain semantic relevance but lose answerability, making them ideal pivots. Evidence: Abstract and section on counterfactual samples as pivots. Break condition: If counterfactual samples are too dissimilar from original passages, they lose their pivoting property.

### Mechanism 2
Answer-awareness rate (AAR) correlates with downstream QA performance. Higher AAR indicates better synchronization between relevance scores and answerability, leading to improved retrieval and end-to-end QA accuracy. Core assumption: Dense retrievers that rank answerable passages higher than counterfactual counterparts will provide more informative evidence to readers. Evidence: Sections on answer-unawareness and AAR correlations. Break condition: If AAR improvements don't translate to performance gains, the correlation may not hold for certain question types.

### Mechanism 3
Counterfactual contrastive learning improves retrieval efficiency. Using counterfactual samples as pivots allows learning with fewer negative samples while maintaining discriminative power, as counterfactuals serve dual roles as hard negatives and pseudo-positives. Core assumption: The semantic overlap between counterfactual and original passages provides meaningful contrastive signals. Evidence: Sections on PiCL efficiency and average similarity differences. Break condition: If semantic overlap is insufficient, counterfactual samples may not provide effective contrastive signals.

## Foundational Learning

- Concept: Counterfactual inference and contrastive learning
  - Why needed here: Understanding how synthetic samples can be used to learn invariant representations is crucial for grasping the pivoting mechanism.
  - Quick check question: What makes counterfactual samples effective pivots between positive and negative samples in embedding space?

- Concept: Dense passage retrieval architecture
  - Why needed here: Knowing how dual-encoder models like DPR work is essential for understanding where answer-unawareness occurs.
  - Quick check question: How does DPR measure relevance between questions and passages, and where does answer-awareness break down?

- Concept: Answerability measurement in ODQA
  - Why needed here: Understanding how answer spans serve as causal signals helps explain why removing them creates effective counterfactuals.
  - Quick check question: Why is the presence of answer spans critical for determining answerability in ODQA tasks?

## Architecture Onboarding

- Component map: Question encoder fq -> Passage encoder fp -> PiCL training framework -> ANN search using FAISS -> Reader model

- Critical path:
  1. Generate counterfactual samples by removing answer spans
  2. Train DPR with PiCL loss combining three objectives
  3. Use trained retriever for efficient passage retrieval
  4. Pass top-k passages to reader for answer extraction

- Design tradeoffs:
  - Simple counterfactual sampling vs. more sophisticated masking strategies
  - Computational cost of PiCL vs. model-centric approaches
  - Balance between semantic relevance and answerability in contrastive learning

- Failure signatures:
  - Low AAR despite training with counterfactuals
  - Minimal performance improvement over vanilla DPR
  - Counterfactual samples ranked higher than original passages

- First 3 experiments:
  1. Measure AAR of vanilla DPR on NQ test set to establish baseline
  2. Train DPR+PiCL with in-batch negatives and compare retrieval accuracy
  3. Evaluate end-to-end QA performance with different reader models (extractive vs. FiD)

## Open Questions the Paper Calls Out

### Open Question 1
How would the performance of EADPR change if we used more sophisticated counterfactual sampling strategies beyond simply removing the answer span? The paper acknowledges that their simple counterfactual sampling strategy leaves room for improvement and mentions that more elaborate construction methods would lead to better counterfactual samples and further enhance performance. This remains unresolved as the paper only experiments with one simple method.

### Open Question 2
Can the concept of evidentiality-aware retrieval be extended to non-factoid ODQA tasks such as multi-hop reasoning or commonsense reasoning? The paper focuses on factoid questions, but the core insight about answer-unawareness and the general approach of using counterfactuals could potentially apply to other ODQA settings. This remains unresolved as the paper only evaluates on factoid QA benchmarks.

### Open Question 3
How does the performance of EADPR scale with increasing corpus size and question complexity? The paper evaluates on standard ODQA benchmarks with fixed corpus sizes, but real-world applications often involve much larger corpora and more complex questions. This remains unresolved as the paper doesn't explore how EADPR's effectiveness changes as the search space grows or as questions become more complex.

## Limitations

- Reliance on counterfactual samples that may not perfectly represent the semantic space between positive and negative passages
- Computational overhead of generating and processing counterfactual samples during training could impact scalability
- Limited exploration of alternative counterfactual generation strategies beyond simple answer span removal

## Confidence

**High Confidence**: The core mechanism of using counterfactual samples as pivots in contrastive learning is well-supported by experimental results, with consistent improvements across multiple retrieval metrics and end-to-end QA performance.

**Medium Confidence**: The claim that counterfactual samples serve as effective hard negatives and pseudo-positives is supported by ablation studies, but the optimal balance between these roles may vary across different datasets.

**Medium Confidence**: The correlation between answer-awareness rate (AAR) and downstream QA performance is observed in experiments, but causation is not established and the relationship may not hold for all question types.

## Next Checks

1. **Dataset Generalization Test**: Evaluate EADPR on additional ODQA datasets beyond Natural Questions and TriviaQA to assess generalizability and identify potential failure modes for different question types or domains.

2. **Counterfactual Generation Ablation**: Systematically compare different counterfactual generation strategies (e.g., random masking, entity replacement, semantic masking) to determine whether the specific method of answer span removal is critical to performance.

3. **Scaling Analysis**: Measure the computational overhead and performance trade-offs when scaling EADPR to larger document collections (e.g., full Wikipedia vs. curated subsets) to evaluate practical deployment considerations.