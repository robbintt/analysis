---
ver: rpa2
title: Constructing and Interpreting Causal Knowledge Graphs from News
arxiv_id: '2305.09359'
source_url: https://arxiv.org/abs/2305.09359
tags:
- causal
- arg1
- arg0
- https
- relations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to construct causal knowledge graphs
  from news using BERT-based extraction models alongside pattern-based ones. The approach
  includes argument clustering and representation into a knowledge graph to improve
  connectivity and interpretability.
---

# Constructing and Interpreting Causal Knowledge Graphs from News

## Quick Facts
- arXiv ID: 2305.09359
- Source URL: https://arxiv.org/abs/2305.09359
- Reference count: 40
- One-line primary result: F1 score of 74.23% on human-annotated test set for causal relation extraction from news

## Executive Summary
This paper proposes a method to construct causal knowledge graphs from news using BERT-based extraction models alongside pattern-based ones. The approach includes argument clustering and representation into a knowledge graph to improve connectivity and interpretability. Experiments on a human-annotated test set show an F1 score of 74.23%, with the BERT-based method outperforming the pattern-based approach in recall. The final knowledge graph effectively captures and conveys causal relationships, validated through use cases and user feedback. The method addresses the challenge of extracting and representing causal events from unstructured news text.

## Method Summary
The method involves two main phases: extraction and representation. For extraction, the authors use both pattern-based and BERT-based approaches to identify causal relations in news articles. The BERT-based method employs fine-tuned models (CSC, CPC, CESD) on multiple causal datasets, while pattern-based extraction uses linguistic patterns. For representation, arguments are clustered using SimCSE embeddings and K-Means after removing named entities, then represented in a knowledge graph where nodes represent topics and edges represent causal relations weighted by supporting sentence count.

## Key Results
- BERT-based extraction achieved 71.43% recall compared to 4.08% for pattern-based extraction
- Argument clustering reduced disconnected subgraphs from 15,686 to 1 connected graph
- Final knowledge graph achieved F1 score of 74.23% on human-annotated test set

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: BERT-based extraction significantly outperforms pattern-based extraction in recall while maintaining reasonable precision for causal relation extraction from news.
- **Mechanism**: The BERT-based model leverages contextualized embeddings and fine-tuning on multiple causal datasets, enabling it to recognize implicit and varied causal constructions that rigid linguistic patterns miss.
- **Core assumption**: Pre-trained language models, when fine-tuned, generalize better to linguistic variations than handcrafted patterns.
- **Evidence anchors**:
  - [abstract]: "BERT-based method outperforming the pattern-based approach in recall" with 71.43% recall vs 4.08%.
  - [section 4.1]: BERT-based extracted 19x more causal relations than pattern-based for the full dataset.
  - [corpus]: No direct corpus evidence; claim is supported by internal evaluation only.
- **Break condition**: If the BERT model is not sufficiently fine-tuned or if the training data does not represent the domain's linguistic patterns, recall gains may diminish.

### Mechanism 2
- **Claim**: Argument clustering based on semantic embeddings increases graph connectivity and interpretability.
- **Mechanism**: Named-entity removal followed by SimCSE embeddings and K-Means clustering groups semantically similar arguments, reducing sparsity and enabling transitive inference.
- **Core assumption**: Semantically similar arguments (ignoring named entities) refer to the same underlying event or concept.
- **Evidence anchors**:
  - [abstract]: "topic modelling approach to cluster our arguments, so as to increase the connectivity of our graph."
  - [section 3.3]: Clustering increased graph density, reduced subgraphs from 15,686 to 1 connected graph.
  - [section 4.2]: Clustering enabled detection of richer causal relationships (e.g., pandemic → supply chain disruption).
- **Break condition**: If embeddings fail to capture semantic similarity accurately, clusters may merge unrelated concepts or split related ones.

### Mechanism 3
- **Claim**: Post-processing BERT predictions with Causal Pair Classification (CPC) recovers additional valid causal relations missed by initial filtering.
- **Mechanism**: By retaining examples with multiple arguments and using CPC to validate pairs, the system avoids discarding potentially causal relations prematurely.
- **Core assumption**: CPC predictions are reliable indicators of causality between any given argument pair.
- **Evidence anchors**:
  - [section 3.2]: "We also used CPC to check for additional causal examples to retain."
  - [section 4.1]: 5 out of 12 additional relations predicted by BERT were correct after CPC validation.
  - [corpus]: No external corpus evidence; validation is internal and small-scale.
- **Break condition**: If CPC model's precision drops, false positives will increase, harming overall system quality.

## Foundational Learning

- **Concept**: Dependency parsing and linguistic pattern matching
  - Why needed here: Pattern-based extraction relies on dependency paths between nouns to identify causal relations.
  - Quick check question: Can you manually trace the shortest dependency path between "shortage" and "impact" in a sample sentence?

- **Concept**: Named Entity Recognition (NER) and its role in argument clustering
  - Why needed here: Named entities (organizations, locations, dates) are removed before clustering to avoid grouping by entity identity rather than event semantics.
  - Quick check question: Given "Tesla in China produced EVs", which tokens would NER mark and remove before clustering?

- **Concept**: Knowledge graph construction and edge weighting
  - Why needed here: Edges are weighted by the number of sentences supporting a causal relation, reflecting confidence and importance.
  - Quick check question: If a causal relation appears in 5 different sentences, what should its edge weight be in the KG?

## Architecture Onboarding

- **Component map**: Data Ingestion → BERT-based Extractor + Pattern-based Extractor → Post-processing (CPC, merging) → NER Neutralization → SimCSE Embedding → K-Means Clustering → Knowledge Graph Construction
- **Critical path**: Extraction → Post-processing → Clustering → Graph construction. Failures early in extraction cascade downstream.
- **Design tradeoffs**:
  - BERT vs patterns: recall vs precision balance.
  - Clustering granularity: too few clusters merge distinct events; too many split coherent ones.
  - Edge weighting: frequency-based vs confidence-based weighting.
- **Failure signatures**:
  - Low recall: BERT not fine-tuned, patterns too restrictive, CPC not applied.
  - High false positives: CPC too permissive, clustering merges unrelated concepts.
  - Sparse graph: NER not removing entities, clustering too fine-grained.
- **First 3 experiments**:
  1. Run BERT and pattern extractors on a small annotated subset; compare TP/FP/FN counts.
  2. Apply NER and clustering on extracted arguments; visualize clusters to check coherence.
  3. Construct KG from clustered nodes; measure connectivity (average degree, clustering coefficient) before and after clustering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the causal extraction models vary across different news domains or topics?
- Basis in paper: [inferred] The paper mentions focusing on electronics and supply-chain industry news, but does not provide a detailed analysis of model performance across different news topics or domains.
- Why unresolved: The paper only presents a single aggregated performance metric (F1 score of 74.23%) without breaking down performance by news domain or topic. This leaves uncertainty about whether the model performs equally well across different types of news content.
- What evidence would resolve it: A comprehensive evaluation showing F1 scores, precision, and recall for each news domain or topic, along with examples of domain-specific challenges encountered during extraction.

### Open Question 2
- Question: What is the optimal number of clusters for argument clustering to balance graph connectivity and interpretability?
- Basis in paper: [explicit] The paper uses 3,000 clusters based on previous work [25,29], but does not explore how different cluster counts affect the final knowledge graph's utility.
- Why unresolved: The choice of 3,000 clusters appears to be based on prior research rather than empirical validation for this specific application. The paper does not investigate how changing the number of clusters impacts graph density, connectivity, or user interpretability.
- What evidence would resolve it: A systematic evaluation comparing knowledge graphs with different numbers of clusters, measuring metrics like average edge weight, clustering coefficient, and user feedback on interpretability across different cluster counts.

### Open Question 3
- Question: How do temporal and sentiment elements enhance the causal knowledge graph's predictive capabilities?
- Basis in paper: [explicit] The paper mentions that users would like to see temporal and sentiment elements added in the future, suggesting these could improve the KG's utility for prediction tasks.
- Why unresolved: The paper does not implement or evaluate temporal or sentiment features, leaving their potential impact on causal relationship prediction and trend analysis unexplored.
- What evidence would resolve it: An implementation of temporal and sentiment-enriched causal extraction, followed by user studies or quantitative evaluation of how these features improve the KG's ability to predict future events or identify causal trends over time.

## Limitations
- Evaluation relies on a single human-annotated test set without external validation on other corpora, limiting generalizability.
- BERT-based extraction improvements come at the cost of significantly lower precision, raising concerns about scalability to larger datasets.
- The clustering methodology assumes named-entity removal improves semantic grouping, but this may discard meaningful contextual information.
- CPC post-processing adds computational overhead and introduces additional complexity without clear precision gains shown in the paper.

## Confidence

- **High confidence**: BERT-based extraction recall advantage (supported by clear F1 and recall metrics).
- **Medium confidence**: Argument clustering improves graph connectivity (supported by density metrics but not qualitative analysis of cluster coherence).
- **Medium confidence**: CPC post-processing recovers valid causal relations (limited by small sample size and no precision measurement).

## Next Checks

1. Validate BERT-based extraction on an external causal dataset to assess generalizability beyond the in-house test set.
2. Conduct ablation study removing NER neutralization to measure its impact on clustering quality and downstream KG interpretability.
3. Measure precision-recall trade-offs at scale by running the full pipeline on a larger corpus and analyzing the ratio of false positives to true positives in the final KG.