---
ver: rpa2
title: Large Multimodal Model Compression via Efficient Pruning and Distillation at
  AntGroup
arxiv_id: '2312.05795'
source_url: https://arxiv.org/abs/2312.05795
tags:
- pruning
- arxiv
- performance
- distillation
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-stage compression strategy for Large
  Multimodal Models (LMMs), specifically targeting the AntGMM model. The approach
  addresses the challenges of high latency and energy consumption in deploying large
  models like AntGMM.
---

# Large Multimodal Model Compression via Efficient Pruning and Distillation at AntGroup
## Quick Facts
- arXiv ID: 2312.05795
- Source URL: https://arxiv.org/abs/2312.05795
- Reference count: 40
- Primary result: 18x model size reduction (5.4B→0.3B parameters) with 700ms→90ms latency improvement while maintaining performance

## Executive Summary
This paper presents a multi-stage compression strategy for Large Multimodal Models (LMMs), specifically targeting the AntGMM model used in Alipay's advertising scenarios. The approach addresses the challenges of high latency and energy consumption in deploying large models. Through a three-stage pruning pipeline combined with advanced distillation techniques, the method achieves an 18x reduction in model size and 87% reduction in latency while maintaining competitive performance across five advertisement auditing tasks.

## Method Summary
The compression strategy employs a three-stage pruning approach: block pruning (removing entire blocks), inter-module dimension pruning (reducing FFN and attention dimensions), and input/output dimension pruning. Parameter importance is calculated using first-order gradient information on a small dataset. A modified distillation loss function incorporates both KL divergence and pairwise loss to ensure correct token generation. The process is applied to the AntGMM model on a custom MAAD dataset, using AdaW optimizer with learning rate 3e-5 on A100 GPUs.

## Key Results
- 18x model size reduction (from 5.4B to 0.3B parameters)
- Latency improvement from 700ms to 90ms (87% reduction)
- Performance maintained across five tasks with only slight decreases (color: 0.924→0.920, manifestations: 0.971→0.971, content: 0.916→0.916, logo: 0.981→0.981, style: 0.825→0.825, overall: 0.923→0.923)
- Estimated 75 million kWh annual electricity savings compared to direct AntGMM deployment

## Why This Works (Mechanism)
### Mechanism 1: Multi-stage pruning addresses multi-level redundancy
The approach systematically reduces redundancy across three levels—block numbers, intermediate-module dimensions, and input/output dimensions—using structured pruning guided by first-order gradient importance scores. Each stage is followed by distillation to recover performance. The core assumption is that redundancy exists at multiple levels and can be reduced incrementally without catastrophic performance degradation.

### Mechanism 2: First-order gradient-based importance calculation
Parameter importance is estimated using the absolute value of the gradient of the loss with respect to that parameter, calculated on a small dataset. This guides which dimensions or blocks to prune at each stage. The core assumption is that the first-order gradient approximation accurately reflects the importance of parameters for maintaining model performance.

### Mechanism 3: Pairwise loss in distillation ensures correct token generation
The distillation loss includes both KL divergence between teacher and student distributions and a pairwise loss that ensures the logit for the correct token is higher than for the most probable incorrect token. The core assumption is that the pairwise loss effectively addresses the cascading nature of token generation errors in LMMs.

## Foundational Learning
- **Multimodal model architecture (BLIP-2-like structure)**: Understanding the model structure is essential for comprehending how pruning and distillation affect different components. *Quick check*: What are the main components of the AntGMM model and how do they interact?
- **Structured pruning vs. unstructured pruning**: The paper specifically uses structured pruning, which removes entire neurons or layers rather than individual weights. *Quick check*: What is the key difference between structured and unstructured pruning, and why is structured pruning preferred for hardware efficiency?
- **Knowledge distillation and loss functions**: The paper uses a modified distillation loss with pairwise components, requiring understanding of standard distillation approaches. *Quick check*: What is the purpose of knowledge distillation, and how does the standard KL divergence loss differ from the pairwise loss proposed in this paper?

## Architecture Onboarding
- **Component map**: AntGMM model (visual encoder → Q-Former → language model) → Three-stage pruning pipeline (block → inter-module → input/output) → Distillation with pairwise loss → Evaluation on MAAD dataset (5 tasks)
- **Critical path**: 1) Calculate parameter importance using first-order gradients 2) Perform block pruning and distillation 3) Perform inter-module dimension pruning and distillation 4) Perform input/output dimension pruning and distillation 5) Evaluate performance on test dataset
- **Design tradeoffs**: Multi-stage vs. one-shot pruning (multi-stage allows gradual recovery but takes longer); Small dataset vs. large dataset for pruning (small is more efficient but may miss redundancies); Pairwise loss vs. standard distillation (pairwise better addresses token errors but adds complexity)
- **Failure signatures**: Performance drops exceeding threshold α after pruning stage; Distillation fails to recover performance after pruning; Model size reduction without corresponding latency improvement
- **First 3 experiments**: 1) Run block pruning on a small subset of the dataset and measure performance impact 2) Apply inter-module dimension pruning to a single block type and verify importance calculation 3) Test pairwise loss distillation on a simple sequence generation task to observe error correction

## Open Questions the Paper Calls Out
### Open Question 1
How does the performance of the compressed model vary across different types of advertisement content (e.g., e-commerce products, services, digital content)? The paper provides overall performance metrics but does not break down performance by specific task types or content categories. Detailed performance metrics for each of the five tasks would clarify how the model's compression affects its ability to handle various kinds of multimodal data.

### Open Question 2
What is the impact of using a smaller dataset for training the compressed model on its generalization capabilities in real-world scenarios? The paper discusses using a small training dataset (12,000 samples) for the compression process but does not explore how the compressed model performs when exposed to data that significantly differs from the training set or how it generalizes to new, unseen advertisement scenarios.

### Open Question 3
How does the multi-stage pruning approach compare to other model compression techniques in terms of computational efficiency and model performance? The paper introduces a novel multi-stage pruning strategy and compares it to single-stage and one-shot pruning, but does not provide a comprehensive comparison with other state-of-the-art model compression techniques beyond the ones mentioned.

## Limitations
- Lack of detailed architectural specifications for the AntGMM model makes exact reproduction challenging
- MAAD dataset is not publicly available, preventing independent validation of results
- Energy consumption estimates (75 million kWh annually) are based on assumptions about deployment scale rather than direct measurement

## Confidence
- **High Confidence**: The effectiveness of multi-stage pruning for reducing model size (18x reduction) and improving inference speed (latency reduction from 700ms to 90ms)
- **Medium Confidence**: The maintenance of model performance across tasks after compression, with slight decreases but close to original levels
- **Low Confidence**: The energy consumption estimates and the superiority of the pairwise loss approach, which rely on external assumptions and lack direct experimental validation

## Next Checks
1. Reimplement and benchmark the pairwise distillation loss: Create a controlled experiment comparing standard KL divergence distillation with the proposed pairwise loss on a simple sequence generation task, measuring both accuracy and error propagation rates.
2. Validate the three-stage pruning approach on a public multimodal model: Apply the multi-stage pruning methodology to a publicly available model like BLIP-2 or MiniGPT-4, documenting performance changes at each stage and testing the recovery effectiveness of the distillation process.
3. Analyze the energy consumption model assumptions: Conduct a sensitivity analysis on the energy consumption estimates by varying the deployment assumptions (number of models, inference frequency, hardware efficiency) to determine the robustness of the 75 million kWh annual savings claim.