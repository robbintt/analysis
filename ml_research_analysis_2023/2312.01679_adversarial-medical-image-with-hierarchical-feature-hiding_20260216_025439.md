---
ver: rpa2
title: Adversarial Medical Image with Hierarchical Feature Hiding
arxiv_id: '2312.01679'
source_url: https://arxiv.org/abs/2312.01679
tags:
- adversarial
- medical
- attacks
- feature
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of medical images to
  adversarial attacks, which pose a security flaw in clinical decision-making. The
  authors theoretically prove that conventional adversarial attacks change the outputs
  by continuously optimizing vulnerable features in a fixed direction, leading to
  outlier representations in the feature space.
---

# Adversarial Medical Image with Hierarchical Feature Hiding

## Quick Facts
- arXiv ID: 2312.01679
- Source URL: https://arxiv.org/abs/2312.01679
- Reference count: 40
- Primary result: HFC-enhanced attacks bypass state-of-the-art medical adversarial detectors more efficiently than competing methods

## Executive Summary
This paper addresses the vulnerability of medical images to adversarial attacks by proposing a hierarchical feature constraint (HFC) method. The authors demonstrate that medical images are more susceptible to adversarial perturbations than natural images due to their feature distributions being easier to push into outlier regions. They theoretically prove that conventional attacks create detectable outliers by optimizing features in fixed gradient directions. To counter this, HFC uses Gaussian mixture models to model clean feature distributions and encourages adversarial features to move toward high-density regions, effectively hiding them from detection.

## Method Summary
The proposed method extends conventional white-box adversarial attacks by adding a hierarchical feature constraint term to the loss function. The HFC module fits Gaussian mixture models to clean feature distributions for each target class using training data. During attack generation, it calculates the log-likelihood of adversarial features under these GMMs and maximizes it to pull features back into normal density regions. The method is evaluated on three medical datasets (Fundoscopy, Chest X-ray, Brain CT) using various attack methods including BIM, PGD, and CW, and tested against multiple state-of-the-art adversarial detectors.

## Key Results
- Medical images show higher vulnerability to adversarial attacks compared to natural images
- HFC-enhanced attacks achieve significantly lower detection rates than standard attacks
- The method successfully bypasses KD, MAHA, LID, SVM, DNN, and BU detectors with higher TPR@90 scores

## Why This Works (Mechanism)

### Mechanism 1
Medical image features are more vulnerable to adversarial perturbation than natural image features due to their lower-dimensional or more constrained feature distributions, making them easier to push into outlier regions under attack.

### Mechanism 2
Conventional adversarial attacks push features in a consistent gradient direction, creating detectable outliers. The attack optimizes logits by increasing target class logit and decreasing others, leading to fixed gradient directions for certain feature components across iterations and attacks.

### Mechanism 3
Hierarchical Feature Constraint (HFC) hides adversarial features by moving them into high-density regions of clean feature distributions. HFC fits GMM to clean features per layer and maximizes log-likelihood, pulling adversarial features toward normal clusters during attack optimization.

## Foundational Learning

- Concept: Gaussian Mixture Models (GMM)
  - Why needed here: HFC uses GMM to model clean feature distributions for each layer to guide adversarial features back into normal regions
  - Quick check question: What parameters define a GMM and how are they learned?

- Concept: t-SNE dimensionality reduction
  - Why needed here: Used to visualize high-dimensional feature space differences between clean and adversarial samples
  - Quick check question: How does t-SNE preserve local structure and what hyperparameters control cluster separation?

- Concept: Adversarial attack optimization (FGSM, PGD, BIM)
  - Why needed here: Understanding how gradient-based attacks manipulate model outputs and features is critical for extending them with HFC
  - Quick check question: What is the difference between one-step and iterative attacks in terms of perturbation accumulation?

## Architecture Onboarding

- Component map: Victim model (ResNet-50/VGG-16) -> Attacker (BIM/PGD/CW) -> HFC module (GMM trainer + log-likelihood calculator) -> Detector (KD/MAHA/LID/SVM/DNN/BU)
- Critical path: 1) Train victim model on clean images, 2) Train HFC GMMs on clean features of target class, 3) Generate adversarial examples with base attack + HFC loss, 4) Evaluate detector success rate
- Design tradeoffs: GMM complexity vs. fitting accuracy (M=16,64 tuned per dataset), HFC weight λl vs. attack success vs. feature hiding, perturbation budget vs. detector bypass capability
- Failure signatures: HFC ineffective if clean features are sparse or multimodal, detector bypass fails if perturbation budget too small or λl too low, GMM fitting fails if training data too small or noisy
- First 3 experiments: 1) Validate GMM fits clean features well (visualize density contours), 2) Test HFC alone (no attack) to ensure it doesn't alter clean predictions, 3) Compare feature t-SNE plots: clean vs. adversarial vs. HFC-adversarial

## Open Questions the Paper Calls Out

### Open Question 1
How can the robustness of medical deep learning models be improved to reduce vulnerability to adversarial attacks? The paper states "We hope it can inspire stronger defenses in the future" and discusses the need for robust defenses against medical AEs, but does not provide a comprehensive solution for improving model robustness against adversarial attacks.

### Open Question 2
Can the proposed Hierarchical Feature Constraint (HFC) method be extended to other domains beyond medical imaging? The paper focuses on medical images but mentions that the HFC can be applied to any existing attack, suggesting potential applicability to other domains, though it only evaluates HFC on medical image datasets.

### Open Question 3
What are the limitations of using HFC for detecting out-of-distribution (OOD) signals in real-world scenarios? The paper mentions "It is a limitation of our method to directly apply on auto-PGD (APGD)" and discusses the potential of HFC for OOD detection, but does not provide a comprehensive analysis of the limitations and challenges of using HFC for OOD detection in real-world scenarios.

## Limitations

- The theoretical claims about fixed gradient directions rely on simplifying assumptions about DNN optimization dynamics that may not hold for all architectures or datasets
- The GMM-based feature hiding assumes feature distributions are approximately Gaussian and unimodal, which may not generalize to all medical imaging domains or feature layers
- The transferability of HFC-augmented attacks to black-box settings is not extensively evaluated

## Confidence

- Confidence: Medium - The theoretical claims about fixed gradient directions rely on simplifying assumptions about DNN optimization dynamics
- Confidence: Medium - The GMM-based feature hiding assumes feature distributions are approximately Gaussian and unimodal
- Confidence: Low - The transferability of HFC-augmented attacks to black-box settings is not extensively evaluated

## Next Checks

1. Test HFC effectiveness across different GMM complexity levels (varying M) and alternative density estimation methods to verify robustness to distribution assumptions

2. Evaluate HFC performance against a broader range of detector architectures including ensemble methods and neural network-based detectors to assess generalizability

3. Conduct ablation studies removing HFC from different layers to identify which feature hierarchies contribute most to attack success and detector evasion