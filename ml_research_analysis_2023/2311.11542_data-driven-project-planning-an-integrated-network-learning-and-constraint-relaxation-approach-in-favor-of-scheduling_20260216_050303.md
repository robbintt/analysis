---
ver: rpa2
title: 'Data-driven project planning: An integrated network learning and constraint
  relaxation approach in favor of scheduling'
arxiv_id: '2311.11542'
source_url: https://arxiv.org/abs/2311.11542
tags:
- project
- projects
- petri
- decision
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a data-driven approach for project planning
  that leverages historical project data and process mining techniques to automatically
  construct flexible project models. By relaxing temporal constraints embedded in
  individual projects, the approach uncovers scheduling flexibility and provides planners
  with multiple project variations from which to select.
---

# Data-driven project planning: An integrated network learning and constraint relaxation approach in favor of scheduling

## Quick Facts
- arXiv ID: 2311.11542
- Source URL: https://arxiv.org/abs/2311.11542
- Reference count: 34
- This work introduces a data-driven approach for project planning that leverages historical project data and process mining techniques to automatically construct flexible project models, demonstrating up to 26% reduction in critical path duration and 63 days of additional scheduling flexibility.

## Executive Summary
This paper presents a novel data-driven approach for project planning that automatically constructs flexible project models from historical project data. By leveraging process mining techniques and relaxing temporal constraints embedded in individual projects, the approach uncovers hidden scheduling flexibility and provides planners with multiple project variations from which to select. The methodology employs inductive mining algorithms to learn project Petri nets from event logs, filters infrequent paths based on frequency thresholds, and applies machine learning to identify decision rules that guide project configuration.

## Method Summary
The approach combines inductive mining algorithms with constraint relaxation to automatically discover project variations hidden in historical project data. It starts by constructing a project Petri net model from complete event logs using inductive mining, which recursively splits logs into sub-logs using sequence, exclusive choice, parallel, and redo loop cuts. The method then filters infrequent paths based on frequency thresholds to simplify the model while preserving common execution patterns. Finally, decision rules are learned from project features to explain the model's exclusive choices and guide planners in selecting appropriate project variations.

## Key Results
- Up to 26% reduction in critical path duration demonstrated through experimental results
- 63 days of additional scheduling flexibility identified for residential construction projects
- Decision rules provide explainable models that aid planners in selecting optimal project variations and resource allocations

## Why This Works (Mechanism)

### Mechanism 1
The inductive mining algorithm discovers project variations that are hidden in individual project plans by analyzing multiple similar projects. IM recursively splits the event log into sub-logs using four types of cuts (sequence, exclusive choice, parallel, redo loop) based on directly-follows relationships. This uncovers parallel execution opportunities and alternative sequences not visible in any single project.

### Mechanism 2
Filtering rare project variations based on frequency thresholds simplifies the model while preserving the most common and likely execution paths. Flow relations with frequencies below a threshold (e.g., 5% of cases) are removed, eliminating infrequent paths that may represent exceptions or noise.

### Mechanism 3
Decision rules learned from project features explain the model's exclusive choices and guide planners in selecting appropriate project variations. Machine learning models (e.g., decision trees) are trained on project-level features to predict which path to take at decision points, providing interpretable rules for planners.

## Foundational Learning

- Concept: Petri nets and process trees
  - Why needed here: They provide constructs (AND, XOR, loops) that compactly represent multiple project variations and execution semantics needed for learning from event logs.
  - Quick check question: Can you explain how a process tree with XOR operators represents alternative project paths that an AON diagram cannot capture?

- Concept: Inductive mining algorithm
  - Why needed here: It automatically discovers process models from event logs by applying cuts based on directly-follows relationships, enabling data-driven project network construction.
  - Quick check question: What are the four types of cuts used in inductive mining and what process tree operators do they correspond to?

- Concept: Resource-constrained project scheduling
  - Why needed here: After relaxing constraints and uncovering flexibility, standard scheduling techniques are applied to find optimal schedules considering resource limitations.
  - Quick check question: How does relaxing precedence constraints in the learned model affect the potential duration reduction when applying resource-constrained scheduling?

## Architecture Onboarding

- Component map: Event log preprocessing → Inductive mining → Frequency filtering → Decision rule learning → Petri net enrichment → Resource-constrained scheduling
- Critical path: The sequence from event log to enriched Petri net must complete before scheduling can begin; delays in mining or filtering directly impact planning timelines.
- Design tradeoffs: Higher frequency thresholds simplify the model but may lose flexibility; more decision rules provide better guidance but require more data and computation.
- Failure signatures: Empty Petri nets (no paths remain after filtering), decision rules that always predict the same path regardless of features, or scheduling algorithms that fail to converge due to overly relaxed constraints.
- First 3 experiments:
  1. Apply inductive mining to a small synthetic event log with known variations and verify the discovered Petri net matches expected structure.
  2. Test frequency filtering with different thresholds on the same log and observe how the number of paths and critical path duration change.
  3. Generate synthetic project features and verify that decision rules learned from them correctly predict path selection in the Petri net.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the data-driven project planning approach compare to traditional manual project planning methods in terms of reducing project duration and improving resource allocation?
- Basis in paper: The paper mentions that the proposed approach can provide significant flexibility, such as up to a 26% reduction in critical path duration, but does not directly compare it to traditional methods.
- Why unresolved: The paper does not provide a direct comparison with traditional project planning methods, which would require a controlled study.
- What evidence would resolve it: Conducting a study that compares the outcomes of projects planned using the data-driven approach versus those planned using traditional methods, focusing on metrics like project duration and resource allocation efficiency.

### Open Question 2
- Question: What are the long-term impacts of using the data-driven project planning approach on organizational efficiency and project success rates?
- Basis in paper: The paper suggests potential benefits of the approach, such as flexibility and improved scheduling, but does not explore long-term impacts on organizational efficiency or success rates.
- Why unresolved: The paper focuses on the immediate benefits of the approach without examining its sustained effects over time.
- What evidence would resolve it: Longitudinal studies tracking organizational efficiency and project success rates over multiple projects and time periods using the data-driven approach.

### Open Question 3
- Question: How can the approach be adapted to handle projects with high variability and uncertainty in activity durations and dependencies?
- Basis in paper: The paper acknowledges the complexity and uncertainty levels of projects but does not address how the approach can be adapted for projects with high variability in activity durations and dependencies.
- Why unresolved: The paper does not provide a framework for adapting the approach to projects with significant variability and uncertainty.
- What evidence would resolve it: Developing and testing an extension of the approach that incorporates techniques for handling variability and uncertainty, such as stochastic modeling or robust optimization.

### Open Question 4
- Question: What are the limitations of using Petri nets for modeling projects with non-linear dependencies and feedback loops?
- Basis in paper: The paper discusses the use of Petri nets for modeling projects but does not explore their limitations in handling non-linear dependencies and feedback loops.
- Why unresolved: The paper does not address potential challenges in using Petri nets for complex project structures with non-linear dependencies.
- What evidence would resolve it: Analyzing case studies of projects with non-linear dependencies and feedback loops to identify specific limitations of Petri nets and proposing alternative modeling techniques if necessary.

## Limitations
- Limited external validation due to absence of citations and low relevance scores in related literature
- Effectiveness claims rely primarily on experimental results without broader community validation
- Potential dataset-specific results that may not generalize across different project domains

## Confidence

- **High confidence** in the methodological framework combining inductive mining, frequency filtering, and decision rule learning as a coherent approach for uncovering project scheduling flexibility.
- **Medium confidence** in the specific effectiveness metrics (26% reduction, 63 days) due to limited corpus validation and potential dataset-specific results.
- **Low confidence** in generalizability across different project domains without additional testing on diverse project types beyond residential construction.

## Next Checks

1. **Cross-domain validation**: Apply the methodology to project datasets from different domains (manufacturing, software development, healthcare) to test generalizability of the 26% critical path reduction claim.

2. **Sensitivity analysis**: Systematically vary the frequency threshold parameter (γ) across a wider range (1%-20%) to quantify its impact on model complexity, decision rule quality, and scheduling flexibility.

3. **Comparative analysis**: Benchmark against established project planning approaches (e.g., CPM, PERT) using identical datasets to establish whether the data-driven approach provides statistically significant improvements in both planning time and schedule performance.