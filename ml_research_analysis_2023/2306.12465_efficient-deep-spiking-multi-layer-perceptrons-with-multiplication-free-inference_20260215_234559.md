---
ver: rpa2
title: Efficient Deep Spiking Multi-Layer Perceptrons with Multiplication-Free Inference
arxiv_id: '2306.12465'
source_url: https://arxiv.org/abs/2306.12465
tags:
- spiking
- network
- neural
- networks
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a spiking Multi-Layer Perceptron (MLP) architecture
  that is fully compatible with Multiplication-Free Inference (MFI), which is essential
  for efficient spike-based computation. The key idea is to use batch normalization
  and a spiking patch encoding module to retain MFI compatibility while enhancing
  local feature extraction.
---

# Efficient Deep Spiking Multi-Layer Perceptrons with Multiplication-Free Inference

## Quick Facts
- arXiv ID: 2306.12465
- Source URL: https://arxiv.org/abs/2306.12465
- Reference count: 40
- Top-1 accuracy of 66.39% on ImageNet-1K, surpassing directly trained spiking ResNet-34 by 2.67%

## Executive Summary
This paper introduces a novel spiking Multi-Layer Perceptron (MLP) architecture that achieves Multiplication-Free Inference (MFI) compatibility while maintaining competitive accuracy on image classification tasks. The key innovation is using batch normalization and a spiking patch encoding module to enable efficient spike-based computation without matrix multiplications. By combining global receptive fields with enhanced local feature extraction, the proposed spiking MLP achieves state-of-the-art results among spiking networks on ImageNet-1K, CIFAR10/100, and CIFAR10-DVS datasets.

## Method Summary
The method proposes a multi-stage spiking MLP network that uses batch normalization instead of layer normalization to enable parameter fusion with linear weights during inference, achieving MFI compatibility. A spiking patch encoding (SPE) module replaces traditional patch partitioning with a directed acyclic graph structure incorporating convolution operations and batch normalization for improved local feature extraction. The network architecture combines global receptive fields from token mixing with local feature extraction from SPE, using LIF neurons and trained with backpropagation through time using surrogate gradients.

## Key Results
- Achieves 66.39% top-1 accuracy on ImageNet-1K, outperforming directly trained spiking ResNet-34 by 2.67%
- Larger variant reaches 71.64% top-1 accuracy, rivaling spiking VGG-16 with 2.1x smaller model capacity
- Sets new benchmarks on CIFAR10 (96.08%), CIFAR100 (80.57%), and CIFAR10-DVS (81.12%)
- Reduces computational costs, model parameters, and simulation steps compared to existing spiking networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch Normalization enables MFI compatibility by allowing parameter fusion during inference
- Mechanism: BN parameters can be integrated with linear projection weights, eliminating separate normalization operations and enabling addition-only computation
- Core assumption: BN integration with weights doesn't significantly impact accuracy compared to LN
- Evidence anchors: [abstract] mentions using BN to retain MFI compatibility; [section] describes moving normalization after FC operation and replacing LN with BN

### Mechanism 2
- Claim: Spiking patch encoding enhances local feature extraction through directed acyclic graph structure
- Mechanism: SPE uses spike-based DAG with convolution operations and batch normalization instead of fixed patch partitioning
- Core assumption: DAG structure with convolution is more effective for local feature extraction than simple patch partitioning
- Evidence anchors: [abstract] introduces SPE to reinforce local feature extraction; [section] describes constructing SPE with spike-based DAG

### Mechanism 3
- Claim: Combination of global and local feature extraction creates more effective network architecture
- Mechanism: Global receptive fields capture long-range dependencies while local extraction provides spatial detail
- Core assumption: Both global and local feature extraction are necessary for optimal SNN performance
- Evidence anchors: [abstract] describes enriching local feature extraction and enabling downsampling; [section] describes two-branch structure for spiking token block

## Foundational Learning

- Concept: Multiplication-Free Inference (MFI) principle
  - Why needed here: Entire spiking MLP architecture designed to adhere to MFI for efficient spike-based computation and neuromorphic hardware compatibility
  - Quick check question: Can you explain why matrix multiplications are problematic for SNNs in the context of MFI?

- Concept: Batch Normalization (BN) and its integration with linear weights
  - Why needed here: BN enables parameter fusion during inference, essential for achieving MFI compatibility
  - Quick check question: How does integrating BN parameters with linear weights enable multiplication-free inference?

- Concept: Directed Acyclic Graph (DAG) structures in neural networks
  - Why needed here: SPE module uses DAG structure to replace patch partitioning for flexible local feature extraction
  - Quick check question: What advantages does a DAG structure offer over fixed patch partitioning for local feature extraction?

## Architecture Onboarding

- Component map: Input image → Spiking Patch Encoding (SPE) → Spiking MLP-Mixer stages → Spiking MLP block → Linear classifier
- Critical path: Image → SPE → Spiking MLP-Mixer stages → Spiking MLP block → Linear classifier
- Design tradeoffs:
  - Global vs. local feature extraction: Balancing receptive field size with local detail
  - MFI compatibility vs. model accuracy: Using BN instead of LN may impact accuracy slightly
  - Model complexity vs. computational efficiency: More complex architectures may improve accuracy but increase computation cost
- Failure signatures:
  - Poor accuracy: Likely due to inadequate feature extraction or training issues
  - High computational cost: May indicate inefficient implementation or unnecessary complexity
  - MFI violations: Incorrect implementation of BN integration or residual connections
- First 3 experiments:
  1. Test basic MLP-Mixer with spiking neurons without SPE module on CIFAR-10 to establish baseline performance
  2. Implement SPE module and compare local feature extraction quality with standard patch partitioning
  3. Test full spiking MLP-SPE network on ImageNet-1K to verify global-local feature combination effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of spiking MLPs compare to traditional CNNs and Transformers when trained directly on the same datasets without pre-training?
- Basis in paper: [inferred] The paper demonstrates spiking MLPs outperform directly trained spiking ResNets but relies on pre-training from ImageNet for CIFAR datasets
- Why unresolved: The paper focuses on spiking MLPs as an alternative architecture but doesn't benchmark against non-spiking models trained under identical conditions
- What evidence would resolve it: Direct training results of spiking MLPs, CNNs, and Transformers on CIFAR/ImageNet without pre-training, with identical training protocols

### Open Question 2
- Question: What is the impact of different surrogate gradient functions on the training stability and final accuracy of spiking MLPs?
- Basis in paper: [explicit] The paper mentions using sigmoid function for surrogate gradients but acknowledges other functions have been used in previous studies
- Why unresolved: Only one surrogate gradient function was tested. The choice could significantly affect convergence and accuracy
- What evidence would resolve it: Systematic comparison of multiple surrogate gradient functions on spiking MLP performance across different datasets

### Open Question 3
- Question: How do the trained receptive fields in spiking MLPs relate to biological neural organization beyond the initial layers?
- Basis in paper: [explicit] The paper observes that early-stage spiking MLP receptive fields resemble cortical "off-center on-surround" receptive fields
- Why unresolved: The analysis is limited to initial layers and visual inspection. No quantitative comparison with biological data or deeper layer analysis is provided
- What evidence would resolve it: Detailed quantitative analysis of receptive field organization across all layers, comparison with biological cortical recordings

## Limitations
- The specific implementation details for parameter fusion during MFI are not fully specified
- Effectiveness of spiking patch encoding needs more rigorous ablation studies
- Computational efficiency claims lack detailed analysis of energy consumption and latency on neuromorphic hardware

## Confidence
- High Confidence: ImageNet-1K benchmark results (66.39% top-1 accuracy)
- Medium Confidence: CIFAR10/100 and CIFAR10-DVS performance claims
- Medium Confidence: MFI compatibility claims and computational efficiency assertions

## Next Checks
1. Implement ablation studies comparing the spiking MLP with and without the spiking patch encoding module on CIFAR-10 to quantify the local feature extraction benefits
2. Verify the MFI compatibility by analyzing the inference operations of the trained model and confirming the absence of multiplications
3. Measure the actual computational efficiency on neuromorphic hardware by comparing energy consumption and latency against baseline spiking ResNet-34 implementations