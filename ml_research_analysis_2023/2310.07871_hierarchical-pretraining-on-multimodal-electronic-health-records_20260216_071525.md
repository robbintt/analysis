---
ver: rpa2
title: Hierarchical Pretraining on Multimodal Electronic Health Records
arxiv_id: '2310.07871'
source_url: https://arxiv.org/abs/2310.07871
tags:
- data
- clinical
- pretraining
- medhmp
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of pretraining on hierarchical
  multimodal EHR data to improve generalization across diverse downstream tasks. The
  authors propose MEDHMP, a novel pretraining framework that incorporates five modalities
  (demographics, clinical features, ICD codes, drug codes, and clinical notes) and
  employs a bottom-to-up approach with level-specific self-supervised learning tasks.
---

# Hierarchical Pretraining on Multimodal Electronic Health Records

## Quick Facts
- **arXiv ID**: 2310.07871
- **Source URL**: https://arxiv.org/abs/2310.07871
- **Reference count**: 29
- **Primary result**: MEDHMP outperforms eighteen baselines on eight downstream tasks spanning three EHR levels with significant improvements in AUROC and AUPR

## Executive Summary
This paper addresses the challenge of pretraining on hierarchical multimodal EHR data by proposing MEDHMP, a novel framework that incorporates five modalities (demographics, clinical features, ICD codes, drug codes, and clinical notes) with level-specific self-supervised learning tasks. The authors employ a bottom-to-up pretraining approach, first pretraining on stay-level clinical feature reconstruction, then using those representations for admission-level masked code prediction and contrastive learning. MEDHMP demonstrates superior performance compared to eighteen baselines on eight downstream tasks spanning three levels, with significant improvements in metrics such as AUROC and AUPR.

## Method Summary
MEDHMP implements a two-stage hierarchical pretraining framework. The first stage trains on stay-level clinical feature reconstruction using demographic-guided LSTM-based reconstruction of sparse clinical monitoring features. The second stage performs admission-level pretraining with masked code prediction (MCP) for ICD and drug codes combined with inter-modality contrastive learning to maximize similarity within admissions while minimizing similarity across admissions. The framework uses five modalities encoded through different architectures: MLP for demographics, LSTM for time-series clinical features, MLP for codes, and fixed ClinicalT5 for clinical notes. Training proceeds for 200 epochs at stay-level followed by 300 epochs at admission-level with batch size 4096 for contrastive learning.

## Key Results
- Outperforms eighteen baselines on eight downstream tasks spanning three levels
- Significant improvements in AUROC and AUPR metrics
- Demonstrates effectiveness of bottom-to-up hierarchical pretraining approach
- Shows robustness across different task types and data distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bottom-to-up pretraining hierarchy aligns pretraining objectives with downstream task structure.
- Mechanism: By first pretraining on stay-level clinical feature reconstruction, then using those representations for admission-level masked code prediction and contrastive learning, MEDHMP builds representations at each EHR level that are directly reusable in downstream tasks.
- Core assumption: Representations learned at one level remain useful when transferred to higher levels without requiring full patient-level pretraining.
- Evidence anchors:
  - [abstract] "adopt a 'bottom-to-up' approach and introduce level-specific self-supervised learning tasks"
  - [section 2.2.3] "The stay-level pretraining allows MEDHMP to acquire the sufficient capability of representing stays, laying the groundwork for the pretraining at the admission level"
  - [corpus] Weak evidence; no direct corpus match for this hierarchical pretraining claim
- Break condition: If downstream tasks require cross-admission temporal dependencies that aren't captured by stay-level representations, performance would degrade.

### Mechanism 2
- Claim: Bimodal fusion of demographics and clinical features improves sparse clinical feature reconstruction.
- Mechanism: Demographic embeddings guide LSTM-based reconstruction of sparse clinical monitoring features by providing context (e.g., age/gender-related examination likelihood).
- Core assumption: Clinical feature sparsity is predictable from demographic patterns.
- Evidence anchors:
  - [section 2.2.2] "the clinical feature vector mj i,k ∈ Si is extremely sparse... To accurately reconstruct such a sparse matrix, we need to use the demographic information D as the guidance"
  - [section 2.2.2] "Using the fused representation bj i, MEDHMP then reconstructs the input clinical feature matrix Si"
  - [corpus] Weak evidence; no direct corpus match for demographic-guided clinical reconstruction
- Break condition: If clinical feature patterns are not correlated with demographics, the guidance would be misleading.

### Mechanism 3
- Claim: Inter-modality contrastive learning improves admission-level representation quality.
- Mechanism: By masking one modality and contrasting its representation against the aggregated representation of remaining modalities versus other admissions, MEDHMP learns to maximize intra-admission similarity while minimizing inter-admission similarity.
- Core assumption: Modalities within the same admission share more information than modalities across different admissions.
- Evidence anchors:
  - [section 2.3.3] "the four representations {si, ci, gi, li} within Ai share similar information... the similarity between ri and the aggregated representation ai\ri learned from the remaining ones should be still larger than that between ri and another admission's representation aj\rj"
  - [section 2.3.3] "we propose to use the noise contrastive estimation (NCE) loss as the inter-modality modeling objective"
  - [corpus] Weak evidence; no direct corpus match for this specific contrastive learning design
- Break condition: If admissions contain heterogeneous data unrelated across modalities, contrastive learning would enforce incorrect similarity.

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: EHR data contains structured demographics, time-series clinical features, discrete codes, and unstructured clinical notes requiring different modeling approaches
  - Quick check question: What architectural component handles unstructured clinical notes versus structured clinical features?

- Concept: Contrastive learning
  - Why needed here: Learning inter-modality relationships requires distinguishing representations from the same admission versus different admissions
  - Quick check question: Which loss function is used to maximize similarity within admissions while minimizing similarity across admissions?

- Concept: Masked prediction for sets
  - Why needed here: ICD and drug codes are sets (not sequences) requiring different masking strategy than traditional MLM
  - Quick check question: How does MEDHMP handle the fact that ICD codes are sets rather than sequences when applying masked prediction?

## Architecture Onboarding

- Component map: Demographics MLP → Demographics vector → Clinical features LSTM → Time-series vector → ICD/drug codes MLP → Code vectors → Clinical notes ClinicalT5 → Note vector → Admission-level Transformer → Fused admission representation
- Critical path: Clinical features → LSTM encoding → Demographic fusion → Bimodal reconstruction (stay-level) → ICD/drug code prediction + contrastive (admission-level)
- Design tradeoffs:
  - Fixed ClinicalT5 encoder vs fine-tuning: Trade memory/compute for potentially better clinical note representations
  - Large batch size (4096) for contrastive learning: Improves contrastive quality but increases memory requirements
  - Two-stage training: More complex pipeline but better aligned with EHR hierarchy
- Failure signatures:
  - Poor stay-level reconstruction → Admission-level performance degrades (foundational representations missing)
  - Contrastive learning collapse → All representations become similar regardless of admission
  - Demographic-guided reconstruction fails → Clinical features remain sparse and noisy
- First 3 experiments:
  1. Train stay-level reconstruction only and evaluate on ARF/shock/mortality tasks to verify baseline performance
  2. Add admission-level MCP task (without contrastive) and evaluate readmission task to test code prediction capability
  3. Add full admission-level training (MCP + contrastive) and evaluate all downstream tasks to measure final performance gain

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but based on the content and methodology, several potential areas for future research can be identified:

1. How does MEDHMP's performance scale with the size and diversity of the pretraining dataset?
2. Can MEDHMP's pretraining framework be extended to incorporate other types of medical data beyond EHR, such as genomic data or medical imaging?
3. How does MEDHMP's performance compare to other state-of-the-art medical pretraining models, such as BEHRT or ClinicalBERT, on specific downstream tasks?

## Limitations

- Critical gaps in reproducibility due to unclear ClinicalT5 encoder specification and masking strategy details
- Contrastive learning design relies on assumptions about modality similarity that may not hold for all admission types
- No direct comparison to other state-of-the-art medical pretraining models like BEHRT or ClinicalBERT

## Confidence

- **High Confidence**: The overall hierarchical pretraining framework and its alignment with EHR data structure
- **Medium Confidence**: The specific loss functions and optimization details for both training stages
- **Low Confidence**: The exact implementation of ClinicalT5 integration and modality masking strategies

## Next Checks

1. **Architectural Validation**: Implement a simplified version with a single clinical feature modality and verify stay-level reconstruction performance before adding complexity
2. **Contrastive Learning Stability**: Test contrastive learning with varying batch sizes (512, 2048, 4096) to confirm the large batch size requirement is critical for performance
3. **Cross-Modality Consistency**: Evaluate representation similarity across modalities within the same admission versus across admissions to verify the contrastive learning assumption holds empirically