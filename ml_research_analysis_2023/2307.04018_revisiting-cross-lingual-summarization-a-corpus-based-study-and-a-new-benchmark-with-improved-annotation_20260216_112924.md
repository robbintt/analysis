---
ver: rpa2
title: 'Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark
  with Improved Annotation'
arxiv_id: '2307.04018'
source_url: https://arxiv.org/abs/2307.04018
tags:
- summaries
- translation
- which
- summarization
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a key issue in existing cross-lingual summarization
  (CLS) datasets: the pipeline annotation protocol (summarize then translate) can
  introduce errors from both summarization and translation, reducing data quality.
  To address this, the authors propose a new annotation protocol that explicitly considers
  source input context, and use it to create ConvSumX, a high-quality benchmark focusing
  on conversational text with 2 sub-tasks and 3 language directions.'
---

# Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation

## Quick Facts
- arXiv ID: 2307.04018
- Source URL: https://arxiv.org/abs/2307.04018
- Reference count: 35
- This paper identifies key issues in existing cross-lingual summarization (CLS) datasets and proposes a new annotation protocol with ConvSumX benchmark and a 2-Step framework that improves CLS performance by incorporating source input context.

## Executive Summary
This paper addresses critical issues in existing cross-lingual summarization (CLS) datasets, where pipeline annotation protocols (summarize then translate) introduce errors from both summarization and translation, degrading data quality. The authors propose a new annotation protocol that explicitly considers source input context and create ConvSumX, a high-quality benchmark focusing on conversational text across 3 language directions. Based on the intuition that source text and summary complement each other, they introduce a 2-Step framework that takes both source input text and summary as input to simulate human annotation. Experiments on ConvSumX demonstrate that the 2-Step method outperforms strong baselines on both automatic and human evaluation, showing the necessity of both source text and summary for modeling cross-lingual summaries.

## Method Summary
The paper proposes a 2-Step framework for cross-lingual summarization that fine-tunes a multilingual pretrained language model (specifically mBART-large-50-many-to-many-mmt) using concatenated source summary (Ssrc) and source input text (Dsrc) as input, with target summary (Stgt) as output. This approach contrasts with traditional pipeline methods (summarize then translate) and end-to-end methods. The framework is trained on the newly created ConvSumX dataset, which consists of DialogSumX and QMSumX datasets annotated with the improved protocol that preserves source input context. The model is initialized with mBART-large-50-many-to-many-mmt and fine-tuned on ⟨{Ssrc; Dsrc}, Stgt⟩ pairs to generate target language summaries.

## Key Results
- The 2-Step framework outperforms strong pipeline and end-to-end baselines on ConvSumX under both automatic (ROUGE, BERTScore) and human evaluation metrics
- Both source input text and source summary are crucial for modeling cross-lingual summaries, with the framework achieving better faithfulness and coherence
- The approach shows particular benefits for low-resource languages, though performance varies across language pairs (best on En→Zh and En→Fr, lower on En→Ukr)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The 2-Step framework improves faithfulness by incorporating both source input text and source summary
- Mechanism: By concatenating source summary (Ssrc) and source input text (Dsrc) as input to the cross-lingual summarizer, the model can disambiguate polysemous words and correct hallucinations present in the source summary
- Core assumption: Source input text provides disambiguating context that resolves translation errors and hallucinations in the source summary
- Evidence anchors:
  - [abstract]: "Experimental results show that 2-Step method surpasses strong baselines on ConvSumX under both automatic and human evaluation. Analysis shows that both source input text and summary are crucial for modeling cross-lingual summaries."
  - [section 2.1]: "Empirical results show that existing corpora suffer from the two aforementioned problems, containing significantly many hallucinations and factual errors."
  - [corpus]: Weak - corpus evidence is limited to the ConvSumX dataset which is newly created
- Break condition: If source input text is unavailable or too noisy, the model may not benefit from the additional context

### Mechanism 2
- Claim: The 2-Step framework generates more fluent and coherent summaries by leveraging the source summary as a guiding template
- Mechanism: The source summary provides a high-level overview of the salient information, which guides the cross-lingual summarizer to focus on relevant content and maintain coherence
- Core assumption: The source summary contains the essential information that should be preserved in the cross-lingual summary
- Evidence anchors:
  - [abstract]: "Based on the same intuition that Dsrc and Ssrc can serve as a critical complement to each other, we propose a 2-Step framework for CLS, which fine-tunes a multi-lingual PLM using concatenated Ssrc and Dsrc as input, and Stgt as output."
  - [section 4.2]: "Compared with End2End methods, 2-Step can focus on relevant information with the help of mono-lingual summaries."
  - [corpus]: Weak - corpus evidence is limited to the ConvSumX dataset which is newly created
- Break condition: If the source summary is of poor quality or contains irrelevant information, the model may be misled

### Mechanism 3
- Claim: The 2-Step framework achieves better performance on low-resource languages by leveraging the strong alignment between source text and summary
- Mechanism: By fine-tuning on the concatenated source text and summary, the model learns the alignment between the two languages, which is particularly beneficial for low-resource languages where direct translation may be unreliable
- Core assumption: The alignment between source text and summary is strong enough to guide the cross-lingual summarization, even in low-resource settings
- Evidence anchors:
  - [abstract]: "Experimental results show that 2-Step method surpasses strong baselines on ConvSumX under both automatic and human evaluation."
  - [section 5.1]: "All CLS systems perform better at En2Zh and En2Fr than En2Ukr. The high performance on En2Zh and En2Fr can be explained by that both Zh and Fr are highly-rich resource data on which mBART-50 is pre-trained."
  - [corpus]: Weak - corpus evidence is limited to the ConvSumX dataset which is newly created
- Break condition: If the alignment between source text and summary is weak or noisy, the model may not benefit from the additional context

## Foundational Learning

- Concept: Cross-lingual summarization (CLS)
  - Why needed here: CLS is the main task being addressed in this paper. Understanding the task definition and challenges is crucial for designing effective models.
  - Quick check question: What is the main goal of CLS, and what are the key challenges in this task?

- Concept: Error analysis in CLS datasets
  - Why needed here: The paper highlights the issues with existing CLS datasets, such as hallucinations and factual errors. Understanding these issues is important for designing a better annotation protocol.
  - Quick check question: What are the main types of errors found in existing CLS datasets, and how do they arise?

- Concept: 2-Step framework for CLS
  - Why needed here: The 2-Step framework is the proposed solution to address the issues in existing CLS datasets. Understanding the key idea and implementation details is crucial for evaluating its effectiveness.
  - Quick check question: How does the 2-Step framework incorporate both source input text and source summary, and what are the benefits of this approach?

## Architecture Onboarding

- Component map: Source text (Dsrc) -> 2-Step framework -> Target summary (Stgt)
- Critical path: Dsrc + Ssrc → Cross-lingual summarizer → Stgt
- Design tradeoffs:
  - Using a 2-Step framework instead of pipeline or end-to-end approaches
  - Incorporating source input text and summary as input to the cross-lingual summarizer
  - Fine-tuning on the concatenated source text and summary
- Failure signatures:
  - Poor quality of source summary leading to misleading information
  - Noisy or unreliable source input text
  - Weak alignment between source text and summary
- First 3 experiments:
  1. Evaluate the 2-Step framework on the ConvSumX dataset and compare with pipeline and end-to-end baselines.
  2. Analyze the types of errors in the generated summaries and their sources (e.g., source summary, translation, model generation).
  3. Investigate the impact of different components of the 2-Step framework (e.g., source summary, source input text) on the model performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the 2-Step method compare to end-to-end methods when trained on large-scale CLS datasets rather than few-shot settings?
- Basis in paper: [explicit] The paper notes that 2-Step method performs well in few-shot settings, but does not explore performance on larger datasets.
- Why unresolved: The experiments only evaluate on few-shot ConvSumX dataset, so the scalability of 2-Step to large-scale CLS tasks is unknown.
- What evidence would resolve it: Experiments comparing 2-Step to end-to-end methods when trained on large-scale CLS datasets like CLIDSum.

### Open Question 2
- Question: What is the impact of different concatenation strategies (e.g. token-level, sentence-level) on the performance of the 2-Step method?
- Basis in paper: [inferred] The paper notes that they simply concatenate source text and summary at token level, but do not explore other strategies.
- Why unresolved: Only one concatenation strategy is tested, so the impact of different strategies on 2-Step performance is unknown.
- What evidence would resolve it: Experiments testing different concatenation strategies (e.g. sentence-level, hierarchical) on the 2-Step method.

### Open Question 3
- Question: How does the 2-Step method perform on CLS tasks involving language pairs beyond those tested in ConvSumX?
- Basis in paper: [explicit] ConvSumX only covers 3 language directions (En-Zh, En-Fr, En-Uk), so performance on other language pairs is unknown.
- Why unresolved: Experiments only cover the 3 language directions in ConvSumX, so generalizability to other language pairs is unknown.
- What evidence would resolve it: Experiments testing the 2-Step method on CLS tasks involving language pairs not covered in ConvSumX.

## Limitations
- The ConvSumX dataset is newly created and may not fully represent the diversity of cross-lingual summarization scenarios, focusing primarily on conversational text
- Error attribution is unclear - it's uncertain how much error stems from source summary quality versus translation errors, as the 2-Step framework assumes source summaries are reliable
- Low-resource language performance evidence is limited to Ukrainian, with effectiveness on other truly low-resource languages remaining uncertain

## Confidence
- **High Confidence**: The core finding that existing CLS datasets suffer from pipeline annotation errors (hallucinations and factual mistakes) is well-supported by the error analysis in Section 2.1. The effectiveness of incorporating source input context in the 2-Step framework is demonstrated across multiple experiments.
- **Medium Confidence**: The claim that the 2-Step framework improves fluency and coherence is supported by automatic metrics and human evaluation, but the human evaluation sample size (50 instances) is relatively small.
- **Medium Confidence**: The assertion that the 2-Step framework is particularly beneficial for low-resource languages is based on limited evidence (primarily En→Ukr direction) and requires further validation across more low-resource language pairs.

## Next Checks
1. **Cross-Domain Evaluation**: Test the 2-Step framework on non-conversational CLS datasets (e.g., news articles) to assess generalizability beyond the ConvSumX domain.
2. **Source Summary Quality Impact**: Conduct an ablation study where source summaries are intentionally degraded (e.g., by introducing controlled noise) to quantify how sensitive the 2-Step framework is to source summary quality.
3. **Scalability to More Low-Resource Languages**: Evaluate the 2-Step framework on additional low-resource language pairs (e.g., En→Swahili, En→Hausa) to determine if the observed benefits extend beyond Ukrainian.