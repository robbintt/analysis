---
ver: rpa2
title: 'CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image
  Generation'
arxiv_id: '2310.01407'
source_url: https://arxiv.org/abs/2310.01407
tags:
- diffusion
- conditional
- steps
- distillation
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a conditional distillation method for accelerating
  image generation with diffusion models. The key idea is to distill a conditional
  diffusion model from an unconditional pre-trained model in a single stage, simplifying
  previous two-stage procedures.
---

# CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation

## Quick Facts
- arXiv ID: 2310.01407
- Source URL: https://arxiv.org/abs/2310.01407
- Reference count: 18
- Key outcome: Single-stage conditional diffusion distillation from unconditional models achieves superior visual quality (FID/LPIPS) at 1-4 sampling steps compared to two-stage methods

## Executive Summary
This paper introduces CoDi, a conditional diffusion distillation method that accelerates image generation by distilling a conditional model from an unconditional pre-trained diffusion model in a single stage. The approach addresses limitations of two-stage procedures by jointly optimizing conditional guidance and self-consistency, avoiding degradation of diffusion priors. A parameter-efficient adapter mechanism enables conditional inputs without modifying the backbone, and the PREv-predictor improves sampling quality at very low step counts.

## Method Summary
CoDi performs single-stage distillation by adapting an unconditional pre-trained diffusion model with a conditional encoder (e.g., ControlNet-style adapter). The method jointly optimizes two loss functions: a self-consistency loss that enforces trajectory alignment between the distilled model and an exponential moving average (EMA) target network, and a conditional guidance loss that ensures outputs match conditional targets. The PREv-predictor improves sampling by incorporating the original noise sample rather than deterministic predictions. Training uses parameter-efficient adapters that share the unconditional backbone while only training small conditional encoders for each task.

## Key Results
- Outperforms previous conditional diffusion distillation methods on super-resolution, image editing, and depth-to-image generation
- Achieves superior FID and LPIPS scores when using only 1-4 sampling steps
- Demonstrates parameter-efficient adaptation through shared unconditional backbone with conditional adapters
- Shows robustness to over-saturation issues common in distillation without conditional guidance

## Why This Works (Mechanism)

### Mechanism 1
Single-stage conditional distillation outperforms two-stage approaches by jointly optimizing self-consistency and conditional guidance without degrading the diffusion prior. The conditional encoder (initialized to zero) gradually incorporates conditional information while preserving the unconditional backbone's learned priors.

### Mechanism 2
The PREv-predictor improves distillation by mixing actual sampled noise with signal predictions in the latent update, helping the model handle stochasticity better than deterministic DDIM predictions.

### Mechanism 3
Conditional guidance loss (ℓ2 in signal space) prevents over-saturation and local minima by acting as a lower bound on the distillation loss, avoiding degenerate solutions that overfit to noise patterns.

## Foundational Learning

- **Concept**: Variance-preserving diffusion SDEs and deterministic PF-ODE
  - Why needed: The paper relies on understanding how diffusion models can be distilled via consistency in the ODE trajectory
  - Quick check: What is the relationship between the stochastic SDE (3) and the deterministic PF-ODE (5) in terms of score matching?

- **Concept**: Score-based model parameterization (signal vs noise prediction)
  - Why needed: The distillation loss is defined in both noise and signal spaces; knowing how to transform between them is critical
  - Quick check: Given a velocity prediction vθ, how do you recover the signal prediction xθ and noise prediction ϵθ using equations (7)?

- **Concept**: Parameter-efficient tuning via adapter layers (e.g., ControlNet, T2I-Adapter)
  - Why needed: The paper proposes to share the unconditional backbone and only train small adapters for each conditional task
  - Quick check: In a ControlNet-style adapter, which U-Net components are duplicated and which remain frozen during distillation?

## Architecture Onboarding

- **Component map**: Unconditional backbone -> Conditional adapter -> Signal/noise heads -> Loss computation
- **Critical path**: 
  1. Sample (x, c) and time t
  2. Sample noise ϵ → generate zt
  3. Forward pass through adapter + backbone → ˆxθ, ˆϵθ
  4. Compute ˆzs via PREv-predictor
  5. Forward pass target network (EMA) on ˆzs → ˆϵθ−
  6. Compute losses and update online network
- **Design tradeoffs**: 
  - Single-stage vs two-stage: Simpler pipeline vs potential for better initial alignment
  - PREv-predictor vs DDIM: Stochastic vs deterministic updates; more noise robustness vs deterministic stability
  - Conditional guidance strength: Controls trade-off between fidelity and refinement quality
- **Failure signatures**:
  - Model collapses to mode collapse: Loss of diversity, FID drops but LPIPS high
  - Over-saturation: High FID, low LPIPS, visually saturated colors
  - Slow convergence: High variance in training loss, unstable EMA updates
- **First 3 experiments**:
  1. Ablation: Replace PREv-predictor with DDIM predictor; compare FID/LPIPS at 1-4 steps
  2. Ablation: Remove conditional guidance; observe saturation artifacts and FID degradation
  3. Ablation: Compare single-stage vs two-stage distillation on depth-to-image; measure FID/LPIPS

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. However, based on the content, several implicit open questions emerge regarding theoretical limits of sampling steps, scalability to more complex conditional tasks, and generalizability of the parameter-efficient distillation mechanism to other generative model architectures.

## Limitations

- Claims of superiority over two-stage methods lack direct ablation studies comparing both approaches under identical conditions
- PREv-predictor advantage demonstrated only in one figure without statistical significance analysis
- Method's performance on more complex conditional tasks beyond the three demonstrated applications remains untested
- Lack of theoretical analysis of convergence properties or fundamental limits of the distillation process

## Confidence

- **High**: Technical formulation of joint optimization with self-consistency and conditional guidance is internally consistent
- **Medium**: Empirical results showing improved FID/LPIPS at low step counts are reproducible but may not generalize to all conditional tasks
- **Low**: Claim that this method "outperforms" previous work without extensive head-to-head comparisons across diverse benchmarks

## Next Checks

1. Conduct controlled ablation comparing single-stage vs two-stage distillation with identical hyperparameters and model architectures
2. Perform significance testing across multiple random seeds for the FID/LPIPS improvements reported in Figure 5
3. Test the method on a held-out conditional task (e.g., sketch-to-image) not used during development to assess generalizability