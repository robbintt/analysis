---
ver: rpa2
title: Unimodal Intermediate Training for Multimodal Meme Sentiment Classification
arxiv_id: '2308.00528'
source_url: https://arxiv.org/abs/2308.00528
tags:
- memes
- meme
- baseline
- training
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using supplementary training on sentiment-labelled
  unimodal data to improve multimodal meme sentiment classifiers. The authors present
  a novel variant of the supervised intermediate training (STILT) approach that incorporates
  sentiment-labelled images and texts as intermediate tasks in training a multimodal
  meme sentiment classifier.
---

# Unimodal Intermediate Training for Multimodal Meme Sentiment Classification

## Quick Facts
- arXiv ID: 2308.00528
- Source URL: https://arxiv.org/abs/2308.00528
- Reference count: 24
- One-line primary result: Text-only sentiment data intermediate training significantly improves multimodal meme sentiment classification and reduces required labeled meme data by 40%.

## Executive Summary
This paper investigates whether unimodal sentiment data can improve multimodal meme sentiment classification through supervised intermediate training (STILT). The authors propose a novel variant that incorporates sentiment-labelled images and texts as intermediate tasks before fine-tuning on memes. Their results demonstrate that text-only intermediate training (Text-STILT) significantly outperforms direct training on memes alone, while image-only training does not. Additionally, Text-STILT enables maintaining classifier performance with only 60% of the labelled memes required by baseline approaches.

## Method Summary
The authors employ a multimodal architecture based on CLIP encoders with attention-based fusion. For intermediate training, they freeze one modality encoder while training on unimodal sentiment data (CrowdFlower for images, DynaSent for text). After unimodal intermediate training, both encoders are combined and fine-tuned on the Memotion 2.0 meme dataset. Three experimental conditions are evaluated: Baseline (direct meme training), Image-STILT (image-only intermediate training), and Text-STILT (text-only intermediate training), with statistical significance assessed using Wilcoxon Signed-Rank tests.

## Key Results
- Text-STILT significantly outperforms Baseline on meme sentiment classification (statistically significant improvement)
- Image-STILT does not improve performance compared to Baseline
- Text-STILT maintains performance while using only 60% of labeled memes compared to Baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-only sentiment data can transfer relevant skills to multimodal meme classification.
- Mechanism: Intermediate training on unimodal text sentiment tasks updates the text encoder's representations to better capture sentiment-bearing linguistic patterns, which are then reused when classifying memes.
- Core assumption: Sentiment cues in meme text share underlying linguistic features with general sentiment-bearing text.
- Evidence anchors:
  - [abstract] "Our results show a statistically significant performance improvement from the incorporation of unimodal text data."
  - [section 4.1] "Text-STILT was found to outperform Baseline, at a level of statistical significance."
  - [corpus] Weak - related papers focus on multimodal meme analysis without explicitly addressing unimodal transfer.
- Break condition: If meme text is semantically divergent from general sentiment text (e.g., sarcasm, cultural references not present in unimodal data).

### Mechanism 2
- Claim: Freezing one modality encoder during unimodal intermediate training preserves its general-purpose representations.
- Mechanism: While training on unimodal text, the image encoder remains frozen so its pretrained visual features are not overwritten, allowing it to later be combined with the updated text encoder for multimodal tasks.
- Core assumption: Image representations useful for memes overlap sufficiently with those from generic image pretraining.
- Evidence anchors:
  - [section 3.3] "Unimodal training ends with early stopping based on the model’s performance on the Memotion 2.0 validation set. This model is then trained and tested on the Memotion 2.0 training and testing sets."
  - [section 3.1] "For unimodal inputs, the encoder for the missing modality is fed a blank input."
  - [corpus] Weak - no direct corpus evidence of freezing strategy effectiveness in multimodal meme tasks.
- Break condition: If visual features required for memes are fundamentally different from generic image features, freezing would prevent useful adaptation.

### Mechanism 3
- Claim: Limited labelled memes can be supplemented effectively by leveraging abundant unimodal sentiment data.
- Mechanism: Unimodal intermediate training acts as a form of data augmentation, allowing the model to learn useful representations without needing as many meme examples.
- Core assumption: Skills learned from unimodal sentiment tasks are sufficiently transferable to multimodal meme sentiment classification.
- Evidence anchors:
  - [abstract] "Furthermore, we show that the training set of labelled memes can be reduced by 40% without reducing the performance of the downstream model."
  - [section 4.2] "We found that Text-STILT significantly improves performance over Baseline across varying amounts of labelled meme availability between 50% and 80%."
  - [corpus] Weak - related papers focus on multimodal approaches, not unimodal transfer for data efficiency.
- Break condition: If unimodal sentiment tasks are too dissimilar from multimodal meme tasks, the transfer benefit disappears.

## Foundational Learning

- Concept: Intermediate training (STILT) - a transfer learning technique where a model is first trained on an auxiliary task before the target task.
  - Why needed here: To leverage abundant unimodal sentiment data to improve performance on the data-scarce multimodal meme sentiment classification task.
  - Quick check question: What is the key difference between standard fine-tuning and STILT?

- Concept: Modality fusion in multimodal models - combining representations from different input types (text, image) into a unified representation.
  - Why needed here: Memes require understanding both visual and textual content; fusion mechanisms allow the model to integrate these complementary signals.
  - Quick check question: Why might simple concatenation of text and image features be insufficient for multimodal tasks?

- Concept: Cross-modal transfer learning - applying knowledge gained from one modality to improve performance on tasks involving multiple modalities.
  - Why needed here: The paper investigates whether unimodal sentiment analysis skills transfer to multimodal meme sentiment classification.
  - Quick check question: What factors might determine whether transfer learning between modalities is successful?

## Architecture Onboarding

- Component map: CLIP ViT-B/16 image encoder → CLIP text encoder → Dropout + BatchNorm → Attention-based fusion → Dropout + BatchNorm → Classification head (GeLU-activated dense layers)
- Critical path: Text encoder → Unimodal intermediate training → Fusion with frozen image encoder → Multimodal fine-tuning → Classification
- Design tradeoffs: Freezing the image encoder preserves general visual features but prevents adaptation to meme-specific visual patterns; using CLIP encoders provides strong initialization but may not be optimal for memes
- Failure signatures: Poor unimodal intermediate training performance suggests the auxiliary task isn't well-aligned; failure to improve on multimodal task suggests modality fusion isn't effective or the frozen encoder is incompatible
- First 3 experiments:
  1. Train Baseline model on all available memes to establish performance floor
  2. Train Text-STILT with all memes available to verify unimodal transfer works in the abundant-data regime
  3. Train Text-STILT with 60% of memes to confirm the data-efficiency claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does Text-STILT significantly improve meme sentiment classification performance compared to Baseline, while Image-STILT does not?
- Basis in paper: [explicit] The paper shows Text-STILT improves performance with statistical significance, while Image-STILT does not, but does not explain why.
- Why unresolved: The paper does not investigate the underlying reasons for the difference in effectiveness between Text-STILT and Image-STILT.
- What evidence would resolve it: Experiments comparing the impact of text-only vs. image-only intermediate tasks on meme classifiers with different architectures or meme datasets. Analysis of which types of memes are classified better by Text-STILT vs. Image-STILT.

### Open Question 2
- Question: To what extent can Text-STILT reduce the amount of labeled meme data needed while maintaining classifier performance?
- Basis in paper: [explicit] The paper shows Text-STILT can maintain performance with 60% of labeled memes, but does not explore lower meme fractions.
- Why unresolved: The paper only tests Text-STILT with meme fractions from 5% to 80%. Lower meme fractions could potentially work.
- What evidence would resolve it: Testing Text-STILT with even smaller meme fractions, like 1% or 10%, to find the minimum needed. Comparing performance to meme-only training at the same low fractions.

### Open Question 3
- Question: What types of memes are most challenging for Text-STILT and Baseline to classify correctly?
- Basis in paper: [inferred] The paper notes 41% of test memes were misclassified by both models, but does not analyze which memes these are.
- Why unresolved: The paper does not analyze the misclassified memes to find patterns or characteristics that make them difficult.
- What evidence would resolve it: Analyzing the misclassified memes to identify common features like text length, image complexity, or sentiment polarity. Testing if certain preprocessing or model modifications help with these challenging cases.

## Limitations
- Transfer mechanism assumptions lack rigorous validation, particularly regarding semantic overlap between unimodal and multimodal data
- Findings are demonstrated on a single dataset (Memotion 2.0), limiting generalizability to other meme datasets
- Limited ablation analysis on fusion mechanisms and intermediate task choices prevents isolating essential components

## Confidence

**High Confidence**: The core experimental methodology and statistical significance testing are sound. The Text-STILT approach shows consistent improvement over baseline across multiple meme availability scenarios.

**Medium Confidence**: The data efficiency claims (40% reduction in labeled memes) are well-supported within the tested range (50-80% of memes), but extrapolation beyond this range is uncertain.

**Low Confidence**: The explanation for why image-only intermediate training fails while text-only succeeds is speculative and lacks empirical validation.

## Next Checks
1. Test the Text-STILT approach on at least one additional multimodal meme sentiment dataset to verify that the transfer benefits generalize beyond Memotion 2.0
2. Train a variant where the image encoder is fine-tuned during multimodal training to assess whether the frozen strategy is optimal for meme-specific visual feature adaptation
3. Systematically compare different fusion approaches (concatenation, attention, cross-attention) with and without intermediate training to isolate the contribution of fusion architecture versus unimodal pretraining