---
ver: rpa2
title: 'FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain
  Dialogue with Facial Expressions'
arxiv_id: '2308.15214'
source_url: https://arxiv.org/abs/2308.15214
tags:
- robot
- system
- dialogue
- furhat
- facial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FurChat, an embodied conversational agent designed
  for the National Robotarium. The system leverages the state-of-the-art GPT-3.5 model
  to generate a mixture of open and closed-domain dialogue along with facial expressions,
  deployed on a Furhat robot.
---

# FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions

## Quick Facts
- arXiv ID: 2308.15214
- Source URL: https://arxiv.org/abs/2308.15214
- Authors: 
- Reference count: 7
- Key outcome: FurChat uses GPT-3.5 to generate both open and closed-domain dialogue along with facial expressions for a Furhat robot receptionist at the National Robotarium.

## Executive Summary
FurChat is an embodied conversational agent designed for the National Robotarium that leverages GPT-3.5 to generate a mixture of open and closed-domain dialogue along with facial expressions. The system uses prompt engineering to elicit appropriate responses from GPT-3.5, combining personality, context, database data, and dialogue history. The generated emoticons are integrated with the robot's facial gestures to create expressive robot faces. The system serves as a receptionist while also engaging in general conversations, demonstrating the potential of LLMs to enhance embodied AI systems.

## Method Summary
The FurChat system integrates ASR, NLU, dialogue management, prompt engineering with GPT-3.5, gesture parsing, and TTS to create a multimodal conversational agent. The dialogue manager constructs prompts containing robot personality, context, scraped National Robotarium data, and conversation history, then sends them to GPT-3.5 to generate responses with emoticons. These emoticons are mapped to Furhat's facial gestures, while responses are converted to speech via Amazon Polly. A custom database of scraped National Robotarium information helps reduce hallucination for closed-domain queries.

## Key Results
- GPT-3.5 successfully generates both open and closed-domain dialogue when prompted with structured context
- Emoticons from GPT-3.5 can be mapped to Furhat robot facial gestures for expressive faces
- Custom database integration helps reduce hallucination for factual queries about the National Robotarium

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5 can generate both open and closed-domain dialogue when prompted with a structured prompt containing personality, context, database data, and dialogue history.
- Mechanism: The prompt engineering approach combines few-shot learning with explicit context specification to guide the LLM toward contextually appropriate responses.
- Core assumption: The LLM's response quality depends on the richness and specificity of the prompt context.
- Evidence anchors:
  - [abstract]: "The system utilises the state-of-the-art GPT-3.5 model to generate such information along with domain-general conversations and facial expressions based on prompt engineering."
  - [section]: "The prompt engineering methodology involves using the LLM to generate an appropriate emoticon based on the conversation... The explicit specification of the personality and context in the prompt aids in creating a natural conversation between the robot and the human that is coherent and relevant to the topic."
  - [corpus]: No direct evidence in corpus papers about this specific prompt structure; this appears to be a novel approach in the paper.
- Break condition: If the prompt context is too sparse or contradictory, the LLM may produce irrelevant or incoherent responses.

### Mechanism 2
- Claim: Emoticons generated by GPT-3.5 can be mapped to pre-defined facial gestures to create expressive robot faces.
- Mechanism: GPT-3.5's text-based emotion recognition capability is leveraged to generate emoticons that are then translated into robot-specific facial gestures through conditional mapping.
- Core assumption: The emoticons generated by GPT-3.5 accurately reflect the emotional content of the conversation and can be reliably mapped to robot gestures.
- Evidence anchors:
  - [abstract]: "generate a mixture of open and closed-domain dialogue along with facial expressions, by using a large language model (LLM)"
  - [section]: "The latest GPT models have the ability to recognise emotions and sentiments from text, which is used in the system... After receiving the response from the model, the matched conditional clause in the dialogue manager will trigger an expression from the pre-developed set of gestures"
  - [corpus]: No corpus evidence directly supporting this specific emoticon-to-gesture mapping approach.
- Break condition: If the generated emoticons don't match the conversation context or the gesture mapping is incomplete, the facial expressions may appear inappropriate or disconnected.

### Mechanism 3
- Claim: A custom database of scraped National Robotarium data reduces LLM hallucination for closed-domain queries.
- Mechanism: When an intent is recognized, the dialogue manager retrieves relevant data from the database and includes it in the prompt to GPT-3.5, constraining the response to factual information.
- Core assumption: Including factual data in the prompt significantly reduces the likelihood of the LLM generating hallucinated content.
- Evidence anchors:
  - [abstract]: "in order to tone-down this effect, we create a custom database following suggestions from Kumar (2023)"
  - [section]: "When an appropriate intent is triggered, the dialogue manager accesses the database to retrieve the scraped data, which is then sent with the prompt to elicit a response from the LLM."
  - [corpus]: No corpus evidence about this specific hallucination mitigation technique.
- Break condition: If the database is incomplete or outdated, or if the LLM ignores the provided context, hallucinations may still occur.

## Foundational Learning

- Concept: Prompt engineering for LLMs
  - Why needed here: The system relies on carefully structured prompts to guide GPT-3.5 toward generating appropriate dialogue and emoticons
  - Quick check question: What key elements should be included in the prompt to ensure coherent responses from GPT-3.5?

- Concept: Dialogue state management
  - Why needed here: The dialogue manager needs to maintain conversation flow and decide when to query the database or generate open-domain responses
  - Quick check question: How does the system distinguish between open-domain and closed-domain user intents?

- Concept: Emotion recognition and expression mapping
  - Why needed here: The system must translate text-based emotional content into robot-appropriate facial gestures
  - Quick check question: What mapping strategy is used to convert GPT-3.5 generated emoticons into Furhat robot gestures?

## Architecture Onboarding

- Component map:
  ASR (Google Cloud Speech-to-Text) → NLU (Furhat NLU) → DM (Furhat DM) → Prompt Builder → GPT-3.5 → Gesture Parser → TTS (Amazon Polly) → Furhat Robot
  Database storage for National Robotarium data accessed by DM when closed-domain intents are detected

- Critical path:
  1. User speech → ASR → text
  2. NLU → intent classification
  3. DM → database lookup (if closed-domain) + prompt construction
  4. GPT-3.5 → response + emoticon
  5. Gesture parser → facial expression
  6. TTS → speech synthesis
  7. Furhat robot → multimodal output

- Design tradeoffs:
  - Using GPT-3.5 via prompt engineering rather than fine-tuning: Faster deployment but less control over response consistency
  - Database approach for hallucination mitigation: Requires manual maintenance but provides factual grounding
  - Emoticon-based gesture generation: Simple to implement but may not capture nuanced expressions

- Failure signatures:
  - ASR failure → no text input to system
  - NLU misclassification → wrong database lookup or inappropriate prompts
  - GPT-3.5 API failure → no response generated
  - Gesture parser missing emoticon mapping → default expression used
  - TTS failure → text response only, no speech

- First 3 experiments:
  1. Test ASR accuracy with various accents and background noise levels
  2. Verify NLU intent classification accuracy on a test set of closed-domain queries
  3. Validate GPT-3.5 response quality with different prompt structures and database data inclusion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the issue of hallucination in large language models be effectively mitigated when deployed in embodied conversational agents like FurChat?
- Basis in paper: [explicit] The paper mentions that hallucination in generated responses potentially undermines user trust and raises concerns of safety, and plans to address this by fine-tuning the language model and directly generating conversations from it without relying on any NLU components.
- Why unresolved: While the paper acknowledges the problem and proposes future work, it does not provide concrete solutions or results from implementing these mitigation strategies.
- What evidence would resolve it: Implementing and testing the proposed fine-tuning of the language model and direct generation of conversations, along with an evaluation of the effectiveness of these strategies in reducing hallucinations.

### Open Question 2
- Question: How can the FurChat system be extended to handle multi-party interactions effectively?
- Basis in paper: [explicit] The paper states plans to extend the system to handle multi-party interaction, which is an active research topic in developing receptionist robots.
- Why unresolved: The current system is designed for one-to-one interaction, and there is no detailed discussion or implementation of multi-party interaction capabilities.
- What evidence would resolve it: Developing and testing a multi-party interaction module for FurChat, along with an evaluation of its performance in handling multiple users simultaneously.

### Open Question 3
- Question: How can the integration of facial expressions with generated text be improved to enhance the naturalness and appropriateness of the robot's responses?
- Basis in paper: [inferred] The paper describes the use of GPT models to generate emoticons based on the conversation, which are then integrated with the robot's facial gestures. However, the effectiveness of this approach in terms of naturalness and appropriateness is not explicitly evaluated.
- Why unresolved: The paper does not provide an in-depth analysis of how well the generated facial expressions match the context of the conversation or user satisfaction with the robot's expressions.
- What evidence would resolve it: Conducting user studies to evaluate the naturalness and appropriateness of the robot's facial expressions in response to different conversational contexts, and comparing the results with alternative methods of expression generation.

## Limitations
- The exact prompt format and few-shot examples for GPT-3.5 are not specified, making exact replication difficult
- The emoticon-to-gesture mapping strategy is not described in detail
- No quantitative evaluation of system performance or user studies are presented
- The hallucination mitigation effectiveness is assumed rather than demonstrated
- No analysis of edge cases or failure modes beyond basic diagnostics

## Confidence
- High confidence: The general system architecture and component integration approach
- Medium confidence: The effectiveness of prompt engineering for generating appropriate dialogue and emoticons
- Low confidence: The reliability of the emoticon-to-gesture mapping and hallucination mitigation effectiveness

## Next Checks
1. Conduct a user study comparing user satisfaction and engagement between the FurChat system and a baseline Furhat robot without LLM integration
2. Measure hallucination rates by testing the system with factual queries outside the custom database scope and analyzing response accuracy
3. Evaluate the appropriateness of generated facial expressions by having human raters match emoticons to conversation contexts and corresponding robot gestures