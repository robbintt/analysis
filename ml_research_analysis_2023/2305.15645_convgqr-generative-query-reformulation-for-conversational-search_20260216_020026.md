---
ver: rpa2
title: 'ConvGQR: Generative Query Reformulation for Conversational Search'
arxiv_id: '2305.15645'
source_url: https://arxiv.org/abs/2305.15645
tags:
- query
- search
- retrieval
- convgqr
- expansion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ConvGQR, a framework for generative query
  reformulation in conversational search. The core idea is to integrate both query
  rewriting and query expansion using two generative pre-trained language models (PLMs),
  one for rewriting the current query and another for generating potential answers
  to expand it.
---

# ConvGQR: Generative Query Reformulation for Conversational Search

## Quick Facts
- arXiv ID: 2305.15645
- Source URL: https://arxiv.org/abs/2305.15645
- Reference count: 22
- Primary result: ConvGQR significantly outperforms state-of-the-art baselines on four conversational search datasets using both dense and sparse retrieval

## Executive Summary
ConvGQR introduces a novel framework for generative query reformulation in conversational search that integrates both query rewriting and query expansion. The approach uses two separate generative pre-trained language models - one for rewriting the current query and another for generating potential answers to expand it. To optimize reformulation for retrieval performance, ConvGQR employs a knowledge infusion mechanism that aligns query representations with relevant passage representations during training. Extensive experiments demonstrate significant improvements over existing methods on multiple conversational search datasets.

## Method Summary
ConvGQR consists of two generative PLMs (T5-base) - one trained to rewrite queries to match human-rewritten versions, and another trained to generate potential answers for expansion. The final reformulated query is the concatenation of both outputs. A knowledge infusion mechanism is applied during training where the hidden states of the generative PLMs are forced to produce session query representations similar to relevant passage representations through mean squared error loss. This alignment optimizes query reformulation for retrieval performance. The system is evaluated using both dense retrieval (ANCE) and sparse retrieval (BM25) on four conversational search datasets.

## Key Results
- ConvGQR achieves state-of-the-art performance on QReCC, TopiOCQA, CAsT-19, and CAsT-20 datasets
- Both query rewriting and expansion contribute complementary effects to retrieval performance
- Knowledge infusion mechanism significantly improves retrieval results compared to ablations without it
- Generated potential answers, even when incorrect, contain expansion terms that co-occur with correct answers in relevant passages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative query expansion using potential answers improves retrieval by adding co-occurring terms from relevant passages
- Mechanism: A generative PLM produces potential answers to the conversational query. These generated answers are concatenated to the rewritten query, providing additional terms that often co-occur with the correct answer in relevant passages
- Core assumption: Generated answers, even when incorrect, contain expansion terms that co-occur with the correct answer in relevant passages, improving retrieval relevance
- Evidence anchors:
  - [abstract] "training a rewriting model on them would limit the model's ability to produce good search queries. Another useful hint is the potential answer to the question."
  - [section 3.2.1] "the generated answer can still act as useful expansion terms (Mao et al., 2021), which can guide the search toward a passage with the potential answer or a similar answer."
  - [corpus] Weak - the cited paper (Mao et al., 2021) is referenced but the corpus analysis doesn't confirm co-occurrence patterns directly
- Break condition: Generated answers contain no terms that co-occur with correct answers in relevant passages, or generated answers are consistently irrelevant to the query topic

### Mechanism 2
- Claim: Knowledge infusion aligns query reformulation with retrieval performance by requiring query representations to match relevant passage representations
- Mechanism: During training, the hidden states of the generative PLM are forced to produce a session query representation similar to that of a relevant passage through mean squared error loss
- Core assumption: If the query representation matches that of a relevant passage, the reformulated query will retrieve relevant passages more effectively
- Evidence anchors:
  - [abstract] "to relate query reformulation to retrieval performance, we propose a knowledge infusion mechanism to optimize both query reformulation and retrieval."
  - [section 3.2.2] "an effective way is to inject the knowledge included in the relevant passage representation into the query representation when fine-tuning the generative PLMs."
  - [corpus] Moderate - ablation studies show performance degradation when removing knowledge infusion, but doesn't isolate whether MSE vs contrastive loss makes difference
- Break condition: The representation alignment doesn't translate to improved retrieval, or the MSE loss creates suboptimal generation directions

### Mechanism 3
- Claim: Combining query rewriting and query expansion produces complementary effects that improve retrieval performance beyond either approach alone
- Mechanism: Two separate generative PLMs are trained - one for rewriting queries to match human-rewritten versions, another for generating potential answers for expansion. The final query is the concatenation of both outputs
- Core assumption: Query rewriting addresses ambiguity and missing information while query expansion adds supplementary relevant terms, and these effects are complementary
- Evidence anchors:
  - [abstract] "Both effects are important for query reformulation. It is thus beneficial to use both of them."
  - [section 3.2.1] "Both query rewriting and query expansion use the historical context... The final form of the reformulated query is the concatenation of the rewritten query and the generated potential answer."
  - [corpus] Strong - ablation studies in Table 2 show significant performance drops when removing either component
- Break condition: Generated expansion terms consistently conflict with rewritten query terms or add irrelevant information that degrades retrieval

## Foundational Learning

- Concept: Generative Pre-trained Language Models (PLMs)
  - Why needed here: ConvGQR relies on T5-base models for both query rewriting and expansion, requiring understanding of how PLMs generate text and capture knowledge
  - Quick check question: What training objective does T5 use that enables it to generate both rewritten queries and potential answers?

- Concept: Dense vs Sparse Retrieval
  - Why needed here: The paper evaluates ConvGQR with both ANCE (dense) and BM25 (sparse) retrievers, requiring understanding of their different mechanisms
  - Quick check question: How does the knowledge infusion mechanism work differently for dense retrievers compared to sparse retrievers?

- Concept: Query Reformulation vs Query Expansion
  - Why needed here: ConvGQR integrates both approaches, requiring understanding of their distinct goals and methods
  - Quick check question: What specific problems does query rewriting address that query expansion does not, and vice versa?

## Architecture Onboarding

- Component map: Conversational context → Query rewriting PLM → Rewritten query + Expansion PLM → Potential answer → Concatenated reformulated query → Retrieval
- Critical path: Query reformulation flow: conversational context → query rewriting PLM → rewritten query + expansion PLM → potential answer → concatenated reformulated query → retrieval
- Design tradeoffs: Using two separate PLMs increases model complexity and storage requirements versus an integrated approach, but allows specialized training for each task. Knowledge infusion adds training complexity but aligns generation with retrieval goals
- Failure signatures: Poor retrieval performance despite good generation quality indicates knowledge infusion isn't working; degraded performance when removing expansion suggests generated answers aren't providing useful terms
- First 3 experiments:
  1. Train the rewriting PLM alone on QReCC and evaluate retrieval performance to establish baseline
  2. Train the expansion PLM alone on QReCC using gold answers and evaluate retrieval to measure expansion-only contribution
  3. Train both PLMs with knowledge infusion and compare performance to the ablations to validate the combined approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design an integrated model that simultaneously generates query rewrite and expanded terms instead of using two separate PLMs?
- Basis in paper: [explicit] The paper mentions that using two PLMs for rewriting and expansion introduces additional training load and model parameters, and designing an integrated model would be a promising improvement
- Why unresolved: The paper does not explore or propose a specific approach for integrating the two generation tasks into a single model
- What evidence would resolve it: Experimental results comparing the performance of an integrated model versus the current two-model approach on the same datasets would provide evidence of its effectiveness

### Open Question 2
- Question: What alternative methods beyond PLMs can be used to generate potential answers for query expansion, such as pseudo-relevant feedback or knowledge graphs?
- Basis in paper: [explicit] The paper suggests that the potential answer acting as expansion terms could be generated from more resources beyond just PLMs, but does not explore these alternatives
- Why unresolved: The paper focuses solely on using PLMs for generating expansion terms and does not investigate other methods or their potential benefits
- What evidence would resolve it: Comparative experiments using different methods for generating expansion terms (e.g., pseudo-relevant feedback, knowledge graphs) and their impact on retrieval performance would provide insights into the effectiveness of alternative approaches

### Open Question 3
- Question: How can the knowledge infusion mechanism be improved beyond the current mean squared error (MSE) loss and contrastive learning (CL) loss approaches?
- Basis in paper: [explicit] The paper mentions that more alternative methods for knowledge infusion can be tested to connect query reformulation with the search task
- Why unresolved: The paper only explores two specific loss functions for knowledge infusion and does not investigate other potential approaches or their effectiveness
- What evidence would resolve it: Experiments comparing different knowledge infusion methods (e.g., other loss functions, attention mechanisms) and their impact on retrieval performance would provide insights into the potential improvements over the current approaches

## Limitations

- The knowledge infusion mechanism relies on MSE loss for representation alignment, which may not be optimal compared to contrastive losses
- Generated potential answers can be incorrect or irrelevant, yet the paper assumes they still provide useful expansion terms without full validation
- The evaluation focuses on retrieval performance without examining the quality or coherence of the reformulated queries themselves
- The computational overhead of training and using two separate PLMs is not discussed

## Confidence

- High confidence: The complementary effect of combining query rewriting and expansion is well-supported by ablation studies
- Medium confidence: The knowledge infusion mechanism improves performance, though the specific loss function choice could be optimized
- Low confidence: The assumption that incorrect generated answers still provide useful expansion terms lacks direct validation

## Next Checks

1. Conduct human evaluation of reformulated queries to assess whether generated potential answers, even when incorrect, contain relevant expansion terms that improve query quality
2. Compare knowledge infusion performance using MSE loss versus contrastive loss to determine optimal representation alignment method
3. Analyze the computational cost difference between ConvGQR and single-model alternatives, including training time, inference latency, and storage requirements