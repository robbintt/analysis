---
ver: rpa2
title: Explaining Reinforcement Learning Agents Through Counterfactual Action Outcomes
arxiv_id: '2312.11118'
source_url: https://arxiv.org/abs/2312.11118
tags:
- agent
- coviz
- reward
- explanation
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a novel local explanation method, COViz, that visualizes
  counterfactual action outcomes to help users understand agent preferences. COViz
  generates trajectories showing the consequences of both the agent's chosen action
  and an alternative (counterfactual) action from the same state.
---

# Explaining Reinforcement Learning Agents Through Counterfactual Action Outcomes

## Quick Facts
- arXiv ID: 2312.11118
- Source URL: https://arxiv.org/abs/2312.11118
- Reference count: 37
- We propose a novel local explanation method, COViz, that visualizes counterfactual action outcomes to help users understand agent preferences. COViz generates trajectories showing the consequences of both the agent's chosen action and an alternative (counterfactual) action from the same state. We evaluate COViz by comparing it with reward decomposition, a local explanation method that shows the agent's expected utility for different actions. We conduct two user studies where participants characterize agent preferences in a highway driving domain. Results show that combining COViz with reward decomposition significantly improves participants' performance compared to either method alone. While participants ranked COViz higher in preference, the integrated approach yielded better objective understanding of agent behavior. This demonstrates the complementary benefits of integrating different explanation methods.

## Executive Summary
This paper introduces COViz, a novel local explanation method for reinforcement learning agents that visualizes counterfactual action outcomes. COViz generates trajectories showing the consequences of both the agent's chosen action and an alternative (counterfactual) action from the same state. The method is evaluated through user studies comparing it with reward decomposition, a local explanation method that shows the agent's expected utility for different actions. Results demonstrate that combining COViz with reward decomposition significantly improves participants' performance in understanding agent preferences compared to either method alone, though participants subjectively preferred COViz.

## Method Summary
COViz generates counterfactual trajectories by simulating alternative actions from each state in an agent's execution trace. For each state, the method simulates the chosen action (fact) and a counterfactual action (foil), typically the second-best action, for k steps to observe their outcomes. The importance of each state is calculated based on the divergence between the chosen and counterfactual trajectories using a Last-State Importance metric. The top-ranked states are then visualized with both COViz trajectories and reward decomposition bars, allowing users to see both "what if" scenarios and quantitative utility breakdowns. The method was tested in a highway driving domain with agents trained using DDQN with hierarchical reward decomposition.

## Key Results
- Combining COViz with reward decomposition significantly improves participants' ability to characterize agent preferences compared to either method alone
- COViz provides a significant advantage over reward decomposition in terms of participant preference ratings
- The integrated approach yielded better objective understanding of agent behavior despite lower subjective preference scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining COViz (counterfactual outcome visualization) with reward decomposition improves user understanding of agent preferences compared to either method alone.
- Mechanism: COViz provides visual trajectories showing the outcomes of both the chosen action and a counterfactual action, allowing users to see the consequences of different choices. Reward decomposition quantifies the agent's expected utility for different actions by breaking down the reward function. Together, they provide both "what if" and "why" explanations, giving users a more complete picture of the agent's decision-making process.
- Core assumption: Users can integrate information from both visual trajectories and quantitative reward decompositions to form a coherent understanding of agent preferences.
- Evidence anchors:
  - [abstract]: "Results show that combining COViz with reward decomposition significantly improves participants' performance compared to either method alone."
  - [section]: "Our results show that both explanation types, COViz, and reward decomposition, resulted in better-than-random participant performance, but the integration of the two enabled participants to reach a significantly higher success rate."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.328, average citations=0.0. Top related titles: TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models, Explainable Reinforcement Learning Agents Using World Models, Understanding Individual Agent Importance in Multi-Agent System via Counterfactual Reasoning. Weak evidence in corpus - no direct citations to this specific combination approach.
- Break condition: If users cannot effectively integrate visual and quantitative information, or if the explanations become overwhelming, the combination may not improve understanding.

### Mechanism 2
- Claim: COViz's contrastive nature aligns with how humans naturally provide and prefer explanations.
- Mechanism: By showing alternative trajectories (counterfactuals), COViz answers "why this action instead of another?" which is a form of contrastive explanation. This matches the human tendency to explain by contrasting alternatives rather than providing absolute descriptions.
- Core assumption: Humans process and prefer contrastive explanations over absolute ones.
- Evidence anchors:
  - [abstract]: "The contrastive nature of the explanation is in line with the literature on explanation in the social sciences which show that people typically provide and prefer explanations that contrast alternatives [8]."
  - [section]: "Many explanation techniques are not contrastive and concentrate primarily on the agent's chosen actions (fact p), without considering alternative foils. One policy-summarization (global explanation) approach that does provide contrastive information is the DISAGREEMENTS algorithm [15]."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.328, average citations=0.0. Top related titles: TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models, Explainable Reinforcement Learning Agents Using World Models, Understanding Individual Agent Importance in Multi-Agent System via Counterfactual Reasoning. Weak evidence in corpus - no direct citations to this specific contrastive explanation mechanism.
- Break condition: If users do not naturally prefer contrastive explanations or cannot process the counterfactual information effectively, this mechanism would fail.

### Mechanism 3
- Claim: The global extension of COViz, which selects important states based on trajectory divergence, effectively highlights key decision points.
- Mechanism: By quantifying the importance of states based on the difference in outcomes between chosen and counterfactual trajectories (Last-State Importance metric), the system can focus on states where the agent's decision had the most significant impact.
- Core assumption: States where the difference between chosen and counterfactual trajectories is largest are the most informative for understanding agent preferences.
- Evidence anchors:
  - [section]: "After collecting counterfactual trajectories for each of the execution traces' states, their importance is evaluated to generate a ranking. The top- n ranked states will then be used to construct the output summary. To determine the importance of a state si, we compare the two trajectories that branch out of it, 1) the one chosen (fact p) and 2) the counterfactual (foil q)."
  - [section]: "This measure utilizes the agent's inherent value function V(s) to describe the estimated utility loss of choosing the foil over the fact (i.e. optimal action). This reflects how 'far off' from the original plan the counterfactual action has led the agent."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.328, average citations=0.0. Top related titles: TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models, Explainable Reinforcement Learning Agents Using World Models, Understanding Individual Agent Importance in Multi-Agent System via Counterfactual Reasoning. Weak evidence in corpus - no direct citations to this specific importance-based state selection mechanism.
- Break condition: If the trajectory divergence does not correlate with decision importance, or if the value function is not a good proxy for utility loss, this mechanism would fail.

## Foundational Learning

- Concept: Reinforcement Learning in Markov Decision Processes (MDPs)
  - Why needed here: The paper's explanation methods are applied to RL agents operating in MDP environments, so understanding the basic RL framework is crucial.
  - Quick check question: What are the key components of an MDP, and how do they relate to RL agent behavior?

- Concept: Counterfactual explanations
  - Why needed here: COViz is based on counterfactual reasoning, showing what would happen if the agent took a different action. Understanding counterfactual explanations is essential to grasp the method's purpose and implementation.
  - Quick check question: How do counterfactual explanations differ from factual explanations, and why might they be more effective for understanding agent behavior?

- Concept: Reward decomposition
  - Why needed here: The paper compares COViz with reward decomposition and integrates the two methods. Understanding reward decomposition is necessary to appreciate the comparison and integration.
  - Quick check question: How does reward decomposition break down an agent's expected utility, and what information does it provide that COViz does not?

## Architecture Onboarding

- Component map: COViz algorithm -> Reward decomposition -> Visualization module -> Global extension -> User interface
- Critical path:
  1. Train RL agent in target environment
  2. Run simulations to collect execution traces
  3. Generate counterfactual trajectories for each state
  4. Calculate state importance based on trajectory divergence
  5. Select top important states for explanation
  6. Combine COViz trajectories with reward decomposition
  7. Present integrated explanation to user
  8. Collect user feedback on understanding and preferences

- Design tradeoffs:
  - Trajectory length (k): Longer trajectories show more consequences but increase computational cost and may introduce noise from states far from the decision point
  - Number of simulations: More simulations provide better coverage but increase computational cost
  - Counterfactual action selection: Second-best action is most informative for most users, but other options (worst action, user-selected) might be useful in some contexts
  - State selection method: Last-state importance focuses on significant decisions, but other methods (Q-value differences, frequency) might highlight different aspects of behavior

- Failure signatures:
  - Users consistently perform no better than random guessing
  - Users report high cognitive load or confusion when viewing explanations
  - COViz trajectories converge too quickly with chosen trajectories, providing little contrastive information
  - Reward decomposition bars do not align with observed trajectories, creating inconsistency

- First 3 experiments:
  1. Implement COViz algorithm for a simple gridworld environment and verify that counterfactual trajectories diverge from chosen trajectories
  2. Integrate COViz with reward decomposition for a single state and confirm that both visualizations are displayed correctly
  3. Run a small user study (5-10 participants) comparing COViz, reward decomposition, and their combination for a simple agent to validate the basic effectiveness of each method

## Open Questions the Paper Calls Out
1. How do different counterfactual action selection methods (e.g., worst action, user-selected action) impact the effectiveness of COViz explanations compared to the second-best action heuristic?
2. Does the length of counterfactual trajectories (parameter k) impact the effectiveness of COViz explanations, and is there an optimal value for different domains?
3. How does the integration of COViz with other local explanation methods, such as saliency maps or action graphs, compare to its integration with reward decomposition in terms of improving user understanding?

## Limitations
- The user study had only 15 participants (9 in pilot, 6 in final), which may not provide sufficient statistical power for robust conclusions
- The highway driving domain may not generalize to other RL environments with different state-action spaces or reward structures
- The counterfactual action selection strategy (always using the second-best action) may not capture the most informative contrasts in all scenarios

## Confidence
- **High confidence**: COViz generates counterfactual trajectories and can be combined with reward decomposition for visualization
- **Medium confidence**: The integration of COViz and reward decomposition improves objective understanding of agent preferences compared to either method alone
- **Low confidence**: The user preference for COViz over reward decomposition is strong enough to warrant standalone use, given the small sample size and potential cognitive load issues

## Next Checks
1. Replicate with larger sample size: Conduct the user study with at least 30-50 participants to improve statistical power and generalizability of results
2. Cross-domain validation: Test COViz in different RL environments (e.g., gridworld, Atari games) to assess generalizability of the method
3. Counterfactual selection optimization: Experiment with different counterfactual action selection strategies (e.g., worst action, user-selected, random) to determine which provides the most informative contrasts for understanding agent behavior