---
ver: rpa2
title: Does GPT-4 pass the Turing test?
arxiv_id: '2310.20216'
source_url: https://arxiv.org/abs/2310.20216
tags:
- human
- test
- turing
- games
- witness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated GPT-4's ability to pass the Turing test by
  conducting an online interactive experiment. Participants engaged in text-based
  conversations with either human witnesses or AI witnesses powered by different language
  models (GPT-4, GPT-3.5, ELIZA).
---

# Does GPT-4 pass the Turing test?

## Quick Facts
- arXiv ID: 2310.20216
- Source URL: https://arxiv.org/abs/2310.20216
- Reference count: 40
- Primary result: GPT-4 passes Turing test in 41% of games, outperforming GPT-3.5 (14%) and ELIZA (27%) but below human baseline (63%)

## Executive Summary
This study evaluates GPT-4's ability to pass the Turing test through an online interactive experiment where participants engage in text-based conversations with either human or AI witnesses. The research found that while the best-performing GPT-4 prompt achieved a 41% success rate in deceiving human interrogators, this falls short of human-level performance at 63%. The study demonstrates that GPT-4 can successfully impersonate humans to a significant degree, though it has not yet reached human parity in this task.

## Method Summary
The experiment used a two-player implementation of the Turing test where participants were randomly assigned as interrogators or witnesses. Interrogators engaged in 5-minute conversations with either human witnesses or AI witnesses controlled by various language models (GPT-4, GPT-3.5, ELIZA). The AI witnesses were controlled using 25 different prompts designed to make them appear more human-like, with responses sent using human-like typing delays. Participants then decided whether their conversation partner was human or AI, providing confidence ratings and reasons for their decisions.

## Key Results
- Best GPT-4 prompt achieved 41% success rate in passing the Turing test
- GPT-3.5 prompt achieved 14% success rate
- ELIZA prompt achieved 27% success rate
- Human participants achieved 63% success rate
- Interrogators relied primarily on linguistic style (35%) and socioemotional traits (27%) to detect AI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interrogators primarily rely on linguistic style and socio-emotional traits rather than intelligence to detect AI
- Mechanism: Participants judge AI based on surface-level cues (formal/informal tone, verbosity, personality) rather than deep reasoning or knowledge
- Core assumption: Linguistic style and socio-emotional factors are more salient and easier to detect than intelligence-based cues
- Evidence anchors:
  - [abstract]: "Participants' decisions were based mainly on linguistic style (35%) and socioemotional traits (27%), supporting the idea that intelligence, narrowly conceived, is not sufficient to pass the Turing test."
  - [section 4.6]: "Interrogator's reasons for making a correct AI verdict most often pertained to Linguistic Style... The second most frequent reason category provided was Social & Emotional..."

### Mechanism 2
- Claim: GPT-4 can pass the Turing Test with specific prompts, but not robustly across all prompt variations
- Mechanism: Carefully engineered prompts that make GPT-4 appear more human-like (using errors, informality, skepticism) increase success rates, but performance varies significantly by prompt
- Core assumption: Prompt engineering can overcome some AI detection cues but cannot completely eliminate them
- Evidence anchors:
  - [abstract]: "The best-performing GPT-4 prompt passed in 41% of games, outperforming GPT-3.5 (14%), ELIZA (27%)"
  - [section 3.1]: "We found substantial variation in performance depending on the prompt that was used... It seems a priori likely that a prompt exists which would outperform the ones tested here"

### Mechanism 3
- Claim: Experience with LLMs does not significantly improve detection accuracy
- Mechanism: Familiarity with LLM characteristics (formality, verbosity, helpfulness) does not translate to better detection because prompts are designed to avoid these common cues
- Core assumption: Interrogators' mental models of typical LLM behavior are based on outdated or generic interactions
- Evidence anchors:
  - [abstract]: "Participant knowledge about LLMs and number of games played positively correlated with accuracy in detecting AI, suggesting learning and practice as possible strategies to mitigate deception"
  - [section 3.2]: "We observed no significant effects of any of these factors on accuracy... suggesting that experience interacting with LLMs might not be helpful for distinguishing them from humans"

## Foundational Learning

- Concept: Interactive evaluation design
  - Why needed here: Understanding the difference between static benchmarks and interactive evaluations like the Turing Test
  - Quick check question: What are the key advantages of interactive evaluations over static benchmarks for assessing AI capabilities?

- Concept: Prompt engineering principles
  - Why needed here: The study shows how different prompts lead to vastly different performance outcomes
  - Quick check question: How do prompt characteristics like temperature settings and persona instructions affect AI detection rates?

- Concept: Statistical significance and null hypothesis testing
  - Why needed here: The paper discusses the challenges of confirming AI performance relative to human baselines
  - Quick check question: Why is confirming the null hypothesis particularly challenging when evaluating Turing Test performance?

## Architecture Onboarding

- Component map:
  - Chat interface with message limits and typing animations
  - Random assignment system for human/AI matches
  - Prompt management system with explore/exploit sampling
  - Decision interface for verdicts and confidence ratings
  - Demographic survey system
  - Content filtering and abuse reporting

- Critical path:
  1. Participant joins waiting room
  2. Random assignment to interrogator/witness role
  3. Chat interface loads with time/message limits
  4. Conversation occurs within 5-minute window
  5. Interrogator submits verdict with confidence
  6. True identity revealed and data recorded

- Design tradeoffs:
  - 2-minute vs 5-minute conversation limits (shorter reduces depth, longer increases dropout)
  - 300-character vs longer message limits (shorter prevents complex reasoning, longer enables wall-of-text detection)
  - Random vs skill-based matching (random ensures fairness, skill-based could create more adversarial conditions)

- Failure signatures:
  - High dropout rates during waiting room or conversations
  - Systematic bias in verdict accuracy across demographics
  - Prompt sampling getting stuck on poor-performing models
  - Content filter blocking legitimate messages

- First 3 experiments:
  1. Test different conversation time limits (2, 5, 10 minutes) to find optimal balance
  2. Compare random matching vs allowing participants to choose opponents
  3. Test different message length limits to see impact on detection accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic or socio-emotional cues are most effective at revealing AI models in Turing tests?
- Basis in paper: [explicit] The paper discusses various reasons interrogators gave for identifying AI, including linguistic style and socio-emotional factors
- Why unresolved: While the paper provides examples of reasons, it doesn't systematically analyze which cues are most predictive of AI detection
- What evidence would resolve it: A large-scale study comparing the effectiveness of different interrogation strategies and analyzing which specific linguistic or socio-emotional cues most often lead to correct AI identification

### Open Question 2
- Question: How does the performance of AI models in Turing tests change as models are trained on more diverse and up-to-date data?
- Basis in paper: [inferred] The paper mentions that AI models often make errors when converting between timezones or lack knowledge of current events, suggesting that access to real-time information is a limitation
- Why unresolved: The paper only tests a limited set of prompts and doesn't explore how model performance might change with access to more current information
- What evidence would resolve it: A longitudinal study comparing the performance of AI models in Turing tests over time as they are trained on increasingly diverse and up-to-date data

### Open Question 3
- Question: What are the ethical implications of AI models that can convincingly impersonate humans in Turing tests?
- Basis in paper: [explicit] The paper discusses the potential societal consequences of creating "counterfeit humans," including automation of client-facing roles and loss of trust in human interaction
- Why unresolved: While the paper acknowledges these potential consequences, it doesn't delve into the ethical considerations or potential mitigation strategies
- What evidence would resolve it: A comprehensive ethical analysis of the implications of AI deception, including potential risks and benefits, and recommendations for responsible development and deployment of such technologies

## Limitations

- The study's success rate findings are limited by the specific prompts tested, which represent only a small subset of possible variations
- Self-reported data for detection strategies may not capture all subtle cues participants actually used
- The online format may have introduced selection bias in participant recruitment and engagement levels

## Confidence

- High confidence: The finding that GPT-4 with optimized prompts can deceive human interrogators in approximately 40% of interactions
- Medium confidence: The claim that linguistic style and socio-emotional traits are primary detection cues
- Low confidence: The assertion that LLM experience does not improve detection accuracy, given the study's null findings and small effect sizes

## Next Checks

1. Test additional prompt variations systematically to determine if the 41% success rate represents a ceiling or if higher performance is achievable
2. Conduct controlled experiments comparing novice and experienced LLM users in parallel sessions to verify the relationship between experience and detection accuracy
3. Implement eye-tracking and interaction logging to empirically validate which message features participants actually attend to during conversations