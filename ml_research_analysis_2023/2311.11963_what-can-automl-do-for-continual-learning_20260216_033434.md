---
ver: rpa2
title: What Can AutoML Do For Continual Learning?
arxiv_id: '2311.11963'
source_url: https://arxiv.org/abs/2311.11963
tags:
- learning
- incremental
- tasks
- automl
- learner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This position paper identifies a key limitation in incremental
  (continual) learning: the static nature of task sequences, architectures, and hyperparameters
  throughout the learning process. The authors propose leveraging AutoML techniques
  to make incremental learners more dynamic by: (1) using curriculum learning to automatically
  design optimal task sequences, (2) applying neural architecture search to adapt
  model capacity to task complexity, and (3) employing hyperparameter optimization
  to adjust learning objectives per task.'
---

# What Can AutoML Do For Continual Learning?

## Quick Facts
- arXiv ID: 2311.11963
- Source URL: https://arxiv.org/abs/2311.11963
- Reference count: 3
- Key outcome: Position paper proposing AutoML techniques to address static nature of incremental learning components

## Executive Summary
This position paper identifies three key limitations in incremental (continual) learning: static task sequences, fixed architectures, and static hyperparameters. The authors propose leveraging AutoML techniques to make incremental learners more dynamic by automatically designing optimal task sequences through curriculum learning, adapting model capacity via neural architecture search, and tuning hyperparameters per task using optimization methods. While the paper presents no experimental results, it outlines promising research directions and calls for more investigation into applying AutoML to improve continual learning systems.

## Method Summary
The paper proposes applying AutoML techniques to address three limitations in incremental learning: static task sequences, fixed architectures, and static hyperparameters. The authors suggest using curriculum learning to automatically design optimal task sequences, neural architecture search to adapt model capacity to task complexity, and hyperparameter optimization to adjust learning objectives per task. These approaches aim to make incremental learners more dynamic and better suited for diverse real-world tasks, though no specific implementation details or experimental results are provided.

## Key Results
- Identifies static task sequences, architectures, and hyperparameters as key limitations in current incremental learning approaches
- Proposes three AutoML applications: curriculum learning for task sequence optimization, NAS for dynamic architecture adaptation, and HPO for per-task hyperparameter tuning
- Highlights preliminary work demonstrating potential of these approaches while calling for more research
- Emphasizes computational cost challenges when scaling AutoML techniques across many learning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AutoML can optimize the order and structure of incremental learning tasks through curriculum learning
- Mechanism: Curriculum learning algorithms can automatically determine the optimal sequence of learning tasks to improve downstream performance by considering task difficulty, semantic similarity, and knowledge transfer potential
- Core assumption: The order and structure of learning tasks significantly impacts learning efficiency and final performance
- Evidence anchors:
  - [section]: "We argue that following sequence 2, which incorporates a more transferable progression of knowledge relevant to our target learning goal, is more suitable, highlighting the need for automatic design of learning sequences."
  - [abstract]: "We outline three key areas of research that can contribute to making incremental learners more dynamic, highlighting concrete opportunities to apply AutoML methods in novel ways"
  - [corpus]: Weak evidence - corpus contains related papers on continual learning but no direct evidence of curriculum learning for task sequence optimization
- Break condition: The mechanism fails when task relationships are too complex to model effectively or when the computational cost of searching through task sequences becomes prohibitive

### Mechanism 2
- Claim: AutoML can dynamically adjust model architecture capacity based on task complexity and data volume
- Mechanism: Neural Architecture Search (NAS) can be adapted to incrementally grow or shrink model capacity as new tasks arrive, balancing performance gains against computational costs
- Core assumption: Model capacity should scale with task complexity and data volume for optimal performance
- Evidence anchors:
  - [section]: "The limitation of the backbone is the fixed capacity of the learner through all learning sessions... Ideally, we should be able to automatically adjust the size of the backbone to fit the needs of the learning task(s)."
  - [abstract]: "We strongly believe that AutoML offers promising solutions to address these limitations, enabling incremental learning to adapt to more diverse real-world tasks."
  - [corpus]: Weak evidence - corpus contains related papers on continual learning but no direct evidence of NAS for dynamic architecture adaptation in incremental settings
- Break condition: The mechanism fails when NAS search becomes too computationally expensive for real-time adaptation or when architecture changes cause instability in preserving previous knowledge

### Mechanism 3
- Claim: AutoML can automatically tune anti-forgetting hyperparameters per task based on semantic similarity to previous tasks
- Mechanism: Hyperparameter optimization techniques like Bayesian optimization or reinforcement learning can predict optimal regularization strength (λ) for each task, balancing new learning with forgetting prevention
- Core assumption: The optimal balance between learning new information and retaining old knowledge varies depending on task similarity
- Evidence anchors:
  - [section]: "Such an assumption is unrealistic... to learn about a completely distinct concept of car, it may be viable to anti-forget less and learn more."
  - [abstract]: "We outline three key areas of research that can contribute to making incremental learners more dynamic"
  - [corpus]: Weak evidence - corpus contains related papers on continual learning but no direct evidence of per-task hyperparameter optimization for anti-forgetting
- Break condition: The mechanism fails when the relationship between task similarity and optimal λ is too complex to model effectively or when hyperparameter optimization adds too much computational overhead

## Foundational Learning

- Concept: Incremental (Continual) Learning
  - Why needed here: Understanding the limitations of current incremental learning approaches is essential to identify where AutoML can provide improvements
  - Quick check question: What are the three main components of an incremental learning pipeline according to the paper?
- Concept: AutoML Techniques (NAS, HPO, Curriculum Learning)
  - Why needed here: The paper proposes applying these AutoML techniques to solve specific limitations in incremental learning
  - Quick check question: Which AutoML technique would be most appropriate for optimizing the sequence of learning tasks?
- Concept: Catastrophic Forgetting
  - Why needed here: Understanding this core challenge in incremental learning is crucial for appreciating why dynamic hyperparameter tuning is necessary
  - Quick check question: How do current incremental learning methods typically prevent catastrophic forgetting?

## Architecture Onboarding

- Component map: Task Sequence Generator -> Incremental Learning Model -> Architecture Search Module -> Hyperparameter Optimizer
- Critical path: For a new engineer, the critical path would be understanding the incremental learning pipeline, then exploring how each AutoML technique can address its limitations, and finally implementing a proof-of-concept for one mechanism
- Design tradeoffs: Balancing computational cost of AutoML searches against performance gains, ensuring stability of knowledge preservation when architectures change, and managing the complexity of multi-objective optimization
- Failure signatures: Poor performance due to suboptimal task ordering, instability in model performance when architectures change, or excessive computational overhead making the system impractical
- First 3 experiments:
  1. Implement curriculum learning to optimize task sequence on a simple synthetic dataset and measure impact on final performance
  2. Create a simple NAS-based architecture growth mechanism that adds capacity when validation loss plateaus on new tasks
  3. Implement per-task hyperparameter optimization for the anti-forgetting regularization strength and compare against fixed hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal methods for scaling AutoML techniques to incremental learning tasks without incurring exponential computational costs?
- Basis in paper: [explicit] The paper explicitly mentions that "most AutoML techniques require substantial computational resources" and that "applying these techniques directly to incremental learning would necessitate re-running the process for each incremental step, resulting in time-consuming computations."
- Why unresolved: Current AutoML methods are computationally expensive, and running them for each incremental learning step would be prohibitively slow. The paper calls for exploring methods to scale sub-linearly with the number of learning tasks.
- What evidence would resolve it: Experimental results showing successful implementation of efficient AutoML methods that scale sub-linearly with the number of learning tasks in incremental learning scenarios.

### Open Question 2
- Question: How can we automatically design optimal task sequences and structures that align with natural learning progression in incremental learning?
- Basis in paper: [explicit] The paper discusses the limitation of random task ordering and mentions that "we argue that following sequence 2, which incorporates a more transferable progression of knowledge relevant to our target learning goal, is more suitable."
- Why unresolved: Current incremental learning methods typically use randomly ordered tasks, which may not align with how humans naturally learn. The paper suggests using curriculum learning but doesn't provide specific solutions for optimal task sequence design.
- What evidence would resolve it: Experimental results demonstrating improved performance in incremental learning when using automatically designed task sequences versus random ordering.

### Open Question 3
- Question: What are the best approaches for dynamic neural architecture search in incremental learning that balances model capacity growth with computational efficiency?
- Basis in paper: [explicit] The paper identifies the limitation of fixed architecture capacity and suggests using NAS, but notes the challenge of being "extremely efficient" while "(re)designing neural architectures that achieve maximum performance while minimizing computation."
- Why unresolved: While NAS can optimize architectures, doing so efficiently in the context of incremental learning where tasks arrive sequentially remains challenging. The paper mentions preliminary work but calls for more research.
- What evidence would resolve it: Demonstrated methods for NAS in incremental learning that show significant performance improvements while maintaining reasonable computational costs.

## Limitations
- No experimental results provided to validate the proposed approaches
- Computational complexity of applying AutoML techniques to incremental learning not fully addressed
- Limited discussion of how to scale AutoML techniques across many learning tasks

## Confidence
- High Confidence: The identification of static task sequences, fixed architectures, and static hyperparameters as limitations in current incremental learning approaches
- Medium Confidence: The proposed mechanisms for applying AutoML techniques (curriculum learning, NAS, HPO) to address these limitations
- Low Confidence: The assertion that these AutoML approaches will provide significant practical benefits

## Next Checks
1. Implement a proof-of-concept system applying curriculum learning to optimize task sequences on a standard benchmark (e.g., CIFAR-100 split into incremental tasks). Measure both performance improvements and computational overhead, specifically tracking how search time scales with the number of tasks.

2. Test a NAS-based architecture adaptation mechanism where model capacity grows/shrinks with task complexity. Crucially, measure not just final performance but also the stability of previous task knowledge retention when architectures change.

3. Implement per-task hyperparameter optimization for anti-forgetting regularization and compare against fixed hyperparameters across multiple datasets. Quantify the trade-off between improved per-task optimization and the increased computational cost of running hyperparameter searches for each new task.