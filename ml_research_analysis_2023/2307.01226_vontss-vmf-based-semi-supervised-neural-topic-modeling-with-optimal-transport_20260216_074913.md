---
ver: rpa2
title: 'vONTSS: vMF based semi-supervised neural topic modeling with optimal transport'
arxiv_id: '2307.01226'
source_url: https://arxiv.org/abs/2307.01226
tags:
- topic
- topics
- methods
- distribution
- vontss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces vONTSS, a semi-supervised neural topic modeling
  method that leverages von Mises-Fisher (vMF) distributions and optimal transport
  to incorporate human knowledge through keywords. The method addresses limitations
  of existing neural topic models (NTM) by using learnable temperature functions and
  concentration parameters to improve topic coherence and clusterability.
---

# vONTSS: vMF based semi-supervised neural topic modeling with optimal transport

## Quick Facts
- arXiv ID: 2307.01226
- Source URL: https://arxiv.org/abs/2307.01226
- Authors: [Not specified in input]
- Reference count: 40
- Key outcome: vONTSS outperforms state-of-the-art methods on benchmark datasets in both unsupervised and semi-supervised settings, achieving superior topic coherence, clusterability, diversity, and classification accuracy.

## Executive Summary
This paper introduces vONTSS, a semi-supervised neural topic modeling method that leverages von Mises-Fisher (vMF) distributions and optimal transport to incorporate human knowledge through keywords. The method addresses limitations of existing neural topic models by using learnable temperature functions and concentration parameters to improve topic coherence and clusterability. In unsupervised settings, vONTSS outperforms state-of-the-art methods on benchmark datasets. In semi-supervised settings, it achieves superior classification accuracy and stability compared to existing semi-supervised topic modeling methods while being significantly faster than weakly supervised text classification approaches.

## Method Summary
vONTSS uses a variational autoencoder framework with von Mises-Fisher distributions as priors for topic proportions. The method introduces a learnable temperature function to scale vMF samples before softmax, improving expressibility beyond the unity constraint of basic vMF distributions. A learnable concentration parameter κ enables better clustering properties. For semi-supervised learning, optimal transport loss aligns keyword sets with learned topics through probabilistic matching, avoiding redundant topic assignments while maintaining coherence through entropy regularization. The model is trained in two stages: first optimizing reconstruction and KL divergence, then jointly optimizing the optimal transport loss for few epochs.

## Key Results
- vONTSS outperforms state-of-the-art neural topic models on topic coherence (Cv, NPMI) and diversity metrics on DBLP, AgNews, 20News, and R8 datasets
- In semi-supervised settings, vONTSS achieves higher classification accuracy and stability than existing semi-supervised topic modeling methods
- vONTSS is 10x faster than weakly supervised text classification approaches while maintaining competitive performance
- The vMF prior provides better clustering than Gaussian priors, especially in low-dimensional spaces

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learnable temperature function improves expressibility of vMF distribution for topic modeling
- **Mechanism:** By introducing a learnable temperature parameter τ, the radius of the vMF distribution can be scaled, allowing samples to cover a wider range of probability vectors when passed through softmax. This addresses the unity constraint limitation of basic vMF distributions.
- **Core assumption:** The temperature scaling preserves the directional properties of vMF while enhancing the range of possible topic probability distributions
- **Evidence anchors:**
  - [section] "We introduce the notion of temperature and make the spread of vMF distribution (κ) learnable, which leads to strong coherence and cluster-inducing properties."
  - [section] "To overcome entangled topic latent space introduced by Gaussian distribution and limited expressibility of vMF distribution, we make two improvements: 1. Introduce a temperature function τ(ηi) prior to softmax() to modify the radius of vMF distribution."
- **Break condition:** If temperature scaling becomes too extreme, it may lose the directional clustering properties that make vMF effective for topic separation

### Mechanism 2
- **Claim:** Optimal transport loss aligns keyword sets with learned topics better than cross-entropy alone
- **Mechanism:** Optimal transport provides a probabilistic matching between keyword sets and topics that encourages one-to-one correspondence, avoiding redundant topic assignments while maintaining topic coherence through entropy regularization
- **Core assumption:** The cost matrix based on negative log-likelihood provides meaningful distances between keywords and topics
- **Evidence anchors:**
  - [section] "Optimal Transport (OT) distances... have been widely used for comparing the distribution of probabilities"
  - [section] "Our goal is to design the loss function that is aligned with derived cross-entropy loss at the global minimum"
  - [section] "We further show that LOT = Lce when L(X, T) is minimized"
- **Break condition:** If the entropy penalty λ is too high, topics may become overly uniform; if too low, multiple topics may collapse to the same keyword set

### Mechanism 3
- **Claim:** vMF prior induces better topic clustering than Gaussian prior in low-dimensional spaces
- **Mechanism:** The vMF distribution's KL divergence encourages concentration around mean directions without penalizing polarization, naturally separating topics into distinct clusters on the hypersphere
- **Core assumption:** Topic separation is beneficial for both coherence and classification performance
- **Evidence anchors:**
  - [section] "vMF distribution has better clusterability of data points especially in low dimensions (Guu et al., 2018)"
  - [section] "vMF distribution, on the other hand, repels four document classes into different quadrants, presents more structures when compared to Gaussian distribution, and creates better separable clusters"
- **Break condition:** In high-dimensional spaces, the clustering advantage may diminish as vMF becomes less expressive

## Foundational Learning

- **Concept:** von Mises-Fisher distribution and its properties
  - **Why needed here:** vMF serves as the prior distribution for topic proportions, enabling better clustering than Gaussian priors
  - **Quick check question:** What is the key difference between vMF and Gaussian distributions that makes vMF better for directional data?

- **Concept:** Optimal transport theory and Sinkhorn algorithm
  - **Why needed here:** OT provides the mathematical framework for aligning keyword sets with topics in the semi-supervised setting
  - **Quick check question:** How does the entropy regularization term in Sinkhorn distances affect the resulting topic-keyword mapping?

- **Concept:** Variational autoencoders and KL divergence regularization
  - **Why needed here:** The VAE framework provides the training objective and regularization that balances reconstruction quality with prior matching
  - **Quick check question:** Why does KL divergence behave differently for vMF versus Gaussian distributions in terms of encouraging polarization?

## Architecture Onboarding

- **Component map:** Document → Encoder → vMF sampling → Temperature scaling → Softmax → Topic proportions → Decoder → Reconstruction
- **Critical path:** Document → Encoder → vMF sampling → Temperature scaling → Softmax → Topic proportions → Decoder → Reconstruction
  - The semi-supervised extension adds: Keyword set → Optimal transport → Topic assignment
- **Design tradeoffs:**
  - Fixed vs. learnable κ: Learnable κ improves clustering but may reduce stability
  - Temperature function: Improves expressibility but requires careful initialization
  - Spherical vs. standard embeddings: Spherical embeddings improve clustering but may lose some semantic nuance
- **Failure signatures:**
  - Poor topic coherence: May indicate insufficient temperature scaling or inappropriate κ values
  - Unstable semi-supervised training: Could result from incorrect entropy penalty λ in optimal transport
  - Vanishing gradients: May occur if KL divergence dominates early in training
- **First 3 experiments:**
  1. Train vONT with different temperature values (1, 10, 20) on a small dataset to observe effect on topic coherence
  2. Compare vMF vs Gaussian priors on clustering metrics using the same encoder architecture
  3. Test optimal transport vs direct cross-entropy matching in semi-supervised setting with varying λ values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the learnable temperature parameter in vONTSS affect topic interpretability compared to fixed temperature values?
- Basis in paper: [explicit] The paper mentions that making the temperature function learnable leads to strong coherence and cluster-inducing properties, but does not provide a direct comparison of topic interpretability between learnable and fixed temperatures.
- Why unresolved: The paper focuses on the performance metrics like coherence and classification accuracy but does not delve into how learnable temperature affects human interpretability of topics.
- What evidence would resolve it: Conducting human evaluation studies comparing topic interpretability between vONTSS with learnable and fixed temperature parameters.

### Open Question 2
- Question: Can the optimal transport loss in vONTSS be further optimized to improve topic diversity without compromising classification accuracy?
- Basis in paper: [explicit] The paper demonstrates that vONTSS achieves high topic diversity and classification accuracy, but does not explore further optimization of the optimal transport loss for balancing these metrics.
- Why unresolved: The paper establishes the effectiveness of the current implementation but leaves room for exploring whether further tuning of the optimal transport loss could enhance topic diversity.
- What evidence would resolve it: Experiments testing different configurations of the optimal transport loss to find a balance between topic diversity and classification accuracy.

### Open Question 3
- Question: How does vONTSS perform in real-world applications with noisy or unstructured data compared to synthetic datasets?
- Basis in paper: [explicit] The paper evaluates vONTSS on benchmark datasets, but does not test its performance on real-world data that may contain noise or lack structure.
- Why unresolved: The controlled environment of benchmark datasets may not fully represent the challenges posed by real-world data, leaving uncertainty about vONTSS's robustness.
- What evidence would resolve it: Applying vONTSS to real-world datasets with known noise levels and evaluating its performance metrics and stability.

## Limitations

- The vMF prior's clustering advantage may diminish in high-dimensional topic spaces, potentially limiting the method's effectiveness on datasets with very large vocabularies
- The optimal transport loss hyperparameters, particularly the entropy penalty λ, require careful tuning and may be sensitive to dataset characteristics
- The method's performance on real-world noisy or unstructured data remains untested, raising questions about its robustness outside controlled benchmark environments

## Confidence

**High**: Claims about improved topic coherence and diversity compared to baseline NTMs (Section 5.1, Table 1-2)

**Medium**: Claims about superior semi-supervised classification accuracy and stability (Section 5.2, Table 3-4)

**Medium**: Claims about vMF providing better clustering than Gaussian priors (Section 3.2, Figure 2)

## Next Checks

1. **Dimensionality Sensitivity Test**: Evaluate vONTSS on datasets with varying vocabulary sizes to test the claim that vMF clustering advantage diminishes in high dimensions. Compare clustering metrics against Gaussian-based NTMs across the dimensionality spectrum.

2. **Hyperparameter Stability Analysis**: Systematically vary the temperature function range, concentration parameter κ, and optimal transport entropy penalty λ to identify sensitivity thresholds and failure modes. Report performance variance across multiple random seeds.

3. **Ablation Study on Topic Interpretability**: Conduct human evaluation studies comparing topic interpretability between vONTSS and baseline methods, particularly focusing on whether the claimed coherence improvements translate to human judgment and whether the topics are genuinely distinct versus artificially polarized.