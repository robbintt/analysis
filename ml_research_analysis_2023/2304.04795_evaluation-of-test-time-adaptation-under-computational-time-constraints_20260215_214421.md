---
ver: rpa2
title: Evaluation of Test-Time Adaptation Under Computational Time Constraints
arxiv_id: '2304.04795'
source_url: https://arxiv.org/abs/2304.04795
tags:
- methods
- online
- evaluation
- adaptation
- speed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an online evaluation protocol for Test Time
  Adaptation (TTA) methods that accounts for adaptation speed by simulating a constant-speed
  data stream, thereby penalizing slower methods with fewer adaptation samples. Experiments
  show that when inference speed is factored in, simple and fast TTA approaches (e.g.,
  SHOT from 2020) can outperform more sophisticated but slower methods (e.g., SAR
  from 2023).
---

# Evaluation of Test-Time Adaptation Under Computational Time Constraints

## Quick Facts
- arXiv ID: 2304.04795
- Source URL: https://arxiv.org/abs/2304.04795
- Reference count: 40
- Primary result: Simple TTA methods (e.g., SHOT) outperform sophisticated but slower methods (e.g., SAR) when evaluation accounts for adaptation speed

## Executive Summary
This paper introduces an online evaluation protocol for Test Time Adaptation (TTA) methods that accounts for adaptation speed by simulating a constant-speed data stream. The protocol penalizes slower methods by providing them fewer samples for adaptation, revealing that computationally efficient approaches maintain performance while complex methods degrade significantly. Experiments on ImageNet-C, ImageNet-R, and ImageNet-3DCC with 15 TTA methods demonstrate that sample rejection-based methods (e.g., EATA) and efficient approaches (e.g., AdaBN, BN) perform best under these realistic constraints.

## Method Summary
The paper proposes an online evaluation protocol that simulates a constant-speed data stream where TTA methods with higher computational overhead miss adaptation opportunities. The protocol computes a relative adaptation speed C(g) for each method by measuring the time ratio between adaptation and forward pass. During evaluation, adaptation steps are conditionally performed based on C(g), with slower methods adapting to fewer samples. The study benchmarks 15 TTA methods including AdaBN, SHOT, SAR, and EATA on three ImageNet benchmark datasets under episodic and continual adaptation scenarios, comparing results against a baseline offline protocol.

## Key Results
- Simple methods like SHOT (2020) outperform newer sophisticated methods like SAR (2023) when adaptation speed is factored in
- Methods adapting during forward pass (AdaBN, BN) maintain performance with C(g) ≈ 1
- Sample rejection methods (EATA) achieve best overall performance through efficient filtering
- Complex data-dependent methods (MEMO, DDA) suffer significant degradation under online constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online evaluation penalizes slower TTA methods by providing them fewer samples for adaptation
- Mechanism: When a TTA method takes longer to adapt to a sample, the constant-speed data stream causes it to miss subsequent samples. These missed samples are not available for adaptation, directly reducing the method's accuracy
- Core assumption: The data stream speed is constant and independent of the method's processing speed
- Evidence anchors:
  - [abstract] "penalizes slower methods by providing them fewer samples for adaptation"
  - [section] "the slower the TTA method, the fewer samples it can leverage for adapting to the distribution shift"
  - [corpus] Weak evidence; related works focus on adaptation effectiveness, not speed-dependent sample access
- Break condition: If the stream speed is variable or can be paused, the penalization mechanism breaks down

### Mechanism 2
- Claim: Methods that adapt during the forward pass maintain performance under online evaluation
- Mechanism: These methods incur negligible additional computation compared to the baseline forward pass, so their relative adaptation speed C(g) ≈ 1. They adapt to every sample, preserving their accuracy
- Core assumption: The additional computation for these methods is small enough to be negligible compared to the base forward pass time
- Evidence anchors:
  - [section] "Very efficient methods, with C(g) = 1, such as LAME and BN, do not lose in performance"
  - [section] "methods generally perform worse in the more realistic online setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance"
  - [corpus] Weak evidence; neighbors discuss adaptation but not forward-pass adaptation speed
- Break condition: If the additional computation becomes non-negligible (e.g., very large batch sizes or complex models), C(g) increases and performance degrades

### Mechanism 3
- Claim: Sample rejection methods perform well under online evaluation due to efficient sample filtering
- Mechanism: These methods quickly reject unreliable or redundant samples, allowing them to adapt to more samples within the constant stream speed. This combination of good performance and adaptation speed makes them robust under online evaluation
- Core assumption: Sample rejection can be performed efficiently (e.g., via forward pass only) without significant computational overhead
- Evidence anchors:
  - [section] "EATA adapts efficiently due to its fast sample rejection algorithm, which relies solely on the forward pass to admit samples for adaptation"
  - [section] "EATA's low error rate of 55.6%, combined with a small performance drop of less than 4%, positions it as the top performer under the online evaluation protocol"
  - [corpus] Weak evidence; neighbors do not discuss sample rejection in the context of online evaluation
- Break condition: If sample rejection becomes computationally expensive (e.g., requires gradient computation), the method's speed decreases and performance degrades

## Foundational Learning

- Concept: Online Learning and Data Stream Processing
  - Why needed here: The paper's evaluation protocol simulates a constant-speed data stream, which is a core concept in online learning. Understanding how data streams are processed and how model adaptation interacts with stream speed is crucial for grasping the paper's contributions
  - Quick check question: What is the key difference between offline and online evaluation protocols in the context of test-time adaptation?

- Concept: Test-Time Adaptation (TTA) Methods
  - Why needed here: The paper benchmarks various TTA methods under the proposed online evaluation protocol. Understanding the different types of TTA methods (e.g., entropy minimization, fine-tuning, sample rejection) and their computational characteristics is essential for interpreting the results
  - Quick check question: How do entropy minimization methods like TENT differ from fine-tuning methods like SHOT in terms of computational overhead?

- Concept: Batch Normalization and its Variants
  - Why needed here: Several TTA methods in the paper leverage batch normalization statistics for adaptation. Understanding how batch normalization works and its variants (e.g., AdaBN, BN) is important for grasping the efficiency of certain methods under online evaluation
  - Quick check question: How does AdaBN differ from standard batch normalization in terms of adapting to test-time data?

## Architecture Onboarding

- Component map: Pre-trained ResNet-50-BN -> TTA methods (15 total) -> Online evaluation protocol -> ImageNet-C/R/3DCC datasets
- Critical path:
  1. Load pre-trained ResNet-50 model
  2. Implement each TTA method as a function that takes the model and input batch
  3. Implement online evaluation protocol:
     - Simulate constant-speed data stream
     - For each batch, check if TTA method is available (based on C(g))
     - If available, apply TTA method and update model
     - If not available, use current model for prediction
  4. Evaluate performance on benchmark datasets
- Design tradeoffs:
  - Batch size: Larger batch sizes generally improve TTA method performance but increase computational overhead
  - Relative adaptation speed C(g): Higher C(g) means fewer samples for adaptation, leading to lower accuracy
  - Stream speed: Slower stream speeds allow TTA methods more time for adaptation, potentially improving accuracy
- Failure signatures:
  - Degraded performance under online evaluation compared to offline evaluation
  - High C(g) values indicating slow adaptation speed
  - Large performance drops when switching from offline to online evaluation
- First 3 experiments:
  1. Implement and evaluate a simple TTA method (e.g., AdaBN) under the online protocol on ImageNet-C
  2. Compare the performance of a computationally expensive TTA method (e.g., SAR) with a simple method (e.g., AdaBN) under varying stream speeds
  3. Evaluate the impact of batch size on the performance of different TTA methods under the online protocol

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do TTA methods perform under extreme computational constraints, such as mobile or edge devices?
- Basis in paper: [explicit] The paper notes that efficient methods like BN and AdaBN maintain performance, while data-dependent methods like MEMO and DDA suffer significant degradation due to high computational overhead
- Why unresolved: The paper focuses on comparing methods under a constant-speed stream, but does not explore performance under resource-constrained environments like mobile or edge devices
- What evidence would resolve it: Benchmarking TTA methods on mobile or edge devices with varying computational budgets and stream speeds would provide insights into their real-world applicability

### Open Question 2
- Question: Can sample rejection mechanisms be further optimized to improve both speed and accuracy for TTA methods?
- Basis in paper: [explicit] EATA is highlighted as the top performer due to its efficient sample rejection algorithm, but the paper does not explore whether its rejection mechanism can be further optimized
- Why unresolved: While EATA performs well, the paper does not investigate whether alternative or improved rejection strategies could yield better results
- What evidence would resolve it: Experiments comparing EATA's rejection mechanism with alternative strategies (e.g., adaptive thresholds or machine learning-based rejection) would clarify the potential for further optimization

### Open Question 3
- Question: How do TTA methods generalize to non-image data domains, such as audio or text?
- Basis in paper: [inferred] The paper focuses exclusively on image classification tasks, leaving open the question of how TTA methods perform in other domains
- Why unresolved: The evaluation is limited to ImageNet-based datasets, which are image-centric, and does not extend to other modalities like audio or text
- What evidence would resolve it: Applying TTA methods to tasks like speech recognition or natural language processing and comparing their performance under the online protocol would address this question

## Limitations
- The online evaluation protocol assumes consistent computational overhead across different data points and batch sizes
- The protocol doesn't account for GPU parallelization effects or memory constraints that might affect real-world deployment
- Variable adaptation times for methods like ETA and EATA could introduce noise in C(g) measurements

## Confidence
- **High Confidence:** The core finding that simple, fast methods (AdaBN, BN) maintain performance under online evaluation while complex methods degrade
- **Medium Confidence:** The relative ranking of TTA methods under online constraints, as hardware-specific timing variations could affect C(g) calculations differently across experimental setups
- **Low Confidence:** The exact threshold values for adaptation speed that would cause significant performance degradation, as these may vary with model architecture and hardware configurations

## Next Checks
1. **Ablation on Batch Size Scaling:** Systematically vary batch sizes beyond the tested range (1-128) to identify at what point computational overhead causes dramatic performance drops for complex methods
2. **Cross-Architecture Validation:** Test the online protocol with architectures other than ResNet-50 (e.g., EfficientNet, ViT) to verify if the speed-performance tradeoff generalizes across model families
3. **Real-Time Streaming Benchmark:** Implement the evaluation protocol on actual streaming data infrastructure rather than simulation to validate whether the theoretical speed constraints match real-world processing delays