---
ver: rpa2
title: 'Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces
  of Pre-trained Language Models'
arxiv_id: '2305.17446'
source_url: https://arxiv.org/abs/2305.17446
tags:
- subspace
- intrinsic
- fine-tuning
- dimensions
- outlier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the intrinsic task-specific subspaces of pre-trained
  language models (PLMs) by analyzing the dynamics of the fine-tuning process. The
  authors propose a method to uncover these subspaces by finding the principal directions
  of the fine-tuning trajectory.
---

# Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models

## Quick Facts
- arXiv ID: 2305.17446
- Source URL: https://arxiv.org/abs/2305.17446
- Authors: Not specified in input
- Reference count: 16
- Key outcome: Fine-tuning pre-trained language models in low-dimensional intrinsic task-specific subspaces achieves comparable performance to full fine-tuning across GLUE tasks.

## Executive Summary
This paper explores the intrinsic task-specific subspaces of pre-trained language models (PLMs) by analyzing the dynamics of the fine-tuning process. The authors propose a method to uncover these subspaces by finding the principal directions of the fine-tuning trajectory. They demonstrate that PLMs can be effectively fine-tuned in these low-dimensional subspaces with a small number of free parameters, achieving comparable performance to full fine-tuning across various tasks and models including BERT and RoBERTa. Additionally, the authors observe outlier dimensions emerging during fine-tuning in the intrinsic task-specific subspaces, which are crucial for inducing task-specific knowledge to downstream tasks.

## Method Summary
The method involves first performing normal fine-tuning while saving model checkpoints to capture the optimization trajectory. These checkpoints are then stacked and processed using Singular Value Decomposition (SVD) to find the principal directions of the trajectory, which form the basis of the intrinsic task-specific subspace. The model is re-parameterized to optimize only in this discovered subspace while keeping other parameters frozen. The fine-tuning process uses an ensemble approach to reduce variance. The method is tested on GLUE benchmark tasks using BERT-base and RoBERTa-base models, comparing performance with full fine-tuning and random subspace baselines.

## Key Results
- Fine-tuning in intrinsic task-specific subspaces with small number of free parameters achieves comparable performance to full fine-tuning on GLUE tasks.
- Outlier dimensions emerge during fine-tuning in intrinsic subspaces and are crucial for task adaptation - disabling them significantly degrades performance.
- Intrinsic task-specific subspaces show transferability across related tasks, with performance correlating with task similarity and scale.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PLMs can be fine-tuned effectively in low-dimensional subspaces that capture task-specific information.
- Mechanism: The fine-tuning trajectory of a PLM lies in a low-dimensional subspace spanned by principal directions of parameter updates. By finding these directions via SVD on the trajectory, we can re-parameterize the model to train only in this subspace.
- Core assumption: The optimization trajectory is constrained to a low-dimensional subspace regardless of the full parameter space dimensionality.
- Evidence anchors:
  - [abstract] "by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace"
  - [section 3.2] "the fine-tuning trajectory will lie in a low-dimensional subspace. Thus we can resort to the fine-tuning trajectory to obtain an approximation of the intrinsic task-specific subspace"
  - [corpus] Weak - related work mentions "intrinsic language-specific subspaces" but no direct mechanism evidence
- Break condition: If the optimization trajectory spreads across many dimensions or if the principal directions don't capture task-relevant information, the subspace approximation fails.

### Mechanism 2
- Claim: Outlier dimensions emerge during fine-tuning in intrinsic subspaces and are crucial for task adaptation.
- Mechanism: When fine-tuning in the intrinsic subspace, certain dimensions show abnormal spikes in parameter updates. These outlier dimensions represent parameters critical for adapting the model to the downstream task.
- Core assumption: The magnitude of parameter updates correlates with their importance for task-specific adaptation.
- Evidence anchors:
  - [abstract] "we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly"
  - [section 4.5] "we find that PLMs have a small number of outlier dimensions exhibiting abnormal spikes when fine-tuning in the intrinsic task-specific subspaces"
  - [corpus] Weak - related work mentions "outlier dimensions" but focuses on output channels rather than parameter weights
- Break condition: If disabling outlier dimensions doesn't significantly impact performance, or if random dimensions have similar importance, the outlier significance claim breaks.

### Mechanism 3
- Claim: Intrinsic task-specific subspaces have transferability across related tasks.
- Mechanism: Subspaces learned from one task can be applied to similar tasks with reasonable performance, as the intrinsic dimensionality captures generalizable task-specific patterns.
- Core assumption: Related tasks share overlapping intrinsic subspaces in the parameter space.
- Evidence anchors:
  - [abstract] "we further show that the uncovered intrinsic task-specific subspaces have a certain transferability"
  - [section 4.3] "the intrinsic task-specific subspace has a certain transferability"
  - [corpus] Weak - no direct transferability evidence in corpus, mentions "task heterogeneity-aware" approaches but not subspace transfer
- Break condition: If transferred subspaces consistently underperform random subspaces or if transferability only works for very similar tasks, the generalizability claim weakens.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is used to find principal directions of the fine-tuning trajectory, which form the basis of the intrinsic subspace.
  - Quick check question: What do the right singular vectors from SVD of the trajectory matrix represent in this context?

- Concept: Intrinsic dimensionality
  - Why needed here: Understanding intrinsic dimensionality helps explain why low-dimensional subspaces can capture most of the task-relevant information.
  - Quick check question: How does intrinsic dimensionality differ from the total number of parameters in a model?

- Concept: Parameter re-parameterization
  - Why needed here: The method relies on re-parameterizing the model to optimize only in the discovered subspace while keeping other parameters frozen.
  - Quick check question: What is the mathematical relationship between the full parameter space and the intrinsic subspace in the re-parameterization?

## Architecture Onboarding

- Component map:
  - Trajectory collector -> SVD processor -> Re-parameterization layer -> Fine-tuning engine -> Outlier detector

- Critical path:
  1. Fine-tune model normally to collect trajectory checkpoints
  2. Stack checkpoints and apply SVD to find principal directions
  3. Re-parameterize model using discovered subspace
  4. Fine-tune in subspace with ensemble approach
  5. Analyze outlier dimensions if needed

- Design tradeoffs:
  - Layer-wise vs full-model re-parameterization: Layer-wise reduces memory but may miss global patterns
  - Trajectory length: Longer trajectories better capture dynamics but increase computation
  - Ensemble size: Larger ensembles reduce variance but increase parameters

- Failure signatures:
  - Performance similar to random subspace fine-tuning
  - Outlier dimensions don't exist or don't affect performance
  - Subspace transferability fails completely across all tasks

- First 3 experiments:
  1. Compare intrinsic subspace fine-tuning vs random subspace on a simple task (e.g., SST-2) to verify the core mechanism
  2. Test outlier dimension disabling on a single layer to confirm their importance
  3. Transfer a subspace from a large dataset task to a small dataset task to test transferability limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the outlier dimensions identified in this paper relate to the previously discovered outlier dimensions in PLMs (e.g., in terms of their emergence and impact)?
- Basis in paper: [explicit] The paper explicitly states that the outlier dimensions in their context refer to specific model parameters, which differs from previous studies that considered output neurons or channels.
- Why unresolved: While the paper identifies and analyzes these outlier dimensions, the underlying mechanism responsible for their emergence remains unclear.
- What evidence would resolve it: Further empirical analysis or theoretical insights into the emergence and propagation of outlier dimensions through layers would help clarify their nature and role in the model.

### Open Question 2
- Question: Can a task-specific global subspace be discovered within the entire parameter space of a pre-trained language model, as opposed to the local subspaces identified in this paper?
- Basis in paper: [inferred] The paper mentions that the layer-wise re-parameterization setting restricts the identification of local subspaces rather than global subspaces within the entire parameter space.
- Why unresolved: The existence of a task-specific global subspace and its correlation with the identified local subspaces are yet to be ascertained.
- What evidence would resolve it: Developing a method to discover and analyze global subspaces within the entire parameter space, and comparing their effectiveness with local subspaces, would provide insights into the existence and relevance of global subspaces.

### Open Question 3
- Question: How transferable are the intrinsic task-specific subspaces across different tasks, and what factors influence their transferability?
- Basis in paper: [explicit] The paper conducts inductive intrinsic subspace fine-tuning to examine the transferability of the discovered subspaces and observes that the transferability seems to correlate with the scale of the transferred task.
- Why unresolved: While the paper demonstrates the transferability of intrinsic task-specific subspaces, the factors influencing their transferability and the extent of transferability across tasks remain unclear.
- What evidence would resolve it: Conducting a systematic study on the transferability of intrinsic task-specific subspaces across various tasks, considering factors such as task similarity, dataset size, and model architecture, would provide insights into the transferability and its influencing factors.

## Limitations
- The method requires full fine-tuning to first collect trajectory data, which negates some computational benefits of the approach.
- Subspace discovery depends heavily on the quality and length of the fine-tuning trajectory, with insufficient trajectories potentially leading to poor subspace approximation.
- Transferability claims need more rigorous testing across task types, as current evidence suggests limited generalizability.

## Confidence

**High Confidence:** The core mechanism of using SVD on fine-tuning trajectories to discover low-dimensional subspaces is well-established mathematically and the experimental results consistently show performance comparable to full fine-tuning across multiple tasks and models.

**Medium Confidence:** The claims about outlier dimensions being crucial for task adaptation are supported by empirical evidence but lack theoretical grounding. The observation that disabling outliers degrades performance is clear, but why these specific dimensions emerge remains unexplained.

**Medium Confidence:** The transferability claims are supported by experimental results but the evaluation is limited to relatively similar tasks within the GLUE benchmark. The extent to which these subspaces transfer to more distant task types remains uncertain.

## Next Checks
1. **Ablation study on trajectory length:** Systematically vary the number of checkpoints collected during fine-tuning to determine the minimum trajectory length needed for effective subspace discovery, establishing practical limits of the method.

2. **Theoretical analysis of outlier emergence:** Develop a mathematical framework explaining why certain dimensions become outliers during fine-tuning in intrinsic subspaces, connecting this phenomenon to the underlying optimization dynamics.

3. **Cross-domain transferability test:** Evaluate subspace transferability from GLUE tasks to completely different NLP tasks (e.g., text summarization or question answering) to establish the boundaries of subspace generalizability beyond similar classification tasks.