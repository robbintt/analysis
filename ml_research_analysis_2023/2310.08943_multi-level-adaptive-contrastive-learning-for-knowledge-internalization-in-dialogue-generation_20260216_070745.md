---
ver: rpa2
title: Multi-level Adaptive Contrastive Learning for Knowledge Internalization in
  Dialogue Generation
arxiv_id: '2310.08943'
source_url: https://arxiv.org/abs/2310.08943
tags:
- knowledge
- responses
- macl
- dataset
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a specific degeneration problem in knowledge-grounded
  dialogue generation called "Knowledge Regurgitation", where models copy knowledge
  snippets without genuine comprehension. To address this, the authors propose Multi-level
  Adaptive Contrastive Learning (MACL), which uses dynamic negative sampling and penalizes
  degeneration behaviors at both token and sequence levels.
---

# Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation

## Quick Facts
- arXiv ID: 2310.08943
- Source URL: https://arxiv.org/abs/2310.08943
- Reference count: 27
- Primary result: MACL significantly reduces knowledge regurgitation (7.93% PoD vs 21.35% for strongest baseline) while maintaining high response quality

## Executive Summary
This paper addresses the knowledge regurgitation problem in knowledge-grounded dialogue generation, where models copy knowledge snippets without genuine comprehension. The authors propose Multi-level Adaptive Contrastive Learning (MACL), which uses dynamic negative sampling and contrastive losses at both token and sequence levels to penalize degeneration behaviors. Experiments on the WoW dataset demonstrate MACL significantly reduces knowledge regurgitation while maintaining response quality, outperforming strong baselines including KDConv, GDC, and CDT.

## Method Summary
MACL is a framework that combines token-level and sequence-level contrastive learning to prevent knowledge regurgitation. The token-level component dynamically samples negative tokens (selecting the knowledge token with highest predicted probability) and applies an unlikelihood loss with reweighting based on prediction probability. The sequence-level component uses group beam search to generate hard negative responses, then applies contrastive loss to pull ground truth responses closer while pushing away degenerated responses. The final training objective combines standard MLE with both contrastive losses, optimized on pre-trained encoder-decoder models.

## Key Results
- MACL achieves 7.93% PoD on WoW test seen, significantly lower than strongest baseline (21.35% for KDConv)
- Maintains high response quality with 0.52 KUD vs 5.02 for strongest baseline
- Outperforms baselines on human evaluations across coherence, engagingness, informativeness, and interactiveness
- Ablation study shows both token-level and sequence-level contrastive learning are essential for performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The model learns to avoid directly copying knowledge snippets by dynamically sampling negative tokens that the model is most likely to incorrectly predict.
- **Mechanism**: Dynamic negative token sampling targets the model's prediction shortcuts by selecting the knowledge token with the highest predicted probability as a negative example. This breaks the generation inertia where the model tends to duplicate knowledge due to spurious correlations.
- **Core assumption**: The model's prediction shortcuts can be effectively disrupted by targeting tokens it is most likely to incorrectly select, rather than uniformly penalizing all knowledge tokens.
- **Evidence anchors**:
  - [section] "we adopt a strategy of selecting the knowledge token with the highest prediction probability as the negative token"
  - [abstract] "dynamically samples negative examples and subsequently penalizes degeneration behaviors at both the token-level and sequence-level"
- **Break condition**: If the model develops alternative shortcuts not captured by prediction probability, or if the knowledge tokens are too similar to the response tokens for effective discrimination.

### Mechanism 2
- **Claim**: Sequence-level contrastive learning exposes the model to potential degeneration patterns during training, helping it learn to avoid them at inference.
- **Mechanism**: The model is trained with hard negative examples that exhibit knowledge regurgitation, using an InfoNCE loss that pulls the ground truth response closer while pushing away degenerated responses in representation space.
- **Core assumption**: Exposing the model to its own potential degeneration mistakes during training helps it learn to avoid these patterns when generating responses.
- **Evidence anchors**:
  - [section] "we employ a group beam search strategy to sample negative responses from the degenerator's predictions, then use a novel metric as an oracle function to score them"
  - [abstract] "penalizes degeneration behaviors at both the token-level and sequence-level"
- **Break condition**: If the hard negative sampling fails to capture the most common degeneration patterns, or if the contrastive loss over-penalizes legitimate knowledge integration.

### Mechanism 3
- **Claim**: Dynamic reweighting of the unlikelihood loss based on model sensitivity helps the model converge better by focusing on harder examples.
- **Mechanism**: Tokens with higher prediction probability receive lower penalty weights, while tokens with lower prediction probability receive higher penalty weights, following a cosine-based reweighting function.
- **Core assumption**: Different tokens contribute differently to the degeneration problem, and adjusting the penalty strength based on prediction difficulty improves training efficiency.
- **Evidence anchors**:
  - [section] "we enhance the unlikelihood loss by introducing an additional control parameter β(yc) to dynamically reweight the punishment strength for different tokens"
  - [section] "we suppress the gradients of easy tokens while amplifying the gradients of hard tokens"
- **Break condition**: If the reweighting function is poorly calibrated, leading to under-penalization of easy negatives or over-penalization of hard negatives.

## Foundational Learning

- **Concept**: Contrastive learning
  - Why needed here: To create a training signal that explicitly distinguishes between knowledge-integrated responses and knowledge-regurgitated responses
  - Quick check question: How does contrastive learning differ from traditional cross-entropy training in terms of what it optimizes for?

- **Concept**: Knowledge-grounded dialogue generation
  - Why needed here: The work addresses a specific problem in this task where models fail to genuinely internalize knowledge
  - Quick check question: What distinguishes knowledge-grounded dialogue from standard open-domain dialogue generation?

- **Concept**: Unlikelihood training
  - Why needed here: Forms the foundation for the token-level contrastive learning approach to break generation inertia
  - Quick check question: How does unlikelihood training complement maximum likelihood estimation in addressing text degeneration?

## Architecture Onboarding

- **Component map**: Context + Knowledge → Encoder → Decoder → Response, with contrastive losses applied at both token and sequence levels during training

- **Critical path**: Context + Knowledge → Encoder → Decoder → Response, with contrastive losses applied at both token and sequence levels during training

- **Design tradeoffs**:
  - Computational cost vs. effectiveness: Generating hard negative examples increases training time but improves degeneration prevention
  - Knowledge retention vs. regurgitation prevention: Need to balance preventing copying while maintaining ability to integrate knowledge

- **Failure signatures**:
  - High perplexity on ground truth responses
  - Increased knowledge regurgitation (high PoD scores)
  - Degraded response quality metrics (BLEU, embedding similarity)
  - Model converges to a suboptimal solution where it avoids both copying and proper knowledge integration

- **First 3 experiments**:
  1. Ablation study: Remove token-level contrastive loss to measure its impact on knowledge regurgitation
  2. Ablation study: Remove sequence-level contrastive loss to measure its impact on degeneration patterns
  3. Hyperparameter sensitivity: Test different values for α (unlikelihood weight) and λ (sequence loss weight) to find optimal balance

## Open Questions the Paper Calls Out

The authors explicitly acknowledge limitations regarding evaluation on larger language models beyond 1 billion parameters and express plans to explore degeneration in larger language models (LLMs) in future work.

## Limitations

- The study focuses exclusively on one knowledge-grounded dialogue dataset (Wizard of Wikipedia), limiting generalizability to other domains or knowledge sources.
- Computational overhead of generating hard negative samples through group beam search likely increases training time substantially, though this is not quantified.
- The paper does not provide inter-annotator agreement scores or specify the number of annotators in human evaluations, making it difficult to assess reliability.

## Confidence

**High Confidence**: The core mechanism of using contrastive learning to prevent knowledge regurgitation is well-supported by the quantitative results. The significant reduction in PoD scores across multiple baselines, combined with maintained or improved response quality metrics, provides strong empirical evidence that the approach works as claimed.

**Medium Confidence**: The claim that dynamic negative sampling specifically targets the model's prediction shortcuts is supported by the methodology description but lacks ablation studies isolating this component's contribution.

**Low Confidence**: The assertion that MACL achieves better alignment with human knowledge utilization patterns (KUD metric) is based on a single reference model and lacks comparison to a broader range of human-human dialogue data.

## Next Checks

1. **Ablation Study on Dynamic Components**: Conduct controlled experiments removing the dynamic reweighting mechanism and static negative sampling to quantify their individual contributions to knowledge regurgitation reduction.

2. **Generalization Test Across Knowledge Sources**: Evaluate MACL on a different knowledge-grounded dialogue dataset (e.g., CMU-DoG or Topical-Chat) to verify the approach generalizes beyond Wikipedia-based knowledge.

3. **Computational Overhead Analysis**: Measure and report training time, inference latency, and memory requirements for MACL compared to baseline models to quantify practical deployment costs.