---
ver: rpa2
title: 'REFINER: Reasoning Feedback on Intermediate Representations'
arxiv_id: '2304.01904'
source_url: https://arxiv.org/abs/2304.01904
tags:
- critic
- feedback
- reasoning
- refiner
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces REFINER, a novel framework for improving
  language models'' reasoning abilities through iterative feedback on intermediate
  reasoning steps. The method employs two models: a generator that produces intermediate
  reasoning steps and a critic that provides structured feedback on errors.'
---

# REFINER: Reasoning Feedback on Intermediate Representations

## Quick Facts
- **arXiv ID**: 2304.01904
- **Source URL**: https://arxiv.org/abs/2304.01904
- **Reference count**: 40
- **Key outcome**: REFINER framework improves language model reasoning through structured feedback on intermediate steps, outperforming baselines by 3-9.2 points without requiring fine-tuning of large models.

## Executive Summary
REFINER introduces a novel framework for improving language models' reasoning abilities through iterative feedback on intermediate reasoning steps. The method employs two models: a generator that produces intermediate reasoning steps and a critic that provides structured feedback on errors. The generator learns to refine its outputs based on this feedback during training, and the framework can also be applied during inference. The approach is evaluated on three diverse reasoning tasks and shows significant improvements over baseline language models of comparable scale.

## Method Summary
REFINER uses a two-stage training approach: a warm-up phase where the generator learns basic task completion, followed by an interaction loop where the generator produces multiple reasoning hypotheses, the critic selects and evaluates one hypothesis providing structured feedback on specific error types, and the generator refines its output based on this feedback. The framework is trained on perturbed examples where correct intermediate steps are intentionally corrupted to create training data for the critic. During inference, the trained critic can be applied to improve reasoning without further fine-tuning of the generator.

## Key Results
- REFINER significantly outperforms baseline language models of comparable scale across math word problems, synthetic natural language reasoning, and moral norm/action generation tasks
- When using GPT-3.5 as the generator, the trained critic alone improves performance by 3-9.2 points without requiring fine-tuning of the large model
- Structured, fine-grained feedback on reasoning errors proves more effective than scalar value feedback approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative feedback loop with structured error types enables targeted corrections
- Mechanism: The critic identifies fine-grained errors in intermediate steps and provides textual feedback that guides the generator to refine its outputs in a targeted way.
- Core assumption: Errors in intermediate reasoning steps can be decomposed into discrete, identifiable types that can be corrected independently.
- Evidence anchors:
  - [abstract] "structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments"
  - [section] "We create pairs of incorrect intermediate representations and structured feedback on fine-grained reasoning errors"
  - [corpus] "MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models" (moderate similarity) suggests this is an active research area
- Break condition: If the error types cannot be cleanly separated or if feedback is too ambiguous to guide correction

### Mechanism 2
- Claim: Training generator with imperfect critic is still beneficial up to a noise threshold
- Mechanism: Generator learns to interpret and act on imperfect feedback, becoming more robust to noise while still improving over no feedback.
- Core assumption: The generator can distinguish useful feedback from noise and learn patterns in feedback quality.
- Evidence anchors:
  - [section] "training with a bit of noise (< 50%) does not seem to harm the model"
  - [section] "We observe, in Fig. 6 (a), that when training with a very noisy critic (> 75% noise), the generator LM learns to ignore the critic"
  - [corpus] "SMART: Self-learning Meta-strategy Agent for Reasoning Tasks" (moderate similarity) explores adaptive reasoning strategies
- Break condition: If noise exceeds ~75% where generator starts ignoring all feedback

### Mechanism 3
- Claim: Structured feedback on intermediate steps is more effective than scalar value feedback
- Mechanism: Fine-grained textual feedback allows the generator to understand specific reasoning errors rather than just receiving a quality score.
- Core assumption: Reasoning errors have meaningful structure that can be communicated through natural language.
- Evidence anchors:
  - [abstract] "providing structured feedback on reasoning errors is more effective than scalar value feedback"
  - [section] "we observe that REFINER could even outperform PPO, which uses BLEU-score as a reward function"
  - [corpus] "MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models" (moderate similarity) directly compares structured vs scalar feedback approaches
- Break condition: If the feedback structure becomes too complex for the generator to parse or act upon

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: REFINER builds on the observation that explicit intermediate reasoning steps improve model performance
  - Quick check question: Can you explain why generating intermediate steps helps with complex reasoning tasks?

- Concept: Structured error classification
  - Why needed here: The critic needs to identify specific error types to provide targeted feedback
  - Quick check question: How would you design a taxonomy of reasoning errors for a new task domain?

- Concept: Interactive training loops
  - Why needed here: The generator-critic interaction requires understanding of how feedback influences learning dynamics
  - Quick check question: What happens to learning when feedback is delayed or noisy?

## Architecture Onboarding

- Component map:
  - Generator: T5-base or T5-large model that produces intermediate reasoning steps
  - Critic: T5-base model that evaluates intermediate steps and generates feedback
  - Data pipeline: Creates perturbed examples and corresponding feedback
  - Training loop: Iteratively generates, critiques, and refines reasoning steps

- Critical path:
  1. Warm-up phase: Generator learns basic task
  2. Exploration: Generator produces multiple hypotheses
  3. Critique: Critic selects and evaluates one hypothesis
  4. Feedback: Critic provides structured error feedback
  5. Refinement: Generator updates based on feedback
  6. Iterate: Repeat exploration-critic-feedback cycle

- Design tradeoffs:
  - Generator size vs performance: Smaller generators require more iterations
  - Feedback granularity: Too fine-grained feedback may be hard to parse; too coarse may miss important errors
  - Noise tolerance: Training with imperfect critics improves robustness but may slow convergence

- Failure signatures:
  - Generator ignores feedback consistently (feedback too noisy or generator too large)
  - Critic provides incorrect feedback patterns (training data quality issues)
  - No improvement over baseline (exploration phase too limited or feedback not actionable)

- First 3 experiments:
  1. Baseline: Generator without any critic feedback
  2. Oracle test: Generator with perfect critic feedback to establish upper bound
  3. Noise sensitivity: Train with critics of varying noise levels to find optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does REFINER perform on other reasoning tasks beyond the three tested (math word problems, synthetic natural language reasoning, and moral norm/action generation)?
- Basis in paper: [explicit] The paper states "Our REFINER framework is very general and in principle, might be applied to steer language models in performing different reasoning tasks."
- Why unresolved: The paper only evaluates REFINER on three specific reasoning tasks due to their diversity and the sheer number of possible reasoning tasks.
- What evidence would resolve it: Experiments applying REFINER to other reasoning tasks (e.g., logical reasoning, commonsense reasoning, causal reasoning) and comparing performance to baselines.

### Open Question 2
- Question: What is the optimal number of iterations (T) for the REFINER framework during inference?
- Basis in paper: [explicit] The paper mentions using T=3 iterations during training and T=1 for the automatic critic at inference, but also states "we trained models with T=3 iterations" and "we use greedy decoding for the generator and critic model with T = 1 for the automatic critic and T = 3 for the oracle critic."
- Why unresolved: The paper uses different values of T for different components and does not provide a systematic analysis of the optimal T value.
- What evidence would resolve it: Ablation studies varying T during inference and measuring performance on reasoning tasks.

### Open Question 3
- Question: How does the performance of REFINER change when using different critic models (e.g., larger models, models trained on different data)?
- Basis in paper: [explicit] The paper states "Our critic model acts as a 'reasoning refinement tool' for LLMs" and "we train a UnifedQa-T5-base model (UQA-base) as a critic" but does not explore variations in critic model size or training data.
- Why unresolved: The paper only uses one specific critic model and does not explore how critic model variations affect REFINER's performance.
- What evidence would resolve it: Experiments using different critic models (e.g., larger models, models trained on different data or tasks) and measuring their impact on REFINER's performance.

## Limitations
- Effectiveness depends on quality and granularity of error taxonomy - may fail when errors are ambiguous or multiple types co-occur
- Requires training a separate critic model, adding computational overhead and complexity
- Evaluation limited to controlled tasks; performance on complex, open-ended reasoning remains untested

## Confidence
- **High confidence**: Core mechanism of iterative feedback improving reasoning performance (consistent gains across all three tasks)
- **Medium confidence**: Structured feedback outperforms scalar feedback (based on PPO comparisons but limited ablation)
- **Medium confidence**: Noise tolerance findings (synthetic noise experiments, not tested with naturally imperfect critics)

## Next Checks
1. **Cross-task error taxonomy validation**: Test whether the same error classification scheme works across different reasoning domains or if task-specific taxonomies are required, and measure the impact on performance when transferring critic models between tasks.
2. **Real-world critic noise evaluation**: Replace the trained critic with human-provided feedback on a subset of examples to measure the actual noise level in practice versus the synthetic noise experiments, and assess whether human feedback provides additional benefits.
3. **Scaling relationship analysis**: Systematically vary generator model size (small to large) and measure the iteration count required for convergence with REFINER, comparing against the theoretical expectation that smaller models need more iterations, to determine practical scaling limits.