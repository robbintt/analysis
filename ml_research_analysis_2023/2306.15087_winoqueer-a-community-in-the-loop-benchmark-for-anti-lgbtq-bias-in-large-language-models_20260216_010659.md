---
ver: rpa2
title: 'WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large
  Language Models'
arxiv_id: '2306.15087'
source_url: https://arxiv.org/abs/2306.15087
tags:
- bias
- language
- lgbtq
- community
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents WinoQueer, a new benchmark dataset for measuring
  anti-LGBTQ+ bias in large language models. The dataset is community-sourced through
  a survey of LGBTQ+ individuals, and is designed to detect stereotypes that have
  caused harm to specific subgroups within the community.
---

# WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models

## Quick Facts
- arXiv ID: 2306.15087
- Source URL: https://arxiv.org/abs/2306.15087
- Reference count: 14
- Key outcome: WinoQueer dataset reveals significant anti-LGBTQ+ bias in LLMs, partially mitigated by finetuning on community-generated data

## Executive Summary
This paper introduces WinoQueer, a novel benchmark for measuring anti-LGBTQ+ bias in large language models. The dataset is community-sourced through surveys of LGBTQ+ individuals, ensuring it captures stereotypes that have caused real-world harm. The authors apply their benchmark to multiple popular LLM architectures and find significant anti-queer bias across model types and sizes. They demonstrate that bias can be partially mitigated by finetuning on data written by or about the LGBTQ+ community, with social media text showing greater effectiveness than news text. The paper's key contributions include the WinoQueer dataset, the community-in-the-loop benchmark development method, and baseline results demonstrating the need for careful human supervision throughout the training pipeline.

## Method Summary
The WinoQueer benchmark is developed through a community-in-the-loop approach where LGBTQ+ survey participants identify harmful stereotypes. These responses are converted into sentence templates with identity descriptors, names/pronouns, and predicates to create 45,540 sentence pairs. Models are evaluated using a pseudo-log-likelihood metric comparing stereotypical versus counterfactual sentences. The study evaluates multiple LLM architectures including BERT, RoBERTa, ALBERT, BART, GPT-2, OPT, and BLOOM. To assess bias mitigation, selected models are finetuned on QueerNews (news articles) and QueerTwitter (tweets) corpora, then re-evaluated on the benchmark.

## Key Results
- All 20 evaluated models showed evidence of anti-queer bias, ranging from slight (55.93, ALBERT-xxl-v2) to gravely concerning (97.86, GPT2)
- Finetuning on LGBTQ+ community-generated data partially mitigated bias, with QueerTwitter showing greater effectiveness than QueerNews
- The community-in-the-loop approach successfully created a benchmark grounded in real-world harms experienced by LGBTQ+ individuals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Community-in-the-loop benchmark development produces bias metrics that are grounded in actual harms experienced by marginalized groups.
- Mechanism: By surveying LGBTQ+ individuals about their lived experiences of discrimination, the benchmark captures stereotypes that have caused real-world harm, rather than relying on researcher assumptions about what constitutes bias.
- Core assumption: LGBTQ+ individuals can accurately identify and articulate the stereotypes and biases that have caused them harm.
- Evidence anchors:
  - [abstract] "The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey."
  - [section 3.1] "Participants saw a general call for recruitment and were asked to self-identify if interested in participating. Participants who met the screening criteria (i.e. English-speaking adults who identify as LGBTQ+) were directed to the informed consent form."
  - [corpus] Weak - The corpus evidence shows related papers but doesn't directly validate the community-grounded approach.
- Break condition: If survey respondents are not representative of the broader LGBTQ+ community, the benchmark may miss important biases affecting underrepresented subgroups.

### Mechanism 2
- Claim: Finetuning on data written by or about a marginalized community can reduce model bias against that community.
- Mechanism: By exposing the model to more representative and positive examples of LGBTQ+ people and issues, finetuning shifts the model's learned associations away from harmful stereotypes.
- Core assumption: The finetuning data contains sufficient diverse, positive representations of LGBTQ+ people to counteract the biases learned during pretraining.
- Evidence anchors:
  - [abstract] "Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members."
  - [section 3.5] "We selected the following large pre-trained language model architectures for evaluation... We produce two fine-tuned versions of each model: one fine-tuned on QueerNews, and one fine-tuned on QueerTwitter."
  - [corpus] Weak - The corpus shows related work on debiasing but doesn't directly validate the specific finetuning approach used.
- Break condition: If the finetuning data itself contains biases or if the finetuning process is insufficient to overwrite deeply learned associations, bias reduction may be minimal or temporary.

### Mechanism 3
- Claim: The pseudo-log-likelihood metric can effectively measure bias in both masked and autoregressive language models.
- Mechanism: By masking each token in a sentence and comparing the model's likelihood of predicting stereotypical vs. counterfactual sentences, the metric quantifies the model's propensity to associate harmful stereotypes with marginalized groups.
- Core assumption: The difference in likelihood between stereotypical and counterfactual sentences is a valid proxy for bias.
- Evidence anchors:
  - [section 3.4] "Evaluation on WQ follows the methodology of Nangia et al. (2020), which introduced a novel pseudo-log-likelihood metric for bias in masked language models... We generalize their metric by introducting an alternative scoring function for autoregressive language models."
  - [section 4.1] "All 20 models show some evidence of anti-queer bias, ranging from slight (55.93, ALBERT-xxl-v2) to gravely concerning (97.86, GPT2)."
  - [corpus] Weak - The corpus doesn't provide direct evidence for the validity of this metric.
- Break condition: If the metric is too coarse to capture nuanced forms of bias or if the likelihood differences are driven by factors unrelated to social bias, the metric may not accurately reflect model fairness.

## Foundational Learning

- Concept: Understanding of bias in NLP and its real-world impacts
  - Why needed here: The paper builds on extensive prior work on bias in NLP, and a deep understanding of this literature is necessary to contextualize the contributions and limitations of the WinoQueer benchmark.
  - Quick check question: What are some key challenges in defining and measuring bias in NLP, and how do these challenges impact the development of bias benchmarks?

- Concept: Familiarity with large language model architectures and training
  - Why needed here: The paper evaluates a range of LLM architectures and sizes, and understanding their similarities and differences is important for interpreting the results and generalizability of the findings.
  - Quick check question: How do the training objectives and data sources of masked language models (e.g., BERT) and autoregressive models (e.g., GPT) differ, and how might these differences impact their propensity to encode social biases?

- Concept: Experience with participatory research methods
- Why needed here: The paper introduces a novel community-in-the-loop approach to benchmark development, and understanding the principles and best practices of participatory research is important for evaluating the validity and ethics of this approach.
  - Quick check question: What are some key considerations when designing and conducting surveys with marginalized communities, and how can researchers ensure that their methods are respectful, inclusive, and beneficial to the community?

## Architecture Onboarding

- Component map: Survey platform -> Template generation system -> Sentence pair creation -> Evaluation pipeline -> Finetuning pipeline -> Data collection systems
- Critical path:
  1. Recruit LGBTQ+ survey respondents
  2. Collect responses about harmful stereotypes
  3. Generate sentence pairs from templates and predicates
  4. Evaluate baseline models on WinoQueer benchmark
  5. Finetune models on QueerNews and QueerTwitter data
  6. Evaluate finetuned models on WinoQueer benchmark
- Design tradeoffs:
  - Using community input ensures the benchmark is grounded in real harms but may limit generalizability to other contexts or communities
  - Finetuning can reduce bias but may also introduce new biases or reduce model performance on other tasks
  - Evaluating on a broad range of models provides comprehensive results but requires significant computational resources
- Failure signatures:
  - High variance in bias scores across different LGBTQ+ subgroups may indicate that the benchmark is not capturing intersectional biases
  - Finetuned models scoring below the ideal bias score of 50 may suggest that the finetuning process is overcorrecting or introducing new biases
  - Low correlation between model size and bias score may indicate that other factors (e.g., architecture, training data) are more important determinants of bias
- First 3 experiments:
  1. Evaluate a small masked language model (e.g., BERT-base) and a small autoregressive model (e.g., GPT2-small) on WinoQueer to establish baseline bias levels
  2. Finetune BERT-base on QueerTwitter and evaluate on WinoQueer to assess the impact of community-generated data on bias reduction
  3. Compare the bias scores of BERT-base finetuned on QueerTwitter vs. QueerNews to determine which data source is more effective for debiasing

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but it identifies several limitations that point to important areas for future research. The authors note that their benchmark is limited to biases about gender and sexual orientation and does not consider intersectional biases or the disparate effects on individuals with multiple marginalized identities. They also acknowledge that their results are limited to open-source models and do not include closed-source or proprietary models. Additionally, while the paper demonstrates that fine-tuning on community-generated data can reduce bias, it does not explore the long-term effects of this approach or its impacts on other model capabilities.

## Limitations
- The benchmark may not capture intersectional biases affecting LGBTQ+ individuals with multiple marginalized identities
- Results are limited to open-source models and do not include closed-source or proprietary models
- The long-term stability and impacts of bias reduction through finetuning remain unexplored

## Confidence
High confidence: The methodology for creating the WinoQueer benchmark is clearly described and grounded in established principles of participatory research. The pseudo-log-likelihood metric for measuring bias is well-defined and has been validated in prior work. The overall trend of significant anti-queer bias across model types and sizes is robust and aligns with existing literature on social bias in NLP.

Medium confidence: The relative effectiveness of finetuning on QueerNews vs. QueerTwitter data is suggestive but based on a limited set of experiments. The specific bias scores for individual models should be interpreted with caution, as they may be sensitive to factors such as random seeds and hyperparameter choices.

Low confidence: The generalizability of the WinoQueer benchmark to other marginalized communities or cultural contexts is unknown and would require additional validation studies.

## Next Checks
1. Conduct a follow-up study with a more diverse sample of LGBTQ+ participants, including individuals from different countries, age groups, and intersectional identities, to assess the representativeness and generalizability of the WinoQueer benchmark.

2. Perform a larger-scale evaluation of finetuning on additional LGBTQ+ corpora, including user-generated content from other platforms and multilingual datasets, to determine the most effective strategies for reducing anti-queer bias across a wider range of models and contexts.

3. Investigate the relationship between model size, architecture, and anti-queer bias by training and evaluating models with varying parameter counts and training objectives, to identify the key factors driving the observed bias patterns.