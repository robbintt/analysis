---
ver: rpa2
title: Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting
arxiv_id: '2308.11946'
source_url: https://arxiv.org/abs/2308.11946
tags:
- input
- temporal
- forecasting
- length
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MTPNet addresses the challenge of multivariate time series forecasting
  by capturing temporal dependencies at multiple unconstrained scales, rather than
  at fixed or exponentially increasing scales. It introduces a dimension invariant
  embedding mechanism that preserves both spatial and temporal dimensions while projecting
  data into higher-dimensional space, and a multi-scale transformer pyramid architecture
  that models dependencies from fine to coarse resolutions through interconnected
  transformer encoder-decoder pairs.
---

# Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting

## Quick Facts
- **arXiv ID**: 2308.11946
- **Source URL**: https://arxiv.org/abs/2308.11946
- **Reference count**: 21
- **Primary result**: MTPNet achieves 45 best and 19 second-best results out of 72 cases, with average improvements of 19.53% in MSE and 16.72% in MAE compared to multi-scale CNN approaches.

## Executive Summary
MTPNet introduces a novel approach to multivariate time series forecasting that captures temporal dependencies at multiple unconstrained scales using a transformer pyramid architecture. The method employs dimension invariant embedding to preserve spatial and temporal dimensions while projecting data into higher-dimensional space, followed by a multi-scale transformer pyramid that models dependencies from fine to coarse resolutions. Experimental results on nine benchmark datasets demonstrate superior performance compared to state-of-the-art methods, particularly excelling at capturing diverse seasonal patterns across different scales.

## Method Summary
MTPNet addresses multivariate time series forecasting through a multi-scale transformer pyramid architecture. The method begins with dimension invariant embedding using a 1-layer CNN with kernel size 3×1 to preserve spatial and temporal dimensions while capturing short-term temporal dependencies. The architecture then employs multiple transformer encoder-decoder pairs at various patch sizes (ranging from 4 to 96 time steps), with inter-scale connections enabling information flow between different resolution levels. The model processes decomposed seasonal and trend components separately before combining predictions. Training uses Adam optimizer with cosine annealing scheduler, L1 loss, and batch size of 32 with grid search for hyperparameters.

## Key Results
- MTPNet achieves 45 best and 19 second-best results out of 72 cases across nine benchmark datasets
- Average improvements of 19.53% in MSE and 16.72% in MAE compared to multi-scale CNN approaches
- Average improvements of 39.84% in MSE and 30.32% in MAE compared to transformer-based multi-scale methods
- Particularly effective at capturing diverse seasonal patterns through unconstrained scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DI embedding preserves both spatial and temporal dimensions while projecting data into higher-dimensional space, enabling effective capture of short-term temporal dependencies.
- Mechanism: The 1-layer CNN with kernel size 3×1 processes the MTS data without mixing variables or collapsing time steps, maintaining the full structure for subsequent patch operations.
- Core assumption: The 3×1 convolution kernel captures relevant short-term temporal patterns while preserving dimensionality.
- Evidence anchors:
  - [abstract] "We introduce a dimension invariant embedding technique that captures short-term temporal dependencies and projects MTS data into a higher-dimensional space, while preserving the dimensions of time steps and variables"
  - [section 3.3] "the DI embedding technique which utilizes a 1-layer CNN with a kernel size of 3 × 1 to embed the MTS data into feature maps while preserving both spatial and temporal dimensions invariant"
  - [corpus] Weak evidence - no direct comparison to DI embedding in related papers
- Break condition: If the kernel size is too large or too small to capture meaningful short-term dependencies, or if the MTS data has very high dimensionality causing computational issues.

### Mechanism 2
- Claim: Multi-scale transformer pyramid architecture captures temporal dependencies across multiple unconstrained scales, improving forecasting accuracy.
- Mechanism: Multiple transformer encoder-decoder pairs at different levels process patched inputs of varying scales (pk ∈ {4, 6, 8, 12, 24, 32, 48, 96}), with inter-scale connections allowing information flow between levels.
- Core assumption: Temporal dependencies exist at multiple scales and can be effectively captured by transformers operating on appropriately-sized patches.
- Evidence anchors:
  - [abstract] "a multi-scale transformer pyramid architecture that models dependencies from fine to coarse resolutions through interconnected transformer encoder-decoder pairs"
  - [section 3.2] "The primary objective of the MTPNet is to capture temporal dependencies across diverse unconstrained scales, ranging from fine to coarse resolutions"
  - [corpus] Weak evidence - related papers focus on single-scale or exponentially-increasing scales, not unconstrained scales
- Break condition: If the chosen patch sizes don't align with actual seasonal patterns in the data, or if inter-scale connections introduce noise rather than useful information.

### Mechanism 3
- Claim: Inter-scale connections in both bottom-up (encoders) and top-down (decoders) directions enable effective information flow across scales.
- Mechanism: Encoders progressively learn representations from fine to coarse scales, while decoders generate fine-scale representations guided by coarse-scale information, with skip connections preserving original information.
- Core assumption: Information from different scales is complementary and can be effectively fused to improve predictions.
- Evidence anchors:
  - [section 3.4.1] "The inter-scale connections facilitate information flow between transformers at different levels within the pyramid architecture"
  - [section 3.4.2] "the decoder's inter-scale connections follow a top-down order, thus the decoder's output latent representations flow from coarse-scale to fine-scale"
  - [corpus] Weak evidence - ablation study shows inter-scale connections provide trivial improvement, suggesting they may not be critical
- Break condition: If the fusion mechanism (1×1 convolution) doesn't effectively combine information from different scales, or if the top-down vs bottom-up difference is negligible.

## Foundational Learning

- Concept: Time series decomposition into seasonal and trend-cyclical components
  - Why needed here: The paper uses decomposition to separate MTS data into seasonal and trend components, modeling each separately before combining predictions
  - Quick check question: Why does separating seasonal and trend components improve forecasting accuracy compared to modeling the raw series directly?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The core of MTPNet uses transformer encoder-decoder pairs to capture temporal dependencies at multiple scales
  - Quick check question: How does the scaled dot-product attention mechanism work in transformers, and why is positional encoding necessary?

- Concept: Patch-based processing of sequential data
  - Why needed here: The DI embedding applies patch operations to divide time series into segments of varying lengths, enabling multi-scale analysis
  - Quick check question: What are the advantages and disadvantages of using patches versus processing the entire sequence at once?

## Architecture Onboarding

- Component map: Input MTS → Decomposition → DI Embedding + Patch (all scales) → Multi-scale Transformer Pyramid (Encoders + Decoders) → Concatenate latent representations → 1×1 Conv → Output predictions
- Critical path: The data flows through DI embedding and patch operations at all levels, then through the transformer pyramid with inter-scale connections, finally through the prediction layer
- Design tradeoffs: Unconstrained scales vs. computational cost; inter-scale connections vs. simplicity; direct input vs. only latent representations
- Failure signatures: Poor performance on datasets with specific seasonal patterns not aligned with chosen patch sizes; instability when inter-scale connections are present; sensitivity to look-back window length
- First 3 experiments:
  1. Test MTPNet with only single scale (e.g., pk=24) to verify basic functionality
  2. Compare DI embedding vs. spatial/temporal embedding on a small dataset
  3. Test with and without inter-scale connections to validate their impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do MTPNet's unconstrained scales compare in performance to exponentially increasing scales (base 2) for MTS forecasting?
- Basis in paper: [explicit] The paper states that prior methods are confined to modeling temporal dependencies at either a fixed scale or multiple scales that exponentially increase (most with base 2), which limits their effectiveness in capturing diverse seasonalities.
- Why unresolved: While the paper demonstrates that MTPNet outperforms state-of-the-art methods, it doesn't explicitly compare its unconstrained scales to exponentially increasing scales within the same model.
- What evidence would resolve it: An ablation study comparing MTPNet's performance with and without exponentially increasing scales would provide direct evidence of the benefits of unconstrained scales.

### Open Question 2
- Question: What is the impact of the dimension invariant (DI) embedding mechanism on the performance of MTPNet compared to spatial and temporal embedding techniques?
- Basis in paper: [explicit] The paper introduces the DI embedding technique and states that it outperforms spatial and temporal embeddings in most cases.
- Why unresolved: The paper provides a comparison between DI embedding and other embedding techniques, but it doesn't explore the individual contributions of each component within the DI embedding mechanism.
- What evidence would resolve it: An ablation study isolating the effects of the convolution layer and patch procedure within the DI embedding mechanism would clarify their individual contributions to performance.

### Open Question 3
- Question: How does the choice of patch sizes affect the performance of MTPNet?
- Basis in paper: [explicit] The paper mentions that patch sizes are selected via grid search from a set of predefined values.
- Why unresolved: The paper doesn't explore the sensitivity of MTPNet's performance to different patch size configurations or the optimal number of patch sizes.
- What evidence would resolve it: A systematic study varying the number and size of patch sizes within MTPNet would reveal the impact of this hyperparameter on forecasting accuracy.

### Open Question 4
- Question: What is the role of inter-scale connections in MTPNet, and how do they contribute to its performance?
- Basis in paper: [explicit] The paper introduces inter-scale connections and mentions that their removal leads to a trivial improvement in performance.
- Why unresolved: The paper suggests that inter-scale connections may not be crucial for MTS forecasting due to the low semantic characteristics of time series data, but it doesn't provide a definitive explanation for their minimal impact.
- What evidence would resolve it: Further analysis of the information flow between scales and its impact on latent representations could clarify the role of inter-scale connections in MTPNet.

## Limitations
- Limited ablation studies on key design choices, particularly the necessity of unconstrained scales versus fixed scale ranges
- Computational costs and scalability concerns not adequately addressed for the multi-scale pyramid architecture
- Claim about "unconstrained scales" being superior to exponentially-increasing scales lacks direct comparison evidence
- Inter-scale connections showed only trivial improvement in ablation, suggesting potential redundancy

## Confidence

- **High confidence**: The dimension invariant embedding preserves spatial and temporal dimensions as claimed (direct evidence from methodology section)
- **Medium confidence**: Multi-scale architecture improves performance over single-scale baselines (supported by experimental results but limited ablation)
- **Low confidence**: Unconstrained scales are fundamentally better than exponentially-increasing scales (no direct comparison provided)

## Next Checks

1. Conduct ablation comparing unconstrained scales vs. exponentially-increasing scales using identical experimental setup
2. Measure computational complexity and training/inference time across different dataset sizes
3. Test MTPNet on datasets with known non-standard seasonal patterns to verify patch size selection robustness