---
ver: rpa2
title: Multimodal Multi-Hop Question Answering Through a Conversation Between Tools
  and Efficiently Finetuned Large Language Models
arxiv_id: '2309.08922'
source_url: https://arxiv.org/abs/2309.08922
tags:
- arxiv
- question
- tool
- tools
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a tool-interacting divide-and-conquer strategy
  to enable large language models (LLMs) to answer complex multimodal multi-hop questions.
  The core idea is to have the LLM break down the input question into unimodal single-hop
  sub-questions, which are then answered by appropriate specialized tools.
---

# Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models

## Quick Facts
- arXiv ID: 2309.08922
- Source URL: https://arxiv.org/abs/2309.08922
- Authors: 
- Reference count: 32
- Primary result: The proposed tool-interacting divide-and-conquer strategy achieves substantial improvements over existing state-of-the-art solutions on two recently introduced complex question-answering datasets.

## Executive Summary
This paper introduces a novel approach for multimodal multi-hop question answering (MMH QA) that leverages the power of large language models (LLMs) to decompose complex questions into simpler sub-questions, which are then answered by specialized tools. The core idea is to have the LLM interact with a set of tools (text, table, image, and web search) in a divide-and-conquer fashion, breaking down the original MMH question into unimodal single-hop sub-questions. To enhance the reasoning capability of LLMs, the authors prompt ChatGPT to generate a tool-interacting divide-and-conquer dataset, which is then used to efficiently finetune smaller LLMs using QLoRA. Experiments on two recent MMH QA benchmarks demonstrate that this strategy significantly outperforms existing state-of-the-art methods.

## Method Summary
The proposed approach involves a tool-interacting divide-and-conquer strategy where an LLM breaks down a multimodal multi-hop question into unimodal single-hop sub-questions. Each sub-question is answered by a specialized tool (text, table, image, or web search) based on the required modality. The LLM iteratively generates new sub-questions based on the tools' answers until the final answer is found. To enhance the reasoning capabilities of smaller LLMs, ChatGPT is prompted to generate a tool-interacting divide-and-conquer dataset from the training splits of two MMH QA benchmarks. This dataset is then used to efficiently finetune smaller LLMs (7B, 13B, 30B, and 40B) using QLoRA for one epoch. The finetuned LLMs are evaluated on the test splits of both benchmarks, and their performance is compared against baseline methods.

## Key Results
- The tool-interacting divide-and-conquer strategy significantly outperforms existing state-of-the-art methods on two recent MMH QA benchmarks (MultiModalQA and MMCoQA).
- Finetuning smaller LLMs on the generated tool-interacting divide-and-conquer dataset using QLoRA leads to substantial improvements in their performance on MMH QA tasks.
- The proposed approach demonstrates the effectiveness of leveraging specialized tools and efficient finetuning techniques to enhance the reasoning capabilities of LLMs for complex multimodal reasoning tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can decompose multimodal multi-hop questions into unimodal single-hop sub-questions through a divide-and-conquer strategy.
- Mechanism: The LLM receives the input MMH question and generates a USH sub-question by specifying the tool name required to answer that sub-question. This process continues iteratively until the final answer is found.
- Core assumption: LLMs possess sufficient reasoning capability to identify the appropriate tool and formulate relevant USH sub-questions based on previous answers.
- Evidence anchors:
  - [abstract]: "harness the power of large language models to divide a given multimodal multi-hop question into unimodal single-hop sub-questions to be answered by the appropriate tool from a predefined set of tools"
  - [section 2.1]: "the LLM first divides the MMH original question into a USH sub-question by asking for the movie's title from the relevant tools"
  - [corpus]: Weak evidence from corpus neighbors - related papers discuss multi-hop reasoning but don't specifically address the LLM-as-divider mechanism
- Break condition: The LLM fails to generate coherent USH sub-questions, or the generated sub-questions are irrelevant to the original question.

### Mechanism 2
- Claim: Finetuning LLMs on a tool-interacting divide-and-conquer dataset enhances their reasoning capabilities for MMH QA tasks.
- Mechanism: ChatGPT is prompted to generate a tool-interacting divide-and-conquer dataset, which is then used to efficiently finetune the corresponding LLM. This process encourages the LLM to follow the divide-and-conquer strategy while interacting with the required tools.
- Core assumption: The generated dataset effectively captures the patterns of tool interaction and reasoning required for MMH QA tasks.
- Evidence anchors:
  - [abstract]: "To increase the reasoning ability of LLMs, we prompt chatGPT to generate a tool-interacting divide-and-conquer dataset. This dataset is then used to efficiently finetune the corresponding LLM"
  - [section 3]: "To enhance the reasoning and tool-interacting capabilities of typical-sized LLMs, such as 7, 13, 30, and 40 billions, we efficiently finetune LLMs of different sizes for one epoch using QLoRA on a tool-interacting divide-and-conquer dataset"
  - [corpus]: Weak evidence from corpus neighbors - related papers discuss few-shot learning and data synthesis but don't specifically address the finetuning mechanism
- Break condition: The finetuned LLM does not show improved performance on MMH QA tasks compared to the baseline.

### Mechanism 3
- Claim: The interaction between LLMs and specialized tools enables accurate answering of MMH questions by leveraging the strengths of both components.
- Mechanism: Each tool is specialized in handling a specific data modality (text, table, or image). The LLM generates USH sub-questions, and the corresponding tool provides answers based on its associated data modality. This interaction continues until the final answer is found.
- Core assumption: The tools are sufficiently accurate in answering USH questions, and the LLM can effectively integrate the answers from multiple tools to form the final answer.
- Evidence anchors:
  - [abstract]: "we harness the power of large language models to divide a given multimodal multi-hop question into unimodal single-hop sub-questions to be answered by the appropriate tool from a predefined set of tools"
  - [section 2.2]: "each time that the LLM asks a sub-question, the corresponding tool is invoked to find the requested answer. As the sub-question is a USH question, it is highly likely that the tool successfully obtains the answer"
  - [corpus]: Weak evidence from corpus neighbors - related papers discuss multimodal QA but don't specifically address the tool-LLM interaction mechanism
- Break condition: The tools provide inaccurate answers to USH questions, or the LLM fails to integrate the answers from multiple tools effectively.

## Foundational Learning

- Concept: Chain of thought prompting
  - Why needed here: Enables LLMs to generate intermediate reasoning steps, which is crucial for decomposing MMH questions into USH sub-questions
  - Quick check question: How does chain of thought prompting differ from standard prompting, and why is it beneficial for complex reasoning tasks?

- Concept: Tool use and function calling in LLMs
  - Why needed here: Allows LLMs to interact with external tools and leverage their specialized capabilities for answering USH sub-questions
  - Quick check question: What are the key considerations when designing a system for tool use in LLMs, and how does it impact the overall performance?

- Concept: Few-shot learning and data synthesis
  - Why needed here: Enables the generation of a tool-interacting divide-and-conquer dataset using ChatGPT, which is then used to finetune smaller LLMs for improved performance
  - Quick check question: How does few-shot learning differ from traditional supervised learning, and what are the advantages of using few-shot learning for data synthesis?

## Architecture Onboarding

- Component map:
  LLM -> Tool Handler -> Tools (TextQA, TableQA, ImageQA, Web Search) -> LLM

- Critical path:
  1. LLM receives MMH question and generates USH sub-question
  2. Tool Handler processes LLM output and invokes the appropriate tool
  3. Tool provides answer to the USH sub-question
  4. LLM integrates the answer and generates the next USH sub-question
  5. Repeat steps 2-4 until final answer is found

- Design tradeoffs:
  - Tool specialization vs. generalization: Specialized tools may provide more accurate answers but require more components
  - LLM size vs. performance: Larger LLMs may have better reasoning capabilities but are more computationally expensive
  - Dataset size vs. finetuning effectiveness: Larger datasets may lead to better finetuning but require more resources

- Failure signatures:
  - LLM generates irrelevant or incoherent USH sub-questions
  - Tools provide inaccurate answers to USH questions
  - LLM fails to integrate answers from multiple tools effectively
  - Finetuning does not lead to improved performance on MMH QA tasks

- First 3 experiments:
  1. Evaluate the performance of different-sized LLMs on MMH QA tasks without finetuning to establish a baseline
  2. Generate a tool-interacting divide-and-conquer dataset using ChatGPT and assess its quality and diversity
  3. Finetune smaller LLMs using the generated dataset and evaluate their performance on MMH QA tasks compared to the baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the tool-interacting divide-and-conquer strategy scale with increasing complexity of multimodal multi-hop questions?
- Basis in paper: [explicit] The paper evaluates the strategy on two recent MMH QA benchmarks but does not explore the impact of question complexity on performance.
- Why unresolved: The paper does not provide an analysis of how the strategy's performance changes as the complexity of the questions increases, such as the number of hops or the diversity of modalities involved.
- What evidence would resolve it: Experiments evaluating the strategy's performance on a range of question complexities, including varying the number of hops and modalities, would provide insights into its scalability.

### Open Question 2
- Question: How does the proposed strategy compare to other state-of-the-art methods that use different reasoning techniques, such as chain-of-thought prompting or least-to-most prompting?
- Basis in paper: [explicit] The paper compares the proposed strategy to mm-ReAct and a baseline method but does not include comparisons to other reasoning techniques.
- Why unresolved: The paper does not explore the relative strengths and weaknesses of the proposed strategy compared to other advanced reasoning techniques that have been proposed for LLMs.
- What evidence would resolve it: Experiments comparing the proposed strategy to other state-of-the-art methods that use different reasoning techniques would provide insights into its relative performance and potential advantages.

### Open Question 3
- Question: How does the performance of the tool-interacting divide-and-conquer strategy vary across different types of tools and modalities?
- Basis in paper: [explicit] The paper uses a specific set of tools (TextQA, TableQA, ImageQA, and Web Search) but does not explore the impact of using different tools or modalities.
- Why unresolved: The paper does not investigate how the performance of the strategy might change if different tools or modalities were used, or if the tools were fine-tuned for the specific task.
- What evidence would resolve it: Experiments evaluating the strategy's performance using different combinations of tools and modalities, or using fine-tuned tools, would provide insights into its robustness and adaptability.

## Limitations

- The paper does not provide details on the specific few-shot examples used to prompt ChatGPT for generating the training dataset, making it difficult to assess the quality and diversity of the generated dataset.
- The specific implementation details of the tool handler that processes LLM outputs and invokes the appropriate tools are not described, introducing ambiguity in the reproducibility of the results.
- The paper does not provide a comprehensive analysis of the qualitative aspects of the generated answers, such as coherence, relevance, and completeness, which would provide a more holistic evaluation of the system's performance.

## Confidence

- High Confidence: The overall concept of using LLMs to decompose multimodal multi-hop questions into unimodal single-hop sub-questions and leveraging specialized tools for answering is well-established and supported by the experimental results. The improvements over baseline methods are statistically significant and consistent across different LLM sizes and datasets.
- Medium Confidence: The effectiveness of the tool-interacting divide-and-conquer strategy and the quality of the synthetic dataset generated by ChatGPT are crucial for the success of the proposed approach. While the experimental results are promising, the lack of transparency in the dataset generation process and the tool handler implementation introduces some uncertainty.
- Low Confidence: The paper does not provide a detailed analysis of the failure modes and edge cases of the proposed system. Understanding the limitations and potential pitfalls of the approach would help in assessing its real-world applicability and robustness.

## Next Checks

1. Conduct a thorough analysis of the quality and diversity of the synthetic dataset generated by ChatGPT by manually inspecting a sample of the generated examples and comparing them against the original training data. Additionally, evaluate the impact of different few-shot prompts on the quality of the generated dataset.

2. Implement and evaluate different versions of the tool handler to understand its impact on the overall system performance. Compare the performance of the system with and without the tool handler, and analyze the effect of different tool invocation strategies on the accuracy and efficiency of the approach.

3. Perform a qualitative analysis of the answers generated by the proposed system by manually inspecting a sample of the answers and assessing their coherence, relevance, and completeness. Compare the generated answers against the ground truth answers and identify any patterns or trends in the errors made by the system.