---
ver: rpa2
title: 'PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration
  in Domain Adaptation'
arxiv_id: '2307.07489'
source_url: https://arxiv.org/abs/2307.07489
tags:
- calibration
- pseudocal
- target
- domain
- tempscal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of uncertainty calibration in
  unsupervised domain adaptation (UDA) models, which is critical for safe deployment
  but has been underexplored. The authors propose PseudoCal, a source-free calibration
  method that generates a labeled pseudo-target set using mixup with real target samples
  and pseudo-labels.
---

# PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation

## Quick Facts
- arXiv ID: 2307.07489
- Source URL: https://arxiv.org/abs/2307.07489
- Reference count: 40
- Key outcome: Source-free calibration method that generates pseudo-target set via mixup, achieving significantly reduced calibration error across 10 UDA methods and 5 UDA scenarios

## Executive Summary
This paper addresses the underexplored problem of uncertainty calibration in unsupervised domain adaptation (UDA) models, which is critical for safe deployment. The authors propose PseudoCal, a source-free calibration method that transforms unsupervised calibration into a supervised problem by generating a labeled pseudo-target set using mixup with real target samples and pseudo-labels. Extensive experiments demonstrate that PseudoCal consistently achieves significantly reduced calibration error compared to existing methods across multiple UDA scenarios including image classification and semantic segmentation tasks.

## Method Summary
PseudoCal is a source-free calibration method that exclusively relies on unlabeled target data. It generates a labeled pseudo-target set by applying mixup between target samples with different pseudo-labels, then applies temperature scaling on this pseudo-target set to obtain a pseudo temperature. This pseudo temperature is used to calibrate predictions on the real target set. The method transforms the unsupervised calibration problem into a supervised one, enabling effective calibration using standard in-domain methods like temperature scaling.

## Key Results
- PseudoCal consistently achieves significantly reduced calibration error (ECE) compared to existing methods across 10 UDA methods
- The method demonstrates superior performance and robustness across different settings including image classification and semantic segmentation
- PseudoCal maintains accuracy while improving calibration, with negligible accuracy degradation (<0.1%) in most cases
- The approach works effectively for both domain shift and label shift scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pseudo-target set generated via mixup captures the same accuracy-confidence distribution as the real target set.
- Mechanism: Mixup creates samples that naturally include both correctly and incorrectly classified cases, aligning with the cluster assumption that samples far from the decision boundary are more likely to be correctly classified.
- Core assumption: The cluster assumption holds - decision boundaries lie in low-density regions, and correctly predicted samples are generally farther from boundaries than incorrectly predicted ones.
- Break condition: The cluster assumption fails, such as in cases where the model's decision boundary is highly irregular or when pseudo-labels are extremely noisy (near-random accuracy).

### Mechanism 2
- Claim: Converting unsupervised calibration to supervised calibration using pseudo-labels enables effective temperature scaling.
- Mechanism: By generating a labeled pseudo-target set through mixup, the unsupervised calibration problem (lacking target labels) is transformed into a supervised one where standard temperature scaling can be applied.
- Core assumption: The pseudo-target temperature obtained via supervised TempScal on the pseudo-target set approximates the oracle target temperature needed for real target calibration.
- Break condition: The pseudo-label quality is extremely poor (near-random accuracy), breaking the correspondence between pseudo-target and real target distributions.

### Mechanism 3
- Claim: Source-free calibration without access to source data is possible by focusing only on target domain characteristics.
- Mechanism: Unlike previous approaches that treat UDA calibration as a covariate shift problem requiring source data, PseudoCal treats it as an unsupervised calibration problem specific to the target domain, synthesizing the needed supervision internally.
- Core assumption: The target domain's accuracy-confidence distribution can be modeled without source data, making source-free calibration feasible.
- Break condition: Severe domain shifts where the target domain characteristics cannot be modeled from unlabeled target data alone.

## Foundational Learning

- Concept: Temperature Scaling (TempScal)
  - Why needed here: It's the core calibration method used on the pseudo-target set to estimate the target temperature
  - Quick check question: How does temperature scaling modify the softmax output probabilities?

- Concept: Mixup data augmentation
  - Why needed here: It's used to generate the pseudo-target set with controlled perturbations that maintain the accuracy-confidence distribution
  - Quick check question: What is the mathematical formulation of mixup for generating synthetic samples?

- Concept: Expected Calibration Error (ECE)
  - Why needed here: It's the primary metric used to evaluate calibration performance across experiments
  - Quick check question: How is ECE computed from accuracy and confidence values across bins?

## Architecture Onboarding

- Component map: Unlabeled target data -> Pseudo-label inference -> Mixup module -> Pseudo-target set -> Temperature scaling optimization -> Calibrated model
- Critical path: 1) Inference with UDA model to get predictions on target data 2) Mixup to create pseudo-target set 3) Temperature scaling optimization on pseudo-target set 4) Apply learned temperature to calibrate real target predictions
- Design tradeoffs:
  - Fixed mix ratio (λ=0.65) vs. learned ratio: Fixed ratio ensures simplicity and reproducibility
  - Mixup with hard vs. soft labels: Hard labels are simpler and perform similarly in experiments
  - Single epoch inference vs. multiple passes: Single pass reduces computational overhead
- Failure signatures:
  - Calibration performance degrades when initial model accuracy is very low (<30%)
  - Results worsen when domain shift is extremely severe
  - Performance may slightly degrade when starting from already well-calibrated models
- First 3 experiments:
  1. Reproduce Table 2 results for Office-Home with ATDOC to verify basic functionality
  2. Test sensitivity to mix ratio λ by varying from 0.5 to 0.9 on one task
  3. Compare PseudoCal vs. no calibration on a source-free task (SHOT or DINE) from Table 6

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PseudoCal's performance vary with different mix ratios (λ) beyond the range of 0.51 to 0.9 explored in the paper?
- Basis in paper: [explicit] The authors found that PseudoCal achieves optimal performance within a medium range of λ values (0.6 to 0.7), but did not extensively explore beyond this range.
- Why unresolved: The paper only tested λ values between 0.51 and 0.9. Exploring a wider range could reveal if there's an even better λ value or if performance degrades significantly outside this range.
- What evidence would resolve it: Additional experiments testing λ values outside the range of 0.51 to 0.9, ideally across multiple datasets and UDA methods.

### Open Question 2
- Question: How does PseudoCal's performance compare to other state-of-the-art calibration methods not mentioned in the paper, such as Platt scaling with class-conditional temperature scaling?
- Basis in paper: [inferred] The paper compares PseudoCal to several calibration methods, but does not mention class-conditional temperature scaling, which could potentially offer improved calibration in UDA settings.
- Why unresolved: The paper focuses on comparing PseudoCal to a specific set of calibration methods, but does not exhaustively explore all possible alternatives.
- What evidence would resolve it: Experiments comparing PseudoCal to other state-of-the-art calibration methods, including class-conditional temperature scaling, across multiple datasets and UDA methods.

### Open Question 3
- Question: How does PseudoCal's performance scale with increasing dataset size and number of classes in the target domain?
- Basis in paper: [explicit] The paper evaluates PseudoCal on datasets of varying scales (Office-31, Office-Home, VisDA, DomainNet, Image-Sketch), but does not explicitly analyze how performance scales with dataset size and number of classes.
- Why unresolved: While the paper demonstrates PseudoCal's effectiveness across different datasets, it does not provide a detailed analysis of how performance changes with increasing dataset size and number of classes.
- What evidence would resolve it: Experiments systematically varying dataset size and number of classes in the target domain, while measuring PseudoCal's calibration error and other relevant metrics.

## Limitations
- PseudoCal fails when initial model accuracy drops below 30%, representing a performance floor that could be problematic in real-world deployment scenarios
- The method relies on the cluster assumption, which may not hold for highly irregular decision boundaries or extremely noisy pseudo-labels
- Some experiments show that PseudoCal can increase ECE in cases where initial calibration is already good, risking safety-critical systems

## Confidence

- **High confidence**: The source-free nature of PseudoCal and its consistent improvement over baseline methods across multiple datasets and UDA methods (Tables 2, 3, 6)
- **Medium confidence**: The claim that mixup-generated pseudo-targets capture real target distribution characteristics, as this relies on the cluster assumption without extensive validation of when this assumption breaks
- **Medium confidence**: The generalizability across all 5 UDA scenarios, particularly for semantic segmentation and label shift scenarios, where fewer experimental results are provided

## Next Checks

1. Test PseudoCal's performance when starting from models with varying initial accuracies (10%, 30%, 50%, 70%) to characterize the accuracy threshold below which calibration fails

2. Evaluate safety-critical failure modes by measuring not just average ECE but also maximum calibration error across confidence bins, particularly for high-confidence predictions that could lead to overconfident wrong decisions

3. Validate the cluster assumption empirically by visualizing decision boundaries and sample distributions for cases where PseudoCal succeeds versus fails, to identify clear failure patterns and domain shift severity thresholds