---
ver: rpa2
title: Learning Compact Compositional Embeddings via Regularized Pruning for Recommendation
arxiv_id: '2309.03518'
source_url: https://arxiv.org/abs/2309.03518
tags:
- embedding
- pruning
- embeddings
- size
- cerp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of memory efficiency in recommender
  systems, where large embedding tables dominate storage costs. Existing approaches
  either reduce embedding dimensions or use hashing, but both compromise accuracy
  under tight memory budgets.
---

# Learning Compact Compositional Embeddings via Regularized Pruning for Recommendation

## Quick Facts
- **arXiv ID**: 2309.03518
- **Source URL**: https://arxiv.org/abs/2309.03518
- **Reference count**: 40
- **Primary result**: CERP achieves NDCG@10 improvements of 8.3-16.2% over best baselines at 90% sparsity on Gowalla and Yelp2020 datasets

## Executive Summary
The paper addresses the challenge of memory efficiency in recommender systems, where large embedding tables dominate storage costs. Existing approaches either reduce embedding dimensions or use hashing, but both compromise accuracy under tight memory budgets. The authors propose CERP, a compositional embedding framework that combines two smaller, pruned meta-embedding tables using a novel regularized pruning mechanism. This ensures mutual complementarity between the pruned tables, preserving embedding expressiveness. CERP is evaluated with MLP and LightGCN recommenders on Gowalla and Yelp2020 datasets under various sparsity rates.

## Method Summary
CERP represents each entity using a pair of meta-embeddings from two codebooks. The framework uses a deterministic hashing scheme to map entities to meta-embedding indices, ensuring balanced reuse. Pruning is applied to both codebooks jointly using element-wise soft thresholds, with a regularization loss encouraging complementary sparsity patterns. The final entity embedding is the sum of the two pruned meta-embeddings. The model is trained end-to-end with both recommendation loss (BPR) and pruning regularization. Exponential decay on the regularization weight allows early focus on embedding quality, then later accelerates sparsity to meet memory budget.

## Key Results
- At 90% sparsity, CERP achieves NDCG@10 improvements of 8.3% (MLP) and 10.3% (LightGCN) on Gowalla compared to best baseline
- At 90% sparsity, CERP achieves NDCG@10 improvements of 12.7% (MLP) and 16.2% (LightGCN) on Yelp2020 compared to best baseline
- CERP consistently outperforms competitive baselines across different sparsity rates (90%, 95%, 99%) and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint pruning of two meta-embedding tables ensures compositional embeddings remain dense even under tight memory budgets.
- Mechanism: CERP represents each entity using a pair of meta-embeddings from two codebooks. By pruning both codebooks jointly using element-wise soft thresholds, the method ensures that when one dimension is deactivated in one meta-embedding, the corresponding dimension in the other meta-embedding is likely active. This complementary pattern keeps the sum of the two vectors dense.
- Core assumption: The pruning regularizer encourages complementary sparsity patterns between the two codebooks, so that deactivated dimensions do not overlap.
- Evidence anchors:
  - [abstract]: "we innovatively design a regularized pruning mechanism in CERP, such that the two sparsified meta-embedding tables are encouraged to encode information that is mutually complementary."
  - [section]: "To alleviate this, we further design a pruning regularizer to facilitate complementary pruning so that the average size of entities’ final embedding vectors is not compromised amid robust pruning."
  - [corpus]: Weak evidence. The corpus mentions "Coarse-to-Fine Lightweight Meta-Embedding for ID-Based Recommendation" and "Personalized Elastic Embedding Learning for On-Device Recommendation," but neither directly addresses joint complementary pruning.
- Break condition: If the pruning regularizer fails to enforce complementary sparsity, overlapping zero dimensions will lead to sparse compositional embeddings and degraded accuracy.

### Mechanism 2
- Claim: Hashing each entity to two indexes ensures balanced reuse of meta-embeddings, preventing any single meta-embedding from dominating the compositional representation.
- Mechanism: Each entity index is mapped to two distinct meta-embedding indices via a deterministic formula (`k mod b` and `k div (|U|+|I|)/b`). This guarantees that each meta-embedding is reused exactly `⌈(|U|+|I|)/b⌉` times, spreading information evenly across all compositional embeddings.
- Core assumption: The hashing scheme ensures uniform distribution of meta-embedding usage across all entities.
- Evidence anchors:
  - [section]: "Intuitively, this can be accomplished by applying some hash functions to map the original entity index k to (kp, kq). Intuitively, this can be accomplished by applying some hash functions to map the original entity index k to (kp, kq)."
  - [corpus]: Weak evidence. The corpus contains "A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems," but it does not specifically validate the hashing balance mechanism.
- Break condition: If the hash mapping is not uniform (e.g., due to non-uniform entity indexing), some meta-embeddings will be overused, causing loss of uniqueness in compositional embeddings.

### Mechanism 3
- Claim: Exponential decay on the pruning regularization weight allows early focus on embedding quality, then later accelerates sparsity to meet memory budget.
- Mechanism: The regularization weight γ starts high to preserve embedding fidelity, then is halved after each pruning epoch. This gradual reduction shifts the optimization from quality preservation to aggressive pruning.
- Core assumption: Early preservation of embedding quality is more critical than rapid pruning; later, meeting the memory budget becomes the priority.
- Evidence anchors:
  - [section]: "Rationale of Exponential Decay on γ. We first set γ to an initial value, then by the end of each pruning epoch, γ is reduced by half."
  - [corpus]: No direct evidence. The corpus does not discuss learning rate scheduling or decay strategies for pruning regularization.
- Break condition: If decay is too aggressive, the model may prune important dimensions too early; if too slow, it may fail to meet the target sparsity.

## Foundational Learning

- Concept: L1 regularization for inducing sparsity
  - Why needed here: L0-norm is non-convex and intractable; L1 relaxation provides a differentiable approximation to enable end-to-end training.
  - Quick check question: What is the main drawback of directly optimizing for L0 sparsity, and how does L1 regularization help?

- Concept: Compositional embeddings via sum pooling
  - Why needed here: Allows a small number of meta-embeddings to represent a large number of entities without explicit mapping, reducing storage.
  - Quick check question: Why is sum pooling preferred over element-wise product or concatenation in this context?

- Concept: Hash-based balanced assignment
  - Why needed here: Ensures each meta-embedding is reused a bounded number of times, maintaining diversity in compositional embeddings.
  - Quick check question: How does the formula `kp = k mod b` and `kq = k div (|U|+|I|)/b` guarantee balanced reuse?

## Architecture Onboarding

- Component map:
  - Two small meta-embedding tables (codebooks) P and Q
  - Hash mapping layer to convert entity IDs to meta-embedding indices
  - Pruning layer with learnable soft thresholds Sp and Sq
  - Regularization loss Lprune to enforce complementary sparsity
  - Base recommender (MLP or LightGCN) consuming compositional embeddings
  - Joint training loop with recommendation loss and pruning loss

- Critical path:
  1. Entity ID → hash indices (kp, kq)
  2. Retrieve meta-embeddings p = P[kp], q = Q[kq]
  3. Apply pruning: bP = prune(P, Sp), bQ = prune(Q, Sq)
  4. Compose entity embedding: e = bP + bQ
  5. Feed into base recommender to compute score
  6. Compute BPR loss + pruning regularization
  7. Backpropagate to update P, Q, Sp, Sq, and base model parameters

- Design tradeoffs:
  - Larger bucket size → fewer reused meta-embeddings but denser vectors → higher memory usage
  - Smaller bucket size → more memory savings but risk of hash collisions and less unique meta-embeddings
  - Higher γ → slower pruning but better embedding quality; lower γ → faster pruning but risk of losing expressiveness
  - Sum pooling vs. concatenation: sum keeps dimension fixed but risks zero entries; concatenation increases dimension but preserves sparsity patterns

- Failure signatures:
  - High overlap rate in non-zero dimensions → poor pruning regularizer
  - Training stalls before reaching target sparsity → γ decay too slow or regularization too weak
  - Performance drop vs. baselines → insufficient bucket size or aggressive pruning
  - Memory usage exceeds target → bucket size too large or pruning not effective

- First 3 experiments:
  1. Train CERP with MLP backbone on Gowalla, target 90% sparsity, monitor average embedding dimension and NDCG@10 vs. baseline.
  2. Disable pruning regularizer (Lprune), retrain, compare overlap rate and performance to confirm regularizer impact.
  3. Vary bucket size (4k, 6k, 8k), retrain with same sparsity target, record performance to find sweet spot.

## Open Questions the Paper Calls Out
- **Question 1**: How does CERP's performance scale when applied to datasets with significantly more entities (e.g., billions of users/items) compared to the datasets used in the experiments?
- **Question 2**: What is the impact of different sparsity rates on CERP's performance for different types of recommender systems (e.g., sequential, graph-based, or knowledge-graph enhanced)?
- **Question 3**: How does CERP's performance compare to other state-of-the-art lightweight embedding methods when applied to datasets with different levels of sparsity in user-item interactions?

## Limitations
- The mechanism of joint pruning ensuring dense compositional embeddings is plausible but not directly validated through ablation studies isolating the pruning regularizer's effect on overlap rates.
- The hashing mechanism for balanced reuse is theoretically sound but not empirically verified for uniformity across the entity space.
- The exponential decay strategy's optimal schedule is not explored, and its impact on the quality-pruning tradeoff is not quantified.

## Confidence
- **High confidence**: The overall performance improvement claims (NDCG@10 gains of 8.3-16.2% over baselines at 90% sparsity) are well-supported by the experimental results on two datasets with two backbone recommenders.
- **Medium confidence**: The mechanism of joint pruning ensuring dense compositional embeddings is plausible but not directly validated through ablation studies isolating the pruning regularizer's effect on overlap rates.
- **Medium confidence**: The hashing mechanism for balanced reuse is theoretically sound but not empirically verified for uniformity across the entity space.
- **Low confidence**: The exponential decay strategy's optimal schedule is not explored, and its impact on the quality-pruning tradeoff is not quantified.

## Next Checks
1. Measure overlap rates: Implement a metric to quantify the average overlap rate between non-zero dimensions in the two pruned meta-embeddings for each entity. Compare this rate with and without the pruning regularizer to directly validate its role in ensuring complementary sparsity.

2. Analyze hash balance: Compute the distribution of meta-embedding usage across all entities. Verify that each meta-embedding is reused approximately equally often, confirming the hash mapping's effectiveness in preventing any single meta-embedding from dominating.

3. Vary decay schedule: Experiment with different exponential decay schedules for γ (e.g., halve every 2 epochs vs. every epoch) and measure their impact on the quality-pruning tradeoff. Identify the optimal schedule that balances early embedding preservation with late sparsity achievement.