---
ver: rpa2
title: Examining the Effect of Implementation Factors on Deep Learning Reproducibility
arxiv_id: '2312.06633'
source_url: https://arxiv.org/abs/2312.06633
tags:
- software
- different
- hardware
- factors
- environments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined how hardware and software environment variations
  impact deep learning reproducibility by running three deterministic deep learning
  experiments (MNIST classification, sentiment analysis, and credit card fraud detection)
  five times each across 13 different hardware environments (4 Intel CPUs, 6 AMD CPUs,
  3 NVIDIA GPUs) and four software environments (TensorFlow containers). The analysis
  of 780 combined runs revealed that hardware and software environment variations
  alone introduced greater than 6% accuracy range in the bidirectional LSTM and binary
  classification examples, while the simple MNIST example showed no significant accuracy
  differences.
---

# Examining the Effect of Implementation Factors on Deep Learning Reproducibility

## Quick Facts
- arXiv ID: 2312.06633
- Source URL: https://arxiv.org/abs/2312.06633
- Reference count: 7
- Primary result: Hardware and software environment variations alone introduced greater than 6% accuracy range in bidirectional LSTM and binary classification experiments

## Executive Summary
This study examined how hardware and software environment variations impact deep learning reproducibility by running three deterministic deep learning experiments (MNIST classification, sentiment analysis, and credit card fraud detection) five times each across 13 different hardware environments (4 Intel CPUs, 6 AMD CPUs, 3 NVIDIA GPUs) and four software environments (TensorFlow containers). The analysis of 780 combined runs revealed that hardware and software environment variations alone introduced greater than 6% accuracy range in the bidirectional LSTM and binary classification examples, while the simple MNIST example showed no significant accuracy differences. A software bug in cuDNN within TensorFlow containers caused non-deterministic outputs when running on GPUs, which was resolved by rebuilding the container with updated CUDA and cuDNN versions.

## Method Summary
The researchers conducted a comprehensive reproducibility study by executing three Keras examples (MNIST classification, bidirectional LSTM for sentiment analysis, and binary classification for credit card fraud detection) five times each across 13 hardware environments and four software environments. They used the Open Science Grid's heterogeneous computing resources to test across 4 Intel CPUs, 6 AMD CPUs, and 3 NVIDIA GPUs, running each experiment in Docker/Singularity containers with different TensorFlow versions. The study systematically compared accuracy ranges to determine the impact of hardware and software variations on reproducibility, with particular attention to whether accuracy differences exceeded 6%.

## Key Results
- Hardware and software environment variations alone introduced greater than 6% accuracy range in bidirectional LSTM and binary classification experiments
- Simple MNIST example showed no significant accuracy differences across environments (all runs >99% accuracy)
- A cuDNN software bug in TensorFlow containers caused non-deterministic GPU outputs, resolved by updating CUDA and cuDNN versions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hardware and software environment variations introduce significant accuracy variance in deep learning models
- Mechanism: Different hardware architectures (CPU vs GPU) and software environments (TensorFlow container versions) produce non-deterministic floating-point calculations and potentially expose software bugs in ML libraries
- Core assumption: The same deterministic deep learning experiment run multiple times should produce identical results
- Evidence anchors:
  - [abstract] "hardware and software environment variations alone introduced greater than 6% accuracy range in the bidirectional LSTM and binary classification examples"
  - [section] "The accuracy range of the bidirectional LSTM example exceeded 8% using the tensorflow/tensorflow containers on GPU hardware; the cause was a software bug"
  - [corpus] No direct corpus evidence - weak anchor
- Break condition: When experiments are run in identical hardware and software environments with proper seeding and deterministic settings

### Mechanism 2
- Claim: Software bugs in ML libraries can cause non-deterministic outputs across different hardware configurations
- Mechanism: A specific cuDNN bug in TensorFlow containers caused non-deterministic outputs when running on GPUs, which was resolved by rebuilding with updated CUDA and cuDNN versions
- Core assumption: Container versions should be free of known software bugs that affect reproducibility
- Evidence anchors:
  - [abstract] "A software bug in cuDNN within TensorFlow containers caused non-deterministic outputs when running on GPUs"
  - [section] "The bug in the version of cuDNN included in the tensorflow/tensorflow containers caused runs using a Bidirectional RNN network to return non-deterministic outputs"
  - [corpus] No direct corpus evidence - weak anchor
- Break condition: When containers are rebuilt with patched library versions or when the specific bug is no longer present in the environment

### Mechanism 3
- Claim: Different ML model architectures show different sensitivity to hardware/software variations
- Mechanism: Simple models (MNIST) showed no significant accuracy differences across environments, while more complex models (bidirectional LSTM, binary classification) showed >6% accuracy range, indicating model complexity affects reproducibility sensitivity
- Core assumption: Model complexity and architecture type influence sensitivity to implementation factors
- Evidence anchors:
  - [abstract] "the simple MNIST example showed no significant accuracy differences" and "greater than 6% accuracy range on the bidirectional LSTM and binary classification examples"
  - [section] "The simple MNIST example didn't show significant accuracy differences as all the runs returned a greater than 99% accuracy"
  - [corpus] No direct corpus evidence - weak anchor
- Break condition: When all model architectures show similar variance patterns across environments, suggesting the relationship is not consistently model-dependent

## Foundational Learning

- Concept: Reproducibility vs Repeatability
  - Why needed here: The paper distinguishes between running experiments in the same vs different environments, which is fundamental to understanding their findings
  - Quick check question: If you run the same experiment on identical hardware with the same software configuration, is this reproducibility or repeatability?

- Concept: Floating-point arithmetic non-determinism
  - Why needed here: The study reveals that hardware and software variations cause accuracy differences, which relates to how floating-point operations are handled across different environments
  - Quick check question: Why might a GPU and CPU produce slightly different results for the same floating-point calculation in a deep learning model?

- Concept: Containerization and environment isolation
  - Why needed here: The paper uses Docker/Singularity containers to control software environments, making understanding containerization essential to replicating their methodology
  - Quick check question: How do containers help isolate software environment variables when testing across different hardware?

## Architecture Onboarding

- Component map: Keras examples -> TensorFlow containers -> Hardware environments (Intel CPUs -> AMD CPUs -> NVIDIA GPUs)
- Critical path: Select hardware resource -> Pull appropriate container -> Run deterministic model training 5 times -> Collect accuracy metrics -> Repeat across all hardware/software combinations
- Design tradeoffs: OSG provides heterogeneous hardware access but introduces resource allocation complexity; containers ensure software consistency but require Docker-to-Singularity conversion for OSG compatibility
- Failure signatures: Non-deterministic outputs despite deterministic settings, accuracy variance >6% across identical experiments, bidirectional LSTM showing higher variance than simpler models
- First 3 experiments:
  1. Run MNIST classification on a single CPU with the TensorFlow 2.8 container 5 times to verify baseline determinism
  2. Run the same MNIST experiment on a GPU with the same container to test hardware consistency
  3. Run bidirectional LSTM on CPU with TensorFlow 2.9 container to observe model-specific variance patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum number of hardware and software environments required to achieve statistically significant reproducibility validation for deep learning experiments?
- Basis in paper: [explicit] The paper states "international bodies recommend reproducing experiments in at least 8 different labs" and suggests a similar strategy for ML experiments, but notes "exactly what the strategy should be is the objective of future work."
- Why unresolved: The paper only tested 13 hardware environments and 4 software environments, which may not be sufficient to establish a minimum threshold for reproducibility validation.
- What evidence would resolve it: A systematic study varying the number of environments (e.g., 3, 5, 8, 10, 15) and measuring the statistical significance of result variations across these configurations.

### Open Question 2
- Question: How do specific hardware characteristics (CPU architecture, GPU model, memory size) contribute to variations in deep learning model performance beyond what can be explained by software environment differences?
- Basis in paper: [explicit] The paper tested 4 Intel CPUs, 6 AMD CPUs, and 3 NVIDIA GPUs but did not analyze how specific hardware characteristics influenced the >6% accuracy range.
- Why unresolved: The study grouped hardware by vendor type rather than analyzing individual hardware characteristics, making it impossible to determine which specific hardware features cause performance variations.
- What evidence would resolve it: Controlled experiments isolating individual hardware variables (clock speed, core count, memory bandwidth) while keeping other factors constant.

### Open Question 3
- Question: Which types of deep learning architectures are most susceptible to hardware/software environment variations, and can this susceptibility be predicted from architectural properties?
- Basis in paper: [explicit] The study found >6% accuracy range in bidirectional LSTM and binary classification examples but not in simple MNIST, suggesting architectural differences matter, though this was not systematically analyzed.
- Why unresolved: The paper only tested three examples and did not establish a relationship between architectural properties (depth, parameter count, operation types) and susceptibility to environment variations.
- What evidence would resolve it: A comprehensive study testing diverse architectures across a wide range of hardware/software environments, correlating architectural metrics with observed variation patterns.

## Limitations
- Limited generalizability beyond tested models and environments
- Three model architectures tested may not represent broader deep learning landscape
- No analysis of specific hardware characteristics beyond vendor grouping

## Confidence
- Hardware/software variations causing >6% accuracy range: High confidence
- cuDNN bug identification and resolution: High confidence
- Model sensitivity to environment variations: Medium confidence
- Underlying computational reasons for variance: Medium confidence

## Next Checks
1. Replicate experiments with additional model architectures (e.g., CNNs, transformers) to test whether observed sensitivity patterns hold across diverse model types
2. Conduct micro-benchmarking to measure floating-point operation variance across different hardware/software combinations to validate non-determinism mechanism
3. Test impact of explicit random seed setting and deterministic GPU operations to determine if 6% variance threshold can be reduced through proper configuration