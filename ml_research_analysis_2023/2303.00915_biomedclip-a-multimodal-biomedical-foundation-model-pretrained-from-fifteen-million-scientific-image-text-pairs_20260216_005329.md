---
ver: rpa2
title: 'BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen
  million scientific image-text pairs'
arxiv_id: '2303.00915'
source_url: https://arxiv.org/abs/2303.00915
tags:
- image
- biomedical
- biomedclip
- pretraining
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BiomedCLIP presents a novel approach to pretraining vision-language
  models for biomedical applications. The key innovation is curating a massive dataset,
  PMC-15M, containing 15 million figure-caption pairs from PubMed Central articles,
  spanning diverse biomedical image types.
---

# BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs

## Quick Facts
- arXiv ID: 2303.00915
- Source URL: https://arxiv.org/abs/2303.00915
- Reference count: 26
- BiomedCLIP significantly outperforms prior approaches on biomedical tasks, including cross-modal retrieval, image classification, and visual question answering

## Executive Summary
BiomedCLIP presents a novel approach to pretraining vision-language models for biomedical applications. The key innovation is curating a massive dataset, PMC-15M, containing 15 million figure-caption pairs from PubMed Central articles, spanning diverse biomedical image types. This dataset is two orders of magnitude larger than existing biomedical image-text datasets. Based on PMC-15M, BiomedCLIP employs domain-specific adaptations, including using a biomedical language model (PubMedBERT) for text encoding, larger image sizes, and a gradual increase in batch size during training. Extensive experiments demonstrate that BiomedCLIP significantly outperforms prior approaches on various biomedical tasks, including cross-modal retrieval, image classification, and visual question answering. Notably, BiomedCLIP even surpasses radiology-specific models on radiology tasks, highlighting the benefits of large-scale pretraining across diverse biomedical image types.

## Method Summary
BiomedCLIP is pretrained using contrastive learning on the PMC-15M dataset, which contains 15 million figure-caption pairs from PubMed Central articles. The model employs domain-specific adaptations including a PubMedBERT text encoder, ViT-B/16 image encoder with 336px input size, and a gradual batch size scaling strategy (4k→64k). The training uses InfoNCE loss with sharded gradient computation. The model is evaluated on multiple biomedical tasks including cross-modal retrieval, image classification on datasets like PCam and LC25000, and visual question answering on VQA-RAD and SLAKE datasets.

## Key Results
- PMC-15M dataset contains 15 million biomedical image-text pairs, two orders of magnitude larger than existing datasets
- BiomedCLIP achieves significant performance improvements on cross-modal retrieval, image classification, and VQA tasks
- The model outperforms radiology-specific models on radiology tasks, demonstrating the benefits of diverse biomedical pretraining
- Domain-specific adaptations (PubMedBERT, larger images, batch size scaling) contribute to performance gains

## Why This Works (Mechanism)

### Mechanism 1
Large-scale pretraining on diverse biomedical image-text pairs improves cross-modal alignment more than radiology-only pretraining. The model learns richer visual-semantic associations by seeing varied biomedical image types (microscopy, radiology, charts, etc.) rather than only X-rays, improving its ability to generalize across tasks.

### Mechanism 2
Using PubMedBERT as text encoder and increasing context length captures domain-specific terminology better than GPT-2. PubMedBERT's biomedical vocabulary and longer context allow the model to encode complex biomedical captions more accurately, improving retrieval and classification.

### Mechanism 3
Gradual batch size scaling (4k→64k) stabilizes training and improves performance compared to constant large batches. Starting with smaller batches allows the model to learn stable initial representations before scaling up, reducing instability from large gradients early on.

## Foundational Learning

- **Vision-language contrastive pretraining**: Aligns images and text in shared embedding space without labels, enabling zero-shot and few-shot downstream use. Quick check: How does contrastive loss push matching pairs closer and non-matching pairs apart?

- **Domain-specific tokenizers and encoders**: Biomedical text contains specialized terms not well handled by general vocabularies, hurting representation quality. Quick check: What is the difference between WordPiece and Byte-Pair Encoding in handling rare biomedical terms?

- **Gradual batch size scaling during training**: Stabilizes large-scale pretraining by starting with smaller gradients and scaling up once representations are stable. Quick check: Why might starting with a huge batch from epoch 1 hurt convergence?

## Architecture Onboarding

- **Component map**: Image encoder (ViT-B/16) → linear projection → embedding; Text encoder (PubMedBERT + WordPiece) → embedding; Contrastive loss (sharded InfoNCE) → gradients back to both encoders

- **Critical path**: Forward pass through image encoder, forward pass through text encoder, compute cosine similarities, apply InfoNCE loss, backward pass

- **Design tradeoffs**: Larger image size (336px) improves detail capture but increases memory; gradual batch scaling trades wall-clock time for stability

- **Failure signatures**: Retrieval R@1 far below 70% indicates poor cross-modal alignment; classification accuracy drops on held-out data indicate overfitting or poor encoder adaptation

- **First 3 experiments**:
  1. Train with PubMedBERT text encoder and ViT-B/16 on 224px images; evaluate on PMC-15M validation retrieval
  2. Increase image resolution to 336px and re-evaluate retrieval to check for improvement
  3. Apply 50% patch dropout with unmasked tuning; compare retrieval R@1 against baseline

## Open Questions the Paper Calls Out

1. **Compound figures impact**: What is the impact of compound figures on BiomedCLIP's performance and how could splitting them improve the model? The paper mentions that splitting compound figures into sub-figures could increase data size and potentially lead to better vision-language representations, but does not explore this.

2. **In-line reference context**: How does the inclusion of in-line reference context impact BiomedCLIP's training and performance? The paper states that in-line references can naturally be paired with corresponding figures to create additional training signals, but their current data pipeline leaves this untouched.

3. **Optimal vision encoder size**: What is the optimal vision encoder size and input image size for BiomedCLIP, and how do they impact performance? The paper mentions that the largest vision encoder used is ViT-B and the input image size is 448, both constrained by compute.

## Limitations

- The paper doesn't systematically isolate the contribution of each image type (e.g., radiology vs. microscopy) to performance
- BiomedCLIP's performance on general-domain tasks is not evaluated, leaving open questions about domain adaptation costs
- Several hyperparameters remain unspecified (exact learning rate schedules, warmup steps, augmentation details), which could affect replication attempts

## Confidence

- **High confidence**: The claim that large-scale pretraining on PMC-15M improves cross-modal alignment across biomedical tasks is well-supported by extensive experimental results
- **Medium confidence**: The assertion that PubMedBERT text encoding specifically drives performance improvements is supported by ablation studies, though the exact contribution relative to other domain-specific adaptations remains unclear
- **Medium confidence**: The claim that gradual batch size scaling improves training stability is based on observed performance gains and theoretical reasoning, but lacks direct ablation evidence

## Next Checks

1. **Ablation study on dataset composition**: Replicate BiomedCLIP training with subsets of PMC-15M (e.g., only radiology images vs. only microscopy) to quantify the contribution of image diversity to downstream task performance

2. **Cross-domain evaluation**: Test BiomedCLIP on general-domain vision-language tasks (e.g., Flickr30k, MSCOCO) to measure potential performance degradation when applied outside the biomedical domain

3. **Hyperparameter sensitivity analysis**: Systematically vary key hyperparameters (learning rate, batch size schedule, image resolution) to identify which factors most strongly influence final model performance and stability