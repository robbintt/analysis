---
ver: rpa2
title: Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs
arxiv_id: '2311.17371'
source_url: https://arxiv.org/abs/2311.17371
tags:
- agent
- answer
- agents
- debate
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper benchmarks multi-agent debate (MAD) strategies for\
  \ improving the accuracy of large language models (LLMs) on medical question-answering\
  \ tasks. The authors compare several MAD strategies\u2014including Society of Minds,\
  \ Multi-Persona, ChatEval, and Ensemble Refinement\u2014against single-agent approaches\
  \ like self-consistency."
---

# Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs

## Quick Facts
- **arXiv ID**: 2311.17371
- **Source URL**: https://arxiv.org/abs/2311.17371
- **Reference count**: 40
- **Key outcome**: Multi-agent debate strategies generally improve LLM accuracy on medical QA tasks, with Multi-Persona and Society of Minds achieving up to 61% accuracy on MedQA, though they require more API calls and cost compared to single-agent approaches.

## Executive Summary
This paper benchmarks multi-agent debate (MAD) strategies for improving large language model accuracy on medical question-answering tasks. The authors compare several MAD approaches including Society of Minds, Multi-Persona, ChatEval, and Ensemble Refinement against single-agent baselines like self-consistency. Experiments across three medical QA datasets using GPT-3.5 and PaLM2 models show that while MAD strategies generally improve accuracy, they come with higher resource requirements. The study introduces a novel agreement modulation strategy that significantly improves performance by adjusting how often agents agree during debate, achieving up to 15% improvement for Multi-Persona.

## Method Summary
The paper implements and compares various MAD strategies that use in-context prompting to orchestrate debates between multiple LLM agents. Different strategies employ various debate structures - from simple two-agent debates to more complex Society of Minds approaches with multiple specialized agents. The agreement modulation strategy modulates how often agents should agree during debate via prompts. Experiments are conducted on three medical QA datasets using GPT-3.5-turbo and PaLM2 models via API, measuring accuracy, cost, time, tokens, debate consensus, and agent agreement.

## Key Results
- MAD strategies generally improve accuracy over single-agent approaches, with Multi-Persona and Society of Minds achieving best performance (up to 61% on MedQA)
- Agreement modulation significantly improves debate performance, with 15% improvement for Multi-Persona strategy
- MAD strategies require more API calls and cost compared to simpler approaches like self-consistency
- Performance is highly sensitive to hyperparameter tuning, particularly agreement intensity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modulating agent agreement intensity during debate improves performance by encouraging convergence on correct answers while avoiding premature consensus on incorrect ones
- Mechanism: Agents are prompted to agree with each other at specific percentages, creating balance between consensus-building and critical evaluation
- Core assumption: Agent agreement level correlates with debate quality and final answer correctness
- Evidence anchors: Abstract shows MAD doesn't reliably outperform single-agent strategies without careful prompting; agreement modulation significantly enhances performance
- Break condition: If agreement intensity is set too high, agents converge on incorrect answers; if too low, agents fail to reach consensus

### Mechanism 2
- Claim: Multi-agent debate improves accuracy by exposing models to diverse reasoning paths and allowing adaptation based on in-context information
- Mechanism: Multiple agents generate independent answers, share them in debate rounds, and revise based on new information
- Core assumption: LLMs can effectively use in-context information from other agents to improve reasoning without parameter updates
- Evidence anchors: Abstract describes agents adapting behavior based on information provided by other agents at inference time
- Break condition: If agents provide poor quality reasoning or debate prompts don't facilitate effective information sharing

### Mechanism 3
- Claim: Different agent personas and debate structures create varied reasoning dynamics that outperform generic debate approaches
- Mechanism: Assigning specific roles/personalities to agents (angel vs devil, expertise areas) generates more diverse and comprehensive reasoning
- Core assumption: Structured diversity in agent roles leads to more thorough exploration of solution space than unstructured debate
- Evidence anchors: Multi-Persona performs better with accuracy up to 61% on MedQA
- Break condition: If persona assignments don't create meaningful diversity or debate structure doesn't facilitate productive interaction

## Foundational Learning

- **Concept**: Chain-of-Thought (CoT) reasoning
  - Why needed here: Many MAD strategies use CoT prompts to guide agents through step-by-step reasoning
  - Quick check: How does CoT prompting help LLMs solve complex reasoning tasks compared to direct question answering?

- **Concept**: Self-consistency sampling
  - Why needed here: Paper compares MAD strategies against self-consistency, which samples multiple reasoning paths
  - Quick check: What's the key difference between self-consistency and multi-agent debate in terms of answer generation and selection?

- **Concept**: Prompt engineering and in-context learning
  - Why needed here: MAD strategies rely entirely on prompt engineering without parameter updates
  - Quick check: How can you design prompts that encourage productive debate while avoiding premature consensus or endless disagreement?

## Architecture Onboarding

- **Component map**: Base LLM agents -> Debate coordinator -> Prompt templates -> Agreement modulation system -> Evaluation pipeline
- **Critical path**: Initialize debate with question and agent prompts → Each agent generates initial answer → Debate coordinator shares answers → Agents revise based on received information → Repeat until consensus or max rounds → Select final answer → Evaluate performance
- **Design tradeoffs**: More debate rounds improve accuracy but increase cost/latency; higher agreement intensity improves consensus but may reduce diversity; complex structures outperform simpler ones but require more API calls
- **Failure signatures**: Agents change answers without converging (agreement too low); agents immediately agree on incorrect answers (agreement too high); debate gets stuck in repetitive cycles; performance doesn't improve despite multiple rounds; cost per question becomes prohibitively high
- **First 3 experiments**: 1) Compare single-agent FS-CoT against basic multi-agent debate with 2 agents and 2 rounds 2) Test agreement modulation by running Multi-Persona with 0%, 50%, 90%, and 100% agreement intensity 3) Compare different debate strategies (SoM vs Multi-Persona vs ChatEval) on same dataset with identical cost budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MAD performance scale with increasing model size beyond GPT-3.5 and PaLM2?
- Basis in paper: Interest in continuing work on dedicated in-house infrastructure suggests exploration of larger models
- Why unresolved: Paper only benchmarks GPT-3.5 and PaLM2, leaving open how MAD strategies perform with frontier models
- What evidence would resolve it: Systematic experiments comparing MAD strategies across multiple model sizes and architectures

### Open Question 2
- Question: What is the optimal number of debate rounds for different MAD strategies and question types?
- Basis in paper: Observation that "more agents over more debate rounds perform better" but with diminishing returns
- Why unresolved: Paper uses fixed round counts per strategy without exploring optimal tradeoff between performance and computational costs
- What evidence would resolve it: Ablation studies varying debate round counts for each strategy, analyzing accuracy improvements per additional round

### Open Question 3
- Question: How do different agent agreement intensity levels affect debate dynamics beyond just final accuracy?
- Basis in paper: Novel agreement modulation experiments showing 15% improvement for Multi-Persona
- Why unresolved: Paper only measures final accuracy impact, not how agreement intensity affects agent behavior, debate length, or answer diversity
- What evidence would resolve it: Detailed analysis of debate transcripts at different agreement levels, measuring answer changes, consensus formation patterns, and agent confidence

## Limitations
- Exact prompt templates and debate protocols for different MAD strategies are not fully specified in the main text
- Agreement modulation mechanism lacks detailed implementation specifications needed for faithful reproduction
- Cost-benefit tradeoff analysis is based on API costs that may not generalize across different deployment scenarios

## Confidence

- **High Confidence**: MAD strategies generally improve accuracy over single-agent approaches; Multi-Persona and Society of Minds achieve best performance on medical QA tasks
- **Medium Confidence**: Agreement modulation significantly improves debate performance (15% improvement), though implementation details affect results
- **Low Confidence**: Generalizability of findings across different domains beyond medical QA given specialized nature of datasets

## Next Checks
1. Implement minimal reproduction of agreement modulation strategy using publicly available medical QA datasets to verify claimed 15% improvement
2. Conduct cost-effectiveness analysis comparing MAD strategies against self-consistency under different budget constraints
3. Test sensitivity of MAD performance to agreement intensity thresholds across multiple domains (not just medical QA) to assess generalizability of agreement modulation findings