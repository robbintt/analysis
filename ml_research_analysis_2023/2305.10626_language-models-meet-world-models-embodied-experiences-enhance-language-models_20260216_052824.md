---
ver: rpa2
title: 'Language Models Meet World Models: Embodied Experiences Enhance Language Models'
arxiv_id: '2305.10626'
source_url: https://arxiv.org/abs/2305.10626
tags:
- tasks
- world
- arxiv
- embodied
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models often struggle with physical reasoning and
  planning due to their lack of embodied experiences. To address this, we propose
  finetuning LMs with embodied experiences from world models (E2WM).
---

# Language Models Meet World Models: Embodied Experiences Enhance Language Models

## Quick Facts
- arXiv ID: 2305.10626
- Source URL: https://arxiv.org/abs/2305.10626
- Reference count: 40
- Primary result: Small LMs (1.3B, 6B) match or outperform ChatGPT on physical reasoning tasks after finetuning with embodied experiences

## Executive Summary
Large language models struggle with physical reasoning and planning due to lack of embodied experiences. This work proposes finetuning LMs with experiences collected from a world model (VirtualHome) using goal-oriented planning and random exploration, while preserving generality with EWC-LoRA regularization. Experiments show the approach improves base LMs on 18 downstream tasks by 64.28% on average, with small LMs matching or outperforming ChatGPT on many tasks.

## Method Summary
The E2WM paradigm collects embodied experiences through goal-oriented planning using MCTS and random exploration in VirtualHome, then finetunes LMs with these experiences while preserving generality via EWC-LoRA regularization. The method uses GPT-Neo-1.3B and GPT-J-6B as base models and evaluates on 18 downstream tasks including plan generation, activity recognition, object tracking, and housework QA.

## Key Results
- Base LMs improved by 64.28% average across 18 downstream tasks
- Small LMs (1.3B, 6B) match or outperform ChatGPT on many physical reasoning tasks
- EWC-LoRA regularization successfully preserves LM generality while enabling embodied task learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Collecting goal-oriented planning experiences via MCTS teaches LMs to generate valid action sequences for household tasks.
- Mechanism: MCTS explores action space in VirtualHome, rewarding goal completion (+2) and penalizing irrelevant actions (-0.1), creating structured plans that LMs can learn from.
- Core assumption: The world model accurately simulates real-world physics and object interactions so plans generated in simulation generalize to real reasoning.
- Evidence anchors: [abstract] "Our approach deploys an embodied agent in a world model... and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration."

### Mechanism 2
- Claim: Random exploration experiences enable LMs to learn object permanence and tracking without explicit supervision.
- Mechanism: Agents wander randomly while recording object locations and movements, creating implicit tracking data that LMs can use to infer object persistence across observations.
- Core assumption: Observing object movements during random actions provides sufficient signal for LMs to learn that objects continue existing when out of sight.
- Evidence anchors: [abstract] "These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc."

### Mechanism 3
- Claim: EWC-LoRA regularization preserves LM generality while enabling efficient adaptation to embodied tasks.
- Mechanism: EWC constrains important parameter updates based on Fisher information matrix, while LoRA reduces computational cost by only training low-rank adapters.
- Core assumption: Parameters important for general language modeling can be identified via Fisher matrix and protected during finetuning.
- Evidence anchors: [abstract] "Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations."

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: MCTS provides systematic exploration of action space to generate valid plans that LMs can learn from
  - Quick check question: What is the reward structure used in MCTS for this application?

- Concept: Elastic Weight Consolidation (EWC)
  - Why needed here: EWC prevents catastrophic forgetting by protecting parameters important for pretraining tasks during finetuning
  - Quick check question: How is the Fisher information matrix calculated for EWC in this work?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA enables efficient finetuning by only updating small adapter matrices instead of full model parameters
  - Quick check question: What is the relationship between the original weight matrix and the LoRA adapter matrices?

## Architecture Onboarding

- Component map: VirtualHome simulator -> MCTS planner -> Experience collector -> Experience formatter -> EWC-LoRA finetuning module -> Evaluation tasks
- Critical path: Experience collection -> Finetuning -> Evaluation. Any failure in experience quality directly impacts downstream performance.
- Design tradeoffs: Full finetuning vs LoRA (parameter efficiency vs potential performance), EWC vs KL regularization (memory efficiency vs effectiveness), random vs goal-oriented exploration (coverage vs task-specific learning)
- Failure signatures: High perplexity on Pile test indicates loss of language modeling ability; poor performance on unseen tasks indicates overfitting to training experiences
- First 3 experiments:
  1. Verify VirtualHome simulator correctly executes actions and maintains consistent state
  2. Test MCTS planner generates valid plans that achieve goals in VirtualHome
  3. Validate EWC-LoRA implementation by checking parameter updates and memory usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of EWC-LoRA compare to other regularization methods (KL penalty, pure EWC, pure LoRA) when scaling to even larger language models beyond GPT-J-6B?
- Basis in paper: [explicit] The paper compares EWC-LoRA against EWC, LoRA, and KL penalty regularization methods on GPT-Neo-1.3B and GPT-J-6B models, finding EWC-LoRA to be most effective at preserving language modeling abilities while improving downstream performance.
- Why unresolved: The paper only tests these methods on relatively small models (1.3B and 6B parameters). The computational requirements of pure EWC and KL penalty make them impractical for larger models, but EWC-LoRA's scalability to models like GPT-3 (175B) or beyond remains untested.
- What evidence would resolve it: Systematic experiments applying EWC-LoRA to increasingly larger models (20B, 70B, 175B parameters) while measuring both downstream task performance and language modeling retention compared to alternative regularization methods.

### Open Question 2
- Question: Can the embodied knowledge acquired from one world model (VirtualHome) be effectively transferred to and generalized across different world models with varying environments and object dynamics?
- Basis in paper: [inferred] The paper uses VirtualHome as the sole world model and demonstrates knowledge transfer from training tasks to novel downstream tasks. However, it doesn't explore whether knowledge from VirtualHome can transfer to fundamentally different environments like AI2-THOR or Minecraft.
- Why unresolved: The paper's experiments are confined to a single household environment. Embodied knowledge may be highly environment-specific, and the model might need retraining for each new world model rather than achieving true cross-environment generalization.
- What evidence would resolve it: Experiments training models on VirtualHome and evaluating their performance on entirely different world models (different simulators, different object properties, different physics) without additional fine-tuning, measuring performance degradation across domains.

### Open Question 3
- Question: What is the minimum amount of embodied experience required to achieve significant improvements in physical reasoning and planning, and how does this scale with model size?
- Basis in paper: [explicit] The paper demonstrates that finetuning with embodied experiences significantly improves performance across 18 downstream tasks, but doesn't systematically explore the relationship between amount of experience, model size, and performance gains.
- Why unresolved: The paper doesn't investigate whether smaller models could achieve similar improvements with less data, or whether larger models require proportionally more embodied experiences to realize benefits. The scaling relationship between experience quantity and performance remains unclear.
- What evidence would resolve it: Controlled experiments varying the quantity and diversity of embodied experiences while measuring performance improvements across different model sizes, identifying the point of diminishing returns for both small and large models.

## Limitations
- Generalizability Beyond VirtualHome: The approach relies entirely on experiences from VirtualHome, but there's no validation that these simulated experiences transfer to real-world physical reasoning.
- Experience Quality and Coverage: The paper doesn't specify the quantity and diversity of experiences collected, which could limit the model's ability to generalize.
- Regularization Effectiveness: Without ablation studies comparing EWC-LoRA to alternatives, we cannot determine which components are actually driving the performance gains.

## Confidence
- High Confidence: Base LM performance metrics (accuracy, Rouge-L scores) are reliably measured using standard evaluation protocols.
- Medium Confidence: The general improvement pattern across multiple task types suggests real gains rather than overfitting.
- Low Confidence: Claims about learning "object permanence" through random exploration and the specific effectiveness of MCTS-generated plans lack direct empirical support.

## Next Checks
1. **Transferability Test**: Evaluate the finetuned models on physical reasoning tasks outside VirtualHome (e.g., real-world robotics benchmarks or different simulation environments) to verify that embodied knowledge generalizes beyond the training simulator.

2. **Ablation Study**: Run controlled experiments removing EWC regularization, removing LoRA, or using only one type of experience collection (goal-oriented vs random exploration) to isolate which components contribute most to performance improvements.

3. **Experience Quantity Sensitivity**: Systematically vary the amount of collected experiences and measure performance to determine if the reported gains are robust or if they saturate quickly, which would indicate whether the approach scales with more data.