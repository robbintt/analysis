---
ver: rpa2
title: Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment
arxiv_id: '2301.02791'
source_url: https://arxiv.org/abs/2301.02791
tags:
- graph
- explanations
- explanation
- neural
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating faithful and consistent
  explanations for graph neural networks (GNNs), particularly for instance-level explanations
  that identify critical input elements (nodes or edges) relied upon by the model
  for predictions. The authors identify that existing methods, which focus on preserving
  original predictions, often produce spurious explanations due to confounding effects
  and different causal factors.
---

# Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment

## Quick Facts
- arXiv ID: 2301.02791
- Source URL: https://arxiv.org/abs/2301.02791
- Reference count: 40
- Key outcome: Proposes a novel framework for faithful and consistent GNN explanations using embedding alignment, outperforming existing methods on multiple benchmarks

## Executive Summary
This paper addresses the challenge of generating faithful and consistent explanations for Graph Neural Networks (GNNs), particularly for instance-level explanations that identify critical input elements (nodes or edges) relied upon by the model for predictions. The authors identify that existing methods, which focus on preserving original predictions, often produce spurious explanations due to confounding effects and different causal factors. To address this, they propose a novel framework that aligns intermediate embeddings between the original graph and the identified subgraph explanation. This alignment is achieved through various strategies, including anchor-based alignment, distributional alignment using Gaussian mixture models, and mutual information-based alignment. Theoretical analysis shows that this approach optimizes a more faithful explanation objective. Experiments on multiple real-world and synthetic datasets demonstrate that the proposed framework significantly improves the faithfulness and consistency of explanations compared to existing methods.

## Method Summary
The proposed framework incorporates an alignment loss that minimizes the distance between intermediate embeddings of the original graph and the explanation subgraph. This alignment is achieved through three strategies: (1) anchor-based alignment using Euclidean distance to representative anchors, (2) distributional alignment using Gaussian mixture models to capture embedding distributions, and (3) mutual information-based alignment. The framework is integrated with existing explanation methods like GNNExplainer and PGExplainer. The combined loss function balances explanation quality with embedding alignment, encouraging the explanation subgraph to be processed similarly to the original graph.

## Key Results
- The proposed framework significantly improves faithfulness metrics (AUROC score on edges, explanation fidelity) compared to baseline methods across 5 benchmark datasets
- Explanation consistency, measured by Structural Hamming Distance (SHD), is substantially higher with the alignment framework
- The alignment strategies demonstrate robustness to noise and distribution shifts in graph data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding alignment prevents spurious explanations by ensuring the explanation subgraph is processed similarly to the original graph.
- Mechanism: The proposed framework aligns intermediate node embeddings between the original graph and the explanation subgraph using various strategies (anchor-based, distributional, mutual information-based). This alignment ensures that both graphs are mapped to similar regions in the embedding space, making the GNN process them in comparable ways.
- Core assumption: If two graphs are mapped to embeddings near each other by a GNN layer, then these graphs are seen as similar by it and would be processed similarly by following layers.
- Evidence anchors:
  - [abstract] "Observing that both confounding effects and diverse causal rationales are encoded in internal representations, we propose a new explanation framework with an auxiliary alignment loss, which is theoretically proven to be optimizing a more faithful explanation objective intrinsically."
  - [section] "With this assumption, a proxy task can be designed by aligning internal graph embeddings between G′ and G. This new task can be incorporated into Framework 1 as an auxiliary optimization objective."
- Break condition: If the alignment strategies fail to capture the true semantic similarity between graphs, or if the GNN's internal representation space is not sufficiently discriminative.

### Mechanism 2
- Claim: Distribution-aware alignment strategies (Gaussian mixture model, mutual information) provide more robust and scale-invariant similarity measures compared to simple Euclidean distance.
- Mechanism: Instead of using absolute distances, the framework uses relative distances to representative anchors or probability distributions (e.g., Gaussian mixture model) to measure similarity. This accounts for varying importance of dimensions and avoids sensitivity to scale.
- Core assumption: The distribution of embeddings in the latent space reflects the semantic similarity of the corresponding graphs, and modeling this distribution can improve alignment.
- Evidence anchors:
  - [section] "Comparison Compared with the alignment loss based on relative distances against anchors... this variant uses a Gaussian distance kernel (Eq. 7) while the other one uses Euclidean distance in Sec. 5.2, which may amplify the influence of distant anchors."
  - [section] "In computing the distance between representations of two inputs, this variant adopts the KL divergence as in Eq. 8, which would be scale-agnostic compared to the other one directly using Euclidean distance as in Eq. 5."
- Break condition: If the assumed distribution (e.g., Gaussian mixture) does not accurately model the true embedding distribution, or if the number of anchors is not sufficient.

### Mechanism 3
- Claim: The alignment loss optimizes a more faithful explanation objective by maximizing the mutual information between the original graph, the explanation, and the prediction.
- Mechanism: The paper theoretically shows that the alignment loss is equivalent to maximizing I(G, G′, Y), which encourages the explanation to capture both the discriminative and representative parts of the original graph, preventing spurious explanations.
- Core assumption: Maximizing mutual information between the original graph, explanation, and prediction leads to more faithful explanations.
- Evidence anchors:
  - [section] "In this part, we expand this objective, connect it to Equation 6, and construct its proxy optimizable form as: I(G,G′, ˆY ) = ..."
  - [section] "In maxG′I( ˆY, G′) + I(G′, G), the first term maxG′I( ˆY, G′) is the same as Eq.(1). Following [12], We relax it as minG′HC( ˆY, ˆY′|G′), optimizing G′ to preserve original prediction outputs. The second term, maxG′I(G′, G), corresponds to maximizing consistency between G′ and G."
- Break condition: If the theoretical connection between the alignment loss and the mutual information objective is not valid under certain conditions, or if the relaxation of the objective introduces significant error.

## Foundational Learning

- Concept: Causality and confounding effects in machine learning
  - Why needed here: Understanding the paper's motivation requires grasping how confounding variables can lead to spurious correlations and how causal analysis can help identify and mitigate these issues.
  - Quick check question: Can you explain the difference between a causal relationship and a spurious correlation, and provide an example in the context of graph neural networks?

- Concept: Graph neural networks and message passing
  - Why needed here: The paper's methodology is built upon the principles of GNNs and their message-passing mechanisms. Understanding how GNNs process graph-structured data is crucial for comprehending the alignment strategies.
  - Quick check question: Describe the basic message-passing framework used in GNNs and explain how node representations are updated in each layer.

- Concept: Mutual information and its estimation
  - Why needed here: The paper's theoretical analysis and some of its alignment strategies (e.g., mutual information-based alignment) rely on the concept of mutual information and its estimation.
  - Quick check question: Define mutual information and explain how it can be used to measure the dependence between two random variables. How can mutual information be estimated from data?

## Architecture Onboarding

- Component map: Target GNN model (f) -> Explanation model -> Alignment loss module -> Optimization module
- Critical path:
  1. Input graph G is processed by the target GNN to obtain embeddings
  2. The explanation model learns importance masks (MA, MF) to identify a subgraph G′
  3. The alignment loss module computes the alignment loss between the embeddings of G and G′
  4. The optimization module updates the explanation model's parameters using the combined loss
- Design tradeoffs:
  - Choice of alignment strategy: Different strategies (anchor-based, distributional, mutual information-based) have different strengths and weaknesses in terms of robustness, scalability, and interpretability
  - Weight of alignment loss (λ): Balancing the alignment loss with the MMI-based loss is crucial for achieving faithful and consistent explanations
  - Number of anchors or Gaussian components: These hyperparameters affect the granularity and accuracy of the distribution modeling
- Failure signatures:
  - Poor faithfulness: The explanation subgraph does not accurately reflect the decision-making process of the target GNN
  - Low consistency: The explanation subgraph varies significantly across different runs or perturbations
  - High computational cost: The alignment strategies introduce significant overhead in terms of memory and runtime
- First 3 experiments:
  1. Implement and test the basic alignment strategy (Euclidean distance) on a small graph classification dataset (e.g., Mutag) to verify the core functionality
  2. Compare the performance of different alignment strategies (anchor-based, distributional, mutual information-based) on a larger dataset (e.g., Graph-SST5) to identify the most effective approach
  3. Conduct ablation studies to understand the impact of the alignment loss weight (λ) and the number of anchors/ Gaussian components on the faithfulness and consistency of explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more robust explanations that are invariant to different types of distribution shifts in graph data?
- Basis in paper: [explicit] The paper discusses spurious explanations arising from confounding effects of latent variables like distribution shift, and proposes embedding alignment as a solution.
- Why unresolved: The proposed alignment methods are heuristic and may not generalize well to all types of distribution shifts. More rigorous theoretical analysis and empirical validation on diverse real-world datasets are needed.
- What evidence would resolve it: Rigorous theoretical analysis proving the effectiveness of alignment methods against various distribution shifts, and extensive empirical validation on diverse real-world datasets with known distribution shifts.

### Open Question 2
- Question: Can we develop evaluation metrics for GNN explanations that go beyond comparing to ground-truth explanations and capture the true faithfulness and consistency of the explanations?
- Basis in paper: [explicit] The paper acknowledges the limitations of existing benchmarks that are designed to be less ambiguous and contain only one oracle cause of labels. It suggests the need for evaluation methods that can test the faithfulness of explanations in more complex scenarios.
- Why unresolved: Existing evaluation metrics are often limited to comparing with ground-truth explanations, which may not be available in real-world scenarios. Developing metrics that can capture the true faithfulness and consistency of explanations is an open challenge.
- What evidence would resolve it: Development of novel evaluation metrics that can capture the true faithfulness and consistency of explanations, validated through extensive experiments on diverse real-world datasets.

### Open Question 3
- Question: How can we extend the proposed alignment framework to other types of graph neural networks, such as graph attention networks (GATs) and graph transformers?
- Basis in paper: [inferred] The paper focuses on message-passing GNNs and proposes alignment strategies based on the internal embeddings of these models. It would be valuable to extend the framework to other types of GNNs.
- Why unresolved: The proposed alignment strategies are designed for message-passing GNNs and may not directly apply to other types of GNNs that have different architectures and learning mechanisms. Adapting the framework to other GNN types is an open challenge.
- What evidence would resolve it: Successful extension of the alignment framework to other types of GNNs, validated through extensive experiments on diverse real-world datasets and comparison with existing explanation methods.

## Limitations

- The theoretical claims connecting embedding alignment to faithful explanations rely heavily on assumptions about intermediate representations capturing causal relationships
- The paper doesn't extensively discuss computational overhead introduced by alignment strategies or their scalability to larger graphs
- Claims about addressing confounding effects and improving robustness to noise are not thoroughly validated with diverse real-world datasets

## Confidence

- **High Confidence**: The proposed alignment strategies (anchor-based, distributional, mutual information-based) effectively improve faithfulness metrics (AUROC, fidelity) and consistency (SHD) compared to baselines. The experimental methodology is sound and the results are reproducible.
- **Medium Confidence**: The theoretical analysis connecting alignment loss to a more faithful explanation objective (maximizing mutual information) is valid, but its practical implications and generalizability require further investigation.
- **Low Confidence**: The paper's claims about addressing confounding effects and improving robustness to noise are not thoroughly validated. The impact of different alignment strategies on the interpretability of explanations is not explicitly discussed.

## Next Checks

1. **Ablation Study**: Conduct an ablation study to isolate the contribution of each alignment strategy (anchor-based, distributional, mutual information-based) to the overall performance improvement.
2. **Robustness Analysis**: Evaluate the explanations' robustness to adversarial attacks and noise injection to assess their practical utility in real-world scenarios.
3. **Scalability Evaluation**: Test the proposed framework on larger, more complex graphs to understand its computational scalability and identify potential bottlenecks.