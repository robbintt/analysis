---
ver: rpa2
title: Linear Recurrent Units for Sequential Recommendation
arxiv_id: '2310.02367'
source_url: https://arxiv.org/abs/2310.02367
tags:
- lrurec
- sequential
- training
- recommendation
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Linear Recurrent Units for Sequential Recommendation
  (LRURec), a novel model that achieves both training efficiency and inference efficiency
  while outperforming state-of-the-art methods. LRURec leverages linear recurrence
  with matrix diagonalization for efficient sequence modeling, combined with recursive
  parallelization for fast training and inference.
---

# Linear Recurrent Units for Sequential Recommendation

## Quick Facts
- arXiv ID: 2310.02367
- Source URL: https://arxiv.org/abs/2310.02367
- Authors: 
- Reference count: 40
- Primary result: LRURec achieves up to 17.43% improvement in NDCG@10 on sparse datasets while being 2.6x faster than SASRec in training and 7.3x faster in incremental inference.

## Executive Summary
This paper proposes Linear Recurrent Units for Sequential Recommendation (LRURec), a novel model that achieves both training efficiency and inference efficiency while outperforming state-of-the-art methods. LRURec leverages linear recurrence with matrix diagonalization for efficient sequence modeling, combined with recursive parallelization for fast training and inference. Extensive experiments on real-world datasets show that LRURec consistently outperforms baselines in terms of recommendation accuracy and efficiency.

## Method Summary
LRURec employs linear recurrence units with matrix diagonalization to enable efficient sequence modeling. The core innovation is decomposing the transition matrix A into PΛP⁻¹, enabling element-wise multiplication in the eigenbasis for logarithmic time complexity. Position-wise feed-forward networks compensate for the absence of non-linearity in linear recurrence, while recursive parallelization accelerates forward feeding of sequential input. The architecture includes embedding layers, two-layer LRU blocks with PFFN and layer normalization, and prediction layers.

## Key Results
- Up to 17.43% improvement in NDCG@10 on sparse datasets
- 2.6x faster training compared to SASRec
- 7.3x faster incremental inference compared to SASRec

## Why This Works (Mechanism)

### Mechanism 1
Linear recurrence with matrix diagonalization reduces computational complexity while preserving sequence modeling capacity. By decomposing the transition matrix A into PΛP⁻¹, the recurrence operation becomes element-wise multiplication in the eigenbasis, enabling logarithmic time complexity via recursive parallelization. Core assumption: The eigenbasis captures the essential dynamics of sequential patterns in user-item interactions. Evidence anchors: [abstract] and [section 3.3] discuss the decomposition and parallelization benefits. Break condition: When eigenvalues cluster near 1, numerical instability emerges despite polar form parameterization.

### Mechanism 2
Position-wise feed-forward networks compensate for the absence of non-linearity in linear recurrence. PFFN introduces hierarchical non-linear transformations that capture complex transition patterns missed by purely linear dynamics. Core assumption: User-item transition patterns contain sufficient non-linear structure that can be modeled by post-recurrence transformations. Evidence anchors: [abstract] and [section 3.4] discuss the architecture modifications to address lack of non-linearity. Break condition: When transition patterns are predominantly linear, PFFN becomes redundant and adds unnecessary parameters.

### Mechanism 3
Recursive parallelization enables logarithmic time complexity for full-sequence processing. The divide-and-conquer approach splits sequences into subsequences, processes them in parallel, then combines results with appropriate time-step corrections. Core assumption: Sequence patterns can be captured through hierarchical composition of subsequence representations. Evidence anchors: [abstract] and [section 3.3] describe the parallelization scheme and its benefits. Break condition: When sequence length is small or highly irregular, the overhead of padding and splitting outweighs benefits.

## Foundational Learning

- **Matrix diagonalization and eigendecomposition**: Enables transformation of matrix multiplication into element-wise operations, critical for the logarithmic complexity achieved through recursive parallelization. Quick check: If A = PΛP⁻¹, what is Aⁿ expressed in terms of P, Λ, and P⁻¹?

- **Complex space representation for eigenvalues**: Eigenvalues often lie in complex space; representing them as exp(-exp(log(ν)) + jθ) ensures numerical stability and allows optimization in real space. Quick check: Why does representing Λ in polar form with r = e⁻ν help control numerical stability?

- **Layer normalization and residual connections**: Compensate for the absence of non-linearity in linear recurrence and improve training dynamics, as evidenced by ablation results showing significant performance drops without these components. Quick check: How do layer normalization and residual connections specifically help training in the absence of non-linear activation functions?

## Architecture Onboarding

- **Component map**: Embedding layer → LRU block(s) with PFFN and LayerNorm → Prediction layer
- **Critical path**: Input embedding → LRU recurrence → PFFN transformation → LayerNorm → residual addition → prediction
- **Design tradeoffs**: Linear recurrence enables training efficiency and incremental inference but requires PFFN to capture non-linear patterns; recursive parallelization adds implementation complexity but provides logarithmic time complexity
- **Failure signatures**: Poor performance on sparse datasets without sufficient non-linearity; numerical instability when eigenvalues approach 1; degraded performance when sequence length doesn't benefit from parallelization overhead
- **First 3 experiments**:
  1. Compare single-layer vs two-layer LRU with and without PFFN on ML-1M to validate the non-linearity compensation hypothesis
  2. Test recursive parallelization on sequences of varying lengths to identify the crossover point where benefits outweigh overhead
  3. Remove layer normalization and residual connections to confirm their contribution to training stability and performance

## Open Questions the Paper Calls Out
None explicitly called out in the provided content.

## Limitations
- Numerical stability concerns when eigenvalues cluster near 1, which the polar form parameterization partially addresses but doesn't fully resolve
- Performance dependency on sequence length and regularity, with unclear crossover points where parallelization overhead becomes prohibitive
- Moderate relevance of related work (FMR=0.391) suggests active research area with competing approaches, limiting confidence in novelty claims

## Confidence
- **High confidence**: Training efficiency improvements (2.6x faster than SASRec) - direct computational claim with measurable outcomes
- **Medium confidence**: Inference efficiency gains (7.3x faster incremental inference) - depends on implementation details not fully specified
- **Medium confidence**: Recommendation accuracy improvements (up to 17.43% NDCG@10) - performance claims are supported by experiments but depend on hyperparameter tuning and dataset characteristics

## Next Checks
1. **Eigenvalue stability test**: Systematically vary the initialization range for eigenvalues and measure training stability and performance across different sequence types. Track how close eigenvalues get to 1 and measure the resulting numerical error.

2. **Parallelization overhead analysis**: Implement the recursive parallelization algorithm and benchmark it across sequences of varying lengths (50, 100, 200, 500, 1000) to identify the exact crossover point where benefits outweigh padding/splitting overhead.

3. **PFFN necessity validation**: Conduct controlled experiments removing PFFN from LRURec and comparing against a purely linear recurrent baseline to quantify the exact contribution of the feed-forward component to overall performance.