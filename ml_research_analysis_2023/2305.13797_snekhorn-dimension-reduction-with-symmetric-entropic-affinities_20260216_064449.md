---
ver: rpa2
title: 'SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities'
arxiv_id: '2305.13797'
source_url: https://arxiv.org/abs/2305.13797
tags:
- problem
- affinities
- entropic
- affinity
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel symmetrization of entropic affinities,
  a key component in the t-SNE algorithm, to address the inherent asymmetry in these
  affinities. The authors propose a principled approach that leverages optimal transport
  theory, enabling the control of entropy in each point while producing a symmetric
  doubly stochastic affinity matrix.
---

# SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities

## Quick Facts
- arXiv ID: 2305.13797
- Source URL: https://arxiv.org/abs/2305.13797
- Reference count: 40
- Primary result: Introduces SNEkhorn, a dimensionality reduction algorithm using symmetric entropic affinities to address asymmetry in t-SNE and improve clustering performance on noisy data.

## Executive Summary
This paper introduces SNEkhorn, a novel dimensionality reduction algorithm that addresses the asymmetry problem in t-SNE's entropic affinities through a principled symmetrization approach based on optimal transport theory. The method constructs symmetric entropic affinities (SEA) by solving a convex optimization problem that enforces row-wise entropy constraints while producing a doubly stochastic affinity matrix. SNEkhorn couples these SEA with doubly stochastic kernels in the latent space, eliminating spherical constraints on embeddings and demonstrating improved clustering performance and robustness to heteroscedastic noise on both synthetic and real-world datasets.

## Method Summary
SNEkhorn computes symmetric entropic affinities (SEA) by solving a convex optimization problem min P∈Hξ∩S ⟨P, C⟩, where Hξ is the set of matrices with row-wise entropy constraints and S is the set of doubly stochastic matrices. The SEA affinity is computed using a dual ascent algorithm with closed-form gradients. For dimensionality reduction, SNEkhorn optimizes embedding coordinates Z to minimize the KL divergence between SEA and a doubly stochastic kernel Qds_Z in the latent space, computed via Sinkhorn iterations. This approach ensures symmetry in both input and latent spaces while controlling the entropy of each point.

## Key Results
- SEA produces symmetric doubly stochastic affinity matrices with controlled row-wise entropy
- SNEkhorn demonstrates superior clustering performance compared to t-SNE on synthetic data with heteroscedastic noise
- SNEkhorn preserves local and global data structures better than baseline methods on MNIST and other real-world datasets
- The method is robust to varying noise levels due to equal mass spreading enforced by entropy constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symmetric entropic affinities (SEA) solve the asymmetry problem of traditional entropic affinities by enforcing symmetry through convex optimization.
- Mechanism: SEA reformulates the entropic affinity problem as a symmetric optimal transport problem with entropy constraints, producing a doubly stochastic affinity matrix where each row's entropy is controlled to the desired value.
- Core assumption: The optimal solution to the SEA problem will have saturated entropy for at least n-1 points, ensuring proper control over the perplexity parameter.
- Evidence anchors:
  - [abstract]: "we uncover a novel characterization of EA as an optimal transport problem, allowing a natural symmetrization that can be computed efficiently using dual ascent."
  - [section 3.2]: "we consider the convex optimization problem min P∈Hξ∩S ⟨P, C⟩" and "Proposition 4 (Saturation of the entropies)...for at least n − 1 indices i ∈ [[n]] the solution satisfies H(Pse i:) = log ξ + 1."
- Break condition: If the cost matrix C has constant rows or violates the symmetry assumption, the SEA formulation may not produce meaningful results.

### Mechanism 2
- Claim: SEA provides robustness to heteroscedastic noise by forcing equal mass spreading among samples.
- Mechanism: By imposing row-wise entropy constraints, SEA ensures that each sample distributes its mass over neighbors in a way that compensates for varying sampling densities, unlike doubly stochastic affinities that spread mass unevenly.
- Core assumption: The entropy constraints in SEA effectively capture the local structure of data with varying noise levels, leading to better clustering performance.
- Evidence anchors:
  - [section 3.2]: "The per-row entropy constraints of Pse force equal spreading among samples. This can have benefits, particularly for clustering..."
  - [section 5]: "Figure 2 we illustrate the ability of our proposed affinity Pse to adapt to varying noise levels...Pse is robust to varying noise levels."
- Break condition: In datasets with uniform sampling density and noise levels, the additional complexity of SEA may not provide significant advantages over simpler methods.

### Mechanism 3
- Claim: SNEkhorn eliminates spherical constraints on embeddings by using doubly stochastic affinities in both input and latent spaces.
- Mechanism: Unlike previous approaches that used symmetric Gaussian affinities in the latent space, SNEkhorn employs a doubly stochastic kernel, allowing embeddings to freely adapt to the geometry of the target space without artificial constraints.
- Core assumption: The doubly stochastic normalization in both spaces enables more flexible and accurate low-dimensional representations.
- Evidence anchors:
  - [section 4]: "in SNEkhorn, the latent affinity is also doubly stochastic so that latent coordinates Z are not subject to spherical constraints anymore."
  - [section 5]: "Figure 3: Left: SNEkhorn embedding on the simulated data of Section 5 using eQZ instead of Qds Z with ξ = 30."
- Break condition: If the low-dimensional space has inherent spherical constraints (e.g., when projecting onto a sphere), the benefits of doubly stochastic normalization may be limited.

## Foundational Learning

- Concept: Optimal Transport (OT) and Entropy-Regularized OT
  - Why needed here: The SEA formulation is derived from an entropy-constrained OT problem, and understanding this connection is crucial for grasping the mathematical foundation of the method.
  - Quick check question: How does the entropy regularization in OT relate to the row-wise entropy constraints in SEA?

- Concept: Doubly Stochastic Matrices
  - Why needed here: Both the SEA affinity and the latent space kernel in SNEkhorn are doubly stochastic, and understanding their properties is essential for comprehending the method's advantages.
  - Quick check question: What are the key benefits of using doubly stochastic matrices in spectral clustering and dimensionality reduction?

- Concept: Kullback-Leibler (KL) Divergence and Bregman Projections
  - Why needed here: The SEA affinity can be interpreted as a KL projection of a Gaussian kernel onto the space of matrices with controlled row-wise entropy, and this interpretation is used in the computational approach.
  - Quick check question: How does the KL projection interpretation of SEA relate to the alternating Bregman projections algorithm described in Appendix B?

## Architecture Onboarding

- Component map: Input data matrix X → Cost matrix CX → SEA affinity Pse → Latent coordinates Z → Cost matrix CZ → Doubly stochastic kernel Qds_Z → KL divergence objective

- Critical path:
  1. Compute cost matrix CX from input data
  2. Compute SEA affinity Pse using dual ascent algorithm
  3. Initialize latent coordinates Z (e.g., from N(0,1))
  4. For each optimization step:
     a. Compute cost matrix CZ from current Z
     b. Compute Qds Z via Sinkhorn iterations
     c. Compute KL divergence KL(Pse|Qds Z)
     d. Backpropagate gradients to update Z

- Design tradeoffs:
  - SEA vs. traditional entropic affinities: SEA provides symmetry and better noise robustness but requires solving a more complex optimization problem.
  - Doubly stochastic latent affinity vs. Gaussian: Doubly stochastic avoids spherical constraints but may require more iterations to converge.
  - Entropy constraints: Tighter constraints provide better noise robustness but may lead to slower convergence.

- Failure signatures:
  - Slow convergence: May indicate that the entropy constraints are too tight or the cost matrix has problematic structure.
  - Poor clustering performance: Could suggest that the perplexity parameter ξ is not well-tuned for the dataset.
  - Spherical embeddings: Might indicate that the latent affinity is not sufficiently doubly stochastic.

- First 3 experiments:
  1. Compare SEA vs. traditional entropic affinities on a simple synthetic dataset with heteroscedastic noise.
  2. Evaluate SNEkhorn vs. t-SNE on a standard benchmark dataset (e.g., MNIST) using silhouette and trustworthiness scores.
  3. Test the effect of the perplexity parameter ξ on SEA's clustering performance using spectral clustering on a small image dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Proposition 4 be extended to guarantee that all n entropies are saturated, not just n-1?
- Basis in paper: [inferred] The paper states that in practice, with the algorithmic solution, all n entropies are observed to be saturated, but this is not proven for all cases.
- Why unresolved: The authors believe the proposition can be extended with a few more assumptions on C, but do not provide the specific conditions or proof.
- What evidence would resolve it: A formal proof demonstrating that under certain conditions on the cost matrix C, all n entropies are saturated in the solution to (SEA).

### Open Question 2
- Question: How does the choice of σ in the KL projection method affect the convergence and accuracy of computing Pse?
- Basis in paper: [explicit] The paper mentions that choosing σ too high might result in some entropies being unsaturated while a σ too small generally leads to slow convergence, but does not provide a detailed analysis of the impact of σ on the method's performance.
- Why unresolved: The authors only briefly discuss the trade-off in choosing σ but do not provide a thorough analysis of how different values of σ affect the convergence rate and accuracy of the alternating Bregman projections method.
- What evidence would resolve it: Empirical studies comparing the convergence and accuracy of the alternating Bregman projections method for different values of σ, along with a theoretical analysis of the impact of σ on the method's performance.

### Open Question 3
- Question: How can multi-scale versions of symmetric entropic affinities be constructed to capture dependencies at different scales?
- Basis in paper: [explicit] The authors mention that building multi-scale versions of symmetric entropic affinities is a promising research direction in the conclusion.
- Why unresolved: The paper does not provide any details on how to construct such multi-scale affinities or discuss the potential benefits and challenges of doing so.
- What evidence would resolve it: A proposed method for constructing multi-scale symmetric entropic affinities, along with empirical studies demonstrating the benefits of using such affinities in dimensionality reduction tasks.

## Limitations

- Computational complexity remains a concern for very large datasets (n > 10,000) due to the need to solve constrained optimization problems and perform Sinkhorn iterations
- Method sensitivity to the perplexity parameter ξ and its interaction with entropy constraints requires further systematic investigation
- Performance on high-dimensional real-world data beyond MNIST is not thoroughly explored

## Confidence

**High Confidence:** The mathematical formulation of SEA as an optimal transport problem and its dual characterization is well-established, supported by Proposition 4 and the theoretical analysis in Section 3.2. The experimental results showing improved clustering performance on synthetic data with heteroscedastic noise are convincing.

**Medium Confidence:** The claim that SEA provides superior robustness to varying noise levels is supported by the experiments in Figure 2, but the analysis is limited to a specific synthetic dataset. The benefits of doubly stochastic normalization in the latent space are demonstrated qualitatively in Figure 3 but lack quantitative comparison with alternative approaches.

**Low Confidence:** The computational efficiency claims for large-scale datasets are not empirically validated, and the method's performance on high-dimensional real-world data (beyond MNIST) is not thoroughly explored.

## Next Checks

1. **Scalability Test:** Evaluate SNEkhorn's runtime and memory requirements on progressively larger datasets (n = 1,000, 5,000, 10,000, 50,000) and compare with t-SNE and UMAP implementations.

2. **Parameter Sensitivity Analysis:** Systematically vary the perplexity parameter ξ and the entropy constraints to quantify their impact on clustering performance across multiple datasets with different noise characteristics.

3. **Robustness to Noise:** Design controlled experiments with synthetic data containing varying levels and types of noise (additive Gaussian, heteroscedastic, outliers) to quantify SNEkhorn's robustness compared to baseline methods.