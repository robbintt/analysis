---
ver: rpa2
title: 'MACO: A Modality Adversarial and Contrastive Framework for Modality-missing
  Multi-modal Knowledge Graph Completion'
arxiv_id: '2308.06696'
source_url: https://arxiv.org/abs/2308.06696
tags:
- maco
- visual
- information
- missing
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MACO tackles the missing modality problem in multimodal knowledge
  graph completion by generating realistic visual features for entities lacking them.
  It uses a conditional generator-discriminator adversarial framework to produce visual
  features conditioned on structural embeddings, along with a cross-modal contrastive
  loss to improve feature quality.
---

# MACO: A Modality Adversarial and Contrastive Framework for Modality-missing Multi-modal Knowledge Graph Completion

## Quick Facts
- **arXiv ID:** 2308.06696
- **Source URL:** https://arxiv.org/abs/2308.06696
- **Reference count:** 24
- **Key outcome:** MACO achieves state-of-the-art link prediction results on FB15K-237 with missing visual rates up to 80%, reaching MRR of 32.1 and Hit@10 of 49.6 across multiple score functions.

## Executive Summary
MACO addresses the missing modality problem in multimodal knowledge graph completion by generating realistic visual features for entities lacking them. The framework uses a conditional generator-discriminator adversarial setup to produce visual features conditioned on structural embeddings, along with a cross-modal contrastive loss to improve feature quality. Evaluated on FB15K-237 with missing rates up to 80%, MACO outperforms baselines and achieves state-of-the-art link prediction results across multiple score functions. The method is shown to be robust, generalizable, and effective in preserving semantic correspondence between modalities.

## Method Summary
MACO is a framework for handling missing visual modalities in multimodal knowledge graph completion. It trains a generator and discriminator adversarially to produce visual features conditioned on structural embeddings from an R-GCN. The generator takes structural embeddings and random noise as input to produce visual features, while the discriminator evaluates whether the structural-visual pairs are compatible. A cross-modal contrastive loss maximizes mutual information between generated visual features and their corresponding structural embeddings. The completed visual features are then used to enhance downstream MMKGC models. The method is evaluated on FB15K-237 with varying missing rates and shows consistent improvements across different MMKGC architectures.

## Key Results
- MACO achieves MRR up to 32.1 and Hit@10 up to 49.6 on FB15K-237 across multiple score functions
- The framework maintains robust performance with missing rates up to 80%, outperforming baselines by significant margins
- MACO demonstrates strong generalization by improving various MMKGC models (IKRL, TBKGC, RSME) without architectural modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MACO generates realistic visual features by conditioning on structural embeddings
- Mechanism: The generator takes as input both the structural embedding of an entity and random noise, then produces a visual feature vector. The adversarial discriminator then evaluates whether this generated pair (structural, visual) is compatible, encouraging the generator to produce features that match the structural context.
- Core assumption: Structural embeddings contain sufficient information to guide the generation of appropriate visual features for each entity.
- Evidence anchors:
  - [abstract]: "MACO trains a generator and discriminator adversarially to generate missing modality features that can be incorporated into the MMKGC model."
  - [section]: "The generator G is a conditional generator, aiming to generate the visual information given the structural feature of an entity."
  - [corpus]: Weak - the related papers focus on modality completion but don't specifically address the conditioning mechanism used here.
- Break condition: If the structural embedding doesn't capture enough semantic information about the entity's visual appearance, the generated features will be unrealistic.

### Mechanism 2
- Claim: Cross-modal contrastive loss improves the quality of generated visual features
- Mechanism: After generating visual features, MACO applies a contrastive loss that maximizes the similarity between the structural embedding and its corresponding generated visual feature while minimizing similarity with other entities' visual features. This enforces alignment between modalities.
- Core assumption: Maximizing mutual information between structural and visual modalities produces more semantically meaningful visual features.
- Evidence anchors:
  - [abstract]: "Meanwhile, we design a cross-modal contrastive loss to improve the performance of the generator."
  - [section]: "we propose another cross-modal contrastive module to contrast the structural features and the generated visual features, aiming to maximize their mutual information and improve the quality of the generated visual features."
  - [corpus]: Missing - no direct evidence in related papers about contrastive approaches for modality completion in MMKGC.
- Break condition: If the temperature parameter τ is poorly tuned, the contrastive loss may either be too weak to be effective or too strong causing training instability.

### Mechanism 3
- Claim: MACO serves as a general enhancement framework compatible with different MMKGC models
- Mechanism: By generating missing visual features as input features rather than modifying the core MMKGC architecture, MACO can be plugged into existing models like IKRL, TBKGC, and RSME, improving their performance regardless of their specific scoring functions.
- Core assumption: The downstream MMKGC model can effectively utilize the enhanced visual features without architectural modifications.
- Evidence anchors:
  - [abstract]: "MACO could achieve state-of-the-art results and serve as a versatile framework to bolster various MMKGC models."
  - [section]: "we conduct comprehensive experiments on the public benchmarks with further exploration, which prove that MACO could achieve SOTA results in the modality-missing MMKGC."
  - [corpus]: Weak - related papers focus on different MMKGC approaches but don't demonstrate this kind of modular enhancement capability.
- Break condition: If the MMKGC model has a strong bias toward its original visual features, adding generated features may not provide meaningful improvement.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: The adversarial framework between generator and discriminator is the core mechanism for producing realistic visual features.
  - Quick check question: What are the two components of a GAN and what is each component trying to achieve during training?

- Concept: Graph Neural Networks (specifically R-GCN)
  - Why needed here: R-GCN is used to encode structural information from the knowledge graph into entity embeddings that condition the visual feature generation.
  - Quick check question: How does R-GCN aggregate information from neighboring entities through multiple relation types?

- Concept: Contrastive Learning
  - Why needed here: The cross-modal contrastive loss helps align the generated visual features with their corresponding structural embeddings by maximizing mutual information.
  - Quick check question: What is the difference between instance discrimination and cross-modal contrastive learning?

## Architecture Onboarding

- Component map: R-GCN → Generator → Discriminator → Contrastive loss → Generated visual features → MMKGC model
- Critical path: The structural embeddings from R-GCN condition the generator to produce visual features, which are evaluated by the discriminator and aligned with contrastive loss, then used to enhance the downstream MMKGC model
- Design tradeoffs:
  - Conditioning the generator on structural embeddings vs. unconditional generation: Conditioning ensures semantic consistency but may limit diversity
  - Number of generated visual features (K): Higher K provides better coverage but increases computational cost
  - Two strategies (Gen vs All-Gen): Gen preserves original information but may miss enhancement opportunities; All-Gen ensures consistency but risks losing useful original features
- Failure signatures:
  - Mode collapse: Generator produces very similar visual features for different entities
  - Training instability: Discriminator loss becomes very small while generator loss remains high
  - Poor downstream performance: Generated features don't improve MMKGC metrics despite good generator metrics
- First 3 experiments:
  1. Run MACO with only the adversarial loss (no contrastive loss) to verify the basic generation capability
  2. Test with different missing rates (20%, 40%, 60%, 80%) to confirm robustness across scenarios
  3. Compare the two strategies (Gen vs All-Gen) on a single MMKGC model to determine which is more effective for different scoring functions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The method requires modality-complete data for training the generator, which may not always be available in real-world scenarios
- The effectiveness depends heavily on the quality of initial structural embeddings from R-GCN
- The computational cost of generating multiple visual features (K=512) and additional contrastive loss may be prohibitive for very large knowledge graphs

## Confidence

- **High confidence** in the core adversarial framework's ability to generate visual features conditioned on structural embeddings
- **Medium confidence** in the cross-modal contrastive loss's contribution to feature quality
- **Medium confidence** in the general enhancement capability claim across different MMKGC models

## Next Checks

1. Conduct ablation studies removing the contrastive loss to quantify its exact contribution to performance improvements
2. Test MACO on a different multimodal knowledge graph dataset (e.g., NELL-995 with images) to verify generalizability beyond FB15K-237
3. Perform sensitivity analysis on the number of generated visual features (K) to determine if the current setting is optimal or if computational costs can be reduced