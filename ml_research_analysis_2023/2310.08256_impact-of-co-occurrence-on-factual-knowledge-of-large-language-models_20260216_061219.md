---
ver: rpa2
title: Impact of Co-occurrence on Factual Knowledge of Large Language Models
arxiv_id: '2310.08256'
source_url: https://arxiv.org/abs/2310.08256
tags:
- language
- co-occurrence
- knowledge
- llms
- finetuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how pre-training data co-occurrence statistics
  affect factual knowledge in large language models (LLMs). The authors find that
  LLMs rely heavily on subject-object co-occurrence in pre-training corpora, often
  preferring frequently co-occurring words over correct answers, especially for rare
  facts.
---

# Impact of Co-occurrence on Factual Knowledge of Large Language Models

## Quick Facts
- arXiv ID: 2310.08256
- Source URL: https://arxiv.org/abs/2310.08256
- Reference count: 9
- This paper investigates how pre-training data co-occurrence statistics affect factual knowledge in large language models (LLMs).

## Executive Summary
This paper investigates how pre-training data co-occurrence statistics affect factual knowledge in large language models (LLMs). The authors find that LLMs rely heavily on subject-object co-occurrence in pre-training corpora, often preferring frequently co-occurring words over correct answers, especially for rare facts. This "co-occurrence bias" persists despite scaling model size or finetuning. While scaling and finetuning improve overall factual knowledge probing accuracy, they don't resolve the bias. Debiased finetuning by filtering high co-occurrence samples helps models learn rare training facts but doesn't generalize to unseen rare facts. The study highlights the importance of addressing co-occurrence bias for building more reliable LLMs.

## Method Summary
The study preprocesses the LAMA-TREx dataset and splits it into training and test sets (70:30). Co-occurrence statistics of subject-object pairs are computed in the Pile dataset. Target models (GPT-Neo and GPT-J of different sizes) are evaluated on factual knowledge probing with different output candidate set restrictions. The correlation between co-occurrence statistics and model performance is analyzed using hits@1 and MRR metrics. Debiasing is attempted through undersampling high co-occurrence samples during finetuning.

## Key Results
- LLMs exhibit co-occurrence bias, preferring frequently co-occurring words over correct answers for factual knowledge retrieval
- Scaling model size and finetuning improve overall accuracy but don't resolve co-occurrence bias
- Debiased finetuning helps models learn rare training facts but doesn't generalize to unseen rare facts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs preferentially generate frequently co-occurring words over correct answers, especially for rare facts.
- **Mechanism**: The pre-training data's subject-object co-occurrence statistics create spurious correlations that LLMs learn as shortcuts, overriding accurate semantic understanding.
- **Core assumption**: Co-occurrence counts in pre-training corpora directly influence token generation probabilities during inference.
- **Evidence anchors**:
  - [abstract]: "Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer."
  - [section 5.2.4]: "Table 1 showcases failure cases of GPT-J 6B in the gold objects (relation-wise) setting. The examples demonstrate that the model fails to recall facts by selecting words with higher co-occurrence counts over the correct answers."
  - [corpus]: Weak - related papers discuss factual knowledge but don't directly address co-occurrence bias mechanisms.

### Mechanism 2
- **Claim**: Scaling model size or finetuning improves overall factual knowledge probing accuracy but doesn't resolve co-occurrence bias.
- **Mechanism**: Larger models and finetuning increase general knowledge capacity and memorization ability, but the learned co-occurrence shortcuts persist because they're reinforced throughout training.
- **Core assumption**: Co-occurrence bias is learned early and becomes deeply embedded in the model's generation patterns.
- **Evidence anchors**:
  - [abstract]: "We show that co-occurrence bias remains despite scaling up model sizes or finetuning."
  - [section 5.2.3]: "Figure 5 shows hits@1 of the target models against Ppretrain(obj|subj), the conditional probability of the gold object given a subject. In both zero-shot and finetuned settings, we observe that hits@1 is lower as the co-occurrence count is lower."
  - [corpus]: Weak - scaling law papers exist but don't specifically address co-occurrence bias persistence.

### Mechanism 3
- **Claim**: Debiased finetuning on filtered datasets helps models learn rare training facts but doesn't generalize to unseen rare facts.
- **Mechanism**: Removing high co-occurrence samples during finetuning forces the model to rely on other cues for rare facts, improving memorization of those specific training examples but not providing generalizable knowledge.
- **Core assumption**: The model can memorize specific rare facts when high co-occurrence distractors are removed, but lacks the semantic understanding to apply this knowledge to new contexts.
- **Evidence anchors**:
  - [abstract]: "Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning."
  - [section 6]: "Figure 9 shows that the performance of the debiased model and the baseline are similar regardless of co-occurrence counts, implying that the effect of debiased finetuning is not generalizable."
  - [corpus]: Weak - related papers discuss knowledge editing but don't specifically address debiased finetuning limitations.

## Foundational Learning

- **Concept: Spurious correlations in machine learning**
  - Why needed here: Understanding how models can learn incorrect associations from training data that don't reflect true causal relationships.
  - Quick check question: Can you explain the difference between correlation and causation in the context of language model training?

- **Concept: Distant supervision and relation extraction**
  - Why needed here: The paper uses distant supervision principles where co-occurrence implies relationship, which is exactly what creates the bias.
  - Quick check question: How does the concept of distant supervision relate to why co-occurrence becomes a problematic signal for factual knowledge?

- **Concept: Memorization vs. generalization in neural networks**
  - Why needed here: The paper shows that memorization of rare facts is necessary but insufficient, highlighting the distinction between rote memorization and true understanding.
  - Quick check question: What evidence from the paper suggests that memorization alone is insufficient for reliable factual knowledge retrieval?

## Architecture Onboarding

- **Component map**:
  - Pre-training corpus analysis module (co-occurrence counting) -> LAMA dataset processing pipeline (fact representation and probing) -> Model evaluation framework (hits@1, MRR metrics) -> Debiasing pipeline (sample filtering and finetuning) -> Correlation analysis tools (statistical relationship between co-occurrence and accuracy)

- **Critical path**:
  1. Compute co-occurrence statistics from pre-training corpus
  2. Process LAMA-TREx dataset into probing format
  3. Evaluate base model performance on factual knowledge
  4. Analyze correlation between co-occurrence and accuracy
  5. Apply debiasing techniques
  6. Evaluate debiased model performance

- **Design tradeoffs**:
  - Counting co-occurrences at document level vs. sliding window (computational efficiency vs. precision)
  - Restricting output candidates (improves accuracy measurement but may not reflect real-world usage)
  - Filtering ratio selection in debiasing (too aggressive hurts frequent facts, too lenient maintains bias)

- **Failure signatures**:
  - High correlation between co-occurrence counts and accuracy across all model sizes indicates persistent bias
  - Debiased finetuning improving training set performance but not test set generalization
  - Models consistently preferring high co-occurrence words even when incorrect

- **First 3 experiments**:
  1. Reproduce the correlation analysis between co-occurrence counts and factual accuracy on a small subset of the data
  2. Implement a simple debiasing filter and test its effect on a single relation type
  3. Compare term frequency baseline performance against model predictions on held-out facts

## Open Questions the Paper Calls Out

- **Open Question 1**: What mechanisms or architectural changes could effectively mitigate co-occurrence bias in LLMs beyond simple debiasing through undersampling?
- **Open Question 2**: How does co-occurrence bias affect LLMs' performance on downstream tasks beyond factual knowledge probing, such as question answering or summarization?
- **Open Question 3**: What is the relationship between the scale of pre-training data and the severity of co-occurrence bias in LLMs?

## Limitations

- Findings primarily limited to English language models trained on web-scale corpora similar to The Pile
- Analysis focuses on cloze-style factual knowledge probing using LAMA-TREx, which may not fully capture how LLMs use factual knowledge in open-ended generation tasks
- Debiasing experiments show limited generalization, suggesting the need for more sophisticated interventions that address the underlying semantic understanding

## Confidence

**High Confidence (4/5)**: The core finding that LLMs exhibit co-occurrence bias when retrieving factual knowledge is well-supported by multiple experiments across different model sizes and settings.

**Medium Confidence (3/5)**: The claim that scaling and finetuning don't resolve co-occurrence bias is supported by the presented evidence, but the study doesn't test extremely large models (beyond 6B parameters) where the bias might diminish.

**Low Confidence (2/5)**: The assertion that debiased finetuning doesn't generalize to unseen rare facts is based on limited experimental evidence.

## Next Checks

1. Test whether co-occurrence bias affects factual knowledge retrieval in non-cloze formats, such as open-ended question answering or conversational contexts.

2. Implement and evaluate knowledge-aware pretraining or contrastive learning methods to determine if these approaches can overcome co-occurrence bias while maintaining generalization to unseen facts.

3. Evaluate whether co-occurrence bias persists in models trained on non-English corpora or specialized domains (e.g., scientific literature).