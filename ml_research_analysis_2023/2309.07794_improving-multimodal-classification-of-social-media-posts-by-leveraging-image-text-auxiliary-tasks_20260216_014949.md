---
ver: rpa2
title: Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text
  Auxiliary Tasks
arxiv_id: '2309.07794'
source_url: https://arxiv.org/abs/2309.07794
tags:
- text
- image
- multimodal
- pages
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces two auxiliary losses\u2014Image-Text Contrastive\
  \ (ITC) and Image-Text Matching (ITM)\u2014to enhance multimodal classification\
  \ of social media posts by capturing complex cross-modal semantics. ITC minimizes\
  \ the distance between image-text representations within a post while separating\
  \ them from different posts, and ITM penalizes unrelated image-text pairs."
---

# Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary Tasks

## Quick Facts
- arXiv ID: 2309.07794
- Source URL: https://arxiv.org/abs/2309.07794
- Reference count: 25
- Primary result: Auxiliary losses ITC and ITM improve multimodal classification F1 scores by up to 2.6 points

## Executive Summary
This paper introduces two auxiliary losses—Image-Text Contrastive (ITC) and Image-Text Matching (ITM)—to enhance multimodal classification of social media posts by capturing complex cross-modal semantics. ITC minimizes the distance between image-text representations within a post while separating them from different posts, and ITM penalizes unrelated image-text pairs. The authors evaluate these auxiliary tasks with five multimodal models across four social media datasets (TIR, MVSA, MHP, MSD), showing consistent improvements in F1 scores of up to 2.6 points compared to base models. The Ber-ViT-Att model with both ITC and ITM achieved the best overall performance, particularly when text is represented in the image. The analysis reveals that ITC is more effective when images contain relevant visual content, while ITM is better when images do not contribute to post meaning.

## Method Summary
The method introduces two auxiliary losses to existing multimodal models: Image-Text Contrastive (ITC) which brings representations of matching image-text pairs closer while pushing apart non-matching pairs, and Image-Text Matching (ITM) which uses binary classification to distinguish matching from mismatched pairs. These losses are combined with standard cross-entropy loss during fine-tuning of five multimodal models (Ber-ViT-Conc, Ber-ViT-Att, MMBT, LXMERT, ViLT) on four Twitter datasets. The auxiliary losses operate on separate or fused modality embeddings, with ITC using cosine similarity-based contrastive learning and ITM using binary cross-entropy classification. The approach is evaluated on sentiment analysis, sarcasm detection, and hate speech classification tasks using weighted F1 score as the primary metric.

## Key Results
- ITC and ITM auxiliary losses improve F1 scores by up to 2.6 points across all tested multimodal models and datasets
- Ber-ViT-Att with both auxiliary losses achieved the best overall performance (F1=70.37 on MVSA)
- ITC performs better when images contain relevant visual content, while ITM is more effective when images do not contribute to post meaning
- Dual-stream architectures (Ber-ViT-Att) consistently outperform single-stream models (ViLT) when using auxiliary losses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ITC forces the model to learn a unified embedding space where image and text of the same post are close, while cross-post pairs are separated.
- Mechanism: By applying a contrastive loss, ITC encourages the model to recognize underlying dependencies between modalities, even when explicit image-text alignment is weak.
- Core assumption: Hidden cross-modal semantics exist in social media posts and can be captured by pulling representations of matching pairs closer together.
- Evidence anchors:
  - [abstract] "Image-Text Contrastive (ITC) brings image-text representations of a post closer together and separates them from different posts, capturing underlying dependencies."
  - [section] "While the cosine similarity of the pair Ln and In is minimized, the cosine similarity of all other random pairs... is maximized."
- Break condition: If post image-text pairs lack any meaningful relationship, ITC may force artificial alignment without semantic benefit.

### Mechanism 2
- Claim: ITM teaches the model to recognize semantic correspondence between image and text by penalizing unrelated pairs.
- Mechanism: ITM uses a binary classification loss that distinguishes matching vs mismatched image-text pairs during training, enhancing the model's ability to handle ambiguous or loosely related modalities.
- Core assumption: Many social media posts have weakly related image-text pairs, and the model benefits from explicit training on this distinction.
- Evidence anchors:
  - [abstract] "Image-Text Matching (ITM) enhances the model's ability to understand the semantic relationship between images and text, thus improving its capacity to handle ambiguous or loosely related modalities."
  - [section] "ITM involves a binary classification loss that penalizes the model when a given text and image do not appear together in a post."
- Break condition: If most image-text pairs are strongly related, the mismatch penalty may add noise rather than useful signal.

### Mechanism 3
- Claim: Dual-stream architectures (Ber-ViT-Att) combined with auxiliary losses consistently outperform single-stream models (ViLT) for multimodal classification.
- Mechanism: Dual-stream models encode text and image separately before fusing, allowing auxiliary tasks to operate on distinct modality embeddings before combination, capturing complementary features better.
- Core assumption: Separate encoding followed by fusion preserves more modality-specific information that auxiliary tasks can exploit.
- Evidence anchors:
  - [section] "We observe that both ITC and ITM objectives contribute to the performance improvements of Ber-ViT-Att... These findings indicate that dual-stream approaches are effective in leveraging information from image-text auxiliary tasks."
  - [corpus] Weak corpus evidence; most related papers focus on different auxiliary mechanisms rather than architectural comparisons.
- Break condition: If the fusion mechanism is poor or modalities are highly correlated, dual-stream may not provide additional benefit.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: ITC relies on maximizing/minimizing distances between representations in embedding space.
  - Quick check question: What loss function pulls similar pairs together and pushes dissimilar pairs apart in embedding space?

- Concept: Binary classification for alignment
  - Why needed here: ITM uses a binary cross-entropy loss to distinguish matching vs mismatched image-text pairs.
  - Quick check question: How does a binary cross-entropy loss penalize mismatched image-text pairs during training?

- Concept: Dual-stream vs single-stream fusion
  - Why needed here: Understanding why Ber-ViT-Att outperforms ViLT requires knowledge of how modality-specific encoding affects feature combination.
  - Quick check question: What is the key architectural difference between dual-stream and single-stream multimodal models?

## Architecture Onboarding

- Component map: Text encoder (BERT/Bernice) → Image encoder (ViT) → Fusion (concatenation or attention) → Classification head. Auxiliary losses (ITC/ITM) branch from modality or fused embeddings.
- Critical path: Input → Separate modality encoders → Fusion → Main classification loss. Auxiliary losses run in parallel to main loss.
- Design tradeoffs: Dual-stream preserves modality-specific features but adds complexity; single-stream is simpler but may miss modality-specific nuances.
- Failure signatures: Poor performance on TIR suggests weak visual-text correspondence handling; overfitting on small datasets indicates auxiliary loss weighting issues.
- First 3 experiments:
  1. Test ITC alone with Ber-ViT-Conc on MVSA to isolate contrastive effect.
  2. Test ITM alone with Ber-ViT-Att on MHP to isolate alignment effect.
  3. Compare dual-stream (Ber-ViT-Att) vs single-stream (ViLT) with both auxiliary losses on MSD to verify architectural benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ITC and ITM auxiliary losses perform across different language datasets and cultural contexts?
- Basis in paper: [inferred] The paper mentions that all datasets used are in English and acknowledges this as a limitation, noting that the generalizability of findings to other languages is untested.
- Why unresolved: The current study is limited to English datasets, preventing any analysis of cross-linguistic or cross-cultural performance differences in auxiliary task effectiveness.
- What evidence would resolve it: Experimental results comparing ITC and ITM performance across multilingual datasets from diverse cultural contexts would provide insight into whether these auxiliary tasks generalize beyond English social media posts.

### Open Question 2
- Question: What is the optimal balance between ITC and ITM losses for different types of multimodal relationships in social media posts?
- Basis in paper: [explicit] The paper shows that ITC performs better when images contain relevant visual content while ITM is better when images do not contribute to post meaning, suggesting task-specific optimization is needed.
- Why unresolved: The paper uses fixed hyperparameters for λ2 and λ3 across all datasets without exploring whether different ratios of ITC to ITM would be optimal for specific types of multimodal relationships.
- What evidence would resolve it: A systematic study varying the λ2/λ3 ratio across posts with different image-text relationships (complementary, independent, redundant) would identify optimal configurations for each relationship type.

### Open Question 3
- Question: How do ITC and ITM auxiliary losses affect model performance on smaller social media datasets versus larger ones?
- Basis in paper: [explicit] The paper acknowledges that three of four datasets contain fewer than 5,000 examples each and notes this as a limitation, while including MSD with 24,635 examples for comparison.
- Why unresolved: While the paper includes one larger dataset, it doesn't provide a systematic analysis of how auxiliary losses perform across different dataset sizes or whether their effectiveness scales with data volume.
- What evidence would resolve it: Controlled experiments training on datasets of varying sizes (from few hundred to tens of thousands of examples) while measuring the relative improvement from auxiliary losses would reveal their effectiveness across different data scales.

## Limitations
- Small dataset sizes (max 6,308 samples) may limit generalizability of findings
- Focus on Twitter-specific data restricts external validity to other social media platforms
- Lack of comparison with larger foundation models like CLIP that have shown superior cross-modal performance
- Qualitative analysis of when ITC vs ITM performs better lacks systematic measurement

## Confidence

High confidence: The core finding that auxiliary losses improve multimodal classification performance across multiple models and datasets is well-supported by the consistent F1 score improvements (up to 2.6 points) shown in Table 6.

Medium confidence: The claim that Ber-ViT-Att with both auxiliary losses performs best is supported but may be sensitive to hyperparameter tuning and dataset characteristics. The comparative advantage of dual-stream vs single-stream architectures is shown but not extensively explored across different task types.

Low confidence: The qualitative analysis of when ITC vs ITM is more effective (Section 4.3) lacks systematic measurement and could benefit from quantitative metrics of image-text relevance or alignment strength.

## Next Checks

1. Conduct ablation studies with varying auxiliary loss weights to determine optimal scaling factors and test whether improvements are robust across different weight settings.

2. Implement quantitative metrics for image-text alignment (e.g., visual-semantic similarity scores) to systematically validate the qualitative observations about when ITC vs ITM performs better.

3. Test the proposed approach on additional multimodal datasets beyond Twitter (e.g., social media with different content types or multimodal news articles) to assess external validity and generalizability.