---
ver: rpa2
title: Multimodal Attention Merging for Improved Speech Recognition and Audio Event
  Classification
arxiv_id: '2312.14378'
source_url: https://arxiv.org/abs/2312.14378
tags:
- merging
- speech
- attention
- layers
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multimodal Attention Merging (MAM), which
  transfers knowledge from attention matrices of high-resource modality models (e.g.,
  text and images) to low-resource ones (e.g., speech and audio). MAM improves zero-shot
  performance of HuBERT (ASR) and BEATs (Audio Event Classification) by merging attention
  matrices with BERT and Vision Transformer.
---

# Multimodal Attention Merging for Improved Speech Recognition and Audio Event Classification

## Quick Facts
- arXiv ID: 2312.14378
- Source URL: https://arxiv.org/abs/2312.14378
- Reference count: 0
- Key outcome: MAM improves zero-shot ASR WER by up to 6.70% and AEC classification error by 10.63% through attention matrix interpolation from high-resource to low-resource modalities.

## Executive Summary
This paper introduces Multimodal Attention Merging (MAM), a technique that transfers knowledge from attention matrices of high-resource modality models (text and images) to low-resource ones (speech and audio). The approach leverages the observation that attention mechanisms in Transformers capture modality-agnostic sequence representations that can be interpolated to improve performance. Learnable-MAM (L-MAM) extends this by learning the interpolation factor during fine-tuning, achieving up to 2.90% relative WER reduction on ASR and 18.42% classification error reduction on audio classification compared to regular fine-tuning.

## Method Summary
MAM works by interpolating attention matrices (Query, Key, Value) from pre-trained source models (BERT for text, Vision Transformer for images) with target models (HuBERT for speech, BEATs for audio) using a factor λ ∈ [0,1]. The merged model is then fine-tuned on the target task. L-MAM extends this by making λ a learnable parameter optimized during fine-tuning. The method is evaluated on ASR using LJ Speech, VCTK, and AEC using ESC-50 datasets, showing significant improvements over regular fine-tuning baselines.

## Key Results
- MAM reduces ASR WER by up to 6.70% and AEC classification error by 10.63% in zero-shot transfer
- L-MAM achieves 2.70% WER reduction on LJ Speech, 2.90% on VCTK, and 18.42% classification error reduction on ESC-50
- Best fixed λ values are 0.05 for ASR and 0.10 for AEC tasks
- Layer-wise attention interpolation provides additional performance gains by selectively merging attention matrices from specific layers

## Why This Works (Mechanism)

### Mechanism 1
Attention matrices from high-resource modalities contain generalizable sequence representations that transfer to low-resource modalities. The self-attention mechanism learns modality-agnostic contextual dependencies at the token level that can be interpolated with attention matrices from low-resource modality models. Core assumption: self-attention captures fundamental sequence properties transcending specific modalities. Break condition: source and target models have fundamentally different attention layer architectures or modalities are too dissimilar.

### Mechanism 2
Interpolating attention matrices with optimal λ factor improves downstream performance more than using either model alone. Convex combination of Query, Key, and Value matrices from source and target models creates a merged model leveraging strengths of both. Core assumption: optimal interpolation factor λ maximizes benefit of combining attention matrices from different modalities. Break condition: λ too high (dominated by source) or too low (dominated by target), degrading performance below baseline.

### Mechanism 3
Learning interpolation factor λ during fine-tuning (L-MAM) provides additional performance gains over fixed interpolation. λ is treated as learnable parameter optimized during fine-tuning, allowing dynamic adjustment of balance between source and target attention matrices based on downstream task requirements. Core assumption: optimal interpolation factor varies depending on specific downstream task and dataset characteristics. Break condition: inappropriate learning rate for λ or too small dataset to reliably estimate optimal interpolation factor.

## Foundational Learning

- Concept: Self-attention mechanism in Transformers
  - Why needed here: MAM relies on self-attention being modality-agnostic and transferable across different input types (text, images, speech, audio)
  - Quick check question: What are the three main components computed in self-attention mechanism, and how do they interact to produce final output?

- Concept: Knowledge transfer between models
  - Why needed here: MAM fundamentally transfers knowledge from well-trained high-resource modality models to improve low-resource modality models
  - Quick check question: What is difference between zero-shot transfer and fine-tuning-based transfer, and which approach does MAM use in basic form?

- Concept: Model merging and interpolation
  - Why needed here: MAM uses interpolation of attention matrices from different models, and L-MAM extends this by making interpolation factor learnable
  - Quick check question: How does interpolating model parameters differ from ensemble methods, and what are potential advantages of interpolation for attention matrices specifically?

## Architecture Onboarding

- Component map: Source attention matrices → MAM interpolation → Merged attention matrices → Target model processing → Downstream task output
- Critical path: Source attention matrices → MAM layer → Merged attention matrices → Target model → Downstream task head
- Design tradeoffs:
  - Fixed λ vs. learnable λ: Fixed λ provides stability but may not be optimal for all tasks; learnable λ adapts to task but requires more computation and data
  - Full model merging vs. layer-wise merging: Full merging is simpler but may not capture layer-specific importance; layer-wise merging can be more precise but requires more experimentation
  - Zero-shot vs. fine-tuning: Zero-shot is faster and requires no labels but may underperform; fine-tuning improves performance but requires labeled data and computation
- Failure signatures:
  - Performance worse than baseline: Likely issues with λ selection, incompatible model architectures, or insufficient similarity between modalities
  - Slow convergence during fine-tuning: Potential issues with learning rate for λ or model size too large for available compute
  - Mode collapse (merged model performs like one source): λ too extreme (close to 0 or 1) or insufficient diversity in attention patterns between source and target
- First 3 experiments:
  1. Implement basic MAM with fixed λ = 0.1 and evaluate zero-shot performance on small ASR dataset subset
  2. Experiment with different λ values (0.05, 0.1, 0.15, 0.2) to find optimal interpolation factor for ASR task
  3. Implement L-MAM and compare performance against fixed MAM and regular fine-tuning on full ASR dataset

## Open Questions the Paper Calls Out
- Can MAM and L-MAM be effectively extended to merge models with different architectures beyond Transformers?
- Does MAM performance degrade when merging models trained on completely different tasks within same modality?
- What is theoretical limit of model size for MAM to remain effective, and does it scale with billion-parameter models?

## Limitations
- The generality of attention matrix transferability across modalities remains empirically under-validated across diverse modality combinations
- Methodology for determining optimal λ values is not transparent, and λ stability across runs is unreported
- Evaluation scope is limited - paper doesn't explore failure cases or provide ablation studies on individual attention matrix components

## Confidence
- **High Confidence**: Attention matrix interpolation implementation and fixed λ performance improvements are well-specified and reproducible
- **Medium Confidence**: Zero-shot transfer capability shows promise but mechanism for cross-modal sequence representation transfer is not fully explained
- **Low Confidence**: L-MAM superiority over fixed interpolation has lowest confidence due to limited evidence and unreported λ stability

## Next Checks
1. **Architectural Compatibility Analysis**: Systematically test MAM across model pairs with varying attention layer counts and hidden dimensions to quantify impact of architectural mismatches on transfer effectiveness
2. **λ Stability and Transferability Study**: Conduct multiple runs of L-MAM on each dataset to analyze distribution and consistency of learned λ values, testing generalization across similar datasets
3. **Attention Component Ablation**: Perform systematic ablation studies to determine individual contribution of WQ, WK, and WV matrix merging to overall performance, comparing effectiveness of different merging strategies