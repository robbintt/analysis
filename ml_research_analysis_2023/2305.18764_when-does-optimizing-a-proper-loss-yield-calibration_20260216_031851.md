---
ver: rpa2
title: When Does Optimizing a Proper Loss Yield Calibration?
arxiv_id: '2305.18764'
source_url: https://arxiv.org/abs/2305.18764
tags:
- loss
- calibration
- dual
- function
- proper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the relationship between optimizing proper
  loss functions and calibration of predictors. The main question is: under what conditions
  does optimizing a proper loss over a restricted family of predictors yield calibrated
  models?'
---

# When Does Optimizing a Proper Loss Yield Calibration?

## Quick Facts
- arXiv ID: 2305.18764
- Source URL: https://arxiv.org/abs/2305.18764
- Reference count: 40
- Primary result: A predictor is smoothly calibrated if its loss cannot be reduced much by post-processing with Lipschitz functions

## Executive Summary
This paper investigates when optimizing proper loss functions yields calibrated predictors. The key insight is replacing global optimality with a local optimality condition: the loss cannot be reduced significantly by post-processing predictions with Lipschitz functions. The main result establishes a quadratic relationship between the local optimality gap and smooth calibration error. This provides theoretical justification for why deep neural networks trained with proper losses are often well-calibrated in practice.

## Method Summary
The paper introduces a local optimality condition for calibration, where a predictor is considered well-calibrated if its proper loss cannot be reduced significantly by post-processing with a family of Lipschitz functions. This condition replaces the impractical requirement of global optimality. The authors prove that any predictor satisfying this local optimality condition is smoothly calibrated, with a quadratic relationship between the local optimality gap and calibration error. They show this holds for both squared loss and cross-entropy loss, and discuss implications for deep neural network training.

## Key Results
- Any predictor satisfying the local optimality condition is smoothly calibrated
- There is a quadratic relationship between the local optimality gap and smooth calibration error
- Well-trained DNNs are plausibly locally optimal, explaining their calibration from proper loss minimization alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A predictor is smoothly calibrated if its loss cannot be reduced by post-processing with Lipschitz functions.
- Mechanism: Local optimality replaces global optimality. Instead of requiring the predictor to be globally optimal, it only needs to be locally optimal against a restricted class of post-processing functions.
- Core assumption: The post-processing functions form a rich enough family to capture calibration adjustments, but are simple enough to be implemented by models of constant depth.
- Evidence anchors:
  - [abstract] "We replace the global optimality with a local optimality condition stipulating that the (proper) loss of the predictor cannot be reduced much by post-processing its predictions with a certain family of Lipschitz functions."
  - [section 2] "We show that any predictor with this local optimality satisfies smooth calibration as defined in Kakade and Foster (2008); Gopalan et al. (2022b); Błasiok et al. (2023b)."
  - [corpus] Weak evidence - corpus papers focus on calibration methods but don't directly address the local optimality mechanism.
- Break condition: If the post-processing family Kℓ doesn't contain functions that could improve calibration, the condition fails to guarantee calibration.

### Mechanism 2
- Claim: There is a quadratic relationship between the local optimality gap and smooth calibration error.
- Mechanism: The gap between current loss and the minimum achievable loss after post-processing provides a quantitative measure of calibration error.
- Core assumption: The proper loss function induces a well-behaved convex structure that allows this relationship to be established.
- Evidence anchors:
  - [abstract] "We prove that there is a quadratic relationship between the local optimality gap and the smooth calibration error."
  - [section 2] "For the squared loss, we present a tight connection between smooth calibration error and the reduction in loss that is possible from post-processing with Kℓ."
  - [corpus] No direct evidence in corpus - papers focus on calibration methods but not this specific quadratic relationship.
- Break condition: If the loss function doesn't induce the required convex structure, the quadratic relationship breaks down.

### Mechanism 3
- Claim: Deep neural networks trained with proper loss minimization are often calibrated because they can implement rich post-processing functions.
- Mechanism: DNNs can represent a rich family of Lipschitz post-processing functions with small added depth, so if a better post-processing existed, the network could have implemented it during training.
- Core assumption: SGD on DNNs implicitly optimizes over a function family closed under Lipschitz post-processing, or would have found improvements if they existed.
- Evidence anchors:
  - [abstract] "Local optimality is plausibly satisfied by well-trained DNNs, which suggests an explanation for why they are calibrated from proper loss minimization alone."
  - [section 1.1] "Poor calibration implies that the test loss can be reduced noticeably by post-processing with a simple function. But simple post-processings can be represented by adding a few layers to the DNN."
  - [corpus] Weak evidence - corpus papers mention calibration in DNNs but don't establish this specific mechanism.
- Break condition: If the DNN architecture cannot represent the needed post-processing functions, or if SGD doesn't explore the parameter space adequately, this mechanism fails.

## Foundational Learning

- Concept: Proper loss functions
  - Why needed here: The entire analysis relies on the properties of proper losses, which ensure the ground truth minimizes expected loss
  - Quick check question: Can you verify that both squared loss and cross-entropy loss are proper losses?

- Concept: Calibration measures
  - Why needed here: The work defines and uses smooth calibration as the target property, which is more robust than standard ECE
  - Quick check question: What is the key advantage of smooth calibration over Expected Calibration Error (ECE)?

- Concept: Bregman divergences and convex duality
  - Why needed here: The extension to general proper losses relies on the correspondence between proper losses and convex functions
  - Quick check question: How does the dual prediction concept relate to the original prediction in the cross-entropy loss case?

## Architecture Onboarding

- Component map: Proper loss function ℓ -> Post-processing family Kℓ -> Distribution D -> Predictor f -> Calibration measure
- Critical path: Compute loss → find minimum over Kℓ → compare to original loss → derive calibration error bound
- Design tradeoffs: Wider Kℓ families give stronger calibration guarantees but may be harder to implement; narrower families are easier but provide weaker guarantees
- Failure signatures: If pGapD(f) is large, the predictor is far from calibrated; if optimization gets stuck in poor local minima, the mechanism fails
- First 3 experiments:
  1. Verify the quadratic relationship on a simple synthetic distribution with known optimal predictor
  2. Test the mechanism on logistic regression with different feature families to see when calibration fails
  3. Apply to a small DNN trained with cross-entropy and measure both pGap and smooth calibration error

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we formally prove that state-of-the-art deep neural networks trained with SGD on natural distributions will yield calibrated predictors?
- Basis in paper: [inferred] The paper discusses informal intuitions about why DNNs might be calibrated but acknowledges these are heuristic and speculative.
- Why unresolved: This requires formalizing what "DNN," "SGD," and "natural distributions" mean in practice, which is a known definitional barrier in deep learning theory.
- What evidence would resolve it: A rigorous mathematical proof showing that under specific formal definitions of DNN architectures, SGD optimization, and natural data distributions, the resulting models satisfy the local optimality condition for calibration.

### Open Question 2
- Question: Does the local optimality condition for calibration extend beyond binary classification to multi-class settings?
- Basis in paper: [inferred] The authors mention they "intuitively expect the results from our work to generalize beyond the binary label setting" but leave this to future work.
- Why unresolved: The paper focuses on binary classification and the extension to multi-class requires new mathematical formulations and proofs.
- What evidence would resolve it: A formal theorem extending the main results to multi-class classification, with appropriate definitions of proper losses and calibration measures for the multi-class case.

### Open Question 3
- Question: Can we identify the exact form of the complexity regularizer that implicitly governs generalization in deep learning?
- Basis in paper: [explicit] The paper discusses how their results connect to the implicit regularization hypothesis in deep learning, but notes "The exact form of this complexity regularizer remains elusive."
- Why unresolved: Despite extensive research, the specific mathematical form of the implicit regularizer in deep learning remains unknown.
- What evidence would resolve it: A formal proof that SGD on neural networks with specific architectures and training procedures implicitly performs minimization on a well-defined complexity-regularized objective, where the regularizer satisfies the mild assumptions needed for calibration.

## Limitations
- The quadratic relationship is only rigorously proven for squared loss
- The practical implications depend on DNNs being able to implement the required post-processing functions
- The exact form of the complexity regularizer governing generalization remains unknown

## Confidence
- Overall mechanism connecting local optimality to calibration: Medium
- DNN explanation for calibration: Medium

## Next Checks
1. Test the quadratic relationship between pGap and smooth calibration error on synthetic distributions with different proper loss functions
2. Measure whether well-trained DNNs actually achieve small pGap values on benchmark datasets
3. Verify that the post-processing function families Kℓ can be implemented by realistic DNN architectures with reasonable depth increases