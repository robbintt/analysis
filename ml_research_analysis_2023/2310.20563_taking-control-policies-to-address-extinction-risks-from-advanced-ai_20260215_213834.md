---
ver: rpa2
title: 'Taking control: Policies to address extinction risks from advanced AI'
arxiv_id: '2310.20563'
source_url: https://arxiv.org/abs/2310.20563
tags:
- risks
- advanced
- extinction
- would
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that advanced AI poses an extinction risk to humanity,
  primarily due to "scaling" - building increasingly large, autonomous, and opaque
  AI systems. It contends that voluntary commitments from AI companies (e.g., "Responsible
  Scaling Policies") are inadequate to address these risks.
---

# Taking control: Policies to address extinction risks from advanced AI

## Quick Facts
- arXiv ID: 2310.20563
- Source URL: https://arxiv.org/abs/2310.20563
- Reference count: 4
- Primary result: The paper proposes three key policy recommendations to address extinction risks from advanced AI: establishing MAGIC, implementing a global compute cap, and requiring affirmative safety evaluations before high-risk experiments.

## Executive Summary
The paper argues that advanced AI poses an extinction risk to humanity, primarily due to the scaling of increasingly large, autonomous, and opaque AI systems. It contends that voluntary commitments from AI companies are inadequate to address these risks. Instead, the authors propose three key policy recommendations: establishing a secure, internationally-governed "Multinational AGI Consortium" (MAGIC) to oversee advanced AI development and conduct safety research, implementing a global cap on computing power used to train AI systems, and requiring AI developers to provide affirmative evidence of safety before undertaking high-risk experiments. The proposals aim to enable democratic oversight of advanced AI while preventing a dangerous race toward superintelligent systems. Public polling in the US and UK shows strong support for AI regulation to mitigate these risks.

## Method Summary
The paper provides background on extinction risks from AI, argues against voluntary commitments, and describes three policy proposals: (1) establishing MAGIC, a secure, safety-focused, internationally-governed institution responsible for reducing risks from advanced AI and performing research to safely harness the benefits of AI; (2) implementing a global compute cap to end the corporate race toward dangerous AI systems while enabling the vast majority of AI innovation to continue unimpeded; and (3) requiring AI developers to present affirmative evidence that their models keep extinction risks below an acceptable threshold before undertaking high-risk experiments. The paper outlines the need for urgent international coordination to implement these proposals, including national compute caps, emergency response infrastructure, and a timeline for regulation.

## Key Results
- Advanced AI poses an extinction risk primarily due to scaling of large, autonomous systems
- Voluntary commitments from AI companies are inadequate to address these risks
- Public polling in the US and UK shows strong support for AI regulation to mitigate these risks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAGIC as a secure, internationally-governed institution can effectively control advanced AI development and reduce extinction risks.
- Mechanism: By centralizing advanced AI research in a secure facility with democratic oversight, MAGIC prevents a dangerous race among private companies while enabling focused safety research.
- Core assumption: International cooperation can be achieved and maintained to support a single, secure AGI consortium.
- Evidence anchors:
  - [abstract] MAGIC would be a secure, safety-focused, internationally-governed institution responsible for reducing risks from advanced AI and performing research to safely harness the benefits of AI.
  - [section 4.1] MAGIC would be the world's unified AGI safety project. MAGIC would hire talented researchers from around the world to perform research on how to control highly powerful AI systems and develop additional measures to reduce extinction risks from advanced AI.
  - [corpus] Found 25 related papers with average FMR=0.386. One directly relevant paper is "Multinational AGI Consortium (MAGIC): A Proposal for International Coordination on AI".
- Break condition: International cooperation fails, MAGIC becomes compromised, or it cannot attract top talent.

### Mechanism 2
- Claim: A global compute cap can effectively throttle the development of dangerous AI systems while allowing most AI innovation to continue.
- Mechanism: By limiting the amount of computing power available for AI training, the cap restricts the most dangerous AI systems (which require massive compute) while leaving the vast majority of AI applications unaffected.
- Core assumption: Compute is a bottleneck for dangerous AI systems, and this bottleneck can be effectively monitored and controlled.
- Evidence anchors:
  - [abstract] The global compute cap would end the corporate race toward dangerous AI systems while enabling the vast majority of AI innovation to continue unimpeded.
  - [section 4.2] Compute Limitations address this bottleneck by enforcing a cap on the hardware used to build these AI systems. This mitigates extinction risk by throttling the very few dangerous AI systems that rely on advanced hardware, while leaving the rest of the AI ecosystem largely unhindered.
  - [corpus] Weak evidence - no direct corpus support for compute cap effectiveness found.
- Break condition: New algorithms make AI more powerful without requiring additional compute, or monitoring/compliance mechanisms fail.

### Mechanism 3
- Claim: Gating critical experiments with affirmative safety evaluations ensures companies prove their systems are safe before development.
- Mechanism: By requiring companies to demonstrate affirmative evidence of safety before developing high-risk AI systems, this approach places the burden of proof on developers rather than waiting for dangerous capabilities to emerge.
- Core assumption: It's possible to develop reliable safety evaluations and that companies can provide affirmative evidence of safety.
- Evidence anchors:
  - [abstract] Gating critical experiments would ensure that companies developing powerful AI systems are required to present affirmative evidence that these models keep extinction risks below an acceptable threshold.
  - [section 4.3] Gating critical experiments places the burden of proof on Frontier AI developers to show that their practices keep risks below an acceptable level. This contrasts with the RSP approach, highlighted earlier.
  - [corpus] Weak evidence - no direct corpus support for effectiveness of affirmative safety evaluations found.
- Break condition: Safety evaluations prove inadequate or companies cannot provide convincing affirmative evidence.

## Foundational Learning

- Concept: Extinction risks from advanced AI
  - Why needed here: The entire policy framework is designed to address these specific risks, so understanding what they are and why they matter is foundational.
  - Quick check question: What does the paper identify as the primary source of extinction risk from AI?

- Concept: Compute as a bottleneck for AI scaling
  - Why needed here: The global compute cap proposal relies on this concept, so understanding why compute matters for dangerous AI systems is essential.
  - Quick check question: Why does the paper argue that limiting compute can effectively reduce extinction risks while allowing most AI innovation to continue?

- Concept: Affirmative safety evaluations
  - Why needed here: The gating critical experiments mechanism depends on this concept, so understanding how it differs from other approaches is important.
  - Quick check question: How does the affirmative safety evaluation approach differ from the RSP framework in terms of burden of proof?

## Architecture Onboarding

- Component map:
  - MAGIC (Multinational AGI Consortium)
  - Global compute cap system
  - National/International regulatory framework
  - Emergency response infrastructure
  - Safety evaluation protocols
  - Compute monitoring systems
  - International treaty mechanisms

- Critical path:
  1. Establish national compute cap
  2. Develop emergency response infrastructure
  3. Draft international treaty
  4. Establish MAGIC
  5. Implement global compute cap
  6. Set up national regulatory frameworks
  7. Develop safety evaluation protocols

- Design tradeoffs:
  - Compute cap level: Too low delays safe innovation; too high allows dangerous systems
  - MAGIC location: Security vs. accessibility for researchers
  - Safety evaluation stringency: False positives vs. false negatives
  - International enforcement: Sovereignty concerns vs. effectiveness

- Failure signatures:
  - Compute cap evasion through distributed training
  - MAGIC compromised by internal threats
  - Safety evaluations missing emergent dangerous capabilities
  - International cooperation breakdown

- First 3 experiments:
  1. Simulate compute monitoring system with test data centers
  2. Run emergency response drill with cooperating AI company
  3. Test safety evaluation protocols on existing AI systems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop and implement reliable dangerous capability evaluations that can accurately identify potentially harmful AI systems before they are deployed?
- Basis in paper: [explicit] The paper highlights the inadequacy of current dangerous capability evaluations used in Responsible Scaling Policies, stating that they rely on incomplete lists of scenarios and capabilities, and lack proven accuracy.
- Why unresolved: Developing comprehensive and reliable evaluations is challenging due to the complexity and evolving nature of AI capabilities. There is no established framework for identifying all potential dangers, and new capabilities can emerge after deployment.
- What evidence would resolve it: Developing a comprehensive taxonomy of potential AI dangers, creating robust testing methodologies that can reliably detect these dangers, and conducting extensive testing and validation of evaluation frameworks.

### Open Question 2
- Question: What are the specific technical and political challenges in establishing and maintaining the Multinational AGI Consortium (MAGIC) as a secure, internationally-governed institution?
- Basis in paper: [explicit] The paper proposes MAGIC as a solution but acknowledges the need for national-security grade standards and international coordination, implying significant challenges in its establishment and operation.
- Why unresolved: Establishing MAGIC requires navigating complex international relations, securing buy-in from major AI powers, defining its mandate and governance structure, and ensuring its operational security and independence.
- What evidence would resolve it: Successful negotiation and ratification of international treaties establishing MAGIC, demonstration of its ability to attract and retain top AI talent, and evidence of its effectiveness in overseeing and regulating advanced AI development.

### Open Question 3
- Question: What is the optimal threshold for the global compute cap, and how should it be adjusted over time to account for advancements in AI algorithms and hardware efficiency?
- Basis in paper: [explicit] The paper recommends a moratorium threshold of 10^24 FLOP but acknowledges the need for MAGIC to regularly update this threshold based on AI progress.
- Why unresolved: Determining the optimal threshold requires balancing the need to prevent dangerous AI development with the desire to enable beneficial AI innovation. The pace of AI progress and the emergence of new algorithms make it difficult to set a static threshold.
- What evidence would resolve it: Analysis of the relationship between compute power and AI capabilities, expert consensus on the threshold beyond which AI systems pose significant risks, and the development of metrics to assess the effectiveness of the compute cap in mitigating risks.

## Limitations
- The absence of direct empirical evidence supporting compute caps as effective risk reduction tools
- Unresolved questions about how MAGIC would maintain security while fostering innovation
- Significant political economy challenges in implementing these proposals globally

## Confidence
- Mechanism 1 (MAGIC): Medium-high theoretical effectiveness, low confidence in international cooperation feasibility
- Mechanism 2 (Compute cap): Medium confidence in theoretical effectiveness, significant implementation challenges
- Mechanism 3 (Affirmative safety): Medium confidence in conceptual soundness, low confidence in current evaluation science

## Next Checks
1. Conduct a formal game-theoretic analysis of compute cap enforcement mechanisms under different international cooperation scenarios
2. Perform a technical audit of current safety evaluation methodologies to identify their detection limits for potential existential risks
3. Develop a pilot compute monitoring system using existing data center infrastructure to test feasibility of enforcement mechanisms