---
ver: rpa2
title: Most Neural Networks Are Almost Learnable
arxiv_id: '2305.16508'
source_url: https://arxiv.org/abs/2305.16508
tags:
- networks
- learning
- lemma
- neural
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that random constant-depth neural networks are
  efficiently learnable in polynomial time, providing the first polynomial-time approximation
  scheme (PTAS) for networks deeper than two layers. The key insight is that such
  networks can be well-approximated by low-degree polynomials using Hermite polynomial
  approximations of the activation functions.
---

# Most Neural Networks Are Almost Learnable

## Quick Facts
- arXiv ID: 2305.16508
- Source URL: https://arxiv.org/abs/2305.16508
- Reference count: 34
- Primary result: Random constant-depth neural networks are efficiently learnable in polynomial time

## Executive Summary
This paper establishes that random constant-depth neural networks are efficiently learnable in polynomial time, providing the first polynomial-time approximation scheme (PTAS) for networks deeper than two layers. The key insight is that such networks can be well-approximated by low-degree polynomials using Hermite polynomial approximations of the activation functions. For Lipschitz activations, the algorithm runs in time (d)poly(ε⁻¹) where d is network size and ε is desired accuracy. For specific activations like sigmoid and ReLU-like functions, this improves to (d)polylog(ε⁻¹), yielding a quasi-polynomial time algorithm. The result holds for any input distribution on the unit sphere, demonstrating that standard SGD on ReLU networks can be used as a PTAS for learning these random networks.

## Method Summary
The paper constructs a polynomial approximation of random neural networks by replacing each activation function with its Hermite polynomial approximation, creating a "shadow network." This shadow network is then learned using standard polynomial regression algorithms. For Xavier-initialized networks with Lipschitz activations, the approximation error decreases sufficiently fast with polynomial degree, enabling efficient learning. The approach shows that standard SGD on ReLU networks initialized with Xavier initialization serves as a practical PTAS implementation.

## Key Results
- Random constant-depth neural networks can be learned in polynomial time
- The algorithm achieves (d)poly(ε⁻¹) time complexity for Lipschitz activations
- For sigmoid and ReLU-like activations, the complexity improves to (d)polylog(ε⁻¹)
- Standard SGD on ReLU networks serves as an implementable PTAS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constant-depth random neural networks can be efficiently learned because their outputs can be well-approximated by low-degree polynomials.
- Mechanism: The paper replaces each activation function with its polynomial approximation using Hermite polynomials, creating a "shadow network" that approximates the original network. Since constant-degree polynomials are efficiently learnable, this allows for polynomial-time learning algorithms.
- Core assumption: The activation functions have good polynomial approximations, specifically that the error ϵσ(n) decreases sufficiently fast with increasing polynomial degree n.
- Evidence anchors:
  - [abstract] "The key insight is that such networks can be well-approximated by low-degree polynomials using Hermite polynomial approximations of the activation functions."
  - [section] "The main technical idea in our work is that constant-depth random neural networks with Lipschitz activations can be approximated sufficiently well by low-degree polynomials."
- Break condition: If the activation functions cannot be well-approximated by low-degree polynomials, or if the polynomial approximation error doesn't decrease fast enough with degree, this mechanism would fail.

### Mechanism 2
- Claim: The standard SGD algorithm on ReLU networks can be used as a polynomial-time approximation scheme (PTAS) for learning these random networks.
- Mechanism: Since the shadow network (with polynomial activations) approximates the original network well, and SGD can efficiently learn polynomials with polynomially-bounded coefficients, running SGD on a ReLU network initialized with Xavier initialization serves as a PTAS.
- Core assumption: SGD on ReLU networks can efficiently learn the polynomial approximation of the random network's output.
- Evidence anchors:
  - [abstract] "The approach demonstrates that standard SGD on ReLU networks can be used as a PTAS for learning these random networks."
  - [section] "Thus, those PTAS can be standard SGD on neural networks."
- Break condition: If SGD on ReLU networks doesn't converge efficiently to the polynomial approximation, or if the polynomial coefficients aren't bounded, this mechanism would fail.

### Mechanism 3
- Claim: The Xavier initialization scheme ensures that the scale across the network remains consistent, which is crucial for the polynomial approximation to work.
- Mechanism: By initializing weights with Xavier matrices (each entry is a centered Gaussian with variance 1/d_j-1), the second moment of neuron outputs remains 1, maintaining consistent scaling throughout the network. This consistent scaling is necessary for the polynomial approximation error to be bounded.
- Core assumption: The Xavier initialization maintains consistent scaling across network depth.
- Evidence anchors:
  - [section] "This choice is motivated by the fact that it is a standard practice to initialize the network's weights with Xavier matrices, and furthermore, it ensures that the scale across the network is the same."
- Break condition: If the initialization doesn't maintain consistent scaling, or if the scaling varies significantly across layers, the polynomial approximation may not hold.

## Foundational Learning

- Concept: Polynomial approximation of functions
  - Why needed here: The paper's core insight relies on approximating activation functions with polynomials to make the network learnable.
  - Quick check question: Why is it important that the polynomial approximation error decreases with increasing degree?

- Concept: Hermite polynomials and their properties
  - Why needed here: Hermite polynomials are used specifically for approximating activation functions in this work.
  - Quick check question: What property of Hermite polynomials makes them suitable for approximating functions with respect to Gaussian measure?

- Concept: Statistical query (SQ) learning model
  - Why needed here: The paper discusses hardness results in the SQ model for learning neural networks.
  - Quick check question: How does the SQ model relate to the computational complexity of learning neural networks?

## Architecture Onboarding

- Component map:
  Input → Xavier weights → Activation → Output
  Input → Xavier weights → Polynomial activation → Shadow network output

- Critical path: Input → Xavier weights → Activation → Output
  The critical path for learning is: Polynomial approximation of activations → Shadow network creation → Efficient learning algorithm (SGD)

- Design tradeoffs:
  - Depth vs. learnability: Shallower networks are easier to learn but may have less representational power
  - Polynomial degree vs. approximation quality: Higher degree gives better approximation but may increase computational complexity
  - Initialization scheme: Xavier initialization is crucial for maintaining consistent scaling

- Failure signatures:
  - Poor polynomial approximation of activations (high ϵσ(n))
  - Inconsistent scaling across network layers
  - SGD not converging to the polynomial approximation

- First 3 experiments:
  1. Verify polynomial approximation quality: Compute ϵσ(n) for different polynomial degrees and activation functions
  2. Test scaling consistency: Measure variance of neuron outputs across different layers with Xavier initialization
  3. Validate learning efficiency: Compare learning curves of SGD on original vs. shadow network for small random networks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we achieve polynomial time and sample complexity in terms of 1/ε for random networks of any constant depth (not just depth 2)?
- Basis in paper: [explicit] The paper notes this as a central open question arising from their work, stating "achieving time and sample complexity of (¯d)O(ε−2) for random networks of any constant depth (and not only for depth two)" as a direction for future work.
- Why unresolved: Current PTAS has complexity (¯d)poly(ε⁻¹) which is polynomial only for constant ε, not for ε approaching 0.
- What evidence would resolve it: A PTAS algorithm with running time and sample complexity polynomial in both 1/ε and the network size ¯d for any constant depth.

### Open Question 2
- Question: Can we develop a PTAS for random convolutional networks?
- Basis in paper: [explicit] The paper explicitly mentions "the analysis of random convolutional networks" as a natural next step and open direction.
- Why unresolved: The current work only handles fully connected networks, and convolutional networks have additional structure and weight sharing that complicates the analysis.
- What evidence would resolve it: A polynomial-time algorithm that can learn random convolutional networks up to any desired constant accuracy.

### Open Question 3
- Question: Can we find a PTAS for random networks of depth ω(1)?
- Basis in paper: [explicit] The paper lists "finding a PTAS for random networks of depth ω(1)" as an open direction.
- Why unresolved: The current work only handles constant depth networks, and as depth increases, the polynomial approximation techniques may become insufficient.
- What evidence would resolve it: A polynomial-time algorithm that can learn random networks of super-constant depth up to any desired constant accuracy.

## Limitations
- The polynomial-time approximation scheme requires choosing polynomial degree n based on ε, which may be impractically high for very small ε
- The analysis assumes Xavier initialization and input distributions on the unit sphere, limiting generalizability
- While the shadow network approximation works theoretically, practical convergence of SGD on ReLU networks to this approximation is not fully characterized

## Confidence
- **High confidence**: The polynomial approximation mechanism for Lipschitz activations using Hermite polynomials
- **Medium confidence**: The quasi-polynomial time result for sigmoid and ReLU-like activations
- **Medium confidence**: The claim that standard SGD on ReLU networks serves as a PTAS

## Next Checks
1. Implement empirical validation of the polynomial approximation quality for different activation functions across varying network depths, measuring actual approximation error versus theoretical bounds
2. Test SGD convergence on small random networks with Xavier initialization, comparing learning curves against the shadow network predictions
3. Analyze the scaling behavior of the required polynomial degree n as ε approaches zero for different activation functions