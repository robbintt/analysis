---
ver: rpa2
title: 'AdapterDistillation: Non-Destructive Task Composition with Knowledge Distillation'
arxiv_id: '2312.16261'
source_url: https://arxiv.org/abs/2312.16261
tags:
- adapter
- adapters
- knowledge
- task
- adapterdistillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdapterDistillation, a two-stage knowledge
  distillation algorithm for efficient multi-tenant learning in transformer models.
  The method trains task-specific adapters on local data, then distills knowledge
  from existing teacher adapters into the student adapter without adding fusion layers
  during inference.
---

# AdapterDistillation: Non-Destructive Task Composition with Knowledge Distillation

## Quick Facts
- arXiv ID: 2312.16261
- Source URL: https://arxiv.org/abs/2312.16261
- Reference count: 15
- Outperforms AdapterFusion with 86.04% average accuracy vs 85.85% while using only 1.45% additional parameters per task

## Executive Summary
AdapterDistillation introduces a two-stage knowledge distillation algorithm for efficient multi-tenant learning in transformer models. The method trains task-specific adapters on local data, then distills knowledge from existing teacher adapters into the student adapter without adding fusion layers during inference. This approach maintains low resource consumption and fast inference time while outperforming AdapterFusion in accuracy. Experiments on FAQ retrieval tasks show AdapterDistillation achieves 86.04% average accuracy with only 1.45% additional parameters per task, compared to 85.85% accuracy with 21.36% parameters for AdapterFusion. The method also supports 20 times more tenants than AdapterFusion under the same storage constraints, demonstrating significant improvements in scalability and efficiency for practical multi-tenant applications.

## Method Summary
AdapterDistillation is a two-stage knowledge distillation algorithm for efficient multi-tenant learning in transformer models. In stage 1, a student adapter is trained on local task data to capture task-specific knowledge. In stage 2, knowledge is distilled from existing teacher adapters into the student adapter using a fusion layer only during training. The student adapter learns to combine task-specific and transferred knowledge, making the fusion layer unnecessary during inference. This eliminates the need for fusion layers during inference, reducing parameter count while maintaining or improving accuracy through effective knowledge transfer. The method uses Bert-base-Chinese as the pre-trained model and adapter modules for task-specific parameters.

## Key Results
- AdapterDistillation achieves 86.04% average accuracy on FAQ retrieval tasks vs 85.85% for AdapterFusion
- Uses only 1.45% additional parameters per task compared to 21.36% for AdapterFusion
- Supports 20 times more tenants than AdapterFusion under the same storage constraints
- Maintains low resource consumption and fast inference time while outperforming existing algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AdapterDistillation enables efficient knowledge transfer between tasks without adding fusion layers during inference
- Mechanism: The method trains a student adapter on local data first, then distills knowledge from existing teacher adapters into the student adapter using a fusion layer only during training. The student adapter learns to combine task-specific and transferred knowledge, making the fusion layer unnecessary during inference.
- Core assumption: The student adapter's weights, after knowledge distillation, already contain the combined knowledge from multiple tasks
- Evidence anchors:
  - [abstract]: "We distill the knowledge from the existing teacher adapters into the student adapter to help its inference."
  - [section]: "Since the adapter structure is consistent during prediction and no additional parameters are required, the scalability and low-resource nature of the model itself are retained."
  - [corpus]: Weak evidence - only 8 related papers found, with average neighbor FMR=0.403, suggesting limited direct research on this specific mechanism

### Mechanism 2
- Claim: The two-stage training process improves adapter performance while maintaining efficiency
- Mechanism: Stage 1 trains the student adapter on local task data to capture task-specific knowledge. Stage 2 uses knowledge distillation from existing teacher adapters to enhance the student adapter's capabilities without adding parameters during inference.
- Core assumption: The distillation process can effectively transfer useful knowledge from multiple teacher adapters to the student adapter
- Evidence anchors:
  - [abstract]: "In the first stage, we extract task specific knowledge by using local data to train a student adapter. In the second stage, we distill the knowledge from the existing teacher adapters into the student adapter to help its inference."
  - [section]: "We employ knowledge distillation to transfer knowledge from the existing tasks to the new adapter, which means the parameter weight of this new adapter will be updated in the second stage."
  - [corpus]: Weak evidence - no direct citations found, suggesting this specific two-stage approach may be novel

### Mechanism 3
- Claim: AdapterDistillation achieves better accuracy than AdapterFusion with significantly fewer parameters
- Mechanism: By distilling knowledge into the student adapter during training, AdapterDistillation eliminates the need for a fusion layer during inference, reducing parameter count while maintaining or improving accuracy through effective knowledge transfer.
- Core assumption: The distilled knowledge in the student adapter is sufficient to match or exceed the performance of a separate fusion layer
- Evidence anchors:
  - [abstract]: "We show that AdapterDistillation outperforms existing algorithms in terms of accuracy, resource consumption and inference time."
  - [section]: "Interestingly enough, in the N teacher adapters, we not only use the previous (N − 1) fully trained adapters ϕn as teacher adapters to enable sharing of information between different tasks but also add the N-th partially trained adapter ϕf irst N obtained in the first stage as a teacher adapter to insert some task specific knowledge."
  - [corpus]: Weak evidence - only 8 related papers found, suggesting limited direct research on this specific comparison

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: AdapterDistillation uses knowledge distillation to transfer knowledge from teacher adapters to the student adapter
  - Quick check question: What is the primary purpose of using knowledge distillation in AdapterDistillation?

- Concept: Adapter modules
  - Why needed here: Adapters are the core mechanism for adding task-specific parameters to transformer layers in this approach
  - Quick check question: How do adapter modules differ from full fine-tuning in terms of parameter efficiency?

- Concept: Multi-task learning vs. task-specific learning
  - Why needed here: The paper contrasts AdapterDistillation with traditional multi-task learning approaches that may interfere with existing tasks
  - Quick check question: What is the key advantage of AdapterDistillation over multi-task learning in a streaming tenant scenario?

## Architecture Onboarding

- Component map:
  Pre-trained BERT model (frozen during training) -> Student adapter (trained on local task data in stage 1) -> Teacher adapters (existing task adapters used for knowledge distillation) -> Fusion layer (used only during training for knowledge distillation) -> Query/Key/Value matrices (used for adapter fusion during training)

- Critical path: Local task data → Student adapter training (stage 1) → Knowledge distillation from teacher adapters (stage 2) → Inference using only student adapter
- Design tradeoffs: Reduced inference time and parameter count vs. additional training complexity; potential performance vs. simplicity of separate adapters
- Failure signatures: Decreased accuracy compared to baseline adapters; increased inference time (indicating fusion layer wasn't properly removed); memory issues during training
- First 3 experiments:
  1. Train student adapter on local data only (stage 1) and evaluate baseline performance
  2. Add knowledge distillation from one teacher adapter and compare performance
  3. Test with multiple teacher adapters and measure accuracy vs. parameter count tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AdapterDistillation vary with different numbers of teacher adapters and what is the optimal number of teacher adapters for different task domains?
- Basis in paper: [inferred] The paper mentions using 9 existing tenant models as teacher adapters and evaluating performance on 8 different student tenants from various domains, but does not systematically explore the impact of varying the number of teacher adapters.
- Why unresolved: The paper does not provide an analysis of how the number of teacher adapters affects performance across different domains, which is crucial for understanding the scalability and adaptability of the method.
- What evidence would resolve it: A systematic study varying the number of teacher adapters and evaluating performance across different domains would provide insights into the optimal configuration for different scenarios.

### Open Question 2
- Question: What is the impact of the order in which tasks are added on the performance of AdapterDistillation?
- Basis in paper: [inferred] The paper assumes that tasks are added in a streaming manner, but does not investigate how the order of task addition affects the final performance of the model.
- Why unresolved: The assumption of streaming task addition is made, but the effect of different orderings on the model's ability to learn and retain knowledge from previous tasks is not explored.
- What evidence would resolve it: An experimental study varying the order of task addition and measuring the performance of AdapterDistillation would clarify the importance of task order in the learning process.

### Open Question 3
- Question: How does the choice of adapter structure (e.g., Lora, MAD-X) affect the performance of AdapterDistillation?
- Basis in paper: [explicit] The paper mentions that AdapterDistillation is independent of the adapter structure and can be combined with various adapter methods like Lora and MAD-X, but does not provide empirical evidence comparing different adapter structures.
- Why unresolved: While the paper claims compatibility with different adapter structures, it does not empirically demonstrate the performance differences when using different adapter types within the AdapterDistillation framework.
- What evidence would resolve it: Comparative experiments using different adapter structures within AdapterDistillation and measuring their impact on performance would provide concrete evidence of the best adapter choice for this method.

## Limitations

- Evaluation scope limited to FAQ retrieval tasks in Chinese using Bert-base-Chinese
- Small sample size with only 8 student tenants tested
- Assumes existing teacher adapters are well-trained and beneficial without analyzing scenarios with conflicting or low-quality teacher knowledge

## Confidence

- High confidence: The core mechanism of using knowledge distillation to transfer knowledge from teacher to student adapters without requiring fusion layers during inference is technically sound and well-supported by the experimental setup.
- Medium confidence: The claimed superiority over AdapterFusion in terms of accuracy, parameter efficiency, and scalability is supported by the experimental results, but limited by the narrow evaluation scope and small sample size.
- Low confidence: The scalability claim of supporting 20 times more tenants under the same storage constraints is based on parameter count analysis but lacks experimental validation with actual deployment scenarios involving many tenants.

## Next Checks

1. Cross-domain validation: Test AdapterDistillation on non-FAQ tasks (e.g., text classification, sentiment analysis) and with multilingual models to assess generalizability beyond the Chinese FAQ retrieval domain.

2. Teacher quality analysis: Systematically evaluate how varying the quality, relevance, and number of teacher adapters affects student adapter performance, including scenarios with conflicting knowledge.

3. Large-scale scalability test: Implement a prototype system with 100+ tenants to empirically verify the claimed 20x improvement in tenant capacity under realistic storage constraints and measure actual inference latency improvements.