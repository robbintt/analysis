---
ver: rpa2
title: 'CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion'
arxiv_id: '2303.11916'
source_url: https://arxiv.org/abs/2303.11916
tags:
- image
- text
- dataset
- compodiff
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CompoDiff, a diffusion-based model for solving
  Composed Image Retrieval (CIR) with latent diffusion, and presents a new synthetic
  dataset named SynthTriplets18M. SynthTriplets18M contains 18.8 million reference
  images, conditions, and corresponding target image triplets, which is over 500 times
  larger than existing CIR datasets.
---

# CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion

## Quick Facts
- arXiv ID: 2303.11916
- Source URL: https://arxiv.org/abs/2303.11916
- Reference count: 40
- Key outcome: CompoDiff achieves new state-of-the-art zero-shot performance on four CIR benchmarks (FashionIQ, CIRR, CIRCO, GeneCIS) using a massive synthetic dataset 500x larger than existing CIR datasets

## Executive Summary
CompoDiff introduces a diffusion-based approach to Composed Image Retrieval (CIR) that leverages latent diffusion models and a massive synthetic dataset (SynthTriplets18M) containing 18.8 million reference-target image triplets. The model achieves state-of-the-art zero-shot performance across four CIR benchmarks by training on synthetic data generated through Stable Diffusion and language models. CompoDiff uniquely enables versatile conditioning through classifier-free guidance, allowing flexible control over condition strength between image and text queries, and supporting complex conditions like negative text and image masks.

## Method Summary
CompoDiff employs a two-stage training strategy: first training on LAION-2B for text-to-image generation, then fine-tuning on the massive synthetic dataset (SynthTriplets18M) with multi-task learning. The model uses a transformer-based denoising architecture operating in CLIP embedding space, with classifier-free guidance enabling versatile conditioning. The synthetic dataset is generated using Stable Diffusion with keyword-based and LLM-based caption generation, creating diverse compositional scenarios. During inference, the model controls condition strength through weighted interpolation between unconditional, image-only, and text-only predictions.

## Key Results
- Achieves new state-of-the-art zero-shot performance on FashionIQ, CIRR, CIRCO, and GeneCIS benchmarks
- SynthTriplets18M dataset is over 500 times larger than existing CIR datasets (18.8M vs. typical 30K-40K triplets)
- Enables versatile CIR with support for negative text conditions and image mask conditions
- Provides control over condition strength between text and image queries and trade-off between inference speed and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classifier-free guidance enables versatile conditioning by allowing flexible weight adjustment between image and text conditions during inference
- Mechanism: During inference, the denoising process interpolates between unconditional, image-only, and text-only predictions using weighted coefficients
- Core assumption: The model has been trained with random condition dropping (10% probability) during training, making it robust to varying condition combinations
- Evidence anchors: Abstract states model can be deployed in various scenarios; section notes easy handling of versatile conditions
- Break condition: If the model wasn't trained with condition dropping, the interpolation would produce poor results for missing conditions

### Mechanism 2
- Claim: The massive synthetic dataset (18.8M triplets) provides sufficient diversity to train a generalizable CIR model without overfitting to specific domains
- Mechanism: Large-scale synthetic data generation using Stable Diffusion and language models creates diverse image-text pairs covering a wide range of compositional scenarios
- Core assumption: The synthetic data generation process captures realistic image modification patterns and maintains semantic consistency
- Evidence anchors: Abstract mentions dataset is 500x larger than existing CIR datasets; section compares to Brooks et al. dataset
- Break condition: If synthetic data doesn't maintain semantic consistency or lacks diversity in object types and modifications

### Mechanism 3
- Claim: The two-stage training strategy prevents catastrophic forgetting while maintaining CLIP space compatibility
- Mechanism: Stage 1 trains on LAION-2B for text-to-image generation, then Stage 2 fine-tunes on synthetic triplets with multi-task learning to preserve knowledge
- Core assumption: The model can effectively balance learning new CIR capabilities while retaining text-to-image generation abilities from Stage 1
- Evidence anchors: Abstract lists contributions including diffusion-based CIR method; section notes empirical observation of knowledge forgetting during stage 2
- Break condition: If multi-task learning ratio is poorly balanced, the model may forget either CIR capabilities or text-to-image generation

## Foundational Learning

- Concept: Diffusion probabilistic models and denoising processes
  - Why needed here: Understanding how the model iteratively denoises noisy image embeddings based on conditions is crucial for both inference and training
  - Quick check question: What is the role of the noise schedule in diffusion models and how does it affect the denoising process?

- Concept: Cross-attention mechanisms in transformer architectures
  - Why needed here: The model uses cross-attention to apply conditions (text, masks, images) during denoising, requiring understanding of how conditions influence the generation process
  - Quick check question: How does cross-attention differ from self-attention in transformers, and why is it suitable for condition application?

- Concept: CLIP embeddings and visual-semantic space
  - Why needed here: The model operates in CLIP embedding space, so understanding how CLIP encodes and compares images and text is fundamental to the retrieval process
  - Quick check question: How does CLIP ensure that images and text with similar semantic content are close in the embedding space?

## Architecture Onboarding

- Component map: CLIP Encoders -> Denoising Transformer -> Prompt-to-Prompt module -> MLP for mask conditioning
- Critical path:
  1. Encode reference image and conditions using CLIP encoders
  2. Add noise to reference image embedding based on diffusion timestep
  3. Denoise through transformer using classifier-free guidance
  4. Use denoised embedding for retrieval in CLIP space
- Design tradeoffs:
  - Inference speed vs. quality: Fewer denoising steps = faster but potentially lower quality
  - Condition strength: Balancing image and text weights affects retrieval focus
  - Model complexity: Using transformer instead of U-Net for denoising affects performance and training stability
- Failure signatures:
  - Poor retrieval when conditions are missing: Indicates insufficient training with condition dropping
  - Inconsistent image modifications: Suggests prompt-to-prompt mechanism not working effectively
  - Catastrophic forgetting: Model performs poorly on either CIR or text-to-image tasks
- First 3 experiments:
  1. Test classifier-free guidance by varying image and text weights on a small dataset to verify condition control works
  2. Compare performance with different denoising step counts (5, 10, 25) to find optimal speed-quality tradeoff
  3. Evaluate retrieval quality with different condition combinations (text only, image only, both) to verify multi-condition handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CompoDiff scale with dataset size beyond 18.8M triplets, and is there a point of diminishing returns?
- Basis in paper: Experiments only test up to 18.8M triplets, leaving relationship between dataset size and performance at larger scales unexplored
- Why unresolved: The experiments focus on the 18.8M scale without testing larger datasets or identifying saturation points
- What evidence would resolve it: Training CompoDiff on datasets significantly larger than 18.8M (e.g., 50M or 100M triplets) and measuring performance gains on CIR benchmarks to identify scaling trends and potential saturation points

### Open Question 2
- Question: How would CompoDiff perform on domain-specific CIR tasks outside of fashion and generic images, such as medical imaging or satellite imagery?
- Basis in paper: Only evaluates CompoDiff on FashionIQ (fashion domain) and CIRR (generic images), leaving performance on specialized domains untested
- Why unresolved: Experiments focus on general-purpose and fashion-specific CIR tasks without exploring specialized domains with different visual and textual characteristics
- What evidence would resolve it: Applying CompoDiff to domain-specific CIR datasets (e.g., medical image retrieval with clinical descriptions or satellite image retrieval with geographic queries) and comparing performance to domain-adapted baselines

### Open Question 3
- Question: What is the impact of using different text encoders (e.g., T5 variants, GPT-based models) on CompoDiff's performance, and is there an optimal choice?
- Basis in paper: Tests CLIP + T5-XL and shows performance gains but does not explore other text encoder combinations or conduct exhaustive comparisons
- Why unresolved: Only two text encoder configurations are tested without investigating whether other combinations (e.g., T5-Small, GPT-3, or domain-specific encoders) could yield better results
- What evidence would resolve it: Systematically evaluating CompoDiff with multiple text encoder combinations across various CIR tasks and identifying the configuration that maximizes retrieval accuracy and efficiency

## Limitations

- Uncertainty about whether the synthetic dataset truly captures the semantic complexity and diversity needed for real-world CIR scenarios, as quality validation is limited
- Limited empirical validation of the classifier-free guidance mechanism's importance, with experiments focusing on performance metrics rather than ablation studies
- Claims about dataset superiority are based on quantitative comparisons but lack qualitative validation of dataset quality and semantic consistency

## Confidence

- **High confidence**: The core architecture (diffusion-based CIR model using CLIP embeddings) is well-established and implementation details are sufficiently specified for reproduction
- **Medium confidence**: The two-stage training strategy and synthetic dataset generation pipeline are described with enough detail to attempt replication, though some implementation specifics are missing
- **Low confidence**: Claims about the superiority of the synthetic dataset size and diversity are based on quantitative comparisons but lack qualitative validation of dataset quality

## Next Checks

1. Generate a small subset (1,000 triplets) of the synthetic dataset and manually inspect the quality of image-text pairs and their semantic consistency to validate the data generation pipeline
2. Implement a minimal CompoDiff model and test the classifier-free guidance mechanism by varying condition weights on a simple CIR task to verify the claimed versatility
3. Perform an ablation study comparing CompoDiff performance with different denoising step counts (5, 10, 25) to empirically validate the inference speed-quality tradeoff claims