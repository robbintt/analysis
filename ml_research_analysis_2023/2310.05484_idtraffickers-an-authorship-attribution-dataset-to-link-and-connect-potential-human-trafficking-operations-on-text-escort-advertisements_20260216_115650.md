---
ver: rpa2
title: 'IDTraffickers: An Authorship Attribution Dataset to link and connect Potential
  Human-Trafficking Operations on Text Escort Advertisements'
arxiv_id: '2310.05484'
source_url: https://arxiv.org/abs/2310.05484
tags:
- dataset
- data
- authorship
- escort
- vendor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IDTraffickers, a dataset of 87,595 text escort
  advertisements collected from the Backpage escort market, with 5,244 vendor labels
  generated using phone number extraction and community detection. The authors establish
  benchmarks for authorship attribution tasks using a DeCLUTR-small model, achieving
  a macro-F1 score of 0.8656 for closed-set classification and a mean r-precision
  score of 0.8852 for open-set ranking.
---

# IDTraffickers: An Authorship Attribution Dataset to link and connect Potential Human-Trafficking Operations on Text Escort Advertisements

## Quick Facts
- arXiv ID: 2310.05484
- Source URL: https://arxiv.org/abs/2310.05484
- Reference count: 40
- Key outcome: DeCLUTR-small achieves macro-F1 of 0.8656 for closed-set classification and r-precision of 0.8852 for open-set ranking on 87,595 Backpage escort ads with 5,244 vendor labels

## Executive Summary
This paper introduces IDTraffickers, a dataset of 87,595 text escort advertisements collected from the Backpage escort market, with 5,244 vendor labels generated using phone number extraction and community detection. The authors establish benchmarks for authorship attribution tasks using a DeCLUTR-small model, achieving a macro-F1 score of 0.8656 for closed-set classification and a mean r-precision score of 0.8852 for open-set ranking. The dataset enables the identification and verification of potential human trafficking vendors through authorship analysis, providing a foundation for future research in this area.

## Method Summary
The methodology involves three main stages: First, phone numbers are extracted from ads using CNN-LSTM-CRF classifier and TJBatchExtractor, then NetworkX creates vendor communities based on shared phone numbers. Second, a DeCLUTR-small model is trained on the vendor-labeled dataset for closed-set classification. Third, style representations extracted from the trained model are used with FAISS similarity search for open-set authorship verification. The approach leverages contrastive learning to generate universal sentence representations that capture stylistic patterns rather than content.

## Key Results
- DeCLUTR-small model achieves macro-F1 score of 0.8656 on closed-set vendor classification
- Open-set authorship verification achieves mean r-precision score of 0.8852 using cosine similarity on style representations
- Phone number extraction and community detection successfully generates 5,244 vendor labels from 87,595 ads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Authorship attribution through style representations enables vendor identification in escort ads.
- Mechanism: DeCLUTR-small model learns universal sentence representations that capture stylistic features of text. These representations are then used to perform closed-set classification to predict which vendor posted a given advertisement.
- Core assumption: Stylistic patterns in text are sufficiently distinct between vendors to allow accurate classification.
- Evidence anchors:
  - [abstract] "To establish a benchmark for authorship identification, we train a DeCLUTR-small model, achieving a macro-F1 score of 0.8656 in a closed-set classification environment."
  - [section] "The success of the Style-Embedding model comes from its ability to latch onto content correlations. However, since escort ads have similar content types, the model struggles to effectively adapt to the noise in our data and distinguish between different writing styles. In contrast, DeCLUTR, trained to generate universal sentence representations, excels at capturing stylometric patterns and associating them with individual vendors."
  - [corpus] Weak - related papers focus on general authorship attribution rather than this specific domain.
- Break condition: If stylistic patterns are too similar between vendors or if content dominates over style in the text.

### Mechanism 2
- Claim: Open-set ranking using cosine similarity on style representations enables vendor verification.
- Mechanism: Style representations extracted from the trained classifier are used to perform similarity search using FAISS. This allows verification of whether two ads came from the same vendor by computing cosine similarity between their representations.
- Core assumption: Style representations maintain vendor-specific information even in an open-set setting where vendors may not be known during training.
- Evidence anchors:
  - [abstract] "Next, we leverage the style representations extracted from the trained classifier to conduct authorship verification, resulting in a mean r-precision score of 0.8852 in an open-set ranking environment."
  - [section] "By employing cosine similarity, we identify the K closest index documents for each query document. Since we treat the authorship verification task as a ranking task, we evaluate the effectiveness of this similarity search operation using Precision@K, Recall@ K, Mean Average Precision (MAP@K), and average R-Precision scores."
  - [corpus] Weak - no direct evidence in corpus about open-set ranking for this domain.
- Break condition: If style representations lose vendor-specific information when applied to unseen vendors or if cosine similarity is not a good measure of style similarity.

### Mechanism 3
- Claim: Phone number extraction and community detection create reliable vendor labels for supervised learning.
- Mechanism: Phone numbers are extracted from ads using CNN-LSTM-CRF classifier, then NetworkX is used to create vendor communities based on shared phone numbers. Each community is assigned a label ID.
- Core assumption: Phone numbers are reliable indicators of vendor identity and ads with shared phone numbers belong to the same vendor.
- Evidence anchors:
  - [section] "To generate ground truth, i.e., vendor labels, we employ the TJBatchExtractor and CNN-LSTM-CRF classifier to extract phone numbers from the ads. Subsequently, we utilize NetworkX to create vendor communities based on these phone numbers. Each community is assigned a label ID, which forms the vendor labels."
  - [section] "While we do not claim that all the ads in our dataset comprise sex trafficking operations, investigations have uncovered the facilitation of numerous sex trafficking operations within the Backpage escort market."
  - [corpus] Weak - corpus papers don't discuss phone number extraction for vendor labeling.
- Break condition: If phone numbers are shared between different vendors or if some vendors use multiple phone numbers inconsistently.

## Foundational Learning

- Concept: Stylometry and authorship attribution techniques
  - Why needed here: Understanding how writing style can be used to identify authors is fundamental to this approach. The paper relies on distinguishing vendors based on their writing patterns rather than content.
  - Quick check question: What are the key differences between content-based and style-based approaches to text classification?

- Concept: Contrastive learning and universal sentence representations
  - Why needed here: DeCLUTR is a contrastive learning model that generates universal sentence representations. Understanding how contrastive learning works is crucial for grasping why this model was chosen.
  - Quick check question: How does contrastive learning differ from traditional supervised learning in NLP?

- Concept: Open-set vs closed-set classification
  - Why needed here: The paper uses both closed-set classification (for vendor identification) and open-set ranking (for vendor verification). Understanding the difference is important for comprehending the experimental setup.
  - Quick check question: What are the key differences between open-set and closed-set classification tasks?

## Architecture Onboarding

- Component map: Phone number extraction -> Vendor community creation -> DeCLUTR-small training -> Style representation extraction -> FAISS similarity search -> Evaluation

- Critical path: Phone number extraction → Vendor community creation → Model training (closed-set) → Style representation extraction → Similarity search (open-set) → Evaluation

- Design tradeoffs:
  - Using phone numbers for ground truth vs manual annotation (scalability vs accuracy)
  - DeCLUTR-small vs larger models (computational efficiency vs potential performance)
  - Masking sensitive information vs preserving useful signals (privacy vs performance)

- Failure signatures:
  - Low macro-F1 scores indicating poor vendor discrimination
  - High variance in R-Precision scores suggesting inconsistent performance across vendors
  - Similar writing patterns between vendors causing misclassification

- First 3 experiments:
  1. Train DeCLUTR-small on vendor identification task and evaluate macro-F1 score
  2. Extract style representations from trained model and perform open-set ranking to compute R-Precision
  3. Analyze misclassified examples to identify patterns in vendor similarity or labeling errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DeCLUTR-small classifier effectively identify potential HT vendors across different escort markets beyond Backpage?
- Basis in paper: [explicit] The authors acknowledge they plan to expand data collection efforts to evaluate zero-shot capabilities on unseen data from other escort markets.
- Why unresolved: The current dataset is limited to Backpage advertisements, and the classifier's performance on other markets is unknown.
- What evidence would resolve it: Testing the DeCLUTR-small classifier on a dataset from a different escort market and comparing its performance to the Backpage results.

### Open Question 2
- Question: How does the performance of larger transformer architectures compare to the DeCLUTR-small classifier for authorship identification and verification tasks?
- Basis in paper: [inferred] The authors mention that due to limited computational resources, they used a distilled and small transformer architecture, and that training on larger architectures has the potential to improve overall performance.
- Why unresolved: The authors did not have the resources to train larger architectures, so the comparison is not available.
- What evidence would resolve it: Training and comparing the performance of larger transformer architectures (e.g., BERT-base, RoBERTa-large) on the IDTraffickers dataset.

### Open Question 3
- Question: How can more reliable explainability approaches be developed for authorship attribution tasks in the context of human trafficking?
- Basis in paper: [explicit] The authors acknowledge the limitations of local and global feature attribution techniques and plan to develop more dependable explainability approaches in the future.
- Why unresolved: The authors recognize the need for better explainability methods but have not yet developed them.
- What evidence would resolve it: Developing and evaluating new explainability techniques specifically tailored for authorship attribution tasks in the context of human trafficking, and comparing their performance to existing methods.

## Limitations
- Ground truth generation relies on phone number extraction, which may not be reliable if vendors share phones or use multiple numbers inconsistently
- Dataset is limited to Backpage platform and may not generalize to other escort advertising markets
- Evaluation metrics may not directly translate to real-world trafficking investigation effectiveness

## Confidence
- Mechanism 1 (stylistic patterns for classification): Medium - high classification accuracy but potential noise in ground truth
- Mechanism 2 (open-set verification): Medium - strong performance but untested on unseen vendors
- Mechanism 3 (phone-based labeling): Low - critical assumption about phone number reliability is not validated

## Next Checks
1. Conduct manual validation on a subset of vendor labels to estimate the accuracy of phone-number-based ground truth generation
2. Test the model's performance on cross-platform data to assess generalizability beyond Backpage
3. Perform adversarial testing by introducing ads with deliberately mimicked writing styles to evaluate robustness