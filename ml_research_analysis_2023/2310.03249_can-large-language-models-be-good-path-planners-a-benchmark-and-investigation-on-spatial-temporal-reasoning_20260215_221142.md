---
ver: rpa2
title: Can Large Language Models be Good Path Planners? A Benchmark and Investigation
  on Spatial-temporal Reasoning
arxiv_id: '2310.03249'
source_url: https://arxiv.org/abs/2310.03249
tags:
- obstacles
- right
- task
- down
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PPNL, a new benchmark to assess LLMs' spatial-temporal
  reasoning abilities through path planning tasks in grid environments. The benchmark
  evaluates LLMs on navigating to target locations while avoiding obstacles and adhering
  to constraints.
---

# Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning

## Quick Facts
- arXiv ID: 2310.03249
- Source URL: https://arxiv.org/abs/2310.03249
- Reference count: 40
- Primary result: LLMs show promise in spatial reasoning with proper prompting but struggle with long-term temporal planning in path planning tasks.

## Executive Summary
This paper introduces PPNL, a benchmark to evaluate LLMs' spatial-temporal reasoning through grid-based path planning tasks. The study compares GPT-4 using various few-shot prompting strategies against fine-tuned BART and T5 models. Results demonstrate that while GPT-4 excels at spatial reasoning when using situated feedback mechanisms like ReAct, it struggles with long-term temporal planning. Fine-tuned models perform well on in-distribution tasks but fail to generalize to novel environments, suggesting memorization rather than true reasoning. The findings highlight the need for improved approaches to enhance LLMs' planning capabilities, particularly for complex, out-of-distribution scenarios.

## Method Summary
The PPNL benchmark generates synthetic NxN grid environments with random obstacles and verbalizes path planning tasks for LLMs. Four prompting strategies are tested for GPT-4: naive few-shot, Chain-of-Thought, ReAct (interleaved reasoning and acting with feedback), and action-and-effect prompting. Fine-tuned BART and T5 models are trained on the same synthetic data. Performance is evaluated against A* (single-goal) and TSP+A* (multi-goal) ground truth using metrics including success rate, optimal rate, exact match, feasible rate, distance to goal, and unreachable accuracy.

## Key Results
- GPT-4 with ReAct prompting achieves the highest spatial reasoning success rates by leveraging environmental feedback for iterative correction
- Fine-tuned models achieve near-perfect performance on in-distribution tasks but catastrophically fail on out-of-distribution environments
- LLMs struggle with long-term temporal planning, often finding feasible paths but not optimal ones
- Action-and-effect prompting improves performance by explicitly tracking state changes without requiring multi-step lookahead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 exhibits strong spatial reasoning when prompted with situated feedback mechanisms like ReAct.
- Mechanism: ReAct allows the model to interleave reasoning, action selection, and observation of local environment signals, enabling step-by-step correction of its trajectory without requiring full long-term planning.
- Core assumption: The model's spatial reasoning is sufficient for local decision-making if given real-time environmental feedback.
- Evidence anchors:
  - [abstract] "experimental results show the promise of few-shot GPT-4 in spatial reasoning, when it is prompted to reason and act interleavedly"
  - [section 3] "Executing the valid portion of the plan and continuously improving the path by explicitly observing the effect of an action sequence through feedback from the environment, allows the model to generate one portion of the path at a time"
- Break condition: If the environment feedback becomes too sparse or the path requires long-range coordination beyond immediate correction, ReAct fails due to insufficient temporal planning.

### Mechanism 2
- Claim: Action-and-effect prompting improves GPT-4 performance by decomposing long-term planning into short-term decisions with explicit state tracking.
- Mechanism: By prompting the model to reason about the effects of its actions (i.e., updating its location after each move), the need for multi-step lookahead is eliminated, allowing better immediate obstacle avoidance.
- Core assumption: The model can maintain accurate state representation when explicitly asked to track it, but cannot infer it implicitly over long horizons.
- Evidence anchors:
  - [section 4.1] "Guiding LLMs to reason about the situated knowledge can help them make better immediate decisions"
  - [section 3] "the model can decide to take an action that avoids an obstacle immediately after realizing that it is about to encounter one"
- Break condition: In scenarios with many obstacles or complex spatial layouts, the model still struggles to maintain awareness without feedback, as shown by the zero distance metric indicating obstacle blockage failures.

### Mechanism 3
- Claim: Fine-tuned LLMs excel at in-distribution spatial reasoning tasks but fail at generalization due to memorization rather than learned reasoning.
- Mechanism: Fine-tuning on synthetic in-distribution data allows the model to learn exact patterns of grid layouts and obstacle configurations, achieving high success rates but lacking robustness to novel configurations.
- Core assumption: The fine-tuned model is effectively interpolating within the training distribution rather than learning generalizable spatial reasoning principles.
- Evidence anchors:
  - [abstract] "fine-tuned LLMs achieved impressive results on in-distribution reasoning tasks, they struggled to generalize to larger environments or environments with more obstacles"
  - [section 4.1] "while the fine-tuned LLMs can achieve a close-to-perfect success rate when planning in environments similar to where they were trained (i.e., in-distribution environments), they suffer from the lack of generalization"
- Break condition: When tested on OOD environments (different grid sizes or obstacle counts), the model's performance drops catastrophically, indicating lack of true reasoning capability.

## Foundational Learning

- Concept: Grid-based spatial reasoning and pathfinding algorithms
  - Why needed here: The benchmark requires understanding how to navigate 2D grids with obstacles, which is foundational to evaluating LLMs' spatial reasoning
  - Quick check question: Can you describe how A* search works and why Manhattan distance is used as a heuristic for grid navigation?

- Concept: Temporal planning and long-horizon reasoning
  - Why needed here: The paper distinguishes between spatial reasoning (local decisions) and temporal reasoning (multi-step planning), which is critical for understanding LLM limitations
  - Quick check question: What is the difference between making a local decision to avoid an obstacle versus planning a multi-step path to a distant goal?

- Concept: Few-shot prompting strategies and their effects on model performance
  - Why needed here: Different prompting methods (naive, CoT, ReAct, action-effect) are compared, requiring understanding of how prompt structure influences LLM reasoning
  - Quick check question: How does providing step-by-step reasoning instructions (CoT) differ from interleaved reasoning and acting (ReAct) in terms of cognitive load on the model?

## Architecture Onboarding

- Component map: Environment generator -> Task verbalizer -> LLM executor -> Ground truth generator -> Evaluator
- Critical path: Generate environment → Verbalize task → LLM generates action sequence → Evaluate against ground truth
- Design tradeoffs:
  - Synthetic data generation allows controlled experiments but may not capture real-world complexity
  - ReAct provides best performance but at high computational cost due to iterative API calls
  - Fine-tuning achieves high in-distribution performance but fails generalization, creating tension between accuracy and robustness
- Failure signatures:
  - High success but low optimal rate: Model reaches goal but doesn't find shortest path (temporal reasoning weakness)
  - High feasible but low success: Model stays in bounds but can't reach goal (spatial reasoning failure)
  - Catastrophic OOD failure: Model memorized training patterns rather than learned reasoning
- First 3 experiments:
  1. Run naive few-shot prompting with 5, 10, and 15 exemplars to establish baseline performance and observe improvement with more examples
  2. Implement action-and-effect prompting to test whether explicit state tracking improves performance without feedback
  3. Compare CoT versus ReAct on a small sample to quantify the benefit of environmental feedback versus pure reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be effectively prompted to consistently find optimal paths in path planning tasks?
- Basis in paper: [explicit] The paper notes that while GPT-4 can find optimal paths in single-goal settings without explicit prompting for optimality, its performance drops significantly in multi-goal scenarios. An experiment with explicit optimality prompting showed only slight improvements, achieving around 50% optimal rates.
- Why unresolved: The study indicates that achieving optimal path planning requires significant temporal reasoning, which current prompting methods struggle with. The hierarchical approach used for multi-goal tasks does not consistently yield optimal results.
- What evidence would resolve it: Experiments demonstrating a consistent improvement in optimal path finding with enhanced prompting techniques or novel methods that better integrate temporal reasoning.

### Open Question 2
- Question: How do different action space representations affect the performance of LLMs in path planning tasks?
- Basis in paper: [explicit] The paper explores an alternative action space where the agent’s movement is constrained to the direction it is facing, involving actions like "turn left," "turn right," and "move forward." This representation led to a slight decrease in performance, suggesting task presentation is crucial.
- Why unresolved: The study only briefly explored this alternative representation due to time and cost constraints, leaving its full impact on LLM performance unclear.
- What evidence would resolve it: Comprehensive experiments comparing different action space representations across various path planning scenarios to determine their impact on LLM performance.

### Open Question 3
- Question: Can fine-tuned models generalize better to novel initial and goal placements within environments they were trained on compared to entirely new environments?
- Basis in paper: [explicit] The paper investigates whether fine-tuned models perform better on unseen placements within training environments versus completely new environments. Results showed almost the same performance on both, indicating no significant overfitting to specific environments.
- Why unresolved: While initial findings suggest no overfitting, the study did not explore the full range of potential generalization capabilities or limitations of fine-tuned models.
- What evidence would resolve it: Further experiments testing fine-tuned models on a wider variety of placements and environments to assess their generalization capabilities comprehensively.

## Limitations
- The synthetic grid environments may not capture real-world path planning complexity
- ReAct's effectiveness may reflect pattern matching with environmental feedback rather than genuine reasoning
- Fine-tuned models show memorization rather than learning generalizable spatial reasoning principles

## Confidence

**High Confidence**: GPT-4 with ReAct prompting achieves superior performance in spatial reasoning tasks
**Medium Confidence**: Fine-tuned models fail to generalize beyond their training distribution
**Low Confidence**: LLMs struggle with long-term temporal planning (inferred rather than directly measured)

## Next Checks
1. Design a controlled experiment isolating temporal planning by varying only temporal horizon while holding spatial complexity constant
2. Conduct systematic probing of fine-tuned models on interpolated test sets that gradually increase in complexity from training distribution
3. Compare ReAct performance against a modified version with delayed or noisy environmental feedback to quantify feedback dependency