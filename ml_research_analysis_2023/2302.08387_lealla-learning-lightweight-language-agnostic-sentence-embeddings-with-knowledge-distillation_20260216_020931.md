---
ver: rpa2
title: 'LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge
  Distillation'
arxiv_id: '2302.08387'
source_url: https://arxiv.org/abs/2302.08387
tags:
- labse
- sentence
- embeddings
- distillation
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the efficiency challenge of large-scale language-agnostic\
  \ sentence embedding models like LaBSE, which have high inference costs due to their\
  \ size and embedding dimensionality. The authors propose LEALLA, a lightweight model\
  \ that generates low-dimensional (128\u2013256) sentence embeddings for 109 languages."
---

# LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation

## Quick Facts
- arXiv ID: 2302.08387
- Source URL: https://arxiv.org/abs/2302.08387
- Reference count: 40
- Primary result: Achieves near-LaBSE performance on multilingual sentence alignment with embeddings reduced to 128-256 dimensions and models with 69M-147M parameters

## Executive Summary
This paper addresses the efficiency challenge of large-scale language-agnostic sentence embedding models like LaBSE, which have high inference costs due to their size and embedding dimensionality. The authors propose LEALLA, a lightweight model that generates low-dimensional (128–256) sentence embeddings for 109 languages. They first show that reducing LaBSE embeddings to 128–256 dimensions retains strong performance, then explore thin-deep encoder architectures to achieve comparable results with far fewer parameters (69M–147M vs. 471M). They further improve performance via knowledge distillation from LaBSE using feature and logit distillation losses combined with the AMS loss. LEALLA models achieve only minor performance drops (up to 3.0, 1.3, and 0.3 P@1 points on Tatoeba, UN, and BUCC respectively) compared to LaBSE, while greatly reducing model size and embedding dimensionality.

## Method Summary
The LEALLA approach combines three key innovations: (1) demonstrating that reducing LaBSE embeddings to 128-256 dimensions retains strong parallel sentence alignment performance, (2) designing thin-deep encoder architectures with 24 layers and 128-256 hidden dimensions that achieve better parameter efficiency than shallow-wide alternatives, and (3) applying knowledge distillation from LaBSE using a combination of AMS loss for parallel alignment, feature distillation (MSE between transformed student and teacher embeddings), and logit distillation (MSE between similarity matrices). The models are trained on parallel sentence data for 109 languages using a shared 501k BPE vocabulary from LaBSE.

## Key Results
- Reducing LaBSE embeddings to 128-256 dimensions retains most performance, with sharp drops below 128 dimensions
- Thin-deep encoder architecture (24 layers, 128-256 dimensions) outperforms shallow-wide alternatives with fewer parameters
- Knowledge distillation provides consistent improvements across all three evaluation tasks (Tatoeba, UN, BUCC)
- LEALLA-small (69M parameters, 128-dim embeddings) achieves only 3.0 P@1 drop on Tatoeba compared to LaBSE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing LaBSE embeddings to 128-256 dimensions retains strong performance for parallel sentence alignment
- Mechanism: The semantic information necessary for identifying parallel sentences remains intact even after dimensionality reduction
- Core assumption: The intrinsic dimensionality of language-agnostic sentence embeddings is lower than 768 for the task of parallel sentence alignment
- Evidence anchors:
  - [abstract]: "We first show that reducing LaBSE embeddings to 128–256 dimensions retains strong performance"
  - [section]: "As shown in Fig. 1, the performance drops more than 5 points when the dimension is 32 on Tatoeba, UN, and BUCC. Meanwhile, given sentence embeddings with a dimension over 128, they performs slightly worse than 768-d LaBSE embeddings with a performance drop of fewer than 2 points"
  - [corpus]: Weak - the corpus contains related work on knowledge distillation and sentence embeddings but no direct evidence about dimensionality reduction performance
- Break condition: If the semantic similarity structure collapses below the threshold where cosine similarity can still distinguish parallel from non-parallel sentences

### Mechanism 2
- Claim: Thin-deep encoder architecture is superior for learning language-agnostic sentence embeddings compared to simply reducing model size
- Mechanism: Deeper networks with fewer parameters per layer can learn more abstract representations that generalize better across languages
- Core assumption: The depth-to-width ratio matters more than absolute parameter count for capturing cross-lingual semantic relationships
- Evidence anchors:
  - [abstract]: "we show that a thin-deep encoder can construct robust low-dimensional sentence embeddings for 109 languages"
  - [section]: "Second, increasing the number of layers (#5 and #7) improves the performance of 12-layer models (#3 and #4) with a limited parameter increase less than 10%"
  - [corpus]: Weak - corpus contains related distillation work but no specific evidence about thin-deep architectures for multilingual embeddings
- Break condition: When depth increases without corresponding width, causing optimization difficulties or overfitting on the training data

### Mechanism 3
- Claim: Knowledge distillation from LaBSE to lightweight models preserves performance through feature and logit distillation
- Mechanism: The student model learns to mimic both the intermediate representations (feature distillation) and the final similarity judgments (logit distillation) of the teacher
- Core assumption: LaBSE's representations contain generalizable knowledge about language-agnostic semantic similarity that can be transferred to smaller architectures
- Evidence anchors:
  - [abstract]: "With our proposed distillation methods, we achieve further improvements by incorporating knowledge from a teacher model"
  - [section]: "By adding Lfd or Lld, LEALLA trained only with Lams is improved on Tatoeba and UN tasks. By further combining Lfd and Lld, LEALLA consistently achieves superior performance"
  - [corpus]: Moderate - corpus contains related work on knowledge distillation for sentence embeddings, supporting the general approach
- Break condition: When the capacity gap between teacher and student is too large for effective knowledge transfer, or when the distilled knowledge doesn't generalize beyond the training distribution

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: To transfer the multilingual semantic understanding from LaBSE to smaller models
  - Quick check question: What are the two main paradigms of knowledge distillation used in this work, and how do they differ in what knowledge they transfer?

- Concept: Thin-Deep Architecture
  - Why needed here: To achieve parameter efficiency while maintaining representational power for multilingual embeddings
  - Quick check question: Why does increasing depth while reducing width help maintain performance in multilingual sentence embedding models?

- Concept: Bidirectional Additive Margin Softmax (AMS) Loss
  - Why needed here: To optimize the separation between translations and non-translations in the embedding space
  - Quick check question: How does the margin parameter in AMS loss affect the geometry of the learned embedding space?

## Architecture Onboarding

- Component map:
  - Input sentences processed through shared vocabulary
  - Thin-deep encoder generates language-agnostic embeddings
  - Feature distillation head creates teacher-dimension embeddings
  - Cosine similarities computed for AMS loss
  - Similarity matrices compared to teacher for logit distillation
  - Combined loss optimized via AdamW

- Critical path:
  1. Input sentences processed through shared vocabulary
  2. Thin-deep encoder generates language-agnostic embeddings
  3. Feature distillation head creates teacher-dimension embeddings
  4. Cosine similarities computed for AMS loss
  5. Similarity matrices compared to teacher for logit distillation
  6. Combined loss optimized via AdamW

- Design tradeoffs:
  - Parameter count vs. performance: LEALLA-small (69M) vs. LEALLA-large (147M)
  - Embedding dimensionality vs. task performance: 128-256 dimensions retain most information
  - Distillation strength vs. student capacity: Stronger distillation may hurt if student is too small

- Failure signatures:
  - Performance collapse on low-resource languages
  - Degraded cross-lingual retrieval accuracy
  - Training instability with aggressive distillation
  - Overfitting to specific language pairs

- First 3 experiments:
  1. Dimension reduction on LaBSE embeddings to identify performance threshold
  2. Architecture ablation comparing thin-deep vs. shallow-wide designs
  3. Distillation ablation testing feature vs. logit distillation contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LEALLA models vary when using multilingual pre-trained models instead of LaBSE as the teacher for knowledge distillation?
- Basis in paper: [inferred] The paper states that "no multilingual pre-trained models are available for the lightweight architectures" and therefore uses LaBSE for knowledge distillation. This suggests that using other pre-trained models as teachers could be explored.
- Why unresolved: The paper does not experiment with different teacher models for distillation, leaving this comparison unexplored.
- What evidence would resolve it: Experimental results comparing LEALLA performance when using different multilingual pre-trained models (e.g., mBERT, XLM-R) as teachers for knowledge distillation.

### Open Question 2
- Question: What is the impact of using different embedding reduction techniques (e.g., PCA, random projection) compared to the dense layer approach used in the paper?
- Basis in paper: [explicit] The paper investigates dimension reduction of LaBSE embeddings using an extra dense layer and states that "existing multilingual sentence embedding models such as LASER, SBERT, EMS, and LaBSE use 768-d or 1024-d sentence embeddings, and whether a low-dimensional space can align parallel sentences over tens of languages with a solid accuracy (>80%) remains unknown."
- Why unresolved: The paper only explores one method (dense layer) for reducing embedding dimensionality, not comparing it with other techniques.
- What evidence would resolve it: Experimental results comparing LEALLA performance using different embedding reduction techniques for creating low-dimensional sentence embeddings.

### Open Question 3
- Question: How does the performance of LEALLA models change when trained with different training data distributions (e.g., balanced vs. imbalanced across languages)?
- Basis in paper: [explicit] The paper acknowledges that "more training data for high-resource languages may cause the biased model accuracy for those languages" and that "all the training data used in this work are English-centric sentence pairs."
- Why unresolved: The paper uses the same training data distribution as LaBSE without exploring how different distributions might affect performance.
- What evidence would resolve it: Experimental results showing LEALLA performance when trained with different training data distributions, particularly balanced datasets across languages.

## Limitations

- Evaluation is limited to parallel sentence alignment tasks, with uncertainty about performance on other multilingual NLP tasks
- The specific thin-deep architecture configuration (24 layers, 128-256 dimensions) may be overfit to the LaBSE distillation setting
- Knowledge distillation effectiveness varies across tasks, with feature distillation helping more on Tatoeba while logit distillation is more beneficial for UN and BUCC

## Confidence

- High Confidence: The dimension reduction experiments showing that 128-256 dimensions retain most performance are well-supported by systematic testing across multiple tasks
- Medium Confidence: The thin-deep architecture design choice is supported by ablation studies, but the specific optimal configuration may be overfit to this particular training setup
- Low Confidence: The knowledge distillation effectiveness claims, while showing consistent improvements, lack detailed analysis of why certain components work better for specific tasks

## Next Checks

1. Evaluate LEALLA models on non-parallel alignment tasks including cross-lingual semantic textual similarity benchmarks (STS) and zero-shot cross-lingual classification tasks to verify the embeddings' utility beyond parallel sentence retrieval

2. Systematically vary the depth-width ratio beyond the tested configurations (24 layers, 128-256 dimensions) to identify whether the thin-deep advantage holds across a broader range of architectures, particularly testing even thinner configurations

3. Conduct controlled experiments isolating each distillation component (feature, logit) with different teacher-student capacity gaps to better understand the knowledge transfer mechanisms and identify optimal distillation strategies for various model sizes