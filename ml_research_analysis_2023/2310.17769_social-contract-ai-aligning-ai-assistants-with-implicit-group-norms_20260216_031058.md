---
ver: rpa2
title: 'Social Contract AI: Aligning AI Assistants with Implicit Group Norms'
arxiv_id: '2310.17769'
source_url: https://arxiv.org/abs/2310.17769
tags:
- learning
- assistant
- policy
- users
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores Social Contract AI (SCAI), a method for aligning
  AI assistants with implicit group norms by inferring users' preferences from observed
  interactions. The authors validate their approach through proof-of-concept simulations
  in the economic ultimatum game, formalizing user preferences as policies that guide
  simulated players' actions.
---

# Social Contract AI: Aligning AI Assistants with Implicit Group Norms

## Quick Facts
- arXiv ID: 2310.17769
- Source URL: https://arxiv.org/abs/2310.17769
- Reference count: 32
- One-line primary result: SCAI accurately learns group norms from observed interactions but shows limited generalization to new currencies and struggles with inconsistent language-behavior relationships

## Executive Summary
This paper introduces Social Contract AI (SCAI), a novel approach for aligning AI assistants with implicit group norms by inferring users' preferences from observed interactions. The authors validate their approach through proof-of-concept simulations in the economic ultimatum game, where the AI assistant learns to offer shares that align with users' behavior. The study demonstrates that SCAI can effectively learn group norms in controlled settings but reveals important limitations in generalization to out-of-distribution contexts and when language use conflicts with behavioral preferences.

## Method Summary
The authors develop a simulation framework where AI assistants learn group norms through verbal reinforcement learning in the ultimatum game. The system represents user preferences as policies (e.g., "be selfish when making offers" or "be altruistic when making offers") and uses a MetaLM to sample policies based on observed interactions. The AssistantLM then executes these policies in the game environment. The AI assistant revises its policy after each training epoch based on observed interactions between users and itself, aiming to converge on offers that match the users' shared policy.

## Key Results
1. The AI assistant accurately learns policies resulting in offered shares aligning with users' offers in both one-group norm and mixed-group norm settings
2. In out-of-distribution settings with new currencies, the assistant's learned policies show limited generalization and lack robustness
3. Inconsistent use of language (e.g., altruistic policy combined with rude language) slows the assistant's learning of users' shared policy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AI assistant learns group norms by inverting a model of user preferences from observed interactions.
- Mechanism: The system uses verbal reinforcement learning where the MetaLM samples policies based on observed user interactions, and the AssistantLM executes these policies in the ultimatum game. This creates a Bayesian updating process where observed behaviors inform policy inference.
- Core assumption: Users' preferences can be adequately represented as policies that guide their actions in the ultimatum game.
- Evidence anchors:
  - [abstract] "Unlike CAI, which operates on a set of fixed, formal rules or constitutional principles, SCAI aims to infer group norms from observed interactions among users."
  - [section] "We represent users' preferences (i.e., the shared group norm(s)) as a shared policy, such as 'be selfish when making offers' or 'be altruistic when making offers'."
  - [corpus] Weak - corpus neighbors focus on privacy preferences and user alignment but don't directly address inverse reinforcement learning for group norm inference.
- Break condition: If user behavior cannot be adequately modeled as policies, or if the mapping between observed interactions and underlying preferences is non-deterministic.

### Mechanism 2
- Claim: The system generalizes learned policies to out-of-distribution contexts with varying success.
- Mechanism: When testing learned policies on new currencies (grams of medicine vs. dollars), the assistant relies on its prior (set to altruistic) when the learned policy lacks robustness, leading to different behavior in OOD settings.
- Core assumption: The assistant can apply learned principles across different currencies and contexts without retraining.
- Evidence anchors:
  - [abstract] "However, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution."
  - [section] "The left panel in Fig. 3b shows that testing a selfish policy results in selfish offers in-distribution (i.e., testing on dollars), whereas OOD offers were strongly influenced by the assistant's prior, which we here arbitrarily set to altruistic."
  - [corpus] Weak - corpus doesn't address currency generalization specifically, though related work on contextual MDPs suggests this is a known challenge.
- Break condition: If the assistant's prior dominates learned behavior in OOD settings, preventing policy application.

### Mechanism 3
- Claim: Inconsistent language use between user behavior and communication slows learning of group norms.
- Mechanism: When users express altruistic preferences using rude language (or selfish preferences using sycophantic language), the assistant takes longer to converge on the correct policy, though it eventually does learn.
- Core assumption: The assistant can distinguish between behavioral preferences and communication style.
- Evidence anchors:
  - [abstract] "Additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed."
  - [section] "We observed that when the manner in which users communicate their proposals (e.g., rude) conflicts with the expectations set by a given policy (e.g., altruistic), the assistant still learns a policy that results in similar offers to those of users; however, convergence is slower and fails to fully match the offered shares of users within five training epochs."
  - [corpus] Weak - corpus neighbors discuss user preferences but not the impact of communication style on preference learning.
- Break condition: If communication style consistently masks true behavioral preferences, preventing accurate policy learning.

## Foundational Learning

- Concept: Bayesian inverse reinforcement learning
  - Why needed here: The system needs to infer reward functions (user preferences) from observed behavior rather than from explicit rewards, which is the core of inverse reinforcement learning.
  - Quick check question: Can you explain how the assistant updates its beliefs about user preferences after observing each interaction?

- Concept: Contextual Markov Decision Processes (CMDPs)
  - Why needed here: The ultimatum game has different contexts (proposer vs. responder roles) that affect the assistant's policy, requiring a CMDP framework rather than a standard MDP.
  - Quick check question: How does the context variable differentiate between proposer and responder roles in the ultimatum game?

- Concept: Thompson Sampling for posterior approximation
  - Why needed here: The MetaLM samples policies from the posterior distribution over plausible MDPs given observed interactions, which is analogous to Thompson Sampling in Bayesian RL.
  - Quick check question: Why is sampling from the posterior distribution more appropriate than using a point estimate of the best policy?

## Architecture Onboarding

- Component map: MetaLM -> AssistantLM -> Environment -> Policy update
- Critical path: User interaction → MetaLM policy sampling → AssistantLM execution → Environment response → Policy update → Next interaction
- Design tradeoffs:
  - Temperature setting (0 for deterministic behavior vs. higher for exploration)
  - Prior selection (altruistic vs. selfish) affecting OOD generalization
  - Policy specification granularity (general principles vs. specific rules)
- Failure signatures:
  - Assistant offers consistently deviate from user offers despite multiple training epochs
  - Policy convergence stalls when language style conflicts with behavioral preferences
  - OOD testing shows complete reliance on prior rather than learned policy
- First 3 experiments:
  1. Run with temperature=0.5 to assess impact of stochastic user behavior on policy learning
  2. Test with different priors (selfish vs. altruistic) to measure effect on OOD generalization
  3. Implement a multi-policy learning approach where assistant learns separate policies for different user subgroups within a single run

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the AI assistant's learned policy exhibit robustness when tested with currencies that were not part of its training distribution?
- Basis in paper: [explicit] The paper states that in out-of-distribution settings with new currencies, the assistant's learned policies show limited generalization and lack robustness.
- Why unresolved: While the paper shows that the assistant's policy lacks robustness with new currencies, it does not explore whether this limitation can be overcome through additional training or other methods.
- What evidence would resolve it: Further experiments testing the assistant's performance with a wider variety of currencies and amounts, and potentially training the assistant on a more diverse set of currencies, could provide evidence on whether the robustness issue can be mitigated.

### Open Question 2
- Question: How does the presence of inconsistent language (e.g., altruistic policy combined with rude language) affect the speed and quality of the AI assistant's learning of users' shared policy?
- Basis in paper: [explicit] The paper finds that inconsistent use of language slows the assistant's learning of users' shared policy.
- Why unresolved: The paper does not explore whether this effect can be reduced or eliminated, or what specific aspects of the language inconsistency contribute most to the slowdown.
- What evidence would resolve it: Experiments varying the degree and type of language inconsistency, and measuring the resulting impact on learning speed and policy quality, could help identify the key factors and potential mitigation strategies.

### Open Question 3
- Question: Can the AI assistant learn multiple policies within a single simulation run, rather than just a single policy, to better match the distribution of user policies?
- Basis in paper: [inferred] The paper suggests that the assistant learns a distribution over policies across simulation runs, but does not explore whether it can learn multiple policies within a single run.
- Why unresolved: Learning multiple policies within a run could allow the assistant to better adapt to groups with diverse preferences, but the paper does not investigate this capability.
- What evidence would resolve it: Modifying the simulation to allow the assistant to learn and maintain multiple policies, and comparing its performance to the single-policy approach, could demonstrate the potential benefits and limitations of this strategy.

## Limitations
- The simulation framework relies on the ultimatum game as a proxy for real-world group norm dynamics, which may not capture the full complexity of natural human interactions
- The MetaLM's policy inference depends heavily on the assumption that user preferences can be adequately represented as deterministic policies
- The out-of-distribution generalization tests used only currency variations, leaving open questions about generalization to fundamentally different task structures

## Confidence
- High confidence in the basic SCAI mechanism for policy learning within the simulated environment
- Medium confidence in the generalizability of findings to real-world AI assistant scenarios
- Low confidence in the robustness of policy learning when language use and behavioral preferences are inconsistent

## Next Checks
1. Implement a multi-currency test suite with currencies varying in psychological distance (e.g., money, favors, time) to better assess OOD generalization limits
2. Design experiments with mixed-policy environments where subgroups of users have distinct preferences to test the assistant's ability to identify and adapt to multiple group norms simultaneously
3. Conduct ablation studies on the impact of different prior distributions and temperature settings on policy convergence and robustness across diverse user populations