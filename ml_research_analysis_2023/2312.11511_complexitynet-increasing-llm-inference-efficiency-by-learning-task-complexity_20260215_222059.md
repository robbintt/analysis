---
ver: rpa2
title: 'ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity'
arxiv_id: '2312.11511'
source_url: https://arxiv.org/abs/2312.11511
tags:
- complexity
- prompt
- each
- dataset
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents ComplexityNet, a method to optimize LLM inference
  efficiency by assigning tasks to models based on their complexity. The authors define
  task complexity as the simplest LLM capable of correctly accomplishing a given task.
---

# ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity

## Quick Facts
- arXiv ID: 2312.11511
- Source URL: https://arxiv.org/abs/2312.11511
- Reference count: 17
- Primary result: Fine-tuned complexity model achieves 79% accuracy in task complexity prediction, enabling 90% computational resource reduction while maintaining 86.7% code generation accuracy

## Executive Summary
ComplexityNet addresses the inefficiency of using large language models (LLMs) by learning to predict task complexity and routing prompts to appropriately-sized models. The authors define task complexity as the simplest LLM capable of correctly solving a given task. By fine-tuning a smaller model to classify tasks into complexity levels based on empirical success rates of Code Llama, GPT-3.5, and GPT-4, they achieve significant computational savings while maintaining high accuracy on code generation tasks.

## Method Summary
The authors create a dataset of prompts labeled with complexity scores by running each prompt through three LLMs multiple times and determining which is the simplest model that consistently produces correct outputs. They then fine-tune DaVinci-002 on this labeled dataset to predict task complexity. The system routes tasks to the appropriate LLM based on the predicted complexity score, using a 5-trial labeling method to reduce stochastic variability in LLM outputs.

## Key Results
- Fine-tuned complexity model achieves 79% accuracy in determining task complexity
- 90% reduction in computational resource usage compared to using the highest complexity model
- Maintains high code generation accuracy of 86.7% while achieving efficiency gains
- 5-trial labeling method provides 15% accuracy improvement over single-trial approach

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a smaller model to classify task complexity enables efficient routing to appropriate LLMs. The fine-tuned model learns to map prompts to complexity scores based on empirical success rates of larger models on those prompts, allowing prediction of which model will solve a task successfully without running expensive models.

### Mechanism 2
The 5-Trial Labeling Method improves prediction accuracy by running each model multiple times on the same prompt and averaging results. This reduces the impact of stochastic variations in LLM outputs, leading to more stable complexity classifications.

### Mechanism 3
Task complexity correlates with the minimal LLM capability needed to solve it, enabling cost-effective resource allocation. The system defines complexity as the simplest LLM that can solve a task, routing tasks to the least capable model that can handle them to conserve computational resources.

## Foundational Learning

- **Task complexity definition and measurement**: Understanding how complexity is defined and measured is crucial for interpreting results and extending the approach to new domains. *Quick check*: How does the paper define task complexity, and what empirical method is used to measure it?

- **Fine-tuning for classification tasks**: The approach relies on fine-tuning a smaller model to classify task complexity, so understanding fine-tuning principles is essential. *Quick check*: What is the difference between zero-shot, few-shot, and fine-tuning approaches in the context of LLM adaptation?

- **Stochastic nature of LLM outputs**: The 5-Trial Labeling Method addresses stochasticity in LLM outputs, so understanding this concept is crucial for evaluating the approach. *Quick check*: Why might running the same prompt through an LLM multiple times yield different outputs, and how can this variability be managed?

## Architecture Onboarding

- **Component map**: Prompt pipeline -> Complexity model (fine-tuned Davinci-002) -> Model router -> LLM executors (Code Llama, GPT-3.5, GPT-4) -> Verification system

- **Critical path**: User submits prompt → Complexity model predicts complexity score → Router selects appropriate LLM based on complexity → Selected LLM executes the task → Output is returned to user

- **Design tradeoffs**: Model selection (fine-tuned vs. heuristic rules), trial count (accuracy vs. latency), model hierarchy (which LLMs to include), cost vs. accuracy trade-offs

- **Failure signatures**: Low complexity prediction accuracy (< 70%), high variance in LLM outputs, mismatch between predicted and actual performance, unexpectedly high resource usage

- **First 3 experiments**: 1) Evaluate complexity model accuracy on held-out test set, 2) Test full routing system on diverse tasks, 3) Vary number of trials in complexity assessment

## Open Questions the Paper Calls Out

The paper raises several important questions about generalizability and applicability. Can the ComplexityNet approach be generalized to non-deterministic tasks like essay or poem generation where there is no single correct answer? How does the 5-Trial Labeling Method compare to the 1-Trial Labeling Method in terms of computational efficiency and accuracy for different types of tasks? What is the optimal number of trials to use in the 5-Trial Labeling Method for different task types?

## Limitations

- The approach is evaluated only on Python coding tasks from the MBPP dataset, raising questions about generalizability to other domains
- The complexity determination methodology relies on a fixed set of three LLMs with predetermined capability hierarchies
- The 5-trial labeling method introduces significant latency overhead not fully accounted for in resource efficiency calculations

## Confidence

- **High Confidence**: The core mechanism of fine-tuning for complexity prediction and routing is technically sound with measurable improvements
- **Medium Confidence**: The correlation between task complexity and minimal LLM capability is supported but relies on specific model choices
- **Low Confidence**: Generalizability to non-coding tasks and long-term stability of complexity hierarchy are not empirically tested

## Next Checks

1. Test ComplexityNet on non-coding tasks (mathematical reasoning, text summarization) to evaluate cross-domain generalization
2. Implement periodic re-evaluation of the complexity hierarchy as new LLM versions are released
3. Conduct experiments varying trial counts (1, 3, 5, 10) to quantify the precise relationship between labeling accuracy, latency, and overall computational efficiency