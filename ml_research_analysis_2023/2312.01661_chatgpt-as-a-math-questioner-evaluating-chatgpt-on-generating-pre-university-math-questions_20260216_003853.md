---
ver: rpa2
title: ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university
  Math Questions
arxiv_id: '2312.01661'
source_url: https://arxiv.org/abs/2312.01661
tags:
- chatgpt
- math
- questions
- question
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates ChatGPT''s capability in generating pre-university
  math questions through a comprehensive analysis across two main settings: context-aware
  and context-unaware. In the context-aware setting, ChatGPT was evaluated on three
  established math benchmarks (SVAMP, GSM8K, MATH) and achieved automatic evaluation
  scores ranging from 19.48 to 23.57 BLEU-4.'
---

# ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions

## Quick Facts
- **arXiv ID**: 2312.01661
- **Source URL**: https://arxiv.org/abs/2312.01661
- **Reference count**: 40
- **Primary result**: ChatGPT generates grammatically correct and context-relevant pre-university math questions, but tends to produce easier questions than expected with varying mathematical depth.

## Executive Summary
This paper evaluates ChatGPT's capability in generating pre-university math questions through comprehensive analysis across two main settings: context-aware and context-unaware. The study develops TopicMath, a novel dataset containing 428 math lessons across 121 topics, and generates 16K question-answer pairs. Through automatic and human evaluations on established benchmarks (SVAMP, GSM8K, MATH), the research reveals that while ChatGPT produces grammatically correct and contextually relevant questions, it struggles with difficulty control and often generates questions that are too simple compared to the provided context.

## Method Summary
The study employs a two-pronged evaluation approach. In the context-aware setting, ChatGPT is prompted with established math benchmarks (SVAMP, GSM8K, MATH) using constraints on contextual independence, tense matching, and word limits, with outputs evaluated using automatic metrics (BLEU-4, ROUGE-L, METEOR, BERTScore) and human assessment. In the context-unaware setting, the study develops TopicMath from Khan Academy, containing 121 math topics and 428 lessons, and generates 16K QA pairs using zero-shot Chain-of-Thought prompting. Human evaluations assess difficulty, relevancy, grammaticality, answerability, usefulness, and topic alignment. The study also compares ChatGPT's performance against fine-tuned baseline models.

## Key Results
- ChatGPT achieved automatic evaluation scores of 19.48-23.57 BLEU-4 on established math benchmarks in context-aware settings
- Human evaluations revealed ChatGPT produces grammatically correct questions (>4.9 scores) but tends to generate easier questions than expected
- The model excels at creating real-life scenarios and drawing interdisciplinary connections, but struggles with maintaining mathematical depth and difficulty control
- TopicMath dataset successfully captures 121 math topics across pre-university levels, enabling comprehensive evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can generate grammatically correct and context-relevant math questions when given sufficient context.
- Mechanism: By providing clear prompts with constraints on word limit, tense matching, and contextual independence, ChatGPT can produce questions that are both grammatically sound and aligned with the provided context.
- Core assumption: ChatGPT's underlying language model has been sufficiently trained on diverse text data to understand grammatical structures and context.
- Evidence anchors: [abstract] states that human evaluations revealed ChatGPT produces grammatically correct and context-relevant questions. [section 6.2] mentions that ChatGPT consistently attains grammaticality scores exceeding > 4.9 across all pre-university levels. [corpus] shows related works focusing on evaluating LLMs' capabilities in math and reasoning tasks.
- Break condition: If the context is too complex or lacks sufficient information, ChatGPT may struggle to generate relevant questions, as seen in some instances where it repeats information from the context.

### Mechanism 2
- Claim: ChatGPT can generate diverse math questions by leveraging topic-specific prompts and demonstrations.
- Mechanism: By providing ChatGPT with topic definitions and example questions, it can generate a variety of questions that cover different aspects of the topic while maintaining difficulty levels.
- Core assumption: ChatGPT can effectively learn from a few examples and apply that knowledge to generate new, diverse questions.
- Evidence anchors: [section 5.3] describes the process of prompting ChatGPT to generate QA pairs from TopicMath, a dataset with 121 topics and 428 lessons. [section 7.3] mentions that ChatGPT excels at crafting real-life scenarios that integrate with the context, even drawing connections to other subjects. [corpus] includes papers discussing LLMs' abilities in arithmetic and reasoning tasks, supporting the idea of leveraging examples for generation.
- Break condition: If the provided examples are too narrow or specific, ChatGPT may struggle to generate diverse questions, as observed in some lessons with fewer generated QA pairs.

### Mechanism 3
- Claim: ChatGPT's performance in generating math questions varies depending on the difficulty level and topic complexity.
- Mechanism: ChatGPT can generate questions for different difficulty levels, but its ability to maintain the desired difficulty and mathematical depth may be inconsistent, especially for more complex topics.
- Core assumption: ChatGPT's understanding of mathematical concepts and its ability to assess difficulty levels are not perfect, leading to variations in generated question quality.
- Evidence anchors: [abstract] mentions that ChatGPT tends to generate easier questions than expected and often struggles with difficulty control and mathematical depth in the context-unaware setting. [section 7.3] discusses how ChatGPT's question difficulty solely depends on the difficulty of the demonstration, struggling to enhance the question's difficulty when given a simpler example. [corpus] includes papers evaluating LLMs' mathematical and coding competencies, supporting the idea of varying performance based on task complexity.
- Break condition: If the topic is highly complex or abstract, ChatGPT may struggle to generate questions that accurately represent the intended difficulty level, as seen in its performance on tertiary-level math topics.

## Foundational Learning

- Concept: Large Language Models (LLMs) and their capabilities in natural language processing tasks.
  - Why needed here: Understanding LLMs is crucial for comprehending ChatGPT's strengths and limitations in generating math questions.
  - Quick check question: What are the key components of an LLM, and how do they contribute to its performance in language-related tasks?

- Concept: Mathematical reasoning and problem-solving skills.
  - Why needed here: To effectively generate math questions, ChatGPT must have a solid understanding of mathematical concepts and the ability to reason through problems.
  - Quick check question: What are the fundamental steps involved in mathematical problem-solving, and how can they be applied to generate diverse math questions?

- Concept: Prompt engineering and constraint-based generation.
  - Why needed here: Crafting effective prompts with appropriate constraints is essential for guiding ChatGPT to produce high-quality math questions.
  - Quick check question: What are some common techniques for prompt engineering, and how can they be applied to improve the quality and diversity of generated math questions?

## Architecture Onboarding

- Component map:
  - ChatGPT (language model)
  - TopicMath dataset (math curriculum and lessons)
  - Prompt templates (for context-aware and context-unaware settings)
  - Evaluation metrics (automatic and human-based)
  - Fine-tuned baseline models (for comparison)

- Critical path:
  1. Collect and prepare the TopicMath dataset.
  2. Design and implement prompt templates for both context-aware and context-unaware settings.
  3. Generate math questions using ChatGPT with the designed prompts.
  4. Evaluate the generated questions using automatic metrics and human evaluations.
  5. Compare ChatGPT's performance with fine-tuned baseline models.

- Design tradeoffs:
  - Using a large language model like ChatGPT allows for flexibility in generating diverse questions but may result in less control over the difficulty and mathematical depth of the generated questions.
  - Relying on a comprehensive dataset like TopicMath ensures a wide coverage of math topics but requires significant effort in data collection and curation.
  - Employing both automatic and human evaluations provides a holistic assessment of the generated questions but increases the overall evaluation time and cost.

- Failure signatures:
  - Generated questions that are too simple or repetitive, lacking mathematical depth.
  - Questions that are not well-aligned with the intended topic or difficulty level.
  - Grammatical errors or inconsistencies in the generated questions.

- First 3 experiments:
  1. Generate a set of math questions using ChatGPT with a simple prompt template and evaluate their quality using automatic metrics and human evaluations.
  2. Compare ChatGPT's performance with a fine-tuned baseline model on a specific math topic or difficulty level.
  3. Experiment with different prompt templates and constraints to improve the quality and diversity of the generated math questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ChatGPT's performance on math question generation vary across different mathematical domains (e.g., algebra, geometry, calculus)?
- Basis in paper: [explicit] The paper mentions that ChatGPT's performance varies across different math topics and lessons, but does not provide a detailed breakdown by mathematical domain.
- Why unresolved: The paper focuses on a broad evaluation across pre-university levels but does not analyze domain-specific strengths and weaknesses.
- What evidence would resolve it: A detailed analysis of ChatGPT's performance on math question generation across different mathematical domains, including quantitative metrics and qualitative assessments.

### Open Question 2
- Question: Can ChatGPT be effectively fine-tuned to improve its performance on generating math questions that require multi-step reasoning?
- Basis in paper: [inferred] The paper mentions that existing state-of-the-art models struggle with generating questions involving multi-step reasoning, and ChatGPT's performance on complex problems is not as strong as simpler ones.
- Why unresolved: The paper only evaluates ChatGPT in a zero-shot setting and does not explore the potential benefits of fine-tuning.
- What evidence would resolve it: Experiments comparing ChatGPT's performance on multi-step reasoning questions before and after fine-tuning, along with an analysis of the fine-tuning process and its impact.

### Open Question 3
- Question: How does the quality of ChatGPT-generated math questions compare to those created by human experts in terms of educational effectiveness?
- Basis in paper: [explicit] The paper evaluates ChatGPT's generated questions using human evaluators, but does not directly compare them to human-created questions.
- Why unresolved: The human evaluation focuses on the quality of ChatGPT's questions but does not benchmark them against expert-created questions.
- What evidence would resolve it: A comparative study where human experts evaluate both ChatGPT-generated and human-created math questions using the same criteria, along with an analysis of student performance on these questions.

## Limitations

- The study relies on a small pool of 4 native English speakers for human evaluation, which may not capture the full diversity of perspectives needed for robust assessment of educational content quality.
- Automatic evaluation metrics (BLEU-4, ROUGE-L, METEOR, BERTScore) are primarily designed for text generation tasks and may not fully capture the mathematical correctness and pedagogical value of generated questions.
- The study lacks direct comparison with dedicated math question generation systems that have been specifically trained on educational datasets, limiting the ability to contextualize ChatGPT's performance relative to specialized approaches.

## Confidence

**High Confidence**: ChatGPT consistently generates grammatically correct questions that are contextually relevant when provided with clear constraints and demonstrations. This is supported by both automatic metrics showing BLEU-4 scores of 19.48-23.57 on established benchmarks and human evaluations consistently scoring grammaticality above 4.9.

**Medium Confidence**: ChatGPT's ability to generate questions across diverse mathematical topics is demonstrated, but the quality varies significantly depending on topic complexity and difficulty level. The study shows ChatGPT excels at basic arithmetic and real-world scenarios but struggles with abstract concepts and maintaining consistent difficulty levels.

**Low Confidence**: The study's findings about ChatGPT's effectiveness as a math questioner for pre-university education are limited by the evaluation methodology. Without comparison to specialized math question generation systems or long-term studies on student learning outcomes, it's difficult to assess whether ChatGPT-generated questions are truly effective for educational purposes.

## Next Checks

1. **Comparative Performance Analysis**: Conduct head-to-head comparisons between ChatGPT and dedicated math question generation systems (such as those using encoder-decoder architectures trained specifically on educational datasets) on the same evaluation benchmarks to establish relative performance.

2. **Longitudinal Learning Impact Study**: Implement a controlled study where students learn from both ChatGPT-generated and human-generated questions, measuring learning outcomes, engagement, and retention over time to assess educational effectiveness beyond question quality metrics.

3. **Scaling Analysis**: Systematically vary the number and quality of demonstration examples provided to ChatGPT across different mathematical domains and difficulty levels to identify optimal prompting strategies and quantify the relationship between prompt quality and output quality.