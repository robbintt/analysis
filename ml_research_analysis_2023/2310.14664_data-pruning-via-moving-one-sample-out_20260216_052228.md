---
ver: rpa2
title: Data Pruning via Moving-one-Sample-out
arxiv_id: '2310.14664'
source_url: https://arxiv.org/abs/2310.14664
tags:
- moso
- training
- pruning
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel data pruning method called Moving-one-Sample-out
  (MoSo) to identify and remove the least informative samples from training sets.
  MoSo measures sample importance by assessing how the optimal empirical risk changes
  when a sample is excluded from training.
---

# Data Pruning via Moving-one-Sample-out

## Quick Facts
- arXiv ID: 2310.14664
- Source URL: https://arxiv.org/abs/2310.14664
- Reference count: 40
- Primary result: MoSo significantly outperforms state-of-the-art data pruning methods on CIFAR-100, Tiny-ImageNet, and ImageNet-1K, especially at high pruning ratios (0.6-0.8).

## Executive Summary
This paper introduces Moving-one-Sample-out (MoSo), a novel data pruning method that identifies the least informative samples by measuring how the optimal empirical risk changes when each sample is excluded from training. Unlike existing approaches that rely on difficulty metrics or influence functions, MoSo directly quantifies each sample's impact on model performance. The authors propose an efficient first-order approximation that reduces computational complexity from O(Tn²) to O(Tn) while maintaining effectiveness. Experiments demonstrate that MoSo achieves superior performance across multiple pruning ratios and is robust to label noise and architecture variations.

## Method Summary
MoSo measures sample importance by approximating the change in optimal empirical risk when a sample is removed from the training set. The exact score requires leave-one-out retraining, which is computationally prohibitive. Instead, MoSo uses a gradient-based approximation that computes the expectation of the inner product between individual sample gradients and the average training set gradient across multiple training stages. This approach captures how samples influence the learning process throughout training rather than just at convergence. The method uses 10 randomly sampled time steps during surrogate training to estimate this expectation, achieving linear complexity while maintaining pruning effectiveness.

## Key Results
- MoSo achieves 4.5-5.2% higher accuracy than Core-Set at 80% pruning ratio on CIFAR-100
- Maintains robustness to label noise up to 20% corruption
- Generalizes well across ResNet-50, SENet, and EfficientNet architectures
- Reduces computational complexity from O(Tn²) to O(Tn) compared to exact leave-one-out methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoSo score measures importance by calculating optimal empirical risk change when a sample is removed.
- Mechanism: Quantifies change in optimal empirical risk when excluding samples to differentiate informative samples that reduce risk from harmful noise.
- Core assumption: Change in optimal empirical risk when removing a sample reliably indicates its importance.
- Evidence anchors: [abstract] "The core insight behind MoSo is to determine the importance of each sample by assessing its impact on the optimal empirical risk."
- Break condition: If optimal empirical risk is insensitive to individual samples (e.g., highly redundant datasets), MoSo scores may not distinguish importance effectively.

### Mechanism 2
- Claim: MoSo's gradient-based approximator efficiently estimates importance through gradient alignment with average training set gradients.
- Mechanism: Samples with gradients consistently aligned with training set average are more informative because optimizing with these samples produces similar effects on all remaining samples.
- Core assumption: Gradient alignment between individual samples and training set average is a valid proxy for sample importance.
- Evidence anchors: [abstract] "The key idea behind our approximation is that samples with gradients that are consistently aligned with the average gradient of the training set are more informative."
- Break condition: If gradients become highly volatile or non-stationary during training, alignment-based approximation may become unreliable.

### Mechanism 3
- Claim: MoSo's training-dynamic-awareness improves measurement by considering information from multiple training stages.
- Mechanism: Incorporating gradient information from different training stages captures how samples influence the learning process throughout training, not just at convergence.
- Core assumption: Samples consistently important across multiple training stages are more likely to be genuinely informative.
- Evidence anchors: [abstract] "The second limitation is addressed since MoSo comprehensively considers information from different training stages."
- Break condition: If early training stages are dominated by random initialization effects, incorporating early-stage gradients may add noise rather than signal.

## Foundational Learning

- Concept: Empirical Risk Minimization
  - Why needed here: MoSo fundamentally relies on measuring changes in empirical risk when samples are removed.
  - Quick check question: What is the difference between empirical risk and true risk in machine learning?

- Concept: Gradient Descent Optimization
  - Why needed here: MoSo approximator uses gradient information from training iterations.
  - Quick check question: How does the direction of a gradient vector relate to the optimization objective in gradient descent?

- Concept: Leave-One-Out Cross-Validation
  - Why needed here: Exact MoSo score conceptually resembles leave-one-out analysis applied to risk.
  - Quick check question: What is the computational complexity of exact leave-one-out cross-validation for a dataset with n samples?

## Architecture Onboarding

- Component map: Surrogate network training -> Gradient collection across epochs -> MoSo score computation -> Sample ranking -> Pruning decision -> Final model training on pruned dataset
- Critical path: Surrogate network training → Gradient collection across epochs → MoSo score computation → Sample ranking → Pruning decision → Final model training on pruned dataset
- Design tradeoffs: Accuracy vs. efficiency tradeoff between exact leave-one-out retraining (high accuracy, infeasible computation) and gradient-based approximation (lower accuracy, practical computation)
- Failure signatures: (1) Poor pruning performance indicates MoSo scores not capturing true importance; (2) Excessive computation time suggests parallelization not properly implemented; (3) Inconsistent results across runs may indicate insufficient gradient sampling
- First 3 experiments:
  1. Implement exact MoSo score calculation on a tiny dataset (n<10) to verify correctness against ground truth.
  2. Compare approximated MoSo scores against exact scores on small datasets to measure approximation error.
  3. Test MoSo pruning on CIFAR-10 with varying pruning ratios to establish baseline performance characteristics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MoSo method perform on datasets with extremely high noise rates (e.g., 50% or more)?
- Basis in paper: The paper mentions that MoSo's effectiveness is not guaranteed when noise dominates the dataset.
- Why unresolved: The paper only tests MoSo's robustness up to 20% label noise.
- What evidence would resolve it: Experiments on synthetic datasets with varying noise rates (30%, 50%, 70%) and comparison with other noise-robust data pruning methods.

### Open Question 2
- Question: Can MoSo be effectively extended to non-classification tasks like object detection or semantic segmentation?
- Basis in paper: The paper only evaluates MoSo on classification tasks and mentions multimodal tasks are worth considering.
- Why unresolved: The paper does not explore MoSo's applicability beyond image classification.
- What evidence would resolve it: Applying MoSo to object detection datasets (COCO) and segmentation datasets (Cityscapes).

### Open Question 3
- Question: What is the optimal number of time steps to sample when calculating the MoSo score for a given dataset size?
- Basis in paper: The paper mentions sampling fewer time steps reduces computational cost but may affect quality.
- Why unresolved: The paper only tests sampling 10 time steps without exploring this hyperparameter.
- What evidence would resolve it: Systematic experiments varying time steps (5, 10, 20, 50) on multiple datasets with different sizes.

## Limitations

- The gradient-alignment proxy assumes training dynamics characteristics that may not hold for all loss landscapes.
- Performance may be sensitive to optimizer choice, learning rate schedules, and batch size.
- Results on ResNet-50 may not generalize to architectures like transformers or recurrent networks.

## Confidence

**High confidence:** Empirical improvements over state-of-the-art methods are well-documented across multiple datasets and pruning ratios.

**Medium confidence:** Theoretical justification for gradient alignment as importance proxy relies on assumptions requiring further validation.

**Low confidence:** Method's behavior in extreme pruning scenarios (above 80%) and with different optimization algorithms remains unclear.

## Next Checks

1. Implement exact MoSo scores on a small dataset (n<50) and compare against approximated scores to quantify approximation error.

2. Repeat key experiments using Adam and SGD with momentum to assess optimizer sensitivity.

3. Apply MoSo to a significantly different architecture (Vision Transformer) to test generalization beyond standard CNNs.