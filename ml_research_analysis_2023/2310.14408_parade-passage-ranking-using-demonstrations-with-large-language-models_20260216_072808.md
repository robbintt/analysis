---
ver: rpa2
title: 'PaRaDe: Passage Ranking using Demonstrations with Large Language Models'
arxiv_id: '2310.14408'
source_url: https://arxiv.org/abs/2310.14408
tags:
- demonstrations
- ranking
- shot
- demonstration
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for improving passage reranking with
  large language models (LLMs) by selecting challenging demonstrations for few-shot
  prompting. The key idea is to use difficulty-based selection, which chooses demonstrations
  that the LLM finds hard to rank correctly.
---

# PaRaDe: Passage Ranking using Demonstrations with Large Language Models

## Quick Facts
- arXiv ID: 2310.14408
- Source URL: https://arxiv.org/abs/2310.14408
- Reference count: 20
- Primary result: Difficulty-based demonstration selection improves LLM passage reranking, outperforming zero-shot and matching manual selection

## Executive Summary
This paper addresses the challenge of improving passage reranking with large language models (LLMs) by developing a novel demonstration selection strategy. Rather than using semantic similarity to select demonstrations, the authors propose difficulty-based selection (DBS) that identifies challenging examples the LLM struggles to rank correctly. The method leverages query likelihood scoring to rank demonstrations by difficulty, selecting those with lowest likelihood for inclusion in prompts. Experiments show DBS significantly outperforms zero-shot reranking and matches the performance of manually selected demonstrations across multiple datasets.

## Method Summary
The method involves retrieving top-100 documents per query using BM25, then reranking them using an LLM with query likelihood scoring. The key innovation is demonstration selection: DBS ranks candidate demonstrations by their query likelihood score (DQL), selecting those with lowest likelihood as most challenging. These demonstrations are included in the prompt to guide the LLM's ranking. The approach is tested on TREC 2019/2020 datasets and BEIR benchmarks using Flan-T5-XXL as the ranking model.

## Key Results
- Difficulty-based selection (DBS) outperforms zero-shot reranking across TREC and BEIR datasets
- DBS performance matches that of manually selected demonstrations
- Including even one well-chosen demonstration significantly improves ranking performance
- Demonstrations effective for ranking also transfer to question generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Demonstrations selected based on difficulty (low likelihood) improve ranking performance by targeting samples that would produce large gradient updates if the model were fine-tuned.
- **Mechanism**: DBS ranks demonstrations by their query likelihood score, selecting those with lowest likelihood as most difficult.
- **Core assumption**: The relationship between demonstration difficulty and ranking performance mirrors the relationship between hard samples and effective gradient updates in fine-tuning.
- **Evidence anchors**: Abstract states DBS is based on difficulty rather than semantic similarity; section defines DQL as DQL(z) ∝ 1/|q(z)| log P(q(z)|d(z))
- **Break condition**: If LLM in-context learning dynamics do not align with fine-tuning gradient behavior

### Mechanism 2
- **Claim**: Including demonstrations in prompts for ranking tasks is more effective than zero-shot approaches when demonstrations are selected appropriately.
- **Mechanism**: DBS demonstrates that adding challenging demonstrations to the prompt string improves performance over zero-shot methods.
- **Core assumption**: LLMs can learn to rank effectively through in-context learning when provided with appropriate demonstrations.
- **Evidence anchors**: Abstract states adding even one demonstration is significantly beneficial; Figure 1 shows LLMs are sensitive to demonstration choice
- **Break condition**: If LLM cannot effectively perform in-context learning for ranking tasks

### Mechanism 3
- **Claim**: Demonstrations effective for ranking are also effective for question generation, suggesting shared underlying mechanisms.
- **Mechanism**: Same demonstrations selected for ranking also improve question generation quality, indicating they teach useful representations for both tasks.
- **Core assumption**: Representations learned from demonstrations for ranking transfer to question generation tasks.
- **Evidence anchors**: Abstract states demonstrations helpful for ranking are also effective at question generation; section shows findings indicate shared effectiveness
- **Break condition**: If transfer between ranking and question generation does not hold across different datasets or LLM architectures

## Foundational Learning

- **Concept**: In-context learning
  - Why needed here: Paper relies on LLMs learning to rank through demonstrations rather than fine-tuning
  - Quick check question: What is the difference between in-context learning and traditional fine-tuning?

- **Concept**: Query likelihood ranking
  - Why needed here: Paper uses query likelihood scoring as ranking mechanism for both zero-shot and few-shot approaches
  - Quick check question: How does query likelihood differ from other ranking methods like cross-entropy?

- **Concept**: Demonstration selection strategies
  - Why needed here: Paper compares different strategies (random, similarity-based, difficulty-based) for selecting demonstrations
  - Quick check question: Why might difficulty-based selection outperform semantic similarity-based selection?

## Architecture Onboarding

- **Component map**: BM25 retrieval -> demonstration selection (DBS or manual) -> prompt construction -> LLM ranking -> evaluation (nDCG@10)
- **Critical path**: BM25 retrieval → demonstration selection → prompt construction → LLM ranking → evaluation (nDCG@10)
- **Design tradeoffs**: Trade-off between demonstration quality (affecting ranking performance) and computational cost (selecting demonstrations vs. random sampling)
- **Failure signatures**: Poor ranking performance could indicate ineffective demonstration selection, LLM unable to learn from demonstrations, or prompt format issues
- **First 3 experiments**:
  1. Compare zero-shot vs. one-shot ranking with random demonstrations to establish baseline sensitivity
  2. Test DBS on TREC datasets to validate difficulty-based selection effectiveness
  3. Evaluate DBS demonstrations on question generation to test transferability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does demonstration selection improve ranking performance when using non-zero-shot reranking methods?
- Basis in paper: [explicit] Paper mentions they only use zero-shot reranking and suggest future work to explore other ranking paradigms
- Why unresolved: Paper does not provide evidence or analysis on how demonstration selection affects non-zero-shot reranking methods
- What evidence would resolve it: Experiments comparing effectiveness of demonstration selection on both zero-shot and non-zero-shot reranking methods

### Open Question 2
- Question: Is there a correlation between the difficulty of demonstrations and their effectiveness for ranking tasks?
- Basis in paper: [explicit] DBS proposed based on assumption that hard demonstrations may correspond to large gradient updates, but no direct evidence on correlation between difficulty and ranking performance
- Why unresolved: Paper does not provide empirical evidence to support assumption that hard demonstrations are more effective for ranking
- What evidence would resolve it: Experiments measuring correlation between demonstration difficulty and ranking performance across different datasets and models

### Open Question 3
- Question: How does the effectiveness of demonstration selection vary with the size of the training dataset?
- Basis in paper: [inferred] Paper uses MSMarco dataset for demonstration selection, a large-scale dataset; unclear how effectiveness would change with smaller or larger datasets
- Why unresolved: Paper does not explore impact of dataset size on effectiveness of demonstration selection
- What evidence would resolve it: Experiments comparing effectiveness of demonstration selection on datasets of varying sizes

## Limitations

- No mechanistic explanation for why difficulty-based selection works better than semantic similarity approaches
- Results based on specific LLM architectures (Flan-T5-XXL) and may not generalize to other models
- Demonstration selection process is computationally expensive and not well-optimized for large-scale deployment
- Limited ablation studies on prompt formatting and demonstration ordering effects

## Confidence

- Medium confidence: Effectiveness of difficulty-based selection for reranking (supported by TREC and BEIR results, but with limited theoretical grounding)
- Medium confidence: Claim that one demonstration significantly improves performance (shown in Figure 1, but with small sample sizes)
- Low confidence: Transferability of demonstrations between ranking and question generation (supported by limited evidence, needs broader validation)

## Next Checks

1. Conduct ablation studies testing different prompt formats (demonstration ordering, separator tokens, instruction phrasing) to isolate contribution of DBS versus prompt engineering
2. Evaluate DBS performance across additional ranking metrics beyond nDCG@10, such as mean reciprocal rank (MRR) and precision@k, to test robustness
3. Test transferability hypothesis on a broader range of generation tasks (not just question generation) to validate whether DBS demonstrations have general utility across NLP tasks