---
ver: rpa2
title: Variational Linearized Laplace Approximation for Bayesian Deep Learning
arxiv_id: '2302.12565'
source_url: https://arxiv.org/abs/2302.12565
tags:
- approximation
- gaussian
- neural
- posterior
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for approximating the predictive
  posterior distribution of a deep neural network using a sparse Gaussian process
  approximation. The method uses a variational approach based on the dual RKHS formulation
  of GPs to approximate the covariance function of the GP, while retaining the predictive
  mean of the original DNN.
---

# Variational Linearized Laplace Approximation for Bayesian Deep Learning

## Quick Facts
- arXiv ID: 2302.12565
- Source URL: https://arxiv.org/abs/2302.12565
- Reference count: 5
- Key outcome: Introduces VaLLA method for scalable uncertainty estimation in deep learning using variational sparse GP approximation that retains DNN predictive mean

## Executive Summary
This paper presents Variational Linearized Laplace Approximation (VaLLA), a method for approximating the predictive posterior distribution of deep neural networks. VaLLA combines the linearized Laplace approximation with a variational sparse Gaussian process approach, retaining the pre-trained DNN's MAP prediction as the mean function while approximating the covariance using inducing points in the RKHS dual formulation. The method achieves sub-linear training time with respect to dataset size, making it suitable for large-scale applications while providing improved uncertainty quantification compared to Nyström-based approximations.

## Method Summary
VaLLA approximates the predictive posterior of a deep neural network by retaining the MAP solution's predictive mean while learning a sparse Gaussian process to approximate the covariance function. The method uses the dual RKHS formulation of GPs, where inducing points are optimized via stochastic gradient descent to capture the essential covariance structure without requiring all training data. This variational approach decouples the mean (fixed to the pre-trained DNN output) from the covariance approximation, enabling scalable inference with computational cost independent of training set size.

## Key Results
- VaLLA outperforms Nyström-based ELLA in both predictive distribution quality and computational time on synthetic regression tasks
- Training cost is independent of the number of training points, enabling scalability to large datasets
- The KL divergence between true GP and VaLLA approximation (110.79) is significantly lower than Nyström approximation (245.34)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VaLLA retains the predictive mean of the original DNN while approximating the covariance using a sparse GP.
- Mechanism: By fixing the mean function to the pre-trained DNN's MAP prediction in the RKHS formulation, VaLLA avoids the under-fitting issue of standard LLA while still enabling scalable uncertainty estimation.
- Core assumption: The pre-trained DNN's output lies in or can be well-approximated by the RKHS.
- Evidence anchors:
  - [abstract]: "retains, as the predictive mean, the output of the original DNN"
  - [section]: "Proposition 1. If g(·, ˆθ)∈H, ∀ϵ > 0, there exists a set of Mα inducing points Zα ⊂X and a collection of scalar values a ∈ RMα such that the dual representation in the RKHS of the corresponding sparse Gaussian process... corresponds to a posterior approximation GP (m⋆,K⋆) with mean and covariance functions defined as m⋆(x) = hϵ(x)..."
  - [corpus]: No direct evidence found in corpus papers for RKHS embedding of DNNs; this is a novel contribution.
- Break condition: If the DNN's function space is not well-approximated by the chosen RKHS (e.g., highly discontinuous or non-smooth functions), the mean retention fails and VaLLA loses its advantage.

### Mechanism 2
- Claim: VaLLA achieves sub-linear training time in the size of the training dataset.
- Mechanism: By using inducing points and stochastic optimization, VaLLA avoids the cubic scaling of exact GP methods and the linear scaling of Nyström-based approximations.
- Core assumption: The variational approach with inducing points can capture the essential covariance structure without needing all training points.
- Evidence anchors:
  - [abstract]: "its training cost is independent of the number of training points, making it suitable for large datasets"
  - [section]: "The variational approach proposed has a computational cost that does not depend on N, the number of observed data instances"
  - [corpus]: No direct evidence in corpus; this is a novel scalability claim specific to VaLLA's sparse GP formulation.
- Break condition: If the number of inducing points needed grows with dataset size to maintain accuracy, the claimed independence breaks down.

### Mechanism 3
- Claim: VaLLA provides better uncertainty quantification than Nyström-based approximations like ELLA.
- Mechanism: By optimizing inducing point locations rather than using random sub-sampling, VaLLA can better capture the true covariance structure of the linearized DNN.
- Evidence anchors:
  - [section]: "The capability of tuning the inducing locations provides better uncertainty estimations compared with the Nyström approximation, which relies on random sub-sampling of the training set"
  - [section]: "the KL divergence... between the true GP distribution and the obtained by the Nyström approximation is 245.34 whereas the one obtained using VaLLA is 110.79"
  - [corpus]: No direct comparison evidence in corpus papers; this is specific to VaLLA vs. ELLA comparison.
- Break condition: If the optimization of inducing points gets stuck in poor local minima or if the problem structure doesn't benefit from location tuning, the advantage over random sampling diminishes.

## Foundational Learning

- Concept: Laplace Approximation (LA)
  - Why needed here: LLA builds on LA by linearizing the model; understanding LA is essential to grasp why linearization helps with under-fitting.
  - Quick check question: What is the key difference between LA and LLA in terms of the model used for predictions?

- Concept: Gaussian Process (GP) dual formulation in RKHS
  - Why needed here: VaLLA's core innovation relies on the RKHS dual representation to fix the mean while approximating covariance sparsely.
  - Quick check question: How does the RKHS dual formulation allow us to separate mean and covariance approximation in VaLLA?

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: The NTK appears in the covariance function of the linearized model, and understanding it helps in grasping the kernel structure VaLLA approximates.
  - Quick check question: What role does the Neural Tangent Kernel play in the linearized Laplace approximation?

## Architecture Onboarding

- Component map: Pre-trained DNN -> Inducing point optimization -> Variational sparse GP -> KL divergence computation -> Predictive distribution
- Critical path:
  1. Load pre-trained DNN weights
  2. Initialize inducing points
  3. Optimize inducing points and variational parameters via stochastic gradient descent
  4. Compute predictive distribution using fixed DNN mean and learned sparse covariance

- Design tradeoffs:
  - More inducing points → better approximation but higher computational cost
  - Fixed vs. learned mean function → simplicity vs. potential accuracy gains
  - Stochastic vs. full-batch optimization → scalability vs. stability

- Failure signatures:
  - Predictive uncertainty that is too narrow (under-fitting covariance)
  - Predictive uncertainty that is too wide (over-approximating covariance)
  - Slow convergence of inducing point optimization
  - KL divergence not decreasing during training

- First 3 experiments:
  1. Reproduce 1D synthetic regression results from paper to verify implementation matches reported KL divergence improvements
  2. Compare VaLLA's predictive uncertainty calibration on UCI regression benchmarks vs. exact LLA and ELLA
  3. Stress-test scalability by running VaLLA on increasing dataset sizes and measuring training time independence from N

## Open Questions the Paper Calls Out

- How does the performance of VaLLA scale with increasing number of inducing points M?
- How does VaLLA's uncertainty estimation compare to other scalable BNN methods like MC dropout or SWAG on real-world datasets?
- How sensitive is VaLLA to the choice of inducing point locations Zβ?

## Limitations

- The RKHS embedding assumption for DNN outputs lacks empirical validation beyond the synthetic 1D example
- Scalability claims are based on theoretical analysis rather than extensive large-scale experiments
- The fixed mean function assumption may limit performance on complex datasets where the DNN's MAP solution has systematic biases

## Confidence

- **High confidence**: The mathematical framework for variational sparse GP approximation using RKHS dual formulation
- **Medium confidence**: The computational complexity analysis showing independence from training set size
- **Low confidence**: The empirical superiority claims against Nyström-based methods, as only synthetic experiments are provided

## Next Checks

1. Validate RKHS embedding assumption by testing VaLLA on DNNs trained on non-smooth or discontinuous functions where RKHS approximation may fail
2. Conduct scalability experiments on real-world large datasets (e.g., ImageNet-scale) to verify training time independence from N
3. Compare VaLLA's uncertainty calibration on UCI regression benchmarks against multiple baselines including SWAG, MC dropout, and exact LLA with varying computational budgets