---
ver: rpa2
title: Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning
arxiv_id: '2307.07091'
source_url: https://arxiv.org/abs/2307.07091
tags:
- learning
- tasks
- compositional
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces four large-scale offline RL datasets for
  robotic manipulation, collected from 256 compositional tasks in the CompoSuite benchmark.
  The datasets vary in difficulty (expert, medium, random, medium-replay-subsampled)
  and contain 256 million transitions each.
---

# Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.07091
- Source URL: https://arxiv.org/abs/2307.07091
- Reference count: 27
- Primary result: Four large-scale offline RL datasets for robotic manipulation with 256 million transitions each, enabling benchmarking of compositional RL methods

## Executive Summary
This paper introduces four large-scale offline RL datasets for robotic manipulation, collected from 256 compositional tasks in the CompoSuite benchmark. The datasets vary in difficulty (expert, medium, random, medium-replay-subsampled) and each contains 256 million transitions. The authors provide comprehensive training and evaluation protocols for compositional RL, including uniform, compositional, and restricted sampling strategies. Benchmarking experiments reveal that while current offline RL methods can learn training tasks to some extent, with compositional methods showing advantages over non-compositional ones, all methods struggle to generalize to unseen tasks, highlighting the need for improved compositional RL algorithms.

## Method Summary
The paper introduces four offline RL datasets for robotic manipulation, each containing 256 million transitions from 256 compositional tasks. Tasks are created by composing elements from four axes: robot (3), object (4), obstacle (4), and objective (4). Four dataset variants are collected using PPO-trained agents at different performance levels: expert (90% success), medium (30% success), random (0% success), and medium-replay-subsampled. The paper evaluates three offline RL methods - Behavioral Cloning (BC), Implicit Q-Learning (IQL), and Compositional Policy IQL (CP-IQL) - on these datasets using three sampling strategies: uniform, compositional, and restricted. Performance is measured on both training tasks and zero-shot generalization to unseen task compositions.

## Key Results
- All three evaluated methods (BC, IQL, CP-IQL) achieve high performance on training tasks using expert datasets
- Compositional methods (CP-IQL) outperform non-compositional methods on training tasks across all dataset types
- Zero-shot generalization to unseen tasks remains near zero for all methods, indicating current compositional RL approaches cannot extract compositional structure effectively
- Medium-replay-subsampled dataset poses particular challenges for current offline RL methods despite containing diverse trajectory data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compositional RL enables learning from fewer components to solve many tasks.
- Mechanism: By decomposing tasks into robot, object, obstacle, and objective components, the agent can reuse learned behaviors across tasks sharing the same components.
- Core assumption: The reward function remains constant for a given objective regardless of other components.
- Evidence anchors:
  - [abstract] "it permits creating many tasks from few components"
  - [section] "Every CompoSuite task is created by composing elements of four different axes"
- Break condition: If reward functions change with object or obstacle variations, compositionality fails.

### Mechanism 2
- Claim: Compositional architecture improves training and zero-shot transfer.
- Mechanism: Hierarchical neural network modules process task components separately then combine them, allowing learned modules to transfer to unseen task combinations.
- Core assumption: The compositional structure in the environment data matches the network architecture.
- Evidence anchors:
  - [section] "the compositional network architecture consists of hierarchically stacked modules"
  - [section] "CP-IQL agent achieves the strongest training performance"
- Break condition: If task components interact in non-compositional ways, the architecture cannot capture these dependencies.

### Mechanism 3
- Claim: Offline RL with expert data can learn training tasks but struggles with zero-shot generalization.
- Mechanism: Offline RL learns from fixed datasets without exploration, requiring sufficient data diversity for generalization.
- Core assumption: Expert datasets contain sufficient state-action coverage for the training tasks.
- Evidence anchors:
  - [section] "all three agents can achieve high performance on the expert datasets"
  - [section] "current methods are unable to extract the tasks' compositional structure to generalize to unseen tasks"
- Break condition: If datasets lack diversity or contain significant distribution shift, learning fails.

## Foundational Learning

- Concept: Markov Decision Processes
  - Why needed here: The paper formalizes offline RL as an MDP problem with states, actions, rewards, and transitions.
  - Quick check question: What are the four components of an MDP and how do they relate to reinforcement learning?

- Concept: Function composition in neural networks
  - Why needed here: The compositional architecture uses hierarchical function composition to process task components.
  - Quick check question: How does function composition differ from temporal composition in reinforcement learning?

- Concept: Distribution shift in offline RL
  - Why needed here: Offline RL agents cannot explore new states and must generalize from training data, leading to distribution shift.
  - Quick check question: What is distribution shift and why is it a particular challenge for offline reinforcement learning?

## Architecture Onboarding

- Component map: obstacle → object → objective → robot (16 MLP modules total)
- Critical path:
  1. State features → module-specific pre-processing
  2. Pre-processed outputs → concatenation with previous module output
  3. Concatenated input → post-processing MLP
  4. Final robot module output → policy action
- Design tradeoffs:
  - Fixed hierarchical structure vs. learned connectivity
  - Explicit compositional bias vs. general function approximation
  - Module specialization vs. parameter sharing
- Failure signatures:
  - Poor training performance on training tasks
  - Zero-shot success rates near zero
  - Large gap between training and zero-shot performance
- First 3 experiments:
  1. Train BC, IQL, and CP-IQL on expert dataset with uniform sampling
  2. Train on medium-replay dataset with uniform sampling
  3. Train on expert-random combination with compositional sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does compositional structure in datasets influence offline RL generalization?
- Basis in paper: [explicit] The paper states that current methods fail to extract compositional structure to generalize to unseen tasks, and that compositional methods outperform non-compositional ones.
- Why unresolved: The paper only tests a limited set of compositional methods and baselines, and does not explore the full space of possible compositional architectures or training strategies.
- What evidence would resolve it: Systematic experiments comparing different compositional architectures, training regimes, and datasets with varying degrees of compositional structure.

### Open Question 2
- Question: What are the benefits and limitations of using replay data for offline RL?
- Basis in paper: [explicit] The paper introduces a medium-replay-subsampled dataset, which contains data from trajectories during training of an online RL agent. The authors note that this setting should be sufficient to learn good policies via offline RL, yet current approaches struggle in this setting.
- Why unresolved: The paper only evaluates a few baselines on this dataset and does not explore the reasons for the failure of current methods. It also does not compare the replay dataset to other types of datasets, such as expert or medium datasets.
- What evidence would resolve it: Detailed analysis of the distribution and quality of replay data, comparison of replay data to other types of data, and exploration of methods that can handle the challenges of replay data.

### Open Question 3
- Question: How can offline RL methods be adapted to leverage compositional structure in datasets?
- Basis in paper: [explicit] The paper suggests that compositional methods can outperform non-compositional methods, and that compositional structure can facilitate transfer to unseen tasks. However, the authors also note that current methods are unable to extract the compositional structure to generalize to unseen tasks.
- Why unresolved: The paper does not provide a clear explanation of why current methods fail to leverage compositional structure, and does not propose specific solutions or modifications to existing methods.
- What evidence would resolve it: Development and evaluation of new offline RL methods that explicitly model and exploit compositional structure, and comparison of their performance to existing methods.

## Limitations

- All methods fail to generalize to unseen tasks despite compositional architecture advantages
- Limited exploration of alternative compositional architectures and training strategies
- No analysis of why compositional methods fail at generalization despite success on training tasks

## Confidence

High confidence: The dataset collection methodology and benchmark design are well-specified and reproducible. The claim that current methods struggle with compositional generalization is strongly supported by experimental results.

Medium confidence: The assertion that the compositional architecture should enable better generalization, while intuitively plausible, lacks empirical validation given the observed failure to generalize.

Low confidence: Claims about the specific mechanisms by which compositional structure should improve learning are not fully supported by the results, as the architecture fails to deliver the expected generalization benefits.

## Next Checks

1. Conduct ablation studies varying the degree of compositional bias in the architecture to determine if simpler compositional structures might work better than the proposed hierarchical approach.

2. Analyze the dataset statistics to identify whether distribution shift or insufficient diversity in the training data explains the generalization failures.

3. Implement and test alternative compositional architectures that explicitly model component interactions rather than assuming purely hierarchical composition.