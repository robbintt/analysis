---
ver: rpa2
title: Softmax Bias Correction for Quantized Generative Models
arxiv_id: '2309.01729'
source_url: https://arxiv.org/abs/2309.01729
tags:
- softmax
- quantization
- bias
- correction
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the sensitivity of softmax activation in generative
  models, such as stable diffusion and large language models, to quantization noise.
  The authors propose an offline bias correction technique to improve the quantizability
  of softmax without additional compute during deployment.
---

# Softmax Bias Correction for Quantized Generative Models

## Quick Facts
- arXiv ID: 2309.01729
- Source URL: https://arxiv.org/abs/2309.01729
- Reference count: 40
- The study proposes an offline bias correction technique to improve the quantizability of softmax activation in generative models without additional compute during deployment.

## Executive Summary
This paper addresses a critical quantization challenge in generative models by targeting the sensitivity of softmax activation to quantization noise. The authors introduce an offline bias correction method that calculates empirical bias estimates using calibration data and absorbs the correction into asymmetric quantization parameters. The technique is demonstrated on stable diffusion v1.5 and a 125M-size OPT language model, showing significant improvements in both SQNR and perplexity metrics for 8-bit quantized softmax.

## Method Summary
The method involves calculating an empirical estimate of quantization bias in softmax outputs using calibration data, then absorbing this correction factor into the zero-point of asymmetric quantization. This offline approach requires no additional computation during deployment. The bias correction can be applied at different granularities (per-tensor or per-attention-head), with per-attention-head correction providing the best accuracy-computation tradeoff. The technique maintains the normalization property of softmax while improving quantization performance.

## Key Results
- Significant improvement in SQNR for 8-bit quantized softmax in stable diffusion v1.5
- Better perplexity scores for 8-bit quantized softmax in 125M-size OPT language model
- Per-attention-head bias correction scheme performs on par or better than all other schemes
- Bias correction can be absorbed into quantization parameters without additional compute during deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Softmax outputs are sensitive to quantization because most quantized values round to zero, causing a systematic bias.
- Mechanism: Quantization rounds small softmax values to zero, breaking the normalization property (sum to 1) and leading to biased probability distributions.
- Core assumption: Softmax outputs follow a smooth distribution where many values are small but non-zero.
- Evidence anchors:
  - [abstract] "quantization operation leads to a large bias in the softmax output, causing accuracy degradation"
  - [section 2.3] "up to 99% of the values are rounded to zero" and "quantization error is biased, and the softmax probabilities are not correctly normalized anymore"
  - [corpus] No direct evidence, but related work on quantization bias exists
- Break condition: If softmax output distribution is extremely sparse with only a few large values, quantization bias may be negligible.

### Mechanism 2
- Claim: Bias correction can be absorbed into asymmetric quantization parameters without additional compute.
- Mechanism: The bias correction factor is added to the zero-point of asymmetric quantization, effectively shifting the quantization grid.
- Core assumption: Asymmetric quantization is used with a floating-point offset that can be modified.
- Evidence anchors:
  - [section 3.2] "we only have to absorb the correction factor into the offset, c′ = s · z − β, while keeping everything else the same"
  - [section 3.1] "it can be readily absorbed into the quantization parameters"
  - [corpus] No direct evidence, but this is a standard technique in quantization literature
- Break condition: If symmetric quantization is used instead of asymmetric, this absorption technique won't work.

### Mechanism 3
- Claim: Per-attention-head bias correction provides the best trade-off between accuracy and computational overhead.
- Mechanism: Applying bias correction at the granularity of individual attention heads captures local distribution characteristics while minimizing computation.
- Core assumption: Attention heads have distinct output distributions that benefit from separate bias correction.
- Evidence anchors:
  - [section 4.1] "the per-attention head correction scheme performs on par or better than all other schemes"
  - [section 3.1] "we could have a per-tensor or per attention-head correction factor"
  - [corpus] No direct evidence, but per-channel techniques are common in quantization
- Break condition: If attention heads have very similar distributions, per-tensor correction may be sufficient.

## Foundational Learning

- Concept: Asymmetric quantization with zero-point offset
  - Why needed here: The bias correction technique relies on modifying the zero-point of asymmetric quantization
  - Quick check question: What is the mathematical relationship between the zero-point and the bias correction factor?

- Concept: Softmax normalization and probability distributions
  - Why needed here: Understanding why quantization breaks the normalization property is crucial to grasping the bias problem
  - Quick check question: If softmax outputs are [0.1, 0.2, 0.7] and quantization rounds them to [0, 0, 1], what is the sum and why is it problematic?

- Concept: Signal-to-Quantization Noise Ratio (SQNR)
  - Why needed here: SQNR is used to measure the effectiveness of the bias correction technique
  - Quick check question: Given a signal with power 10 and quantization noise with power 2, what is the SQNR in dB?

## Architecture Onboarding

- Component map:
  - Pre-trained FP32 model -> Calibration data -> Bias calculator -> Asymmetric quantizer with modified zero-point -> Quantized model

- Critical path:
  1. Load pre-trained model and calibration data
  2. Compute softmax outputs on calibration data
  3. Calculate bias correction factors per attention head
  4. Modify quantization parameters to absorb bias
  5. Perform quantization of weights and activations

- Design tradeoffs:
  - Per-tensor vs. per-attention-head bias correction: Accuracy vs. computation
  - Time-step aware vs. static correction: Better accuracy vs. more complex implementation
  - 8-bit vs. higher precision: Performance vs. quality

- Failure signatures:
  - SQNR remains low after bias correction: Bias calculation or absorption may be incorrect
  - Generated images/text quality degrades: Bias correction may be overcompensating or undercompensating
  - Calibration set too small: Bias estimates may be unreliable

- First 3 experiments:
  1. Apply bias correction at per-tensor granularity and measure SQNR improvement
  2. Compare per-tensor vs. per-attention-head bias correction on a subset of layers
  3. Test time-step aware bias correction on a small diffusion model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bias correction scale with increasingly large sequence lengths, and at what point does the computational overhead outweigh the benefits?
- Basis in paper: [inferred] The paper mentions that softmax accounts for a significant fraction of runtime in transformers, especially for sequence lengths larger than 2048, and suggests that low-bit softmax is imperative for on-device performance. However, it does not explore the scalability of bias correction with increasing sequence lengths.
- Why unresolved: The paper does not provide empirical data on the performance of bias correction for sequence lengths beyond those tested, nor does it analyze the trade-off between computational overhead and accuracy improvement at larger scales.
- What evidence would resolve it: Experiments testing bias correction on transformers with varying sequence lengths, particularly those exceeding 4096, and a detailed analysis of the computational cost versus accuracy gain at each scale.

### Open Question 2
- Question: Is the bias correction method applicable and beneficial for other activation functions beyond softmax, such as GELU or ReLU, in generative models?
- Basis in paper: [inferred] The paper focuses on the sensitivity of softmax to quantization noise and introduces a bias correction technique specifically for softmax. However, it does not explore whether similar quantization biases exist in other activation functions or if the proposed method can be generalized.
- Why unresolved: The study is limited to softmax activations in transformers and diffusion models, without investigating the quantization behavior of other activations or the potential for extending the bias correction approach.
- What evidence would resolve it: A comprehensive study applying the bias correction technique to various activation functions in different neural network architectures, measuring the quantization bias and performance impact for each.

### Open Question 3
- Question: What is the impact of the bias correction method on the convergence and stability of fine-tuning or training quantized models from scratch?
- Basis in paper: [inferred] The paper presents an offline bias correction technique for post-training quantization without additional compute during deployment. It does not address how this correction might affect the training dynamics if applied during fine-tuning or training.
- Why unresolved: The method is designed for post-training quantization, and there is no exploration of its effects on the training process or whether it could be integrated into quantization-aware training.
- What evidence would resolve it: Empirical studies comparing the training and fine-tuning performance of models with and without the bias correction integrated into the training loop, including metrics on convergence speed and stability.

### Open Question 4
- Question: How does the bias correction method perform with different quantization schemes, such as non-uniform or learned quantization, and are there specific schemes that synergize better with the correction?
- Basis in paper: [explicit] The paper mentions the use of per-tensor symmetric quantization for weights and asymmetric quantization for activations but does not explore other quantization schemes or their interaction with the bias correction method.
- Why unresolved: The study is limited to a specific quantization scheme, and there is no analysis of how alternative quantization methods might affect the efficacy of the bias correction or if certain schemes are more compatible.
- What evidence would resolve it: Comparative experiments using various quantization schemes, including non-uniform and learned quantization, with and without the bias correction, to determine which combinations yield the best performance in terms of accuracy and efficiency.

## Limitations
- The method relies on asymmetric quantization and may not work with symmetric quantization schemes commonly used in production systems.
- The paper doesn't specify calibration data requirements or analyze sensitivity to calibration data quality.
- Generalizability to other generative model architectures beyond stable diffusion and OPT remains unverified.

## Confidence
- **High Confidence**: The mechanism of quantization-induced softmax bias and the mathematical formulation for bias correction are well-established and reproducible.
- **Medium Confidence**: The per-attention-head correction approach appears optimal but lacks comprehensive comparison with alternative granularities.
- **Low Confidence**: Claims about generalizability to other architectures and deployment impact lack empirical validation.

## Next Checks
1. Test the bias correction technique on additional generative models including GANs and autoregressive transformers beyond the stable diffusion and OPT models evaluated in the paper.
2. Systematically vary the size and composition of calibration datasets to determine minimum requirements for reliable bias estimation.
3. Measure actual inference latency and memory usage with the absorbed bias correction compared to naive quantization, including comparison with symmetric quantization alternatives.