---
ver: rpa2
title: 'Focused Transformer: Contrastive Training for Context Scaling'
arxiv_id: '2307.03170'
source_url: https://arxiv.org/abs/2307.03170
tags:
- context
- memory
- training
- length
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Focused Transformer (FoT) to address the
  distraction issue in long-context language models, where irrelevant keys interfere
  with relevant ones. FoT employs a memory attention layer with k-nearest neighbors
  lookup and a contrastive-inspired training procedure that exposes the model to both
  relevant and irrelevant keys.
---

# Focused Transformer: Contrastive Training for Context Scaling

## Quick Facts
- arXiv ID: 2307.03170
- Source URL: https://arxiv.org/abs/2307.03170
- Reference count: 40
- Key outcome: Focused Transformer achieves 94.5% accuracy on 100k-token passkey retrieval and maintains short-context performance through contrastive training that mitigates the "distraction issue"

## Executive Summary
The Focused Transformer (FoT) addresses a critical challenge in long-context language models where irrelevant keys interfere with relevant ones during retrieval, limiting effective context length. The authors introduce memory attention layers that access external memory via k-nearest neighbors (kNN) lookup and a crossbatch training procedure inspired by contrastive learning. This approach improves the structure of the (key, value) space, enabling the model to distinguish between semantically different values even when their keys overlap. The method is applied to fine-tune 3B and 7B OpenLLaMA models, resulting in LongLLaMA, which demonstrates strong performance on both synthetic retrieval tasks and long-context language modeling.

## Method Summary
The Focused Transformer extends standard Transformer architectures with memory attention layers that access an external memory database during inference. Each query in these layers attends to both local context and the top k most matching keys retrieved from memory using kNN search. The crossbatch training procedure exposes these memory attention layers to both relevant and irrelevant keys during training, inspired by contrastive learning. The authors remove positional encodings in memory layers to enable extrapolation beyond training context length. The method is evaluated through fine-tuning 3B and 7B OpenLLaMA models on a dataset mixture based on RedPajama, using 8k context length and specific memory layer configurations.

## Key Results
- Achieves 94.5% accuracy on 100k-token passkey retrieval, demonstrating effective context scaling beyond training length
- Maintains strong performance on short-context tasks while excelling at long-context language modeling on datasets like PG-19 and arXiv
- Outperforms baseline models including Local Attention + NTK-Aware, LongNet, and RePE on long-context tasks while preserving few-shot in-context learning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Crossbatch training with negatives mitigates the "distraction issue" by improving the structure of the (key, value) space.
- Mechanism: During training, the memory attention layer is exposed to keys from both relevant (current document) and irrelevant (other documents) sources. This contrastive-like setup encourages the model to learn representations that distinguish semantically different values, reducing overlap between keys linked to different values.
- Core assumption: The overlap between keys connected to different semantic values is a major cause of poor performance in long-context retrieval and language modeling tasks.
- Evidence anchors:
  - [abstract] "We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish."
  - [section 3.3] "We measure that the attention mass is evenly spread on the related and unrelated documents; see Figure 3. More precisely, for a document δ, let wij be the softmax weights related to pδij constructed as described in Section 3.2. We define the positive attention mass as rd := Σj w1j/Σi=1,...,d Σj wij. We observe that rd ≈ 1/d, which can be interpreted as the fact that the attention is equally distracted by the positive (coming from the current document at i = 1) and negative keys."
- Break condition: If the negative samples do not provide meaningful contrast (e.g., if they are too similar to the positive samples), the model may not learn to effectively distinguish between relevant and irrelevant keys.

### Mechanism 2
- Claim: Memory attention layers with k-nearest neighbors (kNN) lookup effectively extend the context length by retrieving relevant information from an external memory.
- Mechanism: Queries in the memory attention layer attend to the top k most matching keys from the external memory, retrieved using kNN search. This allows the model to access information beyond its local context, effectively extending the context length.
- Core assumption: The kNN search algorithm can efficiently retrieve the most relevant keys from a large external memory, and the model can effectively use these retrieved keys to improve its predictions.
- Evidence anchors:
  - [section 3.1] "Memory attention layers L are endowed with access to an external memory database during inference. Namely, each query in ℓ ∈ L attends to preceding keys from the local context and the top k most matching keys from memory. The memory keys are ranked by the inner product with the query and retrieved using the kNN search algorithm."
  - [section 4.2] "We use passkey retrieval introduced in [Mohtashami and Jaggi, 2023], a synthetic task designed to measure this property. In this task, the model has to retrieve a passkey placed randomly in a long prompt. Results are shown in Figure 1 - importantly, our 3B model is capable of solving this task much beyond its training context length 8K, achieving 94.5% accuracy for prompts of length 100k and 73% for 256k."
- Break condition: If the external memory becomes too large or the kNN search becomes too slow, the method may become impractical. Additionally, if the keys in the memory are not well-structured, the kNN search may not retrieve the most relevant keys.

### Mechanism 3
- Claim: Removing positional encodings in memory attention layers allows for extrapolation beyond the training context length.
- Mechanism: By not using positional encodings in the memory attention layers, the model is not constrained by the positional information learned during training. This allows it to effectively use the external memory for sequences longer than those seen during training.
- Core assumption: Positional encodings are not necessary for the memory attention layers to effectively retrieve relevant information from the external memory.
- Evidence anchors:
  - [section 4.1] "We use L = {6, 12, 18} (resp. L = {8, 16, 24}) as the memory layers for 3B (resp. 7B) LONG LLAMA model. We fine-tune the models on 10B (resp. 3B) tokens using FOT, 8k context length and our dataset mixture based on RedPajama [TogetherComputer, 2023]."
  - [section 4.2] "We first measure the effective context length of LONG LLAMA, namely the distance for which tokens can effectively attend each other. We use passkey retrieval introduced in [Mohtashami and Jaggi, 2023], a synthetic task designed to measure this property. In this task, the model has to retrieve a passkey placed randomly in a long prompt. Results are shown in Figure 1 - importantly, our 3B model is capable of solving this task much beyond its training context length 8K, achieving 94.5% accuracy for prompts of length 100k and 73% for 256k."
- Break condition: If the local context still requires positional information for effective processing, removing positional encodings in the memory layers may lead to a performance degradation.

## Foundational Learning

- Concept: k-Nearest Neighbors (kNN) search algorithm
  - Why needed here: kNN is used to retrieve the most relevant keys from the external memory for each query in the memory attention layer.
  - Quick check question: What is the time complexity of kNN search in a database of size N with k neighbors to find?

- Concept: Contrastive learning
  - Why needed here: The crossbatch training procedure is inspired by contrastive learning, using negative samples to improve the structure of the (key, value) space.
  - Quick check question: In contrastive learning, what is the goal of using negative samples?

- Concept: Transformer architecture
  - Why needed here: The Focused Transformer builds upon the standard Transformer architecture, adding memory attention layers and a novel training procedure.
  - Quick check question: What is the role of the attention mechanism in the Transformer architecture?

## Architecture Onboarding

- Component map: Standard Transformer layers -> Memory attention layers (with kNN lookup) -> Crossbatch training pipeline
- Critical path:
  1. Tokenize input sequence
  2. Process local context through standard Transformer layers
  3. For memory attention layers:
     a. Retrieve top k keys from external memory using kNN
     b. Attend to both local context and retrieved keys
  4. Apply crossbatch training procedure during training
- Design tradeoffs:
  - Using kNN lookup vs. dense attention: kNN is more memory-efficient but may be slower for very large memories
  - Including/excluding positional encodings in memory layers: Excluding them allows for longer context extrapolation but may hurt local context processing
  - Number of negatives (d) in crossbatch training: More negatives can improve the (key, value) space structure but may require more memory and computation
- Failure signatures:
  - Poor performance on long-context tasks: May indicate issues with the (key, value) space structure or kNN retrieval
  - Degradation in short-context performance: May indicate issues with the integration of memory attention layers or the removal of positional encodings
  - High memory usage or slow inference: May indicate issues with the kNN search or the size of the external memory
- First 3 experiments:
  1. Evaluate the impact of the number of negatives (d) on the performance of a small model on a synthetic long-context retrieval task.
  2. Compare the performance of the model with and without positional encodings in the memory attention layers on a long-context language modeling task.
  3. Investigate the scalability of the kNN search by evaluating the model's performance with increasing external memory sizes.

## Open Questions the Paper Calls Out
- The paper discusses that scaling up memory is a significant future research direction and mentions the engineering challenges of storing and efficiently searching through millions of (key, value) pairs.
- The authors note that developing long-context methods is an active research field and believe some of these methods could be combined with FoT, resulting in mutually beneficial interactions.

## Limitations
- The method's effectiveness depends heavily on the quality and diversity of the external memory database, which is not thoroughly characterized.
- The computational overhead of kNN search at inference time, particularly for very large memories, is not fully explored.
- The claim that removing positional encodings enables extrapolation beyond training length is demonstrated but the mechanism is not deeply analyzed.

## Confidence
- High confidence: The core claim that crossbatch training with negative samples improves (key, value) space structure and mitigates the distraction issue, supported by empirical evidence showing improved passkey retrieval accuracy.
- Medium confidence: The claim that memory attention layers with kNN lookup effectively extend context length, as the method shows extrapolation capability but the efficiency and scalability for very large memories needs more validation.
- Medium confidence: The claim that removing positional encodings enables extrapolation beyond training length, as this is demonstrated empirically but the underlying mechanism and potential drawbacks are not fully explored.

## Next Checks
1. **Memory database quality analysis**: Evaluate how the composition and quality of the external memory database affects performance across different domains (code, scientific text, web data) to understand generalization limits.
2. **Inference efficiency benchmarking**: Measure the wall-clock time and memory overhead of kNN search during inference with varying memory sizes (10M, 100M, 1B keys) to determine practical scalability limits.
3. **Real-world task validation**: Test the model on open-ended generation tasks requiring coherent reasoning over long contexts (e.g., multi-document question answering, long-form story continuation) to validate claims beyond synthetic benchmarks.