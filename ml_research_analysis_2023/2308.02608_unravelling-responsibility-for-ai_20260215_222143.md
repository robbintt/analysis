---
ver: rpa2
title: Unravelling Responsibility for AI
arxiv_id: '2308.02608'
source_url: https://arxiv.org/abs/2308.02608
tags:
- responsibility
- human
- individual
- moral
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides a cross-disciplinary vocabulary for discussing\
  \ responsibility in complex situations involving AI-enabled systems. It introduces\
  \ a three-part formulation\u2014\"Actor A is responsible for Occurrence O\"\u2014\
  and breaks down each component into subcategories."
---

# Unravelling Responsibility for AI

## Quick Facts
- arXiv ID: 2308.02608
- Source URL: https://arxiv.org/abs/2308.02608
- Reference count: 9
- Primary result: Provides a cross-disciplinary vocabulary of 85 responsibility strings for discussing responsibility in AI-enabled systems

## Executive Summary
This paper introduces a structured vocabulary for discussing responsibility in complex scenarios involving AI-enabled systems. By decomposing the three-part formulation "Actor A is responsible for Occurrence O" into subcategories of actors, occurrences, and four senses of responsibility, the framework systematically identifies 85 distinct "responsibility strings." These strings are grouped into role-responsibility, causal responsibility, legal liability-responsibility, and moral responsibility. The vocabulary is demonstrated through examples from healthcare AI and an autonomous vehicle accident, aiming to support clear, specific discussions of responsibility across disciplines.

## Method Summary
The authors employ a systematic decomposition approach, breaking down the three-part formulation "Actor A is responsible for Occurrence O" into orthogonal dimensions: Actor types (AI-enabled systems, individual humans, institutions), Occurrence types (decisions, actions, omissions, and consequences), and four senses of responsibility. They identify valid combinations of these subcategories to create "responsibility strings," ensuring each string is unique and represents a distinct way of assigning responsibility. The framework is validated through application to two running examples: a hypothetical Diabetes AI decision support system and the real Uber Tempe autonomous vehicle accident.

## Key Results
- Identifies 85 distinct responsibility strings through systematic decomposition of the triadic formulation
- Groups responsibility strings into four senses: role-responsibility, causal responsibility, legal liability-responsibility, and moral responsibility
- Distinguishes between AI-enabled systems (causally responsible only) and human/institutional actors (eligible for all responsibility types)
- Provides concrete examples demonstrating how the vocabulary clarifies responsibility attribution in complex AI scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The triadic formulation enables systematic decomposition by separating actors, actions, and types of responsibility
- Mechanism: Breaking responsibility into three orthogonal dimensions allows exhaustive enumeration without overlap or ambiguity
- Core assumption: Each sense of responsibility has distinct conditions and actor eligibility requirements
- Evidence anchors: [abstract] mentions 85 responsibility strings from the three-part formulation; [section 3.1] discusses the four senses of responsibility
- Break condition: If actor eligibility requirements overlap significantly between senses

### Mechanism 2
- Claim: Distinguishing actor categories prevents category errors in responsibility attribution
- Mechanism: Explicitly excluding AI systems from moral and legal responsibility while allowing causal responsibility prevents treating systems as moral agents
- Core assumption: AI-enabled systems are neither moral agents nor legal persons
- Evidence anchors: [section 3.2] states AI systems are not moral agents; [section 4.4] restricts moral responsibility to individual human actors
- Break condition: If future AI systems develop moral agency capabilities

### Mechanism 3
- Claim: Decomposing role-responsibility into tasks, moral obligations, and legal duties enables conflict detection
- Mechanism: Distinguishing between different types of responsibilities allows identification of situations where they conflict
- Core assumption: Boundaries between tasks, moral obligations, and legal duties are sufficiently distinct
- Evidence anchors: [section 4.1] subdivides role-responsibility and acknowledges potential conflicts
- Break condition: If boundaries between subcategories become too porous

## Foundational Learning

- Concept: Causal vs. Moral Responsibility
  - Why needed here: Separates any actor's ability to cause from only voluntary agents' ability to be morally responsible
  - Quick check question: Can an AI-enabled system be morally responsible for an action? Why or why not?

- Concept: Legal Person vs. Moral Agent
  - Why needed here: Determines which actors can have legal duties versus moral obligations
  - Quick check question: Why can institutions have legal duties but not moral obligations according to this framework?

- Concept: Attribution vs. Accountability
  - Why needed here: Distinguishes between being morally responsible as voluntary causation versus liability to social sanction
  - Quick check question: What's the difference between being morally responsible and being held morally responsible?

## Architecture Onboarding

- Component map: Actor (AI-enabled system, individual human, institution) -> Occurrence (decision*, action*, omission*, decision, action, omission, consequence) -> Responsibility Sense (role, causal, legal, moral)
- Critical path: First identify actor type, then determine occurrence type, then select appropriate responsibility sense based on actor eligibility
- Design tradeoffs: Prioritizes exhaustiveness and clarity over simplicity; 85 strings provide comprehensive coverage but require cognitive overhead
- Failure signatures: Responsibility strings being misattributed due to actor category errors, sense confusion, or eligibility violations
- First 3 experiments:
  1. Systematically work through all 85 responsibility strings for a simple scenario to identify applicable and non-applicable strings
  2. Create a decision tree for selecting appropriate responsibility strings based on actor type and occurrence type
  3. Test conflict detection by identifying scenarios where different responsibility types might conflict for the same actor-occurrence pair

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the 85 responsibility strings connect to form a coherent "responsibility map" for tracing responsibility in complex AI incidents?
- Basis in paper: explicit
- Why unresolved: Authors identify individual strings but have not yet connected them into a systematic map or graph
- What evidence would resolve it: Visual representation showing how different responsibility strings interconnect with principles for tracing responsibility pathways

### Open Question 2
- Question: Can responsibility assurance cases be effectively adapted from safety assurance cases to provide confidence in responsibility attribution for AI systems?
- Basis in paper: explicit
- Why unresolved: Suggested as potential application but not yet implemented or tested
- What evidence would resolve it: Case studies or pilot implementations demonstrating how responsibility strings structure assurance arguments

### Open Question 3
- Question: How should conflicts between an actor's tasks, moral obligations, and legal duties be resolved when they lead to different responsibility attributions?
- Basis in paper: inferred
- Why unresolved: Paper identifies that conflicts can exist but provides no method for resolving them
- What evidence would resolve it: Framework or decision procedure for prioritizing or resolving conflicts between different types of role-responsibilities

## Limitations

- The categorical exclusion of AI systems from moral and legal responsibility relies on philosophical assumptions that may need revision as AI systems become more sophisticated
- The boundaries between role-responsibility subcategories (tasks, moral obligations, legal duties) may prove too porous in practice, limiting conflict detection capabilities
- The framework does not yet provide a systematic way to connect individual responsibility strings into a coherent responsibility map for tracing complex incidents

## Confidence

- High Confidence: The triadic formulation mechanism is well-established in philosophy and the 85 responsibility strings appear internally consistent
- Medium Confidence: The distinction between actor types is reasonable but relies on philosophical assumptions about moral agency that may evolve
- Medium Confidence: The separation of four responsibility senses is methodologically sound, though practical application may reveal ambiguities

## Next Checks

1. **Cross-disciplinary validation**: Test the framework with stakeholders from law, philosophy, and computer science to identify whether the 85 responsibility strings capture all relevant distinctions or miss important cases specific to different domains.

2. **Edge case analysis**: Apply the framework to increasingly complex scenarios (e.g., autonomous weapons, AI-driven medical triage) to identify where categorical boundaries break down or where new actor types may need incorporation.

3. **Conflict resolution testing**: Create scenarios where multiple responsibility strings apply simultaneously (e.g., legal duty conflicts with moral obligation) and evaluate whether the framework provides actionable guidance for resolving such conflicts or merely identifies their existence.