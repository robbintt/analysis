---
ver: rpa2
title: Multi-view Graph Convolution for Participant Recommendation
arxiv_id: '2311.12136'
source_url: https://arxiv.org/abs/2311.12136
tags:
- social
- group
- recommendation
- user
- participant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MVPRec, a multi-view graph convolution model
  for participant recommendation in group buying scenarios. The method differentiates
  user roles (initiator/participant) by reconstructing historical GB data into initiator-view
  and participant-view graphs, then fuses these with social graph representations
  using attention mechanisms.
---

# Multi-view Graph Convolution for Participant Recommendation

## Quick Facts
- arXiv ID: 2311.12136
- Source URL: https://arxiv.org/abs/2311.12136
- Reference count: 40
- This paper proposes MVPRec, a multi-view graph convolution model for participant recommendation in group buying scenarios

## Executive Summary
This paper addresses the participant recommendation problem in group buying E-commerce platforms by proposing MVPRec, a multi-view graph convolution model that differentiates user roles (initiator/participant) through explicit reconstruction of historical GB data. The method processes initiator-item and participant-item interactions separately using LightGCN encoders, then fuses these with social graph representations using attention mechanisms. MVPRec learns matching scores via multi-head attention to recommend top-k participants from an initiator's social friends. Experiments on three datasets (BeiBei, Ciao, Epinion) demonstrate MVPRec's effectiveness, outperforming baselines by 2.93-34.49% in NDCG@3 and 2.80-19.70% in Recall@3 metrics.

## Method Summary
MVPRec reconstructs historical group buying interactions into initiator-view and participant-view bipartite graphs, then encodes these along with social connections using LightGCN. The model fuses these representations through an attention mechanism and uses multi-head attention to compute matching scores between initiators/items and potential participants. A consistency loss ensures item representations are aligned across views. The model is trained with negative log-likelihood loss plus consistency regularization, and evaluated using NDCG@K and Recall@K metrics.

## Key Results
- MVPRec outperforms baselines by 2.93-34.49% in NDCG@3 and 2.80-19.70% in Recall@3 metrics
- The multi-view approach with role differentiation significantly improves participant recommendation accuracy
- Performance gains are consistent across three datasets (BeiBei, Ciao, Epinion)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role differentiation improves recommendation accuracy by modeling initiator and participant behaviors separately.
- Mechanism: MVPRec reconstructs historical group buying data into two bipartite graphs: Ginit for initiator-item interactions and Gpart for participant-item interactions. Each view is processed through LightGCN to capture distinct role-specific patterns, then fused with social information.
- Core assumption: Initiators and participants exhibit fundamentally different interaction patterns with items that should be modeled separately for accurate recommendations.
- Evidence anchors:
  - [abstract] "To differentiate the roles of users (Initiator/Participant) within the GB process, we explicitly reconstruct historical GB data into initiator-view and participant-view graphs."
  - [section] "To reflect the role difference, MVPRec reconstructs the group buying interactions P = {(u, i, Up)|u ∈ U , i ∈ I , Up ⊆ U } into two subgraphs Ginit = {(u, i)|u ∈ U , i ∈ I} and Gpart = {(up, i)|up ∈ U p, i ∈ I}."

### Mechanism 2
- Claim: Multi-view attention effectively combines heterogeneous information sources for better participant recommendation.
- Mechanism: MVPRec uses a fusion function with learnable parameters to combine initiator-view, participant-view, and social-view representations. A multi-head attention mechanism then computes matching scores between the query (derived from initiator and item) and potential participants.
- Core assumption: Social relationships and group buying behaviors provide complementary information that can be effectively combined through attention mechanisms.
- Evidence anchors:
  - [abstract] "Then MVPRec fuses the GB and social representation with an attention module to obtain the user representation and learns a matching score with the initiator's social friends via a multi-head attention mechanism."
  - [section] "Then we compute the likelihood of each participant on the query in a multi-head manner" and "MVPRec fuses the GB and social representation with an attention module."

### Mechanism 3
- Claim: Consistency loss ensures item representations are aligned across initiator and participant views.
- Mechanism: A consistency loss term Lconsistency = -einit · epart / (||einit|| · ||epart||) regularizes item embeddings from Ginit and Gpart to be similar, reflecting that both initiators and participants interact with the same items.
- Core assumption: Items should have consistent representations regardless of whether they're viewed from initiator or participant perspective in group buying interactions.
- Evidence anchors:
  - [abstract] "We assume item representation should reveal their common interest, and einit and epart should not be differentiated in a large margin."
  - [section] "We assume item representation should reveal their common interest, and einit and epart should not be differentiated in a large margin."

## Foundational Learning

- Concept: Graph Neural Networks (specifically LightGCN)
  - Why needed here: MVPRec uses LightGCN to encode both social networks and bipartite graphs (initiator-item and participant-item) into meaningful representations.
  - Quick check question: What distinguishes LightGCN from standard GCN, and why is it preferred for recommendation tasks?

- Concept: Multi-head attention mechanisms
  - Why needed here: MVPRec employs multi-head attention to compute matching scores between initiators/items and potential participants, allowing the model to capture different aspects of participant suitability.
  - Quick check question: How does multi-head attention differ from single-head attention, and what advantages does it provide in this recommendation context?

- Concept: Role differentiation in collaborative scenarios
  - Why needed here: Understanding that initiators and participants have different behaviors and objectives is fundamental to MVPRec's design, which separates these roles in the modeling process.
  - Quick check question: What behavioral differences might exist between initiators and participants in group buying, and why would modeling them separately improve recommendations?

## Architecture Onboarding

- Component map: Embedding Layer -> Multi-view Learning (Ginit, Gpart, Gsocial) -> Fusion Function -> Participant Prediction (Multi-head Attention) -> Consistency Loss
- Critical path: Embedding → Multi-view Encoding → Fusion → Attention-based Prediction → Consistency Loss
- Design tradeoffs:
  - Separate vs unified role modeling: MVPRec chose separation to capture role-specific patterns
  - Number of attention heads: More heads increase model capacity but also complexity
  - Consistency loss weight: Balancing between view-specific learning and cross-view alignment
- Failure signatures:
  - Poor performance despite correct implementation: Likely indicates role differentiation assumption is invalid for the dataset
  - High variance in training: Could indicate attention mechanism isn't stabilizing or consistency loss is too strong
  - Overfitting on social graph: May need more regularization or simpler social encoding
- First 3 experiments:
  1. Ablation study removing multi-view learning to verify role differentiation improves performance
  2. Hyperparameter sweep for consistency loss weight λ1 to find optimal balance
  3. Comparison of different attention mechanisms (single-head vs multi-head) to validate the multi-head approach

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations
- The role differentiation assumption may not hold across all E-commerce contexts where initiators and participants exhibit similar behaviors
- Model effectiveness depends on having sufficient historical GB data to construct meaningful initiator and participant views
- The attention-based fusion mechanism introduces computational overhead that scales with the number of attention heads and social connections

## Confidence
- High confidence: The core methodology (multi-view GCN with role differentiation) is technically sound and well-supported by the experimental results across three distinct datasets
- Medium confidence: The generalizability of findings to platforms with different group buying dynamics or less dense social networks
- Medium confidence: The optimal balance between view-specific learning and consistency regularization (λ1=0.6) may require tuning for different domains

## Next Checks
1. Cross-domain validation: Test MVPRec on a platform with different group buying patterns (e.g., travel bookings vs consumer goods) to assess generalizability of role differentiation
2. Ablation on consistency loss: Systematically vary λ1 from 0 to 1.0 to quantify the impact of item representation consistency on recommendation quality
3. Scalability analysis: Evaluate performance and runtime efficiency on larger social networks (10M+ users) to identify computational bottlenecks in the multi-head attention mechanism