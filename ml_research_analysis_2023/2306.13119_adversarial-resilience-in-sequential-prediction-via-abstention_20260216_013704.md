---
ver: rpa2
title: Adversarial Resilience in Sequential Prediction via Abstention
arxiv_id: '2306.13119'
source_url: https://arxiv.org/abs/2306.13119
tags:
- learning
- examples
- algorithm
- dimension
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of sequential prediction in the
  presence of adversarial (out-of-distribution) examples injected into a stream of
  i.i.d. data.
---

# Adversarial Resilience in Sequential Prediction via Abstention

## Quick Facts
- arXiv ID: 2306.13119
- Source URL: https://arxiv.org/abs/2306.13119
- Reference count: 19
- Key outcome: Achieves error rates scaling with VC dimension rather than Littlestone dimension in sequential prediction with abstention against adversarial injections

## Executive Summary
This paper addresses the challenge of sequential prediction when an adversary can inject out-of-distribution examples into a stream of i.i.d. data. The key insight is that allowing the learner to abstain from predictions on suspicious examples enables error rates that scale with the VC dimension rather than the Littlestone dimension, which characterizes fully adversarial settings. The authors propose algorithms that achieve O(d² log T) error for VC classes when the marginal distribution is known, and O(√T log T) for VC dimension 1 classes without distribution access. The framework introduces a novel uncertainty measure based on shattering probabilities that enables effective discrimination between i.i.d. and adversarial examples.

## Method Summary
The paper proposes two main algorithmic approaches for sequential prediction with abstention under adversarial attacks. For known distributions, a level-based algorithm uses shattering probabilities of k examples from the marginal distribution as an uncertainty measure - the learner abstains when both classes have high shattering probability for the current example. For unknown distributions, two structure-based algorithms are designed specifically for VC dimension 1 classes and axis-aligned rectangles, exploiting their geometric properties. These algorithms use robust disagreement region estimates that are resilient to adversarial injections. The key technical innovation is showing how VC dimension rather than Littlestone dimension can characterize achievable error rates when abstention is permitted.

## Key Results
- Achieves error O(d² log T) for general VC classes with known marginal distribution
- Achieves error O(√T log T) for VC dimension 1 classes without distribution access
- Demonstrates separation between stochastic sequential prediction and adversarial settings with abstention
- Introduces novel uncertainty measure based on shattering probabilities for VC classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The learner can achieve VC-dimension-scaled error instead of Littlestone-dimension error by abstaining on adversarial examples.
- Mechanism: The learner uses a measure of uncertainty based on the probability of shattering k examples from the marginal distribution. When both classes (predicting 0 or 1) have high shattering probability for the current example, the learner abstains. This allows the learner to avoid making mistakes on adversarial examples while maintaining low error on i.i.d. data.
- Core assumption: The learner has access to the marginal distribution over non-adversarial examples.
- Evidence anchors:
  - [abstract]: "we design a learner whose error scales with the VC dimension (mirroring the stochastic setting) of the hypothesis class, as opposed to the Littlestone dimension which characterizes the fully adversarial setting."
  - [section]: "The algorithm abstains if both these probabilities are large, else predicts according to whichever one is larger."
- Break condition: If the marginal distribution is not accessible or is corrupted, the shattering probability estimates become unreliable.

### Mechanism 2
- Claim: In the unknown distribution setting, the learner can still achieve VC-dimension-scaled error for VC dimension 1 classes using a structure-based approach.
- Mechanism: The algorithm uses a leave-one-out type estimate of the disagreement region (Γ) that is robust to adversarial injections. The learner predicts when the difference between Γ for the two classes is large, and abstains otherwise. This allows the learner to avoid making mistakes on adversarial examples while maintaining low error on i.i.d. data.
- Core assumption: The hypothesis class has VC dimension 1, allowing for a tree representation of the class.
- Evidence anchors:
  - [section]: "We will design algorithms for two commonly studied hypothesis classes: VC dimension 1 classes, and axis-aligned rectangles."
  - [section]: "The key lemma for our analysis is that the number of attackable examples is bounded."
- Break condition: If the hypothesis class has VC dimension greater than 1, the tree representation no longer holds and the algorithm may not work.

### Mechanism 3
- Claim: For axis-aligned rectangles, the learner can achieve VC-dimension-scaled error in the unknown distribution setting.
- Mechanism: The algorithm maintains the smallest rectangle enclosing the positive examples seen so far. It predicts 0 if there are enough examples that would expand this rectangle if labeled positive, and abstains otherwise. This allows the learner to avoid making mistakes on adversarial examples while maintaining low error on i.i.d. data.
- Core assumption: The hypothesis class is axis-aligned rectangles, which have a specific geometric structure.
- Evidence anchors:
  - [section]: "In this section, we will design algorithms for two commonly studied hypothesis classes: VC dimension 1 classes, and axis-aligned rectangles."
  - [section]: "Since the algorithm never predicts 1 unless the true label is 1, we never misclassify a negative example."
- Break condition: If the hypothesis class is not axis-aligned rectangles, the algorithm may not work.

## Foundational Learning

- Concept: VC dimension
  - Why needed here: The VC dimension characterizes the complexity of the hypothesis class and determines the achievable error rates in the stochastic setting.
  - Quick check question: What is the VC dimension of the class of axis-aligned rectangles in p dimensions?

- Concept: Littlestone dimension
  - Why needed here: The Littlestone dimension characterizes the complexity of the hypothesis class in the fully adversarial setting and provides a lower bound on the achievable error rates.
  - Quick check question: What is the Littlestone dimension of the class of thresholds on [0,1]?

- Concept: Shattering
  - Why needed here: The algorithm uses the probability of shattering k examples as a measure of uncertainty to decide when to abstain.
  - Quick check question: What is the probability of shattering k examples for a hypothesis class with VC dimension d?

## Architecture Onboarding

- Component map: Learner -> Uncertainty measure -> Distribution access -> Prediction/Abstention -> Update hypothesis
- Critical path:
  1. Receive example from adversary.
  2. Compute uncertainty measure using the current hypothesis class and example.
  3. If uncertainty is high, abstain; otherwise, predict according to the class with lower uncertainty.
  4. Update hypothesis class based on the true label.
- Design tradeoffs:
  - Known vs unknown distribution: Known distribution allows for more accurate uncertainty estimates but may not be realistic in practice.
  - Abstention rate vs misclassification rate: The learner can trade off between abstaining more often to reduce misclassification rate or predicting more often to reduce abstention rate.
- Failure signatures:
  - High abstention rate: The uncertainty measure may be too conservative, causing the learner to abstain too often.
  - High misclassification rate: The uncertainty measure may be too aggressive, causing the learner to make too many mistakes.
- First 3 experiments:
  1. Test the algorithm on a simple hypothesis class (e.g., thresholds) with a known distribution to verify that it achieves VC-dimension-scaled error.
  2. Test the algorithm on a more complex hypothesis class (e.g., axis-aligned rectangles) with an unknown distribution to verify that it still achieves VC-dimension-scaled error.
  3. Test the algorithm with different abstention thresholds to see how it affects the tradeoff between abstention rate and misclassification rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the error bounds for known distribution case be improved from O(d² log T) to O(d log T) for general VC classes?
- Basis in paper: [explicit] The paper states "it would be interesting to improve the upper bound or perhaps even more interesting to show a separation between stochastic sequential prediction and our model" and conjectures "the correct dependence on VC dimension d should be linear and not quadratic."
- Why unresolved: The current analysis uses a level-based approach that may have inherent quadratic dependence on VC dimension in its analysis.
- What evidence would resolve it: Either a new algorithm achieving O(d log T) error for known distribution case, or a lower bound showing Ω(d² log T) is necessary.

### Open Question 2
- Question: Can the abstention-based framework be extended to work for general VC classes in the unknown distribution setting?
- Basis in paper: [explicit] The paper states "Extending the structure-based algorithms for general VC classes is wide open" and notes the main challenge is that "it is not immediately obvious how to compute" the necessary quantities without knowing the distribution.
- Why unresolved: The current structure-based approach relies heavily on VC dimension 1 structure theorems, and extending this to higher dimensions requires new technical tools.
- What evidence would resolve it: Either a new algorithm achieving sublinear error for general VC classes without distribution knowledge, or a proof that this is impossible for some classes.

### Open Question 3
- Question: What is the computational complexity of learning with abstentions for halfspaces in high dimensions?
- Basis in paper: [explicit] The discussion section asks "for halfspaces in d dimensions, is there a polynomial time algorithm for learning with abstentions, even for well-behaved distributions such as Gaussians."
- Why unresolved: The current algorithms are statistical in nature but don't address computational efficiency, and halfspaces are known to be computationally challenging even in standard settings.
- What evidence would resolve it: Either a polynomial-time algorithm for learning halfspaces with abstentions, or a proof that this requires super-polynomial time under standard complexity assumptions.

## Limitations

- The algorithms require access to the marginal distribution or specific structural properties of the hypothesis class, which may not hold in practice
- The error bounds for general VC classes with known distribution have quadratic dependence on VC dimension, suggesting room for improvement
- Extension to general VC classes in the unknown distribution setting remains an open problem

## Confidence

- **High**: The theoretical framework and proofs are sound. The algorithms achieve the claimed error rates under the stated assumptions.
- **Medium**: The effectiveness of the uncertainty measures (shattering probability, disagreement region) in practice depends on accurate estimation, which may be challenging with limited data.
- **Low**: The applicability of the algorithms to more complex hypothesis classes or in the absence of structural assumptions is unclear.

## Next Checks

1. **Empirical validation on synthetic data**: Implement the algorithms and test them on synthetic binary classification tasks where the marginal distribution D is known. Vary the number of adversarial injections and measure the error and abstention rates. Compare the performance to the theoretical bounds.

2. **Analysis of the uncertainty measures**: For the known distribution case, analyze the empirical shattering probabilities computed by the algorithm on different hypothesis classes and datasets. Check if they correlate well with the actual risk of making a mistake on adversarial examples.

3. **Extension to multiclass**: Investigate how the algorithms and analysis extend to the multiclass setting. Identify which components of the framework generalize and which require significant modification. Propose potential extensions or limitations for multiclass classification.