---
ver: rpa2
title: 'MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning'
arxiv_id: '2304.08981'
source_url: https://arxiv.org/abs/2304.08981
tags:
- emotion
- recognition
- pages
- learning
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces the MER 2023 challenge, focusing on system
  robustness in multimodal emotion recognition. It presents three sub-challenges:
  MER-MULTI (recognizing discrete and dimensional emotions), MER-NOISE (evaluating
  modality robustness with added noise), and MER-SEMI (leveraging unlabeled data for
  semi-supervised learning).'
---

# MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning

## Quick Facts
- arXiv ID: 2304.08981
- Source URL: https://arxiv.org/abs/2304.08981
- Reference count: 40
- Primary result: Introduces MER 2023 challenge with baseline achieving 77.57% F1 and 0.82 MSE for MER-MULTI

## Executive Summary
The MER 2023 challenge introduces a new benchmark for multimodal emotion recognition with three sub-challenges: MER-MULTI (discrete and dimensional emotions), MER-NOISE (modality robustness with added noise), and MER-SEMI (semi-supervised learning with unlabeled data). The challenge uses an extended version of the CHEAVD dataset with high-quality annotations and focuses on system robustness rather than just performance. The baseline system employs pre-trained models for feature extraction from audio, text, and visual modalities, followed by attention-based fusion and multi-task learning for emotion prediction.

## Method Summary
The baseline system uses pre-trained models (wav2vec 2.0, BERT variants, ResNet/SENet) to extract features from audio, text, and visual modalities, respectively. These features are combined using an attention mechanism that computes weighted fusion scores. The fused representation is then passed through a multi-task learning framework with separate branches for discrete emotion classification and dimensional emotion regression. Training employs Adam optimizer with dropout, and five-fold cross-validation is used for hyperparameter tuning.

## Key Results
- MER-MULTI baseline achieves 77.57% F1 score and 0.82 MSE on the test set
- MER-NOISE baseline achieves 69.82% F1 score and 1.12 MSE under noisy conditions
- MER-SEMI baseline achieves 86.75% F1 score using confidence-based semi-supervised learning
- Visual modality generally performs better than audio and text, highlighting the importance of modality selection
- Task similarity in transfer learning significantly impacts performance, with emotion-focused pre-training yielding better results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fusion improves emotion recognition by combining complementary information from audio, text, and visual modalities
- Mechanism: The baseline system uses weighted fusion where attention scores are computed for each modality's hidden representation, then combined to form multimodal features that feed into a multi-task framework for predicting both discrete and dimensional emotions
- Core assumption: Different modalities contribute differently to emotion recognition, and their combination captures more comprehensive emotional information than any single modality
- Evidence anchors: The paper shows that integrating multimodal information helps the model better understand video content and achieve better performance compared to existing challenges

### Mechanism 2
- Claim: Pre-trained models provide robust feature representations that transfer well to emotion recognition tasks
- Mechanism: The baseline extracts features using pre-trained models like wav2vec 2.0 for audio, BERT variants for text, and ResNet/SENet for visual data, leveraging their learned representations from large-scale training on related tasks
- Core assumption: Feature representations learned from large datasets capture general patterns that are useful for emotion recognition, even across different domains
- Evidence anchors: Recent works have proved that pre-training language models on large corpora can learn universal lexical representations beneficial for tasks with limited samples

### Mechanism 3
- Claim: Task similarity in transfer learning significantly impacts performance, with emotion-focused pre-training yielding better results
- Mechanism: The baseline demonstrates that models pre-trained on facial emotion datasets (FER-2013, RAF-DB) outperform those trained on general image classification (ImageNet) or face recognition (MS-Celeb-1M) datasets
- Core assumption: Pre-training on tasks closely related to the target task provides better initialization and feature representations
- Evidence anchors: The paper observes that models pre-trained on facial emotion datasets generally achieve better performance than others because facial emotion recognition is more relevant to video emotion recognition than image classification and face recognition

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: The challenge requires combining audio, text, and visual modalities to recognize emotions, which necessitates understanding how to extract and fuse features from different data types
  - Quick check question: How would you modify the attention mechanism if one modality was consistently underperforming across all samples?

- Concept: Transfer learning and pre-trained models
  - Why needed here: The baseline relies heavily on pre-trained models for feature extraction, making it essential to understand how to select, fine-tune, and evaluate pre-trained models for new tasks
  - Quick check question: What criteria would you use to decide whether to fine-tune a pre-trained model or use it as a fixed feature extractor?

- Concept: Semi-supervised learning and confidence-based sample selection
  - Why needed here: MER-SEMI requires leveraging unlabeled data, using a confidence-based selection strategy to identify high-quality samples for annotation
  - Quick check question: How would you adjust the quantile threshold if the class distribution in your unlabeled data differed significantly from the labeled data?

## Architecture Onboarding

- Component map: Feature extraction (audio, text, visual) -> Attention-based fusion -> Multi-task prediction (classification + regression) -> Loss computation -> Backpropagation
- Critical path: Feature extraction → Fusion → Prediction → Loss computation → Backpropagation
- Design tradeoffs:
  - Pre-trained vs. trained-from-scratch: Pre-trained models offer better performance but less flexibility
  - Single vs. multi-task learning: Multi-task learning leverages correlations between discrete and dimensional emotions but increases complexity
  - Attention-based vs. simple concatenation: Attention allows dynamic weighting but adds parameters and complexity
- Failure signatures:
  - Poor performance on MER-NOISE: Indicates modality robustness issues, particularly with audio corruption
  - Large performance gap between Train&Val and test sets: Suggests overfitting or distribution shift
  - One modality consistently underperforming: May indicate feature extraction issues or modality irrelevance
- First 3 experiments:
  1. Ablation study: Test unimodal vs. multimodal performance to verify fusion benefits
  2. Noise robustness test: Evaluate baseline performance on corrupted audio to identify weak points
  3. Pre-training comparison: Compare features from emotion-focused vs. general-purpose pre-trained models to validate task similarity hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different levels of audio noise (SNR values) affect emotion recognition performance across modalities?
- Basis in paper: The paper describes adding noise at 5-10 SNR to audio, but does not systematically evaluate performance across different SNR levels
- Why unresolved: The paper only mentions using random SNR between 5-10 but doesn't analyze how performance varies across different noise intensities
- What evidence would resolve it: Empirical results showing emotion recognition performance across a range of SNR values (e.g., 0-20) for both audio and visual modalities

### Open Question 2
- Question: What is the optimal confidence threshold for selecting reliable samples in the semi-supervised learning setup?
- Basis in paper: The paper uses a class-specific threshold based on the η-quantile of confidence scores but doesn't explore optimal threshold values
- Why unresolved: The paper describes the confidence-based selection strategy but doesn't evaluate how different threshold values affect downstream performance
- What evidence would resolve it: Systematic experiments comparing performance across different confidence thresholds and their impact on final emotion recognition accuracy

### Open Question 3
- Question: How does the correlation between discrete and dimensional emotions vary across different emotion categories?
- Basis in paper: The paper mentions observing correlations between discrete and dimensional emotions but doesn't provide detailed analysis of category-specific relationships
- Why unresolved: While the paper shows general correlation patterns, it doesn't break down how these relationships differ across specific emotion categories like anger vs happiness
- What evidence would resolve it: Detailed statistical analysis showing correlation patterns for each discrete emotion category and their corresponding valence distributions

## Limitations

- The reliance on pre-trained models introduces domain shift risks when emotion recognition differs significantly from pre-training tasks
- The attention-based fusion mechanism assumes modality complementarity, which may not hold when modalities provide contradictory information
- The semi-supervised approach using confidence-based sample selection could introduce bias if unlabeled data distribution differs substantially from labeled set

## Confidence

- **High**: The core methodology of multimodal fusion and pre-trained feature extraction is well-established in the literature
- **Medium**: The specific performance metrics and their interpretation are sound, though MER-SEMI results may be influenced by the particular confidence threshold chosen
- **Low**: The generalizability of the baseline system to other datasets or domains is uncertain given the specific characteristics of the CHEAVD extension dataset

## Next Checks

1. Conduct an ablation study comparing unimodal vs. multimodal performance across different noise levels to quantify the true benefit of fusion under corrupted conditions
2. Test the baseline system on an external emotion recognition dataset to evaluate domain generalization and identify potential overfitting issues
3. Perform sensitivity analysis on the confidence threshold parameter in the semi-supervised learning setup to determine its impact on final performance and identify optimal settings for different data distributions