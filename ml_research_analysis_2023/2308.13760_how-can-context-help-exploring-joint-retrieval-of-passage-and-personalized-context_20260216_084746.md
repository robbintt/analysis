---
ver: rpa2
title: How Can Context Help? Exploring Joint Retrieval of Passage and Personalized
  Context
arxiv_id: '2308.13760'
source_url: https://arxiv.org/abs/2308.13760
tags:
- context
- retrieval
- document
- pcas
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of context-aware passage retrieval
  for document-grounded dialogue systems. It proposes a novel approach, PCAS, that
  jointly predicts the most relevant document and context pair by combining the document-query
  relevance with the document-context relevance.
---

# How Can Context Help? Exploring Joint Retrieval of Passage and Personalized Context

## Quick Facts
- arXiv ID: 2308.13760
- Source URL: https://arxiv.org/abs/2308.13760
- Reference count: 18
- Key outcome: PCAS outperforms baselines in retrieving relevant passages and identifying pertinent context in document-grounded dialogue systems

## Executive Summary
This paper introduces the task of context-aware passage retrieval for document-grounded dialogue systems and proposes PCAS (Passage and Context Aware Selection), a novel approach that jointly predicts the most relevant document and context pair. PCAS combines document-query relevance with document-context relevance using a weighted sum to disambiguate queries and improve document selection. Experimental results on the ORCA-ShARC dataset using multiple dense retrieval systems demonstrate that PCAS outperforms baseline approaches in both retrieving relevant passages and identifying pertinent context among available contexts.

## Method Summary
The paper proposes PCAS, which uses a convex combination score λ * scoredq(d,q) + (1-λ) * scorecd(c,d) to select the most relevant document-context pair from top-K documents retrieved by a query. The method is evaluated on ORCA-ShARC, a dataset constructed from OR-ShARC by sampling 10 context pieces per example. Four pre-trained dense retrieval systems (ColBERT, DPR, ANCE, S-BERT) are used as backbone models. Baselines include concatenating all contexts (B1), sequential retrieval (B2), and context-first approach (B3). The evaluation uses MAP@5 and Recall@K metrics for both document and context retrieval.

## Key Results
- PCAS achieves higher Recall@1 for document retrieval than all baselines across all retrievers (DPR: 67.7%, ANCE: 70.1%, S-BERT: 61.7%, ColBERT: 57.6%)
- PCAS significantly outperforms baselines in context retrieval (DPR: 49.4%, ANCE: 51.1%, S-BERT: 49.8%, ColBERT: 50.7%)
- Sequential retrieval (B2) outperforms context-first approach (B3), supporting the motivation for PCAS design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint optimization of document and context retrieval improves overall retrieval accuracy
- Mechanism: PCAS combines document-query relevance with document-context relevance using a weighted sum (λ * score(d,q) + (1-λ) * score(d,c)), allowing context to disambiguate ambiguous queries and improve document selection
- Core assumption: The most relevant context for a document is the one that, when combined with the document, best matches the user's intent
- Evidence anchors:
  - [abstract] "Experimental evaluations conducted on multiple popular dense retrieval systems demonstrate that our proposed approach not only outperforms the baselines in retrieving the most relevant passage but also excels at identifying the pertinent context among all the available contexts."
  - [section 4.2] "A convex combination score λ * scoredq(d, q) + (1 − λ) * scoredc(d, c) is used to select the most relevant pair (d, c) where 0 < λ < 1."
  - [corpus] Weak evidence - no corpus papers directly support this specific mechanism

### Mechanism 2
- Claim: Sequential retrieval (query → document → context) is more effective than parallel or reversed sequential approaches
- Mechanism: B2's approach of first retrieving documents from the query, then selecting the most relevant context for each document, outperforms B3 which first selects context then retrieves documents based on question+context
- Core assumption: Initial document retrieval provides better signal for context selection than initial context selection provides for document retrieval
- Evidence anchors:
  - [section 5] "Furthermore, the comparison between B2 and B3 illustrates that the retrieval process of q → d → c is better than q → c → d, which supports the motivation of our PCAS design."
  - [section 4.1] "B2 question − →document; document − →context: A baseline that uses the user question to retrieve documents based on scoredq(d, q), then uses the top predicted documents to select contexts based on scorecd(c, d)."
  - [corpus] Weak evidence - no corpus papers directly support this specific sequential advantage

### Mechanism 3
- Claim: Dense retrieval models with different architectures have varying effectiveness for this task
- Mechanism: ColBERT, DPR, ANCE, and S-BERT show different performance patterns, with ColBERT generally performing best on document retrieval but B2 outperforming OR and PCAS with ColBERT
- Core assumption: The underlying model architecture affects how well joint context-document retrieval works
- Evidence anchors:
  - [section 5] "For the same approach, the results largely varies across the retrievers, due to the distinct models and different pre-training data and processes."
  - [section 5] "Importantly, we observe that when the original relevant context is unknown, our proposed PCAS approach achieves better retrieval results than all baselines, which indicates that jointly considering the documents and contexts can improve the performances of both document and context retrieval."
  - [corpus] Weak evidence - no corpus papers directly support these specific performance differences

## Foundational Learning

- Concept: Dense passage retrieval and its comparison to sparse retrieval
  - Why needed here: The paper uses multiple dense retrieval systems (DPR, ANCE, S-BERT, ColBERT) and understanding their differences is crucial for interpreting results
  - Quick check question: What is the key difference between dense and sparse retrieval methods in terms of how they represent queries and documents?

- Concept: Multi-stage retrieval pipelines and their design choices
  - Why needed here: The paper compares different retrieval strategies (B1, B2, B3, PCAS) that use different combinations of query→document, document→context, and joint optimization
  - Quick check question: Why might a two-stage retrieval approach (query→document then document→context) be more effective than a single-stage approach?

- Concept: Context-aware retrieval and its applications
  - Why needed here: The core contribution is a method for incorporating external personalized context into document retrieval for dialogue systems
  - Quick check question: What are the potential benefits and risks of incorporating user-specific context information into information retrieval systems?

## Architecture Onboarding

- Component map: Query (q) → Retrieval model → Top-K documents → Context selection → Joint scoring (λ * scoredq(d,q) + (1-λ) * scorecd(c,d)) → Final document-context pair

- Critical path: Query → Retrieval model → Top-K documents → Context selection → Joint scoring → Final document-context pair

- Design tradeoffs:
  - Context inclusion: Including too much context adds noise; including too little misses disambiguation opportunities
  - Retrieval order: q→d→c vs q→c→d vs joint optimization
  - Model selection: Different dense retrievers have different strengths and weaknesses for this task
  - Hyperparameter λ: Balancing document-query relevance vs document-context relevance

- Failure signatures:
  - Low recall@1: Initial document retrieval is poor, leading to cascade failures
  - High MAP but low recall: System is finding somewhat relevant documents but missing the most relevant ones
  - Inconsistent results across retrievers: The approach may be sensitive to the underlying retrieval model's characteristics
  - Poor context selection: The document→context scoring function may not be capturing relevance well

- First 3 experiments:
  1. Replicate the validation results using the same hyper-parameters (λ and beam values) for each retriever to establish baseline performance
  2. Perform ablation studies by varying λ (e.g., 0.3, 0.5, 0.7) to understand the sensitivity to the weighting parameter
  3. Test the robustness of each approach by adding controlled noise to the context set and measuring performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed PCAS approach scale to larger and more heterogeneous context sets in realistic settings?
- Basis in paper: Inferred from limitations section stating "The size of the context set that we construct is limited" and "More work is needed to discover how our approach scales to these settings."
- Why unresolved: The paper only evaluates on a dataset with a limited context set size (up to 10 contexts per example) and does not address the scalability of PCAS to larger, more diverse context sets.
- What evidence would resolve it: Experiments evaluating PCAS on datasets with larger and more heterogeneous context sets, comparing its performance to baseline approaches and analyzing the impact of context set size on retrieval accuracy.

### Open Question 2
- Question: How can the PCAS method be extended to the training process to further improve retrieval performance?
- Basis in paper: Explicitly mentioned in the conclusion as a potential direction for future research.
- Why unresolved: The paper only presents PCAS as a retrieval method and does not explore its potential integration into the training process of retrieval models.
- What evidence would resolve it: Experiments comparing the performance of retrieval models trained with and without PCAS integration, analyzing the impact on retrieval accuracy and efficiency.

### Open Question 3
- Question: How does the quality of the generated responses using PCAS-retrieved documents and contexts compare to responses generated using baseline approaches?
- Basis in paper: Explicitly mentioned in the limitations section as an area for future work.
- Why unresolved: The paper focuses solely on the retrieval aspect and does not evaluate the quality of the generated responses using the retrieved information.
- What evidence would resolve it: Human evaluations or automated metrics comparing the quality of responses generated using PCAS-retrieved documents and contexts to responses generated using baseline approaches, analyzing factors such as relevance, coherence, and informativeness.

## Limitations
- The paper evaluates on a dataset with a limited context set size (up to 10 contexts per example) and doesn't address scalability to larger, more diverse context sets
- The fixed λ values across different retrievers (0.6 for DPR/ANCE/S-BERT, 0.55 for ColBERT) lack theoretical justification for why these specific values were chosen
- The paper focuses solely on the retrieval aspect and doesn't evaluate the quality of the generated responses using the retrieved information

## Confidence
- **High confidence**: The experimental results showing PCAS outperforming baselines on the ORCA-ShARC dataset, as the methodology and metrics are clearly defined and reproducible
- **Medium confidence**: The claim that joint optimization improves retrieval accuracy, as this depends on the quality of the underlying dense retrieval models and the assumption that context is relevant
- **Low confidence**: The sequential advantage claim (q→d→c > q→c→d) as the evidence is limited to comparison with a single baseline and doesn't explore edge cases

## Next Checks
1. Perform ablation studies by varying λ systematically (0.3, 0.5, 0.7) to determine if the fixed values are optimal or if the approach is sensitive to this hyperparameter
2. Test the robustness of each approach by introducing controlled noise into the context set (e.g., irrelevant contexts, contradictory contexts) to measure performance degradation
3. Evaluate on datasets with varying context quality to determine if the joint optimization approach is brittle when contexts are not consistently relevant or when the most relevant context is not available