---
ver: rpa2
title: Visual AI and Linguistic Intelligence Through Steerability and Composability
arxiv_id: '2312.12383'
source_url: https://arxiv.org/abs/2312.12383
tags:
- image
- task
- lego
- design
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated GPT-4 Vision's performance on 14 creative
  and complex tasks designed to test multimodal AI capabilities in maker spaces. Tasks
  ranged from image-to-instruction challenges (e.g., AI Lego Designer, AI Bartender)
  to stepwise text-based tasks (e.g., AI Negotiator, AI Genetic Programmer).
---

# Visual AI and Linguistic Intelligence Through Steerability and Composability

## Quick Facts
- arXiv ID: 2312.12383
- Source URL: https://arxiv.org/abs/2312.12383
- Reference count: 40
- GPT-4 Vision showed varying proficiency across 14 multimodal tasks, with simpler tasks showing low difficulty and complex tasks showing high difficulty

## Executive Summary
This study evaluates GPT-4 Vision's performance on 14 creative and complex tasks designed to test multimodal AI capabilities in maker spaces. The research examines how well the model can follow instructions (steerability) and combine disparate elements (composability) across tasks ranging from image-to-instruction challenges to stepwise text-based reasoning. Through 800 human-machine dialogs, the study reveals significant variation in task completion difficulty and highlights the need for improved long-term memory and contextual awareness in multimodal LLMs to handle creative problem-solving scenarios effectively.

## Method Summary
The study employed 800 guided human-machine dialogs across 14 multimodal tasks including AI Lego Designer, AI Bartender, AI Genetic Programmer, and others. Human evaluators engaged with GPT-4 Vision following specific prompt structures and scored task completion based on two key metrics: steerability (model's ability to follow instructions) and composability (model's ability to rearrange elements according to instruction). Tasks were categorized as low, mid, or high difficulty based on completion rates and qualitative assessment of the model's performance in processing visual data, maintaining context across multiple steps, and generating coherent instructions.

## Key Results
- Simpler tasks like Image-to-Ingredient Bartender showed low difficulty levels
- Complex tasks like AI Game Self-Player exhibited high difficulty
- GPT-4 Vision demonstrated varying proficiency in processing visual data and maintaining context across multiple steps
- The model struggled with precise spatial reasoning tasks and handling non-Latin alphabets in images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 Vision's steerability allows it to follow complex multi-step instructions across multimodal tasks.
- Mechanism: The model processes visual input and translates it into actionable text instructions, maintaining context through iterative dialog.
- Core assumption: GPT-4 Vision can interpret images and retain contextual information across multiple turns of conversation.
- Evidence anchors: [abstract] "evaluating 800 guided dialogs" and "notable disparities in task completion difficulty" suggest the model maintains context but with varying success.

### Mechanism 2
- Claim: GPT-4 Vision's composability enables it to assemble or disassemble visual and textual elements to create novel outcomes.
- Mechanism: The model combines disparate data elements (text, images) to generate new, coherent outcomes, akin to cognitive LEGO building.
- Core assumption: The model can creatively recombine elements to solve problems in maker spaces.
- Evidence anchors: [abstract] Tasks like "AI Origami" and "AI Kintsugi" require the model to piece together information from images to create instructions.

### Mechanism 3
- Claim: GPT-4 Vision's long-term memory and context understanding are crucial for handling complex, multistep tasks.
- Mechanism: The model leverages memory to maintain context over extended interactions, enabling it to handle tasks that require sequential logic.
- Core assumption: The model has mechanisms for long-term memory that allow it to retain and use information from earlier in the interaction.
- Evidence anchors: [abstract] "emphasize the need for improved long-term memory and contextual awareness" and tasks like "AI Game Self-Player" showing high difficulty.

## Foundational Learning

- Concept: Multimodal learning integration
  - Why needed here: GPT-4 Vision must process and integrate visual and textual data to perform tasks effectively.
  - Quick check question: Can the model accurately interpret an image and generate relevant text instructions based on that interpretation?

- Concept: Iterative design and feedback loops
  - Why needed here: Many tasks require the model to refine its outputs based on user feedback, necessitating iterative design processes.
  - Quick check question: Does the model adjust its responses appropriately when given corrective feedback during a task?

- Concept: Contextual awareness in complex problem-solving
  - Why needed here: Tasks often require the model to understand and maintain context over multiple steps to achieve the desired outcome.
  - Quick check question: Can the model maintain the context of a task across several interactions without losing track of the objective?

## Architecture Onboarding

- Component map: Image input → Vision encoder → Multimodal integration → Context maintenance → Text generation → User feedback → Refinement loop
- Critical path: Visual data flows through the vision encoder, integrates with textual context via multimodal transformer, generates outputs through language decoder, with feedback mechanisms for iterative refinement
- Design tradeoffs: Balancing multimodal integration complexity with real-time responsiveness; managing memory usage for long-term context retention versus computational efficiency
- Failure signatures: Inability to maintain context across interactions; failure to accurately interpret visual data; inability to generate coherent text instructions from visual input
- First 3 experiments:
  1. Test basic image-to-text translation with simple, unambiguous images
  2. Evaluate context retention by asking follow-up questions based on previous interactions
  3. Assess iterative refinement by providing corrective feedback and measuring response adjustments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do current multimodal LLMs perform on tasks requiring precise spatial reasoning compared to human performance?
- Basis in paper: [explicit] The paper states "The model struggles with tasks requiring precise spatial reasoning, such as identifying chess positions."
- Why unresolved: The paper mentions this limitation but does not provide quantitative comparisons to human performance or explore potential reasons for this deficiency.

### Open Question 2
- Question: What architectural modifications would enable LLMs to better maintain context over multiple steps in complex tasks?
- Basis in paper: [explicit] The paper notes that "Tasks such as 'AI Genetic Programmer' and 'AI Negotiator' showed high completion difficulty, emphasizing challenges in maintaining context over multiple steps."
- Why unresolved: While the paper identifies the problem, it does not propose specific architectural solutions or test their effectiveness.

### Open Question 3
- Question: How can multimodal LLMs be trained to better handle non-Latin alphabets in image analysis tasks?
- Basis in paper: [explicit] The paper states that "The model may not perform optimally when handling images with text of non-Latin alphabets, such as Japanese or Korean."
- Why unresolved: The paper identifies this limitation but does not explore training methodologies or data augmentation techniques to address it.

## Limitations
- The evaluation relies on human judgment for task difficulty scoring, introducing potential subjectivity
- The 800 dialogs may not capture the full range of task complexities or edge cases
- The study lacks specific prompt formulations and scoring rubrics, limiting reproducibility

## Confidence
- High Confidence: GPT-4 Vision shows varying proficiency across different task types, with clear patterns of higher difficulty in complex, multistep tasks versus simpler image-to-text translations
- Medium Confidence: The claim that steerability and composability are the primary mechanisms for task completion is supported by observed patterns but could benefit from more direct evidence
- Low Confidence: The assertion that long-term memory and context understanding are the main bottlenecks for complex tasks is inferred from difficulty patterns but lacks direct measurement

## Next Checks
1. Systematically vary prompt complexity and length across tasks to isolate the impact of prompt design on task completion rates
2. Implement automated context tracking to measure the model's ability to reference earlier information without explicit reminders
3. Test the same task suite with other multimodal models to determine if observed difficulty patterns are specific to GPT-4 Vision