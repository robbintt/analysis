---
ver: rpa2
title: 'Characterizing Learning Curves During Language Model Pre-Training: Learning,
  Forgetting, and Stability'
arxiv_id: '2308.15419'
source_url: https://arxiv.org/abs/2308.15419
tags:
- language
- learning
- pre-training
- token
- surprisal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes learning dynamics during pre-training of autoregressive
  language models by extracting learning curves for 1M tokens across five training
  runs. The authors observe that models first generate short repetitive phrases before
  producing longer coherent text, and that performance stabilizes early for frequent
  tokens.
---

# Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability

## Quick Facts
- arXiv ID: 2308.15419
- Source URL: https://arxiv.org/abs/2308.15419
- Authors: Kaushik Lakshminarayanan, Ameya Godbole, Nivedita Vallurupalli, Ankur Taly, Surya Ganguli
- Reference count: 22
- Primary result: More frequent tokens are learned earlier, with less variability, and are less likely to be forgotten during pre-training

## Executive Summary
This paper analyzes learning dynamics during pre-training of autoregressive language models by extracting learning curves for 1M tokens across five training runs. The authors observe that models first generate short repetitive phrases before producing longer coherent text, and that performance stabilizes early for frequent tokens. They introduce metrics including final surprisal, variability, age of acquisition, forgettability, and cross-run variability, finding that more frequent tokens are learned earlier, with less variability, and are less likely to be forgotten. N-gram probabilities and context likelihoods further influence learning speed and stability. The study highlights the importance of token and n-gram frequencies, with some POS tags (e.g., nouns) learned more slowly and less stably. These findings inform robust model deployment by identifying which examples are prone to instability during pre-training.

## Method Summary
The authors train five GPT-2 style Transformer language models with 124M parameters for 1M steps on 5.1B tokens from the first 128M lines of deduplicated OSCAR English corpus. They use a checkpointing strategy where steps between checkpoints increase linearly as a function of current step, computing surprisal for 10 tokens per sequence at each checkpoint. The study extracts learning curves for 1M tokens in context, calculating metrics including final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability. They predict these learning curve metrics from six features: target token log-frequency, target 5-gram log-probability, context log-length, context log-probability, target contextual diversity, and target part-of-speech.

## Key Results
- More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be "forgotten" during pre-training
- Higher n-gram probabilities further accentuate these effects, predicting lower learning variability and forgettability
- More frequent contexts are predictive of lower variance within and across pre-training runs, earlier acquisition, and lower forgettability
- Some POS tags (e.g., nouns) are learned more slowly and less stably than others
- The six predictors account for less than 30% of variance in some learning curve metrics, indicating important features remain unidentified

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frequent tokens are learned earlier and more stably during pre-training
- Mechanism: Token frequency creates a positive feedback loop where repeated exposure leads to lower surprisal, which in turn reduces variability and prevents forgetting
- Core assumption: Language models prioritize learning frequent patterns due to their prevalence in training data
- Evidence anchors:
  - [abstract] "More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be 'forgotten' during pre-training."
  - [section] "We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts."
  - [corpus] Weak evidence - related papers discuss forgetting but don't directly address frequency-based learning stability
- Break condition: If token frequency becomes too dominant, it could lead to overgeneralization and poor handling of rare but important tokens

### Mechanism 2
- Claim: N-gram probabilities influence both learning speed and stability
- Mechanism: Higher conditional probabilities provide clearer learning signals, reducing uncertainty and variability in the learning curve
- Core assumption: Language models can effectively capture and utilize local context information during pre-training
- Evidence anchors:
  - [abstract] "Higher n-gram probabilities further accentuate these effects" (referring to effects of token frequency)
  - [section] "More notably, higher 5-gram log-probabilities are predictive of lower learning variability both within and across pre-training runs, along with lower forgettability"
  - [corpus] Weak evidence - corpus neighbors discuss pre-training dynamics but not specifically n-gram probability effects
- Break condition: If n-gram probabilities are artificially inflated or manipulated, they may not reflect true learning stability

### Mechanism 3
- Claim: Context length and frequency affect learning stability
- Mechanism: Shorter and more frequent contexts provide clearer learning signals with less ambiguity, leading to more stable predictions
- Core assumption: Language models benefit from consistent contextual patterns during pre-training
- Evidence anchors:
  - [abstract] "Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions"
  - [section] "More frequent contexts are predictive of lower variance within and across pre-training runs, earlier acquisition, and lower forgettability"
  - [corpus] Weak evidence - corpus neighbors don't directly address context length effects
- Break condition: If contexts become too short, they may lose important semantic information needed for accurate predictions

## Foundational Learning

- Concept: Surprisal as a measure of prediction quality
  - Why needed here: Surprisal quantifies how well the model predicts tokens, serving as the primary metric for tracking learning progress
  - Quick check question: If a model achieves low surprisal for a token, what does this indicate about its prediction quality?

- Concept: Learning curves and their metrics
  - Why needed here: Learning curves track how model performance evolves during pre-training, with metrics like final surprisal, variability, and forgettability providing insights into learning dynamics
  - Quick check question: What does high forgettability indicate about a token's learning curve?

- Concept: GAM (Generalized Additive Model) curve fitting
  - Why needed here: GAM curves smooth raw surprisal data to identify trends and inflection points in learning curves
  - Quick check question: Why might raw surprisal curves be less stable than fitted GAM curves?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training with checkpoints -> Evaluation on unseen tokens -> Learning curve extraction -> Metric computation
- Critical path: Data preprocessing → Model training with checkpoints → Evaluation on unseen tokens → Learning curve extraction → Metric computation
- Design tradeoffs: Smaller model size (124M parameters) allows faster experimentation but may not capture full learning dynamics of larger models
- Failure signatures: High variability in learning curves, unexpected forgetting patterns, or inconsistent cross-run correlations
- First 3 experiments:
  1. Verify checkpoint strategy by comparing surprisal curves at different intervals
  2. Test frequency effects by comparing learning curves for high vs. low frequency tokens
  3. Validate context effects by analyzing learning curves for tokens in different context lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do learning dynamics change across different model scales (e.g., 124M vs. 1B+ parameters)?
- Basis in paper: [explicit] The authors note that previous work has shown learning curves differ across model sizes, with larger models able to "learn" examples that smaller models fail to converge on, and exhibit less forgetting. However, they only analyze a single model size (124M parameters).
- Why unresolved: The paper explicitly acknowledges this limitation but does not investigate how scaling affects the specific learning curve metrics they introduce (surprisal, variability, age of acquisition, forgettability, cross-run variability).
- What evidence would resolve it: Comparative analysis of the same learning curve metrics across multiple model sizes trained on the same data would show whether and how these metrics scale.

### Open Question 2
- Question: Do the same features (token frequency, n-gram probability, context length/likelihood, POS) predict learning curve metrics for training examples versus unseen evaluation examples?
- Basis in paper: [inferred] The authors analyze only unseen evaluation examples and note that "future work might identify features of the training examples that generalize to unseen examples."
- Why unresolved: The paper focuses exclusively on how features predict learning for unseen tokens in context, leaving open whether these predictors generalize to the examples the model actually trains on.
- What evidence would resolve it: Parallel analysis of learning curves for training examples using the same predictors and metrics would reveal whether feature importance differs between training and evaluation data.

### Open Question 3
- Question: What other features beyond the six predictors analyzed (token frequency, n-gram probability, context length/likelihood, contextual diversity, POS) influence learning speed and stability?
- Basis in paper: [explicit] The authors state that their six predictors account for less than 30% of variance in some learning curve metrics, and suggest "future work might investigate other features."
- Why unresolved: Despite finding statistically significant effects for their predictors, the authors acknowledge substantial unexplained variance, indicating important features remain unidentified.
- What evidence would resolve it: Systematic exploration of additional predictors (e.g., morphological features, semantic properties, syntactic complexity, or positional information) and their contribution to explained variance would identify missing predictive factors.

## Limitations
- Findings based on 124M parameter models trained on a single corpus (OSCAR English), limiting generalizability to larger models and different domains
- Analysis of 1M tokens represents a small fraction of the total vocabulary (50K tokens), potentially missing edge cases in learning dynamics
- Checkpoint strategy with linearly increasing intervals may miss critical learning transitions that occur at different rates

## Confidence
**High Confidence:** The relationship between token frequency and learning stability (earlier acquisition, lower variability, reduced forgettability) is well-supported by the data and consistent across multiple metrics and cross-run comparisons.

**Medium Confidence:** The influence of n-gram probabilities on learning dynamics, while observed, may be confounded by token frequency effects since high-probability n-grams often contain frequent tokens.

**Medium Confidence:** The effect of context length and frequency on learning stability shows clear statistical relationships but the practical significance for model performance requires further validation.

## Next Checks
1. **Generalization Test:** Train identical 124M models on different corpora (e.g., Wikipedia, Common Crawl) to verify whether frequency-based learning patterns hold across domains.

2. **Scale Sensitivity Analysis:** Compare learning curves from 124M parameter models against intermediate-sized models (355M, 774M) to identify where frequency effects begin to saturate or change character.

3. **Cross-Architecture Validation:** Apply the same analysis framework to encoder-decoder models (e.g., T5) to determine if frequency-based learning dynamics are architecture-specific or universal across transformer variants.