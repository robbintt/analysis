---
ver: rpa2
title: 'Foundational Models Defining a New Era in Vision: A Survey and Outlook'
arxiv_id: '2307.13721'
source_url: https://arxiv.org/abs/2307.13721
tags:
- arxiv
- language
- vision
- image
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews foundational models in computer
  vision, focusing on their architecture types, training objectives, downstream task
  adaptation, and prompting designs. It categorizes models into textually-prompted,
  visually-prompted, and heterogeneous modality models.
---

# Foundational Models Defining a New Era in Vision: A Survey and Outlook

## Quick Facts
- arXiv ID: 2307.13721
- Source URL: https://arxiv.org/abs/2307.13721
- Reference count: 40
- Primary result: Comprehensive survey of foundational vision models covering architecture types, training objectives, and adaptation strategies

## Executive Summary
This survey provides a systematic review of foundational models in computer vision, categorizing them into textually-prompted, visually-prompted, heterogeneous modality models, and embodied agents. The work analyzes how these models generalize across vision tasks through different prompting mechanisms and discusses their applications in zero-shot recognition, localization, and cross-modal understanding. The survey identifies critical challenges including limited contextual understanding, biases, vulnerability to adversarial attacks, and the need for better evaluation benchmarks. It emphasizes the importance of developing multimodal open-source models with instruction-following capabilities comparable to large language models.

## Method Summary
This survey systematically reviews foundational vision models through comprehensive literature analysis, categorizing models by their prompting mechanisms (textual, visual, heterogeneous) and their architectural approaches. The authors analyze training objectives including contrastive learning, generative methods, and hybrid approaches used across different model families. Performance evaluation spans various vision tasks including zero-shot classification, object detection, segmentation, and visual reasoning. The survey synthesizes findings from multiple sources to identify common challenges and research directions in the field.

## Key Results
- Foundational vision models achieve zero-shot generalization through prompt engineering that leverages cross-modal alignment learned during pre-training
- Visual prompt-based models like SAM can generalize to diverse segmentation tasks by producing valid masks even for ambiguous prompts
- Heterogeneous modality models learn unified representation spaces by aligning paired data from different modalities (image-text, video-audio, image-depth)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Textually prompted VLMs can generalize to new visual tasks without task-specific fine-tuning by leveraging carefully engineered prompts.
- Mechanism: Prompt engineering converts task labels or descriptions into natural language templates that guide the pre-trained VLM to produce outputs aligned with the task. This exploits the cross-modal alignment learned during pre-training to map language prompts to visual concepts.
- Core assumption: The pre-trained model has learned a rich cross-modal embedding space where textual prompts can serve as class descriptors or task specifications.
- Evidence anchors:
  - [abstract]: "The output of such models can be modified through human-provided prompts without retraining, e.g., segmenting a particular object by providing a bounding box..."
  - [section]: "Most vision datasets consist of images and corresponding one-word labels. To leverage vision-language models on vision datasets, several works have utilized template-based prompt engineering."
  - [corpus]: Weak evidence. The related paper "A Survey of State of the Art Large Vision Language Models" mentions alignment but not prompt engineering in detail.
- Break Condition: If the pre-trained model's cross-modal embeddings are not sufficiently discriminative for the target task, or if the prompt templates do not capture the necessary task semantics, zero-shot generalization will fail.

### Mechanism 2
- Claim: Visual prompt-based models like SAM can generalize to diverse segmentation tasks by learning to produce valid masks for any prompt, even ambiguous ones.
- Mechanism: SAM is trained on a massive dataset of masks and images using a model-in-the-loop data engine. This enables it to handle various prompt types (points, boxes, masks) and output valid segmentations even when the prompt is ambiguous (e.g., a point on a person wearing a shirt).
- Core assumption: The training data is sufficiently diverse and the model architecture can effectively combine image and prompt embeddings to produce accurate masks.
- Evidence anchors:
  - [abstract]: "The output of such models can be modified through human-provided prompts without retraining, e.g., segmenting a particular object by providing a bounding box..."
  - [section]: "SAM is trained to output a valid segmentation mask even when the given prompt is ambiguous e.g., given a point prompt on a person wearing a shirt, the model has to segment the shirt or the person wearing it."
  - [corpus]: Weak evidence. The related paper "How Can Time Series Analysis Benefit From Multiple Modalities?" is not directly relevant to visual segmentation.
- Break Condition: If the prompt is too ambiguous or the model has not seen similar prompts during training, the output mask may be incorrect or nonsensical.

### Mechanism 3
- Claim: Heterogeneous modality models can learn a unified representation space by aligning paired data from different modalities (e.g., image-text, video-audio, image-depth).
- Mechanism: These models use contrastive or generative objectives to align the representations of different modalities in a shared embedding space. This allows for cross-modal retrieval and understanding.
- Core assumption: The paired data is sufficiently aligned and the model architecture can effectively fuse the information from different modalities.
- Evidence anchors:
  - [abstract]: "We discuss the open challenges and research directions for foundational models in computer vision, including difficulties in their evaluations and benchmarking, gaps in their real-world understanding, limitations of their contextual understanding..."
  - [section]: "Image Bind combines large-scale paired data (image, text) with other paired data modalities (video, audio) or (image, depth) to develop a joint feature representation, thereby aligning each other modality (audio, depth) with textual embedding."
  - [corpus]: Weak evidence. The related paper "A Survey on Mamba Architecture for Vision Applications" focuses on a different architecture and is not directly relevant to heterogeneous modalities.
- Break Condition: If the paired data is not well-aligned or the model architecture cannot effectively fuse the information, the learned representation space may not be meaningful for cross-modal tasks.

## Foundational Learning

- Concept: Cross-modal embedding spaces
  - Why needed here: Understanding how VLMs map images and text into a shared representation space is crucial for designing effective prompts and understanding their generalization capabilities.
  - Quick check question: How does the choice of prompt template affect the model's ability to generalize to new tasks?
- Concept: Self-supervised learning objectives
  - Why needed here: Many foundational models are trained using self-supervised objectives (e.g., contrastive learning, masked modeling) to learn from large-scale unlabeled data. Understanding these objectives is key to understanding the model's capabilities and limitations.
  - Quick check question: How do different self-supervised objectives (e.g., contrastive vs. generative) affect the model's performance on downstream tasks?
- Concept: Prompt engineering techniques
  - Why needed here: Prompt engineering is a crucial technique for adapting VLMs to new tasks without fine-tuning. Understanding different prompt engineering strategies is essential for leveraging the full potential of these models.
  - Quick check question: How can we design prompt templates that are both general enough to work across tasks and specific enough to guide the model effectively?

## Architecture Onboarding

- Component map: Image encoder -> Text encoder -> Cross-modal fusion -> Prompt encoder (if applicable) -> Mask decoder (if applicable) -> Output
- Critical path: Image/Text encoder → Cross-modal fusion → Prompt encoder (if applicable) → Mask decoder (if applicable) → Output
- Design tradeoffs:
  - Separate encoders vs. shared encoders: Separate encoders allow for efficient inference but may not capture fine-grained cross-modal interactions. Shared encoders can capture more complex interactions but are computationally expensive.
  - Global features vs. local features: Global features are efficient but may not be suitable for localization tasks. Local features are more informative but require more complex architectures.
  - Pre-training data size vs. model size: Larger datasets and models generally lead to better performance but require more computational resources.
- Failure signatures:
  - Poor zero-shot performance: Indicates that the cross-modal embeddings are not sufficiently discriminative.
  - Hallucinations: Indicates that the model is ignoring the visual input and relying solely on the text prompt.
  - Limited contextual understanding: Indicates that the model is struggling to understand complex visual scenes or sarcasm.
- First 3 experiments:
  1. Evaluate the model's zero-shot performance on a held-out dataset using different prompt templates.
  2. Analyze the model's attention maps to understand how it is processing the visual and textual inputs.
  3. Test the model's robustness to adversarial attacks by perturbing the input images or text prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop multimodal open-source models with ChatGPT-like instruction-following and human-feedback capabilities?
- Basis in paper: Explicit - The paper discusses the importance of instruction-following and human-feedback-based reinforcement learning for large language models, and the need for similar capabilities in multimodal open-source foundation models.
- Why unresolved: Current multimodal open-source models like BLIP2, GIT, and Flamingo have not yet achieved the same level of instruction-following and human-intent alignment as ChatGPT. Developing such models requires further research in areas like instruction tuning and human feedback integration.
- What evidence would resolve it: Demonstration of a multimodal open-source foundation model that matches or surpasses ChatGPT's capabilities in instruction-following and human-intent alignment across diverse multimodal tasks and scenarios.

### Open Question 2
- Question: What are the most effective strategies to mitigate hallucinations in large multimodal models?
- Basis in paper: Explicit - The paper discusses hallucination as a significant challenge for large language and vision models, including those based on generative pretrained models for open-ended conversations.
- Why unresolved: While some strategies like explicit instructions, chain-of-thought prompting, and knowledge base integration are mentioned, their effectiveness in mitigating hallucinations in large multimodal models is not fully established.
- What evidence would resolve it: Empirical evaluation and comparison of different hallucination mitigation strategies on a comprehensive set of multimodal tasks and scenarios, demonstrating significant reductions in hallucinated outputs.

### Open Question 3
- Question: How can we develop robust evaluation and benchmarking methods for open-ended conversational vision-language models?
- Basis in paper: Explicit - The paper highlights the challenges in comprehensively evaluating open-ended conversational vision-language models due to their diverse capabilities and reasoning aspects.
- Why unresolved: Existing evaluation approaches like "LLM-as-a-judge" and task-specific benchmarks have limitations and may not fully capture the complexities of open-ended conversational vision-language models.
- What evidence would resolve it: Development and validation of a comprehensive evaluation framework that effectively measures the performance, reasoning abilities, and robustness of open-ended conversational vision-language models across a wide range of tasks and scenarios.

## Limitations

- The analysis relies heavily on reported zero-shot performance metrics that vary across different benchmarks and evaluation protocols
- The survey's scope excludes some emerging approaches like Mamba-based architectures and time-series multimodal models
- Evaluation of real-world deployment challenges remains largely theoretical without concrete case studies or deployment metrics

## Confidence

- High confidence in the categorization of model types and their general capabilities
- Medium confidence in the mechanisms of zero-shot generalization and prompt engineering effectiveness
- Medium confidence in the identification of open challenges and research directions
- Low confidence in specific performance comparisons across different benchmarks

## Next Checks

1. Conduct controlled experiments testing zero-shot generalization across multiple foundational models using standardized prompt templates and evaluation metrics
2. Perform ablation studies on prompt engineering components to identify which elements most strongly influence task performance
3. Design and implement adversarial robustness tests specifically for vision-language models to quantify their vulnerability to attacks mentioned in the survey