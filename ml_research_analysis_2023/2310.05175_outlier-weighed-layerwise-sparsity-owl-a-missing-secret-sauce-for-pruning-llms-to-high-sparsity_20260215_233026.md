---
ver: rpa2
title: 'Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning
  LLMs to High Sparsity'
arxiv_id: '2310.05175'
source_url: https://arxiv.org/abs/2310.05175
tags:
- sparsity
- pruning
- layerwise
- wanda
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of compressing large language
  models (LLMs) to make them more practical for deployment, focusing on unstructured
  pruning techniques. While existing methods uniformly prune all layers at the same
  sparsity level, the authors identify a key insight: outlier features, which have
  significantly larger magnitudes, play a crucial role in LLM performance and are
  distributed non-uniformly across layers.'
---

# Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity

## Quick Facts
- arXiv ID: 2310.05175
- Source URL: https://arxiv.org/abs/2310.05175
- Reference count: 12
- This paper proposes a method to compress large language models by preserving outlier features through non-uniform layerwise sparsity ratios.

## Executive Summary
This paper introduces Outlier Weighed Layerwise Sparsity (OWL), a novel unstructured pruning method that significantly improves LLM compression by assigning layerwise sparsity ratios proportional to outlier distributions within each layer. The key insight is that outlier features—those with significantly larger magnitudes—play a disproportionate role in LLM performance and are distributed non-uniformly across layers. By preserving more outliers in layers where they matter most, OWL achieves state-of-the-art performance at high sparsity levels, delivering 2.6x end-to-end inference speed-up while maintaining model quality.

## Method Summary
OWL computes Layerwise Outlier Distribution (LOD) by measuring the magnitude of each weight multiplied by its corresponding activation norm, then identifying outliers as features exceeding M times the layer average. The method assigns layerwise sparsity ratios inversely proportional to these outlier ratios, constrained within [S-λ, S+λ] around target sparsity S, and normalized to maintain average sparsity. OWL can be combined with existing pruning metrics like Wanda or SparseGPT, introducing minimal computational overhead while delivering significant performance gains at high sparsity levels (50-70%).

## Key Results
- OWL surpasses state-of-the-art methods by 61.22 perplexity at 70% sparsity compared to Wanda
- OWL outperforms SparseGPT by 6.80 perplexity at 70% sparsity while maintaining 2.6x inference speed-up
- OWL delivers consistent improvements across LLaMA-7B, 13B, 30B, 65B and OPT-6.7B models on WikiText validation and seven zero-shot downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layerwise sparsity that aligns with outlier distribution preserves more important features
- Mechanism: OWL assigns lower sparsity to layers with higher outlier ratios, ensuring critical high-magnitude features are retained where they matter most
- Core assumption: Outlier features in LLMs have disproportionate importance for model performance
- Evidence anchors: [abstract] "The sparsity ratio of OWL is directly proportional to the outlier ratio observed within each layer"

### Mechanism 2
- Claim: Non-uniform layerwise sparsity adapts to architectural importance variations
- Mechanism: Different layers have different outlier distributions, so treating them uniformly wastes capacity on less important regions
- Core assumption: Not all layers contribute equally to LLM performance
- Evidence anchors: [section] "LLaMA-7B, 13B, and 30B... loosely follows a 'U' shape, with notable proportions at both ends"

### Mechanism 3
- Claim: OWL provides computational efficiency while maintaining pruning effectiveness
- Mechanism: Uses existing pruning metrics (Wanda) but applies them with layerwise awareness rather than global or uniform approaches
- Core assumption: Computational overhead of layerwise adaptation is manageable
- Evidence anchors: [section] "OWL introduces nearly negligible overhead when compared to SparseGPT"

## Foundational Learning

- Concept: Outlier features in neural networks
  - Why needed here: Understanding why preserving outliers matters is central to OWL's design rationale
  - Quick check question: What distinguishes outlier features from regular features in terms of magnitude and importance?

- Concept: Layerwise sparsity in neural network pruning
  - Why needed here: OWL's core innovation is non-uniform layerwise sparsity allocation
  - Quick check question: How does uniform layerwise sparsity differ from global sparsity and when would each be preferable?

- Concept: Weight pruning metrics and their computational costs
  - Why needed here: OWL builds on existing pruning metrics while adding layerwise adaptation
  - Quick check question: What are the computational trade-offs between magnitude pruning, second-order methods, and activation-based metrics?

## Architecture Onboarding

- Component map: Pre-trained LLM -> LOD computation -> OWL sparsity calculation -> Sparse weight matrices
- Critical path: 1. Compute LOD for each layer using activation magnitudes and weight values 2. Calculate layerwise sparsity ratios using OWL formula 3. Apply pruning with computed layerwise sparsity levels
- Design tradeoffs: OWL vs uniform (better performance vs higher computational cost); OWL vs global (more targeted preservation vs avoiding outlier collapse); OWL vs ERK (better LLM-specific adaptation vs more general vision applicability)
- Failure signatures: Performance degradation at high sparsity despite OWL application; excessive layerwise sparsity variance causing training instability; computational overhead becoming prohibitive for very large models
- First 3 experiments: 1. Run OWL on small LLM (7B) at moderate sparsity (50%) and compare to uniform baseline 2. Vary the hyperparameter λ to find optimal range for different model sizes 3. Compare OWL.w Wanda vs OWL.w SparseGPT to validate method independence

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Method may not generalize well to architectures where outlier features are less pronounced or distributed differently
- Computational efficiency claims are specific to DeepSparse inference engine and may not translate to other frameworks
- Limited exploration of robustness across different training regimes or fine-tuning scenarios

## Confidence
- High confidence: The correlation between outlier features and layer importance, the basic OWL formula derivation, and empirical superiority over uniform sparsity baselines
- Medium confidence: Computational efficiency claims and generalization to other LLM architectures beyond LLaMA-V1 family
- Low confidence: Effectiveness on highly sparse regimes (>80%) and behavior under different training/fine-tuning conditions

## Next Checks
1. Test OWL on vision transformer architectures to verify if outlier-based layerwise sparsity generalizes beyond LLMs
2. Evaluate the impact of different outlier thresholds (M parameter) on final performance to establish sensitivity
3. Compare OWL's performance against adaptive layerwise methods that use different importance metrics (not just outliers)