---
ver: rpa2
title: Time-Series Contrastive Learning against False Negatives and Class Imbalance
arxiv_id: '2312.11939'
source_url: https://arxiv.org/abs/2312.11939
tags:
- learning
- contrastive
- loss
- classification
- 'false'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two critical issues in time-series contrastive
  learning: false negatives and class imbalance. The authors propose a novel Semi-supervised
  Instance-graph-based Pseudo-Label Distribution Learning (SIP-LDL) framework to mitigate
  these challenges.'
---

# Time-Series Contrastive Learning against False Negatives and Class Imbalance

## Quick Facts
- arXiv ID: 2312.11939
- Source URL: https://arxiv.org/abs/2312.11939
- Reference count: 10
- Key outcome: SIP-LDL framework significantly outperforms state-of-the-art methods on four time-series datasets, achieving 84.32% accuracy and 74.46% F1-score on Sleep-EDF with only 10% labeled data

## Executive Summary
This paper addresses two critical challenges in time-series contrastive learning: false negatives and class imbalance. The authors propose a novel Semi-supervised Instance-graph-based Pseudo-Label Distribution Learning (SIP-LDL) framework that combines multiple-instances discrimination, instance graph convolution, and semi-supervised consistency classification. The method demonstrates significant improvements over state-of-the-art approaches, particularly for minority class performance, while maintaining effectiveness in semi-supervised scenarios with limited labeled data.

## Method Summary
SIP-LDL introduces a multiple-instances discrimination task that approximates supervised contrastive learning by learning pseudo-label distributions based on instance similarity, reducing false negatives. The framework employs graph convolution to enhance inter-instance relationships and a semi-supervised consistency classification strategy to improve minority class representation using limited labeled data. The method combines single-instance discrimination, multiple-instance discrimination, and semi-supervised consistency classification losses with weighted parameters λ1=λ2=1.

## Key Results
- Achieved 84.32% accuracy and 74.46% F1-score on Sleep-EDF dataset
- Outperformed baselines like SimCLR and TSTCC across all four tested datasets
- Showed optimal results with only 10% labeled data in semi-supervised scenarios
- Demonstrated effectiveness particularly for minority class performance

## Why This Works (Mechanism)

### Mechanism 1
The multiple-instances discrimination task approximates supervised contrastive learning by learning a pseudo-label distribution for each instance based on instance similarity. This reduces false negatives by creating a supervised-like signal without requiring true labels, assuming instance similarity correlates with semantic similarity.

### Mechanism 2
Instance graph convolution enhances feature interaction learning and improves minority class representation by leveraging neighborhood information through graph propagation. Majority class instances gain more learning from same-class neighbors, while minority class instances receive enhanced loss through semi-supervised consistency classification.

### Mechanism 3
Semi-supervised consistency classification with limited labeled data improves minority class representation by enforcing consistency between representations before and after graph convolution. The minority class suffers from more noisy neighbor instances after graph propagation, creating a stronger learning signal through enhanced consistency loss.

## Foundational Learning

- **Concept**: Contrastive learning and InfoNCE loss
  - Why needed here: The paper builds on contrastive learning framework and addresses specific issues with InfoNCE loss in time-series data
  - Quick check question: What is the difference between supervised and unsupervised contrastive loss formulations?

- **Concept**: Graph convolutional networks
  - Why needed here: GCN is used to replace the MLP projection head for enhanced feature interaction learning
  - Quick check question: How does GCN differ from standard MLP in terms of feature propagation?

- **Concept**: Class imbalance and its effects on representation learning
  - Why needed here: The paper specifically addresses how class imbalance affects contrastive learning and proposes solutions
  - Quick check question: Why do minority classes suffer from underfitting in imbalanced datasets?

## Architecture Onboarding

- **Component map**: Input → Augmentation → Encoder → Instance Graph Construction → GCN Projection → Multiple-instance Discrimination + Single-instance Discrimination + Semi-supervised Consistency Classification → Output
- **Critical path**: Augmentation → Encoder → Similarity Matrix → Instance Graph → GCN → Contrastive Loss → Classification Loss
- **Design tradeoffs**: Using GCN instead of MLP increases computational complexity but improves feature interaction; requiring labeled data for consistency classification reduces the fully unsupervised nature
- **Failure signatures**: Poor performance on minority classes suggests GCN construction or consistency classification is failing; overall performance degradation suggests multiple-instance discrimination is not effectively reducing false negatives
- **First 3 experiments**:
  1. Run with only single-instance discrimination (MLP+LID) to establish baseline
  2. Add multiple-instance discrimination without GCN (MLP+LMID) to isolate its effect
  3. Add GCN with single-instance discrimination (GCN+LID) to isolate GCN effect

## Open Questions the Paper Calls Out

### Open Question 1
How does SIP-LDL perform on time-series datasets with extremely severe class imbalance (e.g., minority class ratio below 1%) compared to moderately imbalanced datasets? The paper demonstrates effectiveness on datasets with imbalance ratios ranging from 1.44 to 40.34 but does not test on extremely severe imbalance cases.

### Open Question 2
What is the impact of varying the proportion of labeled data used for semi-supervised consistency classification on SIP-LDL's performance? The paper uses 10% labeled data but does not explore how different proportions affect overall performance.

### Open Question 3
How does SIP-LDL's performance compare to other semi-supervised learning approaches specifically designed for imbalanced time-series data? The paper does not compare to other semi-supervised methods tailored for handling class imbalance in time-series data.

## Limitations
- Requires labeled data for consistency classification, reducing fully unsupervised applicability
- Performance depends on quality of instance similarity measurements, which may be domain-specific
- GCN-based projection head increases computational complexity compared to standard MLP approaches

## Confidence
- Multiple-instance discrimination mechanism: Medium - theoretical framework sound but needs empirical validation across diverse domains
- GCN effectiveness for minority classes: Medium - assumes neighborhood information is beneficial but may contain noisy edges
- Semi-supervised consistency classification: Medium - effectiveness depends heavily on quality and representativeness of limited labeled data

## Next Checks
1. **Ablation study on labeled data quantity**: Systematically vary percentage of labeled data (1%, 5%, 10%, 20%) to determine minimum requirement for effective consistency classification
2. **Similarity graph quality analysis**: Analyze correlation between instance similarity (dot product) and semantic similarity across different time-series domains
3. **Cross-domain generalization test**: Evaluate SIP-LDL on additional time-series datasets from different domains to assess generalizability beyond the four tested datasets