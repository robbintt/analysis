---
ver: rpa2
title: A White-Box False Positive Adversarial Attack Method on Contrastive Loss Based
  Offline Handwritten Signature Verification Models
arxiv_id: '2308.08925'
source_url: https://arxiv.org/abs/2308.08925
tags:
- attack
- loss
- image
- signature
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of white-box false positive adversarial
  attacks on contrastive loss based offline handwritten signature verification models.
  The core method idea is to treat the attack as a style transfer between closely
  related but distinct writing styles, introducing two new loss functions to guide
  the generation of deceptive images by perturbing the Euclidean distance between
  the embedding vectors of the original and synthesized samples while ensuring minimal
  perturbations.
---

# A White-Box False Positive Adversarial Attack Method on Contrastive Loss Based Offline Handwritten Signature Verification Models

## Quick Facts
- arXiv ID: 2308.08925
- Source URL: https://arxiv.org/abs/2308.08925
- Reference count: 8
- This paper addresses the problem of white-box false positive adversarial attacks on contrastive loss based offline handwritten signature verification models.

## Executive Summary
This paper presents a novel white-box adversarial attack method specifically designed for contrastive loss-based offline handwritten signature verification systems. The approach treats the attack as a style transfer problem between similar writing styles, introducing two new loss functions to generate deceptive images while maintaining minimal perturbations. The method achieves state-of-the-art performance in false positive attacks, successfully fooling signature verification models with high success rates on both CEDAR and BHSig260-B datasets.

## Method Summary
The proposed method initializes with a forged signature image and generates a synthesized image through iterative optimization. The synthesized image is processed through a frozen Siamese neural network (pre-trained with contrastive loss) alongside the genuine signature. Four loss functions are computed: Attack Loss (encourages embedding distance below threshold τ), Difference Loss (minimizes difference between synthesized and forged images), Style Loss (aligns visual attributes using Gram matrices), and TV Loss (reduces noise). These are combined into a Total Loss optimized via Adam over 50 epochs to produce adversarial examples that visually resemble genuine signatures while deceiving the verification model.

## Key Results
- Achieves 99.71% success rate in false positive attacks on the CEDAR dataset
- Achieves 82.93% success rate on the BHSig260-B dataset
- Significantly outperforms existing white-box attack methods (FGSM, MIM, IGS)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed method succeeds by treating signature verification as a style transfer problem between similar writing styles, effectively bridging the stylistic gap between genuine and forged signatures.
- Mechanism: The attack uses style transfer to align the visual attributes of manipulated signatures with those of genuine signatures by minimizing the difference in Gram matrices of feature representations.
- Core assumption: Genuine and forged signatures share identical content but have slightly different writing styles, making style transfer an effective approach.
- Evidence anchors:
  - [abstract] "treats the attack as a style transfer between closely related but distinct writing styles"
  - [section] "we propose to think of signature verification as a comparison of writing styles with identical content"
  - [corpus] Weak evidence - the corpus neighbors don't directly discuss style transfer in signature verification contexts
- Break condition: If the assumption that forged signatures share identical content but different styles doesn't hold, or if the model doesn't rely on stylistic features for verification.

### Mechanism 2
- Claim: The two novel loss functions (Attack Loss and Difference Loss) effectively guide the generation of deceptive images while maintaining minimal perturbations.
- Mechanism: Attack Loss encourages the Euclidean distance between genuine and synthesized embeddings to fall below threshold τ, while Difference Loss ensures the synthesized image remains close to the original forged image.
- Core assumption: The contrastive loss model's decision boundary is sensitive to small perturbations in the embedding space when the Euclidean distance falls below threshold τ.
- Evidence anchors:
  - [abstract] "two new loss functions that enhance the attack success rate by perturbing the Euclidean distance between the embedding vectors"
  - [section] "encouraging the reduction of the Euclidean distance between the vector representations of the synthesized image and the genuine image"
  - [corpus] No direct evidence - corpus doesn't discuss contrastive loss-based attacks specifically
- Break condition: If the model's decision boundary becomes insensitive to embedding distance perturbations, or if the Difference Loss prevents sufficient perturbation magnitude.

### Mechanism 3
- Claim: The combination of style transfer with novel loss functions creates adversarial samples that preserve high stealth and visual consistency while effectively confusing the verification model.
- Mechanism: The composite Total Loss function balances Attack Loss, Difference Loss, Style Loss, and TV Loss to generate images that are visually similar to genuine signatures but close enough in vector space to deceive the model.
- Core assumption: The signature verification model's vulnerability can be exploited through a multi-objective optimization that considers both visual similarity and embedding proximity.
- Evidence anchors:
  - [abstract] "ensuring minimal perturbations by reducing the difference between the generated image and the original image"
  - [section] "generate adversarial samples that preserve a high level of stealth and visual consistency while still effectively confusing the signature verification model"
  - [corpus] No direct evidence - corpus doesn't discuss multi-objective optimization for adversarial attacks on signature verification
- Break condition: If the model incorporates additional verification steps that are insensitive to the combined optimization, or if the weights in Total Loss cannot be tuned effectively.

## Foundational Learning

- Concept: Contrastive loss function
  - Why needed here: The attack specifically targets contrastive loss-based signature verification models, which learn representations where similar signatures are close in latent space and dissimilar ones are far apart
  - Quick check question: How does a contrastive loss function differ from a classification loss in terms of what it optimizes for?

- Concept: Siamese neural network architecture
  - Why needed here: The target model uses a Siamese architecture that processes genuine and forged signatures through twin networks to compute embedding distances
  - Quick check question: What is the purpose of having identical sub-networks in a Siamese architecture, and how does this help with signature verification?

- Concept: Style transfer using Gram matrices
  - Why needed here: The attack employs style transfer principles to align the visual attributes of forged signatures with genuine ones by matching Gram matrices of feature representations
  - Quick check question: How do Gram matrices capture style information in images, and why is this useful for making adversarial examples appear more genuine?

## Architecture Onboarding

- Component map: Genuine signature image → Siamese network (frozen) → Embedding vector; Forged signature image → Synthesized image (trainable) → Siamese network (frozen) → Embedding vector; Total Loss computation → Adam optimizer → Synthesized image update

- Critical path: Forged image → Synthesized image generation → Siamese network processing → Loss computation → Synthesized image update → Attack success when embedding distance < threshold

- Design tradeoffs:
  - Balance between attack success rate and visual perceptibility of perturbations
  - Weight selection for Total Loss components affects performance
  - Epoch count impacts both success rate and computational cost

- Failure signatures:
  - Attack fails if synthesized image remains too far from genuine in embedding space
  - Attack fails if perturbations become visually obvious despite low embedding distance
  - Style transfer may fail if genuine and forged signatures have fundamentally different content

- First 3 experiments:
  1. Test basic Attack Loss on CEDAR dataset with no style transfer - expect moderate success rate
  2. Test full method with style transfer on CEDAR dataset - expect high success rate
  3. Test method without style transfer on BHSig260-B dataset - compare to full method to quantify style transfer contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would the proposed method be against models using different loss functions, such as triplet loss or ArcFace?
- Basis in paper: [inferred] The authors mention that their technique has been effective against models trained using contrastive loss but has not been tested against models based on other loss functions.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis on the effectiveness of the proposed method against models using different loss functions.
- What evidence would resolve it: Experimental results comparing the proposed method's performance on models trained with contrastive loss, triplet loss, and ArcFace would provide insights into its generalizability.

### Open Question 2
- Question: Can the proposed method work effectively even with lower-quality signature examples?
- Basis in paper: [inferred] The authors acknowledge that their method assumes attackers have access to a clear, high-quality example of the target signature, which may not always be the case in real-world scenarios.
- Why unresolved: The paper does not provide any experimental results or analysis on the proposed method's performance when applied to lower-quality signature examples.
- What evidence would resolve it: Experimental results comparing the proposed method's performance on high-quality and lower-quality signature examples would provide insights into its robustness.

### Open Question 3
- Question: How does the proposed method's performance compare to other state-of-the-art adversarial attack methods on signature verification models using contrastive loss?
- Basis in paper: [explicit] The authors compare their method to FGSM, MIM, and IGS, demonstrating superior performance in FP attacks on contrastive loss-based signature verification models.
- Why unresolved: The paper does not provide a comprehensive comparison with other state-of-the-art adversarial attack methods specifically designed for contrastive loss-based signature verification models.
- What evidence would resolve it: Experimental results comparing the proposed method's performance with other state-of-the-art adversarial attack methods on contrastive loss-based signature verification models would provide insights into its relative effectiveness.

## Limitations
- The method assumes access to high-quality genuine signature examples, which may not always be available in real-world scenarios
- The effectiveness against signature verification systems using different loss functions or architectural designs remains untested
- The evaluation focuses primarily on success rates without comprehensive analysis of visual perceptibility or defensive robustness

## Confidence
- **High confidence**: The core mechanism of using style transfer combined with contrastive loss perturbation is theoretically sound and supported by the mathematical formulation presented
- **Medium confidence**: The reported success rates (99.71% on CEDAR, 82.93% on BHSig260-B) are likely accurate for the specific experimental setup, but may not generalize to all contrastive loss-based verification systems
- **Low confidence**: The method's performance against real-world signature verification systems with additional security measures or different architectural designs

## Next Checks
1. Conduct experiments varying the loss function weights (α, β, γ, δ) to determine optimal configurations and identify if the reported results depend on specific weight choices

2. Implement quantitative metrics to measure the visual similarity between original forged signatures and generated adversarial examples, correlating perturbation magnitude with attack success rate

3. Evaluate the attack's effectiveness against defensive mechanisms such as adversarial training, input preprocessing, or alternative feature extraction methods in the verification pipeline