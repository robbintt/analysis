---
ver: rpa2
title: Learning Safe Numeric Planning Action Models
arxiv_id: '2312.10705'
source_url: https://arxiv.org/abs/2312.10705
tags:
- action
- n-sam
- learning
- observations
- numeric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: N-SAM learns safe numeric action models but requires many observations
  per action, limiting its practicality. N-SAM overcomes this by projecting observations
  onto a lower-dimensional subspace where a convex hull can be constructed with fewer
  samples.
---

# Learning Safe Numeric Planning Action Models

## Quick Facts
- arXiv ID: 2312.10705
- Source URL: https://arxiv.org/abs/2312.10705
- Reference count: 9
- Key outcome: N-SAM* learns safe numeric action models with fewer observations than N-SAM by projecting onto lower-dimensional subspaces

## Executive Summary
This paper addresses the challenge of learning safe numeric action models for planning domains with both discrete and continuous state variables. The proposed N-SAM* algorithm improves upon N-SAM by projecting observations onto a lower-dimensional subspace, enabling safe action model learning with significantly fewer samples. The approach guarantees that generated plans are applicable and achieve their intended goals, while being proven optimal in sample complexity compared to any other safe learning algorithm.

## Method Summary
N-SAM* learns numeric action models by projecting observations onto the subspace spanned by observed states. For each action, it computes an orthonormal basis for this subspace using a modified Gram-Schmidt process, then projects all observations onto this basis. Preconditions are learned by constructing convex hulls in the projected space and translating them to PDDL preconditions, while effects are learned via linear regression. The algorithm includes every observed action after just one observation while maintaining safety guarantees, achieving optimal sample complexity through this projection-based approach.

## Key Results
- N-SAM* solves significantly more planning problems with fewer observations than N-SAM across benchmark domains
- The algorithm is proven optimal in sample complexity compared to any other safe numeric action model learning algorithm
- N-SAM* maintains ϵ-safety guarantees while requiring fewer than n+1 observations per action

## Why This Works (Mechanism)

### Mechanism 1
- Claim: N-SAM* learns safe action models with fewer observations by projecting onto a lower-dimensional subspace
- Mechanism: Projects observations onto the subspace spanned by observed states, learning preconditions/effects in this reduced space to enable learning with fewer than n+1 affinely independent samples
- Core assumption: Actions are applicable in states linearly dependent on observations and within the projected convex hull
- Evidence anchors: Abstract states N-SAM* "overcomes this by projecting observations onto a lower-dimensional subspace where a convex hull can be constructed with fewer samples." Section states the key observation is that actions can be safely applicable if states lie on the observation subspace and in the projected convex hull.

### Mechanism 2
- Claim: N-SAM* is optimal in sample complexity compared to any other safe numeric action model learning algorithm
- Mechanism: By learning in the observation subspace, N-SAM* can declare actions applicable in any state linearly dependent on observations and within the projected convex hull, a superset of what other safe algorithms can declare
- Core assumption: Safety requires learned models only allow plans applicable in the real environment
- Evidence anchors: Abstract claims N-SAM* "is optimal in terms of sample complexity compared to any other algorithm that guarantees safety." Theorem 3.2 proves that for any safe model M and state s, if a is applicable in s according to M, it's also applicable according to N-SAM*.

### Mechanism 3
- Claim: N-SAM* maintains ϵ-safety guarantees while requiring fewer observations
- Mechanism: Uses the same safety framework as N-SAM (ϵ-safety where plans execute with total error magnitude at most ϵ times plan length) but applies it in the projected subspace
- Core assumption: The ϵ-safety guarantee from N-SAM extends to the projected subspace representation
- Evidence anchors: Section states N-SAM* "ensures the acquisition of safe preconditions and ϵ-safe effects, generating action models that are ϵ-safe." The paper mentions N-SAM learns models with ϵ-safety guarantees.

## Foundational Learning

- Concept: Convex hulls and their role in defining safe preconditions
  - Why needed here: N-SAM and N-SAM* use convex hulls to define numeric preconditions - actions are safe to apply in states within the convex hull of observed pre-states
  - Quick check question: If you observe states at points (1,0,0), (0,1,0), and (0,0,1) in 3D space, how many more affinely independent points do you need to construct a convex hull?

- Concept: Linear dependence and subspace projection
  - Why needed here: N-SAM* projects observations onto the subspace spanned by observed states, requiring understanding of linear algebra concepts like basis, projection, and linear dependence
  - Quick check question: Given vectors v1=(1,0,0) and v2=(0,1,0), what is the projection of vector (2,3,4) onto the subspace they span?

- Concept: Gram-Schmidt orthogonalization process
  - Why needed here: N-SAM* uses a modified Gram-Schmidt process to find orthonormal bases for subspaces and their complements
  - Quick check question: What property does the Gram-Schmidt process guarantee for the output vectors when applied to a set of linearly independent vectors?

## Architecture Onboarding

- Component map: Observation extraction → Subspace computation (Base, CompBase) → Projection onto Base → Convex hull construction → Precondition translation → Effect learning (linear regression) → Action model output

- Critical path: Observation → Subspace computation → Projection → Convex hull → Precondition translation → Effect learning → Action model output

- Design tradeoffs:
  - Memory vs. accuracy: Using polynomial representations increases memory requirements exponentially with degree
  - Computational complexity vs. safety: More sophisticated projection methods might improve sample efficiency but increase computation time
  - Generality vs. specificity: Learning in full space (N-SAM) vs. projected space (N-SAM*) trades generality for sample efficiency

- Failure signatures:
  - Empty action model: No actions learned - likely insufficient observations or poor representation of action effects
  - Incomplete action model: Some actions learned, others not - may indicate inconsistent observations or complex preconditions requiring more samples
  - Plans not applicable: Generated plans fail in real environment - safety guarantee may be violated due to numerical precision issues or unmodeled state variables

- First 3 experiments:
  1. Run N-SAM* on a simple 2D domain (like the Counters domain) with 1-3 observations per action to verify basic functionality
  2. Compare N-SAM* vs N-SAM on a domain with 4-5 numeric variables, gradually increasing observations from 1 to n+1 to observe the sample complexity advantage
  3. Test N-SAM* on a domain with linearly dependent state variables to verify proper subspace projection and precondition learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can N-SAM* be extended to handle domains with polynomial preconditions and effects beyond quadratic polynomials?
- Basis in paper: [explicit] The paper mentions that N-SAM supports learning polynomial domains by creating new numeric functions for each monomial up to the desired degree, but notes that learning high-degree polynomials may become intractable due to the exponential growth in the number of monomials.
- Why unresolved: The paper does not provide a solution or analysis for handling polynomial domains beyond the quadratic case.
- What evidence would resolve it: Experimental results demonstrating N-SAM*'s performance on domains with higher-degree polynomials, or theoretical analysis of its computational complexity in such domains.

### Open Question 2
- Question: How does the choice of epsilon (ϵ) in the safety guarantees affect the performance and practicality of N-SAM*?
- Basis in paper: [explicit] The paper mentions that the safety guarantees of N-SAM* are with respect to a norm and a parameter epsilon (ϵ), but does not discuss the impact of different epsilon values on the algorithm's performance.
- Why unresolved: The paper does not provide a sensitivity analysis or guidelines for choosing epsilon in practical applications.
- What evidence would resolve it: Empirical studies showing how varying epsilon affects the number of observations needed, the safety guarantees, and the quality of the learned models.

### Open Question 3
- Question: Can the projection technique used in N-SAM* be applied to other safe action model learning algorithms that operate in high-dimensional spaces?
- Basis in paper: [inferred] The paper presents N-SAM* as an enhancement to N-SAM that overcomes its limitation of requiring many observations by projecting observations onto a lower-dimensional subspace. This technique could potentially be applied to other safe action model learning algorithms that face similar challenges in high-dimensional spaces.
- Why unresolved: The paper does not explore the applicability of the projection technique to other algorithms or domains.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the projection technique when applied to other safe action model learning algorithms, or theoretical analysis of its generalizability.

## Limitations

- The paper's claims about sample complexity optimality depend on specific assumptions about observation distributions that aren't fully characterized
- Numerical precision thresholds (ϵ values) are not specified, which could significantly impact the safety guarantees in practice
- The extension to polynomial representations beyond degree 1 is mentioned but not evaluated experimentally

## Confidence

- **High confidence**: The core mechanism of projecting observations onto lower-dimensional subspaces to reduce sample requirements is well-founded and mathematically sound
- **Medium confidence**: The sample complexity optimality proof appears rigorous, but its practical implications depend on unquantified assumptions about observation distributions
- **Medium confidence**: The experimental results demonstrate significant improvements over N-SAM, but the benchmark selection and problem generation methods could influence the magnitude of observed gains

## Next Checks

1. **Numerical precision sensitivity analysis**: Systematically vary ϵ thresholds to determine their impact on safety guarantees and model quality across different benchmark domains
2. **Representation capacity test**: Evaluate N-SAM* with polynomial representations (degree > 1) on domains with non-linear effects to assess scalability and practical utility of the generalization
3. **Observation distribution robustness**: Test N-SAM* with non-uniform observation distributions to quantify how deviations from assumed representative sampling affect sample complexity guarantees and model performance