---
ver: rpa2
title: Empirical study of pretrained multilingual language models for zero-shot cross-lingual
  knowledge transfer in generation
arxiv_id: '2310.09917'
source_url: https://arxiv.org/abs/2310.09917
tags:
- mbart
- nllb
- language
- rate
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies zero-shot cross-lingual generation with alternative
  multilingual language models (mPLMs) beyond mT5. The authors investigate mBART,
  both pretrained and translation-finetuned, and NLLB-200, comparing full finetuning
  and adapter-based parameter-efficient adaptation.
---

# Empirical study of pretrained multilingual language models for zero-shot cross-lingual knowledge transfer in generation

## Quick Facts
- **arXiv ID**: 2310.09917
- **Source URL**: https://arxiv.org/abs/2310.09917
- **Reference count**: 40
- **Primary result**: mBART with adapters performs similarly to mT5-base, while NLLB-200 is competitive in summarization for high-resource Latin-alphabet languages but weaker in QA.

## Executive Summary
This work investigates zero-shot cross-lingual generation using multilingual pretrained language models beyond mT5, specifically examining mBART (both pretrained and translation-finetuned) and NLLB-200. The study compares full finetuning with adapter-based parameter-efficient adaptation across two tasks: summarization and question answering. Experiments reveal that mBART with adapters achieves performance comparable to mT5-base, while NLLB-200 excels in summarization for high-resource Latin-script languages but underperforms in QA. Crucially, the authors demonstrate that reducing learning rates during finetuning effectively mitigates generation in the wrong language without sacrificing task performance.

## Method Summary
The study evaluates three multilingual pretrained language models (mT5, mBART, NLLB-200) using two adaptation methods (full finetuning and adapter-based finetuning) across summarization and QA tasks. Models are trained on English data for 20K steps with batch size 4000 tokens, using Adam optimizer with inverse square root learning rate schedule. Learning rate grid search is performed per task-model-adaptation combination, selecting the rate that maximizes average non-English performance while maintaining >50% correct language generation. Evaluation includes task-specific metrics (ROUGE for summarization, F1 for QA) and language correctness rate using fasttext language identification.

## Key Results
- Adapter-based finetuning performs on par with full finetuning across mT5, mBART, and NLLB models
- mBART with language codes achieves robust cross-lingual generation performance
- NLLB-200 shows competitive summarization performance for high-resource Latin-alphabet languages
- Reducing learning rate effectively mitigates wrong language generation without performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing learning rate during full finetuning mitigates generation in the wrong language without sacrificing performance.
- Mechanism: Lower learning rates prevent the model from overfitting to the source language distribution during task-specific adaptation, preserving multilingual representations that enable correct language generation in zero-shot settings.
- Core assumption: The multilingual representations learned during pretraining contain sufficient language-specific information that can be preserved with careful LR tuning.
- Evidence anchors: [abstract] "we find that simply reducing learning rate helps to alleviate this problem almost completely, without hurting performance"; [section] "With too small or too large LR the model does not learn even the English task because of too short steps or divergence. For the range of LRs when the English task is learned well, we observe that larger LRs lead to the effect reported in other works, when the model overfits to the source English language"

### Mechanism 2
- Claim: Adapter-based finetuning performs on par with full finetuning while enabling higher LR magnitudes without wrong language generation.
- Mechanism: Adapters introduce task-specific parameters while freezing pretrained weights, preserving multilingual representations and reducing catastrophic forgetting of language-specific knowledge.
- Core assumption: Freezing pretrained weights while tuning lightweight adapter modules maintains cross-lingual capabilities better than full parameter updates.
- Evidence anchors: [abstract] "Adapter-based finetuning generally performs on par with full finetuning across models"; [section] "Adapters in general enable higher LR magnitude without wrong language generation"

### Mechanism 3
- Claim: The use of language codes in decoder input/output (as in mBART and NLLB) helps guide generation in the correct language.
- Mechanism: Language codes prepended to input and target sequences provide explicit language identity signals that the decoder uses to condition generation appropriately.
- Core assumption: Language codes are effectively integrated into the model's generation process and influence token prediction probabilities.
- Evidence anchors: [abstract] "the use of target language code in decoding, such as in mBART, may help guiding generation in a correct language"; [section] "mBART is pretrained using the denoising objective... Both input sequence X and target sequence Y are prepended with the language code: [lang_code, X] and [lang_code, Y]"

## Foundational Learning

- Concept: Zero-shot cross-lingual transfer
  - Why needed here: This work investigates how models finetuned on one language can generalize to others without additional target language training data
  - Quick check question: What is the key difference between zero-shot and few-shot cross-lingual transfer?

- Concept: Catastrophic forgetting in multilingual settings
  - Why needed here: Understanding how finetuning can cause models to lose ability to generate in languages not seen during adaptation is central to this work
  - Quick check question: How does catastrophic forgetting manifest differently in cross-lingual generation versus understanding tasks?

- Concept: Parameter-efficient finetuning (adapters)
  - Why needed here: The work compares adapter-based adaptation with full finetuning to understand trade-offs in preserving multilingual capabilities
  - Quick check question: What is the typical parameter reduction ratio when using adapters versus full model finetuning?

## Architecture Onboarding

- Component map:
  - Base models: mT5 -> mBART (pretrained and translation-finetuned) -> NLLB-200
  - Adaptation methods: Full finetuning -> Adapter-based finetuning
  - Tasks: Summarization (XL-Sum) -> Question Answering (XQuAD)
  - Evaluation: Task-specific metrics (ROUGE, F1) -> Language correctness rate
  - Hyperparameter tuning: Learning rate grid search for each task-model-adaptation combination

- Critical path:
  1. Load multilingual pretrained model
  2. Apply adaptation method (full finetuning or adapters)
  3. Tune learning rate through validation on target languages
  4. Evaluate on test sets measuring both task performance and language correctness

- Design tradeoffs:
  - Adapter dimension vs. performance: Higher dimensions may improve task performance but reduce parameter efficiency
  - Learning rate magnitude vs. language correctness: Higher LRs may improve task performance but increase wrong language generation
  - Model size vs. resource constraints: Larger models achieve better absolute performance but require more compute

- Failure signatures:
  - High task performance but low language correctness rate indicates wrong language generation
  - Both metrics low suggests insufficient adaptation or model capacity
  - Language switching within single outputs indicates unstable language conditioning

- First 3 experiments:
  1. Compare full finetuning vs. adapters on mT5-base for summarization with various LRs
  2. Test mBART with language codes vs. mT5 without for QA task
  3. Evaluate NLLB performance on high-resource Latin-script languages for summarization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of zero-shot cross-lingual generation vary across different language families and scripts beyond the tested Latin and non-Latin scripts?
- Basis in paper: [explicit] The authors tested languages covering Latin and non-Latin scripts but did not extensively explore different language families.
- Why unresolved: The study focused on a representative subset of languages but did not systematically investigate performance variations across language families.
- What evidence would resolve it: Conducting experiments with a broader range of languages from diverse families and scripts, comparing performance metrics across these groups.

### Open Question 2
- Question: What is the impact of different pretraining objectives on the effectiveness of zero-shot cross-lingual generation?
- Basis in paper: [inferred] The paper mentions different pretraining procedures (e.g., masked language modeling for mT5, denoising for mBART) but does not directly compare their impact on zero-shot generation performance.
- Why unresolved: While architectural differences are noted, a direct comparison of pretraining objectives' effects on zero-shot generation is not provided.
- What evidence would resolve it: Designing experiments that isolate the effect of pretraining objectives by training models with the same architecture but different objectives, then evaluating zero-shot generation performance.

### Open Question 3
- Question: How do different parameter-efficient fine-tuning methods (beyond adapters) perform in zero-shot cross-lingual generation?
- Basis in paper: [explicit] The authors consider full finetuning and parameter-efficient finetuning with adapters but do not explore other methods like prompt tuning.
- Why unresolved: The study focuses on adapters and full finetuning, leaving the performance of other parameter-efficient methods unexplored.
- What evidence would resolve it: Implementing and comparing various parameter-efficient fine-tuning methods, such as prompt tuning, LoRA, or prefix tuning, in the context of zero-shot cross-lingual generation.

## Limitations
- Experimental scope limited to two specific tasks (summarization and QA) with constrained output formats
- Analysis relies on manual grid search rather than systematic hyperparameter space exploration
- Comparison of adapter dimensions lacks comprehensive ablation to understand capacity impact
- Does not investigate the role of multilingual pretraining data quality or domain alignment

## Confidence

**High Confidence Claims:**
- Adapter-based finetuning performs comparably to full finetuning across mT5, mBART, and NLLB models
- Reducing learning rate effectively mitigates wrong language generation without performance degradation
- NLLB-200 shows competitive summarization performance for high-resource Latin-alphabet languages
- mBART with language codes demonstrates robust cross-lingual generation capabilities

**Medium Confidence Claims:**
- mBART-pretrained performs better than mBART-translation-finetuned in zero-shot settings
- Adapter-based approaches enable higher LR magnitudes without wrong language generation
- NLLB-200 shows weaker QA performance compared to mT5 and mBART

**Low Confidence Claims:**
- Generalizability of findings to non-Latin script languages beyond those tested
- Long-term stability of language code mechanisms for generation guidance
- Optimal adapter dimension across all tasks and language pairs

## Next Checks

1. **Extended Language Coverage Test**: Validate the reported learning rate findings across a broader set of 15-20 languages including diverse scripts (e.g., Japanese, Hindi, Swahili) to assess generalizability beyond the current 10-language subset.

2. **Adapter Capacity Scaling Study**: Systematically vary adapter dimensions from 32 to 4096 across all three model architectures (mT5, mBART, NLLB) for both summarization and QA tasks to quantify the trade-off between parameter efficiency and cross-lingual performance.

3. **Dynamic LR Scheduling Validation**: Replace the fixed LR grid search with adaptive learning rate scheduling strategies (e.g., ReduceLROnPlateau, cosine annealing) during training to compare whether dynamic approaches can automatically maintain language correctness while optimizing task performance.