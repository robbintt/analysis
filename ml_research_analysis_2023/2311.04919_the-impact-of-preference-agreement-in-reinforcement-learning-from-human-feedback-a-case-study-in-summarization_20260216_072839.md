---
ver: rpa2
title: 'The Impact of Preference Agreement in Reinforcement Learning from Human Feedback:
  A Case Study in Summarization'
arxiv_id: '2311.04919'
source_url: https://arxiv.org/abs/2311.04919
tags:
- agreement
- reward
- quality
- human
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how preference agreement rates affect the performance
  of reward models in RLHF for summarization. It shows that training on a balanced
  mix of agreement levels yields higher accuracy and better downstream ROUGE scores
  than using only high or low agreement samples.
---

# The Impact of Preference Agreement in Reinforcement Learning from Human Feedback: A Case Study in Summarization

## Quick Facts
- arXiv ID: 2311.04919
- Source URL: https://arxiv.org/abs/2311.04919
- Reference count: 7
- Primary result: Balanced agreement levels in training data yield higher reward model accuracy and better downstream summarization quality than homogeneous agreement levels

## Executive Summary
This paper investigates how the agreement levels in human preference data affect the performance of reward models in Reinforcement Learning from Human Feedback (RLHF) for text summarization. The authors train reward models on datasets with varying agreement levels (high, low, balanced, random) and find that models trained on a balanced mix of agreement levels achieve the highest accuracy and produce summaries with better ROUGE scores. The study challenges the assumption that maximizing agreement in preference data is always optimal, suggesting instead that diverse agreement levels provide a curriculum-like learning approach that captures both clear and nuanced quality differences.

## Method Summary
The authors analyze the impact of agreement levels in human preference data on reward model training and downstream summarization quality. They create four datasets from the Stiennon et al. (2020) preference dataset, each containing 2,000 comparisons sampled based on different agreement levels (MAX for high agreement, MIN for low agreement, DIST for balanced distribution, RAND for random). Four T5-XXL reward models (13B parameters) are trained on these datasets and evaluated for accuracy on a held-out test set. The best-performing models are then integrated into an RLHF framework to assess downstream summarization quality using ROUGE metrics, and their outputs are correlated with SummEval quality measures.

## Key Results
- Training on balanced agreement levels yields higher reward model accuracy than using only high or low agreement samples
- Low agreement comparisons lead to worse accuracy and poorer downstream ROUGE scores
- High agreement alone does not maximize performance, indicating the importance of learning from diverse agreement levels
- Models trained on different agreement levels capture distinct aspects of summary quality, as evidenced by varying correlations with SummEval measures

## Why This Works (Mechanism)

### Mechanism 1
Training on a balanced mix of agreement levels yields higher accuracy and better downstream ROUGE scores than using only high or low agreement samples. Diverse agreement levels provide a curriculum-like learning approach, allowing the model to learn from both easy and challenging examples. High agreement comparisons represent clear quality differentials, while low agreement comparisons require nuanced understanding of subtle quality differences.

### Mechanism 2
Low agreement comparisons led to worse accuracy and poorer downstream results, while high agreement alone did not maximize performance. Low agreement comparisons are more challenging to learn from due to the ambiguity in preferences, leading to less accurate reward models. High agreement comparisons, while easier to learn from, may only capture superficial quality differences, limiting the reward model's ability to capture nuanced aspects of quality.

### Mechanism 3
The quality differential between choices plays a crucial role in agreement and models trained with examples that elicit different levels of agreement capture distinct aspects of quality. High agreement comparisons occur when there is a high quality differential between the two instances, allowing the model to learn clear distinctions in quality. Low agreement comparisons, on the other hand, require the model to learn more nuanced aspects of quality, as the differences between the summaries are less obvious.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the overarching framework used in this paper to train reward models based on human preferences. Understanding RLHF is crucial for grasping the context and motivation behind the study of agreement levels.
  - Quick check question: What is the main goal of RLHF, and how does it differ from traditional supervised learning approaches?

- Concept: Reward Models in RLHF
  - Why needed here: Reward models are the key component trained using human preferences in RLHF. The paper focuses on how the agreement levels in the training data impact the performance of these reward models.
  - Quick check question: How are reward models trained in RLHF, and what is their role in the downstream generation process?

- Concept: Agreement Levels and Quality Differential
  - Why needed here: The paper proposes that agreement levels in human preferences are primarily driven by the quality differential between the compared summaries. Understanding this relationship is crucial for interpreting the results and implications of the study.
  - Quick check question: How does the quality differential between two summaries influence the agreement level in human preferences, and why is this relationship important for reward model training?

## Architecture Onboarding

- Component map:
  - Data collection: Human preferences for summary comparisons
  - Data preprocessing: Filtering and sampling based on agreement levels
  - Reward model training: Training T5-XXL models on different agreement datasets
  - Evaluation: Assessing reward model accuracy and downstream ROUGE scores
  - Analysis: Correlating reward model output with SummEval quality measures

- Critical path:
  1. Collect human preferences for summary comparisons
  2. Filter and sample comparisons based on agreement levels
  3. Train reward models on different agreement datasets
  4. Evaluate reward model accuracy on held-out test set
  5. Assess downstream ROUGE scores using reward models in RLHF framework
  6. Correlate reward model output with SummEval quality measures

- Design tradeoffs:
  - Agreement diversity vs. dataset size: Balancing the need for diverse agreement levels with the limited availability of annotated data
  - Reward model complexity vs. training efficiency: Using a large T5-XXL model for potentially better performance but increased computational cost
  - Downstream evaluation metrics: Relying on ROUGE scores as a proxy for overall summary quality, which may not capture all aspects of human judgment

- Failure signatures:
  - Poor reward model accuracy on held-out test set: Indicates issues with the training data or reward model architecture
  - Low correlation between reward model output and SummEval quality measures: Suggests that the reward model is not capturing the intended aspects of summary quality
  - No improvement or degradation in downstream ROUGE scores: Implies that the reward model is not effectively guiding the generation process

- First 3 experiments:
  1. Train and evaluate reward models on individual agreement datasets (MAX, MIN, DIST, RAND) to assess the impact of agreement levels on model accuracy.
  2. Analyze the correlation between reward model output and SummEval quality measures for each agreement dataset to understand the distinct aspects of quality captured by the models.
  3. Integrate the best-performing reward models into the RLHF framework and evaluate their impact on downstream ROUGE scores to assess the practical benefits of agreement diversity in training data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of preference agreements within training data impact the accuracy of reward models?
- Basis in paper: [explicit] The paper states that training on a balanced mix of agreement levels yields higher accuracy and better downstream ROUGE scores than using only high or low agreement samples.
- Why unresolved: While the paper provides initial findings, further research is needed to understand the optimal balance of agreement levels for maximizing reward model accuracy.
- What evidence would resolve it: Conducting additional experiments with varying levels of agreement diversity in training data and measuring the resulting reward model accuracy would provide more insights into this question.

### Open Question 2
- Question: Do the characteristics of summary quality captured by a reward model vary according to the agreement of input preferences?
- Basis in paper: [explicit] The paper shows that training on a balanced mix of agreement levels captures different aspects of summary quality compared to using only high or low agreement samples.
- Why unresolved: Further investigation is needed to fully understand how the agreement of input preferences influences the specific quality characteristics captured by reward models.
- What evidence would resolve it: Conducting a comprehensive analysis of the features and dimensions of summary quality captured by reward models trained on different agreement levels would provide insights into this question.

### Open Question 3
- Question: What are the downstream consequences of training reward models with differing agreement rates?
- Basis in paper: [explicit] The paper demonstrates that training on a balanced mix of agreement levels leads to better downstream ROUGE scores compared to using only high or low agreement samples.
- Why unresolved: Further research is needed to explore the broader impact of agreement rates on downstream tasks beyond summarization, such as text generation or dialogue systems.
- What evidence would resolve it: Conducting experiments with reward models trained on different agreement rates and evaluating their performance on various downstream tasks would provide insights into this question.

## Limitations
- The study's findings are based on a relatively small dataset of 2,000 instances per agreement level, which may limit the generalizability of the results.
- The analysis assumes that agreement levels are primarily driven by quality differentials, but other factors such as annotator bias or task ambiguity could also play a role.
- The downstream evaluation relies on ROUGE scores, which may not fully capture all aspects of summary quality that human raters consider.

## Confidence
- **High confidence**: The core finding that training on a balanced mix of agreement levels yields higher accuracy and better downstream performance is well-supported by the empirical results and multiple lines of evidence.
- **Medium confidence**: The claim that low agreement comparisons are more challenging to learn from due to ambiguity in preferences is plausible based on the results, but the exact mechanisms and implications for data curation require further investigation.
- **Low confidence**: The suggestion that agreement diversity can inform synthetic data generation strategies is speculative and not directly tested in this study.

## Next Checks
1. Replicate with larger datasets: Train and evaluate reward models on larger subsets of the preference data (e.g., 5,000-10,000 instances per agreement level) to assess the stability and generalizability of the findings.
2. Analyze agreement factors: Conduct a detailed analysis of the factors contributing to agreement levels, such as annotator bias, task ambiguity, or quality differentials, to better understand the mechanisms behind the observed results.
3. Evaluate with human judgments: Compare the downstream summary quality produced by the different reward models using direct human evaluations, rather than relying solely on ROUGE scores, to assess the practical impact of agreement diversity on summary quality.