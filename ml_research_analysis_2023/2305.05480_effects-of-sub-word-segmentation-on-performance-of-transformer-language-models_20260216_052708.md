---
ver: rpa2
title: Effects of sub-word segmentation on performance of transformer language models
arxiv_id: '2305.05480'
source_url: https://arxiv.org/abs/2305.05480
tags: []
core_contribution: This paper investigates the impact of sub-word segmentation algorithms
  on the performance of transformer language models. The authors compare Byte Pair
  Encoding (BPE), Morfessor, and StateMorph across Finnish and Russian datasets, training
  both GPT and BERT models.
---

# Effects of sub-word segmentation on performance of transformer language models

## Quick Facts
- arXiv ID: 2305.05480
- Source URL: https://arxiv.org/abs/2305.05480
- Reference count: 6
- Primary result: Morphological segmentation methods achieve lower perplexity and faster convergence compared to BPE, enabling smaller models to perform comparably to larger BPE-based models.

## Executive Summary
This paper investigates how different sub-word segmentation algorithms impact transformer language model performance. The authors compare Byte Pair Encoding (BPE) against morphological segmentation methods (Morfessor and StateMorph) on Finnish and Russian datasets, training both GPT and BERT models. Results demonstrate that morphological segmentation consistently outperforms BPE in terms of perplexity and convergence speed, with smaller morphologically-segmented models achieving performance comparable to larger BPE-based models. This finding has important implications for model sustainability by reducing computational costs during training and inference.

## Method Summary
The study compares three segmentation algorithms (BPE, Morfessor, and StateMorph) on Finnish and Russian corpora, training GPT-2 and BERT models with each tokenizer. The Finnish corpus contains ~17M instances from Helsingin Sanomat and YLE, while the Russian corpus has ~66M instances from Taiga. Models are evaluated on perplexity and downstream tasks including named entity recognition, paraphrasing, linguistic acceptability, topic classification, and grammatical error correction. The experimental design systematically varies model sizes and vocabulary sizes to assess the trade-offs between segmentation method, model capacity, and performance.

## Key Results
- Morphological segmentation (Morfessor and StateMorph) achieves significantly lower perplexity than BPE across both languages and model architectures
- Models using morphological segmentation converge faster during training, requiring fewer epochs to reach optimal performance
- Smaller models with morphological segmentation perform comparably to larger BPE-based models on downstream tasks
- StateMorph generally outperforms Morfessor in convergence speed and validation scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Morphological segmentation preserves linguistic meaning better than statistical segmentation
- Mechanism: By segmenting words into their meaningful morphological components (morphemes), the model can learn more generalizable representations and handle out-of-vocabulary words more effectively
- Core assumption: Morphemes carry semantic meaning that is preserved across different word forms
- Evidence anchors: [abstract] "morpheme can not be split into smaller segments without changing its meaning or leaving a meaningless remainder"; [section] "BPE... ignores the linguistic property of words, that is, morpheme"
- Break condition: If morphemes do not carry consistent semantic meaning across contexts or if morphological segmentation creates overly fragmented representations

### Mechanism 2
- Claim: Morphological segmentation improves model efficiency through faster convergence
- Mechanism: By providing more meaningful and consistent token representations, the model requires fewer training steps to reach optimal performance
- Core assumption: More meaningful token representations lead to faster learning of language patterns
- Evidence anchors: [abstract] "training with morphological segmentation allows the LMs to: 1. achieve lower perplexity, 2. converge more efficiently in terms of training time"; [section] "StateMorph can help the model to converge more efficiently and achieve a better validation score"
- Break condition: If the overhead of morphological segmentation during preprocessing outweighs the benefits in convergence speed

### Mechanism 3
- Claim: Morphological segmentation enables smaller models to perform comparably to larger BPE-based models
- Mechanism: By providing richer linguistic information through segmentation, smaller models can achieve similar performance with fewer parameters
- Core assumption: Morphological information compensates for model size limitations
- Evidence anchors: [abstract] "LMs of smaller size using morphological segmentation can perform comparably to models of larger size trained with BPE"; [section] "StateMorph can help the model to converge more efficiently and achieve a better validation score"
- Break condition: If the linguistic information provided by morphological segmentation is not sufficient to compensate for the reduced model capacity

## Foundational Learning

- Concept: Subword tokenization and its trade-offs
  - Why needed here: Understanding the difference between word-based, character-based, and subword tokenization is crucial for evaluating the impact of different segmentation approaches
  - Quick check question: What are the main problems with word-based and character-based tokenization that subword methods aim to solve?

- Concept: Morphological analysis and its relevance to NLP
  - Why needed here: The paper compares statistical segmentation with morphological segmentation, so understanding what morphology is and why it matters for language modeling is essential
  - Quick check question: How does morphological segmentation differ from statistical segmentation methods like BPE?

- Concept: Language model evaluation metrics
  - Why needed here: The paper discusses perplexity, convergence speed, and downstream task performance, which are key metrics for evaluating language models
  - Quick check question: What does perplexity measure in language models, and why is it important?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> Segmentation algorithm training -> Corpus tokenization -> Language model training -> Evaluation framework

- Critical path:
  1. Prepare corpora and extract word lists
  2. Train segmentation algorithms (BPE, Morfessor, StateMorph)
  3. Tokenize corpora using each segmentation method
  4. Train language models on each tokenized corpus
  5. Evaluate models on perplexity and downstream tasks

- Design tradeoffs:
  - Lexicon size vs. segmentation quality
  - Computational cost of morphological vs. statistical segmentation
  - Model size vs. performance trade-off

- Failure signatures:
  - Poor downstream task performance despite low perplexity
  - Convergence issues with morphological segmentation
  - Inconsistent results across different morphological algorithms

- First 3 experiments:
  1. Compare perplexity of BPE vs. morphological segmentation on a small corpus
  2. Measure training time convergence differences between segmentation methods
  3. Evaluate downstream task performance with different model sizes using morphological vs. BPE segmentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do morphological segmentation algorithms perform on languages with different morphological complexity levels beyond Finnish and Russian?
- Basis in paper: [inferred] The study focused on morphologically rich languages (Finnish and Russian) but did not test performance across a broader typological range of languages with varying morphological complexity
- Why unresolved: The paper's experimental scope was limited to two languages, preventing conclusions about performance generalizability across different morphological typologies
- What evidence would resolve it: Comparative experiments training and evaluating models on languages spanning the morphological complexity spectrum (isolating, agglutinative, fusional, polysynthetic) using the same segmentation algorithms

### Open Question 2
- Question: What is the precise relationship between vocabulary size and downstream task performance for morphologically segmented models versus BPE?
- Basis in paper: [explicit] The paper notes that larger vocabulary sizes generally improve performance but computational complexity increases, and mentions this as a limitation for practical applications
- Why unresolved: The paper acknowledges this trade-off but doesn't provide systematic analysis of the relationship or optimal vocabulary size for different tasks
- What evidence would resolve it: Systematic experiments varying vocabulary sizes for morphological segmentation algorithms across multiple downstream tasks to identify optimal trade-offs between performance and computational efficiency

### Open Question 3
- Question: How do morphological segmentation algorithms affect the quality of language generation (text generation) in addition to classification tasks?
- Basis in paper: [inferred] The paper focused primarily on classification and evaluation tasks, with only brief mention of GPT models without detailed analysis of generation quality
- Why unresolved: The experimental evaluation emphasized downstream classification tasks while generation quality metrics were not thoroughly investigated
- What evidence would resolve it: Comparative evaluation of generated text quality using metrics like perplexity, human evaluation, and linguistic quality measures across different segmentation approaches

## Limitations

- The study focuses only on two morphologically rich languages (Finnish and Russian), limiting generalizability to other language families
- The computational efficiency claims lack comprehensive analysis of full pipeline costs including preprocessing time and memory usage
- The paper doesn't provide statistical significance testing for downstream task performance differences between segmentation methods

## Confidence

**High Confidence**: The observation that morphological segmentation leads to lower perplexity and faster convergence is well-supported by the experimental results across multiple model architectures and languages.

**Medium Confidence**: The claim that smaller models using morphological segmentation can match larger BPE-based models needs more rigorous statistical testing.

**Low Confidence**: The sustainability and computational efficiency arguments are weakly supported without comprehensive cost-benefit analysis.

## Next Checks

1. Conduct t-tests or ANOVA on downstream task performance across segmentation methods to determine if observed differences are statistically significant rather than just directional trends.

2. Replicate the core experiments with at least one additional morphologically rich language (e.g., Turkish or Arabic) and one morphologically simpler language (e.g., English) to test the generalizability of the findings.

3. Measure total training time including preprocessing for each segmentation method, calculate FLOPs per epoch, and compare memory usage to provide concrete evidence for the sustainability claims rather than relying on convergence speed alone.