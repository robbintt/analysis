---
ver: rpa2
title: Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering
arxiv_id: '2309.11247'
source_url: https://arxiv.org/abs/2309.11247
tags:
- combat
- learning
- policy
- aircraft
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces a hierarchical multi-agent reinforcement
  learning framework for air-to-air combat scenarios involving multiple heterogeneous
  agents. The approach splits decision-making into two abstraction levels: low-level
  policies controlling individual units (e.g., maneuvering, shooting) and a high-level
  commander issuing mission-oriented commands.'
---

# Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering

## Quick Facts
- **arXiv ID**: 2309.11247
- **Source URL**: https://arxiv.org/abs/2309.11247
- **Reference count**: 40
- **Primary result**: Hierarchical MARL framework with low-level fight/escape policies and high-level commander improves coordination in small air combat teams (2vs2, 3vs3+)

## Executive Summary
This paper presents a hierarchical multi-agent reinforcement learning framework for air-to-air combat scenarios involving multiple heterogeneous agents. The approach divides decision-making into two abstraction levels: low-level policies controlling individual unit maneuvers and shooting decisions, and a high-level commander issuing mission-oriented commands. The framework uses curriculum learning and self-play to train policies for different aircraft types in increasingly complex scenarios. Experiments demonstrate effective combat performance in small team sizes, with the hierarchical structure improving decision-making efficiency and coordination. However, scalability to larger team sizes remains challenging due to partial observability and stochastic dynamics.

## Method Summary
The framework implements a hierarchical reinforcement learning approach with two abstraction levels. Low-level policies (πf for fighting, πe for escaping) control individual aircraft maneuvers and are trained using curriculum learning across five complexity levels with league-based self-play. A high-level commander policy (πh) coordinates the overall mission by selecting which low-level policy to invoke for each aircraft. The architecture employs recurrent neural networks and attention mechanisms to enable decentralized execution with centralized training. Training uses PPO with parameter sharing between agents of the same type and a shared layer between actor and critic networks.

## Key Results
- The hierarchical structure improves decision-making efficiency and coordination in small team scenarios (2vs2, 3vs3+)
- Curriculum learning with increasing opponent complexity stabilizes training and prevents local optima
- Parameter sharing between agents of the same type enables implicit coordination without explicit communication
- Performance degrades in larger team sizes due to partial observability and stochastic dynamics limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical splitting into low-level and high-level policies reduces effective state-action space complexity.
- Mechanism: Low-level policies handle unit-specific decisions (fight/escape maneuvers) with partial observability, while the high-level commander aggregates observations across multiple units and selects which low-level policy to invoke for each. This division limits each policy's input space to only relevant features.
- Core assumption: The split between tactical control and mission coordination maps well to the problem structure of air combat.
- Evidence anchors:
  - [abstract] "decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets."
  - [section] "Hierarchical Reinforcement Learning...employs temporal abstraction by decomposing the overall task into a nested hierarchy of sub-tasks, enhancing efficiency in learning and decision-making."
- Break condition: If the low-level policies are too specialized, they may not generalize across unit combinations; if the high-level policy lacks sufficient global context, coordination will fail.

### Mechanism 2
- Claim: Shared neural network parameters across agents of the same type improve coordination without explicit communication.
- Mechanism: A shared parameter layer between actor and critic networks (and across agents of the same type) ensures that all agents of a given type learn similar decision-making patterns, reducing policy divergence and implicit coordination.
- Core assumption: Agents of the same aircraft type can effectively coordinate by sharing policy parameters, even without direct communication.
- Evidence anchors:
  - [section] "Sharing parameters improves agents' coordination [52]."
  - [section] "Low-level AC1 and AC2 agents have distinct neural network instances (with different input and output dimensions) but share one layer (green box). This layer is further shared between the actor and critic inside the network."
- Break condition: If environmental conditions vary significantly between agents, forcing identical policy parameters may degrade individual performance.

### Mechanism 3
- Claim: Curriculum learning with increasing opponent complexity stabilizes training and prevents local optima.
- Mechanism: Training begins with static or scripted opponents and progressively moves to policies from earlier levels, allowing the agent to incrementally master simpler behaviors before facing more adaptive adversaries.
- Core assumption: Gradually increasing opponent difficulty improves policy robustness and avoids the instability of direct self-play against highly skilled opponents.
- Evidence anchors:
  - [abstract] "Their training is organized in a learning curriculum with increasingly complex training scenarios and league-based self-play."
  - [section] "The training of low-level policies is done in five levels following a curriculum learning scheme. The complexity is increased at each level by making the opponents more competitive."
- Break condition: If curriculum levels are too coarse, the agent may overfit to early stages and struggle to adapt to later, more dynamic opponents.

## Foundational Learning

- Concept: Multi-agent partial observability
  - Why needed here: Agents can only sense a subset of the environment (closest opponents/friendlies), so understanding POMDP frameworks is essential to designing effective observation spaces.
  - Quick check question: How does the observation space for πf differ from πe in terms of what agents can sense?
- Concept: Hierarchical decision-making
  - Why needed here: The framework splits tactical unit control from strategic coordination, requiring knowledge of how options and sub-policies interact in HRL.
  - Quick check question: What triggers the high-level commander to switch between fight and escape policies?
- Concept: Centralized training with decentralized execution
  - Why needed here: Training uses global information (all agents' states and actions) to update policies, but execution must be local and fast.
  - Quick check question: Why is parameter sharing between actor and critic beneficial in this setup?

## Architecture Onboarding

- Component map:
  - Low-level policies (πf, πe) → Aircraft-specific maneuvering decisions
  - High-level commander (πh) → Mission-level policy selection
  - Shared neural network layer → Coordination between same-type agents
  - PPO trainer with curriculum levels → Training loop structure
  - 2D simulation environment → Dynamic state updates and reward calculation
- Critical path:
  1. Train low-level policies in curriculum levels
  2. Fix low-level policies as options
  3. Train high-level commander using pre-trained low-level policies
  4. Evaluate coordination in 3vs3+ scenarios
- Design tradeoffs:
  - Pros: Reduced state-action space, implicit coordination via parameter sharing, robust curriculum training
  - Cons: Limited to predefined aircraft types, partial observability may hurt coordination in larger teams, no explicit communication
- Failure signatures:
  - Low-level policies: Poor performance in mixed-type teams, failure to generalize beyond training scenarios
  - High-level policy: Excessive draw rates, win/loss ratio near 1:1 in evaluation, poor adaptability to opponent behavior shifts
- First 3 experiments:
  1. Evaluate πf trained at L5 against scripted opponents in 2vs2 to verify curriculum effectiveness
  2. Test πe in 3vs3 with varying opponent fight/escape ratios to measure fleeing performance
  3. Deploy πh in 2vs2 to check if commander improves win rate over random policy selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the hierarchical MARL framework scale when the number of heterogeneous agents increases beyond the tested 5vs5 scenarios?
- Basis in paper: [explicit] The paper notes that as the number of aircraft increases, the portion of draws also rises, suggesting that the low-level policy has reached its peak learning capacity, and the commander policy struggles with larger team sizes due to partial observability and stochastic dynamics.
- Why unresolved: The paper only evaluates up to 5vs5 scenarios and does not explore the performance limits of the framework with significantly larger teams.
- What evidence would resolve it: Experiments testing the framework with team sizes larger than 5vs5, analyzing win/loss/draw ratios and decision-making efficiency, would provide insights into scalability limits.

### Open Question 2
- Question: How would the introduction of a dedicated communication mechanism between agents affect the coordination and performance of the hierarchical MARL framework?
- Basis in paper: [inferred] The paper mentions that coordination is achieved without an explicit communication channel, and future work plans to incorporate such a mechanism to improve tactical decisions.
- Why unresolved: The current framework relies on parameter sharing and centralized training for coordination, but the impact of explicit communication on performance is not explored.
- What evidence would resolve it: Implementing a communication protocol between agents and comparing performance metrics (e.g., win rates, decision-making speed) with and without communication would clarify its impact.

### Open Question 3
- Question: How does the hierarchical MARL framework perform in a 3D environment with more realistic aircraft dynamics compared to the 2D simulation used in this study?
- Basis in paper: [explicit] The paper uses a 2D simulation platform and mentions plans to switch to 3D models for more realistic environments in future work.
- Why unresolved: The current framework is validated only in 2D, and its effectiveness in handling the added complexity of 3D dynamics is unknown.
- What evidence would resolve it: Testing the framework in a 3D simulation with realistic aircraft dynamics and comparing performance metrics to the 2D results would demonstrate its adaptability to more complex environments.

## Limitations
- Scalability to larger team sizes remains unproven, with partial observability and stochastic dynamics identified as potential bottlenecks
- Reliance on pre-defined aircraft types limits adaptability to more diverse scenarios
- Performance degradation in larger teams suggests hierarchical structure benefits are constrained to small team scenarios

## Confidence

**Major Uncertainties:**
The framework's scalability to larger team sizes remains unproven, with partial observability and stochastic dynamics identified as potential bottlenecks. The reliance on pre-defined aircraft types limits adaptability to more diverse scenarios. The paper acknowledges that the hierarchical structure improves efficiency in small teams but doesn't demonstrate effectiveness at scale.

**Confidence Assessment:**
- High confidence in the hierarchical RL framework design and its theoretical benefits for decomposing complex multi-agent tasks
- Medium confidence in the curriculum learning approach effectiveness, as results show improvement but don't isolate curriculum impact from other factors
- Medium confidence in parameter sharing benefits, as the mechanism is theoretically sound but empirical validation is limited to specific aircraft types
- Low confidence in scalability claims, as the paper explicitly notes limitations with larger teams

## Next Checks
1. Test the trained hierarchical policy in 4vs4 and 5vs5 scenarios to quantify performance degradation from partial observability and assess scalability limits
2. Evaluate policy robustness by introducing unexpected aircraft types or modified dynamics during inference to test generalization beyond training assumptions
3. Compare against a non-hierarchical baseline with identical observation spaces to isolate the specific benefits of the hierarchical decomposition approach