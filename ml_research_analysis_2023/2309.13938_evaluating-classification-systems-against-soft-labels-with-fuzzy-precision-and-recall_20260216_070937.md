---
ver: rpa2
title: Evaluating Classification Systems Against Soft Labels with Fuzzy Precision
  and Recall
arxiv_id: '2309.13938'
source_url: https://arxiv.org/abs/2309.13938
tags:
- soft
- labels
- data
- hard
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating classification systems
  when reference labels are not binary but represent uncertainty. Traditional precision
  and recall metrics require binary labels, which can lead to loss of information
  and misleading interpretations when used with soft labels.
---

# Evaluating Classification Systems Against Soft Labels with Fuzzy Precision and Recall

## Quick Facts
- arXiv ID: 2309.13938
- Source URL: https://arxiv.org/abs/2309.13938
- Reference count: 0
- This paper introduces fuzzy set-based precision, recall, and F-score metrics for evaluating classification systems with soft labels, avoiding binary thresholding to preserve uncertainty information.

## Executive Summary
This paper addresses the challenge of evaluating classification systems when reference labels represent uncertainty rather than binary truth. Traditional evaluation metrics require binarization of soft labels, leading to information loss and potentially misleading results. The authors propose novel fuzzy set-based definitions of precision, recall, and F-score that directly operate on soft labels without binarization. The method uses fuzzy set cardinality with minimum T-norm to measure overlap between predicted and reference values, coinciding with standard metrics when applied to binary labels. Experiments on sound event detection models show that soft F-score correlates well with KL-divergence while providing more stable evaluation with narrower confidence intervals.

## Method Summary
The method extends classical precision, recall, and F-score metrics to handle soft labels using fuzzy set theory. Membership functions are defined as µL(xi) = ŷi for predictions and µG(xi) = yi for reference labels, where ŷi and yi are continuous values in [0,1]. Fuzzy cardinality is computed using the minimum T-norm: |A ∩ B| = ∑ min(µL(xi), µG(xi)). Precision becomes |A ∩ B|/|A| and recall becomes |A ∩ B|/|B|. These definitions reduce to standard binary metrics when labels are 0 or 1. The approach enables differentiable evaluation scores that can serve as loss functions during training, potentially aligning training objectives with evaluation metrics.

## Key Results
- Soft F-score correlates well with KL-divergence while showing narrower confidence intervals compared to optimal threshold F-score
- Soft metrics better capture system performance differences when labels are close to threshold values
- Optimal threshold F-score can mask true performance differences by adapting thresholds to predictions
- The proposed metrics provide more stable evaluation across random seeds compared to threshold-dependent metrics

## Why This Works (Mechanism)

### Mechanism 1
Fuzzy set cardinality using minimum T-norm preserves information that binary thresholding loses. Instead of mapping soft labels to {0,1} via threshold, the method treats each label as a fuzzy membership grade and sums the minimum of prediction-reference pairs across all samples. This yields a "soft overlap" that quantifies how well the system tracks uncertainty. Core assumption: Soft labels represent degrees of presence rather than binary truth, so their raw values should contribute to evaluation. Evidence anchors: [abstract] "The proposed metrics extend the well established metrics as the definitions coincide when used with binary labels." [section] Equations (2)-(3) define membership functions µL(xi) = ŷi, µG(xi) = yi and use ∑ min(ŷi, yi) for cardinality. Break condition: If labels are extremely noisy or adversarial, the soft metric could inflate scores for predictions that are consistently wrong but match the noise pattern.

### Mechanism 2
Soft metrics are differentiable, enabling gradient-based optimization directly on evaluation scores. Because precision, recall, and F-score are expressed as differentiable sums and minima (which are subdifferentiable), they can serve as loss functions during training, aligning training objectives with evaluation. Core assumption: Model architectures and optimizers can handle the subdifferentiability of min operations. Evidence anchors: [section] "Based on brief experiments with the baseline model, the soft F-score based loss did not bring any significant advantage compared to the MSE loss used in the baseline." [abstract] "Furthermore, the soft precision and recall based loss for model training could be studied in more detail." Break condition: If the optimization landscape becomes too flat or noisy due to the min operations, training may stall or diverge.

### Mechanism 3
Optimal threshold binarization can mask true system performance differences by adapting thresholds to predictions. OT thresholds are chosen to maximize macro F-score per class, so two systems with different underlying uncertainties can yield identical OT scores if they can be thresholded to match. Soft metrics bypass this by evaluating raw scores. Core assumption: The true uncertainty structure is preserved in the raw predictions and should be measured directly. Evidence anchors: [section] "Interestingly, the OT F-score is very similar irrespective of the beta sampling method... the optimal threshold follows the distribution of the predictions." [section] "In contrast to the hard F-scores, the soft F-score decreases as the KL-divergence between the predictions and reference labels increases." Break condition: If class-wise thresholds are truly needed due to domain-specific calibration, OT may remain preferable.

## Foundational Learning

- **Fuzzy set theory (membership functions, T-norms, cardinality)**: Provides the mathematical foundation for treating soft labels as fuzzy sets and defining overlap measures. Quick check: What is the result of applying the minimum T-norm to membership values 0.7 and 0.4? Answer: 0.4.

- **Kullback-Leibler divergence as a distributional similarity measure**: Serves as a continuous, information-theoretic baseline to compare against discrete F-score metrics. Quick check: If two distributions are identical, what is their KL-divergence? Answer: 0.

- **Jackknife resampling for confidence intervals**: Allows estimation of metric stability across multiple random seeds or runs without strong parametric assumptions. Quick check: In jackknife, how many leave-one-out estimates are produced from n samples? Answer: n.

## Architecture Onboarding

- **Component map**: Data pipeline: soft labels (0-1 floats) → model predictions (0-1 floats) → Evaluation layer: compute soft precision/recall/F using fuzzy cardinality → Comparison layer: compute hard metrics (thresholded at 0.5 and optimal threshold) + KL-divergence → Reporting layer: jackknife estimates + confidence intervals

- **Critical path**: Model training → soft metric computation → system ranking → hyperparameter tuning

- **Design tradeoffs**: Soft metrics vs hard: richer information vs interpretability; Minimum T-norm vs other T-norms: idempotency vs sensitivity; Differentiable vs non-differentiable: training alignment vs computational simplicity

- **Failure signatures**: Soft metrics too high → labels may be overly diffuse or predictions over-smoothed; Wide CI for OT F-score → threshold instability across runs; Soft F decreasing with KL increasing → metrics behaving as expected

- **First 3 experiments**: 1) Train baseline model with MSE loss, evaluate with both hard and soft metrics, compare ranking stability; 2) Replace MSE with soft F-score loss, measure convergence and final ranking changes; 3) Generate synthetic soft labels with controlled uncertainty, verify soft metrics track KL-divergence monotonically

## Open Questions the Paper Calls Out

### Open Question 1
How do soft precision and recall metrics behave when the reference labels are highly skewed (e.g., 90% positive vs. 10% positive)? Basis in paper: [inferred] The paper mentions that soft metrics can handle uncertainty in reference labels, but doesn't explicitly discuss performance with highly skewed label distributions. Why unresolved: The paper focuses on cases where the reference labels are relatively balanced, and doesn't explore scenarios with highly skewed distributions. What evidence would resolve it: Experiments comparing soft and hard metrics across different label skew scenarios, showing how the metrics perform and how they correlate with each other.

### Open Question 2
Can the soft precision and recall metrics be extended to handle multi-class classification problems with more than two classes? Basis in paper: [explicit] The paper discusses the application of soft metrics to binary classification, but doesn't explicitly address multi-class scenarios. Why unresolved: The paper focuses on binary classification, and extending the metrics to multi-class problems would require additional theoretical development and experimental validation. What evidence would resolve it: Theoretical extension of the soft precision and recall definitions to multi-class scenarios, along with experiments demonstrating their effectiveness in real-world multi-class classification tasks.

### Open Question 3
How do the soft precision and recall metrics compare to other existing metrics for evaluating classification systems with soft labels, such as the soft Jaccard index or the soft Fowlkes-Mallows index? Basis in paper: [inferred] The paper introduces soft precision and recall as an extension of classical metrics, but doesn't compare them to other existing metrics specifically designed for soft labels. Why unresolved: The paper focuses on the development and evaluation of the proposed metrics, without providing a comprehensive comparison to other existing metrics. What evidence would resolve it: Experiments comparing the performance of soft precision and recall to other existing metrics on a variety of classification tasks with soft labels, showing how they correlate and which metrics are more suitable for different scenarios.

## Limitations
- Evaluation is based on a single sound event detection dataset with 17 classes, limiting generalizability to other domains
- Experiments use CRNN and variants designed for audio, benefits may not transfer equally to other architectures
- The paper does not extensively explore edge cases where soft labels are extremely close to 0 or 1
- Soft F-score loss was briefly tested but not optimized thoroughly, leaving open questions about training stability

## Confidence
- **High confidence**: The mathematical formulation of fuzzy precision/recall using minimum T-norm is sound and correctly extends classical metrics
- **Medium confidence**: The claim that soft metrics provide more stable evaluation across runs is supported by jackknife estimates, but sample size limits generalizability
- **Low confidence**: The assertion that soft metrics will consistently improve training outcomes when used as loss functions lacks empirical validation beyond preliminary experiments

## Next Checks
1. **Cross-domain validation**: Apply the soft metrics to a medical imaging dataset with probabilistic labels (e.g., lesion detection with radiologist uncertainty scores) and compare ranking stability against binary thresholding
2. **Architecture ablation**: Test soft metrics on a CNN-only architecture and a transformer-based model to verify that benefits are not specific to recurrent architectures used in sound event detection
3. **Extreme label perturbation analysis**: Generate synthetic soft labels with controlled noise levels (uniform noise added to binary labels) and measure how soft vs hard metrics degrade, identifying the noise threshold where soft metrics lose advantage