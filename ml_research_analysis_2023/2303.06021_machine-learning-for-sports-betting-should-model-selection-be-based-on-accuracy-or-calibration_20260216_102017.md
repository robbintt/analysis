---
ver: rpa2
title: 'Machine learning for sports betting: should model selection be based on accuracy
  or calibration?'
arxiv_id: '2303.06021'
source_url: https://arxiv.org/abs/2303.06021
tags:
- betting
- each
- strategy
- accuracy
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors aim to identify whether a sports betting forecasting
  model should be optimized for accuracy or calibration, as most sports betting literature
  focuses solely on accuracy. They hypothesize that for sports betting, calibration
  is a more important metric than accuracy, as the goal is to estimate true probabilities
  of outcomes to identify mispriced odds rather than simply predict the winner.
---

# Machine learning for sports betting: should model selection be based on accuracy or calibration?

## Quick Facts
- arXiv ID: 2303.06021
- Source URL: https://arxiv.org/abs/2303.06021
- Reference count: 6
- Primary result: Betting systems using calibration-optimized SVM models achieved 110.42% average ROI vs 2.98% for accuracy-optimized models

## Executive Summary
This paper investigates whether sports betting forecasting models should be optimized for accuracy or calibration. The authors hypothesize that calibration is more important than accuracy for sports betting because the goal is to estimate true probabilities to identify mispriced odds, not simply predict winners. They test this by constructing NBA game outcome prediction models using logistic regression, random forests, support vector machines, and multi-layer perceptrons, optimizing each for either accuracy or calibration (measured by classwise-ECE). The models are then used in Kelly criterion-based betting simulations to compare profitability.

Results show that betting systems using calibration-optimized support vector machine models achieved significantly higher average ROI (110.42% vs 2.98%) and maximum ROI (902.01% vs 222.84%) compared to systems using accuracy-optimized logistic regression models. Statistical tests confirmed that the calibration-optimized model led to greater profit generation, especially when betting only on highly confident predictions. These findings suggest that for sports betting applications, calibration is a more important metric than accuracy for forecasting model evaluation.

## Method Summary
The authors construct NBA game outcome prediction models using four machine learning algorithms (logistic regression, random forests, SVM, and MLP). They apply feature selection using Spearman correlation, Sequential Forward Selection, and L1 penalty, followed by hyperparameter optimization using Bayesian optimization with Tree-structured Parzen Estimator (TPE). Models are optimized for either accuracy or calibration (measured by classwise-ECE). Betting simulations are implemented using the Kelly criterion with different strategies (wager on all value bets, avoid high-probability differences, wager only on strong favorites) and betting rules (full-kelly, half-kelly, quarter-kelly, eighth-kelly). ROI is calculated for each strategy and rule combination, and statistical tests compare the performance of accuracy-optimized versus calibration-optimized models.

## Key Results
- Calibration-optimized SVM models achieved average ROI of 110.42% compared to 2.98% for accuracy-optimized logistic regression models
- Maximum ROI reached 902.01% for calibration-optimized models versus 222.84% for accuracy-optimized models
- Statistical tests confirmed that calibration-optimized models led to significantly greater profit generation, particularly when betting on highly confident predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Calibration is more important than accuracy for sports betting forecasting models because the goal is to estimate true probabilities rather than just predict winners.
- Mechanism: In sports betting, the model's predicted probability must closely match the true probability to identify value bets (where true probability exceeds implied probability). Accuracy only measures correct predictions, not how close predicted probabilities are to true probabilities.
- Core assumption: The primary goal is to identify value bets by comparing predicted probabilities to bookmaker odds, not simply to predict winners.
- Evidence anchors: [abstract] "We hypothesise that for the sports betting problem, model calibration is more important than accuracy."

### Mechanism 2
- Claim: Kelly criterion-based betting systems are more profitable when using calibration-optimized models because bet sizing depends on predicted probabilities.
- Mechanism: The Kelly criterion determines optimal bet size as a function of predicted probability and odds. If predicted probabilities are well-calibrated (close to true probabilities), the Kelly criterion will size bets appropriately, maximizing long-term growth. Poorly calibrated probabilities lead to over/under-betting.
- Core assumption: The Kelly criterion is the optimal betting strategy for maximizing long-term bankroll growth.
- Evidence anchors: [abstract] "We show that using calibration, rather than accuracy, as the basis for model selection leads to greater returns."

### Mechanism 3
- Claim: Classwise Expected Calibration Error (classwise-ECE) provides a better evaluation metric than accuracy for sports betting models.
- Mechanism: Classwise-ECE measures the average deviation between predicted probabilities and actual outcomes across probability bins, capturing how well the model's confidence matches reality. This directly relates to the betting problem where we need reliable probability estimates to compare against bookmaker odds.
- Core assumption: The classwise-ECE metric accurately captures the calibration quality needed for betting applications.
- Evidence anchors: [abstract] "We show that using calibration, rather than accuracy, as the basis for model selection leads to greater returns."

## Foundational Learning

- Concept: Probabilistic classification and probability calibration
  - Why needed here: The entire paper revolves around using predicted probabilities (not just class labels) for betting decisions, requiring understanding of how well models generate calibrated probabilities
  - Quick check question: What's the difference between a model that predicts "Team A will win" with 51% confidence versus 99% confidence?

- Concept: Kelly criterion and optimal bet sizing
  - Why needed here: The betting strategy used in the paper depends on predicted probabilities to determine bet sizes, making this fundamental to understanding why calibration matters
  - Quick check question: If a model predicts a 60% chance of winning and the odds are 2.0, what fraction of bankroll should be wagered according to the Kelly criterion?

- Concept: Expected calibration error and binning methods
  - Why needed here: The paper uses classwise-ECE as the primary metric for evaluating model calibration, requiring understanding of how this metric works
  - Quick check question: How does splitting the probability interval [0,1] into bins help measure whether predicted probabilities match true probabilities?

## Architecture Onboarding

- Component map: Data preprocessing (feature engineering using box score statistics and player-level features) -> Feature selection (filter, wrapper, and embedded methods) -> Hyperparameter optimization (Bayesian optimization with TPE) -> Model training and evaluation (logistic regression, random forest, SVM, MLP) -> Betting simulation (Kelly criterion-based betting with different strategies and rules) -> ROI calculation -> Statistical comparison

- Critical path: Feature engineering → Feature selection → Hyperparameter optimization → Model selection → Betting simulation → ROI calculation → Statistical comparison

- Design tradeoffs:
  - Using relative performance features (vs opponent) vs absolute performance features
  - Classwise-ECE vs traditional ECE (classwise handles multi-class better but is more complex)
  - Conservative betting strategies (avoiding high-confidence predictions) vs aggressive strategies (betting on all value bets)

- Failure signatures:
  - High kurtosis in bin weight distributions indicates predictions are too concentrated in few bins
  - Calibration-optimized models performing worse than accuracy-optimized models suggests the betting strategy doesn't depend on probability quality
  - Statistical tests failing to reject null hypothesis indicates no significant difference between calibration and accuracy approaches

- First 3 experiments:
  1. Implement feature engineering using season-to-date differences vs opponent performance and verify covariate shift detection works correctly
  2. Test feature selection pipeline with synthetic data where optimal features are known, verifying the SFS method selects correct features
  3. Implement Bayesian optimization for one model type and verify it improves performance over random search baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between calibration error and accuracy in sports betting models, and is there a point where increasing accuracy leads to diminishing returns in terms of calibration?
- Basis in paper: [explicit] The authors state that accuracy is not the most appropriate metric for sports betting, but they do not provide a theoretical framework for the relationship between calibration and accuracy.
- Why unresolved: The paper does not explore the theoretical underpinnings of why calibration is more important than accuracy in sports betting, or if there is a trade-off between the two metrics.
- What evidence would resolve it: A theoretical analysis showing the mathematical relationship between calibration error and accuracy in sports betting models, and empirical evidence demonstrating how changes in accuracy affect calibration and vice versa.

### Open Question 2
- Question: How do different betting strategies (e.g., Kelly criterion variations, reinforcement learning) compare in terms of profitability when combined with calibration-optimized models?
- Basis in paper: [explicit] The authors test several betting strategies (wager on all value bets, avoid bets with high predicted probability, wager only on strong favorites) but do not explore other strategies like reinforcement learning.
- Why unresolved: The paper focuses on a limited set of betting strategies and does not compare the performance of calibration-optimized models across a wider range of strategies.
- What evidence would resolve it: Empirical results showing the profitability of different betting strategies when combined with calibration-optimized models, and a comparison of these results to determine which strategies are most effective.

### Open Question 3
- Question: How does the performance of calibration-optimized models vary across different sports and betting markets?
- Basis in paper: [inferred] The authors focus on NBA basketball, but the principles of calibration vs. accuracy could apply to other sports and betting markets.
- Why unresolved: The paper does not test the hypothesis that calibration is more important than accuracy in other sports or betting markets, so it is unclear if the findings are generalizable.
- What evidence would resolve it: Empirical results showing the performance of calibration-optimized models in different sports and betting markets, and a comparison of these results to determine if the importance of calibration holds across different contexts.

## Limitations
- The study focuses on NBA moneyline betting which may not generalize to other sports or betting markets
- The betting simulation uses historical data which may not capture real-world market dynamics where odds adjust based on model predictions
- The paper doesn't establish causal mechanisms - it's unclear whether improved calibration directly causes better betting performance or if other factors drive the results

## Confidence
- High confidence: Empirical finding that calibration-optimized models achieved higher ROI in tested NBA season
- Medium confidence: Claim that calibration is inherently more important than accuracy for sports betting (doesn't control for other variables)
- Medium confidence: Generalizability of results to other sports, betting markets, or time periods
- Low confidence: Classwise-ECE is the optimal metric for sports betting model evaluation without further validation

## Next Checks
1. **Control experiment**: Re-run the entire analysis using identical feature selection and hyperparameter tuning processes for both accuracy and calibration-optimized models to isolate the effect of calibration from other factors

2. **Cross-validation**: Test the same methodology on multiple NBA seasons and different sports (e.g., NFL, EPL) to assess generalizability of the calibration vs accuracy finding

3. **Market efficiency test**: Compare the calibration-optimized model's performance against a baseline of betting randomly on value bets to determine if the improvement is statistically significant beyond simple value identification