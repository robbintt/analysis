---
ver: rpa2
title: 'Topology Only Pre-Training: Towards Generalised Multi-Domain Graph Models'
arxiv_id: '2311.03976'
source_url: https://arxiv.org/abs/2311.03976
tags:
- graph
- learning
- graphs
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Topology Only Pre-Training (ToP), a graph
  pre-training method that excludes node and edge features to enable transfer learning
  across multiple graph domains. Unlike existing domain-specific graph representation
  learning approaches, ToP uses adversarial contrastive learning to learn topological
  features from unlabelled graphs across diverse domains.
---

# Topology Only Pre-Training: Towards Generalised Multi-Domain Graph Models

## Quick Facts
- arXiv ID: 2311.03976
- Source URL: https://arxiv.org/abs/2311.03976
- Authors: 
- Reference count: 40
- Key outcome: Topology Only Pre-Training (ToP) enables transfer learning across multiple graph domains by excluding node and edge features, achieving superior performance on 75% of tasks compared to supervised baselines.

## Executive Summary
This paper introduces Topology Only Pre-Training (ToP), a novel graph pre-training method that learns transferable representations by focusing exclusively on graph topology without node or edge features. The approach uses adversarial contrastive learning with learnt augmentations to train a single model across multiple graph domains simultaneously. Remarkably, ToP demonstrates that pre-training on non-molecule graphs can outperform molecule-specific pre-training on 79% of molecular benchmarks, challenging conventional assumptions about domain-specific transfer learning.

## Method Summary
ToP employs adversarial graph contrastive learning (AD-GCL) with edge-dropping augmentations to pre-train graph neural networks on diverse graph domains. The method trains on 10 different graph types simultaneously, using dummy node labels during pre-training to ensure the model learns structural patterns rather than memorizing feature-label relationships. Three models are trained: one on all domains, one on molecular-only, and one on non-molecular-only. Each model is then fine-tuned on downstream tasks with and without node labels, comparing performance against supervised baselines.

## Key Results
- ToP outperforms supervised baselines on 75% of tasks (p ≤ 0.01)
- Including node labels during fine-tuning improves performance on 85.7% of tasks
- Non-molecule pre-training achieves better transfer than molecule pre-training on 79% of molecular benchmarks
- The All model consistently outperforms domain-specific models on most datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Excluding node and edge features during pre-training forces the model to learn generalizable topological patterns that transfer across domains.
- Mechanism: By training only on graph structure (nodes and edges) without domain-specific features, the model learns invariant structural representations rather than memorizing domain-specific patterns.
- Core assumption: Graph topological structures contain sufficient information for downstream tasks across multiple domains, and this information is preserved when features are excluded.
- Evidence anchors:
  - [abstract] "We present Topology Only Pre-Training (ToP), a graph pre-training method based on node and edge feature exclusion."
  - [section] "Node labels are excluded during training to ensure that FOTOM learns generalised graph structures."
  - [corpus] "Weak - no direct corpus evidence for feature exclusion mechanism."
- Break condition: If downstream tasks require domain-specific features that cannot be recovered from topology alone, or if certain domains have non-informative topology relative to their features.

### Mechanism 2
- Claim: Adversarial contrastive learning with learnt augmentations produces more robust representations than fixed augmentation strategies.
- Mechanism: The view-learner component learns domain-specific edge-dropping strategies that create challenging but informative augmentations, improving the encoder's ability to capture essential topological features.
- Core assumption: Learning augmentations specific to each domain simultaneously is more effective than using fixed-parameter augmentations across all domains.
- Evidence anchors:
  - [abstract] "We present Topology Only Pre-Training (ToP), a graph pre-training method based on node and edge feature exclusion. We show positive transfer on evaluation datasets from multiple domains... running directly contrary to assumptions made in contemporary works."
  - [section] "An intuitive and potentially more flexible idea we use here is that of learnt augmentations, specifically as proposed by Suresh et al. [31]."
  - [corpus] "Weak - corpus contains related work but no direct evidence for adversarial augmentation mechanism."
- Break condition: If the adversarial learning process becomes unstable or if the learnt augmentations no longer provide useful contrastive signals.

### Mechanism 3
- Claim: Training on multiple domains improves performance on individual domains compared to domain-specific pre-training.
- Mechanism: Exposure to diverse topological patterns during pre-training creates richer representations that capture universal graph structures, enabling better transfer to specific domains.
- Core assumption: Out-of-domain topologies can provide more useful pre-training than in-domain pre-training for certain tasks.
- Evidence anchors:
  - [abstract] "We further show that out-of-domain topologies can produce more useful pre-training than in-domain. Under ToP we show better transfer from non-molecule pre-training, compared to molecule pre-training, on 79% of molecular benchmarks."
  - [section] "The All model consistently out-performs the other models here on the majority of datasets... Most significantly it does so on datasets that were shown under linear transfer (Section 4.1.1) to require specific node-label representation."
  - [corpus] "Moderate - related papers discuss multi-domain pre-training but this work provides novel evidence for out-of-domain benefits."
- Break condition: If certain domains have such specialized topological patterns that multi-domain pre-training dilutes rather than enriches the learned representations.

## Foundational Learning

- Graph Neural Networks and Message Passing
  - Why needed here: Understanding how GNNs aggregate information across graph structures is crucial for comprehending the model architecture and why topology-only pre-training works.
  - Quick check question: How does a GNN's message passing mechanism differ from traditional convolutional networks when processing graph data?

- Contrastive Learning Principles
  - Why needed here: The adversarial contrastive learning framework is central to how the model learns representations without labels, making it essential for understanding the pre-training process.
  - Quick check question: What is the difference between instance-level and contrastive learning, and why is this distinction important for graph representation learning?

- Transfer Learning in Graph Domains
  - Why needed here: The work challenges conventional assumptions about transfer learning boundaries, requiring understanding of when and why transfer works across graph domains.
  - Quick check question: Why might traditional transfer learning approaches fail when moving between molecular and social network graphs?

## Architecture Onboarding

- Component map: Encoder (GIN) -> View-learner (edge-dropping network) -> Projection head -> Fine-tuning MLP
- Critical path:
  1. Multi-domain graph data preparation with dummy node labels
  2. Adversarial contrastive pre-training loop (encoder + view-learner)
  3. Validation on diverse downstream tasks
  4. Fine-tuning on specific target tasks
- Design tradeoffs:
  - Feature exclusion vs. feature inclusion: Excluding features enables broader transfer but may lose domain-specific information
  - Single model vs. domain-specific models: Single model reduces complexity but may sacrifice peak performance on individual domains
  - Augmentation strategy: Learnt augmentations vs. fixed parameters (more flexible but potentially unstable)
- Failure signatures:
  - Poor transfer performance on tasks requiring domain-specific features
  - Unstable training during adversarial contrastive learning phase
  - Dimensionality collapse in learned representations (as measured by singular value analysis)
  - High variance in downstream task performance across runs
- First 3 experiments:
  1. Linear evaluation on Facebook dataset (clustering prediction) to verify basic representation quality
  2. Fine-tuning on Twitch ego networks (binary classification) to test transfer learning capability
  3. Compare performance on molecular datasets with and without node labels to validate topology-only benefits

## Open Questions the Paper Calls Out

- Open Question 1: How does the choice of graph augmentation strategies beyond edge dropping affect the performance of topology-only pre-training?
- Open Question 2: What is the optimal composition of domain data for training a topology-only pre-trained model to maximize downstream task performance?
- Open Question 3: How do topology-only pre-trained models perform when extended to include node and edge features beyond labels?

## Limitations

- Evaluation is constrained to specific graph domains, leaving questions about performance on other graph types like temporal graphs or hypergraphs
- Use of dummy node labels may artificially inflate performance metrics
- Paper doesn't thoroughly address potential negative transfer effects
- Limited analysis of failure cases and edge conditions where the approach might break down

## Confidence

**High Confidence:** The core experimental results showing ToP outperforming supervised baselines on 75% of tasks (p ≤ 0.01) are well-supported by the presented data and statistical analysis.

**Medium Confidence:** The claim about adversarial contrastive learning being superior to fixed augmentation strategies is supported by experimental results but lacks direct ablation studies.

**Low Confidence:** The generalizability claims beyond tested domains remain largely speculative, with limited analysis of when and why transfer might fail.

## Next Checks

1. **Ablation Study on Feature Inclusion:** Systematically test ToP with varying levels of feature inclusion (0%, 50%, 100%) to quantify the exact contribution of topology-only pre-training versus feature inclusion.

2. **Cross-Domain Robustness Testing:** Evaluate ToP on graph types not included in pre-training (e.g., temporal graphs, hypergraphs, or knowledge graphs) to validate claims about truly general topology learning.

3. **Negative Transfer Analysis:** Conduct controlled experiments to identify specific conditions under which transfer learning degrades performance, including analysis of graph size distributions and domain similarity metrics.