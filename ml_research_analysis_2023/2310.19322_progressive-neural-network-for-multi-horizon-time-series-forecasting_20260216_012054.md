---
ver: rpa2
title: Progressive Neural Network for Multi-Horizon Time Series Forecasting
arxiv_id: '2310.19322'
source_url: https://arxiv.org/abs/2310.19322
tags:
- forecasting
- pronet
- time
- series
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProNet, a novel deep learning approach for
  multi-horizon time series forecasting. ProNet strategically blends autoregressive
  (AR) and non-autoregressive (NAR) strategies by segmenting the forecasting horizon
  and predicting a subset of steps non-autoregressively within each segment while
  maintaining autoregressive decoding for the remaining steps.
---

# Progressive Neural Network for Multi-Horizon Time Series Forecasting

## Quick Facts
- arXiv ID: 2310.19322
- Source URL: https://arxiv.org/abs/2310.19322
- Authors: 
- Reference count: 40
- Key outcome: ProNet achieves superior performance in accuracy and prediction speed compared to state-of-the-art AR and NAR forecasting models by strategically blending autoregressive and non-autoregressive strategies with latent-variable-based segmentation.

## Executive Summary
This paper introduces ProNet, a novel deep learning approach for multi-horizon time series forecasting that strategically blends autoregressive (AR) and non-autoregressive (NAR) strategies. ProNet segments the forecasting horizon and predicts a subset of steps non-autoregressively within each segment while maintaining autoregressive decoding for the remaining steps. The segmentation process relies on latent variables, optimized through variational inference, to capture the significance of individual time steps. Through comprehensive evaluations on four large datasets and an ablation study, ProNet demonstrates superior performance in terms of accuracy and prediction speed compared to state-of-the-art AR and NAR forecasting models.

## Method Summary
ProNet uses a partially autoregressive forecasting mechanism with segmentation based on latent variables optimized through variational inference. The architecture includes an encoder for pattern extraction, a posterior model to identify significant steps using ground truth, a prior model to predict step significance using only covariates, and a decoder with progressive masking for partially AR forecasting. Training involves variational inference to generate latent variables, and the model uses a progressive masking mechanism for the Transformer architecture. The approach is evaluated on four datasets (Sanyo, Hanergy, Solar, and Electricity) using ρ0.5 and ρ0.9-quantile losses.

## Key Results
- ProNet achieves superior accuracy compared to both pure AR and pure NAR models by modeling dependencies within the target sequence
- Requires fewer AR iterations than full AR models, resulting in faster prediction speed and mitigated error accumulation
- Outperforms state-of-the-art methods (DeepAR, DeepSSM, LogTrans, N-BEATS, Informer, SARIMAX, Persistence) on four large time series datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProNet reduces error accumulation compared to AR models by using fewer AR iterations.
- Mechanism: The model segments the forecasting horizon and predicts only a subset of steps non-autoregressively within each segment, while autoregressively decoding the rest. This reduces the number of AR steps from the full horizon length to the maximum segment length.
- Core assumption: Dependencies between steps can be captured with fewer AR iterations if strategically placed.
- Evidence anchors:
  - [abstract] "ProNet showcases remarkable advantages, requiring fewer AR iterations, resulting in faster prediction speed, and mitigating error accumulation."
  - [section] "ProNet employs AR predictions a maximum of nstep = max(Ti:ng) times, where nstep represents the maximum segment length."
  - [corpus] Weak evidence - no direct corpus matches for "error accumulation reduction" mechanism.

### Mechanism 2
- Claim: ProNet improves accuracy over NAR models by modeling dependencies within the target sequence.
- Mechanism: Uses latent variables optimized via variational inference to identify significant steps and segment the horizon. This allows autoregressive decoding for steps with strong dependencies while still benefiting from non-autoregressive parallelism.
- Core assumption: Not all steps are equally dependent; some can be predicted non-autoregressively without accuracy loss.
- Evidence anchors:
  - [abstract] "ProNet takes into account the interdependency of predictions in the output space, leading to improved forecasting accuracy."
  - [section] "ProNet adopts a partially AR prediction strategy by segmenting the forecasting horizon... The initiation of horizon segments is determined by latent variables."
  - [corpus] Weak evidence - no direct corpus matches for "dependency modeling improvement" mechanism.

### Mechanism 3
- Claim: ProNet achieves faster prediction speed than AR models while maintaining accuracy.
- Mechanism: Progressive masking allows parallel prediction of multiple steps within segments while maintaining necessary dependencies, reducing total inference steps compared to full AR decoding.
- Core assumption: Parallel prediction within segments can be done without violating temporal dependencies.
- Evidence anchors:
  - [abstract] "requiring fewer AR iterations, resulting in faster prediction speed"
  - [section] "The progressive mask M is created by Algorithm 1... facilitates access to the first t steps of all segments during the t-th step prediction."
  - [corpus] Weak evidence - no direct corpus matches for "speed improvement" mechanism.

## Foundational Learning

- Concept: Variational Inference and ELBO optimization
  - Why needed here: To learn latent variables that identify significant forecasting steps for optimal segmentation
  - Quick check question: What is the difference between the prior pθ(z|x) and posterior qϕ(z|y,x) distributions in this context?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Forms the backbone for both encoder and decoder components in ProNet
  - Quick check question: How does the progressive mask differ from standard causal masking in Transformer decoders?

- Concept: Multi-horizon forecasting and the AR vs NAR trade-off
  - Why needed here: The fundamental problem ProNet addresses - balancing speed and accuracy across different forecasting horizons
  - Quick check question: Why do AR models accumulate more error for longer forecasting horizons compared to NAR models?

## Architecture Onboarding

- Component map: Historical data -> Encoder -> Posterior model -> Latent z -> Decoder with progressive mask -> Forecasts
- Critical path: Historical data → Encoder → Posterior model → Latent z → Decoder with progressive mask → Forecasts
- Design tradeoffs:
  - More segments (higher ng) → faster inference but potentially less accurate due to more NAR predictions
  - Fewer segments → more accurate but slower due to more AR iterations
  - Latent variable quality directly impacts segmentation quality and overall performance
- Failure signatures:
  - High ρ0.5/ρ0.9 loss indicates poor segmentation or latent variable learning
  - Inference speed similar to AR models suggests progressive masking isn't working correctly
  - Error accumulation pattern similar to pure AR models indicates segmentation strategy failure
- First 3 experiments:
  1. Implement baseline Transformer with standard causal masking and compare ρ0.5/ρ0.9 loss against ProNet
  2. Test ProNet with varying numbers of segments (ng=2,5,10,15) to find optimal speed-accuracy tradeoff
  3. Compare ProNet's latent variable learned segmentation against random segmentation baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ProNet compare to other models when dealing with extremely long forecasting horizons beyond 200 steps?
- Basis in paper: [inferred] The paper mentions that AR models suffer from error accumulation for long horizons, and ProNet is designed to mitigate this issue.
- Why unresolved: The paper only tests forecasting horizons up to 200 steps (10 days) on the Sanyo and Hanergy datasets.
- What evidence would resolve it: Experiments comparing ProNet's performance to AR and NAR models on datasets with extremely long forecasting horizons, e.g., months or years.

### Open Question 2
- Question: How sensitive is ProNet's performance to the choice of the number of segments (ng) in the partially autoregressive decoding mechanism?
- Basis in paper: [explicit] The paper mentions that the optimal number of segments is around 5-10 for the tested datasets.
- Why unresolved: The paper only tests a limited range of segment numbers and does not explore the impact of different choices on ProNet's performance across various datasets and forecasting tasks.
- What evidence would resolve it: Extensive experiments testing ProNet with a wide range of segment numbers on diverse datasets and forecasting tasks to determine the optimal choice and its sensitivity.

### Open Question 3
- Question: How does ProNet's performance compare to other models when dealing with multivariate time series forecasting tasks?
- Basis in paper: [inferred] The paper mentions that ProNet can handle multivariate series forecasting by incorporating covariates, but it only tests univariate time series forecasting tasks.
- Why unresolved: The paper only evaluates ProNet on univariate time series datasets (Sanyo, Hanergy, Solar, and Electricity) and does not explore its performance on multivariate time series forecasting tasks.
- What evidence would resolve it: Experiments comparing ProNet's performance to other models on multivariate time series forecasting tasks with multiple correlated variables.

## Limitations
- The quality of segmentation through latent variables is not directly evaluated, making it unclear whether the variational inference truly identifies optimal steps
- The progressive masking mechanism lacks detailed implementation specifications that could impact reproducibility
- The paper does not provide ablation studies comparing learned segmentation against simpler heuristics

## Confidence
- High confidence: The overall framework combining AR and NAR strategies is well-established in the literature. The performance improvements over baselines are clearly demonstrated through quantitative metrics.
- Medium confidence: The specific mechanisms for error reduction and speed improvement are theoretically plausible but depend heavily on implementation details not fully specified in the paper.
- Low confidence: The claim that variational inference provides superior segmentation compared to alternative methods lacks direct comparative evidence.

## Next Checks
1. Implement ProNet with random segmentation (same number of segments but random boundaries) and compare performance against learned segmentation to quantify the benefit of variational inference.
2. For AR models and ProNet, measure and plot the cumulative prediction error at each forecasting step to empirically verify the claimed reduction in error accumulation.
3. Systematically vary the number of segments (ng parameter) and plot the tradeoff between prediction speed and accuracy to identify the optimal operating point and validate the claimed improvements.