---
ver: rpa2
title: The Opaque Law of Artificial Intelligence
arxiv_id: '2310.13192'
source_url: https://arxiv.org/abs/2310.13192
tags:
- which
- human
- responsibility
- intelligence
- artificial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the legal responsibility for AI-caused
  harm, focusing on the "black box" problem where the internal workings of AI algorithms
  are opaque. The author proposes using a modified Turing Test to evaluate the performance
  of GPT-4, a large language model, to understand its capabilities and limitations.
---

# The Opaque Law of Artificial Intelligence

## Quick Facts
- arXiv ID: 2310.13192
- Source URL: https://arxiv.org/abs/2310.13192
- Reference count: 32
- Primary result: Proposes using legal fictions and the EU AI Liability Directive to address responsibility gaps for AI-caused harm by retroactively attributing responsibility to human operators

## Executive Summary
This paper examines the legal challenges of attributing responsibility for harm caused by AI systems, focusing on the "black box" problem where algorithmic opacity makes causal chains difficult to prove. The author proposes a modified Turing Test to evaluate GPT-4's capabilities and argues that the halting problem from computability theory demonstrates AI's inability to act independently. The paper suggests the EU's AI Liability Directive, which introduces a presumption of causal link, as a solution to bypass proving each step in the causal chain by attributing responsibility back to human operators through a "fiction of continuity."

## Method Summary
The paper employs a modified Turing Test methodology to evaluate GPT-4's language and reasoning capabilities, using this technical analysis alongside Italian legal concepts of causality, intent, and fault. The author applies computability theory (halting problem) to argue AI's lack of independent agency, then proposes the EU AI Liability Directive's presumption of causal link as a legal solution. The approach involves conversational testing of GPT-4 with meme recognition and generation tasks, followed by analysis of legal implications through the lens of Italian criminal law.

## Key Results
- AI systems lack independent agency due to algorithmic execution and the halting problem
- The "black box" problem creates a responsibility gap in legal systems
- The EU AI Liability Directive's presumption of causal link can bypass the need to prove each causal step by attributing responsibility to human operators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper proposes using a "finzione di continuità" (fiction of continuity) to bypass the need to prove each step in the AI causal chain by retroactively attributing responsibility to the human operator.
- Mechanism: By legally treating the AI's output as if it were directly caused by the human action, the complex intermediate steps (the black box) are ignored. This works because AI cannot act independently; it merely executes programmed algorithms.
- Core assumption: AI lacks independent agency and all outputs are traceable back to human input, making retroactive attribution legally valid.
- Evidence anchors:
  - [abstract] "The author suggests the EU's AI Liability Directive, which introduces a presumption of causal link, as a potential solution to the responsibility gap. This approach would attribute responsibility for AI-caused harm back to the human operator, bypassing the need to prove each step of the causal chain."
  - [section] "The presumption of a causal link aims to close the gap by avoiding the necessary assessment of every causal step that has occurred between human action and the event."
  - [corpus] Weak evidence; no direct mention of "finzione di continuità" in related papers.
- Break condition: If AI systems develop true autonomous decision-making capability (AGI) or if human oversight cannot be proven, the fiction of continuity would fail.

### Mechanism 2
- Claim: The paper uses the halting problem from computability theory to argue that AI cannot act independently because its behavior is determined by algorithms that may loop indefinitely.
- Mechanism: Since algorithms either halt or loop, and humans can intervene, AI's outputs are ultimately controlled by human design and input. This limits AI's legal agency.
- Core assumption: All AI actions are reducible to algorithmic execution, and the halting problem proves that AI cannot make independent choices.
- Evidence anchors:
  - [abstract] "The author argues that the halting problem from computability theory demonstrates that AI is incapable of independent action, only executing programmed algorithms."
  - [section] "If AI can be defined as mere computation, for every case involving AI execution, the generation of action has no legal relevance, but, in every case, the responsibility must be brought back to the human operator."
  - [corpus] Weak evidence; no direct mention of halting problem in related papers.
- Break condition: If AI systems develop non-algorithmic decision processes or if quantum computing enables non-halting computations, the argument weakens.

### Mechanism 3
- Claim: The AI Liability Directive's presumption of causal link shifts the burden of proof from demonstrating each causal step to proving human fault or non-compliance with AI Act obligations.
- Mechanism: By presuming a causal link when certain conditions are met (fault proven, refusal to disclose evidence, or complexity making proof impossible), the directive reduces the legal burden on claimants and compensates for algorithmic opacity.
- Core assumption: The legal system can treat the presumption as equivalent to proven causation in cases where algorithmic opacity prevents full causal analysis.
- Evidence anchors:
  - [abstract] "The author suggests the EU's AI Liability Directive, which introduces a presumption of causal link, as a potential solution to the responsibility gap."
  - [section] "Article 4 defines three cases in which the presumption of causal link operates... These three cases show a strong relationship between fault and causation."
  - [corpus] Weak evidence; no direct mention of AI Liability Directive in related papers.
- Break condition: If courts reject legal fictions as insufficient for criminal liability or if defendants can easily rebut the presumption, the mechanism fails.

## Foundational Learning

- Concept: Turing Test and its limitations
  - Why needed here: The paper uses a modified Turing Test to evaluate GPT-4's capabilities, which informs the discussion of AI's legal status.
  - Quick check question: What is the fundamental difference between passing a Turing Test and having true understanding or agency?

- Concept: Black box problem in AI
  - Why needed here: Central to the paper's argument about responsibility gaps; understanding why AI decisions are opaque is crucial.
  - Quick check question: Why does the complexity of machine learning models create legal challenges for proving causation?

- Concept: Legal fictions in jurisprudence
  - Why needed here: The "finzione di continuità" is a legal fiction proposed to solve the responsibility gap; understanding how legal fictions work is essential.
  - Quick check question: How do legal fictions like "constructive notice" or "corporate personhood" function in law?

## Architecture Onboarding

- Component map: Legal analysis (Italian criminal law concepts, EU AI Liability Directive, AI Act) -> Technical analysis (machine learning, natural language processing, Turing Test, halting problem) -> Argumentative structure (problem identification → technical examination → legal solution proposal)

- Critical path: 1. Establish the responsibility gap using Italian law concepts 2. Demonstrate AI's limitations using technical analysis (halting problem, Turing Test) 3. Propose legal solutions (presumption of causal link, fiction of continuity) 4. Apply solutions across legal domains

- Design tradeoffs: Legal fiction vs. empirical proof: Using fiction simplifies cases but may reduce accuracy; Technical determinism vs. legal agency: Treating AI as purely computational may oversimplify complex systems; EU harmonization vs. national legal traditions: Directive approach may not fit all legal systems

- Failure signatures: Inability to prove human oversight or control over AI system; Development of truly autonomous AI systems that can make independent decisions; Courts rejecting legal fictions as insufficient for liability attribution

- First 3 experiments: 1. Apply the presumption of causal link to a simple case study (e.g., AI-powered recommendation causing financial loss) 2. Test the fiction of continuity by tracing responsibility in a multi-agent AI system 3. Examine how the halting problem argument holds up against emergent AI behaviors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the "finzione di continuità" legal fiction be effectively adapted to different legal systems beyond Italy, particularly those with distinct concepts of causality and responsibility?
- Basis in paper: [explicit] The paper discusses the potential application of the "finzione di continuità" concept in Italian law to address the responsibility gap in AI-related cases, suggesting its extension to other legal domains and systems.
- Why unresolved: The paper primarily focuses on the Italian legal context and does not provide a detailed analysis of how this concept can be translated and applied to other legal systems with different legal traditions and concepts of causality and responsibility.
- What evidence would resolve it: Comparative legal analysis examining the feasibility and effectiveness of adapting the "finzione di continuità" concept to various legal systems, considering their unique legal frameworks and approaches to causality and responsibility.

### Open Question 2
- Question: How can the presumption of causal link introduced by the AI Liability Directive be practically implemented in legal proceedings, considering the complexities of AI algorithms and the potential for disputes over the disclosure of evidence?
- Basis in paper: [explicit] The paper discusses the AI Liability Directive's measures to address the black box problem and the responsibility gap, including the disclosure of evidence and the presumption of causal link.
- Why unresolved: The paper acknowledges the practical challenges of implementing these measures, such as the complexity of AI algorithms and the potential for disputes over the disclosure of evidence, but does not provide specific solutions or guidelines for their practical implementation.
- What evidence would resolve it: Case studies and legal analyses of how the presumption of causal link has been applied in practice, including the challenges encountered and the effectiveness of different approaches to disclosure of evidence.

### Open Question 3
- Question: How can the legal framework for AI responsibility be further developed to address the potential emergence of artificial general intelligence (AGI) and its unique challenges for attributing responsibility?
- Basis in paper: [explicit] The paper discusses the current legal approaches to AI responsibility and their limitations, but does not specifically address the potential emergence of AGI and its implications for legal responsibility.
- Why unresolved: The paper acknowledges the potential for AGI to pose unique challenges for attributing responsibility due to its ability to perform a wide range of tasks and potentially exhibit more autonomous behavior, but does not provide a comprehensive analysis of how the legal framework should be adapted to address these challenges.
- What evidence would resolve it: Expert opinions and legal analyses on the potential emergence of AGI and its implications for legal responsibility, including proposals for adapting the legal framework to address the unique challenges posed by AGI.

## Limitations

- The paper's argument relies on the assumption that AI systems lack true agency, which may not hold for future AI architectures
- The proposed legal fictions are untested in actual court cases involving AI systems, making their practical effectiveness uncertain
- The halting problem argument may not fully capture emergent behaviors in complex neural networks

## Confidence

High confidence in: The description of the responsibility gap in AI cases and the technical analysis of algorithmic opacity.

Medium confidence in: The application of Italian legal concepts to AI cases and the theoretical validity of the halting problem argument for limiting AI agency.

Low confidence in: The practical effectiveness of proposed legal fictions in real-world litigation and the long-term validity of treating AI as purely algorithmic systems.

## Next Checks

1. **Case Law Analysis**: Examine actual court decisions where AI-caused harm was alleged to identify patterns in how judges currently handle the responsibility gap without the proposed legal fictions.

2. **Technical Boundary Testing**: Design experiments to test whether GPT-4 or similar models exhibit behaviors that cannot be reduced to simple algorithmic execution, particularly in long-form reasoning tasks.

3. **Legal Fiction Stress Testing**: Create hypothetical AI harm scenarios involving multiple interacting AI systems and test whether the fiction of continuity can consistently attribute responsibility to human operators.