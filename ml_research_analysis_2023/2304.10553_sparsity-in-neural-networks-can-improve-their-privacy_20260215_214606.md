---
ver: rpa2
title: Sparsity in neural networks can improve their privacy
arxiv_id: '2304.10553'
source_url: https://arxiv.org/abs/2304.10553
tags:
- sparsity
- network
- networks
- training
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether neural network sparsity can improve
  robustness to membership inference attacks (MIAs) while preserving accuracy. It
  compares unstructured sparsity (via Iterative Magnitude Pruning) and structured
  "butterfly" sparsity in ResNet-20 models trained on CIFAR-10.
---

# Sparsity in neural networks can improve their privacy

## Quick Facts
- arXiv ID: 2304.10553
- Source URL: https://arxiv.org/abs/2304.10553
- Reference count: 21
- Primary result: Neural network sparsity between 3.4% and 17.3% can improve privacy against membership inference attacks while preserving accuracy

## Executive Summary
This paper investigates whether neural network sparsity can improve robustness to membership inference attacks (MIAs) while preserving accuracy. The authors compare unstructured sparsity (via Iterative Magnitude Pruning) and structured "butterfly" sparsity in ResNet-20 models trained on CIFAR-10. Results show that for sparsity levels between 3.4% and 17.3% of non-zero weights, a relative loss in accuracy leads to a relative gain in MIA defense, with structured butterfly sparsity achieving similar trade-offs to unstructured pruning. This suggests sparsity is a promising defense mechanism against MIAs at low computational cost, though larger scale experiments are needed for confirmation.

## Method Summary
The authors evaluate MIA robustness by training ResNet-20 models on CIFAR-10 with varying sparsity levels using both IMP and butterfly factorization. They partition CIFAR-10 into training and test subsets for target and shadow networks, then train discriminators to distinguish training from test data based on model outputs. Privacy is measured by the discriminator's accuracy, with lower accuracy indicating better privacy. The study compares accuracy and defense metrics across different sparsity levels and structures.

## Key Results
- Sparsity levels between 3.4% and 17.3% of non-zero weights achieve optimal privacy-accuracy trade-offs
- A relative loss of p% in accuracy leads to a relative gain of 3.6×p% in defense against MIAs
- Structured butterfly sparsity achieves similar trade-offs to unstructured pruning while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparsity reduces the neural network's memorization capacity, which limits the ability of membership inference attacks to exploit model confidence around training data.
- Mechanism: By removing low-magnitude weights, the network's representational power is reduced, preventing overfitting and thereby limiting the storage of sensitive training data patterns that attackers exploit.
- Core assumption: Membership inference attacks rely on detecting subtle overfit patterns in model outputs, which sparsity disrupts by limiting the network's capacity to memorize.
- Evidence anchors:
  - [abstract] "sparsity improves the privacy of the network, while preserving comparable performances on the task at hand"
  - [section] "sparsity can limit the model's ability to store confidential information about the data it has been trained on"
  - [corpus] Weak evidence; related papers do not directly support this specific mechanism
- Break condition: If sparsity is too high and causes significant accuracy loss, the model becomes ineffective, and if too low, memorization may still occur.

### Mechanism 2
- Claim: Structured butterfly sparsity provides a similar privacy-accuracy trade-off as unstructured pruning while maintaining computational efficiency.
- Mechanism: Butterfly factorization enforces a fixed sparse structure that reduces parameters while allowing efficient matrix-vector multiplication, thereby limiting memorization without degrading performance.
- Core assumption: The butterfly structure constrains the weight matrices in a way that reduces model capacity to overfit while maintaining the ability to learn generalizable patterns.
- Evidence anchors:
  - [abstract] "structured 'butterfly' sparsity... achieving similar trade-offs to unstructured pruning"
  - [section] "Butterfly networks can reach empirical performances comparable to a dense network on image classification tasks"
  - [corpus] Weak evidence; no direct support for butterfly structure's privacy benefits in corpus
- Break condition: If the fixed structure does not align well with the data distribution, it may fail to learn effectively despite sparsity.

### Mechanism 3
- Claim: The trade-off between accuracy loss and privacy gain follows a predictable pattern where a relative loss of p% in accuracy leads to a relative gain of 3.6×p% in defense against MIAs.
- Mechanism: As sparsity increases, the model becomes less confident in its predictions around training data, making it harder for discriminators to distinguish training from test data based on confidence patterns.
- Core assumption: Membership inference attacks exploit high confidence on training data; reducing confidence through sparsity disrupts this signal.
- Evidence anchors:
  - [abstract] "a relative loss of p% in accuracy, compared to the trained dense network, leads to a relative gain of 3.6×p% in defense against MIA"
  - [section] "between 3.4% and 17.3%, a relative loss of p% in accuracy... leads to a relative gain of 3.6×p%"
  - [corpus] Weak evidence; related papers do not discuss this specific quantitative relationship
- Break condition: If sparsity levels exceed 17.3%, the accuracy drops too much to be useful, breaking the trade-off.

## Foundational Learning

- Concept: Membership Inference Attacks
  - Why needed here: Understanding how MIAs work is crucial to grasp why sparsity can defend against them
  - Quick check question: What information does a membership inference attack try to infer about training data?

- Concept: Neural Network Pruning and Sparsity
  - Why needed here: The paper relies on understanding how removing weights affects model behavior and memorization
  - Quick check question: How does iterative magnitude pruning (IMP) select which weights to remove?

- Concept: Structured vs Unstructured Sparsity
  - Why needed here: The paper compares butterfly structured sparsity with unstructured IMP, requiring understanding of both approaches
  - Quick check question: What is the key difference between structured butterfly sparsity and unstructured IMP?

## Architecture Onboarding

- Component map:
  Target network -> Shadow network -> Discriminator -> MIA evaluation

- Critical path:
  1. Partition CIFAR-10 into training and test subsets for target and shadow
  2. Train target and shadow networks under same conditions
  3. Train discriminator using shadow network outputs and membership labels
  4. Evaluate discriminator on target network to measure privacy
  5. Repeat across sparsity levels to find optimal trade-off

- Design tradeoffs:
  - Sparsity level vs accuracy: Higher sparsity reduces privacy but may hurt performance
  - Structured vs unstructured: Butterfly sparsity is more efficient but less flexible than IMP
  - Discriminator complexity: More complex discriminators may detect subtle privacy leaks

- Failure signatures:
  - If discriminator accuracy is near 50%, privacy is preserved
  - If accuracy drops significantly with sparsity, the model may be too weak
  - If structured sparsity fails to match unstructured performance, the factorization may be suboptimal

- First 3 experiments:
  1. Train dense ResNet-20 on CIFAR-10 to establish baseline accuracy and privacy
  2. Apply IMP with 50% sparsity and compare accuracy/privacy trade-off
  3. Implement butterfly sparsity with same parameter count and compare results to IMP

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does sparsity improve privacy against membership inference attacks (MIAs) for larger scale neural networks and datasets beyond ResNet-20 on CIFAR-10?
- Basis in paper: [explicit] The authors note that "larger scale experiments are needed before confirming this trend" and suggest extending experiments to "a richer class of models, datasets and attacks."
- Why unresolved: The experiments were limited to ResNet-20 on CIFAR-10 with specific sparsity levels. The generalizability to larger models (e.g., ResNet-50, Transformers) and datasets (e.g., ImageNet) remains untested.
- What evidence would resolve it: Empirical studies showing consistent privacy-accuracy trade-offs across diverse architectures (e.g., ResNets, Transformers) and datasets (e.g., CIFAR-100, ImageNet) using both structured and unstructured sparsity.

### Open Question 2
- Question: How does structured butterfly sparsity compare to unstructured sparsity in terms of computational efficiency and privacy benefits across different hardware architectures?
- Basis in paper: [explicit] The authors highlight that butterfly sparsity allows "fast matrix-vector multiplication" and achieves "similar trade-offs" to unstructured sparsity, but note this needs further investigation.
- Why unresolved: While theoretical advantages of butterfly sparsity are mentioned, practical performance on real hardware (e.g., GPUs, TPUs) and its scalability to larger models is not evaluated.
- What evidence would resolve it: Benchmarking experiments comparing inference time, memory usage, and privacy metrics for both sparsity types on hardware-optimized implementations (e.g., cuSPARSE, custom butterfly kernels).

### Open Question 3
- Question: Can sparsity serve as a reliable baseline defense against MIAs without requiring knowledge of the attacker's strategy?
- Basis in paper: [explicit] The authors argue that sparsity "does not require to know the kind of attack in advance" and could serve as a baseline, but acknowledge this needs validation.
- Why unresolved: The experiments only tested against a specific MIA setup (shadow model + discriminator). The robustness of sparsity against adaptive or unknown attack strategies is untested.
- What evidence would resolve it: Empirical studies testing sparsity defenses against a diverse set of MIA variants (e.g., property inference, reconstruction attacks) and adaptive attackers who optimize their strategies based on sparse model characteristics.

## Limitations
- Results are based on a single model architecture (ResNet-20) and dataset (CIFAR-10), limiting generalizability
- The specific quantitative relationship between accuracy loss and privacy gain (3.6× factor) needs validation across different architectures and tasks
- The theoretical mechanisms explaining why sparsity improves privacy are not fully supported by evidence

## Confidence
- **High confidence**: The experimental methodology is sound and reproducible, with clear procedures for training sparse models and evaluating MIA robustness using discriminator-based attacks
- **Medium confidence**: The empirical results showing improved privacy-accuracy trade-offs for sparsity levels between 3.4% and 17.3% are convincing within the tested configuration, but require broader validation
- **Low confidence**: The theoretical mechanisms explaining why sparsity improves privacy are not fully supported by evidence, and the specific 3.6× quantitative relationship appears to be an empirical observation from limited experiments

## Next Checks
1. Cross-architecture validation: Replicate experiments on larger models (ResNet-50, EfficientNet) and datasets (ImageNet, CIFAR-100) to verify if the 3.6× privacy gain factor holds across different architectures and complexity levels
2. Mechanism isolation: Design ablation studies to separate the effects of reduced model capacity versus reduced confidence in predictions, determining which factor primarily drives the privacy improvements observed
3. Long-term stability analysis: Evaluate whether sparse models maintain their privacy-accuracy trade-off over extended training periods and with different initialization strategies, as sparse pruning can introduce optimization instabilities