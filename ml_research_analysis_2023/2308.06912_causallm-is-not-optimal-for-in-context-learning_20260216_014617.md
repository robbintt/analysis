---
ver: rpa2
title: CausalLM is not optimal for in-context learning
arxiv_id: '2308.06912'
source_url: https://arxiv.org/abs/2308.06912
tags:
- in-context
- causallm
- linear
- prefixlm
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence behavior of transformer-based
  in-context learning (ICL) using causal language models (causalLM) versus prefix
  language models (prefixLM). The authors prove that both converge to stationary points
  at a linear rate, but while prefixLM converges to the optimal least squares solution
  of linear regression, causalLM converges to the weights of an online gradient descent
  algorithm without decaying step sizes, which is not guaranteed to be optimal.
---

# CausalLM is not optimal for in-context learning

## Quick Facts
- arXiv ID: 2308.06912
- Source URL: https://arxiv.org/abs/2308.06912
- Authors: 
- Reference count: 30
- Primary result: PrefixLM converges to optimal linear regression solution while causalLM converges to suboptimal online gradient descent weights in in-context learning.

## Executive Summary
This paper analyzes the convergence behavior of transformer-based in-context learning (ICL) using causal language models (causalLM) versus prefix language models (prefixLM). The authors prove that both converge to stationary points at a linear rate, but while prefixLM converges to the optimal least squares solution of linear regression, causalLM converges to the weights of an online gradient descent algorithm without decaying step sizes, which is not guaranteed to be optimal. Experiments on synthetic tasks (linear/nonlinear regression, multiclass classification) and large language models (PaLM2) consistently show prefixLM outperforming causalLM in ICL settings.

## Method Summary
The paper theoretically analyzes the convergence of prefixLM and causalLM in in-context learning settings by constructing a linear self-attention model where the attention mechanism is designed to implement gradient descent dynamics. The key insight is that prefixLM allows full attention within the in-context examples, enabling optimal convergence to the least squares solution, while causalLM restricts attention to previous positions, leading to online gradient descent dynamics without decaying step sizes. Experiments validate these theoretical findings on synthetic regression and classification tasks, and demonstrate the practical advantage of prefixLM over causalLM on a large language model (PaLM2) for MMLU tasks.

## Key Results
- PrefixLM converges to the optimal least squares solution of linear regression, while causalLM converges to suboptimal weights from online gradient descent without decaying step sizes
- Both models converge to their respective stationary points at a linear rate, but prefixLM achieves better accuracy and faster convergence
- Empirical results on synthetic tasks and PaLM2 consistently show prefixLM outperforming causalLM in in-context learning settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PrefixLM converges to the optimal least squares solution of linear regression, while causalLM converges to the weights of an online gradient descent algorithm without decaying step sizes.
- Mechanism: PrefixLM allows full attention within the in-context examples, enabling each example to attend to all others. This creates a dynamics that exactly follows the multi-step gradient descent update rule for linear regression. In contrast, causalLM restricts attention to previous positions, leading to a dynamics equivalent to online gradient descent where each position maintains its own weight vector updated only from previous examples.
- Core assumption: The theoretical analysis assumes a linear self-attention (LSA) construction where K = Q = identity on the input dimension, V encodes the initial weights and labels, and P scales by the learning rate divided by the number of examples.
- Evidence anchors:
  - [abstract] "Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely."
  - [section 3.1] "A prefixLM ICL treats the in-context examples Z as the prefix and uses full attention on the first n positions, so that they can each freely attend to each other. The last query vector zquery can also attend to any example in Z, but cannot attend to itself."
- Break condition: The mechanism breaks if the attention patterns deviate significantly from the theoretical construction, or if the number of in-context examples is too small for the online gradient descent to approximate the global optimum.

### Mechanism 2
- Claim: The stationary points of causalLM follow an online gradient descent algorithm, which is not guaranteed to converge to the optimal solution without decaying step sizes.
- Mechanism: In causalLM, each position j maintains its own weight vector w_j that is updated based only on previous examples (1 through j). The stationary points w*_j satisfy an online learning update rule without decaying step sizes. This contrasts with prefixLM where all positions share a common weight vector w that converges to the global optimum.
- Core assumption: The update rule for causalLM is w_j^(l) = w_j^(l-1) + η/j * Σ_{i=1}^j (y_i - w_i^(l-1) x_i) x_i^T, and the stationary points can be characterized as following online gradient descent dynamics.
- Evidence anchors:
  - [abstract] "causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely."
  - [section 4.2] "the stationary points w*_j of causalLM-ICL are different from w*, the least square solution of linear regression. However, a natural question is: if j increases, would w*_j ultimately converge to the optimal solution?"
- Break condition: The mechanism breaks if the step size decay assumption is violated in a way that causes divergence, or if the problem structure allows online gradient descent to converge despite non-decaying step sizes.

### Mechanism 3
- Claim: Both prefixLM and causalLM converge to their respective stationary points at a linear rate of convergence.
- Mechanism: The convergence analysis shows that both models follow linear dynamical systems where the error decreases geometrically with each layer. For prefixLM, the error w^(l) - w* contracts by a factor (I - η/n XX^T) at each layer. For causalLM, the coefficients a^(l) - a* contract by a factor (I - η/n T) where T is a structured matrix based on the input examples.
- Core assumption: The convergence rate analysis assumes the spectral radius of the contraction matrices is less than 1, which requires appropriate choice of the learning rate η relative to the condition number of the input data matrix.
- Evidence anchors:
  - [abstract] "Our analysis shows that both LM types converge to their stationary points at a linear rate"
  - [section 4.1] "the iterative weights w^(l) converge to w* with a linear rate of convergence: w^(l) - w* = (w^(l-1) - w*)(I - η/n XX^T)"
  - [section 4.2] "the coefficients a^(l) converges to the stationary point a* with linear rate of convergence: a^(l) - a* = (a^(l-1) - a*)(I - η/n T)"
- Break condition: The mechanism breaks if the learning rate is too large causing divergence, or if the input data has pathological structure that prevents linear convergence.

## Foundational Learning

- Concept: Linear regression and gradient descent optimization
  - Why needed here: The theoretical analysis explicitly connects the in-context learning behavior of both prefixLM and causalLM to gradient descent dynamics for linear regression problems.
  - Quick check question: Can you derive the gradient descent update rule for linear regression and explain how it relates to the weight updates in the theoretical construction?

- Concept: Online learning and non-decaying step sizes
  - Why needed here: The analysis shows that causalLM corresponds to online gradient descent without decaying step sizes, which is a key reason why it may not converge to the optimal solution.
  - Quick check question: What is the difference between standard gradient descent and online gradient descent, and why do online methods typically require decaying step sizes for convergence?

- Concept: Attention mechanisms and their restrictions
  - Why needed here: The fundamental difference between prefixLM and causalLM is their attention patterns - prefixLM allows full attention within the context while causalLM restricts to previous positions only.
  - Quick check question: How does the attention mask differ between prefixLM and causalLM, and what are the implications for information flow between in-context examples?

## Architecture Onboarding

- Component map:
  Input formatting -> Attention mechanism -> Attention outputs -> Prediction extraction -> Performance evaluation

- Critical path:
  1. Format input examples as [x^T, y]^T
  2. Apply attention mechanism (full for prefixLM, autoregressive for causalLM)
  3. Compute attention outputs using the constructed K, Q, V, P matrices
  4. Extract the prediction from the last coordinate of each output
  5. Compare predictions to true labels to evaluate performance

- Design tradeoffs:
  - Full vs autoregressive attention: Full attention enables optimal convergence but may be computationally expensive; autoregressive attention is more efficient but leads to suboptimal solutions
  - Learning rate η: Must be chosen carefully to ensure convergence without divergence
  - Number of layers: More layers enable better approximation of the stationary points but increase computational cost

- Failure signatures:
  - PrefixLM: If the learning rate is too large, the model may diverge even with full attention
  - CausalLM: Even with appropriate learning rate, the model may converge to suboptimal solutions that do not improve with more examples
  - Both: Poor conditioning of the input data matrix can slow convergence or cause numerical instability

- First 3 experiments:
  1. Implement the theoretical construction with K = Q = identity, V encoding initial weights, and P scaling by η/n
  2. Compare convergence of prefixLM vs causalLM on a simple linear regression problem with well-conditioned data
  3. Vary the number of in-context examples to test whether causalLM improves with more data or plateaus at a suboptimal solution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of prefixLM-ICL compare to causalLM-ICL in high-dimensional linear regression settings with varying input distributions?
- Basis in paper: [explicit] The paper demonstrates that causalLM-ICL converges to suboptimal solutions and shows this effect worsens with increasing input mean (µx) in synthetic experiments.
- Why unresolved: While the paper shows qualitative differences in convergence behavior, it does not provide a detailed quantitative analysis of how convergence rates scale with dimensionality and input distribution parameters.
- What evidence would resolve it: A comprehensive study varying input dimension, input distribution statistics (mean, variance), and number of in-context examples, measuring convergence rates and test error for both prefixLM and causalLM.

### Open Question 2
- Question: Does the theoretical advantage of prefixLM-ICL over causalLM-ICL extend to non-linear regression tasks with more complex function classes?
- Basis in paper: [explicit] The paper shows empirical superiority of prefixLM-ICL on synthetic non-linear regression and classification tasks, but does not provide theoretical analysis for these cases.
- Why unresolved: The theoretical framework developed for linear regression does not directly apply to non-linear settings, and the paper does not extend the analysis to more complex function classes.
- What evidence would resolve it: Extending the theoretical analysis to non-linear regression using techniques like kernel methods or neural tangent kernels, and comparing convergence properties with empirical results on synthetic non-linear tasks.

### Open Question 3
- Question: How does the choice of attention mechanism (full vs. causal) affect the in-context learning capabilities of transformer-based models on real-world tasks beyond the synthetic settings?
- Basis in paper: [explicit] The paper demonstrates that prefixLM outperforms causalLM on a large language model (PaLM2) for MMLU tasks, but does not provide a systematic study across diverse real-world tasks.
- Why unresolved: While the paper shows a promising result on one specific task, it does not provide a comprehensive analysis of how attention mechanisms affect ICL across different types of real-world tasks and domains.
- What evidence would resolve it: A systematic study comparing prefixLM and causalLM on a diverse set of real-world tasks (e.g., natural language understanding, computer vision, reinforcement learning) using various transformer architectures and sizes.

## Limitations

- The theoretical analysis assumes a highly simplified linear self-attention construction that may not fully capture the complexity of real transformer implementations
- The characterization of causalLM convergence as online gradient descent without decaying step sizes may not directly translate to practical settings with additional architectural components
- The experimental validation focuses on synthetic tasks with relatively small model sizes, leaving open questions about scaling to larger models and real-world applications

## Confidence

**High confidence**: The theoretical analysis of linear convergence rates for both prefixLM and causalLM in the simplified LSA setting is mathematically rigorous and the core distinction between optimal convergence (prefixLM) and potentially suboptimal convergence (causalLM) is well-established.

**Medium confidence**: The experimental results showing prefixLM outperforming causalLM on synthetic tasks are reproducible, but the generalization to larger models and real-world applications requires further validation.

**Low confidence**: The exact mechanisms by which prefixLM achieves optimal convergence versus causalLM's online gradient descent dynamics may differ in practice due to additional architectural complexity not captured in the theoretical model.

## Next Checks

1. **Scale-up validation**: Replicate the experiments with larger transformer architectures (e.g., LLaMA-7B size) on both synthetic and real-world ICL tasks to test whether the prefixLM advantage persists at scale.

2. **Architectural ablation study**: Systematically remove or modify architectural components (residual connections, layer normalization, MLP layers) in the theoretical construction to identify which components are critical for the observed convergence behavior differences.

3. **Alternative optimization analysis**: Compare the convergence dynamics of prefixLM and causalLM when trained with different optimization algorithms (Adam, SGD with momentum) to determine if the theoretical findings are specific to the simplified gradient descent setting or generalize to more sophisticated optimizers.