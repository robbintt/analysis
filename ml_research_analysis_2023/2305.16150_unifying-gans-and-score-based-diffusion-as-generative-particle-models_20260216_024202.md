---
ver: rpa2
title: Unifying GANs and Score-Based Diffusion as Generative Particle Models
arxiv_id: '2305.16150'
source_url: https://arxiv.org/abs/2305.16150
tags:
- discriminator
- generator
- flows
- score
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a unified framework that bridges two prominent
  classes of deep generative models: score-based diffusion models and generative adversarial
  networks (GANs). The key insight is that both can be viewed as particle models where
  generated samples follow a differential equation parameterized by a vector field,
  with or without a generator network.'
---

# Unifying GANs and Score-Based Diffusion as Generative Particle Models

## Quick Facts
- arXiv ID: 2305.16150
- Source URL: https://arxiv.org/abs/2305.16150
- Reference count: 40
- Key outcome: A unified framework showing GANs and score-based diffusion models can both be viewed as particle models with differential equations parameterized by vector fields

## Executive Summary
This paper presents a unifying mathematical framework that bridges two prominent classes of deep generative models: score-based diffusion models and generative adversarial networks (GANs). The key insight is that both can be viewed as particle models where generated samples follow a differential equation parameterized by a vector field, with or without a generator network. The authors formalize this as Interacting Particle Models (Int-PMs), showing that GANs naturally fit within this framework. By decoupling the generator from the flow, they propose two novel hybrid models: Score GANs, which train a generator using score-based gradients instead of adversarial training, and Discriminator Flows, which remove the generator from GANs entirely. Empirical results on MNIST and CelebA demonstrate the viability of these models, with Discriminator Flows showing faster convergence than standard diffusion models.

## Method Summary
The method presents a unified framework where both GANs and score-based diffusion models are treated as Interacting Particle Models (Int-PMs). In this framework, samples evolve according to differential equations parameterized by a functional h(ρt) of the current particle distribution. For GANs, this functional is the negative discriminator output, while for diffusion models it's the score function. The authors propose two novel architectures: Score GANs, which train generators using score-based gradients instead of adversarial training, and Discriminator Flows, which remove the generator entirely and synthesize samples directly using discriminator-guided particle evolution. The framework is implemented with specific training procedures and architectures detailed in tables, using alternating updates between generators/score networks and discriminators.

## Key Results
- Score GANs and Discriminator Flows are viable alternatives to standard GANs and diffusion models
- Discriminator Flows show faster convergence than standard diffusion models on tested datasets
- The unified framework successfully bridges the theoretical gap between adversarial and score-based generative modeling approaches
- Novel hybrid models demonstrate practical applicability on MNIST and CelebA datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Particle models and GANs can be unified under a single mathematical framework by treating generator outputs as following the same gradient field as particles in score-based diffusion models, with the generator acting as a smoothing operator.
- Mechanism: The framework treats both types of models as instances of Interacting Particle Models (Int-PMs) where particles (either directly in data space or through a generator) evolve according to a differential equation parameterized by a functional h(ρt) that depends on the current particle distribution. For GANs, this functional is the negative discriminator output, while for diffusion models it's the score function. The generator introduces a Neural Tangent Kernel (NTK) that smooths the gradient field, effectively creating "interacting particles" where moving one particle affects its neighbors through the shared generator parameters.
- Core assumption: The generator's NTK is approximately constant during training, allowing the smoothing effect to be modeled as a linear operator on vector fields.
- Evidence anchors:
  - [abstract] "The key insight is that both can be viewed as particle models where generated samples follow a differential equation parameterized by a vector field"
  - [section 3.1] "By optimizing θt via gradient descent for the loss of Equation (10), idealized in the continuous training time setting, we obtain using the chain rule: dθt/dt = ∇θt Ez∼pz[hρt(gθt(z))]"
  - [corpus] Weak - no direct citations found, but this is a theoretical contribution
- Break condition: If the generator's NTK varies significantly during training, the smoothing approximation breaks down and the unified framework no longer accurately describes the particle dynamics.

### Mechanism 2
- Claim: Score GANs can be trained by using score-based gradients instead of adversarial training, where the generator is updated using the difference between the data score and the generated distribution score.
- Mechanism: The Score GAN framework splits the gradient flow ∇hρt into two components: one that depends only on the data distribution (estimated before training using pretrained score models) and one that depends on the generated distribution (continuously estimated during training). This is implemented by alternating updates between a score network for the generated distribution and the generator itself, using denoising score matching techniques.
- Core assumption: The data score can be accurately estimated and remains static during training, while the generated distribution score can be efficiently learned through alternating updates.
- Evidence anchors:
  - [section 4.1] "We propose training a generator with the score-based diffusion flow of NCSN, Equation (9), left. This involves applying Equation (13) with hρt = log[pdata ⋆ kσRBF] − log[ρt ⋆ kσRBF]"
  - [section 4.1] "Composed of the scores of, respectively, the noised data distribution and the generated distribution, ∇hρt can be efficiently estimated via score matching techniques"
  - [corpus] Weak - no direct citations found, this appears to be a novel contribution
- Break condition: If the generated distribution score cannot be accurately estimated during training, or if the alternating updates between score network and generator become unstable, the Score GAN training process will fail.

### Mechanism 3
- Claim: Discriminator Flows can be implemented by removing the generator from GANs and synthesizing samples directly using the discriminator to guide particle evolution in data space.
- Mechanism: The Discriminator Flow framework treats the GAN training process as a particle model without a generator by defining hρt = −c ◦ fρt, where fρt is the discriminator between the current particle distribution and the data distribution. Samples are generated by numerically integrating the differential equation dxt = −∇c ◦ fρt(xt)dt, with the discriminator being trained alongside the generation process.
- Core assumption: The discriminator can be effectively conditioned on time and trained to provide meaningful gradients for sample synthesis throughout the entire generation process.
- Evidence anchors:
  - [section 4.2] "Based on Findings 4 and 5, we see that removing the generator from GANs to make them PMs, as in Definition 1, simply requires that we define hρt = −c ◦ fρt"
  - [section 4.2] "This makes Equation (20) the equivalent of GAN training, but as a PM without parameterization by a generator"
  - [corpus] Weak - no direct citations found, this appears to be a novel contribution
- Break condition: If the discriminator cannot provide useful gradients for sample synthesis (e.g., due to mode collapse or poor training), or if the numerical integration becomes unstable, the Discriminator Flow generation process will fail.

## Foundational Learning

- Concept: Interacting Particle Models (Int-PMs)
  - Why needed here: This is the core theoretical framework that unifies particle models and GANs by showing how both can be viewed as particles evolving according to differential equations parameterized by functionals of the current distribution
  - Quick check question: How does an Int-PM differ from a standard particle model, and what role does the generator play in this framework?

- Concept: Neural Tangent Kernel (NTK) and its smoothing effect
  - Why needed here: The NTK explains how the generator creates interactions between particles, effectively smoothing the gradient field and causing particles to influence each other's trajectories
  - Quick check question: Why does the NTK cause particles to interact, and how does this relate to the generator's role in GANs?

- Concept: Score matching and denoising score estimation
  - Why needed here: Score matching is the key technique used in Score GANs to estimate the gradient of the log-density, which is then used to update the generator instead of adversarial gradients
  - Quick check question: How does denoising score matching work, and why is it preferred over other score estimation methods in this context?

## Architecture Onboarding

- Component map:
  Core framework -> Int-PM definition with functional h(ρt) and differential equation
  Generator component -> Optional, parameterized by θ, with NTK smoothing
  Score network -> Estimates ∇ log ρt for generated distribution
  Discriminator -> Estimates c ◦ fρt for GAN objectives
  Data score model -> Pretrained network estimating ∇ log[pdata ⋆ kσRBF]

- Critical path:
  1. Define h(ρt) based on chosen objective (e.g., KL divergence, MMD)
  2. Implement particle evolution equation (with or without generator)
  3. Set up alternating training between generator/score network and discriminator
  4. Implement numerical integration for sample generation

- Design tradeoffs:
  - Generator vs no generator: Generators provide efficient sampling and low-dimensional latent space but risk mode collapse; generator-free models are more flexible but slower
  - Score estimation vs adversarial training: Score-based methods may be more stable but require accurate score estimation; adversarial methods are more direct but can be unstable
  - Time discretization: Finer discretization improves accuracy but increases computational cost

- Failure signatures:
  - Mode collapse: Generator produces limited variety of samples
  - Training instability: Oscillating losses or exploding/vanishing gradients
  - Poor sample quality: Generated samples don't match data distribution
  - Slow convergence: Training or sampling takes excessively long

- First 3 experiments:
  1. Implement and train a basic Score GAN on MNIST using a pretrained data score model and alternating updates
  2. Implement and train a Discriminator Flow on a simple dataset (e.g., mixture of Gaussians) to verify the generation process
  3. Compare FID scores of Score GAN and Discriminator Flow against baseline EDM and GAN on CelebA to validate performance claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the NTK kernel affect the convergence speed of Discriminator Flows compared to diffusion models?
- Basis in paper: [explicit] The authors mention that NTKs of standard discriminators act as kernels in MMD gradient flows and accelerate convergence towards pdata by several orders of magnitude compared to Gaussian kernels used in diffusion models.
- Why unresolved: While the authors provide theoretical intuition, they do not conduct direct empirical comparisons between Discriminator Flows and diffusion models on convergence speed.
- What evidence would resolve it: Experiments measuring convergence speed of Discriminator Flows vs diffusion models (e.g., EDM) on standard benchmarks with varying numbers of NFEs, showing quantitative differences in FID scores.

### Open Question 2
- Question: What is the theoretical relationship between the training dynamics of GANs and the convergence of the underlying particle model?
- Basis in paper: [explicit] The authors show that GANs are Int-PMs with hρ = -c ◦ fρ, but they acknowledge that the discriminator is continuously trained with the generator, which complicates the analysis.
- Why unresolved: The authors state that taking into account the continuous training of the discriminator is a challenging task that they do not address in this work.
- What evidence would resolve it: Theoretical analysis or empirical studies showing how the alternating updates between the generator and discriminator affect the convergence properties of the underlying particle model.

### Open Question 3
- Question: Can Score GANs be made more practical by addressing the challenges of estimating the score of the generated distribution?
- Basis in paper: [explicit] The authors propose using denoising score matching to estimate the score of the generated distribution, but they acknowledge that this approach is less performant than denoising score matching and leads to estimation issues when the generated distribution lies on a manifold.
- Why unresolved: The authors propose a solution (matching pdata ⋆ kσ RBF and ρt ⋆ kσ RBF) but do not provide empirical results showing its effectiveness or compare it to other approaches.
- What evidence would resolve it: Experiments comparing the proposed solution to other methods for estimating the score of the generated distribution, showing improvements in generative performance and addressing the estimation issues.

## Limitations
- Theoretical framework relies on idealized continuous-time formulations that may not perfectly translate to practical discrete-time implementations
- Assumption of constant Neural Tangent Kernel during GAN training may not hold in practice, particularly for complex datasets and architectures
- Empirical validation is limited to relatively simple image datasets (MNIST and CelebA) with only 5-10K training samples per model

## Confidence
- High Confidence: The mathematical framework connecting particle models, GANs, and score-based diffusion is rigorously defined and internally consistent
- Medium Confidence: The proposed Score GAN and Discriminator Flow architectures are theoretically sound, but their practical performance and stability remain to be thoroughly validated
- Low Confidence: The assumption of constant NTK during GAN training and its implications for the unified framework may not hold in practice

## Next Checks
1. Extended Benchmark Testing: Evaluate Score GANs and Discriminator Flows on more challenging datasets (e.g., CIFAR-10, ImageNet) with larger model sizes to assess scalability and performance relative to established methods.

2. NTK Analysis: Empirically measure the variation of the Neural Tangent Kernel during GAN training across different architectures and datasets to quantify the validity of the constant NTK assumption.

3. Training Stability Investigation: Conduct controlled experiments to identify conditions under which Score GANs and Discriminator Flows exhibit training instability, and develop mitigation strategies based on the unified framework insights.