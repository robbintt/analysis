---
ver: rpa2
title: Fine-Grained Human Feedback Gives Better Rewards for Language Model Training
arxiv_id: '2306.01693'
source_url: https://arxiv.org/abs/2306.01693
tags:
- reward
- feedback
- human
- training
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Fine-grained human feedback enables more targeted reinforcement\
  \ learning from human feedback (RLHF) by providing localized, per-segment error\
  \ labels instead of just holistic preference rankings. This approach uses multiple\
  \ specialized reward models\u2014each focusing on a specific error type (e.g., factual\
  \ errors, irrelevance, incompleteness)\u2014to give dense rewards after every segment\
  \ (e.g., sentence)."
---

# Fine-Grained Human Feedback Gives Better Rewards for Language Model Training

## Quick Facts
- arXiv ID: 2306.01693
- Source URL: https://arxiv.org/abs/2306.01693
- Reference count: 40
- Primary result: Fine-grained feedback with multiple specialized reward models outperforms holistic preference-based RLHF in both detoxification and long-form QA tasks.

## Executive Summary
Fine-grained human feedback enables more targeted reinforcement learning from human feedback (RLHF) by providing localized, per-segment error labels instead of just holistic preference rankings. This approach uses multiple specialized reward models—each focusing on a specific error type (e.g., factual errors, irrelevance, incompleteness)—to give dense rewards after every segment (e.g., sentence). Experiments on detoxification and long-form question answering show significant performance gains over traditional preference-based RLHF: toxicity drops faster with fewer training steps in detoxification; long-form QA models achieve lower error rates and more complete answers. Fine-grained feedback also allows customization of model behavior by adjusting reward model weights. The method is more sample-efficient and gives clearer learning signals than holistic rewards.

## Method Summary
The method introduces fine-grained RLHF where human feedback is collected at the segment level, labeling specific text spans with error types. Multiple specialized reward models are trained to detect different error categories (irrelevance, factual errors, incompleteness). During training, the policy model receives combined rewards from all reward models at different granularities—sub-sentence, sentence, and full sequence—depending on the error type. Proximal Policy Optimization (PPO) is used to optimize the policy model against these fine-grained rewards. The approach was validated on detoxification using REALTOXICITY PROMPTS and long-form QA using a reformulated ASQA dataset (QA-FEEDBACK), showing improved sample efficiency and reduced error rates compared to traditional preference-based RLHF.

## Key Results
- Detoxification: Toxicity drops faster with fewer training steps when using fine-grained rewards compared to holistic rewards.
- Long-form QA: Models trained with fine-grained RLHF achieve lower error rates across all error types (relevance, factuality, completeness) and generate more complete answers.
- Customization: Adjusting reward model weights allows tuning of model behavior (e.g., shorter vs. longer outputs with different error trade-offs).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-grained feedback provides denser training signals than holistic rewards, improving sample efficiency.
- **Mechanism:** Holistic RLHF assigns a single reward after full sequence generation. Fine-grained RLHF assigns rewards after every segment (e.g., sentence) for specific error types, creating more frequent and localized updates.
- **Core assumption:** Localized errors are easier for the model to attribute and correct than aggregate feedback.
- **Evidence anchors:**
  - [abstract]: "Such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors."
  - [section 3.1]: "FINE -GRAINED RLHF has the toxicity drop much faster while keeping a low-level perplexity. This shows that learning from denser fine-grained reward is more sample efficient than holistic reward."
  - [corpus]: Weak. No direct citations found supporting this mechanism, though the abstract claim aligns with the general RLHF literature.
- **Break condition:** If the model cannot effectively segment outputs or if segment-level rewards conflict, the training signal may become noisy rather than informative.

### Mechanism 2
- **Claim:** Multiple specialized reward models allow targeted correction of distinct error types, improving overall quality.
- **Mechanism:** Separate reward models focus on different error categories (irrelevance, factual errors, incompleteness). During training, each model provides a reward for its domain, and the combined reward guides the policy toward multiple desired behaviors.
- **Core assumption:** Decomposing errors into categories yields clearer, more actionable feedback than a single aggregate score.
- **Evidence anchors:**
  - [abstract]: "incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness)."
  - [section 4.4]: "FINE -GRAINED RLHF outperforms SFT and Preference RLHF on all error types... It generates fewer irrelevance & repetition & incoherence errors, compared with SFT and Preference RLHF."
  - [corpus]: Weak. The related works mention token-level rewards and decomposition but no direct evidence from the corpus supports the multi-reward mechanism.
- **Break condition:** If reward models are poorly aligned or conflict, the combined reward may cancel out gains in some areas.

### Mechanism 3
- **Claim:** Adjusting reward model weights allows customizable behavior tuning of the final model.
- **Mechanism:** By changing the weights (w1, w2, w3) assigned to each fine-grained reward model during RL training, the policy model can be biased toward certain behaviors (e.g., more factual vs. more complete responses).
- **Core assumption:** The policy model can interpolate between reward model objectives without catastrophic forgetting or instability.
- **Evidence anchors:**
  - [section 4.5]: "we explore three configurations of reward model weights... and name them 'short', 'medium', and 'long' according to the LM's average generation length."
  - [section 4.5]: "This flexibility can potentially fit users with diverse needs."
  - [corpus]: Weak. No direct corpus evidence supports the customizability claim, but the section description aligns with RL weight tuning literature.
- **Break condition:** If reward model objectives are too antagonistic, adjusting weights may lead to unstable training or degraded overall performance.

## Foundational Learning

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - **Why needed here:** RLHF is the base framework for converting human judgments into reward signals for language model training.
  - **Quick check question:** What is the difference between supervised fine-tuning and RLHF in terms of the learning signal?

- **Concept:** Markov Decision Process (MDP) formulation of language generation
  - **Why needed here:** The paper models text generation as an MDP to justify the use of RL algorithms like PPO.
  - **Quick check question:** In the MDP formulation, what constitutes a state, an action, and a reward?

- **Concept:** Proximal Policy Optimization (PPO)
  - **Why needed here:** PPO is the RL algorithm used to optimize the policy model against the fine-grained reward models.
  - **Quick check question:** How does PPO's clipped surrogate objective help stabilize training compared to vanilla policy gradient methods?

## Architecture Onboarding

- **Component map:**
  Task prompts -> Policy model -> Value model -> Multiple fine-grained reward models -> PPO training loop -> Human feedback collection

- **Critical path:**
  1. Collect human feedback on model outputs (error spans + categories).
  2. Train specialized reward models on this feedback.
  3. Integrate reward models into PPO loop.
  4. Optimize policy model to maximize combined reward.
  5. Evaluate model outputs using both automatic and human metrics.

- **Design tradeoffs:**
  - Segment granularity vs. annotation effort (sentence-level vs. sub-sentence).
  - Number of reward models vs. computational cost (more models = more reward calls).
  - Weight tuning flexibility vs. potential reward conflicts.
  - Using off-the-shelf reward models (e.g., Perspective API) vs. training custom ones.

- **Failure signatures:**
  - High variance in rewards → unstable training.
  - Reward models disagreeing → contradictory gradients.
  - Overfitting to human feedback → poor generalization.
  - Excessive length or repetition in outputs → relevance reward dominating.

- **First 3 experiments:**
  1. Compare holistic vs. sentence-level reward on detoxification task.
  2. Ablation study: train policy with only one of the three fine-grained reward models.
  3. Test LM customization by varying reward model weights and measuring output length and error rates.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the fine-grained reward models handle cases where errors overlap across categories (e.g., a sentence that is both irrelevant and factually incorrect)?
- **Basis in paper:** [explicit] The paper states that workers are instructed not to annotate C2 errors for spans already labeled as C1, but this restriction is only for human annotation, not for the reward models themselves.
- **Why unresolved:** The paper does not explicitly address how the reward models handle such overlapping errors during training or inference, which could lead to conflicts or redundancy in the rewards.
- **What evidence would resolve it:** Experiments showing the impact of overlapping errors on reward model performance, or a clarification of how the reward models are trained to handle such cases.

### Open Question 2
- **Question:** What is the impact of using different densities (e.g., sub-sentence vs. sentence-level) for different error categories on the overall performance of the fine-grained reward models?
- **Basis in paper:** [explicit] The paper mentions that different error categories are associated with different densities (e.g., sub-sentence for C1, sentence for C2, full sequence for C3), but does not explore the impact of varying these densities.
- **Why unresolved:** The paper does not provide a systematic comparison of how different density choices affect the performance of the reward models or the final LM outputs.
- **What evidence would resolve it:** Experiments comparing the performance of reward models with different density choices, or an analysis of how density affects the learning signals provided to the LM.

### Open Question 3
- **Question:** How does the performance of the fine-grained reward models compare to human judgment in terms of accuracy and reliability?
- **Basis in paper:** [explicit] The paper reports the performance of the reward models in terms of classification accuracy and F1 scores, but does not compare these metrics to human judgment.
- **Why unresolved:** The paper does not provide a direct comparison of the reward models' performance to human judgment, which would help assess the reliability of the reward models as proxies for human feedback.
- **What evidence would resolve it:** A study comparing the predictions of the reward models to human judgments on a held-out set of examples, or an analysis of the agreement between the reward models and human annotators.

## Limitations
- Experimental validation limited to two specific tasks (detoxification and long-form QA), uncertain generalizability to other domains.
- Reliance on human feedback for training reward models introduces potential scalability constraints.
- Computational overhead from multiple reward models per output may limit practical deployment.

## Confidence

**High Confidence:**
- The core technical contribution of fine-grained reward modeling is well-specified and reproducible. The mechanism of providing denser training signals through segment-level rewards is theoretically sound and supported by observed performance gains.

**Medium Confidence:**
- The detoxification results show clear improvements, but the long-form QA evaluation relies heavily on reward model predictions rather than direct human preference judgments, which may introduce bias.

**Low Confidence:**
- Claims about customizability through reward model weight tuning are demonstrated only qualitatively. The trade-off between different error types when adjusting weights is not systematically explored.

## Next Checks
1. Conduct ablation studies removing one reward model at a time to quantify the marginal contribution of each fine-grained feedback type.
2. Perform head-to-head human preference evaluations comparing fine-grained RLHF outputs against baselines across multiple domains beyond the two tested tasks.
3. Measure computational overhead and inference latency when scaling to production workloads with multiple concurrent reward model calls.