---
ver: rpa2
title: Federated Wasserstein Distance
arxiv_id: '2310.01973'
source_url: https://arxiv.org/abs/2310.01973
tags:
- distance
- fedwad
- clients
- interpolating
- wasserstein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for computing the Wasserstein distance
  between two distributions in a federated setting, where the data is distributed
  across different devices/clients and the central server does not have direct access
  to the samples. The proposed algorithm, FedWad, leverages the geometric properties
  of the Wasserstein distance and its geodesics to iteratively approximate the distance.
---

# Federated Wasserstein Distance

## Quick Facts
- arXiv ID: 2310.01973
- Source URL: https://arxiv.org/abs/2310.01973
- Reference count: 40
- Primary result: Proposes FedWad algorithm for computing Wasserstein distances between distributions in federated settings without direct data access

## Executive Summary
This paper introduces FedWad, an algorithm for computing the Wasserstein distance between two distributions in a federated setting where data is distributed across clients. The method leverages the geometric properties of Wasserstein distance, specifically using geodesics and the triangle inequality, to iteratively approximate the distance. The algorithm shows convergence guarantees and demonstrates effectiveness in applications like federated coresets and optimal transport dataset distance, with experimental results indicating improved federated learning performance by reducing dataset heterogeneity among clients.

## Method Summary
FedWad is designed to compute the Wasserstein distance between two distributions stored on different clients in a federated setting. The algorithm iteratively approximates an interpolating measure on the geodesic between the two distributions using the triangle inequality property of Wasserstein distance. Clients compute local interpolating measures using their data and send them to a central server, which aggregates these to update the estimate of the interpolating measure. The method supports both exact and approximated interpolating measures, with the latter reducing computational complexity by fixing the support size. The algorithm converges to the true Wasserstein distance as the sequence of approximations becomes increasingly accurate.

## Key Results
- FedWad converges to the true Wasserstein distance between distributions in federated settings
- The algorithm effectively reduces dataset heterogeneity among clients, improving federated learning performance
- Approximated interpolating measures maintain accuracy while significantly reducing computational complexity
- Experimental results show FedWad outperforms non-federated approaches in terms of communication efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedWad exploits the triangle inequality property of Wasserstein distance to iteratively approximate distances without direct sample access
- Mechanism: By leveraging that Wp(µ, ν) = Wp(µ, ξ⋆) + Wp(ξ⋆, ν) where ξ⋆ is an interpolating measure on the geodesic, FedWad iteratively estimates ξ⋆ through intermediate measures computed by clients and aggregated by the server
- Core assumption: The triangle inequality becomes an equality for interpolating measures on the geodesic between two distributions
- Evidence anchors:
  - [abstract]: "we take advantage of the geometric properties of the Wasserstein distance – in particular, the triangle inequality – and that of the associated geodesics"
  - [section]: "Property 2 (Interpolating point, (Ambrosio et al., 2005)) . Any point xt from a constant speed geodesic (x(t))t∈[0,1] is an interpolating point and verifies, d(x0, x1) = d(x0, xt) + d(xt, x1)"
- Break condition: If distributions are too dissimilar or the geodesic is not well-defined, the triangle inequality may not provide a good approximation

### Mechanism 2
- Claim: Approximated interpolating measures reduce computational complexity while maintaining accuracy
- Mechanism: By approximating interpolating measures using barycentric mapping (Equation 10), the support size is fixed, reducing computational cost of computing Wasserstein distances at each iteration
- Core assumption: Approximated interpolating measures are sufficiently close to exact ones for algorithm convergence
- Evidence anchors:
  - [section]: "we resort to approximations of the interpolating measures which goal is to fix the support size of the interpolating measures to a small number S"
  - [section]: "Theorem 1. Consider two discrete distributions µ and ν with the same number of samples n and uniform weights, then for any t, the approximated interpolating measure, between µ and ν given by Equation (10) is equal to the exact one Equation (5)"
- Break condition: If distributions have very different support sizes or non-uniform weights, approximation may become inaccurate

### Mechanism 3
- Claim: The algorithm converges to the true Wasserstein distance between distributions
- Mechanism: By iteratively updating interpolating measures and leveraging the non-increasing property of sequence A(k), the algorithm converges to Wp(µ, ν)
- Core assumption: Sequence (A(k)) is non-increasing and bounded below by Wp(µ, ν)
- Evidence anchors:
  - [section]: "Theorem 2. Let µ and ν be two measures in Pp(X). For k ∈ N, let ξ(k) µ , ξ(k) ν and ξ(k) be interpolating measures computed at iteration k as defined in Algorithm 1. Define A(k) = Wp(µ, ξ(k) µ ) + Wp(ξ(k) µ , ξ(k)) + Wp(ξ(k), ξ(k) ν ) + Wp(ξ(k) ν , ν) Then, the sequence (A(k)) is non-increasing and converges to Wp(µ, ν)."
- Break condition: If initial estimate ξ(0) is too far from true interpolating measure, convergence may be slow or unstable

## Foundational Learning

- Concept: Wasserstein distance and optimal transport
  - Why needed here: Understanding Wasserstein distance is crucial for grasping how FedWad leverages its geometric properties to compute distances in a federated manner
  - Quick check question: What is the Kantorovich relaxation of the Wasserstein distance, and how does it relate to optimal transport plans?

- Concept: Geodesics in metric spaces
  - Why needed here: Geodesics are central to FedWad's approach, as the algorithm iteratively estimates an interpolating measure on the geodesic between two distributions
  - Quick check question: How does the triangle inequality relate to geodesics in a metric space?

- Concept: Federated learning and privacy-preserving computations
  - Why needed here: FedWad is designed for federated settings where data is distributed across devices, and privacy is a key concern
  - Quick check question: What are the main challenges in federated learning, and how does FedWad address them?

## Architecture Onboarding

- Component map:
  - Clients -> Server -> Interpolating measure computation -> Wasserstein distance computation

- Critical path:
  1. Server sends current estimate ξ(k-1) to clients
  2. Clients compute ξ(k) µ and ξ(k) ν using their local distributions and ξ(k-1)
  3. Clients send ξ(k) µ and ξ(k) ν to the server
  4. Server computes ξ(k) by minimizing the sum of Wasserstein distances
  5. Repeat until convergence

- Design tradeoffs:
  - Exact vs. approximated interpolating measures: Exact measures are more accurate but computationally expensive, while approximated measures are faster but may introduce errors
  - Number of iterations: More iterations can lead to better convergence but increase communication costs

- Failure signatures:
  - Slow convergence: May indicate that the initial estimate is too far from the true interpolating measure
  - Numerical instability: Can occur when computing Wasserstein distances for high-dimensional data or with large support sizes

- First 3 experiments:
  1. Verify convergence: Test FedWad on simple distributions (e.g., Gaussians) and compare the computed distance to the known true distance
  2. Assess approximation quality: Compare results of FedWad using exact and approximated interpolating measures for various support sizes
  3. Evaluate communication efficiency: Measure communication costs of FedWad compared to non-federated approaches

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas warrant further investigation based on the content.

## Limitations
- Computational complexity of computing Wasserstein distances between high-dimensional distributions remains a challenge, even with approximated interpolating measures
- Performance in very high-dimensional spaces is uncertain due to the curse of dimensionality
- Limited experimental results on large-scale, high-dimensional real-world datasets

## Confidence
- High confidence: Geometric properties of Wasserstein distance and convergence guarantees of FedWad are well-established in theoretical analysis
- Medium confidence: Effectiveness of approximated interpolating measures in reducing computational complexity while maintaining accuracy, based on theoretical results and experimental validation on toy examples
- Low confidence: Performance of FedWad on large-scale, high-dimensional real-world datasets, as paper only provides limited experimental results on smaller datasets like MNIST and CIFAR10

## Next Checks
1. Conduct experiments on high-dimensional real-world datasets to assess scalability and accuracy of FedWad in practical federated learning scenarios
2. Compare communication efficiency and convergence speed of FedWad to other federated learning approaches (e.g., FedAvg) in terms of wall-clock time and communication rounds
3. Investigate impact of different hyperparameters (e.g., support size, number of iterations) on performance of FedWad and provide guidelines for hyperparameter selection in various federated learning tasks