---
ver: rpa2
title: Can AI-Generated Text be Reliably Detected?
arxiv_id: '2303.11156'
source_url: https://arxiv.org/abs/2303.11156
tags:
- text
- detectors
- paraphrasing
- watermarked
- ai-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the reliability of AI-generated text detection
  systems, which are crucial for preventing misuse of Large Language Models (LLMs)
  such as plagiarism and fake news generation. The authors propose a recursive paraphrasing
  attack that evades various detectors including watermarking schemes, neural network-based
  detectors, zero-shot classifiers, and retrieval-based detectors.
---

# Can AI-Generated Text be Reliably Detected?

## Quick Facts
- arXiv ID: 2303.11156
- Source URL: https://arxiv.org/abs/2303.11156
- Reference count: 4
- Primary result: Paraphrasing attacks can evade AI-generated text detectors with minimal quality degradation

## Executive Summary
This paper investigates the reliability of AI-generated text detection systems, which are crucial for preventing misuse of Large Language Models (LLMs). The authors demonstrate that current detection methods are vulnerable to recursive paraphrasing attacks and spoofing attacks on watermarking schemes. Their theoretical analysis shows that as LLMs improve, the total variation distance between human and AI-generated text distributions diminishes, making reliable detection increasingly difficult. The study concludes that practical AI-generated text detection faces fundamental limitations, requiring careful consideration of trade-offs between detection performance and LLM utility.

## Method Summary
The authors propose a recursive paraphrasing attack that transforms AI-generated text to evade various detectors. They conduct experiments on 300-token passages from the XSum dataset, using OPT-1.3B and GPT-2 Medium LLMs with T5 and PEGASUS paraphrasers. The attack is evaluated against watermarking schemes, neural network-based detectors, zero-shot classifiers, and retrieval-based detectors. The theoretical analysis bounds detection performance using total variation distance between human and AI-generated text distributions, proving that even the best possible detector can only marginally outperform random guessing for advanced LLMs.

## Key Results
- Paraphrasing attacks reduce watermarking detector accuracy from 97% to 57% with only 3.5 perplexity degradation
- Neural network-based detectors like DetectGPT can be evaded by paraphrasing attacks
- Theoretical upper bound shows AUROC ≤ 1/2 + TV(M,H) - TV(M,H)²/2 for any detector
- Spoofing attacks can make human text appear as AI-generated, damaging LLM developers' reputations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paraphrasing attacks can effectively remove LLM-specific signatures from generated text, evading detection.
- Mechanism: The paraphraser generates sentences with meanings similar to the original AI-generated text but with different token distributions. Since detectors rely on specific statistical patterns or watermarks in the text, replacing these with paraphrased versions that maintain semantic meaning but alter the statistical signature breaks the detection.
- Core assumption: The paraphrased text maintains semantic meaning while significantly altering the statistical properties that detectors rely on.
- Evidence anchors:
  - [abstract] "Our experiments conducted on passages, each approximately 300 tokens long, reveal the varying sensitivities of these detectors to our attacks."
  - [section 2.1] "The paraphraser takes the watermarked LLM text sentence by sentence as input. We use 100 passages from the Extreme Summarization (XSum) dataset [Narayan et al., 2018] for our evaluations. The detector's accuracy drops from 97% to 80% with only a trade-off of 3.5 in perplexity score."

### Mechanism 2
- Claim: The theoretical impossibility result shows that for sufficiently advanced LLMs, even the best possible detector can only marginally outperform random guessing.
- Mechanism: As language models improve, the total variation distance between human and AI-generated text distributions diminishes. This makes it increasingly difficult to distinguish between them, as the statistical differences become smaller.
- Core assumption: The total variation distance between human and AI-generated text distributions is a good measure of detectability.
- Evidence anchors:
  - [abstract] "They prove that for sufficiently advanced LLMs, even the best possible detector can only marginally outperform random guessing, due to diminishing total variation distance between human and AI-generated text distributions."
  - [section 3] "Theorem 1 bounds the area under the ROC curve of the best possible detector D as: AUROC(D) ≤ 1/2 + TV(M,H) - TV(M,H)²/2 where TV(M,H) is the total variation distance between the text distributions produced by an AI-model M and humans H."

### Mechanism 3
- Claim: Watermarking schemes are vulnerable to spoofing attacks where adversaries can infer hidden watermark signatures and generate human text that gets detected as AI-generated.
- Mechanism: By querying the watermarked LLM multiple times, an attacker can learn the probability distributions of green list tokens for common words. This information can be used to compose human-written text that has similar statistical properties to watermarked text, causing it to be detected as AI-generated.
- Core assumption: The watermarking scheme uses a predictable pattern of green list tokens that can be inferred through repeated queries.
- Evidence anchors:
  - [abstract] "They demonstrate that an attacker can infer hidden AI text signatures without white-box access to the detection method, potentially leading to reputational risks for LLM developers."
  - [section 4] "We use a small value of N = 181 for our experiments. The attacker can query the watermarked LLM multiple times to learn the pair-wise occurrences of these N words in the LLM output."

## Foundational Learning

- Concept: Total variation distance
  - Why needed here: It's the key theoretical measure used to bound the detection performance of AI-generated text.
  - Quick check question: If the total variation distance between human and AI-generated text distributions is 0.1, what is the theoretical upper bound on the AUROC of the best possible detector?

- Concept: Paraphrasing and semantic similarity
  - Why needed here: The attack relies on generating paraphrased text that maintains semantic meaning while altering statistical properties.
  - Quick check question: If a paraphraser reduces the detection accuracy from 97% to 80% while only degrading perplexity by 3.5, what does this tell us about the relationship between semantic similarity and statistical detectability?

- Concept: Watermarking schemes and green lists
  - Why needed here: Understanding how watermarking works is crucial for understanding the spoofing attack.
  - Quick check question: In a soft watermarking scheme, if a token is selected from a green list with high probability, what does this mean for the statistical properties of watermarked text?

## Architecture Onboarding

- Component map:
  - Source LLM (e.g., OPT-1.3B) -> Paraphraser (e.g., PEGASUS-based) -> Detectors (various types) -> Query interface

- Critical path:
  1. Generate text with source LLM
  2. Apply paraphrasing attack
  3. Evaluate detection evasion

- Design tradeoffs:
  - Between detection performance and LLM utility (as per theoretical result)
  - Between paraphrase quality (semantic preservation) and detection evasion
  - Between watermarking robustness and text quality

- Failure signatures:
  - Detection accuracy remains high after paraphrasing
  - Paraphrased text loses semantic meaning
  - Watermarked text is easily distinguishable from human text

- First 3 experiments:
  1. Test paraphrasing attack on watermarked text using PEGASUS-based paraphraser
  2. Evaluate non-watermarked detectors (e.g., DetectGPT) on paraphrased GPT-2 output
  3. Demonstrate spoofing attack by composing human text detected as watermarked

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental limitations of AI-generated text detection as language models improve?
- Basis in paper: [explicit] The paper proves that for sufficiently advanced LLMs, even the best detector can only marginally outperform random guessing due to diminishing total variation distance between human and AI-generated text distributions.
- Why unresolved: This is a theoretical result showing an upper bound on detection performance, but it doesn't provide specific thresholds or practical guidelines for when detection becomes fundamentally unreliable.
- What evidence would resolve it: Empirical studies measuring total variation distance between human and AI-generated text distributions across different LLM architectures and sizes, identifying the point where detection performance approaches random chance.

### Open Question 2
- Question: How effective are current paraphrasing attacks against watermarking schemes in practical scenarios?
- Basis in paper: [explicit] The paper demonstrates that paraphrasing attacks can reduce watermarking detector accuracy from 97% to 57% while only slightly degrading text quality.
- Why unresolved: While the paper shows effectiveness of paraphrasing attacks, it doesn't explore the full range of potential countermeasures or more sophisticated paraphrasing techniques that might be developed.
- What evidence would resolve it: Comprehensive testing of various paraphrasing methods against different watermarking schemes, including adaptive watermarking that could resist paraphrasing, and evaluation of potential countermeasures.

### Open Question 3
- Question: What are the risks and practical implications of spoofing attacks on watermarking schemes?
- Basis in paper: [explicit] The paper shows that adversaries can infer watermark signatures and create human text that gets detected as AI-generated, potentially damaging LLM developers' reputations.
- Why unresolved: The paper provides proof-of-concept attacks but doesn't fully explore the scale of potential damage or the practical limitations of such attacks in real-world scenarios.
- What evidence would resolve it: Real-world case studies of spoofing attacks, analysis of potential damage scenarios, and evaluation of the ease with which attackers could execute such attacks at scale.

## Limitations

- The theoretical results depend on assumptions about total variation distance as a measure of detectability
- The recursive paraphrasing attack's quality degradation over multiple iterations is not fully characterized
- The practical feasibility of watermark spoofing attacks in real-world scenarios with rate limiting is unclear

## Confidence

**High Confidence**: The experimental results showing that paraphrasing attacks can reduce detection accuracy while maintaining text quality are well-supported by the data presented. The AUROC scores and perplexity measurements provide concrete evidence for the effectiveness of the attack.

**Medium Confidence**: The theoretical impossibility result showing that even the best possible detector can only marginally outperform random guessing is mathematically sound, but its practical implications depend on how well the total variation distance captures real-world detection challenges. The assumption that diminishing TV distance implies diminishing detectability is reasonable but not definitively proven for all detector types.

**Low Confidence**: The practical feasibility of the watermark spoofing attack is uncertain. While the theoretical framework is presented, the actual implementation details of how an attacker would efficiently learn the watermarking scheme's probability distributions in a real-world setting are not fully explored. The scalability and robustness of this attack against adaptive watermarking schemes is questionable.

## Next Checks

1. **Sensitivity Analysis**: Conduct experiments varying the number of paraphrasing iterations to quantify how detection evasion scales with quality degradation. Measure the point at which semantic preservation becomes unacceptably compromised.

2. **Real-World Attack Simulation**: Implement a realistic watermark spoofing attack that includes practical constraints such as query rate limiting, token usage costs, and adaptive watermarking schemes that change their green list probabilities based on query patterns.

3. **Detector Diversity Testing**: Evaluate the paraphrasing attack against a broader range of detection methods including recent advances in statistical fingerprinting, embedding-based classifiers, and hybrid detection systems that combine multiple approaches.