---
ver: rpa2
title: 'FigGen: Text to Scientific Figure Generation'
arxiv_id: '2306.00800'
source_url: https://arxiv.org/abs/2306.00800
tags:
- text
- arxiv
- diffusion
- image
- figures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FigGen, a diffusion-based approach for text-to-figure
  generation, which aims to create scientific figures from text descriptions. The
  authors train a latent diffusion model on the Paper2Fig100k dataset, consisting
  of figure-text pairs from research papers.
---

# FigGen: Text to Scientific Figure Generation

## Quick Facts
- arXiv ID: 2306.00800
- Source URL: https://arxiv.org/abs/2306.00800
- Reference count: 10
- Key outcome: FigGen achieves FID 45.2, IS 3.1, KID 0.08, OCR-SIM 0.35 on Paper2Fig100k dataset but generated figures lack practical utility for researchers

## Executive Summary
FigGen introduces a diffusion-based approach for generating scientific figures from text descriptions. The system uses a latent diffusion model trained on the Paper2Fig100k dataset, combining a BERT-based text encoder with a U-Net architecture. While the model successfully learns relationships between figures and their textual descriptions, the generated outputs currently fall short of practical research utility, particularly in areas requiring precise data visualization and text rendering.

## Method Summary
The FigGen system employs a latent diffusion model operating in compressed latent space, using a U-Net architecture conditioned on BERT text embeddings through cross-attention mechanisms. The model is trained on the Paper2Fig100k dataset of figure-text pairs, with an image autoencoder incorporating OCR perceptual loss. Training uses classifier-free guidance to balance text alignment with generation diversity, though the current implementation produces figures that, while learning statistical relationships, lack the precision required for scientific communication.

## Key Results
- Achieved FID 45.2, IS 3.1, KID 0.08, and OCR-SIM 0.35 on validation set
- Demonstrated learned relationships between figures and text descriptions
- Generated figures show poor practical utility for research purposes
- BERT-based text encoder outperforms general-purpose alternatives like CLIP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent diffusion models can learn complex mappings from text descriptions to structured scientific figures by capturing high-level visual concepts through denoising in compressed latent space.
- Mechanism: The model learns to denoise corrupted latent representations conditioned on text embeddings, gradually reconstructing images that align with semantic content described in the captions.
- Core assumption: The compressed latent space preserves enough semantic structure for figure generation while reducing computational complexity.
- Evidence anchors:
  - [abstract] "we explore diffusion models to generate scientific figures"
  - [section] "We train a latent diffusion model (Rombach et al., 2021) from scratch... The diffusion model interacts directly in the latent space"
  - [corpus] Weak - related papers focus on captioning rather than generation, missing direct evidence for this mechanism
- Break condition: If latent compression discards essential structural information needed for scientific figures, such as discrete component relationships or text rendering quality.

### Mechanism 2
- Claim: BERT-based text encoders trained from scratch on technical descriptions capture domain-specific terminology better than general-purpose encoders like CLIP.
- Mechanism: The BERT encoder learns technical vocabulary and context during diffusion training, creating embeddings that better align with scientific figure semantics.
- Core assumption: Technical language in scientific papers requires specialized understanding not present in general multimodal models.
- Evidence anchors:
  - [abstract] "We find that using a general-purpose text encoder (e.g., CLIP) is not well-suited for our task"
  - [section] "We define Bert (Devlin et al., 2018) transformer that is trained from scratch during the diffusion process"
  - [corpus] Assumption: Related papers don't directly address text encoder specialization for scientific domains
- Break condition: If the BERT encoder overfits to training corpus patterns without generalizing to new scientific domains or figure types.

### Mechanism 3
- Claim: Classifier-free guidance improves conditional alignment between text descriptions and generated figures by balancing text conditioning strength.
- Mechanism: Varying CFG scales adjusts how strongly the diffusion process is guided by text embeddings, with higher values producing more text-aligned outputs.
- Core assumption: There exists an optimal balance between text alignment and generation diversity that can be controlled through guidance scales.
- Evidence anchors:
  - [abstract] "we use classifier-free guidance (CFG) to test super-conditioning (Ho & Salimans, 2022)"
  - [section] "We use classifier-free guidance (CFG) to test super-conditioning... Table 1 presents results of different text encoders, and Figure 1 shows generated samples"
  - [corpus] Assumption: Related papers focus on captioning quality metrics rather than generation alignment techniques
- Break condition: If CFG tuning causes mode collapse or excessive text alignment that reduces figure diversity and quality.

## Foundational Learning

- Concept: Diffusion probabilistic models and denoising processes
  - Why needed here: Understanding how latent diffusion reverses noise corruption is essential for grasping the core generation mechanism
  - Quick check question: What is the relationship between noise schedule and image quality in diffusion models?

- Concept: Latent space representation and compression trade-offs
  - Why needed here: The model operates in compressed latent space, requiring understanding of what information is preserved vs lost
  - Quick check question: How does the compression factor (f=8) affect the ability to generate detailed scientific figures?

- Concept: Cross-attention mechanisms for text-image conditioning
  - Why needed here: The BERT embeddings interact with the U-Net through cross-attention layers to guide image generation
  - Quick check question: How does the embedding dimension (512) relate to the model's ability to capture complex figure-text relationships?

## Architecture Onboarding

- Component map: Image autoencoder (encoder/decoder with KL + perceptual losses) → Latent diffusion U-Net (BERT encoder → cross-attention → denoising blocks) → Generated figures
- Critical path: Text encoding → cross-attention conditioning → latent denoising → image decoding
- Design tradeoffs:
  - Latent compression (speed vs detail preservation)
  - Text encoder depth (parameter efficiency vs semantic capture)
  - CFG scale (alignment vs diversity)
- Failure signatures:
  - Poor text rendering: Issues in image autoencoder OCR perceptual loss
  - Misaligned content: Inadequate BERT training or CFG tuning
  - Low quality: Insufficient diffusion steps or poor noise schedule
- First 3 experiments:
  1. Ablation study: Remove OCR perceptual loss to assess impact on text rendering quality
  2. CFG sweep: Generate samples across CFG scales to find optimal alignment-diversity balance
  3. Encoder comparison: Train with CLIP vs BERT encoders to validate domain-specific advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal text encoder architecture and training strategy for scientific figure generation?
- Basis in paper: [explicit] The paper explores using BERT as the text encoder and finds it more suitable than general-purpose encoders like CLIP due to the domain gap with technical descriptions. It also experiments with different BERT sizes (8, 32, 128 layers).
- Why unresolved: The paper shows that larger BERT models perform better, but it doesn't explore other architectures or training strategies. The optimal text encoder design for this specific task remains unknown.
- What evidence would resolve it: A systematic comparison of different text encoder architectures (e.g., T5, RoBERTa, custom designs) and training strategies (e.g., pretraining on scientific text corpora, multi-task learning) on the same FigGen framework and dataset.

### Open Question 2
- Question: How can we effectively evaluate the quality and utility of generated scientific figures?
- Basis in paper: [explicit] The paper uses FID, IS, KID, and OCR-SIM as evaluation metrics, but notes that qualitative samples are not yet useful for researchers. It highlights the need to design validation metrics and loss functions for generative models of discrete objects.
- Why unresolved: Current metrics may not capture the specific requirements of scientific figure generation, such as accuracy of rendered text, clarity of diagrams, and relevance to the text description. There's no established evaluation framework for this task.
- What evidence would resolve it: Development and validation of a comprehensive evaluation framework that includes both automated metrics and human assessment, covering aspects like visual quality, textual accuracy, semantic relevance, and utility for researchers.

### Open Question 3
- Question: What are the most effective ways to improve alignment between text descriptions and generated figures?
- Basis in paper: [explicit] The paper identifies the variability in text and images and the need for better alignment between modalities as a main challenge. It shows that increasing classifier-free guidance improves alignment but doesn't explore other methods.
- Why unresolved: The paper only experiments with one conditioning technique (classifier-free guidance). There may be more effective ways to align text and images in the generation process.
- What evidence would resolve it: A comparative study of different conditioning techniques (e.g., classifier guidance, attention mechanisms, multi-modal embeddings) and alignment strategies (e.g., contrastive learning, cycle consistency) on the same FigGen framework and dataset.

## Limitations

- Generated figures lack sufficient quality for practical research use, particularly in precise data visualization and mathematical notation
- Latent compression factor of 8 may discard critical structural information needed for scientific figures
- Current evaluation metrics may not adequately capture requirements specific to scientific figure generation

## Confidence

**High confidence**: The core mechanism of using latent diffusion models for text-to-figure generation is technically sound and supported by experimental results showing learned relationships between figures and text descriptions.

**Medium confidence**: The choice of BERT-based text encoder over general-purpose alternatives is justified by the paper's findings, though evidence could be stronger with more comprehensive comparisons.

**Low confidence**: Claims about the model's ability to generalize beyond the training distribution and produce practically useful scientific figures require further validation, as the paper itself acknowledges current quality limitations.

## Next Checks

1. **Domain transfer experiment**: Evaluate FigGen's performance on scientific figures from disciplines not represented in Paper2Fig100k, such as chemistry reaction diagrams or mathematical proofs, to assess generalization capability.

2. **Expert usability study**: Conduct a structured evaluation with domain experts who assess whether generated figures meet minimum standards for publication and practical research use, focusing on accuracy of data representation and semantic correctness.

3. **Component ablation study**: Systematically remove and test individual components (OCR perceptual loss, CFG guidance, BERT encoder) to quantify their specific contributions to figure quality and text alignment, identifying critical bottlenecks in the generation pipeline.