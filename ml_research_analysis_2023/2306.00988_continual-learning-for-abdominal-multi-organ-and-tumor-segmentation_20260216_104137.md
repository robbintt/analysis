---
ver: rpa2
title: Continual Learning for Abdominal Multi-Organ and Tumor Segmentation
arxiv_id: '2306.00988'
source_url: https://arxiv.org/abs/2306.00988
tags:
- learning
- segmentation
- classes
- continual
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continual learning for abdominal
  multi-organ and tumor segmentation in CT images, where privacy regulations prevent
  access to previous data. The authors propose a method that uses high-quality pseudo
  labels to mitigate catastrophic forgetting and introduces a novel architecture with
  class-specific heads for independent predictions.
---

# Continual Learning for Abdominal Multi-Organ and Tumor Segmentation

## Quick Facts
- arXiv ID: 2306.00988
- Source URL: https://arxiv.org/abs/2306.00988
- Authors: 
- Reference count: 30
- Primary result: Proposes method using pseudo labels and class-specific heads with CLIP embeddings to address catastrophic forgetting in multi-organ and tumor segmentation

## Executive Summary
This paper addresses the challenge of continual learning for abdominal multi-organ and tumor segmentation in CT images, where privacy regulations prevent access to previous data. The authors propose a method that uses high-quality pseudo labels to mitigate catastrophic forgetting and introduces a novel architecture with class-specific heads for independent predictions. The method incorporates CLIP embeddings to enhance semantic understanding. Evaluated on in-house and public datasets, the approach outperforms baseline methods, achieving higher segmentation performance on both newly-introduced and previously-learned classes while maintaining memory efficiency.

## Method Summary
The method employs Swin UNETR as backbone with class-specific heads for each organ class, replacing the standard Softmax output layer. Each head consists of an MLP that generates convolution parameters conditioned on CLIP text embeddings of organ names, concatenated with global features from the backbone. Training uses pseudo labels generated from previous step predictions for old classes, with independent binary cross-entropy loss per class. The approach requires minimal additional parameters (661.6 GFLOPs vs 659.4 GFLOPs for baseline) while maintaining semantic priors through CLIP embeddings.

## Key Results
- Outperforms baseline methods (LwF, ILT, PLOP) on all three datasets with consistent gains across learning steps
- Achieves higher Dice scores on both newly-introduced and previously-learned classes compared to competitors
- Maintains memory efficiency with minimal parameter increase while preserving semantic understanding through CLIP embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality pseudo labels for old classes on new data preserve previous knowledge during continual learning
- Mechanism: The model generates pseudo labels by using predictions from the previous learning step as soft targets for old classes on new data. This allows the model to "remember" previously learned classes without storing the original data
- Core assumption: The pseudo labels generated from the previous step are sufficiently accurate to guide learning without introducing significant noise
- Evidence anchors:
  - [abstract]: "We first empirically demonstrate that simply using high-quality pseudo labels can fairly mitigate this problem in the setting of organ segmentation."
  - [section 2.1]: "We found the use of pseudo-labeling can largely mitigate this issue and preserve the existing knowledge."
- Break condition: If pseudo label quality degrades significantly between steps, the forgetting problem will resurface and performance on old classes will collapse

### Mechanism 2
- Claim: Class-specific heads with independent predictions minimize interference between old and new classes
- Mechanism: Instead of a single output layer, the model uses separate lightweight heads for each class. These heads process shared decoder features independently, allowing new classes to learn without disrupting old class representations
- Core assumption: Independent heads can learn class-specific features effectively while sharing a common backbone
- Evidence anchors:
  - [section 2.2]: "These heads enable independent predictions for newly introduced and previously learned classes, effectively minimizing the impact of new classes on old ones during the course of continual learning."
  - [section 2.3]: "This is because we used lightweight output convolution heads with a small number of channels."
- Break condition: If heads become too interdependent or if shared backbone features become overly class-specific, interference will increase and the independence benefit will diminish

### Mechanism 3
- Claim: CLIP text embeddings provide semantic priors that improve segmentation quality for both old and new classes
- Mechanism: CLIP embeddings capture semantic relationships between organ names and visual features. These embeddings guide the MLP modules to generate better convolution parameters for each organ-specific head, improving segmentation accuracy
- Core assumption: CLIP's learned semantic representations transfer effectively to medical image domains and organ segmentation tasks
- Evidence anchors:
  - [abstract]: "We further propose incorporating Contrastive Language-Image Pretraining (CLIP) embeddings into the organ-specific heads. These embeddings encapsulate the semantic information of each class, informed by extensive image-text co-training."
  - [section 2.2]: "CLIP embeddings carry high-level semantic meanings and have the ability to connect correlated concepts. Therefore, it guides the MLP module to generate better convolution parameters for each organ class."
- Break condition: If CLIP embeddings don't capture relevant medical semantics or if the text-image alignment in medical context differs significantly from CLIP's pretraining data, the performance benefit will disappear

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper explicitly addresses this problem as the core challenge being solved
  - Quick check question: What happens to a model's performance on old classes when trained on new classes without any mitigation strategies?

- Concept: Knowledge distillation and pseudo-labeling
  - Why needed here: The method uses pseudo labels as a simple form of knowledge transfer without needing stored data
  - Quick check question: How does using predictions from a previous model as training targets help prevent forgetting?

- Concept: Contrastive learning and vision-language models
  - Why needed here: CLIP embeddings are used to provide semantic priors for segmentation heads
  - Quick check question: What is the key insight behind CLIP that allows it to perform zero-shot classification?

## Architecture Onboarding

- Component map: Input image -> Swin UNETR backbone -> GAP layer on last encoder features -> Concatenate with CLIP embeddings -> MLP modules generate class-specific parameters -> Apply convolution heads to decoder features -> Binary Cross Entropy loss per class

- Critical path:
  1. Encode input image through backbone
  2. Extract global feature via GAP
  3. Concatenate with CLIP embedding
  4. Generate class-specific parameters through MLP
  5. Apply convolution heads to decoder features
  6. Compute BCE loss for each class independently

- Design tradeoffs:
  - Memory efficiency vs. parameter sharing: Separate heads increase parameters slightly but allow independent learning
  - Semantic priors vs. learned features: CLIP provides strong priors but may limit adaptation to medical domain specifics
  - Simplicity vs. performance: Pseudo labeling is simple but may be less effective than complex distillation methods

- Failure signatures:
  - Poor performance on old classes in later steps → pseudo label quality degradation
  - Slow convergence on new classes → insufficient semantic guidance from CLIP
  - High parameter count growth → inefficient head design

- First 3 experiments:
  1. Baseline test: Run Swin UNETR on BTCV → LiTS with only pseudo labeling, measure forgetting
  2. Head isolation test: Compare single vs. multiple heads on a small organ subset, measure interference
  3. CLIP ablation: Train with and without CLIP embeddings on organ segmentation, measure semantic benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are pseudo labels in continual learning compared to other knowledge distillation techniques?
- Basis in paper: [explicit] The paper states that "simply using high-quality pseudo labels can fairly mitigate this problem in the setting of organ segmentation."
- Why unresolved: The paper compares the proposed method with other distillation techniques (LwF, ILT, PLOP) but does not provide a comprehensive ablation study comparing pseudo labeling with these methods
- What evidence would resolve it: A detailed ablation study comparing the performance of pseudo labeling with other distillation techniques on the same datasets and tasks

### Open Question 2
- Question: How does the proposed method perform in scenarios with more than three learning steps?
- Basis in paper: [inferred] The paper evaluates the method on three learning steps (organ, gastrointestinal, cardiac) but does not test its performance with additional steps
- Why unresolved: The paper does not provide results for more than three learning steps, leaving the scalability of the method untested
- What evidence would resolve it: Experimental results showing the performance of the method on datasets with more than three learning steps

### Open Question 3
- Question: How does the CLIP text embedding contribute to the segmentation performance compared to using one-hot encodings?
- Basis in paper: [explicit] The paper mentions that "CLIP embeddings carry high-level semantic meanings and have the ability to connect correlated concepts" and compares the proposed method with one using one-hot embeddings
- Why unresolved: While the paper shows improvement with CLIP embeddings, it does not provide a detailed analysis of how much each aspect of CLIP (semantic information, correlated concepts) contributes to the performance
- What evidence would resolve it: A detailed ablation study isolating the contributions of semantic information and correlated concepts in CLIP embeddings to the segmentation performance

## Limitations
- Pseudo-labeling approach assumes consistent pseudo-label quality across all learning steps, but the paper doesn't report degradation metrics over time
- CLIP embeddings are claimed to provide semantic priors, but there's no ablation study showing the performance drop without CLIP
- The in-house JHH dataset is not publicly available, limiting independent verification of the primary results

## Confidence

- **High confidence**: The catastrophic forgetting problem is well-established in continual learning, and the proposed architecture with class-specific heads follows standard design patterns for addressing interference between tasks
- **Medium confidence**: The pseudo-labeling mechanism is supported by empirical demonstration, but lacks quantitative analysis of pseudo-label quality or comparison with alternative knowledge distillation methods
- **Low confidence**: The CLIP embedding integration claims semantic benefits, but lacks rigorous ablation studies or analysis of what specific semantic relationships are captured and how they translate to segmentation performance

## Next Checks

1. **Pseudo-label quality validation**: Implement a systematic evaluation of pseudo-label accuracy across all learning steps, measuring how pseudo-label quality degrades over time and quantifying its impact on forgetting

2. **CLIP ablation study**: Train identical architectures with and without CLIP embeddings across all three datasets to isolate the semantic contribution and determine if the performance benefit justifies the additional complexity

3. **Baseline comparison completeness**: Re-run the experiments with additional continual learning baselines (e.g., Elastic Weight Consolidation, Synaptic Intelligence) to establish whether the simple pseudo-labeling approach is truly competitive with more sophisticated methods