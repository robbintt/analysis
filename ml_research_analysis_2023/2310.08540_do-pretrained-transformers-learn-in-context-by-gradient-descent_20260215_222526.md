---
ver: rpa2
title: Do pretrained Transformers Learn In-Context by Gradient Descent?
arxiv_id: '2310.08540'
source_url: https://arxiv.org/abs/2310.08540
tags:
- demonstrations
- arxiv
- epoch
- uni00000048
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the claim that in-context learning (ICL)
  in large language models is equivalent to gradient descent (GD). The authors highlight
  limitations in prior theoretical works that establish this equivalence, including
  unrealistic assumptions about training objectives, model weights, and task spaces.
---

# Do pretrained Transformers Learn In-Context by Gradient Descent?

## Quick Facts
- arXiv ID: 2310.08540
- Source URL: https://arxiv.org/abs/2310.08540
- Reference count: 40
- This paper challenges the claim that in-context learning in large language models is equivalent to gradient descent

## Executive Summary
This paper critically examines the hypothesis that in-context learning (ICL) in pretrained Transformers functions equivalently to gradient descent. Through both theoretical analysis and empirical comparisons, the authors demonstrate that ICL and GD exhibit different sensitivities to demonstration order, produce different output distributions, and rely on weight matrices that don't match theoretical requirements. The study provides substantial evidence against the equivalence claim across various datasets, model sizes, and evaluation metrics.

## Method Summary
The study compares in-context learning (few-shot prompting) with gradient descent fine-tuning on identical demonstrations using LLaMa-7B and GPT-J models across four classification datasets. The authors evaluate performance using three metrics: accuracy, token overlap, and overlap cosine similarity. They systematically vary the number of demonstrations and analyze how ICL and GD behave differently with respect to demonstration order and output distributions. Additionally, they examine the sparsity characteristics of weight matrices in real-world models compared to theoretical requirements.

## Key Results
- ICL and GD show different sensitivities to demonstration order across all tested datasets and model sizes
- The output distributions produced by ICL and GD are significantly different, even when achieving similar performance
- Real-world transformer models have weight matrices with much lower sparsity than theoretically required for ICL to simulate GD
- The performance gap between ICL and GD does not systematically decrease with increasing model size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL in pretrained Transformers is not equivalent to gradient descent
- Mechanism: ICL and GD have different sensitivities to demonstration order and produce different output distributions
- Core assumption: ICL emerges from pretraining on natural language data, while GD requires fine-tuning on specific tasks
- Evidence anchors:
  - [abstract]: "Our comparisons of three performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and the number of demonstrations."
  - [section 3.2.1]: "ICL can not be equivalent to GD (arrow 1 in Figure 1)."
- Break condition: If ICL and GD show similar sensitivity to demonstration order and produce similar output distributions, this mechanism would be invalid

### Mechanism 2
- Claim: Real-world pretrained Transformers do not arrive at the sparse weight matrices required for ICL to simulate GD
- Mechanism: The paper shows that the weight matrices constructed by prior works to prove ICL ≈ GD equivalence are much sparser than those found in real-world models like LLaMa and GPT-J
- Core assumption: The sparsity ratio of weight matrices is a key factor in ICL's ability to simulate GD
- Evidence anchors:
  - [section 3.3]: "The sparsity ratio in WV should be close to ≈ 75% if we assume each element in W0 to be non-zero. In practice, we find that the sparsity ratio is much lower for real-world models like LLaMa and GPT-J."
  - [figure 4]: Shows the sparsity ratio in LLaMA averaged across layers is much less than required by previous works to implement GD
- Break condition: If real-world models are found to have weight matrices with the required sparsity ratio, this mechanism would be invalid

### Mechanism 3
- Claim: The equivalence between ICL and GD is an open hypothesis that requires nuanced considerations
- Mechanism: The paper argues that the gap between ICL and GD in realistic settings is significant and does not change with model size, indicating that the equivalence is not a universal property of pretrained Transformers
- Core assumption: The gap between ICL and GD is a reliable indicator of their functional equivalence or lack thereof
- Evidence anchors:
  - [abstract]: "Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations."
  - [figure 6]: Shows the substantial gap between ICL and GD does not seem to change with model size
- Break condition: If the gap between ICL and GD is found to be negligible or consistent across different settings, this mechanism would be invalid

## Foundational Learning

- Concept: Gradient descent (GD)
  - Why needed here: The paper compares ICL to GD and argues against their equivalence
  - Quick check question: What is the key difference between the standard definition of GD and the variant used in prior works to prove ICL ≈ GD equivalence?

- Concept: In-context learning (ICL)
  - Why needed here: The paper studies the dynamics of ICL and compares it to GD
  - Quick check question: How does ICL differ from standard fine-tuning in terms of model updates?

- Concept: Transformer architecture
  - Why needed here: The paper analyzes the weight matrices of Transformers to understand their ability to simulate GD
  - Quick check question: What are the key components of a Transformer architecture, and how do they contribute to its function?

## Architecture Onboarding

- Component map:
  Pretrained Transformers (LLaMa, GPT-J) -> In-context demonstrations -> Fine-tuning with GD -> Evaluation metrics (performance, token overlap, confidence overlap)

- Critical path:
  1. Pretrain Transformer on natural language data
  2. Provide in-context demonstrations to the pretrained model
  3. Fine-tune the model on the same demonstrations using GD
  4. Compare the outputs of ICL and GD using various metrics

- Design tradeoffs:
  - Tradeoff between model size and ICL performance
  - Tradeoff between number of demonstrations and ICL performance
  - Tradeoff between learning rate and convergence speed in GD

- Failure signatures:
  - High sensitivity to demonstration order in ICL but not in GD
  - Large gap between ICL and GD performance across different datasets and model sizes
  - Weight matrices in real-world models not matching the sparsity requirements for ICL ≈ GD equivalence

- First 3 experiments:
  1. Compare ICL and GD sensitivity to demonstration order using LLaMa-7B on AGNews dataset
  2. Analyze the sparsity of weight matrices in LLaMa and GPT-J and compare to theoretical requirements
  3. Evaluate the gap between ICL and GD performance across different model sizes and datasets using various metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do Transformers implicitly implement gradient descent during in-context learning, or is this behavior dependent on specific model architectures or training objectives?
- Basis in paper: [explicit] The paper argues against the equivalence of ICL and GD, particularly challenging the hypothesis that Transformers learn in-context by gradient descent
- Why unresolved: The paper highlights limitations in prior theoretical works and empirical evidence showing that ICL and GD operate differently in real-world settings, but it does not conclusively prove or disprove the existence of implicit models that simulate gradient descent
- What evidence would resolve it: Detailed analysis of Transformer architectures to identify potential implicit models or mechanisms that could simulate gradient descent, and empirical studies comparing ICL behavior across different model architectures and training objectives

### Open Question 2
- Question: How do the distributional properties of pretraining data influence the emergent ability of in-context learning in Transformers?
- Basis in paper: [explicit] The paper mentions that recent works have explained ICL via distributional frameworks and the relevant properties of LLMs, suggesting that distributional properties might play a role in ICL
- Why unresolved: While the paper acknowledges the role of distributional properties, it does not provide a comprehensive analysis of how specific properties of pretraining data contribute to ICL
- What evidence would resolve it: Systematic experiments varying the distributional properties of pretraining data and measuring their impact on ICL performance and behavior

### Open Question 3
- Question: Can alternative optimization algorithms, such as accelerated gradient descent or adaptive methods, explain the dynamics of in-context learning in Transformers?
- Basis in paper: [inferred] The paper discusses the equivalence of ICL and GD but does not explore other variants of gradient descent or higher-order optimization methods
- Why unresolved: The paper focuses on vanilla gradient descent and its equivalence to ICL, leaving open the question of whether other optimization algorithms might better explain ICL dynamics
- What evidence would resolve it: Comparative studies of ICL behavior under different optimization algorithms, including accelerated and adaptive methods, to determine if they align more closely with ICL dynamics

## Limitations

- The study focuses primarily on LLaMa and GPT-J architectures, limiting generalizability to other transformer variants
- Empirical analysis is restricted to classification tasks, excluding regression, structured prediction, and other task types
- The fine-tuning protocol uses fixed hyperparameters (200 epochs, specific learning rates) without exploring sensitivity to these choices

## Confidence

- High Confidence: ICL and GD show different sensitivities to demonstration order across multiple datasets and model sizes
- Medium Confidence: Real-world weight matrices lack the sparsity required for theoretical ICL ≈ GD equivalence in LLaMa and GPT-J
- Medium Confidence: The performance gap between ICL and GD does not systematically decrease with model size

## Next Checks

**Check 1: Cross-Architecture Validation**
- Test ICL vs GD comparison on additional transformer architectures (BERT, RoBERTa, OPT, BLOOM) and model families (GPT-3, Claude, etc.)
- Validate whether demonstration order sensitivity and output distribution differences persist across diverse architectures

**Check 2: Hyperparameter Sensitivity Analysis**
- Systematically vary fine-tuning hyperparameters (epochs, learning rates, batch sizes) for GD comparisons
- Assess whether different fine-tuning protocols can reduce or eliminate observed gaps between ICL and GD

**Check 3: Alternative Task Types**
- Extend empirical comparisons to include regression tasks, sequence-to-sequence tasks, and structured prediction problems
- Determine whether ICL vs GD relationship varies significantly across different task modalities beyond classification