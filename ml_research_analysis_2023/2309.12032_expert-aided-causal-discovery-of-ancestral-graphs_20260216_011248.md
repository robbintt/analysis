---
ver: rpa2
title: Expert-Aided Causal Discovery of Ancestral Graphs
arxiv_id: '2309.12032'
source_url: https://arxiv.org/abs/2309.12032
tags:
- agfn
- causal
- expert
- distribution
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Causal discovery is brittle under latent confounding and scarce
  data, producing unreliable single-graph estimates without uncertainty quantification.
  We address this by proposing Ancestral GFlowNets (AGFN), a method that samples ancestral
  graphs proportionally to a score-based belief distribution.
---

# Expert-Aided Causal Discovery of Ancestral Graphs

## Quick Facts
- arXiv ID: 2309.12032
- Source URL: https://arxiv.org/abs/2309.12032
- Reference count: 13
- Primary result: AGFN samples ancestral graphs proportionally to a score-based belief distribution and outperforms non-probabilistic baselines in structural Hamming distance and BIC scores when incorporating expert feedback.

## Executive Summary
This paper addresses the challenge of causal discovery under latent confounding and scarce data by proposing Ancestral GFlowNets (AGFN), a method that samples ancestral graphs proportionally to a score-based belief distribution. The approach combines probabilistic inference with active expert elicitation, using an optimal experimental design to query variable pairs that maximize uncertainty reduction. Human feedback is incorporated via importance sampling without retraining, making the framework scalable and efficient. Experiments demonstrate that AGFN accurately approximates the target distribution over ancestral graphs and substantially improves inference quality when human feedback is integrated.

## Method Summary
AGFN uses GFlowNets to sample ancestral graphs (AGs) according to a score function (BIC), building graphs iteratively through edge additions while respecting AG constraints. The method employs optimal experimental design to select variable pairs for expert queries that maximize expected cross-entropy reduction. Expert feedback, modeled as noisy categorical observations, is incorporated through importance sampling without retraining the GFlowNet. The framework trains on synthetic observational data from linear Gaussian structural causal models and validates performance using structural Hamming distance (SHD) and BIC scores.

## Key Results
- AGFN accurately approximates the target distribution over ancestral graphs on synthetic data
- Outperforms non-probabilistic baselines (FCI, GFCI, DCD) in structural Hamming distance and BIC scores
- Achieves faster reduction in expected BIC compared to random querying when incorporating expert feedback
- First iterative causal discovery framework under latent confounding that combines probabilistic inference with noisy expert interaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GFlowNet-based sampling produces diverse ancestral graphs proportionally to a score-based belief distribution
- Mechanism: AGFN learns a flow network where each trajectory corresponds to an ancestral graph built by iteratively adding edges. The reward function is a strictly decreasing transformation of BIC, ensuring high-scoring graphs are more likely to be sampled. The flow-matching condition ensures correct sampling proportions
- Core assumption: The compositional nature of ancestral graphs allows valid graph construction through iterative edge additions without cycles
- Evidence anchors: [abstract]: "AGFN samples ancestral graphs (AGs) proportionally to a score-based belief distribution"; [section]: "We propose AGFN, a GFlowNet-based method for sampling AGs using a score function"

### Mechanism 2
- Claim: Active expert feedback reduces uncertainty in the posterior over ancestral graphs
- Mechanism: AGFN uses an optimal experimental design that selects variable pairs maximizing the expected cross-entropy reduction between prior and posterior beliefs. Human feedback is modeled as noisy categorical observations and incorporated via importance sampling without retraining
- Core assumption: The cross-entropy acquisition function effectively identifies the most informative edge queries without requiring partition function estimation
- Evidence anchors: [abstract]: "We propose an experimental design to query potentially noisy expert insights on relationships among pairs of variables that lead to optimal uncertainty reduction"; [section]: "To make the most out of possibly costly human interactions, we query the human about the relation that maximally reduces the expected cross-entropy"

### Mechanism 3
- Claim: Importance sampling allows efficient posterior updates without retraining GFlowNets
- Mechanism: After collecting K feedbacks, AGFN weights existing samples by the product of posterior probabilities over relations given feedback. This creates a new belief distribution q(G; fK) without modifying the original GFlowNet parameters
- Core assumption: The original GFlowNet samples adequately cover the high-probability regions of the updated posterior
- Evidence anchors: [abstract]: "Human feedback is incorporated via importance sampling, avoiding retraining"; [section]: "We use importance sampling [Marshall, 1954, Geweke, 1989] to update our initial belief with the human feedback, which avoids retraining GFlowNets after each human interaction"

## Foundational Learning

- Concept: Ancestral graphs encode causal relationships including latent confounding through directed and bidirected edges
  - Why needed here: AGFN operates on the space of ancestral graphs, not DAGs, to handle unobserved confounders
  - Quick check question: How does a bidirected edge differ from a directed edge in terms of causal interpretation?

- Concept: Flow-matching condition ensures GFlowNets sample proportionally to the reward function
  - Why needed here: Without flow-matching, AGFN would not produce samples proportional to BIC scores
  - Quick check question: What equation must hold at each state for the flow-matching condition to be satisfied?

- Concept: Importance sampling reweights samples from a proposal distribution to approximate expectations under a target distribution
  - Why needed here: AGFN uses this to incorporate expert feedback without retraining the GFlowNet
  - Quick check question: What happens to the effective sample size when importance weights become highly variable?

## Architecture Onboarding

- Component map: AGFN consists of (1) GFlowNet with forward and backward flow networks, (2) score function (BIC) providing rewards, (3) expert feedback model with categorical distributions, (4) importance sampling module for posterior updates
- Critical path: Training → Sampling → Expert Query → Feedback → Importance Weighting → Inference
- Design tradeoffs: GFlowNets provide diversity but require careful architectural design; importance sampling avoids retraining but can suffer from weight degeneracy; active querying optimizes information gain but requires tractable acquisition computation
- Failure signatures: Non-ancestral graph sampling (action constraints fail), degenerate importance weights (posterior shift too large), poor expert acquisition (cross-entropy estimates inaccurate)
- First 3 experiments:
  1. Train AGFN on synthetic 5-node graphs and verify sample distribution matches theoretical BIC-based distribution
  2. Compare AGFN SHD/BIC against FCI and GFCI on chain4 dataset with 500 samples
  3. Simulate expert feedback with varying reliability (π) and measure cross-entropy reduction over iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the reliability parameter π be effectively calibrated in real-world expert elicitation scenarios?
- Basis in paper: [explicit] The paper discusses the sensitivity of the method to the reliability of the expert, suggesting that improvements become more effective as the expert's reliability increases from 0.1 to 0.9
- Why unresolved: The paper does not provide a concrete method for calibrating π, and it is unclear how to accurately assess the reliability of expert feedback in practice
- What evidence would resolve it: A study comparing the performance of the method with different calibration strategies for π, or a method for estimating π based on expert performance on known cases

### Open Question 2
- Question: Can the proposed method be extended to handle more complex data generating models beyond linear Gaussian SCMs?
- Basis in paper: [inferred] The paper focuses on linear Gaussian models and uses the BIC score, but mentions that the implementation of AGFNs is not restricted by this choice and could be adapted to different types of variables
- Why unresolved: The paper does not explore the extension of the method to non-linear or non-Gaussian models, and it is unclear how the score function and the generative process would need to be adapted
- What evidence would resolve it: Experiments demonstrating the performance of the method on non-linear or non-Gaussian data generating models, or a theoretical analysis of the requirements for extending the method

### Open Question 3
- Question: How does the performance of the method scale with the number of variables in the causal diagram?
- Basis in paper: [inferred] The paper mentions that AGFNs are GPU-powered and can be accelerated using cluster architectures, but does not provide an analysis of the scalability of the method
- Why unresolved: The paper does not report the performance of the method on causal diagrams with a large number of variables, and it is unclear how the computational complexity and the quality of the inferred causal structure would be affected
- What evidence would resolve it: Experiments comparing the performance of the method on causal diagrams with varying numbers of variables, or a theoretical analysis of the computational complexity of the method

## Limitations
- Method assumes linear Gaussian models and may not generalize well to non-linear or discrete data without modification
- Computational overhead of optimal experimental design may become prohibitive in high-dimensional variable spaces
- Sensitivity to expert reliability parameter π requires careful calibration that is not addressed in the paper

## Confidence
High: GFlowNet-based sampling with flow-matching condition for proportional sampling
Medium: Expert feedback incorporation via optimal experimental design and importance sampling
Medium: Overall performance claims based on synthetic data experiments

## Next Checks
1. Test AGFN on larger benchmark datasets (e.g., ALARM, CHILD networks) to evaluate scalability beyond 5-6 node graphs
2. Conduct sensitivity analysis on expert reliability parameter π to quantify robustness to noisy feedback
3. Compare wall-clock training and inference times against established FCI/GFCI implementations on identical hardware