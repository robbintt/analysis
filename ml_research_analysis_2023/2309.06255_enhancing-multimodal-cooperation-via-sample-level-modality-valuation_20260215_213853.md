---
ver: rpa2
title: Enhancing multimodal cooperation via sample-level modality valuation
arxiv_id: '2309.06255'
source_url: https://arxiv.org/abs/2309.06255
tags:
- modality
- contribution
- uni-modal
- multi-modal
- sample-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of imbalanced multi-modal learning,
  where one modality tends to dominate others, leading to poor cooperation between
  modalities. The authors introduce a sample-level modality valuation metric based
  on Shapley value from game theory to evaluate the contribution of each modality
  for each sample.
---

# Enhancing multimodal cooperation via sample-level modality valuation

## Quick Facts
- arXiv ID: 2309.06255
- Source URL: https://arxiv.org/abs/2309.06255
- Reference count: 18
- Key outcome: Introduces Shapley value-based sample-level modality valuation and targeted re-sampling to improve multimodal cooperation in imbalanced learning settings

## Executive Summary
This paper addresses the problem of imbalanced multimodal learning where one modality dominates others, leading to poor cooperation between modalities. The authors propose a novel sample-level modality valuation metric based on Shapley value from game theory to evaluate each modality's contribution per sample. They observe that modality discrepancy varies across samples and introduce methods to enhance the discriminative ability of low-contributing modalities through targeted re-sampling. The approach achieves considerable improvement in multimodal model performance across various datasets.

## Method Summary
The method introduces a sample-level modality valuation metric based on Shapley value to evaluate each modality's contribution for each sample. Low-contributing modalities are identified and their discriminative ability is enhanced through targeted re-sampling with frequency inversely proportional to their contribution. Two strategies are proposed: sample-level (higher accuracy but higher computational cost) and modality-level (lower cost but less precise targeting). The approach is validated on multimodal action recognition tasks using datasets with audio, visual, and optical flow modalities.

## Key Results
- Achieves considerable improvement in multimodal model performance across various datasets
- Demonstrates effectiveness of Shapley value-based valuation for identifying low-contributing modalities
- Shows targeted re-sampling enhances discriminative ability of previously underperforming modalities
- Code and dataset available at https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shapley value-based modality valuation accurately measures each modality's contribution to prediction per sample
- Mechanism: Computes marginal contribution by permuting modality inclusion order and averaging outcomes, ensuring fair allocation even when modalities vary in informativeness across samples
- Core assumption: Marginal contributions are non-negative and reflect true informativeness
- Evidence anchors:
  - [abstract] "introduce a sample-level modality valuation metric to evaluate the contribution of each modality for each sample"
  - [section] "the Shapley value in game theory... provides the theoretical support of our valuation"
- Break condition: If marginal contributions can be negative (e.g., contradictory modalities), valuation will misrepresent true contribution

### Mechanism 2
- Claim: Enhancing the discriminative ability of low-contributing modalities improves their contribution and overall multimodal cooperation
- Mechanism: Re-sampling low-contributing modalities with frequency inversely proportional to their contribution forces the model to strengthen those representations, balancing modality influence
- Core assumption: Re-sampling improves discriminative ability and is correlated with increased contribution
- Evidence anchors:
  - [abstract] "enhance the discriminative ability of low-contributing modalities in a targeted manner"
  - [section] "strengthening the discriminative ability of low-contributing modality is able to improve its contribution to multi-modal prediction"
- Break condition: If re-sampling does not improve discriminative ability or if it causes overfitting, contribution gains will not materialize

### Mechanism 3
- Claim: Sample-level and modality-level re-sampling strategies address fine-grained and coarse-grained modality discrepancy respectively
- Mechanism: Sample-level adjusts per-sample re-sampling frequency based on exact contribution; modality-level uses subset-based estimation to reduce cost while targeting overall low contributors
- Core assumption: Modality contribution discrepancy is consistent enough at dataset level to justify modality-level estimation
- Evidence anchors:
  - [abstract] "methods are proposed to enhance the discriminative ability of low-contributing modalities in a targeted manner"
  - [section] "we also propose the more efficient modality-wise method"
- Break condition: If contribution discrepancy is not consistent at dataset level, modality-level estimation will misidentify targets

## Foundational Learning

- Concept: Shapley value from cooperative game theory
  - Why needed here: Provides theoretical grounding for fair contribution allocation among modalities
  - Quick check question: What property ensures Shapley values fairly distribute benefits among players?

- Concept: Marginal contribution in permutation space
  - Why needed here: Enables calculation of each modality's contribution under all possible inclusion orders
  - Quick check question: How does averaging over all permutations prevent bias toward any specific modality?

- Concept: Imbalanced multimodal learning dynamics
  - Why needed here: Understanding why one modality dominates informs the need for targeted enhancement
  - Quick check question: What happens to overall multimodal performance when one modality overwhelmingly dominates?

## Architecture Onboarding

- Component map: Uni-modal encoders (RGB, optical flow, audio, text) -> Fusion layer (concatenation or cross-modal interaction module) -> Modality valuation module (Shapley-based contribution calculator) -> Re-sampling controller (frequency scheduler for low contributors)

- Critical path: Forward pass → Fusion → Prediction → Modality valuation → Identify low contributors → Adjust re-sampling schedule → Backward pass

- Design tradeoffs:
  - Sample-level: Higher accuracy in targeting low contributors but higher computational cost
  - Modality-level: Lower cost but less precise targeting, relies on dataset-level consistency
  - Re-sampling vs. gradient modulation: Re-sampling directly improves representation; gradient modulation adjusts training dynamics

- Failure signatures:
  - Low contributors remain low despite re-sampling: discriminative ability not improving
  - Performance drops after re-sampling: over-fitting to re-sampled data
  - No improvement over baseline: valuation not capturing true contribution

- First 3 experiments:
  1. Validate Shapley valuation against oracle contribution on synthetic multimodal data
  2. Compare sample-level vs modality-level re-sampling on UCF-101 with fixed budget
  3. Test different fs(·) and fm(·) functions to confirm robustness to design choices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed methods scale with increasing number of modalities beyond three?
- Basis in paper: [inferred] The paper mentions experiments on datasets with two and three modalities, but does not explore scenarios with more than three modalities
- Why unresolved: The paper does not provide experiments or analysis on datasets with more than three modalities, leaving the scalability of the proposed methods unclear
- What evidence would resolve it: Experiments on datasets with more than three modalities, such as multimodal datasets in the medical or autonomous driving domains, would help determine the scalability of the proposed methods

### Open Question 2
- Question: How does the proposed sample-level modality valuation method perform when applied to modalities with very different temporal resolutions, such as audio and video?
- Basis in paper: [inferred] The paper discusses the effectiveness of the sample-level method but does not address scenarios where modalities have different temporal resolutions
- Why unresolved: The paper does not provide experiments or analysis on modalities with different temporal resolutions, leaving the effectiveness of the sample-level method in such scenarios unclear
- What evidence would resolve it: Experiments on datasets with modalities of different temporal resolutions, such as audio and video, would help determine the effectiveness of the sample-level method in such scenarios

### Open Question 3
- Question: How does the proposed method perform when the modality discrepancy varies not only across samples but also across different classes or categories within the dataset?
- Basis in paper: [explicit] The paper mentions that modality discrepancy can vary across samples and provides an example of two samples from the same category having different modality contributions
- Why unresolved: The paper does not provide experiments or analysis on scenarios where modality discrepancy varies across classes or categories within the dataset
- What evidence would resolve it: Experiments on datasets with classes or categories that have different modality discrepancies would help determine the performance of the proposed method in such scenarios

## Limitations
- Limited evaluation scope - only tested on action recognition tasks with specific modalities (RGB, optical flow, audio), generalizability to other multimodal domains is uncertain
- No ablation studies on how the choice of function fs(·) and fm(·) affects final performance - these could be critical hyperparameters
- Shapley value computation requires O(n!) complexity for n modalities; computational optimizations or approximations are not clarified

## Confidence
- **High Confidence**: Shapley value provides theoretical grounding for fair contribution allocation among modalities
- **Medium Confidence**: Re-sampling low-contributing modalities improves their discriminative ability and overall multimodal performance (based on observed improvements but without mechanistic proof)
- **Low Confidence**: Modality-level re-sampling is sufficiently accurate compared to sample-level (no direct quantitative comparison provided)

## Next Checks
1. Conduct ablation studies varying fs(·) and fm(·) functions to identify optimal configurations and robustness to design choices
2. Perform computational complexity analysis of Shapley value calculation and evaluate approximation methods for scalability
3. Test transferability of the approach to different multimodal tasks (e.g., visual question answering, multimodal sentiment analysis) with different modality combinations to assess generalizability