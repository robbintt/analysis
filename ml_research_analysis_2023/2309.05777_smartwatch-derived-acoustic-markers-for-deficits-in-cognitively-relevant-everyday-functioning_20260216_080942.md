---
ver: rpa2
title: Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday
  Functioning
arxiv_id: '2309.05777'
source_url: https://arxiv.org/abs/2309.05777
tags:
- cognitive
- features
- acoustic
- everyday
- alzheimer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated using acoustic features from voice data
  collected via smartwatch to detect deficits in everyday functioning in older adults.
  Acoustic features were extracted from voice responses to cognitive tasks and daily
  life questions, along with demographic information.
---

# Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday Functioning

## Quick Facts
- arXiv ID: 2309.05777
- Source URL: https://arxiv.org/abs/2309.05777
- Reference count: 0
- Primary result: Smartwatch acoustic features detect everyday functioning deficits with 77.8% accuracy, outperforming standard neuropsychological tests at 68.5%

## Executive Summary
This study demonstrates that acoustic features extracted from voice data collected via smartwatch can detect deficits in cognitively relevant everyday functioning in older adults with higher accuracy than traditional neuropsychological tests. The research collected voice responses from 54 older adults during both cognitive tasks and daily conversation using Apple Watch devices, extracting 42 acoustic features including MFCCs, pitch, formants, and voice quality measures. Machine learning models using these features achieved 77.8% accuracy in classifying individuals with high versus low everyday functioning scores, compared to 68.5% accuracy using standard neuropsychological assessments. The study also identified common acoustic features that were robust markers across both types of voice data.

## Method Summary
The study collected voice data from 54 older adults using Apple Watch Series 6 devices worn on the non-dominant wrist. Participants provided voice responses during cognitive tasks (counting backward, subtraction, verbal fluency, picture description) and daily conversation questions about past experiences and future plans. Researchers extracted 42 acoustic features using Python libraries (librosa, Signal_Analysis) including MFCCs, pitch, formants, jitter, shimmer, and HNR. Machine learning models (k-NN, logistic regression, SVM, XGBoost, LightGBM) with nested 10×3 cross-validation classified participants based on their Everyday Cognition (ECog) scale scores. Feature selection was performed using Boruta, and SHAP values analyzed feature importance. The ECog scale served as the ground truth for cognitively relevant everyday functioning.

## Key Results
- Smartwatch acoustic features achieved 77.8% accuracy in detecting everyday functioning deficits, outperforming standard neuropsychological tests at 68.5%
- Four acoustic features showed statistically significant correlations with ECog scale scores across both cognitive tasks and daily conversation
- Common acoustic features were identified that robustly discriminated deficits in everyday functioning across both voice data types

## Why This Works (Mechanism)

### Mechanism 1
Acoustic features from voice data collected via smartwatch can detect deficits in everyday functioning in older adults with higher accuracy than standard neuropsychological tests. The study uses machine learning models to analyze 42 acoustic features extracted from voice responses to cognitive tasks and daily life questions. These features capture subtle changes in speech patterns that correlate with cognitive impairments affecting everyday functioning. Core assumption: Changes in acoustic features due to cognitive impairment are detectable in everyday conversation and cognitive task performance, and these changes correlate with deficits in cognitively relevant everyday functioning. Evidence: Machine learning models using acoustic features achieved 77.8% accuracy, higher than the 68.5% accuracy with standard neuropsychological tests.

### Mechanism 2
Common acoustic features can be robustly used to detect deficits in everyday functioning across different types of voice data. The study identifies acoustic features that are consistently selected by feature selection procedures across both cognitive task and daily conversation voice data, suggesting these features capture universal markers of cognitive impairment affecting everyday functioning. Core assumption: The same acoustic features that indicate cognitive impairment in structured tasks also indicate impairment in spontaneous conversation, and these features correlate with everyday functioning deficits. Evidence: Four acoustic features showed statistically significant correlations with the ECog scale for responses to both cognitive tasks and daily life questions.

### Mechanism 3
Smartwatch-based passive voice data collection enables continuous, objective monitoring of everyday functioning deficits. Smartwatches can passively collect voice data during daily activities without requiring active participation from users, allowing for frequent and unobtrusive assessment of cognitive status through acoustic feature analysis. Core assumption: Passive voice data collection via smartwatch is feasible and acceptable to older adults, and the collected data maintains sufficient quality for acoustic analysis. Evidence: The study demonstrates the feasibility of using a smartwatch-based application to collect acoustic features as objective markers for detecting deficits in everyday functioning.

## Foundational Learning

- Concept: Acoustic feature extraction from voice data
  - Why needed here: The study relies on extracting 42 acoustic features from voice responses, including spectrum-based features, pitch, formant, and voice quality measures.
  - Quick check question: What are the four main types of acoustic features extracted in this study, and what speech characteristics do they capture?

- Concept: Machine learning classification for cognitive impairment detection
  - Why needed here: The study uses supervised machine learning with nested cross-validation to classify participants into groups with high and low ECog scores based on acoustic features.
  - Quick check question: What machine learning algorithms were used in this study, and how was the model performance evaluated?

- Concept: Everyday Cognition (ECog) scale
  - Why needed here: The study uses the ECog scale as a measure of cognition-relevant everyday functioning, with higher scores indicating greater limitations.
  - Quick check question: How is the ECog score calculated, and what cutoff score was used to discriminate between cognitively normal and impaired older adults?

## Architecture Onboarding

- Component map: Apple Watch voice recording -> Acoustic feature extraction (librosa, Signal_Analysis) -> Machine learning classification (k-NN, logistic regression, SVM, XGBoost, LightGBM) -> Performance evaluation and comparison
- Critical path: Voice data collection → Feature extraction → Model training and validation → Performance evaluation and comparison
- Design tradeoffs:
  - Passive vs. active data collection: Passive collection enables continuous monitoring but may compromise data quality
  - Feature complexity: More features may improve detection but increase computational cost and risk overfitting
  - Model selection: Simpler models may be more interpretable but potentially less accurate than complex models
- Failure signatures:
  - Low model performance: Insufficient discriminative power of acoustic features or inadequate model selection
  - Poor generalization: Overfitting to training data or lack of diversity in participant sample
  - Technical issues: Poor audio quality from smartwatch, missing data, or feature extraction errors
- First 3 experiments:
  1. Test acoustic feature extraction pipeline on a small sample of voice data to ensure correct implementation and identify potential quality issues
  2. Train and evaluate a simple baseline model (e.g., logistic regression) to establish minimum performance expectations
  3. Perform feature importance analysis to identify the most discriminative acoustic features and validate their consistency across different voice data types

## Open Questions the Paper Calls Out

### Open Question 1
How do acoustic markers of cognitive impairment vary across different languages, and what are the implications for cross-lingual diagnostic tools? While some studies suggest common acoustic changes across languages, direct comparisons and validations across multiple languages are limited. Large-scale studies comparing acoustic markers across multiple languages would provide stronger evidence for cross-linguistic applicability.

### Open Question 2
Can subsetting voice data based on cognitive load improve the accuracy of detecting deficits in everyday functioning? The paper suggests that cognitive tasks elicit larger discernible differences in acoustic features related to functional limitation compared to daily life questions. Studies comparing the performance of models using complete voice data versus models using subsets of data with varying cognitive loads would clarify the potential benefits of data subsetting.

### Open Question 3
How do the acoustic markers of deficits in everyday functioning change over time, and can they be used to track disease progression? The study is cross-sectional and does not track changes in acoustic markers over time. Longitudinal studies following individuals with cognitive impairments over time would provide insights into the potential of these markers for tracking disease progression.

## Limitations

- Limited sample size of 54 participants may affect generalizability and capture of cognitive impairment patterns
- Reliance on a single ECog cutoff score (9.8) for classification may not be universally applicable across different populations
- Passive voice data collection raises questions about data quality consistency, particularly regarding environmental noise interference

## Confidence

- High Confidence: Technical feasibility of extracting acoustic features from smartwatch-recorded voice data and using them for machine learning classification
- Medium Confidence: Superiority of acoustic features over standard neuropsychological tests for detecting everyday functioning deficits
- Low Confidence: Robustness of common acoustic features across different voice data types (cognitive tasks vs. daily conversation)

## Next Checks

1. Cross-population validation: Test the model's performance on an independent dataset from a different geographic region or demographic group to assess generalizability and identify potential cultural or linguistic biases in the acoustic feature selection.

2. Longitudinal assessment: Conduct a longitudinal study tracking the same participants over 6-12 months to evaluate whether the acoustic markers can detect changes in everyday functioning over time and distinguish between stable and progressive cognitive impairment.

3. Technical validation under real-world conditions: Deploy the smartwatch application in a larger cohort (n > 100) for 30+ days of continuous monitoring to assess data quality issues, user compliance rates, and the practical challenges of passive voice data collection in uncontrolled environments.