---
ver: rpa2
title: 'BigTranslate: Augmenting Large Language Models with Multilingual Translation
  Capability over 100 Languages'
arxiv_id: '2305.18098'
source_url: https://arxiv.org/abs/2305.18098
tags:
- language
- translation
- languages
- multilingual
- bigtrans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'BigTranslate adapts the LLaMA-13B model to support multilingual
  translation across 102 languages. The approach involves three steps: first, the
  model is fine-tuned with massive Chinese monolingual data to improve Chinese language
  understanding; second, it is trained on a large-scale parallel corpus covering 102
  languages; and third, instruction tuning is applied using 240,000 multilingual translation
  examples.'
---

# BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages

## Quick Facts
- arXiv ID: 2305.18098
- Source URL: https://arxiv.org/abs/2305.18098
- Reference count: 12
- One-line result: BigTranslate adapts LLaMA-13B to support 102 languages, achieving comparable performance to ChatGPT and Google Translate with notable improvements in 8 language pairs

## Executive Summary
BigTranslate extends the LLaMA-13B foundation model to support multilingual translation across 102 languages through a three-step training pipeline. The approach first continues training on massive Chinese monolingual data, then on a large-scale parallel corpus covering all target languages, and finally applies instruction tuning with 240,000 multilingual translation examples. The resulting model is evaluated against established baselines using BLEU and GPT-4-based scoring, demonstrating competitive performance with specific improvements in certain language pairs.

## Method Summary
The BigTranslate approach involves three progressive fine-tuning stages: first, LLaMA-13B is trained on massive Chinese monolingual data to establish strong Chinese language understanding; second, the model continues training on a large-scale parallel corpus covering 102 languages using an incremental multilingual pre-training approach with curriculum learning; finally, instruction tuning is applied using 240,000 multilingual translation examples across 242 language pairs to activate task-specific capabilities.

## Key Results
- BigTranslate performs comparably to ChatGPT and Google Translate across many language pairs
- The model outperforms ChatGPT in 8 language pairs when evaluated using GPT-4 scoring
- Translation quality is maintained across diverse language families and script systems

## Why This Works (Mechanism)

### Mechanism 1: Multi-step continuous training pipeline
The model progressively builds linguistic competence through staged training, starting with Chinese monolingual data, then multilingual parallel data, and finally instruction tuning. This approach extends foundation model capabilities through continued training rather than complete retraining.

### Mechanism 2: Incremental multilingual pre-training with curriculum learning
Languages are introduced in intervals based on sample size, starting with high-resource language pairs before gradually exposing the model to low-resource pairs. This curriculum-like approach prevents the model from focusing disproportionately on high-resource languages.

### Mechanism 3: Instruction tuning to activate translation capabilities
A diverse instruction dataset with 240,000 examples teaches the model to respond to translation requests across various contexts and styles. The foundation model's latent translation capabilities are activated rather than new capabilities being injected.

## Foundational Learning

- **Multilingual representation learning and cross-lingual transfer**: Needed to map between languages and leverage high-resource language knowledge for low-resource languages. Quick check: How does the model handle languages with different scripts and linguistic structures during training?

- **Curriculum learning and balanced training**: Required to prevent focus on high-resource languages and ensure adequate representation for low-resource languages. Quick check: What determines the order in which languages are introduced during training?

- **Tokenization and vocabulary management for multilingual text**: Essential for processing text across 102 languages with different writing systems. Quick check: How does the vocabulary expansion process ensure coverage of all languages while maintaining efficiency?

## Architecture Onboarding

- **Component map**: Foundation model (LLaMA-13B) → Chinese monolingual training → Multilingual parallel training → Instruction tuning → Evaluation pipeline
- **Critical path**: Chinese training → Parallel corpus training → Instruction tuning (each stage must complete successfully before proceeding)
- **Design tradeoffs**: Larger vocabulary increases coverage but computational cost; more parallel data improves quality but increases training time; instruction diversity improves generalization but requires more examples
- **Failure signatures**: Degraded performance in any language during evaluation indicates issues in the corresponding training stage; imbalanced performance suggests curriculum learning problems
- **First 3 experiments**:
  1. Evaluate baseline LLaMA-13B on a subset of translation pairs to establish starting performance
  2. Test Chinese monolingual training effectiveness by measuring Chinese-specific tasks
  3. Validate multilingual parallel training by testing translation pairs from high-resource languages first

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific architectural or algorithmic innovations could improve BigTranslate's performance on extremely low-resource languages beyond the incremental multilingual pre-training approach? The paper suggests future work to enhance low-resource language ability but doesn't propose specific solutions.

- **Open Question 2**: How does BigTranslate's performance on natural language processing tasks beyond translation compare to other large language models when fine-tuned with multilingual data? The paper mentions BigTranslate can be applied to other NLP tasks but doesn't evaluate or compare its performance on these tasks.

- **Open Question 3**: What is the impact of different tokenization strategies on BigTranslate's multilingual translation quality, particularly for languages with non-Latin scripts or complex morphology? The paper implements vocabulary expansion but doesn't systematically test alternative tokenization methods.

## Limitations

- Data quality and scale remain uncertain, with massive monolingual and parallel corpora lacking exact size specifications and quality metrics
- Evaluation scope is limited to BLEU and GPT-4 scoring, potentially missing semantic fidelity and cultural nuances
- Generalization claims are based on specific language pairs without comprehensive coverage across all 102 languages or diverse translation domains

## Confidence

- **High Confidence**: The three-step training methodology is clearly described and represents a plausible approach for extending language models
- **Medium Confidence**: The incremental multilingual pre-training with curriculum learning is theoretically sound but lacks detailed validation
- **Low Confidence**: Claims about outperforming ChatGPT in 8 language pairs require verification across broader test sets and domains

## Next Checks

1. Test BigTranslate on specialized domains (legal, medical, technical) to verify translation quality beyond general web text
2. Evaluate the model's ability to handle language pairs not explicitly trained to test cross-lingual transfer capabilities
3. Assess performance degradation under adversarial inputs, noisy text, and code-switching scenarios to measure practical reliability