---
ver: rpa2
title: Real-Time Recurrent Reinforcement Learning
arxiv_id: '2311.04830'
source_url: https://arxiv.org/abs/2311.04830
tags:
- learning
- rtrl
- reward
- recurrent
- rflo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a biologically plausible reinforcement learning
  algorithm for partially observable Markov decision processes. The method combines
  three key components: a Meta-RL architecture inspired by the mammalian basal ganglia,
  a biologically plausible reinforcement learning algorithm using temporal difference
  learning and eligibility traces, and an online automatic differentiation algorithm
  for computing gradients.'
---

# Real-Time Recurrent Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.04830
- Source URL: https://arxiv.org/abs/2311.04830
- Reference count: 14
- One-line primary result: Combines RFLO with TD(λ) to create a biologically plausible, recurrent actor-critic algorithm that solves POMDP tasks with improved exploration and convergence speed.

## Executive Summary
This paper presents RTRRL, a biologically plausible reinforcement learning algorithm for partially observable Markov decision processes. The method combines RFLO for online gradient computation with TD(λ) and eligibility traces in an actor-critic architecture. Experimental results show RTRRL can solve diverse POMDP tasks, often outperforming state-of-the-art methods in convergence speed and exploration while maintaining biological plausibility through its basal ganglia-inspired design.

## Method Summary
RTRRL combines random feedback local online learning (RFLO) with temporal difference learning (TD(λ)) and eligibility traces to create a fully online actor-critic algorithm for POMDPs. The algorithm uses a CT-RNN backbone with separate linear actor and critic heads, where RFLO computes approximate gradients using random fixed feedback matrices instead of weight transport. Eligibility traces accumulate credit over time, and updates occur online using TD-errors without replay buffers or unrolling. The method maintains O(n²) complexity while achieving performance comparable to more expensive alternatives.

## Key Results
- RTRRL solves diverse POMDP tasks including MemoryChain, MountainCarContinuous, and Acrobot
- Shows improved exploration capabilities compared to TD(λ) baseline
- Achieves competitive performance to PPO with BPTT while using online updates instead of batching

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RFLO approximates gradients of RNN parameters without weight transport
- Mechanism: Uses random fixed feedback matrices to propagate errors backward, bypassing symmetric forward weights
- Core assumption: Random feedback matrices are sufficiently uncorrelated with forward weights to provide useful gradients
- Evidence anchors: Abstract mentions RFLO for online gradient computation; section explains random feedback matrices lead to acceptable function approximators

### Mechanism 2
- Claim: TD(λ) with eligibility traces enables credit assignment for delayed rewards
- Mechanism: Eligibility traces accumulate gradients over time and decay with γλ, allowing past actions to be reinforced proportionally to their contribution
- Core assumption: Decay rate γλ appropriately balances short-term and long-term credit assignment
- Evidence anchors: Abstract mentions TD(λ) and eligibility traces for policy and value-function training; section explains eligibility traces unify Monte-Carlo and TD methods

### Mechanism 3
- Claim: Combining RFLO with TD(λ) creates a fully online actor-critic algorithm for POMDPs
- Mechanism: Approximate Jacobian from RFLO multiplied by TD-error and eligibility traces updates all parameters online without unrolling
- Core assumption: Actor-critic architecture with separate value and policy heads can learn effective POMDP representations using online updates
- Evidence anchors: Abstract describes combining RFLO with TD(λ) for biologically plausible recurrent actor-critic; section explains online RNN gradient computation with TD(λ)

## Foundational Learning

- Concept: Temporal Difference Learning
  - Why needed here: Provides credit assignment mechanism for delayed rewards in reinforcement learning
  - Quick check question: What is the difference between the reward r and the TD-error δ in the actor-critic update?

- Concept: Eligibility Traces
  - Why needed here: Allows attributing delayed rewards to past actions without complete episodes
  - Quick check question: How does the decay factor γλ affect which past actions receive credit?

- Concept: Online Gradient Computation
  - Why needed here: Enables learning from single experience stream without replay buffers or separate phases
  - Quick check question: What is the computational complexity of RTRL vs BPTT for RNNs?

## Architecture Onboarding

- Component map: Observation → CT-RNN → Actor/Critic outputs → Action → Environment → TD-error → Update all parameters
- Critical path: Observation → CT-RNN → Actor/Critic outputs → Action → Environment → TD-error → Update all parameters
- Design tradeoffs:
  - RFLO vs RTRL: RFLO is O(n²) vs RTRL's O(n⁴), but RTRL has lower gradient variance
  - Online vs batched: Online has higher variance but enables continuous adaptation
  - Fixed vs learnable τ: Fixed τ simplifies Jacobian computation
- Failure signatures:
  - Policy collapses to random: Learning rates too high or variance too large
  - No learning progress: Eligibility traces decaying too fast or gradients vanishing
  - Divergence: Learning rates mismatched between actor/critic/RNN
- First 3 experiments:
  1. CartPole with RFLO CT-RNN - verify basic learning capability
  2. MemoryChain with increasing length - test memory capacity
  3. Acrobot with RFLO vs BPTT - compare exploration performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RTRRL's performance scale with increasing number of neurons in the recurrent network?
- Basis in paper: The paper shows RTRRL can solve POMDP tasks but doesn't explore performance scaling with network size
- Why unresolved: Only tests with fixed 32 neurons across different tasks
- What evidence would resolve it: Systematic experiments varying neurons in recurrent network across multiple POMDP tasks

### Open Question 2
- Question: How does RTRRL compare to model-based RL approaches on POMDP tasks?
- Basis in paper: Focuses on model-free RL and doesn't compare to model-based approaches
- Why unresolved: Only compares to other model-free RL algorithms
- What evidence would resolve it: Empirical comparison against model-based RL algorithms on same POMDP tasks

### Open Question 3
- Question: What is the biological plausibility of RTRRL's eligibility trace mechanism?
- Basis in paper: Claims RTRRL is fully biologically plausible but doesn't provide evidence for biological plausibility of eligibility traces
- Why unresolved: Discusses biological plausibility in general terms without specific evidence
- What evidence would resolve it: Neuroscientific studies demonstrating neural mechanisms implementing eligibility traces

## Limitations

- Performance claims lack direct ablation studies showing whether improvements come from RFLO, actor-critic architecture, or eligibility traces
- Several critical implementation details are underspecified including random feedback matrix initialization and continuous action handling
- Biological plausibility claims rest heavily on theoretical connections to basal ganglia without empirical validation against neural recordings

## Confidence

- High confidence: Mathematical framework combining RFLO with TD(λ) is sound and O(n²) complexity improvement over RTRL is well-established
- Medium confidence: Experimental results showing improved exploration and convergence speed, though limited by small-scale environments
- Low confidence: Claims about direct biological modeling and basal ganglia correspondence without empirical validation in biological systems

## Next Checks

1. Implement ablation study comparing RTRRL against versions using RTRL instead of RFLO, keeping all other components constant
2. Run MemoryChain experiments with varying λ values (0.5, 0.8, 0.95) to quantify how eligibility trace decay affects memory capacity
3. Add variance monitoring during training to empirically measure whether online updates in RTRRL have higher variance than batched alternatives