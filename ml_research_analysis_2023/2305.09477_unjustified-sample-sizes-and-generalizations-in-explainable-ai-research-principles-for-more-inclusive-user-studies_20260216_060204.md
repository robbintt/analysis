---
ver: rpa2
title: 'Unjustified Sample Sizes and Generalizations in Explainable AI Research: Principles
  for More Inclusive User Studies'
arxiv_id: '2305.09477'
source_url: https://arxiv.org/abs/2305.09477
tags:
- studies
- sample
- user
- papers
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed XAI user studies (n=220) published between
  2012-2022, finding that 88.2% did not justify sample sizes, and 68.2% overgeneralized
  results beyond their samples. Only 11.8% reported power analyses.
---

# Unjustified Sample Sizes and Generalizations in Explainable AI Research: Principles for More Inclusive User Studies

## Quick Facts
- arXiv ID: 2305.09477
- Source URL: https://arxiv.org/abs/2305.09477
- Reference count: 0
- 88.2% of XAI user studies don't justify sample sizes, yet 68.2% overgeneralize results

## Executive Summary
This study systematically analyzed 220 XAI user studies published between 2012-2022, revealing critical methodological weaknesses in the field. The research found that the vast majority of studies (88.2%) failed to justify their sample sizes, yet most (68.2%) drew broad conclusions beyond their target populations. Notably, no correlation existed between sample size and the scope of conclusions drawn. Additionally, 70.4% of studies targeting lay-users neglected to check for AI/ML background that could influence results. The authors propose three principles to improve XAI research: sample size justification, reporting relevant background, and generalization checks.

## Method Summary
The researchers conducted a systematic literature review across Scopus, Web of Science, and arXiv, screening 220 XAI user studies published between 2012-2022. They used dual coding with inter-rater reliability checks to extract data on sample size justification, scope of conclusions, user types, and background checks. Statistical analyses including Mann-Whitney U tests, chi-square tests, and rank-biserial correlation were performed to examine relationships between sample characteristics and generalization practices.

## Key Results
- 88.2% of reviewed user study papers did not justify their sample sizes
- 68.2% of studies overgeneralized conclusions beyond their target populations
- 70.4% of studies targeting lay-users did not check for AI/ML background that could affect results
- No correlation found between broader conclusions and larger samples (p = .061)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sample size justification is rare in XAI user studies, leading to overgeneralization.
- Mechanism: Without formal justification (power analysis, heuristics, or saturation), researchers cannot assess whether their sample is representative enough for broad conclusions, yet they still make claims about users in general.
- Core assumption: Researchers intend to make broad claims about human explanatory needs, not just about their sample.
- Evidence anchors:
  - [abstract] "Most studies did not offer rationales for their sample sizes" and "most papers generalized their conclusions beyond their target population"
  - [section] "We found that 88.2% (n = 194) of the reviewed user study papers did not justify their sample sizes"
- Break condition: If future XAI studies routinely include sample size justifications (power analyses, saturation criteria, or other rationales), the mechanism breaks.

### Mechanism 2
- Claim: Broader conclusions in XAI user studies are not correlated with larger samples.
- Mechanism: Researchers make sweeping claims about users' preferences or understanding without having collected data from sufficiently large or representative samples to support such generalizations.
- Core assumption: The scope of conclusions should correlate with sample size when researchers aim for generalizability.
- Evidence anchors:
  - [abstract] "there was no evidence that broader conclusions in quantitative studies were correlated with larger samples"
  - [section] "A rank-biserial correlation test also did not provide evidence of an association between 'generalized' papers and sample size, rrb (124) = .168, 95% CI [.013, .337], p = .061"
- Break condition: If subsequent analyses show a statistically significant correlation between broader claims and larger sample sizes.

### Mechanism 3
- Claim: Technical background (AI/ML expertise) is not checked in most XAI studies targeting lay-users, yet overgeneralizations occur.
- Mechanism: Without controlling for technical background, results may reflect the perceptions of technically savvy participants rather than lay-users, yet conclusions are still drawn about lay-users in general.
- Core assumption: Technical background significantly influences how users perceive and understand XAI explanations.
- Evidence anchors:
  - [abstract] "70.4% of studies targeting lay-users did not check for AI/ML background that could affect results"
  - [section] "70.4% (n = 107) did not contain evidence that the participants... were questioned about or categorized according to their technical affinity"
- Break condition: If future studies consistently check and report technical background and findings remain unchanged.

## Foundational Learning

- Concept: Sample size justification methods (power analysis, saturation, heuristics)
  - Why needed here: Without justification, it's unclear if samples are large enough to support the scope of conclusions drawn
  - Quick check question: What are the three main approaches to sample size justification in research?

- Concept: Generalizability in user studies
  - Why needed here: The study's core finding is that many XAI user studies overgeneralize from unrepresentative samples
  - Quick check question: What does it mean for study results to be generalizable to a target population?

- Concept: Confounding variables in experimental design
  - Why needed here: Technical background is identified as a key confounder that affects how users perceive XAI explanations
  - Quick check question: Why is it important to control for technical background when studying lay-user perceptions of XAI?

## Architecture Onboarding

- Component map: Literature review system (search, screening, classification, analysis) -> Data extraction framework (sample size, scope of conclusions, user type, background checks) -> Statistical analysis pipeline (correlation tests, significance checks)
- Critical path: Systematic literature search -> Title/abstract screening -> Full-text analysis -> Data classification -> Statistical analysis -> Principle formulation
- Design tradeoffs: Balancing breadth of literature search (multiple databases, forward snowballing) with depth of analysis (manual coding, inter-rater reliability checks)
- Failure signatures: Low inter-rater agreement, missing data fields, inconsistent classification criteria
- First 3 experiments:
  1. Test inter-rater reliability on a small subset of papers before full coding
  2. Validate classification scheme on a pilot set of papers from different subfields
  3. Run preliminary correlation analyses on a subset to check if statistical approach is appropriate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the lack of sample size justification in XAI user studies correlate with the presence of new XAI techniques or comparisons between different techniques?
- Basis in paper: [inferred] The paper notes that analyzing whether papers included user experiments with functional metrics tests, introduced new XAI techniques, or offered comparisons between techniques could reveal different correlations regarding the generalizability of study results in subsets of the reviewed papers.
- Why unresolved: The paper did not record this information, making it impossible to test for correlations between sample size justification and the introduction of new techniques or comparisons.
- What evidence would resolve it: A systematic analysis of XAI user studies that includes information on whether studies introduced new techniques or compared existing ones, along with sample size justifications, would reveal potential correlations.

### Open Question 2
- Question: Does the exclusion of not yet peer-reviewed papers from the analysis change the observed trend that broader conclusions are not correlated with larger samples?
- Basis in paper: [explicit] The paper states that when re-running the analyses only with peer-reviewed papers, the key findings retained the same trend, except that quantitative papers with broader conclusions now had larger samples.
- Why unresolved: The paper did not conduct a separate analysis focusing solely on the correlation between broader conclusions and larger samples using only peer-reviewed papers.
- What evidence would resolve it: A separate analysis of the correlation between broader conclusions and larger samples using only peer-reviewed XAI user studies would clarify if the trend changes when excluding not yet peer-reviewed papers.

### Open Question 3
- Question: Does the use of control variables in XAI user studies affect the generalizability of results to lay-users?
- Basis in paper: [inferred] The paper mentions that including or excluding control variables requires justification because it can affect the available degrees of freedom, power, and explainable variance, potentially influencing false negatives or positives.
- Why unresolved: The paper did not record whether studies included or excluded control variables, nor did it analyze the impact of control variables on the generalizability of results to lay-users.
- What evidence would resolve it: A systematic analysis of XAI user studies that includes information on the use of control variables and their impact on the generalizability of results to lay-users would reveal potential effects.

## Limitations

- Literature search may have missed relevant papers due to database coverage gaps or search term limitations
- Manual coding process introduces potential subjectivity in classifying generalization scope and sample size justification
- Correlation analysis had limited statistical power (p = .061), suggesting the negative finding should be interpreted cautiously

## Confidence

- **High confidence**: The finding that 88.2% of studies lack sample size justification is robust, based on clear coding criteria and high inter-rater reliability.
- **Medium confidence**: The generalization overgeneralization claim (68.2%) is reliable but depends on subjective classification of conclusion scope.
- **Medium confidence**: The correlation analysis showing no relationship between sample size and generalization scope, though statistically weak (p = .061).
- **High confidence**: The finding that 70.4% of lay-user studies don't check technical background is well-supported by clear coding criteria.

## Next Checks

1. Replicate the literature search and coding process with independent researchers to verify the proportion findings and correlation results.
2. Conduct interviews with XAI researchers to understand their rationale for sample size decisions and generalization practices.
3. Analyze a subset of papers with formal sample size justifications to determine if these studies produce more reproducible or generalizable results.