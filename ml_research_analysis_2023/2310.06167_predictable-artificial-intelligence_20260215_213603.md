---
ver: rpa2
title: Predictable Artificial Intelligence
arxiv_id: '2310.06167'
source_url: https://arxiv.org/abs/2310.06167
tags:
- arxivpreprintarxiv
- systems
- page
- https
- predictable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Predictable AI, a research area focused on
  anticipating key validity indicators (performance, safety) of AI ecosystems. The
  authors argue that predictability is crucial for fostering trust, liability, control,
  alignment and safety of AI systems, and should be prioritized over performance.
---

# Predictable Artificial Intelligence

## Quick Facts
- arXiv ID: 2310.06167
- Source URL: https://arxiv.org/abs/2310.06167
- Reference count: 5
- Primary result: Introduces Predictable AI as a research area focused on anticipating key validity indicators of AI ecosystems

## Executive Summary
This paper introduces Predictable AI, a new research area focused on anticipating key validity indicators (performance, safety) of AI ecosystems before deployment. The authors argue that predictability is crucial for fostering trust, liability, control, alignment and safety of AI systems, and should be prioritized over performance. They formally characterize predictability, explore its components, and illustrate what can be predicted using examples. The vision is for scalable oversight where deployed AI systems are only allowed to operate if their user-aligned validity can be anticipated.

## Method Summary
The paper proposes a framework for Predictable AI that involves developing predictive models to anticipate key behavioral indicators of AI systems. The approach involves collecting evaluation data across different granularities (instance vs. benchmark level) and time scales (short-term vs. long-term), then using this data to train predictive models. Three types of prediction actors are identified: humans, the AI systems themselves, and external predictive models. The method requires extracting relevant input features, developing prediction engines, and creating monitoring interfaces to present predictions to stakeholders.

## Key Results
- Predictable AI can be decoupled from full mechanistic understanding of AI systems, focusing instead on anticipating properties
- Predictions exist on multiple granularities (instance-level vs. benchmark-level) and temporal scales (short-term vs. long-term)
- Multiple actors (humans, AI systems, external models) can make predictions, each with different strengths and limitations
- Key challenges include developing appropriate metrics, collecting evaluation data, bridging aggregation levels, and integrating predictors into monitors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predictability can be decoupled from full mechanistic understanding of AI systems.
- Mechanism: The framework focuses on anticipating properties (like performance or safety) rather than explaining how those properties emerge from the system's internal mechanisms. This allows for practical prediction without requiring full interpretability.
- Core assumption: Property-based predictions are sufficient for practical AI deployment and oversight, even without mechanistic explanations.
- Evidence anchors:
  - [abstract] "predictability focuses on anticipating properties rather than explaining mechanisms"
  - [section 2] "Predictable AI aims at any property that can be reliably anticipated and can be used to determine when, how or whether the system is worth being used in a given context"
  - [corpus] Weak evidence - no directly related papers found in corpus
- Break condition: If properties that matter for safety and trust cannot be reliably predicted without understanding mechanisms, this approach fails.

### Mechanism 2
- Claim: Predictability exists on multiple granularities and temporal scales simultaneously.
- Mechanism: The framework distinguishes between instance-level predictions (single input/event) and benchmark-level predictions (aggregate over many inputs), as well as short-term versus long-term predictions. This multi-scale approach allows different prediction methods for different needs.
- Core assumption: Different granularities and time scales require different prediction approaches and data sources.
- Evidence anchors:
  - [section 3] "Predictions can be made at the 'instance level', for a single input or event, or at the 'benchmark level', as an aggregate for a set of inputs"
  - [section 3] "The scale could be short-term, such as predicting an event in the near future, or long-term, which typically involves a forecast well ahead in time"
  - [corpus] Weak evidence - no directly related papers found in corpus
- Break condition: If predictions at different scales cannot be meaningfully connected or if one scale dominates practical utility.

### Mechanism 3
- Claim: Multiple actors can make predictions about AI systems, each with different strengths and limitations.
- Mechanism: The framework identifies three prediction actors: humans (human oversight), AI systems themselves (self-oversight through uncertainty estimation), and external predictive models trained on evaluation data. Each has different capabilities and limitations.
- Core assumption: Different prediction actors are complementary rather than redundant.
- Evidence anchors:
  - [section 4] "we conceive at least three different ways of predicting the behaviour of AI ecosystems, by considering who makes the prediction: humans, the AI systems themselves or an external predictive model"
  - [section 4] "Human predictions about an AI system's behavioural indicators can be useful at the instance level"
  - [corpus] Weak evidence - no directly related papers found in corpus
- Break condition: If one prediction actor consistently outperforms others across all scenarios, reducing the need for multiple approaches.

## Foundational Learning

- Concept: Property-based testing
  - Why needed here: The paper distinguishes predictability from verification/validation by focusing on probabilistic estimates of properties rather than binary correctness, similar to property-based testing in software engineering
  - Quick check question: How does property-based testing differ from traditional example-based testing in terms of what it can reveal about system behavior?

- Concept: Uncertainty quantification
  - Why needed here: Understanding different types of uncertainty (aleatoric vs epistemic) is crucial for evaluating prediction quality and knowing when predictions are reliable
  - Quick check question: What's the difference between uncertainty that comes from inherent randomness versus uncertainty from lack of knowledge?

- Concept: Meta-learning and scaling laws
  - Why needed here: The paper discusses using historical performance data and scaling relationships to predict future AI capabilities, which requires understanding meta-learning approaches
  - Quick check question: How can patterns observed in training larger models help predict the behavior of even larger future models?

## Architecture Onboarding

- Component map:
  - Input feature extraction -> Prediction engine -> Aggregation layer -> Monitoring interface
  - Data collection pipeline (parallel to all components)

- Critical path:
  1. Input features extracted and validated
  2. Appropriate prediction method selected based on granularity/time scale
  3. Prediction generated using selected approach
  4. Results aggregated and uncertainty quantified
  5. Output delivered to monitoring system or user

- Design tradeoffs:
  - Human vs automated prediction: Humans provide contextual understanding but are slow; automated models are fast but may miss nuances
  - Instance-level vs aggregate predictions: Fine-grained predictions are more actionable but require more data; aggregate predictions are easier but less specific
  - Anticipative vs reactive evaluation: Anticipative predictions enable proactive decisions but are harder; reactive predictions are easier but may come too late

- Failure signatures:
  - High variance in predictions across different methods for same input
  - Systematic under/over-prediction of certain properties
  - Prediction degradation when applied to out-of-distribution scenarios
  - Poor correlation between predicted and actual outcomes

- First 3 experiments:
  1. Test prediction accuracy on a simple benchmark where ground truth is known, comparing human, self, and external model predictions
  2. Evaluate how prediction accuracy changes with different input feature sets to identify most informative features
  3. Measure the relationship between prediction uncertainty and actual outcome variance to assess calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific metrics can effectively measure the predictability of AI systems beyond traditional performance metrics?
- Basis in paper: [explicit] The paper identifies metrics as a key challenge, noting that while traditional evaluation metrics exist for performance, usefulness, and safety, new metrics may be needed for alignment, honesty, harmlessness, and helpfulness.
- Why unresolved: The paper highlights this as an open challenge but does not propose specific new metrics or criteria for evaluating predictability itself.
- What evidence would resolve it: Development and validation of new metrics specifically designed to quantify predictability, along with empirical studies demonstrating their effectiveness across different AI systems and contexts.

### Open Question 2
- Question: How can we effectively bridge the gap between local, instance-level predictions and global, benchmark-level predictions of AI system behavior?
- Basis in paper: [explicit] The paper identifies "aggregation and disaggregation" as a key challenge, noting the need to bridge different predictability problems at several granularities.
- Why unresolved: The paper acknowledges this as a challenge but does not provide specific methodologies or frameworks for connecting predictions across different scales and granularities.
- What evidence would resolve it: Development of methods that can effectively translate between instance-level and benchmark-level predictions, validated through case studies showing improved predictability across multiple aggregation levels.

### Open Question 3
- Question: What is the optimal balance between allowing AI systems to predict their own performance (self-oversight) versus using external predictive models for monitoring?
- Basis in paper: [explicit] The paper discusses three approaches to prediction (humans, AI systems themselves, and external predictive models) and notes the potential conflict of interest when AI systems predict their own performance.
- Why unresolved: While the paper identifies this as a consideration, it does not provide guidelines or empirical evidence for determining when each approach is most appropriate or how to combine them effectively.
- What evidence would resolve it: Comparative studies demonstrating the relative effectiveness, reliability, and safety implications of different prediction approaches across various AI system types and applications.

## Limitations
- The framework operates primarily at a theoretical level with limited empirical validation
- No concrete methods are provided for developing predictive models or collecting evaluation data
- Implementation guidance for the three prediction actors is sparse and lacks practical details
- Scalability claims lack empirical support or concrete implementation examples

## Confidence

- **High confidence**: The conceptual distinction between predictability and related fields (interpretability, verification) is well-articulated and internally consistent
- **Medium confidence**: The multi-scale framework (instance vs. benchmark level, short vs. long-term) provides a useful organizational structure, though practical implementation details are sparse
- **Low confidence**: Claims about scalability and practical utility of the framework lack empirical support or concrete implementation examples

## Next Checks

1. **Empirical validation study**: Implement the framework on a specific AI system (e.g., language model or reinforcement learning agent) and measure prediction accuracy against ground truth outcomes across all three prediction actor types.

2. **Uncertainty calibration analysis**: Systematically evaluate how well predicted uncertainties correlate with actual outcome variance across different types of properties and prediction methods.

3. **Scalability benchmark**: Test the framework's performance as AI systems scale up in complexity, measuring prediction accuracy degradation and computational overhead relative to system size.