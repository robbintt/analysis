---
ver: rpa2
title: A knowledge-driven AutoML architecture
arxiv_id: '2311.17124'
source_url: https://arxiv.org/abs/2311.17124
tags:
- data
- synthesis
- feature
- knowledge
- pipeline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a knowledge-driven AutoML architecture that
  unifies pipeline and deep feature synthesis under a shared knowledge system. The
  architecture aims to make AutoML explainable and leverage domain knowledge by enabling
  runtime access to partial solutions and their data outputs.
---

# A knowledge-driven AutoML architecture

## Quick Facts
- arXiv ID: 2311.17124
- Source URL: https://arxiv.org/abs/2311.17124
- Authors: 
- Reference count: 5
- This paper proposes a knowledge-driven AutoML architecture that unifies pipeline and deep feature synthesis under a shared knowledge system.

## Executive Summary
This paper presents a novel AutoML architecture that leverages a knowledge graph to drive the synthesis of machine learning pipelines and features. The architecture aims to make AutoML more explainable and customizable by enabling runtime access to partial solutions and their data outputs. The knowledge graph stores nodes representing operations, features, and preconditions, connected via hierarchical and feature links. The system queries this graph during synthesis to decide which operations to add next, using constraint solving to validate preconditions. Two experiments demonstrate the approach: one on a toy XOR dataset showing effective feature synthesis and classifier selection via domain knowledge, and another on a circles dataset comparing pipeline space sizes under different preconditions.

## Method Summary
The architecture uses a knowledge graph to represent and reason about ML operations, features, and their relationships. A control flow component, modeled as a finite state machine, interacts with the knowledge system to determine valid operations during synthesis. A program execution component assembles and executes code for partial solutions, providing data outputs that inform subsequent synthesis decisions. The knowledge system uses constraint solving to validate preconditions and select appropriate operations based on the current synthesis state and data. The architecture enables modular, interpretable, and customizable AutoML synthesis, with potential for further integration into larger ML ecosystems.

## Key Results
- The knowledge-driven architecture effectively synthesizes ML pipelines and features using a shared knowledge system.
- Runtime access to partial solutions and their data outputs enables dynamic, domain-knowledge-driven decision making.
- Hierarchical organization of nodes and feature AST links enables scalable, modular knowledge representation and efficient search space reduction.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The knowledge-driven AutoML architecture unifies pipeline and deep feature synthesis through a shared knowledge system.
- Mechanism: A knowledge graph stores nodes representing operations, features, and preconditions, connected via hierarchical and feature links. This graph is interactively queried during synthesis to decide which operations to add next, using constraint solving to validate preconditions.
- Core assumption: A unified knowledge representation can effectively guide both pipeline construction and feature synthesis without loss of expressiveness or efficiency.
- Evidence anchors:
  - [abstract]: "The architecture explores several novel ideas: first, the construction of pipelines and deep features is approached in an unified way. Next, synthesis is driven by a shared knowledge system, interactively queried as to what pipeline operations to use or features to compute."
  - [section]: "At the highest level of conceptualization, we assume pipelines represented by directed acyclic graphs (DAG) and features by abstract syntax trees (AST). We shall refer to the basic building block of both structures as node."
  - [corpus]: Weak evidence; the related papers focus on other AutoML approaches and do not directly validate the unified knowledge graph mechanism.
- Break condition: If the knowledge graph becomes too complex or inconsistent, the constraint solver may fail to find valid solutions, or the synthesis process may become intractable.

### Mechanism 2
- Claim: Runtime access to partial solutions and their data outputs enables dynamic, domain-knowledge-driven decision making.
- Mechanism: The architecture allows control flow and program execution components to access intermediary pipeline and feature representations, as well as their outputs, enabling preconditions to be evaluated based on actual data results.
- Core assumption: Real-time feedback from partial solutions can meaningfully inform the synthesis process and lead to better final models.
- Evidence anchors:
  - [abstract]: "Lastly, the synthesis processes takes decisions at runtime using partial solutions and results of their application on data."
  - [section]: "In both synthesis cases, partial solutions are stored in data structures that hold symbolic node-based representations of ML pipelines and feature ASTs. These are shared and accessible by both control flow and program execution components, allowing therefore the knowledge system to interact in the synthesis dynamically, function of the intermediary data output and symbolic solution structures."
  - [corpus]: No direct evidence; related works do not discuss runtime access to partial solutions in the context of knowledge-driven synthesis.
- Break condition: If data outputs are not available or are misleading, preconditions may incorrectly enable or disable operations, leading to suboptimal or invalid pipelines.

### Mechanism 3
- Claim: Hierarchical organization of nodes and feature AST links enables scalable, modular knowledge representation and efficient search space reduction.
- Mechanism: Abstract nodes are organized hierarchically, with operation nodes at the bottom. Hierarchical links control the number of candidate operations added during synthesis, while feature AST links associate feature components with feature types, enabling systematic feature construction.
- Core assumption: A hierarchical, graph-based knowledge structure can effectively capture the relationships and dependencies between operations and features, enabling efficient search and synthesis.
- Evidence anchors:
  - [section]: "The knowledge graph structure employed here, schematically shown in Figure 2, is made up of three node types and three link types: Abstract nodes, Operation nodes, Preconditions, Hierarchical links, Feature AST links (feature synthesis only), Precondition links."
  - [section]: "For pipeline synthesis, the node hierarchy allows control over the number of candidates that can be added to the solution during synthesis and, through this, the dimension of the solution search space."
  - [corpus]: No direct evidence; related papers do not discuss hierarchical knowledge representation in the context of AutoML.
- Break condition: If the hierarchy is not well-designed or is incomplete, the search space may be too large or too constrained, leading to inefficient or ineffective synthesis.

## Foundational Learning

- Concept: Knowledge graphs and their use in representing and reasoning about domain knowledge.
  - Why needed here: The proposed architecture relies on a knowledge graph to store and query information about ML operations, features, and their relationships.
  - Quick check question: What are the key components of a knowledge graph, and how are they typically used to represent and reason about domain knowledge?
- Concept: Constraint satisfaction and solving techniques.
  - Why needed here: The knowledge system uses constraint solving to validate preconditions and select appropriate operations during synthesis.
  - Quick check question: What is a constraint satisfaction problem (CSP), and what are some common techniques for solving CSPs?
- Concept: Finite state machines and their application in modeling and controlling synthesis processes.
  - Why needed here: The control flow component uses finite state machines to model the pipeline synthesis process and determine valid state transitions.
  - Quick check question: What is a finite state machine (FSM), and how can it be used to model and control complex processes?

## Architecture Onboarding

- Component map:
  - Knowledge System (KS) -> Control Flow (CF) -> Program Execution (PE)
- Critical path:
  1. Control flow queries the knowledge system with current synthesis state and data.
  2. Knowledge system uses constraint solving to select valid operations based on preconditions and hierarchy.
  3. Control flow updates the synthesis state and generates new partial solutions.
  4. Program execution assembles and executes code for partial solutions, providing data outputs.
  5. Process repeats until synthesis is complete or a stopping condition is met.
- Design tradeoffs:
  - Expressiveness vs. tractability: More expressive knowledge representations may lead to larger, harder-to-solve CSPs.
  - Modularity vs. efficiency: A more modular architecture may be easier to extend but could incur additional communication overhead.
  - Runtime feedback vs. upfront planning: Runtime access to partial solutions enables dynamic decision-making but may require more computational resources.
- Failure signatures:
  - Synthesis process gets stuck or fails to find valid solutions: May indicate issues with knowledge representation, constraints, or search space.
  - Generated pipelines or features are suboptimal or invalid: May indicate issues with preconditions, hierarchy, or constraint solving.
  - System is slow or unresponsive: May indicate issues with knowledge graph size, constraint solving complexity, or communication overhead.
- First 3 experiments:
  1. Implement a simple pipeline synthesis task (e.g., XOR classification) using the proposed architecture and evaluate the generated pipelines.
  2. Extend the architecture to handle feature synthesis and evaluate the generated features on a toy dataset.
  3. Integrate pipeline and feature synthesis, and evaluate the combined approach on a more complex dataset or task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the knowledge system be effectively decoupled from specific ML libraries and programming languages to enable broader applicability?
- Basis in paper: [explicit] The paper mentions this as an open question, stating "At this point it is unclear to what extent a knowledge system can be decoupled from a specific ML library and programming language."
- Why unresolved: The current implementation relies on implicit semantics and specific ML libraries (e.g., MLJ in Julia), which may limit its generalizability and require significant implementation and maintenance efforts for a production-ready version.
- What evidence would resolve it: Development and testing of a knowledge system that can interface with multiple ML libraries and programming languages, demonstrating its ability to generate and execute pipelines across different platforms without significant modifications.

### Open Question 2
- Question: How can the complexity and size of the synthesis process be estimated and controlled to operate within given computational resource budgets?
- Basis in paper: [explicit] The paper acknowledges this as an open issue, stating "Estimating the complexity or, results space size, of the synthesis process remains challenging as well, as it depends on the data characteristics and types of preconditions present."
- Why unresolved: The complexity of the synthesis process is influenced by various factors, including data characteristics, types of preconditions, and the number of available operations, making it difficult to predict and control the computational resources required.
- What evidence would resolve it: Development and evaluation of techniques to estimate the complexity of the synthesis process based on input data and knowledge base characteristics, along with the implementation of mechanisms to constrain the synthesis within specified computational resource budgets.

### Open Question 3
- Question: How can knowledge base completion mechanisms be implemented to automatically adapt third-party code snippets for use during synthesis?
- Basis in paper: [explicit] The paper suggests this as a potential direction, stating "One could also exploit automatic or human-driven knowledge base completion mechanisms. This would imply learning specific code transformations that would allow code snippets from third-party code repositories to be automatically adapted for use during synthesis."
- Why unresolved: Automatically adapting third-party code snippets requires understanding the semantics and context of the code, as well as identifying and applying appropriate transformations to ensure compatibility with the synthesis process.
- What evidence would resolve it: Development and evaluation of methods to automatically extract, analyze, and transform code snippets from third-party repositories, demonstrating their successful integration into the knowledge base and their effective use during synthesis.

## Limitations
- The architecture's dependency on high-quality, complete knowledge bases could severely degrade synthesis quality if preconditions or hierarchical links are missing or incorrect.
- The use of constraint solvers introduces computational overhead that scales poorly with knowledge graph size, potentially limiting real-world applicability to complex datasets.
- The XOR and circles experiments demonstrate basic functionality but lack statistical rigor and do not test scalability or robustness across diverse problem domains.

## Confidence
- Knowledge graph-driven synthesis: Medium-High
- Runtime partial solution integration: Medium
- Scalability claims: Low

## Next Checks
1. **Scalability test**: Apply the architecture to a multi-class classification problem with 10+ features and measure synthesis time and memory usage as knowledge graph size increases.
2. **Robustness validation**: Systematically remove preconditions from the knowledge base and measure degradation in pipeline quality to quantify sensitivity to incomplete knowledge.
3. **Generalization benchmark**: Compare the architecture's performance against established AutoML frameworks (e.g., TPOT, Auto-sklearn) on standardized OpenML benchmark suites using multiple metrics (accuracy, F1, training time).