---
ver: rpa2
title: A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks
  with Radiological Images
arxiv_id: '2309.02555'
source_url: https://arxiv.org/abs/2309.02555
tags:
- learning
- pretraining
- self-supervised
- lecture
- notes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review evaluates recent research on self-supervised pretraining
  for diagnostic tasks with radiological images. The primary finding is that self-supervised
  pretraining generally improves downstream task performance compared to fully supervised
  learning, particularly when unlabelled examples greatly outnumber labelled examples.
---

# A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images

## Quick Facts
- arXiv ID: 2309.02555
- Source URL: https://arxiv.org/abs/2309.02555
- Reference count: 40
- Primary result: Self-supervised pretraining generally improves downstream task performance compared to fully supervised learning, particularly when unlabelled examples greatly outnumber labelled examples

## Executive Summary
This review evaluates recent research on self-supervised pretraining for diagnostic tasks with radiological images across X-ray, computed tomography, magnetic resonance imaging, and ultrasound imaging. The primary finding is that self-supervised pretraining (SSL) generally improves downstream task performance compared to fully supervised learning, most prominently when unlabelled examples greatly outnumber labelled examples. The review provides practical recommendations for practitioners and identifies future research directions, particularly for ultrasound imaging where research remains limited.

## Method Summary
The review surveys published studies that apply self-supervised learning methods to pretrain feature extractors using unlabelled radiological images, followed by fine-tuning on labelled datasets for specific diagnostic tasks. The methods evaluated include contrastive learning approaches, non-contrastive methods, and generative models. The analysis covers performance comparisons between SSL pretraining, random initialization, and supervised pretraining (including ImageNet initialization), examining downstream tasks such as classification and segmentation across different imaging modalities.

## Key Results
- SSL pretraining improves downstream performance compared to full supervision, especially when unlabelled data greatly outnumbers labelled examples
- SSL pretraining generally outperforms ImageNet pretraining for medical imaging tasks when pretraining is done on domain-specific data
- Some studies report improved model generalization to external datasets when using SSL pretraining

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised pretraining improves downstream performance most when unlabelled examples greatly outnumber labelled examples. Pretraining learns rich feature representations from unlabelled data, which transfer better to supervised tasks when labelled data is scarce. The pretext tasks force the model to capture meaningful structure in the data. Core assumption: The unlabelled dataset is large enough and representative of the downstream task distribution. Evidence anchors: [abstract] "The most pertinent finding is that self-supervised pretraining generally improves downstream task performance compared to full supervision, most prominently when unlabelled examples greatly outnumber labelled examples." Break condition: If the unlabelled data distribution differs significantly from the labelled downstream task, the learned representations may not transfer effectively.

### Mechanism 2
Self-supervised pretraining can outperform ImageNet-pretrained weights for 2D CNNs and vision transformers in medical imaging tasks. Medical image domains (e.g., chest X-rays, CT scans) have different characteristics from natural images. Pretraining directly on medical data captures domain-specific features that ImageNet pretraining misses. Core assumption: The medical dataset used for pretraining is large and diverse enough to learn useful representations. Evidence anchors: [abstract] "the majority of the publications included in this survey suggest that SSL pretraining using unlabelled datasets generally improves the performance of supervised deep learning models for downstream tasks in radiography, computed tomography, magnetic resonance imaging, and ultrasound." Break condition: If the medical dataset is too small or homogeneous, ImageNet pretraining may still provide a better initialization.

### Mechanism 3
SSL pretraining improves model generalizability to external datasets. Pretraining on large, diverse unlabelled datasets helps the model learn more robust and generalizable features, reducing overfitting to the training data distribution. Core assumption: The pretraining dataset is sufficiently diverse and captures the variability in the target domain. Evidence anchors: [abstract] "characterizing the impact of self-supervised pretraining on generalization." Break condition: If the pretraining dataset is not diverse enough, the model may still overfit and fail to generalize to new distributions.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Many SSL methods in the review use contrastive learning as a pretext task to learn representations that are invariant to non-meaningful transformations
  - Quick check question: What is the difference between positive and negative pairs in contrastive learning?

- Concept: Vision transformers
  - Why needed here: The review discusses the use of vision transformers for medical image analysis, and their performance compared to traditional CNNs
  - Quick check question: How do vision transformers differ from CNNs in processing medical images?

- Concept: Multi-modal learning
  - Why needed here: Some studies in the review use multi-modal pretext tasks (e.g., pairing chest X-rays with radiology reports) to learn richer representations
  - Quick check question: What are the benefits of using multi-modal data for self-supervised pretraining?

## Architecture Onboarding

- Component map: Backbone (feature extractor) -> Projector (embedding network) -> Head (downstream task network)
- Critical path: Pretraining (backbone + projector) â†’ Fine-tuning (backbone + head)
- Design tradeoffs:
  - Pretraining vs. training from scratch: Pretraining can improve performance but requires more computation and unlabelled data
  - Domain-specific vs. general pretraining: Pretraining on medical data may be better than ImageNet for medical tasks, but ImageNet may provide a better starting point for some architectures
  - Contrastive vs. non-contrastive methods: Contrastive methods may require more data but can learn more robust representations
- Failure signatures:
  - Poor downstream performance: Pretraining may not have learned useful representations, or the downstream task may be too different from the pretraining task
  - Overfitting: The model may overfit to the pretraining data and fail to generalize to new distributions
  - Computational inefficiency: Pretraining can be computationally expensive, especially for large datasets
- First 3 experiments:
  1. Pretrain a CNN backbone on a large medical dataset using a contrastive pretext task, then fine-tune on a downstream classification task
  2. Compare the performance of a vision transformer pretrained on medical data vs. ImageNet for a medical segmentation task
  3. Investigate the effect of pretraining dataset size on downstream performance for a medical image classification task

## Open Questions the Paper Calls Out

### Open Question 1
Does pretraining with ImageNet weights improve SSL performance for medical imaging tasks compared to random initialization? Basis in paper: [explicit] The paper states that multiple studies found the best performance when initializing feature extractors with ImageNet-pretrained weights prior to self-supervised pretraining. Why unresolved: The paper notes that while some studies compared SSL to ImageNet initialization, many did not, and a direct comparison of SSL with and without ImageNet initialization is missing. What evidence would resolve it: Studies that systematically compare SSL performance with and without ImageNet initialization across different medical imaging modalities and tasks.

### Open Question 2
How does SSL pretraining impact the generalization ability of models to external datasets or different distributions? Basis in paper: [inferred] The paper mentions that models are susceptible to performance drops under distributional shift and that some studies reported improved performance on external test sets with SSL pretraining. Why unresolved: The paper indicates a need for further work to characterize the effect of SSL on generalizability, as the current evidence is limited. What evidence would resolve it: Studies that evaluate SSL-pretrained models on multiple external datasets with varying distributions and compare their generalization performance to models trained from scratch or with supervised pretraining.

### Open Question 3
What is the optimal strategy for SSL pretraining in ultrasound imaging, given its unique challenges? Basis in paper: [explicit] The paper highlights a relative dearth of ultrasound research and notes that US presents additional challenges for machine learning systems compared to other modalities. Why unresolved: The paper suggests that further work is needed to determine suitable pretext tasks and aspects of SSL for ultrasound, as current research is limited. What evidence would resolve it: Studies that propose and evaluate novel or modified SSL methods specifically designed for ultrasound imaging, considering its temporal dimension and acquisition-related differences.

## Limitations
- Limited quantitative comparisons of performance gains across different label ratios
- Lack of comprehensive analysis of computational costs and pretraining dataset requirements
- Heavy reliance on surrogate evidence rather than direct experimental validation

## Confidence

**High confidence**: SSL pretraining generally improves downstream performance compared to random initialization, especially when unlabelled data is abundant

**Medium confidence**: SSL pretraining can outperform ImageNet pretraining for medical imaging tasks, but this depends on dataset size and domain specificity

**Medium confidence**: SSL pretraining improves generalization to external datasets, though the effect size varies significantly across studies

## Next Checks

1. Conduct controlled experiments varying the ratio of unlabelled to labelled data to quantify when SSL pretraining becomes beneficial
2. Compare SSL pretraining directly against ImageNet pretraining across multiple medical imaging tasks and architectures
3. Evaluate generalization performance on truly external datasets not seen during either pretraining or fine-tuning