---
ver: rpa2
title: 'FedSDD: Scalable and Diversity-enhanced Distillation for Model Aggregation
  in Federated Learning'
arxiv_id: '2312.17029'
source_url: https://arxiv.org/abs/2312.17029
tags:
- global
- fedsdd
- ensemble
- distillation
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedSDD, a scalable and diversity-enhanced distillation
  scheme for federated learning. The key idea is to build the ensemble (teacher model)
  from a small group of aggregated models instead of all client models, decoupling
  the training complexity from the number of clients.
---

# FedSDD: Scalable and Diversity-enhanced Distillation for Model Aggregation in Federated Learning

## Quick Facts
- arXiv ID: 2312.17029
- Source URL: https://arxiv.org/abs/2312.17029
- Reference count: 38
- Key outcome: FedSDD improves scalability and performance in federated learning by using a small ensemble of aggregated models and selective knowledge distillation, particularly effective under non-IID data conditions.

## Executive Summary
FedSDD introduces a novel approach to federated learning that addresses scalability and diversity challenges in model aggregation. By building the teacher ensemble from a small group of aggregated models instead of all client models, the method decouples computation cost from client count. The selective knowledge distillation to only the main global model preserves diversity among ensemble members. Temporal ensembling across multiple rounds further enhances ensemble capacity without slowing convergence. Experiments demonstrate superior performance over FedAvg and FedDF, especially in non-IID settings.

## Method Summary
FedSDD partitions clients into K groups, performs local training and group-wise aggregation to create K global models, then builds an ensemble from these models across R rounds. The key innovation is that only the main global model receives knowledge distillation from the ensemble, while other models maintain independent learning paths for diversity. The method uses secure aggregation for privacy protection and achieves better scalability by making computation cost independent of client count. Training uses SGD optimizer with specific learning rates for clients and server, with 40 local epochs per round and temperature-based distillation.

## Key Results
- FedSDD outperforms FedAvg and FedDF on CIFAR10/100 datasets, especially under non-IID conditions
- Scalability achieved by decoupling ensemble size from client count, maintaining performance while reducing computation
- Selective distillation to main model preserves ensemble diversity and improves both ensemble and main model performance
- Temporal ensembling significantly improves performance with high data heterogeneity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling ensemble size from client count improves scalability.
- Mechanism: FedSDD builds the teacher ensemble from K aggregated global models instead of all client models, making computation cost independent of client count.
- Core assumption: A smaller set of well-trained aggregated models can represent client knowledge as effectively as using all client models directly.
- Evidence anchors:
  - [abstract] "the teacher model in FedSDD is an ensemble built by a small group of aggregated (global) models, instead of all client models, such that the computation cost will not scale with the number of clients."
  - [section] "The size of the ensemble is determined by the number of groups, K, instead of the number of participating clients, Nt."
- Break condition: If aggregated global models fail to capture the diversity of all client models, ensemble quality degrades, negating the scalability benefit.

### Mechanism 2
- Claim: Selective distillation to only the main global model preserves diversity.
- Mechanism: FedSDD only distills knowledge into the main global model, allowing other models to maintain independent learning paths and thus higher diversity.
- Core assumption: Forcing all models to mimic the ensemble leads to model collapse and reduces ensemble capacity.
- Evidence anchors:
  - [abstract] "to enhance diversity, FedSDD only performs KD to enhance one of the global models, i.e., the main global model, which improves the performance of both the ensemble and the main global model."
  - [section] "It is worth noticing that, the above workflow performs weights broadcasting, local training, and aggregation in different groups, independently... the direct access to the client models are not required by the server."
- Break condition: If the main model receives too much weight from the ensemble, it may overfit or suppress diversity gains.

### Mechanism 3
- Claim: Temporal ensembling improves ensemble capacity without slowing convergence.
- Mechanism: By combining checkpoints from multiple rounds, FedSDD builds a larger ensemble that captures more diverse client distributions, mitigating slowdown that would occur if K were increased without temporal ensembling.
- Core assumption: Consecutive rounds involve different client subsets, so ensembling across rounds simulates having more clients without extra computation.
- Evidence anchors:
  - [abstract] "We introduce the temporal ensembling which leverage the issues, and provide significant improvement with the heterogeneous settings."
  - [section] "Intuitively, global models from consecutive rounds were trained based on the data from different subsets of the clients. Thus, ensembling global models from multiple checkpoints of different rounds can emulate the case where more clients are participating."
- Break condition: If R is too large, stale checkpoints may degrade ensemble quality; if R is too small, diversity gains are minimal.

## Foundational Learning

- Concept: Knowledge Distillation (KD)
  - Why needed here: KD transfers knowledge from the ensemble teacher to the main global model, enabling model aggregation without direct data sharing.
  - Quick check question: In FedSDD, what loss function is used to align the main model's predictions with the ensemble's output?

- Concept: Federated Averaging (FedAvg)
  - Why needed here: FedSDD modularizes local training and weight averaging, so understanding FedAvg is essential to see how groups of clients produce aggregated global models.
  - Quick check question: In FedSDD, how are clients partitioned before weight averaging occurs?

- Concept: Non-IID Data Distribution
  - Why needed here: FedSDD is designed to handle highly heterogeneous client data; understanding Dirichlet sampling helps explain why temporal ensembling matters.
  - Quick check question: How does the α parameter in Dirichlet sampling affect the degree of data heterogeneity among clients?

## Architecture Onboarding

- Component map: Clients -> Local training -> Server -> Group-wise averaging -> K global models -> Ensemble building -> Knowledge distillation (main model only)
- Critical path:
  1. Partition clients into K groups
  2. Broadcast global model weights to each group
  3. Perform local training and send updated weights
  4. Aggregate weights within each group to form K global models
  5. Build ensemble from K×R checkpoints
  6. Perform KD on the main global model
- Design tradeoffs:
  - K vs R: Larger K improves ensemble diversity but slows individual model convergence; larger R improves ensemble capacity without slowing convergence
  - Parallelism: Only KD on main model allows K-1 models to train in parallel with client-side updates, reducing round time
- Failure signatures:
  - Low ensemble diversity: Models collapse, main model underperforms
  - Poor convergence: Too many groups (small K per group) slows learning
  - High communication cost: If secure aggregation not used, privacy guarantees lost
- First 3 experiments:
  1. Verify that distillation-only on the main model preserves diversity by comparing ensemble accuracy with and without selective KD
  2. Test scalability by increasing client count while keeping K fixed; measure distillation time and accuracy
  3. Evaluate temporal ensembling by varying R on highly Non-IID data; compare convergence speed and final accuracy

## Open Questions the Paper Calls Out
The paper mentions potential extensions to heterogeneous model architectures and labeled server datasets as straightforward but doesn't test these scenarios.

## Limitations
- Assumption that aggregated models adequately represent client diversity needs more rigorous testing
- Selective distillation approach lacks comprehensive ablation studies showing performance degradation when KD is applied to all models
- Temporal ensembling effectiveness depends on assumption that consecutive rounds sample sufficiently different client subsets

## Confidence
- High confidence: Scalability improvements through decoupling ensemble size from client count are well-supported
- Medium confidence: Diversity preservation mechanism through selective distillation is theoretically sound but lacks comprehensive validation
- Low confidence: Temporal ensembling benefits need more rigorous testing across different client sampling patterns

## Next Checks
1. Conduct ablation studies varying K (ensemble size) while keeping client count constant to isolate impact of selective distillation on model diversity
2. Test temporal ensembling with different client sampling strategies to verify consecutive rounds consistently provide diverse client subsets
3. Evaluate performance with varying levels of non-IIDness (different α values) to determine robustness of diversity preservation mechanism