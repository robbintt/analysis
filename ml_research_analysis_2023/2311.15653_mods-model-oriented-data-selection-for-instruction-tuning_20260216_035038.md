---
ver: rpa2
title: 'MoDS: Model-oriented Data Selection for Instruction Tuning'
arxiv_id: '2311.15653'
source_url: https://arxiv.org/abs/2311.15653
tags:
- data
- instruction
- dataset
- arxiv
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a model-oriented data selection (MoDS) approach
  for instruction tuning of large language models (LLMs). The key idea is to select
  instruction data based on a new criteria considering three aspects: quality, coverage
  and necessity.'
---

# MoDS: Model-oriented Data Selection for Instruction Tuning

## Quick Facts
- arXiv ID: 2311.15653
- Source URL: https://arxiv.org/abs/2311.15653
- Authors: Y. Zhou, X. Zhou, H. Wang, et al.
- Reference count: 4
- Key outcome: MoDS-selected instruction data achieves better performance than using the full dataset

## Executive Summary
This paper introduces MoDS (Model-oriented Data Selection), a novel approach for selecting instruction data for fine-tuning large language models (LLMs). The method focuses on selecting high-quality, diverse, and necessary instruction triplets through a three-step process: quality evaluation, diverse seed selection, and necessity evaluation. Experiments demonstrate that fine-tuning with only 4,000 instruction pairs selected by MoDS outperforms using the full dataset of 214k instruction data, achieving state-of-the-art results on multiple benchmarks.

## Method Summary
MoDS operates through three key stages: First, it filters instruction triplets using a reward model to ensure quality, removing low-quality data. Second, it applies a k-center greedy algorithm to select a diverse seed dataset that maximizes coverage across instruction types using BERT embeddings. Third, it fine-tunes the LLM on the seed dataset and evaluates its performance to identify necessary instruction data that the model struggles with, which are then added as augmented data. The final dataset is a combination of the seed and augmented data, which is used for final fine-tuning.

## Key Results
- Fine-tuning with 4,000 MoDS-selected instruction pairs outperforms using the full dataset of 214k instruction data
- MoDS achieves SOTA results on five benchmarks: Koala, WizardLM, Self-instruct, Vicuna, and LIMA
- Ablation studies confirm that each component (quality, diversity, necessity) contributes to overall performance improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoDS improves LLM instruction-following by selecting a small set of high-quality, diverse, and necessary instruction data, rather than using the full dataset.
- Mechanism: The approach filters instruction triplets by quality using a reward model, selects diverse seed data via k-center greedy, and then identifies necessary data by evaluating where the model struggles.
- Core assumption: Quality of instruction data matters more than quantity for fine-tuning LLMs to follow instructions.
- Evidence anchors:
  - [abstract] "Experiments show that fine-tuning with 4,000 instruction pairs selected by MoDS achieves better performance than using the full dataset of 214k instruction data."
  - [section 3.1] "Recent advancements in instruction tuning... suggest that a small, high-quality dataset can significantly equip LLMs with instruction-following capabilities."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.457, average citations=0.0."
- Break condition: If the reward model cannot distinguish quality, or if the k-center greedy algorithm fails to ensure diversity, the selection may degrade performance.

### Mechanism 2
- Claim: Necessity evaluation identifies instruction data that the LLM currently fails on, filling capability gaps.
- Mechanism: After fine-tuning on the seed dataset, the model generates responses for all high-quality instructions; those with low review scores are deemed necessary and selected for augmentation.
- Core assumption: The model's weaknesses on certain instruction types can be identified by low reward model scores.
- Evidence anchors:
  - [section 3.2.3] "If the review scores are less than the threshold β, it represents that the initial LLM could not generate good responses for these instructions..."
  - [section 3.1] "Necessity indicates that the selected instruction data indeed fill the ability gap for the LLM of interested."
  - [corpus] "Average neighbor FMR=0.457, average citations=0.0."
- Break condition: If the necessity evaluation threshold is set too high or low, or if the model's performance is already uniform across instructions, the augmented set may be ineffective.

### Mechanism 3
- Claim: K-center greedy selection ensures broad coverage of instruction types.
- Mechanism: By iteratively selecting data points farthest from already chosen centers (using BERT embeddings), the algorithm maximizes diversity and coverage.
- Core assumption: Instructions with diverse embeddings correspond to diverse instruction-following capabilities.
- Evidence anchors:
  - [section 3.2.2] "K-Center greedy algorithm is proposed by (Sener and Savarese, 2017)... selects a subset of data points that are the farthest apart, thereby making the instruction data we collect are diverse and have broader coverage."
  - [section 3.2.2] "During diverse data selection process, we generate the sentence embeddings for all instructions with BERT (Devlin et al., 2018), which are used to compute the distances of different data points."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.457, average citations=0.0."
- Break condition: If instruction embeddings are not discriminative enough, or if the budget is too small, coverage may be insufficient.

## Foundational Learning

- Concept: Reward model-based quality scoring
  - Why needed here: To filter out low-quality instruction triplets before diversity selection
  - Quick check question: How does the reward model's threshold α affect the number and quality of selected instructions?

- Concept: K-center greedy algorithm
  - Why needed here: To maximize diversity and coverage of the seed dataset
  - Quick check question: What happens if we replace k-center greedy with random sampling for seed selection?

- Concept: Necessity evaluation via fine-tuning and review scoring
  - Why needed here: To identify and fill capability gaps specific to the target LLM
  - Quick check question: How does the necessity threshold β influence the augmented dataset size and model improvement?

## Architecture Onboarding

- Component map:
  Quality Evaluation Model → High-Quality Dataset → K-Center Greedy → Seed Instruction Dataset → Fine-tune LLM → Initial LLM → Necessity Evaluation Model → Augmented Dataset → Merge Seed + Augmented → Final Fine-tuning Dataset

- Critical path: Quality Evaluation → K-Center Greedy → Fine-tuning → Necessity Evaluation → Merge → Final Fine-tuning

- Design tradeoffs:
  - Smaller thresholds for quality/necessity evaluation increase dataset size but risk including lower-quality data
  - Larger thresholds reduce dataset size but may omit useful instructions
  - K-center greedy ensures diversity but can miss rare but important instruction types

- Failure signatures:
  - No performance gain vs full dataset → likely quality/necessity thresholds too permissive or k-center greedy not effective
  - Performance drops vs full dataset → likely over-aggressive filtering or necessity evaluation misses important instructions

- First 3 experiments:
  1. Run with α=0.0, β=0.0 on Alpaca to confirm baseline selection and compare with full dataset fine-tuning
  2. Vary α and β thresholds to observe effect on dataset size and downstream performance
  3. Replace k-center greedy with random sampling for seed selection to test diversity importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MoDS-selected instruction data scale with the size of the pre-training dataset? Would models trained on larger or smaller pre-training corpora require different amounts of instruction tuning data for optimal performance?
- Basis in paper: [inferred] The paper discusses how different LLMs require different instruction data due to variations in pre-training, but doesn't explore how pre-training dataset size affects the optimal amount of instruction data needed.
- Why unresolved: The experiments use a fixed pre-training model (LLaMA 2 7B) and vary only the instruction data selection, not the pre-training corpus size or content.
- What evidence would resolve it: Experiments comparing MoDS performance across LLMs with varying pre-training dataset sizes, or analyzing the relationship between pre-training data characteristics and optimal instruction data requirements.

### Open Question 2
- Question: What is the optimal threshold strategy for the quality evaluation model when dealing with instruction datasets of varying characteristics (e.g., domain specificity, instruction complexity)?
- Basis in paper: [explicit] The paper mentions using fixed thresholds (α=0.0 for Alpaca, α=1.0 for Mixture dataset) but acknowledges that different datasets may require different approaches.
- Why unresolved: The experiments use predetermined thresholds rather than an adaptive or learned threshold selection mechanism, and don't explore how threshold choice affects downstream performance across diverse datasets.
- What evidence would resolve it: A systematic study of threshold selection methods across multiple instruction datasets with varying characteristics, or an adaptive threshold determination algorithm.

### Open Question 3
- Question: How does the model-oriented necessity evaluation handle domain-specific instruction data where the pre-trained LLM may have limited relevant knowledge, potentially leading to false negatives in necessity detection?
- Basis in paper: [inferred] The necessity evaluation relies on the pre-trained LLM's ability to generate responses, but doesn't address scenarios where poor performance stems from domain knowledge gaps rather than instruction-following capability gaps.
- Why unresolved: The paper focuses on general instruction-following capabilities without examining how domain-specific knowledge limitations might confound the necessity evaluation process.
- What evidence would resolve it: Experiments testing necessity evaluation on domain-specific instruction datasets, or analysis of false negative rates in scenarios where pre-training data coverage is limited in certain domains.

## Limitations

- The choice of hyperparameters (α, β) for quality and necessity thresholds is not fully explored, and their impact on performance could be significant.
- The diversity metric relies on BERT embeddings, but instruction embeddings may not always capture semantic diversity effectively.
- The necessity evaluation assumes the reward model can accurately identify the model's weaknesses, which may not always hold true.

## Confidence

- **High confidence**: The core claim that a carefully selected subset of instruction data can outperform the full dataset is well-supported by the experimental results.
- **Medium confidence**: The effectiveness of the k-center greedy algorithm in ensuring diversity and coverage is plausible but depends on the quality of the embeddings.
- **Low confidence**: The necessity evaluation mechanism's ability to identify and fill capability gaps is promising but may be sensitive to threshold settings and reward model accuracy.

## Next Checks

1. Perform ablation studies to assess the impact of each component (quality filtering, diversity selection, necessity evaluation) on final performance.
2. Test the robustness of the method by varying the quality and necessity thresholds across different datasets and LLM sizes.
3. Evaluate the diversity of selected instructions using additional metrics beyond BERT embeddings to ensure comprehensive coverage.