---
ver: rpa2
title: 'JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity
  in Sign Language Processing'
arxiv_id: '2311.10174'
source_url: https://arxiv.org/abs/2311.10174
tags:
- language
- sign
- languages
- translation
- jwsign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces JWSign, a large multilingual dataset of Bible
  translations in 98 sign languages, totaling 2,530 hours of video data from over
  1,500 signers. The dataset is used to conduct machine translation experiments comparing
  bilingual and multilingual models, as well as models clustered by linguistic relatedness.
---

# JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity in Sign Language Processing

## Quick Facts
- **arXiv ID**: 2311.10174
- **Source URL**: https://arxiv.org/abs/2311.10174
- **Reference count**: 22
- **Primary result**: Multilingual training on JWSign improves sign language translation quality over bilingual baselines, especially in higher-resource scenarios.

## Executive Summary
This work introduces JWSign, a large multilingual dataset of Bible translations in 98 sign languages, totaling 2,530 hours of video data from over 1,500 signers. The dataset is used to conduct machine translation experiments comparing bilingual and multilingual models, as well as models clustered by linguistic relatedness. Results show that multilingual training improves translation quality compared to bilingual baselines, particularly in higher-resource scenarios. Clustering language pairs by linguistic family further improves performance when sufficient data is available. The dataset and experiments highlight the importance of diversity in sign language research and provide a foundation for future work in this field.

## Method Summary
The study uses JWSign, a multilingual sign language corpus containing Bible translations in 98 sign languages. Video data is preprocessed and features are extracted using a fine-tuned I3D model. Translation models are built using Transformer architecture, trained in various settings: bilingual (B36), multilingual (M36, M91, MFT), and clustered by linguistic relatedness (CSIG, CSPO). Models are evaluated using BLEU, BLEURT, and chrF metrics to compare translation quality across different training approaches.

## Key Results
- Multilingual training (M36) outperforms bilingual baselines (B36) in translation quality.
- Clustering language pairs by linguistic relatedness (CSIG, CSPO) further improves performance in higher-resource scenarios.
- Fine-tuning multilingual models on additional low-resource languages (MFT) is less effective than training from scratch (M91) in lower-resource scenarios.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual training improves translation quality over bilingual baselines.
- Mechanism: Jointly training on multiple sign languages allows the model to share representations and transfer knowledge across languages, reducing overfitting and improving generalization.
- Core assumption: Sign languages share sufficient structural similarities that a model trained on multiple languages can generalize better than individual bilingual models.
- Evidence anchors:
  - [abstract] "Our experiments highlight that multilingual systems are superior to bilingual baselines"
  - [section 4.2] "we train a single multilingual model using the 36 highest-resource language pairs (M36)"
  - [corpus] JWSign contains 98 sign languages from diverse geographic regions, providing a rich multilingual training set
- Break condition: If sign languages are too structurally dissimilar, multilingual training may introduce interference and hurt performance.

### Mechanism 2
- Claim: Clustering language pairs by linguistic relatedness improves translation quality in higher-resource scenarios.
- Mechanism: Grouping languages by shared linguistic features (either sign language families or spoken language families) allows the model to leverage positive transfer effects within each cluster.
- Core assumption: Languages within the same family share enough linguistic properties that training on them together is more beneficial than training on a random mix.
- Evidence anchors:
  - [abstract] "in higher-resource scenarios, clustering language pairs that are related improves translation quality"
  - [section 4.2] "we cluster the language pairs based on source sign language families... and train on each cluster separately (CSIG)"
  - [corpus] JWSign includes sign languages from different families as documented in Power et al. (2020) and Eberhard et al. (2023)
- Break condition: If clusters are too small or languages within clusters are not sufficiently similar, clustering provides no benefit and may even hurt performance.

### Mechanism 3
- Claim: Fine-tuning a multilingual model on lower-resource languages is less effective than training from scratch.
- Mechanism: The knowledge gained from higher-resource languages may not transfer well to very low-resource languages, and fine-tuning can lead to catastrophic forgetting.
- Core assumption: The model's learned representations from high-resource languages are not easily adaptable to the unique characteristics of very low-resource languages.
- Evidence anchors:
  - [abstract] "our experiments did not show a clear benefit for a fine-tuning approach in lower-resource scenarios"
  - [section 4.2] "we further explore a fine-tuning strategy... (MFT)" and compare it to M91
  - [corpus] JWSign has many very low-resource language pairs (less than 500 training samples) that would benefit from targeted training
- Break condition: If the model architecture and training procedure are specifically designed to preserve knowledge during fine-tuning, this mechanism may not hold.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper uses Transformer-based models for sign language translation, which rely on self-attention to capture long-range dependencies in both video and text sequences.
  - Quick check question: How does multi-head attention in Transformers help capture different aspects of sign language video sequences?

- Concept: Cross-lingual transfer learning
  - Why needed here: The paper explores multilingual training to leverage knowledge transfer between sign languages, which is only possible if the model can effectively share representations across languages.
  - Quick check question: What factors determine whether cross-lingual transfer is beneficial or harmful in multilingual training?

- Concept: Evaluation metrics for machine translation
  - Why needed here: The paper uses BLEU, BLEURT, and chrF to evaluate translation quality, each capturing different aspects of translation performance.
  - Quick check question: Why might BLEURT be more reliable than BLEU for evaluating sign language translation quality?

## Architecture Onboarding

- Component map: Video data → Preprocessing (resize, crop, normalize) → I3D feature extraction → Tokenization → Transformer model → BLEU/BLEURT/chrF evaluation
- Critical path: Load and preprocess video data → Extract features using I3D model → Tokenize target spoken language text → Train Transformer model (bilingual or multilingual) → Evaluate on test set using multiple metrics
- Design tradeoffs:
  - Bilingual vs multilingual training: Multilingual can improve generalization but may introduce interference
  - Vocabulary size: Larger vocabularies can handle more diverse languages but require more data to train effectively
  - Feature extraction: Vision-based approaches (I3D) vs pose-based approaches have different strengths for sign language
- Failure signatures:
  - Low BLEU but high BLEURT: Model may be generating semantically correct but not exact translations
  - High BLEU but low BLEURT: Model may be overfitting to reference translations without capturing true meaning
  - Performance drop with multilingual training: Sign languages may be too dissimilar for effective transfer
- First 3 experiments:
  1. Train a bilingual model (B36) on a single high-resource language pair to establish baseline performance
  2. Train a multilingual model (M36) on the same 36 language pairs to test transfer benefits
  3. Train clustered multilingual models (CSIG and CSPO) to test the impact of linguistic relatedness on translation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does clustering by sign language families consistently improve translation quality compared to bilingual baselines, or are there specific language families where this approach is particularly effective or ineffective?
- Basis in paper: [explicit] The paper compares clustering by sign language families (CSIG) to bilingual baselines (B36) and finds mixed results, with CSIG performing better for high-resource languages but worse for low-resource ones.
- Why unresolved: The results show conflicting trends, and the paper does not provide a detailed analysis of which sign language families benefit most from clustering or why the approach works differently across resource levels.
- What evidence would resolve it: A more granular analysis of translation quality across different sign language families, examining specific pairs where clustering succeeds or fails, and investigating linguistic factors that might explain these differences.

### Open Question 2
- Question: Is the fine-tuning approach (MFT) truly inferior to training from scratch on additional language pairs, or could hyperparameter optimization and different fine-tuning strategies yield better results?
- Basis in paper: [explicit] The paper compares MFT (fine-tuning M36 on additional language pairs) to M91 (training from scratch on all language pairs) and finds M91 performs better, suggesting fine-tuning is less effective.
- Why unresolved: The comparison is limited to a single fine-tuning strategy, and the paper does not explore whether alternative fine-tuning methods or hyperparameter tuning could improve results.
- What evidence would resolve it: Experiments testing various fine-tuning strategies (e.g., gradual unfreezing, different learning rates, or intermediate fine-tuning steps) to determine if a more effective approach exists.

### Open Question 3
- Question: How much does the choice of source language (e.g., English vs. French vs. Spanish) influence translation quality in JWSign, and could source language selection be optimized for better performance?
- Basis in paper: [inferred] The paper mentions that JWSign translations are not all from English and that different source languages are used depending on the country, but does not analyze how this affects translation quality.
- Why unresolved: The paper focuses on sign language-to-spoken language translation but does not investigate whether the source language choice introduces systematic biases or affects model performance.
- What evidence would resolve it: A controlled study comparing translation quality across different source languages for the same target sign language, or an analysis of how source language choice correlates with BLEU/BLEURT scores.

## Limitations
- Data Representativeness and Quality: JWSign is constructed from Bible translations, which may not represent the full linguistic diversity of natural sign language usage.
- Evaluation Metrics for Sign Language Translation: The paper relies on text-based metrics (BLEU, BLEURT, chrF) that may not fully capture semantic equivalence between sign and spoken languages.
- Fine-tuning Effectiveness: The paper reports that fine-tuning approaches did not show clear benefits for lower-resource scenarios, but the experimental setup and analysis are relatively brief.

## Confidence

**High Confidence Claims**:
- Multilingual training improves translation quality compared to bilingual baselines
- Clustering by linguistic relatedness provides benefits in higher-resource scenarios
- JWSign represents a significant advancement in multilingual sign language resources

**Medium Confidence Claims**:
- The superiority of multilingual systems over bilingual baselines generalizes beyond the specific language pairs tested
- The optimal number of languages for multilingual training (36 languages) applies broadly across sign language translation tasks
- The specific clustering approach (by sign language families or spoken language families) is optimal for all multilingual sign language translation scenarios

**Low Confidence Claims**:
- The reasons why fine-tuning fails for lower-resource languages (mechanism not fully explored)
- The long-term generalizability of findings to non-Bible-related sign language content
- The absence of catastrophic forgetting in multilingual models (not explicitly tested)

## Next Checks

1. **Human Evaluation Study**: Conduct a human evaluation study comparing model outputs to professional sign language translations across different domains (not just Bible content) to validate whether text-based metrics accurately reflect translation quality.

2. **Cross-Domain Generalization Test**: Evaluate the trained models on sign language content from domains outside of religious texts (e.g., conversational videos, educational content, news broadcasts) to assess whether the multilingual training approach generalizes to more diverse sign language usage patterns.

3. **Fine-tuning Analysis with Diagnostic Classifiers**: Implement diagnostic classifiers to analyze what knowledge is preserved or lost during fine-tuning from high-resource to low-resource sign languages. This would help identify specific failure modes and guide the development of more effective adaptation strategies for low-resource scenarios.