---
ver: rpa2
title: 'Black-box Adversarial Attacks against Dense Retrieval Models: A Multi-view
  Contrastive Learning Method'
arxiv_id: '2308.09861'
source_url: https://arxiv.org/abs/2308.09861
tags:
- document
- target
- attack
- retrieval
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new adversarial attack task called AREA,
  where the goal is to trick dense retrieval models into retrieving a target document
  that is outside the initial set of candidate documents retrieved by the model for
  a given query. The paper finds that existing adversarial attack methods designed
  for neural ranking models do not perform well against dense retrieval models due
  to the different architectures and behaviors of the two model types.
---

# Black-box Adversarial Attacks against Dense Retrieval Models: A Multi-view Contrastive Learning Method

## Quick Facts
- **arXiv ID:** 2308.09861
- **Source URL:** https://arxiv.org/abs/2308.09861
- **Reference count:** 40
- **Primary result:** Introduces MCARA, a multi-view contrastive learning method that outperforms existing adversarial attacks against dense retrieval models while maintaining naturalness of adversarial examples

## Executive Summary
This paper introduces the Adversarial Retrieval Attack task (AREA), where the goal is to trick dense retrieval models into retrieving a target document outside the initial candidate set. The authors find that existing adversarial attack methods designed for neural ranking models fail against dense retrieval models due to fundamental architectural differences. To address this, they propose MCARA, which uses multi-view contrastive learning to generate adversarial examples that effectively deceive DR models while retaining document semantics and fluency. Experiments on two benchmark datasets show MCARA achieves superior attack success rates and efficiency compared to existing methods.

## Method Summary
MCARA works by first training a surrogate model to mimic the target DR model's behavior. It then uses K-means clustering to derive multiple topical viewers from the initial candidate set, generating multi-view representations of the target document that are pulled toward their corresponding viewers while being pushed away from counter-viewers. The method optimizes a view-wise contrastive loss to guide adversarial perturbations, which are implemented through synonym substitution while maintaining fluency constraints. This approach exploits the representation-focused nature of DR models by creating discriminative semantic signals through multi-view alignment.

## Key Results
- MCARA outperforms existing attack strategies on dense retrieval models with higher attack success rates and lower time costs
- Multi-view representations significantly improve attack effectiveness compared to single-view approaches
- Adversarial examples generated by MCARA maintain naturalness as measured by perplexity and grammar error metrics
- The method successfully attacks both first-stage retrieval and subsequent re-ranking in the multi-stage search pipeline

## Why This Works (Mechanism)

### Mechanism 1
MCARA successfully attacks dense retrieval models by exploiting multi-view contrastive learning to align target document representations with candidate viewers while distancing them from counter-viewers. The method trains a surrogate model to mimic the target DR model, uses K-means clustering to derive multiple topical viewers from initial candidates, and generates multi-view representations that are pulled toward viewers and pushed away from counter-viewers. This view-wise supervision captures discriminative semantic signals that guide effective perturbations.

### Mechanism 2
Existing adversarial attack methods fail against dense retrieval models due to fundamental architectural differences between interaction-focused NRMs and representation-focused DR models. NRMs rely on fine-grained interaction signals through attention mechanisms, while DR models use coarse-grained document embeddings in dual-encoder architectures. Attack methods like PRADA and PAT that exploit interaction signals cannot effectively identify important perturbation directions for DR models, which depend on inter-document representativeness in embedding space.

### Mechanism 3
Multi-view representation learning enhances attack effectiveness by capturing fine-grained semantic information that single document representations miss. Instead of using a single embedding, MCARA generates n multi-view representations aligned with n viewers, encouraging each view to retain specific relevance signals while maintaining distinct information across views. This disentanglement process creates richer semantic representations that guide more precise perturbations.

## Foundational Learning

- **Concept: Contrastive learning and its application to representation learning**
  - Why needed here: MCARA fundamentally relies on contrastive learning to create effective supervision signals for adversarial perturbations. Understanding how contrastive learning works, including positive and negative pairs, temperature scaling, and different contrastive objectives is essential.
  - Quick check question: What is the difference between instance-level and view-level contrastive learning, and why does MCARA use view-level contrastive learning instead of instance-level?

- **Concept: Dense retrieval architecture and the dual-encoder model**
  - Why needed here: The entire attack strategy is designed specifically for dense retrieval models. Understanding how dual-encoder architectures work and how they differ from cross-encoder architectures is crucial for understanding why existing NRM attack methods fail.
  - Quick check question: How does the computational efficiency of dual-encoder architectures enable first-stage retrieval, and what trade-offs does this create in terms of representation quality compared to cross-encoder architectures?

- **Concept: Gradient-based adversarial attack methods and projected gradient descent**
  - Why needed here: MCARA uses projected gradient descent to generate adversarial perturbations in embedding space, then converts these to word-level substitutions. Understanding how gradient-based attacks work and how important words are identified through gradient magnitude is essential.
  - Quick check question: How does the choice of loss function affect the gradient directions computed for adversarial perturbations, and why is this particularly important for attacking DR models versus NRMs?

## Architecture Onboarding

- **Component map:** Surrogate Model Training -> Multi-View Representation Learning -> Attack via view-wise contrastive loss -> Synonym substitution with fluency constraints
- **Critical path:** Surrogate model training → Multi-view representation learning → Attack via view-wise contrastive loss → Synonym substitution with fluency constraints
- **Design tradeoffs:** 
  - Number of viewers (n) vs. computational cost and cluster quality
  - Perplexity threshold for synonym selection vs. attack effectiveness and naturalness
  - Single-view vs. multi-view representations vs. attack performance and complexity
  - Word importance threshold vs. perturbation magnitude and imperceptibility
- **Failure signatures:**
  - Low SRR despite successful surrogate model training: surrogate model doesn't accurately capture target DR model behavior
  - High grammar errors in adversarial examples: perplexity threshold too low or synonym selection inadequate
  - No improvement over baselines: insufficient viewers or poor clustering of initial candidates
  - High time cost: excessive number of viewers or inefficient gradient computation
- **First 3 experiments:**
  1. Validate surrogate model quality by comparing MRR@100 on held-out queries between surrogate and target DR model
  2. Test different numbers of viewers (n=3, 5, 10) on a small set of queries to find optimal balance between performance and cost
  3. Compare single-view vs. multi-view representations on the same query set to verify the claimed improvement in attack effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adversarial attacks against dense retrieval models generalize across different domains and languages?
- Basis in paper: The authors mention that most existing adversarial attack methods focus on neural ranking models, with little attention paid to dense retrieval models, and note the challenge of developing generalizable attack methods for black-box pipelines.
- Why unresolved: The paper primarily focuses on attacking dense retrieval models in a specific setting (MS MARCO datasets) and does not extensively explore generalizability across different domains, languages, or model architectures.
- What evidence would resolve it: Experiments demonstrating effectiveness on diverse datasets, languages, and model architectures would help assess generalizability.

### Open Question 2
- Question: How can we develop effective detection and defense mechanisms against adversarial attacks on dense retrieval models?
- Basis in paper: The authors conclude by highlighting the need for developing effective detection and defense mechanisms against such attacks to ensure robustness in information retrieval systems.
- Why unresolved: The paper focuses on attacking dense retrieval models and does not provide solutions for detecting or defending against these attacks.
- What evidence would resolve it: Proposals and evaluations of detection and defense mechanisms that can identify and mitigate adversarial attacks on dense retrieval models would help address this open question.

### Open Question 3
- Question: How do adversarial attacks against dense retrieval models impact the overall search pipeline, including subsequent re-ranking stages?
- Basis in paper: The authors discuss the multi-stage search pipeline and mention that if a target document fails to pass the first-stage retrieval, it will not have the chance to be promoted in the final ranked list, but they do not extensively explore the impact on subsequent re-ranking stages.
- Why unresolved: The paper primarily focuses on attacking the first-stage retrieval and does not thoroughly investigate the effects of adversarial attacks on the entire search pipeline, including re-ranking.
- What evidence would resolve it: Experiments demonstrating the impact of adversarial attacks on dense retrieval models on the performance of subsequent re-ranking stages would help understand the overall impact on the search pipeline.

## Limitations
- Reliance on surrogate model quality: The effectiveness depends on how well the surrogate model approximates the target DR model's behavior, which isn't thoroughly validated
- Limited generalizability testing: The method's performance across different domains, languages, and model architectures remains unexplored
- Automatic metric dependence: Claims about naturalness rely heavily on automatic metrics rather than comprehensive human evaluation

## Confidence

- **High Confidence:** The architectural distinction between interaction-focused NRMs and representation-focused DR models creating different vulnerability surfaces is well-supported by experimental evidence showing NRM attack methods failing on DR models
- **Medium Confidence:** The effectiveness of multi-view contrastive learning for DR model attacks is supported by experimental results but lacks direct evidence in the corpus, suggesting this may be a novel application
- **Low Confidence:** The claim that MCARA maintains naturalness while achieving high attack success rates relies heavily on automatic metrics (perplexity, grammar errors) rather than comprehensive human evaluation

## Next Checks

1. **Surrogate Model Fidelity Test:** Systematically vary the quality of the surrogate model approximation and measure corresponding changes in MCARA's attack success rate to establish the relationship between surrogate-target model alignment and attack effectiveness

2. **Viewer Cluster Quality Analysis:** Evaluate the semantic coherence of K-means clusters used for viewer identification using intrinsic cluster quality metrics (silhouette score, topic coherence) and measure how cluster quality correlates with attack performance across different cluster numbers

3. **Baseline Adaptation Experiment:** Implement NRM attack methods (PRADA, PAT) with DR-specific modifications such as representation-focused perturbation strategies rather than interaction-focused ones, then compare their performance against the original MCARA to determine whether architectural adaptations could close the performance gap