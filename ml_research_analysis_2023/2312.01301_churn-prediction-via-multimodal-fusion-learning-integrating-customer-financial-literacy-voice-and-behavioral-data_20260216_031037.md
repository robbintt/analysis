---
ver: rpa2
title: Churn Prediction via Multimodal Fusion Learning:Integrating Customer Financial
  Literacy, Voice, and Behavioral Data
arxiv_id: '2312.01301'
source_url: https://arxiv.org/abs/2312.01301
tags:
- churn
- customer
- fusion
- data
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study proposed a multimodal fusion learning model for predicting\
  \ customer churn. To the authors\u2019 best knowledge, this is the first attempt\
  \ to integrate diverse modalities including customer\u2019s voice (CV), financial\
  \ literacy (FL) survey, and CRM record data to predict churn risk at three levels\
  \ (low, mid, and high)."
---

# Churn Prediction via Multimodal Fusion Learning:Integrating Customer Financial Literacy, Voice, and Behavioral Data

## Quick Facts
- arXiv ID: 2312.01301
- Source URL: https://arxiv.org/abs/2312.01301
- Reference count: 17
- Key outcome: Multimodal fusion model predicts customer churn risk with 91.2% accuracy and 66 MAP score, outperforming single-modality and late fusion approaches.

## Executive Summary
This study introduces a novel multimodal fusion learning model for predicting customer churn risk by integrating three distinct data sources: customer voice recordings, financial literacy survey data, and CRM behavioral records. The approach employs a hybrid fusion strategy combining early and late fusion techniques to create coordinated feature representations across modalities. The model demonstrates superior performance compared to single-modality and late fusion approaches, achieving 91.2% test accuracy and 66 MAP score in classifying churn risk into three levels (low, mid, high).

## Method Summary
The proposed method involves training three independent unimodal models: SMOGN-COREG for inferring financial literacy from behavioral data, CNN-VGG16 for emotion recognition from voice recordings, and an ensemble ANN with SMOTE for baseline churn prediction. These models' outputs are mapped to a coordinated representation space using a translation matrix, then fused through a hybrid strategy combining early and late fusion decisions. The system evaluates performance using test accuracy, Mean Average Precision, and Macro-Averaged F1 score metrics.

## Key Results
- Hybrid fusion approach achieved 91.2% test accuracy, outperforming non-fusion and late fusion methods
- Model achieved MAP score of 66, demonstrating strong risk stratification capability
- Significant correlations identified between negative emotions, low financial literacy, and increased churn risk

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fusion improves churn prediction accuracy by integrating complementary signals from voice emotion, financial literacy, and CRM data.
- Mechanism: The model learns coordinated feature representations across modalities (FL, SER, and baseline churn) and fuses them using a hybrid decision-level strategy, enabling richer context than single-source models.
- Core assumption: Each modality contributes unique and non-redundant information about customer churn risk.
- Evidence anchors:
  - [abstract] "Our multimodal approach integrates customer sentiments, financial literacy (FL) level, and financial behavioral data..."
  - [section] "The proposed multimodal is able to analyze customer behavior from a diverse array of data sources..."
  - [corpus] Weak—no direct corpus mention of multimodal fusion for churn prediction; closest is telecom-specific ML models.
- Break condition: If modalities are highly correlated or one modality dominates, the complementary gain vanishes.

### Mechanism 2
- Claim: Financial literacy scores inferred via semi-supervised SMOGN-COREG improve modeling of minority churn classes.
- Mechanism: SMOGN-COREG synthesizes rare but important samples, balancing the FL target distribution and allowing better generalization for low-FL, high-churn customers.
- Core assumption: Unlabeled financial behavior data contains enough signal to infer literacy levels when combined with labeled survey data.
- Evidence anchors:
  - [abstract] "The FL model utilized a SMOGN-COREG supervised model to infer customer FL from their financial behavior data."
  - [section] "The proposed method boosts the learning process by generating synthetic samples for minority class."
  - [corpus] Missing—no corpus evidence for SMOGN-COREG usage in churn prediction.
- Break condition: If financial behavior data is too noisy or irrelevant to literacy, synthetic sampling adds noise instead of signal.

### Mechanism 3
- Claim: Emotion recognition from voice using harmonic-percussive Mel spectrograms captures dissatisfaction signals predictive of churn.
- Mechanism: CNN-VGG16 extracts acoustic features distinguishing positive vs. negative emotions; negative emotion is mapped to high churn risk via fusion rules.
- Core assumption: Vocal tone, pitch, and energy reliably indicate customer satisfaction levels in service calls.
- Evidence anchors:
  - [abstract] "The proposed SER model employed a pre-trained CNN-VGG16 model to discern customer emotions from their vocal attributes..."
  - [section] "We employed a pre-trained CNN to develop a framework decipher emotions from CV..."
  - [corpus] Weak—corpus contains sentiment/complaint identification studies but not CV emotion detection for churn.
- Break condition: If emotional content is too subtle or masked by noise, classification accuracy drops and fusion benefits disappear.

## Foundational Learning

- Concept: Multimodal machine learning and feature fusion
  - Why needed here: To combine heterogeneous data sources (text, voice, numeric) into a unified churn risk model.
  - Quick check question: What are the differences between early, late, and hybrid fusion strategies?
- Concept: Semi-supervised learning and synthetic minority over-sampling
  - Why needed here: To infer financial literacy from unlabeled behavioral data and balance rare churn cases.
  - Quick check question: How does SMOGN differ from SMOTE in handling regression targets?
- Concept: Speech emotion recognition and audio feature engineering
  - Why needed here: To quantify customer sentiment from call center interactions beyond text transcripts.
  - Quick check question: What acoustic features best differentiate positive from negative emotions in voice?

## Architecture Onboarding

- Component map: Data ingestion layer -> Unimodal learners (FL, SER, churn) -> Feature representation/translation layer -> Hybrid fusion engine -> Risk ranking/evaluation
- Critical path: Load and preprocess each modality → Train unimodal models independently → Map unimodal outputs to coordinated representation space → Apply hybrid fusion rules to generate risk score → Evaluate and iterate
- Design tradeoffs:
  - Simpler late fusion reduces complexity but loses early-stage interactions
  - Hybrid fusion adds overhead but captures complementary signals
  - Coordinated representation risks losing modality-specific nuance
- Failure signatures:
  - MAP or MA-F1 close to baseline → fusion adds no value
  - One modality dominates → others may be redundant
  - Misaligned feature scales → poor translation matrix performance
- First 3 experiments:
  1. Train and evaluate each unimodal model independently for baseline comparison.
  2. Implement late fusion only and compare MAP/MA-F1 vs baseline.
  3. Add hybrid fusion and measure gains in risk stratification accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed multimodal hybrid fusion model compare to a joint representation approach that concatenates features from different modalities before learning?
- Basis in paper: [inferred] The paper mentions future work exploring a joint representation approach to potentially enhance learning efficacy.
- Why unresolved: The current study only evaluates the proposed coordinated representation space and translation matrix approach, not a joint representation approach.
- What evidence would resolve it: Experimental results comparing the performance of the proposed hybrid fusion model to a joint representation model on the same dataset.

### Open Question 2
- Question: What is the impact of incorporating textual features as a fourth modality on the multimodal churn prediction model's performance?
- Basis in paper: [inferred] The paper suggests future work to extend the multimodal approach by integrating textual features as a fourth modality.
- Why unresolved: The current study only considers three modalities (voice, financial literacy, and CRM data) and does not include textual features.
- What evidence would resolve it: Experimental results showing the performance improvement when textual features are added as a fourth modality compared to the current three-modality model.

### Open Question 3
- Question: How do different weightings for the feature indicators in the translation dictionary affect the model's performance?
- Basis in paper: [explicit] The paper states that estimating suitable weights for each feature indicator is challenging and that weights were assigned based on expert knowledge.
- Why unresolved: The paper does not provide a systematic analysis of how different weightings impact the model's performance.
- What evidence would resolve it: Experimental results showing the performance of the model with different weightings for the feature indicators in the translation dictionary.

## Limitations
- Limited corpus evidence supporting the SMOGN-COREG semi-supervised approach for churn prediction
- No ablation studies quantifying individual modality contributions to overall performance
- Model complexity may limit practical deployment and interpretability

## Confidence
- Multimodal fusion effectiveness: Medium confidence due to lack of comparative baseline models
- SMOGN-COREG methodology superiority: Low confidence due to limited corpus support
- Speech emotion recognition for churn: Medium confidence due to weak corpus alignment with churn prediction applications

## Next Checks
1. Conduct ablation studies removing each modality to quantify individual contributions
2. Test the model on independent datasets to verify generalizability
3. Implement simpler baseline models to establish whether hybrid fusion provides meaningful improvements over less complex alternatives