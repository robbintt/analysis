---
ver: rpa2
title: Federated Fine-Tuning of Foundation Models via Probabilistic Masking
arxiv_id: '2311.17299'
source_url: https://arxiv.org/abs/2311.17299
tags:
- mask
- bitrate
- federated
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeltaMask enables fine-tuning of foundation models in federated
  learning with ultra-low communication costs by using stochastic masking and probabilistic
  filters to transmit only essential mask updates. It identifies high-performing subnetworks
  and compresses updates into compact grayscale images using BFuse filters, reducing
  bitrate to as low as 0.09 bits-per-parameter while maintaining performance comparable
  to full fine-tuning.
---

# Federated Fine-Tuning of Foundation Models via Probabilistic Masking

## Quick Facts
- arXiv ID: 2311.17299
- Source URL: https://arxiv.org/abs/2311.17299
- Reference count: 40
- One-line primary result: Achieves sub-1 bpp (as low as 0.09 bpp) in federated fine-tuning while maintaining accuracy comparable to full fine-tuning across 8 datasets and 5 architectures.

## Executive Summary
DeltaMask introduces a federated learning method for fine-tuning foundation models using stochastic masking and probabilistic filters to drastically reduce communication costs. By transmitting only essential mask updates as compressed grayscale images, it achieves ultra-low bitrates (down to 0.09 bpp) while preserving model performance. The approach leverages binary fuse filters to encode sparse mask differences, enabling efficient communication in resource-constrained edge devices. Evaluated across diverse datasets and architectures, DeltaMask consistently outperforms baselines in communication efficiency without sacrificing accuracy.

## Method Summary
DeltaMask employs stochastic masking to learn binary masks over frozen foundation model weights in a federated setting. Instead of transmitting full masks each round, it catalogs differences between current and prior masks, ranks updates by importance using KL divergence, and encodes only the top κ% using a binary fuse filter. The compressed updates are sent as grayscale images, decoded by the server, and aggregated via Bayesian update. The method allows adjustable bitrate via probabilistic filter parameters, adapting to heterogeneous client resources while maintaining accuracy comparable to full fine-tuning.

## Key Results
- Achieves bitrates as low as 0.09 bits-per-parameter while maintaining foundation model performance.
- Outperforms baselines like FedPM and DeepReduce in communication efficiency across 8 datasets and 5 architectures.
- Maintains accuracy comparable to full fine-tuning despite up to 6-fold reduction in communication cost.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeltaMask reduces communication bitrate by transmitting only essential mask updates using probabilistic filters.
- Mechanism: Instead of sending full masks each round, DeltaMask catalogs differences between current and prior masks, ranks these updates by importance (using KL divergence), and encodes only the top κ% using a binary fuse filter. The compressed updates are then sent as a grayscale image.
- Core assumption: Mask updates between rounds are sparse, especially after the model adapts quickly.
- Evidence anchors:
  - [abstract] "DeltaMask employs stochastic masking to detect highly effective subnetworks within FMs and leverage stochasticity and sparsity in client masks to compress updates into a compact grayscale image using probabilistic filters"
  - [section 3.2] "Instead of communicating the stochastic binary mask m, we significantly reduce the required bits-per-parameter (bpp) during the training process by solely communicating the subsequent key updates (indicated as position indexes set ∆) between the received and trained mask."
  - [corpus] Weak: Related works focus on federated fine-tuning but do not detail probabilistic filter usage for mask compression.
- Break condition: If mask updates become dense (e.g., early training or non-adaptive scenarios), the bitrate savings diminish because more updates must be transmitted.

### Mechanism 2
- Claim: DeltaMask achieves performance comparable to full fine-tuning while operating below 1 bpp.
- Mechanism: By learning binary masks over pre-trained foundation model weights and using stochastic mask training, DeltaMask preserves model accuracy. The probabilistic filter approach ensures that only minimal, high-impact mask changes are transmitted, enabling low-bitrate communication without sacrificing accuracy.
- Core assumption: Pre-trained foundation models have strong generalization; fine-tuning via masks can match full fine-tuning accuracy.
- Evidence anchors:
  - [abstract] "DeltaMask efficiently achieves bitrates as low as 0.09 bpp, enhancing communication efficiency while maintaining FMs performance, as measured on 8 datasets and 5 pre-trained models of various network architectures."
  - [section 5.1] "DeltaMask can maintain a learning procedure that results in better generalizable model, despite having up to a 6-fold reduction in the communication cost."
  - [corpus] Moderate: Similar works (FedPM, FedMask) achieve 1 bpp but do not reach sub-1 bpp while preserving accuracy.
- Break condition: If the foundation model's generalization is weak or the downstream task is highly specialized, mask-based fine-tuning may underperform full fine-tuning.

### Mechanism 3
- Claim: Adjustable bitrate via probabilistic filters allows DeltaMask to adapt to heterogeneous client resources.
- Mechanism: By varying the bits-per-entry (bpe) parameter in the probabilistic filter (e.g., 8, 16, 32 bpe), DeltaMask trades off between false positive rate and compression efficiency, enabling bitrate tuning per client constraints.
- Core assumption: Clients have varying computational and bandwidth constraints; adjustable compression can accommodate this.
- Evidence anchors:
  - [section 5.4] "DeltaMask enables an adjustable bitrate based on the bpe selection within the probabilistic filter, offering a potential solution to address the resource heterogeneity among clients in FL."
  - [section 3.2] "we demonstrate that DeltaMask enables an adjustable bitrate based on the bpe selection within the probabilistic filter"
  - [corpus] Weak: No direct corpus evidence for bitrate adjustment in federated learning via probabilistic filters.
- Break condition: If clients' heterogeneity is extreme, a single bpe setting may not optimize for all, leading to suboptimal performance for some clients.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: DeltaMask operates in an FL setting where multiple clients collaboratively train a model without centralizing data.
  - Quick check question: What are the main challenges of FL compared to centralized learning?

- Concept: Probabilistic Filters (Binary Fuse Filters)
  - Why needed here: They enable efficient compression of sparse mask updates by mapping keys to fixed-size fingerprints, reducing bitrate.
  - Quick check question: How do binary fuse filters differ from Bloom filters in terms of space efficiency and false positive rates?

- Concept: Stochastic Mask Training
  - Why needed here: Allows learning binary masks via sampling from probability distributions, enabling unbiased aggregation and efficient communication.
  - Quick check question: Why is stochastic mask training preferred over hard thresholding in federated settings?

## Architecture Onboarding

- Component map:
  Pre-trained foundation model (weights frozen) -> Local mask probability vectors (θ) -> Server-side global mask aggregation (Bayesian) -> Probabilistic filter encoder/decoder (BFuse) -> Top-κ ranking mechanism (KL divergence)

- Critical path:
  1. Server sends global mask θ_g,t-1 to clients
  2. Clients sample binary mask, train locally, compute mask differences
  3. Clients encode top κ% differences via BFuse, send grayscale image
  4. Server decodes, reconstructs masks, aggregates via Bayesian update

- Design tradeoffs:
  - Lower bpe in BFuse → higher false positives → potential accuracy loss
  - Higher κ → more updates transmitted → higher bitrate but better accuracy
  - Linear probing vs. full fine-tuning classifier head impacts initialization and convergence

- Failure signatures:
  - Accuracy plateaus or drops → check BFuse false positive rate or κ setting
  - Slow convergence → verify participation rate and aggregation reset frequency
  - High communication cost → inspect mask sparsity or BFuse efficiency

- First 3 experiments:
  1. Run DeltaMask on CIFAR-10 with ρ=1, compare bpp and accuracy vs FedPM.
  2. Vary bpe in BFuse (8, 16, 32) and measure accuracy vs bitrate trade-off.
  3. Test on non-IID data with ρ<1 to evaluate robustness to data heterogeneity.

## Open Questions the Paper Calls Out

- Question: How does DeltaMask's performance scale with extremely large foundation models beyond ViT-L/14, such as GPT-3 or PaLM with billions of parameters?
  - Basis in paper: [explicit] The paper mentions evaluation with CLIP ViT-L/14 and DINOv2-Large, but does not explore models with billions of parameters.
  - Why unresolved: The authors note their method works "across diverse pre-trained architectures" but don't test the theoretical scalability limits or performance degradation when scaling to truly massive models.
  - What evidence would resolve it: Systematic experiments testing DeltaMask on models with 10B+ parameters, measuring both accuracy retention and bitrate efficiency at scale.

- Question: What is the impact of DeltaMask on fairness across heterogeneous client populations in non-IID settings?
  - Basis in paper: [inferred] The paper mentions that "aspects like fairness... in FL still require further exploration" but doesn't investigate differential performance across client groups with varying data distributions.
  - Why unresolved: While the paper shows overall accuracy improvements, it doesn't analyze whether certain client groups (e.g., those with minority classes or limited data) experience disproportionate accuracy drops.
  - What evidence would resolve it: Fairness metrics disaggregated by client group, showing accuracy variance across clients with different data characteristics.

- Question: How does DeltaMask's privacy guarantee compare to other privacy-preserving FL techniques like differential privacy?
  - Basis in paper: [explicit] The paper states that "protecting privacy is essential" and mentions that "absolute privacy guarantees is not the primary objective of DeltaMask," but doesn't provide formal privacy analysis or comparison to DP-SGD.
  - Why unresolved: The paper claims hashing operations "inherently boost privacy" but doesn't quantify this or compare it to established privacy frameworks.
  - What evidence would resolve it: Formal privacy analysis (e.g., Rényi DP or zCDP bounds) comparing DeltaMask's privacy guarantees to DP-SGD with comparable accuracy.

## Limitations
- Limited evaluation to image classification tasks, leaving open questions about performance on other modalities or more complex downstream tasks.
- Lack of detailed implementation specifics for the topκ mechanism and BFuse hashing parameters creates uncertainty in reproducing exact results.
- Claims of maintaining full fine-tuning accuracy at ultra-low bitrates rely heavily on assumptions about mask sparsity and effectiveness of probabilistic filters.

## Confidence
- **High Confidence**: The core mechanism of using probabilistic filters for mask compression is well-established, and the empirical results showing bitrate reduction are reproducible.
- **Medium Confidence**: The claim of maintaining full fine-tuning accuracy at ultra-low bitrates is supported by experiments but depends critically on the correct implementation of the topκ selection and BFuse parameters.
- **Low Confidence**: The generalizability of DeltaMask to non-image tasks and highly non-IID data distributions remains unproven due to limited evaluation scope.

## Next Checks
1. Validate BFuse Implementation: Implement and test the binary fuse filter with varying bpe (8, 16, 32) to measure the trade-off between false positive rates and bitrate compression. Verify that the reconstructed masks match the original mask updates within acceptable error margins.
2. Evaluate Topκ Mechanism Robustness: Conduct ablation studies by varying the topκ threshold (e.g., 0.5, 0.8, 1.0) and measure its impact on both accuracy and communication cost across different datasets and model architectures.
3. Test on Non-IID and Multi-Modal Data: Extend experiments to include non-IID data distributions (e.g., Dirichlet parameter < 0.1) and non-image tasks (e.g., text classification) to assess the robustness and generalizability of DeltaMask beyond the current evaluation scope.