---
ver: rpa2
title: Towards Unsupervised Graph Completion Learning on Graphs with Features and
  Structure Missing
arxiv_id: '2309.02762'
source_url: https://arxiv.org/abs/2309.02762
tags:
- node
- structure
- features
- missing
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving graph neural networks
  (GNN) on graphs with both node features and structure missing. The authors propose
  a novel unsupervised graph completion learning (UGCL) framework that avoids the
  reliance on labels and bias in reconstructed node features and structure relationships
  found in existing supervised methods.
---

# Towards Unsupervised Graph Completion Learning on Graphs with Features and Structure Missing

## Quick Facts
- arXiv ID: 2309.02762
- Source URL: https://arxiv.org/abs/2309.02762
- Reference count: 35
- Key outcome: UGCL improves node classification accuracy by up to 5.51% on graphs with up to 75% missing features and structure

## Executive Summary
This paper addresses the challenge of graph neural networks (GNN) performing on graphs with both node features and structure missing. The authors propose an unsupervised graph completion learning (UGCL) framework that avoids reliance on labels and bias in reconstructed node features found in existing supervised methods. UGCL separates feature reconstruction and structure reconstruction, then introduces a dual contrastive loss on both levels to maximize mutual information between node representations from different paths.

The method achieves state-of-the-art performance on eight real-world graph datasets, demonstrating effectiveness across different GNN variants and missing rates. By decoupling the reconstruction tasks and using contrastive learning for supervision, UGCL provides a novel approach to graph completion that outperforms previous methods like T2-GCN while maintaining flexibility to work with various GNN architectures.

## Method Summary
UGCL implements a dual-path reconstruction framework where feature reconstruction uses an MLP with inner-product decoder and structure reconstruction uses PPNP with Personalized PageRank. The method introduces a dual contrastive loss at both feature and structure levels to maximize mutual information between node representations from the two paths. An attention-based fusion layer then combines the reconstructed features weighted by their importance for the downstream task. The framework operates unsupervised, avoiding reliance on labeled data while providing effective supervision through contrastive objectives.

## Key Results
- Achieves up to 5.51% improvement in node classification accuracy compared to previous best method T2-GCN
- Demonstrates effectiveness across eight real-world graph datasets with up to 75% missing features and structure
- Shows consistent performance improvements across three GNN variants (GCN, GraphSAGE, GAT) and five different missing rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating feature reconstruction and structure reconstruction prevents the mismatch between missing node features and structure during message-passing in GNN.
- Mechanism: By decoupling the two reconstruction paths, the model avoids passing incorrect or incomplete structural information to feature completion and vice versa. Each path can specialize in its respective reconstruction task without interference.
- Core assumption: The message-passing scheme of GNN inherently causes misalignment when both node features and structure are partially missing simultaneously.
- Evidence anchors:
  - [abstract] "To avoid the mismatch between missing node features and structure during the message-passing process of GNN, we separate the feature reconstruction and structure reconstruction and design its personalized model in turn."
  - [section II-B] "The great success of most of the existing GCL methods [22], [28] highly rely on the assumption that either the original node feature attributes are missing or the structure relationships are missing."
- Break condition: If the decoupling leads to information loss that cannot be recovered by the dual contrastive loss, or if the separate paths cannot effectively communicate learned representations to each other.

### Mechanism 2
- Claim: Dual contrastive loss on both structure and feature levels maximizes mutual information between node representations from different reconstruction paths.
- Mechanism: By introducing contrastive losses at both levels, the model encourages consistency between representations learned from feature reconstruction and structure reconstruction paths. This provides additional supervision signals without relying on labeled data.
- Core assumption: Maximizing mutual information between representations from different paths can effectively guide reconstruction quality and provide meaningful supervision in unsupervised setting.
- Evidence anchors:
  - [abstract] "a dual contrastive loss on the structure level and feature level is introduced to maximize the mutual information of node representations from feature reconstructing and structure reconstructing paths"
  - [section II-D] "To reconstruct the effective node features and structure relationships for the downstream task, we design a dual contrastive loss on the structure level and feature level to maximize the mutual information of node representations from feature reconstruction and structure reconstruction paths"
- Break condition: If the contrastive loss optimization becomes unstable or if the representations from different paths fail to align despite the contrastive objective.

### Mechanism 3
- Claim: Attention-based feature fusion combines reconstructed node features from different paths weighted by their importance for the downstream task.
- Mechanism: After reconstruction, the model uses attention mechanism to determine the relative importance of features from the feature reconstruction path versus the structure reconstruction path, creating a weighted combination that best serves the downstream task.
- Core assumption: The attention mechanism can effectively learn which source of reconstructed features is more reliable or relevant for each node and task.
- Evidence anchors:
  - [section II-D] "Considering the difference between reconstructed node features X F R and the learned node representations Z SR aggregating the reconstructed structure information ˆASR, the widely-used attention mechanism is introduced to analyze the importance of Z SR and ˆASR for the current downstream task"
- Break condition: If the attention mechanism fails to learn meaningful importance weights or if the weighted combination performs worse than using either path alone.

## Foundational Learning

- Concept: Graph Neural Networks and message-passing mechanism
  - Why needed here: Understanding how GNNs aggregate information from neighbors is crucial to grasp why missing features and structure cause problems and why decoupling helps
  - Quick check question: How does a standard GCN aggregate information from neighboring nodes, and what happens when some neighbors have missing features?

- Concept: Self-supervised learning and contrastive learning
  - Why needed here: The method relies on contrastive losses to provide supervision without labels, so understanding how contrastive learning works is essential
  - Quick check question: In contrastive learning, what is the goal when comparing positive pairs versus negative pairs, and how does this apply to the dual contrastive loss in UGCL?

- Concept: Personalized PageRank and its role in structure reconstruction
  - Why needed here: The structure reconstruction path uses Personalized PageRank to generate initial structure relationships, so understanding this technique is important
  - Quick check question: How does Personalized PageRank generate ranking scores between nodes, and why is it useful for structure reconstruction in graphs with missing information?

## Architecture Onboarding

- Component map: Feature reconstruction path (MLP + inner-product decoder) -> Structure reconstruction path (PPNP with Personalized PageRank + positional encoding) -> Dual contrastive loss module -> Attention-based feature fusion layer -> Downstream task

- Critical path: The most critical execution path is the forward pass through both reconstruction paths followed by the dual contrastive loss computation. This path determines how well the model learns to reconstruct missing information and how effectively the supervision signals guide the optimization. Any failure in this path directly impacts the quality of reconstructed features and structure.

- Design tradeoffs: The decoupling of feature and structure reconstruction trades off some potential information sharing between the two tasks for more accurate individual reconstruction. The dual contrastive loss adds computational overhead but provides crucial unsupervised supervision. The attention-based fusion adds a small amount of parameters but allows adaptive combination of reconstruction sources based on their reliability.

- Failure signatures: If the model fails to converge, check whether the contrastive loss magnitudes are balanced and whether the learning rates are appropriate for both reconstruction paths. If performance is poor, verify that the decoupling isn't causing too much information isolation - the paths should still produce compatible representations. If the attention fusion fails, check whether the reconstructed features from different paths are too dissimilar to meaningfully compare.

- First 3 experiments:
  1. Run with only the feature reconstruction path (disable structure reconstruction) to verify that the model can still learn useful feature representations and that the contrastive loss on feature level works independently
  2. Run with only the structure reconstruction path (disable feature reconstruction) to verify that structure completion works and that PPNP with Personalized PageRank can generate meaningful connections
  3. Run with both paths enabled but without the attention fusion layer (simple concatenation) to establish a baseline and verify that the dual contrastive loss can guide both paths effectively before adding the complexity of attention-based combination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does UGCL's performance scale with increasing amounts of missing data beyond the 75% tested, and what is the theoretical limit of its effectiveness?
- Basis in paper: [explicit] The paper tests up to 75% missing data but doesn't explore higher percentages or theoretical limits.
- Why unresolved: The paper only reports results up to 75% missing data and doesn't provide theoretical analysis of performance limits.
- What evidence would resolve it: Additional experiments with missing rates above 75% and mathematical analysis of convergence properties as missing data approaches 100%.

### Open Question 2
- Question: Can UGCL's dual contrastive loss framework be extended to other self-supervised learning tasks beyond node classification, such as link prediction or graph classification?
- Basis in paper: [explicit] The paper states UGCL can be combined with "any existing GNN variants" but only evaluates on node classification.
- Why unresolved: The paper only demonstrates UGCL on node classification tasks, leaving open whether the framework generalizes to other graph learning tasks.
- What evidence would resolve it: Experiments applying UGCL to link prediction and graph classification tasks with performance comparisons to task-specific methods.

### Open Question 3
- Question: What is the computational complexity of UGCL compared to supervised methods, and how does it scale with graph size and missing rate?
- Basis in paper: [inferred] The paper doesn't discuss computational complexity or runtime comparisons between UGCL and supervised methods.
- Why unresolved: The paper focuses on accuracy improvements but doesn't analyze the computational trade-offs or scalability of the unsupervised approach.
- What evidence would resolve it: Detailed complexity analysis and runtime experiments comparing UGCL to supervised GCL methods across graphs of varying sizes and missing rates.

## Limitations
- The method's effectiveness on graphs with more than 75% missing data remains untested, leaving uncertainty about its breaking point.
- Computational complexity and scalability to very large graphs are not addressed, potentially limiting practical deployment.
- Evaluation focuses primarily on node classification, leaving open questions about performance on other graph learning tasks.

## Confidence
- **High confidence**: The core mechanism of decoupling feature and structure reconstruction is well-supported by the empirical results and theoretical motivation. The improvement over baselines (up to 5.51%) is substantial and consistent across datasets.
- **Medium confidence**: The effectiveness of the dual contrastive loss in unsupervised settings is supported by results but relies on assumptions about mutual information maximization that could be more rigorously validated.
- **Medium confidence**: The attention-based feature fusion component shows promise but has limited ablation study coverage to isolate its specific contribution.

## Next Checks
1. **Ablation study focus**: Systematically remove the dual contrastive loss while keeping both reconstruction paths to measure the exact contribution of unsupervised supervision versus simple reconstruction.
2. **Sensitivity analysis**: Vary the missing rates beyond 75% to test the method's breaking point and understand how performance degrades as graph information becomes increasingly sparse.
3. **Scalability benchmark**: Test UGCL on larger graph datasets (e.g., OGB datasets) to assess computational requirements and memory usage patterns, particularly for the Personalized PageRank and contrastive loss computations.