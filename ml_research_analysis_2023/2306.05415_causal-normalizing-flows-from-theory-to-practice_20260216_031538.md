---
ver: rpa2
title: 'Causal normalizing flows: from theory to practice'
arxiv_id: '2306.05415'
source_url: https://arxiv.org/abs/2306.05415
tags:
- causal
- page
- graph
- distribution
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper shows that causal models are identifiable from observational\
  \ data when given a causal ordering, enabling recovery via autoregressive normalizing\
  \ flows (ANFs). It proposes causal normalizing flows that satisfy necessary design\
  \ conditions\u2014expressiveness, causal consistency, and causal path preservation\u2014\
  and implements a do-operator to answer interventional and counterfactual queries."
---

# Causal normalizing flows: from theory to practice

## Quick Facts
- arXiv ID: 2306.05415
- Source URL: https://arxiv.org/abs/2306.05415
- Reference count: 0
- This paper shows causal models are identifiable from observational data given causal ordering, enabling recovery via autoregressive normalizing flows (ANFs).

## Executive Summary
This paper bridges causal inference theory and practice by demonstrating that causal models are identifiable from observational data when given a causal ordering. The authors propose causal normalizing flows (causal NFs) that satisfy three key design conditions: expressiveness, causal consistency, and causal path preservation. They implement a do-operator for interventional and counterfactual queries by modifying the exogenous distribution rather than replacing causal equations. The framework works with mixed discrete-continuous data and partial causal graph knowledge, achieving state-of-the-art performance on both synthetic and real-world fairness tasks.

## Method Summary
The approach learns causal relationships by training autoregressive normalizing flows (ANFs) that respect causal structure. Given partial causal knowledge (either ordering π or graph G), the model uses either a generative architecture (u → x) with multiple layers or an abductive architecture (x → u) with a single layer to ensure causal consistency and path preservation. The do-operator is implemented by modifying the exogenous distribution Pu rather than replacing causal equations, enabling both interventional and counterfactual inference. The method handles mixed discrete-continuous data through dequantization and performs causal consistency checks via Jacobian regularization when causal graph knowledge is incomplete.

## Key Results
- Achieves lower KL divergence (0.000–0.001) compared to competing methods
- Better ATE estimation with RMSE of 0.02–0.12
- Faster inference at 0.4–1.5 µs per sample
- Successfully handles mixed discrete-continuous data and partial causal graph knowledge
- Validated on real-world fairness tasks with superior performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal models are identifiable from observational data given a causal ordering.
- Mechanism: Leverages non-linear ICA theory showing that two elements of the family F × Pu producing the same observational distribution must differ only by an invertible, component-wise transformation of variables u. This identifiability allows recovering true exogenous variables up to component-wise transformations.
- Core assumption: Data-generating process is invertible and both function and inverse are continuously differentiable (C^1-diffeomorphic assumption).
- Evidence anchors: Abstract states causal models are identifiable from observational data given causal ordering; Theorem 1 in section 3 proves identifiability under stated conditions.

### Mechanism 2
- Claim: Causal NFs can implement the do-operator for interventions and counterfactuals.
- Mechanism: Modifies exogenous distribution Pu while keeping causal generator untouched. For do(xi = α), density of P^u becomes p^I(u) ∝ p(u) · δ{˜fi(x,u)=α}(u), restricting plausible u values to those yielding intervened value α.
- Core assumption: Causal NF successfully isolates exogenous variables and preserves true causal paths from u to x.
- Evidence anchors: Abstract describes implementation of do-operator in causal NFs; section 5 provides detailed explanation and algorithms.

### Mechanism 3
- Claim: Specific architectural designs ensure causal consistency and path preservation.
- Mechanism: Generative model (u → x) with masking based on causal graph G ensures ∇uT^−1_θ(u) ≡ I + Σ∞n=1 Gn for sufficient layers; abductive model (x → u) with single layer ensures both causal consistency and path preservation by design.
- Core assumption: Knowledge of causal graph G or causal ordering π is available to guide architectural choices.
- Evidence anchors: Section 4 analyzes generative and abductive models with respective design choices; section 6.1 confirms abductive model with L=1 outperforms other designs.

## Foundational Learning

- Concept: Structural Causal Models (SCMs) and causal graphs
  - Why needed here: Understanding SCMs is fundamental to grasping how causal relationships are represented and manipulated in the proposed framework
  - Quick check question: What is the relationship between the causal adjacency matrix G and the structural equations in an SCM?

- Concept: Normalizing Flows and autoregressive architectures
  - Why needed here: Causal NFs build upon normalizing flows, specifically autoregressive flows, to learn the data-generating process
  - Quick check question: How does the autoregressive property of normalizing flows ensure triangular Jacobian matrices?

- Concept: Identifiability in non-linear ICA
  - Why needed here: The theoretical foundation for why causal models are identifiable relies on results from non-linear ICA theory
  - Quick check question: What are the key conditions under which non-linear ICA problems are identifiable?

## Architecture Onboarding

- Component map:
  - Input layer: Exogenous variables u (for generative) or observed variables x (for abductive)
  - Core architecture: Single layer autoregressive normalizing flow (MAF or NSF)
  - Output layer: Observed variables x (for generative) or exogenous variables u (for abductive)
  - Optional: Jacobian regularization for causal consistency when causal graph is unknown

- Critical path:
  1. Obtain partial causal knowledge (ordering π or graph G)
  2. Choose appropriate architecture (generative vs. abductive)
  3. Train causal NF using MLE on observational data
  4. Validate causal consistency and path preservation
  5. Apply do-operator for interventions/counterfactuals

- Design tradeoffs:
  - Generative (u → x): Requires L ≥ diam G layers for causal consistency, but slower training
  - Abductive (x → u): Single layer ensures causal consistency and path preservation, faster training
  - Jacobian regularization: Ensures causal consistency but adds computational overhead
  - Flow architecture: MAF (simpler) vs. NSF (more expressive)

- Failure signatures:
  - Poor KL divergence: Model fails to capture observational distribution
  - High RMSE on ATE: Model fails to preserve causal paths
  - High RMSE on counterfactuals: Model fails to isolate exogenous variables
  - Non-zero L(∇xT_θ(x)): Model violates causal consistency

- First 3 experiments:
  1. Train abductive causal NF with single layer on 3-chain SCM and verify observational fit
  2. Test do-operator implementation on root node intervention and validate results
  3. Gradually increase SCM complexity and compare with CAREFL and VACA baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can causal normalizing flows be extended to handle hidden confounders in causal discovery?
- Basis in paper: The paper discusses causal sufficiency as an assumption and mentions that hidden confounders may break this assumption. It also suggests that future work could address this by using interventional data to handle hidden confounders.
- Why unresolved: The paper does not provide a concrete method for handling hidden confounders within the causal normalizing flow framework.
- What evidence would resolve it: A proposed extension to the causal normalizing flow framework that can effectively handle hidden confounders, validated on datasets with known hidden confounders.

### Open Question 2
- Question: What are the implications of using alternative loss functions (e.g., flow matching) instead of maximum likelihood estimation for learning causal normalizing flows?
- Basis in paper: The paper mentions that future work could explore alternative losses other than MLE, such as flow matching.
- Why unresolved: The paper only uses MLE for learning causal normalizing flows and does not investigate the performance of other loss functions.
- What evidence would resolve it: Empirical comparisons of causal normalizing flows trained with different loss functions (e.g., MLE vs. flow matching) on the same datasets, demonstrating the impact on performance and causal inference accuracy.

### Open Question 3
- Question: How can causal normalizing flows be applied to causal discovery problems?
- Basis in paper: The paper mentions that it would be exciting to see causal NFs applied to other problems such as causal discovery.
- Why unresolved: The paper focuses on causal inference and does not explore the application of causal normalizing flows to causal discovery.
- What evidence would resolve it: A proposed method for using causal normalizing flows in causal discovery, validated on synthetic and real-world datasets with known causal structures.

### Open Question 4
- Question: How can causal normalizing flows be used to ensure counterfactual fairness in decision-making systems?
- Basis in paper: The paper mentions that it would be exciting to see causal NFs applied to fair decision-making.
- Why unresolved: The paper only demonstrates the use of causal normalizing flows for fairness auditing and classification, but does not explore their potential for ensuring counterfactual fairness in decision-making systems.
- What evidence would resolve it: A proposed method for using causal normalizing flows to train and evaluate decision-making systems that are counterfactually fair, validated on real-world datasets with known biases.

## Limitations

- Theoretical identifiability relies on strong C^1-diffeomorphic assumptions about the data-generating process that may not hold in practice
- Performance degrades when causal graph knowledge is incomplete or noisy, with limited diagnostic signals during training
- Computational complexity of O(Ld²) for autoregressive flows may become prohibitive for high-dimensional real-world datasets
- Current framework focuses on causal inference rather than causal discovery, limiting applicability when causal structure is unknown

## Confidence

- Identifiability claims: Medium - relies on non-linear ICA theory with strong assumptions
- Do-operator implementation: High - clear theoretical justification and successful experimental validation
- Architectural guarantees: Medium - depends critically on accurate causal graph knowledge

## Next Checks

1. **Robustness to Noisy Causal Knowledge**: Systematically vary the accuracy of the input causal ordering (from perfect to random) and measure degradation in both observational fit (KL divergence) and interventional accuracy (ATE RMSE) to establish practical limits of the approach.

2. **Real-World Causal Discovery Integration**: Integrate with established causal discovery algorithms (e.g., PC, FCI) to evaluate how the quality of automatically discovered causal orderings affects the causal NF's performance, particularly in the abductive architecture where causal consistency is guaranteed by design.

3. **Scalability Benchmark**: Evaluate the framework on high-dimensional real-world datasets (e.g., NHANES, UK Biobank) with thousands of variables to assess whether the O(Ld²) computational complexity of autoregressive flows becomes prohibitive and whether alternative architectures (e.g., non-autoregressive flows with causal constraints) can maintain identifiability guarantees.