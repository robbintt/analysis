---
ver: rpa2
title: How Far Can We Extract Diverse Perspectives from Large Language Models?
arxiv_id: '2311.09799'
source_url: https://arxiv.org/abs/2311.09799
tags:
- opinions
- diversity
- criteria
- statement
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the extent to which large language models
  (LLMs) can generate diverse perspectives on subjective topics. A criteria-based
  prompting technique is introduced to ground diverse opinions by having the model
  generate stance, reasons, and criteria words.
---

# How Far Can We Extract Diverse Perspectives from Large Language Models?

## Quick Facts
- arXiv ID: 2311.09799
- Source URL: https://arxiv.org/abs/2311.09799
- Authors: 
- Reference count: 40
- One-line primary result: LLMs can generate diverse perspectives on subjective topics, with diversity coverage depending on task subjectivity and measured through criteria clusters rather than semantic similarity.

## Executive Summary
This study investigates the extent to which large language models can generate diverse perspectives on subjective topics by introducing a criteria-based prompting technique. The method grounds diverse opinions by having the model generate stance, reasons, and criteria words that capture underlying values. Through step-by-step recall prompting, the research explores how far LLMs can extract diverse perspectives by iteratively generating more opinions. Experiments across four subjective tasks reveal that semantic diversity does not always correlate with perspective diversity, and that LLMs reach a saturation point in perspective generation depending on task subjectivity, with argumentation tasks yielding more diverse opinions than hate speech labeling.

## Method Summary
The study employs a criteria-based prompting technique where LLMs are prompted to generate stance, reasons, and criteria words for subjective statements. This structured approach forces the model to articulate values underlying opinions. The researchers then use step-by-step recall prompting, where previously generated opinions serve as demonstrations to iteratively generate more diverse perspectives. Perspective diversity is measured by clustering criteria words and counting unique clusters, while semantic diversity is assessed through cosine distance between sentence embeddings. The approach is tested across four subjective tasks: social norms, argumentation, hate speech labeling, and story continuation, with both automated and human evaluations comparing LLM performance against human-generated perspectives.

## Key Results
- Semantic diversity does not always correlate with perspective diversity; diverse perspectives are better captured by distinct criteria clusters than semantic similarity
- LLMs reach saturation points in perspective generation, with highly subjective tasks like argumentation requiring around 11 generated opinions to reach 80% coverage
- Criteria-based prompting produces more diverse opinions compared to free-form prompting, with humans providing fewer opinions on stances they disagree with
- The number of unique criteria clusters varies by task type, with argumentation tasks yielding more diverse opinions than hate speech labeling tasks

## Why This Works (Mechanism)

### Mechanism 1: Criteria-Based Prompting Grounds Perspective Diversity
The criteria-based prompting technique forces LLMs to explicitly generate stance, reasons, and criteria words, grounding diverse opinions in articulated values and principles. This structured approach ensures that generated perspectives are based on explicit criteria rather than vague reasoning.

### Mechanism 2: Step-by-Step Recall Prompts Maximize Diversity Coverage
The iterative step-by-step recall prompting method uses previously generated opinions as demonstrations to prompt the model for more opinions, systematically exploring the space of possible perspectives and pushing the model to generate novel viewpoints.

### Mechanism 3: Perspective Diversity â‰  Semantic Diversity
The study demonstrates that measuring diversity through semantic embeddings (cosine distance) does not capture perspective diversity, which is better measured by grouping criteria words into clusters. Different semantic expressions can represent the same perspective, while diverse perspectives are distinguished by distinct underlying values.

## Foundational Learning

- Concept: Perspective Diversity vs. Semantic Diversity
  - Why needed here: The study distinguishes between generating semantically diverse text and generating diverse perspectives, which is crucial for understanding the LLM's capability
  - Quick check question: Can two opinions have similar semantic content but represent different perspectives based on their underlying criteria?

- Concept: Criteria-Based Prompting
  - Why needed here: The method of prompting the model to generate stance, reasons, and criteria words is central to the study's approach to extracting diverse perspectives
  - Quick check question: How does asking for criteria words help the model generate more diverse opinions compared to free-form prompting?

- Concept: Step-by-Step Recall Prompting
  - Why needed here: The iterative method of prompting the model to generate more opinions is key to measuring the maximum diversity coverage
  - Quick check question: Why does incrementally increasing the number of opinions in the prompt help in exploring more diverse perspectives?

## Architecture Onboarding

- Component map: Subjective statement -> Criteria-based prompting -> Generate stance/reasons/criteria -> Step-by-step recall prompting -> Iterate generation -> Measure semantic diversity (cosine distance) -> Measure perspective diversity (criteria clustering)
- Critical path: The core workflow is to prompt the LLM with a subjective statement, extract diverse opinions with criteria, measure semantic and perspective diversity, and iterate with step-by-step recall prompting
- Design tradeoffs: Using structured criteria-based prompting may limit the model's creative expression but ensures diverse perspectives are grounded in explicit values. The step-by-step recall prompting may be computationally intensive but provides a thorough exploration of diversity
- Failure signatures: If the criteria words are not diverse or meaningful, the perspective diversity evaluation will fail. If the semantic diversity metric does not correlate with human judgment, the semantic diversity evaluation may not be reliable
- First 3 experiments:
  1. Compare criteria-based prompting vs. free-form prompting on a small dataset to see if criteria-based prompting generates more diverse perspectives
  2. Apply step-by-step recall prompting on the same dataset to measure the saturation point of diversity coverage
  3. Test the prompting method on a different type of subjective task (e.g., hate speech labeling) to see if the approach generalizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity coverage of LLMs compare when using different numbers of generated opinions (N) in the step-by-step recall prompting?
- Basis in paper: The paper mentions that as the number of generated opinions (N) increases, the model generates more duplicated criteria clusters, with a saturation point reached when at least 80% of the generated opinions do not reveal a new perspective
- Why unresolved: The paper does not provide a detailed analysis of the diversity coverage for different values of N, nor does it explore the optimal number of opinions to generate for maximizing diversity
- What evidence would resolve it: Conducting experiments with various values of N and analyzing the diversity coverage for each case would provide insights into the optimal number of opinions to generate for maximizing diversity

### Open Question 2
- Question: How does the diversity coverage of LLMs vary across different task types (e.g., social norms, argumentation, hate speech labeling, story continuation)?
- Basis in paper: The paper mentions that the average number of unique criteria clusters generated by LLMs varies across different task types, with argumentation tasks yielding more diverse opinions compared to hate speech labeling tasks
- Why unresolved: The paper does not provide a comprehensive analysis of the diversity coverage across all task types, nor does it explore the reasons behind the variations in diversity coverage
- What evidence would resolve it: Conducting experiments on a wider range of task types and analyzing the diversity coverage for each case would provide insights into the factors influencing diversity coverage across different task types

### Open Question 3
- Question: How does the diversity coverage of LLMs compare to human-generated opinions?
- Basis in paper: The paper mentions that humans tend to provide fewer opinions on stances they disagree with, while GPT-4 can generate more opinions compared to humans
- Why unresolved: The paper does not provide a direct comparison of the diversity coverage between LLM-generated opinions and human-generated opinions
- What evidence would resolve it: Conducting experiments where both LLMs and humans generate opinions on the same set of statements and comparing the diversity coverage of the generated opinions would provide insights into the relative performance of LLMs and humans in generating diverse opinions

## Limitations
- The study assumes criteria words adequately capture human value systems but lacks validation that these criteria are truly representative of human perspectives
- Results are based on four specific subjective tasks in English and may not generalize to other cultural contexts or more complex subjective domains
- Human evaluation reliability is limited by rater bias and the study doesn't fully account for how individual differences affect perspective diversity assessment

## Confidence

**High Confidence**: The core finding that semantic diversity does not correlate with perspective diversity is well-supported by experimental results across multiple datasets.

**Medium Confidence**: The claim that LLMs can generate diverse perspectives is supported but has limitations in practical significance and generalizability.

**Low Confidence**: The specific saturation point of 11 opinions for argumentation tasks is based on limited sampling and may not be robust across different prompting strategies.

## Next Checks

1. **Cross-Cultural Validation**: Test the criteria-based prompting approach on subjective topics from different cultural contexts to verify if diversity patterns hold across different value systems.

2. **Downstream Task Impact**: Evaluate whether generated diverse perspectives actually improve downstream tasks like decision-making or creative writing, rather than just measuring diversity metrics in isolation.

3. **Alternative Diversity Metrics**: Implement additional diversity measurement methods, such as human judgment of perspective uniqueness or topic modeling approaches, to validate the criteria clustering method's effectiveness.