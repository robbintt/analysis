---
ver: rpa2
title: Instruction Tuning with Human Curriculum
arxiv_id: '2310.09518'
source_url: https://arxiv.org/abs/2310.09518
tags:
- learning
- education
- course
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Curriculum Instruction Tuning (Corgi), a novel
  approach to fine-tuning large language models (LLMs) that mimics the structured
  and progressive nature of human education. The key idea is to create a synthetically
  generated instruction dataset with rich metadata, including educational stage and
  cognitive rigor level, aligned with Bloom's taxonomy.
---

# Instruction Tuning with Human Curriculum

## Quick Facts
- arXiv ID: 2310.09518
- Source URL: https://arxiv.org/abs/2310.09518
- Authors: 
- Reference count: 40
- Key outcome: Corgi achieves significant performance gains (+4.76 on TruthfulQA, +2.98 on MMLU, +2.8 on OpenbookQA, +1.28 on ARC-hard) compared to random shuffling on LLaMA 2 13B

## Executive Summary
This paper introduces Curriculum Instruction Tuning (Corgi), a novel approach to fine-tuning large language models that mimics the structured progression of human education. The method creates a synthetically generated instruction dataset with rich metadata including educational stage and cognitive rigor level, aligned with Bloom's taxonomy. The proposed interleaved training curriculum, which follows a global progression of cognitive difficulty while revisiting diverse subjects, consistently outperforms random shuffling and naive blocking strategies.

The approach demonstrates that curriculum-based learning can enhance LLM performance on various tasks without additional computational costs. Experiments on LLaMA 2 13B show significant improvements across nine benchmarks, validating the effectiveness of the pedagogical-inspired fine-tuning approach.

## Method Summary
The Corgi approach involves generating a synthetic instruction dataset using a teacher model (ChatGPT) to create questions from educational syllabi, then filtering low-quality examples using Contriever. The dataset is organized according to Bloom's taxonomy cognitive levels and educational stages. The model is fine-tuned using an interleaved curriculum that cycles through subjects while progressively increasing cognitive difficulty. This curriculum traverses a global progression of cognitive load while interleaving different subjects to reinforce retention and understanding.

## Key Results
- Corgi achieves +4.76 improvement on TruthfulQA compared to random shuffling
- +2.98 improvement on MMLU and +2.8 on OpenbookQA demonstrated
- +1.28 improvement on ARC-hard benchmark achieved
- Consistent performance gains across nine benchmarks on LLaMA 2 13B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving subjects while globally stepping cognitive difficulty prevents catastrophic forgetting and promotes retention.
- Mechanism: Cycling through different subjects at each cognitive level before advancing forces the model to retrieve and apply knowledge from multiple domains in each training batch, reinforcing connections and preventing over-specialization.
- Core assumption: The model benefits from revisiting subjects before fully mastering them at a given cognitive level, as this promotes interleaved practice which enhances retention and transfer.
- Break condition: If the global batch size is too small to include multiple subjects per batch, or if the number of cognitive levels is insufficient to create meaningful interleaving cycles.

### Mechanism 2
- Claim: Curriculum ordering improves sample efficiency by starting with simpler tasks that establish foundational knowledge before progressing to more complex reasoning.
- Mechanism: The model first learns to recall and recognize basic facts, providing a knowledge base that supports more complex application and analysis tasks. This staged progression mirrors human learning where simpler concepts scaffold more complex ones.
- Core assumption: The model can effectively use knowledge gained at lower cognitive levels to support performance at higher levels, and this transfer is more efficient than learning complex tasks from scratch.
- Break condition: If the curriculum is too rigid and doesn't allow revisiting earlier concepts when struggling with advanced ones, or if cognitive difficulty levels are not well-calibrated.

### Mechanism 3
- Claim: High-quality filtered data is essential for realizing the benefits of curriculum learning, especially when the gap between teacher and student model capabilities narrows.
- Mechanism: Synthetic data generated by teacher models contains noise and inconsistencies. Filtering removes low-quality examples that could mislead the student model, allowing the curriculum structure to have a clearer signal to work with.
- Core assumption: Quality of individual training examples matters more than quantity when the student model is already quite capable, and curriculum learning is more sensitive to noise than random training.
- Break condition: If filtering is too aggressive and removes too much data, reducing diversity needed for generalization, or if the teacher model is significantly more capable such that most synthetic data is already high quality.

## Foundational Learning

- Concept: Bloom's Taxonomy of Cognitive Levels
  - Why needed here: The paper explicitly structures its curriculum around Bloom's taxonomy (Remember, Understand, Apply, Analyze, Evaluate, Create), using it to organize questions by cognitive difficulty and to guide training progression.
  - Quick check question: What are the six levels of Bloom's taxonomy in order from lowest to highest cognitive demand?

- Concept: Catastrophic Forgetting in Neural Networks
  - Why needed here: The paper contrasts its interleaving approach with blocking, which can lead to catastrophic forgetting where previously learned concepts are overwritten by new ones, a phenomenon the interleaving approach is designed to mitigate.
  - Quick check question: What is catastrophic forgetting, and how does it manifest in sequential training of neural networks?

- Concept: Curriculum Learning
  - Why needed here: The entire paper is about applying curriculum learning principles to instruction tuning, arguing that presenting examples in a meaningful sequence improves learning efficiency and final performance.
  - Quick check question: What is curriculum learning, and how does it differ from standard training approaches in terms of data presentation?

## Architecture Onboarding

- Component map: Syllabus → Concept extraction → Question generation → Filtering → Curriculum ordering → Training
- Critical path: The most critical sequence is: syllabus → concept extraction → question generation → filtering → curriculum ordering → training. Any failure in earlier stages propagates to later ones.
- Design tradeoffs: The choice of interleaving vs. blocking represents a fundamental tradeoff between retention (interleaving) and depth of understanding (blocking). Filtering threshold represents a tradeoff between data quality and quantity. Number of cognitive levels affects granularity and computational efficiency.
- Failure signatures: If the model overfits to certain subjects or cognitive levels, this suggests the curriculum is too narrow. If performance plateaus early, the curriculum may be too easy. If performance degrades on certain benchmarks, this could indicate catastrophic forgetting or insufficient coverage.
- First 3 experiments:
  1. Train with random shuffling of the same dataset to establish a baseline for comparison.
  2. Train with naive blocking (all concepts of one subject at all cognitive levels before moving to the next) to test if subject continuity helps or hurts.
  3. Train with a simplified two-level curriculum (easy/hard) to test if the full six-level Bloom's taxonomy is necessary.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed interleaved curriculum training approach scale with increasing model size beyond LLaMA 2 13B?
- Basis in paper: [inferred] The paper demonstrates significant improvements using interleaved training on LLaMA 2 13B compared to random shuffling and naive blocking strategies.
- Why unresolved: The experiments were conducted only on LLaMA 2 13B. The paper does not explore the effects of curriculum training on larger or smaller model sizes.
- What evidence would resolve it: Conducting the same experiments on various model sizes (e.g., LLaMA 1 7B, LLaMA 2 70B) and comparing the performance gains of interleaved training across these models would provide insights into the scalability of the approach.

### Open Question 2
- Question: What is the impact of the quality of the teacher model (e.g., ChatGPT) on the effectiveness of the synthetic instruction dataset and subsequent curriculum training?
- Basis in paper: [explicit] The paper acknowledges that the synthetic dataset relies heavily on the teacher language model and that this dependence can result in inconsistency in question-answer pairs, potentially degrading performance.
- Why unresolved: The paper does not explore how variations in the quality of the teacher model affect the final performance of the fine-tuned model. It only mentions that filtering low-quality data can mitigate some issues.
- What evidence would resolve it: Experimenting with synthetic datasets generated by different teacher models of varying quality and comparing the performance of the fine-tuned models would elucidate the impact of teacher model quality.

### Open Question 3
- Question: How does the proposed interleaved curriculum training approach compare to other advanced training strategies, such as meta-learning or continual learning, in terms of performance and computational efficiency?
- Basis in paper: [inferred] The paper compares interleaved training to random shuffling and naive blocking strategies, showing significant improvements.
- Why unresolved: The paper does not compare the interleaved curriculum training approach to other advanced training strategies that are known to address issues like catastrophic forgetting or efficient learning.
- What evidence would resolve it: Conducting experiments comparing the interleaved curriculum training approach to meta-learning, continual learning, or other advanced training strategies on the same benchmarks and measuring both performance and computational efficiency would provide insights into the relative strengths and weaknesses of these approaches.

## Limitations

- The synthetic dataset generation process relies heavily on GPT-4 without detailed disclosure of prompt templates or quality control procedures
- Experiments focus exclusively on LLaMA 2 13B, leaving open questions about scalability to larger models or different architectures
- The filtering methodology is described at a high level but lacks specific metrics or ablation studies showing how different filtering thresholds affect final performance

## Confidence

- Curriculum ordering benefits: High confidence - systematic improvements across nine benchmarks with consistent effect sizes (+1.28 to +4.76) provide strong evidence
- Data quality importance: Medium confidence - while filtering results show clear benefits in LLaMA 1 experiments, LLaMA 2 experiments don't include comparable ablation studies
- Bloom's taxonomy alignment: Medium confidence - theoretical justification is sound but paper doesn't provide empirical evidence that taxonomy levels are correctly assigned

## Next Checks

1. **Ablation on filtering thresholds**: Run experiments with different data filtering ratios (e.g., 50%, 70%, 90% of data retained) to quantify the relationship between dataset size/quality and final model performance.

2. **Curriculum depth validation**: Test whether the full six-level Bloom's taxonomy provides benefits over a simplified three-level (basic/complex/very complex) curriculum to determine if cognitive complexity granularity is actually necessary.

3. **Cross-model generalization**: Evaluate Corgi's effectiveness on a different LLM architecture (e.g., Mistral or Gemma) and size range (7B and 33B parameters) to test whether curriculum benefits are architecture-dependent or represent more general principles.