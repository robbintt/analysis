---
ver: rpa2
title: 'C-Pack: Packed Resources For General Chinese Embeddings'
arxiv_id: '2309.07597'
source_url: https://arxiv.org/abs/2309.07597
tags:
- arxiv
- text
- datasets
- embedding
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'C-Pack advances general Chinese text embeddings with three resources:
  C-MTEB (a 35-dataset benchmark covering 6 tasks), C-MTP (100M training pairs from
  web and labeled sources), and C-TEM (BERT-based models in small/base/large sizes).
  Using contrastive learning, large-batch training, and instruction-based fine-tuning,
  C-TEM outperforms prior Chinese models by up to +10% on C-MTEB, with improvements
  especially in retrieval (+13.8), STS (+5.4), and pair classification (+5.2).'
---

# C-Pack: Packed Resources For General Chinese Embeddings

## Quick Facts
- arXiv ID: 2309.07597
- Source URL: https://arxiv.org/abs/2309.07597
- Reference count: 36
- Primary result: C-Pack advances Chinese text embeddings with C-MTEB benchmark, C-MTP training data, and C-TEM models, achieving up to +10% improvement over prior models

## Executive Summary
C-Pack introduces a comprehensive framework for Chinese text embeddings, featuring three core resources: C-MTEB (a 35-dataset benchmark covering 6 tasks), C-MTP (100M training pairs from web and labeled sources), and C-TEM (BERT-based models in small/base/large sizes). Using a three-stage training pipeline with contrastive learning and instruction-based fine-tuning, C-TEM outperforms prior Chinese models by up to +10% on C-MTEB, with particular gains in retrieval (+13.8), STS (+5.4), and pair classification (+5.2). The framework also demonstrates state-of-the-art performance on English MTEB using the same recipe, establishing C-Pack as a general-purpose embedding solution.

## Method Summary
C-Pack employs a three-stage training pipeline: MAE-style pre-training on Chinese corpora, contrastive fine-tuning on 100M unlabeled text pairs using large-batch in-batch negatives (up to 19,200), and instruction-based multi-task fine-tuning on labeled data. The approach uses BERT-like architectures in three sizes (small: 24M, base: 117M, large: 326M parameters) and evaluates performance across six task types including retrieval, STS, pair classification, re-ranking, classification, and clustering using the newly introduced C-MTEB benchmark.

## Key Results
- C-TEM models outperform prior Chinese embedding models by up to +10% on C-MTEB average score
- Scaling from small (24M) to large (326M) models consistently improves performance across all tasks
- Large-batch contrastive learning with 19,200 batch size significantly improves retrieval performance (+13.8)
- Instruction-based fine-tuning further improves performance in STS (+5.4) and pair classification (+5.2)
- English models using the same recipe achieve state-of-the-art results on MTEB

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large-scale contrastive learning with massive in-batch negatives improves embedding discriminativeness across tasks
- Mechanism: Using batch size of 19,200 with in-batch negatives exposes model to many negative samples, forcing finer-grained distinctions between similar text pairs
- Core assumption: In-batch negatives provide sufficient quality to push model toward better separability
- Evidence anchors: Authors observe consistent improvement with batch size expansion, particularly in retrieval performance
- Break condition: If in-batch negatives become too easy (too semantically dissimilar), contrastive signal weakens and gains plateau or degrade

### Mechanism 2
- Claim: Multi-stage training (pre-training → contrastive fine-tuning → instruction fine-tuning) builds general-purpose embeddings with task adaptability
- Mechanism: Pre-training prepares robust encoder, contrastive fine-tuning teaches discriminative similarity, instruction fine-tuning aligns model to specific task semantics
- Core assumption: Each stage addresses different learning need and stages are complementary
- Evidence anchors: C-MTP (unlabeled) alone shows strong performance, C-MTP (labeled) further improves multiple task types
- Break condition: If labeled data is too small or noisy, instruction fine-tuning may not generalize well

### Mechanism 3
- Claim: Scaling model size from small (24M) to large (326M) yields consistent performance gains across all tasks
- Mechanism: Larger models have higher capacity to encode nuanced semantic distinctions
- Core assumption: More parameters allow better representation of diverse semantic patterns without overfitting given sufficient training data
- Evidence anchors: Average performance improves from 58.28 to 63.96 when scaling from small to large
- Break condition: If training data is insufficient or noisy, scaling up may lead to overfitting or diminishing returns

## Foundational Learning

- Concept: Contrastive learning and negative sampling
  - Why needed here: Trains embeddings to push apart dissimilar pairs and pull together similar ones, essential for retrieval and similarity tasks
  - Quick check question: What happens to embedding space if all negative samples are too easy (very dissimilar)?

- Concept: Multi-task fine-tuning with instruction-based prompts
  - Why needed here: Allows single model to handle multiple task types by conditioning on task-specific instructions
  - Quick check question: How does instruction like "search relevant passages for the query" change model's behavior compared to no instruction?

- Concept: Pre-training objectives for masked auto-encoding (MAE-style)
  - Why needed here: Provides strong initialization by reconstructing corrupted text, making encoder robust before contrastive fine-tuning
  - Quick check question: Why might MAE-style pre-training help more than standard BERT pre-training for retrieval tasks?

## Architecture Onboarding

- Component map: Encoder backbone → Pre-training (MAE-style) → Contrastive fine-tuning (large-batch) → Instruction fine-tuning → Evaluation
- Critical path: Encoder → Pre-train → Contrastive FT → Instruct FT → Evaluation
- Design tradeoffs:
  - Larger models give better accuracy but require more memory and compute
  - Bigger batch sizes improve contrastive learning but demand advanced memory optimizations
  - Instruction-based fine-tuning adds task adaptability but may slow convergence on some tasks
- Failure signatures:
  - Poor retrieval scores → likely insufficient negative sampling or pre-training
  - Inconsistent task performance → possible interference between multi-task fine-tuning stages
  - Out-of-memory errors → batch size or model size too large for hardware
- First 3 experiments:
  1. Train small model with only contrastive fine-tuning on unlabeled pairs; evaluate retrieval only
  2. Add instruction-based fine-tuning on labeled pairs; evaluate all tasks
  3. Scale up to base model; compare performance gains across tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does C-TEM performance scale when trained on datasets larger than C-MTP (100M pairs)?
- Basis: Paper mentions C-MTP is largest dataset but doesn't explore performance beyond this scale
- Why unresolved: Paper establishes C-MTP significantly contributes to performance but doesn't investigate further scaling
- What evidence would resolve it: Training C-TEM models on datasets larger than 100M pairs and comparing C-MTEB performance

### Open Question 2
- Question: How does C-TEM compare to other models on domain-specific tasks not covered in C-MTEB?
- Basis: Paper acknowledges C-MTEB covers 6 tasks and 35 datasets but may not encompass all application scenarios
- Why unresolved: C-MTEB is comprehensive but may not include all potential domains where Chinese embeddings are applied
- What evidence would resolve it: Evaluating C-TEM against other models on additional domain-specific Chinese datasets

### Open Question 3
- Question: What is impact of different negative sampling strategies (e.g., hard negatives) on C-TEM's retrieval performance?
- Basis: Paper uses in-batch negative sampling and mentions hard negative mining could be explored
- Why unresolved: Paper achieves strong results with in-batch negatives but doesn't explore more sophisticated strategies
- What evidence would resolve it: Comparing C-TEM retrieval performance using various negative sampling strategies

### Open Question 4
- Question: How do C-TEM models perform on cross-lingual tasks involving Chinese and other languages?
- Basis: Paper focuses on Chinese embeddings and mentions English models separately but doesn't explore cross-lingual capabilities
- Why unresolved: While C-TEM is optimized for Chinese, performance on tasks requiring understanding of both Chinese and other languages remains unexplored
- What evidence would resolve it: Evaluating C-TEM on cross-lingual benchmarks (e.g., XNLI or cross-lingual retrieval tasks)

## Limitations

- Implementation details for MAE-style pre-training are not fully specified beyond referencing RetroMAE
- Specific instruction templates used for task-specific fine-tuning are not provided
- Temperature parameter τ for contrastive loss and exact threshold for filtering text pairs are not disclosed
- Corpus evidence for key mechanisms (batch size effects, scaling benefits, multi-stage pipeline) is weak

## Confidence

- High Confidence: Existence and composition of C-MTEB benchmark, overall training pipeline structure, reported performance improvements over baselines
- Medium Confidence: Effectiveness of large-batch contrastive learning and benefits of scaling model size
- Low Confidence: Specific implementation details required for exact reproduction (MAE-style pre-training specifics, instruction templates, exact hyperparameters)

## Next Checks

1. Test impact of varying negative sample quality by creating easy vs. hard negative sets and measuring embedding space discriminativeness to validate in-batch negatives provide sufficient contrastive signal

2. Train models skipping individual stages (pre-training, contrastive fine-tuning, or instruction fine-tuning) to isolate each stage's contribution and verify claimed complementarity

3. Apply English version of training recipe to different language with similar resources to C-MTEB/C-MTP to validate whether improvements are architecture-specific or genuinely task-agnostic