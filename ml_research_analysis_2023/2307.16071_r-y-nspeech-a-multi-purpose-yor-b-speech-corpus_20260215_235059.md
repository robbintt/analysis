---
ver: rpa2
title: "\xCCr\xF2y\xECnSpeech: A multi-purpose Yor\xF9b\xE1 Speech Corpus"
arxiv_id: '2307.16071'
source_url: https://arxiv.org/abs/2307.16071
tags:
- sentences
- speech
- hours
- were
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces \xCCr\xF2y\xECnSpeech, a high-quality, multi-purpose\
  \ Yor\xF9b\xE1 speech corpus designed for both Text-to-Speech (TTS) and Automatic\
  \ Speech Recognition (ASR) tasks. The dataset comprises 38.5 hours of speech data\
  \ recorded by 80 volunteers, with 20,000 text sentences curated from news and creative\
  \ writing domains under a CC-BY-4.0 license."
---

# ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus

## Quick Facts
- arXiv ID: 2307.16071
- Source URL: https://arxiv.org/abs/2307.16071
- Reference count: 0
- Primary result: High-quality Yorùbá speech corpus with 38.5 hours of data for TTS and ASR tasks.

## Executive Summary
This paper introduces ÌròyìnSpeech, a high-quality, multi-purpose Yorùbá speech corpus designed for both Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) tasks. The dataset comprises 38.5 hours of speech data recorded by 80 volunteers, with 20,000 text sentences curated from news and creative writing domains under a CC-BY-4.0 license. To encourage participatory data creation, 5,000 sentences were shared on the Mozilla Common Voice platform, yielding 6 hours of validated recordings. The corpus includes 6 hours of single-speaker recordings for TTS and 25.5 hours for ASR. Evaluation results show that a high-fidelity, general-domain, single-speaker Yorùbá TTS voice is achievable with as little as 5 hours of speech, while the ASR baseline achieves a word error rate (WER) of 23.8. The dataset addresses the need for contemporary, high-quality Yorùbá speech data to support NLP research and applications.

## Method Summary
The corpus was created through a multi-stage process: curating 20,000 text sentences from news and creative writing domains under CC-BY-4.0 license, recruiting 80 volunteers screened for standard North West Yorùbá dialect, recording speech using a custom Yorùbá Voice SpeechRecorder application in a controlled environment with portable vocal booths and AT 2020+ USB microphones, and performing post-production fixes for disfluencies, audio fidelity issues, and script corrections. The dataset was partitioned into subsets for TTS (6 hours, single-speaker), ASR (25.5 hours, multi-speaker), and ML-SUPERB tasks (1h 30m).

## Key Results
- High-fidelity, general-domain, single-speaker Yorùbá TTS achievable with 5 hours of speech data
- ASR baseline achieves 23.8 WER on 25.5 hours of multi-speaker data
- Dataset includes 38.5 total hours from 80 volunteers across multiple partitions
- First multi-purpose Yorùbá corpus combining news and creative writing domains under open license

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curating text from multiple modern domains improves model generalization across different use cases.
- Mechanism: By combining news articles and creative writing texts under an open license, the corpus exposes models to varied vocabulary and syntactic structures.
- Core assumption: News and creative writing domains provide sufficiently diverse and representative language use for Yorùbá.
- Evidence anchors:
  - [abstract] "We curated about 23000 text sentences from news and creative writing domains with the open license CC-BY -4.0."
  - [section] "Our goal was to combine news data and fictional texts to create a multi-purpose modern speech dataset, as other Yoruba datasets used utterances from religious texts or biblical data [3, 4]."
- Break condition: If the curated domains lack coverage of everyday conversational language or underrepresented dialects, model performance may degrade in real-world scenarios.

### Mechanism 2
- Claim: Recording by multiple volunteers with dialect screening improves model robustness and speaker variability.
- Mechanism: Using 80 volunteers who are speakers of standard North West Yorùbá, with dialect uniformity screening, provides a balanced multi-speaker dataset for ASR while ensuring consistency.
- Core assumption: Standard North West Yorùbá is sufficiently representative for general ASR use, and dialect screening prevents model bias toward non-standard forms.
- Evidence anchors:
  - [section] "All volunteers were speakers of standard North West Yoruba5, were screened for dialect uniformity, and ranged in age from 18 to 69 years."
  - [section] "The 20 000 lines multi-speaker (ASR) partition had 80 volunteers, 37 male and 43 female."
- Break condition: If the screening excludes important dialects or age groups, ASR performance may suffer on underrepresented populations.

### Mechanism 3
- Claim: Low-resource TTS is feasible with as little as 5 hours of single-speaker speech.
- Mechanism: The evaluation demonstrates that high-fidelity, general-domain, single-speaker Yorùbá TTS can be achieved with minimal data by leveraging quality recordings and appropriate preprocessing.
- Core assumption: The 5-hour single-speaker subset is representative and of sufficient quality to train a robust TTS model.
- Evidence anchors:
  - [abstract] "Our TTS evaluation suggests that a high-fidelity, general domain, single-speaker Yorùbá voice is possible with as little as 5 hours of speech."
  - [section] "6 000 lines were recorded by two single speakers... yielding 5 hours of speech which we envision for TTS tasks."
- Break condition: If the single-speaker data lacks phonetic diversity or contains recording artifacts, TTS quality will degrade despite the short duration.

## Foundational Learning

- Concept: Diacritic handling in tonal languages
  - Why needed here: Yorùbá uses diacritics to mark tones, which are critical for meaning; incorrect handling leads to mispronunciation and ambiguity.
  - Quick check question: Can you explain why "ilé" (house) and "ilẹ" (land) sound the same without diacritics but have different meanings?

- Concept: Data partitioning for multi-task learning
  - Why needed here: The corpus is split into subsets for TTS, ASR, and ML-SUPERB tasks; understanding how to allocate and preprocess data for each task is essential for effective model training.
  - Quick check question: What is the total duration and speaker count for the ASR partition, and how does it differ from the TTS partition?

- Concept: Speech data post-production and quality control
  - Why needed here: Manual fixes for clicks, disfluencies, and tone/spelling mismatches are required to ensure usable training data; knowing when to re-record vs. fix is crucial.
  - Quick check question: What are two examples of issues that would trigger re-recording rather than script editing?

## Architecture Onboarding

- Component map:
  - Text curation (news + creative writing) -> Volunteer recruitment (80 speakers, dialect screening) -> Recording (custom app, controlled environment) -> Post-production (fixes, re-recording) -> Dataset partitioning (TTS, ASR, ML-SUPERB) -> Model training and evaluation

- Critical path:
  1. Curate high-quality, modern Yorùbá text under open license.
  2. Recruit and screen volunteers for dialect and speaker diversity.
  3. Record speech in controlled environment with custom app.
  4. Post-process audio (remove artifacts, fix script mismatches).
  5. Partition data for TTS, ASR, and ML-SUPERB tasks.
  6. Train baseline models and evaluate.

- Design tradeoffs:
  - Single-speaker vs. multi-speaker: 5 hours for TTS vs. 25.5 hours for ASR balances quality and diversity.
  - In-house vs. crowdsourced: 38 hours in-house ensures quality; 6 hours from Common Voice increases scale.
  - Post-editing vs. re-recording: Fixes are faster but may introduce script inconsistencies; re-recording is accurate but time-consuming.

- Failure signatures:
  - High WER (>23.8) indicates poor ASR performance, possibly due to dialect mismatches or noisy recordings.
  - TTS artifacts (robotic voice, mispronunciations) suggest insufficient phonetic coverage or poor post-production.
  - Imbalanced speaker demographics (age, gender) may lead to biased ASR outputs.

- First 3 experiments:
  1. Train a baseline ASR model on the 25.5-hour multi-speaker partition; measure WER and analyze error types.
  2. Train a single-speaker TTS model on the 5-hour subset; evaluate naturalness and intelligibility with native speakers.
  3. Test the ML-SUPERB subset (1h 30m) on a self-supervised learning model; benchmark against other low-resource languages.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much speech data is required to achieve high-fidelity, general-domain, single-speaker Yorùbá TTS with acceptable performance?
- Basis in paper: [explicit] The paper states that a high-fidelity, general-domain, single-speaker Yorùbá voice is possible with as little as 5 hours of speech, but does not provide detailed analysis on the relationship between data quantity and performance.
- Why unresolved: The paper only mentions the minimum threshold of 5 hours without exploring the optimal amount or the impact of varying data sizes on TTS quality.
- What evidence would resolve it: A detailed study comparing TTS performance across different data sizes (e.g., 1h, 3h, 5h, 10h, 20h) using standardized metrics like MOS (Mean Opinion Score) and intelligibility scores.

### Open Question 2
- Question: What is the optimal recording setup for capturing high-quality Yorùbá speech data, considering acoustic environment and equipment?
- Basis in paper: [inferred] The paper describes using a portable vocal booth, AT 2020+ USB microphone, and a custom application for recording, but does not evaluate the impact of different setups on data quality.
- Why unresolved: The paper provides details on the recording setup used but does not compare it with alternative setups or analyze how different environments affect speech quality.
- What evidence would resolve it: Comparative studies evaluating speech data quality using different microphones, acoustic treatments, and recording environments, with objective metrics like SNR (Signal-to-Noise Ratio) and subjective listener evaluations.

### Open Question 3
- Question: How does the inclusion of creative writing domain data affect the performance of ASR and TTS models compared to using only news domain data?
- Basis in paper: [explicit] The paper mentions combining news and creative writing domains to create a multi-purpose dataset but does not analyze the impact of domain diversity on model performance.
- Why unresolved: The paper does not provide a comparative analysis of model performance when trained on single-domain versus multi-domain data.
- What evidence would resolve it: Experiments training ASR and TTS models on news-only data versus combined news and creative writing data, measuring performance differences in terms of WER for ASR and intelligibility for TTS.

## Limitations

- The corpus focuses exclusively on North West Yorùbá dialect, potentially limiting performance on other regional variants.
- The 23.8 WER baseline, while establishing a reference point, indicates substantial room for improvement in ASR quality.
- The evaluation only demonstrates TTS feasibility with 5 hours of single-speaker data without comparing to larger datasets or alternative architectures.
- The post-production process, while described, lacks detailed metrics on the proportion of recordings requiring fixes versus re-recording.

## Confidence

- **High Confidence**: The dataset construction methodology (text curation from modern domains, volunteer recording with dialect screening, post-production quality control) is clearly described and reproducible. The basic statistics (38.5 total hours, 80 speakers, 20,000 sentences) are verifiable.
- **Medium Confidence**: The evaluation results for ASR (23.8 WER) and TTS (5-hour feasibility claim) are presented but lack detailed experimental protocols, hyperparameter settings, or comparison with baseline models, making it difficult to assess whether these represent state-of-the-art or minimal viable results.
- **Low Confidence**: The claim about achieving "high-fidelity" TTS with 5 hours is supported only by qualitative assertion rather than systematic listener studies or objective quality metrics beyond feasibility.

## Next Checks

1. **ASR Benchmark Validation**: Replicate the 23.8 WER baseline using the provided corpus partitions, documenting all training hyperparameters and comparing against established low-resource ASR benchmarks to contextualize performance.

2. **TTS Quality Assessment**: Conduct controlled listening tests with native Yorùbá speakers to evaluate the "high-fidelity" claim for the 5-hour single-speaker model, measuring intelligibility, naturalness, and tone accuracy.

3. **Dialect Coverage Analysis**: Test ASR and TTS models on held-out recordings from speakers with different Yorùbá dialects (not just North West) to quantify performance degradation and identify corpus limitations for broader deployment.