---
ver: rpa2
title: Continual Learning Under Language Shift
arxiv_id: '2311.01200'
source_url: https://arxiv.org/abs/2311.01200
tags:
- language
- learning
- languages
- continual
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies continual pre-training of language models when
  new data comes from new languages. Starting from a monolingual English model, it
  incrementally adds data from Danish, Icelandic, and Norwegian to investigate forward
  and backward transfer effects.
---

# Continual Learning Under Language Shift

## Quick Facts
- arXiv ID: 2311.01200
- Source URL: https://arxiv.org/abs/2311.01200
- Reference count: 19
- This paper studies continual pre-training of language models when new data comes from new languages, showing that forward transfer is largely positive and independent of language order, while backward transfer varies with language characteristics.

## Executive Summary
This paper investigates continual pre-training of language models under language shift, where new data comes from previously unseen languages. Starting with a monolingual English model, the authors incrementally add data from Danish, Icelandic, and Norwegian to study forward and backward transfer effects. The experiments reveal that forward transfer is largely positive and independent of language order, while backward transfer can be either positive or negative depending on the characteristics and order of new languages. The study identifies syntactic similarity and language contamination as key factors influencing these transfer patterns.

## Method Summary
The study uses a GPT architecture with 126M and 356M parameters, trained on sequences of languages using the Nordic Pile corpus. The models are initially trained on English, then sequentially on Icelandic and Norwegian, with 35,000 steps per language stage and a global batch size of 1024. Language similarity is evaluated using token distribution similarity (TDS) and URIEL feature vectors. Forward and backward transfer effects are measured by comparing cross-entropy loss on test sets after each training stage against monolingual baselines and a jointly pre-trained multilingual model.

## Key Results
- Forward transfer to new languages is consistently positive and largely independent of language order
- Backward transfer varies significantly: positive when learning Norwegian after Icelandic, negative when learning Icelandic after English
- Syntactic similarity between languages best correlates with observed transfer patterns
- Model size increases (126M to 356M) improve final performance but don't significantly affect backward transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Positive forward transfer is largely independent of language order.
- Mechanism: Sequential pre-training on diverse languages exposes the model to a broader token distribution, enhancing its ability to generalize to new languages.
- Core assumption: Language diversity in pre-training data improves model generalization.
- Evidence anchors:
  - [abstract] "Results show that forward transfer is largely positive and independent of language order."
  - [section] "There is a positive forward transfer effect to both Icelandic and Norwegian, while the degree of catastrophic forgetting (negative backward transfer) varies with the language and its position in the sequence."
  - [corpus] Weak - no direct evidence from corpus.
- Break condition: If languages are too dissimilar, the model might not generalize effectively.

### Mechanism 2
- Claim: Backward transfer can be positive or negative depending on language order and characteristics.
- Mechanism: The syntactic similarity between languages influences how well the model retains knowledge of previously learned languages.
- Core assumption: Syntactic similarity between languages affects knowledge retention.
- Evidence anchors:
  - [abstract] "Backward transfer can be positive or negative depending on the order and characteristics of new languages."
  - [section] "Syntactic similarity appears to have the best correlation with our results."
  - [corpus] Weak - no direct evidence from corpus.
- Break condition: If syntactic similarity is low, backward transfer is likely to be negative.

### Mechanism 3
- Claim: Increasing model size improves final performance but does not significantly impact backward transfer.
- Mechanism: Larger models have more parameters to store language-specific knowledge, but the core mechanisms of forgetting remain unchanged.
- Core assumption: Model capacity affects performance but not the fundamental dynamics of forgetting.
- Evidence anchors:
  - [abstract] "Increasing model size improves the final performance on all languages, but does not significantly impact backward transfer."
  - [section] "We find that increasing model size improves the final performance on all languages, but does not significantly impact backward transfer."
  - [corpus] Weak - no direct evidence from corpus.
- Break condition: If the model becomes too large, computational costs may outweigh benefits.

## Foundational Learning

- Concept: Continual Learning
  - Why needed here: To understand how models can learn new languages without forgetting previous ones.
  - Quick check question: What is the main challenge in continual learning?

- Concept: Transfer Learning
  - Why needed here: To comprehend how knowledge from one language can be applied to another.
  - Quick check question: How does transfer learning differ from traditional learning?

- Concept: Language Similarity Metrics
  - Why needed here: To evaluate how similar different languages are, which affects transfer.
  - Quick check question: What are the different ways to measure language similarity?

## Architecture Onboarding

- Component map: Data preprocessing -> GPT model training -> Evaluation of forward and backward transfer
- Critical path: Data preprocessing → Model training → Evaluation of forward and backward transfer
- Design tradeoffs: Larger models offer better performance but at higher computational costs
- Failure signatures: Catastrophic forgetting, poor forward transfer, unexpected backward transfer
- First 3 experiments:
  1. Train a 126M model on English, then Icelandic, then Norwegian
  2. Train a 356M model on the same sequence
  3. Compare the performance and transfer effects of the two models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does language contamination from Norwegian into English affect the observed positive backward transfer when Norwegian is learned?
- Basis in paper: [explicit] The paper notes that the Norwegian monolingual model has surprisingly low test loss on English, suggesting possible English text contamination in the Norwegian corpus.
- Why unresolved: The paper cannot definitively determine whether language contamination is responsible for the positive backward transfer from Norwegian to English, or if it's due to syntactic similarity and other linguistic factors.
- What evidence would resolve it: A controlled experiment using a Norwegian corpus verified to be free of English contamination, compared to the original corpus, would show if the positive backward transfer effect persists.

### Open Question 2
- Question: Does increasing model size beyond 356M parameters significantly reduce catastrophic forgetting in continual learning under language shift?
- Basis in paper: [inferred] The paper found that increasing model size from 126M to 356M improved performance on all languages but did not significantly impact backward transfer or forgetting.
- Why unresolved: The paper only tested two model sizes (126M and 356M) and found no significant impact on forgetting with increased size.
- What evidence would resolve it: Training and evaluating models with much larger sizes (e.g., 1B+ parameters) in the same continual learning setup would determine if there's a threshold where model size begins to significantly reduce forgetting.

### Open Question 3
- Question: Is the token distribution similarity (TDS) metric predictive of forward transfer effects in continual learning under language shift?
- Basis in paper: [inferred] The paper found that Norwegian and Icelandic have the highest TDS similarity, but training on Icelandic caused significant forgetting while training on Norwegian caused positive backward transfer to English.
- Why unresolved: The paper found that TDS similarity did not correlate with the observed backward transfer patterns, but did not specifically test if it predicts forward transfer.
- What evidence would resolve it: A study correlating TDS similarity values between language pairs with the magnitude of forward transfer effects across multiple language combinations would establish if TDS is a reliable predictor of forward transfer.

## Limitations

- Findings are limited to Nordic languages and may not generalize to more distant language families
- Only tested two model sizes (126M and 356M), leaving questions about larger models unanswered
- Analysis relies on proxy measures rather than direct investigation of parameter space dynamics

## Confidence

- Forward transfer independence from language order: Medium confidence
- Syntactic similarity as primary predictor of backward transfer: Medium confidence
- Model size not affecting backward transfer: Low confidence

## Next Checks

1. **Cross-family validation**: Test the same experimental protocol with languages from different families (e.g., Romance + Slavic + Uralic) to verify whether forward transfer remains order-independent across typologically diverse languages.

2. **Parameter dynamics analysis**: Conduct singular value decomposition of model parameters before and after each training stage to directly measure parameter displacement and correlate with observed forward/backward transfer patterns.

3. **Vocabulary overlap control**: Design an experiment with synthetic languages that vary in syntactic similarity but have controlled vocabulary overlap to isolate the effect of syntax versus lexical sharing on backward transfer.