---
ver: rpa2
title: Empowering recommender systems using automatically generated Knowledge Graphs
  and Reinforcement Learning
arxiv_id: '2307.04996'
source_url: https://arxiv.org/abs/2307.04996
tags:
- knowledge
- recommender
- systems
- recommendation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses personalized article recommendation in financial
  services using knowledge graphs (KGs) and machine learning. It proposes two approaches:
  (1) a KG-driven reinforcement learning (RL) system that uses path traversal for
  interpretability, and (2) a KG-driven XGBoost system with post-hoc explainability
  via SHAP and ELI5.'
---

# Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.04996
- Source URL: https://arxiv.org/abs/2307.04996
- Reference count: 40
- Key outcome: RL-based KG recommender achieved 43.76% MAP@K=10, outperforming BPR baseline by 32.55 percentage points

## Executive Summary
This paper addresses personalized article recommendation in financial services using knowledge graphs (KGs) and machine learning. It proposes two approaches: (1) a KG-driven reinforcement learning (RL) system that uses path traversal for interpretability, and (2) a KG-driven XGBoost system with post-hoc explainability via SHAP and ELI5. Both methods leverage automatically generated KGs from structured and unstructured data. The RL-based approach achieved 43.76% MAP@K=10, outperforming the XGBoost-based method and a baseline BPR model by 13.38 and 32.55 percentage points respectively. The study highlights the benefits of combining advanced ML with KG-driven insights for accurate and explainable recommendations in customer relationship management.

## Method Summary
The paper presents two KG-driven recommendation approaches for financial article recommendations. First, an RL-based system uses path-directed reasoning (PDR) where an agent traverses a KG starting from user nodes to find relevant items, with the traversal path serving as an interpretable explanation. Second, an XGBoost-based approach uses KG embeddings as features and applies post-hoc explainability through SHAP and ELI5. Both methods utilize automatically generated KGs from structured user/product data and unstructured article content. KG embeddings are generated using TransE and TuckER methods. The system was evaluated on a dataset of 463 customers and 71 articles, targeting MAP@K=10 as the primary metric.

## Key Results
- RL-based KG approach achieved 43.76% MAP@K=10
- Outperformed XGBoost-based KG approach by 13.38 percentage points
- Outperformed BPR baseline by 32.55 percentage points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graph-driven RL improves recommendation accuracy by enabling interpretable path-based reasoning over heterogeneous data
- Mechanism: RL agent traverses a KG starting from a user node, following edges to item nodes, with the traversal path itself serving as an interpretable explanation (Path Directed Reasoning)
- Core assumption: The reasoning paths in the KG capture meaningful relationships between users and items that reflect user preferences
- Evidence anchors:
  - [abstract] "By integrating machine learning with automatically generated KGs, our methods not only improve recommendation accuracy but also provide interpretable insights"
  - [section 3] "The goal is to find a recommendation set of ùëÅ items for a given user ùë¢ from a subset of Item entities I connected to User entities U through relations"
  - [corpus] Weak - corpus lacks direct evidence for RL-based KG traversal mechanisms
- Break condition: If KG relationships do not meaningfully connect users to relevant items, path-based reasoning becomes arbitrary

### Mechanism 2
- Claim: Combining structured and unstructured data in KG creation provides richer features for recommendation than using either source alone
- Mechanism: The combined KG (cKG) integrates user demographics, product information, and article content terms, allowing the model to learn relationships across multiple data types
- Core assumption: Both structured attributes and unstructured content contain complementary information about user preferences
- Evidence anchors:
  - [section 4.1.3] "Therefore, this KG leverages both structured and unstructured data for its creation"
  - [section 5] "The KG-driven XGBoost approach (cKG) and KG-driven RL-based approach using the same cKG achieved 34.47% and 43.76% MAP scores, respectively"
  - [corpus] Weak - corpus lacks evidence comparing performance across different KG construction approaches
- Break condition: If either data source is noisy or irrelevant, combining them may introduce harmful noise

### Mechanism 3
- Claim: TransE embeddings outperform TuckER embeddings for this KG-based recommendation task
- Mechanism: TransE's translation-based approach better captures the directional relationships needed for recommendation paths compared to TuckER's tensor decomposition
- Core assumption: The KG structure in this domain consists of meaningful translational relationships between entities
- Evidence anchors:
  - [section 5] "Based on the results, it can be observed that the KG-driven RL-based approach outperformed... KG-driven XGBoost approaches when considering the MAP score. Additionally, among all the experiments conducted with KG embeddings, the KG embeddings generated from TransE have proven to capture useful information"
  - [section 4.2] "We use TuckER [2, 3] and TransE [38] methods to generate KG embeddings"
  - [corpus] Weak - corpus does not provide comparative evidence between TransE and TuckER performance
- Break condition: If the KG contains non-translational relationships (symmetric, complex patterns), TransE may fail to capture them

## Foundational Learning

- Knowledge Graph Embeddings
  - Why needed here: To represent KG entities and relationships in continuous vector space that ML models can process
  - Quick check question: What is the key difference between TransE and TuckER embedding approaches?
- Reinforcement Learning in Recommendation
  - Why needed here: To learn policies for traversing the KG from user to item nodes based on interaction rewards
  - Quick check question: How does the reward function in this RL setup differ from standard recommendation approaches?
- Explainable AI Techniques
  - Why needed here: To provide interpretable explanations for recommendations through path reasoning (RL) or feature importance (XGBoost)
  - Quick check question: What is the difference between intrinsic and post-hoc explainability in this system?

## Architecture Onboarding

- Component map: Data ingestion ‚Üí KG construction (structured + unstructured) ‚Üí KG embedding generation ‚Üí RL/XGBoost training ‚Üí Recommendation + Explanation
- Critical path:
  - For RL: User ‚Üí KG traversal ‚Üí Item selection ‚Üí Path explanation
  - For XGBoost: User features + Item features ‚Üí Model prediction ‚Üí SHAP/ELI5 explanation
- Design tradeoffs:
  - KG complexity vs. computational efficiency: larger KGs provide richer relationships but increase traversal cost
  - Embedding method choice: TransE provides simpler, potentially more interpretable embeddings vs. TuckER's richer representations
  - Path diversity vs. accuracy: enforcing diverse paths may reduce recommendation precision
- Failure signatures:
  - RL: Agent gets stuck in local optima, fails to explore diverse paths
  - XGBoost: Overfitting to training data, poor generalization to new users
  - KG construction: Missing critical relationships, incorrect entity linking
- First 3 experiments:
  1. Baseline: XGBoost with only structured features (no KG)
  2. KG-only: XGBoost with KG embeddings only, no structured features
  3. Ablation: Compare TransE vs TuckER embeddings on same KG structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the action space affect the performance of the RL-based recommender system, and what are the optimal methods for pruning this space?
- Basis in paper: [explicit] The paper mentions that some nodes in the KG can have very large out-degree, making it inefficient to maintain the large action space. The authors perform an action-pruning step based on a scoring function to address this issue.
- Why unresolved: The paper does not provide a detailed analysis of how different action space sizes impact the RL model's performance or explore alternative pruning strategies.
- What evidence would resolve it: Empirical results comparing RL performance with different action space sizes and various pruning methods, including ablation studies.

### Open Question 2
- Question: What is the impact of using different KG embedding methods (e.g., TransE vs. TuckER) on the explainability and accuracy of the recommender systems?
- Basis in paper: [explicit] The paper compares TransE and TuckER embeddings, noting that TransE embeddings outperformed TuckER in terms of MAP score. However, it does not explore the impact on explainability.
- Why unresolved: The study focuses on comparing MAP scores but does not analyze how different embeddings affect the interpretability of the recommendations.
- What evidence would resolve it: Comparative analysis of recommendation paths and explanations generated using different KG embedding methods, along with user studies on interpretability.

### Open Question 3
- Question: How does the KG-driven XGBoost approach perform on datasets with different characteristics, such as sparsity or diversity of user-item interactions?
- Basis in paper: [inferred] The paper evaluates the XGBoost approach on a specific financial dataset but does not test its generalizability to other domains or data characteristics.
- Why unresolved: The experiments are limited to a single dataset, leaving questions about the model's robustness and adaptability to different scenarios.
- What evidence would resolve it: Experiments on multiple datasets with varying levels of sparsity and interaction diversity, along with performance comparisons across domains.

## Limitations
- Small proprietary dataset (463 customers, 71 articles) limits generalizability
- Limited comparative analysis between different KG construction approaches
- Lacks detailed implementation specifics for RL-based PDR algorithm and reward function

## Confidence
- **High Confidence**: The RL approach achieving 43.76% MAP@K=10 and outperforming BPR baseline by 32.55 percentage points
- **Medium Confidence**: Claims about TransE outperforming TuckER embeddings (limited comparative evidence provided)
- **Low Confidence**: Generalization of results to larger, more diverse financial services datasets

## Next Checks
1. Implement and validate the RL path traversal mechanism with different reward function formulations to test robustness of the 43.76% MAP claim
2. Conduct ablation studies comparing performance across different KG construction approaches (uKG_CN, uKG_DP, cKG) to verify the benefits of combined KGs
3. Test the model on a larger, public financial recommendation dataset to assess scalability and generalization beyond the small proprietary dataset