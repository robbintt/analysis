---
ver: rpa2
title: Business Process Text Sketch Automation Generation Using Large Language Model
arxiv_id: '2309.01071'
source_url: https://arxiv.org/abs/2309.01071
tags:
- process
- activity
- condition
- will
- execute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a divide-and-conquer approach to automatically
  generate Business Process Text Sketches (BPTSs) from Conditional Process Trees (CPTs)
  using Large Language Models (LLMs). The method breaks down complex CPTs into simpler
  ones and solves each sub-problem in turn, transforming the inference task into a
  language rewriting task that LLMs are better at.
---

# Business Process Text Sketch Automation Generation Using Large Language Model

## Quick Facts
- arXiv ID: 2309.01071
- Source URL: https://arxiv.org/abs/2309.01071
- Reference count: 17
- Key outcome: Proposed divide-and-conquer approach achieves 93.42% correct rate on BPTS generation from CPTs, 45.17% improvement over traditional prompting

## Executive Summary
This paper addresses the challenge of automatically generating Business Process Text Sketches (BPTSs) from Conditional Process Trees (CPTs) using Large Language Models (LLMs). The proposed divide-and-conquer approach breaks down complex CPTs into simpler subproblems that LLMs can handle more effectively through language rewriting tasks. By transforming the hierarchical inference problem into a sequence of structured prompt rewrites, the method achieves significantly higher accuracy than traditional prompting approaches. The approach also enables synthetic dataset generation for the Process Model Extraction domain without requiring labeled training data.

## Method Summary
The method uses a divide-and-conquer algorithm to recursively traverse CPTs depth-first, solving subtrees before handling parent nodes. Each solved subtree produces a BPTS fragment, which is then used in operator-specific prompt templates to guide LLM rewriting into natural language. The approach handles four operator types (sequence, parallel, selection, and loop) with tailored prompt templates. By leveraging the structured nature of CPTs, the method can generate large synthetic CPT-BPTS datasets useful for downstream PME tasks.

## Key Results
- Achieves 93.42% correct rate on BPTS generation from CPTs
- 45.17% improvement over traditional prompting methods
- Successfully handles CPTs with depths up to 5 and up to 29 nodes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Divide-and-conquer decomposition reduces cognitive load on LLMs by transforming complex hierarchical inference into simpler language rewriting tasks.
- Evidence: Abstract states the approach breaks down difficult CPTs into basic ones and solves each in turn.
- Break condition: Loss of context necessary for correct operator application or failure to maintain semantic consistency during rewriting.

### Mechanism 2
- Claim: Enables LLM-based business process document generation without labeled datasets by combining rule-based CPT generation with LLM transformation.
- Evidence: Abstract mentions providing solutions for business process document generation in absence of datasets and potential for large PME datasets.
- Break condition: Rule-based CPT generation fails to capture real business process diversity or LLM-generated BPTSs contain systematic errors.

### Mechanism 3
- Claim: Operator-specific prompt templates guide LLMs to produce semantically correct BPTSs by providing structured context about hierarchical relationships.
- Evidence: Section describes using pseudocode-like input to represent operator relationships and instruct LLMs to translate into fluent plain language.
- Break condition: Prompt templates are ambiguous or incomplete, leading to misinterpretation of operator semantics.

## Foundational Learning

- Concept: CPT (Conditional Process Tree) structure and semantics
  - Why needed: Essential for implementing divide-and-conquer algorithm and constructing correct prompt templates
  - Quick check: What's the difference between sequence operator (→) and parallel operator (∧) in CPT, and how should each be represented in BPTS?

- Concept: Language rewriting task formulation
  - Why needed: Core approach relies on converting inference problem into language rewriting task that LLMs handle better
  - Quick check: How does loop operator (∝) prompt template differ from selection operator (×), and why is this distinction important?

- Concept: Divide-and-conquer algorithm design
  - Why needed: Core innovation applies divide-and-conquer to break down complex CPTs
  - Quick check: Why is depth-first traversal suitable for this approach, and what would be consequence of using breadth-first instead?

## Architecture Onboarding

- Component map: CPT Generator -> Divide-and-Conquer Engine -> Prompt Constructor -> LLM Interface -> Evaluation Module
- Critical path:
  1. Generate or input a CPT
  2. Initiate depth-first traversal of the CPT
  3. For each non-leaf node, recursively solve child subtrees
  4. Construct operator-specific prompt using child BPTS fragments
  5. Send prompt to LLM and receive rewritten BPTS
  6. Return completed BPTS to parent node
  7. Repeat until root node is processed

- Design tradeoffs:
  - Prompt complexity vs. LLM performance: More detailed prompts may improve accuracy but increase token usage and processing time
  - Rule-based CPT generation vs. real-world representativeness: Algorithmic generation is fast but may not capture all nuances of actual business processes
  - Depth of recursion vs. LLM context window: Very deep CPTs may exceed LLM context limits

- Failure signatures:
  - Inconsistent BPTS fragments when combining at higher levels
  - LLM generating output that doesn't match input CPT structure
  - Excessive token usage or timeouts with large or deeply nested CPTs

- First 3 experiments:
  1. Test divide-and-conquer algorithm on simple CPT with depth 2 and all four operator types
  2. Compare accuracy against traditional prompting on CPTs with varying depths (2-5)
  3. Generate synthetic dataset of CPT-BPTS pairs and evaluate quality for PME tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does accuracy scale with increasingly complex CPTs exceeding maximum depth and node count tested?
- Basis: Paper reports accuracy for CPTs up to depth 5 and 29 nodes but doesn't explore more complex CPTs
- Why unresolved: Experimental scope limited to specific range, no data on deeper or more intricate CPTs
- Evidence needed: Experiments with CPTs deeper than 5 and node counts exceeding 29

### Open Question 2
- Question: What potential biases are introduced by prompt template style in generating BPTSs, and how can these be mitigated?
- Basis: Paper acknowledges BPTS style depends on prompt template and may not conform to human descriptive habits
- Why unresolved: No method provided for improving prompt templates or detecting biases
- Evidence needed: Testing various prompt templates and creating framework for evaluating/mitigating biases

### Open Question 3
- Question: How can accuracy be further improved, especially in handling complex hierarchical structures and avoiding specific error types?
- Basis: Paper identifies loop condition errors, language confusion, and hierarchy parsing errors
- Why unresolved: Highlights errors but provides no solutions for reducing occurrence or improving overall accuracy
- Evidence needed: Implementing error detection/correction mechanisms and refining algorithm for complex structures

## Limitations
- Evaluation relies on human judgment for correctness scoring, introducing potential subjectivity
- Synthetic dataset generation may not fully capture real-world business process complexity
- Performance on CPTs with depth greater than 5 or extreme branching factors remains untested

## Confidence
- High confidence in the mechanism: Divide-and-conquer effectively reduces cognitive load on LLMs for structured transformation tasks
- Medium confidence in generalizability: Strong performance metrics but based on synthetic data that may not represent real-world complexity
- Medium confidence in dataset utility: Synthetic CPT-BPTS pairs show promise for PME tasks but need real-world validation

## Next Checks
1. Test the approach on real-world business process documents converted to CPTs to assess performance on authentic data
2. Conduct comprehensive failure mode analysis to identify conditions where divide-and-conquer strategy breaks down
3. Evaluate quality and utility of synthetic datasets for downstream PME tasks through actual model training and performance comparison