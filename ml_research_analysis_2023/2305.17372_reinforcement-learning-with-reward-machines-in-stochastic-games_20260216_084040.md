---
ver: rpa2
title: Reinforcement Learning With Reward Machines in Stochastic Games
arxiv_id: '2305.17372'
source_url: https://arxiv.org/abs/2305.17372
tags:
- agent
- reward
- nash
- learning
- equilibrium
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multi-agent reinforcement learning
  in stochastic games with non-Markovian reward functions, where agents aim to learn
  best-response strategies at Nash equilibrium. The authors propose QRM-SG, a Q-learning
  algorithm that incorporates reward machines to represent high-level task specifications
  and decompose complex tasks into structured subproblems.
---

# Reinforcement Learning With Reward Machines in Stochastic Games

## Quick Facts
- arXiv ID: 2305.17372
- Source URL: https://arxiv.org/abs/2305.17372
- Authors: 
- Reference count: 40
- Key outcome: QRM-SG algorithm learns best-response strategies at Nash equilibrium for non-Markovian reward functions in stochastic games, converging in 1500-7500 episodes while baseline methods fail

## Executive Summary
This paper addresses multi-agent reinforcement learning in stochastic games where agents must learn best-response strategies at Nash equilibrium under non-Markovian reward functions. The authors propose QRM-SG, which integrates reward machines into Q-learning to handle complex temporal dependencies in task specifications. The algorithm maintains Q-functions for both agents, uses Lemke-Howson to compute Nash equilibria at each step, and proves convergence when stage games have global optimum points or saddle points. Experimental results on three grid-based case studies demonstrate successful learning of equilibrium strategies while baseline methods fail to converge.

## Method Summary
QRM-SG is a Q-learning algorithm for stochastic games that incorporates reward machines to handle non-Markovian reward functions. Each agent learns Q-functions for both agents (qee, qae for ego agent and qea, qaa for adversarial agent) in an augmented state space combining the game state and reward machine states. At each time step, the algorithm formulates a stage game using current Q-function estimates, applies the Lemke-Howson method to find a Nash equilibrium, and updates Q-functions based on expected rewards under these equilibrium strategies. The method assumes that stage games during learning have global optimum points or saddle points, which ensures convergence of Q-functions to those at Nash equilibrium.

## Key Results
- QRM-SG successfully learns best-response strategies at Nash equilibrium in 1500-7500 episodes across three case studies
- Baseline methods (Nash Q-learning, MADDPG) fail to converge to Nash equilibrium strategies
- Convergence is theoretically guaranteed when stage games have global optimum points or saddle points
- Agents learn to capture opponents and achieve task completion with sparse rewards of 1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QRM-SG converges to Nash equilibrium strategies when stage games have global optimum points or saddle points
- Mechanism: The algorithm formulates a stage game at each time step using current Q-function estimates, then uses the Lemke-Howson method to derive a Nash equilibrium. Q-functions are updated based on the expected rewards when both agents follow these Nash equilibrium strategies.
- Core assumption: The stage game at each time step during learning has a global optimum point or a saddle point
- Evidence anchors:
  - [abstract] "We prove that Q-functions learned in QRM-SG converge to the Q-functions at a Nash equilibrium if the stage game at each time step during learning has a global optimum point or a saddle point"
  - [section] "Assumption 3. Every stage game (qei(s, ve, va), qai(s, ve, va)), i ∈ {e, a}, for all t, s, ve, and va, has a global optimum point or a saddle point"
- Break condition: If the stage game lacks a global optimum point or saddle point during learning, convergence is not guaranteed

### Mechanism 2
- Claim: Reward machines convert non-Markovian rewards into Markovian rewards through augmented state space
- Mechanism: By combining the game state with reward machine states, the algorithm creates an augmented state space where rewards become Markovian. The reward machine transitions track task progress, making complex temporal dependencies learnable with standard Q-learning.
- Core assumption: Reward machines correctly encode the non-Markovian reward structure
- Evidence anchors:
  - [abstract] "The augmented state space integrates the state of the stochastic game and the state of reward machines"
  - [section] "In a stochastic game with reward machines (SGRM) H = (S′, s′I, Ae, Aa, R′e, R′a, p′, γ), the rewards received by the agents are Markovian with respect to the augmented state space S′"
- Break condition: If reward machines incorrectly specify task dependencies, the augmented state space won't properly capture the reward structure

### Mechanism 3
- Claim: Each agent learning both agents' Q-functions enables effective best-response learning
- Mechanism: By maintaining Q-functions for both agents (qee, qae for ego agent and qea, qaa for adversarial agent), each agent can compute Nash equilibria based on its current understanding of both players' value functions. This dual-learning approach allows for coordinated strategy updates.
- Core assumption: Agents have accurate estimates of the other agent's Q-functions
- Evidence anchors:
  - [abstract] "Each agent learns the Q-functions of all agents in the system"
  - [section] "qij represents the Q-function for agent i and learned by agent j"
- Break condition: If Q-function estimates diverge significantly from true values, agents may learn suboptimal best-response strategies

## Foundational Learning

- Concept: Nash equilibrium in stochastic games
  - Why needed here: The paper's solution concept is Nash equilibrium, where each agent plays a best-response to the other's strategy
  - Quick check question: In a two-player game, if Player 1's strategy is a best-response to Player 2's strategy, and Player 2's strategy is a best-response to Player 1's strategy, what solution concept describes this situation?

- Concept: Non-Markovian reward functions and temporal logic
  - Why needed here: The reward functions depend on the history of events, requiring reward machines to capture temporal dependencies in task completion
  - Quick check question: Why can't standard Markov decision processes handle rewards that depend on the sequence of past events rather than just the current state?

- Concept: Lemke-Howson algorithm for finding Nash equilibria
  - Why needed here: The algorithm uses this method to compute Nash equilibria in the stage games formed at each learning step
  - Quick check question: What is the primary purpose of the Lemke-Howson algorithm in the context of finding Nash equilibria in bimatrix games?

## Architecture Onboarding

- Component map: Environment -> Labeling function -> Reward machine state tracker -> QRM-SG agent -> Lemke-Howson solver -> Action selection -> Environment interaction -> Q-function update

- Critical path: State observation → High-level event detection → Reward machine state update → Stage game formulation → Nash equilibrium computation → Action selection → Environment interaction → Q-function update

- Design tradeoffs: The paper trades computational complexity (running Lemke-Howson at each step) for the ability to handle complex non-Markovian rewards. The augmented state space increases dimensionality but enables Markovian treatment of non-Markovian rewards.

- Failure signatures: Convergence to suboptimal strategies, oscillations in Q-function values, failure to reach task completion even when individual components work correctly.

- First 3 experiments:
  1. Implement a simple two-agent grid world with non-Markovian rewards and verify reward machine transitions track task progress correctly
  2. Test the Lemke-Howson integration by verifying it computes Nash equilibria for simple stage games derived from known Q-functions
  3. Run QRM-SG on the motivational example (Figure 1) and verify that agents learn to complete the specified tasks at Nash equilibrium

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed algorithm perform in stochastic games with more than two agents?
- Basis in paper: [inferred] The paper focuses on two-agent general-sum stochastic games, but mentions that extending to games with more agents is worth further investigation.
- Why unresolved: The current algorithm is designed specifically for two-agent games, and its performance in multi-agent settings with more than two agents is not explored.
- What evidence would resolve it: Empirical results comparing the algorithm's performance in stochastic games with varying numbers of agents, demonstrating scalability and effectiveness in multi-agent scenarios.

### Open Question 2
- Question: Can reward machines be learned jointly with best-response strategies during reinforcement learning?
- Basis in paper: [explicit] The paper concludes by mentioning that one immediate extension is to jointly learn reward machines and best-response strategies during RL.
- Why unresolved: The current algorithm assumes that reward machines are given and does not address the problem of learning them from data.
- What evidence would resolve it: A modified version of the algorithm that incorporates reward machine learning, with experimental results showing improved performance in tasks where reward machines are not known a priori.

### Open Question 3
- Question: How does the algorithm handle non-deterministic reward machines or reward machines with cycles?
- Basis in paper: [inferred] The paper focuses on deterministic reward machines and does not address the challenges posed by non-deterministic reward machines or those with cycles.
- Why unresolved: The current algorithm assumes that reward machines are deterministic, and its behavior in the presence of non-determinism or cycles is not explored.
- What evidence would resolve it: Analysis of the algorithm's performance in tasks with non-deterministic reward machines or those with cycles, along with modifications to handle such cases if necessary.

### Open Question 4
- Question: How sensitive is the algorithm to the choice of hyperparameters, such as the learning rate and exploration rate?
- Basis in paper: [explicit] The paper mentions that the algorithm uses hyperparameters like the learning rate and exploration rate, but does not provide a detailed analysis of their impact on performance.
- Why unresolved: The paper does not explore the sensitivity of the algorithm to different hyperparameter settings, which could affect its convergence and effectiveness.
- What evidence would resolve it: A sensitivity analysis showing the impact of different hyperparameter values on the algorithm's performance, along with guidelines for selecting appropriate hyperparameters for different tasks.

## Limitations
- Convergence guarantees only apply when stage games have global optimum points or saddle points, which may not hold in all stochastic game settings
- Computational cost of running Lemke-Howson at each step could be prohibitive for larger state spaces
- Assumes agents can accurately estimate each other's Q-functions, which may not hold in partially observable environments

## Confidence
- Theoretical convergence claims: High
- Reward machine mechanism for non-Markovian rewards: High
- Experimental results with limited evaluation environments: Medium

## Next Checks
1. Test QRM-SG on stage games without global optimum points or saddle points to evaluate robustness when Assumption 3 is violated
2. Implement QRM-SG in larger grid worlds (e.g., 10x10 or 20x20) to assess scalability and computational feasibility
3. Evaluate performance under partial observability where agents cannot directly observe the other agent's position, testing the assumption about accurate Q-function estimation