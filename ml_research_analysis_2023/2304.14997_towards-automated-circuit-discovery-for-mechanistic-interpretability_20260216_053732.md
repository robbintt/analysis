---
ver: rpa2
title: Towards Automated Circuit Discovery for Mechanistic Interpretability
arxiv_id: '2304.14997'
source_url: https://arxiv.org/abs/2304.14997
tags:
- acdc
- circuit
- section
- figure
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Automatic Circuit DisCovery (ACDC), an algorithm
  designed to automate the identification of computational subgraphs in neural networks
  that are responsible for specific behaviors. ACDC operates by iteratively pruning
  edges in a model's computational graph based on their effect on a task-specific
  metric, using a threshold to determine which connections are important.
---

# Towards Automated Circuit Discovery for Mechanistic Interpretability

## Quick Facts
- arXiv ID: 2304.14997
- Source URL: https://arxiv.org/abs/2304.14997
- Authors: 
- Reference count: 40
- Key outcome: ACDC successfully recovered 5 out of 5 manually identified component types in docstring completion, selecting only 68 out of 32,000 edges, and identified all 5 key attention heads from IOI circuit in GPT-2 Small while reducing connections by 91%.

## Executive Summary
This paper introduces Automatic Circuit DisCovery (ACDC), an algorithm designed to automate the identification of computational subgraphs in neural networks responsible for specific behaviors. ACDC operates by iteratively pruning edges in a model's computational graph based on their effect on a task-specific metric, using a threshold to determine which connections are important. The method was validated on several tasks, including reverse-engineering a circuit for Python docstring completion in a small transformer, where ACDC successfully recovered 5 out of 5 manually identified component types, selecting only 68 out of 32,000 edges. It was also applied to the IOI circuit in GPT-2 Small, identifying all 5 key attention heads from prior work while reducing the number of connections by 91%. ACDC demonstrates strong performance in recovering circuits across different tasks, offering a scalable approach to mechanistic interpretability.

## Method Summary
ACDC is an algorithm that identifies computational subgraphs (circuits) in neural networks by iteratively pruning edges based on their effect on task-specific KL divergence. The method processes the computational graph in reverse topological order, evaluating each edge by measuring the KL divergence increase when that edge is replaced with its activation on a corrupted input. Edges below a threshold τ are removed, and the process recurses on the remaining important nodes. The algorithm operates on computational graph representations of transformers, considering the full hypothesis space of possible subgraphs rather than just component-level pruning.

## Key Results
- Successfully recovered 5 out of 5 manually identified component types in docstring completion task, selecting only 68 edges out of 32,000
- Identified all 5 key attention heads from the IOI circuit in GPT-2 Small while reducing connections by 91%
- Demonstrated strong performance across multiple tasks with a single hyperparameter (threshold τ)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ACDC identifies important circuits by iteratively pruning edges whose removal has minimal effect on task-specific KL divergence.
- Mechanism: The algorithm processes the computational graph in reverse topological order, evaluating each edge by measuring the KL divergence increase when that edge is replaced with its activation on a corrupted input. Edges below a threshold τ are removed, and the process recurses on the remaining important nodes.
- Core assumption: KL divergence is a reliable proxy for how much an edge contributes to the model's behavior on the target task.
- Evidence anchors:
  - [abstract] "ACDC operates by iteratively pruning edges in a model's computational graph based on their effect on a task-specific metric, using a threshold to determine which connections are important."
  - [section] "ACDC iterates over nodes in the computational graph, replacing activations of connections between a node and its children, and measuring the effect on the output metric."
  - [corpus] Found 25 related papers; average neighbor FMR=0.507 indicates moderate relatedness to circuit discovery methods.
- Break condition: If KL divergence is not a good proxy for task performance, or if the corrupted inputs do not adequately represent "noisy" behavior.

### Mechanism 2
- Claim: ACDC's search space is more expressive than component-level pruning because it considers mediating effects between components.
- Mechanism: Unlike methods that only prune entire components (like attention heads), ACDC considers all possible subgraphs, including cases where an edge's importance is determined by its role in mediating effects between upstream and downstream components.
- Core assumption: The task-relevant computation flows through specific paths in the computational graph, and these paths can be identified by measuring edge importance.
- Evidence anchors:
  - [section] "Our work considers a hypothesis space that is considerably larger, since it considers the mediating effect components have on later components."
  - [section] "In this case, the component C has no direct path on the output, but it does have an effect through A on the output."
  - [corpus] Several related works focus on automated circuit discovery, suggesting this is a recognized challenge.
- Break condition: If the computational graph representation doesn't capture the actual information flow, or if important computation occurs through non-graph pathways.

### Mechanism 3
- Claim: ACDC can recover circuits found by manual interpretation while being more specific (fewer edges).
- Mechanism: By using KL divergence as the optimization objective, ACDC finds minimal subgraphs that maintain task performance, which often results in more precise circuit identification than manual methods.
- Core assumption: Manual circuit identification tends to include some unnecessary edges, and automated methods can find more minimal representations.
- Evidence anchors:
  - [abstract] "ACDC successfully recovered 5 out of 5 manually identified component types, selecting only 68 out of 32,000 edges."
  - [section] "The ACDC algorithm has one hyperparameter τ, the threshold. We describe the algorithm in the notation of Section 3.2 in the pseudocode in Algorithm 1."
  - [corpus] Multiple papers cite or build upon ACDC, indicating it's a promising approach for circuit discovery.
- Break condition: If the threshold τ is poorly chosen, leading to either over-pruning (missing important edges) or under-pruning (including irrelevant edges).

## Foundational Learning

- Concept: Computational graphs and residual connections in transformers
  - Why needed here: ACDC operates directly on the computational graph representation of transformers, so understanding how residual connections create multiple paths is crucial for interpreting results.
  - Quick check question: In a residual transformer, if you have two attention heads in the same layer, do their outputs combine additively or multiplicatively?

- Concept: KL divergence and its properties
  - Why needed here: ACDC uses KL divergence as its primary metric for evaluating circuit importance, so understanding its behavior (always positive, asymmetric) is essential.
  - Quick check question: If a subgraph perfectly reproduces the model's output distribution, what is the KL divergence between them?

- Concept: Activation patching and causal intervention
  - Why needed here: ACDC's edge evaluation is based on activation patching principles - replacing activations to measure causal effects.
  - Quick check question: In activation patching, what's the difference between replacing with zero activations versus corrupted activations?

## Architecture Onboarding

- Component map: Input (computational graph, datasets) -> Core (reverse topological sort + iterative edge pruning) -> Output (sparse subgraph)
- Critical path:
  1. Parse computational graph representation
  2. Prepare clean and corrupted datasets
  3. Initialize H as full graph
  4. Reverse topological sort
  5. Iterate over nodes and their parents
  6. For each edge, compute KL divergence difference
  7. Remove edges below threshold
  8. Return final subgraph

- Design tradeoffs:
  - Expressiveness vs. tractability: Searching over all subgraphs is intractable, so ACDC uses a greedy approach
  - Metric choice: KL divergence vs. logit difference (KL is always positive but may miss negative components)
  - Edge representation: Including/excluding Q/K/V splits affects granularity and runtime

- Failure signatures:
  - Over-pruning: KL divergence remains low but task performance degrades
  - Under-pruning: Subgraph remains dense, missing the point of automation
  - Threshold sensitivity: Small changes in τ cause large changes in recovered circuits
  - Negative components: Important but harmful components may be removed to minimize KL divergence

- First 3 experiments:
  1. Run ACDC on a simple tracr-compiled transformer with known ground truth to verify it recovers the expected circuit
  2. Compare ACDC with zero activations vs. corrupted activations on the induction task to understand their differences
  3. Apply ACDC to the IOI circuit with varying thresholds to see how the recovered subgraph changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do negative components arise in transformer language models?
- Basis in paper: [explicit] The paper notes that ACDC systematically misses some classes of abstract units that are part of the circuit, for example the negative name mover heads from IOI (Wang et al., 2023). The authors hypothesize that this is because negative components increase performance when ablated, and ACDC appears to not include these components.
- Why unresolved: The paper does not empirically investigate the prevalence or importance of negative components in transformers. It only provides one example where negative components were missed.
- What evidence would resolve it: An empirical study investigating the prevalence of negative components across a range of transformer models and tasks, including their impact on model performance when ablated.

### Open Question 2
- Question: What are better metrics for measuring how well interpretations including negative components reflect model performance?
- Basis in paper: [explicit] The authors note that KL divergence is insufficient for approximating model performance when circuit components are important but harmful for model performance. They discuss alternatives like matching the model's performance or only including locally large effects, but find issues with each approach.
- Why unresolved: The paper does not propose or validate a superior metric. It only highlights limitations of existing approaches like KL divergence and logit difference.
- What evidence would resolve it: Development and validation of a new metric that can better capture the contribution of both positive and negative components to model performance. This would require extensive empirical testing.

### Open Question 3
- Question: How sensitive is ACDC's performance to the iteration order of the search over abstract units?
- Basis in paper: [explicit] The authors note that the behavior of the algorithm is sensitive to the iteration order of the search over abstract units, which remains a hard to tune hyperparameter.
- Why unresolved: The paper does not systematically investigate the impact of iteration order on ACDC's performance. It only mentions that the order of parents can affect experimental results.
- What evidence would resolve it: A comprehensive study varying the iteration order and measuring its impact on ACDC's ability to recover known circuits and its overall performance across different tasks and models.

## Limitations
- ACDC systematically misses negative components that harm performance but are integral to the true circuit
- The method is highly sensitive to the threshold hyperparameter τ, where small changes can lead to dramatically different recovered circuits
- The current implementation doesn't distinguish between Q, K, and V weight matrices in attention heads, potentially missing fine-grained circuit structure

## Confidence
- High confidence in the core algorithmic approach and its implementation, given the specific pseudocode and successful application to known circuits
- Medium confidence in the claims about expressiveness and superiority over component-level pruning, as these rely on specific task examples
- Medium confidence in the scalability claims, as the method has been demonstrated on GPT-2 Small but not on larger models
- Low confidence in the method's ability to handle negative components, as this limitation is explicitly acknowledged

## Next Checks
1. Apply ACDC to a larger model (GPT-2 Medium or GPT-Neo) to test scalability and verify that the method maintains performance on more complex circuits
2. Conduct a systematic ablation study on the threshold τ parameter, testing multiple values on the same task to quantify sensitivity and identify robust settings
3. Design a synthetic task specifically engineered to contain negative components, then verify whether ACDC can recover them or demonstrate why it cannot