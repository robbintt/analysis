---
ver: rpa2
title: 'The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights
  that Matter'
arxiv_id: '2306.03805'
source_url: https://arxiv.org/abs/2306.03805
tags:
- sparsity
- arxiv
- pre-trained
- essential
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large pre-trained transformers are increasingly difficult to fine-tune\
  \ and deploy due to exploding parameter counts. This work introduces the concept\
  \ of \"essential sparsity\" \u2014 the phenomenon that a significant portion of\
  \ weights in large pre-trained models can be removed without performance degradation."
---

# The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter

## Quick Facts
- **arXiv ID:** 2306.03805
- **Source URL:** https://arxiv.org/abs/2306.03805
- **Reference count:** 40
- **Primary result:** Large pre-trained transformers exhibit "essential sparsity" where ~30-50% of weights can be removed without performance degradation using one-shot magnitude pruning.

## Executive Summary
This paper introduces the concept of "essential sparsity" in large pre-trained transformers, demonstrating that a significant portion of weights can be pruned without performance loss. Through one-shot magnitude pruning, the authors identify sparse masks that work universally across downstream tasks with no additional computational overhead. The work reveals that self-supervised learning objectives induce stronger emergent sparsification properties than supervised learning, and that larger pre-training datasets lead to better knowledge condensation into fewer parameters. The paper also observes an intriguing "abrupt sparsification" phenomenon during BERT pre-training where models suddenly become heavily sparse after certain training iterations.

## Method Summary
The paper employs one-shot magnitude pruning (OMP) to identify sparse masks by removing the lowest-magnitude weights from pre-trained transformers. These pruned models are then fine-tuned on downstream tasks to evaluate performance degradation as a function of sparsity level. The authors compare OMP masks with iterative magnitude pruning (IMP)-based lottery tickets using cosine similarity. Layer-wise weight distribution analysis is conducted across different model families (BERT, OPT, ViT, DiNO) and pre-training objectives (self-supervised vs supervised). The study also examines how pre-training data volume affects emergent sparsity patterns through custom BERT-Base experiments with varying dataset sizes.

## Key Results
- Essential sparsity exists across multiple transformer architectures with sharp performance drop-off beyond certain sparsity thresholds
- One-shot magnitude pruning masks achieve >96% cosine similarity with lottery ticket masks within the essential sparsity range
- Self-supervised learning objectives induce stronger emergent sparsification properties than supervised learning
- "Abrupt sparsification" phenomenon observed during BERT pre-training where models suddenly become heavily sparse after specific training iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large pre-trained transformers develop "essential sparsity" with a sharp turning point beyond which pruning degrades performance rapidly
- Mechanism: During pre-training, models naturally concentrate important knowledge into fewer parameters, leaving many weights with minimal magnitudes that can be pruned without loss
- Core assumption: Low-magnitude weights are genuinely redundant and removing them does not affect learned representations
- Evidence anchors: [abstract] "essential sparsity defined with a sharp dropping point beyond which the performance declines much faster w.r.t the rise of sparsity level"; [section] "we define essential sparsity as sharp dropping point, beyond which the fine-tuning performance after one-shot pruning declines much faster w.r.t. the sparsity level rise"
- Break condition: If low-magnitude weights are actually storing important information or if fine-tuning requires those connections for adaptation

### Mechanism 2
- Claim: Self-supervised learning objectives induce stronger emergent sparsification properties than supervised learning
- Mechanism: SSL training forces models to learn more general representations that can be compressed more effectively, while SL may overfit to specific labeled patterns
- Core assumption: SSL's unsupervised nature promotes learning of more efficient representations that concentrate knowledge
- Evidence anchors: [abstract] "self-supervised learning (SSL) objectives trigger stronger emergent sparsification properties than supervised learning (SL)"; [section] "we observed that SSL tends to have better emergent sparsification properties, thereby more friendly to pruning"
- Break condition: If SSL models are simply smaller or if the SSL vs SL comparison is confounded by other factors like architecture differences

### Mechanism 3
- Claim: Larger pre-training datasets lead to better emergent sparsification, allowing models to condense knowledge into fewer parameters
- Mechanism: More diverse training data forces models to develop more generalizable and compact representations
- Core assumption: Pre-training on larger datasets improves models' ability to abstract knowledge efficiently
- Evidence anchors: [abstract] "BERT trained with a larger amount of pre-training data tend to have a better ability to condense knowledge in comparatively relatively fewer parameters"; [section] "BERT trained with a larger amount of pre-training data tend to have better emergence of induced sparsity"
- Break condition: If larger datasets simply lead to overparameterization without improved abstraction, or if the effect is due to other factors like longer training

## Foundational Learning

- Concept: Lottery Ticket Hypothesis
  - Why needed here: Understanding why traditional LTH approaches fail at scale and how essential sparsity provides an alternative
  - Quick check question: What is the key difference between essential sparsity masks and lottery tickets in terms of computational requirements?

- Concept: Magnitude-based pruning
  - Why needed here: The paper uses one-shot magnitude pruning to identify essential sparsity, so understanding this technique is crucial
  - Quick check question: How does one-shot magnitude pruning differ from iterative magnitude pruning in terms of computational cost?

- Concept: Pre-training vs fine-tuning dynamics
  - Why needed here: The paper studies how pre-training affects emergence of sparse patterns and downstream performance
  - Quick check question: Why might a model that generalizes well during pre-training also be more amenable to pruning?

## Architecture Onboarding

- Component map: Pre-trained transformer models (BERT, OPT, ViT, DiNO) -> One-shot magnitude pruning -> Fine-tuning pipeline -> Layer-wise analysis tools
- Critical path: 1) Load pre-trained checkpoint 2) Apply one-shot magnitude pruning to identify sparse mask 3) Fine-tune pruned model on downstream task 4) Evaluate performance vs dense baseline 5) Analyze sparsity patterns across layers
- Design tradeoffs: One-shot pruning vs iterative approaches (computational efficiency vs potentially better masks); Task-agnostic masks vs task-specific masks (universality vs optimal performance); Structured vs unstructured sparsity (hardware efficiency vs flexibility)
- Failure signatures: Performance degradation when pruning beyond essential sparsity threshold; Inconsistent pruning ratios across different downstream tasks; Mask similarity decreasing between LTH and OMP as sparsity increases
- First 3 experiments: 1) Reproduce essential sparsity curve on bert-base using MNLI task with varying pruning ratios 2) Compare one-shot vs iterative magnitude pruning on a small-scale model to understand performance tradeoffs 3) Analyze layer-wise weight distributions of ViT-base vs DINO-base to understand SSL vs SL differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the pre-training data volume influence the emergence of essential sparsity in large transformers, and can this relationship be generalized across different model architectures?
- Basis in paper: [explicit] The paper discusses the impact of pre-training data volume on induced sparse patterns in BERT, finding that models trained with larger datasets tend to have better emergence of induced sparsity
- Why unresolved: The paper focuses on BERT and does not generalize findings to other transformer architectures or different types of pre-training objectives
- What evidence would resolve it: Systematic experiments across various transformer models and pre-training objectives, measuring essential sparsity emergence as a function of pre-training data volume

### Open Question 2
- Question: What is the underlying mechanism that causes the "abrupt sparsification" phenomenon observed during BERT pre-training, and how does it relate to the model's ability to condense knowledge?
- Basis in paper: [explicit] The paper presents the phenomenon of abrupt sparsification, where BERT suddenly becomes heavily sparse after certain training iterations
- Why unresolved: The paper observes the phenomenon but does not provide a theoretical explanation for why it occurs or how it relates to knowledge condensation
- What evidence would resolve it: Theoretical analysis of the training dynamics, combined with experiments showing the relationship between abrupt sparsification and knowledge abstraction capabilities

### Open Question 3
- Question: Can the high cosine similarity (>96%) between Lottery Ticket Hypothesis masks and one-shot magnitude pruning masks within the essential sparsity range be explained by a unified theory of sparse subnetwork selection?
- Basis in paper: [explicit] The paper finds surprisingly high cosine similarity between LTH and OMP masks within the essential sparsity range across multiple downstream tasks
- Why unresolved: While the empirical observation is made, there is no theoretical framework explaining why these two different pruning methods yield similar masks in this specific range
- What evidence would resolve it: Mathematical proof or theoretical framework showing why magnitude-based pruning and iterative training-based pruning converge to similar solutions in the essential sparsity regime

### Open Question 4
- Question: How does the choice of pre-training objective (self-supervised vs supervised) fundamentally influence the distribution of weight magnitudes and subsequent prunability, beyond the observed empirical differences?
- Basis in paper: [explicit] The paper observes that SSL objectives trigger stronger emergent sparsification properties than SL, with SSL-based models having more zero-weight concentrations
- Why unresolved: The paper shows empirical differences but doesn't explain the fundamental reasons why SSL leads to more favorable weight distributions for pruning
- What evidence would resolve it: Theoretical analysis of how different pre-training objectives affect the loss landscape and gradient flow, combined with experiments on the resulting weight distributions

## Limitations
- The relationship between essential sparsity and lottery ticket hypothesis is not fully resolved, particularly why traditional LTH approaches fail at scale
- SSL vs SL comparison lacks rigorous ablation studies to isolate whether observed differences stem from learning objectives or other factors
- "Abrupt sparsification" phenomenon needs more detailed analysis to understand underlying mechanisms driving sudden emergence

## Confidence
- **High confidence**: The existence of essential sparsity (identifiable sharp threshold where performance degrades) is well-supported by empirical evidence across multiple model families and tasks
- **Medium confidence**: The claim that self-supervised learning induces stronger sparsification properties is supported but requires more rigorous ablation studies to rule out confounding factors
- **Medium confidence**: The relationship between pre-training dataset size and emergent sparsity is demonstrated but lacks mechanistic explanation for why larger datasets improve compression efficiency

## Next Checks
1. **Ablation study on SSL vs SL**: Train BERT models with identical architectures but different objectives (MLM vs supervised next-sentence prediction) on the same data to isolate the effect of learning objective on emergent sparsity
2. **Layer-wise essential sparsity analysis**: Perform detailed analysis of essential sparsity thresholds across different transformer layers to understand whether the phenomenon is uniform or layer-dependent
3. **Transferability validation**: Test whether essential sparsity masks discovered on one task transfer effectively to other tasks beyond the MNLI benchmark, particularly for tasks with different characteristics (e.g., token classification vs sequence classification)