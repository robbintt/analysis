---
ver: rpa2
title: '$FastDoc$: Domain-Specific Fast Continual Pre-training Technique using Document-Level
  Metadata and Taxonomy'
arxiv_id: '2306.06190'
source_url: https://arxiv.org/abs/2306.06190
tags:
- fpdm
- pre-training
- dataset
- domain
- robert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FPDM is a compute-efficient transformer pre-training framework
  for domain-specific tasks. It leverages document-level metadata and taxonomy as
  supervision signals, using sentence-level embeddings during pre-training and token-level
  embeddings during fine-tuning.
---

# $FastDoc$: Domain-Specific Fast Continual Pre-training Technique using Document-Level Metadata and Taxonomy

## Quick Facts
- arXiv ID: 2306.06190
- Source URL: https://arxiv.org/abs/2306.06190
- Reference count: 40
- Primary result: 500-4,500x compute reduction vs MLM/NSP while maintaining/improving domain-specific task performance

## Executive Summary
FPDM is a compute-efficient transformer pre-training framework for domain-specific tasks. It leverages document-level metadata and taxonomy as supervision signals, using sentence-level embeddings during pre-training and token-level embeddings during fine-tuning. This approach reduces pre-training compute by 500-4,500x compared to standard methods like MLM and NSP, while maintaining or improving performance on tasks across customer support, scientific, and legal domains. FPDM also mitigates catastrophic forgetting on open-domain tasks.

## Method Summary
FPDM uses a hierarchical encoder architecture with a frozen lower-level sentence transformer (sBERT/sRoBERTa) providing sentence embeddings to a higher-level pre-trained BERT/RoBERTa encoder. The framework employs two supervision signals: triplet margin loss for document similarity based on metadata and hierarchical classification loss using domain taxonomies. Pre-training uses sentence-level embeddings as inputs while fine-tuning downstream tasks uses token-level embeddings, enabling efficient handling of long documents.

## Key Results
- Achieves 500-4,500x reduction in pre-training compute compared to MLM/NSP
- Maintains or improves performance on domain-specific tasks across customer support, scientific, and legal domains
- Shows negligible catastrophic forgetting on open-domain tasks like GLUE benchmark

## Why This Works (Mechanism)

### Mechanism 1
Document-level metadata and taxonomy as supervision signals enable efficient pre-training by replacing local context-based methods with document similarity learning. This leverages richer semantic relationships between documents.

Core assumption: Documents can be meaningfully grouped by metadata or taxonomy, capturing semantic similarity.

Evidence: Triplet network formulation with similar/dissimilar documents based on metadata.

### Mechanism 2
Sentence-level embeddings as inputs during pre-training enable handling long documents and reduce compute through a hierarchical architecture with frozen lower-level encoder.

Core assumption: Sentence embeddings from pre-trained transformers are effective document representations for downstream tasks.

Evidence: Hierarchical architecture with frozen sentence transformer providing embeddings to higher-level encoder.

### Mechanism 3
Interoperability of sentence and token embeddings enables effective fine-tuning after sentence-level pre-training by preserving relative document representations.

Core assumption: Relative representations learned during pre-training are preserved across embedding spaces.

Evidence: Claim of preserved relative document representations enabling token-level fine-tuning.

## Foundational Learning

- **Document-level supervision vs token-level supervision**
  - Why needed: Understanding the shift from MLM/NSP to document-level supervision
  - Quick check: What are the key differences between document-level and token-level supervision?

- **Hierarchical document encoding**
  - Why needed: FPDM uses frozen lower-level and fine-tuned higher-level encoders
  - Quick check: How does freezing the lower-level encoder reduce compute?

- **Contrastive learning with triplet networks**
  - Why needed: FPDM uses triplet margin loss for document similarity
  - Quick check: How does a triplet network learn document similarity?

## Architecture Onboarding

- **Component map**: Document → Sentence embeddings (lower-level) → Higher-level encoder → Document representation → Losses → Fine-tuning

- **Critical path**: Document → Sentence embeddings (lower-level) → Higher-level encoder → Document representation → Losses → Fine-tuning on downstream tasks

- **Design tradeoffs**:
  - Freezing lower-level encoder vs fine-tuning: Reduces compute but may limit flexibility
  - Sentence vs token embeddings: Enables longer documents but may lose fine-grained information
  - Document-level vs token-level supervision: More efficient but may not capture local context

- **Failure signatures**:
  - Poor downstream performance: Issues with pre-training, fine-tuning, or embedding compatibility
  - High compute usage: Lower-level encoder not properly frozen
  - Catastrophic forgetting: Pre-training data too large or model not robust to domain changes

- **First 3 experiments**:
  1. Fine-tune FPDM on simple text classification and compare to MLM/NSP baseline
  2. Vary pre-training data amount and observe impact on performance and compute
  3. Test sentence-token embedding interoperability on token-level task (NER)

## Open Questions the Paper Calls Out

### Open Question 1
How does FPDM perform with noisy or ambiguous document metadata?
- Basis: Paper shows performance with clean metadata but doesn't explore noisy scenarios
- Evidence needed: Experiments with intentionally corrupted metadata

### Open Question 2
Can hierarchical classification adapt to domains without predefined taxonomies?
- Basis: Paper uses existing taxonomies but doesn't explore dynamic taxonomy creation
- Evidence needed: Automatic taxonomy generation and evaluation

### Open Question 3
Impact of scaling FPDM to larger transformer models (BERTLARGE/RoBERTaLARGE)?
- Basis: Paper limited to BERTBASE/RoBERTaBASE due to resource constraints
- Evidence needed: Experiments with larger models comparing performance and efficiency

## Limitations

- Evaluation scope limited to domain-specific tasks; broader generalization uncertain
- Performance critically depends on quality of sentence embeddings from frozen transformers
- Requires document metadata and hierarchical taxonomies which may not be available for all domains

## Confidence

- **High Confidence**: Compute reduction claims well-supported by architecture design
- **Medium Confidence**: Domain-specific performance improvements demonstrated but evaluation could be more comprehensive
- **Low Confidence**: Catastrophic forgetting mitigation claims based on limited GLUE evaluation

## Next Checks

1. **Metadata Availability Stress Test**: Systematically evaluate FPDM's performance across domains with varying metadata quality and completeness through controlled experiments with corrupted metadata.

2. **Open-Domain Generalization Benchmark**: Design comprehensive evaluation suite testing FPDM on diverse open-domain tasks beyond GLUE, including token-level and document-level tasks.

3. **Sentence Embedding Quality Analysis**: Conduct ablation studies varying sentence transformer types and analyze correlation between sentence embedding quality and downstream task performance.