---
ver: rpa2
title: 'C3: Zero-shot Text-to-SQL with ChatGPT'
arxiv_id: '2307.07306'
source_url: https://arxiv.org/abs/2307.07306
tags:
- text-to-sql
- chatgpt
- prompt
- clear
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces C3, a zero-shot Text-to-SQL method using
  ChatGPT that achieves state-of-the-art performance with 82.3% execution accuracy
  on the Spider dataset. The method addresses challenges in zero-shot Text-to-SQL
  by proposing three key components: Clear Prompting (CP) for effective prompt design,
  Calibration with Hints (CH) to mitigate model biases, and Consistent Output (CO)
  to enhance result stability through self-consistency.'
---

# C3: Zero-shot Text-to-SQL with ChatGPT

## Quick Facts
- arXiv ID: 2307.07306
- Source URL: https://arxiv.org/abs/2307.07306
- Reference count: 8
- Execution accuracy: 82.3% on Spider dataset

## Executive Summary
C3 introduces a zero-shot Text-to-SQL method that achieves state-of-the-art performance using ChatGPT without any training. The approach combines three key components: Clear Prompting for effective prompt design, Calibration with Hints to mitigate model biases, and Consistent Output to enhance result stability. By using approximately 1,000 tokens per query and leveraging ChatGPT's capabilities, C3 outperforms fine-tuning-based methods by 2.4% execution accuracy while maintaining cost-effectiveness compared to few-shot approaches.

## Method Summary
C3 is a zero-shot Text-to-SQL method that leverages ChatGPT's capabilities through a three-component approach. The Clear Prompting (CP) component uses selective schema inclusion through table and column recall to reduce noise and token usage. Calibration with Hints (CH) addresses ChatGPT's inherent biases in SQL generation through explicit instructions. Consistent Output (CO) employs self-consistency by generating multiple SQL queries and selecting the most consistent execution result. The method processes natural language questions with database schemas, recalling relevant schema elements, constructing clear prompts with calibration hints, and using self-consistency to select the final SQL query.

## Key Results
- Achieves 82.3% execution accuracy on Spider holdout test set
- Outperforms fine-tuning-based methods by 2.4% execution accuracy
- Uses approximately 1,000 tokens per query compared to >10,000 tokens for few-shot methods

## Why This Works (Mechanism)

### Mechanism 1: Clear Prompt Layout
Clear layout separates instruction, schema, and question with sharp symbols, making prompts easier for ChatGPT to parse. This improves execution accuracy by 7.0% compared to complicated layouts that concatenate everything together. The core assumption is that ChatGPT's performance is highly sensitive to prompt structure and benefits from clear component separation.

### Mechanism 2: Schema Linking with Table and Column Recall
Selective schema inclusion through table and column recall improves performance by 2.3-2.6% by reducing irrelevant information. The method recalls relevant tables using a zero-shot prompt, then recalls relevant columns within those tables, including only these with foreign key information. This reduces prompt length and focuses the model on relevant schema elements.

### Mechanism 3: Calibration with Hints (CH)
Addressing ChatGPT's inherent biases through calibration hints improves accuracy by 1.5%. Two biases are corrected: avoiding unnecessary columns in SELECT clauses, and avoiding misuse of LEFT JOIN, IN, and OR by using JOIN and INTERSECT instead. The core assumption is that ChatGPT has systematic biases that can be corrected through explicit instructions.

## Foundational Learning

- **Concept: Prompt engineering for LLMs**
  - Why needed here: ChatGPT's performance on Text-to-SQL is highly dependent on how instructions and context are presented
  - Quick check question: What are the three key components of the C3 method and what aspect of the model do they address?

- **Concept: Schema linking and retrieval**
  - Why needed here: Including entire database schemas creates noise and increases token usage, so selective retrieval of relevant schema elements is necessary
  - Quick check question: How does the table recall process work in the C3 method?

- **Concept: Self-consistency methods**
  - Why needed here: LLM outputs are inherently random, and self-consistency can improve reliability by selecting the most common execution result
  - Quick check question: What parameter value was used for the number of SQL queries generated in the self-consistency method?

## Architecture Onboarding

- **Component map:** Natural language question + database schema → Table recall → Column recall → Prompt construction → Calibration hints → SQL generation → Self-consistency → Final SQL output
- **Critical path:** Question → Table recall → Column recall → Prompt construction → SQL generation → Self-consistency → Output
- **Design tradeoffs:**
  - Token efficiency vs completeness: Selective schema inclusion saves tokens but risks missing necessary information
  - Prompt complexity vs performance: More sophisticated prompts improve accuracy but increase development complexity
  - Cost vs accuracy: Generating multiple SQLs for self-consistency improves accuracy but increases API costs
- **Failure signatures:**
  - Missing necessary tables/columns in recall → SQL queries fail to answer questions correctly
  - Over-correction in calibration → SQL queries use inappropriate JOIN types or miss necessary columns
  - Inconsistent self-consistency → Different execution results prevent clear winner selection
- **First 3 experiments:**
  1. Test table recall accuracy on a subset of Spider dev set by comparing recalled tables against ground truth SQL tables
  2. Compare execution accuracy with and without calibration hints on a small dataset to verify the 1.5% improvement claim
  3. Measure the impact of different numbers of SQL generations (k values) on accuracy to find optimal self-consistency setting

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Limited to Spider dataset evaluation (200 databases, 138 domains), raising questions about scalability to larger, more complex databases
- Relies heavily on ChatGPT's capabilities, making it vulnerable to model changes or API updates
- Recall-based schema linking may miss relevant tables or columns in complex schemas, potentially limiting generalizability

## Confidence
- **High confidence:** Overall performance claims (82.3% execution accuracy) and comparative advantage over fine-tuning methods are well-supported by experimental results
- **Medium confidence:** Individual component contributions (7.0% for clear layout, 2.3-2.6% for schema recall, 1.5% for calibration) based on ablation studies but depend on specific implementation details not fully disclosed
- **Medium confidence:** Cost-effectiveness claims relative to few-shot methods are reasonable given token usage (~1,000 tokens per query) but lack direct comparative analysis

## Next Checks
1. **Schema recall accuracy validation:** Implement and test the table and column recall prompts on a held-out subset of Spider to measure precision and recall of recalled schema elements against ground truth SQL references
2. **Calibration hint generalization test:** Test the method on a different Text-to-SQL dataset (e.g., WikiSQL or ATIS) to verify that calibration hints effectively address the same biases in different contexts
3. **Cost-efficiency analysis:** Measure actual API costs for C3 versus fine-tuning approaches on equivalent workloads, accounting for the 20-generation self-consistency requirement and comparing against typical fine-tuning expenses