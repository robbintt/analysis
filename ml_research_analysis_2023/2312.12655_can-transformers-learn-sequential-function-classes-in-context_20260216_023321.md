---
ver: rpa2
title: Can Transformers Learn Sequential Function Classes In Context?
arxiv_id: '2312.12655'
source_url: https://arxiv.org/abs/2312.12655
tags:
- sequential
- function
- learning
- data
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether transformers can learn from sequential,
  non-textual function class data distributions through in-context learning. The authors
  introduce a novel sliding window sequential function class and train toy-sized transformers
  with GPT-2 architecture on these tasks.
---

# Can Transformers Learn Sequential Function Classes In Context?

## Quick Facts
- **arXiv ID**: 2312.12655
- **Source URL**: https://arxiv.org/abs/2312.12655
- **Reference count**: 15
- **Primary result**: Transformers can leverage in-context learning on non-textual sequential function classes, showing robustness to label noise.

## Executive Summary
This paper investigates whether transformers can learn from sequential, non-textual function class data distributions through in-context learning. The authors introduce a novel sliding window sequential function class and train toy-sized transformers with GPT-2 architecture on these tasks. Their analysis shows that transformers can indeed leverage in-context learning when trained on non-textual sequential function classes. Additionally, experiments with randomized y-label sequences reveal that transformers retain some in-context learning capabilities even when label associations are obfuscated. The performance deteriorates with increasing randomness in labels, but not as much as expected, suggesting robustness of learned sequentiality against label noise. The findings provide evidence that transformers can reason with and understand sequentiality encoded within function classes.

## Method Summary
The paper introduces a novel sliding window sequential function class and trains toy-sized GPT-2 architecture transformers (12 layers, 8 attention heads, embedding size 256) on these tasks. The authors generate sequential data using recursive relations (recursive bias, recursive linear transformations, recursive two-layer ReLU neural networks with normalization) and train transformers to predict y_i = x_{i+1} by aggregating information from previous k elements. They evaluate performance with and without randomized y-label sequences to test robustness. The training uses a modified loss function for numerical stability, and the model treats each input x_i as a query with access only to embeddings of previous (x_j, y_j) pairs.

## Key Results
- Transformers can learn sequential function classes through in-context learning when trained on non-textual sequential data
- Performance deteriorates with increasing randomness in y-labels, but not to the extent expected, showing robustness to label noise
- The sequential structure of inputs provides enough signal for transformers to perform ICL even with noisy labels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Transformers learn sequentiality through recurrent patterns in the input-output structure, where the output depends on previous inputs in a predictable way.
- **Mechanism**: The transformer builds internal representations that capture dependencies between sequential elements. In the sliding window function class, the model learns to predict y_i = x_{i+1} by aggregating information from previous k elements using attention mechanisms.
- **Core assumption**: The sequential function is linearly separable with respect to the inputs x_i, ..., x_{i-k}.
- **Evidence anchors**:
  - [abstract]: "transformers can reason with and understand sequentiality encoded within function classes"
  - [section]: "We provide evidence that transformers can reason with and understand sequentiality encoded within function classes, as reflected by the effective learning of our proposed tasks."
  - [corpus]: Weak evidence. The related papers focus on ICL in general but don't specifically address sequential non-textual function classes.
- **Break condition**: If the sequential function becomes non-linear or requires higher-order interactions beyond what can be captured through attention and linear combinations.

### Mechanism 2
- **Claim**: Induction heads play a role in sequential learning, where attention heads copy information from previous tokens into each token.
- **Mechanism**: A pair of attention heads work together - one head copies information from the previous token, and the second head (the induction head) searches in the context created by the first head. This creates a chain of dependencies that enables sequential prediction.
- **Core assumption**: Induction heads emerge in transformers trained on sequential data, similar to how they emerge in language models.
- **Evidence anchors**:
  - [abstract]: "Future research may want to look into how previous explanations of transformers, such as induction heads and task vectors, relate to sequentiality in ICL in these toy examples."
  - [section]: "In Olsson et al. [2022], the authors observe that a significant part of in-context learning can be explained by specific attention heads, which they call induction heads."
  - [corpus]: Weak evidence. The corpus mentions induction heads in general ICL contexts but doesn't specifically address sequential non-textual data.
- **Break condition**: If the sequential patterns are too complex for simple induction mechanisms, requiring deeper layers or different attention patterns.

### Mechanism 3
- **Claim**: Transformers maintain robustness to label noise in sequential tasks through learned structural understanding of the input patterns.
- **Mechanism**: Even when y-labels are randomized, the model can still leverage the underlying sequential structure of the inputs to make reasonable predictions. The model learns to ignore corrupted examples while using the remaining structured examples.
- **Core assumption**: The sequential structure of inputs provides enough signal for the model to perform ICL even with noisy labels.
- **Evidence anchors**:
  - [abstract]: "Our results also show that the performance deteriorated with increasing randomness in the labels, though not to the extent one might expect, implying a potential robustness of learned sequentiality against label noise."
  - [section]: "We evaluated the same tasks with randomized labels for our sequences and found that this degrades ICL performance in transformers, suggesting that transformers trained on non-textual sequential meta-learning tasks are less robust than with textual tasks."
  - [corpus]: Weak evidence. The corpus mentions robustness in general ICL contexts but doesn't specifically address sequential non-textual tasks with randomized labels.
- **Break condition**: When the ratio of randomized to valid labels becomes too high, overwhelming the model's ability to extract meaningful patterns.

## Foundational Learning

- **Concept**: Function classes and meta-learning
  - **Why needed here**: Understanding how transformers learn from function classes is fundamental to grasping the paper's contribution. The paper extends Garg et al.'s work by introducing sequential function classes.
  - **Quick check question**: What is the difference between order-invariant function classes and sequential function classes?

- **Concept**: Attention mechanisms and induction heads
  - **Why needed here**: The paper references induction heads as a potential explanation for ICL. Understanding how attention heads work together to create sequential dependencies is crucial.
  - **Quick check question**: How do induction heads differ from regular attention heads in transformers?

- **Concept**: Numerical stability in recursive functions
  - **Why needed here**: The paper discusses numerical stability issues when training on sequential tasks. Understanding why eigenvalues and sequence length matter is important for replicating the experiments.
  - **Quick check question**: Why do functions with eigenvalues greater than 1 cause numerical instability in long sequences?

## Architecture Onboarding

- **Component map**: Data generation -> Sequential function application -> Model training with normalized loss -> Evaluation with randomized labels
- **Critical path**: The key insight is that the model learns to predict y_i = x_{i+1} by aggregating information from previous k elements.
- **Design tradeoffs**: Using smaller toy-sized transformers (10M parameters) versus larger pretrained models. The tradeoff is between computational efficiency and potential performance. The paper also uses L2-normalized loss for numerical stability versus standard MSE loss.
- **Failure signatures**: 
  - Loss not converging to near zero (as seen in recursive bias task)
  - Poor performance when label randomization exceeds 50% of examples
  - Numerical instability when eigenvalues exceed 0.99 in magnitude
- **First 3 experiments**:
  1. Train on recursive bias task (sliding window length 1, additive bias) and observe loss convergence
  2. Train on recursive linear transformations task with eigenvalue constraints and compare with normalized vs standard loss
  3. Train on recursive two-layer ReLU neural networks with normalization and evaluate robustness to label randomization at different noise levels

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What specific mechanisms underlie the difference in in-context learning performance between textual and non-textual sequential data?
- **Basis in paper**: [explicit] The paper states that transformers retain some ICL capabilities with randomized y-label sequences for non-textual sequential data, but performance degrades compared to textual tasks.
- **Why unresolved**: The paper hypothesizes that the mechanisms may be different, but does not provide a definitive explanation for why this difference exists.
- **What evidence would resolve it**: Further analysis of transformer attention patterns and weight distributions when trained on textual vs non-textual sequential data could reveal the underlying mechanisms.

### Open Question 2
- **Question**: How do induction heads manifest in transformers trained on non-textual sequential function classes?
- **Basis in paper**: [inferred] The paper mentions extending the work on induction heads to non-textual sequential data but does not present results on their presence or absence.
- **Why unresolved**: The authors did not have sufficient computational resources to conduct experiments on induction heads in sequential data.
- **What evidence would resolve it**: Visualizing attention patterns and conducting ablation studies on transformers trained on non-textual sequential data would determine if induction heads emerge and their role in ICL.

### Open Question 3
- **Question**: How does the complexity and structure of the function class affect in-context learning capabilities?
- **Basis in paper**: [explicit] The paper introduces novel sequential function classes and finds that transformers can learn from them, but performance varies with different task types.
- **Why unresolved**: The experiments only tested on simple sequential tasks, and the paper suggests exploring more complex functions as a future direction.
- **What evidence would resolve it**: Training transformers on increasingly complex and unstructured sequential function classes would reveal the relationship between task complexity and ICL performance.

## Limitations

- Findings primarily based on toy-sized transformers (10M parameters) rather than larger pretrained models
- Experiments limited to synthetic sequential data, unclear how results generalize to real-world tasks
- Analysis of induction heads and their role in sequential ICL remains speculative without empirical verification

## Confidence

- **High Confidence**: The core finding that transformers can learn basic sequential patterns in synthetic function classes is well-supported by the experimental results.
- **Medium Confidence**: The claim about robustness to label noise is supported by experiments, but the mechanism behind this robustness isn't fully explained.
- **Low Confidence**: The paper's suggestions about how induction heads might explain sequential ICL in these contexts are speculative and not empirically verified.

## Next Checks

1. **Architecture Scaling**: Replicate the experiments with larger transformer architectures (e.g., 125M to 1B parameters) to assess whether the sequential ICL capabilities scale with model size, as is observed in other transformer capabilities.

2. **Real-World Sequential Tasks**: Test the model on more complex sequential function classes derived from real-world problems (e.g., time series forecasting with non-linear dependencies) to validate whether the learned sequentiality transfers beyond synthetic data.

3. **Mechanistic Analysis**: Conduct ablation studies on attention heads, particularly focusing on whether induction heads emerge in these sequential tasks and whether their presence correlates with ICL performance, building on Olsson et al.'s work.