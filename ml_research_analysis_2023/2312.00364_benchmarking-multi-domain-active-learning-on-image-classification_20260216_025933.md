---
ver: rpa2
title: Benchmarking Multi-Domain Active Learning on Image Classification
arxiv_id: '2312.00364'
source_url: https://arxiv.org/abs/2312.00364
tags:
- domain
- sampling
- margin
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks multi-domain active learning on image classification
  tasks. It shows that traditional single-domain active learning strategies, such
  as margin sampling, can be less effective than random selection in multi-domain
  scenarios.
---

# Benchmarking Multi-Domain Active Learning on Image Classification

## Quick Facts
- arXiv ID: 2312.00364
- Source URL: https://arxiv.org/abs/2312.00364
- Reference count: 40
- Primary result: No single multi-domain active learning strategy outperforms across all datasets and metrics; best strategies improve mean-group accuracy by 0.96% and worst-group by 3.6% over margin sampling.

## Executive Summary
This paper benchmarks multi-domain active learning (MDAL) on image classification tasks, revealing that traditional single-domain strategies like margin sampling can underperform random selection in multi-domain scenarios. The authors introduce CLIP-GeoYFCC, a large-scale geographical image dataset, and compare it with Domainnet across multiple domain compositions. Through extensive experiments, they demonstrate that explicit domain structure consideration through hierarchical allocation improves performance, though no single allocation strategy dominates across all metrics. The paper also introduces threshold-margin sampling, which improves ambient accuracy while maintaining comparable performance on group metrics.

## Method Summary
The authors implement a two-step active learning framework combining domain allocation strategies (uniform, error-proportional, loss-exponential, worst-group) with instance-level query methods (margin sampling). Using ResNet-18 pre-trained on ILSVRC12, the method evaluates model performance on per-domain validation sets to determine budget allocation across domains, then applies margin sampling within each domain to select instances. The model is retrained from scratch after each query round. Experiments span 12 domain compositions for Domainnet-40 and all four datasets (CLIP-GeoYFCC-45, CLIP-GeoYFCC-350, Domainnet-40, Domainnet-345), evaluating ambient, mean-group, and worst-group accuracies over 40 query rounds.

## Key Results
- Traditional margin sampling underperforms random selection in multi-domain settings, particularly on worst-group accuracy
- Uniform domain allocation achieves higher mean-group accuracies than margin sampling across all datasets
- Best strategies improve mean-group accuracy by 0.96% and worst-group by 3.6% over margin sampling on average
- Threshold-margin sampling provides small gains on ambient accuracy but shows inconsistent performance across datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-domain active learning (MDAL) outperforms single-domain AL when domain structures are explicitly considered through hierarchical allocation.
- Mechanism: Traditional AL strategies like margin sampling select instances purely based on model uncertainty, ignoring domain-level information. In multi-domain settings, this can lead to suboptimal sampling as different domains may require different labeling priorities. By first allocating budget across domains (using strategies like uniform, error-proportional, or worst-group) and then applying instance-level selection within each domain, the method can better balance exploration and exploitation across domains.
- Core assumption: The domains are disjoint and identifiable, and domain-level validation sets provide reliable error estimates for budget allocation.
- Evidence anchors:
  - [abstract] "traditional single-domain active learning strategies are often less effective than random selection in multi-domain scenarios"
  - [section] "by varying the domain composition of the unlabeled data pool, the same AL method can perform very differently"
  - [corpus] Weak - corpus does not contain direct evidence about hierarchical allocation mechanisms
- Break condition: If domains are not identifiable or validation sets are unreliable, the domain allocation step may allocate budget poorly, negating any benefit.

### Mechanism 2
- Claim: No single domain allocation strategy dominates across all datasets and metrics; performance is highly dataset-dependent.
- Mechanism: Different domain compositions and characteristics (e.g., similarity between domains, class distribution) interact differently with allocation strategies. For example, worst-group allocation works well when one domain is significantly harder, but performs poorly on mean-group accuracy when domains are similar. This forces practitioners to choose strategies based on the specific dataset and metric of interest.
- Core assumption: Domain characteristics vary significantly across datasets, and the optimal strategy depends on these characteristics.
- Evidence anchors:
  - [abstract] "all multi-domain strategies exhibit significant tradeoffs, with no strategy outperforming across all datasets or all metrics"
  - [section] "although uniform domain allocation achieves higher mean-group accuracies than margin sampling on all datasets, in mean-group accuracies error-proportional and loss-exponential domain allocations are sometimes the winner"
  - [corpus] Weak - corpus does not provide specific evidence about strategy-dataset interactions
- Break condition: If a dataset has uniform domain characteristics (e.g., all domains are equally difficult and similar), then a single strategy might dominate across metrics.

### Mechanism 3
- Claim: Threshold-margin improves upon standard margin sampling by adaptively selecting high-uncertainty samples while maintaining diversity.
- Mechanism: Standard margin sampling selects the lowest-margin (most uncertain) samples, which can lead to redundancy and focus on similar error types. Threshold-margin sets a threshold based on validation error rate and randomly samples among instances above this threshold. This balances selection of hard examples with diversity, improving ambient accuracy while maintaining comparable performance on group metrics.
- Core assumption: The validation set error rate is a good proxy for the threshold that separates easy from hard examples in the unlabeled pool.
- Evidence anchors:
  - [section] "Threshold-margin randomly selects points with margin scores above a threshold, where the threshold is automatically re-adjusted after every batch update using a held-out validation set"
  - [section] "Compared with margin sampling that selects instances with lowest margin scores, threshold-margin sampling balances hard examples with diversity and achieves some improvement on ambient accuracies"
  - [corpus] Weak - corpus does not provide evidence about threshold-margin specifically
- Break condition: If the validation set is not representative of the unlabeled pool, the threshold may be poorly calibrated, leading to suboptimal selection.

## Foundational Learning

- Concept: Active learning and uncertainty sampling
  - Why needed here: The paper builds on uncertainty sampling (margin, least confidence, entropy) as the instance-level query strategy within a hierarchical framework
  - Quick check question: What is the difference between margin sampling and entropy sampling in terms of what they measure?

- Concept: Domain adaptation and multi-domain learning
  - Why needed here: The paper distinguishes MDAL from active domain adaptation (ADA) and explains why MDAL is more challenging (both training and test data contain multiple domains)
  - Quick check question: How does multi-domain active learning differ from active domain adaptation in terms of the data distribution?

- Concept: Evaluation metrics for fairness and robustness
  - Why needed here: The paper uses ambient (in-distribution), mean-group, and worst-group accuracies to evaluate performance, which are standard metrics for assessing fairness and robustness to domain shift
  - Quick check question: What does worst-group accuracy measure, and why is it important in multi-domain settings?

## Architecture Onboarding

- Component map: ResNet-18 model -> labeled seed set -> unlabeled training pools organized by domain -> per-domain validation sets -> domain allocation strategies (uniform, error-proportional, loss-exponential, worst-group) -> instance-level query strategies (margin sampling) -> retraining loop
- Critical path: For each query round: evaluate model on validation sets → apply domain allocation strategy to determine per-domain budgets → within each domain, apply margin sampling to select instances → label selected instances → retrain model from scratch on all labeled data
- Design tradeoffs: Using a pre-trained model provides good initialization but may introduce bias; retraining from scratch after each query is computationally expensive but ensures the model is optimized for the current labeled set; using validation sets for allocation provides feedback but requires additional labeled data
- Failure signatures: If a domain allocation strategy consistently underperforms random sampling (data efficiency < 1), it indicates the strategy is poorly calibrated to the dataset; if worst-group allocation leads to very low mean-group accuracy, it suggests the domains are too similar for aggressive worst-group focus
- First 3 experiments:
  1. Run margin sampling on a single-domain dataset to establish a baseline.
  2. Run uniform allocation + margin sampling on a multi-domain dataset to see if explicit domain consideration helps.
  3. Run worst-group allocation + margin sampling on a multi-domain dataset with one clearly harder domain to test targeted improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does threshold-margin sampling consistently outperform margin sampling across diverse multi-domain datasets?
- Basis in paper: [explicit] The paper states that threshold-margin provides small gain on ambient accuracy compared to margin sampling on average across different datasets, but its performance is not consistent across datasets.
- Why unresolved: The paper only presents aggregate results and comparisons on specific datasets (CLIP-GeoYFCC and Domainnet), without conducting a comprehensive evaluation across a wider range of multi-domain datasets with varying characteristics.
- What evidence would resolve it: Conducting extensive experiments on a diverse set of multi-domain datasets, including those with different numbers of domains, class distributions, and domain similarity patterns, would provide conclusive evidence on the consistency of threshold-margin's performance compared to margin sampling.

### Open Question 2
- Question: What is the optimal strategy for updating the validation set used in threshold-margin sampling during the active learning process?
- Basis in paper: [inferred] The paper mentions that the current algorithm uses a fixed held-out validation set sampled at the beginning, but does not investigate updating the validation set during the AL process or evaluate the impact of the initial validation set on threshold-margin's performance.
- Why unresolved: The paper does not explore the effects of dynamically updating the validation set on the performance of threshold-margin sampling. It is unclear whether using a static validation set or periodically updating it would lead to better results.
- What evidence would resolve it: Conducting experiments that compare the performance of threshold-margin sampling using different validation set update strategies (e.g., fixed, periodic, or adaptive updates) would provide insights into the optimal approach for maintaining the validation set during the active learning process.

### Open Question 3
- Question: What is the underlying reason for the improved robustness of threshold-margin sampling to different domain compositions compared to margin sampling?
- Basis in paper: [inferred] The paper suggests that margin sampling tends to select instances with similar types of errors, while threshold-margin increases selection diversity. However, it acknowledges that more rigorous ablation studies are needed to understand the exact reason behind threshold-margin's improved robustness.
- Why unresolved: The paper does not provide a detailed analysis of the selection patterns of margin sampling and threshold-margin sampling, nor does it conduct ablation studies to isolate the specific factors contributing to threshold-margin's improved robustness.
- What evidence would resolve it: Performing ablation studies that compare the selection patterns, diversity, and error types of instances chosen by margin sampling and threshold-margin sampling would help identify the key factors driving threshold-margin's improved robustness. Additionally, analyzing the impact of different threshold calculation methods and the relationship between threshold values and model performance could provide further insights.

## Limitations

- The evaluation relies on only two datasets (Domainnet and CLIP-GeoYFCC), which may not capture the full diversity of multi-domain scenarios
- Implementation details of CLIP-GeoYFCC construction are not fully specified, particularly the CLIP-based relabeling process
- The study focuses on image classification with ResNet-18 and margin sampling, leaving open questions about generalization to other architectures, modalities, or query strategies

## Confidence

- High confidence: The observation that single-domain AL strategies underperform random selection in multi-domain settings is well-supported by experimental results across multiple domain compositions.
- Medium confidence: The claim that no single domain allocation strategy dominates across all datasets and metrics is supported but requires more datasets for robust generalization.
- Medium confidence: The effectiveness of threshold-margin sampling is demonstrated but lacks detailed ablation studies to fully validate the mechanism.

## Next Checks

1. **Dataset generalization test**: Evaluate the same multi-domain strategies on additional datasets with different domain characteristics (e.g., medical imaging with scanner types, or natural language with language domains) to assess whether findings hold beyond image classification.

2. **Hyperparameter sensitivity analysis**: Systematically vary key hyperparameters including initial labeled set size, query batch size, and learning rate schedules to determine the robustness of observed improvements across different training configurations.

3. **Query strategy comparison**: Implement and compare alternative instance-level query strategies (entropy sampling, least confidence) within the same domain allocation framework to isolate whether improvements come from domain allocation versus instance selection methods.