---
ver: rpa2
title: Parallel Ranking of Ads and Creatives in Real-Time Advertising Systems
arxiv_id: '2312.12750'
source_url: https://arxiv.org/abs/2312.12750
tags:
- creative
- ranking
- offline
- online
- creatives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to parallel ranking of ads
  and creatives in real-time advertising systems. The core idea is to decouple creative
  ranking from the main ad pipeline, allowing for more sophisticated modeling and
  reducing overall latency.
---

# Parallel Ranking of Ads and Creatives in Real-Time Advertising Systems

## Quick Facts
- arXiv ID: 2312.12750
- Source URL: https://arxiv.org/abs/2312.12750
- Reference count: 13
- Primary result: Parallel architecture reduces response time by 12ms while improving CTR and CPM through joint ad-creative optimization

## Executive Summary
This paper introduces a novel parallel ranking architecture for real-time advertising systems that decouples ad and creative ranking into separate, simultaneous processes. The approach addresses the fundamental challenge of optimizing both ad selection and creative presentation while meeting strict latency requirements. By implementing an offline joint optimization model and introducing the NSCTR metric for creative ranking evaluation, the system achieves significant improvements in both efficiency and effectiveness compared to traditional sequential approaches.

## Method Summary
The method implements a parallel architecture where ad ranking and creative ranking operate simultaneously after the retrieval stage, rather than sequentially. An offline joint optimization model (JAC) is trained using a cascaded approach that combines ad and creative ranking, then splits into two parallel online models. The creative model uses quantized outputs from the ad model as embeddings, enabling mutual awareness between components. The NSCTR metric is introduced for more reliable offline evaluation of creative ranking performance.

## Key Results
- 12ms reduction in response time compared to sequential approaches
- Significant improvements in CTR and CPM metrics across online A/B testing
- NSCTR metric shows better correlation with online performance than traditional metrics like AUC and GAUC

## Why This Works (Mechanism)

### Mechanism 1
Parallel ranking architecture reduces overall latency while enabling more sophisticated modeling for both ads and creatives. By decoupling creative ranking from the main ad pipeline and running them in parallel after the retrieval stage, the system eliminates dependencies and allows each module sufficient time budget for complex models. Core assumption: The parallel execution time for both modules is less than the sum of sequential execution times, and the modules can operate independently without loss of optimization.

### Mechanism 2
The offline joint optimization model (JAC) improves accuracy by enabling mutual awareness and collaborative optimization between ads and creatives. A large cascaded model is trained offline combining ad and creative ranking, then split into two parallel online models. The creative model uses quantized outputs from the ad model as embeddings, allowing backpropagation of gradients to improve both components. Core assumption: The joint training captures interactions between ads and creatives that benefit both ranking tasks.

### Mechanism 3
The NSCTR metric provides more reliable offline evaluation of creative ranking performance compared to traditional metrics like AUC and GAUC. NSCTR normalizes the sCTR calculation using the ad distribution in the sample, addressing the sample distribution bias present in sCTR that causes metric instability. Core assumption: NSCTR better correlates with online A/B test results because it accounts for the actual ad distribution and provides a more accurate approximation of real-world performance.

## Foundational Learning

- Concept: Click-Through Rate (CTR) prediction in online advertising
  - Why needed here: The entire system architecture and joint optimization framework are designed to improve CTR prediction accuracy for both ads and creatives.
  - Quick check question: How does CTR differ from other engagement metrics like conversion rate or view-through rate, and why is it the primary focus for this advertising system?

- Concept: Multi-stage cascade ranking systems in recommendation and advertising
  - Why needed here: Understanding the traditional two-stage architecture (retrieval + ranking) is crucial for appreciating why the parallel architecture is novel and how it differs from existing approaches.
  - Quick check question: What are the typical latency constraints in real-time advertising systems, and how do these constraints influence model complexity and architecture choices?

- Concept: Embedding techniques and quantization in deep learning
  - Why needed here: The joint optimization model uses quantized embeddings of ad CTR predictions as inputs to the creative model, and understanding this technique is essential for implementing and troubleshooting the system.
  - Quick check question: How does straight-through gradient estimation work for non-differentiable operations like quantization, and what are the trade-offs between quantization levels and model performance?

## Architecture Onboarding

- Component map: Retrieval stage → Parallel Ad Ranking (AR) and Creative Ranking (CR) → Final selection

- Critical path:
  1. User request → Retrieval stage
  2. Retrieved candidates → Parallel Ad and Creative Ranking
  3. AR outputs pctrad for each ad
  4. CR outputs pctrc for each ad-creative pair
  5. Select top L ads based on pctrad
  6. Select top creative per ad based on pctrc
  7. Display results to user

- Design tradeoffs:
  - Parallel vs sequential: Parallel reduces overall latency but requires careful resource allocation and may limit information flow between modules
  - Feature richness: AR uses rich features for precision, CR uses fewer features for speed, creating an imbalance that the joint optimization framework addresses
  - Model complexity: More complex models improve accuracy but increase latency and computational costs

- Failure signatures:
  - High latency: Indicates resource contention or inefficient parallel execution
  - Low CTR: May indicate poor feature selection, inadequate model capacity, or ineffective joint optimization
  - Unstable performance: Could result from quantization errors, gradient propagation issues, or metric calculation problems

- First 3 experiments:
  1. Measure parallel execution time vs sequential execution time under varying load conditions to validate latency improvements
  2. Compare CTR performance of parallel architecture against traditional sequential approaches using A/B testing with identical feature sets
  3. Evaluate the impact of different quantization levels on CR model performance and identify the optimal trade-off between accuracy and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal architecture for joint ad and creative ranking that maximizes both effectiveness (CTR, CPM) and efficiency (latency) in real-time advertising systems?
- Basis in paper: The paper introduces Peri-CR architecture and discusses trade-offs between different architectures (Pre-CR, Post-CR, Peri-CR) and their impact on performance.
- Why unresolved: While the paper demonstrates the superiority of Peri-CR over other architectures, the optimal architecture may depend on specific system constraints and requirements.
- What evidence would resolve it: A comprehensive evaluation of various architectures under different system constraints and performance metrics would help identify the optimal architecture for different use cases.

### Open Question 2
- Question: How can offline evaluation metrics for creative ranking be further improved to better correlate with online performance and reduce the need for expensive A/B testing?
- Basis in paper: The paper proposes NSCTR as an improved offline metric for creative ranking, demonstrating better correlation with online CTR compared to existing metrics.
- Why unresolved: While NSCTR shows promise, there is still room for improvement in offline evaluation metrics to more accurately reflect online performance.
- What evidence would resolve it: Comparing the correlation of various offline metrics with online performance across different creative ranking tasks and datasets would help identify the most effective metrics.

### Open Question 3
- Question: How can the integration of AIGC and online creative ranking be leveraged to generate optimal creatives in real-time tailored to each user?
- Basis in paper: The paper mentions that future systems could integrate offline creative production and online estimation, generating optimal creatives in real-time tailored to each user.
- Why unresolved: While the paper suggests the potential of integrating AIGC with online creative ranking, it does not provide specific details on how to achieve this integration or the challenges involved.
- What evidence would resolve it: Developing and evaluating a system that combines AIGC with online creative ranking would provide insights into the feasibility, performance, and challenges of this integration.

## Limitations
- Reliance on proprietary data from Taobao without public dataset availability
- Limited discussion of failure modes and edge cases in the parallel architecture
- Insufficient ablation studies on the impact of individual design choices

## Confidence
- High confidence: Core architectural innovation (parallel ranking) and latency improvements (12ms reduction)
- Medium confidence: NSCTR metric's superiority with correlation evidence but limited cross-dataset validation
- Low confidence: Exact implementation details of quantization and embedding techniques not fully specified

## Next Checks
1. Conduct a controlled ablation study removing the joint optimization component to quantify its specific contribution to performance gains
2. Test the NSCTR metric on an independent advertising dataset to verify its generalizability and correlation with online performance
3. Implement the parallel architecture in a smaller-scale open-source advertising system to validate reproducibility and identify potential deployment challenges