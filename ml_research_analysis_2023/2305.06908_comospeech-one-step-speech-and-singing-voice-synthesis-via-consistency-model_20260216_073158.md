---
ver: rpa2
title: 'CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model'
arxiv_id: '2305.06908'
source_url: https://arxiv.org/abs/2305.06908
tags:
- comospeech
- synthesis
- arxiv
- consistency
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CoMoSpeech, a one-step consistency model for
  high-quality speech and singing voice synthesis. The method distills a multi-step
  diffusion-based teacher model into a single-step consistency model using a consistency
  constraint, achieving both fast inference speed and high audio quality.
---

# CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model

## Quick Facts
- arXiv ID: 2305.06908
- Source URL: https://arxiv.org/abs/2305.06908
- Reference count: 9
- Key outcome: CoMoSpeech achieves speech and singing synthesis with a single sampling step, more than 150× faster than real-time on NVIDIA A100 GPU, with MOS of 4.239 for TTS and 3.794 for SVS.

## Executive Summary
CoMoSpeech introduces a novel one-step consistency model for high-quality speech and singing voice synthesis. The method distills a multi-step diffusion-based teacher model into a single-step consistency model using a consistency constraint, achieving both fast inference speed and high audio quality. The approach demonstrates that diffusion models can be effectively compressed into consistency models without sacrificing quality, enabling real-time synthesis applications. Experiments on both text-to-speech and singing voice synthesis show that CoMoSpeech generates audio with a single sampling step while maintaining quality comparable to multi-step diffusion baselines.

## Method Summary
CoMoSpeech consists of a diffusion-based teacher model and a consistency model student. The teacher model learns a denoising trajectory in latent space using a score-based diffusion approach. The consistency constraint enforces that the student model's denoising output at any time step maps to the same final data distribution as the teacher's trajectory, enabling single-step synthesis. The architecture processes conditional inputs (phonemes for TTS, phonemes plus music scores for SVS) through an encoder, duration predictor, length regulator, and prior predictor before the denoiser. Different network architectures are used: WaveNet for SVS to capture long-range dependencies and U-Net for TTS for efficiency. The consistency distillation loss trains the student to satisfy the consistency property across different time steps.

## Key Results
- CoMoSpeech achieves real-time factor (RTF) of 0.0058 for TTS and 0.0048 for SVS, more than 150× faster than real-time
- Mean Opinion Scores (MOS) of 4.239 for TTS and 3.794 for SVS, comparable to or better than multi-step diffusion baselines
- Outperforms FastSpeech2 in quality while maintaining similar inference speed
- Single-step sampling eliminates the need for iterative denoising while preserving audio quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency distillation enables single-step synthesis without sacrificing audio quality.
- Mechanism: The teacher diffusion model learns a denoising trajectory in latent space. The consistency constraint enforces that the student model's denoising output at any time step maps to the same final data distribution as the teacher's trajectory, allowing single-step generation.
- Core assumption: The consistency property holds such that denoising at any intermediate time step maps to the same underlying data distribution as initial noise.
- Evidence anchors:
  - [abstract]: "The consistency constraint is applied to distill a consistency model from a well-designed diffusion-based teacher model"
  - [section 2]: "As long as the score function ∇ logpt(x) is known, the probability flow ODE in (3) can be used for sampling"
  - [corpus]: Weak evidence - only neighboring papers cite similar consistency distillation approaches
- Break condition: If the consistency property is not satisfied, single-step sampling will not converge to correct data distribution.

### Mechanism 2
- Claim: Conditional inputs (phonemes, music scores) guide generation of both speech and singing voices.
- Mechanism: Encoder processes conditional inputs into hidden representations, which are length-regulated and used to predict prior mel-spectrogram that acts as conditioning for denoising process.
- Core assumption: Conditional inputs contain sufficient information to guide high-quality mel-spectrogram generation.
- Evidence anchors:
  - [section 3.3]: "We adopt the phoneme as the basic input for TTS and SVS. Then, a simple lookup table is used for embedding the phoneme feature. Additionally, for the SVS task, we add a music score that specifies the note levels time-aligned to the phonemes."
  - [section 3.3]: "A well-designed speech synthesizer is expected to perform well not only on reading speech synthesis (TTS) but also on other more complicated tasks, such as SVS which additionally produces highly dynamic melodies."
  - [corpus]: Moderate evidence - neighboring papers use similar conditional input structures for singing voice synthesis.
- Break condition: If conditional inputs are insufficient or noisy, generated audio will lack desired prosody or melody.

### Mechanism 3
- Claim: Different network architectures (WaveNet for SVS, U-Net for TTS) are optimized for specific task characteristics.
- Mechanism: WaveNet's autoregressive nature captures long-range dependencies and temporal structure in singing voices, while U-Net's parallel structure is efficient for more predictable speech structure.
- Core assumption: Architectural choice impacts quality of denoised mel-spectrogram.
- Evidence anchors:
  - [section 3.3]: "For the neural network and conditional inputs in the denoiser, we investigated different combinations and finally selected a) the WaveNet architecture[Oord et al., 2016] and hiddenmel as featurecond in (13) for SVS and b) U-Net architecture [Ronneberger et al., 2015] and μmel as cond for TTS."
  - [section 4.4]: "In addition, we can observe that our CoMoSpeech-SVS is faster than CoMoSpeech because the denoiser function in SVS follows the WaveNet architecture which is faster than U-Net architecture in TTS."
  - [corpus]: Moderate evidence - neighboring papers also use task-specific architectures for speech and singing synthesis.
- Break condition: If wrong architecture is chosen, model may not capture nuances of target voice type.

## Foundational Learning

- Concept: Denoising diffusion probabilistic models (DDPMs)
  - Why needed here: The teacher model is a diffusion model that learns to denoise mel-spectrograms.
  - Quick check question: What is the forward process in a DDPM, and how does it relate to the reverse process?

- Concept: Consistency models
  - Why needed here: The student model is a consistency model distilled from the teacher diffusion model.
  - Quick check question: What is the consistency property, and how does it enable single-step sampling?

- Concept: Ordinary differential equations (ODEs) and numerical solvers
  - Why needed here: The diffusion process is modeled as an ODE, and numerical solvers (Euler, Heun) are used for sampling.
  - Quick check question: How does the choice of ODE solver affect the quality and speed of sampling?

## Architecture Onboarding

- Component map: Encoder → Duration Predictor → Length Regulator → Prior Predictor → Denoiser (Teacher) → Consistency Distillation → CoMoSpeech (Student) → Audio Output

- Critical path: The flow from conditional inputs through the encoder, duration prediction, length regulation, prior prediction, denoising, and consistency distillation to produce the final audio output.

- Design tradeoffs:
  - Teacher model complexity vs. distillation quality: More complex teacher models may yield better distillation but are slower to train
  - Network architecture choice: WaveNet is slower but captures long-range dependencies better, while U-Net is faster but may miss some nuances
  - Sampling steps: More steps in teacher model may improve quality but increase distillation time

- Failure signatures:
  - Audio quality issues: May indicate problems with denoiser architecture, conditional inputs, or consistency distillation
  - Slow inference speed: Could be due to inefficient denoiser architecture or suboptimal implementation
  - Inconsistent audio: Might suggest the consistency property is not well satisfied

- First 3 experiments:
  1. Train teacher model with different network architectures (WaveNet vs. U-Net) and compare audio quality and inference speed
  2. Vary number of sampling steps in teacher model and observe impact on distillation quality and consistency property satisfaction
  3. Test CoMoSpeech with different conditional input configurations (phonemes only vs. phonemes + music scores) and evaluate impact on singing voice synthesis quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CoMoSpeech be trained directly without distillation from a teacher model while maintaining similar performance?
- Basis in paper: [explicit] The paper states "Since our CoMoSpeech needs to distill from a teacher model for better performance, this makes the pipeline of constructing a speech synthesis system more complicated. Therefore, how to directly train the CoMoSpeech without distillation of the teacher model is our next step to investigate."
- Why unresolved: The paper explicitly identifies this as a limitation and future research direction, indicating that direct training methods for consistency models in speech synthesis have not yet been successfully developed.
- What evidence would resolve it: Successful training of CoMoSpeech directly without teacher model distillation that achieves comparable MOS, RTF, and FD metrics to the current approach would resolve this question.

### Open Question 2
- Question: What causes the performance degradation when increasing sampling steps beyond 10 in CoMoSpeech?
- Basis in paper: [explicit] The paper mentions "the model performance improves in a few sampling steps and then declines slightly as the number of steps increases is called the sampling drift challenge" and notes this occurs after 10 steps for both TTS and SVS.
- Why unresolved: The paper identifies this as an observed phenomenon but does not explain the underlying cause of why performance degrades with more sampling steps despite the theoretical expectation that more steps should improve quality.
- What evidence would resolve it: Analysis showing whether this is due to accumulated numerical errors, vanishing gradients, or other mechanisms during the sampling process, along with methods to mitigate this degradation, would resolve this question.

### Open Question 3
- Question: Can a more efficient denoiser function be designed that runs faster than FastSpeech2's decoder while maintaining or improving quality?
- Basis in paper: [explicit] The paper states "if a more efficient denoiser function that runs faster than the decoder in FastSpeech2 can be designed, we can make CoMoSpeech even faster than non-iterative methods in future work."
- Why unresolved: The current U-Net architecture in TTS and WaveNet in SVS have achieved good quality but the paper suggests there is potential for even faster inference through better architecture design.
- What evidence would resolve it: Development and demonstration of a denoiser architecture that achieves better or comparable MOS/FD scores with lower RTF than FastSpeech2's decoder would resolve this question.

## Limitations

- The paper lacks detailed ablation studies on the consistency constraint itself, making it difficult to quantify its exact contribution to performance gains.
- Architectural details of the WaveNet and U-Net denoiser networks are not fully specified, creating uncertainty in exact reproducibility.
- Generalization to other domains beyond speech and singing synthesis remains untested, suggesting potential limitations in broader applicability.

## Confidence

- **High**: The claim that CoMoSpeech achieves comparable quality to multi-step diffusion models while being significantly faster is well-supported by experimental results.
- **Medium**: The architectural details of WaveNet and U-Net denoiser networks are not fully specified, creating moderate uncertainty in exact reproducibility.
- **Medium**: Generalization to other domains beyond speech and singing synthesis remains untested.

## Next Checks

1. Conduct an ablation study removing the consistency constraint to quantify its contribution to both quality and speed improvements.

2. Test CoMoSpeech on additional voice synthesis tasks (e.g., emotional speech synthesis or cross-lingual synthesis) to evaluate generalizability.

3. Perform a detailed analysis of the learned denoising trajectories in both the teacher and student models to verify that the consistency property is actually being satisfied.