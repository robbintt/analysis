---
ver: rpa2
title: 'The Archive Query Log: Mining Millions of Search Result Pages of Hundreds
  of Search Engines from 25 Years of Web Archives'
arxiv_id: '2304.00413'
source_url: https://arxiv.org/abs/2304.00413
tags:
- search
- query
- https
- queries
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Archive Query Log (AQL) is a publicly available query log containing
  356 million queries, 166 million search result pages, and 1.7 billion search results
  across 550 search providers. It was mined from the Internet Archive's Wayback Machine
  over 25 years.
---

# The Archive Query Log: Mining Millions of Search Result Pages of Hundreds of Search Engines from 25 Years of Web Archives

## Quick Facts
- arXiv ID: 2304.00413
- Source URL: https://arxiv.org/abs/2304.00413
- Reference count: 40
- One-line primary result: The Archive Query Log (AQL) is the largest and most diverse public query log, containing 356 million queries, 166 million search result pages, and 1.7 billion search results from 550 search providers spanning 25 years.

## Executive Summary
The Archive Query Log (AQL) is a groundbreaking dataset mined from the Internet Archive's Wayback Machine, providing unprecedented access to historical search engine data. By extracting queries and search result pages from 25 years of archived web content, the AQL enables large-scale diachronic analysis of search behavior and retrieval models. The dataset covers 550 search providers across 104 languages, making it the most comprehensive public query log available. Available through the TIRA platform, the AQL promotes transparency in the search industry and supports research into algorithmic biases, fairness, and the evolution of search technology.

## Method Summary
The AQL was created by mining the Internet Archive's Wayback Machine using the CDX API to collect URL captures from search providers. Provider-specific parsers were developed to extract queries from SERP URLs and download the corresponding HTML content. The HTML was then parsed to extract search results, which were stored in structured JSON format. The entire process was designed to handle the scale and diversity of 550 search providers while ensuring privacy by excluding user and click data. The resulting corpus is accessible via the TIRA platform for secure analysis.

## Key Results
- The AQL contains 356 million queries, 166 million search result pages, and 1.7 billion search results.
- The dataset spans 25 years and covers 550 search providers in 104 languages.
- The AQL is the largest and most diverse public query log, enabling new research on retrieval models and search engine transparency.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AQL enables large-scale diachronic analysis of search engines because it contains 166 million search result pages spanning 25 years.
- Mechanism: By archiving SERP URLs over time, researchers can reconstruct historical search rankings and compare changes in retrieval models.
- Core assumption: Archived SERPs accurately reflect the original search results as seen by users.
- Evidence anchors:
  - [abstract] states "166 million search result pages" and "25 years" coverage.
  - [section] explains that SERPs are downloaded from archived snapshots in the Internet Archive.
- Break condition: If archived HTML content is incomplete or dynamic content cannot be captured, the historical reconstruction will be inaccurate.

### Mechanism 2
- Claim: The AQL supports development of new retrieval models despite lacking explicit click data.
- Mechanism: Ranked results from SERPs encode the search providers' retrieval model decisions and implicit relevance feedback from their query logs.
- Core assumption: The ranking order in SERPs reflects the search engine's learned relevance signals.
- Evidence anchors:
  - [abstract] notes "ranked results of third-party retrieval models encode the domain expertise and the implicit relevance feedback from query logs."
  - [section] explains that SERPs include full ranking of results for each query.
- Break condition: If search engines heavily personalize results or if rankings change rapidly, the encoded relevance signals may not be generalizable.

### Mechanism 3
- Claim: The AQL promotes transparency and accountability in search by providing independent access to search results data.
- Mechanism: Researchers can analyze search result pages to investigate algorithmic biases, fairness, and changes over time without relying on provider disclosures.
- Core assumption: Search result pages are a reliable representation of a search engine's behavior.
- Evidence anchors:
  - [abstract] states the AQL "promotes open research as well as more transparency and accountability in the search industry."
  - [section] notes that "archived search result pages are perhaps one of the best representations of a search engine's behavior."
- Break condition: If search providers significantly alter their result presentation or if archived snapshots are incomplete, the analysis may be misleading.

## Foundational Learning

- Concept: Web archiving and the Wayback Machine
  - Why needed here: The AQL relies on archived web pages to extract queries and SERPs.
  - Quick check question: How does the Wayback Machine collect and store web page snapshots?

- Concept: Search engine result page structure
  - Why needed here: Parsing SERPs requires understanding HTML structure and CSS selectors.
  - Quick check question: What are common HTML elements used to structure search results on major search engines?

- Concept: Query log analysis and user behavior
  - Why needed here: Understanding query logs helps interpret the AQL data and its potential applications.
  - Quick check question: What are typical patterns in user query behavior that can be analyzed in query logs?

## Architecture Onboarding

- Component map: Search provider collection -> URL prefix generation -> Query extraction -> SERP acquisition -> SERP parsing -> Corpus creation -> TIRA platform for secure analysis access -> Kubernetes cluster for computational resources

- Critical path: Search provider collection -> URL prefix generation -> Query extraction -> Corpus creation

- Design tradeoffs:
  - Privacy vs. data richness: Excluding user and click data to protect privacy limits some analysis capabilities.
  - Scale vs. parsing complexity: Handling 550 search providers requires scalable parsing infrastructure.
  - Historical depth vs. data quality: Older archived pages may be incomplete or formatted differently.

- Failure signatures:
  - Missing or incomplete SERPs indicate issues with archiving or download processes.
  - Parser failures suggest changes in search engine HTML structure.
  - Low overlap with existing benchmarks may indicate sampling biases.

- First 3 experiments:
  1. Verify query extraction accuracy by sampling and manual inspection of parsed queries.
  2. Test SERP parsing on a subset of providers to ensure correct extraction of titles, URLs, and snippets.
  3. Analyze query length and language distributions to confirm dataset diversity.

## Open Questions the Paper Calls Out

- The paper does not explicitly call out open questions, but raises several implicit ones:
  - How does the Archive Query Log compare to query logs from major search engines like Google and Baidu in terms of representativeness of real-world search behavior?
  - What are the potential biases introduced by the Archive Query Log's reliance on archived SERP URLs and how do they impact the validity of research findings?
  - How can the Archive Query Log be used to investigate and address issues of fairness and bias in search engine results over time?

## Limitations
- The AQL lacks user and click data, limiting analysis of user behavior and interaction patterns.
- Parser limitations and changes in search engine HTML structure may result in incomplete or inaccurate data extraction.
- The reliance on archived content may introduce biases due to incompleteness or changes in web archiving practices over time.

## Confidence
- **High confidence** in claims about dataset scale and diversity, as the abstract and technical sections provide concrete numbers and methodological details.
- **Medium confidence** in claims about historical reconstruction accuracy, due to potential incompleteness in archived content.
- **Low confidence** in claims about promoting transparency and accountability, as the paper doesn't provide evidence of actual bias detection or accountability outcomes from using the AQL.

## Next Checks
1. Conduct a systematic sampling study comparing AQL queries with other public query logs to assess coverage and representativeness across languages and domains.
2. Test the historical reconstruction capability by comparing AQL SERPs from specific dates with contemporaneous snapshots from other sources or provider archives.
3. Evaluate parser robustness by analyzing failure rates across different providers and time periods, and assessing the impact of HTML structure changes on data quality.