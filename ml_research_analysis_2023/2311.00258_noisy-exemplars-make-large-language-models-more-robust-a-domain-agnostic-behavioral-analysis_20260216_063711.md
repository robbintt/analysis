---
ver: rpa2
title: 'Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic
  Behavioral Analysis'
arxiv_id: '2311.00258'
source_url: https://arxiv.org/abs/2311.00258
tags:
- eggs
- prompting
- every
- janet
- farmers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the robustness of large language models
  (LLMs) to domain-agnostic perturbations when using few-shot prompting techniques
  for multi-hop reasoning tasks. The authors introduce four perturbation types: typos,
  synonym replacements, repetitions, and shortcuts (adding intermediate reasoning
  steps).'
---

# Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis

## Quick Facts
- arXiv ID: 2311.00258
- Source URL: https://arxiv.org/abs/2311.00258
- Reference count: 18
- Key outcome: Increasing proportion of perturbed exemplars in few-shot prompts improves robustness to test-time perturbations for multi-hop reasoning tasks, except for typos.

## Executive Summary
This paper investigates how large language models (LLMs) respond to domain-agnostic perturbations when using few-shot prompting techniques for multi-hop reasoning tasks. The authors introduce four perturbation types (typos, synonym replacements, repetitions, and shortcuts) and test chain-of-thought (COT), zero-shot (0COT), and least-to-most prompting (LTM) methods on GSM8K and StrategyQA datasets. They find that models are most sensitive to synonym replacements and that including perturbed exemplars during few-shot learning can significantly improve robustness to test-time perturbations. The study provides insights into how LLMs handle noisy inputs and suggests practical approaches for improving model robustness in real-world applications.

## Method Summary
The authors conducted experiments using GPT-3.5-Turbo, LLaMA2-7B, and LLaMA2-13B models on GSM8K and StrategyQA datasets. They implemented four perturbation types: typos, synonym replacements, repetitions, and shortcuts (adding intermediate reasoning steps). The experiments tested three prompting methods (COT, 0COT, and LTM) under various perturbation scenarios, systematically varying the proportion of perturbed exemplars in the prompts. Model performance was evaluated by measuring accuracy on perturbed test questions across different perturbation rates in the exemplars.

## Key Results
- Models show highest sensitivity to synonym replacements, with performance degrading significantly under this perturbation type.
- Increasing the proportion of perturbed exemplars in few-shot prompts improves robustness to test-time perturbations for most perturbation types.
- Few-shot prompting methods demonstrate better robustness to perturbations compared to zero-shot prompting when exemplars are unperturbed.
- Typos are an exception where increasing perturbed exemplars does not improve robustness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing the proportion of perturbed exemplars in few-shot prompts improves robustness to test-time perturbations.
- Mechanism: Exposing the model to perturbed examples during few-shot learning allows it to adapt to similar perturbations when encountered at test time.
- Core assumption: The model can generalize from perturbed exemplars to handle similar perturbations in new examples.
- Evidence anchors:
  - [abstract] "increasing the proportion of perturbed exemplars in the prompts improves robustness of few-shot prompting methods."
  - [section 5.2] "Increasing the proportion of perturbed exemplars improves few-shot prompting performance, except for the Typo perturbation."
  - [corpus] Weak evidence; related work focuses on robustness testing but not specifically on perturbed exemplars improving robustness.
- Break condition: If the model fails to generalize from perturbed exemplars to test-time perturbations, or if the perturbations are too diverse for effective generalization.

### Mechanism 2
- Claim: Models are more sensitive to certain perturbations, such as synonym replacements.
- Mechanism: Synonym replacements create low-frequency phrases that the model is less familiar with, leading to worse performance.
- Core assumption: Models rely on lexical patterns and are less robust to semantic perturbations that change word choice but not meaning.
- Evidence anchors:
  - [abstract] "models are more sensitive to certain perturbations such as replacing words with their synonyms."
  - [section 5.1] "prompting methods are most susceptible to Synonym replacement."
  - [corpus] No direct evidence; related work focuses on robustness but not specifically on synonym sensitivity.
- Break condition: If the model can recognize semantically similar tokens regardless of lexical differences, or if synonym replacement doesn't significantly alter the input.

### Mechanism 3
- Claim: Few-shot prompting methods are more robust to perturbations than zero-shot prompting when exemplars are unperturbed.
- Mechanism: Few-shot learning provides the model with examples of how to reason through problems, making it less sensitive to minor input changes.
- Core assumption: Few-shot exemplars guide the model's reasoning process, reducing sensitivity to input perturbations.
- Evidence anchors:
  - [section 5.1] Comparison of prompting methods shows varying sensitivity to perturbations, with few-shot methods generally performing better.
  - [corpus] Weak evidence; related work focuses on few-shot prompting but not specifically on robustness to perturbations.
- Break condition: If few-shot exemplars don't effectively guide reasoning, or if perturbations significantly alter the exemplars themselves.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Understanding how chain-of-thought prompting guides the model's reasoning process is crucial for analyzing its robustness to perturbations.
  - Quick check question: What is the key idea behind chain-of-thought prompting, and how does it differ from standard prompting?

- Concept: Few-shot learning
  - Why needed here: Few-shot learning is the technique used to provide exemplars to the model, and understanding its mechanics is essential for analyzing how perturbed exemplars affect robustness.
  - Quick check question: How does few-shot learning differ from traditional supervised learning, and what are its advantages?

- Concept: Domain-agnostic perturbations
  - Why needed here: The study introduces various perturbations to test model robustness, and understanding what constitutes a domain-agnostic perturbation is crucial for interpreting the results.
  - Quick check question: What makes a perturbation domain-agnostic, and why is this property important for testing model robustness?

## Architecture Onboarding

- Component map: LLM -> Prompting method (COT/0COT/LTM) -> Exemplars (possibly perturbed) -> Test question (possibly perturbed) -> Model output
- Critical path: Model's ability to reason through test question using provided exemplars and prompting method despite perturbations
- Design tradeoffs: Perturbing exemplars can improve robustness but may also introduce noise that hinders learning. The choice of perturbation type and frequency must balance these effects.
- Failure signatures: Significant performance degradation under certain perturbations indicates sensitivity to those perturbations. If perturbed exemplars don't improve robustness, it suggests the model isn't effectively generalizing from the exemplars.
- First 3 experiments:
  1. Test model sensitivity to individual perturbation types (e.g., typos, synonyms, repetitions) to identify which perturbations have the most impact.
  2. Vary the proportion of perturbed exemplars in few-shot prompts to find the optimal balance between improving robustness and introducing noise.
  3. Compare the robustness of different prompting methods (e.g., chain-of-thought, zero-shot, least-to-most) to identify which is most effective under various perturbations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model size influence the robustness of LLMs to domain-agnostic perturbations in few-shot prompting?
- Basis in paper: [inferred] The paper mentions that future work could explore the influence of model size on robustness but does not investigate this directly.
- Why unresolved: The experiments were limited to GPT-3.5-Turbo, LLaMA2 7B, and LLaMA2 13B models, leaving the impact of larger or smaller models unexplored.
- What evidence would resolve it: Conducting experiments with a wider range of model sizes (e.g., GPT-4, smaller LLaMA models) and analyzing their performance under the same perturbation tests would clarify the relationship between model size and robustness.

### Open Question 2
- Question: How do domain-agnostic perturbations affect the consistency of reasoning steps in chain-of-thought (COT) prompting?
- Basis in paper: [explicit] The paper references prior work showing COT may lead to inconsistent reasoning steps under certain circumstances but does not analyze this in the context of perturbations.
- Why unresolved: The experiments focus on final answer accuracy rather than the quality or consistency of intermediate reasoning steps, especially when perturbations are introduced.
- What evidence would resolve it: Analyzing the intermediate reasoning steps generated by LLMs under perturbations and comparing their logical consistency with unperturbed examples would reveal how perturbations impact reasoning quality.

### Open Question 3
- Question: Can perturbing in-context exemplars improve robustness to combinations of multiple perturbation types simultaneously?
- Basis in paper: [inferred] The paper tests single perturbation types but does not explore their combined effects or whether perturbed exemplars generalize to multi-perturbation scenarios.
- Why unresolved: Real-world applications often involve multiple types of noise simultaneously, and it's unclear if the benefits of perturbed exemplars extend to such complex scenarios.
- What evidence would resolve it: Designing experiments that introduce multiple perturbation types in both exemplars and test questions, then measuring robustness, would determine if perturbed exemplars provide broad protection against complex noise.

## Limitations

- The robustness gains from perturbed exemplars appear to be perturbation-specific rather than generalizable across all perturbation types, with typos being an explicit exception.
- The study focuses on relatively simple perturbations and may not capture more complex real-world noise patterns encountered in practical applications.
- Experiments are limited to two datasets (GSM8K and StrategyQA) and three model sizes, which may not generalize to broader domains or larger model architectures.

## Confidence

**High confidence** in the observation that models show varying sensitivity to different perturbation types, particularly synonym replacements.

**Medium confidence** in the proposed mechanism that perturbed exemplars improve robustness through pattern recognition, though alternative explanations are not fully explored.

**Low confidence** in the generalizability of findings to real-world deployment scenarios, as the controlled perturbation setting may not reflect actual noisy inputs.

## Next Checks

1. **Perturbation Generalization Test**: Design a follow-up experiment where models trained with perturbed exemplars are tested on entirely new perturbation types not seen during training, to assess whether the robustness gains transfer beyond the specific perturbation patterns used.

2. **Mechanism Isolation**: Conduct ablation studies where different components of the prompting process (reasoning steps, exemplar formatting, etc.) are systematically removed or modified to isolate which aspects of the chain-of-thought approach contribute most to perturbation robustness.

3. **Real-world Noise Benchmark**: Test the proposed approach on datasets with naturally occurring noise (OCR errors, speech recognition outputs, user-generated content) rather than synthetic perturbations, to evaluate practical utility beyond controlled experimental conditions.