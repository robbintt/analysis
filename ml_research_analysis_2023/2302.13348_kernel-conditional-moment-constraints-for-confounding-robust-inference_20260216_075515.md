---
ver: rpa2
title: Kernel Conditional Moment Constraints for Confounding Robust Inference
arxiv_id: '2302.13348'
source_url: https://arxiv.org/abs/2302.13348
tags:
- kcmc
- constraints
- policy
- pobs
- conditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a general framework for sharp policy evaluation
  under unobserved confounding in offline contextual bandits. The key idea is to use
  kernel methods to approximate conditional moment constraints, enabling more accurate
  estimation of worst-case policy values over uncertainty sets.
---

# Kernel Conditional Moment Constraints for Confounding Robust Inference

## Quick Facts
- arXiv ID: 2302.13348
- Source URL: https://arxiv.org/abs/2302.13348
- Reference count: 40
- One-line primary result: Kernel conditional moment constraints consistently outperform conventional non-sharp estimators in terms of tighter bounds and more accurate policy value estimation under unobserved confounding.

## Executive Summary
This paper introduces kernel conditional moment constraints (KCMC) as a general framework for sharp policy evaluation in offline contextual bandits with unobserved confounding. The key innovation is leveraging kernel methods to approximate conditional moment constraints, enabling tractable enforcement of distributional assumptions that traditional estimators cannot handle. The method extends existing sensitivity analysis models through a novel f-divergence formulation and provides consistency guarantees for both policy evaluation and learning.

## Method Summary
The KCMC framework approximates conditional moment constraints using kernel ridge regression, where the reparametrized weight errors are estimated in a reproducing kernel Hilbert space. The uncertainty set is defined by f-divergence constraints between the base policy and observed data, combined with the kernel-approximated conditional moments. This dual formulation enables solving a convex program to obtain sharp lower bounds on policy values. The method generalizes marginal sensitivity models and includes extensions to policy learning through M-estimation theory.

## Key Results
- KCMC estimators produce significantly tighter bounds than conventional non-sharp estimators (ZSB) across synthetic and real-world experiments
- The f-divergence extension provides flexible sensitivity modeling with continuous control over confounding strength
- Consistency guarantees are established for both policy evaluation and learning through reduction to M-estimation
- Empirical results on NLS data demonstrate practical effectiveness for average treatment effect estimation

## Why This Works (Mechanism)

### Mechanism 1
The kernel method enables tractable approximation of infinite-dimensional conditional moment constraints, which traditional estimators cannot enforce. By using kernel ridge regression to estimate the conditional expectation of reparametrized weight errors, the method imposes constraints that are close to zero, interpreted via Gaussian process credible sets. This allows the uncertainty set to be much tighter than the ZSB relaxation. The core assumption is that kernel conditional moment constraints sufficiently capture true distributional constraints so that the infimum over the KCMC uncertainty set equals the infimum over full conditional moment constraints.

### Mechanism 2
The f-divergence constraint extension provides a flexible uncertainty set that generalizes marginal sensitivity and enables continuous control over confounding strength. By encoding proximity of πbase(t|x,u) to pobs(t|x) using f-divergence instead of uniform bounds, the model captures local deviations from unconfoundedness more naturally, allowing for a range of sensitivity parameters. The core assumption is that the f-divergence constraint is a valid relaxation of true distributional constraints and is compatible with conditional moment constraints.

### Mechanism 3
The consistency guarantees for policy evaluation and learning are established via reduction to M-estimation using the dual formulation. By defining a loss function ℓθ for the dual problem and showing it satisfies regularity conditions (continuity, convexity, integrability), the convergence of the empirical dual solution to the population solution is guaranteed, which implies consistency of the policy value estimator. The core assumption is that the dual problem has a unique solution and the loss function satisfies conditions of Lemma 3 (Van de Geer, 2000) for M-estimation.

## Foundational Learning

- **Conditional moment restrictions and their relaxation**: Understanding how these are derived and relaxed is key to grasping the method. Quick check: What is the difference between ZSB constraints and conditional moment constraints in terms of the distributions they enforce?

- **Reproducing kernel Hilbert spaces (RKHS) and kernel ridge regression**: Essential to understand the approximation. Quick check: How does kernel ridge regression estimate the conditional expectation in the context of the conditional moment constraints?

- **f-divergence and its role in sensitivity analysis**: Necessary to appreciate the extension. Quick check: How does the f-divergence constraint differ from the box constraints in the marginal sensitivity model?

## Architecture Onboarding

- **Component map**: Data (Y,T,X) -> Base policy estimator -> Kernel conditional moment constraints -> Uncertainty set (f-divergence + KCMC) -> Convex optimization -> Policy value bound

- **Critical path**: 1) Estimate pobs(t|x) from data, 2) Choose kernel and orthogonal function class, 3) Construct kernel conditional moment constraints, 4) Define uncertainty set with f-divergence and KCMC, 5) Solve convex program to get policy value bound, 6) For policy learning, optimize over policy class using the bound as objective

- **Design tradeoffs**: Sharpness vs. tractability (KCMC is sharper than ZSB but requires more complex convex programs), kernel choice affects bound tightness, low-rank approximations speed computation but may reduce sharpness

- **Failure signatures**: Overly conservative bounds (kernel constraints too loose or f-divergence not capturing true confounding), infeasible optimization (constraints too tight or poor kernel choice), inconsistent estimates (violations of M-estimation regularity conditions)

- **First 3 experiments**: 1) Synthetic data with known confounding structure comparing KCMC to ground truth and ZSB bounds, 2) Real-world NLS data estimating average treatment effect and comparing bounds, 3) Policy learning optimizing a policy using KCMC bounds and evaluating on test data

## Open Questions the Paper Calls Out

### Open Question 1
Under what specific conditions does the conditional moment constraints relaxation yield a tight bound (Vinf = VCMC) that matches the true lower bound, especially for non-binary action spaces and general box/f-divergence constraints? The paper provides a counterexample showing strict inclusion can occur for certain box constraints but doesn't characterize when the bound is tight across all constraint classes and policy evaluation scenarios.

### Open Question 2
How sensitive are the kernel conditional moment constraints (KCMC) estimators to the choice of kernel parameters (e.g., bandwidth, kernel type) and the number of constraints D in the low-rank approximation, particularly in terms of specification error and consistency guarantees? The paper uses 100-rank approximation but doesn't systematically study how different kernel parameters affect estimator performance.

### Open Question 3
Can the KCMC framework be extended to handle more complex uncertainty sets beyond f-divergence and box constraints, such as moment conditions involving higher-order moments or non-convex constraints, while maintaining computational tractability and theoretical guarantees? The paper establishes the framework for f-divergence and box constraints but doesn't investigate whether the kernel method approach can be generalized to other constraint types.

## Limitations
- Performance heavily depends on kernel choice and rank truncation, with poor choices potentially leading to overly conservative bounds or infeasible optimization
- Theoretical guarantees assume continuous densities and bounded confounder supports, which may not hold in all practical scenarios
- Extension to f-divergence constraints introduces sensitivity to choice of convex function f that could impact validity

## Confidence
- **High confidence**: The mechanism by which kernel ridge regression approximates conditional moment constraints and the basic framework of the KCMC estimator
- **Medium confidence**: The consistency guarantees under M-estimation framework and the extension to policy learning
- **Medium confidence**: The theoretical conditions for zero specification error and the relationship between kernel choice and bound sharpness

## Next Checks
1. **Kernel Sensitivity Analysis**: Systematically test different kernel choices (RBF, polynomial, Matérn) and rank truncations on synthetic data with known confounding structures to quantify their impact on bound sharpness and feasibility
2. **Violation Robustness**: Evaluate estimator performance when the f-divergence constraint is violated (e.g., when true confounding exceeds assumed sensitivity bounds) using controlled simulations
3. **Sample Complexity**: Conduct experiments varying sample sizes to empirically verify the consistency claims and identify minimum sample requirements for reliable bounds