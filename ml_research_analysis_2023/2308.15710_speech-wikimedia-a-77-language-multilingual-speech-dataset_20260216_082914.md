---
ver: rpa2
title: 'Speech Wikimedia: A 77 Language Multilingual Speech Dataset'
arxiv_id: '2308.15710'
source_url: https://arxiv.org/abs/2308.15710
tags:
- speech
- dataset
- english
- language
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Speech Wikimedia dataset provides 1780 hours of CC-BY-SA licensed
  multilingual speech data from Wikimedia Commons, covering 77 languages. It includes
  audio files with transcriptions in the same or different languages, supporting speech
  recognition, speech translation, and machine translation tasks.
---

# Speech Wikimedia: A 77 Language Multilingual Speech Dataset

## Quick Facts
- arXiv ID: 2308.15710
- Source URL: https://arxiv.org/abs/2308.15710
- Reference count: 5
- 1780 hours of CC-BY-SA licensed multilingual speech data from Wikimedia Commons, covering 77 languages

## Executive Summary
The Speech Wikimedia dataset provides a diverse collection of 1780 hours of audio data across 77 languages from Wikimedia Commons. The dataset includes audio files with transcriptions in the same or different languages, making it suitable for speech recognition, speech translation, and machine translation tasks. Approximately 25% of audio files have multiple transcriptions in different languages, enabling cross-lingual learning opportunities. The dataset is publicly available and licensed under CC-BY-SA, supporting both academic and commercial applications.

## Method Summary
The dataset was constructed by collecting audio files from Wikimedia Commons and extracting their transcriptions. The data was then processed to remove files with incomplete information, resulting in 1780 hours of usable data across 77 languages. The dataset includes audio files with one or more transcriptions, with approximately 25% of files having transcripts in at least three languages. The data is licensed under CC-BY-SA and is available through Hugging Face.

## Key Results
- 1780 hours of multilingual speech data covering 77 languages
- 25% of audio files have multiple transcriptions in different languages
- Dataset is CC-BY-SA licensed and publicly available
- Diverse language pairs and topics support multiple speech and translation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple transcriptions per audio file enable robust cross-lingual alignment for speech translation
- Mechanism: Models learn correspondence between acoustic features and multiple written forms simultaneously
- Core assumption: Transcriptions accurately represent the same spoken content across languages
- Evidence anchors:
  - [abstract]: "Each audio file has one or more transcriptions in different languages, making this dataset suitable for training speech recognition, speech translation, and machine translation models."
  - [section 2.4]: "approximately 25% of the audio files have transcripts in at least three languages"
- Break condition: If transcriptions contain significant semantic drift or are not truly parallel across languages, cross-lingual alignment benefits diminish

### Mechanism 2
- Claim: Diverse language coverage enables better multilingual representation learning
- Mechanism: Models learn shared acoustic-phonetic patterns across languages
- Core assumption: Acoustic patterns across languages share sufficient similarity for transfer learning
- Evidence anchors:
  - [abstract]: "77 different languages" and "diverse set of scenarios and speakers"
  - [section 2.2]: "A total of 77 different languages were detected, with English, Dutch, German, Russian, and Spanish being the most common"
- Break condition: If languages are too linguistically distant, shared representation benefits may be minimal or negative

### Mechanism 3
- Claim: CC-BY-SA licensing enables both academic and commercial applications without legal barriers
- Mechanism: Clear, permissive licensing removes primary bottleneck for real-world deployment
- Core assumption: CC-BY-SA license requirements are understood and acceptable to potential users
- Evidence anchors:
  - [abstract]: "CC-BY-SA licensed transcribed speech" and "suitable for training... commercial usage"
  - [section 2.1]: "Given that all data is public domain, CC-BY-licensed, or CC-BY-SA licensed, we are licensing the dataset as CC-BY-SA"
- Break condition: If attribution requirements or share-alike provisions are too restrictive for specific commercial applications

## Foundational Learning

- Concept: Forced alignment of audio to transcripts
  - Why needed here: Essential for supervised speech recognition training
  - Quick check question: What is the primary challenge in forced alignment when dealing with noisy audio from diverse sources?

- Concept: Bitext alignment for machine translation
  - Why needed here: Required to create parallel text pairs for translation models
  - Quick check question: How do you handle cases where different language transcriptions express the same content with different levels of detail?

- Concept: Multilingual model architecture (e.g., m-CTC, multilingual Transformers)
  - Why needed here: Required to handle 77 languages through shared representations
  - Quick check question: What architectural choice best balances parameter efficiency with language-specific modeling needs?

## Architecture Onboarding

- Component map: Raw audio → Forced alignment → Feature extraction (MFCCs, log-mel) → Multilingual encoder (CNN/LSTM/Transformer) → Language-specific decoders (CTC for ASR, attention for translation)
- Critical path: Raw audio → Forced alignment → Feature extraction → Model training → Evaluation on language-specific test sets
- Design tradeoffs: Model capacity vs. training data per language; shared vs. language-specific components
- Failure signatures: Poor performance on low-resource languages; high WER on accented speech; translation quality drops on language pairs different from training distribution
- First 3 experiments:
  1. Train monolingual ASR model on English to establish baseline
  2. Train multilingual ASR model on all 77 languages to test representation sharing
  3. Train speech-to-text translation model using audio-transcript pairs with different languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of speech recognition models trained on Speech Wikimedia compare to those trained on other multilingual datasets?
- Basis in paper: [explicit] The paper mentions Speech Wikimedia has more diverse audio sources compared to Mozilla Common Voice and contains 77 languages
- Why unresolved: No performance comparisons between datasets provided
- What evidence would resolve it: Empirical studies comparing model performance across datasets using standardized metrics

### Open Question 2
- Question: What is the impact of diverse audio sources (public speeches, music, pronunciation dictionaries) on dataset quality and applicability?
- Basis in paper: [explicit] The paper mentions several audios are public speeches, music, and clearly pronounced single words
- Why unresolved: No analysis of how diversity affects suitability for various tasks
- What evidence would resolve it: Detailed analysis of audio content classification and correlation with model performance

### Open Question 3
- Question: How does availability of multiple transcriptions in different languages affect speech translation model quality?
- Basis in paper: [explicit] Approximately 25% of dataset has multiple associated transcripts in different languages
- Why unresolved: No exploration of impact on speech translation model training or performance
- What evidence would resolve it: Comparative studies of speech translation models trained with and without multiple language transcriptions

### Open Question 4
- Question: What is the effect of not having forced alignment and bitext word alignment on dataset usability?
- Basis in paper: [explicit] The raw data is not yet processed via forced alignment of audio to transcript and bitext word alignment
- Why unresolved: No information on how lack of alignment affects model training
- What evidence would resolve it: Studies comparing model training with aligned vs. non-aligned data

## Limitations
- Language coverage imbalance: Only 5 languages represent majority of data, with 42 languages having fewer than 30 hours
- Variable audio quality and domain diversity from aggregated Wikimedia sources
- Requires additional processing steps (forced alignment, bitext alignment) not provided
- No systematic quality assessment or domain categorization provided

## Confidence
- High confidence: Dataset licensing terms, basic metadata, and availability through Hugging Face
- Medium confidence: Claims about dataset diversity and utility for multiple tasks supported by statistics but lack empirical validation
- Low confidence: Claims about effectiveness for specific applications not supported by experimental results

## Next Checks
1. Quality assessment: Sample audio files across different languages and domains to evaluate actual audio quality, background noise levels, and transcription accuracy
2. Language distribution analysis: Calculate exact hour distribution across all 77 languages to identify true low-resource languages
3. Cross-lingual alignment validation: Test alignment quality between audio files and multiple transcriptions by training a small-scale speech translation model