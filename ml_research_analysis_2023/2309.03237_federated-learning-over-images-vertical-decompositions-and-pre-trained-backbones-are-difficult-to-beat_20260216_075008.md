---
ver: rpa2
title: 'Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones
  Are Difficult to Beat'
arxiv_id: '2309.03237'
source_url: https://arxiv.org/abs/2309.03237
tags:
- fail
- data
- learning
- accuracy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates multiple federated learning algorithms for
  image classification tasks, addressing key concerns about benchmark design, including
  narrow domains, evaluation metrics, and pre-trained backbones. Experiments show
  that vertically decomposing neural networks (IST methods) generally outperforms
  other reconciliation-based methods across diverse datasets and skewed data scenarios.
---

# Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat

## Quick Facts
- **arXiv ID**: 2309.03237
- **Source URL**: https://arxiv.org/abs/2309.03237
- **Reference count**: 40
- **Primary result**: Vertically decomposing neural networks (IST methods) generally outperforms other reconciliation-based methods across diverse datasets and skewed data scenarios.

## Executive Summary
This paper provides a comprehensive evaluation of federated learning algorithms for image classification tasks, addressing critical concerns about benchmark design including narrow domains, evaluation metrics, and pre-trained backbones. The study systematically compares various FL methods across multiple datasets with different degrees of data skewness, focusing on accuracy, computational efficiency (FLOPs), and communication efficiency (GB transferred). The results demonstrate that vertically decomposing neural networks through Independent Subspace Training (IST) methods consistently outperforms traditional reconciliation-based approaches while significantly reducing resource consumption. The paper emphasizes the importance of using pre-trained backbones and considering multiple evaluation metrics beyond accuracy to properly assess federated learning methods in resource-constrained environments.

## Method Summary
The study evaluates federated learning algorithms for image classification using pre-trained feature extractors (ResNet101, DenseNet121) followed by a single-hidden-layer MLP classifier. The federated learning methods tested include FedAvg, FedProx, MOON, FedAdam, FedNova, and IST with its proximal variant. Hyperparameters were tuned on the Birds dataset and then applied to other datasets (Stanford Cars, VGGFlowers, Aircraft, Describable Textures, CIFAR-100). Data was partitioned across clients using Dirichlet sampling to simulate non-i.i.d. distributions with varying concentration parameters (α = 0.1 to 1.0). The evaluation metrics included final accuracy, computational efficiency (GFLOPs to reach 90% of maximum accuracy), and communication efficiency (GB transferred).

## Key Results
- Vertically decomposing neural networks through IST methods consistently outperformed reconciliation-based methods across all tested datasets and data skewness levels
- Using pre-trained backbones with simple MLP classifiers achieved comparable accuracy to full backpropagation while significantly reducing computation and communication costs
- IST methods achieved the best balance between accuracy and resource efficiency, with substantial reductions in both FLOPs and communication volume
- Data skewness had minimal impact on final accuracy but affected convergence speed, with higher α values (less skewed) converging faster

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vertically decomposing neural networks (IST methods) outperforms reconciliation-based methods in federated image classification.
- Mechanism: IST partitions the neural network into independent subnetworks assigned to different clients, eliminating the need for model averaging and reducing communication overhead. Each client trains only a portion of the model, which is then shuffled and reassigned in subsequent rounds.
- Core assumption: The independent training of subnetworks does not significantly degrade model performance compared to centralized training.
- Evidence anchors:
  - [abstract] "Overall, across a wide variety of settings, we find that vertically decomposing a neural network seems to give the best results, and outperforms more standard reconciliation-used methods."
  - [section] "Overall, across the two experiments (i.i.d. and skewed), we find that adding a proximal term to FedAvg (to obtain FedProx) seems to have little effect on accuracy, but does speedup convergence... IST seems to be the best performer overall, allowing for significant reductions in communication and FLOPs."
  - [corpus] Weak evidence; no direct mention of IST or vertical decomposition in related papers.

### Mechanism 2
- Claim: Using pre-trained backbones significantly reduces computation and communication costs compared to full backpropagation.
- Mechanism: Pre-trained backbones (e.g., ResNet101, DenseNet121) are used as feature extractors without modification. Images are processed through these backbones once to obtain feature vectors, which are then used for training a simple MLP classifier.
- Core assumption: The pre-trained backbones capture sufficient features for the new classification task, making further training unnecessary.
- Evidence anchors:
  - [abstract] "The use of pre-trained backbones significantly reduces computation and communication costs compared to full backpropagation."
  - [section] "Training the feature-extraction backbone that is used in most deep image processing networks... is not easy... Given the resource constraints in many FL scenarios, it is unclear why attempting to fully train such a backbone from scratch in a federated environment would be the first approach."
  - [corpus] Weak evidence; no direct mention of pre-trained backbones in related papers.

### Mechanism 3
- Claim: Considering multiple evaluation metrics beyond accuracy is essential for assessing federated learning methods in resource-constrained environments.
- Mechanism: Evaluation includes final accuracy, computational efficiency (FLOPs), and communication efficiency (GB transferred). This provides a more comprehensive understanding of method performance under resource constraints.
- Core assumption: Resource efficiency is as important as accuracy in federated learning scenarios, especially with edge devices.
- Evidence anchors:
  - [abstract] "The study highlights the importance of considering multiple evaluation metrics beyond accuracy, including communication efficiency and computational efficiency, to properly assess federated learning methods in resource-constrained environments."
  - [section] "Choosing an evaluation metric is among the most difficult aspects of evaluating FL methods... we consider three metrics: final accuracy, computational efficiency, and communication efficiency."
  - [corpus] Weak evidence; no direct mention of evaluation metrics in related papers.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: Understanding FL is crucial as the paper evaluates various FL algorithms for image classification tasks.
  - Quick check question: What is the primary goal of federated learning, and how does it differ from centralized learning?

- Concept: Vertical vs. Horizontal Federated Learning
  - Why needed here: The paper focuses on vertical decomposition of neural networks, which is a key aspect of vertical FL.
  - Quick check question: How does vertical FL differ from horizontal FL in terms of data partitioning and model training?

- Concept: Pre-trained Models and Transfer Learning
  - Why needed here: The paper emphasizes the use of pre-trained backbones to reduce computation and communication costs.
  - Quick check question: What are the benefits and potential drawbacks of using pre-trained models in federated learning?

## Architecture Onboarding

- Component map: Pre-trained Backbones -> Feature Extraction -> MLP Classifier -> FL Method (FedAvg/FedProx/MOON/FedAdam/FedNova/IST) -> Client Devices -> Aggregation

- Critical path:
  1. Preprocess images using pre-trained backbones
  2. Partition and assign model components to clients (IST methods)
  3. Train models locally on client devices
  4. Aggregate results (if applicable) and update global model
  5. Evaluate performance using multiple metrics

- Design tradeoffs:
  - Accuracy vs. Resource Efficiency: Balancing final accuracy with computational and communication costs
  - Model Complexity vs. Training Efficiency: Using pre-trained backbones to simplify training but potentially limiting adaptability
  - Centralization vs. Decentralization: Deciding between reconciliation-based methods and independent subnet training

- Failure signatures:
  - High communication costs: Indicates inefficient model aggregation or large model sizes
  - Low accuracy: Suggests inadequate feature extraction or model partitioning issues
  - Slow convergence: May indicate suboptimal hyperparameters or insufficient local training

- First 3 experiments:
  1. Evaluate the impact of using pre-trained backbones on final accuracy and resource efficiency
  2. Compare the performance of IST methods with reconciliation-based methods in terms of accuracy and communication costs
  3. Assess the effect of data skewness on the performance of different FL algorithms

## Open Questions the Paper Calls Out
- None explicitly mentioned in the provided content

## Limitations
- The paper focuses exclusively on image classification tasks, limiting generalizability to other federated learning domains
- Specific implementation details of the IST method (model partitioning and weight shuffling) are not fully elaborated
- Hyperparameter values for each method and dataset combination are not fully specified, potentially affecting reproducibility

## Confidence
- **High Confidence**: The overall superiority of vertically decomposed neural networks (IST methods) in terms of accuracy and resource efficiency
- **Medium Confidence**: The significant reduction in computation and communication costs when using pre-trained backbones
- **Low Confidence**: The specific hyperparameter values and implementation details for optimal performance across all methods and datasets

## Next Checks
1. Reproduce key experiments using the specified pre-trained backbones and federated learning methods to verify reported results and resource efficiency claims
2. Investigate IST method details through a focused study on the model partitioning strategy and weight shuffling mechanism to understand performance benefits
3. Extend evaluation to diverse datasets beyond image classification (e.g., natural language processing or time series data) to assess generalizability of findings