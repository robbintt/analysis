---
ver: rpa2
title: 'Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach'
arxiv_id: '2311.09383'
source_url: https://arxiv.org/abs/2311.09383
tags:
- your
- question
- answer
- answers
- running
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Long-Form Question Answering (LFQA) model
  that iteratively plans, retrieves, and generates answers to produce detailed, paragraph-length
  responses. The model addresses the challenge of LFQA where questions often lack
  sufficient information for direct context retrieval, and detailed answers require
  aggregating knowledge from diverse sources.
---

# Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach

## Quick Facts
- arXiv ID: 2311.09383
- Source URL: https://arxiv.org/abs/2311.09383
- Reference count: 12
- This paper proposes an iterative planning-retrieval-generation approach for long-form question answering that outperforms state-of-the-art baselines on both open-domain and technical datasets.

## Executive Summary
This paper addresses the challenge of Long-Form Question Answering (LFQA) by proposing an Iterative Planning-Retrieval-Generation (IPRG) approach. The model recognizes that questions often lack sufficient information for direct context retrieval and that detailed answers require aggregating knowledge from diverse sources. IPRG generates keyword plans to guide context retrieval, iteratively refines answers through planning-retrieval-generation cycles, and demonstrates superior performance on both open-domain (WikiHow) and technical (Apple Exchange) datasets. The approach shows better recall, entailment scores, and reduced contradiction in generated answers compared to state-of-the-art baselines.

## Method Summary
The IPRG model consists of three interconnected modules: a Keyword Plan Generator that produces keywords to guide retrieval, a Retriever that finds relevant passages based on the question, pretext, and keywords, and an Answer Generator that produces the next sentence of the answer. The model iteratively cycles through these components, using the growing answer pretext to generate new keywords, retrieve additional contexts, and append new sentences until a complete answer is generated. Each module uses BART initialized with pre-trained weights, with separate training for each component before integration into the iterative loop.

## Key Results
- IPRG outperforms DPR+BART baselines in textual metrics (ROUGE-1, ROUGE-L recall/F1) on both WikiHow and Apple Exchange datasets
- The model achieves higher entailment scores and lower contradiction scores, indicating better factual consistency
- Question-only retrieved contexts in single-pass approaches show minimal impact on answer quality compared to the iterative keyword-guided approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative keyword planning guides retrieval toward relevant contexts by progressively refining the information need
- Mechanism: The Keyword Plan Generator produces keywords that act as intermediate plans between the question and the full answer, helping the Retriever find passages that support the next sentence
- Core assumption: Questions alone are insufficient for effective context retrieval, but keyword plans derived from the question and partial answer provide better retrieval signals
- Evidence anchors:
  - [abstract]: "the question alone might not provide sufficient information to identify the relevant contexts"
  - [section 2.1]: "To solve this issue, we predict some key points that will be used to guide the generation of the next answer sentence at each iteration"
- Break condition: If keyword plans become redundant or irrelevant to the question, they may mislead retrieval rather than improve it

### Mechanism 2
- Claim: Iterative refinement through planning-retrieval-generation cycles produces more comprehensive answers by progressively enriching the answer context
- Mechanism: Each iteration uses the growing answer pretext to generate new keywords, retrieve additional contexts, and append new sentences, creating a knowledge accumulation process
- Core assumption: Each new answer sentence provides additional context that helps identify new relevant information for subsequent sentences
- Evidence anchors:
  - [abstract]: "This iterative process continues until a complete answer is generated"
  - [section 2.3]: "we ensemble the question, pretext, generated keywords, and retrieved contexts as input for the final answer generation model"
- Break condition: If the iterative process converges to repetitive content or fails to introduce new information, it becomes inefficient

### Mechanism 3
- Claim: Combining keyword planning with iterative retrieval outperforms single-pass retrieval-augmented generation by addressing the limitation of question-only retrieval
- Mechanism: IPRG uses keyword plans to improve retrieval quality, while single-pass models like DPR+BART rely solely on the question as retrieval query
- Core assumption: Keyword plans contain information not present in the original question that improves retrieval relevance
- Evidence anchors:
  - [section 3.1]: "IPRG achieves more entailment scores as well as less contradictory scores in both datasets. Whereas, DPR+BART generated answers contain more contradiction"
  - [section 3.1]: "DPR+BART performs per BART, indicating that question-only retrieved contexts in a single pass have minimal impacts"
- Break condition: If the keyword generation module produces poor quality keywords, the advantage over single-pass approaches diminishes

## Foundational Learning

- Concept: Sequence-to-sequence modeling for text generation
  - Why needed here: The model uses BART, a seq2seq transformer, for both keyword generation and answer generation
  - Quick check question: What are the key architectural components of a transformer-based seq2seq model and how do they enable text generation?

- Concept: Dense passage retrieval (DPR)
  - Why needed here: The Retriever module uses DPR to find relevant passages based on the question and keyword plan
  - Quick check question: How does DPR encode queries and passages into dense vector representations and what similarity measure does it use for retrieval?

- Concept: Keyword extraction and generation
  - Why needed here: The Keyword Plan Generator converts answer sentences into keyword sets for training and generates keyword plans during inference
  - Quick check question: What are common approaches to keyword extraction from text and how do they differ from keyword generation as a seq2seq task?

## Architecture Onboarding

- Component map: Keyword Plan Generator -> Retriever -> Answer Generator -> Pretext update -> (repeat)
- Critical path: Question → Keyword Plan Generator → Retriever → Answer Generator → Pretext update → (repeat)
- Design tradeoffs:
  - Using iterative approach increases complexity but improves answer quality
  - Separate modules allow flexibility but may introduce error propagation
  - Keyword planning adds an extra step but addresses the limitation of question-only retrieval
- Failure signatures:
  - Repetitive content in generated answers
  - Poor retrieval quality (irrelevant passages)
  - Keyword plans that don't align with answer content
  - Long generation times due to multiple iterations
- First 3 experiments:
  1. Test Keyword Plan Generator alone on validation data to verify keyword quality
  2. Test Retriever alone with ground truth keyword plans to isolate retrieval effectiveness
  3. Run full IPRG with 2-3 iterations on a small dataset to verify the iterative process works as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a task-dependent keyword planning generation module that doesn't rely on existing keyword extraction methods?
- Basis in paper: [explicit] The paper mentions that the keyword plan generator depends on existing keyword extraction methods, which can lead to error propagation.
- Why unresolved: The paper acknowledges this limitation but does not provide a solution for developing a more robust, task-specific keyword generation approach.
- What evidence would resolve it: A new model architecture or training method that generates keywords directly from the question and partial answer without relying on external keyword extraction tools.

### Open Question 2
- Question: Would a joint end-to-end training approach for the IPRG model outperform the current modular approach?
- Basis in paper: [explicit] The paper notes that separately training each module makes it susceptible to error propagation and prevents learning from one module's errors to refine another.
- Why unresolved: The paper only explores separate training of the three modules and does not investigate joint training approaches.
- What evidence would resolve it: Comparative experiments showing performance differences between modular training and end-to-end joint training on the same datasets.

### Open Question 3
- Question: How can we automatically identify informative keywords versus non-informative ones for content planning?
- Basis in paper: [explicit] The paper states that "not all kinds of keywords are important as content plans for future answer generation" but does not address how to distinguish between them.
- Why unresolved: The paper uses extracted keywords without a mechanism to filter or rank their importance for answer generation.
- What evidence would resolve it: A method that evaluates keyword informativeness and demonstrates improved answer quality when filtering out less relevant keywords.

## Limitations
- The iterative approach introduces significant complexity and potential for error propagation across modules
- Empirical validation is limited to two specific datasets (WikiHow and Apple Exchange), restricting generalizability
- Evaluation focuses on textual and factual consistency metrics without extensive examination of computational efficiency or scalability

## Confidence

- High confidence: The core mechanism of using keyword plans to guide retrieval is well-supported by the reported performance improvements over single-pass baselines
- Medium confidence: The iterative refinement process shows benefits, but the diminishing returns across multiple iterations and optimal stopping criteria remain unclear
- Medium confidence: The separation of concerns across three specialized modules is methodologically sound, though the relative contribution of each component to overall performance is not fully isolated

## Next Checks

1. **Ablation study on iteration count**: Systematically evaluate IPRG performance with 1, 2, 3, and 4+ iterations to quantify the marginal benefit of each additional cycle and identify optimal stopping points

2. **Module dependency analysis**: Replace the keyword plan generator with ground truth keywords to isolate the contribution of keyword quality versus the full iterative process, then do the same for the retriever module

3. **Cross-domain generalization test**: Apply the trained IPRG model to a third, distinct dataset (e.g., academic Q&A or technical documentation) to assess robustness beyond the two evaluation domains used in the paper