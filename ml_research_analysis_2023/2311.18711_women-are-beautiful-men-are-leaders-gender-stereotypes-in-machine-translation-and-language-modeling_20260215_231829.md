---
ver: rpa2
title: 'Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation
  and Language Modeling'
arxiv_id: '2311.18711'
source_url: https://arxiv.org/abs/2311.18711
tags:
- gender
- stereotypes
- samples
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GEST, a new dataset designed to measure gender-stereotypical
  reasoning in language models and machine translation systems. The dataset includes
  samples for 16 gender stereotypes about men and women, covering 9 Slavic languages
  and English.
---

# Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling

## Quick Facts
- arXiv ID: 2311.18711
- Source URL: https://arxiv.org/abs/2311.18711
- Reference count: 31
- Primary result: Significant gender-stereotypical reasoning found across MLMs and MT systems; larger models tend to be more stereotypical.

## Executive Summary
This paper introduces GEST, a new dataset designed to measure gender-stereotypical reasoning in language models and machine translation systems. The dataset includes samples for 16 gender stereotypes about men and women, covering 9 Slavic languages and English. The authors used GEST to evaluate English and Slavic masked language models, English generative language models, and machine translation systems. Results show significant and consistent amounts of gender-stereotypical reasoning in almost all evaluated models and languages, confirming the hypothesis that larger models tend to be more stereotypical. The dataset and methodology can be extended to support other languages and stereotypes.

## Method Summary
The GEST dataset contains 3,565 manually created samples for 16 gender stereotypes (7 female, 9 male). For MT evaluation, English samples are translated to 9 Slavic languages using 4 MT systems (Amazon Translate, DeepL, Google Translate, NLLB200), then language-specific heuristics determine grammatical gender of the first person in translations. For MLM evaluation, prompts force gender selection and token probabilities for gender-coded words are calculated. Two metrics are used: masculine rate (fm) measuring male-as-norm behavior, and stereotype rate (fs/gs) measuring stereotypical reasoning. The methodology relies on grammatical gender agreement in Slavic languages to reveal stereotypical associations.

## Key Results
- Gender-stereotypical reasoning is present in practically all evaluated models across multiple languages
- Larger models tend to exhibit more stereotypical reasoning, confirming previous hypotheses
- Consistency of results across different MT systems and MLMs indicates robust measurement of gender stereotypes
- Topical biases (e.g., family-related words) can confound stereotype measurement in some cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset measures stereotypical reasoning by observing grammatical gender agreement in Slavic languages.
- Mechanism: Samples are gender-neutral in English but gender-coded after translation. Models' predictions for gender-coded words reveal their stereotypical associations.
- Core assumption: Models rely on semantic content rather than surface heuristics like word frequency or topical bias.
- Evidence anchors:
  - [abstract]: "The samples can be used only in languages that share certain grammatical similarities with Slovak, in this case the gender agreement of adjectives in the first person."
  - [section 4.3]: "These languages have gender agreements between the gender of the first person and modal verbs... The gender is generally indicated with a suffix."
- Break condition: If models use non-semantic heuristics (e.g., topical bias), results would not reflect true stereotypical reasoning.

### Mechanism 2
- Claim: Consistent results across multiple models and languages indicate robust measurement of gender stereotypes.
- Mechanism: Similar patterns of stereotypical reasoning across different MT systems and MLMs suggest the dataset captures meaningful signal rather than noise.
- Core assumption: Different models learn similar patterns from training data, allowing consistent measurement.
- Evidence anchors:
  - [abstract]: "Our experiments show that stereotypical reasoning is a wide-spread phenomenon present in practically all the models we tested."
  - [section 5.1]: "The consistency of results for individual stereotypes across the systems indicates that we have indeed managed to measure a meaningful signal."
- Break condition: If models have vastly different training data or architectures, consistency might not hold.

### Mechanism 3
- Claim: Extensibility allows adaptation to other languages with similar grammatical features.
- Mechanism: Dataset methodology can be applied to languages where gender-neutral constructs become gender-coded, enabling broader bias measurement.
- Core assumption: Grammatical gender agreement is a reliable indicator of gender associations across languages.
- Evidence anchors:
  - [section 3.2]: "The samples can be used only in languages that share certain grammatical similarities with Slovak."
  - [section 5.2]: "It is possible to reuse, edit, or recreate the dataset for other language combinations."
- Break condition: If target languages lack clear grammatical gender indicators, the methodology would not apply.

## Foundational Learning

- Concept: Gender stereotypes as defined by sociological research
  - Why needed here: Provides theoretical foundation for dataset design and interpretation
  - Quick check question: How are gender stereotypes defined in this work compared to previous NLP studies?

- Concept: Grammatical gender agreement
  - Why needed here: Core mechanism for measuring bias in Slavic languages
  - Quick check question: What grammatical features make Slavic languages suitable for this bias measurement?

- Concept: Template-based prompting for MLMs
  - Why needed here: Method for eliciting gender associations from masked language models
  - Quick check question: How do different prompt templates affect the measurement of gender bias?

## Architecture Onboarding

- Component map: Dataset creation → Translation → Model evaluation → Result aggregation
- Critical path: Sample creation → Translation to target languages → Model prediction → Gender detection → Statistical analysis
- Design tradeoffs: Manual sample creation ensures quality but limits scalability; automatic translation introduces noise but enables large-scale evaluation
- Failure signatures: Inconsistent results across models/languages, low yield of usable samples, high false positive/negative rates in gender detection
- First 3 experiments:
  1. Evaluate a single stereotype across multiple MT systems to verify consistency
  2. Test different prompt templates with a single MLM to check template robustness
  3. Compare masculine rates for stereotypically male samples containing family-related words to detect topical bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do non-binary gender biases manifest in languages that lack established non-binary grammatical structures, and what methodologies could effectively measure them?
- Basis in paper: [inferred] The paper explicitly states its limitation to binary gender analysis due to grammatical constraints in Slavic languages, suggesting non-binary gender measurement requires rethinking the methodology.
- Why unresolved: The paper acknowledges the cultural and linguistic expertise required to address non-binary genders is beyond its scope, and existing languages often lack established ways to handle non-binary identities.
- What evidence would resolve it: Development and validation of new evaluation methodologies that incorporate non-binary gender markers in languages with flexible grammatical structures, combined with cross-cultural linguistic studies.

### Open Question 2
- Question: What is the causal relationship between model size and increased gender stereotypical reasoning, and does this relationship hold across different types of stereotypes?
- Basis in paper: [explicit] The paper confirms the previously postulated hypothesis that larger models tend to be more stereotypical, but does not explore the underlying mechanisms or stereotype-specific variations.
- Why unresolved: While correlation is established, the paper does not investigate whether larger models learn more stereotypes overall, learn them more strongly, or if certain stereotype categories are more affected by model size.
- What evidence would resolve it: Controlled experiments varying model size while controlling for training data and architecture, with detailed analysis of stereotype-specific bias strength across different model scales.

### Open Question 3
- Question: To what extent do topical biases (e.g., family-related words being associated with female gender) confound the measurement of gender stereotypes, and how can evaluation datasets be designed to minimize this effect?
- Basis in paper: [explicit] The paper identifies that semantically male samples containing family-related words were incorrectly associated with female gender, demonstrating a topical bias that affects stereotype measurement.
- Why unresolved: The paper acknowledges this as a limitation but does not provide solutions for distinguishing semantic content from topical associations in bias measurement.
- What evidence would resolve it: Development of evaluation datasets with carefully controlled topical content across stereotypes, and validation studies showing whether current measurement approaches can distinguish between semantic and topical gender associations.

## Limitations

- Methodology relies heavily on grammatical gender agreement in Slavic languages, limiting generalizability to other language families
- Dataset construction is labor-intensive, making scalability to larger stereotype categories or more languages challenging
- Focus on binary gender stereotypes potentially overlooks non-binary gender representations and intersectional biases

## Confidence

- **High Confidence**: The existence of gender-stereotypical reasoning in evaluated models is well-supported by consistent results across multiple MT systems and MLMs.
- **Medium Confidence**: The claim that larger models tend to be more stereotypical is supported by observed patterns but requires more systematic analysis across model sizes.
- **Low Confidence**: The assertion that the dataset captures "true stereotypical reasoning" rather than surface-level patterns could be challenged, as models might rely on spurious correlations or topical associations.

## Next Checks

1. **Heuristic Validation**: Conduct manual validation of the gender detection heuristics on a random sample of MT outputs across all language pairs to quantify false positive/negative rates and assess their impact on overall results.

2. **Cross-linguistic Generalization**: Test the dataset methodology with non-Slavic languages that have grammatical gender (e.g., Romance languages) to evaluate whether the measurement approach generalizes beyond the current scope.

3. **Model Size Analysis**: Perform a controlled experiment comparing stereotype rates across models of varying sizes within the same family (e.g., different GPT or BERT sizes) to more rigorously test the relationship between model scale and stereotypical reasoning.