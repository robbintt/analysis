---
ver: rpa2
title: 'PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in
  non-English Text-to-Image Generation'
arxiv_id: '2311.17086'
source_url: https://arxiv.org/abs/2311.17086
tags:
- data
- training
- clip
- text
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting English text-to-image
  diffusion models to support non-English prompts without expensive retraining. The
  proposed PEA-Diffusion method uses a lightweight 6M-parameter adapter module trained
  with knowledge distillation to align the feature spaces between a non-English CLIP
  encoder and the frozen UNet of a pretrained English model.
---

# PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation

## Quick Facts
- arXiv ID: 2311.17086
- Source URL: https://arxiv.org/abs/2311.17086
- Reference count: 40
- Achieves up to 0.15 CLIPScore improvement for culture-specific image generation while using only 0.2% of trainable parameters

## Executive Summary
This paper introduces PEA-Diffusion, a parameter-efficient method for adapting English text-to-image diffusion models to support non-English prompts without expensive full retraining. The approach uses a lightweight 6M-parameter adapter trained with knowledge distillation to align feature spaces between non-English CLIP encoders and the frozen UNet of pretrained English models. By distilling knowledge from intermediate features and output logits, PEA-Diffusion enables generation of culturally relevant images for specific languages while maintaining general image quality comparable to English models. Experiments across five languages demonstrate superior performance to translation-based and multilingual baselines with minimal computational overhead.

## Method Summary
PEA-Diffusion adapts English text-to-image diffusion models (specifically Stable Diffusion) to non-English languages by training a small MLP adapter while keeping the UNet frozen. The method uses knowledge distillation to align feature spaces between non-English CLIP encoders and the frozen UNet, training on parallel text-image datasets with language-specific CLIP models. The adapter consists of only 6M parameters (≈0.2% of total parameters) and is trained for 70k steps using 64 A100 GPUs. The approach preserves compatibility with downstream tasks like LoRA, ControlNet, and accelerated models by maintaining unchanged UNet parameters.

## Key Results
- Outperforms translation-based and multilingual baselines in generating culture-specific images (CLIPScore gains up to 0.15)
- Maintains general image generation quality comparable to English models while using only 6M trainable parameters
- Demonstrates plug-and-play compatibility with downstream tasks like LoRA, ControlNet, and accelerated models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEA-Diffusion uses a lightweight adapter to align the feature spaces between non-English CLIP encoder and the frozen UNet of an English model.
- Mechanism: By training a small MLP-like adapter (6M parameters) while keeping the UNet frozen, the method aligns the feature and output spaces between source and target UNet for bilingual prompts.
- Core assumption: The frozen UNet still retains the ability to generate culture-specific images when guided by an aligned adapter.
- Evidence anchors:
  - [abstract] "PEA-Diffusion method uses a lightweight 6M-parameter adapter module trained with knowledge distillation to align the feature spaces between a non-English CLIP encoder and the frozen UNet"
  - [section] "PEA merely consists of a light-weight multi-layer perceptron (MLP), greatly simplifying the model construction process and preserving the image generation capabilities of the original SD model"
  - [corpus] Weak - corpus provides related works but doesn't directly confirm the alignment mechanism

### Mechanism 2
- Claim: Knowledge distillation from the UNet's intermediate features and output logits enables the model to understand non-English cultural concepts.
- Mechanism: By distilling knowledge from the UNet's intermediate features and output logits, PEA-Diffusion stimulates the model's potential to generate culturally relevant images for specific languages.
- Core assumption: The intermediate features and output logits contain sufficient information about the generation process to be effectively distilled.
- Evidence anchors:
  - [abstract] "By distilling knowledge from the UNet's intermediate features and output logits, PEA-Diffusion stimulates the model's potential to generate culturally relevant images for specific languages"
  - [section] "we propose KD loss to compel representation alignment at feature maps and logits space from the UNet module between our PEA-Diffusion and pretrained English SD models"
  - [corpus] Weak - corpus mentions KD in related contexts but doesn't confirm its effectiveness for cultural concept understanding

### Mechanism 3
- Claim: The adapter's plug-and-play nature allows it to be used with downstream tasks like LoRA, ControlNet, and accelerated models.
- Mechanism: The adapter maintains compatibility with the English SD community by keeping the core UNet parameters unchanged, allowing seamless integration with various English SD community plugins.
- Core assumption: The unchanged UNet parameters ensure compatibility with downstream tasks that rely on these parameters.
- Evidence anchors:
  - [abstract] "The adapter is also shown to be plug-and-play for downstream tasks like LoRA, ControlNet, and accelerated models, maintaining compatibility with the English SD community"
  - [section] "it naturally maintains compatibility with the English SD community and can seamlessly integrate with various English SD community plug-ins"
  - [corpus] Weak - corpus doesn't provide direct evidence of the adapter's plug-and-play capabilities

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: To transfer knowledge from the pretrained English model to the non-English model without expensive retraining.
  - Quick check question: How does knowledge distillation help in aligning the feature spaces between different language models?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: To adapt the model to non-English languages with minimal computational cost and parameters.
  - Quick check question: What are the advantages of using a lightweight adapter compared to full fine-tuning?

- Concept: Cross-lingual alignment
  - Why needed here: To ensure the non-English model can generate culturally relevant images by aligning with the English model's feature spaces.
  - Quick check question: How does cross-lingual alignment help in generating culture-specific images?

## Architecture Onboarding

- Component map:
  - Non-English CLIP encoder -> PEA adapter (6M parameters) -> Frozen UNet -> Generated image

- Critical path:
  - Input non-English prompt → Non-English CLIP encoder → PEA adapter → Frozen UNet → Generated image

- Design tradeoffs:
  - Freezing UNet for compatibility vs. potential loss of adaptation
  - Lightweight adapter for efficiency vs. potential loss of performance
  - Knowledge distillation for alignment vs. potential noise in distilled features

- Failure signatures:
  - Poor image quality or cultural relevance
  - Incompatibility with downstream tasks
  - Excessive computational cost

- First 3 experiments:
  1. Train PEA adapter with knowledge distillation on a small parallel corpus and evaluate on a culture-specific prompt evaluation set.
  2. Test adapter's compatibility with downstream tasks like LoRA and ControlNet.
  3. Compare performance with translation-based and multilingual baselines on both general and culture-specific prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the PEA-Diffusion method perform when applied to languages with significantly different linguistic structures, such as agglutinative or tonal languages?
- Basis in paper: [inferred] The paper demonstrates effectiveness across five languages (Chinese, Russian, Japanese, Italian, Korean) but does not explore languages with more complex morphological structures.
- Why unresolved: The paper focuses on languages with relatively similar linguistic characteristics, leaving the method's generalizability to more diverse language families untested.
- What evidence would resolve it: Testing PEA-Diffusion on languages like Turkish (agglutinative) or Vietnamese (tonal) and comparing performance metrics with the current results.

### Open Question 2
- Question: What is the upper limit of parameter efficiency for the PEA adapter before performance degradation becomes significant?
- Basis in paper: [explicit] The paper mentions using only 6M parameters (≈0.2% of total parameters) but does not systematically explore the trade-off between adapter size and performance.
- Why unresolved: The authors chose a specific adapter size without exploring how smaller or larger adapters might affect performance or training efficiency.
- What evidence would resolve it: Systematic experiments varying adapter size while keeping other factors constant to identify the optimal balance between parameter efficiency and generation quality.

### Open Question 3
- Question: How does PEA-Diffusion handle domain-specific cultural concepts that may not have direct translations or equivalents in the training data?
- Basis in paper: [explicit] The paper mentions the ability to generate culturally relevant images but does not detail how it handles concepts without direct equivalents across languages.
- Why unresolved: The method's approach to handling unique cultural concepts that don't translate well between languages remains unclear.
- What evidence would resolve it: Analysis of PEA-Diffusion's performance on culturally specific concepts that lack direct translations, comparing results with human-annotated cultural relevance scores.

### Open Question 4
- Question: What is the impact of training data quality versus quantity on PEA-Diffusion's performance for different languages?
- Basis in paper: [explicit] The paper mentions using parallel corpora and cultural-specific data but does not systematically analyze how data quality versus quantity affects performance.
- Why unresolved: The relative importance of high-quality versus large-scale training data for different languages is not explored.
- What evidence would resolve it: Controlled experiments varying both the quality and quantity of training data for different languages while measuring performance impacts.

### Open Question 5
- Question: How does PEA-Diffusion's performance scale with increasing model size and complexity?
- Basis in paper: [inferred] The paper demonstrates effectiveness with SDXL but does not explore how performance scales with larger or more complex diffusion models.
- Why unresolved: The method's effectiveness on larger, more sophisticated models remains untested.
- What evidence would resolve it: Testing PEA-Diffusion on progressively larger diffusion models (e.g., SDXL++, SD3) while measuring performance and efficiency metrics.

## Limitations
- The paper's claims about cross-lingual cultural relevance hinge on the assumption that the 6M-parameter adapter can adequately capture complex cultural concepts through knowledge distillation alone.
- The claim of maintaining compatibility with downstream tasks like LoRA and ControlNet lacks detailed empirical validation across diverse use cases.
- The qualitative evaluation of "culture-specific" image generation is limited to specific examples rather than comprehensive cultural assessment.

## Confidence
- **High Confidence**: The parameter efficiency claim (0.2% trainable parameters) and basic technical implementation of the adapter-based knowledge distillation approach
- **Medium Confidence**: The performance improvements over baselines on the reported benchmarks, though limited to specific languages and evaluation metrics
- **Low Confidence**: The extent of "plug-and-play" compatibility with all downstream tasks and the generalization of cultural relevance across all non-English languages

## Next Checks
1. Conduct ablation studies removing the knowledge distillation component to isolate its contribution to cultural relevance
2. Test adapter compatibility with a broader range of English SD community plugins beyond the mentioned LoRA, ControlNet, and accelerated models
3. Evaluate performance on additional non-English languages and cultures not covered in the initial five-language study