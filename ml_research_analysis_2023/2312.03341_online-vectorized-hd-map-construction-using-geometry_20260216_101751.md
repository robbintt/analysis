---
ver: rpa2
title: Online Vectorized HD Map Construction using Geometry
arxiv_id: '2312.03341'
source_url: https://arxiv.org/abs/2312.03341
tags:
- loss
- shape
- euclidean
- instance
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a geometric-aware framework, GeMap, for vectorized
  HD map construction in autonomous driving. The method leverages Euclidean shape
  and relation clues to represent the geometric properties of map instances and their
  relationships, such as parallelism, perpendicularity, and rectangle shapes.
---

# Online Vectorized HD Map Construction using Geometry

## Quick Facts
- arXiv ID: 2312.03341
- Source URL: https://arxiv.org/abs/2312.03341
- Reference count: 40
- Primary result: 71.8% mAP on Argoverse 2, first method exceeding 70% threshold

## Executive Summary
This paper introduces GeMap, a geometric-aware framework for vectorized HD map construction in autonomous driving. The method leverages Euclidean shape and relation clues through a novel G-Representation and geometry-decoupled attention mechanism. By introducing a geometric loss function and careful architectural design, GeMap achieves state-of-the-art performance on both NuScenes and Argoverse 2 datasets, particularly excelling at handling occluded instances and rotational transformations.

## Method Summary
GeMap transforms multi-view camera images into BEV features using a ResNet backbone with GKT transformation. A Transformer decoder with geometry-decoupled attention (GDA) processes these features through 6 layers, using binary masks to separate intra-instance shape attention from inter-instance relation attention. The model predicts polylines for map instances, which are converted to G-Representation (angles and distances between displacement vectors) and optimized using Euclidean Loss. The framework includes classification, regression, and auxiliary losses for segmentation and depth estimation in the full objective.

## Key Results
- Achieves 71.8% mAP on Argoverse 2 dataset, +4.4% over MapTR V2
- First method to surpass 70% mAP threshold on large-scale Argoverse 2
- Superior robustness to occlusion and rotational transformations
- Significant improvements in handling complex shapes and recovering occluded instances

## Why This Works (Mechanism)

### Mechanism 1
- G-Representation captures Euclidean shape clues using angles and distances between displacement vectors, making map instance geometry invariant to rotation and translation.
- Displacement vectors are computed from consecutive polyline points; their magnitudes and inter-angles encode local geometry without dependence on absolute coordinates.
- Core assumption: Urban road geometry is predominantly composed of instances whose shapes can be well described by local vector relationships (parallel lines, perpendicular crossings, equal widths).
- Evidence: Abstract mentions rotational and translational invariance; section 3.3.1 details displacement vector computation.
- Break condition: If urban road instances contain highly irregular or curved shapes that cannot be approximated by piecewise straight segments.

### Mechanism 2
- Geometry-decoupled attention (GDA) sequentially processes intra-instance shape attention then inter-instance relation attention, preventing cross-instance interference and enhancing geometric learning.
- Two binary masks Mshp and Mrel split attention into "same-instance tokens" and "different-instance tokens", allowing the model to specialize each self-attention pass.
- Core assumption: Shape geometry and relation geometry have fundamentally different optimal attention patterns and should not be mixed.
- Evidence: Section 3.5 describes GDA mechanism with binary masks for shape and relation attention.
- Break condition: If instance point order is shuffled or if the model requires cross-instance dependencies within the same attention block.

### Mechanism 3
- Euclidean Loss (LEuc) optimizes predicted displacement vector magnitudes and angles directly in G-Representation space, aligning predictions more closely with ground truth geometry without relying on point-wise coordinate matching.
- Instead of L1 loss on polyline coordinates, LEuc computes differences between predicted and ground truth displacement vector lengths and angles using sine/cosine to avoid inverse trig.
- Core assumption: Ground truth map instances can be accurately converted into G-Representation without loss of geometric fidelity, and optimizing in this space yields better geometric alignment.
- Evidence: Section 3.4 details the Euclidean Loss formulation with angle and distance optimization.
- Break condition: If ground truth annotations are noisy or inconsistent in polyline point ordering, the G-Representation conversion may be unreliable.

## Foundational Learning

- Concept: Bird's-Eye-View (BEV) representation learning
  - Why needed here: Map construction requires reasoning in a top-down coordinate frame; converting perspective camera views into BEV is fundamental before any geometric modeling.
  - Quick check question: What is the output shape of a BEV feature map if the input image is 1280x720 and the BEV resolution is 0.5m per pixel over a 100m x 100m area?

- Concept: Transformer self-attention and masking
  - Why needed here: The geometry-decoupled attention uses custom binary masks to separate intra-instance and inter-instance processing; understanding how masks alter attention flow is essential.
  - Quick check question: In a self-attention layer with shape mask Mshp, what happens to the attention score between a token from instance A and a token from instance B?

- Concept: Euclidean geometry in 2D space (angles, distances, displacement vectors)
  - Why needed here: The core representation uses vector displacements and their angles/distances; engineers must be comfortable converting polylines into this form and vice versa.
  - Quick check question: Given two displacement vectors v1 = (3,0) and v2 = (0,4), what are their magnitudes and the angle between them?

## Architecture Onboarding

- Component map: Input images -> BEV encoder (ResNet + GKT) -> Geometry-decoupled decoder (6-layer Transformer with GDA) -> Prediction head (MLP) -> Polylines -> Euclidean Loss

- Critical path: 1) Multi-view images → BEV feature map 2) BEV features + N×Nv queries → Geometry-decoupled decoder 3) Decoder outputs N polylines 4) Convert predictions and GT to G-Representation 5) Compute Euclidean Loss and other losses 6) Backpropagate and update model

- Design tradeoffs: Decoupling attention increases complexity but improves geometric learning; naive 2-SA (doubling layers) hurts performance. Using G-Representation avoids rigid transformation sensitivity but requires careful polyline-to-vector conversion. Full objective adds depth estimation and segmentation losses for robustness at cost of speed.

- Failure signatures: Degraded performance on occluded instances (likely GDA not capturing enough context), sensitivity to rotation/translation (Euclidean Loss not properly invariant), overfitting to training scenes (too many queries or excessive decoder depth).

- First 3 experiments: 1) Baseline: Train with vanilla self-attention and coordinate L1 loss; measure mAP 2) Add Euclidean Loss only: Convert to G-Representation but keep vanilla attention; observe performance change 3) Add Geometry-decoupled Attention only: Keep vanilla loss but replace self-attention with GDA; observe performance change

## Open Questions the Paper Calls Out
- What specific geometric patterns in urban road systems could be most effectively captured by extending the Euclidean Shape Clues representation beyond the current implementation?
- How does the performance of GeMap vary across different autonomous driving scenarios, such as highway driving versus urban environments with complex intersections?
- What are the limitations of the current geometry-decoupled attention mechanism when applied to real-time autonomous driving systems, and how can these be addressed?

## Limitations
- Geometric assumptions (parallelism, perpendicularity, rectangular shapes) may not generalize to all urban environments, particularly those with irregular road layouts or curved geometries.
- Effectiveness of geometry-decoupled attention is demonstrated through ablation studies but the mechanism's contribution is somewhat indirect - evaluated as part of complete GeMap framework rather than in isolation.
- Novel geometric loss function shows strong performance gains but exact sensitivity to hyperparameter λ and robustness to noisy ground truth polyline annotations are not thoroughly explored.

## Confidence
- High confidence: Reported performance improvements on Argoverse 2 (71.8% mAP) and claim of first method exceeding 70% mAP threshold are well-supported by experimental results.
- Medium confidence: Claims about superior robustness to occlusion and rotational transformations are supported by qualitative examples and specific metric improvements, but could benefit from more extensive controlled experiments.
- Medium confidence: Mechanism explanations for why G-Representation and GDA work are theoretically sound and internally consistent, but lack direct empirical validation of each component in isolation.

## Next Checks
1. Conduct controlled experiments comparing GeMap's performance on rectangular vs. non-rectangular map instances to quantify the geometric assumption's limitations.
2. Perform ablation studies specifically isolating the geometry-decoupled attention mechanism by comparing it against alternative attention designs while keeping other components constant.
3. Evaluate model robustness by testing performance degradation when ground truth polyline annotations contain geometric noise or inconsistent point ordering to validate the Euclidean Loss's noise tolerance claims.