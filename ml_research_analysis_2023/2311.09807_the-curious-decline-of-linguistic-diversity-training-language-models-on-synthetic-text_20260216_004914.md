---
ver: rpa2
title: 'The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic
  Text'
arxiv_id: '2311.09807'
source_url: https://arxiv.org/abs/2311.09807
tags:
- diversity
- language
- data
- generation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines the impact of training large language models
  on synthetic data generated by their predecessors, a practice that is becoming increasingly
  common due to the limited supply of high-quality human-generated training data.
  The researchers developed a set of novel metrics to assess lexical, syntactic, and
  semantic diversity, and conducted recursive fine-tuning experiments on three natural
  language generation tasks: news summarization, scientific abstract generation, and
  story generation.'
---

# The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text

## Quick Facts
- arXiv ID: 2311.09807
- Source URL: https://arxiv.org/abs/2311.09807
- Reference count: 17
- Primary result: Recursive training on synthetic data causes consistent decline in lexical, syntactic, and semantic diversity, especially for creative tasks

## Executive Summary
This study examines the impact of training large language models on synthetic data generated by their predecessors, a practice that is becoming increasingly common due to limited high-quality human-generated training data. The researchers developed novel metrics to assess lexical, syntactic, and semantic diversity, and conducted recursive fine-tuning experiments on three natural language generation tasks: news summarization, scientific abstract generation, and story generation. Their findings reveal a consistent decrease in the diversity of model outputs through successive iterations, with the trend being more pronounced for tasks requiring higher levels of creativity, such as story generation.

## Method Summary
The researchers used OPT-350M as a base model and conducted recursive fine-tuning for 6 iterations on three language generation tasks of varying "entropy" (news summarization, scientific abstract generation, and story generation). At each iteration, the model was fine-tuned on either human-written data (iteration 1) or synthetic data generated by the previous model. They generated 10,000 samples per iteration using greedy decoding and evaluated diversity using metrics for lexical diversity (TTR, Distinct-n, Self-BLEU), semantic diversity (Sentence-BERT embeddings), and syntactic diversity (Weisfeiler-Lehman graph kernel on dependency trees). Perplexity was monitored to ensure models remained aligned with their training data.

## Key Results
- Consistent decrease in lexical diversity across all three tasks, measured by TTR, Distinct-n, and Self-BLEU metrics
- Syntactic diversity declined more severely than semantic diversity across all tasks
- The decline in diversity was most pronounced for the story generation task, which required the highest level of creativity
- The diversity reduction trend persisted across all six recursive iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive training on synthetic text leads to compounding distributional drift in model outputs
- Mechanism: Each generation of model is trained on synthetic data from the previous model, which already exhibits reduced diversity. This creates a feedback loop where the training distribution increasingly diverges from the original human-written data, amplifying losses in lexical, syntactic, and semantic diversity over successive iterations
- Core assumption: The synthetic data generation process introduces systematic biases that compound over recursive training cycles
- Evidence anchors:
  - [abstract] "diverging from the usual emphasis on performance metrics, we focus on the impact of this training methodology on linguistic diversity, especially when conducted recursively over time"
  - [section 3.1] "we simulate the process of recursively training LLMs on predecessor-generated text, under a finetuning setting... each iteration begins with a new instance of the base model"
  - [corpus] Weak evidence - corpus mentions related work on model collapse but no direct evidence for linguistic diversity decay specifically
- Break condition: If diversity metrics plateau or increase, indicating the model has reached a stable distribution that doesn't continue degrading

### Mechanism 2
- Claim: "High entropy" tasks experience faster diversity decline due to their creative nature
- Mechanism: Tasks requiring more creativity (like story generation) have a wider space of possible valid outputs. When models are trained on increasingly homogeneous synthetic data, they lose access to this creative variation more quickly than constrained tasks like summarization, where valid outputs are more narrowly defined
- Core assumption: Creative tasks have inherently higher linguistic diversity requirements and are more sensitive to distributional shifts in training data
- Evidence anchors:
  - [abstract] "our findings reveal a consistent decrease in the diversity of the model outputs through successive iterations, especially remarkable for tasks demanding high levels of creativity"
  - [section 5] "We deliberately select three language generation tasks of varying 'entropy', which is reflected by the amount and nature of given context, i.e., constraint"
  - [corpus] Weak evidence - corpus mentions creativity in story generation but no direct evidence linking entropy to diversity decay rate
- Break condition: If diversity decline rate becomes similar across all task types, indicating the degradation mechanism is independent of task constraints

### Mechanism 3
- Claim: Syntactic diversity declines more severely than semantic diversity during recursive training
- Mechanism: Syntactic structures are more implicit and harder for models to learn from synthetic data compared to semantic content. As diversity decreases, models default to simpler, more common syntactic patterns while maintaining some semantic variation, leading to disproportionate syntactic degradation
- Core assumption: Syntactic diversity is more fragile than semantic diversity when exposed to synthetic training data
- Evidence anchors:
  - [abstract] "Our research introduces the first automatic metric to quantify syntactic diversity"
  - [section 6] "We notice that syntactic diversity consistently decreases across all three tasks, comparable to the decline in lexical diversity and to a greater extent than in semantic diversity"
  - [corpus] Weak evidence - corpus mentions syntactic diversity metrics but no direct evidence of differential decline rates
- Break condition: If semantic diversity begins declining at the same rate as syntactic diversity, indicating the mechanism is not task-specific

## Foundational Learning

- Concept: Lexical diversity metrics (TTR, Distinct-n, Self-BLEU)
  - Why needed here: These metrics provide quantitative measures of vocabulary variety and repetition in generated text, essential for detecting diversity loss
  - Quick check question: What is the relationship between TTR values and text length, and how does the paper address this issue?

- Concept: Syntactic diversity through graph embeddings
  - Why needed here: Traditional diversity metrics don't capture structural variation in sentence construction, which is crucial for understanding how recursive training affects language richness
  - Quick check question: How does the Weisfeiler-Lehman graph kernel transform dependency trees into comparable vector representations?

- Concept: Recursive training simulation methodology
  - Why needed here: Understanding how the paper models the real-world scenario of models training on their own synthetic outputs is key to interpreting the results and their implications
  - Quick check question: Why does the paper use a new instance of the base model at each iteration rather than continuing training the same model?

## Architecture Onboarding

- Component map: Human-written training data -> Model generation -> Synthetic training data -> Recursive fine-tuning iterations
- Critical path:
  1. Load human-written training data for task
  2. Fine-tune base model to create Model (1)
  3. Generate synthetic data using Model (1)
  4. Fine-tune new base model instance on synthetic data to create Model (2)
  5. Repeat steps 3-4 for remaining iterations
  6. Evaluate all models on test set using diversity metrics
- Design tradeoffs:
  - Using new base model instances vs. continuing training same model: Simpler isolation of recursive effects but requires more compute
  - Fixed-length text truncation for TTR: Ensures fair comparison but may cut off longer, more diverse examples
  - Nucleus sampling parameters: Different settings per task optimize for task-specific quality but make cross-task comparisons harder
- Failure signatures:
  - Perplexity values outside acceptable range: Indicates models aren't learning properly from training data
  - Inconsistent diversity metric trends: Suggests bugs in metric calculation or generation process
  - Synthetic data quality issues: Could cause unexpected model behavior or training instability
- First 3 experiments:
  1. Run single iteration on news summarization with default parameters, verify diversity metrics match expected ranges
  2. Compare perplexity and diversity metrics between human-written and Model (1) outputs to establish baseline degradation
  3. Run full 6-iteration recursive training on scientific abstract generation, verify syntactic diversity decline follows expected pattern

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications arise from the findings:

- How does the decline in linguistic diversity caused by training on synthetic data vary based on the size of the language model?
- Can the decline in linguistic diversity be mitigated by incorporating techniques like retrieval augmentation or model ensembles during training?
- Does the decline in linguistic diversity impact the performance of language models on downstream tasks?

## Limitations
- Findings based on recursive fine-tuning with OPT-350M on three specific tasks with relatively small-scale synthetic data generation
- Synthetic data generated using greedy decoding, which may not accurately represent diversity loss with more sophisticated generation strategies
- Does not explore potential mitigation strategies such as data augmentation, regularization techniques, or hybrid training approaches

## Confidence
**High Confidence**: The observed decline in lexical diversity across all three tasks is well-supported by multiple metrics (TTR, Distinct-n, Self-BLEU) and shows consistent patterns across iterations. The syntactic diversity findings are similarly robust, with clear evidence of greater decline compared to semantic diversity.

**Medium Confidence**: The claim about creativity-driven differences in diversity decline is plausible given the task design, but the evidence is primarily correlational rather than causal. The relationship between task "entropy" and diversity decay rate could be influenced by other factors not controlled for in the study.

**Low Confidence**: The specific mechanisms proposed for why syntactic diversity declines more severely than semantic diversity remain speculative. While the data shows differential rates of decline, the paper doesn't provide strong evidence for why this occurs or whether it's an inherent property of language models or an artifact of the specific experimental setup.

## Next Checks
1. **Cross-model validation**: Replicate the experiments using different base model architectures (e.g., GPT-2, LLaMA) to determine whether the diversity decline pattern is specific to OPT-350M or a general phenomenon across language models.

2. **Generation strategy comparison**: Test whether the diversity decline pattern persists when using different synthetic data generation strategies (e.g., beam search, temperature sampling, nucleus sampling with varying parameters) to isolate whether greedy decoding is the primary driver of the observed effects.

3. **Hybrid training experiment**: Implement a training regime that mixes synthetic and human-written data at each iteration to test whether diversity decline can be mitigated while maintaining model performance, providing insights into practical solutions for the identified problem.