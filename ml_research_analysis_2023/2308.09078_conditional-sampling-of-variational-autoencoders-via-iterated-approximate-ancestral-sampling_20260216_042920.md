---
ver: rpa2
title: Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral
  Sampling
arxiv_id: '2308.09078'
source_url: https://arxiv.org/abs/2308.09078
tags:
- xobs
- xmis
- distribution
- ac-mwg
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of conditional sampling from
  variational autoencoders (VAEs), a critical task for applications like missing data
  imputation. The authors identify limitations of existing methods, particularly the
  Metropolis-within-Gibbs (MWG) sampler, which can get "stuck" due to the structured
  latent space of VAEs.
---

# Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling

## Quick Facts
- arXiv ID: 2308.09078
- Source URL: https://arxiv.org/abs/2308.09078
- Reference count: 40
- Key outcome: Proposed AC-MWG and LAIR methods significantly outperform existing methods for conditional sampling from VAEs, with improvements in FID and Sinkhorn distance metrics

## Executive Summary
This paper addresses the critical challenge of conditional sampling from variational autoencoders (VAEs), particularly for missing data imputation tasks. The authors identify fundamental limitations in existing Metropolis-within-Gibbs (MWG) approaches that can get "stuck" in the structured latent space of VAEs due to strong relationships between latents and observables. To overcome these issues, they propose two novel methods: Adaptive Collapsed-MWG (AC-MWG) and Latent-Adaptive Importance Resampling (LAIR). Both methods are designed to improve exploration and mixing in the latent space, with experimental results demonstrating significant performance improvements over existing methods on both synthetic and real-world datasets.

## Method Summary
The paper proposes two novel methods for conditional sampling from VAEs: Adaptive Collapsed-MWG (AC-MWG) and Latent-Adaptive Importance Resampling (LAIR). AC-MWG modifies the standard Metropolis-within-Gibbs algorithm by introducing a mixture proposal distribution that combines the variational encoder with the prior distribution, and collapses the target distribution by marginalizing missing variables. LAIR uses adaptive importance resampling with a mixture proposal, employing stratified sampling to reduce variance and prevent particle collapse through replenishing prior components. Both methods use LAIR initialization for improved performance, with AC-MWG providing MCMC convergence guarantees and LAIR offering computational efficiency through importance sampling.

## Key Results
- AC-MWG and LAIR significantly outperform existing methods including MWG and pseudo-Gibbs on synthetic and real-world datasets
- The proposed methods achieve better sampling accuracy as measured by Fréchet Inception Distance (FID) and Sinkhorn distance metrics
- Experiments demonstrate that initialization strategy significantly impacts performance, with LAIR initialization outperforming random initialization
- The methods show robustness across different missingness patterns and dataset types, including synthetic 2D VAEs, MNIST, and UCI datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The mixture proposal distribution in AC-MWG improves exploration by balancing exploitation and exploration in the latent space.
- Mechanism: The mixture proposal combines the variational encoder distribution (exploitation) with the prior distribution (exploration) to avoid getting stuck in local modes. This addresses pitfall II where the encoder's proposal distribution is too narrow to explore distant modes.
- Core assumption: The prior distribution p(z) is sufficiently broad to cover the relevant latent space regions.
- Evidence anchors:
  - [abstract]: "we propose two original methods that address these pitfalls"
  - [section 4.1]: "introduce a prior–variational mixture proposal"
  - [corpus]: Weak evidence - no direct citations about mixture proposals in VAEs

### Mechanism 2
- Claim: Marginalizing missing variables in the acceptance probability improves mixing between nearby modes.
- Mechanism: By collapsing the target distribution from p(z|x_obs, x_mis) to p(z|x_obs), the sampler no longer rejects proposals based on disagreement with the current imputation value. This addresses pitfall I where MWG gets stuck due to strong relationships between latents and visibles.
- Core assumption: The marginal likelihood p(x_obs|z) is tractable to compute.
- Evidence anchors:
  - [abstract]: "collapse the target distribution"
  - [section 4.1]: "Marginalising the missing variables x_mis out of the likelihood"
  - [corpus]: Weak evidence - no direct citations about collapsed Metropolis-within-Gibbs in VAEs

### Mechanism 3
- Claim: Adaptive importance resampling in LAIR provides more efficient exploration than MCMC methods.
- Mechanism: LAIR uses a mixture proposal with replenishing prior components to prevent particle collapse, and resamples from the full sequence of proposals to reduce bias. This addresses pitfall III where poor initialization can cause sampling of wrong modes.
- Core assumption: The mixture proposal components are simple distributions (e.g., diagonal Gaussians) making weight evaluation tractable.
- Evidence anchors:
  - [abstract]: "alternative sampling method, called latent-adaptive importance resampling (LAIR)"
  - [section 4.2]: "construct an adaptive proposal distribution q_t(z|x_obs)"
  - [corpus]: Weak evidence - no direct citations about adaptive importance resampling in VAEs

## Foundational Learning

- Concept: Markov Chain Monte Carlo (MCMC) convergence properties
  - Why needed here: Understanding why MWG can get stuck requires knowledge of MCMC mixing properties
  - Quick check question: What conditions must be satisfied for an MCMC sampler to converge to its target distribution?

- Concept: Importance sampling and variance reduction techniques
  - Why needed here: LAIR relies on importance resampling with stratified sampling for variance reduction
  - Quick check question: How does stratified sampling reduce variance compared to simple random sampling in importance sampling?

- Concept: Variational inference and the amortization gap
  - Why needed here: The paper relies on the encoder distribution as a proposal, which approximates but doesn't equal the true posterior
  - Quick check question: What are the main sources of discrepancy between the variational posterior and the true posterior in VAEs?

## Architecture Onboarding

- Component map:
  - VAE model (p(x,z) = p(x|z)p(z)) -> Encoder distribution q(z|x) (variational posterior) -> Decoder distribution p(x|z) -> Proposal distributions (mixture of encoder and prior) -> History management for AC-MWG -> Particle management for LAIR

- Critical path:
  1. Initialize with either random samples or LAIR output
  2. Generate proposal using mixture distribution
  3. Compute acceptance probability (MWG) or importance weights (LAIR)
  4. Accept/reject or resample
  5. Update imputation and history/particles
  6. Repeat until convergence

- Design tradeoffs:
  - AC-MWG vs LAIR: MCMC convergence guarantees vs adaptive importance sampling efficiency
  - Mixture coefficient ϵ: exploration vs exploitation balance
  - Number of particles K in LAIR: accuracy vs computational cost
  - History window size in AC-MWG: adaptation vs computational overhead

- Failure signatures:
  - Poor mixing: sampler gets stuck in local modes (visible as poor FID scores)
  - Particle collapse: LAIR particles concentrate in few regions (visible as weight degeneracy)
  - High rejection rate: AC-MWG proposals frequently rejected (visible as low acceptance probability)

- First 3 experiments:
  1. Verify that mixture proposal improves exploration on synthetic 2D VAE (should match fig. 2 and fig. 3)
  2. Test initialization sensitivity by comparing MWG with different initialization strategies
  3. Evaluate computational cost scaling with number of particles K in LAIR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed AC-MWG and LAIR methods scale to higher-dimensional latent spaces in VAEs?
- Basis in paper: [explicit] The paper mentions that the proposed methods may not work well in high dimensions due to the "curse of dimensionality" and suggests exploring alternative exploration strategies like using annealed versions of the variational encoder distribution.
- Why unresolved: The paper only demonstrates the methods on moderate-dimensional latent spaces and acknowledges that scaling to higher dimensions is an open challenge.
- What evidence would resolve it: Empirical results showing the performance of AC-MWG and LAIR on VAEs with high-dimensional latent spaces (e.g., >100 dimensions) would provide evidence of their scalability. Additionally, theoretical analysis of the methods' convergence properties in high dimensions would be valuable.

### Open Question 2
- Question: What are the optimal values for the hyperparameters in the proposed methods (e.g., the mixture probability ϵ in AC-MWG, the number of particles K and replenishing components R in LAIR)?
- Basis in paper: [explicit] The paper uses specific values for these hyperparameters (e.g., ϵ=0.01 in AC-MWG, K=19 and R=1 in LAIR) but does not provide a systematic study of their impact on performance.
- Why unresolved: The choice of hyperparameters can significantly affect the performance of the methods, and the optimal values may depend on the specific VAE model and data set.
- What evidence would resolve it: A comprehensive study of the sensitivity of AC-MWG and LAIR to their hyperparameters, including their impact on convergence speed, mixing, and sampling accuracy, would provide insights into their optimal values.

### Open Question 3
- Question: How do the proposed methods compare to other conditional sampling techniques for VAEs, such as Hamiltonian Monte Carlo (HMC) or normalizing flows?
- Basis in paper: [inferred] The paper focuses on improving Gibbs-like samplers for VAEs but does not compare the proposed methods to other conditional sampling techniques.
- Why unresolved: There are various conditional sampling techniques available for VAEs, and it is unclear how the proposed methods compare in terms of accuracy, computational efficiency, and scalability.
- What evidence would resolve it: Empirical comparisons of AC-MWG and LAIR to other conditional sampling techniques on a range of VAE models and data sets would provide insights into their relative strengths and weaknesses.

## Limitations

- The proposed methods may not scale well to high-dimensional latent spaces due to the curse of dimensionality, as acknowledged by the authors
- The computational complexity of the methods, particularly LAIR with multiple particles, is not thoroughly analyzed
- The theoretical analysis of convergence properties and mixing improvements is primarily empirical rather than rigorous

## Confidence

- High confidence in the empirical demonstration that existing MWG approaches get stuck in VAE latent spaces
- Medium confidence in the effectiveness of the proposed AC-MWG and LAIR methods, based on strong experimental results
- Low confidence in theoretical claims about convergence rates and mixing improvements, as these are primarily justified through empirical evaluation

## Next Checks

1. Conduct ablation studies to isolate the contributions of collapsing the target distribution versus using mixture proposals in AC-MWG
2. Test the methods on datasets with different missingness mechanisms (MCAR, MAR, MNAR) to evaluate robustness
3. Perform runtime analysis to quantify the computational overhead of AC-MWG and LAIR compared to baseline methods, particularly for larger datasets and higher-dimensional latent spaces