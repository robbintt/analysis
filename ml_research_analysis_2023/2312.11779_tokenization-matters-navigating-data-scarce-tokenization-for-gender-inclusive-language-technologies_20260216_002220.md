---
ver: rpa2
title: 'Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive
  Language Technologies'
arxiv_id: '2312.11779'
source_url: https://arxiv.org/abs/2312.11779
tags:
- pronoun
- pronouns
- tokenization
- neopronouns
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that neopronoun misgendering in large language
  models is influenced by Byte-Pair Encoding tokenization. Specifically, the overfragmentation
  of neopronouns due to their rarity in training corpora causes them to be represented
  as multiple subword tokens, unlike binary pronouns which are single tokens.
---

# Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies

## Quick Facts
- arXiv ID: 2312.11779
- Source URL: https://arxiv.org/abs/2312.11779
- Reference count: 38
- Binary pronouns are single tokens, neopronouns are fragmented subword tokens in BPE, causing misgendering

## Executive Summary
This paper identifies tokenization as a root cause of neopronoun misgendering in large language models. Byte-Pair Encoding (BPE) overfragments neopronouns due to their rarity in training data, breaking them into multiple subword tokens while binary pronouns remain single tokens. This disparate tokenization structure prevents LLMs from learning unified grammatical behavior for neopronouns. The paper proposes pronoun tokenization parity (PTP), which represents neopronouns as single tokens like binary pronouns, combined with lexical fine-tuning that leverages the LLM's existing pronoun knowledge. This approach improves neopronoun accuracy from 14.1% to 58.4% compared to standard finetuning.

## Method Summary
The method involves two key techniques: (1) PTP adds special tokens to the tokenizer vocabulary for each neopronoun case, creating unified token representations, and (2) lexical fine-tuning updates only the embedding layer while freezing transformer weights, leveraging pre-existing binary pronoun grammatical knowledge. The approach uses the Pythia model suite with Wikibios dataset (462,345 examples filtered for binary pronouns) and evaluates on the MISGENDERED dataset using three metrics: pronoun consistency, case agreement, and syntactic perturbation error. The method preserves binary pronoun performance while significantly improving neopronoun handling.

## Key Results
- Neopronoun accuracy improves from 14.1% to 58.4% compared to standard finetuning
- PTP creates unified token representations for neopronouns, matching binary pronoun tokenization
- Lexical fine-tuning leverages existing grammatical knowledge without degrading binary pronoun performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenization structure directly influences syntactic learning for neopronouns
- Mechanism: BPE fragmentation breaks functional morphemes (pronoun cases) into separate tokens, preventing LLMs from learning unified grammatical behavior
- Core assumption: Pronoun consistency requires recognizing neopronouns as single functional units like binary pronouns
- Evidence anchors:
  - [abstract] "Unlike binary pronouns, BPE overfragments neopronouns, a direct consequence of data scarcity during tokenizer training"
  - [section 3] "This out-of-vocabulary (OOV) tokenization subsequently forces the LLM to rely on granular subword tokens to learn the neopronoun's representation"
  - [section 3] "Wang et al. [2019] highlight OOVs' detrimental impact on part-of-speech (POS) discernment, resulting in high error rates for OOV words"
- Break condition: If neopronouns are fragmented even after PTP implementation, indicating the special token approach fails

### Mechanism 2
- Claim: Preserving functional structure improves case agreement through lexical embedding alignment
- Mechanism: PTP creates single tokens for neopronouns, allowing lexical fine-tuning to bootstrap embeddings using existing binary pronoun grammatical knowledge
- Core assumption: Binary pronouns and neopronouns follow identical grammatical rules, enabling transfer
- Evidence anchors:
  - [section 4.2] "PTP centers aligning, or establishing parity, between neopronoun and binary pronoun tokenization"
  - [section 5.2] "Inspired by cross-lingual transfer techniques... we experiment with finetuning only Pythia's lexical embedding layer"
  - [section 5.2] "Unlike Artetxe et al. [2019b], we avoid training the transformer weights after freezing lexical embeddings since the new tokens already conform to English grammar"
- Break condition: If case agreement doesn't improve despite PTP and lexical fine-tuning, suggesting grammatical knowledge transfer isn't effective

### Mechanism 3
- Claim: Special tokens prevent contextual confusion from subword fragments
- Mechanism: Single-token representation eliminates competing contexts from common subwords, improving syntactic coherence
- Core assumption: Fragmented tokens introduce ambiguous contexts that confuse the transformer
- Evidence anchors:
  - [section 3] "Linguistic ambiguity may arise when splitting xem into the tokens x and em, reflected in challenges to use these subword units in their appropriate context"
  - [section 3] "Since frequency in text plays a significant role in neopronoun representation both for the tokenizer and the LLM's context formation, contextual complexity may be introduced"
  - [section 4] "By aligning this approach with binary pronoun tokenization, we hypothesize that this will improve an LLM's grammatical representation of neopronouns"
- Break condition: If syntactic error metrics don't decrease with PTP, indicating contextual confusion persists

## Foundational Learning

- Concept: Byte-Pair Encoding tokenization mechanics
  - Why needed here: Understanding BPE fragmentation is crucial to diagnosing neopronoun representation issues
  - Quick check question: How does BPE decide when to merge tokens, and why would this cause neopronouns to fragment?

- Concept: Part-of-speech learning in transformer architectures
  - Why needed here: Pronoun consistency depends on POS recognition, which is disrupted by tokenization
  - Quick check question: How do transformers learn syntactic relationships, and how might tokenization interfere with this process?

- Concept: Cross-lingual transfer learning techniques
  - Why needed here: Lexical fine-tuning exploits pre-existing grammatical knowledge similar to cross-lingual methods
  - Quick check question: What makes lexical embeddings transferable across related linguistic concepts?

## Architecture Onboarding

- Component map:
  Tokenizer (BPE with special tokens) → Embedding layer (lexical + transformer) → Transformer blocks → Output layer
  - PTP adds special tokens to tokenizer vocabulary and extends embedding matrix
  - Lexical fine-tuning freezes transformer weights while updating only embedding matrix

- Critical path:
  1. Tokenization → 2. Embedding lookup → 3. Transformer processing → 4. Output prediction
  - PTP affects step 1 by creating unified tokens
  - Lexical fine-tuning affects step 2 by updating embeddings while preserving transformer behavior

- Design tradeoffs:
  - PTP vs. vocabulary expansion: PTP is minimal change vs. larger vocabulary
  - Full vs. lexical fine-tuning: Resource efficiency vs. comprehensive adaptation
  - Special token placement: Requires careful handling of space characters to prevent tokenization errors

- Failure signatures:
  - Fragmented tokenization despite PTP (space character handling error)
  - No improvement in pronoun consistency (grammatical knowledge not transferable)
  - Binary pronoun degradation (overly aggressive lexical adaptation)

- First 3 experiments:
  1. Verify PTP tokenization produces single tokens for neopronouns with correct space handling
  2. Test lexical fine-tuning convergence and embedding updates without transformer changes
  3. Validate pronoun consistency metrics improve while case agreement remains stable

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but the methodology raises several important considerations about generalizability, scalability, and broader applications.

## Limitations
- The approach is currently validated only on English neopronouns and may not generalize to other languages
- Long-term stability and performance across extended usage or domain shifts remains untested
- The effectiveness across different tokenization algorithms (WordPiece, SentencePiece) hasn't been explored

## Confidence
- **High Confidence**: The core finding that BPE fragmentation negatively impacts neopronoun consistency is well-supported by both theoretical analysis and empirical results
- **Medium Confidence**: The PTP methodology and implementation details are clearly described and reproducible
- **Low Confidence**: The claim that binary pronoun grammatical knowledge transfers effectively to neopronouns relies on an assumption about grammatical similarity that hasn't been empirically validated beyond task performance

## Next Checks
1. **Cross-tokenizer validation**: Test PTP with different tokenization schemes (WordPiece, SentencePiece) to verify the approach isn't BPE-specific
2. **Adversarial pronoun testing**: Create datasets with systematically varied pronoun contexts to stress-test the consistency improvements
3. **Temporal stability analysis**: Conduct longitudinal studies measuring pronoun consistency across multiple finetuning checkpoints and with domain-shifted data