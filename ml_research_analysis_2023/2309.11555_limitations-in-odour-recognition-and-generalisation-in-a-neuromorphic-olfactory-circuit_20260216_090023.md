---
ver: rpa2
title: Limitations in odour recognition and generalisation in a neuromorphic olfactory
  circuit
arxiv_id: '2309.11555'
source_url: https://arxiv.org/abs/2309.11555
tags:
- odour
- data
- training
- neuromorphic
- sensor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies limitations in a neuromorphic olfactory circuit
  for odour recognition. The dataset used contains sensor drift and lacks randomisation,
  making it unsuitable for odour identification benchmarks.
---

# Limitations in odour recognition and generalisation in a neuromorphic olfactory circuit

## Quick Facts
- arXiv ID: 2309.11555
- Source URL: https://arxiv.org/abs/2309.11555
- Authors: 
- Reference count: 27
- Primary result: A neuromorphic olfactory circuit fails to generalize across different recordings of the same gas and is outperformed by a simple hash table approach

## Executive Summary
This paper identifies fundamental limitations in a neuromorphic olfactory circuit designed for odor recognition. The study reveals that the model's apparent success is largely an artifact of sensor drift in the dataset, which creates systematic patterns that can be exploited without actual odor presence. The model fails to recognize the same gas across separate recordings, even when baseline subtraction is applied. A simple hash table approach matches or exceeds the neuromorphic model's performance in both accuracy and runtime, calling into question the practical utility of the neuromorphic implementation for real-world odor identification tasks.

## Method Summary
The study evaluates a neuromorphic network inspired by the external plexiform layer of the olfactory bulb, which uses spike-time dependent plasticity to learn from 72 MOx gas sensors. The model is trained on one repetition of 10 different gases and tested on separate repetitions with 60% impulse noise occlusion. Performance is measured using Jaccard similarity coefficients and compared against a simple hash table approach that stores training samples directly. The analysis focuses on the model's ability to generalize across different recordings of the same gas stimulus and its robustness to sensor drift effects.

## Key Results
- Recognition accuracy is an artifact of sensor drift, with high scores achievable even without gas presence
- The model fails to recognize gases across separate recordings, even with baseline subtraction
- A simple hash table approach matches or exceeds the neuromorphic model's performance in accuracy and runtime

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High Jaccard similarity scores can be achieved without gas presence due to sensor drift
- Mechanism: Sensor drift causes the baseline resistance to change over time, creating systematic differences between measurements taken at different times. When training and testing use samples from the same time period, these drift patterns can be exploited to achieve high similarity scores even without actual gas presence.
- Core assumption: The dataset contains systematic temporal patterns that correlate with gas identity but are actually artifacts of sensor drift
- Evidence anchors:
  - [section] "Paradoxically, the same level of 'recognition' of Toluene can be obtained in the absence of gas, using samples obtained at t = 15s, before the release of odour into the wind tunnel at t = 20s"
  - [section] "MOx sensors are highly prone to sensor drift, causing short- and long-term fluctuations in the sensors' baseline conductance and their responsiveness"
- Break condition: Using baseline-subtracted data or randomized measurement protocols that eliminate temporal correlations

### Mechanism 2
- Claim: The model fails to generalize across different recordings of the same gas stimulus
- Mechanism: The neuromorphic network learns specific representations tied to particular sensor drift patterns and measurement conditions. When tested on separate recordings with different drift characteristics, the learned representations no longer match, causing recognition failure.
- Core assumption: The model's learning mechanism is overly sensitive to measurement-specific artifacts rather than the underlying gas signature
- Evidence anchors:
  - [section] "gas identity could not be recognised in occluded samples" when using separate repetitions for training and testing
  - [section] "Recognition scores were further reduced when subtracting the baseline from training and testing data"
- Break condition: If the model could learn drift-invariant representations or if drift were properly compensated

### Mechanism 3
- Claim: A simple hash table approach can match or exceed the neuromorphic model's performance
- Mechanism: By storing training samples directly and comparing test samples against them using simple overlap metrics, the hash table approach effectively memorizes the training data without attempting generalization. This works well when test samples are corrupted versions of training samples.
- Core assumption: The task of recognizing corrupted training samples doesn't require complex learning mechanisms
- Evidence anchors:
  - [section] "our implementation employs a concise one-shot approach, consisting of just 8 lines of Python code, yet it matches or surpasses the suggested EPL network in recognition accuracy"
  - [section] "By storing the training samples in a hash table...one can estimate the most likely class"
- Break condition: When test samples differ significantly from training data or require true generalization

## Foundational Learning

- Concept: Sensor drift in metal oxide gas sensors
  - Why needed here: Understanding how sensor drift affects measurements is crucial for interpreting the dataset limitations and why the model appears to work well on flawed data
  - Quick check question: What is the primary cause of sensor drift in MOx sensors and how does it affect baseline resistance measurements?

- Concept: Pattern recognition generalization
  - Why needed here: The paper's core criticism is that the model fails to generalize beyond training samples, making understanding generalization principles essential
  - Quick check question: What is the difference between memorization and generalization in machine learning, and why is generalization important for real-world applications?

- Concept: Jaccard similarity coefficient
  - Why needed here: This metric is used to evaluate the model's performance, and understanding its properties helps interpret why it might be misleading in this context
  - Quick check question: How does the Jaccard similarity coefficient measure similarity between binary vectors, and what are its limitations for continuous sensor data?

## Architecture Onboarding

- Component map: Input layer (72 MOx sensors) -> Processing layer (neuromorphic network) -> Output (Jaccard similarity scores)
- Critical path:
  1. Sensor data acquisition and preprocessing (baseline subtraction optional)
  2. Network training on clean gas presentations
  3. Testing on occluded samples (same or different recordings)
  4. Jaccard similarity computation and thresholding
- Design tradeoffs:
  - Biological realism vs. computational efficiency
  - Learning speed (online vs. batch learning)
  - Sensitivity to measurement conditions vs. robustness to drift
  - Complex neuromorphic implementation vs. simple algorithmic approaches
- Failure signatures:
  - High similarity scores without actual gas presence
  - Failure to recognize the same gas across different recordings
  - Performance degradation when baseline subtraction is applied
  - Hash table approach matching or exceeding performance
- First 3 experiments:
  1. Replicate the baseline experiment: Train on gas presentations and test on the same samples with 60% occlusion to verify the original results
  2. Test on different recordings: Use separate repetitions for training and testing to assess generalization capability
  3. Baseline subtraction comparison: Compare performance with and without baseline subtraction to evaluate drift compensation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the neuromorphic model perform on a dataset with proper randomization and baseline subtraction to mitigate sensor drift effects?
- Basis in paper: [explicit] The paper demonstrates that the current dataset suffers from sensor drift and lacks randomization, leading to spurious recognition results. The authors show that baseline subtraction improves results but the model still fails to generalize across different recordings.
- Why unresolved: The current dataset's limitations prevent accurate assessment of the model's true performance on odour identification tasks. The authors explicitly state that the dataset is "largely unsuitable for classification tasks" due to drift effects.
- What evidence would resolve it: Testing the neuromorphic model on a properly randomized dataset with baseline subtraction, comparing performance metrics against the hash table approach, and demonstrating generalization across different recordings of the same gas.

### Open Question 2
- Question: What specific architectural modifications could improve the neuromorphic model's generalization capability beyond restoring learned data samples?
- Basis in paper: [explicit] The paper demonstrates that the model fails to recognize gases across separate recordings even with baseline subtraction, and that a simple hash table approach matches or exceeds its performance.
- Why unresolved: The current model architecture appears limited to pattern restoration rather than true generalization. The authors conclude that "a validation of the model that goes beyond restoring a learned data sample remains to be shown."
- What evidence would resolve it: Implementing and testing architectural modifications such as additional layers, different plasticity rules, or ensemble methods, then evaluating performance on unseen recordings of learned gases and novel gases.

### Open Question 3
- Question: How does the computational efficiency of the neuromorphic implementation compare to conventional hardware when both are solving the same odorant identification task with proper generalization?
- Basis in paper: [explicit] The authors compare execution times between CPU, Loihi, and hash table implementations, but the hash table approach already matches or exceeds the neuromorphic model's performance.
- Why unresolved: The claimed advantage of neuromorphic computing for power efficiency cannot be properly evaluated when the model fails at the core task. The paper shows that the hash table approach outperforms the neuromorphic model in runtime while achieving similar accuracy.
- What evidence would resolve it: Implementing a properly generalized odorant identification algorithm on neuromorphic hardware and comparing its power consumption and runtime against optimized conventional implementations on equivalent tasks with real-world generalization requirements.

## Limitations
- The dataset contains sensor drift and lacks randomization, making it unsuitable for odor identification benchmarks
- The model fails to generalize across different recordings of the same gas stimulus
- A simple hash table approach achieves similar or better accuracy and runtime, questioning the model's superiority

## Confidence
- High: The paper's identification of sensor drift artifacts in the dataset is well-supported
- Medium: The comparison with hash table approach demonstrates clear performance limitations
- Low: The analysis is limited to a single dataset and model architecture

## Next Checks
1. Conduct controlled experiments using baseline-subtracted data to isolate the effects of sensor drift on recognition accuracy
2. Test the model's performance on a more robust dataset with proper randomization and baseline correction to assess its true generalization capabilities
3. Compare the neuromorphic model's performance against other baseline methods, including simple nearest-neighbor approaches, to establish its relative advantage