---
ver: rpa2
title: 'SODFormer: Streaming Object Detection with Transformer Using Events and Frames'
arxiv_id: '2308.04047'
source_url: https://arxiv.org/abs/2308.04047
tags:
- object
- event
- frames
- detection
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel streaming object detector with Transformer
  (SODFormer) that integrates events and frames to continuously detect objects in
  an asynchronous manner. The key innovation is a spatiotemporal Transformer architecture
  that leverages rich temporal cues from two heterogeneous visual streams, combined
  with an asynchronous attention-based fusion module that eliminates unimodal degradation
  and overcomes the limited inference frequency of synchronized frame-based fusion
  strategies.
---

# SODFormer: Streaming Object Detection with Transformer Using Events and Frames

## Quick Facts
- arXiv ID: 2308.04047
- Source URL: https://arxiv.org/abs/2308.04047
- Reference count: 40
- Primary result: Achieves 50.4% AP50 and 31.3% mAP on PKU-DAVIS-SOD dataset

## Executive Summary
This paper introduces SODFormer, a streaming object detection framework that combines events and frames through a spatiotemporal Transformer architecture. The system leverages the asynchronous nature of event cameras to achieve high temporal resolution and dynamic range while using frames for accurate spatial information. A key innovation is the asynchronous attention-based fusion module that eliminates unimodal degradation and overcomes the limited inference frequency of synchronized fusion strategies. The method demonstrates significant performance improvements on a newly created PKU-DAVIS-SOD dataset, particularly in challenging scenarios like high-speed motion blur and low-light conditions.

## Method Summary
SODFormer processes continuous streams of events and frames through a unified transformer framework. Events are converted to 2D image-like embeddings using kernel functions, then both modalities are processed through spatial transformer encoders using deformable attention. A temporal transformer encoder aggregates features across time using temporal deformable multi-head self-attention. The asynchronous attention-based fusion module combines event and frame features using pixel-wise attention weights that suppress degraded regions. Finally, a temporal transformer decoder generates bounding boxes directly from object queries and fused features, enabling end-to-end sequence prediction without post-processing.

## Key Results
- Achieves 50.4% AP50 and 31.3% mAP on the test set of PKU-DAVIS-SOD dataset
- Demonstrates superior performance in high-speed motion blur and low-light conditions
- Outperforms state-of-the-art methods by significant margins on the new benchmark dataset

## Why This Works (Mechanism)

### Mechanism 1: Asynchronous Attention-Based Fusion
The fusion module computes pixel-wise attention weights between event and frame modalities, where the sum of attention scores represents degradation degree. Weaker associations indicate degraded regions, allowing the module to suppress noise while preserving useful information.

### Mechanism 2: Temporal Transformer with Deformable Attention
TDMSA aggregates multiple spatial feature maps across time, attending to local sampling points in spatial domain while leveraging temporal dependencies. This captures motion patterns and object trajectories invisible in single frames.

### Mechanism 3: End-to-End Sequence Prediction
The spatiotemporal Transformer architecture enables direct bounding box generation from object queries and fused features, eliminating traditional detection pipelines and post-processing while providing continuous output.

## Foundational Learning

- **Event cameras and asynchronous output**: Understanding event cameras' microsecond resolution and brightness-change triggering is essential for grasping SODFormer's high temporal resolution advantage. Quick check: How does an event camera's output fundamentally differ from conventional frame-based cameras?

- **Transformer architecture and attention mechanisms**: The entire framework relies on transformer components for spatial and temporal processing. Quick check: What distinguishes standard self-attention from deformable attention in object detection?

- **Multimodal fusion strategies**: Traditional fusion methods fail to eliminate unimodal degradation, motivating SODFormer's asynchronous approach. Quick check: What limitations do synchronized fusion strategies have when combining event and frame data?

## Architecture Onboarding

- **Component map**: Event Representation → STE → TDTE → Fusion Module → TDTD → Output
- **Critical path**: Events/frames flow through spatial encoders, temporal aggregation, asynchronous fusion, then temporal decoder to generate detections
- **Design tradeoffs**: Accuracy vs. speed (temporal window size), modality complementarity vs. computational complexity, synchronous vs. asynchronous fusion
- **Failure signatures**: Poor performance in static scenes (events), motion blur (frames), or when fusion fails to suppress noise
- **First 3 experiments**:
  1. Test temporal aggregation size (1, 3, 5, 9, 13) to find optimal accuracy-speed tradeoff
  2. Compare fusion strategies (averaging, concatenation, attention-based) to validate the novel fusion approach
  3. Evaluate different event representations (images, voxel grids, sigmoid) to confirm framework generality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would SODFormer perform on event-based datasets with different motion types (very slow, very fast, rotational)?
- Basis: The current dataset focuses on driving scenarios and may not capture all motion types
- What evidence would resolve it: Testing on varied motion datasets like TUM or synthetic controlled datasets

### Open Question 2
- Question: What is the impact of using vision transformer backbones (ViT, Swin) instead of ResNet50 for event streams?
- Basis: Authors acknowledge not exploring pre-trained vision transformers for events
- What evidence would resolve it: Comparative experiments with different backbone architectures

### Open Question 3
- Question: How does SODFormer handle occlusions and can explicit occlusion modeling improve performance?
- Basis: Temporal Transformer helps with partial occlusions but lacks dedicated occlusion mechanisms
- What evidence would resolve it: Experiments with explicit occlusion modeling and visibility masks

## Limitations
- Limited ablation studies on individual component contributions (temporal size, fusion strategy, event representation)
- Relatively small dataset size (25 Hz) may not fully capture continuous event stream nature
- Claims of "eliminating unimodal degradation" lack quantitative degradation reduction evidence

## Confidence
- **High**: Architecture description and dataset creation methodology are well-specified with sufficient implementation detail
- **Medium**: Performance improvements are documented but comparative analysis with specific baseline fusion methods is limited
- **Low**: Claim of "eliminating unimodal degradation" lacks direct measurement or demonstration

## Next Checks
1. Ablation study comparing asynchronous attention-based fusion against synchronized methods with explicit degradation metrics
2. Systematic evaluation of detection performance across different temporal aggregation sizes (1, 3, 5, 9, 13 frames)
3. Robustness test across different event representation methods (images, voxel grids, sigmoid functions)