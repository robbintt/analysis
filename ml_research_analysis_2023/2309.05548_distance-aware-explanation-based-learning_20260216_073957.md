---
ver: rpa2
title: Distance-Aware eXplanation Based Learning
arxiv_id: '2309.05548'
source_url: https://arxiv.org/abs/2309.05548
tags:
- explanations
- learning
- explanation
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Distance-Aware eXplanation Based Learning (XBL-D),
  a method that augments loss functions in interactive machine learning by incorporating
  distance-aware explanation loss. The approach penalizes models based on their focus
  on confounding regions, with penalties scaled by the distance between these regions
  and relevant image parts.
---

# Distance-Aware eXplanation Based Learning

## Quick Facts
- arXiv ID: 2309.05548
- Source URL: https://arxiv.org/abs/2309.05548
- Reference count: 36
- Primary result: XBL-D outperforms baseline methods in classification accuracy and explanation quality on decoyed image datasets

## Executive Summary
This paper introduces Distance-Aware eXplanation Based Learning (XBL-D), a method that enhances interactive machine learning by incorporating distance-aware explanation loss into the training objective. The approach penalizes models based on their attention to confounding regions, with penalties scaled by the distance between these regions and relevant image parts. The authors also propose a new interpretability metric, Activation Recall (AR), which measures how well model explanations cover relevant image regions. Experiments on three decoyed image classification tasks demonstrate that XBL-D achieves superior performance compared to baseline methods, improving both classification accuracy and explanation quality.

## Method Summary
XBL-D augments standard loss functions by adding a distance-aware explanation loss that penalizes model attention on confounding regions. The method uses Grad-CAM for generating visual explanations and optimizes two coefficients (λ1 for classification loss, λ2 for explanation loss) using HyperBand in Keras tuner. The total loss combines categorical cross-entropy, explanation loss, and regularization terms. The approach was tested on Decoy Fashion MNIST, Decoy CIFAR-10, and Decoy MS-COCO datasets with 4x4 pixel confounders added to random corners of training images.

## Key Results
- XBL-D achieved classification accuracies of 0.904, 0.843, and 0.938 on Decoy Fashion MNIST, Decoy CIFAR-10, and Decoy MS-COCO respectively
- Outperformed baseline methods including Right for the Right Reasons (RRR) and Human Importance-aware Network Tuning (HINT)
- Generated more accurate explanations with higher Activation Recall (AR) and Activation Precision (AP) scores across varying thresholds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The explanation loss penalizes a model based on the distance between its attention on confounding regions and the relevant regions.
- Mechanism: The loss is computed as the average of the minimum and maximum Euclidean distances between points in the intersection of the model's explanation (Grad-CAM) and the confounding region mask, and the center of gravity of the relevant regions. This means that the penalty decreases as the model's focus moves closer to the relevant regions.
- Core assumption: Grad-CAM explanations accurately reflect the model's focus of attention.
- Evidence anchors:
  - [abstract] "The approach penalizes models based on their focus on confounding regions, with penalties scaled by the distance between these regions and relevant image parts."
  - [section] "Our explanation loss penalizes a learner based on the amount of wrong attention it gives to Acon, with due consideration of this wrong attention's distance from Aobj."
  - [corpus] Weak corpus evidence; only one related paper mentions "distance" in the context of XBL.

### Mechanism 2
- Claim: The proposed method introduces a new interpretability metric, Activation Recall (AR), which measures how much of the relevant parts of test images are considered relevant by the model.
- Mechanism: AR is computed by thresholding the model's explanation (Grad-CAM) and calculating the ratio of the intersection between the thresholded explanation and the relevant region mask to the relevant region mask. This metric focuses on the model's ability to identify the relevant regions rather than just avoiding confounding regions.
- Core assumption: The relevant region mask accurately represents the important parts of the image for classification.
- Evidence anchors:
  - [abstract] "The authors also introduce a new interpretability metric, Activation Recall (AR), which evaluates how well model explanations cover relevant image regions."
  - [section] "AR measures how much of the user annotated relevant image regions were actually considered relevant by a trained model."
  - [corpus] No direct evidence in the corpus; the related papers focus on other aspects of explanation-based learning.

### Mechanism 3
- Claim: The proposed method balances the classification loss and the explanation loss using coefficients λ1 and λ2, which are optimized using HyperBand in Keras tuner.
- Mechanism: The total loss is computed as the sum of the classification loss (LCE), the explanation loss (Lexpl), and a regularization term. The coefficients λ1 and λ2 determine the relative importance of the classification and explanation losses, respectively. By optimizing these coefficients, the method ensures that the model learns to classify correctly while also focusing on the relevant regions and avoiding confounding regions.
- Core assumption: The optimal values of λ1 and λ2 can be found for each dataset.
- Evidence anchors:
  - [section] "We also consider these coefficients as hyper-parameters and tune them to find their optimal values before starting model training with XBL."
  - [section] "Before starting the model refinement using XBL-D, we searched for optimal values of the coefficients of the categorical cross entropy loss (λ1) and explanation loss (λ2) using HyperBand in Keras tuner and we ended up with λ1 = 2.7 and λ2 = 0.1."
  - [corpus] No direct evidence in the corpus; the related papers do not discuss the optimization of loss coefficients.

## Foundational Learning

- Concept: eXplanation Based Learning (XBL)
  - Why needed here: XBL is the interactive learning approach that forms the basis of the proposed method. It allows users to provide feedback on model explanations to refine the model's focus.
  - Quick check question: What is the main goal of XBL, and how does it differ from traditional interactive machine learning approaches?

- Concept: Gradient-weighted Class Activation Mapping (Grad-CAM)
  - Why needed here: Grad-CAM is the saliency-based explanation method used in the proposed method to generate visual explanations of the model's focus of attention.
  - Quick check question: How does Grad-CAM generate visual explanations, and what are its advantages over other explanation methods?

- Concept: Distance-aware explanation loss
  - Why needed here: The distance-aware explanation loss is the key innovation of the proposed method. It penalizes the model based on the distance between its attention on confounding regions and the relevant regions.
  - Quick check question: How is the distance-aware explanation loss computed, and why is it more effective than traditional explanation losses that only consider the intersection between explanations and annotations?

## Architecture Onboarding

- Component map: Data preprocessing -> Model architecture design -> XBL-D refinement -> Evaluation
- Critical path:
  1. Load and preprocess the datasets.
  2. Design and train a CNN model on the preprocessed datasets.
  3. Optimize the coefficients λ1 and λ2 using HyperBand in Keras tuner.
  4. Refine the trained model using the distance-aware explanation loss and the optimized coefficients.
  5. Evaluate the refined model's performance.
- Design tradeoffs:
  - Model architecture: The choice of CNN architecture affects the model's ability to learn relevant features and avoid confounding regions. A deeper or more complex architecture may improve performance but also increase training time and the risk of overfitting.
  - Loss coefficients: The values of λ1 and λ2 determine the balance between classification accuracy and explanation quality. Higher values of λ2 may improve explanation quality but at the cost of classification accuracy, and vice versa.
  - Threshold selection: The choice of threshold for computing AR and AP affects the evaluation of the model's explanations. Lower thresholds may capture more of the relevant regions but also include more irrelevant areas, while higher thresholds may be more selective but miss some relevant areas.
- Failure signatures:
  - If the model fails to improve its classification accuracy after refinement, it may indicate that the distance-aware explanation loss is not effectively guiding the model away from confounding regions.
  - If the model's explanations have low AR and AP values, it may indicate that the model is not effectively identifying the relevant regions or is focusing too much on irrelevant areas.
  - If the optimized coefficients λ1 and λ2 do not transfer well across datasets, it may indicate that the method is not robust to domain shifts or that the optimization process needs to be adjusted.
- First 3 experiments:
  1. Train a CNN model on the Decoy Fashion MNIST dataset and evaluate its classification accuracy and explanation quality using AR and AP.
  2. Refine the trained model using the distance-aware explanation loss and the optimized coefficients λ1 and λ2, and evaluate its performance on the same dataset.
  3. Repeat the above experiments on the Decoy CIFAR-10 and Decoy MS-COCO datasets to assess the method's generalizability across different image classification tasks.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions, but several opportunities for future research are implied by the methodology and results presented.

## Limitations
- The effectiveness of XBL-D depends on the accuracy of Grad-CAM explanations, which may not always reflect true model attention
- The optimized coefficients (λ1 = 2.7, λ2 = 0.1) may not transfer well across different datasets or domains
- The method's performance on more complex confounding patterns beyond localized 4x4 pixel regions remains unexplored

## Confidence

**High confidence**: Classification accuracy improvements (0.904, 0.843, 0.938 on respective datasets) and the core mechanism of distance-aware explanation loss are well-supported by the paper's methodology and experimental design.

**Medium confidence**: The effectiveness of the Activation Recall (AR) metric depends on the accuracy of relevant region masks, which are assumed to be correctly annotated but not validated in the paper.

**Low confidence**: The optimization process for loss coefficients using HyperBand in Keras tuner is only briefly described, making it difficult to assess whether the reported values represent true optima or are dataset-specific artifacts.

## Next Checks

1. Conduct ablation studies removing the distance component from the explanation loss to quantify its specific contribution to performance improvements.

2. Test coefficient transferability by using optimal values from one dataset (e.g., Fashion MNIST) to train models on others (CIFAR-10, MS-COCO) without re-optimization.

3. Validate AR metric reliability by comparing results using ground truth masks versus automatically generated masks from Grad-CAM explanations.