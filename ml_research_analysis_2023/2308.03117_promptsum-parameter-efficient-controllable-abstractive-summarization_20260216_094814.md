---
ver: rpa2
title: 'PromptSum: Parameter-Efficient Controllable Abstractive Summarization'
arxiv_id: '2308.03117'
source_url: https://arxiv.org/abs/2308.03117
tags:
- entity
- summary
- summarization
- generation
- promptsum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PromptSum, the first abstractive summarization
  model that is simultaneously parameter-efficient, data-efficient, and controllable.
  The core method combines soft prompt tuning with a multi-task objective (entity
  chain generation + summary generation) and discrete entity prompts to achieve these
  goals.
---

# PromptSum: Parameter-Efficient Controllable Abstractive Summarization

## Quick Facts
- **arXiv ID**: 2308.03117
- **Source URL**: https://arxiv.org/abs/2308.03117
- **Reference count**: 16
- **Primary result**: First abstractive summarization model that is simultaneously parameter-efficient, data-efficient, and controllable using soft prompt tuning with entity chain conditioning.

## Executive Summary
PromptSum introduces a novel approach to abstractive summarization that achieves parameter efficiency through soft prompt tuning, data efficiency through multi-task pre-training, and controllability through entity chain conditioning. The model uses only a small fraction of parameters compared to full fine-tuning while maintaining competitive ROUGE scores on standard summarization benchmarks. By conditioning summary generation on entity chains, PromptSum provides users with explicit control over summary content while also reducing hallucinations. The method combines entity chain generation and summary generation in a multi-task framework with pre-training on pseudo-labels before fine-tuning on downstream datasets.

## Method Summary
PromptSum is a parameter-efficient abstractive summarization model that uses soft prompt tuning to adapt a frozen PEGASUS-large backbone. The approach involves two soft prompts: one for entity chain generation (E-prompt) and one for summary generation (S-prompt). The model is pre-trained on the C4 dataset using a multi-task objective that alternates between entity chain prediction and summary generation with pseudo-labels. During fine-tuning, the model first generates entity chains from source text, then generates summaries conditioned on these entity chains. This design enables both parameter efficiency (only tuning prompt embeddings) and controllability (users can input custom entity chains).

## Key Results
- Achieves competitive ROUGE-1/2/L scores compared to PEGASUS full fine-tuning while tuning 3-4 orders of magnitude fewer parameters
- Demonstrates strong data efficiency, performing well even with 1% of training data
- Shows significant controllability through entity chains, with ability to reduce hallucinations by removing hallucinated entities from input chains
- Pre-training is critical for performance, with 10.82 ROUGE-1 drop without it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft prompt tuning enables parameter-efficient adaptation by only tuning prompt embeddings while keeping the PLM frozen
- Mechanism: The model prepends continuous soft prompt tokens to the input, which are learned through backpropagation while all other parameters remain frozen during training
- Core assumption: The soft prompt tokens can effectively encode task-specific information without modifying the underlying language model
- Evidence anchors:
  - [abstract] "Prompt tuning (PT), a parameter-efficient technique that only tunes the additional prompt embeddings while keeping the backbone pre-trained language model (PLM) frozen"
  - [section] "Prompt tuning or PT (Lester et al., 2021) prepends (or appends) the input text Xi with a sequence of tunable prompt tokens P, while all other parameters remain frozen during training"
- Break condition: If the soft prompts cannot capture sufficient task-specific information, the model performance will degrade significantly compared to full fine-tuning

### Mechanism 2
- Claim: Multi-task pre-training with pseudo-labels creates better representations for both entity chain and summary generation
- Mechanism: The model alternates between entity chain prediction and summary prediction tasks during pre-training, with each task using dedicated soft prompts and pseudo-labels derived from C4 documents
- Core assumption: Learning both tasks jointly during pre-training helps the model develop representations that benefit both subtasks during fine-tuning
- Evidence anchors:
  - [section] "We pre-train both soft prompts, in the same vein as PPT (Gu et al., 2022b)... During pre-training, within each batch, each pre-training document is used once for entity chain generation, and also once for summary generation, each time with the corresponding soft prompt being inputted to the model"
  - [section] "Without pre-training, PromptSum collapses in few-shot (-10.82 ROUGE-1 on average)"
- Break condition: If the pseudo-labels are poor quality or the tasks are too dissimilar, pre-training may not provide meaningful benefits

### Mechanism 3
- Claim: Conditioning on generated entity chains provides controllability while maintaining summary quality
- Mechanism: The summary generation model takes both the input text and the generated entity chain as inputs, allowing the summary to be guided by the entity chain while the entity chain itself is generated from the same model
- Core assumption: The entity chain serves as an effective high-level content plan that the summary generation model can follow
- Evidence anchors:
  - [abstract] "Our model achieves competitive ROUGE results on popular abstractive summarization benchmarks coupled with a strong level of controllability through entities"
  - [section] "We can better control the summaries with different entity chains (§4.3), as users can input any entity at inference"
  - [section] "By removing hallucinated entities in the generated entity chain, we show how to significantly reduce hallucinations in the summary (§4.4)"
- Break condition: If the entity chain generation is poor quality, it will negatively impact summary generation and controllability

## Foundational Learning

- Concept: Parameter-efficient fine-tuning methods (prompt tuning, prefix tuning, etc.)
  - Why needed here: Understanding how soft prompts work is crucial for implementing PromptSum's parameter-efficient approach
  - Quick check question: What is the key difference between soft prompt tuning and traditional fine-tuning?

- Concept: Multi-task learning and pre-training objectives
  - Why needed here: The pre-training stage combines entity chain generation and summary generation, requiring understanding of multi-task learning principles
  - Quick check question: How does alternating between two tasks during pre-training benefit the final model performance?

- Concept: Controllable text generation and planning-based approaches
  - Why needed here: The entity chain serves as a content plan, and understanding planning-based summarization is key to understanding PromptSum's controllability
  - Quick check question: What are the benefits of planning content with entities before generating summaries?

## Architecture Onboarding

- Component map:
  - PEGASUS-large (frozen) -> Soft prompts (E-prompt for entity generation, S-prompt for summary generation) -> Entity chain generation -> Summary generation with entity chain conditioning

- Critical path: Pre-training → Entity generation fine-tuning → Summary generation fine-tuning → Inference

- Design tradeoffs:
  - Parameter efficiency vs. full fine-tuning performance
  - Pre-training complexity vs. few-shot performance
  - Entity chain quality vs. controllability effectiveness

- Failure signatures:
  - Poor entity chain generation indicates issues with E-prompt or pre-training
  - Low ROUGE scores with entity chain suggest S-prompt problems
  - Hallucinations in summaries may indicate entity chain generation issues

- First 3 experiments:
  1. Verify soft prompt tuning works by comparing with frozen model baseline
  2. Test pre-training effectiveness by comparing with no-pretraining variant
  3. Evaluate controllability by testing entity chain intervention experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the generated entity chain affect the overall summarization performance?
- Basis in paper: [explicit] The paper states that "the entity chain quality drives the performance of the final summary" and shows strong positive correlation between entity ROUGE and summary ROUGE scores.
- Why unresolved: While the correlation is shown, the exact nature of this relationship and whether there are diminishing returns or optimal entity chain qualities remains unclear.
- What evidence would resolve it: Experiments varying entity chain quality systematically and measuring the corresponding impact on summarization performance would clarify this relationship.

### Open Question 2
- Question: Can the multi-task pre-training mechanism be improved or replaced with a more effective alternative?
- Basis in paper: [explicit] The paper states that "without pre-training, PromptSum collapses in few-shot" and that pre-training is "critical for PromptSum."
- Why unresolved: The paper only evaluates their proposed multi-task pre-training approach and doesn't compare it to other pre-training strategies or investigate ways to improve it.
- What evidence would resolve it: Comparing PromptSum's pre-training to other pre-training methods (e.g., different objectives, data, or architectures) would determine if improvements are possible.

### Open Question 3
- Question: How does PromptSum's controllability mechanism compare to other controllable summarization methods?
- Basis in paper: [explicit] The paper compares PromptSum's controllability to CTRLSum but only on entity coverage, not on other aspects of controllability like summary length or aspect focus.
- Why unresolved: The paper only evaluates one aspect of controllability (entity coverage) and doesn't compare PromptSum to other controllable summarization methods on other aspects or overall controllability.
- What evidence would resolve it: Comparing PromptSum's controllability to other methods across multiple aspects of controllability (e.g., entity coverage, length control, aspect focus) would provide a more complete picture.

## Limitations

- The study primarily evaluates on English-language news and dialogue datasets, leaving cross-lingual and domain generalization questions open
- The entity chain extraction method relies on heuristic string matching without detailed validation of its accuracy or coverage
- Controllability evaluation focuses on entity presence but doesn't comprehensively assess semantic quality or coherence impacts when entity chains are modified
- Computational overhead of the pre-training stage and its necessity for downstream performance remains under-explored

## Confidence

**High Confidence**: The parameter efficiency claim (3-4 orders of magnitude fewer parameters than full fine-tuning) is well-supported by experimental comparisons with PEGASUS and full fine-tuning baselines. The ablation study showing pre-training is necessary (without pre-training, ROUGE-1 drops by 10.82 points) provides strong evidence for the multi-task pre-training mechanism.

**Medium Confidence**: The controllability claims are supported by entity coverage experiments, but evaluation metrics are limited to entity presence without deeper semantic analysis. The hallucination reduction claim is demonstrated through controlled experiments but relies on heuristic entity detection methods.

**Low Confidence**: The data-efficiency claims are primarily demonstrated through few-shot learning experiments (1%, 10%, 50% of training data) but lack comprehensive analysis of the pre-training stage's data requirements or comparison with other parameter-efficient methods on the same data budgets.

## Next Checks

1. **Entity Chain Quality Validation**: Implement and evaluate the entity chain extraction accuracy on a manually annotated subset of the datasets to verify the quality of pseudo-labels used for pre-training and fine-tuning.

2. **Cross-Domain Generalization Test**: Evaluate PromptSum on datasets from different domains (e.g., scientific articles, medical records) to assess whether the entity chain conditioning approach generalizes beyond news and dialogue.

3. **Alternative Parameter-Efficient Method Comparison**: Conduct controlled experiments comparing PromptSum with other parameter-efficient methods (e.g., LoRA, adapters) on the same datasets and data budgets to isolate the benefits of the specific soft prompt tuning approach.