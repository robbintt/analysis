---
ver: rpa2
title: Uniform-in-Time Wasserstein Stability Bounds for (Noisy) Stochastic Gradient
  Descent
arxiv_id: '2305.12056'
source_url: https://arxiv.org/abs/2305.12056
tags:
- have
- proof
- assumption
- lemma
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes uniform-in-time Wasserstein stability bounds
  for stochastic gradient descent (SGD) and its noisy variants under different loss
  function classes (strongly convex, convex, and non-convex). The key innovation is
  connecting algorithmic stability to Markov chain perturbation theory, providing
  a unified three-step framework: (1) show geometric ergodicity, (2) construct Lyapunov
  functions, and (3) bound Markov transition kernel discrepancies.'
---

# Uniform-in-Time Wasserstein Stability Bounds for (Noisy) Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2305.12056
- Source URL: https://arxiv.org/abs/2305.12056
- Reference count: 40
- This paper establishes uniform-in-time Wasserstein stability bounds for stochastic gradient descent and its noisy variants under different loss function classes

## Executive Summary
This paper introduces a unified framework for proving Wasserstein stability bounds for stochastic gradient descent (SGD) and its variants using Markov chain perturbation theory. The key innovation is connecting algorithmic stability to applied probability, providing a three-step approach: (1) establish geometric ergodicity, (2) construct Lyapunov functions, and (3) bound Markov transition kernel discrepancies. The framework recovers known n⁻¹ generalization bounds for strongly convex losses while extending to non-convex and convex loss classes with novel rates, particularly n⁻¹/ᵖ for convex losses with p∈(1,2).

## Method Summary
The paper's approach treats SGD as a Markov chain and analyzes perturbations between chains on different datasets. The three-step framework begins by establishing geometric ergodicity of the Markov process, then constructs Lyapunov functions to quantify convergence, and finally applies Markov chain perturbation theory to bound the Wasserstein distance between iterates on different datasets. This unified approach handles strongly convex, convex, and non-convex loss functions within a single theoretical framework, with particular attention to how noise injection affects stability in non-convex settings.

## Key Results
- Recovers n⁻¹ generalization bounds for strongly convex losses with a unified proof technique
- Extends time-uniform stability to non-convex losses with general noise distributions (beyond Gaussian)
- Proves novel n⁻¹/ᵖ generalization rates for convex losses with p∈(1,2), interpolating between known rates
- Shows ergodicity is crucial for sharp time-uniform bounds, explaining why noise injection enables better stability in non-convex settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Geometric ergodicity is the key enabler for time-uniform stability bounds.
- **Mechanism**: When the SGD Markov chain is geometrically ergodic, the distribution of iterates contracts in Wasserstein distance uniformly in time, preventing divergence of stability bounds as iterations increase.
- **Core assumption**: The loss function satisfies conditions that ensure geometric ergodicity (e.g., strong convexity or presence of additive noise).
- **Evidence anchors**:
  - [abstract]: "Our approach is flexible and can be generalizable to other popular optimizers, as it mainly requires developing Lyapunov functions, which are often readily available in the literature"
  - [section 3.1]: "We start by establishing the geometric ergodicity of the Markov process"
  - [corpus]: Weak - related papers focus on different noise models but don't explicitly discuss ergodicity as the stability mechanism
- **Break condition**: Loss functions that don't satisfy geometric ergodicity conditions (e.g., general non-convex losses without noise) will not yield time-uniform bounds.

### Mechanism 2
- **Claim**: Markov chain perturbation theory provides a unified framework for stability analysis across different loss classes.
- **Mechanism**: By treating SGD as a Markov chain and analyzing perturbations between chains on different datasets, the paper establishes a three-step approach: (1) geometric ergodicity, (2) Lyapunov function construction, (3) kernel discrepancy bounds.
- **Core assumption**: The algorithm can be modeled as a Markov chain with computable transition kernels.
- **Evidence anchors**:
  - [abstract]: "We make a novel connection between learning theory and applied probability and introduce a unified guideline for proving Wasserstein stability bounds"
  - [section 3.1]: "Our approach is based on Markov chain perturbation theory Rudolf and Schweizer (2018), which offers a three-step proof technique"
  - [corpus]: Weak - related work focuses on specific algorithms rather than a unified perturbation framework
- **Break condition**: When the Markov chain structure breaks down or perturbation bounds become too loose.

### Mechanism 3
- **Claim**: Additive noise injection enables stability bounds for non-convex losses by restoring geometric ergodicity.
- **Mechanism**: Without additional noise, non-convex SGD chains may not be geometrically ergodic, causing stability bounds to diverge. Adding noise creates sufficient mixing to achieve geometric ergodicity, enabling time-uniform bounds.
- **Core assumption**: The noise distribution has finite variance and satisfies certain regularity conditions.
- **Evidence anchors**:
  - [abstract]: "for non-convex losses with additive noise, they extend prior Gaussian noise results to more general noise distributions while maintaining time-uniform stability"
  - [section 3.3]: "we consider a class of non-convex loss functions...we consider a noisy version of SGD"
  - [corpus]: Weak - related papers mention noise but don't explicitly connect it to restoring ergodicity
- **Break condition**: Noise with infinite variance or pathological distributions that don't support geometric ergodicity.

## Foundational Learning

- **Concept: Wasserstein distance and its dual representation**
  - Why needed here: The stability bounds are formulated in terms of Wasserstein distance between distributions of iterates, requiring understanding of this metric and its dual form.
  - Quick check question: What is the dual representation of 1-Wasserstein distance according to Villani (2009)?

- **Concept: Algorithmic stability and generalization bounds**
  - Why needed here: The paper's main goal is to bound generalization error through algorithmic stability, requiring understanding of how stability relates to generalization.
  - Quick check question: How does algorithmic stability (Definition 1) connect to generalization error bounds?

- **Concept: Lyapunov functions for Markov chains**
  - Why needed here: Constructing Lyapunov functions is essential for establishing geometric ergodicity and drift conditions in the three-step framework.
  - Quick check question: What conditions must a Lyapunov function satisfy to establish geometric ergodicity of a Markov chain?

## Architecture Onboarding

- **Component map**: Loss function class → Ergodicity verification → Lyapunov function construction → Kernel discrepancy computation → Markov perturbation theorem → Stability bound
- **Critical path**: Identify loss class → Verify ergodicity conditions → Construct appropriate Lyapunov function → Compute kernel discrepancy → Apply Markov perturbation theorem → Obtain stability bound
- **Design tradeoffs**: 
  - Uniform bounds vs. tighter problem-specific bounds
  - General framework vs. specialized techniques for specific loss classes
  - Assumptions on noise vs. broader applicability
- **Failure signatures**:
  - Geometric ergodicity not established → No time-uniform bound possible
  - Lyapunov function construction fails → Framework cannot proceed
  - Kernel discrepancy bounds too loose → Bounds become vacuous
- **First 3 experiments**:
  1. Test strongly convex case with quadratic loss to verify recovery of known bounds
  2. Implement non-convex case with Gaussian noise to compare with Farghly and Rebeschini (2021)
  3. Try convex case with p∈(1,2) to verify new n^(-1/p) rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions on non-convexity level (K from Assumption 3) can time-uniform stability bounds be achieved without additional noise injection?
- Basis in paper: [explicit] Theorem 12 proves a time-uniform bound for non-convex losses without additive noise, but the bound converges to a positive constant that depends on K
- Why unresolved: The paper establishes existence of bounds but doesn't characterize when these bounds become acceptably small or what threshold of non-convexity makes them impractical
- What evidence would resolve it: A quantitative analysis showing how the persistent bias term scales with K and comparing it to acceptable generalization error thresholds for practical applications

### Open Question 2
- Question: Can the n^(-1/p) generalization rate for convex losses with p ∈ (1,2) be improved to n^(-1)?
- Basis in paper: [explicit] Theorem 13 shows n^(-1/p) rate for this intermediate convexity class, while Theorem 8 achieves n^(-1) for strongly convex (p=2)
- Why unresolved: The gap between these rates suggests potential room for improvement, but the paper doesn't identify whether this is fundamental or an artifact of the proof technique
- What evidence would resolve it: Either a matching lower bound showing n^(-1/p) is optimal for this class, or an improved upper bound technique achieving n^(-1)

### Open Question 3
- Question: How do the geometric ergodicity requirements for stability bounds relate to the algorithmic convergence properties of SGD?
- Basis in paper: [explicit] The paper observes that geometric ergodicity is crucial for sharp time-uniform bounds and explains why noise injection helps in non-convex settings
- Why unresolved: The paper identifies the connection but doesn't provide a formal relationship between ergodicity rates and algorithmic convergence rates
- What evidence would resolve it: A unified framework connecting the geometric ergodicity constants (ρ, δ, γ) from the stability analysis to the convergence rate constants in optimization theory

## Limitations
- The framework relies on surrogate loss functions being Lipschitz, which may not hold for original loss functions in practice
- Geometric ergodicity assumptions may not be satisfied for all practical loss functions or optimization scenarios
- The Lyapunov function construction requires problem-specific tailoring that may not always yield tight bounds

## Confidence
- **High** for the three-step Markov chain perturbation framework itself, as it's well-established in applied probability literature
- **Medium** for non-convex stability results without additional noise, as bounds converge to positive constants dependent on non-convexity level
- **Medium** for extension to general noise distributions, requiring further investigation of practical implications

## Next Checks
1. **Empirical Verification of Non-Convex Bounds**: Implement the noisy SGD algorithm for a non-convex test function (e.g., a neural network with one hidden layer) and measure actual stability against the theoretical predictions, particularly for different noise distributions beyond Gaussian.

2. **Lyapunov Function Robustness Test**: Systematically test the Lyapunov function construction approach across different loss function classes to identify when the framework produces vacuous vs. informative bounds, establishing a practical checklist for when the method is likely to succeed.

3. **Comparison with Alternative Stability Frameworks**: Apply the framework to the same problems as traditional uniform stability analysis (e.g., strongly convex losses) to quantify the tradeoff between the unified Markov chain approach and more specialized techniques in terms of bound tightness and proof complexity.