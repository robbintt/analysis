---
ver: rpa2
title: 'Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic
  Review Automation'
arxiv_id: '2308.06610'
source_url: https://arxiv.org/abs/2308.06610
tags:
- review
- chatgpt
- performance
- systematic
- exclusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Bio-SIEVE, a family of instruction-tuned language
  models designed to automate literature screening for systematic reviews. The approach
  involves fine-tuning LLaMA and Guanaco models on a dataset of 7,330 medical systematic
  reviews from Cochrane, using natural language instructions for tasks like inclusion/exclusion
  classification, PICO extraction, and exclusion reasoning.
---

# Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation

## Quick Facts
- arXiv ID: 2308.06610
- Source URL: https://arxiv.org/abs/2308.06610
- Authors: [Authors not provided]
- Reference count: 33
- Primary result: Instruction-tuned models outperform ChatGPT for systematic review screening, with Guanaco7B achieving 0.82 accuracy on test set vs 0.60 for ChatGPT

## Executive Summary
Bio-SIEVE presents a family of instruction-tuned language models designed to automate literature screening for systematic reviews. The approach involves fine-tuning LLaMA and Guanaco models on 7,330 Cochrane systematic reviews using natural language instructions for inclusion/exclusion classification, PICO extraction, and exclusion reasoning. Bio-SIEVE demonstrates superior accuracy and generalization across medical domains compared to both ChatGPT and traditional approaches, with the best model achieving 0.82 accuracy on test data. The system addresses a critical bottleneck in evidence-based medicine by reducing the time-consuming manual screening process.

## Method Summary
Bio-SIEVE fine-tunes LLaMA7B and Guanaco7B models using QLoRA with instruction-formatted datasets derived from Cochrane systematic reviews. The training involves both single-task (inclusion/exclusion classification only) and multi-task approaches (adding PICO extraction and exclusion reasoning). Data preprocessing handles token limits through proportional truncation of abstracts while preserving objectives and selection criteria. Models are trained on 8-10 A100 80GB GPUs with 4-bit quantization, using beam search with temperature=0 for inference. Evaluation includes standard test sets and a safety-first test set where uncertain cases are relabeled as "Include" to simulate full-text screening.

## Key Results
- Bio-SIEVE achieves 0.82 accuracy on test set versus 0.60 for ChatGPT
- Guanaco7B outperforms LLaMA7B base models in both single and multi-task settings
- Multi-task training with PICO extraction and exclusion reasoning reduces classification accuracy compared to single-task training
- Models demonstrate strong generalization across medical domains beyond their training data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Instruction tuning enables Bio-SIEVE to understand and execute domain-specific tasks without retraining for each review.
- **Mechanism:** The model learns to parse and act on structured prompts that include objectives and selection criteria, generalizing across medical domains.
- **Core assumption:** Cochrane reviews contain consistent, machine-readable formats for objectives, selection criteria, and exclusion reasons.
- **Evidence anchors:**
  - [abstract] "Our best model, Bio-SIEVE, outperforms both ChatGPT and trained traditional approaches, and generalises better across medical domains."
  - [section] "By incorporating the existing and expansive Cochrane Review knowledge base via instruction tuning, Bio-SIEVE establishes a strong baseline for inclusion or exclusion classification screening of potential eligible studies given their abstract for unseen SRs."
  - [corpus] Weak - related papers focus on zero-shot and prompt-based approaches but do not detail instruction tuning with Cochrane data.
- **Break condition:** Cochrane review format changes or inconsistent metadata would break the parsing assumptions.

### Mechanism 2
- **Claim:** Fine-tuning on the Instruct Cochrane dataset improves accuracy over zero-shot LLMs like ChatGPT.
- **Mechanism:** Training on a large corpus of labeled Cochrane reviews allows the model to learn nuanced inclusion/exclusion patterns specific to medical SR screening.
- **Core assumption:** Cochrane reviews provide high-quality, expert-labeled inclusion/exclusion decisions that can be used as training data.
- **Evidence anchors:**
  - [abstract] "Bio-SIEVE outperforms both ChatGPT and trained traditional approaches, and generalises better across medical domains."
  - [section] "We gathered a total of 7,330 medical SRs from all possible topic areas available on the Cochrane Reviews website."
  - [corpus] Weak - no direct evidence in corpus that instruction-tuned models outperform ChatGPT in SR screening; only claims made in the paper.
- **Break condition:** If Cochrane data is noisy or biased, the fine-tuned model's performance would degrade.

### Mechanism 3
- **Claim:** Safety-first test set evaluation reveals model's tendency to include uncertain cases rather than exclude them.
- **Mechanism:** Annotators relabeled uncertain cases as "Include" to simulate full-text screening, rewarding models that err on the side of inclusion.
- **Core assumption:** Abstract screening alone cannot always determine inclusion; full-text review is needed for borderline cases.
- **Evidence anchors:**
  - [section] "We curated a safety-first test set by manually annotating include/exclude decisions for a small subset of 108 samples from the test split, with each sample consisting of the objectives and selection criteria for the review and the abstract of the prospective study."
  - [section] "For evaluation purposes, we treat 'Insufficient Information' as 'Include', since these would be samples that should proceed to full-text screening phase, in keeping with a safety-first approach."
  - [corpus] Weak - no evidence in corpus that this safety-first evaluation method is standard or validated.
- **Break condition:** If full-text screening becomes unnecessary due to improved abstracts, this safety-first assumption would be invalid.

## Foundational Learning

- **Concept:** Tokenization and truncation strategies for long medical abstracts
  - Why needed here: Cochrane abstracts and selection criteria can exceed model context limits; proportional truncation preserves key information.
  - Quick check question: How does the model handle abstracts longer than 2048 tokens while preserving objectives and selection criteria?

- **Concept:** Fine-tuning vs. zero-shot performance differences
  - Why needed here: Understanding when and why fine-tuning improves over zero-shot helps justify the computational cost.
  - Quick check question: What performance gap exists between Guanaco7B zero-shot and Guanaco7B fine-tuned on Cochrane data?

- **Concept:** Multi-task learning effects on single-task performance
  - Why needed here: The paper explores whether adding PICO extraction and exclusion reasoning tasks helps or hurts inclusion/exclusion classification.
  - Quick check question: How does the multi-task model's inclusion recall compare to the single-task model's?

## Architecture Onboarding

- **Component map:** Cochrane review data → preprocessing with token truncation → instruction template construction → LLaMA/Guanaco base models with LoRA adapters → fine-tuning on 8-10 A100 GPUs → beam search inference with temperature=0
- **Critical path:** Data preprocessing → instruction template construction → model fine-tuning → evaluation on test/safety-first sets → exclusion reasoning ranking
- **Design tradeoffs:** Single-task training achieves higher accuracy but lacks exclusion reasoning capability; multi-task training adds reasoning but reduces classification accuracy
- **Failure signatures:** Zero-shot models include many irrelevant abstracts; multi-task models show task interference; ChatGPT shows topic-dependent performance variance
- **First 3 experiments:**
  1. Fine-tune Guanaco7B on Cochrane inclusion/exclusion data and evaluate on test set
  2. Compare Guanaco7B zero-shot vs. fine-tuned on safety-first set
  3. Evaluate multi-task model's exclusion reasoning quality vs. ChatGPT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does multi-task training affect the performance of Bio-SIEVE across different medical domains?
- Basis in paper: [explicit] The paper mentions that multi-task training with PICO extraction and exclusion reasoning did not improve performance compared to single-task training.
- Why unresolved: The paper does not provide a detailed analysis of the impact of multi-task training on different medical domains, leaving the question of whether multi-task training is universally detrimental or if it has domain-specific effects.
- What evidence would resolve it: Detailed performance metrics of Bio-SIEVE across various medical domains when trained with single-task and multi-task approaches.

### Open Question 2
- Question: What is the optimal number of few-shot examples to include in the prompt for improved Bio-SIEVE performance?
- Basis in paper: [inferred] The paper discusses the potential benefits of few-shot prompting but notes limitations due to context window size, suggesting that this is an area for future exploration.
- Why unresolved: The paper does not experiment with few-shot prompts, leaving the optimal number and selection of examples untested.
- What evidence would resolve it: Experimental results comparing Bio-SIEVE performance with varying numbers of few-shot examples in the prompt.

### Open Question 3
- Question: How does the performance of Bio-SIEVE compare to human reviewers in terms of accuracy and efficiency?
- Basis in paper: [explicit] The paper mentions that Bio-SIEVE outperforms both ChatGPT and trained traditional approaches, but does not compare it directly to human reviewers.
- Why unresolved: The paper focuses on comparing Bio-SIEVE to automated methods rather than human performance, leaving the question of how it stacks up against human reviewers unanswered.
- What evidence would resolve it: A study comparing the accuracy and efficiency of Bio-SIEVE to human reviewers in the systematic review process.

## Limitations

- Data quality and representativeness concerns: Performance claims rely heavily on Cochrane review metadata consistency, which may not generalize to non-Cochrane systematic reviews
- Evaluation methodology limitations: Safety-first test set approach lacks validation against actual full-text screening outcomes and inter-rater reliability metrics
- Task interference in multi-task learning: Multi-task training actually decreased classification accuracy, suggesting unresolved issues with task interference

## Confidence

- **High confidence**: Basic claim that instruction-tuned models outperform zero-shot LLMs for SR screening, supported by multiple evaluation metrics
- **Medium confidence**: Generalization claims across medical domains, as evaluation primarily uses Cochrane data which may not fully represent practice
- **Low confidence**: Exclusion reasoning quality assessment, as evaluation relies on subjective 5-star rankings without detailed qualitative analysis

## Next Checks

1. **Cross-platform generalization test**: Evaluate Bio-SIEVE on SR screening tasks from non-Cochrane sources (e.g., PROSPERO, MEDLINE) to assess true domain generalization

2. **Full-text screening validation**: Conduct a small-scale study where Bio-SIEVE's "Insufficient Information" cases are actually subjected to full-text review to validate the safety-first assumption

3. **Ablation study on multi-task learning**: Systematically vary training schedule and task weighting in multi-task setup to identify optimal configurations that preserve classification accuracy while adding exclusion reasoning capabilities