---
ver: rpa2
title: NSOAMT -- New Search Only Approach to Machine Translation
arxiv_id: '2309.10526'
source_url: https://arxiv.org/abs/2309.10526
tags:
- sentences
- text
- sentence
- translation
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated a "new search only approach" to machine
  translation by analyzing large text corpora to determine if repeated sentence patterns
  could be leveraged for translation. Text from arXiv, Wikipedia, EUR-LEX, and other
  sources was ingested and processed to measure distinct sentences and repetitions.
---

# NSOAMT -- New Search Only Approach to Machine Translation

## Quick Facts
- arXiv ID: 2309.10526
- Source URL: https://arxiv.org/abs/2309.10526
- Reference count: 33
- Primary result: Exact sentence repetition rates in natural text are too low (<3.39%) for NSOAMT to be practical at scale

## Executive Summary
This study investigates a "new search only approach" to machine translation that leverages exact sentence repetition patterns in large text corpora. The core hypothesis is that if sentences repeat frequently across documents, they can be translated once and reused, reducing the translation workload. The approach processes text from sources like arXiv, Wikipedia, and EUR-LEX to identify distinct sentences and measure repetition rates. Despite processing over 114 GB of English text, the study found that only about 3.39% of distinct sentences had repetitions. Logarithmic projections indicated that achieving even modest repetition rates (e.g., 25%) would require infeasible text volumes, making the approach impractical for large-scale translation coverage and crowd-sourcing.

## Method Summary
The methodology involves importing text documents from diverse sources, splitting them into sentences using consistent tokenization, and storing them in a database with MD5 hashing for indexing. The study tracks distinct sentences, repetitions, and translation coverage while validating sentences using LanguageTool for grammatical correctness. Repetition rates are analyzed by counting how many times each sentence appears across the corpus, and logarithmic projections estimate the volume of text needed to achieve higher repetition rates. The approach assumes that limited vocabulary diversity in specific domains enhances translation accuracy and speed, but the empirical results show this assumption does not hold in practice.

## Key Results
- Only 3.39% of distinct sentences had repetitions across 114+ GB of English text
- Logarithmic projections show achieving 25% repetition rate would require impractical text volumes
- Validation filtering using LanguageTool further reduced viable sentences for translation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repetition of exact sentence strings enables crowd-sourced translation
- Mechanism: If a sentence appears in multiple documents, it can be translated once and reused, reducing the translation workload
- Core assumption: Natural language text contains a high proportion of repeated sentences, making indexing and reuse feasible
- Evidence anchors:
  - [abstract] "The idea is to develop a solution that, by indexing an incremental set of words that combine a certain semantic meaning, makes it possible to create a process of correspondence between their native language record and the language of translation."
  - [section] "This research principle assumes that the vocabulary used in a given type of publication/document is relatively limited in terms of language style and word diversity, which enhances the greater effect of instantaneously and rigor in the translation process through the indexing process."
- Break condition: Repetition rate falls below ~5% even with terabytes of text, making the approach impractical

### Mechanism 2
- Claim: Log-linear growth of distinct sentences with volume is slow enough to be manageable
- Mechanism: As more text is ingested, the rate of new distinct sentences decreases, allowing saturation at a tractable corpus size
- Core assumption: Vocabulary diversity is bounded, so sentence space is finite and searchable
- Evidence anchors:
  - [section] "Based on the previous numbers, a maximum limit was estimated for the universe of sentences with comprehensibility above 10%."
  - [section] "Using the arXiv data source segmented by years 2016 to 2020, Table 8, a trend line was elaborated in Figure 125 The R2 = 0.985 gives some confidence on the logarithmic trend."
- Break condition: Log-linear trend does not hold beyond current corpus; inflection point leads to unbounded distinct sentences

### Mechanism 3
- Claim: Sentence validation filtering removes noise and improves translation accuracy
- Mechanism: Invalid sentences (extraction artifacts, bad tokenization) are excluded, focusing indexing on well-formed, translatable sentences
- Core assumption: LanguageTool validation correlates with translatable quality and higher repetition rates
- Evidence anchors:
  - [section] "There are at least two possible mitigation strategies for this problem: Improve the quality of the text extraction (and sentence tokenization). Exclude the improperly extracted sentences from the metrics."
  - [section] "From Table 10 for 80,399,442,210 text characters (≈ 8.03E+10 text characters) (≈ 74.9 GigaBytes of text), we observe 5.18% distinct valid sentences with repetitions."
- Break condition: Validation filter removes too many sentences, reducing coverage; filter rules do not generalize across domains

## Foundational Learning

- Concept: Logarithmic vs linear growth in data science
  - Why needed here: To understand why sentence repetition saturates slowly and why massive corpora are required for modest coverage
  - Quick check question: If repetition grows logarithmically, what happens to required corpus size when repetition target increases from 5% to 50%?

- Concept: Hash-based indexing and collision handling
  - Why needed here: MD5 hashing is used for sentence lookup; understanding collision resistance and race conditions is critical for system correctness
  - Quick check question: Why can't a simple UNIQUE INDEX be used on the plainText column for long sentences?

- Concept: Natural Language Processing (NLP) tokenization basics
  - Why needed here: Sentence splitting accuracy directly impacts repetition detection and translation quality
  - Quick check question: How might inconsistent sentence tokenization across document styles affect the #distinct sentences metric?

## Architecture Onboarding

- Component map:
  Text source ingestion -> HTML/WikiText/PDF -> plain text extraction -> sentence tokenizer -> MD5 hash -> database (documents, sentences, sources, translations) -> Validation pipeline (LanguageTool) -> Web interface (search/upload/translate)

- Critical path:
  Ingest large volume of structured text -> accurate sentence tokenization -> MD5 indexing -> database consistency -> validation filtering -> repetition analysis -> translation lookup

- Design tradeoffs:
  - Exact string matching (high precision, low recall) vs semantic matching (low precision, high recall)
  - Large memory footprint for concurrent ingestion vs slower sequential processing
  - Open-source validation tools (LanguageTool) vs custom regex filters (less robust but faster)

- Failure signatures:
  - Low repetition rates (<5%) despite terabytes of text -> core assumption broken
  - High hash collision rates or race conditions -> indexing logic error
  - Validation filter removing >70% of sentences -> overly strict rules

- First 3 experiments:
  1. Ingest a controlled corpus with known repeated sentences; verify repetition detection and MD5 indexing
  2. Run LanguageTool validation on a sample; measure precision/recall of valid sentence filtering
  3. Simulate scaling: extrapolate repetition rate vs corpus size using logarithmic trend; check for inflection points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy and effectiveness of NSOAMT compare to neural network-based machine translation models for specific domains?
- Basis in paper: [explicit] The paper discusses a "new search only approach to machine translation" as an alternative to neural network-based techniques, but does not provide a direct comparison of accuracy or effectiveness
- Why unresolved: The paper focuses on the feasibility of the NSOAMT approach based on repetition rates in natural text, without evaluating its translation quality compared to other methods
- What evidence would resolve it: A direct comparison of NSOAMT's translation accuracy and quality against neural network-based models on the same test datasets for specific domains

### Open Question 2
- Question: What is the inflection point in the trend line for distinct sentences with repetitions vs. text volume, and does it change the logarithmic nature of the curve?
- Basis in paper: [inferred] The paper mentions the need for impractical text volumes to achieve even modest repetition rates, suggesting the possibility of an inflection point
- Why unresolved: The paper does not gather enough text volume to observe an inflection point or change in the trend line's curve nature
- What evidence would resolve it: Analysis of text volume trends beyond the current dataset to identify any inflection points or changes in the curve's nature

### Open Question 3
- Question: Can a different sentence model, such as syntax trees with sub-tree matching or attention models, provide higher rates of common text matching in NSOAMT?
- Basis in paper: [explicit] The paper suggests that a different sentence model might provide higher rates of common text matching and mentions syntax trees and attention models as potential approaches
- Why unresolved: The paper does not explore alternative sentence models beyond the current string-based approach
- What evidence would resolve it: Implementation and evaluation of alternative sentence models in NSOAMT to assess their impact on common text matching rates

## Limitations

- Core hypothesis dependent on high exact sentence repetition rates, which natural text does not provide
- Logarithmic projections cannot be fully validated without impractically large text corpora
- LanguageTool validation may introduce domain-specific biases not fully characterized

## Confidence

- Empirical repetition measurements: High
- Logarithmic trend analysis: Medium-High
- Extrapolation to required corpus sizes: Medium
- Translation quality assessment: Low (not evaluated)

## Next Checks

1. **Cross-domain repetition analysis**: Test the approach on specialized corpora (legal documents, technical manuals, scientific papers) where vocabulary is more constrained to determine if domain specificity improves repetition rates sufficiently

2. **Partial matching extension**: Implement and evaluate fuzzy matching algorithms (e.g., edit distance thresholds, semantic similarity) to assess whether relaxing the exact-match requirement significantly improves coverage while maintaining translation quality

3. **Hybrid approach validation**: Design experiments combining NSOAMT with neural translation for non-repeated sentences to quantify whether the repetition-based component still provides meaningful efficiency gains in a mixed system