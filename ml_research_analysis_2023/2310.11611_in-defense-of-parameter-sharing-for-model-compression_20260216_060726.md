---
ver: rpa2
title: In defense of parameter sharing for model-compression
arxiv_id: '2310.11611'
source_url: https://arxiv.org/abs/2310.11611
tags:
- pruning
- stable-rps
- compression
- roast
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the effectiveness of parameter sharing for
  model compression compared to pruning techniques. The authors propose STABLE-RPS,
  an improved version of the state-of-the-art RPS method ROAST, addressing stability
  and Pareto-continuity issues.
---

# In defense of parameter sharing for model-compression

## Quick Facts
- arXiv ID: 2310.11611
- Source URL: https://arxiv.org/abs/2310.11611
- Reference count: 40
- Parameter sharing (STABLE-RPS) outperforms moderately informed pruning methods across entire compression range

## Executive Summary
This paper presents STABLE-RPS, an improved parameter sharing method for model compression that addresses stability and continuity issues in the previous state-of-the-art ROAST approach. The authors demonstrate that parameter sharing consistently outperforms moderately informed pruning techniques (MAG, SNIP, SYNFLOW, GRASP) across all compression levels, particularly excelling at high compression rates. Theoretical analysis shows RPS methods have inherent capacity advantages over sparse models, supporting a paradigm shift toward parameter sharing for memory-efficient neural network representations.

## Method Summary
STABLE-RPS builds on the ROAST framework by introducing two key improvements: gradient scaling to eliminate sensitivity to initialization standard deviation and RANDOMFOLD mapping for optimal load factor. The method uses a shared parameter repository where weights are mapped using hash functions, with gradient updates scaled by layer-specific factors to maintain stability. The RANDOMFOLD mapping optimizes bucket utilization while considering cache efficiency. Experiments compare STABLE-RPS against multiple pruning baselines (MAG, SNIP, SYNFLOW, GRASP, Lottery Ticket Rewinding) on CIFAR-10, CIFAR-100, and Tiny ImageNet using ResNet-20 and VGG-11 architectures.

## Key Results
- STABLE-RPS outperforms moderately informed pruning methods (MAG, SNIP, SYNFLOW, GRASP) across entire compression range
- At high compression rates (>10x), STABLE-RPS significantly outperforms even highly informed pruning techniques like Lottery Ticket Rewinding
- Theoretical analysis shows RPS preserves inner products and norms better than pruning for linear models under data compression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: STABLE-RPS outperforms moderately informed pruning methods across entire compression range
- Mechanism: Parameter sharing with optimal load factor and gradient scaling preserves model capacity without dropping parameters
- Core assumption: Parameter sharing maintains better approximation of original model capacity compared to sparse models
- Evidence anchors: Abstract demonstrates consistent outperformance of moderately informed pruning methods; section shows STABLE-RPS outperforms both RAND and moderately informed methods

### Mechanism 2
- Claim: STABLE-RPS addresses stability and Pareto-continuity issues present in ROAST
- Mechanism: Gradient scaling eliminates sensitivity to initialization standard deviation; RANDOMFOLD mapping ensures optimal load factor
- Core assumption: Stability issues in ROAST primarily stem from global memory sharing and scaling factors
- Evidence anchors: Abstract states provable addressing of both issues; section details gradient scaling mechanism and RANDOMFOLD mapping

### Mechanism 3
- Claim: STABLE-RPS has theoretical capacity advantage over pruning for linear models
- Mechanism: Better preservation of inner products and norms compared to pruning under data compression
- Core assumption: Quality of learning linear models reduces to norm preservation under data compression
- Evidence anchors: Section proves residual depends on norm preservation quality; theoretical analysis shows ED(Vprune) ≥ ED(Vh)STABLE-RPS

## Foundational Learning

- Concept: Parameter sharing and hashing mechanisms
  - Why needed here: Understanding STABLE-RPS mapping is crucial for implementation and debugging
  - Quick check question: How does RANDOMFOLD hash function ensure optimal load factor while maintaining cache efficiency?

- Concept: Gradient scaling and stability analysis
  - Why needed here: Gradient scaling mechanism is key to STABLE-RPS's stability improvements over ROAST
  - Quick check question: How does effective update scaler maintain stability range of learning rate?

- Concept: Pruning methods and their information usage
  - Why needed here: Comparing STABLE-RPS to various pruning methods requires understanding their information usage
  - Quick check question: What distinguishes moderately informed pruning methods from highly informed ones like LTR?

## Architecture Onboarding

- Component map:
  Parameter repository -> Hash functions (h, g) -> Gradient scaling (Γ) -> Layer-wise scaling factors (λ) -> Weight recovery

- Critical path:
  1. Initialize RPS array with given standard deviation
  2. Apply STABLE-RPS mapping to recover weights
  3. Scale gradients using Γ before update step
  4. Update RPS array using scaled gradients

- Design tradeoffs:
  - Memory vs. computation: Parameter sharing reduces memory but may increase computation
  - Cache efficiency vs. load factor: RANDOMFOLD mapping balances these concerns
  - Stability vs. convergence speed: Gradient scaling improves stability but may slow convergence

- Failure signatures:
  - Divergence: Indicates inappropriate gradient scaling or initialization
  - Slow learning: Suggests need for larger learning rates or better gradient scaling
  - Poor accuracy: May indicate sub-optimal mapping or insufficient parameter sharing

- First 3 experiments:
  1. Verify STABLE-RPS mapping produces non-degenerate embeddings at various compression rates
  2. Test gradient scaling mechanism on simple linear model with known scaling factors
  3. Compare STABLE-RPS against RAND pruning on CIFAR-10 with ResNet-20

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RPS-based compression compare to pruning on complex architectures like transformers or larger vision models?
- Basis in paper: Explicit suggestion that RPS-based standard modules could outperform current architectures
- Why unresolved: Paper focuses on simpler architectures (RESNET20, VGG11)
- What evidence would resolve it: Experiments applying RPS to transformers and larger vision models comparing performance against pruning

### Open Question 2
- Question: Can post-training parameter sharing schemes achieve comparable performance with reduced computational overhead?
- Basis in paper: Discussion of need for post-training schemes to realize full RPS benefits
- Why unresolved: Current RPS methods require retraining from scratch
- What evidence would resolve it: Development and evaluation of post-training parameter sharing scheme on various datasets and architectures

### Open Question 3
- Question: How does RPS performance vary with different data distributions, especially imbalanced or power-law characteristics?
- Basis in paper: Theoretical analysis suggests RPS superiority with imbalanced data distributions
- Why unresolved: Experiments focus on balanced datasets (CIFAR-10, CIFAR-100, TINY-IMAGENET)
- What evidence would resolve it: Experiments on datasets with varying imbalance or power-law characteristics comparing RPS and pruning performance

## Limitations

- Theoretical analysis restricted to linear models, limiting generalization to deep neural networks
- Confidence in outperforming highly informed pruning (LTR) is medium due to specific hyperparameter choices
- RANDOMFOLD mapping may introduce cache efficiency concerns not fully explored

## Confidence

- STABLE-RPS vs moderately informed pruning: High
- Theoretical capacity advantage: Medium (linear models only)
- STABLE-RPS vs highly informed pruning: Medium (compression-dependent)

## Next Checks

1. Test STABLE-RPS on non-linear architectures (Transformer-based models) to validate theoretical capacity advantages beyond linear approximations
2. Benchmark cache efficiency and memory access patterns of RANDOMFOLD mapping against standard pruning implementations
3. Conduct ablation studies varying LTR rewind iteration counts to determine if observed advantages persist across hyperparameter settings