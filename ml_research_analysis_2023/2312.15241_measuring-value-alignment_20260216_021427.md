---
ver: rpa2
title: Measuring Value Alignment
arxiv_id: '2312.15241'
source_url: https://arxiv.org/abs/2312.15241
tags:
- values
- alignment
- norms
- value
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a formalism to quantify the alignment between
  AI systems and human values using Markov Decision Processes (MDPs). It models the
  world as an MDP and defines values as desirable goals tied to actions, and norms
  as behavioral guidelines.
---

# Measuring Value Alignment

## Quick Facts
- arXiv ID: 2312.15241
- Source URL: https://arxiv.org/abs/2312.15241
- Reference count: 18
- Primary result: Formalism quantifies AI-human value alignment using MDPs and preference changes

## Executive Summary
This paper proposes a mathematical framework to quantify the alignment between AI systems and human values using Markov Decision Processes (MDPs). The key insight is to model values as desirable goals tied to actions, and norms as behavioral guidelines, then compute the degree of alignment by evaluating how applying norms changes preferences over state transitions. This provides a technical approach for reasoning about value alignment in AI systems, moving from informal notions to a precise methodology. The framework allows mathematically comparing different norms by their alignment degrees with values.

## Method Summary
The formalism models the world as an MDP and defines values as preference relations over state pairs. Norms are represented as rules that modify the transition function, creating a "normative world" where alignment can be measured. The degree of alignment is computed by evaluating preference changes across all paths in the normative world, providing a scalar metric that can be compared across different norms. The approach involves defining the MDP, specifying values and norms, applying norms to create normative worlds, enumerating paths, computing preference changes, and aggregating to get alignment degrees.

## Key Results
- Provides a mathematical framework for quantifying AI-human value alignment
- Defines alignment degree as average preference changes across state transitions under norms
- Enables comparison of different norms by their alignment with values
- Identifies simplifying assumptions and extensions needed for real-world complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The formalism quantifies alignment by measuring average preference changes across state transitions under a norm.
- Mechanism: Alignment degree is computed as the average of value-based preference differences over all paths in the normative world induced by applying the norm. This provides a scalar metric that can be compared across different norms.
- Core assumption: Preferences over state pairs are well-defined and can be aggregated linearly across transitions and paths.
- Evidence anchors:
  - [abstract] "This framework offers a mechanism to evaluate the degree of alignment between norms and values by assessing preference changes across state transitions in a normative world."
  - [section 5] "The degree of alignment of norm n to value v for agent α in world (S, A, T) is: DAlignα n,v (S, A, T) = 1/|P| ∑ p∈P 1/|p| |p|∑ d=1 Rprαv (pI d, pF d)"
  - [corpus] Weak - no direct matching evidence found in corpus papers.
- Break condition: If preference aggregation is non-linear or path probabilities are highly skewed, the average may misrepresent true alignment.

### Mechanism 2
- Claim: Norms modify the transition function to create a "normative world" where alignment can be measured.
- Mechanism: Applying a norm n to an MDP (S, A, T) yields a new transition system Tn = (Sn, A, fn) where the norm influences which transitions are possible or preferred. Alignment is then evaluated within this modified space.
- Core assumption: Norms can be formally represented as constraints or modifications on MDP transitions.
- Evidence anchors:
  - [section 3.2] "Implementing a set of norms N to a world (S, A, T) results in a normative world (S, A, N, TN ), where the norms influence the transitions"
  - [section 4] "For a norm n, the transition system that emerges from applying n to T is Tn"
  - [corpus] No direct evidence in corpus - this appears to be a novel contribution.
- Break condition: If norms cannot be expressed as transition modifications (e.g., norms requiring global state changes), the formalism breaks down.

### Mechanism 3
- Claim: Alignment can be compared across norms by computing relative alignment degrees.
- Mechanism: The relative alignment of norm n1 compared to n2 for value v is defined as DAlignα n1,n2,v = DAlignα n1,v - DAlignα n2,v. Positive values indicate n1 better aligns with v than n2.
- Core assumption: Alignment degrees are comparable across norms and can be meaningfully subtracted.
- Evidence anchors:
  - [section 5.1 Example 1] Shows calculation where DAlignn2,Safety = -0.22 < DAlignn1,Safety, demonstrating that n1 is better aligned with Safety than n2.
  - [section 5] "The degree of alignment Ralign of norm n1 compared to norm n2 for value v in world (S, A, T) describes how much more n1 is aligned with v than n2"
  - [corpus] No matching evidence in corpus papers.
- Break condition: If alignment degrees are on different scales or not commensurable, relative comparisons become meaningless.

## Foundational Learning

- Markov Decision Processes (MDPs)
  - Why needed here: The formalism models AI decision-making as an MDP to provide a mathematical framework for representing states, actions, and transitions where alignment can be computed.
  - Quick check question: What are the four components of an MDP and how do they relate to AI decision-making?

- Value-based preference relations
  - Why needed here: Values are formalized as preference relations over state pairs, allowing mathematical quantification of how much one state is preferred to another for a given value.
  - Quick check question: How does the preference relation Rpr map state pairs and values to preference degrees?

- Normative systems
  - Why needed here: Norms are formalized as rules that modify the transition function of an MDP, creating a "normative world" where alignment can be measured.
  - Quick check question: How does applying a norm to an MDP change the transition system and why is this important for measuring alignment?

## Architecture Onboarding

- Component map:
  MDP model (states, actions, transitions) -> Value function (Φv, fv for each value v) -> Preference relation (Rpr) -> Norm application mechanism (creating Tn from T) -> Alignment computation module (path enumeration, preference aggregation)

- Critical path:
  1. Define MDP and values
  2. Specify norms
  3. Apply norms to create normative worlds
  4. Enumerate paths in normative worlds
  5. Compute preference changes along paths
  6. Aggregate to get alignment degrees
  7. Compare across norms

- Design tradeoffs:
  - Computational complexity vs. accuracy (path enumeration is expensive in large MDPs)
  - Deterministic vs. stochastic transitions (current formalism assumes deterministic)
  - Linear vs. non-linear preference aggregation (current assumes additive)

- Failure signatures:
  - High variance in alignment degrees across different path samples
  - Negative alignment degrees (indicating norms actively misalign with values)
  - Numerical instability in preference aggregation
  - Combinatorial explosion in path enumeration

- First 3 experiments:
  1. Implement simple MDP with 3-5 states and 2 values, test alignment computation on known cases
  2. Compare alignment degrees for different norms on a recommendation system MDP
  3. Test sensitivity of alignment to preference function choices and path sampling methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the formalism be extended to handle uncertainty and stochastic transitions in MDPs?
- Basis in paper: [explicit] The paper states "Extensions could incorporate path probabilities based on environment dynamics and agent policies" and "Extensions could consider more complex AI architectures and stochastic transitions."
- Why unresolved: The current formalism assumes deterministic transitions, which is a simplification of real-world environments that often display non-Markovian and stochastic characteristics.
- What evidence would resolve it: Developing and testing extensions to the formalism that incorporate probabilistic transitions and partially observable MDPs in complex environments.

### Open Question 2
- Question: What is the computational complexity of calculating alignment degrees in large MDPs?
- Basis in paper: [explicit] The paper mentions "Calculating the computational complexity in large MDPs is intractable due to combinatorial path enumeration."
- Why unresolved: The paper does not provide a concrete analysis of the computational complexity of the alignment calculations, which is crucial for understanding the scalability of the approach.
- What evidence would resolve it: A detailed analysis of the computational complexity of the alignment calculations, including the impact of MDP size and the number of values and norms.

### Open Question 3
- Question: How can the formalism be used to monitor and ensure value alignment in dynamic environments where norms, values, and environmental factors evolve over time?
- Basis in paper: [inferred] The paper discusses the importance of aligning AI systems with human values and mentions that "Monitoring mechanisms can evaluate value alignment performance." However, it does not address how to handle the dynamic nature of norms, values, and environments.
- Why unresolved: The current formalism does not account for the evolving nature of societal norms, individual values, and environmental factors, which is essential for maintaining alignment in real-world scenarios.
- What evidence would resolve it: Developing a framework that can adapt to changes in norms, values, and environments over time, and testing its effectiveness in dynamic scenarios.

## Limitations
- Assumes deterministic transitions, limiting applicability to real-world scenarios with uncertainty
- Linear preference aggregation may not capture complex human value structures
- Computational complexity grows exponentially with MDP size due to path enumeration
- No empirical validation on real AI systems or datasets

## Confidence
- **High Confidence**: The mathematical framework is internally consistent and the alignment computation mechanism is well-defined
- **Medium Confidence**: The simplifying assumptions (deterministic transitions, linear preferences) are reasonable first approximations but need validation
- **Low Confidence**: The practical utility of the alignment metric in real AI systems has not been demonstrated

## Next Checks
1. Test the formalism on a stochastic MDP to evaluate how well it handles uncertainty
2. Compare alignment degrees computed by the formalism with human judgments on simple scenarios
3. Implement a scalable approximation method for path enumeration to handle larger MDPs