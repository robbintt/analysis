---
ver: rpa2
title: 'Decom--CAM: Tell Me What You See, In Details! Feature-Level Interpretation
  via Decomposition Class Activation Map'
arxiv_id: '2306.04644'
source_url: https://arxiv.org/abs/2306.04644
tags:
- maps
- saliency
- methods
- interpretability
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new two-stage interpretability method called
  the Decomposition Class Activation Map (Decom-CAM), which offers a feature-level
  interpretation of the model's prediction. Decom-CAM decomposes intermediate activation
  maps into orthogonal features using singular value decomposition and generates saliency
  maps by integrating them.
---

# Decom--CAM: Tell Me What You See, In Details! Feature-Level Interpretation via Decomposition Class Activation Map

## Quick Facts
- arXiv ID: 2306.04644
- Source URL: https://arxiv.org/abs/2306.04644
- Reference count: 35
- Primary result: Decom-CAM significantly outperforms state-of-the-art interpretability methods by generating more precise saliency maps that capture semantic components like eyes, noses, and faces through orthogonal feature decomposition.

## Executive Summary
Decom-CAM introduces a two-stage interpretability method that decomposes intermediate activation maps into orthogonal features using singular value decomposition (SVD), enabling precise localization of semantic components in input images. By integrating these orthogonal feature maps, Decom-CAM generates saliency maps that reveal the specific features CNNs rely on for classification decisions. The method addresses limitations of existing CAM approaches by removing redundant information and capturing local features, resulting in more interpretable visualizations. A novel evaluation protocol divides datasets by classification accuracy to provide comprehensive assessment across different levels of model competence.

## Method Summary
Decom-CAM operates in two stages: first, it computes class-discriminative activation maps and applies SVD to obtain orthogonal feature maps (OFMs) that preserve most variance information; second, these OFMs are upsampled to input size and integrated using activation-based CAM weights to generate final saliency maps. The method is evaluated on Mini-ImageNet and PASCAL VOC 2012 datasets using a new protocol that divides test data by classification accuracy to assess interpretability performance at different competence levels. The approach leverages CLIP's (ResNet-50) trained features and employs pointing game accuracy and deletion/insertion AUC scores for evaluation.

## Key Results
- Decom-CAM achieves significantly higher pointing game accuracy than state-of-the-art methods across all accuracy-based subsets
- The method generates saliency maps that precisely localize semantic components like eyes, noses, and faces in input images
- Decom-CAM demonstrates superior performance in deletion/insertion tests, indicating better preservation of discriminative features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decom-CAM decomposes activation maps into orthogonal features that preserve most variance information while removing redundant correlation.
- Mechanism: SVD decomposes the class-discriminative activation map Sk into singular vectors and values. The top-p singular vectors form an orthogonal basis that retains the primary variance in the original activation map while eliminating redundant information.
- Core assumption: The top singular vectors correspond to the most salient features used by the model for classification.
- Evidence anchors:
  - [abstract] "Decom-CAM decomposes intermediate activation maps into orthogonal features using singular value decomposition and generates saliency maps by integrating them."
  - [section] "According to the principles of linear algebra, the singular vectors are known to convey a significant proportion of the variance information contained in S*k [14, 1]."
  - [corpus] Weak evidence - no direct corpus support for SVD preserving variance in CAM context.
- Break condition: If the singular vectors do not capture the most discriminative features, or if the orthogonality assumption breaks down for non-linear activations.

### Mechanism 2
- Claim: The orthogonal feature maps (OFMs) can be used to generate local saliency maps that highlight specific semantic components in the input image.
- Mechanism: Each OFM is upsampled to input image size and used as a mask to occlude regions of the input. The change in classification confidence when each region is occluded reveals which features are most important for the prediction.
- Core assumption: The OFMs correspond to semantically meaningful features that the model relies on for classification.
- Evidence anchors:
  - [abstract] "The orthogonality of features enables CAM to capture local features and can be used to pinpoint semantic components such as eyes, noses, and faces in the input image."
  - [section] "By extending these OFMs to the original image size, we can obtain local saliency maps that reveal the salient features upon which CNNs rely to make decisions."
  - [corpus] Weak evidence - no direct corpus support for OFMs mapping to semantic components.
- Break condition: If the OFMs do not correspond to interpretable semantic features, or if the upsampling process introduces artifacts that distort the feature representation.

### Mechanism 3
- Claim: The proposed evaluation protocol provides a more comprehensive assessment of interpretability methods by considering the correlation between interpretability performance and model prediction accuracy.
- Mechanism: The test dataset is divided into subsets based on classification accuracy, and interpretability performance is evaluated separately on each subset. This reveals how interpretability methods perform at different levels of model competence.
- Core assumption: Interpretability performance is inherently limited by the model's decision quality, and methods should be evaluated under distinct levels of task performance.
- Evidence anchors:
  - [abstract] "We introduce a new evaluation protocol that addresses the previous oversight regarding the correlation between interpretability performance and the model's decision quality."
  - [section] "This protocol promises that we can obtain a comprehensive assessment of different CAM methods in a rigorous and appropriate manner."
  - [corpus] No direct evidence in corpus for this specific protocol.
- Break condition: If the accuracy-based subset division does not capture meaningful differences in interpretability performance, or if the evaluation protocol itself introduces bias.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is the core mathematical operation that decomposes the activation map into orthogonal features while preserving variance information.
  - Quick check question: What property of SVD ensures that the top singular vectors capture the most variance in the data?

- Concept: Class Activation Maps (CAM)
  - Why needed here: CAM is the foundational technique that Decom-CAM builds upon, using activation maps and class weights to generate saliency maps.
  - Quick check question: How does the original CAM method differ from gradient-based CAM methods like Grad-CAM?

- Concept: Feature orthogonality
  - Why needed here: The orthogonality of the decomposed features is what enables Decom-CAM to remove redundant information and focus on distinct, interpretable features.
  - Quick check question: Why is feature orthogonality important for interpretability in neural networks?

## Architecture Onboarding

- Component map: Input image → Forward pass through CNN → Extract intermediate activation map → Compute gradients for target class → Apply Hadamard product → SVD decomposition → Select top-p orthogonal features → Reshape to spatial dimensions → Upsample to input size → Occlusion testing → Compute contribution weights → Integrate local saliency maps → Final saliency map

- Critical path: Forward pass → Activation map extraction → Gradient computation → SVD decomposition → OFM upsampling → Occlusion testing → Saliency map integration

- Design tradeoffs:
  - SVD decomposition vs. other dimensionality reduction techniques: SVD provides orthogonality but may be computationally expensive for large activation maps
  - Number of OFMs to retain (p): More OFMs capture more information but increase computational cost and may introduce noise
  - Gradient-based vs. activation-based weight computation: Gradients provide class-discriminative information but may be affected by gradient vanishing

- Failure signatures:
  - Noisy or fragmented saliency maps: May indicate problems with SVD decomposition or gradient computation
  - Saliency maps that don't align with semantic components: May indicate that OFMs don't correspond to interpretable features
  - Poor performance on the evaluation protocol: May indicate that the method doesn't effectively capture model decision-making process

- First 3 experiments:
  1. Apply Decom-CAM to a simple CNN (e.g., ResNet-18) on CIFAR-10 and visualize the OFMs to verify they capture semantically meaningful features
  2. Compare the cumulative explained variance of SVD decomposition across different CNN architectures to validate the variance preservation claim
  3. Implement the proposed evaluation protocol on a subset of Mini-ImageNet to test whether interpretability performance correlates with model accuracy as claimed

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implicit questions emerge from the methodology and results:

1. How does the size of the model (e.g., ResNet-50 vs ResNet-101) affect the interpretability performance of Decom-CAM, particularly in terms of identifying fine-grained features?
2. Can Decom-CAM be effectively applied to other types of deep learning models beyond CNNs, such as transformers or recurrent neural networks (RNNs)?
3. How does the choice of the number of orthogonal feature maps (OFMs) to retain affect the interpretability performance of Decom-CAM?

## Limitations

- The semantic interpretability claims lack direct empirical validation, relying on assertions rather than systematic verification of OFMs corresponding to semantic components
- The variance preservation claim is supported by general SVD properties but not specifically demonstrated for CAM applications in the CLIP architecture context
- The proposed evaluation protocol, while novel, has not been tested across multiple architectures to verify its generalizability

## Confidence

- High confidence: The mathematical framework of SVD decomposition and the evaluation protocol design
- Medium confidence: The variance preservation claim in the CAM context, as it relies on general SVD properties
- Low confidence: The semantic interpretability claims and the effectiveness of the proposed evaluation protocol, as these lack direct empirical support

## Next Checks

1. Conduct ablation studies removing the SVD decomposition step to quantify the actual contribution of orthogonality to interpretability performance
2. Perform component analysis on the top singular vectors to verify they correspond to semantically meaningful features rather than arbitrary directions in feature space
3. Test the evaluation protocol on multiple architectures beyond CLIP to assess whether the correlation between interpretability and accuracy holds across different model families