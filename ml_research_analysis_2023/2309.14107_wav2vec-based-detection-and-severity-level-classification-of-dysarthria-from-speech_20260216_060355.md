---
ver: rpa2
title: Wav2vec-based Detection and Severity Level Classification of Dysarthria from
  Speech
arxiv_id: '2309.14107'
source_url: https://arxiv.org/abs/2309.14107
tags:
- speech
- features
- dysarthric
- wav2vec
- severity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dysarthric speech detection and severity level classification is
  a useful tool for speech-language pathologists. This study explored wav2vec 2.0
  as a feature extractor for these tasks using the UA-speech database.
---

# Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech

## Quick Facts
- arXiv ID: 2309.14107
- Source URL: https://arxiv.org/abs/2309.14107
- Reference count: 0
- Dysarthric speech detection and severity classification using wav2vec 2.0 embeddings outperforms traditional baseline features

## Executive Summary
This study investigates the use of wav2vec 2.0 as a feature extractor for detecting dysarthric speech and classifying severity levels using the UA-speech database. The research demonstrates that embeddings from different wav2vec layers perform optimally for different tasks: first-layer embeddings excel at binary dysarthria detection, while final-layer embeddings are superior for multi-class severity classification. SVM classifiers with RBF kernels effectively leverage these features, achieving significant improvements over traditional baseline features like spectrograms and MFCCs.

## Method Summary
The study used the UA-speech database (765 isolated words from 15 dysarthric speakers and 13 healthy controls) and employed a pre-trained wav2vec 2.0 model to extract 768-dimensional embeddings from each non-overlapping 20ms frame. These embeddings were averaged over time to create utterance-level feature vectors for layers 1-13. SVM classifiers with RBF kernels were trained using leave-one-speaker-out cross-validation for detection and 4-fold cross-validation for severity classification, with z-score normalization applied per fold.

## Key Results
- For detection (healthy vs. dysarthric), first-layer wav2vec embeddings achieved 1.23% absolute improvement in accuracy over spectrogram baseline
- For severity level classification (very low, low, medium, high), final-layer wav2vec embeddings achieved 10.62% absolute improvement in accuracy over MFCC baseline
- Wav2vec embeddings consistently outperformed all baseline features across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The first layer of wav2vec 2.0 captures generic speech representations that distinguish dysarthric from healthy speech effectively.
- Mechanism: The initial transformer layer in wav2vec 2.0 learns general acoustic patterns before specializing in phoneme-level features in deeper layers. For dysarthria detection, where distinguishing pathological vs. normal speech patterns is the primary goal, these general representations prove more discriminative than later layers that focus on linguistic content.
- Core assumption: The wav2vec model's initial layers extract features useful for binary classification tasks beyond ASR, particularly for detecting pathological speech patterns.
- Evidence anchors:
  - [abstract]: "best performance was obtained using the embeddings from the first layer of the wav2vec model"
  - [section]: "wav2vec-1 outperformed all the other features in all metrics. In particular, when compared to the best performing baseline feature (spectrogram), wav2vec-1 showed an absolute improvement of 1.23% in accuracy"
- Break condition: If dysarthric speech patterns are highly specific to certain phonemes or linguistic contexts that require deeper layer understanding, the first layer may miss critical discriminative features.

### Mechanism 2
- Claim: Final layers of wav2vec 2.0 capture linguistic content that correlates with speech intelligibility, making them effective for severity classification.
- Mechanism: As wav2vec 2.0 progresses through transformer layers, it extracts increasingly abstract representations including phoneme identity and linguistic content. Since dysarthria severity correlates with speech intelligibility (phoneme clarity), the final layers that capture these linguistic features provide better discriminative power for multi-class severity classification.
- Core assumption: The severity of dysarthria directly correlates with the degradation of linguistic content clarity, which can be captured by ASR-oriented feature extraction.
- Evidence anchors:
  - [abstract]: "best performance was obtained using embeddings from the final layer, giving an absolute improvement of 10.62% in accuracy"
  - [section]: "Unlike in the detection problem, the best-performing wav2vec features were the ones obtained from the final layers. This result was expected because the severity of dysarthria is associated with the intelligibility of speech"
- Break condition: If severity assessment requires features beyond linguistic content (e.g., specific articulatory or prosodic patterns), final layer embeddings may miss these discriminative elements.

### Mechanism 3
- Claim: SVM with RBF kernel effectively leverages wav2vec features for both binary and multi-class dysarthria classification tasks.
- Mechanism: Support Vector Machines with radial basis function kernels can effectively map high-dimensional wav2vec embeddings into discriminative spaces for classification. The RBF kernel handles non-linear decision boundaries that may exist between healthy/dysarthric speech and between different severity levels.
- Core assumption: The decision boundaries between speech classes (healthy vs dysarthric, and severity levels) are non-linear and can be effectively captured by RBF kernels operating on high-dimensional wav2vec features.
- Evidence anchors:
  - [section]: "For the multi-class classification between the four severity levels (very low, low, medium, and high), SVM with the one-vs-one architecture was used"
  - [section]: "For the multi-class classification between the four severity levels (very low, low, medium, and high), SVM with the one-vs-one architecture was used"
- Break condition: If the classification problem requires capturing complex temporal dependencies or sequential patterns, SVM's inability to model such relationships may limit performance.

## Foundational Learning

- Concept: Wav2vec 2.0 architecture and feature extraction
  - Why needed here: Understanding which layers capture what information is crucial for selecting appropriate features for different tasks (detection vs. severity classification)
  - Quick check question: Which wav2vec 2.0 layers would you use for a new task that requires detecting subtle prosodic abnormalities in dysarthric speech?

- Concept: Support Vector Machine fundamentals (kernel selection, regularization)
  - Why needed here: SVM parameters (RBF kernel, C value) significantly impact performance on wav2vec features; understanding these choices helps in model optimization
  - Quick check question: How would you modify SVM parameters if you observed that wav2vec features from different layers have vastly different variance distributions?

- Concept: Dysarthria assessment methodology and speech intelligibility metrics
  - Why needed here: Understanding how speech intelligibility relates to dysarthria severity helps interpret why certain features (particularly from final wav2vec layers) work better for severity classification
  - Quick check question: How would you design a feature selection process if you discovered that intelligibility scores don't perfectly correlate with wav2vec feature distances between severity classes?

## Architecture Onboarding

- Component map: Speech signal → Wav2vec 2.0 feature extractor (layer selection) → Feature averaging (20ms frames) → SVM classifier (RBF kernel, one-vs-one for multi-class) → Classification output
- Critical path: Feature extraction from the appropriate wav2vec layer → Feature vector averaging → SVM training/testing with LOSO or stratified cross-validation
- Design tradeoffs: Layer selection (first vs. final layers) involves balancing between general speech patterns and linguistic content; SVM parameters involve balancing margin maximization vs. training error
- Failure signatures: Poor detection performance may indicate wrong layer selection (using final layers for detection); poor severity classification may indicate using first layers that miss linguistic content
- First 3 experiments:
  1. Reproduce detection results using wav2vec-1 vs. spectrogram baseline with same SVM parameters
  2. Compare severity classification performance across all wav2vec layers to identify optimal layer
  3. Perform ablation study removing specific transformer blocks to understand layer contribution to performance

## Open Questions the Paper Calls Out

- How do wav2vec features generalize across different speech disorders and databases?
- What is the optimal layer selection strategy for wav2vec features in different dysarthria severity levels?
- How do wav2vec features compare to fine-tuned wav2vec models for dysarthria assessment tasks?

## Limitations

- The study used a relatively small dataset (765 utterances from 15 dysarthric and 13 healthy speakers) which may not generalize to broader populations
- SVM was chosen as the classifier without exploring more modern deep learning approaches that might leverage wav2vec features differently
- The wav2vec model was used as a frozen feature extractor rather than fine-tuned for the specific tasks, potentially leaving performance gains unrealized

## Confidence

**Major uncertainties**: High confidence in core finding that wav2vec embeddings outperform traditional baseline features. Medium confidence in layer-specific recommendations based on single dataset. Low confidence in generalizability without comparison to modern classification methods.

## Next Checks

1. Test the layer selection hypothesis on an independent dysarthric speech corpus to verify that first layers consistently outperform deeper layers for detection and vice versa for severity classification
2. Implement a fine-tuning approach where wav2vec weights are updated during classifier training to assess whether performance improvements over frozen features can be achieved
3. Compare SVM performance against transformer-based classifiers (such as linear probes or lightweight fine-tuned heads) using the same wav2vec features to establish whether the classification method or features drive performance differences