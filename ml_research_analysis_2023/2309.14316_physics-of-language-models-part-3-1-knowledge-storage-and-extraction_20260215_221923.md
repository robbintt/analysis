---
ver: rpa2
title: 'Physics of Language Models: Part 3.1, Knowledge Storage and Extraction'
arxiv_id: '2309.14316'
source_url: https://arxiv.org/abs/2309.14316
tags:
- bios
- single
- knowledge
- data
- fullname
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how language models (LMs) extract factual
  knowledge from text, addressing the question of whether such extraction is genuine
  or just memorization of seen question-answer pairs. Using a controlled synthetic
  biography dataset and a close-to-real dataset generated by LLaMA, the authors pretrain
  GPT2 and BERT-style models and then probe their knowledge storage and extraction
  using question-answering tasks.
---

# Physics of Language Models: Part 3.1, Knowledge Storage and Extraction

## Quick Facts
- arXiv ID: 2309.14316
- Source URL: https://arxiv.org/abs/2309.14316
- Reference count: 40
- Key outcome: Without knowledge augmentation in pretraining, models memorize but cannot extract facts; with augmentation, they encode knowledge linearly in name embeddings enabling successful QA.

## Executive Summary
This paper investigates whether language models genuinely extract factual knowledge from text or merely memorize seen question-answer pairs. Using controlled synthetic biography datasets, the authors pretrain GPT2 and BERT-style models and probe their knowledge storage and extraction using question-answering tasks. They find that without knowledge augmentation (e.g., varied sentence templates, permutations, or full-name repetition) in pretraining data, models achieve near-zero accuracy even after fine-tuning. However, with sufficient augmentation, models learn to store knowledge nearly linearly in embeddings of entity names, enabling successful extraction. The study demonstrates that augmenting pretraining data is crucial for enabling robust knowledge extraction in LMs.

## Method Summary
The authors generate synthetic biography datasets (bioS) with 100k individuals, each described by six attributes across six sentences. They pretrain GPT2-small and BERT-style models on this data using standard autoregressive training with AdamW optimizer and cosine learning rate decay. The models are then fine-tuned on question-answer pairs for half the individuals using LoRA fine-tuning, and evaluated on the remaining half. The study systematically varies knowledge augmentation techniques including multiplicity (repeating facts), permutation (reordering sentences), and fullname repetition. They employ P-probing and Q-probing techniques to analyze how knowledge is encoded in model embeddings.

## Key Results
- Without knowledge augmentation, models memorize training data token-by-token but achieve near-zero QA accuracy on unseen individuals
- With sufficient augmentation (multiplicity, permutation, fullname), models learn to store knowledge nearly linearly in name embeddings, achieving high extraction accuracy
- Including augmented "celebrity" data in pretraining significantly improves knowledge extraction for non-augmented "minority" individuals through transfer learning
- P-probing and Q-probing reveal that augmentation forces models to encode attributes directly adjacent to names, while non-augmented data leads to distributed, harder-to-extract representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Without knowledge augmentation in pretraining, factual knowledge is memorized but not linearly encoded in the hidden embeddings of entity names, making extraction nearly impossible.
- Mechanism: The model memorizes the training data at a token level, but the knowledge is distributed across many tokens in the sentence rather than being directly linked to the entity name embeddings. This distribution requires complex nonlinear transformations to retrieve, which QA finetuning cannot overcome.
- Core assumption: Knowledge extraction depends on the model's ability to form a direct, nearly linear mapping from entity names to their attributes.
- Evidence anchors:
  - [abstract] "Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning."
  - [section 4.1] "Despite a 99+% first-token accuracy during pretraining... the model exhibits near-zero QA accuracy on Ptest for all finetuning parameters."
  - [corpus] Weak or missing explicit corpus evidence for this specific mechanism; paper relies on synthetic experiments.
- Break condition: If the pretraining data includes sufficient diversity (varied templates, permutations, full-name repetition), the model learns to encode attributes linearly adjacent to entity names.

### Mechanism 2
- Claim: Knowledge augmentation in pretraining compels the model to encode attributes nearly linearly in the hidden states adjacent to entity names, enabling successful extraction.
- Mechanism: Augmentation introduces multiple ways of expressing the same fact, forcing the model to abstract away surface form and store the core relationship between entity and attribute directly in the name's embedding space.
- Core assumption: Repeated exposure to the same fact in varied linguistic forms encourages the model to learn a direct, abstract representation rather than memorizing surface patterns.
- Evidence anchors:
  - [abstract] "with sufficient augmentation, models learn to store knowledge nearly linearly in embeddings of entity names, enabling successful extraction."
  - [section 5.2] "After applying knowledge augmentations... Q-probing accuracy significantly increases... the model encodes knowledge almost linearly in the hidden states directly adjacent to the person's name."
  - [corpus] Weak corpus evidence; conclusions drawn from controlled synthetic experiments.
- Break condition: If augmentation is removed or insufficient, the model reverts to distributed encoding and extraction fails.

### Mechanism 3
- Claim: Including augmented "celebrity" data in pretraining improves knowledge extraction for non-augmented "minority" individuals by teaching the model to encode knowledge linearly near entity names.
- Mechanism: Celebrity data provides diverse examples that teach the model the general strategy of linear encoding. This learned strategy then transfers to minority data during pretraining, even without direct augmentation of minority examples.
- Core assumption: The model can generalize the linear encoding strategy learned from celebrity data to minority data during pretraining, before QA finetuning.
- Evidence anchors:
  - [abstract] "incorporating more instruction-finetuning data into the pretraining stage before it becomes too late."
  - [section 4.3] "the model's ability to store and extract knowledge from the minority group... significantly improves the model's ability to store and extract knowledge from the minority group."
  - [corpus] Weak corpus evidence; based on synthetic experiment design.
- Break condition: If celebrity data is too different in format or domain from minority data, transfer may not occur.

## Foundational Learning

- Concept: Linear probing as a technique to measure how knowledge is encoded in model embeddings.
  - Why needed here: To determine whether knowledge is stored in a way that allows direct extraction (linearly) or requires complex inference (distributed).
  - Quick check question: If a linear classifier trained on hidden states can predict an attribute with high accuracy, what does that say about how the attribute is encoded?

- Concept: Difference between memorization and knowledge extraction.
  - Why needed here: The paper distinguishes between simply reproducing training data (memorization) and being able to answer novel questions about that data (extraction).
  - Quick check question: If a model can perfectly reproduce a sentence but fails to answer a related question, what does that reveal about its internal representation?

- Concept: Knowledge augmentation through paraphrasing, permutation, and full-name repetition.
  - Why needed here: These techniques are shown to be critical for enabling the model to learn to extract knowledge rather than just memorize surface forms.
  - Quick check question: How might generating multiple paraphrases of the same fact help a model learn to abstract away from specific wording?

## Architecture Onboarding

- Component map: GPT2-style transformer with rotary positional embeddings -> LoRA fine-tuning on query/value matrices and embedding layer -> P-probing and Q-probing using linear classifiers on frozen model outputs
- Critical path: Pretrain on augmented biography data → Freeze model → Apply LoRA updates for QA task → Evaluate extraction accuracy on unseen individuals
- Design tradeoffs: Full fine-tuning vs. LoRA (parameter efficiency vs. potential performance); larger models vs. smaller models (capacity vs. overfitting risk); more augmentation vs. less (extraction ability vs. data efficiency)
- Failure signatures: Near-zero QA accuracy on unseen individuals despite high pretraining token accuracy; P-probing shows low accuracy far from attribute tokens; Q-probing shows low accuracy on name tokens
- First 3 experiments:
  1. Pretrain on bioS single (no augmentation) → LoRA fine-tune on QA for Ptrain → Test on Ptest → Expect near-zero accuracy
  2. Pretrain on bioS multi5+permute (heavy augmentation) → LoRA fine-tune on QA for Ptrain → Test on Ptest → Expect high accuracy
  3. Pretrain on bioS single + celebrity data (bioS multi5+permute for celebrity) → LoRA fine-tune on QA for celebrity only → Test on minority → Expect improved accuracy vs. no celebrity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of knowledge augmentation (e.g., multiplicity, permutation, fullname) differentially impact the storage and extractability of various types of knowledge attributes (e.g., birth date vs. company city)?
- Basis in paper: Explicit - The paper discusses various knowledge augmentation techniques and their impact on knowledge extraction, noting that company city consistently shows weaker performance compared to other attributes.
- Why unresolved: The paper demonstrates the general effectiveness of augmentation but doesn't provide a detailed analysis of how different augmentation types specifically benefit different attribute types.
- What evidence would resolve it: Experiments systematically varying augmentation types and measuring their differential impact on various attribute extraction accuracies, potentially revealing attribute-specific augmentation strategies.

### Open Question 2
- Question: To what extent can bidirectional models like BERT overcome the knowledge extraction limitations observed in GPT models, and what specific architectural or training modifications might improve their performance?
- Basis in paper: Explicit - The paper explicitly compares GPT and BERT models, finding that BERT struggles with knowledge extraction despite its bidirectional nature, particularly for attributes with strong inter-dependencies.
- Why unresolved: While the paper identifies BERT's limitations, it doesn't explore potential architectural modifications or alternative training objectives that might improve its knowledge extraction capabilities.
- What evidence would resolve it: Comparative studies of modified BERT architectures (e.g., different attention mechanisms, training objectives) and their knowledge extraction performance on the same controlled biography dataset.

### Open Question 3
- Question: What is the precise mechanism by which knowledge augmentation in pretraining data leads to improved knowledge extraction during fine-tuning, and can this mechanism be mathematically characterized?
- Basis in paper: Explicit - The paper introduces P-probing and Q-probing techniques to analyze knowledge encoding, observing that augmentation leads to more direct name-attribute associations, but doesn't provide a complete mathematical characterization of this mechanism.
- Why unresolved: The paper demonstrates correlations between augmentation and encoding patterns but doesn't establish a rigorous mathematical framework explaining why augmentation produces these specific encoding changes.
- What evidence would resolve it: Theoretical analysis connecting augmentation patterns to changes in the transformer's attention mechanisms and hidden state representations, potentially through mathematical modeling of the knowledge storage process.

## Limitations
- The study relies entirely on synthetic data (bioS) and near-synthetic data (bioR), which may not fully capture the complexity and variability of real-world knowledge extraction tasks
- The exact mechanisms by which knowledge augmentation leads to linear encoding remain somewhat unclear, with the causal chain from augmentation to linear encoding to extraction success not fully established
- The transfer learning mechanism from celebrity to minority individuals, while demonstrated, lacks a clear explanation of how the model generalizes the linear encoding strategy across different data distributions

## Confidence
- **High Confidence**: The core finding that knowledge augmentation enables successful knowledge extraction (Mechanism 2) is well-supported by controlled experiments showing the stark contrast between augmented and non-augmented conditions
- **Medium Confidence**: The mechanism explaining why non-augmented data fails (Mechanism 1) is plausible but relies on indirect evidence and the assumption that distributed encoding necessarily prevents extraction
- **Medium Confidence**: The transfer learning mechanism from celebrity to minority individuals (Mechanism 3) is demonstrated but the explanation for how this transfer occurs is somewhat speculative

## Next Checks
1. **Real-world Generalization Test**: Apply the same knowledge augmentation techniques to a naturally occurring dataset (e.g., Wikipedia biographies) and verify whether the linear encoding and extraction patterns hold, or if additional complexities emerge that weren't captured in synthetic data
2. **Mechanistic Probe Expansion**: Design experiments that manipulate the timing and type of knowledge augmentation during pretraining to isolate which aspects of augmentation (paraphrasing vs. permutation vs. full-name repetition) are most critical for establishing linear encoding
3. **Cross-model Validation**: Test whether the knowledge extraction patterns observed in GPT2-small extend to larger models and different architectures (BERT, T5) to determine if the findings are architecture-dependent or represent fundamental properties of language model knowledge storage