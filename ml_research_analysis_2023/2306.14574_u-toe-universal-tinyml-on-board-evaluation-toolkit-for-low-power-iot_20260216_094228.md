---
ver: rpa2
title: 'U-TOE: Universal TinyML On-board Evaluation Toolkit for Low-Power IoT'
arxiv_id: '2306.14574'
source_url: https://arxiv.org/abs/2306.14574
tags:
- u-toe
- evaluation
- low-power
- performance
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: U-TOE is a universal toolkit designed to facilitate the evaluation
  of machine learning models on low-power IoT devices. The toolkit combines a low-power
  embedded OS, a generic model transpiler and compiler, an integrated performance
  measurement module, and an open-access remote IoT testbed.
---

# U-TOE: Universal TinyML On-board Evaluation Toolkit for Low-Power IoT

## Quick Facts
- arXiv ID: 2306.14574
- Source URL: https://arxiv.org/abs/2306.14574
- Reference count: 17
- One-line primary result: Successfully evaluates machine learning models on low-power IoT devices, measuring memory consumption, storage consumption, and computational latency

## Executive Summary
U-TOE is a comprehensive toolkit designed to facilitate the evaluation of machine learning models on low-power IoT devices. It combines a model compiler (uTVM), a lightweight OS environment (RIOT), and an RPC mechanism to enable universal evaluation across various IoT hardware platforms. The toolkit provides both Per-Model and Per-Operator evaluation modes to assess overall resource consumption and identify performance bottlenecks at the operator level. U-TOE aims to make TinyML evaluation reproducible, customizable, and accessible to researchers and practitioners, whether they have access to physical IoT hardware or utilize remote testbeds.

## Method Summary
U-TOE provides a universal evaluation framework for TinyML models on low-power IoT devices. The toolkit integrates uTVM for model compilation into efficient C code, RIOT for lightweight OS environment, and an RPC mechanism for remote performance measurement. Users can perform two types of evaluations: Per-Model evaluation for overall resource consumption metrics (memory, storage, latency) and Per-Operator evaluation for identifying performance bottlenecks at the operator level. The toolkit supports multiple low-power boards based on ARM Cortex-M and RISC-V architectures and provides an open-source implementation with configurable parameters for reproducible comparative experiments.

## Key Results
- Successfully evaluated various quantized ML models (LeNet-5, DS-CNN Small, MobileNetV1-0.25x, Deep AutoEncoder, RNNoise) on low-power boards
- Measured key performance metrics including memory consumption (RAM), storage consumption (Flash), and computational latency
- Demonstrated universal compatibility across different MCU architectures (ARM Cortex-M and RISC-V)
- Provided insights into performance bottlenecks through Per-Operator evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: U-TOE enables universal evaluation of ML models on arbitrary low-power IoT hardware
- Mechanism: Integration of uTVM for model compilation, RIOT for lightweight OS, and RPC for remote procedure calls
- Core assumption: Integration generalizes across different ML frameworks and MCU architectures
- Evidence anchors: Abstract states "easily reproducible and customizable comparative evaluation experiments on a wide variety of IoT hardware all-at-once"
- Break condition: If integration fails to support new ML frameworks or MCU architectures

### Mechanism 2
- Claim: U-TOE provides performance bottleneck identification at both model and operator granularity levels
- Mechanism: Two-level evaluation - Per-Model for overall metrics, Per-Operator for specific operator efficiency
- Core assumption: Operator-level metrics can be accurately mapped back to original model layers
- Evidence anchors: Section describes "efficiency and resource footprint of each operator, enabling to discover the performance bottleneck inside models"
- Break condition: If operator metrics cannot be accurately mapped to original model layers

### Mechanism 3
- Claim: U-TOE enables reproducible and customizable comparative evaluation experiments
- Mechanism: Open-source implementation with configurable parameters and open-access IoT testbed connector
- Core assumption: Open-source nature and testbed availability ensure reproducibility and customization
- Evidence anchors: Abstract mentions "open source implementation" and testbed use for "large scale experimental comparative evaluation campaigns"
- Break condition: If open-source implementation or testbed becomes unavailable or incompatible

## Foundational Learning

- Concept: TinyML
  - Why needed here: Understanding TinyML context for U-TOE's purpose in evaluating ML on low-power IoT devices
  - Quick check question: What is the primary challenge addressed by TinyML in the context of IoT devices?

- Concept: Model compilation and optimization
  - Why needed here: Essential for understanding how U-TOE uses uTVM to convert ML models into efficient C code
  - Quick check question: What is the role of uTVM in the U-TOE toolkit?

- Concept: Performance metrics and measurement granularity
  - Why needed here: Important for interpreting U-TOE's Per-Model and Per-Operator evaluation results
  - Quick check question: What are the two levels of performance measurement provided by U-TOE, and what insights do they offer?

## Architecture Onboarding

- Component map: Model Compiler (uTVM) -> RPC Mechanism -> OS Environment (RIOT) -> Evaluation Module -> IoT Testbed Connector
- Critical path: Model compilation using uTVM → Firmware generation by combining uTVM-generated library with RIOT, RPC server, and measurement worker → Firmware flashing on target IoT board → Performance measurement using RPC mechanism → Data analysis and presentation on host device
- Design tradeoffs: Balancing model optimization and compatibility across frameworks and architectures; choosing between Per-Model and Per-Operator evaluation; enabling/disabling data and instruction cache
- Failure signatures: Incompatibility with new ML frameworks or MCU architectures; inaccurate operator-to-layer mapping; issues with open-source implementation or testbed affecting reproducibility
- First 3 experiments: 1) Evaluate quantized LeNet-5 on STM32F746G discovery board using Per-Model evaluation; 2) Compare various quantized models on same STM32F746G board using Per-Model evaluation; 3) Perform Per-Operator evaluation on simple TFLite model to identify bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can U-TOE be extended to support on-device learning scenarios?
- Basis in paper: Inferred from mention of potential to extend U-TOE for on-device learning
- Why unresolved: Paper does not provide details on extension implementation
- What evidence would resolve it: Detailed description of proposed extension with implementation details and experimental results

### Open Question 2
- Question: How can U-TOE be optimized to better exploit multi-core microcontrollers?
- Basis in paper: Inferred from mention of optimizing exploitation of multi-core microcontrollers
- Why unresolved: Paper does not provide details on optimization strategies
- What evidence would resolve it: Detailed description of proposed optimization strategies with implementation details and experimental results

### Open Question 3
- Question: How can U-TOE be further improved to support a wider range of IoT hardware platforms and machine learning frameworks?
- Basis in paper: Explicit mention of expanding support as RIOT and uTVM evolve
- Why unresolved: Paper does not provide details on further improvements
- What evidence would resolve it: Detailed description of proposed improvements with implementation details and experimental results

## Limitations
- Universal evaluation claim depends on compatibility between uTVM, RIOT, and diverse MCU architectures
- Performance bottleneck identification assumes accurate mapping between operator metrics and original model layers
- Reproducibility claim relies on continued availability of open-source implementation and IoT-Lab testbed

## Confidence

- Mechanism 1 (Universal evaluation): Medium confidence - Demonstrates broad compatibility but lacks extensive validation across all potential frameworks and architectures
- Mechanism 2 (Bottleneck identification): Medium confidence - Two-level evaluation approach is sound but mapping accuracy needs more rigorous validation
- Mechanism 3 (Reproducibility): Medium confidence - Open-source nature and testbed integration are strong enablers but depend on external factors

## Next Checks

1. Test U-TOE with at least three additional ML frameworks (e.g., PyTorch, ONNX, Caffe) on various MCU architectures not covered in original evaluation to verify universal compatibility

2. Conduct controlled experiment to validate accuracy of operator-level performance metrics mapping back to original model layers by comparing U-TOE results with manual profiling

3. Perform long-term availability assessment by attempting to reproduce key experiments after 6-month interval, documenting any breaking changes in dependencies or toolchain compatibility