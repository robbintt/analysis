---
ver: rpa2
title: Towards Practicable Sequential Shift Detectors
arxiv_id: '2307.14758'
source_url: https://arxiv.org/abs/2307.14758
tags:
- detection
- shift
- test
- data
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper highlights three critical desiderata for practical sequential
  shift detection that are often overlooked in existing research: (1) the ability
  to configure detectors with known behavior in the absence of change, (2) outsourcing
  the burden of identifying discriminative test statistics, and (3) flexibility to
  permit certain types of expected changes. The authors argue that while statistical
  power and computational efficiency are commonly prioritized, these additional requirements
  are essential for real-world deployment.'
---

# Towards Practicable Sequential Shift Detectors

## Quick Facts
- arXiv ID: 2307.14758
- Source URL: https://arxiv.org/abs/2307.14758
- Authors: 
- Reference count: 22
- Key outcome: Existing sequential shift detectors fail in practice due to extreme underestimation of false positive rates when using time-invariant thresholds

## Executive Summary
Sequential shift detection is critical for monitoring ML models in production, but current approaches have significant practical limitations. The paper identifies three key desiderata often overlooked in existing research: configurable detector behavior, outsourcing discriminative test statistic identification, and flexibility to permit expected changes. Through theoretical analysis and empirical evidence, the authors demonstrate that common threshold-setting approaches using time-invariant thresholds result in false detection rates orders of magnitude lower than intended, rendering detectors practically useless due to extremely low statistical power. They argue that addressing these limitations is essential for real-world deployment of sequential shift detectors.

## Method Summary
The paper proposes a four-stage framework for sequential shift detection: (1) selecting a reference dataset and deployment window, (2) projecting data to summary statistics, (3) computing a test statistic to measure distribution differences, and (4) applying a threshold to make detection decisions. While existing work focuses on improving statistical power and computational efficiency, this paper highlights three critical desiderata for practical deployment: the ability to configure detectors with known behavior in the absence of change, outsourcing the burden of identifying discriminative test statistics, and flexibility to permit certain types of expected changes. The authors discuss relevant existing works and identify future research directions for developing holistic frameworks that address all four stages of sequential shift detection.

## Key Results
- Time-invariant threshold approaches underestimate false detection rates by orders of magnitude due to correlation between consecutive test statistics
- Detectors configured to have 5% false positive rate may actually achieve rates as low as 0.001% in practice
- This extreme conservatism results in statistical power orders of magnitude below desired levels
- Three critical desiderata for practical deployment are often overlooked: configurable behavior, outsourced test statistics, and flexibility for expected changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Detectors using time-invariant thresholds underestimate false positive rates due to correlated test statistics
- Mechanism: When using sliding windows, consecutive test statistics share most observations, creating high correlation. Standard threshold setting assumes independence, resulting in an overly conservative bound on false detection rate
- Core assumption: The correlation structure between consecutive test statistics is not accounted for in traditional threshold setting
- Evidence anchors:
  - [section] "Unlike two-sample testing in the offline setting where only a single test is performed, for sequential shift detection an identical test is repeatedly performed using mostly the same data."
  - [section] "Consecutive test statistics are therefore highly correlated."
  - [section] "We see from Figure 1a that for a moderately sized deployment window of w = 100 and reference set size of n = 3000, the actual E[T∞] is already 11 times larger than the lower bound."

### Mechanism 2
- Claim: Outsourcing discriminative test statistic identification improves detection power in sequential shift detection
- Mechanism: By using a portion of data to train a binary classifier to distinguish reference from deployment samples, the detector learns representations where differences are most pronounced, rather than relying on fixed kernels or distance metrics
- Core assumption: Differences between pre- and post-change distributions can be captured by a learnable feature representation
- Evidence anchors:
  - [section] "Despite the power of two-sample tests based on learned test statistics we are not aware of any work that aims to transfer their success to the sequential detection setting."
  - [section] "Such an approach would likely partition the stream into a substream for updating an online binary classifier (or alternative discriminative representation) and a substream on which to evaluate how well it generalises."

### Mechanism 3
- Claim: Context-aware shift detection allows practitioners to specify which types of changes should be ignored
- Mechanism: By incorporating a context variable alongside the main summary statistic, the detector can attribute changes to expected variations in context (like time-of-day effects) and only trigger on changes unexplained by context shifts
- Core assumption: The context variable captures all expected sources of distribution shift that should not trigger detections
- Evidence anchors:
  - [section] "Alongside the usual summary statistic s(x, y) their framework allows the specification a context variable c, which may be a deterministic transformation of (x, y) or related in some unknown probabilistic manner."
  - [section] "Then only differences between summaries S and ˜S that cannot be attributed to differences between corresponding contexts C and ˜C cause a detection to be made."

## Foundational Learning

- Concept: Statistical power vs. false positive rate tradeoff
  - Why needed here: Understanding this tradeoff is crucial for configuring detectors appropriately and interpreting their performance
  - Quick check question: If you decrease the false positive rate threshold, what happens to the statistical power of the detector?

- Concept: Sequential hypothesis testing
  - Why needed here: Sequential shift detection involves repeatedly testing the same hypothesis as new data arrives, requiring different statistical treatment than one-time tests
  - Quick check question: Why can't we simply use standard two-sample test p-values as detection thresholds in sequential detection?

- Concept: Kernel methods and reproducing kernel Hilbert spaces
  - Why needed here: Many existing approaches use kernel-based test statistics, and understanding these is essential for working with current literature
  - Quick check question: What property must a kernel have to ensure that the maximum mean discrepancy is a metric?

## Architecture Onboarding

- Component map: Detector -> Window Selection -> Summary Computation -> Test Statistic Calculation -> Threshold Comparison -> Detection Decision
- Critical path: Data arrives -> Window selection -> Summary computation -> Test statistic calculation -> Threshold comparison -> Detection decision
- Design tradeoffs:
  - Window size vs. computational cost and detection latency
  - Reference set size vs. statistical power and storage requirements
  - Fixed vs. time-varying thresholds (calibration complexity vs. detection significance interpretation)
  - Learnable vs. fixed summary statistics (flexibility vs. stability)
- Failure signatures:
  - Extremely low false positive rates (detector is too conservative)
  - Frequent detections during expected changes (context not properly accounted for)
  - No detections despite known shifts (insufficient statistical power or poor feature representation)
- First 3 experiments:
  1. Implement a simple detector using MMD with Gaussian kernel and sliding window, verify it detects synthetic shifts
  2. Test the false positive rate calibration by running on data from the reference distribution, measure actual vs. expected rates
  3. Add context-aware extension to ignore time-of-day effects, verify it suppresses detections during expected context shifts while maintaining power for unexpected changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a general framework for setting time-varying thresholds that accounts for the correlation between consecutive test statistics in multivariate nonparametric sequential shift detection?
- Basis in paper: [explicit] The paper highlights that using fixed thresholds results in false detection rates orders of magnitude lower than intended, and existing approaches like Verdier et al. (2008) require complete knowledge of the pre-change distribution which is unrealistic
- Why unresolved: Current approaches either use fixed thresholds that ignore correlation (resulting in extremely low power) or require assumptions (like complete knowledge of the pre-change distribution) that are not met in practical machine learning monitoring scenarios
- What evidence would resolve it: Empirical validation showing a method can achieve the desired false detection rate while maintaining reasonable statistical power across different pre-change distributions, test statistics, and correlation structures

### Open Question 2
- Question: How can we effectively outsource the identification of discriminative test statistics for sequential shift detection without requiring separate data splits for learning and testing?
- Basis in paper: [explicit] The paper identifies this as a key desideratum (D2) and notes existing approaches (Kübler et al., 2020; Schrab et al., 2021) are limited to predefined sets of test statistics
- Why unresolved: Current learned test statistic approaches require separate data splits, which reduces the data available for detection, or are limited to predefined sets, which restricts their ability to discover truly discriminative features
- What evidence would resolve it: Demonstration of an approach that can learn discriminative representations or kernels online using the streaming data itself while maintaining detection performance comparable to methods with separate training data

### Open Question 3
- Question: What framework can provide practitioners with the flexibility to specify which types of changes should and should not be detected as shift, particularly when using complex feature representations like images?
- Basis in paper: [explicit] The paper identifies this as a key desideratum (D3) and discusses the challenge using the example of a vision model where lighting conditions change predictably throughout the day
- Why unresolved: Existing approaches like Cobb & Van Looveren (2022) propose context-aware frameworks but these are limited to specific implementations (kernel-based) and may not generalize well to complex, high-dimensional feature spaces
- What evidence would resolve it: Empirical demonstration of a flexible framework that can successfully distinguish between expected and unexpected shifts across diverse domains (vision, text, tabular) while maintaining detection power for truly malicious shifts

## Limitations
- The paper identifies critical limitations in current sequential shift detection approaches but does not provide detailed implementation guidance for the proposed solutions
- The magnitude of the threshold calibration problem may vary significantly based on data characteristics and window sizes, requiring empirical validation across diverse scenarios
- Context-aware detection and learned test statistics in the sequential setting lack concrete implementation details and comprehensive empirical validation

## Confidence
- **High confidence**: The identification of the threshold-calibration problem and its practical implications for detector usability
- **Medium confidence**: The proposed desiderata for practicable sequential shift detection and their importance for real-world deployment
- **Low confidence**: Specific implementation details for context-aware detection and learned test statistics in the sequential setting

## Next Checks
1. Implement the growing-window approach on synthetic data with known change points to verify that false detection rates match theoretical expectations
2. Test context-aware detection by introducing time-of-day effects into a stream and measuring whether detections are suppressed appropriately while maintaining power for unexpected shifts
3. Evaluate the statistical power tradeoff when outsourcing test statistic identification by comparing learned representations against fixed kernels across multiple data distributions