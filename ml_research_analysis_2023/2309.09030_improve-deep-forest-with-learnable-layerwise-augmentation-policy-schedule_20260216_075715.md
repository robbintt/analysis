---
ver: rpa2
title: Improve Deep Forest with Learnable Layerwise Augmentation Policy Schedule
arxiv_id: '2309.09030'
source_url: https://arxiv.org/abs/2309.09030
tags:
- deep
- forest
- policy
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel approach to enhance Deep Forest
  (DF) performance in tabular data classification by addressing overfitting through
  learnable data augmentation policies. The key contributions include: (1) Cut Mix
  for Tabular data (CMT), a data augmentation technique designed to mitigate overfitting
  in DF by swapping partial features between samples based on feature importance;
  (2) a population-based search algorithm to optimize layerwise augmentation policy
  schedules, enabling fine-grained control over augmentation intensity; and (3) a
  checkpoint ensemble that aggregates outputs from intermediate layers for more stable
  and robust predictions.'
---

# Improve Deep Forest with Learnable Layerwise Augmentation Policy Schedule

## Quick Facts
- arXiv ID: 2309.09030
- Source URL: https://arxiv.org/abs/2309.09030
- Reference count: 36
- One-line primary result: Proposed method achieves state-of-the-art performance across eight benchmark datasets, outperforming traditional shallow tree ensembles, deep forests, deep neural networks, and AutoML competitors.

## Executive Summary
This paper introduces AugDF, a novel approach to enhance Deep Forest performance in tabular data classification by addressing overfitting through learnable data augmentation policies. The method combines three key innovations: a custom Cut Mix for Tabular data (CMT) augmentation technique, a population-based search algorithm for layerwise augmentation policy schedules, and a checkpoint ensemble that aggregates intermediate layer outputs. Experimental results demonstrate that AugDF outperforms existing methods across eight UCI benchmark datasets, showing both improved accuracy and robustness.

## Method Summary
The proposed AugDF method enhances Deep Forest through three components: (1) CMT, which swaps partial features between samples based on feature importance to mitigate overfitting while avoiding semantic ambiguity in categorical variables; (2) a population-based search algorithm that optimizes layerwise augmentation policy schedules, enabling fine-grained control over augmentation intensity per layer; and (3) a checkpoint ensemble that aggregates outputs from intermediate layers for more stable predictions. The method initializes policies via grid search and then iteratively refines them through neighbor exploration and exploitation across a population of candidates, with moderate computational overhead compared to traditional approaches.

## Key Results
- Achieves state-of-the-art performance on eight UCI benchmark datasets
- Outperforms traditional shallow tree ensembles, deep forests, deep neural networks, and AutoML competitors
- Demonstrates effective policy transfer to Deep Forest variants, showing generalizability of learned augmentation schedules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CMT (Cut Mix for Tabular data) addresses overfitting by swapping partial features between samples, leveraging feature importance to blend samples in a way that avoids semantic ambiguity in categorical variables.
- Mechanism: The blending strategy of CMT swaps partial features between samples, drawing all values from the original dataset to avoid semantic ambiguity in categorical variables. The blending coefficient is calculated using feature importance from the preceding layer.
- Core assumption: Feature importance from decision forests provides meaningful information for blending samples in a way that regularizes the model without introducing bias.
- Evidence anchors:
  - [abstract] "We introduce the Cut Mix for Tabular data (CMT) augmentation technique to mitigate overfitting and develop a population-based search algorithm to tailor augmentation intensity for each layer."
  - [section 2.1] "We thus design Cut Mix for Tabular data (CMT) to address these issues. The new samples are formulated as: ~x = w ⊙ xi + (1 − w) ⊙ xj"
  - [corpus] Weak evidence. The corpus contains papers related to tree ensembles and data augmentation but none specifically address CMT or similar tabular-specific augmentation techniques.
- Break condition: If the feature importance from decision forests is not meaningful or does not correlate with the importance for regularization, CMT may not effectively reduce overfitting.

### Mechanism 2
- Claim: Learnable layerwise augmentation policy schedules allow for fine-grained control over augmentation intensity, preventing similar feature representations across layers and reducing variance.
- Mechanism: The paper proposes a population-based search algorithm to optimize layerwise augmentation policy schedules, enabling layer-specific adjustments in augmentation intensity. This approach navigates a vast search space to find optimal policy combinations for each layer.
- Core assumption: Different layers benefit from different augmentation intensities, and optimizing these intensities layer by layer can significantly improve model performance.
- Evidence anchors:
  - [abstract] "We optimize augmentation policy schedules through population-based algorithm, with moderate overhead."
  - [section 2.2] "We thus reformulate the problem to identify an optimal sequence of policy combinations, establishing a layerwise policy schedule for Deep Forest."
  - [corpus] Weak evidence. The corpus contains papers on deep forests and data augmentation but none specifically address learnable layerwise augmentation policy schedules.
- Break condition: If the search space is too large to be effectively navigated or if the computational overhead outweighs the benefits, the approach may not be practical.

### Mechanism 3
- Claim: Checkpoint ensemble aggregates outputs from intermediate layers, serving as a variance reducer to ensure more stable and robust results without additional training overhead.
- Mechanism: The checkpoint ensemble incorporates outputs from intermediate layers into a final prediction, leveraging the hierarchical diversity introduced by the augmentation policy schedule.
- Core assumption: Averaging predictions from intermediate layers can reduce variance and improve generalization without the need for additional training or significant inference overhead.
- Evidence anchors:
  - [abstract] "Additionally, we propose to incorporate outputs from intermediate layers into a checkpoint ensemble for more stable performance."
  - [section 2.3] "In AugDF, we harness the hierarchical diversity introduced by the augmentation policy schedule to construct a checkpoint ensemble, as illustrated in Figure 1, which incurs neither additional training cost nor significant inference overhead."
  - [corpus] Weak evidence. The corpus contains papers on deep forests and ensemble methods but none specifically address checkpoint ensembles in the context of deep forests.
- Break condition: If the intermediate layers do not provide meaningful predictions or if the ensemble does not effectively reduce variance, the checkpoint ensemble may not improve performance.

## Foundational Learning

- Concept: Deep Forest (DF)
  - Why needed here: Understanding the structure and functioning of Deep Forest is crucial to grasp how the proposed improvements address its limitations, particularly overfitting in its greedy multi-layer learning procedure.
  - Quick check question: What is the key difference between Deep Forest and traditional decision forests?

- Concept: Data Augmentation
  - Why needed here: The paper introduces a novel data augmentation technique (CMT) specifically designed for tabular data to mitigate overfitting in Deep Forest.
  - Quick check question: Why is data augmentation particularly challenging for tabular data compared to other data modalities like images or text?

- Concept: Population-based Search Algorithm
  - Why needed here: The paper employs a population-based search algorithm to optimize layerwise augmentation policy schedules, which is a key component of the proposed method.
  - Quick check question: How does a population-based search algorithm differ from a grid search in terms of exploring the hyperparameter space?

## Architecture Onboarding

- Component map:
  CMT -> Population-based Search -> Checkpoint Ensemble -> Final Prediction

- Critical path:
  1. Implement CMT to generate augmented samples.
  2. Develop the population-based search algorithm to find optimal layerwise augmentation policy schedules.
  3. Integrate the checkpoint ensemble to combine predictions from intermediate layers.

- Design tradeoffs:
  - CMT vs. traditional mixup: CMT avoids semantic ambiguity in categorical variables by drawing all values from the original dataset.
  - Population-based search vs. grid search: Population-based search can navigate a vast search space more efficiently but may require more computational resources.
  - Checkpoint ensemble vs. single-layer output: Checkpoint ensemble can reduce variance and improve robustness but may increase inference time slightly.

- Failure signatures:
  - If CMT does not effectively reduce overfitting, it may be due to the feature importance not being meaningful for regularization.
  - If the population-based search algorithm fails to find good policy schedules, it may be due to the search space being too large or the algorithm getting stuck in local optima.
  - If the checkpoint ensemble does not improve performance, it may be due to the intermediate layers not providing meaningful predictions or the ensemble not effectively reducing variance.

- First 3 experiments:
  1. Implement and test CMT on a small tabular dataset to verify its effectiveness in reducing overfitting compared to traditional mixup.
  2. Develop and test the population-based search algorithm on a Deep Forest model with a small number of layers to ensure it can find good augmentation policy schedules.
  3. Integrate the checkpoint ensemble with the Deep Forest model and evaluate its impact on performance and robustness on a benchmark dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AugDF scale with increasingly large datasets beyond the eight benchmarks tested, particularly for datasets with over a million samples?
- Basis in paper: [inferred] The paper shows strong performance on eight benchmark datasets with sizes ranging up to 1.5 million samples, but does not explore scaling behavior for much larger datasets.
- Why unresolved: The study is limited to a fixed set of datasets, and larger-scale empirical validation is missing.
- What evidence would resolve it: Systematic testing of AugDF on datasets with significantly more than 1.5 million samples, comparing training time, accuracy, and policy robustness.

### Open Question 2
- Question: Can the learned augmentation policy schedules be effectively adapted or transferred to non-tree-based models, such as deep neural networks or transformers, for tabular data?
- Basis in paper: [explicit] The paper demonstrates successful policy transfer to Deep Forest variants, but does not explore transfer to other model families.
- Why unresolved: The methodology is tailored to the tree-based architecture of Deep Forest, and its applicability to other architectures is untested.
- What evidence would resolve it: Experimental results showing improved performance of DNNs or transformers on tabular data when initialized or fine-tuned with AugDF-learned policies.

### Open Question 3
- Question: What is the impact of the hyperparameter choices (such as the number of individuals in the population-based search or the maximum number of layers) on the final model performance and computational efficiency?
- Basis in paper: [explicit] The paper sets the number of individuals to 8 and the maximum layer depth to 15, but does not analyze sensitivity to these choices.
- Why unresolved: The chosen values may not be optimal for all datasets, and their effect on performance and efficiency is not explored.
- What evidence would resolve it: Ablation studies varying these hyperparameters and measuring resulting accuracy, training time, and inference latency across multiple datasets.

## Limitations

- CMT technique lacks empirical validation against alternative tabular augmentation methods
- Population-based search algorithm's computational overhead is described as "moderate" without specific runtime comparisons
- Checkpoint ensemble benefits are demonstrated only through final accuracy metrics without ablation studies showing individual component contributions

## Confidence

- Mechanism 1 (CMT): Low confidence - Limited empirical validation and weak supporting evidence in corpus
- Mechanism 2 (Layerwise scheduling): Medium confidence - Sound theoretical foundation but unclear practical efficiency
- Mechanism 3 (Checkpoint ensemble): Medium confidence - Valid concept but insufficient ablation analysis

## Next Checks

1. Implement an ablation study comparing AugDF performance with and without CMT, using alternative tabular augmentation techniques as baselines
2. Measure and report the computational overhead of the population-based search algorithm compared to grid search or random search across different dataset sizes
3. Evaluate the transferability of learned augmentation policies to other deep forest architectures beyond the specific implementation used in the experiments