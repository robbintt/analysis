---
ver: rpa2
title: Empower Large Language Model to Perform Better on Industrial Domain-Specific
  Question Answering
arxiv_id: '2305.11541'
source_url: https://arxiv.org/abs/2305.11541
tags:
- question
- knowledge
- domain-speci
- arxiv
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark dataset, MSQA, for evaluating
  Large Language Models (LLMs) in industrial domain-specific question answering. The
  dataset comprises 23,838 question-answer pairs from the Microsoft Q&A forum, focusing
  on Microsoft products and IT technical problems.
---

# Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering

## Quick Facts
- arXiv ID: 2305.11541
- Source URL: https://arxiv.org/abs/2305.11541
- Authors: 
- Reference count: 13
- Primary result: Model interaction paradigm combining fine-tuned expert model with LLMs outperforms retrieval-based methods on domain-specific QA

## Executive Summary
This paper introduces MSQA, a benchmark dataset of 23,838 question-answer pairs from Microsoft Q&A forum focused on Microsoft products and IT technical problems. To address the challenge of domain-specific knowledge in LLMs, the authors propose a model interaction paradigm where a small language model is fine-tuned on domain documentation to act as an expert knowledge provider. At runtime, this expert model generates domain-specific knowledge that is combined with BM25-retrieved documentation chunks and injected into LLM prompts. Experiments show this composite approach significantly outperforms retrieval-based methods across multiple evaluation metrics.

## Method Summary
The approach involves fine-tuning a small LLaMA-7B model on MSQA training instructions to create an expert model, then using this expert at inference to generate domain-specific knowledge alongside BM25 retrieval. For each question, the expert model provides synthesized domain knowledge while BM25 retrieves relevant documentation chunks from Azure documentation. Both knowledge sources are concatenated into the LLM prompt to enrich context. The method is evaluated on GPT-3.5 and GPT-4 using three augmentation methods: retrieval-only, expert-only, and composite (both). The evaluation uses automated metrics including BLEU, ROUGE, cosine similarity, BERTScore, no-answer rate, and LLM evaluation score.

## Key Results
- The composite method (expert + BM25) outperforms both retrieval-only and expert-only approaches on MSQA benchmark
- Proposed model interaction paradigm shows significant performance improvements across BLEU, ROUGE, cosine similarity, BERTScore, and no-answer rate metrics
- Fine-tuned expert model effectively provides domain-specific knowledge that enhances LLM responses beyond what retrieval alone can achieve

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a small language model on domain documentation enables it to act as an expert knowledge provider to LLMs at runtime.
- Mechanism: The small model is trained on structured instructions from the MSQA training set, aligning it with domain-specific knowledge. At inference, it generates domain-specific background knowledge which is injected into the LLM prompt.
- Core assumption: A small, fine-tuned model can encapsulate and generate sufficient domain knowledge to guide a larger, general-purpose LLM.
- Evidence anchors:
  - [abstract] "fine-tuning a small language model on domain documentation to align it with domain-specific knowledge" and "At runtime, this fine-tuned model provides domain-specific knowledge to larger language models (LLMs)."
  - [section] "To facilitate instruction tuning, we construct instructions from the training set of the MSQA dataset...By engaging in instruction tuning, the small language model learns and assimilates domain-specific knowledge..."
- Break condition: If the domain documentation is too sparse or the fine-tuning data does not capture the domain knowledge distribution, the small model will fail to provide useful guidance.

### Mechanism 2
- Claim: Combining expert-generated knowledge with retrieved documentation chunks improves LLM performance over retrieval alone.
- Mechanism: The expert model provides synthesized domain knowledge, while BM25 retrieves relevant chunks; both are concatenated into the LLM prompt to enrich context.
- Core assumption: Retrieval-based methods alone may miss implicit connections that a trained expert model can provide.
- Evidence anchors:
  - [abstract] "Experiments on the MSQA dataset demonstrate that the proposed approach, which combines the LLM with the fine-tuned expert model, outperforms the commonly used retrieval-based methods..."
  - [section] "We evaluated two LLMs...using three knowledge fusion methods...Composite method Both the information chunks retrieved by BM25 and the responses from the expert model are combined..."
- Break condition: If the expert model's outputs are noisy or contradictory to retrieved facts, the composite prompt may confuse rather than help the LLM.

### Mechanism 3
- Claim: The proposed model interaction paradigm outperforms retrieval-based augmentation in domain-specific QA tasks.
- Mechanism: At runtime, the fine-tuned small model acts as a knowledge provider, eliminating the need for retrieval and context window constraints.
- Core assumption: Generating domain knowledge is more effective than retrieving it when the context window is limited.
- Evidence anchors:
  - [abstract] "Extensive experiments demonstrate that the approach following our model fusion framework outperforms the commonly used LLM with retrieval methods."
  - [section] "The evaluation results highlight the significant performance of our model interaction paradigm in generating answers enriched with domain-specific knowledge."
- Break condition: If the domain documentation is highly structured and retrieval can retrieve precise facts, the expert model may add little value.

## Foundational Learning

- Concept: Instruction tuning on domain-specific QA pairs
  - Why needed here: Aligns the small model's generation patterns with the domain's language and question types.
  - Quick check question: What format do the instruction tuples have in the fine-tuning data?

- Concept: BM25 retrieval for contextual augmentation
  - Why needed here: Provides concrete factual grounding from documentation when the expert model's synthesis is incomplete.
  - Quick check question: How many top chunks are retrieved and how are they integrated into the prompt?

- Concept: Evaluation metrics for long-form answers
  - Why needed here: BLEU, ROUGE, cosine similarity, BERTScore, NAR, and LLM Eval together capture semantic alignment and answer completeness.
  - Quick check question: Which metric is most sensitive to factual correctness in this domain?

## Architecture Onboarding

- Component map: Domain documentation corpus -> Small LLaMA-7B fine-tuned model (expert) -> BM25 retriever -> Large LLM (GPT-3.5 or GPT-4) -> Evaluation pipeline
- Critical path:
  1. Fine-tune expert on MSQA training instructions
  2. At inference, generate expert knowledge for a question
  3. Retrieve top-3 BM25 chunks
  4. Concatenate expert output + BM25 chunks into LLM prompt
  5. Generate response and evaluate against golden answer
- Design tradeoffs:
  - Small model size (7B) balances fine-tuning cost and knowledge fidelity
  - BM25 retrieval is fast but may miss context; expert synthesis is richer but can hallucinate
  - Composite method uses more prompt tokens, risking context window overflow
- Failure signatures:
  - NAR increases → LLM unable to answer even with augmentation
  - BERTScore drops despite BLEU increase → semantic drift
  - Expert model outputs irrelevant facts → composite prompt confusion
- First 3 experiments:
  1. Compare expert-only vs retrieval-only vs composite on a held-out MSQA dev set.
  2. Ablate the number of BM25 chunks (1, 3, 5) to find optimal context length.
  3. Test fine-tuning with different instruction formats (question-answer only vs full three-element tuple).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed model interaction paradigm perform compared to other domain adaptation techniques like domain adaptation fine-tuning or prompt tuning for LLMs?
- Basis in paper: [inferred] The paper mentions fine-tuning and maintaining LLMs to incorporate domain-specific knowledge can be expensive, but doesn't compare the proposed approach to other domain adaptation methods.
- Why unresolved: The paper only compares the proposed method to retrieval-based approaches, not other domain adaptation techniques.
- What evidence would resolve it: Experiments comparing the proposed model interaction paradigm to other domain adaptation methods like domain adaptation fine-tuning or prompt tuning on the same dataset.

### Open Question 2
- Question: What is the impact of the expert model's knowledge on the LLM's responses in terms of factual accuracy and relevance to the domain?
- Basis in paper: [inferred] The paper mentions that the expert model provides domain-specific knowledge to the LLM, but doesn't evaluate the impact on factual accuracy or relevance.
- Why unresolved: The paper only evaluates the overall quality of the LLM's responses using BLEU, ROUGE, cosine similarity, BERTScore, and no-answer rate, but not the factual accuracy or relevance of the domain-specific knowledge.
- What evidence would resolve it: Human evaluation of the factual accuracy and relevance of the domain-specific knowledge in the LLM's responses.

### Open Question 3
- Question: How does the proposed model interaction paradigm scale to larger datasets and more complex domain-specific tasks?
- Basis in paper: [inferred] The paper only evaluates the proposed approach on a single dataset (MSQA) and doesn't discuss its scalability to larger datasets or more complex tasks.
- Why unresolved: The paper doesn't provide any insights into the scalability of the proposed approach.
- What evidence would resolve it: Experiments evaluating the proposed approach on larger datasets and more complex domain-specific tasks, along with analysis of the computational requirements and performance.

## Limitations

- The instruction tuning format for the expert model is only partially specified, making exact replication challenging
- The Azure documentation corpus for BM25 retrieval is referenced but not precisely defined, raising questions about reproducibility
- Evaluation relies entirely on automated metrics without human evaluation of answer quality or factual correctness

## Confidence

**High Confidence**: The overall experimental methodology and benchmark construction are sound. The MSQA dataset creation and the general concept of combining expert knowledge with retrieval are well-established approaches in the literature.

**Medium Confidence**: The claim that the composite method outperforms retrieval alone is supported by the reported metrics, but the lack of human evaluation and detailed error analysis makes it difficult to assess whether the improvements are meaningful in practice.

**Low Confidence**: The mechanism by which the fine-tuned expert model specifically improves LLM performance over retrieval remains unclear. The paper shows correlation between the approach and improved metrics but doesn't provide deep analysis of why the expert model succeeds or fails in specific cases.

## Next Checks

1. **Human Evaluation Study**: Conduct a blind human evaluation where domain experts rate answer quality, factual correctness, and usefulness for both the retrieval-only and composite methods on a subset of 100 questions from the MSQA test set.

2. **Error Analysis**: Perform detailed error analysis on cases where the composite method outperforms vs underperforms retrieval-only, categorizing failures by type (hallucination, missing facts, irrelevant information) to understand the mechanism of improvement.

3. **Ablation Study**: Test the expert model's performance when fine-tuned with different instruction formats (question-answer only vs full three-element tuples) and with varying amounts of training data to determine the minimum viable training set size and optimal instruction structure.