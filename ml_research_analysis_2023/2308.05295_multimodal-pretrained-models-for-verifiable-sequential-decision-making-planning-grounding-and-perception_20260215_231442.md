---
ver: rpa2
title: 'Multimodal Pretrained Models for Verifiable Sequential Decision-Making: Planning,
  Grounding, and Perception'
arxiv_id: '2308.05295'
source_url: https://arxiv.org/abs/2308.05295
tags:
- controller
- 'true'
- green
- 'false'
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops an algorithm that integrates the knowledge from
  pretrained models into sequential decision-making tasks. The algorithm constructs
  and verifies automaton-based controllers using the pretrained model's outputs, and
  grounds these controllers to task environments through visual observations.
---

# Multimodal Pretrained Models for Verifiable Sequential Decision-Making: Planning, Grounding, and Perception

## Quick Facts
- arXiv ID: 2308.05295
- Source URL: https://arxiv.org/abs/2308.05295
- Reference count: 40
- One-line primary result: Integrates knowledge from pretrained models into sequential decision-making tasks through verifiable automaton-based controllers with probabilistic grounding guarantees.

## Executive Summary
This work develops an algorithm that integrates knowledge from multimodal pretrained models into sequential decision-making tasks. The algorithm constructs and verifies automaton-based controllers using the pretrained model's outputs, and grounds these controllers to task environments through visual observations. Specifically, it queries a pretrained model with a user-provided task description, constructs an automaton-based controller encoding the model's task-relevant knowledge, and verifies the controller's consistency with independently available knowledge or user-provided specifications. If inconsistencies are found, the algorithm refines the controller. The grounding method leverages the vision and language capabilities of pretrained models to link image-based observations from the task environment to the controller's text-based control logic. A probabilistic mechanism ensures the controller satisfies user-provided specifications even under perceptual uncertainties.

## Method Summary
The algorithm integrates multimodal pretrained models into sequential decision-making by constructing, verifying, and grounding automaton-based controllers. It queries a GLM with a task description to obtain task steps, which are parsed into FSA-based controllers using LLM2Automata. The controller is verified against user specifications using a model checker and external knowledge from Text2Model. If verification fails, the refinement module synthesizes and merges sub-controllers to resolve inconsistencies. Automata2Env grounds the controller to the real environment using visual observations and confidence scores from vision-language models, ensuring probabilistic guarantees on specification satisfaction under perceptual uncertainties.

## Key Results
- Successfully constructs FSA-based controllers from multimodal pretrained models that encode task-relevant knowledge
- Achieves probabilistic guarantees on satisfying user-provided specifications under perceptual uncertainties through confidence-based grounding
- Demonstrates the approach on real-world tasks including daily life activities and robot manipulation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm can synthesize FSA-based controllers that encode task knowledge from multimodal pretrained models into verifiable structures.
- Mechanism: LLM2Automata queries a GLM for task steps, parses verb phrases into actions, extracts PDDL preconditions and effects, and applies grammar rules to transform them into FSA transitions. This formalizes task knowledge into an automaton that can be formally verified.
- Core assumption: The GLM's generated text includes explicit, actionable steps with clear preconditions and effects that can be parsed into PDDL format.
- Evidence anchors:
  - [abstract]: "The algorithm queries a pretrained model with a user-provided, text-based task description and uses the model's output to construct an automaton-based controller that encodes the model's task-relevant knowledge."
  - [section]: "The algorithm sends the text description as the input prompt (in blue) to a GLM and obtains the GLM's response (in red), which is a list of steps for achieving the task in textual form."
  - [corpus]: Weak evidence - no direct citation of GLM step parsing, but related work [1] supports the GLM2FSA parsing approach.
- Break condition: If the GLM outputs vague, incomplete, or non-actionable steps, the parsing into verb phrases and PDDL fails, breaking FSA construction.

### Mechanism 2
- Claim: The grounding method Automata2Env ensures the controller's actions are only taken when visual observations confidently match required conditions, even under perceptual uncertainties.
- Mechanism: Automata2Env evaluates atomic propositions using a vision-language model that returns confidence scores. It applies thresholds to classify each proposition as true, false, or uncertain, and only takes actions when all required conditions are confidently true. Self-transitions on "uncertain" prevent unsafe actions.
- Core assumption: The vision-language model can reliably estimate confidence scores for proposition-image matches, and the thresholds can be tuned to balance safety and performance.
- Evidence anchors:
  - [abstract]: "The algorithm leverages the vision and language capabilities of pretrained models to ground the controller to the task environment. It collects image-based observations from the task environment and uses the pretrained model to link these observations to the text-based control logic encoded in the controller."
  - [section]: "The algorithm evaluates an atomic proposition and assigns one of the three values: true, false, and uncertain. Algorithm 3 shows how we evaluate the propositions using the confidence scores from the vision-language model."
  - [corpus]: Weak evidence - no direct citation of confidence-based grounding, but vision-language models like CLIP [18] support text-image consistency scoring.
- Break condition: If the vision-language model's confidence scores are unreliable or the thresholds are poorly set, the system may take unsafe actions or become overly conservative.

### Mechanism 3
- Claim: The automatic refinement procedure iteratively synthesizes a sub-controller from the model that resolves verification failures until the specification is satisfied.
- Mechanism: When verification fails, the algorithm synthesizes a counter-example satisfying the negation of the specification from the model, converts it to a trajectory, and constructs an FSA representing the desired sub-controller. This sub-controller is merged into the original controller, adding safety transitions that prevent the violation.
- Core assumption: The model accurately represents environment dynamics and the counter-example from the model's negation of the specification provides a valid example of correct behavior that can be synthesized into a sub-controller.
- Evidence anchors:
  - [abstract]: "It then verifies whether the knowledge encoded in the controller is consistent with other independently available knowledge... If this verification step discovers any inconsistency, the algorithm automatically refines the controller to resolve the inconsistency."
  - [section]: "The refinement procedure starts from the counter-example returned by the model checker and eventually produces a refined controller that will never violate the specification."
  - [corpus]: Weak evidence - no direct citation of counter-example-based refinement, but related work [1] supports automatic refinement from counter-examples.
- Break condition: If the model is incomplete or inaccurate, the synthesized sub-controller may not resolve the issue, leading to infinite refinement loops or incorrect controllers.

## Foundational Learning

- Concept: Finite State Automata (FSA) and their use in representing controllers and models.
  - Why needed here: FSAs provide a formal, verifiable structure for encoding task knowledge and environment dynamics, enabling formal verification against specifications.
  - Quick check question: Can you describe the components of an FSA (states, transitions, input/output alphabets) and how they map to controller logic?

- Concept: Linear Temporal Logic (LTL) for specifying desired controller behaviors over time.
  - Why needed here: LTL allows expressing complex temporal properties (e.g., "eventually reach the other side of the road and never cross at red light") that can be formally verified against the FSA-based controller and model.
  - Quick check question: Can you write an LTL formula expressing "eventually do X, but never do Y until Z is true"?

- Concept: PDDL (Planning Domain Definition Language) for defining actions with preconditions and effects.
  - Why needed here: PDDL provides a structured way to extract and encode the preconditions and effects of actions from the GLM's text output, which are then used to construct the FSA transitions.
  - Quick check question: Can you define a simple action in PDDL with a precondition and an effect?

## Architecture Onboarding

- Component map: LLM2Automata -> Text2Model -> Verification module -> Automata2Env -> Refinement module
- Critical path:
  1. User provides task description.
  2. LLM2Automata queries GLM and constructs controller FSA.
  3. Text2Model constructs model FSA from external knowledge.
  4. Verification module checks if controller satisfies specification over model.
  5. If verification fails, Refinement module synthesizes and merges sub-controller.
  6. Repeat 4-5 until specification is satisfied.
  7. Automata2Env grounds controller to real environment and executes.
- Design tradeoffs:
  - FSA vs. other representations (e.g., behavior trees): FSAs are formally verifiable but may be less expressive for complex tasks.
  - High vs. low confidence thresholds in Automata2Env: Higher thresholds increase safety but may reduce responsiveness.
  - Automatic vs. manual refinement: Automatic is faster but may fail if model is incomplete; manual is slower but more flexible.
- Failure signatures:
  - FSA construction fails: GLM outputs vague or incomplete steps; parsing into verb phrases or PDDL fails.
  - Verification always fails: Model is inaccurate or incomplete; specification is too strict or conflicting.
  - Grounding fails: Vision-language model confidence scores are unreliable; thresholds are poorly set.
  - Refinement loops infinitely: Automatic refinement fails to resolve verification failures due to model limitations.
- First 3 experiments:
  1. Run LLM2Automata on a simple task (e.g., "cross the road at a traffic light") and inspect the constructed FSA for correctness.
  2. Use Text2Model to construct a model from external knowledge (e.g., a tutorial blog) and verify it captures the expected environment dynamics.
  3. Test Automata2Env grounding on a static scene (e.g., detecting traffic light colors) and verify that actions are only taken when conditions are confidently true.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the grounding method be extended to handle dynamic scenes and actions, not just static objects?
- Basis in paper: [explicit] The paper states that the current grounding method relies on vision-language models that perform well on static scenes but cannot capture dynamic scenes, such as motion or action detection.
- Why unresolved: The paper acknowledges this limitation but does not propose a solution or future direction for addressing it.
- What evidence would resolve it: Demonstrating the ability of the grounding method to handle dynamic scenes and actions in real-world scenarios.

### Open Question 2
- Question: Can the algorithm be modified to map high-level actions into low-level operations, such as APIs provided by a robot?
- Basis in paper: [explicit] The paper mentions that the controllers only encode high-level task knowledge and do not specify how the actions will be performed. It suggests developing an algorithm to map high-level actions into low-level operations through further queries to the GLM.
- Why unresolved: The paper does not provide any details or implementation of such an algorithm.
- What evidence would resolve it: Successfully implementing the proposed algorithm to map high-level actions into low-level operations for a specific robot or system.

### Open Question 3
- Question: How can the perception method be improved to handle uncertainties more effectively?
- Basis in paper: [explicit] The paper introduces a high-confidence grounding method that uses confidence scores from vision-language models to ensure safety under perceptual uncertainties. However, it also mentions that the thresholds for evaluating propositions affect decision-making and that setting higher thresholds leads to more conservative decision-making.
- Why unresolved: The paper does not provide a detailed analysis of how to optimize the thresholds or improve the perception method to handle uncertainties more effectively.
- What evidence would resolve it: Demonstrating improved performance in handling uncertainties with optimized thresholds or an enhanced perception method.

## Limitations
- The grounding method currently relies on vision-language models that perform well on static scenes but cannot capture dynamic scenes, such as motion or action detection.
- The algorithm does not address how to map high-level actions encoded in the controllers into low-level operations, such as APIs provided by a robot.
- The perception method's effectiveness depends on the reliability of confidence scores from vision-language models, and setting thresholds too high may lead to overly conservative decision-making.

## Confidence

- **Medium Confidence**: The core mechanism of constructing FSA controllers from GLM outputs (Mechanism 1) - while the approach is sound, its effectiveness depends heavily on the quality and consistency of GLM outputs, which can vary significantly.
- **Medium Confidence**: The Automata2Env grounding method (Mechanism 2) - the threshold-based approach for handling uncertainties is theoretically reasonable, but real-world performance will depend on the specific vision-language model's reliability and appropriate threshold tuning.
- **Low Confidence**: The automatic refinement procedure (Mechanism 3) - while the concept of counter-example-based refinement is established in formal methods, the paper doesn't provide sufficient detail on how this works with multimodal models or how to handle cases where refinement loops infinitely.

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the confidence thresholds in Automata2Env across multiple environments and tasks to determine their impact on safety, performance, and reliability. This would reveal whether the grounding mechanism is robust to different perceptual conditions.

2. **Cross-Model Consistency Test**: Run the full pipeline using different GLMs (e.g., GPT-4, Claude, Llama) on the same tasks and compare the resulting controllers, verification outcomes, and execution performance. This would validate whether the approach depends too heavily on a specific model's characteristics.

3. **Failure Mode Characterization**: Intentionally provide ambiguous, incomplete, or contradictory task descriptions to the GLM and document how the system fails - does it gracefully handle uncertainty, produce unsafe controllers, or enter infinite refinement loops? This would reveal the practical limitations of the approach.