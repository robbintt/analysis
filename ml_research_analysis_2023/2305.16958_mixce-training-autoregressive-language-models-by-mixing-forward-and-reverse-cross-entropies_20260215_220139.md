---
ver: rpa2
title: 'MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse
  Cross-Entropies'
arxiv_id: '2305.16958'
source_url: https://arxiv.org/abs/2305.16958
tags:
- mixce
- text
- human
- reverse
- forward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MIXCE combines forward and reverse cross-entropies to train language
  models, improving generation quality without complex decoding. By mixing these two
  complementary objectives, it avoids the over-generalization seen in standard MLE
  training.
---

# MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies

## Quick Facts
- **arXiv ID**: 2305.16958
- **Source URL**: https://arxiv.org/abs/2305.16958
- **Reference count**: 40
- **Key outcome**: MIXCE combines forward and reverse cross-entropies to train language models, improving generation quality without complex decoding

## Executive Summary
MIXCE addresses over-generalization in language model training by combining forward and reverse cross-entropies. Standard MLE training suffers from a zero-avoiding property that makes models sensitive to dataset noise, leading to over-generalization. MIXCE balances this with reverse cross-entropy's zero-forcing property, using a self-reinforced approximation that encourages generation of high-confidence sequences. The method improves text quality metrics like Mauve and Coherence while maintaining diversity, without requiring complex decoding strategies.

## Method Summary
MIXCE trains autoregressive language models by mixing forward cross-entropy (standard MLE) with reverse cross-entropy, approximated through a self-reinforced objective. The mixing ratio η controls the balance between coverage (forward CE) and quality (reverse CE). During training, both objectives are computed and combined, with reverse CE gradients weighted by the model's confidence Qθ(xt|x<t). The method maintains computational complexity similar to MLE while improving generation quality. Models are finetuned for 5 epochs with best checkpoints selected by development loss.

## Key Results
- MIXCE-finetuned models produce more diverse, coherent, and human-like text than MLE baselines
- Better Mauve and Coherence scores demonstrate improved alignment with human text distribution
- Human evaluations confirm MIXCE's superiority on two out of three datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MIXCE combines forward and reverse cross-entropies to address model over-generalization by balancing coverage and quality.
- **Mechanism:** The reverse cross-entropy penalizes the model for generating text it is not confident about (high loss when Qθ(x) is low but P(x) is high), while the forward cross-entropy ensures the model covers the diversity of the data distribution. By mixing these two, the model avoids putting non-trivial probability mass on P(x)=0 regions while still maintaining diversity.
- **Core assumption:** The learned LM distribution Qθ differs substantially from the human LM distribution P, leading to over-generalization when using only MLE.
- **Evidence anchors:** [abstract]: "we believe that reverse cross-entropy, i.e., the cross-entropy of P relative to Qθ, is a better reflection of how a human would evaluate text generated by a model"

### Mechanism 2
- **Claim:** MIXCE approximates reverse cross-entropy through a self-reinforced objective that promotes high-likelihood sequences.
- **Mechanism:** Since direct optimization of reverse cross-entropy is intractable (P is unknown), MIXCE uses an approximation that weights gradients by the model's own confidence Qθ(xt|x<t) at each step. This encourages the model to generate sequences it is already confident about, effectively pushing the model distribution toward the human distribution.
- **Core assumption:** MLE-pretrained models assign higher probability to human text than to model-generated text on average.
- **Evidence anchors:** [section 3.2]: "Equations (8) and (9) are apparently not equivalent to each other. Nonetheless, they have similar effects...So essentially, they both encourage the model to produce generations in which it is already confident"

### Mechanism 3
- **Claim:** MIXCE balances the zero-avoiding property of forward KL with the zero-forcing property of reverse KL to better fit Qθ to P.
- **Mechanism:** Forward KL (equivalent to MLE) has a zero-avoiding property that makes the model sensitive to dataset noise by trying to cover all possible sequences. Reverse KL has a zero-forcing property that strongly penalizes generating non-human-like samples. MIXCE combines these opposing forces to achieve better coverage without overfitting to noise.
- **Core assumption:** The zero-avoiding property of MLE makes the model sensitive to dataset noise, leading to over-generalization.
- **Evidence anchors:** [section 3.1]: "Forward KL has a zero-avoiding property – avoiding Qθ(x) = 0 when P (x) ̸= 0...Therefore, if there is noise in the data, Qθ will try to cover the noise as well, which leads the model to over generalize"

## Foundational Learning

- **Concept: Cross-entropy and KL divergence relationship**
  - Why needed here: Understanding that minimizing forward cross-entropy is equivalent to minimizing forward KL divergence is crucial for grasping why MIXCE works differently from standard MLE
  - Quick check question: Why does minimizing forward cross-entropy have a "zero-avoiding" property?

- **Concept: Policy gradient theorem**
  - Why needed here: The approximation of reverse cross-entropy relies on switching the expectation from Qθ to P using techniques similar to policy gradient methods
  - Quick check question: How does switching from expectation over Qθ to expectation over P help make reverse cross-entropy optimization tractable?

- **Concept: Text generation evaluation metrics**
  - Why needed here: Understanding metrics like Mauve, Coherence, and Diversity is essential for interpreting MIXCE's performance improvements
  - Quick check question: What aspect of text quality does Mauve primarily measure, and how does it differ from perplexity?

## Architecture Onboarding

- **Component map:** Standard MLE loss + Self-reinforced reverse cross-entropy approximation
- **Critical path:** During training, compute both the standard MLE loss and the self-reinforced reverse cross-entropy approximation, then combine them using the mixing ratio η. During evaluation, use the same unbiased sampling as MLE but expect better quality due to improved training.
- **Design tradeoffs:** Higher η values make MIXCE behave more like standard MLE (emphasizing coverage), while lower η values make it behave more like reverse CE only (emphasizing quality but risking mode collapse). The optimal η must be tuned per dataset.
- **Failure signatures:** If η is too low, the model may collapse to generating only the most likely text (mode collapse). If η is too high, MIXCE behaves like MLE and provides no improvement. Poor mixing can lead to unstable training.
- **First 3 experiments:**
  1. Implement MIXCE with η=0.5 on a small dataset (e.g., WikiText-2) and compare perplexity and diversity to MLE baseline
  2. Vary η from 0.1 to 0.9 on the same dataset to find the optimal mixing ratio
  3. Test MIXCE on a held-out validation set using Mauve score to confirm quality improvements over MLE

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of MIXCE vary with different mixing ratios η across different domains and model sizes?
- **Basis in paper:** [explicit] The paper shows that the best η changes as the experimental setting changes and that it is important to tune η to balance the effects of forward and reverse cross-entropies.
- **Why unresolved:** The paper does not provide a definitive answer on which η should be used for different settings, and the ideal solution is to select η based on the performance of the development set.
- **What evidence would resolve it:** Systematic experiments varying η across different domains and model sizes, and analyzing the resulting performance metrics (Mauve, Coherence, etc.) to identify patterns or guidelines for selecting η.

### Open Question 2
- **Question:** Can MIXCE help improve factuality or reduce biases in language model generations?
- **Basis in paper:** [explicit] The paper acknowledges that GPT-2 models reflect biases inherent to the systems they were trained on and cannot be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use case.
- **Why unresolved:** The paper does not investigate whether MIXCE can help improve factuality or reduce biases in language model generations.
- **What evidence would resolve it:** Experiments evaluating the factuality and bias of MIXCE-finetuned models compared to MLE-finetuned models on various tasks and datasets, using appropriate metrics and human evaluations.

### Open Question 3
- **Question:** How does MIXCE perform in pretraining language models compared to MLE?
- **Basis in paper:** [inferred] The paper suggests that MIXCE can be potentially used for pretraining language models, but does not investigate its performance in this setting.
- **Why unresolved:** The paper only evaluates MIXCE in the context of finetuning pretrained GPT-2 models and does not explore its effectiveness in pretraining.
- **What evidence would resolve it:** Experiments pretraining language models from scratch using MIXCE and comparing their performance to models pretrained with MLE on various downstream tasks and benchmarks.

## Limitations

- **Hyperparameter sensitivity**: MIXCE requires careful tuning of the mixing ratio η, with poor choices potentially leading to mode collapse or degraded performance
- **Assumption dependency**: The self-reinforced reverse cross-entropy approximation relies on the assumption that MLE-pretrained models assign higher probability to human text than to model-generated text
- **Evaluation scope**: While MIXCE shows improvements in certain metrics, it doesn't directly address semantic coherence or factual accuracy of generated text

## Confidence

- **High confidence**: The theoretical framework connecting forward/reverse cross-entropies to zero-avoiding/zero-forcing properties is well-established in information theory
- **Medium confidence**: The claims about MIXCE's superiority on real datasets are supported by multiple evaluation metrics, but the human evaluation results show mixed performance across datasets
- **Low confidence**: The generalizability of MIXCE across different model architectures beyond GPT-2, and its performance on highly structured or specialized text domains, remains largely untested

## Next Checks

1. **Cross-architecture validation**: Test MIXCE on models trained with different objectives (e.g., GPT-Neo, LLaMA) and architectures (e.g., Transformer variants) to verify the approximation assumptions hold beyond GPT-2
2. **Systematic hyperparameter analysis**: Conduct a comprehensive grid search over η values across multiple datasets to map out the performance landscape and identify failure modes
3. **Stress test on pathological distributions**: Create synthetic datasets with extreme characteristics (very sparse distributions, highly multimodal distributions, datasets with significant noise) to probe the boundaries of MIXCE's effectiveness compared to MLE