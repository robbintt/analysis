---
ver: rpa2
title: Detecting and Preventing Hallucinations in Large Vision Language Models
arxiv_id: '2308.06394'
source_url: https://arxiv.org/abs/2308.06394
tags:
- reward
- dataset
- training
- instructblip
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucinations in Large Vision
  Language Models (LVLMs), which are responses containing incorrect or unfaithful
  descriptions of input images. To tackle this issue, the authors introduce M-HalDetect,
  a comprehensive multimodal hallucination detection dataset consisting of 16k fine-grained
  annotations on VQA examples.
---

# Detecting and Preventing Hallucinations in Large Vision Language Models

## Quick Facts
- arXiv ID: 2308.06394
- Source URL: https://arxiv.org/abs/2308.06394
- Reference count: 36
- This paper introduces M-HalDetect, a fine-grained hallucination detection dataset, and demonstrates hallucination reduction methods achieving 41-55% improvement rates.

## Executive Summary
This paper addresses hallucinations in Large Vision Language Models (LVLMs), where models generate incorrect or unfaithful descriptions of input images. The authors introduce M-HalDetect, a comprehensive multimodal hallucination detection dataset with 16k fine-grained annotations covering object hallucinations, entity descriptions, and relationships. They demonstrate two approaches to reduce hallucinations: Fine-grained Direct Preference Optimization (FDPO) and rejection sampling using trained reward models, achieving 41% and 55% hallucination reduction respectively in InstructBLIP.

## Method Summary
The paper introduces M-HalDetect, a fine-grained multimodal hallucination detection dataset containing 16k annotations on VQA examples. Using this dataset, they implement two approaches: (1) FDPO, which directly optimizes InstructBLIP using segment-level hallucination annotations, and (2) multimodal reward models trained on M-HalDetect that score generations for hallucination likelihood. They evaluate these methods using rejection sampling (best-of-n selection) and human evaluation, showing significant reductions in hallucination rates.

## Key Results
- FDPO reduces hallucination rates in InstructBLIP by 41% through direct optimization on fine-grained annotations
- Rejection sampling with reward models achieves 55% hallucination reduction by selecting optimal generations
- Reward models generalize to other LVLMs, reducing hallucinations in LLaVA by 15% and mPLUG-OWL by 57%

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained DPO directly optimizes InstructBLIP on individual hallucination annotations, reducing hallucination rates more effectively than traditional preference pairs. The FDPO loss uses segment-level labels (accurate/inaccurate) to adjust likelihoods for each chunk of generated text, rather than optimizing over full generation pairs. Core assumption: Localized hallucination signals can be directly mapped to policy updates without intermediate reward modeling.

### Mechanism 2
Reward models trained on fine-grained M-HalDetect annotations can accurately score hallucination presence in unseen multimodal generations. Segment-level reward models classify token spans as accurate/inaccurate, enabling detection of localized hallucinations missed by sentence-level models. Core assumption: Fine-grained annotation distributions correlate with actual hallucination severity in new data.

### Mechanism 3
Rejection sampling using reward scores selects less hallucinatory generations from InstructBLIP. Best-of-n sampling chooses the generation with highest reward score (lowest hallucination probability) among n samples. Core assumption: Reward model scores meaningfully differentiate hallucination severity across samples.

## Foundational Learning

- **Cross-modal attention mechanisms (QFormer)**: InstructBLIP's QFormer attends image features to instructions; fine-tuning it affects hallucination detection. Quick check: What happens if you freeze the QFormer during reward model training?
- **Segment-level annotation and classification**: M-HalDetect provides fine-grained labels; understanding span-level classification is crucial for reward model design. Quick check: How do you handle overlapping segments when training a segment-level reward model?
- **Rejection sampling and RLHF performance**: Rejection sampling approximates RLHF without full policy optimization; understanding its limitations is key. Quick check: What is the trade-off between rejection sampling sample size and inference latency?

## Architecture Onboarding

- **Component map**: Image encoder → Linear mapping → QFormer → Frozen Vicuna decoder → Reward head (classification)
- **Critical path**: 1) Load image and instruction, 2) Extract image features, 3) Attend with QFormer, 4) Generate text with decoder, 5) Score with reward head
- **Design tradeoffs**: Fine-tuning decoder vs QFormer (decoder tuning risks overfitting; QFormer tuning preserves generation quality), binary vs ternary classification (binary is simpler but loses analysis class distinction)
- **Failure signatures**: Reward model overfitting to training set annotation style, generation quality degradation during fine-tuning, rejection sampling not improving hallucination rates
- **First 3 experiments**: 1) Train binary sentence-level reward model with decoder fine-tuning; evaluate on M-HalDetect val set, 2) Run rejection sampling (best-of-16) on InstructBLIP with trained reward model; measure hallucination reduction, 3) Fine-tune InstructBLIP with FDPO using M-HalDetect annotations; compare hallucination rates to baseline

## Open Questions the Paper Calls Out

Improving dataset collection and annotation:
- How can we further improve the diversity and quality of the image-description pairs in the M-HalDetect dataset?
- How can we reduce subjectivity and ambiguity in the annotation process for fine-grained hallucination detection?
- How can we leverage more diverse and challenging prompts to generate a wider range of responses from LVLMs?

Enhancing reward model performance:
- How can we improve the performance of the segment-level reward model by training a segment predictor in parallel?
- How can we fine-tune the classification head before fine-tuning the QFormer to improve the ternary classification performance?
- How can we explore different reward model architectures and training strategies to achieve better hallucination detection?

Optimizing generative models with reward models:
- How can we effectively optimize InstructBLIP or other LVLMs using the trained reward models to create a higher quality LVLM for instruction-aware VQA?
- How can we balance reducing hallucinations with maintaining helpfulness and informativeness in the generated descriptions?
- How can we extend the dataset and methods to also account for descriptiveness and informativeness, training multiple reward models for optimizing a more robust final model?

Evaluating hallucination detection and prevention methods:
- How can we develop more comprehensive and reliable evaluation metrics for hallucination detection and prevention in LVLMs?
- How can we conduct large-scale human evaluations to assess the effectiveness of different methods in reducing hallucination rates?
- How can we analyze the trade-offs between reducing hallucinations and other aspects of generated content, such as accuracy and coherence?

## Limitations

- The M-HalDetect dataset is constructed from a limited set of COCO images and may not capture full diversity of real-world scenarios
- Methods are primarily tested on InstructBLIP and may not generalize to other LVLM architectures
- Computational overhead of rejection sampling is not discussed, potentially increasing inference latency significantly
- Human evaluation methodology lacks detail on rater selection and reliability metrics

## Confidence

- **High Confidence**: The basic premise that fine-grained annotations can improve hallucination detection is well-supported
- **Medium Confidence**: Reported hallucination reduction percentages are credible within experimental setup but may not generalize
- **Low Confidence**: Claims about reward model generalization to other LVLMs are based on limited evidence and may exploit dataset-specific patterns

## Next Checks

1. **Cross-dataset validation**: Test M-HalDetect-trained reward models and FDPO-optimized InstructBLIP on entirely separate LVLM hallucination benchmarks (MMMU or ScienceQA) to assess generalization beyond COCO-derived data

2. **Architecture transfer study**: Implement same fine-grained annotation and reward modeling approach on a different LVLM architecture (e.g., using CLIP visual features instead of BLIP) to determine if methodology is architecture-agnostic

3. **Efficiency-accuracy trade-off analysis**: Systematically measure inference latency increase from best-of-n sampling (n = 2, 4, 8, 16) against hallucination reduction rates to identify optimal balance point for practical deployment scenarios