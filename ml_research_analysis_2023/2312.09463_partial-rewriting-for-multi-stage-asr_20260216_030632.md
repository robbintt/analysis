---
ver: rpa2
title: Partial Rewriting for Multi-Stage ASR
arxiv_id: '2312.09463'
source_url: https://arxiv.org/abs/2312.09463
tags:
- partial
- cascaded
- causal
- streaming
- result
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the quality gap in partial streaming ASR results
  between low-latency causal models and higher-quality but delayed cascaded models.
  The authors propose a text-based rewriting algorithm that merges partial outputs
  from both models using Levenshtein alignment.
---

# Partial Rewriting for Multi-Stage ASR

## Quick Facts
- arXiv ID: 2312.09463
- Source URL: https://arxiv.org/abs/2312.09463
- Reference count: 5
- This paper proposes a text-based rewriting algorithm that merges partial outputs from causal and cascaded ASR models using Levenshtein alignment, reducing PWER by 10% with minimal latency increase.

## Executive Summary
This paper addresses the quality gap in partial streaming ASR results between low-latency causal models and higher-quality but delayed cascaded models. The authors propose a text-based rewriting algorithm that merges partial outputs from both models using Levenshtein alignment. The algorithm selects optimal alignment points to blend causal and cascaded partial results without increasing latency. Experimental results on multiple test sets show 10% PWER reduction and reduced flickering, with less than 10ms latency increase. The lightweight algorithm works across diverse multi-stage architectures without model retraining and consumes under 0.1ms per rewrite on mobile devices.

## Method Summary
The method involves a text-based rewriting algorithm that merges partial outputs from causal (low-latency, lower quality) and cascaded (higher latency, higher quality) ASR models. The core mechanism uses Levenshtein alignment to compute optimal merging points between the two partial result streams. The algorithm applies configurable parameters: T for cropping tokens to reduce flickering, K for hysteresis to prevent sudden transitions, and rho_r for alignment cost threshold. The rewriting process occurs after both models generate their partial results but before display, creating composite partial results that combine the timeliness of causal outputs with the accuracy of cascaded outputs.

## Key Results
- 10% reduction in Partial Word Error Rate (PWER) across multiple test sets
- Less than 10ms latency increase compared to baseline causal model
- Reduced unstable partial word ratio (UPWR) indicating fewer flickering transitions
- Computational overhead under 0.1ms per rewrite on mobile devices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Levenshtein alignment enables dynamic merging of causal and cascaded partial results without introducing latency.
- Mechanism: The algorithm computes the alignment cost matrix between causal and cascaded token sequences, then selects the column with minimum cost in the last row to determine how many causal tokens correspond to the time delay.
- Core assumption: The cascaded model's tokens are more accurate but delayed, while the causal model's tokens are less accurate but timely, and the alignment cost accurately reflects token similarity.
- Evidence anchors:
  - [abstract]: "propose using a text manipulation algorithm that merges the streaming outputs of both models"
  - [section 3.1]: "We propose to use a Levenshein alignment... we sweep all the costs on the bottom row and use the cell with the lowest cost"
- Break condition: If the alignment cost threshold is exceeded, the algorithm falls back to using only the causal transcript.

### Mechanism 2
- Claim: The trimming parameter T reduces flickering by removing T tokens from the cascaded transcript before alignment.
- Mechanism: By trimming T tokens from the beginning of the cascaded transcript, the algorithm reduces the number of changes made to already-output partial results.
- Core assumption: Removing tokens from the cascaded transcript reduces the chance of changing previously output words, and the remaining tokens still provide quality improvement.
- Evidence anchors:
  - [section 3.3]: "we can trim T word pieces and only use x[1, . . . ,max(m − T, 1)]"
  - [section 4.3]: "as T increases... the PWER goes up... the UPWR for only the partials goes down"
- Break condition: If T is set too high, the quality improvement from the cascaded model is lost.

### Mechanism 3
- Claim: The hysteresis mechanism prevents sudden transitions between rewriting strategies, reducing flickering.
- Mechanism: When alignment cost exceeds threshold, instead of immediately switching back to causal-only output, the algorithm continues using the last successfully used cascaded transcript.
- Core assumption: Sudden changes between rewriting strategies cause more flickering than gradual transitions.
- Evidence anchors:
  - [section 3.4]: "if the cost is too high, we simply do not create a composite transcript and keep the causal transcript"
  - [section 4.1]: "we applied the same deflickering algorithm... with α = 0.2 for our test model"
- Break condition: If the hysteresis mechanism fails to detect significant quality degradation.

## Foundational Learning

- Concept: Levenshtein distance and alignment algorithms
  - Why needed here: The core merging algorithm relies on computing optimal alignment between two token sequences to determine how to blend them
  - Quick check question: How does the algorithm determine which column in the last row to use for the final alignment?

- Concept: Streaming partial hypothesis management
  - Why needed here: Understanding how partial results are generated, managed, and displayed in real-time ASR systems is crucial for implementing the rewriting algorithm
  - Quick check question: What is the difference between partial and final results in streaming ASR, and why does this distinction matter for the algorithm?

- Concept: Cascaded encoder architecture tradeoffs
  - Why needed here: The algorithm specifically targets the quality gap between causal and cascaded models, requiring understanding of their respective strengths and limitations
  - Quick check question: Why does the cascaded model have better quality but higher latency than the causal model?

## Architecture Onboarding

- Component map: Causal model partial → Levenshtein alignment with cascaded partial → Composite partial output → Display
- Critical path: Causal model partial → Levenshtein alignment with cascaded partial → Composite partial output → Display
- Design tradeoffs: Quality improvement vs. flickering increase vs. computational overhead
- Failure signatures: Excessive flickering, latency increase beyond 10ms, quality degradation in composite results
- First 3 experiments:
  1. Test the base case with no rewriting to establish baseline PWER and UPWR metrics
  2. Implement basic Levenshtein alignment without trimming or hysteresis to measure quality improvement
  3. Add trimming parameter T and measure the tradeoff between quality and flickering reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed algorithm perform on very long-form utterances (e.g., hours of continuous speech) where the cascaded model's delay accumulates significantly?
- Basis in paper: [inferred] The paper mentions the algorithm works across diverse test sets but focuses on test durations up to ~17.63 seconds.
- Why unresolved: The paper only evaluates on test sets with maximum utterance durations around 17.63 seconds, leaving the performance on significantly longer utterances untested.
- What evidence would resolve it: Experimental results showing PWER and UPWR metrics on test sets containing utterances of 30+ minutes or even hours.

### Open Question 2
- Question: Would the algorithm's effectiveness change if the causal and cascaded models used different tokenization schemes (e.g., word-pieces vs. characters)?
- Basis in paper: [explicit] The authors mention that "the approach can also use Unicode codepoints as tokens" and that they use word-pieces, but they don't test different tokenization schemes between the two models.
- Why unresolved: The paper assumes both models use the same tokenization scheme but doesn't explore what happens when they differ.
- What evidence would resolve it: Experimental results comparing the algorithm's performance when the causal and cascaded models use different tokenization schemes.

### Open Question 3
- Question: How does the algorithm's performance scale with the size ratio between the causal and cascaded models? Would a much larger cascaded model (10x weights) provide proportionally better rewriting quality?
- Basis in paper: [explicit] The authors state the cascaded model "has more weights than the causal one" and show improvements with their current size ratio, but they don't explore different model size ratios.
- Why unresolved: The paper uses a specific model architecture but doesn't investigate how the algorithm's effectiveness changes with different size ratios between the two models.
- What evidence would resolve it: Experiments testing the algorithm with various size ratios while measuring PWER improvements and computational costs.

## Limitations
- The algorithm's effectiveness depends heavily on alignment quality between causal and cascaded partial results, which may degrade under diverse acoustic conditions
- The solution adds complexity without addressing fundamental architectural limitations causing the quality gap
- Performance across diverse multi-stage architectures is not thoroughly validated beyond a single conformer-transducer model

## Confidence
- **High confidence**: The algorithm can reduce PWER by approximately 10% while maintaining final WER, as this is directly measured across multiple test sets with clear metrics.
- **Medium confidence**: The claim about reduced flickering through parameter tuning is supported by experiments but lacks comprehensive user perception studies.
- **Low confidence**: The assertion that the algorithm "works across diverse multi-stage architectures without model retraining" is not sufficiently validated.

## Next Checks
1. **Alignment Quality Analysis**: Conduct systematic experiments to measure Levenshtein alignment accuracy across different speaking styles, accents, and acoustic conditions.
2. **Real-World Deployment Validation**: Implement the algorithm in a production ASR system with live user traffic and measure actual user-perceived latency, flickering frequency, and quality improvements.
3. **Architectural Generalization Study**: Test the rewriting algorithm across multiple different ASR architectures and model configurations to validate the claimed architecture-agnostic nature.