---
ver: rpa2
title: State-Wise Safe Reinforcement Learning With Pixel Observations
arxiv_id: '2311.02227'
source_url: https://arxiv.org/abs/2311.02227
tags:
- safety
- latent
- learning
- barrier
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles safe reinforcement learning in unknown environments
  with pixel observations, addressing challenges of state-wise safety constraints,
  high-dimensional inputs, and non-smooth dynamics. It proposes a latent barrier function
  learning mechanism that encodes safety constraints directly in a compressed latent
  space derived from pixel observations.
---

# State-Wise Safe Reinforcement Learning With Pixel Observations

## Quick Facts
- arXiv ID: 2311.02227
- Source URL: https://arxiv.org/abs/2311.02227
- Authors: 
- Reference count: 11
- One-line primary result: Latent barrier function learning mechanism reduces safety violations during training while achieving competitive reward returns on Safety Gym benchmark.

## Executive Summary
This paper addresses the challenge of safe reinforcement learning in unknown environments with pixel observations, where state-wise safety constraints must be enforced without prior knowledge of unsafe regions. The authors propose a novel approach that learns a latent barrier function in a compressed latent space derived from pixel observations, enabling efficient sampling and enforcement of step-wise safety. The method jointly learns a latent dynamics model, a barrier function, and a policy, reducing the need for unsafe interactions during early policy updates.

## Method Summary
The approach uses a recurrent state space model (RSSM) to compress pixel observations into low-dimensional latent states, which are then used for barrier function learning and policy optimization. A latent barrier function is learned to separate safe and unsafe states in the latent space, with forward invariance guaranteed via a positive time derivative. The joint learning framework integrates latent dynamics modeling, barrier function learning, and policy optimization into a single actor-critic loop, allowing for efficient updates without requiring additional real-world unsafe interactions. The method is evaluated on the Safety Gym benchmark, demonstrating significant reductions in safety violations during training while achieving competitive reward returns compared to model-based CMDP methods.

## Key Results
- Significant reduction in safety violations during training on Safety Gym benchmark
- Faster safety convergence compared to model-based CMDP methods
- Competitive reward returns while enforcing step-wise safety constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The latent barrier function enforces step-wise safety by separating latent states into safe and unsafe sets, with forward invariance guaranteed via a positive time derivative.
- Mechanism: A barrier function Bθ(z) is learned over latent states. It must be positive in safe states, negative in unsafe states, and satisfy Bθ(zt) - Bθ(zt-1) + α(Bθ(z)) > 0, which ensures the next state remains in the safe set if the current state is safe.
- Core assumption: The latent state space sufficiently captures the safety-relevant information from pixel observations so that the barrier function can be defined meaningfully over it.
- Evidence anchors:
  - [abstract]: "latent barrier-like function learning mechanism" and "efficiently encodes state-wise safety constraints"
  - [section 4.2]: Definition 1 and Equation (3) formally define the barrier function properties
  - [corpus]: No direct evidence; related papers focus on state-wise constraints but do not cite barrier function learning in latent spaces
- Break condition: If the latent space fails to preserve the safety-relevant structure of the original state, the barrier function cannot correctly separate safe and unsafe states.

### Mechanism 2
- Claim: The joint learning framework reduces safety violations by integrating latent dynamics modeling, barrier function learning, and policy optimization into a single actor-critic loop.
- Mechanism: Trajectories are first generated in the real environment, encoded into latent states via the visual encoder, and used to update the latent model. Barrier function and policy are then updated using latent-model-generated trajectories before new real data is collected. This avoids unsafe interactions during early policy updates.
- Core assumption: The latent dynamics model is accurate enough to simulate realistic trajectories that inform safe policy updates without requiring additional real-world unsafe interactions.
- Evidence anchors:
  - [abstract]: "joint learning framework" and "simultaneously, thereby improving both safety and the total expected return"
  - [section 4.3]: Description of the joint learning loop and trajectory generation from the latent model
  - [corpus]: No direct evidence; related works discuss model-based RL but not joint barrier function learning
- Break condition: If the latent dynamics model's predictions diverge significantly from real dynamics, the policy may be trained on unrealistic safe trajectories and fail in the real environment.

### Mechanism 3
- Claim: Compressing pixel observations into a low-dimensional latent space enables safe RL to scale to high-dimensional visual inputs without losing safety-relevant features.
- Mechanism: A recurrent state space model (RSSM) encodes observations ot into latent states zt, which are then used for both barrier function learning and policy optimization. The latent space is defined only on zt (not ht) to reduce redundancy and computation.
- Core assumption: The visual encoder preserves the mapping from observation to latent state in a way that maintains safety-relevant distinctions.
- Evidence anchors:
  - [abstract]: "efficiently encodes state-wise safety constraints with unknown hazard regions through the introduction of a latent barrier function learning mechanism"
  - [section 4.1]: Description of the latent model components and the choice to define the state space on zt
  - [corpus]: No direct evidence; related works mention pixel-based safe RL but not latent barrier functions
- Break condition: If the encoder fails to distinguish between safe and unsafe observations in the latent space, the barrier function cannot enforce safety.

## Foundational Learning

- Concept: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: The paper contrasts its approach with CMDP-based safe RL methods, highlighting CMDP's limitation in enforcing hard state-wise constraints.
  - Quick check question: What is the key difference between CMDP's soft safety constraints and the hard constraints enforced by the barrier function in this work?

- Concept: Barrier functions from control theory
  - Why needed here: The proposed method adapts barrier functions to a latent space for safe RL, leveraging their ability to encode formal safety guarantees.
  - Quick check question: How does a barrier function ensure forward invariance in a dynamical system, and why is this useful for safe RL?

- Concept: Recurrent state space models (RSSM)
  - Why needed here: The latent dynamics model is built on RSSM to handle pixel observations and capture non-smooth environment dynamics.
  - Quick check question: What are the roles of the posterior zt, prior ˆzt, and hidden state ht in an RSSM, and why does this work define the latent state only on zt?

## Architecture Onboarding

- Component map:
  - Visual Encoder: Maps pixel observations ot to latent states zt
  - Latent Dynamics Model: Predicts next latent state ˆzt, reward ˆrt, and safety ˆκt from zt and action at
  - Barrier Function Network: Estimates Bθ(zt) and is trained to enforce safety via Equation (4)
  - Policy Network: Outputs action distribution πθ(·|zt) and is trained via Equation (5)
  - Value Network: Estimates expected return vω(zt) for actor-critic updates
  - Experience Buffer: Stores real trajectories for latent model training

- Critical path:
  1. Collect real trajectories with initial policy
  2. Update latent model (encoder, dynamics, safety predictor) using real data
  3. Generate latent-model trajectories for joint barrier and policy updates
  4. Update policy and barrier function using generated trajectories
  5. Collect new real trajectories with updated policy
  6. Repeat

- Design tradeoffs:
  - Using only zt (not ht) for the latent state simplifies the barrier function but may lose some predictive information
  - Model-based updates reduce real-world safety violations but depend on latent model accuracy
  - Joint learning of barrier and policy simplifies training but couples their updates

- Failure signatures:
  - High safety violations in evaluation indicate the latent model or barrier function is not capturing safety-relevant features
  - Poor reward return with low safety violations may indicate overly conservative barrier function training
  - Unstable training or divergence may indicate poor latent model quality or inappropriate learning rates

- First 3 experiments:
  1. Train the latent model alone on Safety Gym data and visualize the latent space coloring by safety label to verify it separates safe/unsafe regions
  2. Train only the barrier function (fixed policy) and measure its ability to predict safety in held-out latent states
  3. Run joint learning with a random policy to verify that the barrier function reduces safety violations compared to no barrier

## Open Questions the Paper Calls Out
The paper acknowledges that given the constraints, some safety violations may be unavoidable in the setup due to partial observability. However, it doesn't provide a concrete solution for handling partial observability in real-world deployment scenarios.

## Limitations
- The method assumes safety-relevant features can be preserved in the latent space, which may not hold in all domains
- The paper lacks a detailed analysis of the latent model's predictive accuracy and the barrier function's sensitivity to hyperparameters
- The evaluation is limited to a specific set of Safety Gym tasks, limiting generalizability claims

## Confidence
- Core claims: Medium
- Experimental results: Medium
- Method generalizability: Low

## Next Checks
1. Evaluate the approach on additional safety-critical environments with varying observation dimensions and safety constraint types to assess generalizability.
2. Conduct an ablation study to quantify the individual contributions of the latent model, barrier function, and policy optimization to the overall performance.
3. Analyze the latent model's predictive accuracy and the barrier function's sensitivity to hyperparameters to identify potential failure modes and guide future improvements.