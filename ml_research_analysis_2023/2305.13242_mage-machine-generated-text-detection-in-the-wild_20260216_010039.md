---
ver: rpa2
title: 'MAGE: Machine-generated Text Detection in the Wild'
arxiv_id: '2305.13242'
source_url: https://arxiv.org/abs/2305.13242
tags:
- texts
- detection
- text
- machine-generated
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a comprehensive testbed for deepfake text
  detection in the wild, addressing the challenge of distinguishing machine-generated
  texts from human-authored ones across diverse domains and language models. The authors
  construct a large-scale dataset by collecting human-written texts from various tasks
  and generating corresponding deepfake texts using 27 different large language models.
---

# MAGE: Machine-generated Text Detection in the Wild

## Quick Facts
- arXiv ID: 2305.13242
- Source URL: https://arxiv.org/abs/2305.13242
- Authors: 
- Reference count: 25
- Key outcome: Comprehensive testbed for deepfake text detection across diverse domains and language models

## Executive Summary
This paper introduces MAGE, a comprehensive testbed for deepfake text detection in the wild, addressing the challenge of distinguishing machine-generated texts from human-authored ones across diverse domains and language models. The authors construct a large-scale dataset by collecting human-written texts from various tasks and generating corresponding deepfake texts using 27 different large language models. They organize the data into six testbeds with increasing difficulty, evaluating four representative detection methods. The empirical results show that while detection is feasible, it becomes increasingly challenging when encountering texts from unseen domains or generated by new models. The best-performing method, a fine-tuned PLM-based classifier, achieves over 90% AvgRec in in-domain settings but drops to 68.40% AvgRec for out-of-distribution detection.

## Method Summary
The authors collect human-written texts from 10 diverse datasets and generate corresponding deepfake texts using 27 different large language models, creating a dataset of 447,674 instances. They organize this data into six testbeds with increasing difficulty, ranging from in-domain to out-of-distribution settings. Four detection methods are evaluated: zero-shot (DetectGPT), feature-based (FastText, GLTR), and PLM-based (Longformer) approaches. The PLM-based method uses Longformer fine-tuned with a classification layer on top, trained for 5 epochs using Adam optimizer with learning rate 0.005 and dropout rate 0.1.

## Key Results
- PLM-based fine-tuning (Longformer) achieves over 90% AvgRec in in-domain settings but drops to 68.40% AvgRec for out-of-distribution detection
- Zero-shot and feature-based methods perform poorly, especially in wilder detection settings
- Adjusting the decision boundary using 0.1% in-domain data improves out-of-distribution performance by 13.38% AvgRec
- Linguistic similarity between human-written and machine-generated texts increases as detection settings become wilder

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning PLMs (Longformer) achieves superior performance across all testbed settings compared to other methods
- Mechanism: PLMs capture nuanced linguistic patterns across domains and model types, enabling them to learn generalizable features that distinguish human-written from machine-generated text
- Core assumption: Pre-training of PLMs has exposed them to sufficient linguistic diversity to make them effective at detecting subtle differences between human and machine text generation styles
- Evidence anchors:
  - [abstract]: "The best-performing method, a fine-tuned PLM-based classifier, achieves over 90% AvgRec in in-domain settings but drops to 68.40% AvgRec for out-of-distribution detection"
  - [section]: "Longformer consistently outperforms other detection methods in terms of both AvgRec and AUROC"
  - [corpus]: Weak - the corpus neighbors are about detection methods but do not directly compare PLM fine-tuning performance
- Break condition: If PLMs have not been exposed to sufficient linguistic diversity during pre-training, or if differences between human and machine text generation styles become too subtle

### Mechanism 2
- Claim: Adjusting the decision boundary using a small portion of in-domain data significantly improves out-of-distribution performance
- Mechanism: PLM-based detectors exhibit a bias where they assume machine-generated texts have low perplexity and human-written texts have high perplexity. By adjusting the decision boundary based on in-domain data, this bias can be corrected, leading to improved detection accuracy
- Core assumption: The bias in PLM-based detectors can be effectively corrected by adjusting the decision boundary, and a small amount of in-domain data is sufficient for this adjustment
- Evidence anchors:
  - [abstract]: "The authors demonstrate that adjusting the decision boundary using a small portion of in-domain data can significantly improve performance"
  - [section]: "Based on this observation, we use 0.1% of the in-domain data to select a new decision boundary. This boosts the detector's performance by a large margin (+13.38% AvgRec) under the out-of-distribution setting"
  - [corpus]: Weak - the corpus neighbors do not discuss decision boundary adjustment
- Break condition: If the bias in PLM-based detectors is too complex to be corrected by a simple decision boundary adjustment, or if the in-domain data is not representative enough

### Mechanism 3
- Claim: The linguistic similarity between human-written and machine-generated texts increases as the detection setting becomes wilder
- Mechanism: As the detection setting becomes wilder, the linguistic patterns that can distinguish human-written from machine-generated texts become less distinct, making detection more challenging
- Core assumption: The linguistic patterns that distinguish human-written from machine-generated texts are domain and model specific, and become less distinct as the detection setting becomes wilder
- Evidence anchors:
  - [abstract]: "Empirical results show that while detection is feasible, it becomes increasingly challenging when encountering texts from unseen domains or generated by new models"
  - [section]: "We can observe that the inclusion of texts from more domains and more LLMs decreases the linguistic dissimilarity between the two text sources, making it more difficult for a detector to distinguish them"
  - [corpus]: Weak - the corpus neighbors do not discuss the linguistic similarity between human-written and machine-generated texts
- Break condition: If the linguistic patterns that distinguish human-written from machine-generated texts are not domain and model specific, or if they remain distinct even in wilder detection settings

## Foundational Learning

- Concept: Understanding the limitations of zero-shot and feature-based detection methods
  - Why needed here: The paper shows that zero-shot methods (DetectGPT) and feature-based methods (FastText, GLTR) perform poorly, especially in wilder detection settings. Understanding their limitations is crucial for appreciating the value of PLM-based methods
  - Quick check question: Why do zero-shot and feature-based methods struggle in wilder detection settings?

- Concept: The importance of out-of-distribution generalization in real-world applications
  - Why needed here: The paper emphasizes the challenges of detecting machine-generated texts in out-of-distribution settings, which are common in real-world scenarios. Understanding this concept is key to appreciating the significance of the paper's findings
  - Quick check question: Why is out-of-distribution generalization important for real-world applications of machine-generated text detection?

- Concept: The role of perplexity in machine-generated text detection
  - Why needed here: The paper discusses how PLM-based detectors exhibit a bias related to perplexity, which can be corrected by adjusting the decision boundary. Understanding the role of perplexity is crucial for understanding this mechanism
  - Quick check question: How does perplexity influence the performance of PLM-based detectors in machine-generated text detection?

## Architecture Onboarding

- Component map: Data collection -> Testbed construction -> Detection method implementation -> Evaluation and analysis
- Critical path: 1. Data collection and preprocessing 2. Testbed construction 3. Detection method implementation and training 4. Evaluation and analysis
- Design tradeoffs: Data balance vs. model performance (balancing human-written and machine-generated texts may not significantly impact performance); Decision boundary adjustment vs. model complexity (improving performance without increasing complexity)
- Failure signatures: Poor performance in out-of-distribution settings indicates lack of generalization; High AUROC but low AvgRec suggests issues with decision boundary selection
- First 3 experiments: 1. Evaluate different detection methods in Domain-specific & Model-specific setting to establish baseline 2. Analyze linguistic similarity across testbed settings 3. Investigate impact of decision boundary adjustment on out-of-distribution performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can deepfake text detection methods be made robust to paraphrasing attacks that evade current detection techniques?
- Basis in paper: [explicit] The paper mentions that successful attacks of paraphrasers disclose the vulnerabilities of existing detectors (Sadasivan et al., 2023; Krishna et al., 2023)
- Why unresolved: The paper does not explore potential defenses against paraphrasing attacks or evaluate how well current methods perform against them
- What evidence would resolve it: Experiments evaluating detection performance on paraphrased versions of human-written and machine-generated texts, along with proposed methods to improve robustness to paraphrasing

### Open Question 2
- Question: How can deepfake text detection be improved for unseen domains and language models?
- Basis in paper: [explicit] The paper demonstrates that out-of-distribution detection is challenging, with significant performance drops when encountering texts from unseen domains or generated by new models
- Why unresolved: While the paper shows the feasibility of adjusting decision boundaries using a small portion of in-domain data, it does not explore other potential strategies for improving out-of-distribution detection
- What evidence would resolve it: Experiments evaluating various methods for improving out-of-distribution detection, such as few-shot learning, domain adaptation, or leveraging unlabeled data from unseen domains

### Open Question 3
- Question: Can linguistic patterns be leveraged to improve deepfake text detection in the wild?
- Basis in paper: [explicit] The paper finds that human-written and machine-generated texts share significant similarities in linguistic statistics, making it challenging for detectors to rely on surface patterns
- Why unresolved: The paper does not explore whether more sophisticated linguistic analysis techniques could uncover subtle differences between the two text sources that are not apparent in surface statistics
- What evidence would resolve it: Experiments evaluating the effectiveness of advanced linguistic analysis methods, such as syntactic or semantic features, in improving detection performance on the wild testbed

## Limitations

- The decision boundary adjustment mechanism may not generalize to more complex linguistic patterns or substantial distribution shifts
- The six testbed settings may not fully capture the diversity of real-world detection scenarios despite being comprehensive
- The paper does not fully explore the underlying linguistic theory explaining why machine-generated text becomes more human-like across different domains and models

## Confidence

**High Confidence**: PLM-based fine-tuning (Longformer) outperforms zero-shot and feature-based methods across all testbed settings, with consistent performance improvements across multiple evaluation metrics

**Medium Confidence**: The effectiveness of decision boundary adjustment using small amounts of in-domain data, while demonstrated, requires further validation across different PLM architectures and domain types to ensure generalizability

**Medium Confidence**: The observation that detection becomes increasingly challenging in wilder settings is supported by the data, but the paper doesn't fully explore whether this is due to fundamental limitations in detection methodology or specific characteristics of the generated text

## Next Checks

1. **Cross-PLM Validation**: Test whether the decision boundary adjustment mechanism generalizes to other PLM architectures (e.g., BERT, RoBERTa) beyond Longformer, using the same 0.1% in-domain data strategy to verify the robustness of this approach

2. **Temporal Stability Test**: Evaluate detector performance on texts generated by newer LLMs (post-training dataset cutoff) to assess whether the linguistic patterns identified remain stable over time or require continuous retraining

3. **Domain Transfer Analysis**: Conduct a detailed error analysis comparing detection performance across specific domain pairs (e.g., scientific writing vs. news) to identify which domain combinations pose the greatest challenges and why