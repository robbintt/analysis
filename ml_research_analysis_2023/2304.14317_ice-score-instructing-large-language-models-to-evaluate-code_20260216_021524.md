---
ver: rpa2
title: 'ICE-Score: Instructing Large Language Models to Evaluate Code'
arxiv_id: '2304.14317'
source_url: https://arxiv.org/abs/2304.14317
tags:
- code
- gpt-3
- problem
- snippet
- usefulness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel evaluation framework, ICE-Score,
  based on large language models (LLMs) for assessing code generation quality. The
  framework addresses the limitations of existing metrics by achieving superior correlations
  with functional correctness and human preferences without requiring test oracles
  or references.
---

# ICE-Score: Instructing Large Language Models to Evaluate Code

## Quick Facts
- arXiv ID: 2304.14317
- Source URL: https://arxiv.org/abs/2304.14317
- Reference count: 40
- Primary result: Novel LLM-based evaluation framework achieving superior correlations with functional correctness and human preferences without requiring test oracles or references

## Executive Summary
This paper introduces ICE-Score, a novel evaluation framework that leverages large language models to assess code generation quality. The framework addresses key limitations of existing metrics by achieving superior correlations with both functional correctness and human preferences, while eliminating the need for test oracles or reference implementations. Through comprehensive experiments across four programming languages, the paper demonstrates that ICE-Score significantly outperforms state-of-the-art metrics like CodeBERTScore, offering a more reliable and accessible approach to code evaluation.

## Method Summary
ICE-Score employs a prompt-based evaluation approach using GPT-3.5-turbo to assess code generation quality. The method involves constructing task-specific prompts that define evaluation criteria and instructions for the LLM, which then scores generated code based on usefulness and functional correctness. The framework supports both reference-free evaluation (comparing generated code against problem statements) and reference-enhanced evaluation (including reference solutions), with the option to incorporate zero-shot Chain-of-Thought reasoning to improve reliability. The paper systematically evaluates this approach across multiple datasets and programming languages, comparing results with traditional metrics.

## Key Results
- GPT-3.5-based evaluators significantly outperform traditional metrics (BLEU, CodeBLEU, CodeBERTScore) across four programming languages in both human preference and functional correctness tasks
- ICE-Score achieves superior correlations with human judgment and execution success without requiring test oracles or reference implementations
- Zero-shot Chain-of-Thought reasoning substantially improves evaluation reliability, particularly for reference-enhanced evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs possess a profound understanding of programming concepts across multiple languages, enabling them to evaluate code generation quality effectively.
- Mechanism: The paper's experimental results demonstrate that GPT-3.5-based evaluators consistently outperform traditional metrics like BLEU, CodeBLEU, and CodeBERTScore across four programming languages (Java, Python, C, C++, JavaScript) in both human preference and functional correctness tasks.
- Core assumption: LLMs trained on both text and code data have developed sufficient semantic understanding of source code to assess its quality without requiring explicit test oracles or reference implementations.
- Evidence anchors:
  - [abstract]: "Our metric addresses the limitations of existing approaches by achieving superior correlations with functional correctness and human preferences, without the need for test oracles or references."
  - [section 4]: "GPT-3.5-based evaluators outperforms these metrics by a significant margin, regarding both example- and corpus-level correlations."
  - [corpus]: The corpus contains multiple datasets across different programming languages, providing evidence that the mechanism works broadly across languages.
- Break condition: If the LLM has not been exposed to sufficient code training data or the code generation task involves highly specialized domains not represented in the training corpus.

### Mechanism 2
- Claim: Reference code inputs enhance LLM evaluation reliability by providing concrete examples of correct solutions.
- Mechanism: The paper shows that reference-enhanced evaluators achieve state-of-the-art performance on HumanEval datasets, though reference-free evaluators also perform comparably well on CoNaLa.
- Core assumption: LLMs can leverage reference code to better understand what constitutes a correct solution and identify deviations from expected behavior.
- Evidence anchors:
  - [section 4]: "When compared to other baselines, the reference-free GPT-3.5 evaluator still achieves comparable results to the source-free CodeBERTScore-F3."
  - [section 4]: "we find that reference-enhanced evaluator does not significantly improve the performance, indicating the reference-code may not be optimized."
  - [corpus]: Mixed evidence - reference-enhanced works better on some datasets but not others, suggesting context-dependent effectiveness.
- Break condition: When reference code is of poor quality, incomplete, or not optimized, as the paper notes that "the average score of reference code only achieves 3.4 out of 4."

### Mechanism 3
- Claim: Zero-shot Chain-of-Thought reasoning significantly improves the reliability and alignment of LLM-based code evaluation.
- Mechanism: The paper demonstrates that adding step-by-step evaluation instructions (zero-shot-CoT) improves correlation with human preferences, particularly for reference-enhanced evaluations.
- Core assumption: LLMs can perform logical reasoning about code quality when explicitly instructed to break down their evaluation process into steps.
- Evidence anchors:
  - [section 5]: "Our results show that zero-shot-CoT can significantly improve the reliability of code generation evaluation."
  - [section 5]: "Additionally, we find that reference-enhanced evaluators can achieve better results than reference-free ones via zero-shot-CoT, even though their performances are similar without CoT processing."
  - [corpus]: Limited to CoNaLa dataset due to resource constraints, so corpus evidence is weak here.
- Break condition: If the evaluation task is too complex for step-by-step reasoning or if the LLM fails to properly execute the reasoning steps.

## Foundational Learning

- Concept: Correlation metrics (Kendall-Tau, Pearson, Spearman)
  - Why needed here: To quantitatively measure how well automatic evaluation metrics align with human judgment and functional correctness.
  - Quick check question: What is the difference between Pearson correlation (measuring linear relationships) and Spearman correlation (measuring monotonic relationships)?

- Concept: Reference-free vs reference-enhanced evaluation
  - Why needed here: Understanding when and why providing reference code improves or doesn't improve evaluation performance.
  - Quick check question: Why might reference-enhanced evaluation not always outperform reference-free evaluation, according to the paper's findings?

- Concept: Chain-of-Thought prompting
  - Why needed here: To understand how breaking down complex reasoning tasks into steps can improve LLM performance on code evaluation.
  - Quick check question: What is the key difference between zero-shot-CoT and standard Chain-of-Thought prompting?

## Architecture Onboarding

- Component map: Problem statement + generated code → Prompt construction → LLM inference → Score extraction → Correlation computation
- Critical path: Problem + code → Prompt construction → LLM inference → Score extraction → Correlation computation
- Design tradeoffs:
  - Reference-free vs reference-enhanced: Simpler implementation vs potentially better performance
  - Zero-shot-CoT vs direct scoring: More reliable but requires additional parsing logic
  - Rule-based vs model-based score extraction: Deterministic but potentially brittle
- Failure signatures:
  - Low correlation with human judgment despite high confidence from LLM
  - Inconsistent scores across similar code snippets
  - Failure to properly parse LLM output when using zero-shot-CoT
- First 3 experiments:
  1. Baseline evaluation: Run reference-free evaluator on CoNaLa dataset and compare with traditional metrics
  2. Reference enhancement test: Add reference code to the same CoNaLa examples and measure performance change
  3. Zero-shot-CoT impact: Apply step-by-step evaluation instructions and compare reliability improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ICE-Score vary across different programming languages and domains beyond the ones tested?
- Basis in paper: [explicit] The paper evaluates ICE-Score on four programming languages (Java, Python, C, C++, JavaScript) but does not explore other languages or domains.
- Why unresolved: The paper focuses on a limited set of languages and tasks, leaving open the question of how well ICE-Score generalizes to other programming paradigms and domains.
- What evidence would resolve it: Conducting experiments on a wider range of programming languages and domains, such as functional programming, web development, or scientific computing, would provide evidence of ICE-Score's generalizability.

### Open Question 2
- Question: How does ICE-Score perform when evaluating code generation models with different architectures and training data?
- Basis in paper: [explicit] The paper evaluates ICE-Score on code generated by various models but does not explore the impact of different model architectures or training data on ICE-Score's performance.
- Why unresolved: The paper does not investigate how ICE-Score's performance is affected by the characteristics of the code generation models being evaluated.
- What evidence would resolve it: Evaluating ICE-Score on code generated by models with different architectures (e.g., transformers, recurrent neural networks) and training data (e.g., different programming languages, codebases) would provide insights into its robustness and adaptability.

### Open Question 3
- Question: Can ICE-Score be adapted to evaluate other aspects of code quality beyond functional correctness and usefulness?
- Basis in paper: [explicit] The paper focuses on evaluating code generation based on functional correctness and usefulness but does not explore other aspects of code quality, such as maintainability, readability, or efficiency.
- Why unresolved: The paper's evaluation criteria are limited to functional correctness and usefulness, leaving open the question of whether ICE-Score can be extended to assess other important aspects of code quality.
- What evidence would resolve it: Adapting ICE-Score to incorporate additional evaluation criteria, such as code complexity, adherence to coding standards, or runtime efficiency, and evaluating its performance on these aspects would demonstrate its potential for broader code quality assessment.

## Limitations
- The findings are primarily based on experiments using GPT-3.5-turbo, raising questions about generalizability to other LLM architectures or smaller models
- Evaluation scope is limited to five programming languages, creating uncertainty about performance on less common or domain-specific languages
- Resource constraints prevented comprehensive testing of zero-shot-CoT across all datasets, limiting validation of this promising mechanism

## Confidence
- **High Confidence**: The ICE-Score framework's superior correlation with human judgment and functional correctness compared to traditional metrics (BLEU, CodeBLEU, CodeBERTScore) is well-supported by experimental evidence across multiple languages and datasets
- **Medium Confidence**: The effectiveness of zero-shot-CoT in improving evaluation reliability is demonstrated but only on a subset of datasets due to resource limitations, suggesting the need for broader validation
- **Medium Confidence**: The finding that reference-enhanced evaluation doesn't always improve performance is well-supported, but the mechanism behind this phenomenon (reference quality issues) requires further investigation

## Next Checks
1. **Cross-model validation**: Test ICE-Score framework using different LLM architectures (e.g., Claude, LLaMA, smaller GPT variants) to assess whether the superior correlation performance is model-dependent or a general property of the approach
2. **Language diversity expansion**: Evaluate ICE-Score on additional programming languages, particularly domain-specific languages (SQL, MATLAB, R) and emerging languages, to determine the breadth of the framework's applicability
3. **Resource-constrained deployment testing**: Implement ICE-Score with smaller, more computationally efficient models to assess whether the evaluation quality can be maintained with reduced resource requirements, making it more accessible for practical deployment