---
ver: rpa2
title: Low Resource Summarization using Pre-trained Language Models
arxiv_id: '2310.02790'
source_url: https://arxiv.org/abs/2310.02790
tags:
- summarization
- language
- evaluation
- urdu
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of low-resource language summarization,
  specifically focusing on Urdu. The authors propose adapting pre-trained language
  models like mBERT and mT5 for Urdu summarization.
---

# Low Resource Summarization using Pre-trained Language Models

## Quick Facts
- arXiv ID: 2310.02790
- Source URL: https://arxiv.org/abs/2310.02790
- Reference count: 40
- One-line primary result: urT5 achieves 46.35 ROUGE-1 and 77 BERTScore for Urdu summarization, comparable to English SOTA

## Executive Summary
This paper addresses the challenge of low-resource language summarization by adapting pre-trained multilingual language models for Urdu. The authors construct a new dataset of 76.5k Urdu article-summary pairs and propose urT5, a reduced-size version of mT5 that achieves competitive performance while being 44.78% smaller. The study demonstrates that multilingual models can effectively capture contextual information for low-resource languages and provides insights into model adaptation, training, and evaluation in resource-constrained settings.

## Method Summary
The authors adapt multilingual pre-trained models (mBERT and mT5) for Urdu summarization by first constructing a new dataset from BBC Urdu and DW Urdu news sources. For extractive summarization, they use mBERT-based models with sentence clustering and intelligent truncation based on recall. For abstractive summarization, they create urT5 by reducing mT5's vocabulary from 250k to 40k tokens, achieving a 44.78% size reduction. The models are fine-tuned on 72k training pairs with batch size 4 and gradient accumulation 8 for up to 5 epochs, evaluated using ROUGE and BERTScore metrics.

## Key Results
- urT5 achieves 46.35 ROUGE-1 and 77 BERTScore, comparable to English state-of-the-art models
- urT5 reduces model size by 44.78% compared to mT5 while maintaining performance
- Intelligent truncation based on recall improves extractive summarization within 512-token limits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing multilingual vocabulary to monolingual improves efficiency without hurting performance.
- Mechanism: Most parameters in multilingual models reside in embedding layers. By restricting shared vocabulary to only the target language tokens, the model's memory footprint shrinks while preserving learned representations for the task.
- Core assumption: Embedding layer dominates model size, and shared vocabulary contains many irrelevant tokens for the target language.
- Evidence anchors:
  - [abstract]: "urT5 with up to 44.78% reduction in size as compared to mT5"
  - [section]: "By reducing the vocabulary; input/output embeddings of the model are reduced. As a result size of the model and its memory utilization is reduced retaining almost the same efficiency as of original multilingual model."
  - [corpus]: Weak - corpus does not contain direct evidence about embedding layer parameter distribution.
- Break condition: If target language shares substantial vocabulary with other languages in the model, or if the embedding layer is not the dominant size contributor, efficiency gains will be reduced or performance will degrade.

### Mechanism 2
- Claim: Intelligent truncation based on recall improves extractive summarization for models with input length limits.
- Mechanism: By ranking paragraphs according to their recall overlap with the summary, important content is preserved within the 512-token limit, ensuring the model still has access to the most summary-relevant information.
- Core assumption: Summary content is highly correlated with article content recall, and paragraph-level truncation preserves sentence order and coherence.
- Evidence anchors:
  - [section]: "Intelligent Truncation has been carried out using Recall measure between article text paragraphs and summary text to cater for the limitations of BERT-based models."
  - [section]: "Truncation is explained in Procedure 1 & Fig. 2."
  - [corpus]: Weak - no direct corpus evidence for truncation effectiveness.
- Break condition: If recall-based ranking fails to capture summary-relevant content (e.g., summaries rely on scattered details), or if the model cannot handle paragraph reordering, truncation will harm results.

### Mechanism 3
- Claim: Multilingual pre-trained models capture sufficient language representation for low-resource languages to enable competitive zero-shot or fine-tuned summarization.
- Mechanism: Shared multilingual training provides a generalized language model capable of representing low-resource languages, which can then be adapted with minimal fine-tuning for specific tasks like summarization.
- Core assumption: Even with under-representation, multilingual models retain enough linguistic structure of low-resource languages to be useful after adaptation.
- Evidence anchors:
  - [abstract]: "urT5... can capture contextual information of low resource language effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore) at par with state-of-the-art models in high resource language English"
  - [section]: "Evaluation of these multilingual models on various tasks showed improved results."
  - [corpus]: Moderate - corpus contains related work showing multilingual models help low-resource tasks, though under-representation is acknowledged.
- Break condition: If the target language is too under-represented in pre-training data, fine-tuning will fail to recover sufficient representation for the task.

## Foundational Learning

- Concept: Transfer learning in NLP
  - Why needed here: The paper relies on adapting pre-trained models (mBERT, mT5) for summarization in Urdu, a low-resource language.
  - Quick check question: What is the difference between zero-shot and fine-tuned transfer learning settings?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Both extractive and abstractive models are based on transformer models; understanding self-attention is key to grasping how these models work.
  - Quick check question: How does multi-head self-attention in transformers differ from recurrent networks for sequence modeling?

- Concept: Evaluation metrics for summarization (ROUGE, BERTScore)
  - Why needed here: The paper evaluates summarization quality using ROUGE and BERTScore; understanding their strengths and limitations is crucial for interpreting results.
  - Quick check question: Why might ROUGE score lower for abstractive summaries even when human judgment rates them highly?

## Architecture Onboarding

- Component map: Web scraping -> Preprocessing (cleaning, truncation) -> Tokenization -> Dataset split -> Model training -> Evaluation
- Critical path: Dataset creation -> model adaptation -> training -> evaluation -> analysis
- Design tradeoffs:
  - Size vs. performance: Smaller urT5 vs. full mT5
  - Truncation vs. completeness: 512-token limit may lose context
  - Automated vs. human evaluation: trade-off between scalability and accuracy
- Failure signatures:
  - Low ROUGE/BERTScore but high human score → model captures meaning but uses novel phrasing
  - High automated scores but low human score → model copies or extracts obvious sentences, lacks coherence
  - Training instability or overfitting → learning rate or batch size issues
- First 3 experiments:
  1. Run extractive summarization on truncated vs. non-truncated datasets to confirm truncation benefit.
  2. Compare urT5-base vs. mT5-small on a small subset to verify efficiency gains.
  3. Perform zero-shot summarization with mT5 to establish baseline before fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can low-resource languages be effectively represented in multilingual language models to reduce under-representation?
- Basis in paper: [explicit] The paper discusses the under-representation of low-resource languages in multilingual models and mentions challenges like under-representation and biasness.
- Why unresolved: The paper highlights the problem but doesn't propose specific solutions to address the under-representation of low-resource languages in multilingual models.
- What evidence would resolve it: Developing and evaluating methods that specifically improve representation of low-resource languages in multilingual models, potentially through data augmentation, targeted pre-training, or architectural modifications.

### Open Question 2
- Question: What are the most effective evaluation methods for abstractive summarization in low-resource languages?
- Basis in paper: [explicit] The paper discusses the limitations of current evaluation metrics like ROUGE and BERTScore for abstractive summarization and the need for better evaluation methods.
- Why unresolved: The paper identifies the limitations of existing metrics but doesn't propose or evaluate alternative evaluation methods specifically for low-resource languages.
- What evidence would resolve it: Developing and validating new evaluation metrics or adapting existing ones to better capture the quality of abstractive summaries in low-resource languages, potentially through human evaluation studies or new automated metrics.

### Open Question 3
- Question: How can summarization models be made more modular to improve efficiency in low-resource settings?
- Basis in paper: [explicit] The paper mentions the potential for modular approaches to improve model efficiency in low-resource settings.
- Why unresolved: The paper only briefly mentions modular approaches without exploring them in detail or providing specific methods for implementation.
- What evidence would resolve it: Developing and evaluating modular summarization models that can dynamically load necessary components based on the task and language, potentially leading to improved efficiency and performance in low-resource settings.

## Limitations

- Lack of detailed implementation specifications for critical components like intelligent truncation algorithm
- Heavy reliance on automated metrics without extensive human validation beyond 20 samples
- Limited generalizability to other low-resource language families beyond Urdu

## Confidence

**High Confidence**: The efficiency improvement claims for urT5 are well-supported, with specific numerical reductions (44.78% size reduction) and clear methodology for vocabulary reduction. The dataset construction process and basic evaluation metrics are adequately documented.

**Medium Confidence**: The effectiveness of multilingual pre-trained models for low-resource languages is supported by results but lacks comprehensive ablation studies to isolate the contribution of different components. The comparison to English SOTA, while stated, needs more rigorous cross-linguistic validation.

**Low Confidence**: The generalizability of intelligent truncation based on recall to other low-resource languages and summarization domains is speculative without additional empirical validation. The claim that urT5 captures contextual information "effectively" lacks detailed qualitative analysis of summary quality.

## Next Checks

1. **Reproduce urT5 efficiency gains**: Implement the urT5 adaptation procedure with vocabulary reduction from 250k to 40k tokens and measure actual parameter reduction and memory utilization. Verify that the 44.78% size reduction claim holds across different hardware configurations and that performance remains comparable to the original mT5 model.

2. **Validate truncation effectiveness**: Conduct controlled experiments comparing intelligent truncation (recall-based) against random truncation and full-length input on extractive summarization performance. Measure ROUGE scores and human evaluation quality across different truncation methods to confirm that recall-based truncation provides meaningful advantages.

3. **Cross-linguistic generalization test**: Apply the urT5 methodology to another low-resource language (e.g., Bengali or Swahili) with similar dataset construction and evaluate whether the vocabulary reduction approach yields comparable efficiency gains and summarization quality. This would test the broader applicability of the proposed adaptation technique beyond Urdu.