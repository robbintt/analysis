---
ver: rpa2
title: 'BUCA: A Binary Classification Approach to Unsupervised Commonsense Question
  Answering'
arxiv_id: '2305.15932'
source_url: https://arxiv.org/abs/2305.15932
tags:
- commonsense
- question
- atomic
- knowledge
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a binary classification approach to unsupervised
  commonsense question answering. The method converts knowledge graph triples into
  question-answer pairs and trains a model to distinguish reasonable from unreasonable
  answers.
---

# BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering

## Quick Facts
- arXiv ID: 2305.15932
- Source URL: https://arxiv.org/abs/2305.15932
- Reference count: 11
- Key outcome: Proposed binary classification approach outperforms existing unsupervised methods on five commonsense QA benchmarks while using much less training data

## Executive Summary
This paper introduces BUCA, a binary classification framework for unsupervised commonsense question answering that converts knowledge graph triples into question-answer pairs for training. The method addresses the challenge of unsupervised commonsense reasoning by framing it as a binary classification task of distinguishing reasonable from unreasonable answers. By leveraging knowledge graphs like ConceptNet and ATOMIC, BUCA generates synthetic training data and employs contrastive learning to improve discrimination between plausible and implausible answers. The approach achieves state-of-the-art performance on multiple benchmarks including COPA, OpenBookQA, SIQA, CSQA, and SCT, while being significantly more data-efficient than previous methods.

## Method Summary
BUCA converts knowledge graph triples into question-answer pairs using predefined templates, creating synthetic training data that teaches a binary classifier to distinguish reasonable from unreasonable answers. The framework employs a pre-trained language model (RoBERTa-large) with a binary classification head, trained using margin ranking loss and supervised contrastive learning. During inference on downstream tasks, the model scores each candidate answer for reasonableness and selects the highest-scoring option. The approach uses knowledge graphs as its primary source of commonsense knowledge, transforming structured triples into natural language question-answer pairs that capture commonsense relationships.

## Key Results
- Achieves state-of-the-art performance on all five evaluated datasets (COPA, OpenBookQA, SIQA, CSQA, SCT)
- Uses significantly less training data than previous unsupervised approaches (tens of thousands vs hundreds of thousands of examples)
- The RoBERTa-large variant with contrastive learning outperforms the version without it across all datasets
- Outperforms existing unsupervised commonsense reasoning methods while maintaining data efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting knowledge graph triples into question-answer pairs allows the model to learn a binary classification task of distinguishing reasonable from unreasonable answers.
- Mechanism: The approach transforms each KG triple into a question-answer pair using predefined templates, then trains a model to score the reasonableness of these pairs. This creates a large synthetic training set that teaches the model to discriminate between plausible and implausible answers.
- Core assumption: That the quality of converted QA pairs from KGs is sufficient to train a model that generalizes to downstream tasks.
- Evidence anchors:
  - [abstract] "we convert the knowledge graph triples into reasonable and unreasonable texts"
  - [section 3] "each KG triple is converted into question-answer pairs by using pre-defined templates"
  - [corpus] Weak - corpus shows related work on KGQA but doesn't directly confirm the triple-to-QA conversion quality
- Break condition: If the templates generate unnatural or irrelevant QA pairs, the model won't learn useful patterns for distinguishing reasonableness.

### Mechanism 2
- Claim: Contrastive learning improves the model's ability to distinguish reasonable from unreasonable answers by pulling positive pairs together and pushing negative pairs apart.
- Mechanism: The contrastive loss function considers all QA pairs within a batch, treating those with the same label as positive examples and different labels as negative examples, encouraging the model to learn better representations.
- Core assumption: That the contrastive learning approach is effective when the negative examples are randomly sampled and may not be semantically relevant.
- Evidence anchors:
  - [section 3] "we use supervised contrastive learning... to increase the ability to distinguish reasonable from unreasonable ones"
  - [table 4] "RoBERTa-large variant with contrastive learning outperforms the version without it on all datasets"
  - [corpus] Weak - corpus shows contrastive learning is used but doesn't confirm its effectiveness in this specific setup
- Break condition: If negative examples are too easy or too hard, the contrastive learning signal becomes ineffective or misleading.

### Mechanism 3
- Claim: Using a binary classification framework with much less training data than previous approaches achieves competitive or better performance on commonsense QA tasks.
- Mechanism: By framing the problem as binary classification and using only tens of thousands of examples (compared to hundreds of thousands in prior work), the approach achieves state-of-the-art results while being more data-efficient.
- Core assumption: That the binary classification approach with KG-derived data is sufficient for the downstream tasks without requiring massive training sets.
- Evidence anchors:
  - [abstract] "compared with existing UCR approaches using KGs, ours is less data hungry"
  - [table 1] "BUCA achieves the best performance on all datasets"
  - [table 2] "BUCA only needs tens of thousands... while Ma uses 662,909 and 1,197,742"
  - [corpus] Moderate - corpus shows BUCA uses less data but doesn't directly compare effectiveness
- Break condition: If downstream tasks require domain-specific knowledge not captured in the KGs, the approach may underperform despite data efficiency.

## Foundational Learning

- Concept: Knowledge Graphs (KGs)
  - Why needed here: KGs provide structured commonsense knowledge that can be converted into training examples for the binary classifier
  - Quick check question: What are the components of a knowledge graph triple (h, r, t) and how are they used in this approach?

- Concept: Contrastive Learning
  - Why needed here: Contrastive learning helps the model learn better representations by distinguishing between similar and dissimilar examples
  - Quick check question: How does the contrastive loss function work when applied to binary classification of QA pairs?

- Concept: Binary Classification
  - Why needed here: Framing the commonsense reasoning task as binary classification simplifies the problem and allows effective use of synthetic training data
  - Quick check question: What is the difference between the Traditional Binary Loss (TBL) and Margin Ranking Loss used in this framework?

## Architecture Onboarding

- Component map: KG triples -> Template conversion -> Synthetic QA pairs -> Binary classification training with contrastive loss -> Downstream QA task inference
- Critical path: KG triples → Template conversion → Synthetic QA pairs → Binary classification training with contrastive loss → Downstream QA task inference
- Design tradeoffs: Using less training data improves efficiency but may limit coverage of edge cases; contrastive learning improves discrimination but requires careful negative sampling; template-based conversion is simple but may not capture all KG nuances.
- Failure signatures: Poor performance on tasks with domain-specific knowledge not present in the KGs; overfitting to the synthetic training distribution; failure to generalize when downstream answers require multi-hop reasoning.
- First 3 experiments:
  1. Test the template conversion quality by manually inspecting a sample of generated QA pairs from ConceptNet and ATOMIC
  2. Compare model performance with and without contrastive learning on a small subset of downstream tasks
  3. Evaluate the binary classifier's accuracy on distinguishing reasonable vs unreasonable answers using the validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to select negative examples for training the binary classifier in BUCA?
- Basis in paper: [inferred] from the limitations section stating that randomly selecting negative examples might lead to identifying most examples in the evaluation datasets as reasonable
- Why unresolved: The paper does not explore alternative methods for selecting negative examples, such as using more sophisticated sampling strategies or incorporating domain knowledge
- What evidence would resolve it: Experiments comparing different negative sampling strategies and their impact on model performance and generalization

### Open Question 2
- Question: How does the number of candidate answers per question affect the performance of BUCA?
- Basis in paper: [explicit] from the limitations section stating that the paper only uses 2 candidate answers for each question and did not explore using other numbers
- Why unresolved: The paper does not provide any analysis or experiments on how varying the number of candidate answers impacts model performance or training efficiency
- What evidence would resolve it: Experiments varying the number of candidate answers (e.g., 2, 4, 6) and comparing model performance, training time, and memory usage

### Open Question 3
- Question: Can BUCA be effectively extended to other types of commonsense reasoning tasks beyond multiple-choice question answering?
- Basis in paper: [inferred] from the general nature of the binary classification approach and its potential applicability to other tasks
- Why unresolved: The paper only evaluates BUCA on multiple-choice question answering benchmarks and does not explore its performance on other commonsense reasoning tasks such as abductive reasoning or commonsense knowledge base completion
- What evidence would resolve it: Experiments applying BUCA to other commonsense reasoning tasks and comparing its performance to task-specific models or baselines

## Limitations
- The paper's data efficiency claims may be misleading as the comparison baseline also uses knowledge graphs
- The binary classification framework may not capture complex reasoning required for some downstream tasks
- The effectiveness of contrastive learning with randomly sampled negatives lacks theoretical justification

## Confidence

**High confidence**: The binary classification framework and KG conversion methodology are clearly specified and reproducible. The empirical results showing improved performance over baseline methods on the five benchmarks are well-supported.

**Medium confidence**: The claim about superior data efficiency compared to existing UCR approaches needs careful scrutiny of the actual training data requirements and what constitutes "less data hungry." The contrastive learning benefits are shown but the negative sampling strategy's effectiveness is not thoroughly validated.

**Low confidence**: The generalization claims to other commonsense reasoning tasks beyond the five benchmarks tested. The paper doesn't adequately address potential biases in the KG-derived training data or how template-based conversion might miss nuanced commonsense relationships.

## Next Checks

1. **Negative sampling validation**: Conduct an ablation study comparing different negative sampling strategies (random vs. semantically distant vs. near-miss negatives) to verify that the current random approach is optimal for contrastive learning in this binary classification setting.

2. **Template coverage analysis**: Systematically evaluate what percentage of downstream QA pairs can be reasonably generated from the KG templates versus those requiring multi-hop reasoning or knowledge not present in ConceptNet/ATOMIC.

3. **Cross-domain generalization test**: Apply BUCA to a commonsense QA task from a different domain (e.g., medical or technical commonsense) to test whether the KG-derived training generalizes beyond the benchmark datasets used in the paper.