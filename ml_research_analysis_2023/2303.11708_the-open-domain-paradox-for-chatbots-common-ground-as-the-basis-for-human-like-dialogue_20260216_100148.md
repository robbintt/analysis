---
ver: rpa2
title: 'The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like
  Dialogue'
arxiv_id: '2303.11708'
source_url: https://arxiv.org/abs/2303.11708
tags:
- ground
- common
- open-domain
- speech
- chatbots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the "open-domain paradox" in chatbot research,
  highlighting that the pursuit of open-ended conversations actually results in very
  narrow dialogue forms. This occurs because current open-domain chatbots are evaluated
  in settings that lack common ground - the shared knowledge, beliefs, and context
  that enable human-like communication.
---

# The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like Dialogue

## Quick Facts
- **arXiv ID**: 2303.11708
- **Source URL**: https://arxiv.org/abs/2303.11708
- **Reference count**: 12
- **Key outcome**: The pursuit of open-ended conversations in chatbots actually results in narrow dialogue forms because current evaluations lack common ground, producing predominantly small talk rather than diverse speech events.

## Executive Summary
This paper introduces the "open-domain paradox" in chatbot research, arguing that the current approach to creating open-ended conversational agents is fundamentally flawed. Despite the goal of enabling chatbots to discuss anything, current evaluation practices that instruct users to "just chat about anything" consistently produce narrow, small-talk-focused conversations. The authors attribute this paradox to the lack of common ground - shared knowledge, beliefs, and context that humans naturally establish in communication. Drawing on Clark's common ground theory, they demonstrate that true openness in dialogue requires specific contexts and shared understanding rather than generic prompts, challenging fundamental assumptions in chatbot development.

## Method Summary
This conceptual analysis paper examines current chatbot training and evaluation practices through the lens of common ground theory. The authors analyze existing chatbot datasets and human-human dialogue references, applying the Goldsmith and Baxter (1996) taxonomy of 39 speech events to quantify dialogue diversity. They review evaluation settings from major chatbot papers (Meena, Blender, LaMDA) where users received minimal instructions, and compare these with human-human dialogues collected under similar conditions. The analysis identifies the predominance of small talk in current chatbot evaluations and proposes alternative approaches including repeated interactions, scenario-based evaluation, and situated embodiment to enable common ground development.

## Key Results
- Current open-domain chatbot evaluations produce predominantly small talk despite being instructed to discuss anything
- Human-human dialogues show greater diversity of speech events compared to chatbot conversations
- Common ground is essential for meaningful dialogue but is systematically absent in current evaluation settings
- The assumption that removing constraints increases dialogue openness is empirically false

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Common ground is the prerequisite for meaningful dialogue, not the absence of constraints.
- Mechanism: Human communication relies on shared knowledge, beliefs, and understanding of the joint activity. Without this shared context, dialogue becomes superficial and limited to small talk.
- Core assumption: The paper assumes that current chatbot evaluations incorrectly equate "openness" with minimal context, when in fact common ground is essential for diverse dialogue forms.
- Evidence anchors:
  - [abstract]: "Asking users to 'just chat about anything' results in a very narrow form of dialogue, which we refer to as the open-domain paradox"
  - [section 2]: "Clark (1996) describes common ground among humans as 'the sum of their mutual, common or joint knowledge, beliefs and suppositions'"
  - [corpus]: Weak evidence - corpus shows related papers but doesn't directly test common ground theory

### Mechanism 2
- Claim: Current chatbot training data and evaluation settings lack the contextual diversity found in human-human dialogue.
- Mechanism: Training on generic dialogue data and evaluating with minimal instructions produces chatbots that can only handle small talk, missing the range of speech events humans engage in.
- Core assumption: The paper assumes that human-human dialogue naturally covers diverse speech events (small talk, decision-making, negotiation, etc.) which current chatbots cannot replicate.
- Evidence anchors:
  - [section 4]: "Do˘gruöz and Skantze (2021) annotated a subset of the publicly released chats from the Meena chatbot... almost all of them belonged to the Small talk category"
  - [section 3.3]: "For the Meena chatbot, 'Conversations start with 'Hi!' from the chatbot... crowd workers have no expectation or instructions about domain or topic'"
  - [corpus]: Weak evidence - corpus shows related chatbot papers but doesn't address speech event diversity

### Mechanism 3
- Claim: Repeated interactions and specific contexts enable the development of common ground between chatbots and users.
- Mechanism: Allowing multiple interactions over time lets chatbots and users build shared history and understanding, enabling more diverse and meaningful dialogue forms.
- Core assumption: The paper assumes that common ground can be built incrementally through repeated interactions, similar to human relationships.
- Evidence anchors:
  - [section 5.1]: "Xu et al. (2021) collected and modelled long term conversations, where the speakers learn about each other's interests over time"
  - [section 2]: "Personal common ground is based on personal joint experiences with someone"
  - [corpus]: Weak evidence - corpus mentions long-term dialogue but doesn't validate common ground development

## Foundational Learning

- Concept: Common ground theory
  - Why needed here: Understanding this theory is essential to grasp why current chatbot approaches fail and what solutions might work
  - Quick check question: Can you explain why two strangers would struggle to have a meaningful conversation about nuclear physics without any shared background knowledge?

- Concept: Speech event taxonomy
  - Why needed here: Different types of dialogue (small talk, decision-making, negotiation) require different approaches and evaluation methods
  - Quick check question: What are three speech events that would be difficult for a chatbot to handle without explicit context or common ground?

- Concept: Evaluation methodology in dialogue systems
  - Why needed here: Understanding current evaluation approaches helps identify why they fail to capture true dialogue capabilities
  - Quick check question: How might the evaluation of a chatbot differ if we tested it on specific speech events versus asking it to "just chat"?

## Architecture Onboarding

- Component map: User context manager → Dialogue state tracker → Response generation module → Evaluation interface
- Critical path: Context establishment → Dialogue type identification → Appropriate response generation → User feedback incorporation
- Design tradeoffs: Generic vs. specialized chatbots (broad but shallow vs. narrow but deep capabilities)
- Failure signatures: System defaulting to small talk regardless of user input, inability to maintain conversation state, inconsistent responses to repeated questions
- First 3 experiments:
  1. Implement persona-based dialogue with explicit context provided to both user and chatbot
  2. Create scenario-based evaluation with specific speech event instructions
  3. Build a multi-turn dialogue system with memory of previous interactions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do we define "open-domain" in chatbot research when current systems only handle small talk despite being evaluated with generic prompts?
- Basis in paper: [explicit] The paper identifies the "open-domain paradox" where minimal instructions produce narrow dialogue forms rather than diverse speech events
- Why unresolved: Current evaluation methods assume that removing context maximizes openness, but evidence shows this produces limited dialogue
- What evidence would resolve it: Comparative studies showing dialogue diversity when providing specific contexts versus generic prompts, and analysis of whether small talk-only chatbots should be reclassified

### Open Question 2
- Question: Can repeated interactions between chatbots and users create sufficient common ground to enable diverse speech events?
- Basis in paper: [explicit] The authors discuss repeated interactions as a potential solution, citing Xu et al. (2021) who collected long-term conversations
- Why unresolved: Limited analysis exists on whether users would engage with chatbots over multiple sessions without payment, and whether this would naturally lead to diverse speech events
- What evidence would resolve it: Empirical studies tracking user engagement patterns across multiple sessions and analyzing the diversity of speech events that emerge

### Open Question 3
- Question: How can we design evaluation frameworks that accurately measure chatbot performance across diverse speech events rather than just small talk?
- Basis in paper: [explicit] The authors critique current evaluation metrics and propose scenario-based evaluation with specific speech event instructions
- Why unresolved: No standardized evaluation framework exists for testing chatbots on diverse speech events, and role-playing scenarios may not reflect natural dialogue
- What evidence would resolve it: Development and validation of comprehensive evaluation protocols that test chatbots across multiple speech event categories using realistic scenarios

## Limitations
- Relies primarily on secondary analysis of existing datasets rather than controlled experiments
- Speech event taxonomy application lacks detailed methodology and inter-annotator agreement measures
- Human-human dialogue comparison methodology for data collection is not fully specified
- Proposed solutions lack concrete benchmarks or validation studies for practical implementation

## Confidence
- Central argument about open-domain paradox: **High confidence** - well-established theoretical framework
- Empirical validation of small talk predominance: **Medium confidence** - relies on secondary analysis with methodological limitations
- Proposed solutions (repeated interactions, scenarios, simulated worlds): **Low confidence** - conceptual soundness but lack of practical validation

## Next Checks
1. **Controlled Experiment**: Design a study comparing chatbot dialogues under two conditions - minimal instructions ("just chat") versus explicit context/scenarios - using the same participants and chatbots to isolate the effect of common ground on dialogue diversity.

2. **Speech Event Annotation Validation**: Conduct inter-annotator agreement studies on the Goldsmith and Baxter taxonomy applied to chatbot dialogues, with specific focus on ambiguous cases and borderline categories that might explain the predominance of small talk annotations.

3. **Longitudinal Interaction Study**: Implement a repeated interaction protocol with a chatbot, tracking how common ground develops over multiple conversations and measuring changes in dialogue diversity, topic range, and user satisfaction across interaction sessions.