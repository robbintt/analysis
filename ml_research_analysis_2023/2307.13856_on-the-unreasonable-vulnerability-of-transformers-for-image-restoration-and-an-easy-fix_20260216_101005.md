---
ver: rpa2
title: On the unreasonable vulnerability of transformers for image restoration --
  and an easy fix
arxiv_id: '2307.13856'
source_url: https://arxiv.org/abs/2307.13856
tags:
- adversarial
- image
- attack
- network
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the adversarial robustness of Vision Transformer
  (ViT)-based models for image restoration, focusing on the Restormer model and its
  simplified variants (Baseline network and NAFNet). Using Projected Gradient Descent
  (PGD) and CosPGD attacks on the GoPro dataset for image deblurring, the authors
  find that these models are highly vulnerable to adversarial attacks, exhibiting
  severe artifacts like ringing and aliasing.
---

# On the unreasonable vulnerability of transformers for image restoration -- and an easy fix

## Quick Facts
- arXiv ID: 2307.13856
- Source URL: https://arxiv.org/abs/2307.13856
- Reference count: 40
- This work investigates adversarial robustness of Vision Transformer-based image restoration models and proposes non-smooth activations as a simple fix.

## Executive Summary
This paper examines the vulnerability of transformer-based image restoration models (Restormer, Baseline network, NAFNet) to adversarial attacks. Using the GoPro deblurring dataset, the authors demonstrate that these models exhibit severe artifacts under Projected Gradient Descent (PGD) and CosPGD attacks. The study reveals that smooth activation functions like GELU introduce spectral artifacts and aliasing under attack, while non-smooth activations like ReLU significantly improve robustness. Adversarial training with Fast Gradient Sign Method (FGSM) provides substantial robustness gains for Restormer but less improvement for other architectures.

## Method Summary
The authors evaluate three transformer-based image restoration models on the GoPro deblurring dataset, comparing their performance under standard and adversarial training regimes. They employ PGD and CosPGD attacks with ℓ∞ perturbations to generate adversarial examples. The study introduces an intermediate network architecture to isolate design choices, comparing GELU and ReLU activation functions. Models are evaluated using PSNR and SSIM metrics on both clean and adversarial images to assess quality degradation and robustness.

## Key Results
- Vision transformer-based image restoration models are highly vulnerable to adversarial attacks, showing severe ringing and aliasing artifacts
- Non-smooth activation functions (ReLU) significantly improve robustness compared to smooth activations (GELU)
- Adversarial training with FGSM substantially improves Restormer's robustness but provides limited benefits for Baseline and NAFNet models
- The intermediate network with ReLU activation demonstrates the best overall robustness among all tested architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Smooth activation functions (e.g., GELU) in architectural components like the simplified channel attention introduce aliasing and color mixing artifacts under adversarial attack, degrading model robustness.
- **Mechanism**: The smoothing operation during downsampling in these components creates signal aliases that amplify under adversarial perturbations, leading to severe visual distortions.
- **Core assumption**: The mathematical smoothing introduced by activation functions and channel attention mechanisms alters the frequency domain representation of feature maps in a way that is susceptible to adversarial perturbations.
- **Evidence anchors**:
  - [abstract] "Interestingly, the design choices in NAFNet and Baselines, which were based on iid performance, and not on robust generalization, seem to be at odds with the model robustness."
  - [section] "This is because the channel-wise multiplication would best explain the color mixing artifact and the inherent wrong subsampling during this operation and would account for the accentuated aliasing artifacts."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.489, average citations=0.0. Top related titles: Beware of Aliases -- Signal Preservation is Crucial for Robust Image Restoration...

### Mechanism 2
- **Claim**: Non-smooth activation functions (e.g., ReLU) mitigate spectral artifacts and enhance adversarial robustness by preserving signal characteristics during downsampling.
- **Mechanism**: ReLU maintains the integrity of the frequency domain representation during downsampling, reducing the introduction of aliases and thus making the model less susceptible to adversarial perturbations.
- **Core assumption**: The non-smooth nature of ReLU preserves the high-frequency components of the signal better than smooth activations, which are more prone to aliasing when combined with subsampling operations.
- **Evidence anchors**:
  - [abstract] "An intermediate network with ReLU activation demonstrates better robustness, suggesting that non-smooth activations can mitigate spectral artifacts and enhance adversarial robustness."
  - [section] "Interestingly, we observe that Intermediate+ReLU is significantly more robust, and the degradation in its performance with attack strength is significantly lower than all considered networks, including Restormer."
  - [corpus] Weak. Only one related paper directly addresses aliasing in image restoration.

### Mechanism 3
- **Claim**: The self-attention mechanism in Restormer contributes to its superior robustness compared to baseline and NAFNet models, which use channel attention.
- **Mechanism**: Self-attention allows the model to capture long-range dependencies and focus on relevant features, making it more resilient to adversarial perturbations that target local features.
- **Core assumption**: The self-attention mechanism provides a more robust feature representation that is less susceptible to adversarial attacks compared to the local feature aggregation of channel attention.
- **Evidence anchors**:
  - [abstract] "We attempt to improve their robustness through adversarial training. While this yields a significant increase in robustness for Restormer, results on other networks are less promising."
  - [section] "Interestingly, the gain in performance of Restormer when trained with FGSM is significantly better than that of the Baseline network and NAFNet."
  - [corpus] Weak. Only one related paper directly compares robustness of different attention mechanisms.

## Foundational Learning

- **Concept**: Adversarial attacks and defenses in deep learning
  - Why needed here: Understanding how adversarial attacks work and how to defend against them is crucial for evaluating and improving the robustness of image restoration models.
  - Quick check question: What is the difference between PGD and FGSM attacks, and how do they impact the robustness of a model?

- **Concept**: Frequency domain analysis and aliasing
  - Why needed here: Aliasing is a key factor in the spectral artifacts observed in the restored images under adversarial attack. Understanding frequency domain concepts is essential for analyzing and mitigating these artifacts.
  - Quick check question: How does the choice of activation function affect the frequency domain representation of feature maps, and how does this impact the model's robustness to adversarial attacks?

- **Concept**: Attention mechanisms in deep learning
  - Why needed here: The type of attention mechanism (self-attention vs. channel attention) significantly impacts the model's robustness. Understanding the differences and trade-offs between these mechanisms is crucial for designing robust image restoration models.
  - Quick check question: What are the key differences between self-attention and channel attention, and how do these differences impact the model's ability to resist adversarial attacks?

## Architecture Onboarding

- **Component map**: Input image → Encoder → Attention Mechanism → Activation Functions → Decoder → Output image

- **Critical path**: Input image → Encoder → Attention Mechanism → Activation Functions → Decoder → Output image

- **Design tradeoffs**:
  - Model complexity vs. robustness: Simpler models like NAFNet may be less robust than more complex models like Restormer.
  - Smooth vs. non-smooth activation functions: Smooth activations may introduce spectral artifacts, while non-smooth activations may improve robustness but increase model complexity.
  - Self-attention vs. channel attention: Self-attention may provide better robustness but increase computational cost.

- **Failure signatures**:
  - Severe ringing artifacts (Restormer, Baseline): Indicates vulnerability to adversarial attacks.
  - Strong aliasing and color mixing artifacts (NAFNet): Indicates issues with the simplified channel attention and activation functions.
  - Poor performance on clean images (Intermediate): Indicates issues with the architectural design.

- **First 3 experiments**:
  1. Evaluate the performance of the model on clean and adversarial images to identify failure modes.
  2. Replace smooth activation functions with non-smooth activation functions (e.g., ReLU) and evaluate the impact on robustness.
  3. Replace channel attention with self-attention and evaluate the impact on robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the use of non-smooth activation functions like ReLU consistently improve adversarial robustness across different image restoration architectures beyond just the Intermediate + ReLU network studied here?
- Basis in paper: [explicit] The paper shows that Intermediate + ReLU network significantly outperforms Intermediate network with GELU activation under adversarial attacks, suggesting non-smooth activations may help robustness.
- Why unresolved: The study only compared ReLU to GELU in one modified architecture. It's unclear if this benefit extends to other architectures like Restormer or different restoration tasks.
- What evidence would resolve it: Systematic experiments replacing GELU with ReLU in various transformer-based restoration architectures (Restormer, NAFNet, etc.) and measuring their robustness under multiple attack types.

### Open Question 2
- Question: What specific architectural components of the self-attention mechanism in Restormer contribute most to its superior adversarial robustness compared to channel-attention-based networks?
- Basis in paper: [explicit] The paper notes that Restormer uses multi-headed self-attention while Baseline and NAFNet use channel-attention, and attributes part of Restormer's better robustness to this difference.
- Why unresolved: The analysis doesn't decompose which aspects of self-attention (number of heads, attention patterns, etc.) are most critical for robustness.
- What evidence would resolve it: Ablation studies isolating different components of self-attention (number of heads, attention mechanisms, etc.) and comparing their impact on robustness.

### Open Question 3
- Question: Are there alternative defense strategies beyond adversarial training that could provide similar robustness improvements for NAFNet and Baseline network without the computational cost?
- Basis in paper: [inferred] The paper finds adversarial training improves robustness but notes it comes with computational cost and doesn't fully close the performance gap with Restormer.
- Why unresolved: The study only explores adversarial training as a defense mechanism, leaving open whether other techniques could be more effective or efficient.
- What evidence would resolve it: Comparative evaluation of multiple defense strategies (adversarial training, defensive distillation, input transformations, etc.) on the same architectures with identical computational budgets.

## Limitations

- Study focuses exclusively on ℓ∞-bounded attacks and one dataset (GoPro), limiting generalizability
- Intermediate model experiments lack ablation studies to isolate contribution of each architectural change
- The exact mechanisms linking activation functions to adversarial vulnerability are not rigorously proven

## Confidence

- **High Confidence**: The empirical observation that Restormer shows superior robustness compared to baseline models under adversarial attacks
- **Medium Confidence**: The proposed mechanism linking smooth activation functions (GELU) to spectral artifacts and aliasing
- **Low Confidence**: The claim that self-attention inherently provides better robustness than channel attention

## Next Checks

1. Perform spectral analysis of feature maps to directly measure aliasing artifacts introduced by smooth vs. non-smooth activations under adversarial perturbations.

2. Evaluate the same model architectures and training strategies on multiple image restoration datasets (e.g., HIDE, BSD) to test robustness generalization.

3. Create controlled experiments isolating self-attention vs. channel attention while keeping all other architectural components constant to definitively determine which contributes most to robustness.