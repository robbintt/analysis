---
ver: rpa2
title: Answering Unseen Questions With Smaller Language Models Using Rationale Generation
  and Dense Retrieval
arxiv_id: '2308.04711'
source_url: https://arxiv.org/abs/2308.04711
tags:
- answer
- context
- rationale
- training
- contexts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores combining LLM-generated rationales with retrieved
  contexts to improve question answering with smaller language models. The proposed
  methods involve training a rationale ranking model to score and combine retrieved
  and generated contexts, and training reasoning models on retrieval-augmented datasets.
---

# Answering Unseen Questions With Smaller Language Models Using Rationale Generation and Dense Retrieval

## Quick Facts
- arXiv ID: 2308.04711
- Source URL: https://arxiv.org/abs/2308.04711
- Reference count: 33
- Primary result: Two methods combining LLM-generated rationales with retrieved contexts significantly improve accuracy on unseen datasets over strong baselines.

## Executive Summary
This paper addresses the challenge of answering unseen questions using smaller language models by leveraging rationale generation and dense retrieval. The authors propose two complementary approaches: Rationale Ranking (RR), which trains a smaller transformer to score and combine rationales and retrieved contexts for relevance and truthfulness, and Retrieval-Augmented Training with Datasets (RATD), which trains reasoning models on datasets augmented with retrieved contexts. Both methods substantially improve performance on unseen question-answering datasets compared to prior baselines and even larger models.

## Method Summary
The approach involves generating rationales using few-shot prompting with LLMs (BLOOM, StableVicuna) and retrieving relevant contexts from Wikipedia via multi-hop dense retrieval. Two training methods are employed: (1) RR trains a transformer ranker to score contexts for relevance and truthfulness, then combines them using various strategies (naïve concatenation, score-based filtering, hybrid selection); (2) RATD trains reasoning models (BART) on datasets augmented with retrieved contexts to improve performance on noisy, partially evidential contexts. The models are evaluated on unseen datasets using macro-average accuracy and compared against baselines and larger models.

## Key Results
- Both RR and RATD methods significantly improve accuracy over strong prior baselines on unseen datasets.
- RATD training achieves the best overall results and is simpler to implement than RR.
- Combined rationale and retrieved contexts outperform single-source contexts on most datasets.
- The proposed methods also outperform direct prompts against much larger models in few-shot settings.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rationale ranking improves context quality by filtering both irrelevant and false information.
- Mechanism: A smaller transformer model is trained to score contexts on relevance and truthfulness, then used to select or filter components from rationale and retrieved sources.
- Core assumption: A model trained on mixed positive/negative examples can generalize to unseen question-context pairs and distinguish truthful from false statements.
- Evidence anchors:
  - [abstract] "Our Rationale Ranking method (RR) involves training a smaller Transformer to score both rationales and retrieved explanatory contexts with respect to relevance and truthfulness."
  - [section 2.3] "The model is trained on a mixture of existing datasets for which we acquire or construct positive c (i.e. a set of relevant and truthful gold sentences) and negative c (which omit some or all gold sentences and may be irrelevant, false or both)."
  - [corpus] Weak - the cited corpus is focused on question-answering datasets rather than truth detection.
- Break condition: If the training set does not cover diverse falsehood patterns, the model may fail to detect subtle lies or hallucinated content.

### Mechanism 2
- Claim: RATD training enables smaller models to reason effectively over long, noisy contexts containing partial evidence.
- Mechanism: The reasoning model is pretrained on datasets that require multi-hop reasoning and noisy context parsing, then fine-tuned on rationale-like gold contexts.
- Core assumption: Exposure to retrieval-augmented datasets teaches the model to identify and weigh partially evidential facts within longer sequences.
- Evidence anchors:
  - [abstract] "For the second method (RATD) we train a smaller Reasoning model using retrieval-augmented training datasets... such that it becomes proficient at utilising relevant information from longer text sequences that may be only partially evidential and frequently contain many irrelevant sentences."
  - [section 2.4] "The first are the RATD datasets... whose purpose is to confer an ability to reason over long, noisy, and partially evidential contexts."
  - [corpus] Weak - the corpus neighbors discuss compositional generalization but not the specific RATD dataset formulation.
- Break condition: If the training datasets are too clean or do not include enough partial evidence, the model will not learn to filter irrelevant sentences.

### Mechanism 3
- Claim: Combining rationale and retrieved contexts outperforms either source alone on most datasets.
- Mechanism: Different combination strategies (naïve concatenation, score-based filtering, hybrid selection) are applied depending on dataset characteristics; the best performing method is identified via macro-average scoring.
- Core assumption: Different question types benefit from different knowledge source mixes, and a heuristic can identify the optimal combination without explicit type labeling.
- Evidence anchors:
  - [abstract] "Our proposed models also generally outperform direct prompts against much larger models... in both few-shot chain-of-thought and few-shot answer-only settings."
  - [section 3.3] "We identify combination methods satisfying this criteria as those with the highest unweighted macro-average score over our unseen evaluation datasets."
  - [corpus] Moderate - the corpus contains related work on combining knowledge sources, supporting the premise but not the specific empirical result.
- Break condition: If the heuristic fails to generalize, the model may consistently choose suboptimal context combinations.

## Foundational Learning

- Concept: Dense retrieval and embedding similarity
  - Why needed here: The retrieval component depends on converting queries and documents into vector embeddings and measuring inner product similarity to rank relevance.
  - Quick check question: If a query vector has a higher dot product with Document A than Document B, which document is ranked higher by the dense retriever?
- Concept: Rationale generation and chain-of-thought prompting
  - Why needed here: LLM-generated rationales are used as explanatory context; they require structured prompting to produce coherent reasoning chains before answers.
  - Quick check question: In a chain-of-thought prompt, what is the typical structure of the output after the rationale?
- Concept: Transformer-based ranking and scoring
  - Why needed here: The rationale ranker uses a transformer encoder to produce a scalar relevance/truthfulness score for each context.
  - Quick check question: What loss function is used when training the ranker to distinguish positive from negative contexts?

## Architecture Onboarding

- Component map: Question → Retrieval + Rationale Generation → Ranker scoring → Context combination → Reasoning Model → Answer
- Critical path: Question → Retrieval + Rationale Generation → Ranker scoring → Context combination → Reasoning Model → Answer
- Design tradeoffs:
  - Using RATD training gives stronger results but requires more complex dataset preparation.
  - Naïve concatenation is simple but may dilute useful context with noise.
  - The ranker must balance relevance and truthfulness, which can conflict.
- Failure signatures:
  - Low ranker accuracy → poor context filtering, degraded reasoning.
  - Low RATD training performance → inability to parse noisy multi-hop contexts.
  - Overfitting to training datasets → poor generalization to unseen question types.
- First 3 experiments:
  1. Test the rationale ranker on a held-out relevance/truthfulness dataset to confirm scoring accuracy.
  2. Evaluate the reasoning model on a single dataset using only retrieved contexts vs only rationale contexts to identify knowledge source strengths.
  3. Apply the best combination method to all datasets and measure macro-average accuracy to confirm improvement over baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed methods vary across different types of questions (e.g., commonsense vs. factual) when evaluated on a broader range of datasets beyond those tested in this study?
- Basis in paper: Inferred from the discussion on the relative strengths of LLM-generated rationales and retrieved paragraphs for different question types, such as multi-hop factual questions benefiting more from retrieved paragraphs and commonsense questions benefiting more from LLM-generated rationales.
- Why unresolved: The paper evaluates the proposed methods on a limited set of datasets, which may not fully represent the diversity of question types that could be encountered in real-world applications.
- What evidence would resolve it: Conducting experiments on a more diverse set of datasets, including those with different question types and knowledge domains, to assess the generalizability and robustness of the proposed methods.

### Open Question 2
- Question: How does the performance of the proposed methods change when using different LLM models or retrieval systems as knowledge sources, and what factors contribute to these variations?
- Basis in paper: Inferred from the use of different LLM models (BLOOM and StableVicuna) and the Iterator retrieval system, as well as the discussion on the complementary characteristics of these knowledge sources.
- Why unresolved: The paper focuses on specific LLM models and retrieval systems, and it is unclear how the performance would change with different models or systems that may have varying strengths and weaknesses.
- What evidence would resolve it: Conducting experiments using a range of LLM models and retrieval systems to identify the factors that contribute to performance variations and to determine the optimal combinations for different types of questions and knowledge domains.

### Open Question 3
- Question: How can the combination methods be further improved to better leverage the strengths of both LLM-generated rationales and retrieved paragraphs, and what role could additional knowledge sources or model architectures play in this improvement?
- Basis in paper: Inferred from the discussion on the current combination methods (Naïve Concatenation, Max Score, RationaleDefault, and EitherOrBoth) and the potential for future work on more refined combination strategies.
- Why unresolved: The paper presents several combination methods, but it is unclear how these methods could be further refined or augmented with additional knowledge sources or model architectures to achieve even better performance.
- What evidence would resolve it: Developing and testing new combination methods that incorporate additional knowledge sources, such as knowledge graphs or external databases, and exploring the use of advanced model architectures, such as transformer-based models with specialized attention mechanisms, to improve the integration of information from multiple sources.

## Limitations

- The rationale ranker's truthfulness detection may not generalize well if training data doesn't cover diverse falsehood patterns.
- RATD training effectiveness is weakly supported by evidence, as cited corpus doesn't directly validate noisy context reasoning.
- Macro-average scoring for combination method selection may not optimize per-dataset performance.

## Confidence

- High confidence: Retrieval-augmented training (RATD) significantly improves reasoning model performance over strong baselines.
- Medium confidence: Rationale ranking improves context quality by filtering both irrelevant and false information.
- Medium confidence: Combining rationale and retrieved contexts outperforms either source alone on most datasets.

## Next Checks

1. Test the rationale ranker on a held-out dataset with diverse falsehood patterns to verify truthfulness detection beyond the training distribution.
2. Conduct ablation studies isolating the contribution of RATD training by comparing against reasoning models trained only on clean datasets.
3. Perform per-dataset analysis to determine if combination method selection based on macro-average scoring consistently outperforms per-dataset optimization.