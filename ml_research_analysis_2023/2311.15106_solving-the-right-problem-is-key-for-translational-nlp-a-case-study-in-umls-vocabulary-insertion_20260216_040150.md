---
ver: rpa2
title: 'Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS
  Vocabulary Insertion'
arxiv_id: '2311.15106'
source_url: https://arxiv.org/abs/2311.15106
tags:
- umls
- concept
- atom
- which
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights the importance of formulating NLP problems
  that align well with real-world scenarios. Focusing on the real-world task of UMLS
  vocabulary insertion, the authors demonstrate the significance of problem formulation
  by showcasing the differences between the UMLS vocabulary alignment formulation
  and their own UVI formulation.
---

# Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS Vocabulary Insertion

## Quick Facts
- arXiv ID: 2311.15106
- Source URL: https://arxiv.org/abs/2311.15106
- Reference count: 16
- Key outcome: A novel null-aware and rule-enhanced re-ranking model achieves 93.2% accuracy on UMLS vocabulary insertion, outperforming existing approaches by leveraging both rule-based and neural methods while handling new concept detection.

## Executive Summary
This paper addresses the critical importance of problem formulation in translational NLP through the case study of UMLS vocabulary insertion (UVI). The authors identify a significant gap between existing research formulations and the real-world task of inserting new biomedical terms into the UMLS. By reformulating UVI as predicting whether new atoms should be associated with existing concepts or identified as new concepts, they develop a more practical approach that outperforms existing methods. Their proposed null-aware and rule-enhanced re-ranking model achieves state-of-the-art results while demonstrating robustness across UMLS versions and biomedical subdomains.

## Method Summary
The authors reformulate the UMLS vocabulary insertion task as a concept prediction problem rather than binary synonymy prediction. They develop multiple baselines including a rule-based approximation (RBA) system, neural ranking models (LexLM, PubMedBERT, SapBERT), and a strong baseline combining RBA with neural approaches. The proposed method introduces a null-aware and rule-enhanced re-ranking model that integrates rule-based information into a cross-encoder architecture with NULL injection for new concept detection. The model is trained on UMLS 2020AB data and evaluated across five real-world UMLS update datasets.

## Key Results
- The null-aware and rule-enhanced re-ranking model achieves 93.2% overall accuracy on the UVI task
- RBA integration with neural models provides strong baseline performance, with SapBERT-enhanced RBA achieving 92.3% accuracy
- The approach demonstrates superior robustness across UMLS versions and biomedical subdomains compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating the problem to mirror the real-world task significantly improves model performance.
- Mechanism: By formulating UVI as predicting whether a new atom should be associated with an existing concept or identified as a new concept, the model can leverage the full context of the UMLS rather than just binary synonymy prediction.
- Core assumption: The original UV A formulation was too narrow and did not capture the complexity of the real-world task.
- Evidence anchors:
  - [abstract]: "Previous work aimed to develop an automated NLP system to make this time-consuming, costly, and error-prone task more efficient. Nevertheless, practical progress in this direction has been difficult to achieve due to a problem formulation and evaluation gap between research output and the real-world task."
  - [section 2.1]: "Since the UVI task considers all of the UMLS, UV A sampling leads to a significant distribution shift between these tasks."

### Mechanism 2
- Claim: Integrating rule-based information with neural models significantly improves performance.
- Mechanism: By augmenting the RBA system with neural ranking baselines like SapBERT, the model can leverage both the structured rules and the semantic understanding of neural networks.
- Core assumption: The RBA system provides valuable signal that can be combined with neural models to improve performance.
- Evidence anchors:
  - [section 4.5]: "Given that the neural representation baselines discussed above provide a ranking system missing from the RBA, we create a strong baseline by augmenting the RBA system with each neural ranking baseline."
  - [section 5.1]: "Even though a simple baseline which combines the strengths of neural models and the rule-based system obtains surprisingly strong results."

### Mechanism 3
- Claim: A null-aware and rule-enhanced re-ranking model outperforms all other methods.
- Mechanism: By introducing a 'New Concept' placeholder into the candidate list and integrating rule-based information into the re-ranking process, the model can effectively handle the complexity of the UVI task.
- Core assumption: The re-ranking model can learn to leverage the rule-based information and the null injection mechanism to improve performance.
- Evidence anchors:
  - [section 4.6]: "Our candidate re-ranking approach is inspired by some entity linking systems which use two distinct steps: 1) candidate generation, which uses a bi-encoder like the baselines described above, and 2) candidate re-ranking, in which a more computationally expensive model is used to rank the k most similar concepts obtained by the bi-encoder."
  - [section 5.1]: "Finally, we note that RBA enhancement allows the re-ranking module to obtain a 93.2% accuracy due to boosts in existing concept accuracy and new concept precision of almost 10% and 4% respectively."

## Foundational Learning

- Concept: Understanding the UMLS and its role in biomedical interoperability.
  - Why needed here: The UMLS is a crucial resource for biomedical interoperability, and understanding its structure and function is essential for developing effective NLP solutions for the UVI task.
  - Quick check question: What is the primary purpose of the UMLS, and how does it facilitate biomedical interoperability?

- Concept: Familiarity with biomedical language models and their applications.
  - Why needed here: Biomedical language models like PubMedBERT and SapBERT are used as baselines and components in the proposed solution, so understanding their capabilities and limitations is crucial.
  - Quick check question: How do biomedical language models differ from general-purpose language models, and what advantages do they offer for biomedical NLP tasks?

- Concept: Knowledge of rule-based systems and their integration with neural models.
  - Why needed here: The proposed solution leverages rule-based information (RBA) and integrates it with neural models to improve performance, so understanding how to combine these approaches is essential.
  - Quick check question: What are the advantages and disadvantages of rule-based systems compared to neural models, and how can they be effectively combined?

## Architecture Onboarding

- Component map: UMLS concepts K -> Rule-Based Approximation (RBA) -> Neural ranking baselines (LexLM, PubMedBERT, SapBERT) -> Null-aware and rule-enhanced re-ranking model -> Evaluation metrics

- Critical path:
  1. Formulate the UVI task as predicting whether a new atom should be associated with an existing concept or identified as a new concept
  2. Develop baselines using rule-based and neural approaches
  3. Integrate rule-based information with neural models to create strong baselines
  4. Develop a null-aware and rule-enhanced re-ranking model
  5. Evaluate the performance of the proposed approach on the UVI task

- Design tradeoffs:
  - Using a more complex model (re-ranking) vs. simpler baselines (RBA, neural ranking)
  - Integrating rule-based information vs. relying solely on neural models
  - Handling null cases (new concepts) vs. assuming all atoms are linked to existing concepts

- Failure signatures:
  - Poor performance on the UVI task despite strong baselines
  - Inability to effectively integrate rule-based information with neural models
  - Failure to handle null cases (new concepts) correctly

- First 3 experiments:
  1. Evaluate the performance of the RBA system on the UVI task
  2. Develop and evaluate neural ranking baselines (LexLM, PubMedBERT, SapBERT) on the UVI task
  3. Integrate the RBA system with the best neural ranking baseline and evaluate the performance

## Open Questions the Paper Calls Out
- How does the proposed model's performance compare to human annotators on the UMLS vocabulary insertion task?
- How does the proposed model perform on other biomedical knowledge bases besides the UMLS?
- What is the impact of incorporating contextual information from source vocabularies on the proposed model's performance?

## Limitations
- The approach shows medium confidence for underrepresented semantic groups like Genes and Procedures where RBA signals are weak
- 16% of the UMLS vocabulary consists of ambiguous cases with duplicate concepts, representing a fundamental limitation
- The cross-encoder re-ranking approach requires substantially more computational resources than bi-encoder baselines

## Confidence
**High Confidence**: The claim that reformulating the problem to mirror real-world UVI task improves performance is strongly supported by empirical results showing significant accuracy gains (93.2% vs 87.4% for RBA).

**Medium Confidence**: The assertion that integrating RBA with neural models provides substantial performance benefits is supported but has limitations, with RBA alone achieving 87.4% accuracy.

**Medium Confidence**: The claim about superior robustness across UMLS versions is demonstrated but could benefit from testing on additional update cycles beyond the two-year span covered.

## Next Checks
1. Evaluate the trained model on non-UMLS biomedical knowledge bases (e.g., SNOMED CT, MeSH) to assess whether the approach generalizes beyond the UMLS domain.
2. Conduct head-to-head comparison of inference time and memory requirements between the cross-encoder re-ranker and bi-encoder baselines on identical hardware to quantify the computational overhead.
3. Train models on older UMLS versions (2018-2019) and evaluate their performance on newer updates (2023-2024 when available) to assess how well the approach handles concept evolution over longer time periods.