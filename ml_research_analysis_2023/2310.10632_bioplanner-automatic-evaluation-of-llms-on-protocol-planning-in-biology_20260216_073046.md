---
ver: rpa2
title: 'BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology'
arxiv_id: '2310.10632'
source_url: https://arxiv.org/abs/2310.10632
tags:
- protocol
- protocols
- pseudocode
- gpt-4
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present an automated evaluation framework for measuring language
  models' ability to plan scientific protocols, addressing the challenge of evaluating
  protocol accuracy. Our approach converts free-text protocols into pseudocode using
  a protocol-specific set of pseudofunctions, then evaluates models on reconstructing
  this pseudocode from high-level descriptions.
---

# BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology

## Quick Facts
- arXiv ID: 2310.10632
- Source URL: https://arxiv.org/abs/2310.10632
- Reference count: 21
- Primary result: GPT-4 achieves 70.6% accuracy in next-step prediction and 72.2% function precision in full protocol generation

## Executive Summary
This paper introduces BioPlanner, an automated evaluation framework for measuring language models' ability to plan scientific protocols. The core innovation is converting free-text protocols into pseudocode using protocol-specific pseudofunctions, enabling robust automatic evaluation of protocol accuracy. The framework is validated on a manually verified dataset of 100 biology protocols and demonstrates practical utility by successfully generating and executing a biology experiment in a laboratory setting.

## Method Summary
The approach converts natural language protocols into pseudocode representations using GPT-4 with structured prompts and error-checking loops. A "teacher" model generates ground truth pseudocode and pseudofunctions from full protocol text, while a "student" model must reconstruct this plan from limited information (title, description, and admissible functions). The framework evaluates performance through next-step prediction and full protocol generation tasks, using metrics including function accuracy, argument precision/recall, SciBERTscore, and Levenshtein distance.

## Key Results
- GPT-4 achieves 70.6% accuracy in next-step prediction tasks
- GPT-4 achieves 72.2% function precision in full protocol generation
- GPT-4 outperforms GPT-3.5 across all evaluation metrics
- External validation successfully generates and executes a biology experiment in laboratory

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting free-text protocols into pseudocode using a protocol-specific set of pseudofunctions allows for robust automatic evaluation.
- Mechanism: The pseudocode representation breaks down complex protocols into discrete, machine-readable steps that can be compared directly, avoiding the variability inherent in natural language.
- Core assumption: LLMs can reliably generate accurate pseudocode from natural language protocols when provided with structured prompts and error-checking loops.
- Evidence anchors:
  - [abstract] "Our approach converts free-text protocols into pseudocode using a protocol-specific set of pseudofunctions, then evaluates models on reconstructing this pseudocode from high-level descriptions."
  - [section] "We make use of a one-shot example prompt, and an automatic feedback loop... Finally, GPT-4 is prompted to check for errors or omissions in the pseudofunctions and pseudocode."
- Break condition: If the LLM fails to generate accurate pseudocode or if the pseudocode cannot capture all necessary details of the protocol, the evaluation framework breaks down.

### Mechanism 2
- Claim: Using a "teacher" model to generate ground truth pseudocode and pseudofunctions enables robust evaluation of a "student" model's protocol generation abilities.
- Mechanism: The teacher model has privileged information about the protocol, allowing it to create an accurate action space and step-by-step plan. The student model must then reconstruct this plan from limited information (title, description, and admissible functions).
- Core assumption: The teacher model can consistently generate accurate pseudocode and pseudofunctions that capture the essential steps of any given protocol.
- Evidence anchors:
  - [abstract] "To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions."
  - [section] "Having access to this privileged information, we can then evaluate the performance of a 'student', that has to solve the task from scratch."
- Break condition: If the teacher model's pseudocode is inaccurate or incomplete, the student evaluation becomes meaningless as it's comparing against incorrect ground truth.

### Mechanism 3
- Claim: Evaluating protocol generation through next-step prediction and full protocol generation tasks provides comprehensive assessment of LLM planning abilities.
- Mechanism: Next-step prediction tests the model's ability to continue a partially completed protocol, while full protocol generation requires the model to plan the entire execution from scratch. These complementary tasks assess different aspects of planning capability.
- Core assumption: These two evaluation tasks adequately capture the range of planning skills needed for protocol generation, from incremental reasoning to long-term planning.
- Evidence anchors:
  - [section] "Next Step Prediction... We evaluate the correctness of both the predicted function and the function arguments." and "Protocol Generation... the model needs to plan the entire execution of the protocol."
- Break condition: If either task fails to capture important aspects of protocol planning, or if the metrics used (BLEU, SciBERTscore, Levenshtein distance) don't correlate with actual protocol quality, the evaluation becomes inadequate.

## Foundational Learning

- Concept: Protocol planning in biology
  - Why needed here: Understanding the structure and requirements of biological protocols is essential for creating meaningful pseudocode representations and evaluation tasks.
  - Quick check question: What are the key components of a typical biological protocol (e.g., reagents, equipment, step-by-step instructions)?

- Concept: Pseudocode and function-based representations
  - Why needed here: Converting natural language protocols into structured pseudocode is the core innovation that enables automatic evaluation.
  - Quick check question: How does pseudocode differ from natural language in terms of structure and evaluation potential?

- Concept: LLM evaluation metrics
  - Why needed here: Proper evaluation of LLM-generated protocols requires understanding metrics like BLEU score, cosine similarity, and Levenshtein distance.
  - Quick check question: What are the strengths and limitations of using BLEU score versus BERTScore for evaluating scientific protocol generation?

## Architecture Onboarding

- Component map: Protocol → Pseudocode generation → Evaluation tasks → Performance metrics → Model comparison
- Critical path: Protocol → Pseudocode generation → Evaluation tasks → Performance metrics → Model comparison
- Design tradeoffs:
  - Accuracy vs. automation: Manual verification improves accuracy but reduces automation
  - Granularity vs. flexibility: More detailed pseudocode allows better evaluation but may be harder to generate
  - Real-world applicability vs. evaluation simplicity: More complex tasks better reflect real use but are harder to evaluate
- Failure signatures:
  - Low function accuracy but high argument accuracy: Model is choosing wrong functions but understanding arguments
  - High function accuracy but low Levenshtein distance: Model is using correct functions but in wrong order
  - Poor performance on function retrieval: Model struggles to identify relevant steps from existing protocols
- First 3 experiments:
  1. Evaluate GPT-4 on next-step prediction with shuffled functions to test robustness
  2. Compare performance of GPT-3.5 vs GPT-4 on full protocol generation
  3. Test the LLM agent's ability to generate and execute a new protocol using retrieved pseudofunctions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BioPlanner scale with protocol complexity (number of steps, domain specificity)?
- Basis in paper: [inferred] The paper evaluates on a dataset of 100 protocols with varying complexity, but does not systematically analyze performance across complexity levels.
- Why unresolved: The paper provides aggregate statistics but doesn't break down performance by protocol characteristics like step count or domain.
- What evidence would resolve it: Performance metrics (accuracy, precision, recall) for protocols grouped by step count ranges or biological domains would reveal scaling behavior.

### Open Question 2
- Question: How do open-source models like Llama-2 compare to proprietary models like GPT-4 for protocol planning tasks?
- Basis in paper: [explicit] The paper mentions evaluating Llama-2-7B as a comparison but only briefly reports results in supplementary materials.
- Why unresolved: The paper primarily focuses on GPT models and only provides limited Llama-2 performance data without detailed analysis.
- What evidence would resolve it: Comprehensive evaluation of multiple open-source models across all tasks with statistical significance testing would establish their relative performance.

### Open Question 3
- Question: How robust is the pseudocode conversion process to variations in protocol writing styles and formats?
- Basis in paper: [inferred] The paper uses manual verification but doesn't test the system's ability to handle diverse protocol styles or noisy inputs.
- Why unresolved: The paper doesn't explore how the pseudocode generation handles protocols with varying levels of detail, terminology, or formatting.
- What evidence would resolve it: Testing the pseudocode conversion on protocols with controlled variations in writing style, terminology, and formatting would reveal robustness limitations.

### Open Question 4
- Question: What is the impact of pseudofunction granularity on model performance and protocol accuracy?
- Basis in paper: [inferred] The paper generates pseudofunctions but doesn't systematically analyze how their level of detail affects downstream performance.
- Why unresolved: The paper doesn't explore whether more granular or more abstract pseudofunctions lead to better model performance or protocol accuracy.
- What evidence would resolve it: Experiments varying pseudofunction granularity levels and measuring their impact on model performance metrics would clarify optimal granularity.

## Limitations

- Dependence on GPT-4 as both pseudocode generator and evaluator may limit generalizability to other domains or language models
- The 100-protocol dataset, while manually verified, may not capture the full diversity of biological protocols
- Effectiveness depends heavily on the quality and comprehensiveness of pseudocode representation - important details may be lost in translation

## Confidence

- **High Confidence**: The methodology for converting protocols to pseudocode and the overall evaluation framework design. The manual verification process and successful external validation in a laboratory setting provide strong evidence for the approach's validity.
- **Medium Confidence**: The performance metrics and their ability to capture true protocol quality. While standard metrics like BLEU and Levenshtein distance are used, their correlation with actual protocol usability remains to be fully established.
- **Low Confidence**: The generalizability of results to other domains beyond biology and to different LLMs beyond the GPT family. The framework's reliance on GPT-4 for both generation and evaluation creates potential biases that haven't been thoroughly explored.

## Next Checks

1. **Cross-Domain Validation**: Test the framework on protocols from other scientific domains (e.g., chemistry, physics) to assess generalizability. This would involve creating new pseudocode representations and evaluating whether the same evaluation methodology applies.

2. **Open-Source Model Comparison**: Evaluate open-source LLMs (e.g., Llama-2, Mistral) on the same tasks to determine if the results are specific to proprietary models or represent a broader capability trend in LLMs.

3. **Longitudinal Protocol Generation**: Extend the external validation to include a complete experiment cycle from protocol generation through execution and result analysis. This would test whether generated protocols are not just syntactically correct but also practically useful in real research settings.