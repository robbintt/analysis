---
ver: rpa2
title: 'SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual
  Automatic Speech Recognition'
arxiv_id: '2309.16937'
source_url: https://arxiv.org/abs/2309.16937
tags:
- layers
- layer
- information
- multilingual
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving multilingual automatic
  speech recognition (ASR) by leveraging the hierarchical representations of self-supervised
  learning (SSL) models. The authors propose SSHR, a method that extracts language-related
  information from the middle layers of the SSL model and uses it to guide specific
  language content extraction in subsequent layers.
---

# SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2309.16937
- Source URL: https://arxiv.org/abs/2309.16937
- Reference count: 0
- Key outcome: Achieves state-of-the-art performance with 9.4% and 12.6% improvements on Common Voice and ML-SUPERB multilingual ASR datasets

## Executive Summary
This paper addresses the challenge of improving multilingual automatic speech recognition (ASR) by leveraging hierarchical representations from self-supervised learning (SSL) models. The authors propose SSHR, a method that extracts language-related information from middle layers of SSL models and uses it to guide specific language content extraction in subsequent layers. SSHR introduces Cross-CTC to enhance content-related information in final layers and is evaluated on two multilingual datasets, achieving state-of-the-art performance.

## Method Summary
SSHR fine-tunes the MMS 300M model for multilingual ASR by extracting language-related frames from layer 8 through average pooling, concatenating them with speech frames, and applying self-attention mechanisms. The method removes or replaces final layers that have lost content-related information and uses Cross-CTC to inject information back into remaining layers through cross-attention between intermediate CTC outputs and subsequent layers. The approach is evaluated on Common Voice (8 languages, phonemes) and ML-SUPERB (143 languages, graphemes) datasets.

## Key Results
- Achieves 9.4% improvement in Phoneme Error Rate (PER) on Common Voice dataset compared to baseline
- Achieves 12.6% improvement in Word Error Rate (WER) and Character Error Rate (CER) on ML-SUPERB dataset
- State-of-the-art performance on both multilingual ASR benchmarks
- Demonstrates effectiveness of leveraging hierarchical representations from SSL models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Middle layers of SSL models contain more language-related information that can be extracted and used to guide downstream ASR tasks
- Mechanism: Average pooling on middle layer representations creates a language-related frame, concatenated with speech frames and reinforced through self-attention
- Core assumption: Language information is concentrated in middle layers (8-12) and can be effectively extracted through pooling
- Evidence anchors: Abstract states middle layers capture language-related information; paper describes average-pooling operation on layer outputs
- Break condition: If language information isn't concentrated in middle layers or pooling destroys too much information

### Mechanism 2
- Claim: Content-related information diminishes in final layers of SSL models, reducing ASR performance
- Mechanism: Removes/replaces final layers that have lost content information and uses Cross-CTC to inject information back through cross-attention
- Core assumption: Final layers naturally lose content-related information during pre-training, making them suboptimal for ASR
- Evidence anchors: Abstract mentions content information diminishes in final layers; paper explores deleting/replacing final layers
- Break condition: If content information doesn't actually diminish in final layers or replacement creates harmful interference

### Mechanism 3
- Claim: Cross-attention between intermediate layers' CTC outputs and subsequent layers enhances content-related information in final layers
- Mechanism: Uses cross-attention where posterior probabilities from content-rich intermediate layers serve as query vectors for subsequent layers' key-value pairs
- Core assumption: Content information from intermediate layers can be effectively transferred to final layers through cross-attention
- Evidence anchors: Paper describes using layer outputs as K and V, with linear-transformed posterior probabilities as Q
- Break condition: If cross-attention doesn't effectively transfer information or transformation introduces noise

## Foundational Learning

- Concept: Layer-wise representation analysis in SSL models
  - Why needed here: Understanding which layers contain which types of information is fundamental to SSHR's approach
  - Quick check question: How would you design an experiment to determine which layers contain the most language-related versus content-related information?

- Concept: Cross-attention mechanisms in transformer architectures
  - Why needed here: SSHR uses cross-attention to transfer information between layers, requiring understanding of Q, K, V matrix interactions
  - Quick check question: What would happen if you swapped the roles of Q and K in the Cross-CTC mechanism?

- Concept: Connectionist Temporal Classification (CTC) loss and its applications
  - Why needed here: SSHR uses CTC both for standard ASR and as a source of posterior probabilities for the Cross-CTC mechanism
  - Quick check question: How does CTC handle alignment between variable-length inputs and outputs, and why is this useful for SSHR?

## Architecture Onboarding

- Component map: Input → Encoder layers → Language frame extraction (layer 8) → Cross-CTC integration (layers 18, 20, 22) → Final CTC prediction

- Critical path: Speech input → MMS encoder → Average pooling on layer 8 → Concatenate with speech frames → Self-attention → Cross-CTC on layers 18, 20, 22 → Final CTC prediction

- Design tradeoffs:
  - Layer selection: Choosing layer 8 for language info vs other middle layers
  - Layer deletion vs replacement for final layers
  - Number of Cross-CTC layers (3 vs more/less)
  - Loss weighting between primary CTC and Cross-CTC

- Failure signatures:
  - Degradation in language identification accuracy
  - Increased CER/WER despite Cross-CTC
  - Training instability or convergence issues
  - Overfitting to specific languages while harming others

- First 3 experiments:
  1. Verify language-related information concentration: Train simple classifiers on each MMS layer to identify which layers (8-12) have highest language identification accuracy
  2. Test language frame effectiveness: Compare performance with and without language frame extraction at different layer positions
  3. Validate Cross-CTC impact: Implement Cross-CTC on a single intermediate layer and measure changes in content-related information (MI with phoneme labels)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SSHR's performance vary with different amounts of labeled data across languages?
- Basis in paper: The paper focuses on low-resource settings but does not test SSHR's performance scaling with increased labeled data availability
- Why unresolved: The paper only evaluates SSHR on low-resource multilingual datasets
- What evidence would resolve it: Experiments testing SSHR's performance on high-resource multilingual datasets and analyzing its effectiveness compared to baseline models as labeled data increases

### Open Question 2
- Question: What is the optimal layer selection strategy for Cross-CTC in SSHR?
- Basis in paper: The paper mentions that the choice of multiple layers plays a crucial role in Cross-CTC performance, but does not provide a systematic method for layer selection
- Why unresolved: The paper uses empirical layer selection (18, 20, 22) without a principled approach or theoretical justification
- What evidence would resolve it: A systematic analysis of Cross-CTC performance across different layer combinations and development of a principled layer selection strategy

### Open Question 3
- Question: How does SSHR compare to other methods of leveraging hierarchical representations in SSL models for multilingual ASR?
- Basis in paper: The paper claims SSHR achieves state-of-the-art performance but does not compare it to other recent methods that leverage SSL hierarchical representations
- Why unresolved: The paper focuses on comparing SSHR to baseline models rather than directly comparing it to other recent methods
- What evidence would resolve it: Direct comparison of SSHR with other recent methods (e.g., HierLID, GIC) on the same datasets and evaluation metrics

## Limitations
- Layer selection sensitivity: The paper assumes optimal performance from specific layer choices (8 for language, 18/20/22 for Cross-CTC) without sensitivity analysis
- Cross-CTC implementation details: Exact implementation of how posterior probabilities are transformed and applied in cross-attention is underspecified
- Training stability claims: The paper doesn't adequately address potential training instability when combining multiple CTC losses with different weights

## Confidence
- High Confidence: The overall framework of using SSL representations for multilingual ASR is well-established
- Medium Confidence: Specific claims about language information concentration in middle layers and content information loss in final layers are plausible but not rigorously validated
- Low Confidence: The Cross-CTC mechanism's effectiveness is particularly uncertain given lack of implementation details and absence of ablation studies

## Next Checks
1. **Layer Information Content Analysis**: Train simple linear classifiers on representations from layers 1-24 of the pre-trained MMS model to empirically verify which layers contain the most language identification information versus phoneme content information

2. **Cross-CTC Ablation Study**: Implement SSHR with Cross-CTC disabled on individual layers (18, 20, 22) and in combination to measure the marginal contribution of each cross-attention instance

3. **Language Frame Position Sensitivity**: Test SSHR performance when the language-related frame is extracted from different middle layers (e.g., layers 6, 8, 10, 12) and when it's applied at different positions in the encoder stack