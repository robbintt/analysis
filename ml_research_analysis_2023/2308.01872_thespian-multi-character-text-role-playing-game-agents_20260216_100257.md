---
ver: rpa2
title: 'Thespian: Multi-Character Text Role-Playing Game Agents'
arxiv_id: '2308.01872'
source_url: https://arxiv.org/abs/2308.01872
tags:
- agent
- character
- thespian
- attention
- characters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a new actor-critic agent architecture called
  thespian that can emulate multiple characters in text-based role-playing games.
  The core idea is to learn separate soft prompts and action distributions for each
  character, allowing the agent to switch between characters at runtime.
---

# Thespian: Multi-Character Text Role-Playing Game Agents

## Quick Facts
- arXiv ID: 2308.01872
- Source URL: https://arxiv.org/abs/2308.01872
- Reference count: 4
- Primary result: Spian agents can learn two characters (thief and adventurer) as well as separate agents trained on each, and can learn a third blended character (rogue) 10x faster than baselines without degrading performance on the original characters.

## Executive Summary
Thespian introduces a novel actor-critic architecture for learning multiple characters in text-based role-playing games. The key innovation is maintaining separate soft prompts and action distributions for each character, allowing the agent to switch between characters at runtime. The authors demonstrate that thespian agents can learn two distinct characters as well as separate agents trained individually, and can learn new blended characters through an attention mechanism that combines existing character behaviors in a few-shot manner.

## Method Summary
Thespian agent architecture extends the KG-A2C framework with separate soft prompts for each character and stacked action logits. During training, the agent rotates through character-specific rewards and prompts to prevent interference. For few-shot learning of new characters, an attention module blends the action logits from pre-trained characters based on observation components. The approach enables a single agent to emulate multiple characters and learn new blended characters without retraining the core model.

## Key Results
- Thespian agents match performance of separate agents trained on individual characters
- 10x faster learning of new blended characters compared to unfrozen baselines
- No performance degradation on original characters when learning new blended characters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Thespian agent can learn to emulate multiple characters by using separate soft prompts and action distributions for each character.
- Mechanism: The agent maintains learnable soft prompts that steer internal state representations toward character-specific behavior, while producing stacked action logits—one set per character—allowing independent probability distributions for each character.
- Core assumption: Prompts and action logits are sufficiently disentangled so optimizing for one character doesn't corrupt others.
- Evidence anchors: [abstract] "We present a framework we call a thespian agent that can learn to emulate multiple characters along with a soft prompt that can be used to direct it as to which character to play at any time." [section] "The agent also learns a soft prompt that can later be provided as a cue to emulate a specific character... si = W T i × cat(o, pi)"
- Break condition: If prompts and action logits are not sufficiently separated, training on one character could degrade performance on others.

### Mechanism 2
- Claim: The attention mechanism enables few-shot learning of new blended characters by combining existing character action logits.
- Mechanism: After freezing the pre-trained thespian agent, an attention module receives observations and stacked action logits from all characters, computes attention scores, and produces a weighted average of logits to generate the action distribution for the new character.
- Core assumption: Action logits from pre-trained characters contain sufficient information about behavioral patterns, and attention weights can learn effective blending strategies.
- Evidence anchors: [abstract] "We further describe an attention mechanism that allows the agent to learn new characters that are based on previously learned characters in a few-shot fashion." [section] "We apply an attention mechanism across the action logit scores for each character... learns how to blend the raw logit scores for each character to produce a single final action probability distribution."
- Break condition: If the attention module cannot disentangle useful signals from logits or if pre-trained logits are too entangled, the new character may fail to learn meaningful behavior.

### Mechanism 3
- Claim: Rotating character rewards during training prevents interference between characters and enables simultaneous learning.
- Mechanism: During each game, the agent is provided with a different character-specific reward function and corresponding soft prompt. After two games, the agent switches to the next character, ensuring gradients from one character's reward don't dominate.
- Core assumption: The environment provides distinct, non-overlapping reward signals for each character, and the model's capacity is sufficient to represent all characters simultaneously.
- Evidence anchors: [section] "The character, corresponding character reward function, and character prompt are rotated each game to balance the training of multiple characters." [section] "We train on one character reward, accompanied by the character prompt, for two games, then switch to the next character reward and character prompt for two more games."
- Break condition: If reward signals are too similar or model capacity is exceeded, rotation may not prevent interference, leading to degraded performance.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: Text-based role-playing games are modeled as POMDPs where the agent only observes text descriptions, not the full world state, requiring reasoning over sequences of observations.
  - Quick check question: In a POMDP, what does the agent receive as input instead of the full ground truth state?

- Concept: Advantage-Actor Critic (A2C) Architecture
  - Why needed here: The base KG-A2C agent uses A2C, which separates policy (actor) and value (critic) heads; thespian extends this to produce separate heads per character.
  - Quick check question: In A2C, what are the two outputs of the network used for during training?

- Concept: Soft Prompts in Language Models
  - Why needed here: Soft prompts are learned embeddings that condition the model's behavior without requiring natural language text, enabling character-specific prompting.
  - Quick check question: How does a soft prompt differ from a regular text prompt in terms of representation?

## Architecture Onboarding

- Component map:
  Observation encoder (GRU + graph attention for KG-A2C) -> Soft character prompts (learnable embeddings) -> Actor head (produces stacked logits per character) -> Critic head (produces stacked value estimates per character) -> Thespian attention module (for few-shot new characters) -> Action template system (verb + object selection)

- Critical path:
  1. Encode observation → update knowledge graph
  2. Concatenate observation embedding with active character prompt
  3. Generate stacked action logits and values for all characters
  4. Apply softmax to select action distribution for active character
  5. Execute action and receive reward
  6. Compute A2C loss for active character and backpropagate
  7. For few-shot: freeze core, train attention module to blend logits

- Design tradeoffs:
  - Separate prompts vs. shared prompt: separate prompts allow more expressive character identities but increase parameter count.
  - Stacked logits vs. single logit with conditioning: stacked logits make multi-character reasoning explicit but increase computation.
  - Attention over logits vs. over probabilities: operating on logits preserves raw biases and avoids smoothing.

- Failure signatures:
  - All characters perform similarly: prompts and action logits not disentangled.
  - New character fails to learn: attention module cannot extract useful patterns or logits are too entangled.
  - Performance degrades on original characters: attention module or training interferes with frozen weights.

- First 3 experiments:
  1. Train thespian agent on two characters (thief, adventurer) and verify it matches single-character baselines when prompted.
  2. Train thespian agent on both characters simultaneously and measure cross-character action attempts.
  3. Freeze thespian agent, train attention module on new blended character, compare convergence speed to unfrozen baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the thespian agent learn to blend characters that are more dissimilar than thief and adventurer, such as thief and knight?
- Basis in paper: [inferred] The paper shows the thespian agent can learn a new "rogue" character that blends thief and adventurer. However, the paper does not test blending characters that are more dissimilar.
- Why unresolved: The paper only tests blending thief and adventurer, which are relatively similar characters. It is unclear if the approach would work for more dissimilar characters.
- What evidence would resolve it: Testing the thespian agent's ability to blend more dissimilar characters like thief and knight. Measuring the performance of the blended character compared to separate agents trained on each character.

### Open Question 2
- Question: How does the thespian agent's performance scale with the number of characters it is trained to emulate? Is there a limit to the number of characters?
- Basis in paper: [inferred] The paper only trains the thespian agent on two characters (thief and adventurer) and then learns a third blended character. It does not explore scaling to more characters.
- Why unresolved: The paper does not provide any analysis of how the thespian agent's performance changes as more characters are added. There could be a limit to the number of characters the agent can effectively learn.
- What evidence would resolve it: Training the thespian agent on more than three characters and measuring the performance on each character as more are added. Identifying if there is a point where performance degrades significantly.

### Open Question 3
- Question: How does the thespian agent's performance compare to training separate agents for each character when the number of characters is large?
- Basis in paper: [explicit] The paper shows that for two characters, the thespian agent performs as well as separate agents. However, it does not compare performance for a larger number of characters.
- Why unresolved: The paper only provides a comparison for two characters. It is unclear if the thespian agent's advantage over separate agents persists as the number of characters increases.
- What evidence would resolve it: Comparing the thespian agent's performance to separate agents for a large number of characters (e.g. 10 or more). Measuring the training time and performance of each approach.

## Limitations

- The approach's effectiveness for blending characters that are more dissimilar than thief and adventurer remains untested.
- The paper does not explore scaling to more than three characters, leaving open questions about practical limits.
- Performance comparisons to separate agents are limited to two-character scenarios, not tested for larger character sets.

## Confidence

**High confidence** in the technical implementation details and architectural description. The paper provides clear pseudocode and implementation details for the thespian agent, including the soft prompt mechanism and attention-based few-shot learning. The architectural components are well-specified and reproducible.

**Medium confidence** in the multi-character learning claims. While the experiments show that thespian can learn two characters as well as separate agents, the evaluation is limited to a specific game environment and character pair. The paper does not test robustness across different character types or environments.

**Medium confidence** in the few-shot learning claims. The attention mechanism shows 10x faster learning for the blended character, but this is demonstrated on a single example with limited validation. The mechanism's generalization to other character combinations or more complex blending scenarios remains untested.

## Next Checks

1. **Character interference test**: Systematically vary the similarity between character reward functions and prompts, then measure performance degradation in joint training to establish bounds on the method's robustness to character similarity.

2. **Scaling test**: Train thespian agents on increasing numbers of characters (3, 5, 10) to determine the practical limits of the approach and identify when interference becomes problematic or when model capacity is exceeded.

3. **Cross-environment generalization**: Evaluate the attention-based few-shot learning mechanism on character blends from entirely different game environments to test whether the blending capability transfers beyond the training domain.