---
ver: rpa2
title: 'Sketchy: Memory-efficient Adaptive Regularization with Frequent Directions'
arxiv_id: '2302.03764'
source_url: https://arxiv.org/abs/2302.03764
tags:
- shampoo
- matrix
- learning
- training
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Sketchy, a memory-efficient adaptive regularization
  method that uses the Frequent Directions (FD) sketch to maintain a low-rank approximation
  of the gradient covariance matrix in deep learning. The key idea is that the spectra
  of the Kronecker-factored gradient covariance matrix are concentrated on a small
  leading eigenspace that changes throughout training, motivating a low-rank sketching
  approach.
---

# Sketchy: Memory-efficient Adaptive Regularization with Frequent Directions

## Quick Facts
- arXiv ID: 2302.03764
- Source URL: https://arxiv.org/abs/2302.03764
- Reference count: 40
- The paper proposes Sketchy, a memory-efficient adaptive regularization method that uses the Frequent Directions (FD) sketch to maintain a low-rank approximation of the gradient covariance matrix in deep learning.

## Executive Summary
This paper introduces Sketchy, a memory-efficient adaptive regularization method for deep learning optimization. The key insight is that gradient covariance matrices in deep learning have concentrated spectra that change throughout training, making them amenable to low-rank sketching via Frequent Directions. Sketchy achieves full-matrix AdaGrad regret bounds while using only dk memory instead of O(d²). The method extends to Shampoo, producing a variant competitive with full Shampoo and Adam while requiring sublinear memory for tracking second moments.

## Method Summary
Sketchy maintains a low-rank approximation of gradient covariance matrices using the Frequent Directions sketching algorithm. For each weight matrix, it tracks the top k eigenvalues and eigenvectors of the gradient covariance, adding the escaped mass to diagonal regularization. This enables O(√T) regret bounds for AdaGrad-style updates while using only dk memory. The method extends to Shampoo by applying FD sketching separately to left and right Kronecker factors of the gradient covariance. Experiments validate the approach on ImageNet classification, Librispeech speech recognition, and ogbg-molpcba molecular property prediction tasks.

## Key Results
- Sketchy matches full-matrix AdaGrad regret up to additive error in bottom eigenvalues using only dk memory
- Sketchy-Shampoo achieves competitive performance with full Shampoo and Adam while requiring sublinear memory
- Fast spectral decay observed in Kronecker-factored gradient covariance matrices across architectures
- On ImageNet, Sketchy-Shampoo achieves 23.0% top-1 error vs 22.7% for full Shampoo and 23.5% for Adam

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sketchy recovers full-matrix AdaGrad regret up to additive error in the bottom eigenvalues using only dk memory.
- **Mechanism**: By maintaining a low-rank approximation of the gradient covariance matrix via Frequent Directions sketching, Sketchy captures the dominant eigenspace while only tracking the residual mass (ρt) in the discarded subspace.
- **Core assumption**: The spectrum of the Kronecker-factored gradient covariance matrix is concentrated on a small leading eigenspace that changes throughout training.
- **Evidence anchors**:
  - [abstract]: "spectra of the Kronecker-factored gradient covariance matrix in deep learning (DL) training tasks are concentrated on a small leading eigenspace"
  - [section 5.1]: "spectral investigation into the Kronecker-factored gradient covariance matrix reveals a concentrated, but changing, spectrum"
  - [corpus]: Weak - related work mentions matrix sketching but doesn't specifically validate spectral decay in DL gradient covariance
- **Break condition**: If the gradient covariance spectrum becomes flat (uniform eigenvalues), the additive error term dominates and regret degrades to O(√T).

### Mechanism 2
- **Claim**: Composing Frequent Directions with Shampoo achieves competitive quality with full Shampoo while requiring sublinear memory.
- **Mechanism**: Sketchy maintains low-rank approximations of both left and right Kronecker factors separately, then applies preconditioning as if they were full-rank matrices.
- **Core assumption**: The low-rank structure of gradient covariance factors Lt and Rt can be efficiently tracked with FD while preserving preconditioning effectiveness.
- **Evidence anchors**:
  - [abstract]: "show extensions of our work to Shampoo, resulting in a method competitive in quality with Shampoo and Adam, yet requiring only sublinear memory"
  - [section 4.2]: Formal regret bound showing Sketchy Shampoo matches full Shampoo quality up to spectral error terms
  - [corpus]: Missing - no direct comparison of Shampoo with sketching in related work
- **Break condition**: If the rank k required to capture sufficient spectral mass becomes too large relative to min(m,n), memory savings diminish.

### Mechanism 3
- **Claim**: Dynamic diagonal regularization in Sketchy AdaGrad enables O(√T) regret even with sketching.
- **Mechanism**: By adding the escaped mass ρt to the diagonal of the preconditioner at each step, Sketchy maintains positive definiteness and enables proper gradient scaling.
- **Core assumption**: The accumulated error from sketching (ρ1:t) can be bounded by the sum of the bottom eigenvalues of the gradient covariance.
- **Evidence anchors**:
  - [section 4.1]: Lemma 2 proves ρ1:T ≤ ∑i=ℓ^d λi(GT), establishing the key bound
  - [section 4.1]: Theorem 3 shows regret bound of the form tr(G^1/2_T) + d√ρ1:T, achieving O(√T) overall
  - [corpus]: Weak - related work Ada-FD lacks dynamic regularization and suffers Ω(T^3/4) regret
- **Break condition**: If the lower eigenspace accumulates significant mass relative to the trace, the additive error term overwhelms the main term.

## Foundational Learning

- **Concept**: Online Convex Optimization (OCO) regret framework
  - Why needed here: Sketchy's theoretical guarantees are stated in terms of OCO regret, which translates to convergence in offline optimization via online-to-batch conversion
  - Quick check question: What is the difference between full-matrix AdaGrad regret and diagonal AdaGrad regret in terms of dependence on the gradient covariance matrix?

- **Concept**: Frequent Directions sketching algorithm
  - Why needed here: Sketchy's core mechanism relies on FD to maintain a low-rank approximation of the gradient covariance matrix efficiently
  - Quick check question: What property of the FD algorithm ensures that the sum of escaped eigenvalues ρ1:T can be bounded by the bottom eigenvalues of the true covariance?

- **Concept**: Kronecker-factored preconditioning
  - Why needed here: Shampoo and Sketchy Shampoo both exploit the Kronecker structure of gradient covariance to reduce memory from O(d²) to O(m² + n²) for m×n weight matrices
  - Quick check question: How does the preconditioning operation (L⊗R)vec(W) = vec(LWR) work, and why does it correspond to left and right multiplication?

## Architecture Onboarding

- **Component map**: Gradient -> FD Sketch Update -> Dynamic Regularization -> Preconditioner Computation -> Parameter Update
- **Critical path**:
  1. Receive gradient G
  2. Update FD sketch for each covariance factor
  3. Add escaped mass ρ to diagonal regularization
  4. Compute preconditioner inverse square root
  5. Apply preconditioned update

- **Design tradeoffs**:
  - Memory vs. quality: Higher rank ℓ captures more spectral mass but increases memory
  - Speed vs. accuracy: FD update via SVD vs. iterative methods like LOBPCG
  - Freshness vs. stability: Update preconditioner every step vs. periodically

- **Failure signatures**:
  - Vanishing gradients: ρt becomes large relative to trace, indicating poor spectral approximation
  - Training instability: Insufficient diagonal regularization when sketching error dominates
  - Memory blowup: Rank ℓ too large for available memory, defeating purpose of sketching

- **First 3 experiments**:
  1. Spectral decay analysis: Run Shampoo on small model, extract covariance factors, plot eigenvalue distribution to verify fast decay
  2. Ablation study: Compare Sketchy vs. full Shampoo on ImageNet with varying rank ℓ to find memory-quality Pareto frontier
  3. Step-skipping test: Implement and measure regret degradation when preconditioner updates occur every k steps rather than every step

## Open Questions the Paper Calls Out

- **Question**: How does the rank k of the FD sketch affect the final generalization performance of Sketchy in different architectures and datasets?
  - Basis in paper: [explicit] The paper mentions that "we do not tune" the rank k parameter and recommends "setting it to be as large as memory allows in practice," but also shows in Figure 5 that increasing rank improves optimization on the ogbg-molpcba dataset.
  - Why unresolved: The paper does not systematically explore the trade-off between rank k and final generalization performance across different architectures and datasets. The experiments only show one fixed rank (256) for most cases and limited exploration for the GNN dataset.
  - What evidence would resolve it: A comprehensive study varying k across multiple architectures (ResNet, Conformer, GNN) and datasets (ImageNet, Librispeech, ogbg-molpcba) showing how final test accuracy/validation performance changes with different k values.

- **Question**: Can the step-skipping approach for Shampoo preconditioner updates be theoretically justified to maintain the same regret bounds as full-step updates?
  - Basis in paper: [explicit] The paper mentions implementing Shampoo with "preconditioners were updated every 10 steps instead of every step for speed" and provides Appendix F discussing step-skipping theory.
  - Why unresolved: While Appendix F provides theoretical analysis for AdaGrad with step-skipping, it does not extend this analysis to Shampoo. The paper only mentions this as a practical implementation choice without rigorous justification.
  - What evidence would resolve it: A theoretical analysis extending the step-skipping regret bounds from AdaGrad to Shampoo, showing that the additional error terms do not significantly degrade the overall regret guarantee.

- **Question**: What is the optimal block size for Blocked Shampoo when applied to rectangular weight matrices in transformer architectures?
  - Basis in paper: [explicit] The paper mentions that "the largest dimension for any parameter from the other two architectures is 1024, so we use the Blocked Shampoo variant with block size 1024" but notes this is "dependent on the ordering of neurons in hidden layers."
  - Why unresolved: The paper does not explore how different block sizes affect performance, only using the maximum dimension size. The dependence on neuron ordering suggests the choice may be non-trivial.
  - What evidence would resolve it: An ablation study testing different block sizes (e.g., 64, 128, 256, 512, 1024) on transformer architectures with varying weight matrix dimensions, measuring both memory usage and final task performance.

## Limitations

- The spectral decay assumption may not hold for all deep learning architectures and optimization landscapes
- The choice of sketch rank ℓ requires problem-specific tuning and may become impractically large for certain tasks
- The method's performance on very deep networks and extreme-scale models remains to be thoroughly evaluated

## Confidence

- Regret guarantees: Medium confidence - theoretical bounds are established but depend on unverified spectral assumptions
- Memory efficiency: High confidence - the dk memory requirement is well-established and directly verified
- Training performance: Medium confidence - strong results on tested benchmarks but limited architecture diversity
- Spectral decay hypothesis: Medium confidence - empirical evidence supports it but lacks theoretical foundation

## Next Checks

1. **Spectral robustness test**: Systematically vary sketch rank ℓ across 2× to 8× the intrinsic dimension of covariance factors and measure degradation in both regret bounds and training performance to establish the practical safety margin.

2. **Architecture generalization**: Apply Sketchy to transformer-based language models and vision transformers to test whether the observed spectral decay generalizes beyond ResNet-style architectures.

3. **Break-case analysis**: Construct synthetic gradient covariance matrices with deliberately flat spectra (uniform eigenvalues) to empirically verify the theoretical break condition where additive error terms dominate.