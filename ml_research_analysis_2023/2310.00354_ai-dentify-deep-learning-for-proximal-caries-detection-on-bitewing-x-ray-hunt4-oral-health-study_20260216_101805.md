---
ver: rpa2
title: 'AI-Dentify: Deep learning for proximal caries detection on bitewing x-ray
  -- HUNT4 Oral Health Study'
arxiv_id: '2310.00354'
source_url: https://arxiv.org/abs/2310.00354
tags:
- caries
- images
- detection
- were
- annotators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores deep learning for proximal caries detection
  on bitewing X-ray images from the HUNT4 Oral Health Study. Three object detection
  architectures (RetinaNet, YOLOv5, EfficientDet) were trained on 13,887 bitewing
  images annotated by six dental clinicians.
---

# AI-Dentify: Deep learning for proximal caries detection on bitewing x-ray -- HUNT4 Oral Health Study

## Quick Facts
- arXiv ID: 2310.00354
- Source URL: https://arxiv.org/abs/2310.00354
- Reference count: 30
- Primary result: YOLOv5 achieved 0.647 mAP, 0.548 mF1, and 0.149 mFNR on proximal caries detection, outperforming individual dental clinicians

## Executive Summary
This study demonstrates that deep learning object detection models can outperform individual dental clinicians in detecting proximal caries on bitewing X-ray images. Using a dataset of 13,887 images from the HUNT4 Oral Health Study, six dental clinicians annotated the images individually, and a novel Gaussian Mixture Model-based merging strategy was used to create consensus ground truth labels. Three state-of-the-art object detection architectures (RetinaNet, YOLOv5, EfficientDet) were trained and evaluated, with YOLOv5 showing the best overall performance metrics and significantly lower false negative rates compared to both other models and individual clinicians.

## Method Summary
The study employed three object detection architectures (RetinaNet with ResNet50 backbone, YOLOv5 M, and EfficientDet D0/D1) trained on 13,887 bitewing images from the HUNT4 Oral Health Study. Six dental clinicians individually annotated the images, and a Gaussian Mixture Model-based annotation merging strategy was used to create consensus ground truth labels. The models were trained using 5-fold cross-validation on 8,342 images after data cleaning, with horizontal and vertical flipping augmentation applied. Performance was evaluated on a consensus test set of 197 images using mean average precision, mean F1-score, and mean false negative rate metrics, with comparisons made to individual clinician performance.

## Key Results
- YOLOv5 achieved the best performance with 0.647 mean average precision, 0.548 mean F1-score, and 0.149 mean false negative rate
- All three deep learning models significantly outperformed individual dental clinicians in caries detection
- YOLOv5 showed the largest improvement over baseline methods, demonstrating the potential of deep learning as an assistive tool for caries detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-observer annotation merging strategy using Gaussian Mixture Models creates more robust ground truth labels than any single annotator.
- Mechanism: Individual bounding boxes from six dental clinicians are grouped by IoU, fitted to Gaussian distributions, and combined via a mixture density function to derive consensus boxes.
- Core assumption: Annotators' errors are randomly distributed and can be modeled as Gaussian noise around the true lesion boundary.
- Evidence anchors:
  - "A novel multi-observer annotation merging strategy was used to create ground truth labels."
  - "First, the annotated bounding boxes were grouped based on the intersection over union (IoU) score... A mixture density function (MDF) of a Gaussian Mixture Model in which all distributions have the same weight, was obtained..."

### Mechanism 2
- Claim: YOLOv5's superior performance stems from its efficient feature extraction and object detection pipeline, which better handles the high variance and low contrast in bitewing radiographs.
- Mechanism: YOLOv5 uses a focus layer for efficient feature extraction, cross-stage partial connections, and a PANet neck for feature fusion, enabling it to localize small, low-contrast lesions more accurately than RetinaNet or EfficientDet variants.
- Core assumption: The architectural differences between YOLOv5 and other models translate directly to better performance on this specific domain.
- Evidence anchors:
  - "YOLOv5 achieved the best performance with 0.647 mean average precision..."
  - "Out of the three architectures studied, YOLOv5 shows the largest improvement, reporting 0.647 mean average precision..."

### Mechanism 3
- Claim: The large annotated dataset provides sufficient diversity and sample size for deep learning models to generalize better than individual clinicians.
- Mechanism: With over 13,000 images and consensus annotations from six experts, the training data captures a wide range of lesion presentations, image qualities, and anatomical variations.
- Core assumption: The dataset is representative of real-world clinical variation and free from significant sampling bias.
- Evidence anchors:
  - "A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were annotated individually by six different experts..."
  - "A total of 13,887 images were annotated by one to six of the dental clinicians..."

## Foundational Learning

- Concept: Intersection over Union (IoU)
  - Why needed here: IoU is used to group overlapping bounding boxes from different annotators and to evaluate model predictions against ground truth.
  - Quick check question: If two bounding boxes overlap partially, with areas 50 and 80 pixels, and their intersection is 30 pixels, what is their IoU?

- Concept: Gaussian Mixture Models (GMM)
  - Why needed here: GMM is used to combine multiple annotator boxes into a consensus bounding box by modeling the distribution of box coordinates.
  - Quick check question: What property of GMM makes it suitable for combining multiple uncertain estimates into a single best estimate?

- Concept: Cross-validation
  - Why needed here: Five-fold cross-validation ensures that model performance is not dependent on a particular train/test split and provides more reliable performance estimates.
  - Quick check question: In k-fold cross-validation, how many times is the model trained and tested when k=5?

## Architecture Onboarding

- Component map: Raw bitewing images → annotation merging (GMM) → train/validation/test splits → augmentation (flip) → model input → YOLOv5/RetinaNet/EfficientDet → evaluation (mAP, mF1, mFNR) → comparison with clinicians

- Critical path: Data annotation → GMM merging → 5-fold CV training → consensus test evaluation → statistical comparison

- Design tradeoffs:
  - YOLOv5: Faster inference, better performance, but more parameters than EfficientDet
  - EfficientDet: Fewer parameters, lower FNR, but lower overall mAP and mF1
  - RetinaNet: Balanced but outperformed by others in this task
  - GMM merging: Robust consensus but computationally intensive for large datasets

- Failure signatures:
  - Low mAP with high variance across folds: Overfitting or insufficient data diversity
  - High FNR: Model missing true positives, possibly due to IoU threshold or class imbalance
  - Large gap between training and validation performance: Overfitting
  - Poor bootstrap confidence intervals: Unstable model or insufficient test set size

- First 3 experiments:
  1. Train YOLOv5 on 80% of data, validate on 10%, test on 10%; report mAP, mF1, mFNR.
  2. Compare YOLOv5 predictions to individual annotator annotations on consensus set; visualize IoU distributions.
  3. Train EfficientDet D1 with same splits; compare performance metrics and failure cases to YOLOv5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ensemble strategy of combining the strengths and weaknesses of the different architectures (YOLOv5, RetinaNet, EfficientDet) affect the overall performance compared to individual models?
- Basis in paper: The paper mentions that arranging the trained models in an ensemble fashion is expected to increase the overall performance and the robustness of the predictions. However, the ensemble strategy is not implemented or evaluated in this study.
- Why unresolved: The paper only discusses the potential benefits of an ensemble strategy but does not provide any empirical evidence or results to support this claim.
- What evidence would resolve it: Implementing and evaluating the ensemble strategy on the same dataset used in this study, comparing its performance to the individual models, and reporting the results would resolve this question.

### Open Question 2
- Question: What is the impact of using patch-wise inference on the performance of the object detection models?
- Basis in paper: The paper suggests that a patch-wise inference could further boost the performance by exposing the network to a closer view of the dental pieces, instead of working on the whole picture. However, this approach is not explored or evaluated in this study.
- Why unresolved: The paper only proposes the idea of patch-wise inference as a potential improvement but does not provide any experimental results or analysis to demonstrate its effectiveness.
- What evidence would resolve it: Implementing and evaluating the patch-wise inference approach on the same dataset used in this study, comparing its performance to the current whole-image inference, and reporting the results would resolve this question.

### Open Question 3
- Question: What is the runtime efficiency of the object detection models in a clinical setting, and how does it compare to the time required for manual inspection by dental clinicians?
- Basis in paper: The paper mentions that the models have the potential to improve the efficiency of the analysis of bitewing images and aid in the detection of caries, but it does not provide any information regarding the inference runtime or a comparison with manual inspection time.
- Why unresolved: The paper does not include any runtime measurements or comparisons with the time required for manual inspection by dental clinicians, which is essential for assessing the practical applicability of the models in a clinical setting.
- What evidence would resolve it: Measuring the inference runtime of the object detection models on a representative dataset and comparing it to the average time required for manual inspection by dental clinicians would resolve this question.

## Limitations
- Reliance on a single novel annotation merging strategy without validation against alternative consensus methods
- Single-study dataset limits generalizability to other populations
- No prospective clinical validation studies to assess real-world impact

## Confidence
- YOLOv5's superior performance metrics: High
- Feasibility of deep learning for proximal caries detection: High
- Generalizability to other populations: Medium
- Absolute clinical impact without prospective validation: Low

## Next Checks
1. Replicate the GMM annotation merging with alternative consensus algorithms and compare ground truth consistency
2. Conduct prospective clinical validation on external datasets from multiple geographic regions
3. Perform ROC analysis to determine optimal operating points for clinical deployment