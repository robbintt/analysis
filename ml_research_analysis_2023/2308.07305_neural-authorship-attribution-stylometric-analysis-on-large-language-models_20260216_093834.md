---
ver: rpa2
title: 'Neural Authorship Attribution: Stylometric Analysis on Large Language Models'
arxiv_id: '2308.07305'
source_url: https://arxiv.org/abs/2308.07305
tags:
- attribution
- llms
- neural
- authorship
- open-source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates neural authorship attribution for large
  language models (LLMs), distinguishing between proprietary and open-source categories.
  The authors extract 60 stylometric features spanning lexical, syntactic, and structural
  dimensions from AI-generated news articles and integrate them with RoBERTa-based
  classifiers.
---

# Neural Authorship Attribution: Stylometric Analysis on Large Language Models

## Quick Facts
- arXiv ID: 2308.07305
- Source URL: https://arxiv.org/abs/2308.07305
- Reference count: 15
- Primary result: Achieved F1 scores up to 0.992 for distinguishing proprietary from open-source LLM-generated text using stylometric features combined with RoBERTa embeddings.

## Executive Summary
This study investigates neural authorship attribution for large language models (LLMs), distinguishing between proprietary and open-source categories. The authors extract 60 stylometric features spanning lexical, syntactic, and structural dimensions from AI-generated news articles and integrate them with RoBERTa-based classifiers. Results show strong initial classification between proprietary and open-source LLMs (F1 scores up to 0.992), with lexical diversity, part-of-speech usage, and structural features being key differentifiers. Attribution within proprietary models is accurate (F1 ~0.95), while open-source attribution is more challenging (F1 ~0.80-0.91) due to overlapping writing styles.

## Method Summary
The authors generate 6K AI-generated news articles (1K per model) from GPT-4, GPT-3.5, Llama 1, Llama 2, and GPT-NeoX using headlines from CNN/Washington Post. They extract 60 stylometric features across lexical (average word length, function words, MTTR, hapax legomena), syntactic (sentence length, POS usage, voice/tense), and structural (paragraph length, punctuation, capitalization) dimensions. These features are normalized and combined with RoBERTa embeddings via attention fusion, then classified using XGBoost and RoBERTa-based classifiers. The dataset is split 9:1 for training and testing.

## Key Results
- Initial attribution between proprietary and open-source LLMs achieved F1 scores up to 0.992
- Attribution within proprietary models was highly accurate (F1 ~0.95)
- Attribution within open-source models was more challenging (F1 ~0.80-0.91) due to overlapping writing styles
- Fusion of stylometric features with RoBERTa embeddings yielded near-perfect performance
- Lexical diversity, POS usage, and structural features were identified as key differentiators

## Why This Works (Mechanism)

### Mechanism 1
Stylometric features from lexical, syntactic, and structural dimensions provide distinct, interpretable signals that differentiate proprietary from open-source LLM writing styles. Lexical features capture vocabulary richness; syntactic features capture grammatical patterns; structural features capture layout habits. These combine into a 60-feature vector normalized per article. Differences in feature distributions between proprietary and open-source models are exploited by classifiers. The core assumption is that stylometric features remain stable across prompts and domains when controlling for domain, and that different training regimes induce measurable differences in these features.

### Mechanism 2
Combining stylometric features with PLM embeddings via attention fusion improves attribution accuracy beyond either modality alone. RoBERTa embeddings capture high-level semantic context, while stylometric features capture fine-grained stylistic signatures. The attention layer learns to weight each modality dynamically, then the concatenated representation is classified. The core assumption is that stylometric and semantic embeddings are complementary rather than redundant, and the attention layer can learn effective weighting without overfitting on limited data.

### Mechanism 3
Within-category attribution is harder because intra-group feature variance is high and overlaps between models. Even within proprietary models, RLHF tuning and model size cause measurable style differences; within open-source, shared architectures and pre-training corpora create similarity. This is observable in t-SNE overlap and classification F1 drops. The core assumption is that differences in training pipelines imprint measurable stylometric signatures, but these diminish when models are architecturally or dataset-wise similar.

## Foundational Learning

- Concept: t-SNE for high-dimensional visualization
  - Why needed here: To qualitatively assess separation/clustering of LLM stylometric and embedding feature spaces before formal classification
  - Quick check question: What does a well-separated cluster in t-SNE imply about classification difficulty?

- Concept: Feature normalization and standardization
  - Why needed here: Stylometric features have heterogeneous scales (e.g., counts vs. proportions vs. standard deviations); normalization ensures fair contribution to the classifier
  - Quick check question: Why normalize before computing SHAP importance rather than after?

- Concept: SHAP (Shapley Additive Explanations) for feature importance
  - Why needed here: To interpret which stylometric features drive attribution decisions, ensuring the model's reasoning is transparent and actionable
  - Quick check question: How does SHAP differ from simple Gini importance in tree models?

## Architecture Onboarding

- Component map: Data pipeline → Headline → LLM generation → Tokenization → Stylometric extraction (60 features) → Normalization → PLM embedding (RoBERTa) → Fusion layer (attention) → Classifier (XGBoost/RoBERTa) → Attribution output
- Critical path: Headline → LLM generation → Stylometric extraction → Normalization → Classifier prediction
- Design tradeoffs: Feature richness (60 stylometric dims) vs. computational cost; fusion of modalities vs. simplicity; using RoBERTaZero vs. fine-tuned RoBERTa
- Failure signatures: High overlap in t-SNE plots → classifier confusion; near-random SHAP scores → feature extraction bug; performance drop when open-source models improve → domain shift
- First 3 experiments:
  1. Ablation: Remove each stylometric subset (lexical/syntactic/structural) and measure attribution F1 drop.
  2. Fusion study: Compare RoBERTaStylo vs. RoBERTaF T vs. XGBStylo on same train/test splits.
  3. Open-source convergence: Train on Llama 1&2 only vs. all three open-source models to quantify overlap impact.

## Open Questions the Paper Calls Out

### Open Question 1
How do specific RLHF training steps contribute to the distinct writing signatures observed between GPT-4 and GPT-3.5? The paper notes that GPT-4 and GPT-3.5 exhibit unique writing signatures despite originating from the same organization and having comparable underlying pretraining datasets. The authors suggest that the RLHF training steps might play a pivotal role in shaping these disparities. This remains unresolved as the paper does not delve into the specifics of the RLHF training steps or their impact on the writing signatures of the models.

### Open Question 2
What is the impact of advancements in open-source LLMs on the difficulty of initial attribution between proprietary and open-source models? The authors mention that in a supplementary experiment considering only Llama 2 data to represent the open-source class, there was an average performance decline of 7.4% in the initial attribution task. This suggests that as open-source models advance, the initial stage of attribution becomes increasingly challenging. The paper does not provide a comprehensive analysis of how future advancements in open-source LLMs might further impact the attribution process.

### Open Question 3
What specific factors lead to the convergence of writing styles among open-source LLMs, making them difficult to distinguish? The paper notes that there is a significant decline in attribution performance within the open-source category, with GPT-NeoX and Llama 1 exhibiting overlapping writing styles. The authors hint at possible shared pre-training datasets or architecture similarities as potential reasons. This remains unresolved as the paper does not provide a detailed investigation into the underlying causes of the convergence of writing styles among open-source LLMs.

## Limitations

- Feature extraction reproducibility is uncertain due to unspecified implementation details
- Attribution results may not generalize beyond news articles and fixed generation parameters
- Lower accuracy for open-source attribution suggests convergence in writing styles that may worsen with future model improvements
- Lack of ablation studies comparing fusion performance against single-modality baselines

## Confidence

- High Confidence: The core claim that stylometric features differentiate proprietary from open-source LLMs is well-supported by t-SNE visualizations and classification results
- Medium Confidence: The claim that stylometric-feature fusion with RoBERTa embeddings improves attribution is supported by reported performance but lacks ablation evidence
- Low Confidence: The claim that within-category attribution is inherently harder for open-source models due to overlapping writing styles is plausible but not systematically validated

## Next Checks

1. Ablation study: Systematically remove each stylometric subset (lexical, syntactic, structural) and each modality (stylometric vs. RoBERTa embedding) to quantify their individual and joint contributions to attribution accuracy.

2. Cross-domain robustness: Generate articles from the same LLMs using prompts from multiple domains (technical documentation, creative writing, product reviews) and assess whether stylometric signatures remain stable and discriminative across domains.

3. Open-source convergence test: Train attribution models on pairs of open-source models with varying degrees of architectural and dataset similarity (Llama 1 vs. Llama 2, vs. GPT-NeoX) to quantify how convergence in pretraining or architecture affects attribution accuracy.