---
ver: rpa2
title: 'RoDia: A New Dataset for Romanian Dialect Identification from Speech'
arxiv_id: '2309.03378'
source_url: https://arxiv.org/abs/2309.03378
tags:
- dialect
- speech
- identification
- samples
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces RoDia, the first dataset for Romanian dialect\
  \ identification from speech. RoDia includes 2,805 speech samples from five distinct\
  \ Romanian dialects (Muntenesc, Ardelenesc, Moldovenesc, Oltenesc, and B\u0103ne\u021B\
  ean), totaling 2 hours of manually annotated data."
---

# RoDia: A New Dataset for Romanian Dialect Identification from Speech

## Quick Facts
- **arXiv ID**: 2309.03378
- **Source URL**: https://arxiv.org/abs/2309.03378
- **Reference count**: 0
- **Primary result**: wav2vec 2.0 achieves macro F1 of 59.83% and micro F1 of 62.08% on Romanian dialect identification

## Executive Summary
This paper introduces RoDia, the first dataset specifically designed for Romanian dialect identification from speech. The dataset contains 2,805 audio samples from five distinct Romanian dialects, totaling 2 hours of manually annotated data with dialect, age, and gender labels. The authors evaluate four competitive baseline models (ResNet-18, AST, SepTr, and wav2vec 2.0) and find that wav2vec 2.0 achieves the highest performance. The dataset and code are publicly available, enabling future research in this challenging task.

## Method Summary
The RoDia dataset consists of 2,805 speech samples (2,164 for training, 604 for testing) from five Romanian dialects, each 2.5-5.0 seconds long at 44.1 kHz. Samples are sourced from local TV shows and native speakers, with manual annotations for dialect, age, and gender. Four baseline models are evaluated: ResNet-18, AST, SepTr, and wav2vec 2.0. All models use Short-Time Fourier Transform with window size 512 and hop size 256 to compute magnitude spectrograms. Data augmentation includes noise injection, time shifting, speed perturbation, mix-up, and SpecAugment. Models are trained with Adam optimizer (learning rate 1e-4 for ResNet-18, AST, SepTr; 1e-5 for wav2vec 2.0), batch size 32 (16 for wav2vec 2.0), for 50 epochs with early stopping based on validation loss.

## Key Results
- wav2vec 2.0 achieves the highest macro F1 score of 59.83% and micro F1 score of 62.08%
- ResNet-18 performs worst with macro F1 of 39.82% and micro F1 of 46.11%
- AST and SepTr achieve intermediate performance with macro F1 scores of 51.94% and 47.77% respectively
- Significant confusion observed between certain dialects, particularly Oltenesc vs Bănețean and Moldovenesc vs Bănețean

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manual speaker separation between train and test prevents models from overfitting to speaker-specific features unrelated to dialect.
- Mechanism: By ensuring no speaker overlap between splits, the evaluation forces models to rely on dialect-specific acoustic cues rather than memorizing individual speaker traits.
- Core assumption: Speaker-specific features (pitch, loudness, rate) can confound dialect identification if same speakers appear in both splits.
- Evidence anchors:
  - [section] "We divide the dataset into 2,164 samples for training and 604 samples for testing, such that there is no overlap between speakers in training and test."
  - [section] "Without separating the speakers between training and test, a model that overfits to certain speaker-specific features that are not related to dialect...will reach high scores on the test set."
- Break condition: If dialects are highly correlated with speaker age/gender or if the speaker pool is too small, this separation may reduce effective training data and hurt generalization.

### Mechanism 2
- Claim: Including age and gender labels enables analysis of demographic factors in dialect classification performance.
- Mechanism: By annotating each sample with speaker age group and gender, the dataset allows researchers to study whether certain demographics are harder to classify or if demographic biases exist in the models.
- Core assumption: Dialect features may vary across age and gender, influencing classification difficulty.
- Evidence anchors:
  - [section] "Aside from dialect labels, our annotators also labeled each audio sample with the gender and age of each speaker."
  - [section] "In summary, our audio samples come with dialect, age and gender labels, enabling the study of additional tasks such as gender prediction or age estimation from speech."
- Break condition: If age/gender annotations are inaccurate or if demographic effects are negligible, this additional labeling provides limited benefit.

### Mechanism 3
- Claim: High-quality audio with consistent SNR and SRR reduces noise-related confounding in dialect classification.
- Mechanism: The dataset maintains SNR and SRR values consistently above 23 dB, ensuring that acoustic features used for dialect identification are not obscured by noise or reverberation.
- Core assumption: Low noise and reverberation levels allow phonetic and prosodic dialect features to be clearly captured by the models.
- Evidence anchors:
  - [section] "Regarding data quality, we note that the SNR and SRR values are consistently higher than 23 dB, highlighting that the audio samples have relatively low noise and reverberation."
  - [table] SNR values per dialect ranging from 23.1 to 30.5 dB.
- Break condition: If recording conditions vary widely or if noise characteristics correlate with dialects, even high average SNR may not prevent bias.

## Foundational Learning

- Concept: Understanding Romanian dialect geography and historical background
  - Why needed here: Helps interpret why certain dialects are confused and informs feature engineering.
  - Quick check question: Can you map each dialect (Muntenesc, Ardelenesc, Moldovenesc, Oltenesc, Bănețean) to its geographic region and key historical influences?

- Concept: Speech signal preprocessing and augmentation
  - Why needed here: Baseline models rely on spectrograms and data augmentation to improve generalization.
  - Quick check question: What are the main steps in converting raw audio to a spectrogram, and which augmentation methods are applied?

- Concept: Evaluation metrics for imbalanced classification
  - Why needed here: Dialect classes are imbalanced; micro vs macro F1 scores capture different aspects of performance.
  - Quick check question: How do micro and macro F1 scores differ, and why is reporting both important here?

## Architecture Onboarding

- Component map: Audio loading -> speaker split verification -> preprocessing (spectrogram or wav2vec) -> augmentation -> model (ResNet-18, AST, SepTr, wav2vec 2.0) -> training loop with early stopping -> evaluation (precision, recall, F1 per dialect, micro/macro F1)
- Critical path: Audio loading → speaker split verification → preprocessing → model training → evaluation with confusion matrix analysis
- Design tradeoffs: wav2vec 2.0 (pretrained, hybrid conv/transformer) gives best overall F1 but is slower; ResNet-18 fastest but worst performance; AST and SepTr are middle ground
- Failure signatures: High confusion between neighboring dialects (Oltenesc vs Bănețean, Moldovenesc vs Bănețean) suggests models are picking up regional rather than dialectal cues; low recall on Muntenesc despite high F1 indicates precision issues
- First 3 experiments:
  1. Train ResNet-18 on the full training set without speaker separation to establish upper bound on performance.
  2. Fine-tune wav2vec 2.0 on spectrograms to compare time-frequency vs raw waveform approaches.
  3. Train a dialect classifier with age and gender as auxiliary inputs to assess if demographic information helps disambiguation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do phonetic transcriptions of the audio samples impact the accuracy of Romanian dialect identification models?
- Basis in paper: [explicit] The authors mention providing phonetic transcriptions for the collected audio samples in future work to encourage the use of multimodal systems.
- Why unresolved: The paper does not explore the use of phonetic transcriptions in their experiments, leaving the impact of this feature on model accuracy unknown.
- What evidence would resolve it: Conducting experiments comparing the performance of models trained with and without phonetic transcriptions would provide evidence of their impact on accuracy.

### Open Question 2
- Question: What specific linguistic features cause the confusion between the Moldovenesc and Bănățean dialects in the RoDia dataset?
- Basis in paper: [inferred] The paper discusses the confusion between these dialects, noting similarities in pronunciation and vocabulary, but does not specify the exact linguistic features responsible.
- Why unresolved: While the paper highlights the confusion, it does not delve into the specific phonetic or lexical characteristics that lead to this issue.
- What evidence would resolve it: A detailed linguistic analysis of the audio samples from both dialects, identifying and comparing specific phonetic and lexical features, would clarify the sources of confusion.

### Open Question 3
- Question: How does the inclusion of demographic information (age and gender) affect the performance of dialect identification models?
- Basis in paper: [explicit] The dataset includes age and gender labels, but the paper does not explore their impact on model performance.
- Why unresolved: The paper does not investigate whether incorporating demographic information improves or hinders dialect identification accuracy.
- What evidence would resolve it: Experiments comparing models trained with and without demographic information would reveal its effect on dialect identification performance.

## Limitations
- Small dataset size (2,805 samples) and significant class imbalance may limit model generalization
- Manual annotation process may introduce human bias in dialect classification
- High confusion between certain dialects suggests models may be picking up regional rather than dialectal cues

## Confidence
- **High Confidence**: Dataset creation methodology, speaker separation validation, and basic evaluation metrics are well-documented and reproducible.
- **Medium Confidence**: The choice of baseline models and their implementation details are clearly specified, but performance comparisons may be influenced by hyperparameter choices and implementation specifics.
- **Low Confidence**: The extent to which the dataset captures the full diversity of Romanian dialects and whether the results generalize to real-world scenarios.

## Next Checks
1. **Robustness Testing**: Evaluate model performance across different SNR ranges to verify that the consistently high audio quality (23+ dB) doesn't artificially inflate results.
2. **Demographic Analysis**: Investigate whether age and gender annotations reveal systematic performance differences across demographic groups, potentially indicating bias.
3. **Cross-Validation**: Perform k-fold cross-validation on the training set to assess model stability and variance in performance metrics.