---
ver: rpa2
title: 'Long-Term Ad Memorability: Understanding & Generating Memorable Ads'
arxiv_id: '2309.00378'
source_url: https://arxiv.org/abs/2309.00378
tags:
- memorability
- video
- visual
- scene
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first large-scale dataset for studying
  long-term advertisement memorability, involving 1,203 participants and 2,205 ads
  from 276 brands. The study reveals key insights into ad memorability, such as fast-moving
  ads being more memorable and ad-blocker users remembering fewer ads.
---

# Long-Term Ad Memorability: Understanding & Generating Memorable Ads

## Quick Facts
- **arXiv ID**: 2309.00378
- **Source URL**: https://arxiv.org/abs/2309.00378
- **Reference count**: 40
- **Primary result**: Introduces Sharingan model achieving state-of-the-art memorability prediction across eight datasets with 44% improvement using SEED generation method.

## Executive Summary
This paper addresses the challenge of predicting and generating memorable advertisements by introducing the first large-scale dataset for long-term ad memorability (LAMBDA). The study involves 1,749 participants and 2,205 ads from 276 brands, revealing key insights such as fast-moving ads being more memorable and ad-blocker users remembering fewer ads. The authors propose Sharingan, a novel model that combines large language models with visual encoders to achieve state-of-the-art performance across multiple memorability datasets. Additionally, they introduce SEED, a scalable method to generate more memorable ads, resulting in a 44% increase in memorability scores. The datasets and code are made publicly available for further research.

## Method Summary
The approach combines a visual encoder (EVA-CLIP with ViT+GMHRA), a QFormer for converting visual tokens to language tokens, and a Llama LLM for final memorability prediction. The model uses verbalization of visual content through perception tools (OCR, ASR, emotion detection, color analysis, object tags) to augment visual understanding. Training occurs in two stages: first for visual-language alignment on Webvid/COCO/Visual Genome, then fine-tuning on memorability data with high-quality instructions. The SEED generation method uses prompting techniques to create more memorable ad variations.

## Key Results
- Sharingan achieves state-of-the-art performance across all prominent memorability datasets with Spearman correlation improvements
- Ad-blocker users remember significantly fewer ads than non-ad-blocking users (p=5e-3)
- SEED generation method increases memorability scores by 44% compared to original ads
- Fast-moving ads show higher memorability, while average scene velocity correlates negatively with memorability

## Why This Works (Mechanism)

### Mechanism 1
Combining world knowledge from LLMs with visual semantics from vision encoders improves memorability prediction across both images and videos. The QFormer converts visual tokens from EVA-CLIP into language tokens, which are combined with verbalized scene descriptions and fed into Llama, allowing the model to ground visual understanding in real-world semantic knowledge.

### Mechanism 2
Verbalizing visual scenes into rich semantic descriptions improves model performance more than raw visual embeddings alone. The model uses OCR, ASR, emotion detection, color analysis, and object tags to create detailed textual descriptions of each scene, which are then concatenated with visual tokens before LLM processing.

### Mechanism 3
Training on both short-term and long-term memorability data, while including experimental context, improves generalization across memory types. The model is trained with verbalized experimental conditions (memory type, data distribution, subject descriptions) to disambiguate samples that appear across datasets.

## Foundational Learning

- **Concept**: Multimodal representation learning - combining visual and textual information into unified embeddings
  - **Why needed here**: Ads contain multiple modalities (video, audio, text) that need to be processed together for memorability prediction
  - **Quick check question**: What are the two main components needed to represent multimodal content in a unified space?

- **Concept**: Contrastive learning and cross-modal alignment - learning to associate related visual and textual representations
  - **Why needed here**: The QFormer must learn to align visual features with language tokens so the LLM can reason about visual content
  - **Quick check question**: How does the QFormer help bridge the gap between visual and language representations?

- **Concept**: Instruction tuning - adapting pre-trained models to follow specific task instructions
  - **Why needed here**: The LLM must learn to predict memorability scores (00-99) rather than perform general language tasks
  - **Quick check question**: What is the final output format expected from the instruction-tuned LLM?

## Architecture Onboarding

- **Component map**: Visual encoder (EVA-CLIP with ViT+GMHRA) → QFormer → Language tokens + Verbalized scene descriptions → Llama LLM → Memorability score
- **Critical path**: The data flows from raw video/image through visual encoding, verbalization, LLM processing, to final score prediction
- **Design tradeoffs**: Using frozen visual encoder vs. fine-tuning (chosen: frozen for efficiency); multiple verbalization tools vs. simpler approaches (chosen: comprehensive for richer context); two-stage training vs. end-to-end (chosen: two-stage for better alignment)
- **Failure signatures**: Low performance on both vision-only and language-only ablations suggests poor integration; performance drop when removing experimental context suggests over-reliance on dataset-specific patterns; inconsistent results across runs suggests training instability
- **First 3 experiments**:
  1. Test vision-only baseline (EVA-CLIP + linear layer) to establish visual contribution
  2. Test language-only baseline (verbalization only) to establish LLM contribution
  3. Test combined model with simple QFormer to verify integration quality

## Open Questions the Paper Calls Out

- How does the memorability of advertisements vary across different demographic groups (e.g., age, gender, cultural background)?
- What is the optimal balance between text, audio, and visual elements in an advertisement for maximizing memorability?
- How does the memorability of an advertisement decay over time, and what factors influence the rate of this decay?
- How do ad-blocking users' memorability scores compare to non-ad-blocking users across different types of advertisements?

## Limitations

- The LAMBDA dataset focuses on professionally produced ads from 276 brands, potentially limiting generalization to user-generated or small business content
- The 44% memorability improvement from SEED generation is evaluated through user studies rather than real-world advertising effectiveness metrics
- The model achieves strong correlations but the practical impact on advertising outcomes (engagement, conversions) is not directly measured

## Confidence

**High confidence**: The technical architecture of Sharingan is well-specified and ablation studies provide strong evidence for the effectiveness of combining visual and language representations.

**Medium confidence**: The generalization claims across eight datasets are supported by quantitative results, but differences between datasets and the model's ability to handle these variations could benefit from more detailed analysis.

**Low confidence**: The practical impact on real-world advertising effectiveness is not directly measured; the paper shows memorability prediction accuracy but doesn't demonstrate that more memorable ads lead to better advertising outcomes.

## Next Checks

1. **Domain Generalization Test**: Evaluate Sharingan on advertisements from completely different domains (e.g., user-generated content, small business ads, or ads from countries not represented in LAMBDA) to verify the model's robustness beyond professionally produced content from 276 brands.

2. **Real-World Effectiveness Study**: Conduct an A/B test where SEED-generated memorable ads are compared against standard ads in actual advertising campaigns, measuring not just memorability but also engagement metrics, click-through rates, and brand lift to validate the practical value proposition.

3. **Longitudinal Memory Analysis**: Track memorability over extended periods (6-12 months) rather than the study's timeframe to verify that high-scoring ads maintain their memorability advantage and to identify potential decay patterns that could inform content strategy.