---
ver: rpa2
title: Unsupervised Representation Learning for Diverse Deformable Shape Collections
arxiv_id: '2310.18141'
source_url: https://arxiv.org/abs/2310.18141
tags:
- shape
- mesh
- maps
- meshes
- pooling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an unsupervised method for learning representations
  of diverse deformable shape collections using a novel spectral pooling technique.
  Unlike existing mesh autoencoders that require meshes to have 1-to-1 correspondence,
  this approach works with arbitrary triangular meshes in an unsupervised manner by
  constructing a common latent space using point-to-point maps between shapes.
---

# Unsupervised Representation Learning for Diverse Deformable Shape Collections

## Quick Facts
- arXiv ID: 2310.18141
- Source URL: https://arxiv.org/abs/2310.18141
- Authors: [List of authors from the paper]
- Reference count: 40
- Key outcome: Introduces spectral pooling for unsupervised mesh representation learning, achieving MSE as low as 0.7 on FAUST "unknown individuals" setting

## Executive Summary
This paper introduces an unsupervised method for learning representations of diverse deformable shape collections using a novel spectral pooling technique. Unlike existing mesh autoencoders that require meshes to have 1-to-1 correspondence, this approach works with arbitrary triangular meshes in an unsupervised manner by constructing a common latent space using point-to-point maps between shapes. The method consists of two stages: first, extracting point-to-point maps between shapes using the functional map paradigm, then constructing a shared embedding space using spectral pooling. The spectral pooling technique projects vertex-wise features onto a canonical consistent latent basis (CCLB), reducing dimensionality from the number of vertices to the size of the limit shape.

## Method Summary
The method employs a two-stage approach for unsupervised representation learning of diverse deformable shape collections. First, point-to-point maps between shapes are extracted using the functional map paradigm with DiffusionNet feature extraction and ZoomOut refinement. These maps are then used to construct a canonical consistent latent basis (CCLB). In the second stage, a mesh autoencoder is trained with spectral pooling layers that project vertex features onto the CCLB basis, followed by spectral unpooling for reconstruction. The network is trained using a combination of point-to-point and reconstruction loss functions, enabling the learning of interpretable embeddings independent of mesh connectivity and shape category.

## Key Results
- Achieves MSE reconstruction errors as low as 0.7 on FAUST "unknown individuals" setting
- Successfully handles shape collections with varying mesh connectivity (GALLOP, FAUST, TRUCK)
- Produces smooth and interpretable embedding spaces allowing for shape interpolation and algebraic manipulation
- Demonstrates superior reconstruction quality compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The spectral pooling technique allows the autoencoder to operate on meshes with different connectivities by projecting vertex-wise features onto a canonical consistent latent basis (CCLB).
- Mechanism: By constructing the CCLB from a network of functional maps between shapes, the method creates a shared latent space where features from meshes of varying vertex counts and connectivity can be compared and manipulated. The spectral pooling projects high-dimensional vertex features to the CCLB basis, reducing dimensionality while preserving shape information.
- Core assumption: The CCLB construction is robust to variations in mesh topology and provides a stable common basis for all shapes in the collection.
- Evidence anchors:
  - [abstract]: "Central to our method is a spectral pooling technique that establishes a universal latent space, breaking free from traditional constraints of mesh connectivity and shape categories."
  - [section 3.4]: "To further enhance the stability of this construction and eliminate shape metric ambiguity, [27] introduced the canonical consistent latent basis (CCLB) eYi ∈ Rk1×k2, which has been shown to yield better results."
  - [corpus]: Found related papers on continuous mesh representations and multi-modal learning, suggesting ongoing research in this area, but specific evidence for CCLB stability is limited.

### Mechanism 2
- Claim: The unsupervised functional map learning enables the autoencoder to be trained without ground truth point-to-point correspondences.
- Mechanism: The method uses a DiffusionNet network to generate feature functions that estimate functional maps between shape pairs. Structural properties like bijectivity and orthonormality are imposed as regularization in the loss function to guide the learning process without requiring labeled correspondences.
- Core assumption: The imposed structural properties on functional maps are sufficient to learn meaningful correspondences between shapes without supervision.
- Evidence anchors:
  - [section 4.2]: "To train the network, we predict the functional map in both directions (i.e., C12 and C21) and then penalize the deviation of the predicted maps from bijectivity and orthogonality."
  - [section 4.4]: "Given a point-to-point map (either ground truth or extracted by the first stage) between the template and the input shape, the p2p loss is defined as L1 = ∥ΠS − X∥2F."
  - [corpus]: Limited direct evidence; the method relies on prior work in unsupervised functional map learning, but the effectiveness of this approach without supervision is not extensively validated in the corpus.

### Mechanism 3
- Claim: The combination of spectral pooling and the reconstruction loss enables the autoencoder to produce smooth and realistic interpolations between shapes.
- Mechanism: The spectral pooling projects features to a common basis, and the reconstruction loss (L2 = ∥DS − DX∥2F) ensures that the local geometry is preserved during reconstruction. This combination allows the network to learn a smooth embedding manifold where linear interpolation produces realistic shapes.
- Core assumption: The reconstruction loss, which is rotation invariant, complements the point-to-point loss to provide sufficient supervision for learning a smooth manifold.
- Evidence anchors:
  - [section 4.4]: "This loss computes the cumulative reconstruction error and each point receives reconstruction feedback from the other n − 1 points. Thus, even if the p2p map is faulty in some places, the faulty points receive signals from the non-faulty ones."
  - [section 5.2.3]: "To show that the shape features lie on a smooth manifold and that the network is not overfitting to the training samples, we generate new shapes by sampling from the latent feature space."
  - [corpus]: Related work on generative models and autoencoders suggests that combining different loss functions can improve interpolation quality, but specific evidence for this particular combination is limited.

## Foundational Learning

- Concept: Functional Maps
  - Why needed here: Functional maps provide a low-dimensional representation of correspondences between shapes, which is essential for constructing the CCLB basis and enabling the spectral pooling technique.
  - Quick check question: Can you explain how a functional map C21 = Φ†1Π12Φ2 reduces the dimensionality of a point-to-point map Π12 from quadratic to quadratic matrices?

- Concept: Spectral Graph Theory
  - Why needed here: Spectral graph theory is used to compute the Laplacian decomposition and construct the basis functions (eigenvectors) needed for the CCLB and spectral pooling.
  - Quick check question: What is the significance of the cotangent Laplace-Beltrami decomposition in the context of triangular meshes?

- Concept: Autoencoder Architecture
  - Why needed here: Understanding the standard autoencoder components (encoder, decoder, latent space) is crucial for grasping how the spectral pooling and unpooling operations fit into the overall architecture.
  - Quick check question: How does the spectral pooling operation differ from traditional pooling methods used in image autoencoders?

## Architecture Onboarding

- Component map:
  - Input mesh -> Encoder (DiffusionNet blocks) -> Spectral pooling -> CCLB projection -> Latent code -> Spectral unpooling -> Decoder (DiffusionNet blocks) -> Reconstructed mesh

- Critical path:
  - Input mesh -> Encoder -> Spectral pooling -> Latent code -> Spectral unpooling -> Decoder -> Reconstructed mesh

- Design tradeoffs:
  - Using CCLB allows handling meshes with different connectivities but requires constructing a functional map network, which can be computationally expensive.
  - The unsupervised learning approach eliminates the need for ground truth correspondences but may result in less accurate maps compared to supervised methods.

- Failure signatures:
  - Poor reconstruction quality: May indicate issues with the functional map network construction or the spectral pooling/unpooling operations.
  - Unstable training: Could be caused by the unsupervised learning process or the combination of loss functions.

- First 3 experiments:
  1. Train the autoencoder on a simple dataset (e.g., FAUST with known correspondences) using ground truth maps to validate the basic architecture.
  2. Train the unsupervised functional map network on the same dataset to assess the quality of the learned maps.
  3. Train the full autoencoder using the unsupervised maps and compare the reconstruction quality to the supervised baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the spectral pooling technique compare to other state-of-the-art pooling methods for mesh autoencoders in terms of reconstruction accuracy and smoothness of the embedding space?
- Basis in paper: [inferred] The paper introduces a novel spectral pooling technique and demonstrates its effectiveness in reconstructing shapes and generating smooth embeddings, but does not directly compare it to other pooling methods.
- Why unresolved: The paper focuses on the advantages of spectral pooling but does not provide a comprehensive comparison with other pooling methods, such as global pooling or max pooling.
- What evidence would resolve it: Conducting experiments comparing the reconstruction accuracy and smoothness of the embedding space using different pooling methods on various shape collections would provide insights into the relative performance of spectral pooling.

### Open Question 2
- Question: Can the proposed method handle shape collections with high non-isometry, such as the GALLOP shape collection, where learning accurate point-to-point maps between different categories (e.g., horses and elephants) is challenging?
- Basis in paper: [explicit] The paper acknowledges that the method struggles with shape collections with high non-isometry and leaves handling such cases as future work.
- Why unresolved: The paper does not provide a solution for handling shape collections with high non-isometry and suggests that this is an area for future research.
- What evidence would resolve it: Developing and evaluating techniques to improve the accuracy of point-to-point maps between highly non-isometric shapes would address this open question.

### Open Question 3
- Question: Can the decoder of the proposed autoencoder generate multiple mesh topologies without the use of a template, thereby eliminating the need for fixed templates for reconstruction?
- Basis in paper: [explicit] The paper mentions that using fixed templates for reconstruction is a limitation and suggests investigating whether the decoder can generate multiple mesh topologies without templates as future work.
- Why unresolved: The paper does not explore the possibility of generating multiple mesh topologies without templates and leaves this as an open question for future research.
- What evidence would resolve it: Developing and evaluating a decoder architecture that can generate multiple mesh topologies without the use of templates would provide insights into the feasibility of this approach.

## Limitations
- Limited evaluation on synthetic datasets with controlled variations raises questions about generalization to real-world diverse collections
- Performance on shape collections with high non-isometry (e.g., GALLOP) is not fully addressed
- Requires constructing a functional map network, which can be computationally expensive for large collections

## Confidence

**Confidence Assessment:**
- High confidence in the technical feasibility of the spectral pooling approach for handling varying mesh connectivities
- Medium confidence in the unsupervised map extraction quality without ground truth comparisons
- Medium confidence in reconstruction quality claims due to limited dataset diversity
- Low confidence in scalability to highly diverse, real-world shape collections without further validation

## Next Checks

1. Test the method on a real-world shape collection with significant topological variation (e.g., ShapeNet with multiple categories) to evaluate robustness beyond controlled datasets
2. Compare unsupervised map quality against supervised baselines on a subset of data where ground truth correspondences exist
3. Evaluate interpolation quality by generating shapes at various interpolation weights and measuring perceptual realism and geometric consistency