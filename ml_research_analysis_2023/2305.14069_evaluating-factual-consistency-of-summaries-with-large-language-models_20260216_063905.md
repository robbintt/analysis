---
ver: rpa2
title: Evaluating Factual Consistency of Summaries with Large Language Models
arxiv_id: '2305.14069'
source_url: https://arxiv.org/abs/2305.14069
tags:
- prompting
- llms
- language
- summaries
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using large language models (LLMs) to evaluate
  the factual consistency of automatically generated summaries. Specifically, the
  authors prompt various LLMs (including GPT-4, ChatGPT, text-davinci-003, code-davinci-002,
  and Flan-T5) to classify whether a given summary is factually consistent with its
  source document.
---

# Evaluating Factual Consistency of Summaries with Large Language Models

## Quick Facts
- **arXiv ID:** 2305.14069
- **Source URL:** https://arxiv.org/abs/2305.14069
- **Reference count:** 40
- **Primary result:** LLM prompting achieves up to 12.2 absolute points improvement in binary classification accuracy for summary factuality evaluation

## Executive Summary
This paper investigates using large language models (LLMs) as off-the-shelf evaluators for factual consistency in automatically generated summaries. The authors prompt various LLMs including GPT-4, ChatGPT, and open models to classify whether summaries are factually consistent with their source documents. They experiment with vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence approach for long summaries. Evaluated on multiple benchmarks, the method significantly outperforms existing factuality evaluation methods, achieving up to 88% balanced accuracy on CNNDM data and demonstrating that LLMs can serve as highly effective factuality evaluators when properly prompted.

## Method Summary
The authors frame factual consistency evaluation as an entailment task, directly querying LLMs whether summaries can be inferred from source documents. They implement three prompting methods: vanilla prompting (direct yes/no question), chain-of-thought prompting (explicit reasoning about supporting evidence), and sentence-by-sentence prompting (decomposing long summaries for evaluation). The approach is evaluated zero-shot and with few-shot exemplars across five LLMs on five benchmark datasets containing summaries from diverse summarization systems.

## Key Results
- LLM prompting achieves up to 12.2 absolute points improvement in binary classification accuracy over existing methods
- GPT-4 and ChatGPT achieve the best performance with 88% balanced accuracy on CNNDM benchmark
- Chain-of-thought prompting underperforms vanilla prompting despite being effective in other domains
- Sentence-by-sentence approach significantly improves evaluation of long summaries

## Why This Works (Mechanism)

### Mechanism 1
LLMs can function as factual consistency evaluators because they can perform zero-shot entailment classification when given appropriate prompts. The LLM receives a prompt combining the source document, a yes/no question about factual consistency, and the summary, then uses its pre-trained language understanding to determine if the summary can be inferred from the document. Core assumption: The LLM's pre-training has encoded sufficient world knowledge and reasoning ability to assess factual consistency without additional fine-tuning.

### Mechanism 2
Chain-of-thought prompting can improve factual consistency evaluation by explicitly asking the model to find supporting evidence. The prompt instructs the model to first locate evidence in the document that supports or contradicts the summary, then make a final yes/no determination. This structured reasoning process helps the model be more thorough. Core assumption: Breaking down the reasoning process into explicit steps helps the model avoid superficial judgments and consider evidence more carefully.

### Mechanism 3
Sentence-by-sentence prompting improves evaluation of long summaries by decomposing the task into smaller, more manageable pieces. The summary is broken into individual sentences, each evaluated separately for factual consistency with the document, then results are combined for an overall judgment. Core assumption: Evaluating shorter text spans is easier for the model than evaluating the entire summary at once, particularly when dealing with lengthy documents.

## Foundational Learning

- **Concept: Entailment**
  - Why needed here: The paper casts factual consistency as an entailment task, so understanding entailment is fundamental to grasping the approach
  - Quick check question: If document states "The cat sat on the mat" and summary states "A feline was on a rug", is this entailment? (Answer: Yes, cat → feline, mat → rug)

- **Concept: Zero-shot vs Few-shot learning**
  - Why needed here: The paper compares zero-shot prompting (no examples) with few-shot prompting (with examples), so understanding the difference is important
  - Quick check question: What's the key difference between zero-shot and few-shot prompting? (Answer: Zero-shot provides no examples, few-shot provides 1-5 examples)

- **Concept: Prompt engineering**
  - Why needed here: The paper emphasizes that different prompt formulations yield different results, showing that prompt engineering is critical
  - Quick check question: Why might changing "Can the following statement be inferred from the document?" to "Is the following statement factually consistent with the document?" affect results? (Answer: Different wording can trigger different reasoning patterns in the model)

## Architecture Onboarding

- **Component map:** Document → Prompt construction → LLM inference → Classification → Evaluation
- **Critical path:** Document → Prompt construction → LLM inference → Classification → Evaluation
- **Design tradeoffs:** Using larger models (GPT-4) vs smaller open models (Flan-T5): Larger models generally perform better but are more expensive and less accessible; Zero-shot vs few-shot: Few-shot can improve performance but requires finding good exemplars and increases prompt length; Sentence-by-sentence vs full-summary: Sentence-level is more granular but may miss cross-sentence errors
- **Failure signatures:** Consistently wrong classifications on certain error types; Degradation on highly abstractive summaries; Sensitivity to prompt wording variations; Context window limitations with few-shot examples
- **First 3 experiments:** Test vanilla prompting on a small CNNDM sample with GPT-3.5 to verify basic functionality; Compare zero-shot vs 2-shot prompting on the same sample to measure few-shot benefit; Apply sentence-by-sentence prompting to a long summary to verify decomposition helps

## Open Questions the Paper Calls Out

- **Open Question 1:** What are the underlying reasons why chain-of-thought prompting consistently underperforms vanilla prompting in factual consistency evaluation tasks? The paper only provides speculation about why COT is less effective, without conducting controlled experiments to test this hypothesis or exploring alternative explanations.

- **Open Question 2:** How does the effectiveness of sentence-by-sentence prompting vary with different summary decomposition strategies beyond simple sentence boundary splitting? The paper only tests one simple decomposition method and explicitly defers exploration of alternatives to future work.

- **Open Question 3:** What are the fundamental limitations preventing LLMs from achieving high accuracy on highly abstractive summaries, as evidenced by the performance gap between CNNDM and XSum benchmarks? The paper identifies the performance gap but doesn't investigate the specific characteristics of abstractive summaries that make them harder to evaluate or whether different prompting strategies could close this gap.

## Limitations
- Lack of complete prompt specifications, with only examples shown rather than full templates
- Evaluation focuses exclusively on binary classification accuracy without examining quality of model explanations or confidence calibration
- Experiments don't explore how model performance varies across different domains beyond news summarization

## Confidence
- **LLMs as effective factuality evaluators:** High confidence - consistent improvements over existing methods with multiple LLMs and datasets
- **Chain-of-thought prompting benefits:** Medium confidence - improvements shown but mechanism isn't thoroughly analyzed and could be sensitive to prompt wording
- **Sentence-by-sentence approach effectiveness:** Medium confidence - results support the approach but comparison is limited to specific long-document scenarios
- **Generalizability across summarization systems:** Medium confidence - evaluation covers diverse systems but doesn't test extreme abstractiveness or domain shifts

## Next Checks
1. **Prompt sensitivity analysis:** Systematically vary prompt wording while keeping the same input to quantify how sensitive results are to minor prompt modifications
2. **Error type categorization:** Manually analyze model errors to identify which types of factual inconsistencies (entity, number, relationship) are most challenging for LLMs
3. **Domain transfer experiment:** Test the best-performing LLM on summaries from non-news domains (scientific papers, legal documents) to assess generalization limits