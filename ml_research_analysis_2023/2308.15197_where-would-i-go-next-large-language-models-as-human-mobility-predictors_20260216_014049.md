---
ver: rpa2
title: Where Would I Go Next? Large Language Models as Human Mobility Predictors
arxiv_id: '2308.15197'
source_url: https://arxiv.org/abs/2308.15197
tags:
- mobility
- human
- prediction
- stays
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-Mob, the first framework to apply large
  language models (LLMs) to human mobility prediction. The method formats mobility
  data into "historical stays" and "context stays" to capture long-term and short-term
  movement dependencies, and incorporates time information for time-aware prediction.
---

# Where Would I Go Next? Large Language Models as Human Mobility Predictors

## Quick Facts
- arXiv ID: 2308.15197
- Source URL: https://arxiv.org/abs/2308.15197
- Reference count: 7
- Primary result: LLM-Mob achieves up to 45.1% accuracy@1 on Geolife and 32.2% on Foursquare NYC, outperforming state-of-the-art deep learning models

## Executive Summary
This paper introduces LLM-Mob, the first framework to apply large language models (LLMs) to human mobility prediction. The method formats mobility data into "historical stays" and "context stays" to capture long-term and short-term movement dependencies, and incorporates time information for time-aware prediction. Context-inclusive prompts are designed to enable LLMs to generate accurate and interpretable predictions. Experiments on two public datasets (Geolife and Foursquare NYC) show that LLM-Mob significantly outperforms state-of-the-art deep learning models, achieving up to 45.1% accuracy@1 on Geolife and 32.2% on Foursquare NYC, with superior interpretability through generated explanations.

## Method Summary
LLM-Mob is a framework that reformats human mobility data into sequences of stays (locations with timestamps) and uses this formatted data as input to LLMs for prediction. The framework consists of three main components: (1) data preprocessing that converts raw trajectories into stays using methods from [Hong et al., 2023], (2) data formatting that separates stays into historical stays (long-term patterns) and context stays (recent movements) while incorporating target time information, and (3) context-inclusive prompt generation that guides LLMs to predict the next location and provide explanations. The framework uses GPT-3.5 API for inference and evaluates performance on Geolife and Foursquare NYC datasets using metrics including accuracy@1, accuracy@5, accuracy@10, weighted F1 score, and nDCG@10.

## Key Results
- LLM-Mob achieves up to 45.1% accuracy@1 on Geolife dataset, outperforming state-of-the-art deep learning models
- On Foursquare NYC dataset, LLM-Mob reaches 32.2% accuracy@1, demonstrating superior performance
- The framework provides interpretable predictions through generated explanations that describe reasoning patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-Mob achieves superior performance by formatting mobility data into "historical stays" and "context stays" to capture long-term and short-term dependencies.
- Mechanism: The method separates stays into two categories - historical stays (long-term patterns) and context stays (recent movements). This dual representation allows the LLM to understand both stable patterns (like regular work visits) and recent context (like unexpected events).
- Core assumption: Human mobility has both predictable long-term patterns and variable short-term influences that can be captured through this dual stay representation.
- Evidence anchors:
  - [abstract] "We present concepts of historical stays and context stays to capture both long-term and short-term dependencies in human movement"
  - [section 4.1.1] "We propose to construct historical stays as stays which are relatively more distant from the current time and span over a longer period, while context stays are a few most recent stays"
- Break condition: If human mobility patterns are purely random without any long-term structure, this mechanism would fail as the historical stays would provide no predictive value.

### Mechanism 2
- Claim: Incorporating time information of the prediction target enables time-aware prediction that outperforms time-agnostic approaches.
- Mechanism: The framework includes target time information (stn+1, down+1) in the prediction process, allowing the model to predict where someone will be at a specific future time rather than just the next location in sequence.
- Core assumption: People's mobility patterns vary significantly based on time of day and day of week, making temporal context crucial for accurate prediction.
- Evidence anchors:
  - [abstract] "enable time-aware prediction by using time information of the prediction target"
  - [section 4.1.2] "we predict which place the user will be at the next time (stn+1, down+1)"
- Break condition: If mobility patterns are completely time-independent or if the model cannot effectively process temporal information, this mechanism would provide no advantage.

### Mechanism 3
- Claim: Context-inclusive prompting strategies guide LLMs to leverage their reasoning capabilities for accurate and interpretable mobility predictions.
- Mechanism: The prompt design includes specific instructions to consider historical patterns, recent context, and temporal information, while also asking for explanations. This leverages the LLM's chain-of-thought reasoning ability.
- Core assumption: LLMs can be effectively guided through prompt engineering to perform reasoning tasks outside their original training domain (language).
- Evidence anchors:
  - [abstract] "we design context-inclusive prompts that enable LLMs to generate more accurate predictions"
  - [section 4.2] "we carefully develop context-inclusive prompts that incorporate relevant contextual information"
- Break condition: If LLMs cannot transfer reasoning capabilities to new domains or if prompt engineering is insufficient to overcome training domain limitations.

## Foundational Learning

- Concept: Spatiotemporal pattern recognition in human mobility
  - Why needed here: Understanding how human movement exhibits both regularity and stochasticity is crucial for framing the prediction problem correctly
  - Quick check question: Why can't traditional Markov models alone solve this problem effectively?

- Concept: Large language model in-context learning
  - Why needed here: LLM-Mob relies on LLMs' ability to perform reasoning without additional training, making ICL understanding essential
  - Quick check question: How does chain-of-thought prompting differ from standard prompting in guiding LLM reasoning?

- Concept: Mobility data preprocessing and trajectory analysis
  - Why needed here: Converting raw GPS trajectories into structured "stays" is a prerequisite for the LLM-Mob framework
  - Quick check question: What criteria determine when a sequence of GPS points becomes a "stay" rather than movement?

## Architecture Onboarding

- Component map: Data preprocessing → Historical/context stays formatting → Time-aware target construction → Context-inclusive prompt generation → LLM inference → Prediction and explanation output
- Critical path: The formatting of mobility data into the specific prompt structure is the most critical step, as it determines how effectively the LLM can reason about the problem
- Design tradeoffs: Using GPT-3.5 provides strong reasoning but incurs API costs and potential performance drift; open-source models could reduce costs but may require fine-tuning
- Failure signatures: Hallucinations in explanations, duplicated predictions, or poor performance on datasets with few unique locations indicate issues with the prompting strategy or data formatting
- First 3 experiments:
  1. Test the framework with a simple dataset and verify the historical/context stay separation works as intended
  2. Evaluate the effect of including vs. excluding target time information on prediction accuracy
  3. Compare different prompt variants (with/without reasoning guidance, with/without explanation requests) to identify the most effective configuration

## Open Questions the Paper Calls Out

- Question: How does the performance of LLM-Mob scale with increasing size and diversity of training datasets, and what is the optimal data volume required for peak performance?
  - Basis in paper: [inferred] The paper uses two public datasets (Geolife and Foursquare NYC) and mentions computational inefficiency as a limitation, suggesting dataset size may impact performance.
  - Why unresolved: The paper doesn't explore performance across varying dataset sizes or compositions, nor does it investigate the relationship between data volume and model accuracy.
  - What evidence would resolve it: Systematic experiments varying dataset size and diversity while measuring prediction accuracy and computational costs.

- Question: Can smaller, open-source LLMs trained specifically for mobility prediction match or exceed the performance of proprietary models like GPT-3.5 while addressing cost and hallucination issues?
  - Basis in paper: [explicit] The paper identifies limitations of using proprietary LLMs including computational cost, hallucination problems, and model updates causing performance drift.
  - Why unresolved: The paper relies on GPT-3.5 API and doesn't explore training or fine-tuning open-source alternatives.
  - What evidence would resolve it: Direct comparison between proprietary LLM performance and open-source models trained/fine-tuned on mobility data.

- Question: What are the specific architectural modifications or training strategies needed to minimize hallucination while maintaining high prediction accuracy in mobility LLMs?
  - Basis in paper: [explicit] The paper acknowledges hallucination as a significant limitation where the model fabricated information (e.g., claiming a place was a restaurant when no such information existed).
  - Why unresolved: The paper uses existing LLMs without modifications and doesn't explore techniques to address hallucination.
  - What evidence would resolve it: Ablation studies testing different prompt engineering techniques, fine-tuning strategies, or architectural modifications to reduce hallucinated outputs.

## Limitations
- Computational efficiency and cost: The framework relies on API calls to GPT-3.5, making it computationally expensive and potentially impractical for large-scale deployment
- Proprietary model dependency: Using GPT-3.5 creates vendor lock-in and vulnerability to model updates or API changes
- Hallucination and duplication issues: LLMs may generate inaccurate or repeated predictions, particularly when faced with ambiguous or novel scenarios

## Confidence
**High confidence** (well-supported by evidence):
- The framework architecture (historical/context stays + time-aware prediction + context-inclusive prompts) is clearly specified and implemented
- Performance improvements over baselines on the tested datasets are well-documented
- The interpretability advantage through generated explanations is demonstrated

**Medium confidence** (reasonable but with caveats):
- Generalizability to other mobility datasets beyond Geolife and Foursquare NYC
- The claimed paradigm shift from domain-specific models to general-purpose LLMs, given limited testing across different mobility scenarios
- The explanation quality and usefulness for real-world applications

**Low confidence** (limited evidence):
- Long-term stability of performance given reported "performance drift" with model updates
- Scalability to massive user bases given computational constraints
- Effectiveness in privacy-sensitive contexts where API calls may be problematic

## Next Checks
1. **Cross-dataset validation**: Apply LLM-Mob to at least two additional mobility datasets (e.g., from different geographic regions or with different activity types) to assess generalizability beyond the tested Geolife and Foursquare NYC datasets.

2. **Open-source LLM comparison**: Implement the framework using open-source LLMs (e.g., LLaMA, Mistral) with and without fine-tuning to quantify the tradeoff between proprietary model performance and deployment practicality.

3. **Real-time performance evaluation**: Measure end-to-end latency (preprocessing + API call + post-processing) and cost per prediction on production-scale data to determine practical deployment constraints.