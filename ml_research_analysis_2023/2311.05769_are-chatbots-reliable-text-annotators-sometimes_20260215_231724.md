---
ver: rpa2
title: Are Chatbots Reliable Text Annotators? Sometimes
arxiv_id: '2311.05769'
source_url: https://arxiv.org/abs/2311.05769
tags:
- political
- performance
- chatgpt
- https
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluates the performance of open-source large language
  models (OS LLMs) compared to ChatGPT and traditional supervised classifiers for
  text annotation tasks in social science research. Using a dataset of 2,900 tweets
  from US news media, the authors compare the accuracy and F1-scores of various OS
  models, ChatGPT (GPT-3.5 and GPT-4), and a supervised DistilBERT classifier across
  two binary classification tasks: identifying political vs.'
---

# Are Chatbots Reliable Text Annotators? Sometimes

## Quick Facts
- arXiv ID: 2311.05769
- Source URL: https://arxiv.org/abs/2311.05769
- Reference count: 19
- The study finds that supervised classifiers using DistilBERT consistently outperform both open-source LLMs and ChatGPT for text annotation tasks in social science research.

## Executive Summary
This study evaluates the performance of open-source large language models (OS LLMs) compared to ChatGPT and traditional supervised classifiers for text annotation tasks in social science research. Using a dataset of 2,900 tweets from US news media, the authors compare the accuracy and F1-scores of various OS models, ChatGPT (GPT-3.5 and GPT-4), and a supervised DistilBERT classifier across two binary classification tasks: identifying political vs. non-political content and exemplar vs. non-exemplar content. The results show that the supervised DistilBERT classifier consistently outperforms both OS LLMs and ChatGPT across tasks, while ChatGPT models slightly outperform OS models in some instances. Given ChatGPT's closed-source nature, lack of transparency, and inconsistent performance, the authors advise against using it for substantive text annotation in social science research.

## Method Summary
The authors evaluate model performance using zero-shot and few-shot learning approaches with both generic and custom prompts. They test multiple OS LLMs (FLAN-T5-XXL, StableBeluga-13B, sentence-transformers) and ChatGPT models against a supervised DistilBERT classifier trained on 80% of human-annotated data. All models are evaluated on held-out test sets using accuracy, F1-score, precision, and recall metrics. The dataset consists of 2,900 tweets annotated for political vs. non-political content and exemplar vs. non-exemplar content.

## Key Results
- Supervised classifiers using DistilBERT consistently outperform both OS LLMs and ChatGPT across both classification tasks
- OS models like FLAN-T5-XXL and StableBeluga-13B perform comparably to ChatGPT when using custom prompts
- All LLM models struggle with the more nuanced exemplar classification task compared to the political content task
- ChatGPT's performance varies significantly and unpredictably across tasks, showing inconsistent reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised classifiers outperform LLMs because they are trained on task-specific human-labeled data with optimized model architectures (DistilBERT)
- Mechanism: DistilBERT leverages fine-tuned contextual embeddings trained on large corpora, then further optimized with human-annotated labels, leading to higher precision and recall on binary tasks
- Core assumption: The human-labeled data is representative and high quality, enabling the classifier to generalize well
- Evidence anchors: [abstract] "the supervised classifier using DistilBERT generally outperforms both"; [section] "we find that supervised classifiers consistently outperform both"

### Mechanism 2
- Claim: LLMs struggle with nuanced classification tasks due to lack of task-specific fine-tuning
- Mechanism: Generic LLMs are trained on broad web data without domain-specific labeling, so they perform poorly on subtle distinctions like exemplar vs. non-exemplar content
- Core assumption: Fine-tuning on labeled data is necessary for high performance on nuanced tasks
- Evidence anchors: [abstract] "both are systematically outperformed by supervised models"; [section] "performance is worse for all models regardless of prompt structure"

### Mechanism 3
- Claim: Custom prompts improve LLM performance by providing clearer task definitions
- Mechanism: Adding explicit definitions of political content or exemplar criteria helps LLMs focus on relevant features rather than relying on implicit cues
- Core assumption: LLMs can effectively use provided definitions when they are precise and aligned with the task
- Evidence anchors: [section] "custom prompts seem to generate higher performance than generic prompts"; [section] "GPT-4's F1-score improves and is more reliable across zero- and few-shot with a custom prompt"

## Foundational Learning

- Concept: Binary classification evaluation metrics (accuracy, precision, recall, F1-score)
  - Why needed here: The paper compares model performance using multiple metrics; understanding their differences is critical for interpreting results
  - Quick check question: If a dataset has 90% negative examples and a model predicts all negatives, what is its accuracy and F1-score?

- Concept: Zero-shot vs. few-shot learning
  - Why needed here: The study tests both approaches to see how providing examples affects LLM performance
  - Quick check question: In few-shot learning, if the provided examples are unrepresentative, will the model's performance likely improve or degrade?

- Concept: Prompt engineering and its impact on LLM outputs
  - Why needed here: The paper tests generic vs. custom prompts to assess how instruction clarity affects results
  - Quick check question: If a prompt includes conflicting instructions, what is the most likely outcome for model performance?

## Architecture Onboarding

- Component map: Data → Human annotation → Supervised classifier (DistilBERT) ↔ LLMs (ChatGPT, OS models) → Evaluation (accuracy, F1-score)
- Critical path: Human annotation → Supervised classifier training → Evaluation on held-out test set
- Design tradeoffs: Open-source models offer transparency and lower cost but may underperform closed-source models; supervised classifiers require labeled data but deliver consistent performance
- Failure signatures: If labeled data is imbalanced, F1-score will be much lower than accuracy; if prompts are ambiguous, LLM outputs will be inconsistent
- First 3 experiments:
  1. Train a supervised classifier on the provided human-labeled data and evaluate on the test set
  2. Test zero-shot classification with a generic prompt using an OS LLM (e.g., FLAN-T5-XXL)
  3. Test few-shot classification with a custom prompt using the same OS LLM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of open-source LLMs compare to ChatGPT and supervised classifiers on more complex, multi-class annotation tasks beyond simple binary classification?
- Basis in paper: [inferred] The authors focus on binary classification tasks and note that "simple binary text annotation tasks" were used, suggesting potential differences in performance for more complex tasks
- Why unresolved: The study only examines binary classification tasks, leaving open how these models would perform on more nuanced or multi-class annotation challenges common in social science research
- What evidence would resolve it: Systematic evaluation of OS LLMs, ChatGPT, and supervised classifiers on multi-class annotation tasks using diverse datasets and social science concepts would provide insights into their relative performance and limitations

### Open Question 2
- Question: What specific factors contribute to the inconsistent performance of ChatGPT models across different annotation tasks, and how can these be mitigated through prompt engineering or model selection?
- Basis in paper: [explicit] The authors note "significant inconsistency and unreliability across different tasks and models" for ChatGPT, but cannot pinpoint causes due to its closed-source nature
- Why unresolved: Without access to ChatGPT's internal workings, it's unclear what drives performance variations and how to optimize its use for specific annotation tasks
- What evidence would resolve it: Controlled experiments varying prompt structures, task types, and datasets, combined with ablation studies on different ChatGPT model versions, could identify key factors affecting performance and guide effective prompt engineering strategies

### Open Question 3
- Question: How does the performance of open-source LLMs on text annotation tasks evolve as model size increases, and what is the optimal balance between model complexity and task performance for social science research applications?
- Basis in paper: [explicit] The authors compare various OS LLMs with different parameter sizes, noting that "OS models approach the performance of ChatGPT when custom prompts are used, despite being only a fraction of the size"
- Why unresolved: The study tests a limited range of OS models, leaving questions about the relationship between model size, computational cost, and annotation performance for different task complexities
- What evidence would resolve it: Systematic evaluation of a broader range of OS LLM sizes on diverse annotation tasks, coupled with analysis of computational costs and performance trade-offs, would inform optimal model selection for social science research

## Limitations
- The study's findings are constrained by the specific dataset (2,900 tweets from US news media) and two binary classification tasks
- The relatively modest dataset size may limit generalizability to larger or differently structured annotation tasks
- The closed-source nature of ChatGPT models prevents full transparency in comparing exact implementations and hyperparameters

## Confidence
- High confidence: Supervised classifiers (DistilBERT) outperform both OS LLMs and ChatGPT across tested tasks
- Medium confidence: OS models like FLAN-T5-XXL and StableBeluga-13B can perform comparably to ChatGPT in certain cases with custom prompts
- Low confidence: Generalizing nuanced performance differences between zero-shot and few-shot learning across all possible annotation tasks

## Next Checks
1. Test the same model comparison pipeline on a significantly larger and more diverse text corpus (e.g., news articles or academic abstracts) to assess generalizability beyond tweets
2. Implement fine-tuning of OS LLMs on the human-labeled data to directly compare against supervised classifiers under identical training conditions
3. Conduct ablation studies on prompt engineering by systematically varying prompt specificity, example quality, and instruction clarity to quantify their impact on LLM performance