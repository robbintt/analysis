---
ver: rpa2
title: 'DUCK: Distance-based Unlearning via Centroid Kinematics'
arxiv_id: '2312.02052'
source_url: https://arxiv.org/abs/2312.02052
tags:
- unlearning
- duck
- scenario
- learning
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DUCK, a novel machine unlearning algorithm
  that addresses the challenge of selectively removing information about specific
  data samples from trained deep learning models. DUCK employs metric learning to
  guide the removal process, directing embeddings of forget-samples towards the nearest
  incorrect class centroid in the embedding space.
---

# DUCK: Distance-based Unlearning via Centroid Kinematics

## Quick Facts
- arXiv ID: 2312.02052
- Source URL: https://arxiv.org/abs/2312.02052
- Reference count: 40
- Key outcome: DUCK achieves state-of-the-art performance in selective machine unlearning, demonstrating superior forget-set accuracy while maintaining high retained accuracy across multiple benchmark datasets.

## Executive Summary
DUCK presents a novel machine unlearning algorithm that addresses the challenge of selectively removing information about specific data samples from trained deep learning models. The algorithm employs metric learning to guide the removal process, directing embeddings of forget-samples towards the nearest incorrect class centroid in the embedding space. By introducing a dual loss structure and an adaptive stopping rule, DUCK achieves superior performance in both class removal and homogeneous sampling removal scenarios while introducing a novel Adaptive Unlearning Score (AUS) metric to quantify the trade-off between forget-set accuracy and overall test accuracy.

## Method Summary
DUCK operates by first computing centroids in the embedding space for each class in the retained dataset. During the unlearning process, it minimizes the distance between the embeddings of forget-samples and the nearest incorrect class centroid using a forget loss (LFGT), while simultaneously applying a retain loss (LRET) to preserve overall classification accuracy. The algorithm employs an adaptive stopping criterion based on forget-set accuracy, halting training once this metric falls below a predefined threshold. DUCK also introduces a novel Membership Inference Attack specifically designed to verify the effectiveness of the unlearning process in erasing previously acquired knowledge about forget-samples.

## Key Results
- Achieves superior forget-set accuracy compared to state-of-the-art unlearning methods across CIFAR10, CIFAR100, TinyImagenet, and VGGFace2 datasets
- Maintains high retained accuracy (Ar) and test accuracy (At) while effectively removing forget-set information
- Introduces Adaptive Unlearning Score (AUS) metric that better quantifies the trade-off between forget-set accuracy and overall test accuracy compared to existing metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Moving forget-sample embeddings toward the closest incorrect class centroid erases their association with the correct class while minimizing damage to retained samples.
- Mechanism: During unlearn iterations, each forget sample's embedding is pulled toward the centroid of the nearest wrong class, reducing intra-class similarity and increasing inter-class distance for forget samples.
- Core assumption: Embedding geometry remains meaningful after partial unlearning and the nearest-incorrect-centroid direction provides a safe gradient for forgetting.
- Evidence anchors:
  - [abstract] "DUCK initially computes centroids in the embedding space for each class in the dataset. Then, it minimizes the distance between the embeddings of forget-samples and the incorrect-class closest centroid in the embedding space."
  - [section] "Minimizing the distance between Ψθ(xj) and c∗j (Eq.3) enables DUCK to operate on a single forget-samplesxj rather than the entire class of xj."
  - [corpus] Weak; no corpus neighbor discusses centroid kinematics explicitly.
- Break condition: If class centroids collapse or become ambiguous, the "closest incorrect" assignment becomes unstable and gradients may corrupt retained embeddings.

### Mechanism 2
- Claim: The dual loss structure (LFGT + LRET) preserves overall classification accuracy while driving forget samples toward incorrect classes.
- Mechanism: LFGT pulls forget embeddings toward wrong centroids; LRET pulls retained embeddings toward correct centroids and updates the final layer; λ weights balance the two forces.
- Core assumption: The model's decision boundary can be reshaped by selectively pulling forget samples without over-fitting or destroying general class structure.
- Evidence anchors:
  - [abstract] "DUCK introduces a novel Adaptive Unlearning Score (AUS) metric that quantifies the trade-off between forget-set accuracy and overall test accuracy."
  - [section] "The retain-loss considers the entire network Φθ = Γθ ◦ Ψθ and entails computing the cross-entropy loss of the retain-set batch samples... This loss serves as a knowledge-preserving method for the unlearning algorithm since it counterbalances the effect of the LF GT on the backbone Ψθ weights."
  - [corpus] No direct corpus support; neighbors focus on unlearning methods but not on this dual-loss balance.
- Break condition: If λ1 is too large relative to λ2, retained accuracy collapses; if λ2 dominates, forget-set retention remains high.

### Mechanism 3
- Claim: The stopping rule based on forget-set training accuracy (A∗f) ensures complete erasure without unnecessary retraining.
- Mechanism: Training halts once forget-set accuracy drops below A∗f (1% for CR, oracle accuracy for HR), preventing over-fitting to retained data.
- Core assumption: Forget-set accuracy is a reliable proxy for successful unlearning and can be monitored in real time.
- Evidence anchors:
  - [abstract] "The number of epochs in the unlearning algorithm is not predetermined; instead, it depends on the accuracy Af on the train forget-set Df computed at the conclusion of each epoch."
  - [section] "Once the computed Af accuracy falls below the optimal value, the unlearning process is halted."
  - [corpus] Weak; corpus papers discuss unlearning stopping but not this exact adaptive rule.
- Break condition: If forget-set accuracy plateaus above A∗f due to memorization, the algorithm may terminate prematurely with residual knowledge.

## Foundational Learning

- Concept: Embedding space geometry and centroid computation
  - Why needed here: DUCK relies on accurate class centroids in the latent space to direct forget samples away from correct classes.
  - Quick check question: Given a set of embeddings from class C, how would you compute its centroid vector?

- Concept: Metric learning and cosine distance
  - Why needed here: The algorithm uses cosine distance to select the nearest incorrect centroid and to minimize distance during forgetting.
  - Quick check question: Why might cosine distance be preferred over Euclidean distance in high-dimensional embedding spaces?

- Concept: Adaptive stopping criteria based on accuracy monitoring
  - Why needed here: DUCK's stopping rule depends on tracking forget-set accuracy in real time to balance erasure and retention.
  - Quick check question: What potential pitfalls exist if you use test accuracy instead of training accuracy for this stopping rule?

## Architecture Onboarding

- Component map:
  - Backbone (Ψθ) -> Head (Γθ) -> Centroid store -> Loss modules (LFGT, LRET) -> Stopping monitor

- Critical path:
  1. Precompute centroids on retained set
  2. For each batch: compute closest incorrect centroid per forget sample
  3. Apply LFGT to pull forget embeddings toward wrong centroid
  4. Apply LRET to preserve retained accuracy
  5. Check stop condition; repeat until met

- Design tradeoffs:
  - Batch size vs. centroid stability: larger batches improve centroid stability but increase memory
  - λ1/λ2 balance: high λ1 risks destroying retained accuracy; low λ1 slows forgetting
  - Batch ratio: higher values bias toward more forget updates but risk over-fitting on small forget set

- Failure signatures:
  - Forget accuracy stalls above threshold: λ1 too low or centroids poorly defined
  - Retained accuracy collapses: λ2 too low or forget gradients dominate
  - Slow convergence: batch ratio too high relative to forget set size

- First 3 experiments:
  1. Run on CIFAR10 with one class removed; verify forget accuracy → 0 and retained accuracy ≈ original.
  2. Vary λ1/λ2 ratio; observe impact on forgetting speed vs. retained accuracy.
  3. Test stopping rule sensitivity: change A∗f and confirm algorithm halts appropriately.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Adaptive Unlearning Score (AUS) metric compare to existing unlearning evaluation metrics in terms of sensitivity to different types of unlearning failures?
- Basis in paper: [explicit] The paper introduces AUS as a novel metric to quantify the trade-off between forget-set accuracy and overall test accuracy, but does not compare it to existing metrics.
- Why unresolved: The paper only demonstrates AUS on their proposed method and some baselines, without benchmarking against other unlearning evaluation metrics like F1-score or Area Under the Curve (AUC).
- What evidence would resolve it: Empirical comparison of AUS with other unlearning metrics on a diverse set of unlearning algorithms and datasets, showing relative strengths and weaknesses.

### Open Question 2
- Question: Can the DUCK algorithm be extended to handle unlearning in multi-modal deep learning models?
- Basis in paper: [inferred] The paper focuses on image classification tasks and mentions the increasing prevalence of multi-modal models as a future direction, but does not explore unlearning in this context.
- Why unresolved: The algorithm is specifically designed for metric learning in image embeddings and its applicability to other modalities (e.g., text, audio) is not investigated.
- What evidence would resolve it: Demonstration of DUCK's effectiveness on unlearning tasks in multi-modal models (e.g., CLIP-like architectures) with quantitative comparisons to existing methods.

### Open Question 3
- Question: What is the impact of different centroid initialization strategies on the performance of DUCK?
- Basis in paper: [inferred] The paper uses mean embedding vectors for centroid calculation but does not explore alternative initialization methods or their effects on unlearning performance.
- Why unresolved: The choice of centroid initialization could significantly affect the optimization landscape and the effectiveness of the closest-centroid matching strategy.
- What evidence would resolve it: Comparative analysis of DUCK's performance using different centroid initialization techniques (e.g., k-means, random initialization) on various datasets and unlearning scenarios.

## Limitations

- The stopping rule based on forget-set training accuracy may not be robust across different data distributions or model architectures, potentially leading to premature termination with residual knowledge.
- The algorithm's effectiveness depends on the stability of class centroids in the embedding space, which may become problematic when class boundaries are ambiguous or overlap significantly.
- The Membership Inference Attack implementation details are sparse, making it difficult to assess whether this verification method reliably confirms complete erasure across different datasets and forgetting scenarios.

## Confidence

- **High confidence**: The core mechanism of pulling forget-sample embeddings toward nearest incorrect class centroids is well-specified and theoretically sound. The dual loss structure (LFGT + LRET) has clear mathematical formulation.
- **Medium confidence**: The effectiveness of the stopping rule and the AUS metric depends on empirical validation across diverse scenarios. The paper shows strong results but doesn't thoroughly explore edge cases where the algorithm might fail.
- **Low confidence**: The Membership Inference Attack implementation details are sparse, making it difficult to assess whether this verification method reliably confirms complete erasure across different datasets and forgetting scenarios.

## Next Checks

1. **Centroid stability test**: Run DUCK on a synthetic dataset where class centroids are known to be well-separated vs. overlapping, and measure how centroid instability affects forgetting performance and convergence speed.

2. **A*f threshold sensitivity**: Systematically vary the forget-set accuracy threshold A*f from 0.5% to 5% and measure the trade-off between retained accuracy degradation and residual forget-set knowledge across multiple datasets.

3. **MIA robustness validation**: Implement the Membership Inference Attack on DUCK-unlearned models and test whether an attacker can reliably detect forget-samples using different attack models (SVM vs. neural network) and feature representations.