---
ver: rpa2
title: Towards Generalist Biomedical AI
arxiv_id: '2307.14334'
source_url: https://arxiv.org/abs/2307.14334
tags:
- med-palm
- report
- tasks
- medical
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Med-PaLM Multimodal (Med-PaLM M), a generalist
  biomedical AI system trained to handle diverse biomedical data types and tasks.
  It is a large multimodal generative model built on PaLM-E and finetuned on MultiMedBench,
  a new benchmark spanning 14 tasks including medical question answering, visual question
  answering, medical image classification, radiology report generation/summarization,
  and genomic variant calling.
---

# Towards Generalist Biomedical AI

## Quick Facts
- arXiv ID: 2307.14334
- Source URL: https://arxiv.org/abs/2307.14334
- Reference count: 40
- Key outcome: Med-PaLM M achieves state-of-the-art or competitive performance on 14 biomedical tasks without task-specific customization

## Executive Summary
This paper introduces Med-PaLM Multimodal (Med-PaLM M), a generalist biomedical AI system that can handle diverse biomedical data types and tasks. Built on the PaLM-E architecture and finetuned on MultiMedBench, a new benchmark spanning 14 tasks, Med-PaLM M demonstrates state-of-the-art or competitive performance across medical question answering, visual question answering, medical image classification, radiology report generation/summarization, and genomic variant calling. The system shows benefits of scaling for language-heavy tasks, emergent zero-shot reasoning and generalization capabilities, and encouraging clinician evaluations of chest X-ray report quality, with generated reports preferred over radiologists in up to 40.50% of cases.

## Method Summary
Med-PaLM M is developed by finetuning and aligning the PaLM-E model to the biomedical domain using MultiMedBench, a new multimodal biomedical benchmark. The model is trained on a mixture of tasks including medical question answering, visual question answering, medical image classification, radiology report generation/summarization, and genomic variant calling. Instruction tuning is performed with one-shot exemplars, and the model is evaluated across different scales (12B, 84B, 562B parameters) to assess performance and emergent capabilities.

## Key Results
- Med-PaLM M achieves state-of-the-art or competitive performance on all 14 tasks in MultiMedBench without task-specific customization
- Larger model scales (562B) show consistent improvements on language-heavy tasks
- Generated chest X-ray reports were preferred over radiologist reports in up to 40.50% of cases in human evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Med-PaLM M achieves generalist biomedical performance by finetuning a pretrained multimodal foundation model on diverse biomedical tasks.
- Mechanism: The model leverages PaLM-E's architecture—integrating a large language model with a vision transformer—and is adapted to the biomedical domain through multitask finetuning on MultiMedBench. This allows it to handle various modalities (text, imaging, genomics) and tasks without task-specific customization.
- Core assumption: The PaLM-E architecture, pretrained on vision-language tasks, can be effectively adapted to biomedical data through finetuning.
- Evidence anchors:
  - [abstract]: "Med-PaLM Multimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI system... is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same set of model weights."
  - [section]: "Med-PaLM M is developed by finetuning and aligning the PaLM-E model to the biomedical domain using MultiMedBench."
- Break condition: If the biomedical data distribution is too different from the pretraining data, finetuning may not transfer effectively, or if the model lacks sufficient scale to capture complex biomedical reasoning.

### Mechanism 2
- Claim: Zero-shot generalization to novel medical concepts and tasks is enabled by the model's emergent reasoning capabilities.
- Mechanism: Through large-scale pretraining and multitask finetuning, Med-PaLM M develops the ability to reason about unseen medical concepts (e.g., tuberculosis detection) and perform novel tasks (e.g., two-view chest X-ray report generation) without explicit training on those tasks.
- Core assumption: The combination of large-scale language modeling and multimodal pretraining induces emergent reasoning capabilities applicable to new biomedical scenarios.
- Evidence anchors:
  - [abstract]: "We also report examples of zero-shot generalization to novel medical concepts and tasks, emergent zero-shot medical reasoning."
  - [section]: "We performed an ablation study where we trained a Med-PaLM M 84B variant by excluding the MIMIC-CXR classification tasks from the task mixture. We compared this model variant to the Med-PaLM M 84B variant trained on the complete MultiMedBench mixture on the chest X-ray report generation task..."
- Break condition: If the emergent reasoning capability is not robust, the model may fail on novel tasks, especially those requiring domain-specific knowledge not captured during pretraining.

### Mechanism 3
- Claim: Positive transfer across tasks results from joint training on diverse biomedical tasks within a single model.
- Mechanism: By training on a mixture of tasks (medical question answering, image classification, report generation, etc.), the model learns shared representations and knowledge that improve performance on individual tasks beyond what single-task training would achieve.
- Core assumption: The tasks in MultiMedBench share underlying biomedical knowledge that the model can leverage across tasks.
- Evidence anchors:
  - [abstract]: "We also report examples of... positive transfer learning across tasks..."
  - [section]: "We performed an ablation study where we trained a Med-PaLM M 84B variant by excluding the MIMIC-CXR classification tasks from the task mixture and compared this model variant against Med-PaLM M 84B trained on the full MultiMedBench mixture."
- Break condition: If tasks are too dissimilar or if the model cannot effectively share knowledge across tasks, transfer learning may not occur or may even be negative.

## Foundational Learning

- Concept: Multimodal foundation models
  - Why needed here: Understanding how models like PaLM-E integrate text and vision modalities is crucial for grasping how Med-PaLM M processes biomedical data.
  - Quick check question: How does PaLM-E enable processing of both text and image inputs within the same model?

- Concept: Transfer learning and finetuning
  - Why needed here: Med-PaLM M's performance relies on adapting a pretrained model to the biomedical domain, requiring knowledge of how finetuning affects model capabilities.
  - Quick check question: What are the key differences between pretraining and finetuning, and how do they impact model performance on specialized tasks?

- Concept: Emergent capabilities in large models
  - Why needed here: Zero-shot generalization and novel reasoning abilities are central to Med-PaLM M's value proposition, necessitating an understanding of how these emerge from scale.
  - Quick check question: What evidence supports the idea that scaling up model size can lead to emergent abilities not present in smaller models?

## Architecture Onboarding

- Component map: PaLM-E backbone (LLM + ViT) -> Multimodal tokenization -> Biomedical finetuning on MultiMedBench -> Generative output space for all tasks
- Critical path: Data preprocessing (resize images, tokenization) -> Multitask finetuning (instruction prompting, mixture ratios) -> Evaluation across tasks and model scales
- Design tradeoffs: Using a single model for all tasks simplifies deployment but may limit performance on specialized tasks compared to dedicated models; larger models improve language-heavy tasks but are computationally expensive.
- Failure signatures: Poor performance on tasks requiring fine-grained visual understanding (e.g., mammography classification) may indicate vision encoder limitations; inability to generalize to novel tasks may suggest insufficient emergent reasoning.
- First 3 experiments:
  1. Evaluate Med-PaLM M's performance on a held-out subset of MultiMedBench tasks to assess generalization.
  2. Perform an ablation study removing specific task types to measure transfer learning effects.
  3. Test zero-shot performance on a novel biomedical dataset (e.g., tuberculosis detection) to probe emergent capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would scaling up both the vision encoder and language model components affect performance on medical image classification tasks compared to scaling only the language model?
- Basis in paper: [explicit] The paper notes that scaling the language model has limited benefits for image classification tasks where nuanced visual understanding is required but minimal language reasoning is needed, suggesting the vision encoder may be a bottleneck.
- Why unresolved: The study only scaled the language model while keeping the vision encoder fixed, leaving open the question of how joint scaling would impact performance.
- What evidence would resolve it: Experiments training Med-PaLM M variants with progressively larger vision encoders and language models on classification tasks like PAD-UFES-20 and VinDr-Mammo would reveal if joint scaling provides multiplicative benefits over scaling either component alone.

### Open Question 2
- Question: Would training Med-PaLM M on a larger, more diverse corpus of multimodal biomedical data improve its zero-shot generalization capabilities?
- Basis in paper: [explicit] The paper suggests that the limited size and modality diversity of MultiMedBench may restrict the model's ability to generalize to novel tasks and concepts.
- Why unresolved: The study used a fixed training dataset, so the impact of training on more extensive and varied biomedical data remains untested.
- What evidence would resolve it: Training and evaluating Med-PaLM M on progressively larger multimodal biomedical datasets, then testing its zero-shot performance on tasks like TB detection and novel report generation scenarios, would indicate if scale and diversity enhance generalization.

### Open Question 3
- Question: How would incorporating retrieval mechanisms or tool use affect the performance and applicability of Med-PaLM M in clinical settings?
- Basis in paper: [inferred] The paper discusses the potential of generalist biomedical AI systems but acknowledges limitations in handling complex, multi-modal tasks and the need for safety and equity considerations in real-world applications.
- Why unresolved: The study focused on a purely generative approach, so the benefits of integrating retrieval or tool use are not explored.
- What evidence would resolve it: Developing and evaluating Med-PaLM M variants that use retrieval-augmented generation or tool use for tasks like radiology report generation and medical question answering would reveal if these approaches improve accuracy, reduce hallucinations, and increase clinical utility.

## Limitations

- Performance gaps exist on vision-heavy tasks like mammography classification where Med-PaLM M underperforms specialized models by up to 11.7% AUROC
- Human evaluation results are based on a limited sample of 99 reports from a single center
- Emergent zero-shot capabilities remain largely qualitative with limited systematic evaluation of reliability

## Confidence

**High Confidence:**
- Med-PaLM M successfully integrates multimodal biomedical data through finetuning of the PaLM-E architecture on MultiMedBench
- The model achieves competitive or state-of-the-art performance across the 14 tasks in MultiMedBench
- Larger model scales (562B) show consistent improvements on language-heavy tasks

**Medium Confidence:**
- Positive transfer learning occurs across the diverse task mixture in MultiMedBench
- Emergent zero-shot reasoning capabilities generalize to novel medical concepts and tasks
- Human evaluators show preference for model-generated chest X-ray reports over radiologist reports in up to 40.50% of cases

**Low Confidence:**
- The model demonstrates robust clinical reasoning equivalent to specialist models across all biomedical domains
- Zero-shot generalization capabilities are consistently reliable across diverse clinical scenarios
- The model can replace specialist models in high-stakes medical decision-making contexts

## Next Checks

1. **External Clinical Validation**: Conduct a multi-center clinical study evaluating Med-PaLM M's performance on real-world biomedical data not seen during training, including diverse patient populations and clinical settings. This would validate the model's generalization capabilities beyond the curated MultiMedBench benchmark.

2. **Safety and Reliability Assessment**: Implement systematic evaluation of the model's performance on edge cases, adversarial examples, and clinically critical scenarios. This includes testing the model's ability to recognize its limitations and defer to human expertise when appropriate, addressing potential safety concerns in real-world deployment.

3. **Longitudinal Performance Monitoring**: Establish a framework for tracking the model's performance over time as new biomedical data becomes available, including regular benchmarking against evolving state-of-the-art specialist models. This would assess the sustainability of the generalist approach as biomedical knowledge continues to expand.