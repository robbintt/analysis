---
ver: rpa2
title: 'Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and
  Model Alignment in New Languages'
arxiv_id: '2310.04799'
source_url: https://arxiv.org/abs/2310.04799
tags:
- chat
- vector
- llama2
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simple approach called "chat vector" to
  equip large language models with instruction following and human value alignment
  in new languages. The chat vector is derived by subtracting the weights of a pre-trained
  base model from those of its corresponding chat model.
---

# Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages

## Quick Facts
- arXiv ID: 2310.04799
- Source URL: https://arxiv.org/abs/2310.04799
- Authors: 
- Reference count: 14
- One-line primary result: Chat vector approach enables LLMs to acquire instruction-following and human alignment capabilities in new languages without additional training

## Executive Summary
This paper introduces the "chat vector" approach, a simple method to equip large language models with instruction following and human value alignment capabilities in new languages. The chat vector is derived by subtracting the weights of a pre-trained base model from those of its corresponding chat model, and adding this vector to a continually pre-trained model's weights. The approach is evaluated on Traditional Chinese, Korean, and Simplified Chinese, demonstrating superior efficacy in instruction following, toxicity mitigation, and multi-turn dialogue.

## Method Summary
The chat vector approach involves deriving a vector by subtracting the weights of a pre-trained base model (e.g., LLaMA2) from its corresponding chat model (e.g., LLaMA2-chat). This vector is then added to a continually pre-trained model's weights to endow it with chat capabilities in new languages. The approach eliminates the need for further training, such as supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF), making it computationally efficient.

## Key Results
- The chat vector approach demonstrates superior efficacy in instruction following, toxicity mitigation, and multi-turn dialogue for Traditional Chinese, Korean, and Simplified Chinese.
- Chat vector addition is computationally efficient compared to traditional RLHF methods, significantly reducing the cost of enabling chat capabilities in new languages.
- The approach is simple, effective, and widely applicable for efficiently enabling conversational capabilities in pre-trained language models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Chat vector addition allows LLMs to acquire instruction-following and human alignment capabilities in new languages without additional training.
- **Mechanism:** The chat vector is derived by subtracting the weights of a pre-trained base model from its corresponding chat model. Adding this vector to a continually pre-trained model's weights endows it with chat capabilities.
- **Core assumption:** The chat vector captures the essential differences between a base model and its chat-enhanced counterpart, which can be transferred to other models to enable similar capabilities.
- **Evidence anchors:**
  - [abstract] "By adding the chat vector to a continual pre-trained model's weights, the model can be endowed with chat capabilities in new languages without the need for further training."
  - [section] "The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat)."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.429, average citations=0.0." (Weak evidence)
- **Break condition:** The chat vector fails to transfer capabilities if the base models are too different or if the chat vector becomes outdated due to model updates.

### Mechanism 2
- **Claim:** Chat vector enables toxicity mitigation and multi-turn dialogue capabilities in LLMs.
- **Mechanism:** The chat vector contains information about the instruction-following and alignment aspects learned during the RLHF process. Adding this vector to a pre-trained model allows it to generate safer responses and engage in multi-turn conversations.
- **Core assumption:** The chat vector encodes the alignment criteria and conversational patterns learned by the chat model, which can be transferred to other models.
- **Evidence anchors:**
  - [abstract] "Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue."
  - [section] "We assess the efficacy of the chat vector across multiple target languages, focusing primarily on Traditional Chinese, by considering three aspects: toxicity, the ability to follow instructions and multi-turn dialogue."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.429, average citations=0.0." (Weak evidence)
- **Break condition:** The chat vector fails to mitigate toxicity or enable multi-turn dialogue if the chat model's training data or RLHF process is insufficient or biased.

### Mechanism 3
- **Claim:** Chat vector addition is computationally efficient compared to traditional RLHF methods.
- **Mechanism:** Instead of retraining a model with SFT and RLHF, the chat vector allows for a simple addition of pre-computed weights, significantly reducing the computational cost.
- **Core assumption:** The chat vector captures the essential alignment and instruction-following capabilities, making additional training unnecessary.
- **Evidence anchors:**
  - [abstract] "Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue."
  - [section] "Motivated by ChatGPT's achievements, numerous researchers and pioneers targeting Artificial General Intelligence (AGI) have embarked on creating similar conversational models."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.429, average citations=0.0." (Weak evidence)
- **Break condition:** The chat vector addition becomes inefficient if the vector size grows too large or if the model architecture changes significantly.

## Foundational Learning

- **Concept:** Continual pre-training (CP)
  - **Why needed here:** To enhance the model's understanding and generation capabilities in the target language before adding the chat vector.
  - **Quick check question:** What is the primary goal of continual pre-training in the context of the chat vector approach?

- **Concept:** Supervised Fine-Tuning (SFT)
  - **Why needed here:** To sharpen task-specific performance and ensure instruction-following capabilities in the target language.
  - **Quick check question:** How does SFT differ from the chat vector approach in terms of aligning LLMs with human preferences?

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - **Why needed here:** To refine LLMs by using human feedback, tackling challenges such as misinformation and harmful expressions.
  - **Quick check question:** Why is RLHF considered more complex and computationally intensive compared to the chat vector approach?

## Architecture Onboarding

- **Component map:** Pre-trained base model -> Chat model -> Chat vector derivation -> Continually pre-trained model -> Chat vector addition
- **Critical path:** Continual pre-training → Chat vector derivation → Chat vector addition → Evaluation of instruction following, toxicity mitigation, and multi-turn dialogue.
- **Design tradeoffs:** The chat vector approach trades off some control over the alignment process for computational efficiency. It relies on the quality and relevance of the chat vector derived from the base and chat models.
- **Failure signatures:** If the chat vector fails to transfer capabilities, the model may generate unsafe responses, fail to follow instructions, or struggle with multi-turn dialogue.
- **First 3 experiments:**
  1. Evaluate the instruction-following ability of the model with and without the chat vector using the Vicuna Benchmark.
  2. Assess the toxicity mitigation capabilities using the Real Toxicity Prompts dataset and Perspective API.
  3. Test the multi-turn dialogue proficiency by comparing the model's performance with and without the chat vector in a case study.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal coefficient for the chat vector to achieve the best balance between instruction following, toxicity mitigation, and avoiding excessive English responses?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions that the magnitude of the chat vector can severely affect model performance, and while a weight of 0.5 mitigated excessive English responses for the Chinese-LLaMA, it reduced instruction following and toxicity mitigation abilities. The optimal coefficient likely depends on the specific language and task, and requires further research to determine.
- What evidence would resolve it: Systematic experiments varying the chat vector coefficient across different languages and tasks, evaluating the trade-offs between instruction following, toxicity mitigation, and language preservation.

### Open Question 2
- Question: Can the chat vector approach be extended to multimodal models, and if so, how would it need to be adapted?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on text-based language models, but the concept of the chat vector could potentially be applied to multimodal models. However, the adaptation would likely require significant modifications to account for the additional modalities and their interactions.
- What evidence would resolve it: Experiments applying the chat vector approach to multimodal models, demonstrating its effectiveness and identifying any necessary adaptations.

### Open Question 3
- Question: What are the limitations of the chat vector approach in terms of the types of tasks and capabilities it can effectively transfer?
- Basis in paper: [explicit]
- Why unresolved: The paper demonstrates the chat vector's effectiveness for instruction following, toxicity mitigation, and multi-turn dialogue, but it does not explore its limitations or applicability to other tasks and capabilities.
- What evidence would resolve it: Experiments testing the chat vector's effectiveness on a wider range of tasks and capabilities, identifying any limitations or areas where it is less effective.

## Limitations
- The chat vector approach's effectiveness is primarily demonstrated on Chinese and Korean languages, with limited testing on other language families.
- The approach assumes that the chat vector captures universal alignment and instruction-following capabilities, which may not hold true for languages with vastly different structures or cultural contexts.
- The paper does not address potential degradation of performance when applying chat vectors to smaller models or models with different architectures.

## Confidence
- **High Confidence:** The chat vector approach significantly reduces computational costs compared to traditional RLHF methods for enabling chat capabilities in new languages.
- **Medium Confidence:** The chat vector effectively transfers instruction-following and toxicity mitigation capabilities across Chinese and Korean languages.
- **Low Confidence:** The approach generalizes to languages outside the Chinese-Korean language family and maintains performance across diverse model architectures.

## Next Checks
1. Evaluate the chat vector approach on languages from different families (e.g., Romance, Semitic, or African languages) to test generalizability.
2. Test the approach on smaller model architectures to assess performance degradation and scalability.
3. Conduct a sensitivity analysis on the chat vector derivation process, varying the base and chat model pairs to understand the impact on transferability.