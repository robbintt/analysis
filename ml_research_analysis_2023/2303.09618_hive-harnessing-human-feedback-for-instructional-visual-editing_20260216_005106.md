---
ver: rpa2
title: 'HIVE: Harnessing Human Feedback for Instructional Visual Editing'
arxiv_id: '2303.09618'
source_url: https://arxiv.org/abs/2303.09618
tags:
- image
- reward
- human
- hive
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents HIVE, a framework that incorporates human feedback
  to improve instructional image editing models. The authors collect human feedback
  on edited images and learn a reward function to capture user preferences.
---

# HIVE: Harnessing Human Feedback for Instructional Visual Editing

## Quick Facts
- **arXiv ID:** 2303.09618
- **Source URL:** https://arxiv.org/abs/2303.09618
- **Reference count:** 40
- **Primary result:** HIVE significantly improves instructional image editing by incorporating human feedback through reward learning, outperforming previous state-of-the-art approaches.

## Executive Summary
HIVE addresses the challenge of aligning edited images with user instructions and preferences in image editing tasks. The framework collects human feedback on edited images and learns a reward function to capture user preferences, then introduces scalable diffusion model fine-tuning methods that incorporate these preferences. By using human feedback rather than relying solely on supervised learning, HIVE achieves better alignment between edited images and instructions while maintaining image quality and consistency. The authors contribute new datasets totaling 1M training examples, 3.6K reward examples, and 1K evaluation examples to boost performance.

## Method Summary
HIVE fine-tunes diffusion models for instructional image editing by first collecting human rankings of variant outputs for each image-instruction pair. These rankings train a reward model (based on BLIP architecture) that predicts human preferences. The framework then fine-tunes the diffusion model using either weighted reward loss or conditional reward loss, avoiding expensive sampling-based reinforcement learning. Cycle consistency augmentation is applied to invertible instructions to improve bidirectional editing capabilities. The approach scales to large datasets while maintaining computational efficiency through an offline RL formulation.

## Key Results
- HIVE outperforms previous state-of-the-art instructional image editing approaches by a large margin
- Achieves better image consistency and quality while maintaining instruction alignment
- Successfully handles 29.1% of invertible instructions through cycle consistency augmentation
- Demonstrates significant improvements in quantitative metrics (CLIP similarity) and qualitative user studies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Human feedback improves alignment between edited images and user instructions by providing preference signals that capture nuanced user intent beyond explicit textual instructions.
- **Mechanism:** The framework collects human rankings of variant outputs for each image-instruction pair, trains a reward model to predict these preferences, and then fine-tunes the diffusion model to maximize this learned reward function.
- **Core assumption:** Human preference rankings can be effectively modeled by a reward function that generalizes to unseen image-instruction pairs.
- **Evidence anchors:** [abstract] "We collect human feedback on the edited images and learn a reward function to capture the underlying user preferences."

### Mechanism 2
- **Claim:** The weighted reward loss function enables efficient fine-tuning of diffusion models without requiring expensive sampling-based reinforcement learning approaches.
- **Mechanism:** Instead of using on-policy RL methods that would require sampling full denoising chains for each gradient update, the framework derives an offline RL approach where the optimal target distribution is proportional to the original data distribution weighted by the exponential of the reward.
- **Core assumption:** The reward-weighted data distribution can be approximated well by the original training data with appropriate weights.
- **Evidence anchors:** [abstract] "We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward."

### Mechanism 3
- **Claim:** Cycle consistency augmentation improves the model's ability to handle bidirectional editing instructions by explicitly training on inverted instruction pairs.
- **Mechanism:** The framework identifies invertible instructions (like "add a dog" and "remove the dog"), generates paired edits in both directions, and trains the model to maintain consistency when applying and then reversing edits.
- **Core assumption:** A significant portion of editing instructions are invertible, and training on both forward and reverse pairs improves the model's understanding of object relationships and editing boundaries.
- **Evidence anchors:** [section] "Our analysis revealed that 29.1% of the instructions in the dataset were invertible."

## Foundational Learning

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The framework adapts RLHF techniques from language models to image editing, using human preference data to guide the fine-tuning process rather than relying solely on supervised learning from paired data.
  - Quick check question: How does the Bradley-Terry model used for pairwise preference learning differ from direct regression of reward values, and why is it more appropriate for this application?

- **Concept:** Diffusion Model Fine-Tuning
  - Why needed here: Understanding how diffusion models are typically trained (via denoising autoencoder objectives) is crucial for grasping why standard RL approaches are computationally prohibitive and why the weighted reward approach is necessary.
  - Quick check question: What specific computational challenges arise when trying to apply on-policy RL methods to diffusion models that have 1000+ denoising steps per sample?

- **Concept:** Vision-Language Model Integration
  - Why needed here: The reward model architecture combines image and text embeddings through cross-attention mechanisms, requiring understanding of how multimodal models like BLIP process and align visual and textual information.
  - Quick check question: How does the cross-attention layer between self-attention and feed-forward networks in the image-grounded text encoder enable the reward model to effectively judge alignment between instructions and edited images?

## Architecture Onboarding

- **Component map:** Data Pipeline (1M training triplets + 3.6K reward dataset + 1K evaluation dataset) → Reward Model (BLIP-based with visual transformer + image-grounded text encoder + linear layer) → Fine-tuning Modules (weighted reward loss and conditional reward loss) → Cycle Consistency Module (rule-based system for identifying and generating bidirectional instruction pairs)

- **Critical path:** Input image → Instruction → Diffusion model generation → Human ranking → Reward model training → Fine-tuning with reward-weighted loss → Improved edited images

- **Design tradeoffs:**
  - Using human feedback adds significant annotation cost but provides crucial alignment signals that supervised data lacks
  - The offline RL approach trades some theoretical optimality for computational efficiency, making the fine-tuning feasible
  - The cycle consistency augmentation increases dataset size by ~29% but significantly improves bidirectional editing capabilities

- **Failure signatures:**
  - Poor reward model generalization: The fine-tuned model shows inconsistent behavior on instructions similar to training data but with different visual contexts
  - Reward collapse: The model over-optimizes for high-reward patterns that don't generalize to meaningful edits
  - Instruction misunderstanding: The model consistently misinterprets certain instruction patterns (e.g., spatial reasoning, counting)

- **First 3 experiments:**
  1. **Reward model validation:** Generate 100 random image-instruction pairs, produce 5 variants each, have human annotators rank them, and evaluate if the trained reward model's rankings correlate with human preferences (Spearman correlation > 0.7 expected)
  2. **Weighted vs conditional loss comparison:** Fine-tune two identical models using each loss type on a small subset of data (10K pairs), evaluate on held-out validation set using CLIP similarity metrics and human preference studies
  3. **Cycle consistency ablation:** Train models with and without cycle consistency augmentation on 50K pairs, test on bidirectional instruction pairs to measure consistency score improvements (target: >15% improvement)

## Open Questions the Paper Calls Out

- **Question:** How does HIVE handle instructions that involve complex spatial reasoning or object manipulation?
  - **Basis in paper:** [inferred] The paper mentions that counting and spatial reasoning are common failure cases for HIVE, such as understanding instructions like "one", "two", or "on the right".
  - **Why unresolved:** The paper does not provide a detailed analysis of how HIVE performs on instructions that require complex spatial reasoning or object manipulation.
  - **What evidence would resolve it:** Experiments comparing HIVE's performance on instructions with varying levels of spatial complexity, along with a detailed analysis of the failure modes and potential improvements.

- **Question:** How does the choice of reward model architecture impact HIVE's performance?
  - **Basis in paper:** [explicit] The paper describes the reward model architecture, which uses a visual transformer and an image-grounded text encoder, and mentions that it is initialized from a pre-trained BLIP model.
  - **Why unresolved:** The paper does not provide a comprehensive analysis of how different reward model architectures affect HIVE's performance.
  - **What evidence would resolve it:** Experiments comparing HIVE's performance using different reward model architectures, such as varying the visual transformer, text encoder, or initialization method.

- **Question:** How does HIVE's performance scale with the size of the training dataset?
  - **Basis in paper:** [explicit] The paper mentions that they collect a new 1M training dataset and perform ablation studies on the training data size.
  - **Why unresolved:** While the paper provides some ablation studies on the training data size, it does not provide a detailed analysis of how HIVE's performance scales with the size of the training dataset.
  - **What evidence would resolve it:** Experiments comparing HIVE's performance on datasets of varying sizes, with a focus on quantifying the relationship between dataset size and performance metrics such as CLIP similarity and user study results.

## Limitations

- The framework's performance heavily depends on the quality and representativeness of human preference data, which introduces potential biases and scalability constraints
- The offline RL approach, while computationally efficient, may converge to suboptimal solutions compared to on-policy methods
- The cycle consistency augmentation relies on a rule-based system for identifying invertible instructions that may miss nuanced cases or incorrectly classify non-invertible pairs

## Confidence

- **High:** The core mechanism of using human feedback to train reward models and fine-tune diffusion models is well-established in related work
- **Medium:** The specific weighted reward loss derivation and its effectiveness for diffusion model fine-tuning requires further validation
- **Low:** The generalizability of the framework to diverse editing scenarios and long-tail instructions remains uncertain

## Next Checks

1. Conduct a large-scale user study (n=100+) comparing HIVE outputs against baseline models across diverse instruction types and image domains to validate quantitative metrics with human preferences
2. Test the reward model's generalization by evaluating its predictions on held-out instruction categories and measuring performance degradation
3. Evaluate the framework's robustness to instruction ambiguity by systematically varying instruction clarity and measuring output consistency