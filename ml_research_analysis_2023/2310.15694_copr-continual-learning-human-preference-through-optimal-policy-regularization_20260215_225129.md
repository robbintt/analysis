---
ver: rpa2
title: 'COPR: Continual Learning Human Preference through Optimal Policy Regularization'
arxiv_id: '2310.15694'
source_url: https://arxiv.org/abs/2310.15694
tags:
- learning
- human
- reward
- continual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of continual learning for language
  models to align with evolving human preferences, which is challenging due to catastrophic
  forgetting and the high cost of full retraining. The authors propose Continual Optimal
  Policy Fitting (COPF), a method that computes a sequence of optimal policies using
  Monte Carlo estimation and regularizes the current policy based on historically
  optimal distributions.
---

# COPR: Continual Learning Human Preference through Optimal Policy Regularization

## Quick Facts
- arXiv ID: 2310.15694
- Source URL: https://arxiv.org/abs/2310.15694
- Reference count: 40
- Primary result: COPF achieves higher preference scores and better memory stability than strong CL baselines in task and domain incremental learning settings.

## Executive Summary
This paper addresses the problem of continual learning for language models to align with evolving human preferences, which is challenging due to catastrophic forgetting and the high cost of full retraining. The authors propose Continual Optimal Policy Fitting (COPF), a method that computes a sequence of optimal policies using Monte Carlo estimation and regularizes the current policy based on historically optimal distributions. COPF avoids complex reinforcement learning and can learn from unlabeled data by maintaining a scoring module. Experiments on task and domain incremental learning settings show that COPF outperforms strong baselines in consistently aligning with human preferences, achieving higher preference scores and better memory stability. The method is particularly effective when learning from unlabeled prompts after initial training on labeled data.

## Method Summary
COPF computes an optimal policy distribution for each task using Monte Carlo estimation and preference scores, then fits the current policy to this distribution while applying function regularization to prevent forgetting. The method estimates pairwise human preferences as advantage scores following a normal distribution, and for each task, computes the optimal policy using these scores. When learning a new task, COPF fits the current policy to the new optimal distribution and adds a regularization term that penalizes divergence from the optimal policy of previous tasks. The approach can learn from unlabeled prompts by training a reward value head on labeled data, then using this head to score and rank unlabeled responses. COPF uses a small replay buffer (5% of data) and avoids complex reinforcement learning by directly fitting policies to estimated optimal distributions.

## Key Results
- COPF achieves higher preference scores than strong CL baselines in both task incremental learning and domain incremental learning settings
- The method demonstrates better memory stability (lower BWT/FM metrics) compared to baselines while maintaining alignment with human preferences
- COPF effectively learns from unlabeled prompts after initial training on labeled data, maintaining preference alignment without additional human feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: COPF avoids catastrophic forgetting by maintaining a distribution of historically optimal policies and using function regularization to align the current policy with past preferences.
- Mechanism: For each task, COPF computes an optimal policy distribution using Monte Carlo estimation and a preference score. When learning a new task, it fits the current policy to this distribution and adds a regularization term that penalizes divergence from the optimal policy of previous tasks.
- Core assumption: The optimal policy distribution captures sufficient information about past human preferences to guide regularization without requiring exact historical data.
- Evidence anchors:
  - [abstract] "COPF outperforms strong Continuous Learning (CL) baselines when it comes to consistently aligning with human preferences on incremental tasks and domains."
  - [section] "To preserve the old knowledge, we calculate a regularization loss to ensure that the new and old optimal policies do not differ too significantly in terms of the distribution of old human preferences."
- Break condition: If the optimal policy distribution becomes too noisy or biased, the regularization term may either over-constrain or under-constrain the new policy, leading to either forgetting or poor adaptation.

### Mechanism 2
- Claim: COPF sidesteps reward model training and reinforcement learning by directly fitting the policy to a renormalized optimal policy distribution derived from human preference rankings.
- Mechanism: Instead of learning a reward model and optimizing via RL, COPF estimates the optimal policy distribution from pairwise human preferences, then minimizes KL divergence between the current policy and this distribution.
- Core assumption: The pairwise preference data is sufficient to reconstruct the underlying reward function up to a partition function that can be ignored for relative comparisons.
- Evidence anchors:
  - [abstract] "COPF involves a single learning phase and doesn't necessitate complex reinforcement learning."
  - [section] "Next, we directly fit the re-normalized probability P ∗ y∈Y x,t(y|x) by minimizing the KL loss task Tt."
- Break condition: If human preferences are inconsistent or noisy, the estimated optimal policy distribution may not reflect true preferences, leading to misalignment.

### Mechanism 3
- Claim: COPF can learn from unlabeled prompts by using a reward value head trained on labeled data to score and rank unlabeled responses, then applying the hard reward mode on these rankings.
- Mechanism: After initial training on labeled preference data, COPF attaches a reward head that learns to predict preference scores. This head is then used to rank unlabeled responses, enabling continued learning without new human labels.
- Core assumption: The reward head trained on labeled data generalizes sufficiently to rank unlabeled responses in a way that preserves preference alignment.
- Evidence anchors:
  - [abstract] "it shares the capability with RLHF to learn from unlabeled data by maintaining a scoring module, similar to reward model, making it flexible for continually learning without human feedback."
  - [section] "After learning the labeled data, the reward value head can be used to score the unlabeled data, and ranking unlabeled responses."
- Break condition: If the reward head overfits to the labeled domain or the unlabeled prompts differ significantly, the rankings may become unreliable, causing misalignment.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding RLHF is essential because COPF is positioned as an alternative that avoids its complexity while achieving similar alignment.
  - Quick check question: What are the three phases of the RLHF pipeline, and which does COPF eliminate?

- Concept: Continual Learning (CL) and Catastrophic Forgetting
  - Why needed here: COPF is a CL method; knowing how forgetting occurs and how it's measured is critical to understanding its design.
  - Quick check question: How does function regularization in COPF differ from weight regularization in EWC or MAS?

- Concept: Policy Optimization and KL Constraints
  - Why needed here: COPF's core is fitting a policy to an optimal distribution; understanding KL constraints and policy optimization is foundational.
  - Quick check question: In COPF, why is the partition function Z(x) not computed explicitly, and how does this affect optimization?

## Architecture Onboarding

- Component map:
  Base language model (πθ) -> Optimal policy estimator (Monte Carlo + preference scores) -> Function regularization module (KL divergence to past optimal policies) -> Optional reward value head (for soft reward mode) -> Replay buffer (small subset of historical data)

- Critical path:
  1. Estimate optimal policy distribution for current task using preference rankings.
  2. Fit current policy to this distribution via KL loss.
  3. Apply function regularization using stored optimal policy distributions from past tasks.
  4. (Optional) Train reward head on labeled data for soft reward mode.
  5. Use reward head to rank unlabeled responses for continued learning.

- Design tradeoffs:
  - Avoiding RL simplifies training but relies heavily on quality of preference data.
  - Small replay buffer reduces memory but may limit regularization effectiveness.
  - Soft reward mode enables scalability but introduces risk of reward head overfitting.

- Failure signatures:
  - Performance collapse on old tasks → insufficient regularization or outdated optimal policy distributions.
  - Poor adaptation to new tasks → suboptimal fitting to new optimal policy distribution.
  - Reward head producing degenerate rankings → overfitting or domain shift.

- First 3 experiments:
  1. Train COPF on a single task with synthetic preference data; verify KL loss minimization and policy alignment.
  2. Test continual learning across two tasks; measure forgetting via BWT/FM metrics.
  3. Evaluate soft reward mode by training on labeled data, then ranking unlabeled responses and checking alignment stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the variance parameter σ in the normal distribution for advantage score modeling affect the performance of COPF across different domains and tasks?
- Basis in paper: [explicit] The authors mention modeling advantage score distribution as a normal distribution with hyperparameter σ.
- Why unresolved: The paper does not provide experimental results or analysis on the sensitivity of COPF's performance to different values of σ.
- What evidence would resolve it: Conducting experiments with varying σ values and comparing COPF's performance across different domains and tasks would provide insights into the impact of this hyperparameter.

### Open Question 2
- Question: How does COPF compare to other state-of-the-art continual learning methods in terms of computational efficiency and memory usage?
- Basis in paper: [inferred] The paper claims COPF involves a single learning phase and doesn't necessitate complex reinforcement learning, but doesn't provide a direct comparison with other methods in terms of computational resources.
- Why unresolved: The paper focuses on the performance of COPF in terms of preference scores and memory stability but does not discuss its efficiency compared to other methods.
- What evidence would resolve it: A comprehensive comparison of COPF with other methods in terms of computational time, memory usage, and resource requirements would provide a clearer picture of its efficiency.

### Open Question 3
- Question: How does the choice of replay ratio affect the performance of COPF in task incremental learning and domain incremental learning settings?
- Basis in paper: [explicit] The authors mention maintaining a replay memory buffer and using a function regularization strategy to preserve old knowledge.
- Why unresolved: The paper does not provide an analysis of how different replay ratios impact the performance of COPF in various continual learning scenarios.
- What evidence would resolve it: Conducting experiments with different replay ratios and evaluating COPF's performance in task and domain incremental learning settings would help understand the optimal replay ratio for different scenarios.

### Open Question 4
- Question: How does COPF perform in scenarios where the human preference data is imbalanced or biased towards certain domains or tasks?
- Basis in paper: [inferred] The paper constructs a benchmark for continuous learning of human preferences based on different human preference data but does not explicitly address the issue of imbalanced or biased data.
- Why unresolved: The paper does not discuss how COPF handles imbalanced or biased human preference data, which is a common issue in real-world applications.
- What evidence would resolve it: Conducting experiments with imbalanced or biased human preference data and evaluating COPF's performance would provide insights into its robustness in such scenarios.

## Limitations
- Evaluation based on synthetic and curated preference datasets that may not capture real-world preference dynamics
- Small replay buffer (5% of data) may not be sufficient for long task sequences, limiting scalability
- Dependence on high-quality preference data - noisy or inconsistent preferences could degrade performance

## Confidence
- Claim: COPF effectively prevents catastrophic forgetting while adapting to new preferences → Medium-High
- Claim: COPF avoids RL complexity while achieving similar alignment to RLHF → High
- Claim: COPF can learn from unlabeled data without losing alignment → Medium-High

## Next Checks
1. Test COPF on a longer task sequence (>10 tasks) to evaluate whether the 5% replay buffer remains effective or if forgetting emerges with task sequence length.
2. Evaluate COPF on real-world preference data from deployed language models where preferences naturally evolve over time, rather than curated datasets.
3. Compare COPF against simpler continual learning baselines (EWC, MAS) with the same replay buffer size to isolate the benefit of optimal policy regularization versus basic function regularization.