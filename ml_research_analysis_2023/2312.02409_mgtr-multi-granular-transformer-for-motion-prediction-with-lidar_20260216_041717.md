---
ver: rpa2
title: 'MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR'
arxiv_id: '2312.02409'
source_url: https://arxiv.org/abs/2312.02409
tags:
- lidar
- motion
- prediction
- context
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-granular transformer (MGTR) model for
  motion prediction in autonomous driving, leveraging both LiDAR point cloud data
  and traditional map information. MGTR employs a multi-granular approach to encode
  context features from different sources at various resolutions, allowing for better
  learning of context information tailored to different types of agents.
---

# MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR

## Quick Facts
- **arXiv ID:** 2312.02409
- **Source URL:** https://arxiv.org/abs/2312.02409
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art performance on Waymo Open Dataset motion prediction benchmark, ranking first on leaderboard

## Executive Summary
This paper presents a multi-granular transformer (MGTR) model for motion prediction in autonomous driving, leveraging both LiDAR point cloud data and traditional map information. MGTR employs a multi-granular approach to encode context features from different sources at various resolutions, allowing for better learning of context information tailored to different types of agents. Additionally, a motion-aware context search mechanism is introduced to enhance efficiency and accuracy. The proposed model achieves state-of-the-art performance on the Waymo Open Dataset motion prediction benchmark, ranking first on its leaderboard.

## Method Summary
MGTR is a transformer-based model that uses multi-granular encoding of map and LiDAR features to provide context information at resolutions tailored to different agent types. The model incorporates a motion-aware context search mechanism that uses each agent's current velocity to project a future distance and select the most relevant context tokens. The architecture consists of multi-granular encoders, a Transformer encoder with local self-attention, future state enhancement, and a Transformer decoder that outputs multimodal trajectory predictions using a Gaussian Mixture Model.

## Key Results
- Achieves state-of-the-art performance on Waymo Open Dataset motion prediction benchmark
- Demonstrates significant improvements in mean Average Precision (mAP) for pedestrian and cyclist categories
- Ranks first on the Waymo Open Dataset motion prediction leaderboard

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-granular encoding of map and LiDAR features improves motion prediction accuracy by providing context features at resolutions tailored to different agent types.
- Mechanism: The model encodes map elements and LiDAR voxel features at multiple spatial granularities using different sample rates. This creates token sets that capture both coarse and fine-grained context information. Agents with different motion patterns (e.g., pedestrians vs. vehicles) can then attend to context features at the granularity most relevant to their movement characteristics.
- Core assumption: Different types of traffic agents benefit from context information at different spatial resolutions, and the model can learn to selectively attend to the most relevant granularity level for each agent.
- Evidence anchors:
  - [abstract] "MGTR employs a multi-granular approach to encode context features from different sources at various resolutions, allowing for better learning of context information tailored to different types of agents."
  - [section] "Different types of agents have different movement ranges and requirements for map granularity. In this work we extract map contents in a multi-granular manner. Map elements with topological relationships such as road centerlines and area boundaries are sampled evenly at different sample rates, resulting in polylines with different granularities."
  - [corpus] Weak evidence - only 1 neighbor paper (LiMTR) mentions multi-granular feature integration but doesn't provide detailed mechanism comparison.
- Break condition: If the multi-granular encoding doesn't provide meaningful differentiation in feature resolution, or if the attention mechanism fails to learn which granularity level is most relevant for each agent type.

### Mechanism 2
- Claim: Motion-aware context search improves efficiency by reducing the number of context tokens while maintaining prediction accuracy.
- Mechanism: The model uses each agent's current velocity to project a future distance, then selects the nearest map and LiDAR tokens within that projected region. This creates a motion-aware prior that focuses context attention on the most relevant spatial region for each agent's expected movement.
- Core assumption: The most relevant context features for predicting an agent's future trajectory are located near its projected future position based on current velocity, and this projection is a reasonable approximation of the agent's intended path.
- Evidence anchors:
  - [section] "Through the projected position, we acquire eNm nearest map tokens and eNl nearest LiDAR tokens, resulting in a total of Na + eNm + eNl selected tokens that will be fed into our Transformer encoder for further refinement."
  - [section] "For agents with different velocities, the desired positions of scene context for long-horizon trajectory prediction differ significantly. Therefore, for an agent of interest, we use its current velocity to project a future distance as the context token search prior."
  - [corpus] No direct evidence - this appears to be a novel contribution not discussed in neighbor papers.
- Break condition: If the velocity-based projection fails to capture the actual relevant context region, or if the nearest neighbor selection misses important features that would be relevant for unusual agent behaviors.

### Mechanism 3
- Claim: Incorporating LiDAR voxel features as context improves prediction accuracy by providing 3D scene information not available in traditional HD maps.
- Mechanism: The model uses pre-extracted LiDAR voxel features from an off-the-shelf segmentation network, concatenates semantic labels and 3D positions, and processes these through multi-granular pooling to create context tokens. These tokens provide fine-grained 3D environmental information that complements the 2D map data.
- Core assumption: LiDAR-derived context features contain valuable 3D environmental information that significantly improves motion prediction accuracy, and the voxel-based representation is efficient enough for real-time processing.
- Evidence anchors:
  - [abstract] "To further enhance MGTR's capabilities, we leverage LiDAR point cloud data by incorporating LiDAR semantic features from an off-the-shelf LiDAR feature extractor."
  - [section] "In order to obtain richer 3D context information missing in explicit perception outputs and pre-built HD maps, we propose to integrate LiDAR information into our framework."
  - [corpus] Moderate evidence - multiple neighbor papers (Scene Informer, JointMotion) mention LiDAR integration but focus on different aspects rather than context feature incorporation.
- Break condition: If the LiDAR features don't provide meaningful additional information beyond what's available in HD maps, or if the voxel representation introduces too much noise or computational overhead.

## Foundational Learning

- Concept: Vectorized representation of agent trajectories and map elements
  - Why needed here: MGTR builds on the VectorNet approach of representing continuous spatial data as discrete polylines, which enables efficient processing with graph-based and Transformer models
  - Quick check question: How does sampling rate affect the trade-off between geometric detail and computational complexity in vectorized map representations?

- Concept: Multi-head self-attention with local attention mechanism
  - Why needed here: The model uses local self-attention in the encoder to efficiently process large numbers of context tokens while maintaining the ability to capture long-range dependencies through iterative refinement
  - Quick check question: What is the computational complexity difference between global vs. local self-attention when processing thousands of context tokens?

- Concept: Gaussian Mixture Models for multimodal trajectory prediction
  - Why needed here: The decoder models future trajectories as a mixture of Gaussian distributions, allowing the model to capture the inherent uncertainty and multimodal nature of agent behaviors
  - Quick check question: How does the number of mixture components affect the trade-off between prediction diversity and accuracy in GMM-based trajectory prediction?

## Architecture Onboarding

- Component map: Input preprocessing -> Multi-granular encoding -> Motion-aware context search -> Transformer encoder -> Future state enhancement -> Transformer decoder -> GMM output
- Critical path: Input → Multi-granular encoding → Motion-aware context search → Transformer encoder → Future state enhancement → Transformer decoder → GMM output
- Design tradeoffs:
  - Granularity vs. complexity: Higher sample rates provide more detail but increase computational cost and model complexity
  - Context search radius vs. efficiency: Larger search regions capture more context but reduce computational efficiency
  - Number of intention goals vs. prediction quality: More goals capture more behavioral modes but may lead to mode collapse
- Failure signatures:
  - Poor pedestrian predictions: May indicate insufficient LiDAR context features or inappropriate granularity levels for fine-grained movements
  - Vehicle prediction failures: Could suggest inadequate map granularity or insufficient attention to road topology
  - Computational bottlenecks: Likely caused by excessive token counts or inefficient context search parameters
- First 3 experiments:
  1. Ablation study on granularity levels: Test single vs. multi-granular map and LiDAR encoding to verify the benefit of multi-granular representation
  2. Context search radius sweep: Evaluate different values for eNm and eNl to find optimal balance between context coverage and efficiency
  3. Cross-modal attention analysis: Visualize attention weights between agent tokens and different granularity levels to understand how the model uses multi-granular context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-granular representation of LiDAR data compare to other representations (e.g., raw point clouds, voxel features) in terms of prediction accuracy and computational efficiency?
- Basis in paper: [explicit] The paper states that MGTR uses LiDAR voxel features extracted by an off-the-shelf LiDAR segmentation network, and applies average pooling across various scales to obtain features of different granularities.
- Why unresolved: The paper does not provide a direct comparison between the multi-granular representation of LiDAR data and other representations.
- What evidence would resolve it: An experiment comparing the performance of MGTR using different LiDAR data representations (e.g., raw point clouds, voxel features, multi-granular voxel features) on the same benchmark dataset.

### Open Question 2
- Question: How does the motion-aware context search mechanism perform compared to other context search methods (e.g., random search, distance-based search) in terms of prediction accuracy and computational efficiency?
- Basis in paper: [explicit] The paper introduces a motion-aware context search mechanism that selects context tokens based on the agent's current velocity and projected future position.
- Why unresolved: The paper does not provide a comparison between the motion-aware context search and other context search methods.
- What evidence would resolve it: An experiment comparing the performance of MGTR with the motion-aware context search to other context search methods on the same benchmark dataset.

### Open Question 3
- Question: How does the multi-granular representation of map elements affect the prediction accuracy for different types of agents (e.g., vehicles, pedestrians, cyclists) with varying motion patterns?
- Basis in paper: [explicit] The paper states that different types of agents have different movement ranges and requirements for map granularity, and MGTR processes map elements into sets of tokens at several granular levels.
- Why unresolved: The paper does not provide a detailed analysis of how the multi-granular representation of map elements affects prediction accuracy for different agent types.
- What evidence would resolve it: An experiment analyzing the performance of MGTR on different agent types when using different map granularities, and a discussion of the results.

## Limitations
- The exact implementation details for multi-granular token generation lack precision, particularly how sample rates are determined for different granularity levels
- The motion-aware context search mechanism may not capture complex agent behaviors like sudden direction changes or non-linear motion patterns
- Claims about computational efficiency gains from the motion-aware context search lack quantitative validation

## Confidence
**High Confidence:** The core claim that multi-granular encoding provides context features at resolutions tailored to different agent types is well-supported by the paper's methodology and aligns with established principles in representation learning.

**Medium Confidence:** The assertion that incorporating LiDAR voxel features significantly improves prediction accuracy beyond what HD maps provide is reasonable but lacks sufficient ablation evidence to quantify this improvement.

**Low Confidence:** Claims about computational efficiency gains from the motion-aware context search lack quantitative validation and runtime comparisons.

## Next Checks
1. **Granularity ablation study:** Systematically test the model with single-granularity versus multi-granular map and LiDAR encoding across different agent types. Measure both prediction accuracy and computational overhead to quantify the true benefit of the multi-granular approach and identify optimal granularity levels for different agent categories.

2. **Context search radius sensitivity analysis:** Evaluate the model's performance and efficiency across a range of values for eNm and eNl (nearest neighbor parameters in context search). This will reveal whether the claimed efficiency gains are robust across different operating conditions and identify the optimal trade-off.

3. **Cross-modal attention analysis:** Visualize attention weights between agent tokens and different granularity levels to understand how the model uses multi-granular context. This analysis will reveal whether the model is actually learning to attend to different granularity levels based on agent type and behavior.