---
ver: rpa2
title: Guided Flows for Generative Modeling and Decision Making
arxiv_id: '2311.13443'
source_url: https://arxiv.org/abs/2311.13443
tags:
- flows
- diffusion
- arxiv
- guided
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Guided Flows, extending classifier-free guidance
  to Flow Matching models for conditional generative modeling. The method integrates
  guidance into the velocity fields of Flow Matching, enabling improved sample quality
  across tasks like image generation, text-to-speech synthesis, and offline reinforcement
  learning.
---

# Guided Flows for Generative Modeling and Decision Making

## Quick Facts
- arXiv ID: 2311.13443
- Source URL: https://arxiv.org/abs/2311.13443
- Reference count: 21
- One-line primary result: Guided Flows extend classifier-free guidance to Flow Matching, achieving state-of-the-art performance in image generation, TTS, and RL with a 10x speedup over diffusion models in plan generation.

## Executive Summary
This paper introduces Guided Flows, a method that extends classifier-free guidance to Flow Matching (FM) models for conditional generative modeling. By modifying the velocity field of FM to interpolate between unconditional and conditional flows, Guided Flows improve sample quality across diverse tasks including image generation, text-to-speech synthesis, and offline reinforcement learning. The method leverages the computational efficiency of FM while achieving performance gains through guided sampling, offering a unified approach to high-quality conditional generation.

## Method Summary
Guided Flows integrate classifier-free guidance into Flow Matching by reweighting the velocity field during sampling. The conditional velocity ut(x|y) is interpolated with the unconditional velocity ut(x) using a guidance weight ω, forming a convex combination ˜ut = (1-ω)ut(x) + ωut(x|y). This guided velocity field is then used in the ODE solver to generate samples from the target distribution. The method requires no additional training and preserves the single-step efficiency of FM while steering the flow toward higher-conditional-density regions.

## Key Results
- Achieves state-of-the-art FID scores on ImageNet-64 with face blur augmentation
- Outperforms guided diffusion in zero-shot TTS synthesis on 60K hours of English audiobooks
- Generates RL plans 10x faster than diffusion models while maintaining comparable performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Guided Flows extend classifier-free guidance to Flow Matching by modifying the velocity field rather than the noise schedule.
- Mechanism: The velocity field ut(x|y) is reweighted as a convex combination between unconditional and conditional velocities: ˜ut(x|y) = (1 − ω)ut(x) + ωut(x|y). This directly steers the flow toward higher-conditional-density regions without retraining separate models.
- Core assumption: The modified velocity field still generates a valid probability path that converges to the desired distribution at t=1.
- Evidence anchors:
  - [abstract] "integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields."
  - [section] "Motivated by CFG we define ˜ut(x|y) = (1 − ω)ut(x) + ωut(x|y)" and "this velocity field ˜ut coincides with the one in the Probability Flow ODE (Song et al., 2020) used in Classifier Free Guidance."
- Break condition: If the modified velocity field no longer generates a valid flow (e.g., breaks invertibility or smoothness), sampling will diverge.

### Mechanism 2
- Claim: The guidance parameter ω controls the trade-off between unconditional diversity and conditional fidelity.
- Mechanism: By interpolating between ut(x) (unconditional) and ut(x|y) (conditional), increasing ω shifts the sampling distribution toward regions with higher conditional density while retaining some diversity from the unconditional model.
- Core assumption: The geometric weighting in the marginal probability path ˜pt(x|y) ∝ pt(x)1−ωpt(x|y)ω remains a valid probability measure and approximates the desired conditional distribution.
- Evidence anchors:
  - [abstract] "significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis."
  - [section] "This provides a justification for equation (6)" and "we note, however, that this analysis shows that both Guided Flows and CFG are guaranteed to sample from ˜q(·|y) at time t = 1 if the probability path ˜pt(·|y) defined in equation (10) is close to the marginal probability pathRpt(·|x1)˜q(x1|y)dx1."
- Break condition: Very high ω values may collapse diversity, leading to mode collapse or overfitting to training conditionals.

### Mechanism 3
- Claim: Guided Flows inherit the computational efficiency of Flow Matching while achieving higher sample quality.
- Mechanism: By modifying only the velocity field during sampling (not retraining), Guided Flows preserve the single-step ODE integration advantage of FM over diffusion, while the guidance step increases the likelihood of generating high-quality conditional samples.
- Core assumption: The ODE integration remains stable and efficient under the modified velocity field.
- Evidence anchors:
  - [abstract] "boasting state-of-the-art performance" and "10x speedup in plan generation for RL compared to diffusion models, while maintaining comparable performance."
  - [section] "In offline RL, there is an additional difficulty in generalizing the plans to out-of-distribution states and target return values. Here, we find that guided flows generate reliable execution plans given the current state and a target return values."
- Break condition: If the ODE solver requires many more steps to handle the modified velocity field, the computational advantage is lost.

## Foundational Learning

- Concept: Continuous Normalizing Flows (CNFs) and Flow Matching (FM)
  - Why needed here: Guided Flows build directly on FM, which trains CNFs by regressing a target velocity field instead of maximizing likelihood. Understanding this is critical to see why modifying the velocity field is the right guidance mechanism.
  - Quick check question: What is the difference between the velocity field ut(x|y) in FM and the score function ∇ log pt(x|y)?

- Concept: Classifier-free guidance (CFG) and its adaptation to flows
  - Why needed here: The paper explicitly adapts CFG from diffusion models to FM by reweighting the velocity field. Knowing CFG's mechanism in diffusion is essential to understand why this reweighting works.
  - Quick check question: In CFG for diffusion, which components are interpolated—noise prediction or the velocity field?

- Concept: Probability Flow ODE and its relation to sampling
  - Why needed here: The paper links the guided velocity field to the Probability Flow ODE used in diffusion guidance. This connection justifies the sampling correctness.
  - Quick check question: What is the relationship between the velocity field ut(x|y) and the score function ∇ log pt(x|y) in FM?

## Architecture Onboarding

- Component map:
  - Flow Matching backbone -> Guidance module -> ODE solver -> Sample output
  - Neural network uθ predicting velocity fields -> Linear interpolation with ω -> Numerical integrator applying guided velocity field

- Critical path:
  1. Sample x0 ~ p(x0) (noise).
  2. For t = 0 → 1 in steps: compute ˜ut = (1−ω)uθ(xt) + ωuθ(xt|y).
  3. Integrate ODE: xt+1 = ODEStep(˜ut, xt).
  4. Output x1 as final sample.

- Design tradeoffs:
  - Guidance weight ω vs. sample quality vs. diversity: higher ω improves fidelity but may reduce diversity.
  - ODE solver step size vs. accuracy vs. speed: smaller steps increase accuracy but cost more compute.
  - puncond probability vs. unconditional/conditional balance: higher puncond encourages more unconditional diversity.

- Failure signatures:
  - Divergence in ODE integration → velocity field is invalid or too steep.
  - Mode collapse (low diversity) → ω too high or training lacked null-conditioning.
  - Poor sample quality → guidance weight suboptimal or model underfit.

- First 3 experiments:
  1. Train Guided Flow on a small conditional dataset (e.g., MNIST class-conditional) and sweep ω to observe FID trade-off.
  2. Compare sampling speed and quality between Guided Flow and guided diffusion on the same task.
  3. Test zero-shot generation by conditioning on unseen class labels or return values and evaluate plan reliability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal guidance weight vary across different datasets and tasks in conditional generative modeling?
- Basis in paper: [explicit] The paper shows that the optimal guidance weight can change depending on the compute cost (NFE) and is different for DDPM and FM-OT models.
- Why unresolved: The paper only tests a limited range of guidance weights (1.0 to 3.0) and doesn't provide a systematic study of how the optimal weight varies across tasks.
- What evidence would resolve it: A comprehensive ablation study varying guidance weight across multiple datasets and tasks, identifying optimal weights for each.

### Open Question 2
- Question: What is the theoretical justification for the marginal probability path ˜pt(·|y) being close to the marginal probability path R pt(·|x1)˜q(x1|y)dx1?
- Basis in paper: [explicit] The paper states that the analysis shows Guided Flows and CFG are guaranteed to sample from ˜q(·|y) at time t=1 if the probability path ˜pt(·|y) is close to R pt(·|x1)˜q(x1|y)dx1, but it is not clear to what extent this assumption holds in practice.
- Why unresolved: The paper does not provide any theoretical analysis or empirical evidence to support this assumption.
- What evidence would resolve it: A rigorous theoretical proof of the closeness of the two probability paths, or extensive empirical experiments comparing the two paths.

### Open Question 3
- Question: How does the choice of probability path (e.g., optimal transport vs. cosine scheduling) affect the performance of Guided Flows?
- Basis in paper: [explicit] The paper compares two affine Gaussian probability paths, the optimal transport (FM-OT) path and the cosine scheduling (FM-CS) path, and finds that FM-OT models are slightly more efficient than the other models.
- Why unresolved: The paper only tests two specific probability paths and doesn't explore the full space of possible paths.
- What evidence would resolve it: A comprehensive study comparing Guided Flows with different probability paths on various tasks and datasets.

## Limitations

- The validity of Guided Flows relies on the assumption that the modified velocity field generates a valid probability path close to the marginal path, which requires further theoretical and empirical validation.
- The optimal guidance weight is task-dependent and requires careful tuning, with no systematic study provided across diverse datasets.
- The method's generalization to out-of-distribution states and target returns in RL, while promising, needs rigorous evaluation to rule out overfitting or mode collapse.

## Confidence

- **High Confidence**: The basic mechanism of reweighting velocity fields in Flow Matching is well-grounded in the mathematical framework and directly supported by the derivation linking it to Probability Flow ODE.
- **Medium Confidence**: The claimed 10x speedup in RL plan generation over diffusion models is plausible given the single-step ODE advantage of FM, but the specific comparison methodology and RL task details would benefit from verification.
- **Medium Confidence**: The generalization of Guided Flows to out-of-distribution states and target returns in RL is promising but requires careful evaluation to rule out overfitting or mode collapse.

## Next Checks

1. **Verify Velocity Field Validity**: Implement a sanity check that the guided velocity field preserves the properties required for a valid normalizing flow (e.g., smoothness, invertibility bounds) across the full range of guidance weights ω.
2. **Systematic ω Sweep**: Conduct a comprehensive sweep of the guidance weight ω on a standard conditional image generation task (e.g., CIFAR-10 class-conditional) to map the precise trade-off between sample quality (FID) and diversity, confirming the mechanism's behavior.
3. **Cross-Domain Robustness Test**: Apply Guided Flows to a new conditional generation task (e.g., class-conditional generation on a dataset not in the paper) to test the method's generalization beyond the reported domains.