---
ver: rpa2
title: 'Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on
  the Edge'
arxiv_id: '2312.05693'
source_url: https://arxiv.org/abs/2312.05693
tags:
- quantization
- token
- activation
- pruning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Agile-Quant, an activation-guided quantization
  framework for accelerating inference of Large Language Models (LLMs) on edge devices.
  The key idea is to jointly quantize both model weights and activations while maintaining
  task performance comparable to weight-only quantization.
---

# Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge

## Quick Facts
- arXiv ID: 2312.05693
- Source URL: https://arxiv.org/abs/2312.05693
- Reference count: 20
- Key outcome: Achieves up to 2.55x speedup on edge devices with 4-bit weight/activation quantization while maintaining task performance

## Executive Summary
Agile-Quant introduces an activation-guided quantization framework that jointly quantizes both weights and activations of LLMs to 4-bit integers, significantly accelerating inference on edge devices. The framework combines activation-aware token pruning to reduce outliers and quantization error with a SIMD-based 4-bit multiplier and TRIP matrix multiplication for efficient hardware implementation. Experiments on LLaMA, OPT, and BLOOM models demonstrate substantial speedups while maintaining task performance comparable to weight-only quantization methods.

## Method Summary
The Agile-Quant framework implements activation-guided quantization by first applying uniform or log2 quantizers to both weights and activations, then using activation-aware token pruning to identify and remove outliers based on attention patterns. The framework incorporates a SIMD-based 4-bit multiplier that combines two 4-bit weights into 8-bit values for efficient 4x4 INT4 multiplication, along with TRIP matrix multiplication that integrates token pruning and adaptive quantization. The complete system is designed as an end-to-end accelerator for edge deployment, with token pruning ratios dynamically regulated across different layers.

## Key Results
- Achieves up to 2.55x speedup compared to FP16 counterparts across multiple edge devices
- Maintains task performance comparable to existing weight-only quantization methods
- Reduces memory footprint through 4-bit quantization while preserving model accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 4-bit quantization of weights and activations accelerates inference while preserving task performance
- Mechanism: Reduces memory footprint and computational complexity, enabling faster matrix multiplications on SIMD units supporting 4x4 INT4 operations
- Core assumption: 4-bit quantization combined with token pruning can maintain task performance close to FP16
- Evidence anchors:
  - [abstract] "Experiments show that Agile-Quant achieves simultaneous quantization of model weights and activations while maintaining task performance comparable to existing weight-only quantization methods."
  - [section] "Our method achieves better task performance than most of the other activation quantization works."
- Break condition: If 4-bit quantization introduces too much error, especially for activations with outliers

### Mechanism 2
- Claim: Activation-aware token pruning reduces quantization error and improves task performance
- Mechanism: Removes outliers and inattentive tokens based on attention to start token, reducing quantization error and improving attention locality
- Core assumption: Removing outliers improves quantization by reducing error and enhancing feature capture
- Evidence anchors:
  - [abstract] "We subsequently utilize the activation-aware pruning method to optimize quantization."
  - [section] "Token pruning can reduce the outliers, which can decrease the quantization error caused by them."
- Break condition: If pruning removes too many tokens, model loses important information

### Mechanism 3
- Claim: SIMD-based 4-bit multiplier and TRIP matrix multiplication enable efficient 4-bit matrix operations
- Mechanism: Combines 4-bit weights into 8-bit values for efficient 4x4 INT4 multiplication using SIMD instructions; TRIP incorporates token pruning and adaptive quantization
- Core assumption: Efficient 4-bit matrix multiplication implementation accelerates LLM inference without sacrificing performance
- Evidence anchors:
  - [abstract] "We utilize the SIMD-based 4-bit multiplier and our efficient TRIP matrix multiplication to implement the accelerator for LLMs on the edge."
  - [section] "We designed a specialized 4-bit multiplier based on SIMD architecture."
- Break condition: If hardware implementation is inefficient, speedup gains are offset by overhead

## Foundational Learning

- Concept: Post-Training Quantization (PTQ)
  - Why needed here: Compresses model weights and activations without retraining for resource-constrained edge devices
  - Quick check question: What is the main advantage of using PTQ over quantization-aware training (QAT) for LLMs?

- Concept: Activation Analysis and Token Pruning
  - Why needed here: Understanding attention patterns is essential for designing token pruning strategies that improve quantization
  - Quick check question: How does token pruning based on attention to the start token help reduce quantization error?

- Concept: SIMD-based Matrix Multiplication
  - Why needed here: Efficient implementation using SIMD instructions is crucial for high performance on edge devices
  - Quick check question: What are the key considerations when designing a SIMD-based 4-bit multiplier for LLM inference?

## Architecture Onboarding

- Component map: Input sequence -> Embedding -> Transformer layers (quantized weights/activations with token pruning) -> Output module
- Critical path: Input sequence → Embedding → Transformer layers (with quantized weights and activations) → Output module
- Design tradeoffs:
  - Precision vs. performance: 4-bit offers higher speedup but more quantization error vs 8-bit
  - Token pruning ratio: Balancing pruned tokens to optimize quantization without losing information
- Failure signatures:
  - Significant drop in task performance (e.g., perplexity) compared to FP16 model
  - Inefficient hardware implementation leading to minimal speedup gains
- First 3 experiments:
  1. Quantize LLaMA-7B to 4-bit weights/activations without token pruning and measure task performance and inference speed
  2. Apply activation-aware token pruning to quantized model and evaluate impact on task performance and speedup
  3. Implement SIMD-based 4-bit multiplier and TRIP matrix multiplication on edge device and measure overall inference speedup

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of token sparsity that can be pruned without significantly degrading task performance across different LLM scales?
- Basis in paper: [explicit] The paper shows token pruning improves performance up to a certain sparsity level, after which it degrades (Figure 7 and Figure 8)
- Why unresolved: Experiments only explore sparsity up to 0.8, leaving optimal point for larger models unexplored
- What evidence would resolve it: Systematic ablation studies across diverse model sizes (7B-65B) and input lengths (256-4096 tokens) measuring perplexity degradation

### Open Question 2
- Question: How does the activation-aware token pruning technique generalize to non-causal attention mechanisms like bidirectional attention?
- Basis in paper: [inferred] Pruning strategy relies on causal attention masks and start-token attention patterns specific to LLMs
- Why unresolved: Paper only validates on causal LLMs without testing on bidirectional models like BERT or T5
- What evidence would resolve it: Applying TRIP quantization to bidirectional transformers and comparing perplexity with causal models

### Open Question 3
- Question: What is the trade-off between quantization bit-width (4-bit vs 8-bit) and hardware efficiency across different edge devices with varying SIMD capabilities?
- Basis in paper: [explicit] Paper demonstrates 2.55x speedup for 4-bit vs 1.8x for 8-bit, but only on Snapdragon 870 and Raspberry Pi 4
- Why unresolved: Experiments don't explore broader range of edge hardware with different SIMD widths
- What evidence would resolve it: Benchmarking Agile-Quant across diverse edge platforms (CPUs, NPUs, DSPs) with varying SIMD widths measuring theoretical and actual throughput gains

## Limitations

- Hardware-specific optimization: Speedup claims may not generalize across different edge architectures without hardware-specific tuning
- Token pruning strategy: Lack of detailed explanation on determining optimal pruning ratios across layers
- Task performance benchmarks: Limited evaluation to perplexity on specific datasets, performance on diverse NLP tasks unclear

## Confidence

- **High Confidence**: Core concept of 4-bit quantization accelerating inference through reduced memory footprint and computational complexity
- **Medium Confidence**: Activation-aware token pruning likely effective in reducing quantization error, but implementation details not fully specified
- **Low Confidence**: Claimed speedup gains (2.55x) and task performance maintenance based on limited model and hardware experiments

## Next Checks

1. **Hardware Portability Test**: Implement SIMD-based 4-bit multiplier and TRIP matrix multiplication on ARM-based and RISC-V edge architectures to verify speedup claims and assess hardware-specific optimizations

2. **Comprehensive Task Evaluation**: Evaluate framework performance on GLUE benchmark, SQuAD, and other diverse NLP tasks beyond Wikitext-2 and C4 to validate task performance claims

3. **Ablation Study on Token Pruning**: Conduct systematic ablation study varying pruning ratios across layers and attention heads to determine optimal strategy balancing quantization and task performance