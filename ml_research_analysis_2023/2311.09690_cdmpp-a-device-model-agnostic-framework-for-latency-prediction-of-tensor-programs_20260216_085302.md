---
ver: rpa2
title: 'CDMPP: A Device-Model Agnostic Framework for Latency Prediction of Tensor
  Programs'
arxiv_id: '2311.09690'
source_url: https://arxiv.org/abs/2311.09690
tags:
- tensor
- prediction
- performance
- programs
- devices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CDMPP addresses cross-device and cross-model latency prediction
  for tensor programs across diverse DNN models and hardware. It introduces compact
  ASTs and positional encoding for efficient AST feature extraction, and a CMD-based
  regularization for domain-invariant learning.
---

# CDMPP: A Device-Model Agnostic Framework for Latency Prediction of Tensor Programs

## Quick Facts
- arXiv ID: 2311.09690
- Source URL: https://arxiv.org/abs/2311.09690
- Reference count: 40
- Key outcome: Achieves 14.03% cross-model and 10.85% cross-device prediction error, with 10× higher training efficiency than baselines

## Executive Summary
CDMPP is a framework designed to predict tensor program latency across diverse deep neural network (DNN) models and hardware devices without requiring program profiling on target devices. It addresses the challenge of cross-device and cross-model latency prediction by introducing compact ASTs for efficient feature extraction and CMD-based regularization for domain-invariant learning. The framework leverages KMeans-based sampling for efficient fine-tuning and demonstrates strong performance across both cross-model and cross-device prediction tasks.

## Method Summary
CDMPP works by first converting tensor programs into compact abstract syntax trees (ASTs) that capture computation expressions while eliminating irregular structural variations. These compact ASTs, combined with positional encoding, serve as efficient features for latency prediction. The framework employs a cross-domain learner with transformer-based architecture and CMD regularization to minimize distribution shifts between different models and devices. For target device adaptation, it uses a KMeans-based sampling strategy to select representative tensor programs for fine-tuning, avoiding the need to profile all programs on the target device.

## Key Results
- Achieves 14.03% mean absolute percentage error (MAPE) for cross-model prediction
- Achieves 10.85% MAPE for cross-device prediction
- Demonstrates 10× higher training efficiency compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Compact ASTs reduce training inefficiency by eliminating irregular AST structures. The framework serializes ASTs to retain only leaf nodes (computation expressions) with loop information encoded into computation vectors, creating a regular feature structure with limited length variation. This works because tensor programs have a sufficiently small range of leaf node numbers that enables efficient batch processing without significant information loss.

### Mechanism 2
- CMD-based regularization minimizes domain distribution shifts between different DNN models and devices. The framework applies Central Moment Discrepancy regularization to minimize statistical differences between latent representations from different domains in the embedding space, making the predictor robust across domains. This assumes the latent representation space can effectively capture domain-invariant features that correlate with latency across different devices and models.

### Mechanism 3
- KMeans-based sampling strategy selects representative tensor programs for efficient fine-tuning on target devices. Instead of profiling all tensor programs, the framework clusters all programs using KMeans and selects one representative task per cluster based on average distance to cluster centers. This assumes KMeans clustering in feature space effectively groups similar tensor programs and that selecting representatives from each cluster captures the dataset's diversity.

## Foundational Learning

- **Abstract Syntax Trees (ASTs) for program representation**: Needed because tensor programs have complex, nested loop structures that affect performance, and ASTs naturally capture this hierarchical structure. Quick check: Why can't we use flat feature vectors instead of ASTs for tensor programs?

- **Positional encoding in sequence models**: Needed because compact ASTs are represented as sequences of computation vectors, and positional encoding preserves the structural ordering information lost in serialization. Quick check: How does positional encoding differ from simple index-based ordering?

- **Domain adaptation and distribution discrepancy**: Needed because cross-device and cross-model prediction require handling different data distributions, and domain adaptation techniques minimize the shift between source and target distributions. Quick check: What's the difference between domain adaptation and transfer learning?

## Architecture Onboarding

- **Component map**: Feature Extractor -> Cross Domain Learner -> Replayer -> Auto-tuner
- **Critical path**: Feature extraction → domain-invariant encoding → latency prediction → end-to-end simulation
- **Design tradeoffs**: Compact ASTs vs. full ASTs (efficiency vs. potential information loss), fixed vs. variable model architecture (simplicity vs. adaptability)
- **Failure signatures**: High MAPE on specific devices suggests poor domain adaptation; training instability suggests hyperparameter issues
- **First 3 experiments**: 1) Run cross-model prediction on T4 dataset with default settings to verify basic functionality; 2) Test cross-device prediction from GPUs to EPYC to validate domain adaptation; 3) Evaluate end-to-end performance prediction on a simple DNN model to test the complete pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CDMPP's performance generalize to tensor programs with AST structures significantly different from those in the Tenset dataset?
- Basis in paper: [explicit] The paper notes that CDMPP is tested on the Tenset dataset but does not explicitly evaluate its performance on tensor programs with vastly different AST structures.
- Why unresolved: The paper focuses on the CDCM problem using the Tenset dataset but does not explore the limits of CDMPP's generalization to unseen AST structures.
- What evidence would resolve it: Experimental results showing CDMPP's prediction error on a diverse set of tensor programs with AST structures not present in the Tenset dataset.

### Open Question 2
- Question: What is the impact of varying the number of leaf nodes in Compact ASTs on CDMPP's prediction accuracy and training efficiency?
- Basis in paper: [inferred] The paper discusses Compact ASTs and their design but does not provide a detailed analysis of how varying the number of leaf nodes affects performance.
- Why unresolved: While the paper mentions the range of leaf node numbers and the design of Compact ASTs, it does not investigate the relationship between leaf node count and prediction accuracy or training efficiency.
- What evidence would resolve it: A study varying the number of leaf nodes in Compact ASTs and measuring the corresponding changes in prediction accuracy and training efficiency.

### Open Question 3
- Question: How does CDMPP's performance compare to other domain adaptation methods beyond those mentioned in the paper?
- Basis in paper: [explicit] The paper compares CDMPP to baselines like Habitat and TLP but does not explore other domain adaptation techniques.
- Why unresolved: The paper focuses on CMD-based regularization for domain adaptation but does not evaluate other potential methods that could be applied to the CDCM problem.
- What evidence would resolve it: A comparative study of CDMPP against other domain adaptation methods, such as adversarial training or meta-learning approaches, on the same dataset and tasks.

## Limitations

- Dataset Representation: The framework's effectiveness relies heavily on the Tenset dataset and expanded datasets for training, which may not fully capture the diversity of real-world tensor programs, particularly for emerging DNN architectures or novel hardware platforms.
- Domain Adaptation Scope: CMD-based regularization may struggle with fundamental architectural differences between devices, particularly when transferring between vastly different hardware types like GPUs and specialized accelerators.
- End-to-End Prediction Accuracy: Errors can compound when predicting complex execution graphs, as the accuracy of end-to-end predictions depends on the quality of individual tensor program latency estimates.

## Confidence

- **High Confidence**: The core mechanism of using Compact ASTs for efficient feature extraction is well-supported by empirical evidence and structural analysis.
- **Medium Confidence**: The effectiveness of CMD-based regularization for domain adaptation is supported by theoretical analysis and experimental results, but needs further validation across diverse hardware platforms.
- **Low Confidence**: The assumption that KMeans clustering effectively captures the diversity of tensor programs for sampling is based on limited experimental evidence and may not hold for highly diverse or outlier-rich datasets.

## Next Checks

1. **Dataset Coverage Validation**: Test the framework's performance on tensor programs from DNN models not represented in the Tenset dataset, particularly focusing on emerging architectures like transformers and graph neural networks.

2. **Hardware Diversity Test**: Evaluate cross-device prediction accuracy when transferring from GPUs to non-GPU hardware (e.g., TPUs, FPGAs, or specialized AI accelerators) to assess the limits of domain adaptation.

3. **Sampling Strategy Robustness**: Compare the KMeans-based sampling strategy against alternative approaches (e.g., stratified sampling or active learning) across datasets with varying degrees of class imbalance and outlier presence.