---
ver: rpa2
title: Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks
arxiv_id: '2310.09436'
source_url: https://arxiv.org/abs/2310.09436
tags:
- tasks
- task
- learning
- each
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TSS, a continual learning method that addresses
  both catastrophic forgetting and knowledge transfer for a sequence of mixed NLP
  tasks. TSS uses a fixed backbone network and discovers a subnetwork for each task
  by learning binary gates on popup scores.
---

# Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks

## Quick Facts
- arXiv ID: 2310.09436
- Source URL: https://arxiv.org/abs/2310.09436
- Reference count: 23
- Key outcome: TSS achieves strong performance on 5 NLP datasets, preventing forgetting and enabling knowledge transfer for mixed task sequences.

## Executive Summary
This paper proposes TSS, a continual learning method that addresses both catastrophic forgetting and knowledge transfer for a sequence of mixed NLP tasks. TSS uses a fixed backbone network and discovers a subnetwork for each task by learning binary gates on popup scores. A soft-masking mechanism is introduced to preserve knowledge from previous tasks and enable knowledge transfer to new tasks. Experiments on 5 datasets (classification, generation, and information extraction) show that TSS consistently outperforms strong baselines, achieving similar performance to multi-task learning while preventing forgetting. TSS is particularly effective for heterogeneous task sequences.

## Method Summary
TSS is a continual learning method that prevents catastrophic forgetting and encourages knowledge transfer for mixed NLP tasks. It uses a fixed backbone network (BART-Large) with adapters, and discovers a subnetwork for each task by learning binary gates on popup scores. The soft-masking mechanism reduces gradients for parameters important to previous tasks, preserving their knowledge while allowing new tasks to leverage past knowledge. TSS is trained sequentially on task sequences, with new tasks initialized using the trained popup scores from previous tasks.

## Key Results
- TSS consistently outperforms strong baselines across 5 NLP datasets.
- TSS achieves similar performance to multi-task learning while preventing forgetting.
- TSS is particularly effective for heterogeneous task sequences.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sub-network discovery prevents catastrophic forgetting by isolating task-specific parameters.
- Mechanism: A fixed backbone network is paired with binary gates derived from learned popup scores. Each task activates only its assigned sub-network via element-wise multiplication with adapter parameters.
- Core assumption: Task models are non-interfering if they use disjoint parameter subsets of the backbone.
- Evidence anchors:
  - [abstract] "It overcomes CF by isolating the knowledge of each task via discovering a subnetwork for it."
  - [section 3.1.1] "Since the pre-trained language model (LM) contains useful general knowledge, we fix and use the full LM. The sub-network discovery is only done on adapters."
- Break condition: If tasks share significant parameters in the backbone or if the adapter mask overlaps across tasks, interference and forgetting can occur.

### Mechanism 2
- Claim: Soft-masking preserves previously learned knowledge while allowing new task learning.
- Mechanism: Accumulated importance scores from past tasks are used to reduce gradients (1 - I) for popup scores important to prior tasks, preventing overwriting while still permitting updates.
- Core assumption: Importance scores accurately reflect parameter relevance to past tasks, and gradient reduction is sufficient to preserve them.
- Evidence anchors:
  - [abstract] "A soft-masking mechanism is also proposed to preserve the previous knowledge and to enable the new task to leverage the past knowledge to achieve KT."
  - [section 3.1.2] "Soft-masking the popup scores... reduces (or soft-mask) s(t)_l's gradient flow as follows..."
- Break condition: If importance scores are inaccurate or gradients are reduced too aggressively, new task learning may be hindered or knowledge may still be lost.

### Mechanism 3
- Claim: Popup scores are trained instead of network weights, enabling parameter isolation without retraining the backbone.
- Mechanism: Learnable popup scores are thresholded to produce binary gates; only these scores are updated during training, leaving the backbone fixed.
- Core assumption: Training popup scores is equivalent to training the effective sub-network without modifying the backbone.
- Evidence anchors:
  - [abstract] "It overcomes CF by isolating the knowledge of each task via discovering a subnetwork for it."
  - [section 3.1.1] "Since the pre-trained language model (LM) contains useful general knowledge, we fix and use the full LM."
- Break condition: If the threshold function or straight-through estimator approximation fails, the binary gates may not correctly represent the desired sub-network.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks.
  - Why needed here: The paper aims to prevent CF, which is a core problem in continual learning.
  - Quick check question: What happens to performance on previous tasks when training a model on a new task without any mitigation strategy?

- Concept: Knowledge transfer in continual learning.
  - Why needed here: The paper seeks to encourage KT alongside CF prevention.
  - Quick check question: How can a model leverage knowledge from previously learned tasks to improve learning on a new task?

- Concept: Sub-network discovery and parameter isolation.
  - Why needed here: The proposed TSS method uses sub-network discovery to isolate task-specific parameters and prevent interference.
  - Quick check question: What is the benefit of isolating parameters for each task in continual learning?

## Architecture Onboarding

- Component map:
  - Fixed backbone network (Transformer + adapters)
  - Learnable popup scores (one per parameter in adapters)
  - Binary gates (thresholded popup scores)
  - Soft-masking mechanism (based on accumulated importance)
  - Importance computation module (gradient-based)

- Critical path:
  1. Initialize popup scores (Kaiming for first task, previous task's scores for subsequent tasks)
  2. Forward pass: threshold popup scores to get binary gates, multiply with adapter parameters
  3. Backward pass: compute gradients, apply soft-masking based on accumulated importance, update popup scores
  4. After training, compute importance scores for next task's soft-masking

- Design tradeoffs:
  - Fixed backbone vs. fine-tuning: Fixed backbone prevents CF but may limit expressivity
  - Soft-masking vs. hard masking: Soft-masking allows more flexibility but may be less effective at preventing forgetting
  - Popup scores vs. direct parameter training: Popup scores enable isolation but add complexity

- Failure signatures:
  - Performance degradation on previous tasks (CF)
  - Poor performance on new tasks (lack of KT or over-aggressive soft-masking)
  - Instability in training (incorrect importance scores or soft-masking)

- First 3 experiments:
  1. Train TSS on a single task and verify it matches baseline performance
  2. Train TSS on two similar tasks and check for KT without CF
  3. Train TSS on two dissimilar tasks and verify CF prevention without KT

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The paper does not provide extensive ablations or analysis of the soft-masking mechanism's effectiveness, particularly regarding the impact of different importance score formulations or soft-masking hyperparameters.
- While TSS shows strong performance across diverse task types, the analysis of when and why knowledge transfer occurs (versus preventing forgetting) is limited, especially for dissimilar task pairs.
- The computational overhead of TSS, particularly compared to simpler baselines like EWC or LwF, is not explicitly discussed or quantified.

## Confidence
- **High Confidence:** The core mechanisms of sub-network discovery via popup scores and binary gates are clearly described and technically sound. The overall framework and experimental setup are well-specified.
- **Medium Confidence:** The effectiveness of soft-masking in preserving knowledge and enabling transfer is demonstrated empirically but lacks deeper theoretical justification or extensive ablation analysis.
- **Medium Confidence:** The claim that TSS consistently outperforms strong baselines is supported by experiments, but the results could benefit from additional robustness checks (e.g., more random seeds, different model sizes).

## Next Checks
1. Perform ablations to isolate the impact of soft-masking: Compare TSS with and without soft-masking, and test different importance score formulations to assess their contribution to performance.
2. Analyze KT vs. CF: For heterogeneous task sequences, investigate whether improvements on new tasks stem from genuine knowledge transfer or simply better prevention of forgetting compared to other methods.
3. Evaluate computational efficiency: Measure and compare the training time and memory overhead of TSS against baselines like EWC, LwF, and Piggyback to assess practical viability.