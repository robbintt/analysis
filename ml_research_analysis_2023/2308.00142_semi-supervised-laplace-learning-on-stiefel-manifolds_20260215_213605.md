---
ver: rpa2
title: Semi-Supervised Laplace Learning on Stiefel Manifolds
arxiv_id: '2308.00142'
source_url: https://arxiv.org/abs/2308.00142
tags:
- learning
- graph
- vertices
- laplacian
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a semi-supervised learning method for graph
  data that addresses the degeneracy of classical Laplace learning at low label rates.
  The core idea is to reformulate graph-based SSL as a nonconvex optimization problem
  on a Stiefel manifold, motivated by the well-posedness of Laplacian eigenvectors
  in the limit of infinite unlabeled data.
---

# Semi-Supervised Laplace Learning on Stiefel Manifolds

## Quick Facts
- arXiv ID: 2308.00142
- Source URL: https://arxiv.org/abs/2308.00142
- Reference count: 40
- Primary result: Reformulates graph SSL as a Stiefel manifold optimization problem, resolving degeneracy at low label rates and achieving 93.5% accuracy on Fashion-MNIST with 5 labels per class.

## Executive Summary
This paper addresses the degeneracy of classical Laplace learning in semi-supervised learning when label rates are low. The authors reformulate graph-based SSL as a nonconvex optimization problem on a Stiefel manifold, motivated by the well-posedness of Laplacian eigenvectors in the limit of infinite unlabeled data. An efficient approximate method based on Orthogonal Procrustes Analysis is proposed, along with a Sequential Subspace Method for iterative refinement. Experiments demonstrate lower classification error compared to state-of-the-art methods across low, medium, and high label rate regimes.

## Method Summary
The method reformulates graph semi-supervised learning as a rescaled quadratic program on a compact Stiefel Manifold, addressing the ill-posedness of classical Laplace learning at low label rates. The approach involves three main components: (1) an initial embedding via Laplacian Eigenmaps followed by Procrustes alignment to resolve discrepancies between labeled and unlabeled vertices, (2) refinement using Sequential Subspace Method (SSM) on the Stiefel manifold to find high-quality stationary points, and (3) an optional graph cut-based refinement using multi-class Kernighan-Lin algorithm. Additionally, a novel spectral score based on grounded Laplacian eigenvectors and node degrees is introduced for active learning.

## Key Results
- Achieves 93.5% accuracy on Fashion-MNIST with only 5 labels per class
- Achieves 58.8% accuracy on CIFAR-10 with 5 labels per class
- Demonstrates lower classification error compared to state-of-the-art methods across low, medium, and high label rate regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating graph-based SSL as a trust-region subproblem on the Stiefel manifold resolves degeneracy at low label rates.
- Mechanism: The classical Laplace learning becomes ill-posed when few labeled nodes exist because the solution develops spikes near labeled nodes. By framing the problem as minimizing a quadratic over the Stiefel manifold, the method enforces orthogonality constraints that prevent degenerate solutions and allow stable label propagation even with sparse supervision.
- Core assumption: The graph Laplacian's eigenvectors remain well-defined and informative in the limit of infinite unlabeled data, justifying the reformulation.
- Evidence anchors:
  - [abstract] "motivated by the well-posedness of Laplacian eigenvectors in the limit of infinite unlabeled data"
  - [section] "we introduce a natural formulation of graph semi-supervised learning as a rescaled quadratic program on a compact Stiefel Manifold, i.e. a generalization of a Trust-Region Subproblem"
- Break condition: If the graph Laplacian is poorly conditioned or the label rate is too low for any meaningful structure to emerge, the reformulation may not stabilize predictions.

### Mechanism 2
- Claim: The orthogonal Procrustes alignment step aligns unlabeled vertices with labeled neighbors, yielding an approximate critical point of the optimization problem.
- Mechanism: After computing the initial embedding via Laplacian Eigenmaps, the Procrustes step finds an orthogonal transformation that best maps the unlabeled embedding to match the labeled vertices' coordinates. This reduces label inconsistency and provides a good initialization for further refinement.
- Core assumption: The embedding space produced by Laplacian Eigenmaps is informative enough that an orthogonal alignment can meaningfully reduce the discrepancy between labeled and unlabeled node representations.
- Evidence anchors:
  - [section] "Procrustes embedding. The orthogonal transform Q is derived from Prop. 3.1 and applied to X; XQ resolves the discrepancy between the embeddings and the labeled vertices."
- Break condition: If the labeled and unlabeled embeddings are too dissimilar (e.g., due to highly imbalanced class distribution), the Procrustes alignment may produce a poor initialization.

### Mechanism 3
- Claim: Sequential Subspace Method (SSM) on the Stiefel manifold efficiently refines approximate solutions to high-quality stationary points with global convergence guarantees.
- Mechanism: SSM iteratively solves smaller subproblems in low-dimensional subspaces derived from the current iterate, gradient, and eigenvectors. This avoids the computational burden of full-dimensional optimization while still converging to a stationary point with favorable eigenvalue characteristics.
- Core assumption: The subspace spanned by the current iterate, gradient, and key eigenvectors captures sufficient information to drive the solution toward a high-quality critical point.
- Evidence anchors:
  - [section] "SSM-based algorithms generate a sequence of iterates Xt by solving a series of rescaled quadratic programs in subspaces of dimension much smaller than that of the original problem"
- Break condition: If the subspace selection is poor (e.g., excludes critical eigenvectors), convergence may stall or yield suboptimal stationary points.

## Foundational Learning

- Concept: Graph Laplacian and its properties (eigenvectors, eigenvalues, grounded Laplacian)
  - Why needed here: The method relies on spectral properties of the graph Laplacian to construct embeddings and to define the optimization problem. Understanding how labeling affects the Laplacian submatrix is critical for deriving the reformulation.
  - Quick check question: What is the smallest non-zero eigenvalue of the graph Laplacian called, and why is it important for label propagation?

- Concept: Stiefel manifold and orthogonal constraints
  - Why needed here: The optimization is performed over the Stiefel manifold St(n,k), which enforces orthogonality of the embedding matrix. This is key to avoiding degenerate solutions and ensuring stable label propagation.
  - Quick check question: What is the geometric interpretation of the Stiefel manifold St(n,k), and how does it differ from the unit sphere?

- Concept: Trust-region subproblems and their generalizations
  - Why needed here: The reformulated problem is a generalization of a trust-region subproblem. Understanding this connection explains why the method is well-posed and how iterative refinement can be applied.
  - Quick check question: In a standard trust-region subproblem, what is the role of the Lagrange multiplier, and how does it relate to the eigenvalues of the Hessian?

## Architecture Onboarding

- Component map: Graph construction -> Embedding (Laplacian Eigenmaps + Procrustes) -> SSM refinement -> KL refinement -> (optional) Active learning score computation
- Critical path: Graph construction → Embedding (Laplacian Eigenmaps + Procrustes) → SSM refinement → KL refinement → (optional) Active learning score computation
- Design tradeoffs:
  - Using SSM instead of generic Riemannian optimizers trades implementation complexity for faster convergence on large graphs.
  - The Procrustes initialization is fast but approximate; SSM is needed for high-quality solutions.
  - KL refinement adds robustness at the cost of extra computation.
- Failure signatures:
  - Degenerate embeddings (all points clustered): indicates poor initialization or ill-conditioned Laplacian.
  - SSM not converging: suggests poor subspace selection or ill-conditioned problem.
  - KL refinement not improving: may indicate already optimal partition or poor graph structure.
- First 3 experiments:
  1. Run the method on a small synthetic graph (e.g., two moons) with 1-2 labels per class; verify that SSM converges and predictions improve over Laplace learning.
  2. Compare Procrustes initialization vs. random initialization on the same synthetic graph; check convergence speed and final accuracy.
  3. Apply the active learning score on a small labeled set; verify that selected nodes are diverse and well-connected.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the Sequential Subspace Method (SSM) converge to globally optimal solutions rather than just high-quality local solutions?
- Basis in paper: [explicit] The paper mentions that global convergence is guaranteed when certain eigenvectors of L are included in the subspace, but also notes that proving global optimality requires further analysis, particularly for graphs with specific eigenvalue structures.
- Why unresolved: The paper states that the authors intend to explore conditions on L and U that guarantee convergence to globally optimal solutions with convergence rates, but this analysis is left as future work.
- What evidence would resolve it: A rigorous mathematical proof showing the conditions under which SSM converges to global optima, including analysis of graph Laplacian eigenvalue structures and the relationship between B and eigenvectors of L.

### Open Question 2
- Question: How does the choice of subspaces in SSM affect the quality and rate of convergence compared to other Riemannian optimization methods?
- Basis in paper: [explicit] The paper demonstrates through experiments that SSM converges rapidly to critical points while first-order methods fail to converge, and claims that SSM's Hessian information estimation is usually better than methods used for trust-region approaches.
- Why unresolved: While the paper shows empirical results comparing SSM to first-order methods and mentions theoretical advantages, it doesn't provide a comprehensive comparison with other second-order Riemannian optimization methods or a detailed theoretical analysis of subspace selection's impact.
- What evidence would resolve it: A detailed theoretical analysis comparing the Hessian information estimation quality of SSM versus other methods, along with extensive empirical studies showing convergence rates and solution quality across various problem instances.

### Open Question 3
- Question: What is the relationship between the spectral active learning score and the underlying graph structure, and how can this relationship be leveraged to improve active learning performance?
- Basis in paper: [explicit] The paper proposes a spectral score for active learning based on eigenvector centrality and degree, and provides a lower bound estimate on the smallest eigenvalue of LU, but notes that the relationship between the score and the underlying graph structure needs further investigation.
- Why unresolved: While the paper demonstrates empirically that the proposed score improves active learning performance compared to other methods, it doesn't provide a comprehensive theoretical analysis of how the score relates to different graph structures or how to optimally tune its parameters.
- What evidence would resolve it: A theoretical analysis of how the spectral score behaves on different types of graphs (e.g., random graphs, small-world networks, scale-free networks), along with empirical studies showing the impact of graph structure on active learning performance using the proposed score.

## Limitations
- Convergence guarantees are mentioned but specific rates and conditions are not fully detailed
- Scalability claims are based on moderate-sized graphs (MNIST, Fashion-MNIST, CIFAR-10 with ~10k nodes) and unverified on larger graphs
- Active learning score is proposed but not extensively validated against other active learning criteria

## Confidence

- Low label rate performance: High confidence (extensive experiments show clear superiority over baselines in the low label regime)
- Reformulation on Stiefel manifold: High confidence (mathematical formulation is rigorously derived and justified)
- SSM convergence: Medium confidence (theoretical guarantees are mentioned, but specific rates and conditions are not provided)
- Procrustes initialization: High confidence (simple, effective, and well-justified by alignment of spectral embeddings)
- KL refinement effectiveness: Medium confidence (method is described but not extensively validated in experiments)

## Next Checks

1. **Convergence diagnostics**: Monitor objective value, gradient norm, and iterate changes during SSM iterations to verify convergence behavior and identify potential failure modes.

2. **Sensitivity to hyperparameters**: Systematically vary the number of neighbors k in graph construction, regularization parameters, and SSM subspace dimensions to assess robustness and identify optimal settings.

3. **Comparison on larger graphs**: Apply the method to larger-scale graph datasets (e.g., web graphs, citation networks) to validate scalability claims and compare runtime with other SSL methods.