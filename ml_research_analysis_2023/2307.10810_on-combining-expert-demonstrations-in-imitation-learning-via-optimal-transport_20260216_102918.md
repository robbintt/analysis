---
ver: rpa2
title: On Combining Expert Demonstrations in Imitation Learning via Optimal Transport
arxiv_id: '2307.10810'
source_url: https://arxiv.org/abs/2307.10810
tags:
- expert
- learning
- imitation
- optimal
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of optimally combining multiple
  expert demonstrations in imitation learning, particularly when the demonstrations
  are diverse or multi-modal. The authors propose using a multi-marginal optimal transport
  (OT) distance to combine expert state trajectories in a principled way, as opposed
  to the standard method of simply concatenating trajectories.
---

# On Combining Expert Demonstrations in Imitation Learning via Optimal Transport

## Quick Facts
- arXiv ID: 2307.10810
- Source URL: https://arxiv.org/abs/2307.10810
- Reference count: 27
- Key outcome: SMMOTIL outperforms SCOTIL when combining diverse expert demonstrations in OpenAI Gym environments

## Executive Summary
This paper addresses the challenge of optimally combining multiple expert demonstrations in imitation learning, particularly when demonstrations are diverse or multi-modal. The authors propose using multi-marginal optimal transport (OT) distance to combine expert state trajectories in a principled way, as opposed to the standard method of simply concatenating trajectories. Their approach, called Sliced Multi-Marginal Optimal Transport Imitation Learning (SMMOTIL), uses sliced OT to compute rewards based on the discrepancy between an agent's trajectory and all expert trajectories simultaneously. Experiments on OpenAI Gym control environments (Pendulum-v0 and CartPole-v0) with diverse-length and diverse-mass expert demonstrations show that SMMOTIL consistently outperforms SCOTIL in terms of mean episodic rewards and stability.

## Method Summary
The paper compares two methods for combining multiple expert demonstrations in imitation learning: SCOTIL (concatenates expert trajectories and subsamples) and SMMOTIL (uses multi-marginal optimal transport to compute distances to all experts simultaneously). Both methods use sliced Wasserstein distances to reduce computational complexity. The approach generates dense reward signals from OT distances that can be used by standard RL algorithms like DQN. Experiments use OpenAI Gym environments with 5 diverse expert trajectories each, and evaluate performance through mean episodic rewards and stability across multiple runs.

## Key Results
- SMMOTIL consistently outperforms SCOTIL on Pendulum-v0 and CartPole-v0 with diverse expert demonstrations
- The mean episodic reward for SMMOTIL shows significantly less variance than SCOTIL
- Multi-marginal OT provides a more stable geometric average of diverse expert trajectories compared to concatenation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-marginal OT provides a geometric barycenter of expert demonstrations, leading to smoother reward signals than concatenation.
- Mechanism: The multi-marginal Wasserstein distance aligns atoms across multiple expert trajectories by finding the Euclidean barycenter of the aligned samples in the projection space.
- Core assumption: Experts are diverse in a way that concatenation would introduce noise but multi-marginal averaging preserves meaningful commonalities.
- Evidence anchors:
  - [abstract]: "enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations"
  - [section]: "In theory, minimizing SMMOTIL's loss function is equivalent to minimizing the sliced-Wasserstein between the agent's rollout and the sliced-Wasserstein barycenter of the expert trajectory."
- Break condition: If experts are too diverse with no meaningful geometric average, the barycenter may not represent any useful behavior.

### Mechanism 2
- Claim: Slicing reduces computational complexity while preserving meaningful trajectory distances.
- Mechanism: By projecting high-dimensional state trajectories onto random 1D lines and computing 1D Wasserstein distances, the algorithm achieves O(KT log T) complexity instead of the O(T³ log T) of standard OT.
- Core assumption: The 1D projections preserve sufficient information about the original high-dimensional distance structure.
- Evidence anchors:
  - [section]: "Using the slicing method applied to the Wasserstein distance reduces the complexity of computing the OT distance to O(KT log T) where K is the number of projections and T is the number of samples"
- Break condition: If the trajectory structure is highly sensitive to specific dimensions, random projections may lose critical information.

### Mechanism 3
- Claim: The reward signal defined as the negative sliced distance drives the agent toward expert-like behavior.
- Mechanism: The reward function rt,p(sa_t, S) = (1/(PK)) Σ |⟨sa_t - (1/(P+1))Σ s(j)_ηp,j,k(t), θk⟩|² creates a smooth landscape where higher rewards correspond to states closer to the expert barycenter in the projection space.
- Core assumption: The agent can learn from dense reward signals derived from trajectory distances rather than sparse environmental rewards.
- Evidence anchors:
  - [section]: "The reward signal can be formulated similarly to (5), as derived in Cohen et al. [4], rt,p(sa_t, S) = 1/(PK) Σ |⟨sa_t - (1/(P+1))Σ s(j)_ηp,j,k(t), θk⟩|²"
- Break condition: If the reward landscape becomes too flat or noisy, DQN may fail to learn meaningful policies.

## Foundational Learning

- Concept: Optimal Transport distances (Wasserstein metrics)
  - Why needed here: Provides a principled way to measure similarity between trajectories that respects the geometry of the state space
  - Quick check question: What is the computational complexity of standard 2-Wasserstein distance vs sliced Wasserstein distance?

- Concept: Multi-marginal optimal transport
  - Why needed here: Enables combining multiple expert demonstrations without losing diversity through concatenation
  - Quick check question: How does the multi-marginal Wasserstein distance differ from pairwise Wasserstein distances when combining multiple measures?

- Concept: Reinforcement learning with dense rewards
  - Why needed here: The algorithm needs to train a policy using the OT-based rewards instead of environmental rewards
  - Quick check question: What DQN hyperparameters are most critical when learning from dense reward signals derived from trajectory distances?

## Architecture Onboarding

- Component map: Expert trajectories → Slicing → Distance computation → Rewards → DQN update → Policy evaluation

- Critical path: Expert trajectories → Slicing → Distance computation → Rewards → DQN update → Policy evaluation

- Design tradeoffs:
  - Number of projections K: Higher K gives more accurate distance estimates but increases computation
  - Trajectory length: Longer trajectories provide more information but increase memory and computation
  - Number of experts P: More experts enable better averaging but increase multi-marginal computation cost

- Failure signatures:
  - SCOTIL shows high variance and instability (as reported in results)
  - SMMOTIL shows consistent improvement over SCOTIL
  - Both methods converge slower than standard RL with true rewards
  - Reward signals becoming too small as agent approaches expert behavior

- First 3 experiments:
  1. Implement SCOTIL on Pendulum-v0 with 2 expert trajectories of different lengths, verify convergence
  2. Implement SMMOTIL on the same setup, compare mean rewards and variance
  3. Test both methods with 5 diverse-length experts on CartPole-v0, measure sample efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multi-marginal optimal transport methods perform compared to concatenation methods in higher-dimensional environments like MuJoCo?
- Basis in paper: [explicit] The paper concludes by stating it will be interesting to verify findings on higher-dimensional environments such as MuJoCo control tasks in future work.
- Why unresolved: The current experiments are limited to OpenAI Gym control environments (Pendulum-v0 and CartPole-v0), which are relatively low-dimensional.
- What evidence would resolve it: Running experiments using SMMOTIL and SCOTIL on MuJoCo environments and comparing their performance metrics (mean episodic rewards, stability) to determine if multi-marginal methods maintain their advantage in higher dimensions.

### Open Question 2
- Question: How do sliced Gromov-Wasserstein distances compare to sliced Wasserstein distances for imitation learning when agent and expert trajectories live in different spaces?
- Basis in paper: [explicit] The paper suggests it would be interesting to consider other metrics like sliced-Gromov-Wasserstein distances in future work, which would allow comparing agents and experts living on different spaces.
- Why unresolved: The current work only uses sliced Wasserstein distances for trajectory comparison.
- What evidence would resolve it: Implementing imitation learning algorithms using sliced Gromov-Wasserstein distances and comparing their performance to sliced Wasserstein methods on tasks where state spaces differ between agent and expert.

### Open Question 3
- Question: How does the choice of expert diversity (e.g., different lengths vs. different masses) affect the performance gap between SMMOTIL and SCOTIL?
- Basis in paper: [explicit] The paper specifically tests diverse-length and diverse-mass expert demonstrations and observes performance differences between methods.
- Why unresolved: While the paper shows that SMMOTIL outperforms SCOTIL for both types of diversity, it doesn't systematically analyze how different types or degrees of diversity affect the performance gap.
- What evidence would resolve it: Conducting experiments with controlled variations in expert diversity (gradually increasing differences in length, mass, or other parameters) and measuring how the performance gap between SMMOTIL and SCOTIL changes with increasing diversity.

## Limitations
- Limited empirical evaluation on only two OpenAI Gym environments
- Synthetic diversity in expert demonstrations may not reflect real-world scenarios
- Computational complexity claims rely on theoretical analysis without extensive empirical validation

## Confidence
- Claims about SMMOTIL's superiority: High confidence (consistent improvements shown)
- Claims about mechanism of multi-marginal OT providing better geometric averaging: Medium confidence (limited theoretical justification)
- Claims about sliced OT preserving sufficient information: Low confidence (no comparisons to exact OT computation)

## Next Checks
1. Evaluate SMMOTIL on more complex environments (e.g., HalfCheetah, Hopper) with larger state spaces to assess scalability and verify the O(KT log T) complexity advantage over exact OT methods.

2. Systematically vary the number of projections K and expert trajectories P to determine the sensitivity of SMMOTIL performance to these hyperparameters and identify optimal configurations.

3. Test SMMOTIL with noisy or imperfect expert demonstrations to evaluate its robustness to suboptimal demonstrations, as the current results assume high-quality expert trajectories.