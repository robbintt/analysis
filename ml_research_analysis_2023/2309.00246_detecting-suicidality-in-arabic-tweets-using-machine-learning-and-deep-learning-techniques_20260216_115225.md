---
ver: rpa2
title: Detecting Suicidality in Arabic Tweets Using Machine Learning and Deep Learning
  Techniques
arxiv_id: '2309.00246'
source_url: https://arxiv.org/abs/2309.00246
tags:
- suicidal
- learning
- arabic
- machine
- tweets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of machine learning and
  deep learning techniques for detecting suicidal ideation in Arabic tweets. The authors
  developed a novel Arabic suicidality detection dataset comprising 5,719 tweets,
  with 1,429 labeled as suicidal and 4,290 as non-suicidal.
---

# Detecting Suicidality in Arabic Tweets Using Machine Learning and Deep Learning Techniques

## Quick Facts
- arXiv ID: 2309.00246
- Source URL: https://arxiv.org/abs/2309.00246
- Reference count: 40
- Primary result: AraBERT achieves 91% accuracy and 88% F1-score for detecting suicidal ideation in Arabic tweets

## Executive Summary
This study develops a novel Arabic suicidality detection dataset of 5,719 tweets and evaluates multiple machine learning and deep learning approaches for identifying suicidal ideation. The dataset, annotated by two judges including a psychology expert, contains 1,429 suicidal and 4,290 non-suicidal tweets. The research compares traditional machine learning models with pre-trained Arabic language models, finding that AraBERT significantly outperforms other approaches. The work addresses a critical gap in Arabic natural language processing for mental health applications and demonstrates the potential of transformer-based models for detecting nuanced emotional content in social media text.

## Method Summary
The study collected Arabic tweets using specific keywords related to suicidal ideation, then annotated them using a binary classification scheme (suicidal vs non-suicidal) by two annotators, one with psychology expertise. Multiple machine learning models were trained using various feature extraction methods including word frequency, word embeddings (Word2Vec, FastText), and character n-grams with TF-IDF weighting. Additionally, pre-trained deep learning models (AraBERT, AraELECTRA, AraGPT2) were fine-tuned on the dataset. Performance was evaluated using accuracy and F1-score metrics, with particular attention to handling the class imbalance in the dataset.

## Key Results
- AraBERT achieved the highest performance with 91% accuracy and 88% F1-score
- SVM and Random Forest models with character n-gram features achieved 86% accuracy and 79% F1-score
- Character n-gram TF-IDF features proved particularly effective for machine learning models
- The novel Arabic suicidality dataset contains 5,719 tweets with balanced expert annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AraBERT's transformer architecture with attention mechanism better captures the contextual meaning of Arabic text than traditional n-gram or word embedding models.
- Mechanism: The self-attention layers in AraBERT can dynamically weigh the importance of different words in a tweet, allowing it to understand complex semantic relationships that are difficult to capture with static word embeddings or n-gram features.
- Core assumption: Arabic's morphological richness and dialectal variation make it hard for simpler models to capture nuanced meanings of suicidal expressions.
- Evidence anchors:
  - [abstract] "The results of the deep learning models show that AraBert model outperforms other machine and deep learning models, achieving an accuracy of 91% and an F1-score of 88%."
  - [section] "AraBERT is a pre-trained Arabic language model, trained utilizing a masked language modeling objective on a large corpus of Arabic text data. It has been shown to deliver state-of-the-art results on a variety of Arabic language processing tasks."
- Break condition: If the Arabic dataset lacks sufficient diversity in dialect or context, the model may overfit to the training distribution and fail on unseen variations.

### Mechanism 2
- Claim: Character n-gram TF-IDF features improve detection of suicidal tweets in machine learning models by capturing morphological patterns and misspellings common in informal Arabic social media text.
- Mechanism: Character-level features can identify subword patterns and orthographic variations that signal emotional intensity or suicidal intent, which are often expressed through informal spelling in tweets.
- Core assumption: Users writing about suicidal thoughts may use non-standard spelling or morphological constructions that differ from formal Arabic.
- Evidence anchors:
  - [abstract] "The results indicate that SVM and RF models trained on character n-gram features provided the best performance in the machine learning models, with 86% accuracy and an F1 score of 79%."
  - [section] "The character n-gram has achieved state-of-the-art performance on several text classification tasks [46]"
- Break condition: If the dataset is too small or homogeneous, character n-grams may not generalize beyond the specific spelling patterns present in the training data.

### Mechanism 3
- Claim: Domain-specific annotation by psychology experts improves label quality, leading to better model performance.
- Mechanism: Expert annotators can identify subtle linguistic cues of suicidal ideation that non-experts might miss, reducing false negatives and improving model reliability.
- Core assumption: Suicidal expressions in Arabic may use culturally specific idioms or indirect language that requires psychological expertise to interpret correctly.
- Evidence anchors:
  - [abstract] "The labeling process was conducted by two annotators, one of whom holds a Ph.D. in psychology and has been working on several cyber-psychology projects."
  - [section] "Annotators were asked to read the tweets' text (tweet textual content only) and rate the level of concern of suicide in each tweet."
- Break condition: If annotator bias or disagreement is high, it could introduce noise that degrades model performance.

## Foundational Learning

- Concept: Arabic morphological structure and orthographic variation
  - Why needed here: Arabic words can have many surface forms due to prefixes, suffixes, and missing diacritics, which affects feature extraction and model performance.
  - Quick check question: How does the absence of short vowels in most Arabic text affect word disambiguation?

- Concept: Supervised classification and evaluation metrics
  - Why needed here: Understanding precision, recall, F1-score, and accuracy is essential for interpreting model performance in imbalanced datasets.
  - Quick check question: Why might high accuracy be misleading in a dataset with 75% non-suicidal tweets?

- Concept: Pre-trained transformer models and fine-tuning
  - Why needed here: AraBERT and similar models require understanding of self-attention, masked language modeling, and how to adapt them to specific tasks.
  - Quick check question: What is the difference between pre-training and fine-tuning in the context of AraBERT?

## Architecture Onboarding

- Component map: Data collection -> Annotation -> Feature extraction (BOW, TF-IDF, embeddings) -> Model training (ML and DL) -> Evaluation -> Deployment
- Critical path: Clean data -> Expert annotation -> Character n-gram feature extraction -> AraBERT fine-tuning -> Performance validation
- Design tradeoffs: ML models are faster and interpretable but less accurate; DL models are more accurate but require more resources and less interpretable.
- Failure signatures: High false negatives (missing suicidal tweets) suggest annotation or feature extraction issues; high false positives suggest overfitting to non-suicidal patterns.
- First 3 experiments:
  1. Train SVM and RF on character n-gram TF-IDF and compare to word-level features.
  2. Fine-tune AraBERT on the dataset and compare to other pre-trained models (AraELECTRA, AraGPT2).
  3. Analyze model errors to identify patterns in misclassified tweets (e.g., dialectal variation, sarcasm).

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several important questions emerge:

### Open Question 1
- Question: What are the long-term clinical outcomes of using machine learning models to detect suicidal ideation on social media platforms in Arabic-speaking populations?
- Basis in paper: [explicit] The paper mentions that machine learning techniques can help provide an accurate and effective mechanism to conduct extensive screening through social media platforms with the aim of detecting suicidal ideations and potentially preventing suicidal behaviors.
- Why unresolved: The paper focuses on model performance metrics (accuracy, precision, recall) but does not evaluate real-world clinical outcomes or intervention effectiveness.
- What evidence would resolve it: Longitudinal studies tracking whether detected individuals receive appropriate interventions and whether this reduces actual suicide rates.

### Open Question 2
- Question: How do different Arabic dialects affect the performance of pre-trained models like AraBERT in detecting suicidal ideation?
- Basis in paper: [explicit] The dataset includes tweets from different Arabic-speaking countries using different Arabic dialects, and the paper notes that Arabic varieties have different lexicons, grammar, and morphology.
- Why unresolved: The paper does not perform dialect-specific analysis or compare model performance across different Arabic dialects.
- What evidence would resolve it: Performance analysis of the models stratified by dialect or region to identify potential variations in detection accuracy.

### Open Question 3
- Question: What are the ethical implications and potential harms of using social media data for suicide detection, particularly regarding privacy and false positives?
- Basis in paper: [inferred] While the paper focuses on technical performance, the sensitive nature of suicide detection and use of personal social media data raises ethical considerations that are not addressed.
- Why unresolved: The paper does not discuss ethical frameworks, privacy protections, or the potential consequences of false positives/negatives in suicide detection.
- What evidence would resolve it: Ethical impact assessments and user studies examining perceptions of privacy and trust in automated suicide detection systems.

## Limitations

- The dataset, while substantial for Arabic suicidality detection, may not capture the full diversity of Arabic dialects and cultural contexts where suicidal expressions vary significantly.
- The paper lacks detailed discussion of preprocessing steps specific to Arabic text normalization, which could significantly impact model performance.
- The absence of cross-validation results and explicit handling of class imbalance (75% non-suicidal tweets) raises concerns about potential overfitting to the majority class.

## Confidence

- AraBERT performance claims (91% accuracy, 88% F1): High confidence - supported by clear quantitative results and comparison with other models
- Expert annotation quality claims: Medium confidence - only partially supported by mention of one psychology expert without detailed inter-annotator agreement statistics
- Character n-gram feature effectiveness: Medium confidence - supported by results but lacks comparative analysis with other feature types
- Generalizability to real-world deployment: Low confidence - insufficient discussion of dialectal variation, cultural context, and model robustness

## Next Checks

1. Conduct cross-validation experiments with the existing dataset to assess model stability and prevent overfitting to specific data splits
2. Test the trained models on an independent dataset from different time periods or geographic regions to evaluate real-world generalization
3. Perform ablation studies comparing AraBERT performance with and without Arabic-specific preprocessing (diacritization, dialect normalization, spelling standardization)