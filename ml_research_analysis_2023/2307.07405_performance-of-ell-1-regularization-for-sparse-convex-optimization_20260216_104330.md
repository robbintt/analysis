---
ver: rpa2
title: Performance of $\ell_1$ Regularization for Sparse Convex Optimization
arxiv_id: '2307.07405'
source_url: https://arxiv.org/abs/2307.07405
tags:
- group
- lasso
- convex
- algorithm
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies theoretical guarantees for \u21131 regularization\
  \ algorithms (LASSO and Group LASSO) in sparse convex optimization beyond the linear\
  \ regression setting. Prior work had shown such guarantees only for statistical\
  \ problems with average-case inputs."
---

# Performance of $\ell_1$ Regularization for Sparse Convex Optimization

## Quick Facts
- arXiv ID: 2307.07405
- Source URL: https://arxiv.org/abs/2307.07405
- Reference count: 40
- Primary result: Proves Group LASSO selects same features as Group OMP for any strictly convex function

## Executive Summary
This paper establishes theoretical guarantees for ℓ1 regularization algorithms (LASSO and Group LASSO) in sparse convex optimization beyond the linear regression setting. The authors prove that Group LASSO with strong regularization on unselected features selects the same features as Group Orthogonal Matching Pursuit (OMP), which has known recovery guarantees under restricted strong convexity and smoothness. This resolves open questions about why LASSO and Group LASSO work well in practice for convex optimization. As an application, they provide the first algorithms for column subset selection under general loss functions with restricted strong convexity and smoothness.

## Method Summary
The paper uses Fenchel duality to analyze Group LASSO regularization in sparse convex optimization. By showing that large regularization on unselected features forces the dual solution to have zero gradient on unconstrained coordinates, the authors establish an equivalence between Group LASSO and Group OMP feature selection. This theoretical framework is then applied to derive provable guarantees for column subset selection problems under general loss functions. The analysis also extends to attention-based feature selection algorithms, showing they inherit the same guarantees as Group LASSO.

## Key Results
- Group LASSO with strong regularization on unselected features selects the same features as Group OMP for any strictly convex function
- Group Sequential Attention algorithm is equivalent to Group Sequential LASSO and inherits its guarantees
- First provable algorithms for column subset selection under general loss functions with restricted strong convexity and smoothness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** For any strictly convex function, applying Group LASSO with strong regularization on unselected features selects the same features as Group Orthogonal Matching Pursuit (OMP).
- **Mechanism:** The authors exploit Fenchel duality to show that when regularization λ is large enough, the dual problem's optimizer has zero gradient on unconstrained coordinates, implying the primal solution is supported only on coordinates maximizing the gradient magnitude. This selection rule matches OMP's update.
- **Core assumption:** The objective function is strictly convex and differentiable; groups are disjoint.
- **Evidence anchors:**
  - [abstract] "We show that if a sufficiently large Group LASSO regularization is applied when minimizing a strictly convex function l, then the minimizer is a sparse vector supported on vector-valued features with the largest ℓ2 norm of the gradient."
  - [section] "We show that if we add the Group LASSO regularization only on unselected features i∈ S and take λ as large as possible without causing the solution βλ to be zero, then βλ must be supported on a group of features maximizing the ℓ2 gradient mass at β∞ among the unselected features i∈ S."
  - [corpus] Weak evidence: No corpus papers directly address the Fenchel duality argument for Group LASSO in deterministic convex optimization.
- **Break condition:** If the function is not strictly convex, or groups overlap, the equivalence may fail.

### Mechanism 2
- **Claim:** The selected features by Group Sequential LASSO are the same as those selected by Group OMP, enabling recovery guarantees for sparse convex optimization.
- **Mechanism:** By iteratively applying the equivalence from Mechanism 1, each step of Group Sequential LASSO mimics a Group OMP step, inheriting its theoretical guarantees under restricted strong convexity and smoothness.
- **Core assumption:** The restricted strong convexity and smoothness conditions hold for the objective function.
- **Evidence anchors:**
  - [abstract] "Thus, repeating this procedure selects the same set of features as the Orthogonal Matching Pursuit algorithm, which admits recovery guarantees for any function l with restricted strong convexity and smoothness via weak submodularity arguments."
  - [section] "This answers open questions of Tibshirani et al. [TBF+12] and Yasuda et al. [YBC+23]."
  - [corpus] Weak evidence: No corpus papers explicitly discuss the connection between Group LASSO and OMP beyond statistical settings.
- **Break condition:** If restricted strong convexity or smoothness fails, the recovery guarantees may not hold.

### Mechanism 3
- **Claim:** The Group Sequential Attention algorithm is equivalent to Group Sequential LASSO, inheriting its guarantees.
- **Mechanism:** The authors show that the attention-based formulation with Hadamard products and ℓ2 regularization is equivalent to the Group LASSO objective, so the same Fenchel duality argument applies.
- **Core assumption:** The attention mechanism can be reformulated as a Group LASSO problem.
- **Evidence anchors:**
  - [abstract] "Our result also generalizes provable guarantees for the Sequential Attention algorithm, which is a feature selection algorithm inspired by the attention mechanism proposed by Yasuda et al. [YBC+23]."
  - [section] "Thus, the attention-inspired feature selection algorithm given in Algorithm 3 also enjoys the same guarantees as the Group Sequential LASSO algorithm."
  - [corpus] Weak evidence: No corpus papers directly address the equivalence between attention mechanisms and LASSO regularization.
- **Break condition:** If the reformulation fails or the attention weights cannot be optimized jointly, the equivalence may break.

## Foundational Learning

- **Concept:** Fenchel duality and the Fenchel-Young inequality
  - **Why needed here:** The proof hinges on relating primal and dual variables via the Fenchel-Young inequality to show that large regularization leads to sparse solutions supported on gradient-maximizing features.
  - **Quick check question:** If u = ∇l(z) and z = ∇l*(−u), what does the Fenchel-Young inequality tell us about the relationship between l(z) and l*(−u)?

- **Concept:** Restricted strong convexity and smoothness
  - **Why needed here:** These conditions are required to establish recovery guarantees for the sparse solutions selected by Group OMP (and thus Group LASSO).
  - **Quick check question:** How does restricted strong convexity differ from standard strong convexity, and why is it important for sparse recovery?

- **Concept:** Group sparsity and ℓ2 norm regularization
  - **Why needed here:** The Group LASSO regularizer ∑∥β|Ti∥2 encourages entire groups of features to be selected together, which is the setting studied in this paper.
  - **Quick check question:** Why does using the ℓ2 norm within groups (rather than ℓ1) lead to group-wise selection?

## Architecture Onboarding

- **Component map:** Objective function l (strictly convex) -> Groups T1,...,Tt (disjoint) -> Regularization parameter λ -> Fenchel dual l* -> Restricted strong convexity/smoothness constants (μ, L)

- **Critical path:**
  1. Formulate the Group LASSO objective with regularization on unselected features
  2. Derive the dual problem using Fenchel duality
  3. Show that large λ forces the dual solution to have zero gradient on unconstrained coordinates
  4. Use the Fenchel-Young inequality to relate this to the primal solution's support
  5. Iterate to mimic Group OMP and inherit its guarantees

- **Design tradeoffs:**
  - Strong regularization (large λ) ensures correct feature selection but may hurt optimization stability
  - Using disjoint groups simplifies the analysis but may not capture all practical scenarios
  - Restricted strong convexity/smoothness are sufficient but may be hard to verify in practice

- **Failure signatures:**
  - If the function is not strictly convex, the equivalence may fail
  - If groups overlap, the analysis needs modification
  - If restricted strong convexity or smoothness do not hold, recovery guarantees may not apply

- **First 3 experiments:**
  1. Verify the equivalence between Group LASSO and Group OMP on a simple strictly convex function with known groups
  2. Test the recovery guarantees on a synthetic sparse convex optimization problem with controlled restricted strong convexity and smoothness
  3. Apply the Group Sequential Attention algorithm to a real-world feature selection task and compare its performance to Group LASSO

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Group Sequential LASSO algorithm be analyzed to provide recovery guarantees for one-shot usage, where the regularization parameter λ is chosen to select exactly k features in a single application?
- **Basis in paper:** [explicit] The authors mention that one-shot variants of OMP or greedy algorithms have recovery guarantees, but their proof techniques do not immediately apply to the one-shot LASSO.
- **Why unresolved:** The authors' proof relies on the property that for large enough regularizations λ, the resulting solution is close to the λ = ∞ solution, which is not true when λ can be much smaller.
- **What evidence would resolve it:** A proof showing that the one-shot Group Sequential LASSO with a carefully chosen λ provides recovery guarantees of the form of (1) or (2) with an approximation factor γ.

### Open Question 2
- **Question:** Can the results for ℓ1 regularization be extended to nuclear norm regularization for rank-constrained convex optimization problems?
- **Basis in paper:** [explicit] The authors mention that nuclear norm regularization has been shown to recover low-rank solutions in special cases, such as affine rank minimization, and suggest that their results may have a natural generalization in this setting.
- **Why unresolved:** The authors do not provide any specific evidence or techniques for extending their results to nuclear norm regularization.
- **What evidence would resolve it:** A proof showing that nuclear norm regularization can be used to simulate the Group OMP algorithm for rank-constrained convex optimization problems, leading to recovery guarantees similar to those obtained for ℓ1 regularization.

### Open Question 3
- **Question:** Can the analysis of ℓ1 regularization be extended beyond convex functions to non-convex functions that satisfy the Polyak-Łojasiewicz condition?
- **Basis in paper:** [explicit] The authors mention that the analysis of OMP carries through to smooth functions that satisfy the Polyak-Łojasiewicz condition, and ask if a similar generalization can be shown for their results.
- **Why unresolved:** The authors note that there are several parts of their proofs that crucially use convexity, but the LASSO is known to give good results even for non-convex functions in practice.
- **What evidence would resolve it:** A proof showing that the Group Sequential LASSO algorithm provides recovery guarantees for non-convex functions that satisfy the Polyak-Łojasiewicz condition, using techniques that do not rely on convexity.

## Limitations
- The theoretical analysis relies heavily on Fenchel duality arguments with limited empirical validation in this specific context
- Restricted strong convexity and smoothness conditions, while sufficient for theoretical results, may be difficult to verify in practice
- The analysis assumes disjoint groups, which may not capture all practical scenarios

## Confidence
- **High confidence**: The equivalence between Group Sequential LASSO and Group OMP under strong regularization
- **Medium confidence**: The extension to Group Sequential Attention
- **Medium confidence**: The recovery guarantees under restricted strong convexity and smoothness

## Next Checks
1. Empirically verify the feature selection equivalence between Group Sequential LASSO and Group OMP on synthetic convex functions with varying degrees of strict convexity and group structures
2. Test the Group Sequential Attention algorithm on a benchmark feature selection dataset to assess whether it indeed selects the same features as Group Sequential LASSO
3. Analyze the sensitivity of the algorithms to the regularization parameter λ to determine how large it needs to be for the theoretical guarantees to hold in practice