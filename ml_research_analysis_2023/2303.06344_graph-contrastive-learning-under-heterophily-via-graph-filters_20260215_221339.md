---
ver: rpa2
title: Graph Contrastive Learning under Heterophily via Graph Filters
arxiv_id: '2303.06344'
source_url: https://arxiv.org/abs/2303.06344
tags:
- graph
- representations
- node
- heterophily
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HLCL addresses the challenge of contrastive learning under heterophily
  by leveraging a high-pass graph filter alongside a traditional low-pass filter to
  capture both similarities and dissimilarities between nodes and their neighbors.
  The method contrasts high-pass and low-pass filtered representations of the same
  node to learn rich embeddings that are robust under heterophily.
---

# Graph Contrastive Learning under Heterophily via Graph Filters

## Quick Facts
- arXiv ID: 2303.06344
- Source URL: https://arxiv.org/abs/2303.06344
- Reference count: 37
- Key outcome: HLCL outperforms state-of-the-art graph CL methods by up to 10% under heterophily and large-scale datasets by up to 7%, while maintaining strong performance under homophily.

## Executive Summary
HLCL introduces a novel graph contrastive learning approach that addresses the challenge of learning node representations under heterophily. The method leverages a dual-filter architecture combining high-pass and low-pass graph filters to capture both similarities and dissimilarities in node neighborhoods. By contrasting representations from these two filters, HLCL learns robust embeddings without requiring explicit negative pairs, making it particularly effective for heterophilous graphs where traditional contrastive methods struggle.

## Method Summary
HLCL uses a two-layer GNN encoder with shared weights across two channels: one applying a low-pass filter (normalized adjacency matrix) and another applying a high-pass filter (normalized Laplacian matrix). The method generates two views of each node through these filters and applies InfoNCE contrastive loss to maximize agreement between the filtered representations without using explicit negative pairs. The final node representations are taken from the low-pass channel for downstream tasks. The approach is designed to be scalable through message-passing implementation, avoiding explicit matrix calculations.

## Key Results
- Outperforms state-of-the-art graph CL methods by up to 10% on benchmark datasets under heterophily
- Achieves up to 7% improvement on large-scale real-world datasets
- Maintains strong performance under homophily conditions
- Eliminates need for explicit graph augmentation while remaining competitive

## Why This Works (Mechanism)

### Mechanism 1
Contrasting high-pass and low-pass filtered representations captures both similarity and dissimilarity in a neighborhood. The low-pass filter (normalized adjacency) smooths node features, pulling together representations of similar nodes, while the high-pass filter (normalized Laplacian) amplifies differences, making dissimilar nodes distinct. The contrastive loss pulls together representations of the same node under both filters, forcing the encoder to learn a unified representation that encodes both aspects.

### Mechanism 2
The high-pass filter eliminates the need for negative pairs in contrastive learning. By construction, the high-pass filter produces representations that are naturally dissimilar for nodes with different features in the same neighborhood. When these dissimilar representations are used as positives in the contrastive loss, they implicitly push away representations of truly different nodes, acting as a form of self-supervised negative sampling.

### Mechanism 3
HLCL can be applied to large-scale graphs without explicit augmentation due to its message-passing implementation. The high-pass and low-pass filtered representations can be computed through message passing, avoiding the need to explicitly construct and store the Laplacian matrix. This allows HLCL to scale to graphs with millions of nodes and edges.

## Foundational Learning

- Concept: Graph filters (low-pass vs high-pass)
  - Why needed here: Understanding the spectral properties of the adjacency and Laplacian matrices is crucial for designing effective graph filters. The low-pass filter smooths the graph signal, while the high-pass filter amplifies differences.
  - Quick check question: What is the difference between the eigenvalues of the normalized adjacency matrix and the normalized Laplacian matrix, and how does this relate to their filtering properties?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning is the core learning paradigm used in HLCL. Understanding how it works, including the role of positive and negative pairs, is essential for grasping the method.
  - Quick check question: How does the InfoNCE loss function encourage the model to learn meaningful representations?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: HLCL uses a GNN encoder with two different filters. Understanding how GNNs work, including message passing and aggregation, is necessary for implementing and understanding HLCL.
  - Quick check question: How does a typical GNN layer aggregate information from a node's neighborhood, and how does this relate to the concept of a low-pass filter?

## Architecture Onboarding

- Component map: Input Graph → Encoder (low-pass and high-pass channels) → Projection Head → Contrastive Loss → Updated Encoder → Output

- Critical path: Input → Encoder (low-pass and high-pass channels) → Projection Head → Contrastive Loss → Updated Encoder → Output

- Design tradeoffs:
  - Using a single encoder with two filters vs. two separate encoders: Single encoder reduces parameters and enforces shared representations, but may be less flexible.
  - Using explicit graph augmentation vs. relying solely on graph filters: Augmentation can boost performance under homophily but may harm it under heterophily.

- Failure signatures:
  - Poor performance under heterophily: May indicate that the high-pass filter is not effectively capturing dissimilarities or that the contrastive loss is not properly balancing the two filters.
  - Overfitting on small graphs: May indicate that the model is too complex for the amount of data.

- First 3 experiments:
  1. Train HLCL on a heterophilous dataset (e.g., Chameleon) and evaluate the learned representations using a linear probe. Compare the performance to a baseline GNN.
  2. Visualize the low-pass and high-pass filtered representations using t-SNE to verify that the high-pass filter is capturing dissimilarities.
  3. Train HLCL on a homophilous dataset (e.g., Cora) with and without explicit graph augmentation to assess the impact of augmentation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of HLCL vary when applied to graphs with different levels of heterophily (e.g., medium heterophily vs. low homophily)?
- Basis in paper: The paper mentions that HLCL is particularly beneficial for graphs with heterophily, but it also notes that real-world networks often have neighborhoods with medium homophily ratios.
- Why unresolved: The paper does not provide specific performance metrics for graphs with varying levels of heterophily.
- What evidence would resolve it: Detailed experimental results comparing HLCL's performance across graphs with different heterophily ratios.

### Open Question 2
- Question: What is the impact of using different types of high-pass and low-pass graph filters on the performance of HLCL?
- Basis in paper: The paper mentions that other types of high-pass and low-pass filters can be used in a similar way in the HLCL framework, but it does not explore the impact of different filters.
- Why unresolved: The paper does not investigate the performance of HLCL with various filter types.
- What evidence would resolve it: Experiments comparing HLCL's performance using different combinations of high-pass and low-pass filters.

### Open Question 3
- Question: How does HLCL perform in dynamic graphs where the structure and features of the graph may change over time?
- Basis in paper: The paper does not discuss the application of HLCL to dynamic graphs, but it does mention the scalability of HLCL to large real-world graphs.
- Why unresolved: The paper does not address the adaptability of HLCL to changing graph structures and features.
- What evidence would resolve it: Experimental results demonstrating HLCL's performance on dynamic graphs over time.

## Limitations
- Effectiveness under mixed homophily levels within the same graph is not fully characterized
- Theoretical justification for eliminating negative pairs lacks empirical ablation studies
- Scalability claims depend on efficient message-passing implementation which may not hold for extremely sparse or irregular graphs

## Confidence

- High: The overall design of using dual filters to capture both similarities and dissimilarities is sound and well-supported by spectral graph theory.
- Medium: The experimental results showing performance gains under heterophily are convincing, but the lack of hyperparameter details and ablation studies on the necessity of negative pairs reduces confidence in the completeness of the evaluation.
- Low: The claim about eliminating negative pairs is the least substantiated and would benefit from targeted experiments.

## Next Checks

1. Conduct an ablation study to compare HLCL's performance with and without explicit negative sampling, to validate the claim that high-pass filtering makes negative pairs unnecessary.
2. Test HLCL on graphs with regions of varying homophily levels to assess its ability to balance the competing needs of the two filters in mixed scenarios.
3. Perform a sensitivity analysis on key hyperparameters (e.g., learning rate, temperature in InfoNCE) to determine their impact on performance and robustness.