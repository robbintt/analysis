---
ver: rpa2
title: Non-backtracking Graph Neural Networks
arxiv_id: '2310.07430'
source_url: https://arxiv.org/abs/2310.07430
tags:
- graph
- node
- time
- non-backtracking
- latexit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes non-backtracking graph neural networks (NBA-GNNs)
  to address the issue of message flow redundancy in standard GNNs, where messages
  can backtrack and revisit nodes through the same edge. NBA-GNNs update messages
  using non-backtracking transitions, preventing the reincorporation of messages from
  the previously visited node.
---

# Non-backtracking Graph Neural Networks

## Quick Facts
- arXiv ID: 2310.07430
- Source URL: https://arxiv.org/abs/2310.07430
- Reference count: 40
- This paper proposes non-backtracking graph neural networks (NBA-GNNs) to address message flow redundancy in standard GNNs, showing competitive performance on long-range graph benchmarks and improved expressive power for sparse stochastic block models.

## Executive Summary
This paper introduces non-backtracking graph neural networks (NBA-GNNs) as a novel approach to address message flow redundancy in standard graph neural networks. By updating messages using non-backtracking transitions that prevent reincorporation of messages from previously visited nodes, NBA-GNNs aim to alleviate over-squashing and improve expressive power for recovering sparse stochastic block models. The authors provide theoretical analysis showing how NBA-GNNs reduce the sensitivity bound compared to conventional GNNs, and demonstrate empirical improvements across various tasks and datasets.

## Method Summary
NBA-GNNs extend standard message passing by maintaining hidden features for directed edge transitions (h(t)_j→i) and updating them only using non-backtracking predecessors. The method initializes edge features using node and edge attributes, then iteratively updates messages through non-backtracking aggregation before pooling edge features to produce node predictions. The approach can wrap around existing GNN backbones (GCN, GIN, etc.) and includes a "begrudgingly backtracking" variant to handle degree-1 nodes. The theoretical analysis leverages the non-backtracking matrix B and incidence matrix C to derive improved sensitivity bounds and demonstrate better SBM recovery properties for sparse graphs.

## Key Results
- NBA-GNNs demonstrate competitive performance on long-range graph benchmark tasks, achieving AP scores of 84.9 and MAE of 0.368
- The method consistently improves over conventional GNNs across various node classification tasks, with F1 scores of 84.1-83.5 compared to baseline GNNs
- NBA-GNNs can recover sparse stochastic block models with substantially lower average degree requirements (ω(1) and no(1)) compared to conventional GNNs requiring Ω(log n)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NBA-GNNs reduce message flow redundancy by preventing backtracking transitions, leading to better sensitivity to individual walks.
- Mechanism: By associating hidden features with directed edge transitions and updating them only using non-backtracking predecessors, NBA-GNNs eliminate redundant message flows that would otherwise revisit the same node through the same edge.
- Core assumption: The redundancy in conventional GNNs comes primarily from backtracking transitions, and removing them preserves sufficient expressive power while reducing over-squashing.
- Evidence anchors:
  - [abstract]: "NBA-GNNs update messages using non-backtracking transitions, preventing the reincorporation of messages from the previously visited node."
  - [section]: "We explain how the redundancy is harmful to the GNN since the number of walks increases exponentially as the number of layers grows and the GNN becomes insensitive to a particular walk information."
- Break condition: If backtracking transitions contain unique information not expressible through non-backtracking walks, or if computational overhead outweighs benefits.

### Mechanism 2
- Claim: NBA-GNNs alleviate over-squashing by reducing the Jacobian sensitivity bound compared to conventional GNNs.
- Mechanism: The non-backtracking matrix B has better spectral properties than the adjacency matrix A, leading to slower decay in the sensitivity bound.
- Core assumption: The spectral properties of the non-backtracking matrix directly translate to better information propagation in the GNN framework.
- Evidence anchors:
  - [abstract]: "We theoretically investigate how NBA-GNN alleviates the over-squashing of GNNs"
  - [section]: "To derive our analysis, we introduce the non-backtracking matrix B ∈ {0, 1}^{2|E|×2|E|} and the incidence matrix C ∈ R^{2|E|×|V|} which describe the NBA-GNN message-passing and node-wise aggregation via linear operation"
- Break condition: If theoretical bounds don't translate to practical improvements, or additional complexity negates benefits.

### Mechanism 3
- Claim: NBA-GNNs can recover sparse stochastic block models (SBMs) with lower average degree requirements than conventional GNNs.
- Mechanism: The non-backtracking matrix has spectral separation properties even for sparse graphs, allowing NBA-GNNs to distinguish between ER and SBM graphs.
- Core assumption: The spectral properties of the non-backtracking matrix B are preserved and exploitable in the NBA-GNN framework for community detection tasks.
- Evidence anchors:
  - [abstract]: "NBA-GNNs can detect the underlying structure of SBMs even for very sparse graphs"
  - [section]: "Unlike traditional GNNs that operate on adjacency matrices and necessitate an average degree of at least Ω(log n), NBA-GNN demonstrates the ability to perform node classification with a substantially lower average degree bound of ω(1) and no(1)"
- Break condition: If SBM assumptions don't hold for real-world graphs, or spectral properties degrade due to noise.

## Foundational Learning

- Concept: Message Passing Neural Networks (MPNNs)
  - Why needed here: NBA-GNNs are an extension of MPNNs, so understanding the basic message passing framework is essential to grasp the non-backtracking modification.
  - Quick check question: How does a standard MPNN update node features using messages from neighbors, and what is the mathematical form of this update?

- Concept: Spectral Graph Theory
  - Why needed here: The paper relies heavily on spectral properties of matrices (adjacency, non-backtracking) to analyze expressive power and over-squashing. Understanding eigenvalues and eigenvectors is crucial.
  - Quick check question: What is the relationship between the eigenvalues of a graph matrix and the graph's structural properties like connectivity and community structure?

- Concept: Stochastic Block Models (SBMs)
  - Why needed here: The paper evaluates NBA-GNNs on SBM recovery tasks, so understanding how SBMs generate graphs with community structure is important for interpreting the results.
  - Quick check question: How does an SBM generate edges between nodes based on their community memberships, and what makes sparse SBMs challenging for conventional GNNs?

## Architecture Onboarding

- Component map:
  - Node features -> Edge initialization -> Non-backtracking message update -> Node aggregation -> Predictions

- Critical path:
  1. Initialize edge features using node features and edge attributes
  2. For each layer, update each edge feature using non-backtracking aggregation
  3. After final layer, aggregate edge features to produce node predictions
  4. Apply backbone-specific transformations within the update rule

- Design tradeoffs:
  - Space complexity: NBA-GNNs require O(|E|) memory for edge features vs O(|V|) for node-based GNNs
  - Expressiveness vs efficiency: Non-backtracking updates provide better theoretical properties but add computational overhead
  - Backbone dependency: Performance depends on underlying GNN backbone's ability to leverage edge-based features

- Failure signatures:
  - Memory errors: If implementation doesn't properly handle the 2|E| edge features
  - Degenerate performance: If non-backtracking isn't implemented correctly for degree-1 nodes
  - Spectral issues: If non-backtracking matrix construction has bugs, leading to poor SBM recovery

- First 3 experiments:
  1. Implement NBA-GCN on Cora dataset and verify it improves over standard GCN
  2. Test begrudgingly backtracking on a graph with degree-1 nodes to confirm it fixes dead-end issue
  3. Run NBA-GIN on a synthetic SBM with low average degree to verify the o(log n) recovery claim

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content and discussion, several areas warrant further investigation:

1. How does NBA-GNN performance scale with graph density variations, particularly for very sparse or very dense graphs compared to standard GNNs?
2. Can the non-backtracking approach be extended to other graph-related tasks beyond node classification, such as link prediction or graph generation?
3. How does NBA-GNN handle dynamic graphs where the graph structure changes over time?

## Limitations

- The computational complexity of tracking directed edge features (O(|E|) vs O(|V|)) could be prohibitive for large-scale applications
- Theoretical claims about over-squashing bounds and SBM recovery lack extensive empirical validation on real-world sparse graphs
- The approach relies heavily on spectral graph theory that may not fully capture practical behavior on noisy, irregular real-world networks

## Confidence

- High confidence in the mechanism description and mathematical formulation of NBA-GNNs
- Medium confidence in the theoretical claims about over-squashing bounds and SBM recovery
- Medium confidence in empirical results due to limited dataset diversity and potential implementation details

## Next Checks

1. Implement the begrudgingly backtracking update and test on graphs with degree-1 nodes to verify it resolves dead-end issues without degrading performance
2. Compare computational overhead and memory usage between NBA-GNNs and conventional GNNs on graphs of increasing size to quantify practical trade-offs
3. Evaluate NBA-GNNs on real-world sparse networks (e.g., social networks, citation graphs) with low average degree to test the o(log n) SBM recovery claims in practical settings