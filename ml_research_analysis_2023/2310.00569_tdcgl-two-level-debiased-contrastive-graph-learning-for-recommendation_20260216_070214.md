---
ver: rpa2
title: 'TDCGL: Two-Level Debiased Contrastive Graph Learning for Recommendation'
arxiv_id: '2310.00569'
source_url: https://arxiv.org/abs/2310.00569
tags:
- learning
- knowledge
- graph
- contrastive
- tdcgl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TDCGL addresses the challenge of long-tail distribution in knowledge
  graph-based recommendation systems, where high-degree nodes dominate and harm the
  performance of modeling user preferences. The proposed method, Two-Level Debiased
  Contrastive Graph Learning (TDCGL), employs a novel two-level debiased contrastive
  learning framework that not only focuses on User-Item pairs but also incorporates
  User-User pairs for modeling higher-order relations.
---

# TDCGL: Two-Level Debiased Contrastive Graph Learning for Recommendation

## Quick Facts
- arXiv ID: 2310.00569
- Source URL: https://arxiv.org/abs/2310.00569
- Reference count: 30
- Key outcome: TDCGL addresses long-tail distribution in KG-based recommendation, achieving NDCG@10 of 0.2134 and Recall@10 of 0.1746 on ML-1M.

## Executive Summary
TDCGL tackles the long-tail distribution problem in knowledge graph-based recommendation systems, where high-degree nodes dominate and harm modeling of user preferences. The method introduces a novel two-level debiased contrastive learning framework that operates on both User-Item and User-User pairs to capture higher-order relations. By incorporating noise-based generation of negation to ensure spatial uniformity, TDCGL significantly outperforms state-of-the-art baselines on ML-1M and Amazon-Book datasets, demonstrating effectiveness in mitigating long-tail distribution issues and exhibiting strong anti-noise capabilities.

## Method Summary
TDCGL employs a Two-Level Debiased Contrastive Learning (TDCL) framework deployed on knowledge graphs, operating on both User-Item and User-User pairs to model higher-order relations. The method combines BPR loss for collaborative filtering, TransR-based knowledge graph loss, and contrastive losses with noise-based negative sampling to reduce sampling bias. The model uses instance weighting to penalize high-similarity false negatives via a threshold ϕ, ensuring spatial uniformity in the embedding space. Training involves a combined loss function that regularizes representations to better capture both collaborative and knowledge graph signals.

## Key Results
- NDCG@10 of 0.2134 and Recall@10 of 0.1746 on ML-1M dataset
- NDCG@10 of 0.071 and Recall@10 of 0.0828 on Amazon-Book dataset
- Ablation studies confirm necessity of each TDCL level, with two-level approach outperforming single-level variants
- Significant improvements over state-of-the-art baselines on open-source datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-level contrastive learning mitigates long-tail and user-level bias by separately modeling User-Item and User-User relationships.
- Mechanism: User-Item contrastive learning captures item-level dependencies while User-User contrastive learning models higher-order social signals, both using noise-based negatives to ensure spatial uniformity.
- Core assumption: The joint modeling of both levels captures richer relational context than either alone, and noise-based negatives reduce false negatives.
- Evidence anchors:
  - [abstract] "we design the Two-Level Debiased Contrastive Learning (TDCL) and deploy it in the KG, which is conducted not only on User-Item pairs but also on User-User pairs for modeling higher-order relations."
  - [section II-B] "we design two sorts of negative samples at the User-User level and User-Item level, which are randomly drawn from in-batch and generated based on white noise distribution (Noise-based samples) respectively."
  - [corpus] Weak; no direct mention of user-level contrastive learning in neighbors.
- Break condition: If noise-based sampling fails to ensure spatial uniformity or if User-User pairs are not sufficiently informative, the debiasing effect collapses.

### Mechanism 2
- Claim: Noise-based negation with instance weighting reduces false negatives and anisotropy in embeddings.
- Mechanism: Random Gaussian noise generates negatives; instance weighting penalizes high-similarity false negatives via a threshold ϕ.
- Core assumption: Random sampling from batch leads to non-uniform embedding space; noise-based generation fills the space uniformly.
- Evidence anchors:
  - [abstract] "to reduce the bias caused by random sampling in contrastive learning, with the exception of the negative samples obtained by random sampling, we add a noise-based generation of negation to ensure spatial uniformity."
  - [section II-B] "α = 0 if simC(e, e−) ≥ ϕ else 1" (instance weighting) and "noised negatives" generation in loss definitions.
  - [corpus] No direct mention of noise-based negation in neighbors.
- Break condition: If ϕ threshold is poorly tuned or noise distribution is not representative, false negatives remain and bias persists.

### Mechanism 3
- Claim: The combined loss (CF + KG + contrastive) regularizes representations to better capture both collaborative and knowledge graph signals.
- Mechanism: BPR loss for CF, TransR-based KG loss for entity/relation embedding, and contrastive losses for bias mitigation are summed with L2 regularization.
- Core assumption: KG embedding captures relational structure while contrastive losses remove sampling bias; together they improve overall representation quality.
- Evidence anchors:
  - [section II-A] "We employ the BPR loss [...] and the loss of L is: L = LCF + ˆLui + ˆLuu + L− ui + L− uu + λ∥θ∥2 2"
  - [section II-C] "The training regimen of TransR meticulously considers the relative ordering between valid triplets and their defective counterparts."
  - [corpus] No direct mention of combined CF/KG/contrastive loss in neighbors.
- Break condition: If any loss component dominates or conflicts with others, the regularization benefit is lost.

## Foundational Learning

- Concept: Graph Neural Networks (GNN) for knowledge graph embedding.
  - Why needed here: TDCGL builds on KGAT's GNN architecture to propagate higher-order relations before applying contrastive learning.
  - Quick check question: How does KGAT use attention to aggregate neighbor embeddings in each layer?
- Concept: Contrastive learning and noise-based sampling.
  - Why needed here: The debiased contrastive framework relies on contrastive objectives and noise-based negatives to counter sampling bias.
  - Quick check question: What is the difference between in-batch negatives and noise-based negatives in contrastive learning?
- Concept: TransR knowledge graph embedding.
  - Why needed here: TDCGL's KG loss uses TransR to embed entities/relations into relation-specific spaces.
  - Quick check question: In TransR, how are entities projected into relation-specific spaces before computing the energy score?

## Architecture Onboarding

- Component map: User-Item graph → GNN encoder → User-Item/User-User contrastive heads → combined loss (BPR + KG + contrastive) → optimizer
- Critical path: Forward pass through GNN → generate embeddings → compute three losses (CF, KG, contrastive) → sum → backward pass
- Design tradeoffs: Adding User-User contrastive head increases computation but improves higher-order signal capture; noise-based negatives add sampling overhead but improve uniformity
- Failure signatures: Degraded NDCG/Recall on tail users/items; unstable contrastive loss values; overfitting to head items
- First 3 experiments:
  1. Train KGAT baseline; verify baseline NDCG/Recall
  2. Add User-Item contrastive head only; compare performance
  3. Add both User-Item and User-User heads; verify improvements over step 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TDCGL change when using different knowledge graph embedding methods (e.g., DistMult, ComplEx, RotatE) instead of TransR?
- Basis in paper: [explicit] The paper mentions using TransR for knowledge graph embedding but does not explore alternative methods.
- Why unresolved: The choice of knowledge graph embedding method could significantly impact the model's performance and generalization capabilities.
- What evidence would resolve it: Conducting experiments with different knowledge graph embedding methods and comparing their performance on the same datasets.

### Open Question 2
- Question: What is the optimal number of negative samples and their generation strategy for each level of TDCL?
- Basis in paper: [inferred] The paper introduces noise-based generation of negation and in-batch negative sampling but does not provide a systematic analysis of their optimal parameters.
- Why unresolved: The choice of negative sampling strategy and the number of negative samples can significantly affect the model's ability to learn meaningful representations and its robustness to noise.
- What evidence would resolve it: Conducting extensive experiments with varying negative sampling strategies and numbers to identify the optimal configuration for each level of TDCL.

### Open Question 3
- Question: How does TDCGL perform on datasets with different levels of user-item interaction sparsity and knowledge graph completeness?
- Basis in paper: [inferred] The paper evaluates TDCGL on two datasets but does not explore its performance across a wide range of sparsity and completeness levels.
- Why unresolved: The effectiveness of TDCGL in handling sparse interactions and incomplete knowledge graphs is crucial for its applicability in real-world scenarios.
- What evidence would resolve it: Conducting experiments on datasets with varying levels of sparsity and knowledge graph completeness to assess TDCGL's performance and robustness.

### Open Question 4
- Question: How does the introduction of temporal dynamics and sequential patterns affect the performance of TDCGL?
- Basis in paper: [inferred] The paper focuses on static user-item interactions and knowledge graphs without considering temporal dynamics or sequential patterns.
- Why unresolved: Incorporating temporal information and sequential patterns can potentially improve the model's ability to capture evolving user preferences and item popularity.
- What evidence would resolve it: Extending TDCGL to incorporate temporal dynamics and sequential patterns, and evaluating its performance on datasets with temporal information.

## Limitations

- The paper lacks detailed implementation specifications for the noise-based negation generation method and its spatial uniformity guarantees.
- Hyperparameter values for temperature (τ) and instance weighting threshold (ϕ) are not provided, making exact reproduction difficult.
- Claims about anti-noise capability and long-tail distribution mitigation require more rigorous statistical validation and sensitivity analysis.

## Confidence

- **High confidence**: The two-level contrastive learning framework concept is well-defined and the improvement over baseline KGAT is demonstrated through quantitative metrics.
- **Medium confidence**: The theoretical justification for noise-based negatives ensuring spatial uniformity is sound, but implementation details are insufficient for exact replication.
- **Low confidence**: Claims about anti-noise capability and long-tail distribution mitigation require more rigorous statistical validation and sensitivity analysis.

## Next Checks

1. Implement ablation studies comparing TDCGL with and without noise-based negatives to quantify their specific contribution.
2. Conduct experiments varying the instance weighting threshold (ϕ) to identify optimal settings and robustness.
3. Perform statistical significance testing on performance improvements across multiple random seeds to validate claimed gains.