---
ver: rpa2
title: 'AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent
  Interaction'
arxiv_id: '2310.20352'
source_url: https://arxiv.org/abs/2310.20352
tags:
- generation
- counterargument
- claim
- reasoning
- argument
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for argument generation that decomposes
  the task into sequential discourse-driven actions, generating claims, reasoning,
  and concessions before composing the final argument. An argument refinement module
  iteratively evaluates and revises drafts using LLM agents.
---

# AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction

## Quick Facts
- arXiv ID: 2310.20352
- Source URL: https://arxiv.org/abs/2310.20352
- Reference count: 19
- Key outcome: A framework that decomposes argument generation into discourse-driven actions and uses agent-based refinement outperforms end-to-end and chain-of-thought baselines on both automatic and human evaluations.

## Executive Summary
This paper introduces AMERICANO, a framework for argument generation that decomposes the task into sequential discourse-driven actions, generating claims, reasoning, and concessions before composing the final argument. An argument refinement module iteratively evaluates and revises drafts using LLM agents. Experiments on counterargument generation show the method outperforms end-to-end and chain-of-thought baselines on both automatic and human evaluations, producing more coherent, persuasive, and diverse arguments. The framework demonstrates the effectiveness of discourse-aware decomposition and agent-based refinement for open-ended text generation.

## Method Summary
The AMERICANO framework generates arguments by first decomposing the task into sequential actions: claim generation, reasoning generation, concession generation, and argument composition. An argument refinement module with evaluation and refinement agents iteratively improves draft arguments based on criteria like relevance, logical consistency, coherence, and persuasion. The method uses zero-shot prompting with GPT-3.5 for generation and GPT-4 for evaluation, applied to counterargument generation on the Reddit CMV dataset.

## Key Results
- AMERICANO outperforms end-to-end and chain-of-thought baselines on both automatic and human evaluations
- The framework produces more coherent, persuasive, and diverse arguments with rich discourse elements
- Iterative refinement with feedback loops improves argument quality compared to single-pass generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing argument generation into sequential discourse-driven actions reduces task complexity and improves output quality.
- **Mechanism:** By breaking the generation process into claim, reasoning, concession, and argument composition steps, each sub-task becomes more tractable for the LLM. This structured decomposition leverages argumentation theory to guide content generation.
- **Core assumption:** LLMs perform better on simpler, well-defined sub-tasks than on monolithic complex tasks.
- **Evidence anchors:** [abstract]: "Our approach decomposes the generation process into sequential actions grounded on argumentation theory"; [section 2]: "Driven by this, we break down the generation into sequential actions that first generates the components and then produces a final argument"
- **Break condition:** If the intermediate components are poorly generated or the final composition step fails to properly integrate them, the overall quality suffers.

### Mechanism 2
- **Claim:** Iterative refinement with feedback loops improves argument coherence and persuasiveness.
- **Mechanism:** The evaluation agent provides criteria-based feedback (relevance, logical consistency, coherence, persuasion) on drafts, which the refinement agent uses to revise and improve the argument. This mimics human writing processes.
- **Core assumption:** LLM-generated content can be meaningfully improved through structured feedback and revision cycles.
- **Evidence anchors:** [abstract]: "we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received"; [section 3.1]: "we leverage M to assess the counterargument draft and generate feedback"
- **Break condition:** If the evaluation agent provides vague or unhelpful feedback, or if the refinement agent fails to effectively incorporate feedback.

### Mechanism 3
- **Claim:** Discourse-aware generation produces more diverse and rhetorically rich arguments.
- **Mechanism:** By explicitly generating concessions and using discourse markers, the framework creates arguments with more complex rhetorical structures. The reasoning generation action specifically targets logical coherence.
- **Core assumption:** Incorporating discourse elements like concessions and markers enhances persuasiveness and diversity beyond simple content generation.
- **Evidence anchors:** [abstract]: "can generate more coherent and persuasive arguments with diverse and rich contents"; [section 2.3]: "A concession, or acknowledgement, is typically employed to produce trust and fortify one's position"; [section 5.4]: Analysis showing increased use of discourse markers like "though" and "regardless" in model outputs
- **Break condition:** If the concession generation introduces contradictory elements or if discourse markers are used inappropriately.

## Foundational Learning

- **Concept:** Argumentation Theory and Discourse Structure
  - Why needed here: The framework relies on understanding how arguments are structured (claims, reasoning, concessions) to decompose the generation task effectively
  - Quick check question: What are the three main components of an argument according to classical argumentation theory?

- **Concept:** Chain-of-Thought Prompting and Task Decomposition
  - Why needed here: The framework builds on CoT prompting by extending it to structured task decomposition rather than just reasoning steps
  - Quick check question: How does task decomposition in this framework differ from standard chain-of-thought prompting?

- **Concept:** Iterative Refinement and Feedback Loops
  - Why needed here: The refinement module depends on understanding how iterative feedback can improve generated content
  - Quick check question: What are the key criteria used by the evaluation agent to assess argument quality?

## Architecture Onboarding

- **Component map:** Generation Agent (claim generation → reasoning generation → concession generation → argument composition) → Evaluation Agent → Refinement Agent → Final Output
- **Critical path:** Proposition → Claim Generation → Reasoning Generation → Concession Generation → Argument Composition → Evaluation → Refinement → Final Output
- **Design tradeoffs:** Single-pass generation vs. multi-step refinement (simplicity vs. quality); Fixed discourse structure vs. adaptive generation (predictability vs. flexibility); LLM-based vs. trained models (zero-shot capability vs. potential performance gains)
- **Failure signatures:** Poor intermediate components leading to incoherent final arguments; Feedback loops getting stuck in cycles of minor revisions; Discourse elements being used inappropriately or contradictorily
- **First 3 experiments:** 1) Compare single-pass generation vs. multi-step decomposition on coherence metrics; 2) Test different numbers of claims generated and ranked to find optimal selection process; 3) Measure the impact of different feedback criteria on refinement effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the refinement module be integrated with the generation agent's actions to provide feedback during intermediate steps rather than just refining the final draft?
- Basis in paper: [explicit] The paper mentions that the refinement module only revises the argument draft without directly modifying the generation agent's actions, and suggests incorporating feedback to improve initial generation.
- Why unresolved: The paper acknowledges this as a potential direction but doesn't explore how to implement real-time feedback during the sequential action generation process.
- What evidence would resolve it: An experiment showing improved initial draft quality when feedback is provided during each sequential action generation step, with quantitative comparison to current approach.

### Open Question 2
- Question: How can the framework be extended to handle interactive, conversational argument generation between multiple debating agents?
- Basis in paper: [explicit] The paper mentions that debating is an interactive process and suggests studying interactive argumentation with multiple debating agents as future work.
- Why unresolved: The current framework generates standalone counterarguments without considering dynamic interaction with other arguments or agents.
- What evidence would resolve it: A system demonstration showing multiple agents generating and responding to arguments in a turn-based debate format, with evaluation of interaction quality.

### Open Question 3
- Question: How can users be allowed to collaboratively modify intermediate discourse components and generate final arguments based on their modifications?
- Basis in paper: [explicit] The paper mentions that decomposing generation into intermediate steps enables users to collaboratively modify results but doesn't tackle this in the current work.
- Why unresolved: The framework currently operates as a fully automated system without user interaction points in the intermediate generation steps.
- What evidence would resolve it: A user study showing the effectiveness of a human-in-the-loop system where users can edit intermediate components (claims, reasoning, concessions) and observe the impact on final argument quality.

## Limitations

- The framework's performance is highly dependent on the quality of intermediate components, with limited analysis of failure cases
- Significant variance in human evaluation (F1 = 0.4 between annotators) suggests challenges with subjective aspects of argument quality
- Zero-shot approach may not generalize well to domains outside the Reddit CMV dataset used for evaluation

## Confidence

**High Confidence:** The core mechanism of discourse-driven decomposition reducing task complexity is well-supported by experimental results, showing consistent improvements over end-to-end baselines across both automatic and human evaluations.

**Medium Confidence:** The iterative refinement module's effectiveness is demonstrated, but the paper doesn't fully explore failure modes or edge cases where feedback loops might get stuck or produce diminishing returns.

**Low Confidence:** The generalizability of the framework to other argument generation tasks or domains remains uncertain, with no empirical evidence for performance on other argument types or domains beyond political counterarguments.

## Next Checks

1. **Component Failure Analysis:** Systematically test the framework's robustness by injecting errors or low-quality outputs at each intermediate generation step (claim, reasoning, concession) to measure how cascading failures affect final argument quality.

2. **Cross-Domain Generalization:** Apply the framework to different argument generation tasks (e.g., persuasive essays, policy recommendations, product reviews) and measure performance degradation or adaptation requirements.

3. **Refinement Efficiency Study:** Analyze the relationship between number of refinement iterations and quality improvements to identify optimal stopping criteria and detect potential feedback loop inefficiencies.