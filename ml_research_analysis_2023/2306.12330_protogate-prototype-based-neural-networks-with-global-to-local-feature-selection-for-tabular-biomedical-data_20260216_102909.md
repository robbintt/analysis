---
ver: rpa2
title: 'ProtoGate: Prototype-based Neural Networks with Global-to-local Feature Selection
  for Tabular Biomedical Data'
arxiv_id: '2306.12330'
source_url: https://arxiv.org/abs/2306.12330
tags:
- feature
- protogate
- features
- selection
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProtoGate is a prototype-based neural network designed to tackle
  the challenges of high-dimensional and low-sample-size biomedical data. It combines
  local feature selection with a prototype-based classifier to achieve both high accuracy
  and interpretability.
---

# ProtoGate: Prototype-based Neural Networks with Global-to-local Feature Selection for Tabular Biomedical Data

## Quick Facts
- **arXiv ID:** 2306.12330
- **Source URL:** https://arxiv.org/abs/2306.12330
- **Reference count:** 40
- **Primary result:** ProtoGate outperforms existing methods in classification accuracy while selecting fewer features and providing interpretable predictions for biomedical data.

## Executive Summary
ProtoGate addresses the challenges of high-dimensional, low-sample-size (HDLSS) biomedical data by combining prototype-based neural networks with a global-to-local feature selection strategy. The method achieves superior classification accuracy while maintaining interpretability through sparse feature selection. By using a non-parametric prototype-based predictor without learnable parameters, ProtoGate avoids the co-adaptation problem common in local feature selection methods. The global-to-local approach balances homogeneity and heterogeneity across samples, making it particularly effective for biomedical data where both shared and sample-specific features are important.

## Method Summary
ProtoGate implements a three-layer neural network with ℓ1 regularization on the first layer for global feature selection, followed by a thresholding function with injected noise for local, instance-wise selection. The prototype-based predictor uses a K-nearest neighbor approach with NeuralSort for differentiable sorting, enabling end-to-end training. The model is trained using a combined loss function that includes both classification error and sparsity regularization. Hyperparameters are tuned via grid search, with K (nearest neighbors) in {1,2,3,4,5} and global regularization λg in {1e-4, 2e-4, 3e-4, 4e-4, 6e-4}.

## Key Results
- Outperforms existing methods in classification accuracy on seven real-world biomedical datasets
- Selects fewer features compared to baseline methods while maintaining or improving accuracy
- Provides interpretable predictions through prototype-based classification and sparse feature selection
- Demonstrates robustness to noise and consistent feature selection quality across training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProtoGate achieves better performance than existing local feature selection methods by avoiding co-adaptation through its prototype-based predictor without learnable parameters.
- Mechanism: The prototype-based predictor (DKNN) uses a non-parametric similarity-based classification that evaluates selected features consistently throughout training, preventing the predictor from encoding predictions into feature selection.
- Core assumption: Feature selection quality should be evaluated independently of the predictor's ability to fit arbitrary patterns in the selected features.
- Evidence anchors:
  - [abstract]: "ProtoGate employs a non-parametric prototype-based prediction mechanism to tackle the co-adaptation problem, ensuring the feature selection results and predictions are consistent with underlying data clusters."
  - [section]: "ProtoGate leverages the selected features with a prototype-based predictor of DKNN. The DKNN predictor makes explainable predictions and encodes the clustering assumption into feature selection. Without learnable parameters, DKNN can further avoid the co-adaptation problem by providing consistent evaluations for the selected features while training the feature selector."
  - [corpus]: Weak - corpus papers don't directly address co-adaptation in feature selection context.
- Break condition: If the data distribution doesn't support clustering assumptions, the prototype-based approach may fail as noted in the design where Syn3(-) dataset with even functions is expected to perform poorly.

### Mechanism 2
- Claim: The global-to-local feature selection strategy improves performance by balancing homogeneity and heterogeneity across samples.
- Mechanism: The first layer uses ℓ1 regularization on weights to select features globally across all samples, while the thresholding function with injected noise enables instance-wise local selection, creating a sparse but flexible selection pattern.
- Core assumption: Biomedical data contains both shared informative features across samples and sample-specific features, requiring a balance between global and local selection.
- Evidence anchors:
  - [abstract]: "ProtoGate employs a global-to-local feature selection strategy, balancing homogeneity and heterogeneity across samples"
  - [section]: "ProtoGate performs feature selection in a global-to-local manner with an ℓ1-regularised gating network. The global-to-local design helps ProtoGate consider the homogeneous and heterogeneous patterns across multiple samples."
  - [corpus]: Weak - corpus papers focus on general tabular learning but don't specifically address the global-to-local balance in feature selection.
- Break condition: If the regularization strength λg is set too high, the degree of local sparsity decreases and performance may suffer as shown in Figure 5 where increasing λg doesn't guarantee accuracy improvement.

### Mechanism 3
- Claim: The clustering assumption encoded in the prototype-based predictor provides appropriate inductive bias for biomedical data.
- Mechanism: DKNN classifies samples based on similarity to K nearest prototypes in the masked feature space, encouraging samples of the same class to have similar representations after feature selection.
- Core assumption: Similar samples in biomedical data should belong to the same class, and this similarity can be measured effectively in a reduced feature space.
- Evidence anchors:
  - [abstract]: "The prototype-based predictor confers ProtoGate two important features: (i) an inductive bias aligned with the clustering assumption in biomedical data"
  - [section]: "The prototype-based predictor confers ProtoGate two important features: (i) an inductive bias aligned with the clustering assumption in biomedical data; and (ii) consistent evaluations of the quality of selected features throughout the training process"
  - [corpus]: Weak - corpus papers don't specifically discuss clustering assumptions in the context of feature selection for biomedical data.
- Break condition: As demonstrated in Syn3(-), if the data contains features that violate clustering assumptions (like even functions that make opposite values equally informative), the prototype-based approach performs poorly.

## Foundational Learning

- Concept: Prototype-based classification with K-nearest neighbors
  - Why needed here: Provides the non-parametric predictor that avoids co-adaptation and encodes clustering assumptions
  - Quick check question: How does DKNN compute similarity between samples in the masked feature space?

- Concept: ℓ1 regularization for feature selection
  - Why needed here: Enables global feature selection in the first layer to capture shared patterns across samples
  - Quick check question: What is the mathematical relationship between ℓ1 regularization on weights and sparsity in the selected features?

- Concept: Differentiable sorting operations
  - Why needed here: NeuralSort enables gradient-based training of the prototype-based predictor with discrete-like neighbor selection
  - Quick check question: How does NeuralSort approximate the permutation matrix for neighbor ranking?

## Architecture Onboarding

- Component map: Input → Global-to-local feature selector (3-layer NN with ℓ1 regularization + thresholding) → Prototype-based predictor (DKNN with NeuralSort) → Output
- Critical path: Feature selector training loop → Prototype base construction → K-nearest neighbor classification → Loss computation (classification + sparsity regularization)
- Design tradeoffs: Prototype-based predictor trades model expressivity for interpretability and co-adaptation prevention; global-to-local selection balances between capturing shared vs sample-specific features
- Failure signatures: Poor performance on datasets with features violating clustering assumptions (even functions); sensitivity to noise when K is too small; suboptimal accuracy when global regularization is too strong
- First 3 experiments:
  1. Verify prototype-based predictor works on synthetic data with clear clusters by comparing accuracy vs random feature selection
  2. Test sensitivity to K parameter by varying K ∈ {1,2,3,4,5} and measuring classification accuracy
  3. Evaluate impact of global sparsity λg by training with different values and measuring degree of local sparsity and classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ProtoGate compare to other methods when applied to datasets with a different number of features or samples than those used in the experiments?
- Basis in paper: [explicit] The paper evaluates ProtoGate on seven real-world biomedical datasets with varying numbers of features and samples, but does not explore performance on datasets with different characteristics.
- Why unresolved: The paper does not provide information on the performance of ProtoGate on datasets with different numbers of features or samples.
- What evidence would resolve it: Experiments on additional datasets with different numbers of features and samples would provide evidence for the generalizability of ProtoGate's performance.

### Open Question 2
- Question: How does the performance of ProtoGate compare to other methods when applied to datasets with different levels of noise or missing values?
- Basis in paper: [inferred] The paper does not explicitly discuss the impact of noise or missing values on the performance of ProtoGate.
- Why unresolved: The paper does not provide information on the performance of ProtoGate on datasets with different levels of noise or missing values.
- What evidence would resolve it: Experiments on additional datasets with different levels of noise or missing values would provide evidence for the robustness of ProtoGate's performance.

### Open Question 3
- Question: How does the choice of the number of nearest neighbors (K) in the prototype-based predictor affect the performance of ProtoGate?
- Basis in paper: [explicit] The paper discusses the impact of the number of nearest neighbors (K) on the performance of ProtoGate in the ablation study (Section 4.3).
- Why unresolved: The paper does not provide a definitive answer on the optimal choice of K for ProtoGate.
- What evidence would resolve it: Further experiments on different datasets with varying values of K would provide evidence for the optimal choice of K for ProtoGate.

## Limitations

- Prototype-based approach may underperform on datasets with features that violate clustering assumptions, as demonstrated on Syn3(-) with even functions
- Performance is sensitive to hyperparameter tuning, particularly the regularization strength λg, which can affect both accuracy and feature selection sparsity
- Limited comparison to modern deep learning methods beyond sparse autoencoders, restricting claims about relative performance in the broader context

## Confidence

- **High confidence**: The core architectural design and theoretical justification for avoiding co-adaptation through non-parametric predictors
- **Medium confidence**: The effectiveness of global-to-local feature selection strategy across diverse biomedical datasets
- **Medium confidence**: The reproducibility of results given detailed implementation specifications

## Next Checks

1. Test ProtoGate on datasets with features that explicitly violate clustering assumptions (e.g., even functions) to verify the identified limitation is consistent across implementations.
2. Perform ablation studies varying K (nearest neighbors) and λg (global regularization) to identify optimal hyperparameter ranges and sensitivity.
3. Compare ProtoGate against modern deep learning baselines (TabPFN, TabNet) on the same biomedical datasets to establish relative performance in the broader context.