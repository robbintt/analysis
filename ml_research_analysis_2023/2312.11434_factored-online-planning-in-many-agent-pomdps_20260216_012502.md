---
ver: rpa2
title: Factored Online Planning in Many-Agent POMDPs
arxiv_id: '2312.11434'
source_url: https://arxiv.org/abs/2312.11434
tags:
- agents
- belief
- action
- particle
- observation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of online planning in multi-agent
  POMDPs, where the exponential growth of action and observation spaces with the number
  of agents hinders traditional single-agent methods. The core method introduces weighted
  particle filtering into sample-based online planners for MPOMDPs, presenting a scalable
  belief approximation and leveraging the locality of agent interactions via coordination
  graphs in sparse particle filter trees.
---

# Factored Online Planning in Many-Agent POMDPs

## Quick Facts
- arXiv ID: 2312.11434
- Source URL: https://arxiv.org/abs/2312.11434
- Reference count: 40
- Primary result: Methods scale to 64 agents in FIRE FIGHTING GRAPH and 6 agents in Multi-Agent ROCK SAMPLE, compared to previous limits of 10 and 2 agents respectively

## Executive Summary
This paper addresses the challenge of online planning in multi-agent POMDPs where traditional single-agent methods fail due to exponential growth of action and observation spaces. The authors introduce weighted particle filtering into sample-based online planners and leverage coordination graphs to exploit locality of agent interactions. The approach simultaneously addresses both belief approximation and value estimation challenges, enabling effective planning in environments with many agents.

## Method Summary
The paper introduces weighted particle filtering for belief state estimation in multi-agent POMDPs, using importance sampling with observation likelihoods to maintain particle diversity. It combines this with coordination graphs that factorize value estimation by decomposing joint Q-values into local edge contributions. The algorithms maintain separate weighted particle filters for each edge in the coordination graph and use factored search trees to handle the exponential action space. Four new algorithms are proposed: FS-W-POMCP, FT-W-POMCP, FS-PFT, and FT-PFT, which differ in their tree structures and belief approximation methods.

## Key Results
- Competitive performance in small-agent settings compared to state-of-the-art baselines
- Outperforms baselines in environments with many agents (up to 64 agents in FIRE FIGHTING GRAPH)
- Scales to 6 agents in Multi-Agent ROCK SAMPLE compared to previous limit of 2 agents
- Demonstrates effectiveness of combining weighted particle filtering with coordination graphs

## Why This Works (Mechanism)

### Mechanism 1
Weighted particle filtering improves belief state estimation in large observation spaces by maintaining likelihood-based weights instead of simple state particles. The algorithm uses importance sampling with observation likelihoods and re-samples based on effective sample size to prevent particle impoverishment. This works under the assumption of conditionally independent observations given the successor state and previous action.

### Mechanism 2
Coordination graphs enable factorization of value estimation by decomposing joint Q-values into local edge contributions. The algorithm maintains local Q-value estimates for each edge and combines them using a mixture of experts approach, reducing the effective action space from exponential to polynomial in the graph structure. This relies on the assumption that agent interactions are local and can be represented by sparse coordination graphs.

### Mechanism 3
The combination of weighted particle filtering with coordination graphs provides both accurate belief estimation and scalable value estimation simultaneously. The algorithm maintains separate weighted particle filters for each edge, samples from the ensemble based on filter quality, and uses factored value estimates. This addresses both the curse of dimensionality in observations and actions under the assumption that local approximation can capture global properties adequately.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper builds on single-agent POMDP theory and extends it to multi-agent settings
  - Quick check question: What is the key difference between a POMDP and an MDP?

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: The algorithms use UCT-based MCTS for online planning in belief space
  - Quick check question: How does UCT balance exploration and exploitation in MCTS?

- Concept: Particle filtering
  - Why needed here: Both weighted and unweighted particle filtering are used for belief state approximation
  - Quick check question: What is the purpose of re-sampling in particle filtering?

- Concept: Coordination graphs
  - Why needed here: These graphs structure agent interactions and enable value factorization
  - Quick check question: How does a coordination graph represent local agent interactions?

## Architecture Onboarding

- Component map: Simulator interface (G) -> Weighted particle filters (per edge) -> Search trees with factored statistics -> Action selection via variable elimination or max-plus inference -> Ensemble sampling from edge filters

- Critical path: 1) Initialize offline belief with weighted particles, 2) Sample from ensemble of edge filters to start simulation, 3) Traverse search tree using UCB1 policy, 4) Update edge-specific statistics and particles, 5) Select final action using graphical inference

- Design tradeoffs: More particles per edge vs. computational cost, static coordination graph vs. dynamic coordination discovery, exact inference vs. approximate inference for action selection, memory for maintaining multiple trees vs. search depth

- Failure signatures: Particle filter deprivation (zero particles after observation update), memory exhaustion from tree expansion, poor performance due to suboptimal coordination graph, slow convergence due to inadequate exploration

- First 3 experiments: 1) Compare W-POMCP vs POMCP on FIRE FIGHTING GRAPH with 4 agents, 2) Test FS-W-POMCP vs FS-POMCP on MULTI-AGENT ROCK SAMPLE with 3 agents, 3) Evaluate FT-PFT vs Sparse-PFT on CAPTURE TARGET with 2 agents

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of the coordination graph (CG) heuristic affect the performance of factored algorithms in MPOMDPs where agent interactions change over time? The paper evaluates CG effectiveness but doesn't systematically investigate how heuristic quality impacts performance across scenarios with varying agent dynamics.

### Open Question 2
What is the impact of the exploration constant c in UCB1 on the convergence and performance of factored tree variants (FT-POMCP and FT-PFT) in MPOMDPs? While the paper discusses theoretical implications of c=0, it doesn't empirically investigate how different values of c affect convergence and performance.

### Open Question 3
How does the choice of action selection method (Max-Plus vs. Variable Elimination) interact with the structure of the coordination graph to influence the performance of factored algorithms? The paper demonstrates that method choice matters but doesn't provide systematic analysis of how this choice interacts with graph structure.

## Limitations
- Effectiveness depends heavily on the assumption of conditionally independent observations, which may break down in environments with correlated multi-agent observations
- Static coordination graph heuristic may fail to capture dynamic coordination patterns that emerge during execution
- Does not adequately address how to automatically discover optimal coordination graph structures or adapt them dynamically during planning

## Confidence

- **High confidence**: Successfully demonstrates improved scalability in multi-agent POMDPs compared to baselines with clear quantitative results
- **Medium confidence**: Theoretical framework for combining weighted particle filtering with coordination graphs is sound but depends heavily on environment-specific parameters
- **Low confidence**: Paper doesn't adequately address automatic coordination graph discovery or dynamic adaptation, which are crucial for real-world applications

## Next Checks

1. Test algorithm performance when the conditional independence assumption is violated by introducing correlated observations between agents in FIRE FIGHTING GRAPH
2. Evaluate sensitivity to coordination graph structure by comparing performance using random vs. heuristic graph constructions on MULTI-AGENT ROCK SAMPLE
3. Measure belief estimation accuracy by comparing weighted particle filter beliefs against ground truth beliefs in environments where exact belief computation is feasible