---
ver: rpa2
title: 'DCTX-Conformer: Dynamic context carry-over for low latency unified streaming
  and non-streaming Conformer ASR'
arxiv_id: '2306.08175'
source_url: https://arxiv.org/abs/2306.08175
tags:
- context
- chunk
- streaming
- left
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap between streaming and
  non-streaming ASR systems, particularly in low latency scenarios with limited past
  context. The authors propose the DCTX-Conformer, which integrates a dynamic contextual
  carry-over (CCO) mechanism into a state-of-the-art unified ASR system.
---

# DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer ASR

## Quick Facts
- arXiv ID: 2306.08175
- Source URL: https://arxiv.org/abs/2306.08175
- Reference count: 0
- Primary result: 25% relative WER reduction in low latency streaming ASR

## Executive Summary
This paper addresses the performance gap between streaming and non-streaming ASR systems, particularly in low latency scenarios with limited past context. The authors propose the DCTX-Conformer, which integrates a dynamic contextual carry-over (CCO) mechanism into a state-of-the-art unified ASR system. The key innovation is a non-overlapping CCO that dynamically considers both the left context of a chunk and preceding context embeddings. Experimental results on multiple datasets show that DCTX-Conformer outperforms the SOTA by a relative 25.0% word error rate, with negligible latency impact.

## Method Summary
The DCTX-Conformer extends the standard Conformer architecture by adding a dynamic contextual carry-over mechanism. The model is trained with dynamic chunk sizes (320-1280ms) and varying left context sizes (0 to full past). Context embeddings are passed between chunks and used in self-attention calculations along with the current chunk's left context. At inference, the model can use multiple preceding context embeddings to improve streaming performance while maintaining low latency.

## Key Results
- 25.0% relative WER reduction compared to state-of-the-art unified streaming/non-streaming models
- Performance improvements maintained across multiple datasets (LibriSpeech, MTDialogue, WSJ, VoxPopuli)
- Flexible inference allowing adjustment of left context size based on latency requirements
- Negligible latency impact from the proposed CCO mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic chunk training with varying left context sizes improves streaming ASR robustness.
- Mechanism: The model is trained with chunk sizes randomly sampled between 8 and 32 frames (320-1280ms) and left context varying between zero and all preceding chunks. This forces the model to handle different streaming scenarios during training.
- Core assumption: Exposing the model to diverse context sizes during training makes it more adaptable to varying latency requirements at inference.
- Evidence anchors:
  - [abstract] "The model is trained with dynamic chunk sizes and left context."
  - [section] "We randomly sample a chunk size between 8 (= 320ms) and 32 (= 1280ms) down-sampled self-attention frames 60% of the time and run a full-contextual training the remaining 40%. Moreover, we use the same dynamic left context mechanism that allows to vary the left context between zero and all preceding chunks so that the model becomes robust to numerous left context sizes at inference time."
  - [corpus] Weak evidence - corpus contains related work on unified streaming/non-streaming models but no direct evidence about dynamic left context training effectiveness.

### Mechanism 2
- Claim: Non-overlapping contextual carry-over with dynamic dependency on preceding chunks improves streaming performance.
- Mechanism: Context embeddings are passed from chunk to chunk, and the model dynamically depends on both the left context of the current chunk and preceding context embeddings. The dependency on preceding chunks is maintained even with context embeddings present.
- Core assumption: Combining context embeddings with a dynamic dependency on preceding chunks provides more effective past context incorporation than using all frames from full past context.
- Evidence anchors:
  - [abstract] "Our proposed dynamic context Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over mechanism that takes into account both the left context of a chunk and one or more preceding context embeddings."
  - [section] "We propose to keep a dynamic dependency on preceding chunks despite the presence of context embeddings. We demonstrate that this combination leads to significant performance gains for insignificant latency drops."
  - [corpus] Weak evidence - corpus contains related work on contextual carry-over but no direct evidence about the specific dynamic dependency mechanism.

### Mechanism 3
- Claim: Using multiple preceding context embeddings at inference time further improves streaming performance.
- Mechanism: While trained with one preceding context embedding, the model can utilize Nctx preceding context embeddings at inference time. The keys and values in self-attention calculations include embeddings from multiple preceding chunks.
- Core assumption: More context embeddings provide richer historical information, leading to better streaming performance.
- Evidence anchors:
  - [abstract] "we show further improvements by relying on more than one preceding context embedding at inference time."
  - [section] "we show further improvements by relying on more than one preceding context embedding at inference time. For instance, in Fig. 2, output chunk #4 would depend on all frames of chunks #3 and #4 and context embeddings #1 (dashed orange squares), #2 (orange squares) and #4 (dark gray squares)."
  - [corpus] Weak evidence - corpus contains related work on memory banks but no direct evidence about the specific use of multiple context embeddings.

## Foundational Learning

- Concept: Self-attention mechanism in Transformers
  - Why needed here: The DCTX-Conformer uses self-attention layers, and understanding how they work is crucial for implementing the contextual carry-over mechanism.
  - Quick check question: How does the self-attention mechanism compute attention scores between queries, keys, and values?

- Concept: Connectionist Temporal Classification (CTC) loss
  - Why needed here: The model uses a joint CTC-attention framework for training, so understanding CTC is essential.
  - Quick check question: What is the main advantage of using CTC loss in ASR models?

- Concept: Dynamic chunk training
  - Why needed here: The model is trained with varying chunk sizes, and understanding this technique is crucial for implementing the DCTX-Conformer.
  - Quick check question: How does dynamic chunk training help unify streaming and non-streaming ASR models?

## Architecture Onboarding

- Component map: Input features -> Conformer encoder -> Context embeddings passed between chunks -> Self-attention with dynamic dependency on left context and preceding embeddings -> Output to CTC decoder

- Critical path:
  1. Input features â†’ Conformer encoder
  2. Context embeddings passed between chunks
  3. Self-attention with dynamic dependency on left context and preceding embeddings
  4. Output to CTC decoder

- Design tradeoffs:
  - Number of context embeddings vs. latency
  - Chunk size vs. streaming performance
  - Left context size vs. memory usage

- Failure signatures:
  - Performance degradation when increasing context embeddings
  - Latency increase beyond acceptable limits
  - Memory overflow with large left context sizes

- First 3 experiments:
  1. Test streaming performance with varying chunk sizes (320ms, 640ms, 1280ms)
  2. Evaluate impact of different numbers of context embeddings (1, 4, 8, 16)
  3. Measure latency and memory usage with different left context sizes (0ms, 320ms, 640ms, 1280ms)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DCTX-Conformer scale with the number of preceding context embeddings in larger-scale ASR systems with longer utterance lengths?
- Basis in paper: [explicit] The paper shows improvements with up to 16 context embeddings on small-scale and LibriSpeech datasets, but notes faster saturation for shorter utterances like MTDialogue (average 3s). It also mentions that larger training sets lead to better modeling of context embeddings.
- Why unresolved: The paper only tests up to 16 context embeddings and doesn't explore systems with longer average utterance lengths or truly large-scale training data beyond the 10k hour subset used.
- What evidence would resolve it: Experiments showing WER trends as the number of context embeddings increases (e.g., 32, 64, 128) on datasets with longer average utterance lengths and larger training sets (e.g., 100k+ hours).

### Open Question 2
- Question: What is the impact of varying the dynamic chunk training range (e.g., beyond 8-32 frames) on the performance of unified ASR systems with CCO?
- Basis in paper: [explicit] The paper uses a dynamic chunk training range of 8-32 frames (320-1280ms) and mentions that the model is trained with dynamic chunk sizes, but doesn't explore the effect of wider or narrower training ranges.
- Why unresolved: The paper only tests one specific range and doesn't provide a systematic study of how different ranges affect performance, especially in combination with CCO.
- What evidence would resolve it: Experiments comparing models trained with different chunk size ranges (e.g., 4-64 frames, 16-128 frames) and measuring WER and latency trade-offs.

### Open Question 3
- Question: How does the proposed DCTX-Conformer compare to other memory-based approaches (e.g., memory banks) in terms of parameter efficiency and generalization to out-of-domain data?
- Basis in paper: [inferred] The paper mentions that context embeddings are only initialized in the first layer and are not recomputed at every layer, unlike memory banks in [24,25]. It also tests on diverse public test sets but doesn't explicitly compare parameter efficiency or out-of-domain generalization.
- Why unresolved: The paper doesn't provide a direct comparison of parameter counts or test on truly out-of-domain data, and the discussion on memory banks is brief.
- What evidence would resolve it: A comparative study of DCTX-Conformer vs. memory bank approaches with matched parameter counts, and evaluation on out-of-domain datasets (e.g., different accents, noise conditions, or languages).

## Limitations
- The paper lacks detailed ablation studies on the relative contribution of each mechanism to the overall WER reduction
- Comprehensive latency measurements for different configurations are not provided
- The 60/40 split between dynamic chunk training and full-context training is not justified
- Absolute WER values on some datasets are not provided, making practical significance assessment difficult

## Confidence
- High confidence: The core mechanism of dynamic contextual carry-over and its integration into the Conformer architecture is technically sound and well-implemented
- Medium confidence: The performance improvements are real but the relative contribution of each proposed mechanism needs further validation through ablation studies
- Medium confidence: The flexibility to adjust context size at inference time is valuable, but the practical latency implications across different configurations need more thorough characterization

## Next Checks
1. Conduct controlled experiments isolating the three main mechanisms (dynamic chunk training, CCO with dynamic dependency, multiple context embeddings) to quantify their individual contributions to the 25% relative WER improvement
2. Measure and report end-to-end latency for different configurations (varying chunk sizes, left context sizes, and numbers of context embeddings) to provide a complete picture of the latency-performance tradeoff
3. Evaluate model performance with mismatched inference conditions (e.g., training with one context embedding but inference with different numbers) to verify the claimed robustness to varying latency requirements