---
ver: rpa2
title: Investigating the Effects of Sparse Attention on Cross-Encoders
arxiv_id: '2312.17649'
source_url: https://arxiv.org/abs/2312.17649
tags:
- attention
- window
- tokens
- document
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the effects of sparse attention on cross-encoders,
  which are effective passage and document re-rankers but less efficient than other
  neural or classic retrieval models. The authors close the gap by systematically
  analyzing how token interactions can be reduced without harming the re-ranking effectiveness.
---

# Investigating the Effects of Sparse Attention on Cross-Encoders

## Quick Facts
- arXiv ID: 2312.17649
- Source URL: https://arxiv.org/abs/2312.17649
- Authors: 
- Reference count: 40
- Primary result: Cross-encoders with sparse attention achieve comparable effectiveness to standard cross-encoders while reducing memory requirements by 22-59% and improving inference speed by 1-43%

## Executive Summary
This paper investigates how token interactions in cross-encoders can be reduced without harming re-ranking effectiveness. The authors systematically analyze asymmetric attention patterns and different window sizes, finding that query tokens do not need to attend to passage or document tokens for effective re-ranking, and very small window sizes suffice. Their proposed sparse cross-encoder achieves effectiveness on par with previous models while reducing memory requirements by at least 22% (passages) or 59% (documents) and being 1% (passages) or 43% (documents) faster at inference time.

## Method Summary
The authors develop a new cross-encoder variant that combines windowed self-attention from sparse PLMs with asymmetric cross-attention. The [CLS] token attends to all sub-sequences, query tokens only attend to themselves (independent contextualization), and document tokens attend to all sub-sequences but use windowed self-attention. They implement this using a custom CUDA kernel for efficient windowed matrix multiplication. The model is fine-tuned on MS MARCO-based knowledge distillation triples and evaluated on TREC Deep Learning tasks, testing window sizes of ∞, 64, 16, 4, 1, and 0 tokens.

## Key Results
- Even with window sizes of 4 tokens, the sparse cross-encoder achieves effectiveness on par with standard cross-encoders
- Memory requirements are reduced by at least 22% for passages and 59% for documents
- Inference time improves by 1% for passages and 43% for documents
- Independently contextualizing the query has little to no effect on re-ranking effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query tokens do not need to attend to passage or document tokens for effective re-ranking
- Mechanism: By deactivating attention from query tokens to [CLS] and document tokens, the query is contextualized independently, reducing unnecessary interactions while preserving ranking quality
- Core assumption: The cross-encoder can effectively estimate relevance without symmetric query-document information exchange
- Evidence anchors: 
  - "we find that the query tokens do not need to attend to the passage or document tokens for effective re-ranking"
  - "We conclude that independently contextualizing the query has little to no effect on re-ranking effectiveness"
- Break condition: If the query-document relationship is inherently symmetric or requires fine-grained semantic exchange for relevance estimation, this mechanism would fail

### Mechanism 2
- Claim: Very small window sizes suffice for effective re-ranking
- Mechanism: Document tokens only attend to a small local context window, reducing computational complexity while maintaining sufficient information for relevance scoring
- Core assumption: A "gist" of document semantics captured in small local windows is sufficient for relevance estimation
- Evidence anchors: 
  - "very small window sizes suffice"
  - "even windows of 4 tokens still yield effectiveness on par with previous cross-encoders"
- Break condition: If document relevance requires understanding long-range dependencies beyond the small window, this mechanism would break down

### Mechanism 3
- Claim: Asymmetric attention between [CLS], query, and document tokens reduces token interactions without harming effectiveness
- Mechanism: The [CLS] token attends to all sub-sequences, query tokens only attend to themselves, and document tokens attend to all sub-sequences with windowed self-attention
- Core assumption: Cross-encoders can effectively rank documents with asymmetric attention patterns
- Evidence anchors: 
  - "We develop a new cross-encoder variant that combines windowed self-attention from sparse PLMs with asymmetric cross-attention"
  - "The [CLS] token has full attention over all sub-sequences... The query tokens can only attend to their own sub-sequence... Document tokens can attend to all sub-sequences but use windowed self-attention"
- Break condition: If symmetric attention between query and document tokens is crucial for capturing nuanced relevance signals, this mechanism would fail

## Foundational Learning

- Concept: Cross-encoder architecture
  - Why needed here: Understanding how cross-encoders work is fundamental to grasping the proposed sparse attention patterns and their impact on effectiveness
  - Quick check question: What is the primary difference between a cross-encoder and a bi-encoder in terms of token interaction?

- Concept: Sparse attention mechanisms
  - Why needed here: The paper's core contribution relies on understanding how sparse attention patterns (like windowed self-attention) can reduce computational complexity while maintaining effectiveness
  - Quick check question: How does windowed self-attention differ from full attention in terms of token interactions and computational complexity?

- Concept: Asymmetric attention
  - Why needed here: The proposed sparse cross-encoder relies on asymmetric attention patterns between [CLS], query, and document tokens to reduce unnecessary interactions
  - Quick check question: In what way does asymmetric attention differ from symmetric attention, and why might it be beneficial for cross-encoders?

## Architecture Onboarding

- Component map: [CLS] -> all tokens; Query tokens -> self only; Document tokens -> all tokens with windowed self-attention
- Critical path: Tokenize query and document/passage → Apply proposed attention pattern → Compute relevance score using [CLS] embedding
- Design tradeoffs: Effectiveness vs. efficiency (reducing token interactions improves efficiency but may impact effectiveness); Window size (smaller windows increase efficiency but may lose important context); Independent query contextualization (improves efficiency but may miss query-document interactions)
- Failure signatures: Drastic drop in re-ranking effectiveness; Inconsistent results across different tasks or datasets; Inefficient implementation leading to no practical speedup
- First 3 experiments: 1) Implement proposed sparse cross-encoder with full window size (w=∞) and compare to standard cross-encoder; 2) Gradually reduce window size (w=64, 16, 4) and measure impact on effectiveness and efficiency; 3) Disable attention from query tokens to [CLS] and document tokens, then compare results to baseline with full attention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of sparse cross-encoders with very small window sizes (e.g., 1 token) hold across different types of retrieval tasks beyond those tested in the paper?
- Basis in paper: The authors note that effectiveness drops when window sizes are reduced to 1 or 0 tokens, but the model is still competitive. However, they only tested on TREC Deep Learning tasks.
- Why unresolved: The paper's experiments are limited to specific TREC Deep Learning tasks, and it's unclear if the findings generalize to other retrieval tasks or domains.
- What evidence would resolve it: Testing the sparse cross-encoder model with very small window sizes on a diverse set of retrieval tasks, including those outside the TREC Deep Learning tasks, would provide evidence for generalizability.

### Open Question 2
- Question: How does the performance of sparse cross-encoders with asymmetric attention compare to those with symmetric attention in terms of computational efficiency and effectiveness across different sequence lengths?
- Basis in paper: The authors propose a novel sparse cross-encoder with asymmetric attention and find it to be as effective as previous models. However, they do not directly compare its efficiency and effectiveness to models with symmetric attention across various sequence lengths.
- Why unresolved: The paper focuses on the effectiveness of asymmetric attention but does not provide a comprehensive comparison with symmetric attention models across different sequence lengths.
- What evidence would resolve it: Conducting experiments that compare the computational efficiency and effectiveness of asymmetric and symmetric attention models across a range of sequence lengths would provide insights into their relative performance.

### Open Question 3
- Question: Can the efficiency gains from using sparse attention in cross-encoders be further improved by integrating other optimization techniques, such as knowledge distillation or quantization?
- Basis in paper: The authors mention that previous studies have used knowledge distillation to make cross-encoders more efficient. However, they do not explore combining sparse attention with other optimization techniques.
- Why unresolved: The paper focuses on the impact of sparse attention alone and does not investigate the potential synergies with other optimization methods.
- What evidence would resolve it: Experimenting with combinations of sparse attention and other optimization techniques, such as knowledge distillation or quantization, and measuring their impact on efficiency and effectiveness would provide evidence for potential improvements.

## Limitations

- The asymmetric attention mechanism's effectiveness across different domains and query types remains untested
- The relationship between window size and document length is not fully characterized
- The custom CUDA kernel implementation may affect reproducibility of reported efficiency gains

## Confidence

**High Confidence Claims:**
- Overall effectiveness of proposed sparse cross-encoder compared to standard cross-encoders (nDCG@10 results)
- Memory reduction benefits (22% for passages, 59% for documents)
- Inference time improvements (1% for passages, 43% for documents)

**Medium Confidence Claims:**
- Specific mechanisms underlying why asymmetric attention and small window sizes work
- Generalizability of findings to domains beyond passage and document re-ranking
- Scalability of techniques to much longer documents or different sequence lengths

**Low Confidence Claims:**
- Theoretical explanation for why query tokens don't need to attend to document tokens
- Optimal window size across all possible document lengths and domains
- Practical impact of optimizations in real-world deployment scenarios

## Next Checks

**Validation Check 1:** Replicate the experiment on a different retrieval task with longer documents (e.g., legal documents, scientific papers) to test whether the 4-token window remains effective and whether the asymmetric attention pattern generalizes to more complex semantic matching tasks.

**Validation Check 2:** Conduct ablation studies systematically removing different components of the asymmetric attention pattern (e.g., disabling [CLS] attention to query tokens, enabling query-to-document attention) to better understand which aspects are truly necessary for maintaining effectiveness.

**Validation Check 3:** Implement the custom CUDA kernel independently using different optimization strategies and compare both the effectiveness results and efficiency metrics to ensure the reported gains are reproducible and not dependent on specific implementation details.