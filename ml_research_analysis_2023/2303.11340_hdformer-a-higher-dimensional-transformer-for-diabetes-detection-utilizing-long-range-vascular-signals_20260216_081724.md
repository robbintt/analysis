---
ver: rpa2
title: 'HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing
  Long Range Vascular Signals'
arxiv_id: '2303.11340'
source_url: https://arxiv.org/abs/2303.11340
tags:
- diabetes
- transformer
- dimensional
- long
- signals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes HDformer, a Transformer-based model that leverages
  long-range photoplethysmography (PPG) signals for non-invasive diabetes detection.
  The method introduces Time Square Attention (TSA), which reduces token volume by
  more than 10x while retaining local and global dependencies, converting 1D PPG signals
  into 2D representations.
---

# HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals

## Quick Facts
- **arXiv ID:** 2303.11340
- **Source URL:** https://arxiv.org/abs/2303.11340
- **Reference count:** 20
- **Primary result:** HDformer achieves 98.4% sensitivity, 97.3% accuracy, 92.8% specificity, and 0.929 AUC for diabetes detection using long-range PPG signals

## Executive Summary
This study introduces HDformer, a Transformer-based model that converts 1D photoplethysmography (PPG) signals into 2D representations for non-invasive diabetes detection. The method employs Time Square Attention (TSA) to reduce token volume by over 10x while preserving local and global dependencies, paired with a gated mixture-of-experts network to optimize learning across different attention areas. Tested on the MIMIC-III dataset with 38,597 patients, HDformer achieves state-of-the-art performance with 98.4% sensitivity, 97.3% accuracy, 92.8% specificity, and 0.929 AUC, significantly outperforming existing methods.

## Method Summary
HDformer processes long-range PPG signals by first converting 1D waveforms into 2D tensor representations through Time Square Attention (TSA), which partitions the signal into patches of varying sizes (T, 2T, 4T, T/2, T/4). These 2D representations are then processed by Swin Transformer encoders, with a gated mixture-of-experts network aggregating outputs from different TSA modules. The model is trained on 10-minute PPG segments at 128Hz sampling rate from the MIMIC-III dataset for binary diabetes classification.

## Key Results
- Achieves 98.4% sensitivity, 97.3% accuracy, 92.8% specificity, and 0.929 AUC on diabetes detection
- Reduces token volume by over 10x compared to standard 1D Transformer approaches
- Demonstrates superiority over existing methods including CNN, LSTM, and standard Transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
TSA reduces computational complexity while retaining long-range dependencies by converting 1D PPG signals into 2D representations. It partitions PPG waveforms into patches and constructs them into 2D tensors, reducing tokens from over 77,000 to manageable 2D representations while preserving both local and global dependencies. Core assumption: PPG waveforms contain repeating patterns that can be effectively grouped into 2D patches without losing critical temporal information.

### Mechanism 2
The gated mixture-of-experts (MoE) network optimizes learning on different attention areas by dynamically selecting patch sizes. Multiple TSA modules with different patch sizes (T, 2T, 4T, T/2, T/4) process the same input, and a gated network learns the optimal combination of these representations for final classification. Core assumption: Different patch sizes capture different aspects of the signal - smaller patches capture fine details while larger patches capture broader trends.

### Mechanism 3
Long-range PPG signals contain richer features for diabetes classification compared to short-duration signals. Processing 10+ minute PPG signals captures comprehensive cardiovascular dynamics including heart rate variability patterns that correlate with diabetes-related vascular changes. Core assumption: Diabetes-related vascular changes manifest as measurable patterns in PPG signals over extended time periods.

## Foundational Learning

- **Concept:** Transformer architecture and self-attention mechanism
  - Why needed here: HDformer builds on standard Transformer architecture, and understanding how self-attention works is crucial for grasping TSA modifications
  - Quick check question: How does self-attention in standard Transformers scale with sequence length, and why is this problematic for long PPG signals?

- **Concept:** Photoplethysmography (PPG) signal characteristics
  - Why needed here: Understanding PPG signal composition and what features correlate with diabetes is essential for interpreting HDformer's approach
  - Quick check question: What are the key PPG waveform features that change with diabetes-related vascular conditions?

- **Concept:** Mixture-of-experts networks
  - Why needed here: The gated MoE network is a critical component of HDformer's architecture, and understanding how it works is necessary for implementation
  - Quick check question: How does a mixture-of-experts network differ from standard ensemble methods in terms of parameter efficiency and learning dynamics?

## Architecture Onboarding

- **Component map:** Input → Preprocessing → TSA modules (5 variants with different patch sizes) → 2D Transformer encoders (Swin or ViT) → MLP classifiers → Gated MoE decoder → Output
- **Critical path:** Signal preprocessing → TSA patch construction → 2D Transformer processing → MoE aggregation → Final classification
- **Design tradeoffs:** TSA reduces token count but may lose some fine-grained temporal resolution; MoE increases model capacity but adds complexity; long-range signals improve accuracy but increase computational demands
- **Failure signatures:** Poor performance may indicate issues with patch size selection, insufficient signal quality, or inappropriate Transformer model choice for 2D representations
- **First 3 experiments:**
  1. Test TSA with single fixed patch size (T) against standard 1D Transformer on 10-minute PPG signals
  2. Compare different 2D Transformer architectures (ViT vs Swin) on TSA-generated representations
  3. Evaluate performance impact of different patch size combinations in the MoE network

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal PPG signal length for diabetes detection beyond 10 minutes using HDformer?
- Basis in paper: [explicit] The paper explores signal lengths from 8 seconds to 10 minutes, finding performance improves up to 10 minutes with TSA, but does not investigate longer durations
- Why unresolved: The study only tests up to 10-minute PPG signals, leaving uncertainty about whether longer recordings would yield further accuracy improvements or diminishing returns
- What evidence would resolve it: Systematic testing of HDformer performance on PPG signals of 15, 20, 30+ minutes to determine the point of maximum predictive value

### Open Question 2
- Question: How does HDformer performance vary across different PPG sampling frequencies?
- Basis in paper: [explicit] The paper mentions future work will explore frequencies from 128Hz to 256Hz/512Hz but does not present experimental results
- Why unresolved: The study uses a fixed 128Hz sampling rate without investigating whether higher frequencies improve model performance or reduce required signal duration
- What evidence would resolve it: Comparative experiments testing HDformer with PPG signals sampled at multiple frequencies (128Hz, 256Hz, 512Hz) while maintaining consistent signal lengths

### Open Question 3
- Question: Can HDformer reliably estimate glucose levels from PPG signals rather than just detecting diabetes?
- Basis in paper: [explicit] The authors mention future work will explore glucose level estimation via 2nd derivative PPG analysis, but current results only demonstrate diabetes detection classification
- Why unresolved: The model architecture is designed for binary classification (diabetes vs non-diabetes) rather than continuous glucose level prediction, and no experiments are presented for this capability
- What evidence would resolve it: Experiments training HDformer on PPG data with known glucose measurements to predict actual glucose levels rather than binary classification, with validation against established glucose monitoring methods

## Limitations
- **Limited methodological transparency:** Lacks detailed specifications of the gated mixture-of-experts network architecture and expert weight computation
- **Dataset specificity concerns:** Diabetes detection relies on clinical annotations that may have varying accuracy, with insufficient discussion of label noise
- **Computational claims unverified:** The 10x token reduction claim requires empirical validation through ablation studies comparing different patch sizes

## Confidence

- **HDformer achieves state-of-the-art diabetes detection (sensitivity 98.4%, accuracy 97.3%):** Medium confidence - results are impressive but depend on undisclosed implementation details and dataset characteristics
- **TSA effectively reduces computational complexity while preserving signal information:** Low confidence - claims are supported by theoretical assertions but lack empirical ablation studies
- **Long-range PPG signals provide superior feature representation:** Medium confidence - supported by the dataset used but not compared against other signal duration benchmarks

## Next Checks

1. **Ablation Study on Patch Size Configurations:** Systematically test TSA performance using individual patch sizes (T, 2T, 4T, T/2, T/4) separately, then combinations, to quantify the contribution of each configuration to overall accuracy and computational efficiency.

2. **Cross-Dataset Validation:** Apply the HDformer model trained on MIMIC-III to an independent PPG dataset with diabetes annotations to verify generalization beyond the original training data.

3. **Computational Complexity Analysis:** Measure actual GPU memory usage and inference time for HDformer versus standard 1D Transformer implementations on the same hardware, comparing both training and inference scenarios across different sequence lengths.