---
ver: rpa2
title: 'FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search'
arxiv_id: '2308.03290'
source_url: https://arxiv.org/abs/2308.03290
tags:
- search
- gbops
- quantization
- fliqs
- integer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLIQS, the first one-shot reinforcement learning-based
  quantization search framework for mixed-precision floating-point and integer formats
  without requiring retraining. FLIQS addresses the challenge of assigning per-layer
  formats in mixed-precision quantization by sampling architectures during training
  and applying entropy regularization to enable direct deployment without fine-tuning.
---

# FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search

## Quick Facts
- arXiv ID: 2308.03290
- Source URL: https://arxiv.org/abs/2308.03290
- Reference count: 40
- Primary result: First one-shot RL-based mixed-precision quantization search achieving up to 2.69% higher accuracy than prior methods without retraining

## Executive Summary
FLIQS introduces the first one-shot reinforcement learning-based quantization search framework that simultaneously searches over mixed floating-point and integer formats for deep neural networks. Unlike prior approaches requiring retraining or using memory-intensive branching super-networks, FLIQS employs entropy regularization to enable direct deployment after search while maintaining a single set of weights. The framework achieves higher accuracy than post-training quantization methods by integrating quantization-aware training into the search process, and demonstrates the first joint quantization and neural architecture search capability in floating-point networks.

## Method Summary
FLIQS is a one-shot reinforcement learning framework for mixed-precision quantization search that eliminates the need for retraining. It uses an RL controller to sample per-layer quantization formats (integer and floating-point) during training, applies entropy regularization to balance exploration-exploitation, and dynamically quantizes a single set of weights without branching super-networks. The search integrates quantization-aware training, enabling the model to adapt to quantization noise. After search completion, the final model can be deployed directly without fine-tuning, achieving higher accuracy than post-training quantization while reducing memory overhead compared to differentiable NAS approaches.

## Key Results
- Achieves up to 2.69% higher accuracy than prior mixed-precision quantization methods
- Eliminates retraining requirement through entropy regularization, enabling direct deployment
- Reduces memory overhead compared to differentiable NAS by using weight sharing instead of branching super-networks
- Demonstrates first joint quantization and neural architecture search in floating-point networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FLIQS eliminates retraining need by applying entropy regularization to RL policy
- Mechanism: Entropy regularization controls exploration-exploitation trade-off, allowing search to converge without disrupting model training as entropy decreases and switching error diminishes
- Core assumption: Switching error is proportional to policy entropy, requiring entropy decrease for convergence
- Evidence anchors:
  - [abstract] "FLIQS can achieve higher accuracies than post-training quantization (PTQ) searches. Coupled with additional entropy regularization, the final model can be deployed without the need for further retraining or fine-tuning."
  - [section] "Entropy regularization adds a new loss term to the policy loss Lθ, balanced by a factor βH... FLIQS introduces a cosine entropy regularization schedule... This addresses the key challenge of one-shot quantization search by diminishing the effects of the search on the model training."
  - [corpus] Weak. No related work explicitly discusses entropy regularization in quantization search.

### Mechanism 2
- Claim: FLIQS achieves higher accuracy than PTQ by performing quantization search during training (QAT)
- Mechanism: Unlike PTQ which quantizes post-training without weight updates, FLIQS integrates quantization-aware training allowing model adaptation to quantization noise
- Core assumption: QAT provides better accuracy than PTQ because weights can adapt to quantization during training
- Evidence anchors:
  - [abstract] "FLIQS can achieve higher accuracies than post-training quantization (PTQ) searches."
  - [section] "Since the search takes place during training, FLIQS can achieve higher accuracies than post-training quantization (PTQ) searches."
  - [corpus] Weak. Related work discusses PTQ vs QAT but not specifically in RL-based search context.

### Mechanism 3
- Claim: FLIQS reduces memory overhead compared to differentiable NAS by using one-shot RL instead of branching super-networks
- Mechanism: Differentiable NAS duplicates weights for each quantization option, while FLIQS shares single weights dynamically quantized based on RL controller choice
- Core assumption: Branching super-networks incur memory overhead proportional to number of quantization options
- Evidence anchors:
  - [section] "FLIQS uses a single set of full-precision model weights, which are dynamically quantized and masked at each step... This approach differs from other works like DNAS [37] and EDMIPS [6], which use branched super-networks that allocate each numerical format its own set of weights and activations."
  - [section] "These additional branches can become prohibitively large due to the additional weight memory and gradient buffers especially for larger search spaces, such as the floating-point FLIQS-L space."
  - [corpus] Weak. No related work explicitly compares memory usage between RL-based and differentiable NAS in quantization.

## Foundational Learning

- Concept: Quantization-aware training (QAT) vs. post-training quantization (PTQ)
  - Why needed here: FLIQS uses QAT during search to achieve higher accuracy than PTQ methods
  - Quick check question: What is the main difference between QAT and PTQ in terms of model adaptation to quantization?

- Concept: Reinforcement learning policy optimization with entropy regularization
  - Why needed here: FLIQS uses RL controller with entropy regularization to balance exploration-exploitation during search
  - Quick check question: How does entropy regularization affect the exploration-exploitation trade-off in RL?

- Concept: Mixed-precision quantization search space design
  - Why needed here: FLIQS searches over combinations of integer and floating-point formats to find optimal per-layer configurations
  - Quick check question: What factors should be considered when designing a quantization search space for specific hardware target?

## Architecture Onboarding

- Component map: RL controller → Dynamic quantizer → Model trainer → Reward calculator → Policy optimizer

- Critical path: RL controller proposes formats → Dynamic quantizer applies quantization → Model trainer trains with QAT → Reward calculator computes accuracy/cost → Policy optimizer updates RL policy

- Design tradeoffs:
  - Memory vs. Accuracy: Weight sharing reduces memory but may introduce quantization error
  - Search space size vs. Search time: Larger spaces provide more options but increase search duration
  - Entropy regularization strength vs. Convergence speed: Stronger regularization may slow convergence but improve final accuracy

- Failure signatures:
  - High variance in accuracy across different quantization configurations
  - Slow convergence of RL policy
  - Degradation in model accuracy compared to uniform quantization

- First 3 experiments:
  1. Verify FLIQS can find mixed-precision configurations improving accuracy over uniform quantization for ResNet-18
  2. Compare memory usage of FLIQS with differentiable NAS method for same search space
  3. Evaluate effect of entropy regularization strength on final model accuracy and search convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FLIQS's performance scale when applied to larger and more complex models like GPT-3 or other large language models?
- Basis in paper: [inferred] The paper demonstrates FLIQS on convolutional networks and vision transformers but does not evaluate it on large language models or models with significantly different architectural patterns
- Why unresolved: The paper's experimental scope is limited to computer vision models, and the scaling behavior of FLIQS to larger models with different computational patterns (like attention-heavy architectures) remains untested
- What evidence would resolve it: Applying FLIQS to large language models and comparing its performance and efficiency gains against other quantization methods would provide clarity

### Open Question 2
- Question: What is the impact of FLIQS on training time and convergence when applied to extremely large-scale datasets beyond ImageNet?
- Basis in paper: [explicit] The paper mentions that FLIQS avoids retraining, but it does not provide detailed analysis of how the search process affects training time or convergence on larger datasets
- Why unresolved: The experiments focus on ImageNet, and there is no exploration of how FLIQS scales with dataset size or computational complexity
- What evidence would resolve it: Empirical results showing training time, convergence behavior, and resource utilization on datasets significantly larger than ImageNet would address this question

### Open Question 3
- Question: How does FLIQS perform in cross-platform deployment scenarios where the hardware supports different numerical formats?
- Basis in paper: [explicit] The paper discusses the diversity of hardware formats but does not evaluate FLIQS's adaptability to multiple deployment platforms with varying numerical support
- Why unresolved: While the paper acknowledges hardware diversity, it does not test FLIQS across heterogeneous platforms or provide guidelines for cross-platform quantization
- What evidence would resolve it: Testing FLIQS on multiple hardware platforms with different numerical format support and comparing the resulting model performance and efficiency would provide answers

## Limitations

- Limited experimental scope to computer vision models without evaluation on large language models or attention-heavy architectures
- Missing detailed ablation studies on critical components like entropy regularization strength and search space design
- No cross-platform deployment evaluation to validate adaptability to heterogeneous hardware with varying numerical format support

## Confidence

- Mechanism 1: Medium - Entropy regularization concept is well-justified but lacks extensive empirical validation in quantization search literature
- Mechanism 2: Medium - QAT vs PTQ advantage is established but specific impact in RL-based search context needs more evidence
- Mechanism 3: Medium - Memory savings claim is supported but direct comparisons with differentiable NAS implementations are missing

## Next Checks

1. **Validate Entropy Regularization Impact**: Run FLIQS with different βH values (0.0, 0.01, 0.1) to quantify how entropy regularization affects search convergence and final accuracy across multiple model architectures.

2. **Memory Usage Benchmark**: Implement both FLIQS and a comparable differentiable NAS approach (like DNAS) for the same search space on identical hardware, measuring peak memory consumption during search and final model deployment.

3. **Switching Error Analysis**: Monitor policy entropy and model accuracy during FLIQS search to empirically verify the relationship between entropy levels and switching error, testing whether the claimed convergence without retraining holds across different model scales.