---
ver: rpa2
title: Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through
  Logic
arxiv_id: '2309.13339'
source_url: https://arxiv.org/abs/2309.13339
tags:
- step
- reasoning
- points
- days
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LogiCoT, a neurosymbolic framework designed
  to enhance zero-shot chain-of-thought reasoning in large language models by integrating
  symbolic logic principles, particularly reductio ad absurdum. LogiCoT augments the
  standard reasoning process with a step-by-step verification and revision mechanism,
  enabling the model to detect and correct reasoning errors during inference.
---

# Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic

## Quick Facts
- **arXiv ID**: 2309.13339
- **Source URL**: https://arxiv.org/abs/2309.13339
- **Reference count**: 0
- **Primary result**: LogiCoT improves zero-shot chain-of-thought reasoning accuracy across diverse domains by integrating symbolic logic principles like reductio ad absurdum

## Executive Summary
This paper introduces LogiCoT, a neurosymbolic framework that enhances zero-shot chain-of-thought reasoning in large language models by integrating symbolic logic principles, particularly reductio ad absurdum. The approach augments standard reasoning with step-by-step verification and revision mechanisms, enabling models to detect and correct reasoning errors during inference. Experimental results on diverse tasks—including arithmetic, commonsense, symbolic, causal inference, and social reasoning—demonstrate that LogiCoT consistently improves reasoning accuracy, especially for larger language models, compared to standard chain-of-thought prompting.

## Method Summary
LogiCoT implements a neurosymbolic methodology that combines neural reasoning with symbolic logic verification. For each reasoning step, the framework generates both the step and its negation, then creates post hoc explanations for each possibility. Using either a composing approach (checking for contradictions between explanations) or an adopting approach (prompting the LLM to choose between viewpoints), the model verifies each step's validity. When verification fails, subsequent steps are dropped and a new reasoning chain is generated conditioned on the verified steps and the explanation for why the original step was incorrect. This process continues until all steps pass verification or a maximum iteration limit is reached.

## Key Results
- LogiCoT consistently improves reasoning accuracy across multiple domains including arithmetic, commonsense, and causal inference tasks
- The approach shows particular effectiveness with larger language models (>$7b$ parameters)
- LogiCoT achieves higher accuracy compared to standard chain-of-thought prompting while being more efficient than naive ensemble methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Step-wise verification using reductio ad absurdum improves error detection in LLMs.
- Mechanism: Each reasoning step Ti is checked by generating its negation ¬Ti and post hoc explanations for both Ti and ¬Ti. The model then adopts the more plausible explanation based on logical entailment with prior steps.
- Core assumption: LLMs can generate meaningful post hoc explanations that reveal contradictions when a step is invalid.
- Evidence anchors:
  - [abstract] "LogiCoT augments the standard reasoning process with a step-by-step verification and revision mechanism, enabling the model to detect and correct reasoning errors during inference."
  - [section 3.3] "This double-check procedure unrolls by checking the validity of P, ···, Ti-1 ⊢ Ti, i.e. the contradiction of Ci = P ∧ T1 ∧ ··· ∧ Ti-1 ∧ ¬Ti, once T<i passed the verification."
- Break condition: If the LLM cannot generate coherent post hoc explanations or consistently chooses the wrong option during adoption.

### Mechanism 2
- Claim: Conditional revision based on verification results leads to more efficient reasoning chains.
- Mechanism: When a step Ti fails verification, all subsequent steps T>i are dropped and a new chain is generated conditioned on the verified steps {T<i, E¬i}, where E¬i is the explanation for why Ti is incorrect.
- Core assumption: The LLM can use specific feedback to generate a better replacement step.
- Evidence anchors:
  - [abstract] "Experimental evaluations conducted on language tasks in diverse domains...demonstrate the efficacy of enhanced reasoning by logic."
  - [section 3.3] "Upon the suspect of a step Ti, LogiCoT drops all of the trailing thoughts T>i and branches out for revision T'i conditioned on {T≤i, E¬i}."
- Break condition: If the LLM generates a revised step that still fails verification, leading to repeated revisions without progress.

### Mechanism 3
- Claim: The adopting approach outperforms composing by leveraging the LLM's bias toward consistency.
- Mechanism: Instead of composing explanations for both Ti and ¬Ti and checking for contradictions, the LLM is prompted to adopt one of two viewpoints based on which explanation is more plausible.
- Core assumption: The LLM's autoregressive generation is biased toward maintaining consistency with the prompt.
- Evidence anchors:
  - [abstract] "Experimental evaluations...demonstrate the efficacy of enhanced reasoning by logic."
  - [section 3.3] "The LogiCoT framework employs a neurosymbolic methodology, leveraging logical rules and post hoc arguments to enhance error detection."
- Break condition: If the LLM's bias toward consistency leads it to adopt incorrect explanations, reducing overall accuracy.

## Foundational Learning

- Concept: Reductio ad absurdum
  - Why needed here: Forms the logical foundation for step-wise verification by checking for contradictions when assuming a step is false.
  - Quick check question: Given premises P → Q and ¬Q, can you prove ¬P using reductio ad absurdum? (Answer: Yes, by assuming P and deriving contradiction.)

- Concept: Post hoc explanation
  - Why needed here: Provides a way to verify reasoning steps by generating explanations for both a statement and its negation.
  - Quick check question: If an LLM generates "Step #1 is true because..." and "Step #1 is false because...", what does this tell us about the step's validity? (Answer: We can compare which explanation is more plausible based on the context.)

- Concept: Neurosymbolic AI
  - Why needed here: Combines symbolic logic (verification) with neural generation (reasoning steps) to create a hybrid reasoning system.
  - Quick check question: How does LogiCoT differ from pure symbolic reasoning systems? (Answer: It uses neural models for both generation and verification, making it more flexible than pure symbolic systems.)

## Architecture Onboarding

- Component map: Question → CoT generator → Verification module → Adopter module → Revision module → Final answer
- Critical path: Question → CoT generation → Step-by-step verification → Adoption/Revision → Final answer
- Design tradeoffs:
  - Efficiency vs. accuracy: More thorough verification improves accuracy but increases computation time
  - Complexity vs. simplicity: The adopting approach is simpler than composing but may be less precise in some cases
  - Generality vs. specialization: Zero-shot setting works across domains but may underperform specialized few-shot approaches
- Failure signatures:
  - Looping revisions: Step keeps failing verification and getting revised without convergence
  - Inconsistent adoption: LLM inconsistently chooses between Ti and ¬Ti across similar steps
  - Context loss: Revisions lose track of original question context
- First 3 experiments:
  1. Baseline comparison: Run CoT and LogiCoT on GSM8K with Vicuna-7b, measure accuracy improvement
  2. Ablation study: Compare adopting vs. composing LogiCoT on Date Understanding task
  3. Scale analysis: Test LogiCoT on different model sizes (Vicuna-7b, 13b, 33b) on AQuA dataset

## Open Questions the Paper Calls Out

1. **Efficiency comparison to ensemble methods**: The paper mentions LogiCoT is "more efficient than a naive ensemble" but lacks specific computational cost comparisons. Empirical results showing runtime or token usage comparisons between LogiCoT and ensemble approaches would resolve this.

2. **Performance scaling beyond tested models**: While LogiCoT benefits are noted to be "more consistent when the model size gets considerable (>$7b$)", the paper only tests up to GPT-4, leaving open whether even larger models would show similar or diminishing returns. Experiments with future larger models showing continued performance gains would provide clarity.

3. **Few-shot vs. zero-shot performance**: The paper acknowledges this as an area for future investigation, noting that "expertise revealed in the exemplar prompt that it is always beneficial for better performance in a specific domain." Comparative experiments showing accuracy differences between zero-shot and few-shot LogiCoT applications would resolve this question.

## Limitations

- The approach relies heavily on the LLM's ability to generate coherent post hoc explanations for both statements and their negations, which may vary significantly across model architectures and sizes
- The adopting approach introduces uncertainty about whether the LLM's bias toward consistency might lead to systematic errors when choosing between explanations
- The zero-shot nature means it may not fully leverage domain-specific knowledge that few-shot or fine-tuned methods could capture

## Confidence

**High confidence**: The core claim that LogiCoT improves reasoning accuracy across multiple domains is well-supported by experimental results showing consistent improvements on diverse benchmarks.

**Medium confidence**: The specific implementation details of the adopting approach and its comparative advantage over composing remain somewhat uncertain without access to exact prompt templates.

**Low confidence**: The generalization claims across all reasoning domains may be overstated, as the approach's effectiveness likely varies by task complexity and the degree to which logical contradictions are apparent in the reasoning chain.

## Next Checks

1. **Ablation study on verification depth**: Systematically vary the number of verification steps applied to each reasoning chain to quantify the relationship between verification thoroughness and accuracy gains, particularly on simpler tasks where excessive verification might introduce unnecessary complexity.

2. **Cross-model consistency analysis**: Test LogiCoT across multiple LLM architectures to assess whether the approach's effectiveness depends on specific model properties like instruction-following capability or reasoning style.

3. **Error case analysis**: Conduct detailed qualitative analysis of instances where LogiCoT worsens performance to identify systematic failure modes and determine whether these stem from the verification mechanism itself or from limitations in the base LLM's reasoning capabilities.