---
ver: rpa2
title: 'Explainability for Large Language Models: A Survey'
arxiv_id: '2309.01029'
source_url: https://arxiv.org/abs/2309.01029
tags:
- arxiv
- language
- explanations
- explanation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey paper provides a comprehensive overview of explainability
  techniques for large language models (LLMs). It categorizes explanation methods
  based on two training paradigms: traditional fine-tuning and prompting.'
---

# Explainability for Large Language Models: A Survey

## Quick Facts
- arXiv ID: 2309.01029
- Source URL: https://arxiv.org/abs/2309.01029
- Reference count: 38
- Primary result: Comprehensive survey of explainability techniques for large language models (LLMs), categorizing methods by training paradigms (fine-tuning vs prompting) and exploring local and global explanation approaches

## Executive Summary
This survey provides a comprehensive overview of explainability techniques for large language models, organizing them according to two fundamental training paradigms: traditional fine-tuning and prompting. The paper systematically categorizes local explanation methods (feature attribution, attention-based, example-based, and natural language) and global explanation approaches (probing, neuron activation, and concept-based). It also examines how explanations can be used to debug and improve models, discusses evaluation metrics, and identifies key research challenges in the field.

## Method Summary
This survey paper synthesizes existing research on explainability for large language models by categorizing techniques based on training paradigms. The authors review literature on local explanations (feature attribution, attention-based, example-based, natural language) for fine-tuned models and base model/assistant model explanations for prompted models. They also examine global explanation methods including probing, neuron activation, and concept-based approaches, while discussing evaluation metrics and open research challenges.

## Key Results
- Categorization of LLM explainability techniques based on training paradigms (fine-tuning vs prompting) enables targeted explanation approaches
- Local explanation methods (feature attribution, attention-based, example-based, natural language) provide instance-level interpretability
- Global explanation techniques (probing, neuron activation, concept-based) offer model-level understanding of learned representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Categorizing LLM explainability by training paradigms (fine-tuning vs prompting) enables targeted explanation techniques
- Mechanism: Different training paradigms create distinct model behaviors and knowledge representations, requiring specialized explanation approaches
- Core assumption: The fundamental differences between fine-tuning and prompting paradigms create measurable differences in how explanations should be constructed
- Evidence anchors:
  - [abstract] "We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm"
  - [section] "Due to the substantial distinctions between the two paradigms, various types of explanations have been proposed respectively"
  - [corpus] Weak - related papers focus on general XAI for LLMs without emphasizing paradigm-specific differences
- Break condition: If emerging LLM architectures blur the distinction between fine-tuning and prompting paradigms

### Mechanism 2
- Claim: Feature attribution methods can effectively identify important input components for LLM predictions
- Mechanism: By measuring how perturbations to input features affect model outputs, we can quantify feature importance
- Core assumption: Small, controlled perturbations to input features provide reliable signals about feature importance
- Evidence anchors:
  - [section] "Feature attribution methods aim to measure the relevance of each input feature (e.g., words, phrases, text spans) to a model's prediction"
  - [section] "The most straightforward strategy is leave-one-out, which removes inputs at various levels including vector, hidden units (Li et al., 2017), words (Li et al., 2016), token and spans (Wu et al., 2020b)"
  - [corpus] Weak - no direct evidence in corpus about perturbation-based methods for LLMs
- Break condition: If LLM predictions are highly sensitive to small perturbations or if feature importance is context-dependent

### Mechanism 3
- Claim: Attention mechanisms can provide interpretable insights into LLM decision-making processes
- Mechanism: Analyzing attention weights reveals which input elements the model focuses on when making predictions
- Core assumption: Higher attention weights correspond to more important features for the prediction
- Evidence anchors:
  - [section] "Attention mechanism is often viewed as a way to attend to the most relevant part of inputs"
  - [section] "These explanation techniques can be categorized into three main groups: visualization methods, function-based methods, and probing-based methods"
  - [corpus] Weak - corpus papers discuss attention visualization but don't validate attention as explanation
- Break condition: If attention weights don't correlate with feature importance or if attention is redundant across heads

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: Understanding how LLMs process information is crucial for developing effective explanation techniques
  - Quick check question: What are the key differences between encoder-only, decoder-only, and encoder-decoder transformer architectures?

- Concept: Feature attribution methods
  - Why needed here: These methods form the basis for many local explanation techniques discussed in the survey
  - Quick check question: How do integrated gradients differ from simple gradient-based attribution methods?

- Concept: Probing techniques
  - Why needed here: Probing is a key method for understanding what linguistic knowledge LLMs capture during pre-training
  - Quick check question: What is the difference between classifier-based probing and parameter-free probing?

## Architecture Onboarding

- Component map: Training paradigms (fine-tuning -> prompting) -> Local explanations (feature attribution -> attention-based -> example-based -> natural language) -> Global explanations (probing -> neuron activation -> concept-based) -> Evaluation metrics -> Research challenges
- Critical path: Start with understanding training paradigms → study local explanation methods → explore global explanation approaches → learn evaluation techniques → examine research challenges
- Design tradeoffs: Balancing explanation faithfulness with plausibility; choosing between local and global explanations; selecting appropriate evaluation metrics
- Failure signatures: Explanations that don't align with model predictions; methods that work for one paradigm but not another; evaluation metrics that give inconsistent results
- First 3 experiments:
  1. Implement a simple feature attribution method (like integrated gradients) on a fine-tuned BERT model
  2. Visualize attention weights for a few examples to understand attention-based explanations
  3. Design a probing classifier to test whether BERT encodes part-of-speech information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop evaluation metrics that accurately assess the faithfulness of explanations from large language models?
- Basis in paper: [explicit] The paper discusses challenges in evaluating explanation faithfulness and mentions that existing metrics are often inconsistent.
- Why unresolved: There is no ground truth for explanations, making it difficult to evaluate how well explanations reflect the model's actual reasoning process.
- What evidence would resolve it: A benchmark dataset with ground truth explanations or a comprehensive study comparing multiple evaluation metrics on diverse tasks.

### Open Question 2
- Question: What are the key differences in explanations between the traditional fine-tuning paradigm and the prompting paradigm?
- Basis in paper: [explicit] The paper mentions that a comprehensive comparison of explanations between fine-tuning and prompting is lacking.
- Why unresolved: While some differences in performance are observed, a systematic analysis of the reasoning processes behind predictions in the two paradigms is needed.
- What evidence would resolve it: A large-scale study analyzing explanations generated by fine-tuned and prompted models on the same tasks and datasets.

### Open Question 3
- Question: How can we mitigate shortcut learning in large language models to improve their out-of-distribution generalization?
- Basis in paper: [explicit] The paper discusses shortcut learning in both fine-tuning and prompting paradigms and its impact on generalization.
- Why unresolved: Despite some understanding of shortcut learning, effective methods to prevent models from relying on heuristics and biases are still lacking.
- What evidence would resolve it: New regularization techniques or training strategies that significantly improve OOD performance by reducing reliance on shortcuts.

## Limitations
- The survey assumes clear distinctions between fine-tuning and prompting paradigms, but emerging architectures may blur these boundaries
- Limited empirical validation of explanation methods across different LLM architectures and tasks
- The assumption that attention weights directly indicate feature importance is increasingly questioned in recent literature

## Confidence
- **High**: The categorization of explanation techniques into local (feature attribution, attention-based, example-based, natural language) and global (probing, neuron activation, concept-based) explanations is well-established and theoretically sound
- **Medium**: The claim that feature attribution methods can effectively identify important input components has strong theoretical grounding but limited direct empirical validation in the LLM context
- **Low**: The assumption that attention weights directly correspond to feature importance is increasingly questioned in recent literature, though the survey doesn't adequately address these concerns

## Next Checks
1. Conduct a systematic comparison of feature attribution methods across different LLM architectures (BERT, GPT, T5) to validate their effectiveness and identify architecture-specific limitations
2. Design experiments to test whether attention weights correlate with actual feature importance by comparing attention-based explanations with ground truth feature importance
3. Evaluate the generalization of paradigm-specific explanation techniques by applying them to hybrid models that combine fine-tuning and prompting approaches