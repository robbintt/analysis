---
ver: rpa2
title: Learning to Modulate pre-trained Models in RL
arxiv_id: '2306.14884'
source_url: https://arxiv.org/abs/2306.14884
tags:
- tasks
- learning
- success
- rate
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We evaluate fine-tuning, parameter-efficient fine-tuning, and prompt-based
  tuning methods for Transformers in RL, focusing on how well they adapt to new tasks
  while retaining performance on pre-training tasks. Most fine-tuning approaches suffer
  from catastrophic forgetting, while prompting preserves prior performance but adapts
  poorly to new tasks.
---

# Learning to Modulate pre-trained Models in RL

## Quick Facts
- arXiv ID: 2306.14884
- Source URL: https://arxiv.org/abs/2306.14884
- Authors:
- Reference count: 40
- Key outcome: We evaluate fine-tuning, parameter-efficient fine-tuning, and prompt-based tuning methods for Transformers in RL, focusing on how well they adapt to new tasks while retaining performance on pre-training tasks.

## Executive Summary
This paper investigates how to adapt pre-trained Transformers to new reinforcement learning tasks while avoiding catastrophic forgetting of pre-training performance. The authors evaluate fine-tuning, parameter-efficient fine-tuning, and prompt-based methods, finding that most suffer from forgetting while prompting preserves performance but adapts poorly. They propose Learning-to-Modulate (L2M), which modulates information flow in frozen pre-trained models via learnable modulation matrices, achieving state-of-the-art performance on the Continual-World benchmark while retaining pre-training capabilities. The authors also release a large dataset of trajectories for Meta-World and DMControl benchmarks.

## Method Summary
The authors pre-train Multi-Domain Decision Transformer (MDDT) on Meta-World (MT40) and DMControl (DMC10) datasets, then evaluate various adaptation methods including full fine-tuning, parameter-efficient fine-tuning (LoRA, Adapters, (IA)3), and prompt-based tuning (Prompt-tuning, Prefix-tuning, P-tuning v2). Their proposed L2M method learns a modulation pool with keys associated with learnable modulation matrices. During adaptation, state embeddings are aggregated to form a query vector that selects appropriate modulation matrices based on key matching. These matrices modulate the frozen pre-trained model's output without updating base weights, preserving pre-training representations while adapting to new tasks.

## Key Results
- Fine-tuning methods suffer from catastrophic forgetting, with pre-training task performance dropping by up to 30% when adapting to new tasks.
- Prompt-based methods preserve pre-training performance but adapt poorly to new tasks, showing large performance gaps compared to PEFT methods.
- L2M achieves state-of-the-art performance on Continual-World benchmark while maintaining pre-training task performance (60% for MT40, 94% for DMC10).
- L2M reaches 86% success rate on CW10 with only 2.8% of parameters trainable, significantly outperforming other parameter-efficient methods.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Catastrophic forgetting occurs because fine-tuning updates all parameters, overwriting representations useful for pre-training tasks.
- **Mechanism**: When fine-tuning a pre-trained model, gradient updates propagate through all layers, modifying weights that encode task-specific features from pre-training. This overwrites the learned representations, causing performance degradation on pre-training tasks.
- **Core assumption**: The model has learned distinct representations for different tasks that can be overwritten by new task gradients.
- **Evidence anchors**:
  - [abstract] "Fine-tuning a pre-trained model often suffers from catastrophic forgetting. That is, the performance on the pre-training tasks deteriorates when fine-tuning on new tasks."
  - [section 3.3] "While FT, L2 and EWC experience a severe drop in performance, L2M and L2P-based approaches maintain a similar performance level as prior to fine-tuning."

### Mechanism 2
- **Claim**: L2M avoids forgetting by modulating information flow in a frozen pre-trained model rather than updating weights directly.
- **Mechanism**: L2M uses learnable modulation matrices that are selected based on task context. These matrices adjust the output of frozen layers without changing the underlying weights, preserving pre-training representations while adapting to new tasks.
- **Core assumption**: Task context can be effectively captured through state embeddings, allowing appropriate modulation matrices to be selected.
- **Evidence anchors**:
  - [section 2.2] "L2M operates in a task-agnostic manner and efficiently adapts a pre-trained model to new tasks via a learnable modulation pool."
  - [section 3.1] "Indeed, we show strong performance on the pre-training tasks (60% for MT40, 94% for DMC10)."

### Mechanism 3
- **Claim**: Prompt-based methods preserve pre-training performance but struggle with new tasks because they lack sufficient adaptation capacity.
- **Mechanism**: Prompt-tuning methods prepend learnable prompts to input sequences. While this adds task-specific conditioning without modifying base weights (avoiding forgetting), the prompts may not provide enough capacity to fully adapt to substantially different tasks.
- **Core assumption**: Prompts can capture task-specific information without needing to modify the pre-trained model's internal representations.
- **Evidence anchors**:
  - [abstract] "PBT, in contrast, retains performance on previous tasks but does not adapt well to new tasks."
  - [section 3.2] "Notably, there is a large gap between PBT and PEFT methods, particularly on MT40."

## Foundational Learning

- **Concept**: Catastrophic forgetting in neural networks
  - **Why needed here**: Understanding why fine-tuning fails is essential to appreciate why L2M's approach works.
  - **Quick check question**: What happens to the weights of a neural network when you fine-tune it on a new task without any regularization?

- **Concept**: Parameter-efficient fine-tuning (PEFT)
  - **Why needed here**: L2M builds on PEFT concepts, so understanding LoRA and adapters is crucial.
  - **Quick check question**: How does LoRA modify the information flow in a pre-trained model without updating the original weights?

- **Concept**: Prompt-based tuning
  - **Why needed here**: L2M combines elements of prompt-based tuning with PEFT, so understanding how prompts work is essential.
  - **Quick check question**: What is the key difference between prompt-tuning and prefix-tuning in terms of where learnable parameters are inserted?

## Architecture Onboarding

- **Component map**: State → Embedding → Aggregation → Query → Key Matching → Modulation Matrix Selection → Information Flow Modulation → Action Prediction
- **Critical path**: State → Embedding → Aggregation → Query → Key Matching → Modulation Matrix Selection → Information Flow Modulation → Action Prediction
- **Design tradeoffs**:
  - Full fine-tuning vs. parameter efficiency: L2M trades some adaptation capacity for preserving pre-training performance
  - Task-agnostic vs. task-specific design: L2M avoids task-specific heads but requires effective task inference
  - Modulation complexity: More modulation matrices increase capacity but also parameter count
- **Failure signatures**:
  - Poor performance on new tasks: Modulation matrices aren't capturing necessary adaptations
  - Forgetting of pre-training tasks: Modulation is affecting base weights or task separation is failing
  - Slow adaptation: Modulation pool isn't finding appropriate keys for task contexts
- **First 3 experiments**:
  1. Implement basic L2M with a small modulation pool (10 keys) on a single Meta-World task, comparing to full fine-tuning
  2. Test task separation by visualizing state embeddings from pre-trained model to verify clustering
  3. Ablation study: Remove modulation from attention mechanism to test which components are most critical

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does L2M compare to meta-RL algorithms for continual learning in RL?
- Basis in paper: [inferred] The authors mention that Mandi et al. (2022) showed that FT approaches perform on-par or better on several tasks, but they do not compare against meta-RL algorithms in their experiments.
- Why unresolved: The paper does not provide a direct comparison between L2M and meta-RL algorithms, which could offer a different approach to continual learning in RL.
- What evidence would resolve it: Empirical results comparing L2M to meta-RL algorithms on the same benchmark tasks and metrics.

### Open Question 2
- Question: Can L2M be extended to leverage information from multiple episodes and tasks to improve generalization?
- Basis in paper: [inferred] The authors mention that the context length of the models they consider is currently limited and suggest that extending the context to leverage information from multiple episodes and tasks may improve the model's ability to generalize to new tasks or domains.
- Why unresolved: The paper does not explore this extension and its potential impact on performance and generalization.
- What evidence would resolve it: Experimental results showing the effect of extending the context length on L2M's performance and ability to generalize to new tasks or domains.

### Open Question 3
- Question: How does the choice of modulation targets (attention mechanism vs. position-wise feedforward layer) affect L2M's performance and parameter efficiency?
- Basis in paper: [explicit] The authors conduct an ablation study in Appendix G.2, investigating the impact of modulating different components of the Transformer architecture, including the attention mechanism and the position-wise feedforward layer.
- Why unresolved: The ablation study provides some insights, but the optimal choice of modulation targets may depend on the specific task or domain, and a more comprehensive analysis is needed.
- What evidence would resolve it: A systematic evaluation of L2M with different modulation target configurations across a wide range of tasks and domains, considering both performance and parameter efficiency.

## Limitations
- Evaluation limited to two specific RL benchmarks (Meta-World and DMControl) with specific task distributions.
- Modulation pool size selection was empirical without systematic ablation studies.
- No analysis of computational overhead during inference with large modulation pools.
- Sequential task learning tested without evaluating interleaving or revisiting tasks.

## Confidence

- **High confidence**: The observation that fine-tuning suffers from catastrophic forgetting on pre-training tasks.
- **Medium confidence**: The claim that L2M achieves state-of-the-art performance while preserving pre-training capabilities.
- **Medium confidence**: The assertion that prompting methods retain pre-training performance but adapt poorly to new tasks.

## Next Checks

1. **Generalization test**: Evaluate L2M on a completely unseen task distribution outside Meta-World and DMControl to verify if the modulation mechanism generalizes beyond the specific benchmark suites.

2. **Catastrophic forgetting verification**: Implement an intervention where modulation pool keys are randomly shuffled between tasks to test if task separation is genuinely maintained or if performance gains are due to other factors.

3. **Capacity scaling analysis**: Systematically vary the modulation pool size (10, 30, 50, 100 keys) to quantify the relationship between parameter efficiency and adaptation capacity, establishing clear scaling laws.