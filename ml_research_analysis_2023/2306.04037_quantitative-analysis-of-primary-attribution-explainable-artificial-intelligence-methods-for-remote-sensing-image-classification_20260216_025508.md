---
ver: rpa2
title: Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence
  Methods for Remote Sensing Image Classification
arxiv_id: '2306.04037'
source_url: https://arxiv.org/abs/2306.04037
tags:
- methods
- image
- sensing
- classification
- remote
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantitatively benchmarks explainable AI (XAI) methods
  for remote sensing image classification, evaluating five primary attribution methods
  (HiResCAM, LIME, GradSHAP, Saliency, and Occlusion) across six XAI metric categories
  (robustness, faithfulness, localization, complexity, randomization, and axiomatic)
  using three state-of-the-art models (ConvNeXt, ViT, and FocalNets) on three remote
  sensing datasets (UCMerced, EuroSAT, and MSTAR). The study addresses the challenge
  of selecting appropriate XAI methods for different remote sensing modalities by
  providing systematic evaluation metrics and insights.
---

# Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification

## Quick Facts
- arXiv ID: 2306.04037
- Source URL: https://arxiv.org/abs/2306.04037
- Authors: 
- Reference count: 0
- Primary result: Systematic quantitative benchmarking of five primary attribution XAI methods across six metric categories for three remote sensing datasets reveals no universally best method, with dataset-specific selection being crucial.

## Executive Summary
This study provides the first comprehensive quantitative benchmark of primary attribution XAI methods for remote sensing image classification. The research evaluates five XAI methods (HiResCAM, LIME, GradSHAP, Saliency, and Occlusion) across six metric categories (robustness, faithfulness, localization, complexity, randomization, and axiomatic) using three state-of-the-art models (ConvNeXt, ViT, and FocalNets) on three remote sensing datasets (UCMerced, EuroSAT, and MSTAR). The study addresses the critical challenge of selecting appropriate XAI methods for different remote sensing modalities by providing systematic evaluation metrics and insights. Results show that no single XAI method universally performs best across all datasets and metrics, highlighting the importance of dataset-specific method selection.

## Method Summary
The study employed a systematic benchmarking approach using three remote sensing datasets with equal class representation for evaluation subsets. Three pretrained model architectures (ConvNeXt, ViT, FocalNets) were fine-tuned with only output layer adjustments using Adam optimizer and early stopping criteria. Five primary attribution XAI methods were implemented and evaluated using the Quantus toolkit across six metric categories. The evaluation framework included preprocessing steps such as per-channel Otsu's threshold for ROI masks and superpixel-based feature masks for LIME. Classification accuracy was measured alongside XAI metrics, with results normalized and ranked per dataset and metric category to identify optimal methods for each scenario.

## Key Results
- Occlusion performed best for UCMerced (average rank 1.3), GradSHAP for EuroSAT (average rank 1.7), and HiResCAM for MSTAR (average rank 1.7)
- ConvNeXt achieved highest average accuracy across datasets (96.74% ±1.25 for UCMerced, 97.06% ±0.00 for EuroSAT, 82.27% ±3.53 for MSTAR)
- No single XAI method universally performs best across all datasets and metrics
- All XAI methods violated the completeness axiom constraint, suggesting need for new axiomatic measures with tolerance for error

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Occlusion, GradSHAP, and HiResCAM are the best XAI methods for different remote sensing datasets due to their specific algorithmic properties.
- Mechanism: Occlusion works best for UCMerced because its iterative patch substitution accurately captures model behavior and stability. GradSHAP excels on EuroSAT by effectively linking image pixels to labels through iterative baseline substitution. HiResCAM performs optimally on MSTAR by highlighting centrally-located targets with attention maps.
- Core assumption: The effectiveness of each XAI method depends on the specific characteristics of the dataset and the model's behavior on that dataset.
- Evidence anchors:
  - [abstract] "Occlusion performed best for UCMerced (average rank 1.3), GradSHAP for EuroSAT (average rank 1.7), and HiResCAM for MSTAR (average rank 1.7)."
  - [section] "For EuroSAT, GradientShap's iterative substitution of a baseline value with randomly chosen subsets of attributions can successfully capture the links between image pixels and labels [13]."
  - [corpus] Weak - the corpus neighbors do not directly discuss the specific effectiveness of these XAI methods on different remote sensing datasets.
- Break condition: If the dataset characteristics change significantly (e.g., different image modalities, target distributions), the optimal XAI method may also change.

### Mechanism 2
- Claim: ConvNeXt achieves the highest average accuracy across all datasets due to its generalizable features.
- Mechanism: ConvNeXt's architecture and pretraining allow it to extract more generalizable features that perform well across different remote sensing modalities, even with only the output layer tuned.
- Core assumption: Pretrained models with generalizable features will perform better on diverse datasets when only the output layer is tuned.
- Evidence anchors:
  - [abstract] "Classification accuracy results showed ConvNeXt achieving highest average accuracy across datasets (96.74% ±1.25 for UCMerced, 97.06% ±0.00 for EuroSAT, 82.27% ±3.53 for MSTAR)."
  - [section] "ConvNeXt's features seem more generalizable as this model was fairly robust to the MSTAR dataset."
  - [corpus] Weak - the corpus neighbors do not directly discuss ConvNeXt's performance on remote sensing datasets.
- Break condition: If the dataset requires task-specific feature extraction that is not captured by the pretrained ConvNeXt model, its performance may degrade.

### Mechanism 3
- Claim: No single XAI method universally performs best across all datasets and metrics, highlighting the importance of dataset-specific method selection.
- Mechanism: The effectiveness of XAI methods is context-dependent, varying with dataset characteristics, model architecture, and evaluation metrics. This necessitates a systematic evaluation process to identify the most appropriate method for each specific use case.
- Core assumption: XAI method performance is not universal and must be evaluated on a case-by-case basis considering the specific dataset and model.
- Evidence anchors:
  - [abstract] "The study reveals that no single XAI method universally performs best across all datasets and metrics, highlighting the importance of dataset-specific method selection."
  - [section] "Certain measures of explainability may be more important depending on the application and end-users."
  - [corpus] Weak - the corpus neighbors do not directly discuss the universal performance of XAI methods across different datasets and metrics.
- Break condition: If a new XAI method is developed that outperforms existing methods across all datasets and metrics, the need for dataset-specific selection may be reduced.

## Foundational Learning

- Concept: Understanding of primary attribution XAI methods (HiResCAM, LIME, GradSHAP, Saliency, Occlusion)
  - Why needed here: The paper evaluates these five XAI methods, so understanding their mechanisms and strengths/weaknesses is crucial for interpreting the results.
  - Quick check question: What is the key difference between GradSHAP and Saliency maps in terms of how they attribute importance to input features?

- Concept: Familiarity with XAI evaluation metrics (robustness, faithfulness, localization, complexity, randomization, axiomatic)
  - Why needed here: The paper uses these six categories of metrics to quantitatively compare XAI methods, so understanding what each metric measures is essential for interpreting the results.
  - Quick check question: How does the Sparseness metric in the complexity category differ from the Faithfulness Correlation metric in the faithfulness category?

- Concept: Knowledge of remote sensing image classification datasets (UCMerced, EuroSAT, MSTAR)
  - Why needed here: The paper evaluates XAI methods on these three datasets, so understanding their characteristics (e.g., image modalities, target types) is important for contextualizing the results.
  - Quick check question: What are the main differences between the UCMerced and MSTAR datasets in terms of image modality and target distribution?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> XAI method application -> Metric calculation -> Results analysis
- Critical path: 1) Preprocess and augment the remote sensing datasets 2) Train and validate the three model architectures 3) Apply XAI methods to generate explanations for model predictions 4) Calculate XAI metrics to quantitatively compare methods 5) Analyze results and identify the best XAI method for each dataset
- Design tradeoffs:
  - Using pretrained models vs. training from scratch: Pretrained models offer faster convergence and better generalization but may not capture dataset-specific features.
  - Quantitative vs. qualitative XAI evaluation: Quantitative metrics provide objective comparisons but may not capture all aspects of explanation quality.
  - Subset selection for XAI evaluation: Using a subset of test data ensures equal class representation but may not capture the full diversity of the dataset.
- Failure signatures:
  - Low classification accuracy: Indicates issues with model architecture, training process, or dataset quality.
  - Inconsistent XAI metric results: Suggests problems with XAI method implementation or metric calculation.
  - No clear best XAI method: Implies that the evaluation metrics may not be well-suited for the specific dataset or that more diverse XAI methods are needed.
- First 3 experiments:
  1. Evaluate the classification accuracy of each model on the three datasets to identify the best-performing architecture.
  2. Apply each XAI method to a subset of test samples and calculate the XAI metrics to compare their performance.
  3. Analyze the results to identify the best XAI method for each dataset and investigate any patterns or insights.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different hyperparameter selections affect the performance of XAI methods and metrics across remote sensing datasets?
- Basis in paper: [explicit] "Also, XAI methods as well as metrics are dependent on the selection of the hyperparameters [3]. Future investigations can involve sensitivity analysis of the selection of hyperparameters for both the XAI methods and metrics as this will impact additional analysis results."
- Why unresolved: The paper acknowledges that hyperparameters significantly impact XAI method performance but does not conduct any sensitivity analysis to determine how different hyperparameter choices affect results.
- What evidence would resolve it: A systematic study varying hyperparameters (such as baseline values, perturbation sizes, number of iterations) for each XAI method and metric, showing how performance changes across the different datasets.

### Open Question 2
- Question: Can new axiomatic measures be developed that allow for tolerance of error when evaluating completeness in XAI methods for remote sensing?
- Basis in paper: [explicit] "One observation is that none of the methods satisfied the axiom constraint of completeness... New axiomatic measures could possibly be introduced to allow for some tolerance of error to quantify the extent to which an axiom is violated."
- Why unresolved: The paper identifies that current axiomatic measures like completeness are too strict for practical XAI evaluation, but does not propose or test alternative measures that could better capture real-world performance.
- What evidence would resolve it: Development and validation of new axiomatic measures that incorporate tolerance thresholds, with demonstration that these measures better differentiate between XAI methods while still maintaining meaningful constraints.

### Open Question 3
- Question: How would finetuning all layers of pretrained models (rather than just the output layer) affect both classification accuracy and XAI method performance across different remote sensing modalities?
- Basis in paper: [inferred] The paper only finetuned the output layer of pretrained models and noted this limitation: "Future work includes... finetuning every layer of each model" - suggesting this could impact results.
- Why unresolved: The current results are limited to feature extraction layer evaluation only, which may not represent the full potential of the models or how XAI methods perform with fully adapted models.
- What evidence would resolve it: Experiments comparing classification accuracy and XAI method performance when finetuning all layers versus only the output layer, across the three datasets and model types.

## Limitations
- The evaluation focuses on three specific datasets and model architectures, which may not capture the full diversity of remote sensing applications
- The study does not explore hyperparameter sensitivity for XAI methods or metrics, which could significantly impact results
- The completeness violation observed across all methods suggests potential limitations in the current axiomatic framework for XAI evaluation

## Confidence

**High Confidence**: Classification accuracy results and XAI method rankings within evaluated datasets

**Medium Confidence**: Cross-dataset generalizability of findings

**Low Confidence**: Universal applicability of identified "best" XAI methods to other remote sensing scenarios

## Next Checks
1. **Dataset Diversity Test**: Apply the same benchmarking framework to additional remote sensing datasets with different modalities (e.g., hyperspectral, LiDAR) to validate cross-dataset generalizability
2. **Hyperparameter Sensitivity Analysis**: Systematically vary key hyperparameters for each XAI method and metric to quantify their impact on method rankings
3. **Real-world Application Test**: Deploy the identified "best" XAI methods on operational remote sensing classification tasks to evaluate practical utility and user satisfaction