---
ver: rpa2
title: Typos-aware Bottlenecked Pre-Training for Robust Dense Retrieval
arxiv_id: '2304.08138'
source_url: https://arxiv.org/abs/2304.08138
tags:
- queries
- misspelled
- pre-training
- dense
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of dense retrievers (DRs) being
  less effective on misspelled queries compared to correctly spelled queries. The
  authors propose ToRoDer, a novel pre-training strategy for DRs that increases their
  robustness to misspelled queries while preserving their effectiveness on correctly
  spelled queries.
---

# Typos-aware Bottlenecked Pre-Training for Robust Dense Retrieval

## Quick Facts
- **arXiv ID**: 2304.08138
- **Source URL**: https://arxiv.org/abs/2304.08138
- **Reference count**: 40
- **Key outcome**: Novel pre-training strategy (ToRoDer) that improves dense retriever robustness to misspelled queries while maintaining effectiveness on correctly spelled queries

## Executive Summary
This paper addresses the significant performance gap between dense retrievers (DRs) on correctly spelled versus misspelled queries. The authors propose ToRoDer, a pre-training strategy that uses an encoder-decoder bottleneck architecture to create typos-aware representations. The encoder takes misspelled text with masked tokens as input and outputs compressed [CLS] embeddings, while a shallow decoder attempts to recover the masked tokens. This forces the encoder to capture semantic information that's robust to misspellings. The method significantly closes the effectiveness gap on misspelled queries while preserving performance on clean queries, without requiring complex spell-checker components.

## Method Summary
ToRoDer uses an encoder-decoder architecture where the encoder (BERT-base, 12 layers) processes misspelled text with masked tokens and outputs bottlenecked [CLS] embeddings. A shallow decoder (2 transformer layers) takes these embeddings plus token embeddings of the original text with misspelled tokens masked, and attempts to recover the masked tokens. Pre-training uses a 30% typo injection ratio and 30% masking ratio with masked language modeling. The method includes a three-stage fine-tuning pipeline: contrastive learning with BM25 negatives, hard negative mining with cross-encoder distillation, and self-teaching to align misspelled and correctly spelled query representations.

## Key Results
- ToRoDer significantly improves MRR@10 on misspelled queries (e.g., +0.076 on MS MARCO dev set)
- The method closes the effectiveness gap with pipelines using separate spell-checker components
- ToRoDer maintains effectiveness on correctly spelled queries while improving robustness
- Ablation studies show the bottleneck architecture and typos-aware fine-tuning are critical components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The encoder-decoder bottleneck architecture forces the encoder to compress semantic information into the [CLS] token embedding, making it more robust to misspellings.
- Mechanism: By using a shallow decoder that needs to recover both masked and misspelled tokens, the encoder must distill crucial information into the bottlenecked [CLS] embedding. Since the decoder has limited modeling power, it cannot recover the original tokens without receiving rich compressed information from the encoder.
- Core assumption: A weaker decoder forces the encoder to compress more information into the bottleneck, and this compressed representation can capture the original, correctly spelled form of misspelled tokens.
- Evidence anchors:
  - [abstract]: "the encoder needs to compress as much information as possible into the bottlenecked [CLS] token embeddings"
  - [section]: "Since the decoder has weaker language modeling power than the encoder and the number of masked tokens in ùë•ùëëùëíùëê is also more than that in ùë•ùëíùëõùëê, the encoder needs to learn to compress as much semantic information as possible into the bottleneck [CLS] embeddings"
  - [corpus]: Weak evidence - the related paper "Typo-Robust Representation Learning for Dense Retrieval" suggests similar bottleneck approaches exist but doesn't directly confirm this specific mechanism

### Mechanism 2
- Claim: The pre-training task of recovering both masked and misspelled tokens creates a typos-aware representation that generalizes better to out-of-distribution misspelled queries.
- Mechanism: During pre-training, the model sees pairs of inputs where the encoder input contains misspelled tokens and the decoder input has those same positions masked. The model learns to map from misspelled text to correct text, creating representations that are invariant to certain types of typos.
- Core assumption: The distribution of typos in pre-training data is representative enough of real-world typos to create useful generalization.
- Evidence anchors:
  - [abstract]: "The pre-training task is to recover the masked tokens for both the encoder and decoder" and "the decoder will need to recover the original words, thus the enhanced [CLS] embeddings given by the encoder are expected to also have information about the original correctly spelled words"
  - [section]: "the typo generation is also performed at the word level" and "we make sure all modified tokens (masked tokens and misspelled tokens) in ùë•ùëíùëõùëê are replaced with [MASK] and the remaining input is kept unchanged"
  - [corpus]: Weak evidence - the related paper "Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive Learning" suggests typo-aware training helps but doesn't confirm the specific pre-training mechanism

### Mechanism 3
- Claim: The multi-stage fine-tuning pipeline with typos-aware fine-tuning (Self-Teaching) provides additional robustness gains beyond what pre-training alone achieves.
- Mechanism: After pre-training creates a typos-aware backbone, fine-tuning stages progressively refine the model. The Self-Teaching loss aligns representations of misspelled queries with their correctly spelled counterparts, further reducing the representation gap.
- Core assumption: The pre-trained typos-aware representations provide a better starting point than standard pre-training, making the fine-tuning stages more effective.
- Evidence anchors:
  - [section]: "We believe the pre-training stage is also important for training a typo-robust DR" and "Previous works have proposed several typos-aware fine-tuning methods, which have been shown to significantly improve the robustness of dense retrievers on misspelled queries"
  - [section]: "The ST approach is akin to the knowledge distillation method in that the training objective of ST is to minimize the KL divergence loss between the passage match score distribution obtained by the misspelled query ùëû‚Ä≤ and the distribution obtained from the corresponding query without typos"
  - [corpus]: Weak evidence - the related paper "Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval" suggests document expansion helps but doesn't confirm the specific multi-stage fine-tuning approach

## Foundational Learning

- Concept: Masked Language Modeling (MLM) pre-training
  - Why needed here: Understanding MLM is crucial because ToRoDer builds upon it by adding typo-aware variations and the bottleneck architecture
  - Quick check question: In standard BERT MLM, what percentage of tokens are replaced with [MASK], what percentage are replaced with random tokens, and what percentage remain unchanged?

- Concept: Contrastive learning for dense retrieval
  - Why needed here: The fine-tuning pipeline uses contrastive cross-entropy loss to train the dense retriever, which is fundamental to understanding how the model learns to distinguish relevant from irrelevant passages
  - Quick check question: In contrastive learning for dense retrieval, what is the typical loss function used to pull positive query-passage pairs together while pushing negative pairs apart?

- Concept: Knowledge distillation from cross-encoder rerankers
  - Why needed here: Stage 3 of the fine-tuning pipeline uses knowledge distillation from a cross-encoder teacher to improve the dense retriever's effectiveness
  - Quick check question: What is the primary advantage of using a cross-encoder reranker as a teacher model when distilling knowledge to a dense retriever?

## Architecture Onboarding

- Component map: Encoder (BERT-base) ‚Üí [CLS] bottleneck ‚Üí Decoder (2-layer BERT) ‚Üí Token recovery ‚Üí Downstream retrieval
- Critical path: Encoder processes misspelled text ‚Üí compresses to bottlenecked [CLS] embeddings ‚Üí Decoder recovers masked tokens ‚Üí creates typos-aware representations
- Design tradeoffs: The shallow decoder provides a strong bottleneck effect but may limit the model's ability to recover complex patterns; the typo injection ratio must balance robustness gains against degradation on clean queries
- Failure signatures: If the model overfits to synthetic typos, effectiveness on real-world misspelled queries will be poor; if the bottleneck is too weak, the pre-training won't provide robustness benefits
- First 3 experiments:
  1. Implement the encoder-decoder architecture with MLM pre-training (no typos) to verify the baseline works
  2. Add typo injection to the encoder input while keeping the decoder input typo-free to test the bottleneck effect
  3. Evaluate on a small set of misspelled queries to verify the model can recover original tokens from misspelled inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ToRoDer perform with different backbone PLM models such as RoBERTa, ELECTRA, or CharacterBERT instead of BERT?
- Basis in paper: [explicit] The authors mention that future work should consider modifying the backbone PLM model used in ToRoDer to rely on CharacterBERT rather than BERT, and also plan to investigate the effectiveness of ToRoDer when applied to other PLM backbones like RoBERTa and ELECTRA.
- Why unresolved: The current implementation and experiments are based on BERT as the backbone PLM. The performance and effectiveness of ToRoDer with other PLM models have not been tested or reported.
- What evidence would resolve it: Conducting experiments using ToRoDer with different PLM backbones (RoBERTa, ELECTRA, CharacterBERT) and comparing their performance on the same benchmark datasets would provide evidence on how the choice of backbone PLM affects ToRoDer's effectiveness.

### Open Question 2
- Question: How does ToRoDer perform on other types of query variations beyond misspellings, such as paraphrasing or query simplification?
- Basis in paper: [explicit] The authors mention that future work should evaluate how ToRoDer performs with respect to other query variations rather than just misspelled queries.
- Why unresolved: The current evaluation focuses solely on the robustness of ToRoDer to misspelled queries. Its performance on other types of query variations has not been explored.
- What evidence would resolve it: Conducting experiments using ToRoDer on datasets containing various query variations (paraphrasing, simplification, etc.) and comparing its performance to baselines would provide evidence on its robustness to different types of query variations.

### Open Question 3
- Question: What is the impact of the amount of typos in individual queries on ToRoDer's effectiveness during pre-training and inference?
- Basis in paper: [explicit] The authors analyze the impact of the amount of typos present in individual queries during pre-training and inference on the effectiveness of the DR model, varying the typo injection ratio parameter Œ≤ from 0 to 1.0.
- Why unresolved: While the authors provide some analysis on the impact of typo injection ratio during pre-training, the full extent of how different amounts of typos in queries affect ToRoDer's performance is not completely explored.
- What evidence would resolve it: Conducting a more comprehensive analysis of ToRoDer's performance across a wider range of typo injection ratios and amounts of typos in queries during both pre-training and inference would provide a clearer understanding of its robustness to varying levels of query typos.

## Limitations

- Unknown typo generation method: The paper mentions five typo types but doesn't specify the exact generation process, which could significantly impact results
- Domain generalization uncertainty: While effective on web search and MS MARCO, the method's performance on specialized domains with different typo patterns is untested
- Computational overhead: The encoder-decoder pre-training adds complexity and computational cost, though the paper doesn't provide detailed efficiency comparisons

## Confidence

**High Confidence Claims:**
- The ToRoDer architecture with bottlenecked [CLS] embeddings provides robustness to misspelled queries
- The pre-training task of recovering masked tokens in both encoder and decoder improves the model's ability to handle typos
- The multi-stage fine-tuning pipeline with Self-Teaching improves robustness on misspelled queries

**Medium Confidence Claims:**
- The 30% typo injection ratio is optimal for balancing robustness and effectiveness on clean queries
- The encoder-decoder bottleneck architecture is superior to other typos-aware approaches like contrastive learning alone
- The shallow decoder (2 layers) provides the right balance between bottleneck strength and recovery capability

**Low Confidence Claims:**
- The exact typo generation method doesn't significantly impact the final model performance
- The ToRoDer approach generalizes well to domains beyond web search and MS MARCO
- The computational overhead of ToRoDer pre-training is negligible compared to the performance gains

## Next Checks

**Check 1: Typo Generation Ablation**
Create multiple variants of the text editor with different typo generation methods (e.g., character substitution only, keyboard proximity only, phonetic similarity only) and evaluate their impact on model performance. Compare performance on real user misspellings versus synthetic typos to identify which generation method best matches real-world patterns.

**Check 2: Decoder Depth Sensitivity Analysis**
Implement ToRoDer with decoders of varying depths (1, 2, 4, 8 transformer layers) and measure the trade-off between bottleneck strength and token recovery capability. Evaluate both robustness to misspellings and effectiveness on correctly spelled queries to find the optimal depth.

**Check 3: Domain Transfer Evaluation**
Test the ToRoDer model trained on MS MARCO and web search data on specialized domains (e.g., medical literature, legal documents, technical manuals). Generate domain-specific typos using in-domain corpora and evaluate whether the pre-training generalizes to these different typo patterns and vocabulary distributions.