---
ver: rpa2
title: How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision
  LLMs
arxiv_id: '2311.16101'
source_url: https://arxiv.org/abs/2311.16101
tags:
- image
- vllms
- arxiv
- attack
- unicorns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a comprehensive safety evaluation benchmark\
  \ for Vision Large Language Models (VLLMs), addressing the gap in safety assessments\
  \ beyond standard performance metrics. The benchmark includes two novel datasets\u2014\
  OODCV-VQA and Sketchy-VQA\u2014designed to test out-of-distribution (OOD) generalization\
  \ and adversarial robustness."
---

# How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs

## Quick Facts
- arXiv ID: 2311.16101
- Source URL: https://arxiv.org/abs/2311.16101
- Reference count: 40
- Key outcome: VLLMs excel at OOD visual content but struggle with OOD textual inputs, especially under adversarial attacks.

## Executive Summary
This paper introduces a comprehensive safety evaluation benchmark for Vision Large Language Models (VLLMs), addressing critical gaps in assessing their robustness and safety beyond standard performance metrics. The study presents two novel datasets—OODCV-VQA and Sketchy-VQA—designed to test out-of-distribution generalization and adversarial robustness. Through extensive experiments on 21 models including GPT-4V, the research reveals that while VLLMs handle OOD visual content well, they are vulnerable to OOD textual instructions and adversarial attacks targeting vision encoders. The findings highlight the need for enhanced safety protocols during vision-language training to address these vulnerabilities.

## Method Summary
The study develops a comprehensive evaluation framework comprising two novel VQA datasets (OODCV-VQA and Sketchy-VQA) and adversarial attack strategies targeting CLIP's vision encoder. The benchmark evaluates 21 VLLM models across multiple dimensions including OOD generalization, adversarial robustness, and jailbreaking resistance. The approach includes both visual and textual out-of-distribution scenarios, with experiments designed to test model responses to counterfactual descriptions and adversarial perturbations. The methodology employs Perspective API for toxicity scoring and implements two jailbreaking methods targeting both vision and language components of VLLMs.

## Key Results
- VLLMs demonstrate superior performance on OOD visual content compared to OOD textual instructions, particularly when faced with counterfactual descriptions
- Adversarial attacks on vision encoders prove highly effective, with GPT-4V showing better resistance through refusal mechanisms
- Jailbreaking strategies targeting only the vision encoder show limited effectiveness in inducing toxic outputs, indicating complex attack requirements
- Current vision-language training weakens established safety protocols from LLMs, highlighting training paradigm limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLLMs are better at understanding OOD visual content than following OOD text instructions.
- Mechanism: Vision encoders in VLLMs have been trained on diverse visual data and can generalize to out-of-distribution images, but language models are sensitive to perturbations in textual instructions, especially counterfactual modifications.
- Core assumption: The visual component of VLLMs generalizes better than the language component when exposed to distribution shifts.
- Evidence anchors: [abstract] "VLLMs excel at comprehending OOD visual content but struggle with OOD textual input."
- Break condition: If the language component is fine-tuned with robust counterfactual data or if the vision encoder overfits to training distributions.

### Mechanism 2
- Claim: Attacking the vision encoder alone can mislead VLLMs into generating irrelevant outputs.
- Mechanism: Adversarial perturbations added to images can cause the vision encoder to produce feature representations that align with irrelevant textual objects, leading the language model to generate descriptions unrelated to the actual image content.
- Core assumption: The language model relies heavily on the vision encoder's output and cannot independently verify visual content.
- Evidence anchors: [abstract] "These VLLMs can be easily misled by deceiving vision encoders only."
- Break condition: If the language model incorporates additional verification steps or if the vision encoder has built-in robustness against adversarial perturbations.

### Mechanism 3
- Claim: Vision-language training can weaken safety protocols established in LLMs.
- Mechanism: The integration of vision and language during training introduces new data and tasks that may not include safety-aligned examples, leading to a dilution of the safety measures learned during LLM pre-training.
- Core assumption: Safety protocols are not sufficiently reinforced during vision-language fine-tuning.
- Evidence anchors: [abstract] "Current vision-language training weakens safety protocols in aligned language models."
- Break condition: If safety protocols are explicitly incorporated into the vision-language training data and objectives.

## Foundational Learning

- Concept: Out-of-distribution (OOD) generalization
  - Why needed here: The paper evaluates how VLLMs perform on data that differs from their training distribution, both in visual and textual modalities.
  - Quick check question: What is the difference between in-distribution and out-of-distribution data in the context of machine learning models?

- Concept: Adversarial robustness
  - Why needed here: The paper introduces adversarial attacks targeting the vision encoder to assess the vulnerability of VLLMs.
  - Quick check question: How do adversarial attacks exploit the weaknesses of machine learning models?

- Concept: Vision-language pre-training and fine-tuning
  - Why needed here: Understanding how VLLMs are trained is crucial to interpreting the results and the impact on safety protocols.
  - Quick check question: What are the typical stages involved in training a Vision-Language model?

## Architecture Onboarding

- Component map: Image → Vision encoder → Feature representation → Vision-language connector → Language model → Response
- Critical path: Vision encoder processes image, generates features, which are connected to language model through QFormer/Linear layer, producing final output
- Design tradeoffs: Balancing between visual understanding and language comprehension; trade-off between model size and computational efficiency; incorporating safety measures without compromising performance
- Failure signatures: Incorrect identification of objects in OOD images; generation of irrelevant or toxic content in response to adversarial inputs; inability to refuse answering inappropriate questions
- First 3 experiments:
  1. Evaluate VLLM performance on OODCV-VQA and Sketchy-VQA datasets to assess OOD generalization
  2. Apply the proposed adversarial attack (MIX.ATTACK) to images and observe the impact on VLLM outputs
  3. Test the effectiveness of jailbreaking strategies on both vision and language components of VLLMs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can vision-language training protocols be modified to preserve safety alignment established in LLMs?
- Basis in paper: [explicit] The paper discusses how current vision-language training weakens safety protocols in aligned language models
- Why unresolved: While the paper identifies the problem, it does not propose specific solutions or modifications to training protocols
- What evidence would resolve it: Comparative studies showing safety metrics before and after implementing proposed modifications to vision-language training protocols

### Open Question 2
- Question: What is the minimum level of visual detail required for VLLMs to accurately process and respond to sketch images?
- Basis in paper: [inferred] The paper shows VLLMs struggle with sketchy images, but doesn't specify the threshold of detail needed for accurate processing
- Why unresolved: The study identifies difficulty with sketches but doesn't quantify the relationship between sketch detail and model performance
- What evidence would resolve it: Experiments varying sketch detail levels while measuring model accuracy to identify the threshold where performance significantly improves

### Open Question 3
- Question: Can adversarial training on vision encoders improve VLLM robustness without compromising performance on clean data?
- Basis in paper: [explicit] The paper demonstrates VLLMs are vulnerable to adversarial attacks on vision encoders but doesn't explore defensive training approaches
- Why unresolved: The study focuses on attack methods but doesn't investigate potential defenses or their impact on clean data performance
- What evidence would resolve it: Results comparing model robustness and clean data performance before and after adversarial training on vision encoders

## Limitations

- The experimental setup may not fully capture the complexity of real-world distribution shifts, as OOD datasets are synthetically constructed
- Adversarial attack effectiveness relies on specific implementations that may not generalize across different vision encoder architectures
- Jailbreaking experiments focus primarily on single-turn attacks, potentially underestimating robustness against multi-turn adversarial strategies

## Confidence

- **High Confidence**: The finding that VLLMs show better performance on OOD visual content than OOD textual instructions
- **Medium Confidence**: The claim about vision-language training weakening safety protocols
- **Low Confidence**: The effectiveness of jailbreaking strategies targeting only the vision encoder

## Next Checks

1. Test the same VLLMs on naturally occurring OOD data from real-world sources to validate whether synthetic OOD datasets accurately represent true distribution shifts

2. Evaluate the proposed adversarial attack strategy against different vision encoder architectures (beyond CLIP) to assess its general effectiveness and identify potential architectural vulnerabilities

3. Design and execute multi-turn adversarial strategies that alternate between visual and textual attacks to test whether current VLLMs can maintain safety protocols across extended interactions