---
ver: rpa2
title: Fighting the disagreement in Explainable Machine Learning with consensus
arxiv_id: '2307.01288'
source_url: https://arxiv.org/abs/2307.01288
tags:
- consensus
- features
- function
- learning
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study tackles the problem of inconsistent explanations in machine
  learning interpretability, where different algorithms often disagree in their feature
  attributions. The authors evaluate six consensus functions on five machine learning
  models trained on four synthetic datasets with known underlying rules.
---

# Fighting the disagreement in Explainable Machine Learning with consensus

## Quick Facts
- **arXiv ID:** 2307.01288
- **Source URL:** https://arxiv.org/abs/2307.01288
- **Reference count:** 38
- **Key outcome:** A novel consensus function incorporating model accuracy, class probability, and normalized attributions outperforms traditional approaches in identifying relevant features when interpretability algorithms disagree.

## Executive Summary
This paper addresses the critical problem of inconsistent explanations in machine learning interpretability, where different algorithms often produce conflicting feature attributions. The authors propose a novel consensus function that combines multiple interpretability methods while accounting for model accuracy, prediction confidence, and proper normalization of attribution scales. Evaluated on synthetic datasets with known ground truth, the proposed method demonstrates superior performance in identifying relevant features compared to traditional consensus approaches, particularly when model accuracy is low.

## Method Summary
The study creates four synthetic datasets with known underlying rules and trains five ML models (KNN, RF, SVM, XGB, ANN) on each. Six interpretability algorithms (permutation importance, RF-based, LIME, SHAP, IG, counterfactuals) generate explanations, which are then combined using six consensus functions including arithmetic mean, harmonic mean, geometric mean, voting, ranking, and a novel function. The novel consensus function normalizes attributions, weights them by model accuracy and class probability, and adjusts for differences between global and local interpretability methods. Performance is evaluated by comparing consensus explanations against ground truth feature importance.

## Key Results
- The proposed consensus function outperformed traditional approaches (Amean, Hmean, Gmean, voting, ranking) in identifying relevant features
- Performance advantage was most pronounced when model accuracy was low
- The method better distinguished relevant features from noise by penalizing explanations from less accurate models
- Results demonstrated that effective consensus functions must account for multiple factors beyond simple averaging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Normalizing attributions across interpretability algorithms allows fair comparison and combination.
- Mechanism: Min-max normalization rescales all attribution values to [0,1], ensuring algorithms with different attribution ranges don't dominate the consensus.
- Core assumption: Attribution scale differences are due to scaling rather than relevance.
- Evidence anchors:
  - "The proposed method... demonstrating that consensus functions must account for multiple factors to be effective."
  - "Since SIBILA tool [23] was used to automate and accelerate the calculations, the classification datasets were adapted to resolve binary classification problems... That way, we simulate a normalized dataset improving model accuracy..."
- Break condition: If normalization masks meaningful differences in attribution magnitude that correlate with feature importance.

### Mechanism 2
- Claim: Weighting attributions by model accuracy and class probability improves consensus reliability.
- Mechanism: Attributions are multiplied by α (model AUC) and β (class probability), downweighting contributions from less accurate models or uncertain predictions.
- Core assumption: Explanations from inaccurate models or low-confidence predictions are less trustworthy.
- Evidence anchors:
  - "Results showed the new function better identified relevant features while penalizing explanations from less accurate models..."
  - "An inaccurate model will fail most of its predictions and, consequently, the explanations obtained from it will be inaccurate or wrong too."
- Break condition: If model accuracy or class probability estimates are unreliable.

### Mechanism 3
- Claim: Accounting for differences between global and local interpretability methods prevents double-counting.
- Mechanism: Local method attributions are divided by the number of samples (N) so they contribute proportionally the same as global attributions.
- Core assumption: Global methods produce one attribution per feature, while local methods produce one per sample.
- Evidence anchors:
  - "...consensus functions must account for multiple factors to be effective."
  - "Therefore, if both values were combined in one single equation, local methods would have more importance simply because they contribute with more values."
- Break condition: If sample count is not representative of the dataset.

## Foundational Learning

- **Model-agnostic interpretability**
  - Why needed: Ensures fair comparison across diverse ML models
  - Quick check: Can the interpretability algorithm be applied to any black-box model, or is it tied to a specific architecture?

- **Feature attribution and ranking**
  - Why needed: Consensus functions rely on comparing feature importances from different algorithms
  - Quick check: Does the interpretability algorithm provide absolute attribution scores, relative rankings, or both?

- **Normalization and scaling**
  - Why needed: Consensus function normalizes attributions to [0,1] to handle scale differences
  - Quick check: What scaling method is used, and how does it affect relative feature importance?

## Architecture Onboarding

- **Component map:**
  Synthetic datasets -> ML models (KNN, RF, SVM, XGB, ANN) -> Interpretability algorithms (permutation importance, RF-based, LIME, SHAP, IG, counterfactuals) -> Consensus functions (Amean, Hmean, Gmean, voting, ranking, proposed) -> Evaluation metrics (feature hit rate, attribution ranking)

- **Critical path:**
  1. Generate synthetic datasets with known feature importance rules
  2. Train ML models and select best-performing instance
  3. Apply interpretability algorithms to generate explanations
  4. Normalize and weight attributions per proposed method
  5. Combine attributions via consensus function
  6. Evaluate against ground truth feature importance

- **Design tradeoffs:**
  - Synthetic datasets guarantee ground truth but may not reflect real-world complexity
  - Proposed consensus function adds complexity but improves reliability
  - Multiple weighting factors increase robustness but reduce interpretability

- **Failure signatures:**
  - Consensus fails to identify ground truth features when model accuracy is low
  - Attribution normalization leads to loss of discriminative power
  - Weighting by accuracy/class probability amplifies noise if estimates are unreliable

- **First 3 experiments:**
  1. Run consensus on a simple synthetic dataset with one strong feature and noise; compare hit rates
  2. Introduce a low-accuracy model and observe how proposed consensus penalizes its attributions versus arithmetic mean
  3. Test consensus with a dataset where local methods are applied to a small sample; verify sample count adjustment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do consensus functions perform on real-world datasets with unknown underlying rules compared to synthetic datasets with known rules?
- Basis: The paper concludes future work will focus on applying consensus to real-life datasets whose internal rules are completely unknown.
- Why unresolved: Study only tested synthetic datasets with known rules.
- Evidence needed: Testing consensus functions on real-world datasets and comparing performance metrics to synthetic datasets.

### Open Question 2
- Question: Can the novel consensus function be effectively extended to handle multiclass datasets and more complex models like CNNs or time series?
- Basis: Paper mentions future work on multiclass datasets and more complex models.
- Why unresolved: Current evaluation limited to binary classification and simpler models.
- Evidence needed: Applying the function to multiclass datasets and complex models, evaluating feature identification and accuracy maintenance.

### Open Question 3
- Question: What are the benefits and limitations of developing an ML-based consensus function compared to traditional and novel consensus functions?
- Basis: Paper suggests extending consensus functions and mentions ML-based consensus as an alternative.
- Why unresolved: Only evaluated traditional and novel consensus functions.
- Evidence needed: Developing and testing an ML-based consensus function, comparing performance metrics to traditional and novel approaches.

## Limitations
- Reliance on synthetic datasets with known ground truth may not capture real-world complexity
- Proposed consensus function introduces multiple weighting factors that may be sensitive to estimation errors
- Evaluation focuses on feature identification rather than downstream task performance

## Confidence
- **High Confidence:** Normalizing attributions to enable fair combination across algorithms is well-supported by results
- **Medium Confidence:** Weighting scheme incorporating accuracy and class probability shows empirical success but depends on reliable estimation
- **Low Confidence:** Sample count adjustment for local methods is theoretically justified but lacks extensive empirical validation

## Next Checks
1. Apply consensus function to real-world tabular datasets with documented feature importance to verify performance outside synthetic environments
2. Introduce noise into model accuracy and class probability estimates to quantify consensus function sensitivity to these parameters
3. Measure whether consensus explanations lead to measurable improvements in model debugging, feature engineering, or prediction accuracy in practical scenarios