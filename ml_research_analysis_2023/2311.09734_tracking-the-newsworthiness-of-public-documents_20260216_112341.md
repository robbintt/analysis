---
ver: rpa2
title: Tracking the Newsworthiness of Public Documents
arxiv_id: '2311.09734'
source_url: https://arxiv.org/abs/2311.09734
tags:
- policy
- public
- meeting
- policies
- newsworthiness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of tracking newsworthiness of
  public policy documents by linking them to news articles. It proposes a probabilistic
  relational modeling approach that breaks down the linking task into a chain of supervised
  attribute predictions, achieving 68% F1 in linking articles to policies.
---

# Tracking the Newsworthiness of Public Documents

## Quick Facts
- arXiv ID: 2311.09734
- Source URL: https://arxiv.org/abs/2311.09734
- Reference count: 40
- Primary result: 68% F1 linking articles to policies; 63% F1 newsworthiness prediction; 84% win-rate in human evaluation

## Executive Summary
This paper addresses the challenge of linking public policy documents to news articles and predicting newsworthiness. The authors propose a probabilistic relational modeling approach that breaks down the linking task into a chain of supervised attribute predictions, achieving strong performance in connecting San Francisco Chronicle articles to SFBOS policy proposals. Using the linked data, they formulate a newsworthiness prediction task and find that their models outperform simple baselines while capturing non-temporal trends. Human evaluation with journalists demonstrates the practical utility of the approach, with models identifying newsworthy policies with 63% F1 and providing helpful recommendations with 84% win-rate.

## Method Summary
The authors develop a two-stage approach for tracking newsworthiness of public documents. First, they employ probabilistic relational modeling (PRM) to link news articles to policy proposals by breaking the task into a chain of attribute predictions: whether articles cover SFBOS, cover votes/policy, cover recent SFBOS policy, and ultimately cover specific policies. This linking achieves 68% F1. Second, they use the linked data to train newsworthiness prediction models by extracting features from policy text, meeting discussions, speaker counts, and public comments, then fine-tuning GPT-3 on these features to predict coverage. The models are evaluated both automatically and through human journalist feedback.

## Key Results
- PRM linking approach achieves 68% F1 in connecting articles to policies
- Newsworthiness prediction models identify newsworthy policies with 63% F1
- Human evaluation shows 84% win-rate for model recommendations over baseline
- Models capture non-temporal trends and remain stable across time periods except for major events like COVID-19

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic relational modeling (PRM) enables effective cross-domain linking by decomposing the link prediction problem into a chain of conditional attribute predictions.
- Mechanism: Instead of modeling P(l|a,p) directly, PRM marginalizes over auxiliary variables h₁, h₂, … hₙ, where each hᵢ refines the link prediction by conditioning on progressively more specific attributes (e.g., "covers SFBOS" → "covers SFBOS votes/policy" → "covers recent SFBOS policy").
- Core assumption: Each conditional step in the chain is a simpler, more tractable prediction problem that can be supervised independently.
- Evidence anchors:
  - [abstract] "We show that breaking this problem down into a chain of decisions, each conditional on the previous, an application of probabilistic relational modeling (PRM) helps us outperform other retrieval-based baselines."
  - [section] "Our attribute-based model, as shown in Table 1, helps us retrieve (a, p) ∈ Sgold with 68% f1. We show via an ablation experiment that each attribute hi is important for our final prediction: Table 1 shows how F1 drops from 68% to 16% when we remove hi-conditioning steps."
- Break condition: If any conditional step becomes too noisy or the conditional independence assumptions fail, the chain's effectiveness degrades.

### Mechanism 2
- Claim: Fine-tuning GPT-3 on policy and meeting text features yields strong newsworthiness predictions because the model can jointly model numerical and textual signals in a single prompt.
- Mechanism: The model is prompted with concatenated policy text, meeting discussion length, speaker count, and public comment summaries, then fine-tuned to predict coverage (Y(p) = 1 if covered, 0 otherwise). This allows the model to learn complex interactions between text semantics and engagement metrics.
- Core assumption: The newsworthiness signal is learnable from the combination of policy content and meeting dynamics, and can be captured by a large language model's pretraining.
- Evidence anchors:
  - [abstract] "We show that different aspects of public policy discussion yield different newsworthiness signals."
  - [section] "We find that the full prompt performs the best across all metrics we considered, but only marginally. As expected, ablating 'Public Comment' from the prompt barely impacts performance, while ablating all 'meeting info.' impacts a little more."
- Break condition: If newsworthiness is driven by external, non-textual factors (e.g., political timing, off-meeting events), the model may fail to capture them.

### Mechanism 3
- Claim: Temporal consistency in coverage patterns allows models trained on past data to generalize to future predictions.
- Mechanism: By using a time-based train/test split and retraining models with progressively earlier cutoffs, the authors show that performance remains stable except for major anomalies (e.g., COVID-19), suggesting that coverage patterns are largely stable over time.
- Core assumption: The factors that make a policy newsworthy in the past will remain relevant in the future, barring exceptional events.
- Evidence anchors:
  - [abstract] "We show that different aspects of public policy discussion yield different newsworthiness signals."
  - [section] "Table 3 and Table 8 show that words related to specific events (e.g. those related to 'COVID-19') are reflected in the perceived newsworthiness of policy... To test this question, we retrain our model and increasingly restrict the date cutoffs of our training set... Table 6 shows that, except for a dropoff after excluding data from 2021, our performance does not significantly change."
- Break condition: If the nature of local governance or media coverage shifts (e.g., due to policy changes, newsroom restructuring), the model's assumptions may no longer hold.

## Foundational Learning

- Concept: Probabilistic Relational Modeling (PRM)
  - Why needed here: Enables decomposition of a complex, sparsely labeled link prediction problem into tractable supervised sub-problems by conditioning on auxiliary attributes.
  - Quick check question: How does PRM differ from direct link prediction in terms of data requirements and model complexity?

- Concept: Fine-tuning large language models (LLMs) for classification
  - Why needed here: Allows joint modeling of structured numerical features (e.g., meeting length, speaker count) and unstructured text (policy proposals, meeting transcripts) in a unified prompt-based interface.
  - Quick check question: What are the advantages and limitations of using LLM fine-tuning versus traditional feature-based classifiers for newsworthiness prediction?

- Concept: Time-based train/test splitting
  - Why needed here: Ensures that models are evaluated on their ability to predict future coverage, reflecting real-world deployment scenarios.
  - Quick check question: Why is a random split inappropriate for this task, and how does time-based splitting help assess model robustness?

## Architecture Onboarding

- Component map: Article corpus -> PRM linking chain (h1->h2->h3->link) -> Feature extraction -> GPT-3 fine-tuning -> Newsworthiness prediction -> Human evaluation

- Critical path:
  1. Gather and preprocess corpora.
  2. Train PRM chain models and generate (article, policy) links.
  3. Extract features from linked data and meeting transcripts.
  4. Fine-tune GPT-3 on newsworthiness prediction task.
  5. Evaluate with human journalists.

- Design tradeoffs:
  - PRM vs. embedding-based retrieval: PRM requires more annotation but handles domain shift better; embeddings are cheaper but may miss technical/legal phrasing.
  - LLM fine-tuning vs. traditional classifiers: LLMs can model complex interactions but are opaque and expensive; traditional methods are interpretable but may miss nuanced signals.
  - Public comment inclusion: Adds potential signal but is noisy and sparse; excluding it simplifies the model but may miss emotional tenor.

- Failure signatures:
  - PRM chain: Low recall if any conditional step fails (e.g., if "covers SFBOS" classifier is weak).
  - LLM fine-tuning: Overfitting to training period or specific events; failure to generalize to new policy types.
  - Time-based evaluation: Sudden drops in performance if coverage patterns change (e.g., new editorial focus, external events).

- First 3 experiments:
  1. Ablation study on PRM chain: Remove h₁, h₂, h₃ one at a time and measure impact on linking F1.
  2. Feature ablation for newsworthiness: Remove policy text, meeting info, or public comment from prompt and measure impact on prediction F1.
  3. Time-based retraining: Retrain model with progressively earlier cutoffs and measure performance to assess temporal generalization.

## Open Questions the Paper Calls Out
- How would newsworthiness prediction performance change if we incorporated national-level newsworthy events and language patterns, instead of focusing solely on local policy discussions?
- How would newsworthiness prediction performance change if we used a more advanced alignment method, such as dynamic time warping, instead of the simple approach used in the paper?
- How would newsworthiness prediction performance change if we used a larger dataset of public meeting recordings, or incorporated other channels such as social media, to associate more public commenters with policies?

## Limitations
- Domain generalization: Model performance on other policy domains or geographic regions remains untested
- Temporal stability: Models show sensitivity to major societal events like COVID-19 that could invalidate historical patterns
- Public comment integration: Limited ability to associate public comments with specific policies constrains this potentially valuable signal

## Confidence
- High Confidence: PRM linking approach achieving 68% F1 is well-supported by ablation studies
- Medium Confidence: Newsworthiness prediction performance (63% F1) is supported by cross-validation but may not generalize beyond tested domain
- Medium Confidence: Human evaluation results (84% win-rate) provide qualitative support but represent a small sample

## Next Checks
1. **Domain Transfer Test:** Evaluate the linking and newsworthiness models on policy documents and news articles from a different city council or legislative body to assess generalizability.
2. **Temporal Robustness Analysis:** Systematically test model performance when trained on data from before and after major policy changes or newsroom restructuring events.
3. **Alternative Model Comparison:** Implement and compare against simpler embedding-based retrieval approaches to quantify the added value of the PRM chain methodology.