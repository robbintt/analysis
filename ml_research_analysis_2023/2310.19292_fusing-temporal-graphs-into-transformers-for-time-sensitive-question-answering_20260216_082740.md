---
ver: rpa2
title: Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering
arxiv_id: '2310.19292'
source_url: https://arxiv.org/abs/2310.19292
tags:
- temporal
- timeqa
- question
- graph
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to fuse temporal graphs into Transformer
  models for time-sensitive question answering. The approach uses existing temporal
  information extraction systems to construct graphs of events, times, and relations,
  then fuses these graphs into the input text of Transformer models.
---

# Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering

## Quick Facts
- arXiv ID: 2310.19292
- Source URL: https://arxiv.org/abs/2310.19292
- Reference count: 40
- Key outcome: ERR method enhances temporal reasoning capabilities and achieves new state-of-the-art performance on TimeQA and SituatedQA datasets

## Executive Summary
This paper introduces a method to fuse temporal graphs into Transformer models for time-sensitive question answering. The approach uses existing temporal information extraction systems to construct graphs of events, times, and relations, then fuses these graphs into the input text of Transformer models. Two fusion methods are explored: explicit edge representation (ERR) and graph neural network (GNN) fusion. Experiments on TimeQA and SituatedQA datasets show that the ERR method substantially enhances temporal reasoning capabilities and achieves new state-of-the-art performance. The ERR method is particularly effective for hard questions requiring complex temporal reasoning and can be seamlessly integrated with large language models in an in-context learning setting.

## Method Summary
The paper proposes two methods for fusing temporal graphs into Transformer models for time-sensitive question answering. The explicit edge representation (ERR) method marks temporal nodes and edges in the input text with special tokens like `<before>`, `<after>`, and `<included by>`. The graph neural network (GNN) method applies relational graph convolution over marked nodes to update token embeddings or contextualized representations. Both methods are applied to LongT5-base model and tested on TimeQA and SituatedQA datasets. The ERR method shows consistent improvements across all question types, while the GNN method shows inconsistent performance.

## Key Results
- ERR method achieves new state-of-the-art performance on TimeQA and SituatedQA datasets
- ERR method substantially improves performance on hard questions requiring complex temporal reasoning
- ERR method can be seamlessly integrated with large language models in in-context learning settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly marking temporal graph edges in the input sequence (ERR) improves temporal reasoning by making the model learn to attend to relevant time markers and temporal relations.
- Mechanism: The ERR method introduces special tokens like `<before>`, `<after>`, `<included by>` around events and time expressions in the input text. This creates explicit signals that guide the model's attention to parts of the input that contain answer-relevant temporal information.
- Core assumption: The model can learn to interpret these special markers and use them to improve temporal reasoning without requiring architectural modifications.
- Evidence anchors:
  - [abstract]: "our proposed approach for fusing temporal graphs into input text substantially enhances the temporal reasoning capabilities of Transformer models"
  - [section]: "This approach aims to make the model learn to attend to parts of the input sequence that may contain answer information"
- Break condition: If the model fails to learn the correspondence between special markers and temporal reasoning, or if the marker-based approach introduces too much noise in the input sequence.

### Mechanism 2
- Claim: GNN-based fusion of temporal graphs into token embeddings or contextualized representations can enhance temporal reasoning by incorporating structured graph information into the model.
- Mechanism: A relational graph convolution (RelGraphConv) is applied to encode the temporal graph and update the Transformer's token embeddings or last hidden layer representations. The GNN learns different transformations for different relation types (BEFORE, AFTER, etc.) and propagates this information through the graph structure.
- Core assumption: The GNN can effectively learn to represent temporal graph structure and integrate it with the Transformer's representations to improve reasoning.
- Evidence anchors:
  - [abstract]: "Experimental results show that our proposed approach for fusing temporal graphs into input text substantially enhances the temporal reasoning capabilities of Transformer models"
  - [section]: "In GNN-based fusion, we add `<e>` and `</e>` markers around each node, and apply a relational graph convolution (RelGraphConv) over the marked nodes"
- Break condition: If the GNN fails to capture meaningful temporal relations or if the fusion process introduces noise that degrades the original Transformer representations.

### Mechanism 3
- Claim: Temporal graph fusion methods can be seamlessly integrated with large language models using in-context learning without requiring fine-tuning.
- Mechanism: The ERR method's input modification approach can be applied to prompts given to large language models like ChatGPT. By adding temporal graph markers to the input sequence in the prompt, the model can leverage these explicit temporal signals for reasoning without any parameter updates.
- Core assumption: Large language models can interpret and utilize the special temporal markers in prompts similarly to how fine-tuned models use them.
- Evidence anchors:
  - [abstract]: "our proposed method outperforms various graph convolution-based approaches and establishes a new state-of-the-art performance on SituatedQA and three splits of TimeQA"
  - [section]: "We demonstrate that our input fusion approach can be used seamlessly with large language models in an in-context learning setting"
- Break condition: If the large language model fails to interpret the temporal markers or if the prompt length becomes too constrained by adding graph information.

## Foundational Learning

- Concept: Temporal information extraction and graph construction
  - Why needed here: The entire approach depends on first extracting events, times, and temporal relations from text to build the temporal graphs that will be fused with the model
  - Quick check question: What are the six temporal relation types used in the temporal graphs, and how are they extracted from text?

- Concept: Graph neural networks and relational graph convolutions
  - Why needed here: The GNN-based fusion method uses RelGraphConv to encode temporal graphs and update model representations, requiring understanding of how GNNs process graph-structured data
  - Quick check question: How does RelGraphConv differ from standard graph convolution, and why is this difference important for temporal relation types?

- Concept: Transformer attention mechanisms and input processing
  - Why needed here: Both fusion methods modify how temporal information is presented to the Transformer, requiring understanding of how attention works and how input modifications affect model behavior
  - Quick check question: How does adding special tokens like `<before>` and `<after>` to the input sequence influence the Transformer's attention patterns?

## Architecture Onboarding

- Component map:
  - Temporal information extraction system (CAEVO + SUTime) -> Temporal graph construction module -> Transformer model (LongT5-base) -> Graph fusion module (ERR or GNN-based) -> Question answering output layer

- Critical path: Extract temporal information → Construct temporal graph → Fuse graph into model → Generate answer
- Design tradeoffs:
  - ERR vs GNN fusion: ERR is simpler and works better but requires input sequence modifications; GNN is more architecturally integrated but requires fine-tuning and showed inconsistent performance
  - Full graph vs subgraph fusion: Full graphs provide more information but significantly increase input length; subgraphs are more efficient but may miss relevant temporal relations
  - Token embedding vs contextualized embedding fusion: Token embedding fusion affects all layers but may lose contextual information; contextualized fusion preserves context but only affects final representations

- Failure signatures:
  - Poor performance on hard questions despite success on easy questions: May indicate the fusion method isn't capturing complex temporal reasoning patterns
  - Degradation when using full graphs vs subgraphs: May indicate input length is becoming a bottleneck or the model can't handle excessive graph information
  - Inconsistent performance across datasets: May indicate the fusion method is too specialized to one dataset's temporal patterns

- First 3 experiments:
  1. Implement ERR fusion with DT2QT subgraph on TimeQA Easy development set to verify basic functionality
  2. Compare ERR vs GNN fusion on TimeQA Hard development set to understand performance differences
  3. Test ERR method with in-context learning on ChatGPT using sampled TimeQA examples to verify compatibility

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- ERR method substantially increases input sequence length, which could create practical limitations for deployment on resource-constrained systems
- GNN-based fusion method showed inconsistent performance across different experimental settings
- Generalizability of the approach to other temporal reasoning tasks or domains beyond TimeQA and SituatedQA remains unknown

## Confidence
- **High confidence**: The basic claim that temporal graph fusion can enhance Transformer performance on time-sensitive QA. The experimental results show consistent improvements across multiple dataset splits and question types.
- **Medium confidence**: The specific superiority of ERR over GNN-based fusion methods. While results support this, the GNN approach showed high variance and the paper doesn't provide clear explanations for this inconsistency.
- **Low confidence**: The claim that these methods seamlessly integrate with large language models in practical applications. Only one example is provided without systematic evaluation of success rates or failure modes.

## Next Checks
1. **Cross-dataset generalization test**: Apply the ERR method to a different temporal reasoning dataset (e.g., MATRES or HiTab) to verify whether the performance gains transfer beyond the specific domains of TimeQA and SituatedQA.

2. **Robustness analysis of GNN fusion**: Systematically vary graph construction parameters (subgraph selection criteria, relation types included) and GNN hyperparameters to identify conditions under which GNN fusion succeeds or fails, explaining the observed performance inconsistency.

3. **Large-scale in-context learning evaluation**: Test the ERR method's compatibility with multiple large language models (ChatGPT, Claude, Llama) on a statistically significant sample of time-sensitive questions, measuring both success rates and prompt length constraints.