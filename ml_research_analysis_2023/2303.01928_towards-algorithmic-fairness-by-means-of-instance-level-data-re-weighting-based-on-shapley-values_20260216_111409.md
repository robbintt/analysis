---
ver: rpa2
title: Towards Algorithmic Fairness by means of Instance-level Data Re-weighting based
  on Shapley Values
arxiv_id: '2303.01928'
source_url: https://arxiv.org/abs/2303.01928
tags:
- data
- fairshap
- fairness
- training
- tpra
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FairShap, a novel instance-level data re-weighting
  method for fair algorithmic decision-making based on Shapley Values. FairShap measures
  the contribution of each training data point to a predefined fairness metric, enabling
  interpretable and model-agnostic fairness improvements.
---

# Towards Algorithmic Fairness by means of Instance-level Data Re-weighting based on Shapley Values

## Quick Facts
- **arXiv ID**: 2303.01928
- **Source URL**: https://arxiv.org/abs/2303.01928
- **Reference count**: 40
- **Primary result**: FairShap achieves significant fairness improvements (Equal Opportunity, Equalized Odds) with minimal accuracy loss across multiple datasets and models

## Executive Summary
This paper introduces FairShap, a novel instance-level data re-weighting method for algorithmic fairness based on Shapley Values. FairShap measures each training data point's contribution to fairness metrics (Equal Opportunity and Equalized Odds) and re-weights the training data accordingly. The method is validated across multiple datasets (CelebA, LFW A, FairFace for images; COMPAS, German Credit, Adult Income for tabular data) with different model architectures (Inception Resnet V1, Gradient Boosting Classifier), demonstrating significant fairness improvements while maintaining similar accuracy levels compared to baselines.

## Method Summary
FairShap is a pre-processing method that re-weights training data based on each instance's contribution to fairness metrics, measured using Shapley Values. The method computes Shapley Values for each training point with respect to fairness metrics (EOp or EOds) using an efficient k-NN approximation algorithm. These values are then normalized and used to re-weight the training data before model training. The approach is model-agnostic and interpretable, providing insights into which training instances contribute most to fairness improvements.

## Key Results
- FairShap achieves 10-20% improvement in Equal Opportunity and Equalized Odds across multiple datasets
- The method maintains comparable accuracy levels to baseline models while significantly improving fairness metrics
- Instance-level re-weighting provides interpretability through visualizations showing which data points drive fairness improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FairShap assigns higher weights to training samples that contribute most to reducing fairness disparities
- Mechanism: By computing Shapley Values based on fairness metrics (EOp, EOds) rather than accuracy, FairShap identifies which training points improve group fairness when included in the model. These points receive higher weights during re-training, effectively shifting the data distribution toward fairer outcomes.
- Core assumption: The fairness metric is decomposable and satisfies Shapley Value axioms (efficiency, symmetry, additivity, null element)
- Evidence anchors:
  - [abstract]: "FairShap measures the contribution of each training data point to a predefined fairness metric"
  - [section]: "FairShap leverages Shapley Values (Shapley, 1953) to measure the contribution of each data point to a pre-defined fairness metric"
- Break condition: If the fairness metric is not decomposable or if the reference dataset is highly biased, the Shapley Values may not accurately capture fairness contributions

### Mechanism 2
- Claim: FairShap improves fairness without significant accuracy loss by leveraging fair reference datasets
- Mechanism: When a fair reference dataset T is used, the Shapley Values computed for training data D reflect how much each point contributes to achieving fairness on T. Re-weighting D according to these values biases the training distribution toward fairness while maintaining performance on T.
- Core assumption: A fair reference dataset exists and is representative of the target distribution
- Evidence anchors:
  - [section]: "we perform experiments both with fair and biased reference datasetsT to empirically illustrate the impact ofT"
  - [section]: "The intuition is that eachφi(EOp) quantifies the contribution of the ith data point (image) in LFWA to the fairness metric (e.g. Equal Opportunity) of the model trained on LFWA and tested on the FairFace test split"
- Break condition: If no fair reference dataset is available, the method may amplify existing biases instead of mitigating them

### Mechanism 3
- Claim: FairShap provides interpretability by showing how individual instances affect group fairness metrics
- Mechanism: By computing instance-level weights, FairShap reveals which specific training samples have the largest impact on fairness. This enables stakeholders to understand which data points drive fairness improvements and why.
- Core assumption: Individual instance contributions can be meaningfully aggregated to explain group-level fairness outcomes
- Evidence anchors:
  - [abstract]: "FairShap's effectiveness is illustrated through histograms and latent space visualizations, showcasing its interpretability"
  - [section]: "We illustrate FairShap's interpretability by means of histograms and latent space visualizations"
- Break condition: If the relationship between individual instances and group metrics is non-linear or context-dependent, the interpretability may be misleading

## Foundational Learning

- Concept: Shapley Values and cooperative game theory
  - Why needed here: FairShap is fundamentally built on Shapley Value theory to fairly distribute the "contribution" of each data point to fairness
  - Quick check question: What are the four axioms that Shapley Values must satisfy, and why are they important for fair data valuation?

- Concept: Group fairness metrics (Demographic Parity, Equal Opportunity, Equalized Odds)
  - Why needed here: FairShap uses these metrics as the basis for computing data contributions, so understanding their definitions and relationships is crucial
  - Quick check question: How do Equal Opportunity and Equalized Odds differ mathematically, and what fairness aspects do they each capture?

- Concept: Data valuation methods and their applications
  - Why needed here: FairShap extends data valuation from accuracy-based to fairness-based, so understanding the original concept and its limitations is important
  - Quick check question: What are the main limitations of accuracy-based data valuation methods, and how does FairShap address them?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Shapley Value computation -> Weight normalization -> Model training -> Evaluation

- Critical path:
  1. Compute embeddings for training and reference datasets
  2. Calculate Shapley Values based on fairness metrics
  3. Normalize weights and re-weight training data
  4. Train model with re-weighted data
  5. Evaluate fairness and accuracy

- Design tradeoffs:
  - Computational cost vs. accuracy: Computing Shapley Values is expensive but necessary for fair data valuation
  - Reference dataset choice: Fair datasets improve fairness but may reduce domain relevance
  - Instance-level vs. group-level re-weighting: Instance-level provides more granular control but is more complex

- Failure signatures:
  - Poor fairness improvement despite high computational cost: Likely due to biased reference dataset or inappropriate fairness metric choice
  - Significant accuracy drop: May indicate over-correction in weight assignment or insufficient diversity in training data
  - Unstable results across runs: Could suggest sensitivity to random initialization or data ordering

- First 3 experiments:
  1. Verify Shapley Value computation: Compare φ(EOp) values on a small synthetic dataset with known ground truth
  2. Test weight distribution: Plot histograms of φ(EOp) values to ensure they reflect expected fairness contributions
  3. Validate fairness improvement: Train a simple model on re-weighted data and measure EOp/EOds before/after

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but identifies several areas for future work:
- Exploring the impact of different reference dataset choices on final model performance
- Extending FairShap to other fairness metrics beyond Equal Opportunity and Equalized Odds
- Investigating the computational efficiency of the method for larger datasets

## Limitations
- Computational cost remains significant even with k-NN approximation for Shapley Value computation
- Performance highly dependent on choice of reference dataset (fair vs. biased)
- Limited evaluation to only two fairness metrics (Equal Opportunity and Equalized Odds)

## Confidence

**High Confidence**: The core mechanism of using Shapley Values for instance-level re-weighting is well-established and theoretically sound

**Medium Confidence**: The empirical results show consistent improvements across multiple datasets, but the ablation studies on reference dataset choice are limited

**Medium Confidence**: The interpretability claims are supported by visualizations but lack quantitative measures of interpretability

## Next Checks

1. **Robustness Testing**: Evaluate FairShap across a wider range of model architectures (CNNs, Transformers) to verify architecture-agnostic improvements

2. **Reference Dataset Sensitivity**: Conduct systematic experiments varying the fairness level of reference datasets to quantify the impact on final model fairness

3. **Computational Efficiency Analysis**: Measure the trade-off between k-NN approximation accuracy and computational cost, determining optimal k values for different dataset sizes