---
ver: rpa2
title: Towards a Better Theoretical Understanding of Independent Subnetwork Training
arxiv_id: '2306.16484'
source_url: https://arxiv.org/abs/2306.16484
tags:
- page
- gradient
- which
- cited
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes Independent Subnetwork Training (IST), a distributed
  training technique that partitions a neural network into smaller submodels trained
  independently and then aggregated. The authors study IST theoretically on a quadratic
  loss model, identifying key differences from compressed communication methods.
---

# Towards a Better Theoretical Understanding of Independent Subnetwork Training

## Quick Facts
- arXiv ID: 2306.16484
- Source URL: https://arxiv.org/abs/2306.16484
- Reference count: 40
- One-line primary result: IST converges to optimal solution in interpolation cases but only to a neighborhood in general cases, with irreducible error size depending on step size.

## Executive Summary
This paper provides the first rigorous theoretical analysis of Independent Subnetwork Training (IST), a distributed training technique that partitions neural networks into smaller submodels trained independently and aggregated. The authors develop a theoretical framework for IST on quadratic loss functions, proving convergence results that distinguish between interpolation and non-interpolation settings. The analysis reveals that while IST achieves O(1/K) convergence to the exact optimum in interpolation cases, it only converges to a neighborhood of the optimum in general cases, with the neighborhood size being irreducible and dependent on the step size.

## Method Summary
The paper analyzes IST on quadratic loss functions of the form fi(x) = 1/2 x⊤Lix - x⊤bi. The simplified IST algorithm uses a gradient estimator gk = 1/n Σi Ck i ∇fi(Ck i xk), where Ck i are scaled permutation sketches. The analysis covers both homogeneous settings (all Li ≡ L) and heterogeneous settings (different Li), proving convergence results without relying on restrictive assumptions about gradient estimators. Experiments validate theoretical predictions using synthetic quadratic problems with matrices Bi and vectors bi generated from standard Gaussian distribution N(0,1).

## Key Results
- In interpolation cases (bi ≡ 0), IST achieves O(1/K) convergence to the exact optimum
- In non-interpolation cases, IST converges to a neighborhood of the optimum rather than the exact solution
- The neighborhood size is irreducible and scales with the step size γ
- The analysis avoids restrictive assumptions on gradient estimators, providing tighter characterizations than previous work

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Independent Subnetwork Training (IST) can efficiently combine data and model parallelism by leveraging structured sparsity patterns in the computation graph.
- Mechanism: By partitioning the model into disjoint subnetworks and applying permutation sketches, each worker computes gradients on a subset of parameters without exchanging intermediate activations. The aggregation step reconstructs the full gradient through averaging.
- Core assumption: The sketch matrices satisfy the perfect reconstruction property (1/n ∑ Ci = I) and the gradient estimator aligns with the true gradient direction in expectation.
- Evidence anchors:
  - [abstract]: "partitioning a neural network into smaller submodels trained independently and then aggregated"
  - [section 2]: Definition 2 (Permutation sketch) with Ci = n · Σ eπj e⊤πj
  - [corpus]: Weak - neighbor papers mention model parallelism but not IST's specific sketch-based aggregation mechanism
- Break condition: When the sketch matrices do not satisfy unbiased reconstruction (W ⊁ 0) or when heterogeneity creates misaligned gradient directions.

### Mechanism 2
- Claim: In interpolation cases (bi ≡ 0), IST achieves O(1/K) convergence to the exact optimum.
- Mechanism: The gradient estimator becomes unbiased (gk = Bxk) and the method effectively performs preconditioned gradient descent with convergence rate dependent on the spectral properties of W.
- Core assumption: The condition E[Bk L Bk] ⪯ θ W holds, ensuring the gradient estimator variance is controlled.
- Evidence anchors:
  - [section 3]: Theorem 1 proves O(1/K) convergence under condition (14)
  - [section 3.1]: Identity and permutation sketches satisfy the condition with specific θ values
  - [corpus]: Weak - no direct evidence in neighbor papers about interpolation convergence rates
- Break condition: When the matrices Li are heterogeneous and W becomes indefinite, violating condition (14).

### Mechanism 3
- Claim: In non-interpolation cases, IST converges to a neighborhood of the optimum rather than the exact solution.
- Mechanism: The gradient estimator contains a systematic bias term h = L⁻¹b - 1/√n · D⁻¹/²b that cannot be eliminated regardless of step size, creating an irreducible error floor.
- Core assumption: The heterogeneity between local loss functions creates a bias that persists in the limit.
- Evidence anchors:
  - [section 4.1]: Asymptotic analysis showing E[xk] → 1/√n · D⁻¹/²b ≠ x⋆
  - [section 4.2]: Theorem 2 bounds the neighborhood size with irreducible terms
  - [corpus]: Weak - neighbor papers don't discuss bias in distributed subnetwork training
- Break condition: When n → ∞ or when the linear terms bi align perfectly with the diagonal preconditioning, reducing the bias to zero.

## Foundational Learning

- Concept: Matrix smoothness (L-smoothness with matrix L)
  - Why needed here: The analysis requires weighted norms based on the smoothness matrix rather than standard Euclidean norms, enabling tighter convergence bounds
  - Quick check question: Why does using L⁻¹-weighted norms provide tighter bounds than standard Euclidean norms for quadratic problems?

- Concept: Fenchel-Young inequality
  - Why needed here: Used to decouple the gradient norm and bias terms in the convergence analysis, allowing separate characterization of convergence speed and neighborhood size
  - Quick check question: How does the Fenchel-Young inequality help separate the effects of gradient norm and bias in the convergence bound?

- Concept: Permutation sketches and their unbiased reconstruction properties
  - Why needed here: The core mechanism of IST relies on structured sparsity patterns that allow exact gradient reconstruction through averaging
  - Quick check question: What property must permutation sketches satisfy to guarantee that 1/n ∑ Ci = I?

## Architecture Onboarding

- Component map:
  Model partitioning layer -> Sketch generation module -> Local computation unit -> Aggregation server -> Communication layer

- Critical path:
  1. Model partitioning → Sketch generation → Local gradient computation → Sparse communication → Aggregation → Model update
  2. Each step must complete before the next can begin in synchronous settings

- Design tradeoffs:
  - Sparsity level vs. convergence speed: Higher sparsity (smaller q) reduces communication but may increase bias
  - Step size vs. neighborhood size: Larger step sizes accelerate convergence but increase the irreducible error floor
  - Synchronization vs. efficiency: Synchronous aggregation ensures correctness but may create stragglers

- Failure signatures:
  - Divergence: Step size too large relative to θ
  - Slow convergence: Poor conditioning of W or excessive heterogeneity
  - Large final error: Irreducible bias dominates due to misalignment between bi and diagonal preconditioning

- First 3 experiments:
  1. Validate perfect reconstruction: Test that 1/n ∑ Ci = I for permutation sketches on small synthetic problems
  2. Measure bias characterization: Compare E[xk] → 1/√n · D⁻¹/²b with theoretical predictions on heterogeneous quadratic problems
  3. Step size sensitivity: Plot convergence curves for varying step sizes to identify the trade-off between speed and final error magnitude

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IST convergence behavior change for non-quadratic loss functions beyond the quadratic model studied in this paper?
- Basis in paper: [inferred] The paper explicitly states this as a future research direction: "it would be interesting to generalize our results to non-quadratic scenarios without relying on pathological assumptions."
- Why unresolved: The current analysis is limited to quadratic problems, and the fundamental differences in gradient estimator behavior for non-quadratic functions remain unexplored.
- What evidence would resolve it: Theoretical convergence analysis for IST applied to non-quadratic loss functions (e.g., logistic regression, neural networks) with rigorous proofs showing whether the irreducible bias phenomenon persists.

### Open Question 2
- Question: What is the impact of local gradient steps (multiple iterations on each client before aggregation) on IST convergence in heterogeneous settings?
- Basis in paper: [explicit] The paper mentions in Section 2.2 that "local steps did not bring any theoretical efficiency improvements for heterogeneous settings until very recently (Mishchenko et al., 2022), and even then, only with the introduction of additional control variables."
- Why unresolved: The analysis assumes only one gradient step per iteration, and the benefits/drawbacks of local steps in the IST framework haven't been theoretically characterized.
- What evidence would resolve it: Convergence analysis showing how multiple local steps affect the bias term and convergence rate, potentially comparing with FedAvg-style methods.

### Open Question 3
- Question: Can the irreducible bias in IST be eliminated through algorithmic modifications without sacrificing communication efficiency?
- Basis in paper: [explicit] The paper concludes that "there exists an inherent trade-off between convergence speed and the size of the neighborhood" and identifies this as a fundamental limitation that future work should address.
- Why unresolved: The paper characterizes the bias but doesn't propose solutions to eliminate it while maintaining IST's efficiency benefits.
- What evidence would resolve it: Design and theoretical analysis of modified IST algorithms that achieve exact convergence to the optimum while preserving the communication advantages of submodel training.

## Limitations
- The analysis is limited to quadratic loss functions and may not capture the full complexity of deep learning scenarios
- The irreducible neighborhood size in non-interpolation cases represents a fundamental limitation of IST that may persist in more complex settings
- The theoretical bounds rely on specific conditions (like condition 14) that require careful verification in practical implementations

## Confidence

- High confidence: The mechanism of sketch-based gradient reconstruction and its unbiased properties when Ci = n · Σ eπj e⊤πj (Mechanism 1)
- Medium confidence: The O(1/K) convergence rate in interpolation cases (Mechanism 2) - depends on verifying condition 14 holds in practice
- Medium confidence: The irreducible bias characterization in non-interpolation cases (Mechanism 3) - theoretical but requires experimental validation

## Next Checks

1. **Experimental validation of bias characterization**: Implement the simplified IST algorithm on heterogeneous quadratic problems and measure whether E[xk] converges to 1/√n · D⁻¹/²b as predicted, testing the irreducible bias claims.

2. **Condition verification**: Systematically test condition 14 (E[Bk L Bk] ⪯ θ W) across different problem instances to determine its practical applicability and identify when it fails.

3. **Step size sensitivity analysis**: Conduct comprehensive experiments varying step size γ across orders of magnitude to empirically validate the trade-off between convergence speed and final error neighborhood size.