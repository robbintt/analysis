---
ver: rpa2
title: Effective Prompt Extraction from Language Models
arxiv_id: '2307.06865'
source_url: https://arxiv.org/abs/2307.06865
tags:
- prompt
- extraction
- prompts
- attack
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic study of prompt extraction attacks
  against large language models. The authors design simple text-based attacks to reconstruct
  hidden system prompts by querying model APIs.
---

# Effective Prompt Extraction from Language Models

## Quick Facts
- arXiv ID: 2307.06865
- Source URL: https://arxiv.org/abs/2307.06865
- Reference count: 23
- Key outcome: Simple text-based attacks can extract hidden prompts from LLMs with >80% success on GPT-3.5 and GPT-4

## Executive Summary
This paper presents a systematic study of prompt extraction attacks against large language models. The authors design simple text-based attacks to reconstruct hidden system prompts by querying model APIs. Using 200 prompts each from ShareGPT and Awesome-ChatGPT-Prompts as ground truth, they test attacks across three models: Vicuna-13B, GPT-3.5, and GPT-4. They find that over 80% of prompts can be successfully extracted from GPT-3.5 and GPT-4, while Vicuna-13B achieves 62.5%-83.6% extraction rates. A DeBERTa-based classifier achieves >75% precision in verifying extracted prompts. The paper also shows that common defenses like 5-gram matching are insufficient, as attackers can circumvent them with simple text manipulations.

## Method Summary
The authors conduct prompt extraction attacks by sending simple text-based queries to language models, attempting to elicit responses that contain the hidden system prompts. They use 200 prompts each from ShareGPT and Awesome-ChatGPT-Prompts as ground truth, testing attacks across three models (Vicuna-13B, GPT-3.5, GPT-4). Extractions are evaluated using BLEU score thresholds (0.6) to determine success. A DeBERTa classifier is fine-tuned on SHARE GPT-DEV to verify extracted prompts with >75% precision. The authors also evaluate a 5-gram defense mechanism that filters generations containing hidden prompt overlap.

## Key Results
- Over 80% of prompts successfully extracted from GPT-3.5 and GPT-4 using simple text-based attacks
- Vicuna-13B achieves 62.5%-83.6% extraction rates across different datasets
- DeBERTa-based classifier achieves >75% precision in verifying extracted prompts
- 5-gram defense reduces extraction success to 0% but can be circumvented by simple text manipulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simple text-based attack queries can successfully extract hidden prompts by instructing the model to repeat or reveal its conversation history
- Mechanism: The attack queries leverage the model's tendency to follow instructions literally, causing it to output the hidden prompt that's part of the conversation context
- Core assumption: The model will prioritize following the attack query's instruction over maintaining the hidden prompt as a secret
- Evidence anchors:
  - [abstract] "simple text-based attacks can in fact reveal prompts with high probability"
  - [section] "Taking inspiration from successful prompt injection attacks (Willison, 2022), the authors came up with a list of simple attack queries with the goal of eliciting a response from the language model that contains the prompt"
  - [corpus] Weak evidence - related papers focus on prompt extraction threats but don't detail specific attack query mechanisms
- Break condition: The model is trained to recognize and resist attempts to reveal hidden prompts, or the attack query fails to trigger the desired response pattern

### Mechanism 2
- Claim: A DeBERTa-based classifier can accurately verify whether an extracted prompt is the true hidden prompt
- Mechanism: The classifier compares multiple extractions from different attack queries and computes confidence scores based on their similarity to each other and the ground truth
- Core assumption: Successful extractions will share common patterns that distinguish them from model hallucinations
- Evidence anchors:
  - [abstract] "Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination"
  - [section] "Denoting P(ei |ej̸=i) as the probability that the classifier considers an extraction ei as successful when conditioned on the extractions ej̸=i produced by the other attack queries on the same prompt"
  - [corpus] Weak evidence - no corpus papers specifically discuss DeBERTa-based verification methods
- Break condition: The classifier's training data doesn't capture the full diversity of prompt extraction patterns, leading to false positives or negatives

### Mechanism 3
- Claim: A text-based 5-gram defense can block prompt extraction by filtering generations that contain the hidden prompt
- Mechanism: The defense checks for 5-gram overlap between the LLM's generation and the hidden prompt, returning an empty string if overlap is detected
- Core assumption: Attackers rely on the model generating the prompt verbatim, making 5-gram matching an effective detection mechanism
- Evidence anchors:
  - [section] "we use the following defense: when there is a 5-gram overlap between the LLM's generation and the hidden prompt, the defense simply returns an empty string"
  - [section] "The 5-gram defense is extremely effective against the attack in §3.1: extraction success rate drops to 0% for all model-dataset pairs"
  - [corpus] Weak evidence - related papers mention defenses but don't detail 5-gram matching approaches
- Break condition: Attackers modify their queries to manipulate the generation in ways that bypass 5-gram matching while still revealing the prompt

## Foundational Learning

- Concept: Prompt injection attacks
  - Why needed here: Understanding how malicious inputs can manipulate LLM behavior is crucial for grasping both the attack and defense mechanisms
  - Quick check question: What distinguishes a direct prompt injection attack from an indirect one?

- Concept: BLEU score calculation
  - Why needed here: The paper uses BLEU scores to measure prompt extraction success, requiring understanding of this metric's calculation and interpretation
  - Quick check question: How does BLEU score differ from exact string matching when evaluating prompt extraction?

- Concept: Fine-tuning vs. prompt engineering
  - Why needed here: The paper contrasts traditional model training approaches with prompt-based customization, which is central to understanding why prompts are valuable secrets
  - Quick check question: Why might prompt engineering be preferred over fine-tuning for commercial LLM applications?

## Architecture Onboarding

- Component map: LLM service API (with hidden prompt) -> Attacker client making queries -> DeBERTa classifier for verification -> 5-gram defense filter (optional)

- Critical path: Attack query → LLM generation → Extraction verification → Success determination

- Design tradeoffs:
  - Simpler attack queries are more robust but may have lower success rates
  - More sophisticated classifiers increase verification accuracy but require more computational resources
  - Defense mechanisms must balance security with legitimate user interactions

- Failure signatures:
  - High verification precision but low recall indicates the attack queries aren't effective enough
  - Defense blocking legitimate queries suggests overly aggressive filtering
  - Consistent false negatives in verification suggest classifier overfitting

- First 3 experiments:
  1. Test each attack query individually on a small sample of prompts to establish baseline extraction rates
  2. Evaluate the DeBERTa classifier's precision and recall on a held-out set of extractions
  3. Measure the impact of the 5-gram defense on both attack success and legitimate query blocking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the success rate of prompt extraction attacks vary when the user queries are inserted into the middle of the user instruction instead of being concatenated at the end?
- Basis in paper: [inferred] The paper mentions that the threat model assumes user queries are concatenated to the end of user instruction, which is common in practice, but notes that inserting queries into the middle could make prompts more difficult to extract.
- Why unresolved: The paper explicitly states this as a limitation, acknowledging that this alternative scenario was not tested.
- What evidence would resolve it: Experimental results comparing extraction success rates when queries are inserted at different positions within the prompt versus appended at the end.

### Open Question 2
- Question: Can classifiers designed to detect deviations from designer intentions effectively prevent prompt extraction attacks?
- Basis in paper: [inferred] The paper mentions this as a potential defense mechanism beyond the text-based 5-gram defense, but states it's unclear whether such classifiers can prevent prompt extraction entirely.
- Why unresolved: The paper identifies this as an area for future work and does not provide experimental results for this defense mechanism.
- What evidence would resolve it: Comparative analysis of prompt extraction success rates with and without a classifier-based defense, showing the effectiveness of such an approach.

### Open Question 3
- Question: How would the success rates of prompt extraction attacks change if attackers used adaptive strategies that select attack queries based on the model's responses rather than using a fixed set of queries?
- Basis in paper: [inferred] The paper notes that the attacks used a fixed set of queries and mentions that real-world attackers could achieve greater success by using adaptive strategies.
- Why unresolved: The paper chose to use a fixed set of queries for consistency but acknowledges this as a limitation.
- What evidence would resolve it: Experimental results comparing fixed-query attacks to adaptive-query attacks, measuring differences in success rates and efficiency.

### Open Question 4
- Question: What is the minimum number of queries required to achieve a high confidence level in successful prompt extraction across different models and datasets?
- Basis in paper: [explicit] The paper mentions that the attack is successful if at least one attack query leaks the prompt, but doesn't explore the minimum number of queries needed for reliable extraction.
- Why unresolved: The paper focuses on the effectiveness of the attacks rather than optimizing the number of queries used.
- What evidence would resolve it: Analysis showing the relationship between the number of queries and the confidence level of successful extraction, identifying the point of diminishing returns.

### Open Question 5
- Question: How effective are more sophisticated defense mechanisms, such as rate limiting or query pattern analysis, at preventing prompt extraction attacks?
- Basis in paper: [inferred] The paper discusses the limitations of simple text-based defenses and suggests exploring more effective defenses, but doesn't test these more sophisticated approaches.
- Why unresolved: The paper identifies this as an area for future exploration but doesn't provide experimental results for these defense mechanisms.
- What evidence would resolve it: Experimental comparison of prompt extraction success rates under different defense mechanisms, including rate limiting and query pattern analysis, showing their relative effectiveness.

## Limitations
- The study uses prompts from curated datasets rather than actual production system prompts, which may differ in structure and content
- The 5-gram defense is shown to be effective but acknowledged as a simple baseline that sophisticated attackers could circumvent
- The evaluation focuses on single-turn interactions, while real-world applications often involve multi-turn conversations with different attack vectors

## Confidence
**High confidence**: The core finding that simple text-based attacks can extract hidden prompts with >80% success rates on GPT-3.5 and GPT-4. The methodology is clearly specified, the attack queries are reproducible, and the evaluation metrics (BLEU score threshold of 0.6) are standard in the field. The verification framework using DeBERTa classifiers is also well-defined and achieves the claimed precision levels.

**Medium confidence**: The effectiveness of the 5-gram defense mechanism. While the authors demonstrate that this simple defense reduces extraction success to 0% in their experiments, they acknowledge it's not robust against more sophisticated attacks. The evaluation doesn't explore alternative defense strategies or measure the defense's impact on legitimate user interactions.

**Low confidence**: The generalizability of findings to production systems with unknown prompt structures. The study uses prompts from two specific datasets, but real-world system prompts may have different characteristics, lengths, or formatting that could affect attack success rates and defense effectiveness.

## Next Checks
1. **Cross-dataset validation**: Test the attack and defense mechanisms using prompts from production LLM services with unknown ground truth, comparing extraction success rates against the curated datasets used in the current study. This would validate whether the findings generalize beyond ShareGPT and Awesome-ChatGPT-Prompts.

2. **Adversarial defense evaluation**: Implement and test more sophisticated defense mechanisms beyond 5-gram matching, such as semantic similarity detection, prompt-aware context windows, or dynamic prompt injection detection. Measure both attack success rates and false positive rates on legitimate queries.

3. **Multi-turn conversation analysis**: Extend the attack framework to multi-turn conversations where prompts might be leaked through conversation history or model behavior over multiple interactions. This would better reflect real-world usage patterns where attackers have more opportunities to probe the system.