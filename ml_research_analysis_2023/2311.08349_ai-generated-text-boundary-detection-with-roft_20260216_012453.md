---
ver: rpa2
title: AI-generated text boundary detection with RoFT
arxiv_id: '2311.08349'
source_url: https://arxiv.org/abs/2311.08349
tags:
- text
- detection
- dataset
- roft
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of detecting the boundary between
  human-written and machine-generated text in mixed documents, a task that has received
  little attention in literature despite its increasing importance in real-world applications.
  The authors adapt several state-of-the-art artificial text detection methods to
  this boundary detection setting and evaluate them on the Real or Fake Text (RoFT)
  benchmark, including a new dataset generated with GPT-3.5-turbo.
---

# AI-generated text boundary detection with RoFT

## Quick Facts
- arXiv ID: 2311.08349
- Source URL: https://arxiv.org/abs/2311.08349
- Reference count: 29
- Key outcome: Perplexity-based methods outperform fine-tuned RoBERTa in cross-domain settings for detecting boundaries between human-written and machine-generated text

## Executive Summary
This work addresses the challenge of detecting boundaries between human-written and machine-generated text in mixed documents, a task that has received limited attention despite growing practical importance. The authors adapt state-of-the-art artificial text detection methods to this boundary detection setting and evaluate them on the Real or Fake Text (RoFT) benchmark, including a new dataset generated with GPT-3.5-turbo. They compare methods based on RoBERTa fine-tuning, perplexity-based features, and topological time series approaches. Key findings reveal that perplexity-based classifiers, particularly when using GPT-2, achieve strong results on both in-domain and out-of-domain data, often outperforming fully fine-tuned RoBERTa in cross-domain settings. The study identifies that sentence length serves as a strong in-domain signal but fails to generalize due to dataset artifacts, while topological features provide domain stability but underperform in absolute accuracy.

## Method Summary
The study adapts several state-of-the-art artificial text detection methods to the boundary detection setting using the RoFT benchmark dataset. Methods include RoBERTa fine-tuning on [CLS] embeddings, perplexity-based classifiers using GPT-2 perplexity scores, topological time series approaches using intrinsic dimensionality estimation, and length-based baselines. The authors evaluate these methods using accuracy, soft accuracy, and MSE metrics across in-domain and out-of-domain splits, conducting cross-domain transfer (leave-one-topic-out) and cross-model transfer (leave-one-generator-out) experiments. Feature extraction involves sliding window techniques for topological methods, sentence-wise perplexity calculation, and standard embedding approaches for RoBERTa.

## Key Results
- Perplexity-based classifiers, especially with GPT-2, outperform fine-tuned RoBERTa in cross-domain settings
- RoBERTa classifiers tend to overfit to spurious features like sentence length, leading to poor cross-domain generalization
- Topological features based on intrinsic dimensionality are highly stable under domain shifts but generally underperform in absolute accuracy
- Sentence length is a strong in-domain signal but fails to generalize across different text topics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity-based classifiers outperform fine-tuned RoBERTa in cross-domain settings.
- Mechanism: Language models assign lower perplexity to text from similar model families, creating a domain-invariant signal.
- Core assumption: GPT-2 perplexity reflects "naturalness" of text regardless of specific domain vocabulary.
- Evidence anchors:
  - [abstract] "perplexity-based methods, particularly when using GPT-2 as the underlying model, achieve strong results on both in-domain and out-of-domain data, often outperforming fully fine-tuned RoBERTa in cross-domain settings"
  - [section] "on RoFT-chatgpt the results are significantly better than for data generated by older language models, by all metrics"

### Mechanism 2
- Claim: Sentence length serves as strong in-domain signal but fails cross-domain due to dataset artifacts.
- Mechanism: Human-written and machine-generated texts have different length distributions that vary by topic and model.
- Core assumption: Length differences are consistent within domains but not across them.
- Evidence anchors:
  - [section] "we note that some interesting peculiarities in the texts from the Recipes topic...the first sentence of these texts tends to be very long compared to all the rest ones"
  - [section] "The RoBERTa classifier has the largest quality drop on Recipes when Recipes is used as an out-of-domain part"

### Mechanism 3
- Claim: Topological features (PHD) provide domain stability but lower absolute accuracy.
- Mechanism: Intrinsic dimensionality captures geometric properties of token embeddings that persist across domains.
- Core assumption: Embedding manifold geometry is more stable than surface-level features under domain shift.
- Evidence anchors:
  - [abstract] "topological features based on intrinsic dimensionality are highly stable under domain shifts but generally underperform in absolute accuracy"
  - [section] "topological time series classifiers...is able to handle cross-domain transfer to this subset"

## Foundational Learning

- Concept: Perplexity calculation and interpretation
  - Why needed here: Forms the basis of one primary detection approach
  - Quick check question: How does GPT-2 perplexity differ when scoring human vs. GPT-generated text?

- Concept: Intrinsic dimensionality estimation
  - Why needed here: Core of topological approaches for detecting generation patterns
  - Quick check question: What does lower PHD indicate about text generation quality?

- Concept: Cross-validation and domain generalization
  - Why needed here: Evaluating robustness across different text topics and generation models
  - Quick check question: Why might a model perform well in-domain but poorly out-of-domain?

## Architecture Onboarding

- Component map:
  Data pipeline: RoFT datasets → preprocessing → feature extraction
  Feature extractors: RoBERTa embeddings, GPT-2 perplexity, topological dimension
  Classifiers: Fine-tuned RoBERTa, logistic regression, gradient boosting, time series classifiers
  Evaluation: Accuracy, soft accuracy, MSE across domain/model splits

- Critical path:
  Feature extraction → Classification → Cross-domain validation → Error analysis

- Design tradeoffs:
  - Fine-tuning RoBERTa gives highest in-domain accuracy but poor generalization
  - Perplexity-based methods offer better cross-domain performance with lower computational cost
  - Topological features provide stability but lower absolute performance

- Failure signatures:
  - High accuracy on training domains but sharp drops on held-out domains
  - Confusion matrices showing systematic misclassification patterns
  - MSE scores diverging significantly from accuracy metrics

- First 3 experiments:
  1. Compare perplexity-based classifier performance on RoFT vs. RoFT-chatgpt
  2. Evaluate RoBERTa cross-domain performance on Recipes topic
  3. Test topological time series classifier on domain-shifted data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different normalization techniques for perplexity scores across diverse language models impact cross-model generalization in boundary detection?
- Basis in paper: [inferred] The paper notes that distributions of sentence perplexities vary significantly across generators and suggests that normalization or better choice of model is needed to mitigate this.
- Why unresolved: The authors mention this as a potential direction but do not explore normalization techniques or compare different models for computing perplexity.
- What evidence would resolve it: Experiments comparing various normalization methods (e.g., z-score, min-max scaling) and different base models for perplexity calculation across multiple generators and domains, measuring cross-model accuracy.

### Open Question 2
- Question: Can aggregating multiple feature types (e.g., topological, perplexity, length-based) improve overall accuracy and robustness compared to individual approaches?
- Basis in paper: [inferred] The authors note that each type of classifier handles its own set of spurious features well, and no classifier is universally better than the others, suggesting aggregation might help.
- Why unresolved: The paper analyzes individual feature types separately but does not experiment with combining them.
- What evidence would resolve it: Comparative experiments training classifiers on concatenated feature vectors versus individual features, measuring accuracy and cross-domain/cross-model performance.

### Open Question 3
- Question: How does the choice of window size and step size in sliding window techniques affect the performance of topological time series methods for boundary detection?
- Basis in paper: [explicit] The authors use specific window sizes (20 tokens) and step sizes (5) for their topological time series approach but do not explore the sensitivity to these hyperparameters.
- Why unresolved: The paper fixes these parameters without systematic exploration of their impact on accuracy or robustness.
- What evidence would resolve it: Ablation studies varying window and step sizes, measuring accuracy, cross-domain generalization, and computational efficiency across different settings.

## Limitations
- Limited to English text data from specific domains (short stories, recipes, news articles, political speeches)
- Does not extensively explore scenarios with multiple generation boundaries or nested AI-generated content
- Does not investigate impact of different mixing ratios or non-uniform boundary positions beyond single-boundary case

## Confidence

**High Confidence:** Perplexity-based methods achieve strong cross-domain performance; RoBERTa overfitting to spurious features like sentence length

**Medium Confidence:** Topological features provide domain stability while underperforming in absolute accuracy; superiority of perplexity-based approaches over fine-tuned RoBERTa in cross-domain settings

**Low Confidence:** Claim about perplexity-based methods being "recommended for practical use due to balance of accuracy, robustness, and computational efficiency" lacks direct empirical comparison of computational costs

## Next Checks

1. Cross-linguistic validation: Test perplexity-based detection approach on non-English text datasets to verify cross-domain generalization across languages

2. Multiple boundary detection: Extend experimental setup to include texts with multiple AI-generated segments to evaluate reliability of current methods

3. Computational efficiency benchmarking: Measure inference time and memory usage for perplexity-based versus fine-tuned RoBERTa approaches across varying document lengths