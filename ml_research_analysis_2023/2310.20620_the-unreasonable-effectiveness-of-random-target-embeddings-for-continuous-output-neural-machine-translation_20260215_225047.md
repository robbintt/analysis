---
ver: rpa2
title: The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output
  Neural Machine Translation
arxiv_id: '2310.20620'
source_url: https://arxiv.org/abs/2310.20620
tags:
- embeddings
- random
- discrete
- tokens
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate the use of randomly initialized target
  embeddings in continuous-output neural machine translation (CoNMT), where the model
  predicts continuous vector representations instead of discrete tokens. They find
  that random embeddings perform surprisingly well, even outperforming pre-trained
  embeddings on larger datasets, particularly for rare words.
---

# The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation

## Quick Facts
- arXiv ID: 2310.20620
- Source URL: https://arxiv.org/abs/2310.20620
- Reference count: 13
- Primary result: Random embeddings outperform pre-trained ones on larger datasets, especially for rare words

## Executive Summary
This paper investigates the use of randomly initialized target embeddings in continuous-output neural machine translation (CoNMT), where models predict continuous vector representations instead of discrete tokens. Surprisingly, random embeddings perform as well as or better than pre-trained embeddings, particularly on larger datasets and for rare words. The authors attribute this to the geometric properties of random embeddings and propose a simple combination approach that improves performance on low-resource language pairs. These findings challenge the conventional wisdom that semantic similarity in embedding space is crucial for CoNMT.

## Method Summary
The paper explores continuous-output NMT where models predict continuous vector representations instead of discrete tokens. Random embeddings are sampled from spherical uniform or hypercube distributions and compared against pre-trained embeddings extracted from discrete NMT models. The models are trained using fairseq with cosine distance objective. The authors also propose a hybrid approach that combines random and pre-trained embeddings based on token frequency, using random embeddings for rare words and pre-trained ones for frequent words.

## Key Results
- Random uniform embeddings outperform pre-trained embeddings on English→German dataset
- Random embeddings show superior performance for rare words due to better geometric properties
- Combined random and pre-trained embeddings improve performance on low-resource pairs (English→Turkish and Romanian→English)
- Beam search improves performance with random embeddings but degrades performance with pre-trained embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random embeddings outperform pre-trained ones on larger datasets due to better geometric properties for rare words
- Mechanism: The cosine distance objective doesn't rely on semantic similarity, so random embeddings spread tokens more evenly on the unit sphere. This improves representation disentanglement, particularly benefiting rare words which are often clustered together in pre-trained embeddings
- Core assumption: The geometry of embeddings matters more than semantic similarity for continuous-output NMT
- Evidence anchors:
  - [abstract] "We show that completely random output embeddings can outperform laboriously pretrained ones, especially on larger datasets. Further investigation shows this surprising effect is strongest for rare words, due to the geometry of their embeddings."
  - [section 5] "We find that random uniform embeddings outperform the MTtransfer baseline for en-de, match it closely for ro-en, and only underperform in the low-resource case for en-tr"
- Break condition: When dataset size is too small to benefit from random spread, or when semantic similarity becomes critical for the translation task

### Mechanism 2
- Claim: Combined embeddings improve performance by leveraging strengths of both random and pre-trained embeddings
- Mechanism: Rare words benefit from random embeddings' geometric properties, while frequent words retain semantic structure from pre-trained embeddings. This hybrid approach maintains high accuracy on frequent tokens while improving rare token performance
- Core assumption: Different frequency ranges have different optimal embedding strategies
- Evidence anchors:
  - [abstract] "We propose a simple combination of random and pre-trained embeddings and show that it helps improving models performance on both en-tr and ro-en"
  - [section 5] "This simple approach leads to overall improved performance both on en-tr and ro-en. Furthermore, as shown in Figure 3, we confirm that combined target embeddings preserve the performance of pre-trained embeddings on frequent tokens and increase F1 score on rare tokens"
- Break condition: When threshold for switching between random and pre-trained embeddings is poorly chosen, or when semantic similarity is crucial for the task

### Mechanism 3
- Claim: Beam search improves performance with random embeddings due to better exploration of the embedding space
- Mechanism: Random embeddings create a more uniform embedding space where beam search can find better neighbors. In contrast, pre-trained embeddings may have semantically similar but contextually incorrect neighbors that trap greedy search
- Core assumption: The embedding space geometry affects search algorithm effectiveness
- Evidence anchors:
  - [section 5] "We note from Table 1 that, except for en-tr, the MTtransfer model performs worse when using beam size > 1. But on random uniform embeddings we observe the opposite: beam search is at least as good as greedy search, usually improving"
  - [section 5] "Further investigation in Figure 1 shows that, despite the meaningful distances in embedding space, the MTtransfer model degrades consistently, performing best in the greedy case, while the random embedding model benefits noticeably from a larger beam"
- Break condition: When the embedding space becomes too dense or when beam search computational cost outweighs benefits

## Foundational Learning

- Concept: Continuous-output NMT replaces discrete next-word prediction with embedding prediction
  - Why needed here: Understanding this fundamental shift is crucial for grasping why random embeddings can work - the model isn't predicting specific words but continuous vectors
  - Quick check question: What is the key difference between discrete NMT and continuous-output NMT in terms of the prediction target?

- Concept: Cosine similarity as distance metric
  - Why needed here: The paper relies heavily on cosine distance, which is invariant to vector norms and affects how random embeddings behave
  - Quick check question: Why does the paper use cosine similarity instead of Euclidean distance for continuous-output NMT?

- Concept: Embedding space geometry
  - Why needed here: The paper's main argument about random embeddings relies on understanding how tokens are distributed in the embedding space
  - Quick check question: How does the distribution of rare words in pre-trained embeddings differ from their distribution in random embeddings?

## Architecture Onboarding

- Component map: Source sentence -> Subword tokenization -> Transformer encoder -> Hidden states -> Cosine distance to all target embeddings -> Loss function -> Transformer decoder with frozen embeddings -> Continuous vectors -> Nearest neighbor mapping -> Output tokens

- Critical path:
  1. Source sentence → Subword tokenization → Transformer encoder → Hidden states
  2. Hidden states → Cosine distance to all target embeddings → Loss
  3. During inference: Hidden states → Cosine distance to all target embeddings → Nearest neighbor → Output tokens

- Design tradeoffs:
  - Fixed vs trainable embeddings: Fixed embeddings prevent trivial solutions but limit adaptation
  - Embedding dimensionality: Lower dimensions (128) work better than typical NLP dimensions (512+)
  - Random distribution: Spherical uniform vs hypercube - affects computational efficiency and performance

- Failure signatures:
  - Performance degradation on low-resource languages
  - Beam search worsening performance (indicates poor embedding space geometry)
  - Rare word performance lagging behind frequent word performance

- First 3 experiments:
  1. Compare random uniform vs hypercube embeddings on a small dataset to verify performance difference
  2. Test combined embeddings with different frequency thresholds on a medium-resource language pair
  3. Implement beam search with different beam sizes on random vs pre-trained embeddings to confirm search behavior differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of random target embeddings generalize to other text generation tasks beyond machine translation, such as summarization or language modeling?
- Basis in paper: [inferred] The paper mentions that the findings may not necessarily hold for other text generation tasks and suggests conducting additional experiments on tasks like summarization and language modeling.
- Why unresolved: The paper only tests the effectiveness of random embeddings on three language pairs for machine translation. It does not explore other text generation tasks.
- What evidence would resolve it: Experiments on other text generation tasks like summarization or language modeling would determine if random embeddings are effective in those contexts as well.

### Open Question 2
- Question: Is there an optimal dimensionality for random embeddings in continuous-output models, and how does it vary with dataset size?
- Basis in paper: [explicit] The paper discusses experiments on the ro-en dataset with different embedding dimensionalities and suggests that lower dimensions might work better for CoNMT due to the cosine distance measure. It also mentions that the effectiveness of random embeddings might depend on the dataset size.
- Why unresolved: The paper only explores a few dimensionalities on one dataset and does not systematically investigate the relationship between embedding dimensionality, dataset size, and performance.
- What evidence would resolve it: A comprehensive study varying embedding dimensionality and dataset size across multiple tasks would reveal the optimal settings.

### Open Question 3
- Question: How does the choice of distance metric (other than cosine) affect the performance of continuous-output models with random embeddings?
- Basis in paper: [explicit] The paper uses cosine distance as the loss function and mentions that other distance metrics could potentially improve results with larger embedding dimensionalities.
- Why unresolved: The paper only experiments with cosine distance and does not explore other distance metrics.
- What evidence would resolve it: Experiments comparing different distance metrics (e.g., Euclidean, Manhattan) with random embeddings would show their impact on performance.

### Open Question 4
- Question: What is the optimal way to combine random and pre-trained embeddings to maximize performance across different frequency ranges of tokens?
- Basis in paper: [explicit] The paper proposes a simple combination of random and pre-trained embeddings based on token frequency rank and shows improved performance on en-tr and ro-en. However, it notes that the threshold choice might not be optimal for all language pairs.
- Why unresolved: The paper uses a fixed threshold for all language pairs and does not explore more sophisticated methods of combining embeddings.
- What evidence would resolve it: Developing and testing more refined strategies for combining embeddings, possibly with language-specific thresholds or adaptive methods, would determine the optimal approach.

## Limitations

- The paper demonstrates correlation between random embeddings and improved performance but doesn't definitively prove causation through geometric analysis
- The threshold for combining random and pre-trained embeddings (rank 20k) is chosen empirically rather than derived from theoretical considerations
- Computational efficiency claims regarding hypercube vs spherical uniform embeddings lack detailed timing measurements

## Confidence

- High Confidence: Random embeddings perform well on larger datasets, especially for rare words; beam search benefits random embeddings while harming pre-trained ones
- Medium Confidence: Geometric properties of random embeddings benefit rare words; cosine similarity objective is key to random embeddings' success
- Low Confidence: The specific threshold (rank 20k) for combining embeddings is optimal; hypercube embeddings are significantly more computationally efficient

## Next Checks

1. Perform detailed geometric analysis of rare word distributions in both random and pre-trained embeddings using t-SNE or UMAP visualization, and calculate cluster metrics (DBSCAN, silhouette scores) to empirically verify that rare words are more dispersed in random embeddings and more clustered in pre-trained ones.

2. Systematically vary the threshold for combining random and pre-trained embeddings (e.g., 10k, 20k, 30k token ranks) across all three language pairs to identify optimal thresholds and understand the sensitivity of performance to this hyperparameter choice.

3. Conduct controlled experiments measuring wall-clock training and inference times for models using spherical uniform versus hypercube random embeddings, keeping all other factors constant, to quantify the actual computational advantages claimed in the paper.