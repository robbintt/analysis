---
ver: rpa2
title: 'Language as the Medium: Multimodal Video Classification through text only'
arxiv_id: '2309.10783'
source_url: https://arxiv.org/abs/2309.10783
tags:
- video
- language
- multimodal
- textual
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for multimodal video classification
  using text-only representations of visual and auditory cues. The method chains together
  perception models that generate captions and tags for video frames and audio with
  large language models that reason over these textual inputs to classify the video
  action.
---

# Language as the Medium: Multimodal Video Classification through text only

## Quick Facts
- arXiv ID: 2309.10783
- Source URL: https://arxiv.org/abs/2309.10783
- Reference count: 20
- Key outcome: Proposes a text-only approach to multimodal video classification that chains perception models with LLMs, achieving competitive accuracy on UCF-101 and Kinetics400

## Executive Summary
This paper introduces a novel method for multimodal video classification that leverages large language models (LLMs) to reason over textual representations of visual and auditory cues. By chaining together perception models that generate captions and tags for video frames and audio with LLMs, the method achieves competitive accuracy on action recognition benchmarks compared to existing multimodal approaches. The key insight is that LLMs can use textual descriptors as proxies for "sight" and "hearing" to classify video actions without access to raw modalities.

## Method Summary
The proposed method consists of two phases: perception and reasoning. In the perception phase, visual captions are extracted from video frames using BLIP-2, audio transcripts are generated using Whisper, and audio tags are created using ImageBind. These textual descriptors are then combined and fed into an LLM (GPT-3.5, Claude-instant-1, or Llama2) in the reasoning phase. The LLM uses its world knowledge to interpret the textual cues and match them to the most likely action category from a predefined list, producing JSON-formatted classification results.

## Key Results
- Achieves competitive accuracy on UCF-101 and Kinetics400 action recognition benchmarks compared to existing multimodal video classification approaches
- Demonstrates that LLMs can effectively use textual representations of visual and auditory cues as proxies for direct sensory input
- Shows the potential of chaining perception models with LLMs for more holistic video understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The text-only approach can represent multimodal video information by chaining perception models with a reasoning LLM.
- Mechanism: Visual, audio, and speech cues are converted into text via BLIP-2, Whisper, and ImageBind. These textual descriptors are then used as proxies for "sight" and "hearing" by the LLM, which reasons over them to classify video actions.
- Core assumption: Textual representations of visual and auditory cues retain sufficient context and specificity to enable accurate action classification without access to the raw modalities.
- Evidence anchors:
  - [abstract] "Our method leverages the extensive knowledge learnt by large language models, such as GPT-3.5 or Llama2, to reason about textual descriptions of the visual and aural modalities, obtained from BLIP-2, Whisper and ImageBind."
  - [section] "We use a temperature of 0 or near 0 for all reasoning models to ensure more consistent outputs that are able to better adhere to the instructions given."
- Break condition: If the perception models generate ambiguous or overly generic captions/tags, the LLM will struggle to differentiate between similar actions, leading to poor classification accuracy.

### Mechanism 2
- Claim: The LLM can use textual cues as substitutes for direct sensory input in video classification tasks.
- Mechanism: The LLM is provided with textual descriptions of video frames, speech transcripts, and audio tags, and uses its internal knowledge to match these cues to the most likely action category from a predefined list.
- Core assumption: The LLM has sufficient world knowledge to interpret the textual cues and fill in contextual gaps, enabling accurate classification even without raw visual/audio data.
- Evidence anchors:
  - [abstract] "demonstrate that available LLMs have the ability to use these multimodal textual descriptions as proxies for 'sight' or 'hearing' and perform zero-shot multimodal classification of videos in-context."
- Break condition: If the LLM's training data lacks sufficient coverage of the actions in the video dataset, or if the textual cues are too sparse, the LLM will fail to correctly classify the video.

### Mechanism 3
- Claim: Chaining perception models with LLMs simplifies multimodal video classification into two phases: perception and reasoning.
- Mechanism: The perception phase uses unimodal or multimodal models as surrogates for various senses, generating textual descriptions. The reasoning phase uses a foundation model (LLM) to consolidate these textual inputs into a coherent narrative and identify the most likely video content.
- Core assumption: The division of labor between perception and reasoning models allows for efficient and interpretable multimodal classification, as each model specializes in a specific task.
- Evidence anchors:
  - [abstract] "This method points towards a promising new research direction in multimodal classification, demonstrating how an interplay between textual, visual and auditory machine learning models can enable more holistic video understanding."
- Break condition: If the perception models generate inconsistent or conflicting textual descriptions, the LLM will struggle to form a coherent narrative, leading to poor classification accuracy.

## Foundational Learning

- Concept: Multimodal machine learning
  - Why needed here: Understanding how different modalities (vision, speech, audio) can be combined and processed to enable holistic video understanding.
  - Quick check question: How do current multimodal machine learning approaches handle the complex contextual relationships between different modalities in videos?

- Concept: Large language models and their capabilities
  - Why needed here: Recognizing the potential of LLMs to reason over textual descriptions of multimodal data and perform tasks like video classification.
  - Quick check question: What are some key capabilities of LLMs that make them suitable for interpreting textual representations of visual and auditory cues?

- Concept: Zero-shot learning
  - Why needed here: Understanding how the proposed method can classify video actions without requiring additional finetuning of video-text models or datasets.
  - Quick check question: How does the proposed method leverage the knowledge learned by LLMs to perform zero-shot multimodal classification of videos?

## Architecture Onboarding

- Component map:
  - Perception models: BLIP-2 (visual captioning), Whisper (speech recognition), ImageBind (audio tagging)
  - Reasoning model: LLM (GPT-3.5, Claude-instant-1, or Llama2)
  - Output: JSON-formatted classification results

- Critical path:
  1. Extract visual captions from video frames using BLIP-2
  2. Generate audio transcripts using Whisper
  3. Create audio tags using ImageBind
  4. Combine textual descriptors into a single input
  5. Feed input to LLM and obtain classification results

- Design tradeoffs:
  - Using only 5 equidistant frames per video to ensure diverse sampling, but potentially missing important details
  - Relying on the LLM's world knowledge to interpret textual cues, but may struggle with rare or domain-specific actions
  - Simplifying the pipeline by using text as the primary medium, but potentially losing fine-grained visual and auditory information

- Failure signatures:
  - Low classification accuracy due to ambiguous or overly generic captions/tags
  - Inconsistent or conflicting outputs from the LLM due to conflicting textual descriptions
  - Failure to correctly classify videos with rare or domain-specific actions

- First 3 experiments:
  1. Evaluate the effect of using different numbers of frames and their impact on classification accuracy
  2. Compare the performance of different LLMs (GPT-3.5, Claude-instant-1, Llama2) on the same video dataset
  3. Assess the contribution of each modality (visual, speech, audio) to the overall classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can this method handle temporal dependencies and long-term relationships in videos?
- Basis in paper: [inferred] The paper notes that analyzing frames individually lacks proper temporal modeling of persistent identities and relationships over time.
- Why unresolved: The paper does not explore methods to capture temporal dynamics, such as using video models or attention mechanisms over time.
- What evidence would resolve it: Experiments comparing performance on datasets with long-term temporal dependencies (e.g., longer videos, complex action sequences) using this method versus methods that explicitly model temporality.

### Open Question 2
- Question: How robust is this method to hallucinations and errors from the perception models?
- Basis in paper: [explicit] The paper discusses that generative models are prone to hallucinations and unreliable outputs.
- Why unresolved: The paper does not quantify the impact of perception model errors on final classification accuracy. It is unclear how often hallucinations occur and how they affect performance.
- What evidence would resolve it: A study measuring the frequency of perception model errors, their impact on LLM reasoning, and methods to detect/mitigate hallucinations.

### Open Question 3
- Question: Can this method generalize to complex video understanding tasks beyond action classification?
- Basis in paper: [explicit] The paper notes this method is "more generalizable to video understanding scenarios that require complex contextual reasoning."
- Why unresolved: The paper only evaluates on action classification benchmarks. It does not test on tasks requiring deeper reasoning, like video question answering or visual reasoning.
- What evidence would resolve it: Experiments on video QA or reasoning datasets, comparing performance to specialized models for those tasks.

## Limitations
- Unproven generalizability to other video classification tasks beyond action recognition
- Sensitivity to the quality of captions and tags generated by perception models
- Lack of direct sensory input may result in information loss and reduced accuracy

## Confidence
- Generalizability to other tasks: Medium
- Sensitivity to perception model quality: Medium
- Information loss from using text as proxy: Low
- Failure modes and limitations: Low

## Next Checks
1. Evaluate the method's performance on additional video classification tasks beyond action recognition, such as video retrieval or multimodal sentiment analysis.
2. Assess the impact of using different perception models (e.g., alternative visual captioning or audio tagging models) on the overall classification accuracy.
3. Conduct ablation studies to determine the relative contribution of each modality (visual, speech, audio) to the final classification results.