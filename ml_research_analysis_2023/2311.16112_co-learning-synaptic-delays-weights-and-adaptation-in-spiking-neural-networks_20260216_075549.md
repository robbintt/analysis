---
ver: rpa2
title: Co-learning synaptic delays, weights and adaptation in spiking neural networks
arxiv_id: '2311.16112'
source_url: https://arxiv.org/abs/2311.16112
tags:
- spiking
- neuron
- delays
- adaptation
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores biologically inspired enhancements for spiking
  neural networks (SNN) by co-learning weights, synaptic delays, and neuronal adaptation
  parameters. The authors propose an adaptive neuron model (AdLIF+) and demonstrate
  that incorporating trainable delays and adaptation parameters significantly improves
  SNN performance on speech recognition tasks.
---

# Co-learning synaptic delays, weights and adaptation in spiking neural networks

## Quick Facts
- arXiv ID: 2311.16112
- Source URL: https://arxiv.org/abs/2311.16112
- Authors: 
- Reference count: 20
- This paper achieves state-of-the-art speech recognition accuracy on SHD, SSC, and GSC datasets using spiking neural networks with co-learned weights, synaptic delays, and adaptation parameters.

## Executive Summary
This paper introduces an adaptive leaky integrate-and-fire (AdLIF+) neuron model that co-learns synaptic weights, transmission delays, and neuronal adaptation parameters in spiking neural networks. The key innovation is training all three parameter types simultaneously using backpropagation-through-time with surrogate gradients. Applied to speech recognition tasks, the method significantly outperforms conventional SNNs and matches or exceeds the performance of equivalent artificial neural networks. The approach demonstrates that incorporating biologically inspired features like delays and adaptation can substantially enhance SNN capabilities for temporally rich data.

## Method Summary
The authors propose an AdLIF+ spiking neuron model where each neuron learns three types of parameters: synaptic weights, transmission delays (capped at 25 timesteps), and adaptation parameters (α, β, a, b) that modulate firing behavior. Training uses backpropagation-through-time with surrogate gradients to handle spike non-differentiability. The model is trained on spike-encoded speech data from three datasets: SHD, SSC, and GSC. A two-hidden layer feedforward architecture with 128-512 neurons per layer is optimized using Adam with different learning rates for weights (0.01/0.001) and delays (10× higher). The output neurons have infinite thresholds and membrane potentials are summed over time for classification.

## Key Results
- Achieves 96.26% accuracy on SHD dataset, 79.81% on SSC, and 95.38% on GSC
- Outperforms conventional SNNs without delays or adaptation on all three datasets
- Matches or exceeds ANN performance of similar size on speech recognition tasks
- Co-learning delays and adaptation parameters together provides greater benefit than optimizing each separately

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Co-learning synaptic delays enables explicit temporal correlation of distant features
- Mechanism: Delays shift spike arrival times so neurons can detect temporally separated patterns that would otherwise be out of phase. This acts as a short-term memory that correlates input features over time.
- Core assumption: The task benefits from cross-time correlations; delays are trainable and differentiable.
- Evidence anchors:
  - [abstract]: "enables to learn to explicitly correlate patterns that are temporally distanced"
  - [section]: "A delay-enabled spiking network was also found to be able to compute a richer class of functions than a threshold circuit with adjustable weights"
  - [corpus]: Weak; no neighbor papers discuss delay learning in detail.
- Break condition: If the task has little temporal structure, delays provide no advantage and may hurt performance.

### Mechanism 2
- Claim: Neuronal adaptation parameters increase spike pattern diversity
- Mechanism: Adaptation currents modulate firing thresholds based on recent activity, producing variable spike timing and frequency. This heterogeneity expands the effective feature space.
- Core assumption: Diverse spike patterns improve representational richness and classification accuracy.
- Evidence anchors:
  - [abstract]: "trained adaptation parameters result in neuronal heterogeneity... leads to a greater variety in available spike patterns"
  - [section]: "the trained adaptation allows a greater variety in spike patterns, widening the feature space to be explored"
  - [corpus]: No direct neighbor evidence; weak support.
- Break condition: If adaptation parameters saturate or destabilize neurons, spike diversity drops and performance degrades.

### Mechanism 3
- Claim: Co-optimization of weights, delays, and adaptation parameters mutually improves convergence
- Mechanism: Joint gradients allow weights to exploit learned temporal shifts from delays and firing dynamics from adaptation, leading to more effective feature extraction than optimizing each separately.
- Core assumption: Parameters are interdependent and simultaneous updates yield better local minima.
- Evidence anchors:
  - [abstract]: "co-learning these parameters proved to mutually benefit the optimization of all learned parameters"
  - [section]: "co-learning these parameters proved to mutually benefit the optimization of all learned parameters"
  - [corpus]: Weak; no neighbor studies on co-learning all three.
- Break condition: If gradient interference occurs, co-learning may stall and separate training could be better.

## Foundational Learning

- Concept: Leaky integrate-and-fire (LIF) dynamics
  - Why needed here: Forms the base neuron model; understanding membrane potential accumulation and spike generation is essential before adding delays/adaptation.
  - Quick check question: In a LIF neuron, what triggers a spike and what happens to the membrane potential immediately after?

- Concept: Backpropagation-through-time (BPTT) in SNNs
  - Why needed here: Core training algorithm for the proposed co-learning framework; handles temporal dependencies and surrogate gradients for spike non-differentiability.
  - Quick check question: How does BPTT unroll a spiking network over time, and what surrogate gradient function is used here?

- Concept: Synaptic delay representation in differentiable SNNs
  - Why needed here: Delays are trainable parameters; must be differentiable for gradient flow.
  - Quick check question: What kernel operation is used to implement delay convolution in the spike train?

## Architecture Onboarding

- Component map:
  - Input layer: Spiking representations of speech (140 channels × 100 timesteps)
  - Hidden layers: 2-layer fully connected AdLIF+ neurons with trainable weights, delays, and adaptation parameters
  - Readout layer: Memoryless output neurons with infinite threshold; membrane potentials summed over time
  - Training loop: BPTT with surrogate gradients, Adam optimizer, learning rate scheduler

- Critical path:
  1. Preprocess speech into spike trains
  2. Forward pass through AdLIF+ neurons with delays
  3. Compute cross-entropy loss on summed membrane potentials
  4. Backward pass using surrogate gradients for spikes and delays
  5. Update weights, delays, and adaptation parameters
  6. Apply clipping and schedule learning rate

- Design tradeoffs:
  - Delay range [0,25] limits temporal memory but reduces parameter count
  - Positive-only adaptation parameters avoid chaotic regimes but reduce flexibility
  - 2 hidden layers keep model small for faster experiments

- Failure signatures:
  - Vanishing gradients → neurons stop spiking; check surrogate gradient scale
  - Delayed spikes too long → information loss; reduce delay cap
  - Adaptation saturation → neurons become either too quiet or too active; tune parameter bounds

- First 3 experiments:
  1. Compare LIF vs AdLIF+ baseline on SHD without delays
  2. Add delays to AdLIF+ and evaluate accuracy gain
  3. Run full co-learning on SSC and GSC to confirm state-of-the-art results

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The paper lacks rigorous ablation studies to isolate the contribution of delay learning versus adaptation parameters to the performance gains
- Claims about biological plausibility and computational power increases from delays lack strong empirical support in the speech domain
- The optimal range of neuronal adaptation parameters for different types of input data beyond speech recognition tasks remains unexplored

## Confidence
- **High confidence**: The mathematical formulation of AdLIF+ and the training procedure are sound and well-specified
- **Medium confidence**: The accuracy improvements are real and reproducible, but the relative contribution of each learned parameter type remains unclear
- **Low confidence**: Claims about biological plausibility and computational power increases from delays lack strong empirical support in the speech domain

## Next Checks
1. **Ablation study**: Train AdLIF+ models with only delays learned, only adaptation parameters learned, and both fixed to isolate each mechanism's contribution
2. **Delay capacity test**: Systematically vary the delay range cap to determine whether the 0-25 timestep range is optimal or merely sufficient
3. **Comparison baseline**: Implement a temporal convolution layer as an alternative to learned delays and compare performance to determine if delays provide unique advantages over standard temporal processing methods