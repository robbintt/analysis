---
ver: rpa2
title: 'External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance
  with Human Feedback'
arxiv_id: '2307.12057'
source_url: https://arxiv.org/abs/2307.12057
tags:
- page
- response
- language
- alignment
- lima
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a tiered system called "External Reasoning"
  for multi-large-language-model assistance with human feedback. The method uses a
  three-level policy framework to provide escalating support (entry, intermediate,
  extreme) for handling queries of varying complexity, incorporating external knowledge
  retrieval, summarization, and LLM interchange.
---

# External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback

## Quick Facts
- arXiv ID: 2307.12057
- Source URL: https://arxiv.org/abs/2307.12057
- Reference count: 40
- Primary result: Tiered LLM assistance system achieves state-of-the-art performance vs ChatPDF, more efficient than direct full-text processing

## Executive Summary
This paper introduces "External Reasoning," a three-tier policy framework for multi-LLM assistance that escalates support from entry to intermediate to extreme levels based on query complexity and human feedback. The system combines external knowledge retrieval, summarization, and LLM interchange to handle complex research paper queries more efficiently than direct full-text processing. Evaluations demonstrate superior performance compared to existing solutions like ChatPDF, with particular strength in handling queries requiring synthesis across multiple document sections.

## Method Summary
The system implements a tiered policy where queries are handled through escalating levels of support: entry (local summarization), intermediate (GPT-3 summarization), and extreme (GPT-3-16k refinement). It uses cosine similarity and KNN-based retrieval with adjustable chunk sizes and k-values, memory-enhanced techniques for identifying key references through cached intermediate outputs, and structured prompt engineering templates. The approach is designed to be more efficient than direct full-text LLM processing while maintaining or improving accuracy through selective escalation based on user feedback.

## Key Results
- Achieves state-of-the-art performance compared to existing solutions like ChatPDF
- More efficient than direct full-text LLM processing while maintaining accuracy
- Successfully handles complex queries through tiered escalation system

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level LLM assistance with escalating complexity improves query handling performance.
- Mechanism: Tiered policy (entry → intermediate → extreme) with selective escalation based on user feedback and query complexity assessment.
- Core assumption: Not all queries require full LLM processing; selective escalation leads to better resource utilization and accuracy.
- Evidence anchors: [abstract], [section]; corpus evidence is missing.
- Break condition: If feedback loop fails to detect user dissatisfaction or misclassifies query complexity, escalation may occur too late or unnecessarily.

### Mechanism 2
- Claim: Memory-enhanced retrieval improves key reference identification by caching intermediate outputs and combining them with raw abstracts.
- Mechanism: System retrieves cached summaries from previous interactions, concatenates with raw abstract, and feeds combined text to LLM for reference classification.
- Core assumption: Repeated user interactions provide useful context that enables better identification of seminal references.
- Evidence anchors: [abstract], [section]; corpus evidence is missing.
- Break condition: If cache coherence fails or summaries are outdated/incorrect, LLM may misclassify references.

### Mechanism 3
- Claim: Hybrid retrieval (cosine similarity + KNN) with adjustable parameters optimizes context selection.
- Mechanism: Combines cosine similarity for quick filtering with KNN for nuanced semantic matching, with parameters tuned per query complexity.
- Core assumption: Different query types benefit from different retrieval granularities; combining methods yields better coverage.
- Evidence anchors: [section]; corpus evidence is missing.
- Break condition: If parameter tuning is static or inappropriate for query type, retrieval quality degrades.

## Foundational Learning

- Concept: Embedding models and their trade-offs (Ada vs Davinci).
  - Why needed here: Selecting the right embedding model impacts retrieval quality and computational cost.
  - Quick check question: Why does text-embedding-davinci outperform text-embedding-ada on Type 2 questions?

- Concept: Cosine similarity vs KNN matching.
  - Why needed here: Understanding when to use each retrieval method is key to optimizing context selection.
  - Quick check question: In what scenario would KNN with k=5 outperform cosine similarity?

- Concept: Prompt engineering structure.
  - Why needed here: Well-structured prompts ensure consistent and accurate LLM responses.
  - Quick check question: What role does the [Instructions] section play in guiding LLM output?

## Architecture Onboarding

- Component map: PDF parsing (scipdf parser) → Chunk segmentation → Embedding (Ada/Davinci) → Retrieval (cosine/KNN) → Summarization (BART/LLM) → LLM reasoning (GPT-3/3.5/4) → Caching (memory) → User interface
- Critical path: User query → Retrieval → Summarization (if needed) → LLM response → Feedback loop (for escalation)
- Design tradeoffs: Token efficiency vs accuracy (entry tier saves tokens but may miss nuance; extreme tier is costly but thorough); Speed vs depth (faster retrieval with cosine may miss context that KNN would capture)
- Failure signatures: Escalating too often → high cost, user fatigue; Escalating too late → poor user experience, low accuracy; Cache misses → redundant processing, slower response
- First 3 experiments: 1) Test retrieval accuracy: Compare cosine vs KNN with fixed k on sample query set; 2) Evaluate summarization impact: Measure response quality with vs without summarization on Type 2 queries; 3) Measure escalation accuracy: Simulate user dissatisfaction to test if system escalates correctly

## Open Questions the Paper Calls Out

- Question: What are the limitations of using human feedback as the sole mechanism for escalating assistance levels in the policy system?
  - Basis in paper: [explicit] The paper states "one limitation pertains to the upgrading mechanism, which is currently reliant on human feedback" and discusses potential solutions including sentiment analysis.
  - Why unresolved: The paper acknowledges this limitation but does not empirically test alternative mechanisms like sentiment analysis or automatic escalation systems.
  - What evidence would resolve it: Comparative studies showing performance differences between human-feedback-only escalation versus hybrid systems incorporating automated sentiment analysis.

- Question: How does the system perform on domain-specific scientific literature outside of computer science and NLP?
  - Basis in paper: [inferred] The paper focuses on general research paper analysis but doesn't report results on specialized scientific domains.
  - Why unresolved: The evaluation primarily uses general research papers without domain-specific validation.
  - What evidence would resolve it: Systematic evaluation across multiple scientific domains with domain-specific benchmarks.

- Question: What is the optimal balance between context window size and computational efficiency for complex queries requiring synthesis across multiple documents?
  - Basis in paper: [explicit] The paper identifies context window limitations as a constraint and mentions ALiBi as a potential solution but doesn't explore the tradeoff space.
  - Why unresolved: While the paper proposes ALiBi as a theoretical solution, it doesn't empirically investigate how different context window sizes affect performance.
  - What evidence would resolve it: Controlled experiments varying context window sizes across different query complexity levels.

## Limitations

- Human feedback loop effectiveness depends on user interaction patterns that may vary across domains and expertise levels
- Memory-enhanced reference identification mechanism lacks explicit algorithmic description
- Optimal parameter configurations for retrieval methods not empirically validated

## Confidence

- **High Confidence**: Tiered assistance framework (entry → intermediate → extreme) is well-described and logically sound
- **Medium Confidence**: Claims about efficiency gains over direct full-text processing depend on unvalidated parameter choices
- **Low Confidence**: Memory-enhanced retrieval for key references lacks sufficient technical detail for independent verification

## Next Checks

1. **Retrieval Parameter Sensitivity**: Systematically evaluate how retrieval accuracy varies with different chunk sizes and k-values for KNN across diverse query types
2. **Escalation Accuracy**: Conduct user studies to measure whether feedback loop correctly identifies when to escalate between tiers, and quantify false positive/negative rates
3. **Memory Coherence Testing**: Test reference identification accuracy when cached summaries are noisy, outdated, or missing to assess robustness to cache failures