---
ver: rpa2
title: 'LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids
  From Their Text Descriptions'
arxiv_id: '2310.14029'
source_url: https://arxiv.org/abs/2310.14029
tags:
- llm-prop
- crystal
- prediction
- text
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of predicting physical and electronic
  properties of crystalline solids. The core method idea is to leverage the general-purpose
  learning capabilities of large language models (LLMs) to predict these properties
  from text descriptions of crystal structures.
---

# LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions

## Quick Facts
- arXiv ID: 2310.14029
- Source URL: https://arxiv.org/abs/2310.14029
- Authors: 
- Reference count: 11
- Primary result: LLM-Prop outperforms state-of-the-art GNN-based predictors by ~4% on band gap MAE, ~3% on direct/indirect classification AUC, and 66% on volume MAE

## Executive Summary
This paper introduces LLM-Prop, a novel approach for predicting physical and electronic properties of crystalline solids using large language models (LLMs) trained on text descriptions of crystal structures. The method leverages the rich semantic expressiveness of natural language descriptions to encode structural information, achieving superior performance compared to graph neural network (GNN) baselines on three key property prediction tasks: band gap, unit cell volume, and direct/indirect band gap classification. LLM-Prop uses a pretrained T5 encoder with specialized preprocessing of crystal text descriptions, demonstrating that text-based representations can capture critical symmetry and structural information more effectively than traditional graph representations.

## Method Summary
LLM-Prop uses a pretrained T5 encoder finetuned on text descriptions of crystal structures to predict physical and electronic properties. The method applies specialized preprocessing including stopwords removal, replacement of bond distances and angles with [NUM] and [ANG] tokens, and addition of a [CLS] token. The model is trained on the TextEdge dataset containing 144,931 crystals from the Materials Project database, with properties including band gap, unit cell volume, and direct/indirect band gap classification. Performance is evaluated using MAE for regression tasks and AUC for classification, with results showing consistent improvements over GNN baselines.

## Key Results
- Achieves ~4% improvement in band gap prediction MAE compared to ALIGNN
- Outperforms all baselines on unit cell volume prediction with 66% improvement over MEGNet
- Achieves 3% better AUC for direct/indirect band gap classification while using 3× fewer parameters than MatBERT

## Why This Works (Mechanism)

### Mechanism 1
LLM-Prop leverages rich semantic expressiveness of text descriptions to encode crystal symmetry and Wyckoff site information more effectively than graph representations. Crystal text descriptions inherently contain space group symmetry and Wyckoff site information in natural language, which LLMs can directly parse and encode. Graph neural networks struggle to represent these symmetries explicitly and must approximate them through edge features and convolutions. Core assumption: Text descriptions provide sufficient and explicit structural information that GNNs typically fail to capture, especially for periodic systems and complex symmetries. Evidence anchors: [abstract] "Our empirical results may highlight the current inability of GNNs to capture information pertaining to space group symmetry and Wyckoff sites for accurate crystal property prediction." [section] "The complexity of incorporating critical atomic and molecular information such as bond angles and crystal symmetry information such as space groups." Break condition: If text descriptions become incomplete, ambiguous, or omit critical symmetry information, the advantage over GNNs diminishes.

### Mechanism 2
LLM-Prop achieves superior performance by reducing model complexity while maintaining or improving predictive power. By using only the T5 encoder and discarding the decoder, LLM-Prop reduces parameters by half compared to full encoder-decoder models, enabling training on longer sequences and capturing more contextual information without overfitting. Core assumption: The encoder alone contains sufficient representational capacity for property prediction tasks, and the decoder adds unnecessary complexity without improving performance. Evidence anchors: [section] "LLM-Prop also outperforms a finetuned MatBERT, a domain-specific pre-trained BERT model, despite having 3 times fewer parameters." [section] "This has many desiderata: it allows us to cut the network size in half and importantly enables us to finetune on longer sequences and therefore account for longer-term dependencies in the crystal descriptions." Break condition: If property prediction tasks require generation capabilities or bidirectional context that the decoder provides, encoder-only models may underperform.

### Mechanism 3
Specialized preprocessing strategies significantly enhance LLM-Prop's ability to extract relevant features from crystal text descriptions. Replacing bond distances and angles with [NUM] and [ANG] tokens, removing stopwords, and adding a [CLS] token for pooling creates a more compact and semantically meaningful input representation that improves model performance. Core assumption: Numerical values in bond distances and angles add complexity without providing proportional predictive value, and their compression into special tokens preserves essential information while reducing noise. Evidence anchors: [section] "We replace all bond distances and bond angles in the crystal text descriptions along with their units with [NUM] and [ANG] tokens, respectively." [section] "These considerations motivate our choice to use T5 as our main pretrained model." Break condition: If bond distances and angles are crucial predictive features for certain properties, replacing them with generic tokens may remove valuable information.

## Foundational Learning

- Concept: Text embeddings and language model pretraining
  - Why needed here: LLM-Prop relies on pretrained language models to convert crystal text descriptions into meaningful numerical representations that capture semantic relationships and structural information.
  - Quick check question: What is the difference between encoder-only and encoder-decoder transformer architectures in terms of their suitability for regression tasks?

- Concept: Graph neural networks and their limitations
  - Why needed here: Understanding why GNNs struggle with crystal property prediction provides context for why text-based approaches might be superior, particularly regarding symmetry representation and periodicity.
  - Quick check question: How do GNNs typically handle periodic boundary conditions in crystal structures, and what are the computational challenges?

- Concept: Materials informatics and crystal structure representation
  - Why needed here: Familiarity with how crystals are described in materials science, including space groups, Wyckoff sites, and structural features, is essential for understanding the data and interpreting results.
  - Quick check question: What information is typically included in a crystal text description generated by Robocrystallographer?

## Architecture Onboarding

- Component map: Crystal text descriptions (processed) -> T5 Encoder -> Linear Layer -> Label Normalization -> Property predictions
- Critical path: Text preprocessing → T5 encoding → Linear transformation → Label scaling → Prediction
- Design tradeoffs: Encoder-only vs. full encoder-decoder (reduced parameters vs. potential loss of generation capabilities), special token replacement (improved sequence length vs. potential loss of numerical information), label normalization (better training stability vs. added preprocessing complexity)
- Failure signatures: Poor performance on volume prediction indicates insufficient capture of space group information; degradation with longer sequences suggests tokenizer limitations; overfitting on small datasets indicates need for stronger regularization
- First 3 experiments:
  1. Train LLM-Prop with default T5 tokenizer and no preprocessing on a small subset to establish baseline performance
  2. Implement the full preprocessing pipeline (special tokens, stopwords removal, [CLS] token) and compare performance gains
  3. Test different label normalization strategies (z-score, min-max, log) to identify optimal scaling for regression tasks

## Open Questions the Paper Calls Out

### Open Question 1
How much does accounting for space group symmetry and Wyckoff sites improve crystal property prediction performance in GNNs compared to text-based methods like LLM-Prop? Basis in paper: [explicit] The authors state that "Our empirical results may highlight the current inability of GNNs to capture information pertaining to space group symmetry and Wyckoff sites for accurate crystal property prediction." Why unresolved: While the authors demonstrate LLM-Prop's superior performance, they don't directly compare the impact of incorporating space group and Wyckoff site information in GNNs versus using text descriptions. What evidence would resolve it: Experiments comparing GNN performance with and without explicit incorporation of space group and Wyckoff site information, versus LLM-Prop's text-based approach.

### Open Question 2
What is the optimal balance between text preprocessing (e.g., removing stopwords, replacing bond distances with tokens) and preserving critical numerical information for crystal property prediction? Basis in paper: [explicit] The authors discuss various preprocessing strategies and their impact on performance, noting that replacing bond distances with [NUM] tokens might limit learning from this information. Why unresolved: The paper doesn't systematically explore the trade-off between compression and information retention in text preprocessing for crystal descriptions. What evidence would resolve it: A comprehensive ablation study varying the extent of text preprocessing and measuring its impact on prediction accuracy across different crystal properties.

### Open Question 3
How does the performance of LLM-Prop scale with the size of the training dataset, and what is the minimum dataset size required for accurate crystal property prediction? Basis in paper: [explicit] The authors investigate how much data LLM-Prop needs to achieve SOTA results, finding it outperforms baselines with less data, but don't explore the lower bound of required data. Why unresolved: While the paper shows LLM-Prop's efficiency, it doesn't determine the absolute minimum training data needed for reliable predictions. What evidence would resolve it: Experiments systematically reducing the training dataset size and measuring the degradation in prediction accuracy to identify the minimum viable dataset size.

## Limitations

- Performance improvements may be influenced by differences in data preprocessing and hyperparameter tuning that are not fully transparent in baseline studies
- Replacement of numerical bond distances and angles with special tokens may remove important predictive features for certain properties
- Claims about GNN limitations in capturing symmetry information are primarily supported by LLM-Prop's superior performance rather than direct comparison with symmetry-aware GNN variants

## Confidence

High Confidence: The core finding that LLM-Prop outperforms CGCNN, MEGNet, and ALIGNN on the TextEdge benchmark dataset. This claim is well-supported by the experimental results presented in Tables 1 and 2, showing consistent improvements across all three property prediction tasks.

Medium Confidence: The assertion that LLM-Prop's advantages stem from better capture of space group symmetry and Wyckoff site information. While the experimental results support this hypothesis, the mechanism is not directly verified through ablation studies or comparison with symmetry-aware GNN variants.

Low Confidence: The generalizability of LLM-Prop's performance to other crystal property prediction tasks beyond the three properties tested (band gap, volume, direct/indirect classification). The paper does not provide evidence that the approach would perform similarly well for other material properties.

## Next Checks

1. **Ablation Study on Preprocessing Strategies**: Systematically evaluate the impact of different preprocessing choices (special token replacement, stopwords removal, [CLS] token addition) on prediction performance. This would help quantify how much each preprocessing step contributes to the overall performance gains and identify potential information loss from numerical value compression.

2. **Comparison with Equivariant GNNs**: Benchmark LLM-Prop against recently developed SE(3)-equivariant graph neural networks that explicitly incorporate symmetry constraints. This would test the claim that text-based approaches are superior for capturing symmetry information by providing a stronger baseline that directly addresses this limitation.

3. **Cross-Property Generalization Test**: Evaluate LLM-Prop's performance on a broader set of material properties beyond band gap, volume, and direct/indirect classification. This would include properties like formation energy, elastic moduli, and magnetic properties to assess the method's generalizability across different types of crystal property prediction tasks.