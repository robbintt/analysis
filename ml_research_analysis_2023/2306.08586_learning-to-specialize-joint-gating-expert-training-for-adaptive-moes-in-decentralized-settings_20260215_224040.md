---
ver: rpa2
title: 'Learning to Specialize: Joint Gating-Expert Training for Adaptive MoEs in
  Decentralized Settings'
arxiv_id: '2306.08586'
source_url: https://arxiv.org/abs/2306.08586
tags: []
core_contribution: The paper addresses how to enable expert specialization in Mixture-of-Experts
  (MoE) models within decentralized training, especially when data is non-i.i.d. across
  clients.
---

# Learning to Specialize: Joint Gating-Expert Training for Adaptive MoEs in Decentralized Settings

## Quick Facts
- **arXiv ID**: 2306.08586
- **Source URL**: https://arxiv.org/abs/2306.08586
- **Reference count**: 40
- **Key outcome**: DDOME achieves up to 24% higher accuracy than state-of-the-art FL baselines in federated learning settings

## Executive Summary
This paper addresses the challenge of enabling expert specialization in Mixture-of-Experts (MoE) models within decentralized training environments, particularly when data is non-i.i.d. across clients. The authors propose Dynamically Decentralized Orchestration of MoEs (DDOME), which uses a pretrained common expert to guide a gating function in selecting a subset of experts for each client. This allows experts to specialize on different data characteristics dynamically while reducing communication costs. The approach is evaluated in federated learning settings on CIFAR10 and CIFAR100, demonstrating significant improvements in accuracy and zero-shot generalization on unseen clients.

## Method Summary
The DDOME framework leverages a pretrained common expert (ResNet34) to extract embeddings from client data, which are then used by a gating function (MLP) to route samples to appropriate experts. A subset of anchor clients are pre-assigned to specific experts to stabilize specialization. During federated training, only the gating function and selected experts (top-K) are transmitted to clients, reducing communication overhead. The system is trained using FedAvg with anchor client strategy, enabling dynamic expert specialization that adapts to client-specific data distributions without requiring labeled data from new clients.

## Key Results
- Achieves up to 24% higher accuracy compared to state-of-the-art FL baselines on CIFAR10 and CIFAR100
- Maintains strong zero-shot generalization performance on unseen clients
- Reduces communication costs by transmitting only gating functions and selected experts (not all experts)
- Theoretical analysis confirms joint training of gating and experts is essential for meaningful specialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of gating and experts enables dynamic expert specialization that adapts to client-specific data distributions
- Mechanism: The gating function, informed by a pretrained common expert, selects a subset of experts per client. These experts are then jointly trained on the client's local data, allowing each expert to specialize on different aspects of the data distribution. This specialization emerges from the interaction between the gating function's routing decisions and the experts' adaptation to local data.
- Core assumption: Non-i.i.d. data across clients provides sufficient diversity for experts to specialize meaningfully
- Evidence anchors: [abstract] "DDOME leverages heterogeneity emerging from distributional shifts across decentralized data sources to specialize experts dynamically"; [section] "Our argument is that the above two open challenges...are answers to each other"
- Break condition: If client data distributions are too similar or if the gating function fails to route samples effectively, experts may not specialize and performance degrades

### Mechanism 2
- Claim: Using a pretrained common expert as an embedding mechanism enables the gating function to analyze client data characteristics without requiring access to raw data
- Mechanism: The common expert extracts features from each client's local data, which are then fed into the gating function. This allows the gating function to make routing decisions based on learned representations rather than raw inputs, preserving privacy and reducing communication costs
- Core assumption: The pretrained common expert has learned useful representations that capture relevant data characteristics for routing
- Evidence anchors: [abstract] "By integrating a pretrained common expert to inform a gating function, DDOME achieves personalized expert subset selection on-the-fly"; [section] "The common expert is sent to all clients once, before training, or can be downloaded from a repository"
- Break condition: If the common expert's representations are not discriminative enough for the target task, the gating function cannot make effective routing decisions

### Mechanism 3
- Claim: Anchor clients stabilize expert specialization by providing consistent data distributions for initial expert assignment and ongoing regularization
- Mechanism: A subset of clients (anchor clients) are pre-assigned to specific experts, ensuring that each expert receives consistent gradient updates from similar data distributions. This prevents experts from drifting and helps establish meaningful specialization
- Core assumption: Anchor clients can be selected such that their data distributions are sufficiently distinct to enable meaningful expert specialization
- Evidence anchors: [section] "Anchor clients are destined to be activated more often than normal clients, in order to form a specialized, one-to-one assignment relationship with experts"; [section] "As we see in practice, this strategy stabilizes our system's performance"
- Break condition: If anchor clients are not representative of distinct data distributions or if their ratio is too low, expert specialization may not stabilize

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: MoE allows the model to activate only relevant experts for each client's data, reducing computation and enabling specialization
  - Quick check question: What is the primary advantage of using MoE over a single monolithic model in federated learning?

- Concept: Federated Learning (FL) with non-i.i.d. data
  - Why needed here: The paper addresses how to handle heterogeneous client data distributions in FL, which is crucial for effective expert specialization
  - Quick check question: Why is non-i.i.d. data distribution both a challenge and an opportunity in federated learning with MoE?

- Concept: Zero-shot personalization
  - Why needed here: The goal is to achieve good performance on unseen clients without requiring additional labeled data or fine-tuning
  - Quick check question: How does the gating function enable zero-shot personalization in this framework?

## Architecture Onboarding

- Component map:
  - Server-side: Collection of expert models (M ResNet34s), gating function (MLP), common expert (pretrained ResNet34)
  - Client-side: Local data, common expert for embedding, selected experts for training
  - Communication: Gating function and selected experts (not all experts) are sent to clients

- Critical path:
  1. Pretrain common expert and send to all clients
  2. Clients embed local data using common expert
  3. Server initializes gating function and experts
  4. For each round:
     - Activate anchor and normal clients
     - Send gating function to all clients
     - Select and send top-K experts to each client
     - Clients jointly train gating function and experts on local data
     - Send updates back to server
     - Aggregate updates

- Design tradeoffs:
  - Number of experts (M) vs. communication cost: More experts allow finer specialization but increase communication if all are sent
  - Anchor client ratio vs. specialization stability: Higher ratio provides better stability but reduces diversity in training
  - Common expert quality vs. gating function performance: Better common expert enables better routing decisions

- Failure signatures:
  - Gating function consistently selects same expert for all clients (indicates poor common expert or insufficient data diversity)
  - Expert accuracy plateaus below common expert baseline (indicates poor expert routing or insufficient specialization)
  - High variance in client performance (indicates unstable expert specialization)

- First 3 experiments:
  1. Verify that the gating function can correctly route samples to appropriate experts using a simple synthetic dataset with known expert specializations
  2. Test the impact of anchor client ratio on expert specialization stability using CIFAR10 with varying anchor/normal client ratios
  3. Evaluate zero-shot performance on unseen clients with different common expert initial accuracies to find the breakpoint performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Fed-ZERO scale with an increasing number of experts (M) and varying client-to-expert ratios (Na:Nc)?
- Basis in paper: [explicit] The paper discusses the impact of anchor clients (Na) and normal clients (Nc) on model performance, but does not provide a detailed analysis of how varying these ratios or increasing the number of experts affects scalability and performance
- Why unresolved: The paper does not explore the upper limits of expert scalability or provide a comprehensive analysis of how different client-to-expert ratios impact the system's efficiency and effectiveness
- What evidence would resolve it: Empirical studies varying M and the Na:Nc ratio, measuring performance metrics such as accuracy, communication costs, and training time

### Open Question 2
- Question: Can Fed-ZERO be effectively extended to non-image classification tasks, such as natural language processing or multimodal learning?
- Basis in paper: [inferred] The paper mentions the potential for extending Fed-ZERO to other domains and applications but does not provide experimental evidence or detailed analysis for such extensions
- Why unresolved: The paper focuses on image classification tasks using ResNet34 models, and there is no exploration of how the framework adapts to different types of data or model architectures
- What evidence would resolve it: Experimental results applying Fed-ZERO to NLP tasks using transformer models or multimodal learning tasks, comparing performance with existing methods

### Open Question 3
- Question: What are the effects of using different initialization strategies for experts on the overall performance and specialization of Fed-ZERO?
- Basis in paper: [explicit] The paper mentions that experts are randomly initialized and briefly discusses the impact of initializing experts from a common expert, but does not provide a thorough investigation of different initialization strategies
- Why unresolved: The paper does not explore alternative initialization methods, such as using pre-trained models or domain-specific initialization, and their effects on expert specialization and model performance
- What evidence would resolve it: Comparative studies using various initialization strategies, analyzing their impact on expert specialization, convergence speed, and final model accuracy

### Open Question 4
- Question: How does Fed-ZERO handle scenarios with highly imbalanced or skewed label distributions across clients?
- Basis in paper: [explicit] The paper conducts experiments with Dirichlet allocation to create skewed label distributions but does not provide a comprehensive analysis of how Fed-ZERO performs under extreme imbalance conditions
- Why unresolved: The paper does not explore the robustness of Fed-ZERO in scenarios where some classes are significantly underrepresented or absent in certain clients' local datasets
- What evidence would resolve it: Experiments with extreme label imbalance, measuring performance metrics such as accuracy, fairness, and expert specialization across different classes

## Limitations

- The approach relies heavily on the assumption that non-i.i.d. data distributions across clients provide sufficient diversity for meaningful expert specialization, which lacks validation across diverse real-world datasets
- The anchor client mechanism is presented as crucial for stability but the selection criteria and optimal ratio are not fully specified, leaving significant implementation uncertainty
- The paper does not address potential catastrophic forgetting when experts are exposed to diverse client data during joint training, which could undermine long-term specialization

## Confidence

- **High confidence**: The core claim that joint training of gating and experts enables specialization is well-supported by the empirical results showing 24% accuracy improvement over baselines and the theoretical argument for the necessity of joint training
- **Medium confidence**: The zero-shot generalization capability is demonstrated but relies on the assumption that the gating function can generalize to unseen data distributions. The paper shows this works for CIFAR but doesn't validate on more diverse or real-world federated learning scenarios
- **Low confidence**: The claim that anchor clients are essential for stability is primarily supported by practical observations rather than systematic ablation studies. The exact impact of anchor client ratio and selection method remains unclear

## Next Checks

1. **Ablation study on anchor client ratio**: Systematically vary the anchor client ratio from 0% to 50% to quantify the exact impact on specialization stability and performance, identifying the optimal ratio and minimum viable threshold
2. **Cross-dataset generalization**: Evaluate DDOME on non-image datasets (e.g., healthcare or text data) to test whether the specialization mechanism generalizes beyond CIFAR and whether the common expert approach works for different feature spaces
3. **Long-term stability analysis**: Track expert specialization quality over 1000+ rounds of federated training to detect catastrophic forgetting or expert drift, and test whether periodic retraining of the common expert improves routing performance