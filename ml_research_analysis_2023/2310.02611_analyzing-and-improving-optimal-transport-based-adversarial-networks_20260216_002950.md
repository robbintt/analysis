---
ver: rpa2
title: Analyzing and Improving Optimal-Transport-based Adversarial Networks
arxiv_id: '2310.02611'
source_url: https://arxiv.org/abs/2310.02611
tags:
- uotm
- transport
- training
- optimal
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes and improves Optimal-Transport-based adversarial
  networks for generative modeling. The authors propose a unified framework that encompasses
  various OT-based GANs, including WGAN, OTM, and UOTM, by parameterizing the potential
  and generator using the dual or semi-dual formulations of OT or UOT problems.
---

# Analyzing and Improving Optimal-Transport-based Adversarial Networks

## Quick Facts
- arXiv ID: 2310.02611
- Source URL: https://arxiv.org/abs/2310.02611
- Reference count: 39
- Primary result: Achieves state-of-the-art FID scores of 2.51 on CIFAR-10 and 5.99 on CelebA-HQ-256

## Executive Summary
This paper presents a comprehensive analysis and improvement of Optimal-Transport-based adversarial networks for generative modeling. The authors develop a unified framework that encompasses various OT-based GANs including WGAN, OTM, and UOTM by parameterizing potential and generator using dual or semi-dual formulations of OT or UOT problems. Through extensive comparative analysis, they identify key components that affect training dynamics: the cost function mitigates mode collapse, while strictly convex functions in the discriminator loss enhance stability. Building on these insights, they introduce UOTM with Scheduled Divergence (UOTM-SD), a method that gradually up-weights divergence terms in the UOT problem, achieving state-of-the-art performance on standard benchmarks.

## Method Summary
The paper proposes a unified framework for OT-based GANs that parameterizes the potential and generator using dual or semi-dual formulations of OT or UOT problems. The key innovation is UOTM-SD, which applies α-scheduling to gradually increase the weight of divergence terms in the UOT objective. This scheduling transforms the transport plan from UOT to OT as α→∞, combining the robustness of UOT at small α with the better distribution matching properties of OT at large α. The method uses strictly convex functions (Softplus) for discriminator loss components to provide adaptive sample-wise weighting, and incorporates a quadratic cost function to mitigate mode collapse.

## Key Results
- UOTM-SD achieves state-of-the-art FID scores of 2.51 on CIFAR-10 and 5.99 on CelebA-HQ-256
- The cost function significantly mitigates mode collapse compared to OT-based methods without cost
- Strictly convex functions in the discriminator loss enhance training stability
- UOTM-SD demonstrates superior τ-robustness, maintaining performance across a wide range of τ values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Strictly convex functions for g1 and g2 create adaptive sample-wise weighting that stabilizes training
- Mechanism: The strictly increasing derivative of convex functions increases weight for samples where the generator dominates (small negative losses) or where the discriminator underfits real samples, forcing adaptation
- Core assumption: Gradient descent updates with sample-wise weights based on current loss values
- Evidence anchors: Abstract mentions stability enhancement; section explains gradient updates with sample-wise weights
- Break condition: Replacing strictly convex functions with identity functions causes training instability

### Mechanism 2
- Claim: The quadratic cost function mitigates mode collapse by encouraging local transport
- Mechanism: The quadratic cost creates regularization pressure that discourages mapping inputs to distant modes, incentivizing transport maps that minimize both adversarial objective and transport cost
- Core assumption: Cost function is incorporated through semi-dual formulation into the generator's objective
- Evidence anchors: Abstract mentions mode collapse mitigation; experiments show improved mode coverage with cost term
- Break condition: Setting τ=0 removes cost term and mode collapse reappears

### Mechanism 3
- Claim: α-scheduling gradually transforms transport plan from UOT to OT, improving τ-robustness and performance
- Mechanism: Starting with small α to avoid mode collapse, then gradually increasing α to emphasize cost term, benefits from both regimes as UOT transport plan converges to OT transport plan
- Core assumption: Optimal transport plan of α-scaled UOT problem converges to OT problem as α→∞
- Evidence anchors: Abstract mentions convergence of transport plan; Theorem 4.1 proves this convergence
- Break condition: Too aggressive scheduling (large α jumps) may skip beneficial intermediate regimes

## Foundational Learning

- Concept: Optimal Transport theory and dual/semi-dual formulations
  - Why needed here: The entire framework is built on OT theory, crucial for implementing and modifying these algorithms
  - Quick check question: What is the relationship between Kantorovich-Rubinstein duality and the WGAN objective?

- Concept: Unbalanced Optimal Transport and Csiszár divergences
  - Why needed here: UOTM and UOTM-SD are based on unbalanced OT, which relaxes marginal constraints using divergence terms
  - Quick check question: How does the Csiszár divergence DΨ(π₀|μ) differ from the KL divergence, and when would you choose one over the other?

- Concept: GAN training dynamics and Nash equilibrium
  - Why needed here: Understanding instability issues in GAN training helps explain why modifications like strictly convex g1,g2 are beneficial
  - Quick check question: Why does gradient descent struggle to find Nash equilibria in two-player games, and how does this relate to GAN training instability?

## Architecture Onboarding

- Component map: Prior noise -> Generator network T_θ -> Discriminator/Potential network v_ϕ -> Loss computation -> Gradient updates -> New samples
- Critical path: Prior noise → Generator → Discriminator evaluation → Loss computation (generator + discriminator) → Gradient updates → New samples
- Design tradeoffs:
  - τ vs α tradeoff: Large τ helps with mode collapse but hurts distribution matching; scheduling resolves by transitioning from small to large effective τ
  - g1,g2 choice: Identity functions are simpler but unstable; strictly convex functions add computation but improve stability
  - OT vs UOT: OT has better convergence but is outlier-sensitive; UOT is more robust but requires careful hyperparameter tuning
- Failure signatures:
  - Mode collapse: Generator produces limited variety of samples clustering around few modes
  - Training instability: Loss values oscillate wildly or diverge, visible as generator loss increasing while discriminator loss decreases
  - τ-sensitivity: Performance varies dramatically with small changes in τ parameter
  - Slow convergence: Particularly for OTM, which converges slower than other methods
- First 3 experiments:
  1. Implement Algorithm 1 with c=0, g1=g2=g3=SP (standard GAN) and verify it works on a simple 2D dataset
  2. Add cost function with small τ and compare mode coverage against baseline
  3. Implement UOTM-SD with linear scheduling and test τ-robustness by varying τ across orders of magnitude

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the convergence of the transport plan in Theorem 4.1 imply the convergence of the transport map?
- Basis in paper: Theorem 4.1 proves convergence of optimal transport plan but doesn't address transport map convergence
- Why unresolved: Theorem only addresses transport plan convergence, not the generator (transport map) convergence
- What evidence would resolve it: Mathematical proof of transport map T convergence as α→∞, or empirical evidence from UOTM-SD generator convergence

### Open Question 2
- Question: How does choice of strictly convex functions g1 and g2 affect stability and performance beyond those tested?
- Basis in paper: Only Identity and Softplus functions are analyzed, not the full space of convex functions
- Why unresolved: Paper considers only two specific convex functions without investigating alternatives
- What evidence would resolve it: Experiments comparing performance with various convex functions like exponential functions or other convex choices

### Open Question 3
- Question: What is the theoretical justification for the effectiveness of scheduled divergence approach in UOTM-SD?
- Basis in paper: Proposes UOTM-SD based on empirical evidence without rigorous theoretical explanation
- Why unresolved: Paper provides empirical effectiveness but lacks theoretical analysis of why scheduling works
- What evidence would resolve it: Theoretical analysis of UOT optimization landscape with scheduled divergence showing improved convergence and performance

## Limitations

- The claims about mode collapse mitigation and stability improvements are primarily supported by experimental results rather than theoretical guarantees
- Empirical validation is confined to standard benchmark datasets (CIFAR-10 and CelebA-HQ-256) and may not generalize to more complex data distributions
- The mathematical proofs are limited to specific convergence properties of the α-scheduling, with incomplete theoretical understanding of why strictly convex discriminator functions work across different training dynamics

## Confidence

- **High confidence**: Core theoretical framework connecting OT formulations to GAN training is well-established and correctly applied; convergence proof for α-scheduling is mathematically rigorous
- **Medium confidence**: Claims about mode collapse mitigation through cost functions are supported by ablation studies but exact mechanism and generalizability require further investigation
- **Medium confidence**: Stability improvements from strictly convex discriminator functions are demonstrated empirically, but theoretical understanding of why this works across different training dynamics is incomplete

## Next Checks

1. **Ablation on scheduling schemes**: Systematically compare the three scheduling approaches (Cosine, Linear, Step) across different dataset complexities to identify which provides the most robust performance gains and under what conditions each fails

2. **Cross-dataset generalization**: Test UOTM-SD on datasets with different characteristics (e.g., LSUN bedrooms, ImageNet-32) to evaluate whether the reported FID improvements transfer beyond CIFAR-10 and CelebA

3. **Gradient-based sensitivity analysis**: Quantify how the strictly convex functions affect gradient norms and training stability by measuring gradient magnitudes, cosine similarities between consecutive updates, and identifying conditions that lead to gradient explosion or vanishing