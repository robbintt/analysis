---
ver: rpa2
title: Learning Layer-wise Equivariances Automatically using Gradients
arxiv_id: '2310.06131'
source_url: https://arxiv.org/abs/2310.06131
tags:
- symmetry
- layers
- equivariance
- learning
- conv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of automatically learning layer-wise
  equivariance symmetries in deep neural networks from training data, rather than
  relying on manually specified hard-coded symmetries. The core method idea is to
  relax symmetry constraints using parameterisations that allow interpolation between
  fully connected and equivariant convolutional layers, then use Bayesian model selection
  via Laplace approximations to learn the amount of equivariance per layer.
---

# Learning Layer-wise Equivariances Automatically using Gradients

## Quick Facts
- **arXiv ID**: 2310.06131
- **Source URL**: https://arxiv.org/abs/2310.06131
- **Reference count**: 40
- **Primary result**: Automatically learns layer-wise equivariance symmetries in deep neural networks from training data using marginal likelihood optimization

## Executive Summary
This paper addresses the challenge of automatically discovering equivariance symmetries in neural networks without manual specification. The authors propose a method that relaxes symmetry constraints using parameterisations allowing interpolation between fully connected and equivariant convolutional layers. By optimizing hyperparameters via Bayesian model selection with Laplace approximations, the network learns the appropriate amount of equivariance per layer. Experiments on image classification tasks demonstrate that the method automatically learns to use convolutional structure in early layers while achieving performance comparable to or better than architectures with hard-coded symmetries.

## Method Summary
The method uses parameterisations that allow differentiable interpolation between fully connected and equivariant convolutional layers, then optimizes the amount of equivariance per layer using Bayesian model selection via Laplace approximations. The authors improve upon existing parameterisations to keep the number of parameters close to classical convolutions and derive Kronecker-factored approximations for scalable training. The approach treats equivariance levels as hyperparameters optimized via marginal likelihood, which balances data fit and model complexity. The framework is applied to various layer types including factored layers and sparsified layers to reduce parameters while maintaining expressiveness.

## Key Results
- Automatically learns to use convolutional structure in early layers on image classification tasks
- Achieves similar or improved performance compared to architectures with hard-coded symmetry
- Successfully discovers task-specific equivariance patterns through Bayesian model selection
- Reduces parameter count through factored and sparsified layer designs while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Laplace approximated marginal likelihood objective automatically balances data fit and model complexity, encouraging the network to learn equivariant structure where beneficial.
- Mechanism: By optimizing hyperparameters η via marginal likelihood, the model selects the amount of equivariance in each layer. High prior precision (σ⁻² → ∞) enforces strict equivariance, while finite values relax it. The marginal likelihood naturally penalizes unnecessary complexity (Occam's razor), preventing over-constraining the model.
- Core assumption: The marginal likelihood with Laplace approximation accurately captures the trade-off between model fit and complexity, and this trade-off aligns with optimal symmetry discovery.
- Evidence anchors:
  - [abstract]: "To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations."
  - [section]: "The marginal likelihood p(D|η) forms the normalising constant of the unnormalised posterior over parameters... Complexity control is important when learning symmetry constraints, as previous works [van der Ouderaa and van der der Wilk, 2021, Immer et al., 2021] have shown that just using the regular training loss Lθ w.r.t. data augmentation parameters prefers collapse into a solution without symmetry constraints resulting in fully-connected structure."
  - [corpus]: Weak - no direct experimental evidence in corpus papers cited.

### Mechanism 2
- Claim: Parameterizing layer-wise equivariance as a soft constraint via residual pathways and factored structures allows differentiable interpolation between fully connected and strictly equivariant layers.
- Mechanism: The residual pathway structure (FC + CONV) allows the model to interpolate between general linear mappings and equivariant ones. Factoring (e.g., θ(c′, c, x′, y′, x, y) = θ₁(c′, c, x′, y′)θ₂(c′, c, x, y)) reduces parameters while preserving expressiveness. This parameterization enables gradient-based optimization of equivariance levels.
- Core assumption: The residual pathway parameterization is expressive enough to represent both fully connected and strictly equivariant layers, and the factored version sufficiently approximates the original while reducing parameters.
- Evidence anchors:
  - [abstract]: "To do so, we relax symmetries building upon recent literature on approximate equivariance, including residual pathways [Finzi et al., 2021a] and non-stationary filters [van der Ouderaa et al., 2022]."
  - [section]: "To learn layer-wise equivariances, we take the approach of relaxing symmetry constraints in neural network layers allowing explicit differentiable interpolation between equivariant and non-equivariant solutions."
  - [corpus]: Weak - the corpus papers on "Recurrent Equivariant Constraint Modulation" and "Relaxed Equivariance via Multitask Learning" suggest related but not identical approaches.

### Mechanism 3
- Claim: Kronecker-factored approximations of the Hessian enable scalable Laplace approximations for large neural networks, making the marginal likelihood objective tractable.
- Mechanism: The Gauss-Newton approximation H ≈ Pₙ J(xₙ)⊤Λ(xₙ)J(xₙ) is block-diagonalized and each block is approximated as a Kronecker product. This reduces the computational complexity from O(|θ|²) to manageable levels, enabling hyperparameter optimization in deep networks.
- Core assumption: The Kronecker-factored approximation accurately captures the curvature of the log-likelihood, and this approximation is sufficient for reliable marginal likelihood estimation.
- Evidence anchors:
  - [abstract]: "We derive Kronecker-factored approximations for proposed relaxed equivariance layers to use them in conjunction with this objective."
  - [section]: "In practice, we therefore use a local linearisation [Bottou et al., 2018] resulting in the generalized Gauss-Newton (GGN) approximation, which we further approximate using a block-diagonal Kronecker-factored (KFAC) structure Martens and Grosse [2015]."
  - [corpus]: Weak - no direct evidence in corpus about KFAC's role in symmetry learning.

## Foundational Learning

- Concept: Bayesian model selection and marginal likelihood
  - Why needed here: The method learns equivariance levels by treating them as hyperparameters and optimizing the marginal likelihood, which balances data fit and model complexity.
  - Quick check question: What is the relationship between the marginal likelihood and the Occam's razor principle in model selection?

- Concept: Laplace approximation and its use in high-dimensional spaces
  - Why needed here: The marginal likelihood integral is intractable for large neural networks, so the Laplace approximation is used to estimate it by approximating the posterior with a Gaussian centered at the MAP estimate.
  - Quick check question: How does the Laplace approximation simplify the computation of the marginal likelihood?

- Concept: Kronecker-factored approximate curvature (KFAC)
  - Why needed here: KFAC approximates the Hessian of the log-likelihood in a computationally efficient way, making the Laplace approximation tractable for deep networks.
  - Quick check question: What is the key insight behind KFAC that allows it to approximate the Hessian efficiently?

## Architecture Onboarding

- Component map: Input -> Encoder (series of relax equivariance layers) -> Pooling -> Classifier -> Output
- Critical path: Forward pass through layers → Compute Jacobian and likelihood Hessian → Apply KFAC approximation → Compute marginal likelihood estimate → Update hyperparameters → Update model parameters
- Design tradeoffs:
  - Parameter efficiency vs. expressiveness: Factored and sparsified layers reduce parameters but may limit representational capacity.
  - Strict vs. relaxed equivariance: Strict equivariance provides stronger inductive bias but may be too restrictive; relaxed allows learning but increases complexity.
  - Computational cost vs. accuracy: KFAC approximation is faster but less accurate than full Hessian; affects hyperparameter optimization quality.
- Failure signatures:
  - All layers becoming fully connected (σ² → ∞ for all layers): Suggests the model cannot benefit from equivariance or the marginal likelihood is not properly balancing complexity.
  - Poor performance on tasks requiring translation equivariance: Indicates the model is not learning sufficient equivariance in early layers.
  - High variance in learned equivariance across layers: May suggest instability in hyperparameter optimization or inappropriate initialization.
- First 3 experiments:
  1. Reproduce the toy problem (Section 6.1): Test if the model learns to use convolutional structure on translation-invariant tasks and relaxes it on tasks requiring positional information.
  2. Train on CIFAR-10 with FC+CONV architecture: Verify that the model learns to use convolutional layers in early layers and achieves comparable performance to a strictly convolutional baseline.
  3. Test on a dataset with known symmetries (e.g., rotated MNIST): Check if the model automatically learns to use the appropriate equivariance (e.g., rotation equivariance for rotated MNIST).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform when learning symmetries for non-translation groups beyond discrete 2D translations?
- Basis in paper: [explicit] The paper discusses generalizing the proposed method to other symmetry groups in Appendix A and mentions potential applications to continuous groups, but does not provide empirical results.
- Why unresolved: The paper only presents experimental results for discrete 2D translation symmetry, leaving the performance on other groups unverified.
- What evidence would resolve it: Experimental results showing the method's performance on tasks requiring symmetries of continuous groups or other discrete groups beyond Z2.

### Open Question 2
- Question: What is the computational overhead of using the proposed sparsified layers compared to standard convolutional layers in practice?
- Basis in paper: [inferred] The paper claims sparsified layers reduce parameters but doesn't provide runtime comparisons or GPU memory usage data.
- Why unresolved: While parameter counts are reduced, the actual computational cost during training and inference is not discussed, which is crucial for practical adoption.
- What evidence would resolve it: Benchmark data comparing training/inference time and memory usage between standard convolutions and the proposed sparsified layers on real tasks.

### Open Question 3
- Question: How sensitive is the learned symmetry structure to the choice of prior distributions beyond the Gaussian priors explored in the paper?
- Basis in paper: [explicit] The paper uses Gaussian priors and discusses the limiting case of strict equivariance, but doesn't explore alternative prior distributions.
- Why unresolved: The paper only considers one type of prior, leaving open questions about how different prior choices might affect the learned symmetries.
- What evidence would resolve it: Experiments comparing learned symmetries and performance when using different prior distributions (e.g., Laplace, Student's t, or mixture priors) on the same tasks.

## Limitations
- The method relies heavily on the accuracy of Laplace approximations in high-dimensional spaces, which may not capture the true posterior well
- Experimental validation focuses primarily on image classification with translation equivariance, limiting generalizability to other symmetry types
- The computational benefits of KFAC approximations for these specific layers need further verification
- Performance gains over manually designed architectures are modest rather than consistently superior

## Confidence
- **High confidence**: The theoretical framework connecting marginal likelihood optimization to automatic symmetry discovery is sound and well-grounded in Bayesian principles.
- **Medium confidence**: The experimental results demonstrate that the method can learn to use convolutional structure where appropriate, but the performance gains over baselines are modest.
- **Medium confidence**: The parameterization improvements (factored and sparsified layers) effectively reduce parameters while maintaining expressiveness, though comprehensive ablation studies are needed.

## Next Checks
1. **Cross-symmetry generalization test**: Apply the method to a rotation-equivariant task (e.g., rotated MNIST or object classification under rotation) to verify whether the model automatically learns rotation-equivariant structure rather than just translation equivariance.
2. **Ablation study on parameterization**: Systematically compare the performance of F-FC, S-FC, and strict FC layers across multiple depths and datasets to quantify the tradeoff between parameter efficiency and expressiveness.
3. **Laplace approximation validation**: Compare the learned equivariance patterns when using exact marginal likelihood (on small networks) versus Laplace approximation to assess the impact of approximation error on symmetry discovery.