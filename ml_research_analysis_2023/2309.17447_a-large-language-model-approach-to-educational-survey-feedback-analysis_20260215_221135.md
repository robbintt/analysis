---
ver: rpa2
title: A Large Language Model Approach to Educational Survey Feedback Analysis
arxiv_id: '2309.17447'
source_url: https://arxiv.org/abs/2309.17447
tags:
- classification
- course
- survey
- comments
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study demonstrates that large language models (LLMs) like\
  \ GPT-4 and GPT-3.5 can effectively analyze unstructured educational survey feedback\
  \ using a zero-shot approach. By treating feedback analysis as a sequence of NLP\
  \ tasks\u2014classification, extraction, thematic analysis, and sentiment analysis\u2014\
  LLMs achieved human-level performance on multiple tasks."
---

# A Large Language Model Approach to Educational Survey Feedback Analysis

## Quick Facts
- arXiv ID: 2309.17447
- Source URL: https://arxiv.org/abs/2309.17447
- Reference count: 40
- One-line primary result: LLMs like GPT-4 can analyze unstructured educational survey feedback with human-level performance using a zero-shot approach

## Executive Summary
This study demonstrates that large language models can effectively analyze unstructured educational survey feedback by treating it as a sequence of natural language processing tasks. Using GPT-4 and GPT-3.5, the researchers achieved human-level performance across multiple tasks including multi-label classification, extraction, thematic analysis, and sentiment analysis without requiring fine-tuning or labeled training data. The approach leverages zero-shot chain-of-thought prompting to provide transparent reasoning for decisions, enabling comprehensive workflows that can extract insights from survey data at scale. The findings suggest that LLMs offer a versatile and scalable solution for educational institutions seeking to derive actionable insights from student feedback.

## Method Summary
The study collected 2500 end-of-course survey comments from biomedical science courses and applied GPT-4 and GPT-3.5 via OpenAI API to perform various NLP tasks. The researchers used zero-shot prompting with chain-of-thought reasoning to conduct multi-label classification, extraction, thematic analysis, and sentiment analysis. They employed function calling for structured JSON output and developed custom evaluation rubrics to assess model performance against human annotations. The approach treated each NLP task as a modular component that could be composed into workflows, such as extracting relevant excerpts followed by multi-class classification or deriving themes from batches of comments.

## Key Results
- GPT-4 achieved 80.60% Jaccard similarity with human annotators on multi-label classification, comparable to the 81.24% inter-rater agreement among human experts
- Chain-of-thought reasoning provided consistent, logical justifications that could be used for prompt tuning and building user confidence
- The zero-shot approach enabled comprehensive survey analysis workflows without requiring specialized models or fine-tuning
- GPT-4 outperformed GPT-3.5 across all tasks, with the most significant differences in multi-label classification and thematic analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 achieves human-level performance on multi-label classification of educational survey feedback using zero-shot prompting.
- Mechanism: The model leverages its pre-trained knowledge to understand the context of survey comments and apply appropriate classification labels based on provided descriptions. Zero-shot chain-of-thought prompting enables the model to reason through the classification process step-by-step.
- Core assumption: GPT-4's pre-training data includes sufficient examples of educational contexts and feedback patterns to enable accurate classification without fine-tuning.
- Evidence anchors:
  - [abstract] "We achieve human-level performance on multiple tasks with GPT-4, enabling workflows necessary to achieve typical goals."
  - [section] "The average Jaccard similarity coefficient including all 2500 rows (averaged across the six unique pairings of four human raters for all rows) was 81.24%, suggesting that this was a challenging task even for expert human annotators... GPT-4 agreement with human annotators is shown; the average across all pairings including GPT-4 was 80.60%."

### Mechanism 2
- Claim: Chain-of-thought reasoning in GPT-4 provides consistent, logical justifications that can be used for prompt tuning and building user confidence.
- Mechanism: By asking the model to explain its reasoning step-by-step, users can inspect the logical trajectory of decisions. This reasoning is consistent with the classification results and changes predictably when prompts are modified.
- Core assumption: The reasoning provided by GPT-4 reflects its actual decision-making process rather than just plausible-sounding explanations.
- Evidence anchors:
  - [abstract] "We also show the potential of inspecting LLMs' chain-of-thought (CoT) reasoning for providing insight that may foster confidence in practice."
  - [section] "The prompts for binary classification, multi-label classification, multi-class classification, sentiment analysis, and evaluation of extraction results all used zero-shot chain-of-thought (CoT) to enhance the quality of the results... Figures 7 and 8 show the model's CoT reasoning related to Example 3.2.3... Evaluation of the extraction task used a custom LLM evaluation... we inspected the CoT reasoning along with the structured eval results... and made modifications to the evaluation prompts in an iterative fashion."

### Mechanism 3
- Claim: The combination of multiple NLP tasks (classification, extraction, thematic analysis, sentiment analysis) into workflows enables comprehensive survey feedback analysis without specialized models.
- Mechanism: Each NLP task serves as a building block that can be composed into more complex workflows. Multi-label classification provides initial categorization, extraction isolates relevant text, thematic analysis identifies patterns, and sentiment analysis adds nuance.
- Core assumption: The output of one NLP task can reliably serve as input to the next task in the workflow.
- Evidence anchors:
  - [abstract] "We demonstrate a versatile approach to such goals by treating them as sequences of natural language processing (NLP) tasks including classification (multi-label, multi-class, and binary), extraction, thematic analysis, and sentiment analysis, each performed by LLM."
  - [section] "A workflow for finding and quantifying suggestions for course improvement is shown in Figure 3, and consists of extraction of relevant excerpts, followed by multi-class classification... A workflow for finding and summarizing the main themes... consists of three LLM steps: 1) themes are first derived and summarized for batches of comments... 2) comments are classified using the derived themes; and 3) sets of themes from these batches are coalesced..."

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The study demonstrates that GPT-4 can perform classification tasks without any labeled training data, which is crucial for educational settings where labeled data is scarce.
  - Quick check question: What is the key difference between zero-shot and few-shot learning in the context of LLMs?

- Concept: Chain-of-thought prompting
  - Why needed here: CoT prompting significantly improves the accuracy of LLM outputs and provides transparency into the model's reasoning process.
  - Quick check question: How does chain-of-thought prompting differ from standard prompting approaches?

- Concept: Multi-label classification
  - Why needed here: Survey comments often contain multiple topics, requiring classification into more than one category simultaneously.
  - Quick check question: What metric is most appropriate for evaluating multi-label classification performance?

## Architecture Onboarding

- Component map: Survey comment → Multi-label classification → Extraction (if needed) → Further classification/analysis → Structured output with reasoning

- Critical path: Survey comment → Multi-label classification → Extraction (if needed) → Further classification/analysis → Structured output with reasoning

- Design tradeoffs:
  - Cost vs. performance: GPT-4 offers better accuracy but higher cost compared to GPT-3.5
  - Zero-shot vs. few-shot: Zero-shot is more practical for real-world use but may have lower performance on certain tasks
  - Prompt complexity vs. reliability: More detailed prompts improve accuracy but increase token usage

- Failure signatures:
  - Inconsistent reasoning: GPT-4's chain-of-thought reasoning doesn't align with its classification output
  - Context loss: Extraction fails to capture necessary context for understanding excerpts
  - Category confusion: Multi-label classification assigns conflicting or overlapping labels

- First 3 experiments:
  1. Test GPT-4 zero-shot classification on a small subset of survey comments with human-annotated ground truth
  2. Implement chain-of-thought prompting and compare accuracy to standard prompting
  3. Create a simple workflow combining classification and extraction, then evaluate the integrated output quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 compare to fine-tuned BERT models for multi-label classification tasks?
- Basis in paper: [inferred]
- Why unresolved: The paper compares GPT-4's zero-shot performance to human annotators and a fine-tuned SetFit model, but does not provide a direct comparison to fine-tuned BERT models. This comparison would provide valuable context for understanding the relative strengths of different approaches.
- What evidence would resolve it: A direct comparison of GPT-4's zero-shot performance to a fine-tuned BERT model on the same multi-label classification task, using the same dataset and evaluation metrics.

### Open Question 2
- Question: What is the impact of few-shot prompting on the performance of GPT-4 for sentiment analysis tasks?
- Basis in paper: [explicit]
- Why unresolved: The paper acknowledges that few-shot prompting could improve GPT-4's performance on sentiment analysis, particularly for negative comments. However, it does not explore the impact of few-shot prompting in detail. Understanding the potential gains from few-shot prompting would inform the optimal use of GPT-4 for sentiment analysis.
- What evidence would resolve it: A comparison of GPT-4's performance on sentiment analysis tasks using zero-shot prompting versus few-shot prompting, with a focus on negative comments. This comparison should use the same dataset and evaluation metrics.

### Open Question 3
- Question: How generalizable are the developed labels for multi-label classification across different educational contexts and course types?
- Basis in paper: [explicit]
- Why unresolved: The paper develops a set of labels for multi-label classification that are intended to be generalizable across different course types. However, the evaluation only considers their performance on biomedical science courses. Assessing their generalizability to other educational contexts would provide valuable insights into their broader applicability.
- What evidence would resolve it: An evaluation of the developed labels on a diverse set of educational contexts and course types, including different disciplines, levels of education, and delivery modes. This evaluation should use the same dataset and evaluation metrics as the original study.

## Limitations

- The dataset is limited to 2500 survey comments from biomedical science courses, potentially limiting generalizability to other educational contexts
- Human inter-rater agreement was only 81.24%, indicating the task is inherently challenging and subjective even for experts
- The study does not address cost implications of using GPT-4 at scale for large educational institutions
- Long-term reliability of zero-shot approaches versus fine-tuned models for specific educational domains remains unclear

## Confidence

- **High Confidence**: GPT-4 achieves human-level performance on multi-label classification of educational survey feedback (80.60% Jaccard similarity with human annotators)
- **Medium Confidence**: Chain-of-thought reasoning can reliably be used for prompt tuning and building user confidence (supported but requires further validation)
- **Low Confidence**: Scalability claims for educational institutions using this approach are speculative (no cost-benefit analyses provided)

## Next Checks

1. **Cross-disciplinary validation**: Test the zero-shot approach on survey feedback from non-biomedical fields (e.g., humanities, social sciences) to assess generalizability across educational contexts.

2. **Long-term reliability assessment**: Conduct a longitudinal study comparing zero-shot LLM performance against periodically fine-tuned models on the same dataset over time to evaluate which approach maintains better accuracy.

3. **Cost-effectiveness analysis**: Calculate the total cost of processing large-scale survey data (100,000+ comments) using GPT-4 versus alternative approaches (fine-tuned models, traditional NLP pipelines) and assess the trade-offs between accuracy and computational expense.