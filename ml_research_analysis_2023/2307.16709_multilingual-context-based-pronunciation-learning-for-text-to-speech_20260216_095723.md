---
ver: rpa2
title: Multilingual context-based pronunciation learning for Text-to-Speech
arxiv_id: '2307.16709'
source_url: https://arxiv.org/abs/2307.16709
tags:
- multilingual
- words
- level
- pronunciation
- monolingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a unified multilingual neural front-end for
  text-to-speech that jointly addresses pronunciation-related tasks such as grapheme-to-phoneme
  conversion, homograph and polyphone disambiguation, post-lexical rules, and implicit
  diacritization. The model is trained on word- and sentence-level pronunciation data
  from 24 locales using a transformer architecture.
---

# Multilingual context-based pronunciation learning for Text-to-Speech

## Quick Facts
- arXiv ID: 2307.16709
- Source URL: https://arxiv.org/abs/2307.16709
- Reference count: 0
- Primary result: Unified multilingual neural front-end for TTS achieves competitive results across 24 locales, with average PER of 2.09% at word level and 0.97% at sentence level

## Executive Summary
This paper presents a unified multilingual neural front-end for text-to-speech that jointly addresses pronunciation-related tasks including grapheme-to-phoneme conversion, homograph and polyphone disambiguation, post-lexical rules, and implicit diacritization. The transformer-based model is trained on word- and sentence-level pronunciation data from 24 locales, achieving competitive performance with monolingual solutions while reducing deployment complexity. The approach demonstrates that multilingual training can effectively handle context-dependent pronunciation phenomena while maintaining reasonable accuracy across diverse linguistic patterns.

## Method Summary
The authors developed a 12-layer transformer encoder-decoder model trained jointly on word- and sentence-level pronunciation data from 24 locales. The model uses language codes as input prefixes to condition pronunciation generation for specific languages. Training was conducted for 1M steps on 6 GPUs with dynamic batching up to 4096 tokens, using Adam optimizer and a "noam" learning rate schedule with warmup steps=8000. The model contains approximately 52.5M parameters and generates space-separated phonemes in X-SAMPA format. Evaluation was performed using WER%, SER%, and PER% metrics, with additional language-specific task evaluations including homograph disambiguation for English, polyphone disambiguation for Mandarin, post-lexical rules for French, and diacritization for Arabic.

## Key Results
- Word-level performance: Average phone error rate of 2.09% and word error rate of 8.45%
- Sentence-level performance: Phone error rate of 0.97% and sentence error rate of 27.55%
- The multilingual model achieves competitive results across all tasks but shows trade-offs on specific language tasks (Mandarin polyphone disambiguation, French post-lexical rules)
- Language-specific performance varies, with some locales showing strong results while others indicate areas for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A unified multilingual model can learn pronunciation patterns across 24 locales, reducing the need for separate monolingual modules.
- Mechanism: The transformer encoder-decoder architecture is trained jointly on word-level and sentence-level pronunciation data, using language codes as input prefixes. This allows the model to capture shared grapheme-to-phoneme patterns while adapting to language-specific features like stress and diacritics.
- Core assumption: Pronunciation knowledge is partially transferable across languages, especially among related languages sharing the same script or phonetic features.
- Evidence anchors: The model achieves competitive results across languages and tasks, with some trade-offs compared to monolingual solutions.

### Mechanism 2
- Claim: Including sentence-level data improves handling of context-dependent pronunciation phenomena like homograph disambiguation and polyphone resolution.
- Mechanism: By training on full sentences, the model learns contextual dependencies that are not visible at the word level, enabling it to disambiguate words whose pronunciation varies with context.
- Core assumption: Context is necessary to resolve pronunciation ambiguities that cannot be handled by lexicon lookup alone.
- Evidence anchors: The model is trained on "word- and sentence-level pronunciation data" and evaluates on tasks like homograph and polyphone disambiguation.

### Mechanism 3
- Claim: Language codes prepended to input allow the model to condition pronunciation generation on the target language, enabling multilingual output from a single model.
- Mechanism: Each input token sequence is prefixed with a unique language identifier (e.g., "en-gb"), so the transformer can switch behavior based on the language context during both training and inference.
- Core assumption: The model can effectively use a discrete language tag as a conditioning signal for pronunciation generation.
- Evidence anchors: "In order to handle multilingual information, we prepend languages codes to the input entries, as recommended in [18]."

## Foundational Learning

- Concept: Grapheme-to-phoneme (G2P) conversion
  - Why needed here: This is the core task the model is solving—mapping written characters to phonetic symbols across multiple languages.
  - Quick check question: What is the difference between a grapheme and a phoneme in the context of TTS?

- Concept: Context-dependent pronunciation
  - Why needed here: Tasks like homograph disambiguation and polyphone resolution require the model to use surrounding text to determine the correct pronunciation.
  - Quick check question: Why can't a simple dictionary lookup handle homograph disambiguation?

- Concept: Multilingual model conditioning
  - Why needed here: The model must generate correct pronunciations for 24 different languages using a single architecture, requiring explicit language identification.
  - Quick check question: How does prepending a language code help the model switch between languages during inference?

## Architecture Onboarding

- Component map: Character-level input with language code prefix -> 12-layer transformer encoder-decoder -> Space-separated phonemes in X-SAMPA format
- Critical path: 1. Input text → prepend language code → character tokens. 2. Encoder processes sequence → decoder generates phoneme sequence. 3. Output phonemes → space-separated string for evaluation.
- Design tradeoffs: Joint training on word and sentence data improves context handling but increases data complexity. Character-level input keeps vocabulary small but may limit subword modeling. Multilingual training reduces per-language accuracy slightly but saves deployment cost.
- Failure signatures: High WER/PER on specific locales → likely data imbalance or script mismatch. Errors on homographs → context window may be too short or training data insufficient. Misspellings in output → tokenizer or language code prepending bug.
- First 3 experiments: 1. Train a monolingual model on a single locale and compare PER/WER to multilingual baseline. 2. Remove sentence-level data and retrain; measure impact on homograph disambiguation accuracy. 3. Swap language codes in input and observe if model generates pronunciations for the wrong language.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively balance word-level and sentence-level data for languages with complex post-lexical rules like French?
- Basis in paper: The paper notes that the multilingual model performs worse on French post-lexical rules compared to the monolingual model, possibly due to limited entries affected by PLRs in the training data for the multilingual scenario.
- Why unresolved: The paper does not provide a clear solution or methodology for balancing these two types of data to improve PLR handling in multilingual models.
- What evidence would resolve it: Experiments comparing different ratios of word-level to sentence-level data for French PLRs, measuring PER/WER on test sets with varying proportions of PLR-affected words.

### Open Question 2
- Question: What is the minimum amount of data required to effectively learn pronunciation for low-resource languages using the multilingual model?
- Basis in paper: The paper mentions exploring low-resource language expansion as a potential future direction, but does not provide specific findings or thresholds for minimum data requirements.
- Why unresolved: The paper does not present experiments or analysis on low-resource languages, making it unclear how much data is necessary for effective pronunciation learning in such cases.
- What evidence would resolve it: Comparative studies training the model on varying amounts of data for low-resource languages, measuring performance metrics like PER and WER to determine minimum effective data thresholds.

### Open Question 3
- Question: How can the model's performance on polyphone disambiguation be improved for Mandarin, particularly in the multilingual setting?
- Basis in paper: The paper shows that the monolingual model outperforms the multilingual model on Mandarin polyphone disambiguation, despite the multilingual model's overall competitive performance across tasks.
- Why unresolved: The paper does not provide insights into why Mandarin specifically does not benefit from multilingual knowledge or what techniques could improve the multilingual model's performance on this task.
- What evidence would resolve it: Experiments testing different approaches to incorporating multilingual knowledge for Mandarin, such as task-specific loss functions, data augmentation techniques, or model architecture modifications tailored to Mandarin's unique characteristics.

## Limitations
- The multilingual model performs slightly worse on specific language-specific tasks (Mandarin polyphone disambiguation, French post-lexical rules) compared to monolingual solutions
- Data quality and availability trade-offs are not fully quantified, making it unclear how data scarcity for certain locales impacts performance
- Standard evaluation metrics don't capture perceptual quality aspects of pronunciation that would matter in production TTS systems

## Confidence
- **High Confidence**: The core mechanism of using language codes as input prefixes for multilingual conditioning is well-supported by the results and methodology.
- **Medium Confidence**: The claim that context from sentence-level data improves homograph disambiguation is supported by the evaluation results, but the magnitude of improvement isn't directly measured.
- **Low Confidence**: The assertion that pronunciation knowledge is partially transferable across languages is plausible given the results, but the paper doesn't provide ablation studies showing which specific language pairs benefit most.

## Next Checks
1. **Ablation study on data types**: Train separate models using only word-level data, only sentence-level data, and the combined multilingual approach. Measure the specific contribution of sentence-level context to homograph disambiguation accuracy across different languages.

2. **Language-pair transfer analysis**: Train the model with different subsets of languages (e.g., Romance languages only, Germanic languages only, all languages) and measure how the choice of training languages affects performance on target languages, particularly for polyphone disambiguation in Mandarin and post-lexical rules in French.

3. **Error analysis on failure cases**: Conduct a detailed analysis of the specific homograph and polyphone disambiguation errors to determine whether failures are due to insufficient context, incorrect language conditioning, or fundamental limitations in the model's ability to capture certain pronunciation patterns.