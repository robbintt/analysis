---
ver: rpa2
title: Rethinking the Role of Token Retrieval in Multi-Vector Retrieval
arxiv_id: '2304.01982'
source_url: https://arxiv.org/abs/2304.01982
tags:
- token
- retrieval
- document
- tokens
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper rethinks token retrieval in multi-vector retrieval models,
  introducing XTR (ConteXualized Token Retriever). Unlike existing approaches that
  require gathering all document tokens for scoring, XTR trains the token retrieval
  stage to prioritize important document tokens, enabling direct scoring using only
  retrieved tokens.
---

# Rethinking the Role of Token Retrieval in Multi-Vector Retrieval

## Quick Facts
- arXiv ID: 2304.01982
- Source URL: https://arxiv.org/abs/2304.01982
- Reference count: 40
- XTR advances state-of-the-art by 2.8 nDCG@10 without distillation on BEIR

## Executive Summary
This paper rethinks token retrieval in multi-vector retrieval models, introducing XTR (ConteXualized Token Retriever). Unlike existing approaches that require gathering all document tokens for scoring, XTR trains the token retrieval stage to prioritize important document tokens, enabling direct scoring using only retrieved tokens. This eliminates the expensive gathering stage and reduces scoring complexity by two to three orders of magnitude. XTR introduces a novel training objective that encourages retrieval of relevant document tokens and proposes missing similarity imputation to account for non-retrieved tokens during inference.

## Method Summary
XTR modifies the traditional multi-vector retrieval pipeline by focusing on improving the token retrieval stage. During training, it uses a novel objective that simulates token retrieval by setting alignment scores to 1 only when tokens are retrieved in the top-k. This encourages the model to retrieve important document tokens first. For inference, XTR uses missing similarity imputation to approximate scores for non-retrieved tokens, allowing scoring using only the retrieved tokens without accessing all document vectors. The model is based on a T5 encoder and uses ScaNN for efficient MIPS-based token retrieval.

## Key Results
- XTR advances state-of-the-art by 2.8 nDCG@10 without distillation on BEIR benchmark
- XTR's scoring stage is 4000× faster than ColBERT
- XTR demonstrates much better recall of relevant tokens compared to ColBERT
- Better contextual matching with lower reliance on lexical matching

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: XTR's training objective directly optimizes token retrieval quality by simulating the token retrieval stage during training.
- **Mechanism**: Instead of training with the sum-of-max operator that only considers document-level scores, XTR modifies the alignment matrix to only consider token similarities when tokens are retrieved in the top-k. This encourages the model to retrieve important document tokens first.
- **Core assumption**: If token retrieval is trained to prioritize relevant document tokens, the scoring stage can be simplified to use only retrieved tokens without significant performance loss.
- **Evidence anchors**: [abstract] introduces a simple, yet novel, objective function that encourages the model to retrieve the most important document tokens first; [section] We simulate the token retrieval stage during training by using a different alignment matrix A.
- **Break condition**: If the token retrieval stage fails to retrieve relevant tokens even with the new objective, the simplified scoring stage would perform poorly.

### Mechanism 2
- **Claim**: XTR's missing similarity imputation allows scoring using only retrieved tokens while approximating the original sum-of-max behavior.
- **Mechanism**: During inference, when some query tokens don't retrieve tokens from a candidate document, XTR imputes missing similarity scores using the k′-th retrieval score as an upper bound. This allows reusing retrieval scores without loading all document vectors.
- **Core assumption**: The missing similarity can be reasonably estimated from the token retrieval stage scores, and this estimation is sufficient for accurate ranking.
- **Evidence anchors**: [abstract] proposes missing similarity imputation to account for non-retrieved tokens during inference; [section] For every candidate document ˆD, we first define the following scoring function for the inference: fXTR′(Q, ˆD)= 1/n Σ max[Aijq⊗i dj+(1−Aij)mi].
- **Break condition**: If the imputed similarity values are consistently poor estimates of the true missing similarities, ranking quality would degrade significantly.

### Mechanism 3
- **Claim**: XTR achieves better contextual matching compared to lexical matching, improving retrieval effectiveness.
- **Mechanism**: By training with the in-batch token retrieval objective, XTR learns to match query tokens semantically rather than relying solely on exact lexical matches. This is evidenced by lower lexical matching probabilities but better gold token retrieval.
- **Core assumption**: Semantic matching is more effective than lexical matching for information retrieval, especially for out-of-domain datasets.
- **Evidence anchors**: [abstract] Detailed analysis confirms our decision to revisit the token retrieval stage, as XTR demonstrates much better recall of the token retrieval stage compared to ColBERT; [section] XTR effectively lowers the reliance on the lexical matching while preserving a good amount of lexical precision.
- **Break condition**: If datasets heavily rely on exact lexical matching, XTR's semantic approach might underperform compared to lexical-focused methods.

## Foundational Learning

- **Concept**: Maximum Inner Product Search (MIPS)
  - Why needed here: XTR uses MIPS algorithms for efficient token retrieval during both training and inference stages
  - Quick check question: What is the computational complexity of MIPS algorithms compared to exhaustive search for finding the top-k most similar vectors?

- **Concept**: Cross-entropy loss with in-batch negatives
  - Why needed here: XTR uses this loss function with its modified scoring function to train the model to discriminate between relevant and irrelevant document tokens
  - Quick check question: How does using in-batch negatives affect the gradient computation compared to using a fixed set of negatives?

- **Concept**: Vector quantization and embedding storage
  - Why needed here: XTR stores token-level embeddings and needs to efficiently retrieve and score them, making storage and retrieval efficiency critical
  - Quick check question: What are the tradeoffs between storage cost and retrieval accuracy when using different vector quantization techniques?

## Architecture Onboarding

- **Component map**: Encoder -> Token Retrieval (MIPS) -> Scoring (with imputation) -> Document Ranking
- **Critical path**: Query → Token Encoding → Token Retrieval (MIPS) → Scoring (with imputation) → Document Ranking
- **Design tradeoffs**:
  - k′ (inference token retrieval count) vs. computational cost: Larger k′ improves recall but increases scoring cost
  - ktrain (training token retrieval count) vs. model effectiveness: Larger ktrain provides better supervision but requires larger batch sizes
  - Token embedding dimensionality vs. storage cost: Higher dimensions improve expressiveness but increase storage requirements
- **Failure signatures**:
  - Poor retrieval recall: Model fails to retrieve relevant tokens even with large k′
  - Inaccurate imputation: Missing similarity estimates are poor, leading to ranking errors
  - Training instability: Model fails to converge with the in-batch token retrieval objective
- **First 3 experiments**:
  1. Verify token retrieval recall improves with the new training objective by comparing gold token retrieval probability against baseline
  2. Test different k′ values to find the optimal tradeoff between recall and computational cost
  3. Validate the missing similarity imputation by comparing scores with and without imputation on a held-out set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of ktrain during training for different datasets and query lengths?
- Basis in paper: [explicit] The paper shows that XTR with different ktrain values (32, 64, 128, 256, 320) achieves different performances on MS MARCO and ArguAna, and hypothesizes that longer query lengths in ArguAna make XTR with larger ktrain fail.
- Why unresolved: The paper only provides a qualitative analysis and does not provide a systematic study of the relationship between ktrain and query length or dataset characteristics.
- What evidence would resolve it: A comprehensive ablation study varying ktrain across different datasets with varying query lengths, identifying the optimal ktrain for each scenario.

### Open Question 2
- Question: How does the choice of imputation method affect XTR's performance compared to the top-k' score imputation method?
- Basis in paper: [explicit] The paper mentions that using a fixed constant for imputation (e.g., mi=0.2) does not perform well, and that more sophisticated imputation methods (e.g., power-law based estimation) slightly improve performance, but uses the top-k' score imputation due to its simplicity.
- Why unresolved: The paper does not provide a detailed comparison of different imputation methods or explore the trade-off between imputation accuracy and computational efficiency.
- What evidence would resolve it: A systematic comparison of different imputation methods (e.g., fixed constant, power-law, learned imputation) on various datasets, measuring both retrieval performance and computational cost.

### Open Question 3
- Question: What is the impact of XTR's improved token retrieval on downstream tasks beyond retrieval, such as passage re-ranking or question answering?
- Basis in paper: [inferred] The paper focuses on XTR's retrieval performance, but the improved token retrieval could potentially benefit other tasks that rely on retrieved passages.
- Why unresolved: The paper does not explore the impact of XTR's token retrieval on tasks beyond retrieval.
- What evidence would resolve it: Experiments evaluating XTR's performance on downstream tasks like passage re-ranking and question answering, comparing it to other retrieval models and analyzing the contribution of improved token retrieval to these tasks.

## Limitations
- Reliance on strong assumptions about token retrieval effectiveness across diverse datasets
- Missing similarity imputation introduces approximation errors that could accumulate
- Computational efficiency claims based on specific hardware configurations may not translate universally

## Confidence

**High Confidence Claims:**
- The theoretical framework for simplifying multi-vector retrieval by eliminating the gathering stage is sound
- XTR's scoring complexity is significantly reduced compared to ColBERT (two to three orders of magnitude)
- The novel training objective for token retrieval is correctly formulated and implemented

**Medium Confidence Claims:**
- The 2.8 nDCG@10 improvement on BEIR represents a consistent advance over baselines
- Missing similarity imputation adequately approximates the original sum-of-max scoring behavior
- Better contextual matching translates to improved retrieval effectiveness across all evaluated datasets

**Low Confidence Claims:**
- The semantic matching approach will generalize to all information retrieval tasks
- The optimal k′ values for different model sizes are universally applicable
- The efficiency gains will scale linearly with larger document collections

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate XTR on additional information retrieval benchmarks beyond BEIR, particularly datasets with different characteristics (e.g., legal, medical, or scientific domains) to verify the claimed improvement in contextual matching generalizes beyond the tested datasets.

2. **Ablation Study on Missing Similarity Imputation**: Conduct a systematic ablation study comparing XTR's ranking performance with different imputation strategies (top-k′, mean imputation, zero imputation) and with perfect imputation (where all token similarities are available) to quantify the impact of approximation errors on retrieval quality.

3. **Efficiency Scalability Analysis**: Measure XTR's actual memory usage and query latency on document collections of increasing size (10M, 100M, 1B documents) to validate whether the claimed efficiency gains scale linearly and identify potential bottlenecks in real-world deployment scenarios.