---
ver: rpa2
title: Adaptive Optimization Algorithms for Machine Learning
arxiv_id: '2311.10203'
source_url: https://arxiv.org/abs/2311.10203
tags:
- local
- newton
- convergence
- algorithm
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis explores adaptivity in machine learning optimizers,
  focusing on five key areas: hyperparameter optimization, geometric adaptation, high-dimensional
  scalability, data heterogeneity, and user/client personalization. The work introduces
  novel algorithms with improved convergence guarantees and analyzes popular practical
  algorithms.'
---

# Adaptive Optimization Algorithms for Machine Learning

## Quick Facts
- arXiv ID: 2311.10203
- Source URL: https://arxiv.org/abs/2311.10203
- Reference count: 40
- This thesis introduces novel adaptive optimization algorithms with improved convergence guarantees across five key areas: hyperparameter optimization, geometric adaptation, high-dimensional scalability, data heterogeneity, and user/client personalization.

## Executive Summary
This thesis explores adaptivity in machine learning optimizers, introducing novel algorithms that achieve improved convergence guarantees across five key areas. The work develops theoretical frameworks for hyperparameter optimization through adaptive batch size learning, geometric adaptation via affine-invariant Newton methods, and high-dimensional scalability using sketch-and-project techniques. The research provides both theoretical analysis and empirical validation of these methods, demonstrating their effectiveness across various machine learning scenarios including federated learning and meta-learning.

## Method Summary
The thesis presents five main algorithmic contributions: an adaptive batch size learning algorithm for SGD that estimates optimal minibatch sizes online, an affine-invariant cubic Newton method achieving O(k^-2) global convergence, a sketch-and-project Newton method (SGN) for high-dimensional problems with low-rank updates, lower bounds and optimal algorithms for personalized federated learning, and improved convergence guarantees for first-order meta-learning algorithms. Each method addresses specific limitations in existing optimization approaches while maintaining theoretical rigor through convergence analysis and practical implementation considerations.

## Key Results
- The adaptive batch size algorithm learns optimal minibatch sizes online, achieving near-optimal performance without prior knowledge of the optimal size
- SGN achieves global O(k^-2) convergence for self-concordant functions using low-rank updates, making it scalable to high-dimensional problems
- Lower bounds demonstrate the optimality of local methods for federated learning with heterogeneous data distributions
- FO-MAML can be viewed as a solver for a personalized objective, leading to faster convergence in meta-learning

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Batch Size Learning
- **Claim:** The algorithm learns the optimal minibatch size τ* online, adapting to the current estimate of the model parameters.
- **Mechanism:** At each iteration k, the algorithm estimates the gradient variance at the optimum σ(xk, τk) by using the current gradient noise σ(xk, τk). This estimate, combined with the expected smoothness L(τk), is used to compute the stepsize γk and the next estimate for the optimal minibatch size τk+1(xk). The SGD step is then performed using this adaptive batch size.
- **Core assumption:** The gradient variance at the optimum σ(x*) is finite and can be approximated by the current gradient noise σ(xk, τk) as xk gets closer to x*.
- **Evidence anchors:**
  - [abstract]: "Our method does this provably, and in our experiments with synthetic and real data robustly exhibit nearly optimal behavior; that is, it works as if the optimal minibatch size was known a-priori."
  - [section]: "Although Algorithm 1 is a heuristic, we provide a stepsize bound and convergence guarantees under the assumption that gradient variance is bounded by a constant C."
  - [corpus]: Missing explicit comparison to existing adaptive batch size methods; assumes novelty without citing related work.
- **Break condition:** If the gradient variance at the optimum is infinite or cannot be approximated by the current gradient noise, the algorithm will fail to learn the optimal batch size.

### Mechanism 2: Affine-Invariant Geometry
- **Claim:** The algorithm preserves affine invariance throughout the convergence, making it independent of the choice of basis.
- **Mechanism:** The algorithm uses local Hessian norms ∥h∥x = ⟨∇2f(x)h, h⟩1/2, which are affine-invariant. This means that the level sets {y ∈ Rd | ∥y − x∥2x ≤ c} are balls centered around x, regardless of the choice of basis. In contrast, the standard Euclidean norm ∥h∥I is not affine-invariant.
- **Core assumption:** The function f is twice differentiable with a positive definite Hessian.
- **Evidence anchors:**
  - [abstract]: "Our method is based on the work of Nesterov and Polyak (2006), which demonstrated that the incorporation of cubic regularization can facilitate fast O(k−2) global convergence, albeit at the cost of a more complex algorithm structure and implicit subproblem to be solved in each iteration."
  - [section]: "One of the main geometric properties of the Newton method is its affine invariance, i.e., invariance to affine transformations of variables."
  - [corpus]: Missing explicit comparison to other affine-invariant methods; assumes novelty without citing related work.
- **Break condition:** If the function f is not twice differentiable or has a non-positive definite Hessian, the algorithm will fail to preserve affine invariance.

### Mechanism 3: Sketch-and-Project with Low-Rank Updates
- **Claim:** The algorithm achieves global O(k−2) convergence with low-rank updates, making it scalable to high-dimensional problems.
- **Mechanism:** The algorithm uses a sketch-and-project approach, projecting updates of the Newton method onto random low-dimensional subspaces. This allows the algorithm to use cheap, τ-dimensional updates, where τ is the rank of the sketching matrix. The algorithm also uses a damped Newton method with a stepsize schedule, which preserves affine invariance and achieves fast global convergence.
- **Core assumption:** The function f is convex and has a Lipschitz-continuous Hessian.
- **Evidence anchors:**
  - [abstract]: "Our algorithm accommodates the strengths of three distinct lines of research (as outlined in Table 1.2), and can be expressed in three equivalent formulations."
  - [section]: "Our algorithm combines the best of three worlds (Table 6.2) and we can write it in three different ways."
  - [corpus]: Missing explicit comparison to other sketch-and-project methods; assumes novelty without citing related work.
- **Break condition:** If the function f is not convex or does not have a Lipschitz-continuous Hessian, the algorithm will fail to achieve global O(k−2) convergence.

## Foundational Learning

- **Concept: Expected Smoothness**
  - **Why needed here:** Expected smoothness is a key assumption used to analyze the convergence of SGD with minibatch sampling. It provides a bound on the second moment of the stochastic gradient, which is crucial for deriving convergence rates.
  - **Quick check question:** Can you explain the difference between expected smoothness and the standard smoothness assumption used in SGD analysis?

- **Concept: Semi-Strong Self-Concordance**
  - **Why needed here:** Semi-strong self-concordance is a geometric property of the function f that is used to analyze the convergence of second-order methods. It provides a bound on the third-order derivatives of the function, which is crucial for deriving fast global convergence rates.
  - **Quick check question:** Can you explain the difference between semi-strong self-concordance and the standard self-concordance assumption used in interior-point methods?

- **Concept: Moreau Envelopes**
  - **Why needed here:** Moreau envelopes are used to formulate the meta-learning problem in a way that allows for the application of first-order optimization methods. They provide a smooth approximation of the original problem, which is crucial for deriving convergence rates.
  - **Quick check question:** Can you explain the relationship between Moreau envelopes and the original meta-learning problem?

## Architecture Onboarding

- **Component map:** SGD with Adaptive Batch Size -> Affine-Invariant Cubic Newton -> Sketch-and-Project with Low-Rank Updates
- **Critical path:** The critical path for implementing this architecture is to first implement the SGD with Adaptive Batch Size algorithm, then integrate the Affine-Invariant Cubic Newton method, and finally add the Sketch-and-Project with Low-Rank Updates component.
- **Design tradeoffs:** The main design tradeoff is between the speed of convergence and the computational cost per iteration. The adaptive batch size learning algorithm may require more iterations to converge, but each iteration is computationally cheap. The second-order methods may converge faster, but each iteration is computationally expensive.
- **Failure signatures:** If the algorithm fails to learn the optimal batch size, it may converge slowly or get stuck in a suboptimal solution. If the affine-invariance is not preserved, the algorithm may be sensitive to the choice of basis. If the sketch-and-project approach fails, the algorithm may not scale to high-dimensional problems.
- **First 3 experiments:**
  1. Implement the SGD with Adaptive Batch size algorithm on a simple convex optimization problem and verify that it learns the optimal batch size.
  2. Integrate the Affine-Invariant Cubic Newton method and compare its convergence rate to the adaptive batch size algorithm.
  3. Add the Sketch-and-Project with Low-Rank Updates component and evaluate its scalability to high-dimensional problems.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the geometric adaptivity properties of SGN be further improved to achieve superlinear convergence rates in local norms?
- **Basis in paper:** [inferred] The paper shows that SGN achieves local linear convergence rate independent of the condition number and discusses the limitations of achieving superlinear rates due to the sketch-and-project nature of the algorithm.
- **Why unresolved:** While the paper demonstrates the benefits of affine-invariance and geometric properties, it also highlights that achieving superlinear convergence with sketch-and-project methods is fundamentally limited.
- **What evidence would resolve it:** Developing a new variant of SGN or a related algorithm that can provably achieve superlinear convergence rates in local norms while maintaining the benefits of sketch-and-project methods would resolve this question.

### Open Question 2
- **Question:** How can the convergence guarantees of SGN be extended to handle non-convex objective functions?
- **Basis in paper:** [inferred] The paper focuses on convex objective functions and discusses the limitations of existing second-order methods in handling non-convexity, particularly when initialized far from the solution.
- **Why unresolved:** While the paper demonstrates strong convergence results for convex functions, extending these guarantees to the more challenging non-convex setting remains an open problem.
- **What evidence would resolve it:** Proving global convergence rates or identifying conditions under which SGN can find local minima for non-convex functions would provide evidence to resolve this question.

### Open Question 3
- **Question:** Can the adaptive batch size learning algorithm be extended to handle more complex sampling strategies and objective functions beyond finite-sum empirical risk minimization?
- **Basis in paper:** [explicit] The paper develops an adaptive batch size learning algorithm for SGD and discusses its effectiveness for various sampling strategies and machine learning models, but acknowledges the limitations of the current approach.
- **Why unresolved:** While the algorithm shows promising results, extending it to handle more complex scenarios, such as non-smooth objectives or distributed optimization with heterogeneous data, requires further research.
- **What evidence would resolve it:** Demonstrating the effectiveness of the adaptive batch size learning algorithm on a wider range of objective functions and sampling strategies, along with theoretical guarantees for these extensions, would resolve this question.

## Limitations

- The assumption of bounded gradient variance in the adaptive batch size algorithm has limited empirical verification beyond synthetic settings
- The semi-strong self-concordance assumption for high-dimensional scalability has not been tested on practical machine learning objectives like deep neural networks
- The communication complexity lower bounds for federated learning assume worst-case data heterogeneity distributions that may not reflect real-world federated learning scenarios

## Confidence

- **High confidence** in the convergence guarantees for the damped Newton method (AICN) with affine-invariant properties, supported by established theory from Nesterov and Polyak (2006) and consistent empirical results
- **Medium confidence** in the sketch-and-project Newton method (SGN) for high-dimensional scalability, as the theoretical analysis assumes specific matrix sketching distributions that may not translate directly to practical implementations
- **Medium confidence** in the adaptive batch size algorithm's ability to learn optimal minibatch sizes in practice, as the current analysis relies on strong assumptions about gradient variance that may not hold for complex, non-convex objectives

## Next Checks

1. **Empirical robustness test**: Evaluate the adaptive batch size algorithm across diverse loss landscapes (convex, non-convex, ill-conditioned) to verify whether the assumption of bounded gradient variance at the optimum holds in practice

2. **Sketching distribution impact**: Systematically vary the sketching distribution parameters in SGN and measure the effect on convergence rates and approximation quality, comparing theoretical predictions with empirical observations

3. **Communication lower bound verification**: Design synthetic federated learning scenarios that progressively increase data heterogeneity and measure whether the communication complexity of practical algorithms aligns with the theoretical lower bounds