---
ver: rpa2
title: 'Supervised Learning and Large Language Model Benchmarks on Mental Health Datasets:
  Cognitive Distortions and Suicidal Risks in Chinese Social Media'
arxiv_id: '2309.03564'
source_url: https://arxiv.org/abs/2309.03564
tags:
- language
- large
- social
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the performance of supervised learning models
  (BERT and LSAN) and large language models (LLMs) on two Chinese social media tasks:
  suicide risk classification and cognitive distortion detection. We compared zero-shot,
  few-shot, and fine-tuning approaches across multiple LLMs (GPT-3.5, GPT-4, GLM-130B,
  ChatGLM2-6B).'
---

# Supervised Learning and Large Language Model Benchmarks on Mental Health Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media

## Quick Facts
- arXiv ID: 2309.03564
- Source URL: https://arxiv.org/abs/2309.03564
- Reference count: 40
- Supervised learning models outperform zero-shot and few-shot LLMs on mental health classification tasks

## Executive Summary
This study evaluates the performance of supervised learning models (BERT and LSAN) and large language models (LLMs) on two Chinese social media tasks: suicide risk classification and cognitive distortion detection. The research compares zero-shot, few-shot, and fine-tuning approaches across multiple LLMs (GPT-3.5, GPT-4, GLM-130B, ChatGLM2-6B). Results demonstrate that supervised learning models significantly outperform zero-shot and few-shot LLMs on both tasks, though fine-tuning GPT-3.5 substantially reduces this performance gap, achieving comparable results to supervised learning on the suicide risk task.

## Method Summary
The study evaluates supervised learning models (BERT and LSAN) and large language models (GPT-3.5, GPT-4, GLM-130B, ChatGLM2-6B) on two novel Chinese social media datasets: SOS-HL-1K for suicide risk classification and SocialCD-3K for cognitive distortion detection. The evaluation framework includes zero-shot, few-shot, and fine-tuning approaches across all models. Performance is measured using precision, recall, and F1-score for both binary classification (suicide risk) and multi-label classification (cognitive distortions).

## Key Results
- Supervised learning models significantly outperform zero-shot and few-shot LLMs on both suicide risk and cognitive distortion tasks
- Fine-tuning GPT-3.5 achieves comparable performance to supervised learning on suicide risk classification (F1-score 78.44% vs 82.07%)
- The performance gap is more pronounced on the cognitive distortion task (31.53 percentage points in F1-score)
- Prompt engineering has limited impact on LLM performance for complex mental health tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised learning models (BERT and LSAN) outperform zero-shot and few-shot LLMs on mental health classification tasks.
- Mechanism: Supervised models are trained on labeled data with task-specific architectures, allowing them to learn nuanced category distinctions that generic LLMs miss without fine-tuning.
- Core assumption: The task categories (suicide risk levels, cognitive distortions) require fine-grained discrimination that cannot be captured by general language understanding alone.
- Evidence anchors:
  - [abstract] "Results show that supervised learning models outperform zero-shot and few-shot LLMs on both tasks."
  - [section] "The performance gap is more pronounced on the cognitive distortion task (31.53% points in F1-score)"

### Mechanism 2
- Claim: Fine-tuning GPT-3.5 significantly reduces the performance gap between LLMs and supervised learning.
- Mechanism: Fine-tuning adapts the pre-trained model weights to the specific distribution and classification requirements of the mental health tasks, effectively specializing the model.
- Core assumption: The pre-trained knowledge in GPT-3.5 can be effectively transferred and specialized through task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "However, fine-tuning GPT-3.5 significantly reduces the performance gap, achieving comparable results to supervised learning on the suicide risk task (F1-score 78.44% vs 82.07%)"
  - [section] "After fine-tuning, this difference is significantly reduced. In the suicide and cognitive distortion classification tasks, the gap decreases to 4.31% and 3.14%, respectively."

### Mechanism 3
- Claim: Prompt engineering (zero-shot and few-shot) has limited impact on LLM performance for complex mental health tasks.
- Mechanism: LLMs rely on pattern matching from their training data; without sufficient relevant examples or fine-tuning, they cannot accurately classify subtle mental health indicators.
- Core assumption: The complexity and nuance of mental health classification requires more than pattern matching from general language understanding.
- Evidence anchors:
  - [abstract] "there is still a huge gap between LLMs relying only on prompt engineering and supervised learning"
  - [section] "In the suicide classification task, this gap is 6.95% points in F1-score, while in the cognitive distortion task, the gap is even more pronounced, reaching 31.53% points in F1-score."

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: Cognitive distortion detection requires identifying multiple simultaneous distortions in a single text
  - Quick check question: Can a single social media post contain both "over-generalization" and "emotional reasoning" distortions?

- Concept: Fine-tuning mechanisms
  - Why needed here: Understanding how supervised learning models adapt to specific tasks is crucial for interpreting the performance improvements
  - Quick check question: What happens to a model's weights during fine-tuning versus pre-training?

- Concept: Prompt engineering strategies
  - Why needed here: Different prompt types (basic, role-definition, scene-definition, hybrid) yield varying performance results
  - Quick check question: How does adding "you are a psychologist" to a prompt potentially improve mental health classification accuracy?

## Architecture Onboarding

- Component map: Data collection (Weibo posts) -> Annotation (psychologist labeling) -> Model training (BERT, LSAN, LLMs) -> Evaluation (precision, recall, F1-score) -> Analysis

- Critical path: Data → Annotation → Model Training → Evaluation → Analysis

- Design tradeoffs:
  - Model size vs. deployment cost (GLM-130B vs. ChatGLM2-6B)
  - Zero-shot vs. fine-tuning (generalization vs. specialization)
  - Token limits vs. prompt complexity (affecting few-shot performance)

- Failure signatures:
  - Poor performance on cognitive distortion task suggests model struggles with multi-label classification
  - Inconsistent few-shot performance indicates prompt design sensitivity
  - Large gap between zero-shot and fine-tuned results shows pre-training limitations

- First 3 experiments:
  1. Run zero-shot basic prompt on both tasks to establish baseline LLM performance
  2. Fine-tune GPT-3.5 on suicide risk classification and compare to supervised baselines
  3. Test hybrid prompt with background knowledge on cognitive distortion task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned GPT-3.5 on the cognitive distortion task compare to supervised learning models?
- Basis in paper: [explicit] The paper states that fine-tuning GPT-3.5 reduced the performance gap but did not eliminate it, with an F1-score of 6.78% compared to 78.91% for BERT on the cognitive distortion task.
- Why unresolved: The study focused on comparing zero-shot and few-shot approaches with supervised learning, but did not extensively explore the potential of fine-tuning for the cognitive distortion task.
- What evidence would resolve it: Additional experiments fine-tuning GPT-3.5 on the cognitive distortion task with various training strategies and prompt designs could provide insights into its performance relative to supervised learning models.

### Open Question 2
- Question: What are the optimal prompt engineering techniques for large language models in mental health-related tasks?
- Basis in paper: [explicit] The study explored different prompt strategies, including zero-shot, few-shot, and hybrid approaches, but found that the effectiveness varied depending on the task and model size.
- Why unresolved: The paper acknowledges the need for task-specific customization and further investigation into fine-tuning mechanisms for complex tasks, suggesting that optimal prompt engineering techniques are not yet fully understood.
- What evidence would resolve it: Systematic experimentation with various prompt designs, training strategies, and model sizes across different mental health-related tasks could identify the most effective prompt engineering techniques.

### Open Question 3
- Question: How do the performance of large language models on Chinese social media data compare to English social media data for mental health-related tasks?
- Basis in paper: [explicit] The study focused on Chinese social media data, but the authors mention that most existing research on large language models for mental health-related tasks has been conducted on English data.
- Why unresolved: The paper does not provide a direct comparison between the performance of large language models on Chinese and English social media data for mental health-related tasks.
- What evidence would resolve it: Conducting similar experiments on English social media data and comparing the results with those obtained in the study could provide insights into the performance differences between the two languages.

## Limitations
- The study focuses exclusively on Chinese social media data from Weibo, limiting generalizability to other languages and cultural contexts
- Relatively small dataset sizes (1,249 posts for suicide risk, 3,407 posts for cognitive distortions) raise concerns about model robustness and potential overfitting
- The comparison between supervised learning and LLM approaches is complicated by the fact that supervised models were likely trained on substantially more data than the few-shot examples used for LLMs

## Confidence

**High Confidence Claims:**
- Supervised learning models outperform zero-shot and few-shot LLMs on both mental health tasks
- Fine-tuning GPT-3.5 significantly improves performance compared to prompt-based approaches
- Cognitive distortion detection is more challenging than suicide risk classification for all model types

**Medium Confidence Claims:**
- The performance gap between supervised learning and fine-tuned LLMs is task-dependent
- Prompt engineering has limited impact on complex mental health classification tasks
- Multi-label classification presents greater challenges than binary classification for LLMs

**Low Confidence Claims:**
- Specific performance percentages may vary significantly with different dataset splits or hyperparameter choices
- The relative effectiveness of different prompt engineering strategies across tasks
- Generalizability of findings to other mental health classification tasks or languages

## Next Checks
1. **Cross-linguistic validation**: Replicate the study using English-language mental health datasets (such as those from Reddit or Twitter) to test whether the observed performance patterns hold across languages.

2. **Dataset size sensitivity analysis**: Systematically vary the training dataset sizes for both supervised learning models and fine-tuned LLMs to determine the data efficiency threshold where LLMs begin to match supervised learning performance.

3. **Annotation agreement verification**: Conduct an independent annotation study with multiple psychologists labeling a subset of the Weibo posts to establish inter-rater reliability scores.