---
ver: rpa2
title: What Learned Representations and Influence Functions Can Tell Us About Adversarial
  Examples
arxiv_id: '2309.10916'
source_url: https://arxiv.org/abs/2309.10916
tags:
- adversarial
- examples
- influence
- training
- nnif
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper adapts two adversarial detection methods from image
  processing to NLP: nearest neighbor influence functions (NNIF) and Mahalanobis distance
  (MAHAL). NNIF combines influence functions with nearest neighbors to detect adversarial
  examples by identifying differences in learned representation subspaces.'
---

# What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples

## Quick Facts
- **arXiv ID**: 2309.10916
- **Source URL**: https://arxiv.org/abs/2309.10916
- **Reference count**: 40
- **Key outcome**: NNIF achieves 100% accuracy on character attacks and 90% on word attacks for sentiment analysis

## Executive Summary
This paper adapts two adversarial detection methods from image processing to NLP: nearest neighbor influence functions (NNIF) and Mahalanobis distance (MAHAL). NNIF combines influence functions with nearest neighbors to detect adversarial examples by identifying differences in learned representation subspaces. MAHAL uses Mahalanobis distances from Gaussian distributions over learned representations. Both methods achieve state-of-the-art results on sentiment analysis and natural language inference tasks against character-level and word-level attacks.

## Method Summary
The paper adapts NNIF by computing influence scores for training points, selecting top M helpful/harmful points, constructing a DKNN classifier with these points, and training a logistic regression detector using influence function and nearest neighbor features. MAHAL measures Mahalanobis distance between test samples and class-conditional Gaussian distributions. Both methods use BERTBASE as the target model, extracting hidden layer representations for detection.

## Key Results
- NNIF achieves 100% detection accuracy on character attacks for sentiment analysis
- MAHAL provides strong baseline performance with less computational overhead
- Influence functions provide unique information for detecting adversarial examples in complex NLP tasks
- Both methods outperform existing adversarial detection approaches in NLP

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: NNIF detects adversarial examples by comparing two views of "nearby" points: influence functions and nearest neighbors
- **Mechanism**: For normal examples, influential training samples and nearest neighbors overlap in learned representation space; for adversarial examples they diverge, creating a detectable pattern
- **Core assumption**: Adversarial examples occupy distinct subspaces in learned representation space that can be characterized by differences in training point proximity
- **Evidence anchors**: [abstract] "NNIF combines influence functions with nearest neighbors to detect adversarial examples by identifying differences in learned representation subspaces"

### Mechanism 2
- **Claim**: Mahalanobis distance detection works by measuring deviation from class-conditional Gaussian distributions in learned representation space
- **Mechanism**: Computes Mahalanobis distance between test samples and closest class distribution; adversarial examples show larger distances from their true class distribution
- **Core assumption**: Adversarial perturbations push examples away from their original class distribution in learned representation space
- **Evidence anchors**: [abstract] "MAHAL uses Mahalanobis distances from Gaussian distributions over learned representations"

### Mechanism 3
- **Claim**: Influence functions provide unique information about adversarial subspaces in NLP that differs from image processing
- **Mechanism**: Influence scores identify training points most responsible for predictions; adversarial examples show different influence patterns than normal examples
- **Core assumption**: The nature of influential training points differs systematically between normal and adversarial examples, and this difference varies by NLP task complexity
- **Evidence anchors**: [abstract] "the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing"

## Foundational Learning

- **Concept**: Learned representation subspaces
  - **Why needed here**: Detection methods rely on characterizing how adversarial examples occupy different regions of the learned representation space than normal examples
  - **Quick check question**: Can you explain how neural networks transform input text into learned representations and why these might differ for adversarial examples?

- **Concept**: Influence functions in machine learning
  - **Why needed here**: NNIF method uses influence functions to identify which training points most influence predictions, creating one view of "nearby" points for detection
  - **Quick check question**: What does it mean for a training point to have high influence on a test example's prediction, and how is this quantified?

- **Concept**: Mahalanobis distance and Gaussian distributions
  - **Why needed here**: MAHAL method measures Mahalanobis distance from class-conditional Gaussian distributions to detect adversarial examples
  - **Quick check question**: How does Mahalanobis distance differ from Euclidean distance, and why might it be more suitable for measuring distributional deviation?

## Architecture Onboarding

- **Component map**: Input text -> BERTBASE encoding -> Hidden layer extraction -> NNIF/MAHAL computation -> Feature generation -> Logistic regression -> Detection decision
- **Critical path**: Input text → BERTBASE encoding → Hidden layer extraction → NNIF/MAHAL computation → Feature generation → Logistic regression → Detection decision
- **Design tradeoffs**: NNIF offers higher accuracy but computational expense vs MAHAL offering good accuracy with less computation; both require significant training data
- **Failure signatures**: High false positive rates suggest learned representations poorly separate normal from adversarial examples; computational timeouts indicate scalability issues with influence function calculations
- **First 3 experiments**:
  1. Implement MAHAL detection on IMDB dataset with BERTBASE to establish baseline performance
  2. Add NNIF detection to compare accuracy and computational requirements against MAHAL
  3. Test detection on CHAR ATT attacks first, then WORDATT to evaluate performance across attack types

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What specific characteristics of NLP tasks (compared to image processing) lead to the need for larger M values in NNIF to achieve optimal performance?
- **Basis in paper**: [explicit] The paper notes that unlike image processing where smaller M values (e.g., 30) worked better, NLP tasks showed a monotonic increase in accuracy with larger M values
- **Why unresolved**: The paper observes this difference but does not provide a theoretical explanation for why NLP tasks require larger M values
- **What evidence would resolve it**: Comparative analysis of influence function distributions and neighbor overlaps across different NLP and image tasks, with varying M values

### Open Question 2
- **Question**: How would adding additional dense layers to the LLM-based model affect the performance of the Mahalanobis detector?
- **Basis in paper**: [inferred] The paper notes that feature ensemble improves MAHAL performance and that the Lee et al. (2018) target model had more hidden layers
- **Why unresolved**: The paper only analyzes the current architecture without exploring modifications to the target model structure
- **What evidence would resolve it**: Experiments comparing MAHAL performance across models with varying numbers of additional dense layers

### Open Question 3
- **Question**: Why do influence functions provide different perspectives for sentiment classification versus natural language inference tasks?
- **Basis in paper**: [explicit] The paper references Han et al. (2020) finding that IFs were consistent for sentiment classification but differed for NLI
- **Why unresolved**: While the observation is made, the paper does not explain the underlying reasons for this difference in IF behavior between tasks
- **What evidence would resolve it**: Systematic analysis of influence function distributions and their relationship to task complexity

## Limitations
- **Computational expense**: NNIF's influence function calculations are computationally expensive and may not scale well
- **Task generalization**: The methods are primarily validated on sentiment analysis and NLI, with uncertain performance on other NLP tasks
- **Attack vulnerability**: The effectiveness against adaptive attacks specifically designed to evade these detection methods is unknown

## Confidence
- **High confidence**: The adaptation of Mahalanobis distance from image processing to NLP is methodologically sound and the core mechanism is well-established
- **Medium confidence**: The specific claim about influence functions providing unique information for complex NLP tasks is supported by the NLI vs sentiment comparison but needs broader validation
- **Medium confidence**: The assertion that NNIF achieves 100% accuracy on character attacks is credible given the methodology but may not generalize across all datasets and attack variations

## Next Checks
1. Implement NNIF detection on a third NLP task (e.g., question answering or named entity recognition) to verify generalizability beyond sentiment analysis and NLI
2. Conduct ablation studies removing influence function features from NNIF to quantify their specific contribution to detection accuracy
3. Measure detection performance against adaptive attacks specifically designed to evade Mahalanobis distance-based detection methods