---
ver: rpa2
title: Towards Communication-Efficient Model Updating for On-Device Session-Based
  Recommendation
arxiv_id: '2308.12777'
source_url: https://arxiv.org/abs/2308.12777
tags:
- update
- compression
- recommendation
- on-device
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient model updating for
  on-device session-based recommendation systems, where cloud-based models are periodically
  retrained with new data and need to be updated on resource-constrained devices via
  network communication. The proposed solution uses compositional codes to compress
  item embeddings and introduces two novel approaches - stack-based and queue-based
  - to further compress model updates using traditional data structures.
---

# Towards Communication-Efficient Model Updating for On-Device Session-Based Recommendation

## Quick Facts
- arXiv ID: 2308.12777
- Source URL: https://arxiv.org/abs/2308.12777
- Reference count: 40
- Primary result: On-device models achieve comparable accuracy to server-side counterparts through transferring updates 60x smaller in size

## Executive Summary
This paper addresses the challenge of efficient model updating for on-device session-based recommendation systems, where cloud-based models are periodically retrained with new data and need to be updated on resource-constrained devices via network communication. The proposed solution uses compositional codes to compress item embeddings and introduces two novel approaches - stack-based and queue-based - to further compress model updates using traditional data structures. These approaches selectively transfer a subset of new parameters while utilizing previous knowledge, enabling progressive and self-adaptive parameter replacement. The effectiveness is validated across multiple session-based recommendation models (NARM, SR-GNN, SASRec) on two datasets (Gowalla, Lastfm), demonstrating that on-device models can achieve comparable accuracy to retrained server-side counterparts through transferring updates 60x smaller in size.

## Method Summary
The paper proposes a communication-efficient model updating framework for on-device session-based recommendation. It introduces compositional codes to compress item embeddings by representing each item with a unique n-dimensional code that indexes vectors from multiple codebooks, reducing storage requirements significantly. The framework further employs stack-based and queue-based data structures to compress model updates by selectively transferring and updating a subset of new parameters using Last-In-First-Out (LIFO) and First-In-First-Out (FIFO) principles. An adaptive compression ratio selection mechanism based on Maximum Mean Discrepancy (MMD) measures data distribution shifts between consecutive model versions, enabling flexible adjustment of update sizes. The approach is evaluated on three recommendation models (NARM, SR-GNN, SASRec) using Gowalla and Lastfm datasets, showing that on-device models can maintain comparable accuracy to server-side models while reducing communication overhead by up to 60x.

## Key Results
- On-device models achieve comparable accuracy to server-side models with 60x smaller update transfers
- Stack-based and queue-based update compression approaches outperform baseline methods
- MMD-based adaptive compression effectively balances update size and recommendation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compositional codes compress item embeddings by replacing full embeddings with short codes indexing vectors from multiple codebooks.
- Mechanism: Each item is represented by a unique n-dimensional code where each component indexes a vector from a corresponding codebook. The final embedding is the sum of these indexed vectors, reducing storage from |V|Ã—d to nÃ—kÃ—d + nÃ—|V|.
- Core assumption: The sum of codebook vectors can approximate the original item embeddings well enough for recommendation accuracy.
- Evidence anchors:
  - [abstract] "compositional codes to compress the model update" and "on-device model can achieve comparable accuracy to the retrained server-side counterpart through transferring an update 60x smaller in size"
  - [section] "Each item ğ‘£ is represented by a unique code ğ¶ğ‘£ = (ğ¶1ğ‘£, ğ¶2ğ‘£, ..., ğ¶ğ‘›ğ‘£ ) âˆˆ B ğ‘› where each component in the code is ranged in [1, ğ‘˜]" and "The final embedding of item ğ‘£ is: ğ’†ğ‘£ = âˆ‘ğ‘–=1ğ‘› ğ‘¬ğ‘– (ğ¶ğ‘–ğ‘£)"
  - [corpus] No direct evidence found in related papers - this appears to be novel to this paper
- Break condition: If the codebook size k is too small relative to |V|, unique codes cannot be assigned and embeddings become ambiguous.

### Mechanism 2
- Claim: Stack-based and queue-based data structures enable progressive parameter replacement during model updates.
- Mechanism: For stack-based approach, new parameters replace top elements of codebook stack (LIFO). For queue-based approach, new parameters replace front elements (FIFO). This allows selective transfer of ğ›½ parameters rather than full model.
- Core assumption: Replacing parameters using LIFO/FIFO principles maintains model performance while significantly reducing communication overhead.
- Evidence anchors:
  - [abstract] "two novel approaches for compressing model updates that integrate traditional data structures, specifically stack and queue" and "selectively transfer and update a subset of new parameters, using the principles of Last-In-First-Out (LIFO) and First-In-First-Out (FIFO)"
  - [section] "Imagine there is a stack as depicted in Fig. 3, and ğ‘¬ is pushed in as a whole at ğ‘‡1" and "we also recycle most of the parameters in the codebooks. The difference is that every time when new parameters are delivered to the device, the same number of existing parameters will be popped out from the front of the queue"
  - [corpus] No direct evidence found in related papers - this appears to be novel to this paper
- Break condition: If ğ›½ is too small, insufficient new knowledge is incorporated; if too large, communication benefits are lost.

### Mechanism 3
- Claim: Maximum Mean Discrepancy (MMD) enables self-adaptive update compression by measuring data distribution shift between consecutive model versions.
- Mechanism: MMD calculates the difference between item embedding distributions at consecutive time slices. Larger MMD indicates greater data shift, triggering larger update sizes through the formula ğ‘Ÿ = âŒˆ1/(ğ¶Ã—2ğœ(MMD(X(ğ‘¡),X(ğ‘¡+1)))âˆ’1)âŒ‰.
- Core assumption: MMD value correlates with the amount of parameter change needed to maintain recommendation quality.
- Evidence anchors:
  - [abstract] "Maximum Mean Discrepancy (MMD) metric [9] to measure the data distribution shift, which enables flexible adjustment of the update size"
  - [section] "we adopt Maximum Mean Discrepancy (MMD) to measure the data shift between the item embedding tables" and "A larger MMD indicates a substantial data shift between item embeddings, necessitating the consideration of a larger update size for stable recommendation quality"
  - [corpus] No direct evidence found in related papers - this appears to be novel to this paper
- Break condition: If data distribution changes are gradual, MMD may not capture subtle but important shifts requiring parameter updates.

## Foundational Learning

- Concept: Compositional codes and embedding compression
  - Why needed here: Traditional item embeddings require |V|Ã—d parameters which is prohibitive for on-device deployment; compositional codes reduce this to nÃ—kÃ—d + nÃ—|V| where n â‰ª d and k â‰ª |V|
  - Quick check question: If you have 10,000 items with 128-dimensional embeddings, and use compositional codes with n=20 and k=32, what's the compression ratio?

- Concept: Model update vs full model transfer
  - Why needed here: Retraining cloud models and transferring full models to devices is communication-intensive; updating only changed parameters dramatically reduces bandwidth requirements
  - Quick check question: If a compressed model requires 1MB but only 10% of parameters change, how much bandwidth is saved by transferring only updates versus the full model?

- Concept: Maximum Mean Discrepancy for distribution comparison
  - Why needed here: Need an automated way to determine how much of the model should be updated based on how much the data distribution has changed; MMD provides a principled metric for this
  - Quick check question: What would happen to the MMD value if two consecutive embedding tables are identical versus completely different?

## Architecture Onboarding

- Component map: Cloud server with full model â†’ Compositional coding compression â†’ Device with compressed model â†’ Progressive update compression using stack/queue â†’ Optional MMD-based self-adaptation
- Critical path: Data collection â†’ Cloud model retraining â†’ Compositional code generation â†’ Update compression selection â†’ Parameter transfer â†’ Device model reconstitution â†’ Inference
- Design tradeoffs: Stack-based preserves early knowledge better but may become stale; queue-based stays current but loses historical patterns; MMD adds complexity but enables adaptation
- Failure signatures: Model accuracy drops when ğ›½ is too small; communication overhead increases when ğ›½ approaches full model size; MMD may trigger unnecessary updates if data distribution fluctuates
- First 3 experiments:
  1. Validate compositional coding compression ratio and accuracy trade-off with fixed n and varying k on both datasets
  2. Compare stack-based vs queue-based update approaches with fixed compression ratio across all three base models
  3. Test MMD-based adaptive compression by artificially creating different data shift scenarios and measuring update size vs accuracy impact

## Open Questions the Paper Calls Out
No explicit open questions were called out in the paper.

## Limitations
- Performance heavily depends on the quality of compositional codes, which may struggle with very large vocabularies
- Stack and queue-based compression approaches assume predictable parameter replacement patterns that may not hold for all recommendation scenarios
- MMD-based adaptive compression relies on statistical distribution shifts that might not always correlate with actual recommendation quality changes

## Confidence

**High Confidence**: The core mechanism of compositional coding for embedding compression is well-established and the mathematical formulation is sound.

**Medium Confidence**: The stack/queue-based update compression approaches show promising results but may have dataset-specific limitations not fully explored.

**Low Confidence**: The MMD-based self-adaptive compression ratio selection needs more extensive validation across diverse data drift scenarios.

## Next Checks

1. Test the proposed approach on datasets with varying vocabulary sizes (100-10,000 items) to validate the scalability limits of compositional codes.

2. Evaluate performance under controlled data distribution shifts where ground truth parameter changes are known, to validate MMD's effectiveness as a proxy metric.

3. Compare the proposed update compression approaches against other compression techniques like quantization or pruning on the same recommendation tasks.