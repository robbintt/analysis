---
ver: rpa2
title: 'Stacked networks improve physics-informed training: applications to neural
  networks and deep operator networks'
arxiv_id: '2311.06483'
source_url: https://arxiv.org/abs/2311.06483
tags:
- stacking
- networks
- training
- physics-informed
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training physics-informed
  neural networks (PINNs) and physics-informed deep operator networks (PI-DeepONets)
  for complex physical systems where standard approaches can fail. The authors propose
  a novel multifidelity stacking framework that builds a chain of networks iteratively,
  where each network's output serves as a low-fidelity input for the next, progressively
  increasing the model's expressivity.
---

# Stacked networks improve physics-informed training: applications to neural networks and deep operator networks

## Quick Facts
- arXiv ID: 2311.06483
- Source URL: https://arxiv.org/abs/2311.06483
- Authors: 
- Reference count: 40
- Primary result: Stacked multifidelity networks improve physics-informed training for PINNs and PI-DeepONets, achieving better accuracy with fewer parameters than single fidelity approaches

## Executive Summary
This paper addresses the challenge of training physics-informed neural networks (PINNs) and physics-informed deep operator networks (PI-DeepONets) for complex physical systems where standard approaches can fail. The authors propose a novel multifidelity stacking framework that builds a chain of networks iteratively, where each network's output serves as a low-fidelity input for the next, progressively increasing the model's expressivity. The equations imposed at each stacking step can be the same or different, allowing for a gradual increase in problem complexity similar to simulated annealing.

The authors demonstrate their approach on benchmark problems including a nonlinear pendulum, the wave equation, and the viscous Burgers equation. For the pendulum problem, stacking PINNs successfully capture the dynamics where standard PINNs fail, achieving relative ℓ2 errors close to constant after 4-8 stacking steps. For a multiscale problem, stacking PINNs reach similar or better accuracy than single fidelity PINNs while using significantly fewer trainable parameters (61% in one case). For the wave equation, stacking PINNs achieve up to one order of magnitude reduction in error compared to single fidelity training. For the viscous Burgers equation, stacking PI-DeepONets with gradually decreasing viscosity achieve relative ℓ2 errors 3-4 times lower than single fidelity training.

## Method Summary
The stacking framework iteratively trains a sequence of multifidelity PINNs or PI-DeepONets. Starting with a single fidelity network (Step 0), each subsequent step trains a multifidelity network that takes the previous network's output as a low-fidelity input while enforcing the physics constraints as high-fidelity terms. The multifidelity loss combines initial/boundary conditions, physics residuals, and multifidelity terms with weighting parameters λic, λbc, and λr. Weights from the previous network initialize the current network through transfer learning, and parameters are frozen after each training step. The number of stacking levels is user-defined, with the option to use the same or progressively different equations at each level, similar to simulated annealing.

## Key Results
- For the nonlinear pendulum problem, stacking PINNs successfully capture dynamics where standard PINNs fail, achieving relative ℓ2 errors close to constant after 4-8 stacking steps
- For a multiscale problem, stacking PINNs reach similar or better accuracy than single fidelity PINNs while using 61% fewer trainable parameters
- For the wave equation, stacking PINNs achieve up to one order of magnitude reduction in error compared to single fidelity training
- For the viscous Burgers equation, stacking PI-DeepONets with gradually decreasing viscosity achieve relative ℓ2 errors 3-4 times lower than single fidelity training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The iterative stacking approach progressively refines the solution by using each network's output as low-fidelity input for the next network.
- Mechanism: At each stacking step, a multifidelity PINN/PI-DeepOnet takes the previous network's output as low-fidelity input. This creates a chain where each network refines the approximation, similar to simulated annealing where problem complexity is gradually increased.
- Core assumption: The previous network's output provides a sufficiently accurate low-fidelity approximation for the next network to build upon.
- Evidence anchors:
  - [abstract]: "We successively build a chain of networks, where the output at one step can act as a low-fidelity input for training the next step, gradually increasing the expressivity of the learned model."
  - [section]: "We iteratively train a multifidelity PINN/PI-DeepOnet for a user-defined number of steps, where the low-fidelity model at each step takes the output of the previous step as input."
- Break condition: If the previous network's output is too inaccurate, the next network may fail to converge or learn the correct dynamics.

### Mechanism 2
- Claim: Starting with simpler equations and gradually increasing complexity improves training for difficult problems.
- Mechanism: For the wave equation example, training begins with a lower wave speed (easier to train) and gradually increases to the target value. This allows the network to first learn a simpler, more stable solution before correcting to the complex target equation.
- Core assumption: Simpler versions of the target equation are easier to learn and provide stable starting points for refinement.
- Evidence anchors:
  - [section]: "When c is smaller, the PINN is much easier to train. This, by starting c small and increasing to our target value of c = 2 or c = 4, we hope to begin with a very accurate prediction, for a slightly modified equation, and then correct with the prediction for the correct equation."
  - [section]: "We have found that a well-trained low-fidelity model, possibly for a slightly different and simpler equation, can produce more robust results than training with the same equation for all stacking levels."
- Break condition: If the initial simpler equation is too different from the target, the network may learn incorrect dynamics that are hard to correct.

### Mechanism 3
- Claim: Using smaller networks in stacking reduces total trainable parameters while achieving similar or better accuracy than single large networks.
- Mechanism: Each stacking level uses a smaller network than would be needed for a single fidelity approach. The cumulative effect of stacking allows reaching the same or better accuracy with fewer total parameters.
- Core assumption: The sequential refinement process compensates for the smaller individual network sizes.
- Evidence anchors:
  - [section]: "Stacking PINNs can use smaller network sizes, reducing the total number of trainable parameters needed to reach a given error."
  - [section]: "Stacking PINNs allow for more expressive solutions and can train for cases where single fidelity PINNs and traditional multifidelity PINNs cannot reach a satisfactory solution."
- Break condition: If the problem requires high expressivity that smaller networks cannot provide, stacking may fail to achieve the target accuracy.

## Foundational Learning

- Concept: Physics-informed neural networks (PINNs)
  - Why needed here: The stacking method builds on PINN architecture, using their ability to incorporate physical laws directly into the loss function.
  - Quick check question: What are the three main components of the PINN loss function?

- Concept: Deep operator networks (DeepONets)
  - Why needed here: The stacking approach extends to DeepONets, which learn mappings between function spaces rather than single solutions.
  - Quick check question: How does a DeepONet differ from a standard neural network in terms of input and output structure?

- Concept: Multifidelity learning
  - Why needed here: The stacking method uses multifidelity networks at each level, combining low-fidelity (previous output) and high-fidelity (physics-informed) components.
  - Quick check question: What is the role of the α parameter in the multifidelity network formulation?

## Architecture Onboarding

- Component map: Step 0 (single fidelity PINN/DeepONet) -> Step i (multifidelity network with previous output as low-fidelity input) -> Transfer learning (weights from previous step initialize current step) -> Loss function (initial/boundary conditions + physics residuals + multifidelity terms)

- Critical path:
  1. Train single fidelity network (Step 0)
  2. For each stacking step:
     - Transfer weights from previous network
     - Train multifidelity network with previous output as low-fidelity input
     - Freeze parameters after training
  3. Evaluate final accuracy and decide if additional stacking levels are needed

- Design tradeoffs:
  - Smaller networks per step reduce memory usage but require sequential training
  - Fixed equations per step vs. gradually changing equations (simpler to complex)
  - Number of stacking levels vs. accuracy gains
  - Fixed vs. neural tangent kernel weights for loss function balancing

- Failure signatures:
  - Relative ℓ2 error plateaus at unsatisfactory value
  - Training loss decreases but validation error increases (overfitting)
  - Network fails to train at a particular stacking level (divergence)
  - Initial condition or boundary condition errors accumulate through stacking

- First 3 experiments:
  1. Reproduce the damped pendulum results from Fig. 3 to establish baseline PINN failure
  2. Implement stacking for the pendulum problem and verify improvement shown in Fig. 6
  3. Test the multiscale problem from Table 1 to compare parameter efficiency between single fidelity and stacking approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for changing the equations or parameters across stacking levels to minimize training time while maintaining accuracy?
- Basis in paper: [inferred] The paper mentions that starting with simpler equations and gradually increasing complexity can improve training, but doesn't provide a systematic method for determining the optimal progression.
- Why unresolved: The authors note that the rate at which the equation should change is an open problem and likely target application specific, suggesting this requires further investigation.
- What evidence would resolve it: Comparative studies showing the performance impact of different scheduling strategies (linear, exponential, adaptive) across various PDE problems, along with theoretical analysis of convergence rates.

### Open Question 2
- Question: How does the stacking approach perform when applied to different neural network architectures beyond feed-forward networks and DeepONets?
- Basis in paper: [explicit] The authors state "each block in the stacking setup could be replaced by different architectures" and suggest exploring convolutional neural networks or other operator learning methods.
- Why unresolved: The paper only demonstrates stacking with feed-forward networks and DeepONets, leaving the generalizability to other architectures unexplored.
- What evidence would resolve it: Empirical studies applying stacking to recurrent networks, graph neural networks, and other architectures for various physical systems, comparing performance to standard training.

### Open Question 3
- Question: What are the theoretical guarantees for convergence and generalization when using stacking PINNs/PI-DeepONets?
- Basis in paper: [inferred] While the paper demonstrates practical success, it doesn't provide theoretical analysis of why stacking works or under what conditions it guarantees convergence.
- Why unresolved: The paper focuses on empirical results rather than theoretical foundations, which is common in deep learning research but leaves important questions unanswered.
- What evidence would resolve it: Mathematical proofs establishing conditions for convergence, error bounds as a function of stacking depth, and generalization guarantees for the stacked approach.

## Limitations

- The approach requires careful hyperparameter tuning for each stacking level, and the optimal number of stacking steps is problem-dependent
- The method's effectiveness appears to rely heavily on the quality of intermediate solutions, which may not generalize well to highly chaotic or multi-scale systems
- The computational overhead of sequential training across multiple stacking levels may offset benefits for some applications

## Confidence

- High confidence: The fundamental mechanism of using previous outputs as low-fidelity inputs is theoretically sound and mathematically well-defined
- Medium confidence: The specific numerical improvements shown (e.g., 3-4x error reduction for Burgers equation) may be sensitive to problem setup and hyperparameters
- Medium confidence: The parameter efficiency claims assume optimal stacking configurations that may not be easy to discover for new problems

## Next Checks

1. Test stacking performance on a fourth, previously unseen physics problem (e.g., Navier-Stokes) to assess generalizability beyond the three benchmark problems
2. Compare training stability metrics (gradient norms, loss curves) between single fidelity and stacked approaches to understand convergence behavior
3. Perform ablation studies removing different components of the multifidelity loss function to quantify the contribution of each term