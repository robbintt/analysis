---
ver: rpa2
title: The Pros and Cons of Adversarial Robustness
arxiv_id: '2312.10911'
source_url: https://arxiv.org/abs/2312.10911
tags:
- robustness
- classi
- adversarial
- example
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically examines the widely accepted notion of machine
  learning (ML) model robustness, particularly focusing on adversarial examples and
  robustness certification. The authors argue that existing definitions of both local
  and global robustness are fundamentally flawed, as they lead to the conclusion that
  no non-trivial ML classifier can be truly robust when considering all possible inputs
  in real-valued feature spaces.
---

# The Pros and Cons of Adversarial Robustness

## Quick Facts
- arXiv ID: 2312.10911
- Source URL: https://arxiv.org/abs/2312.10911
- Reference count: 40
- One-line primary result: The paper argues that existing definitions of both local and global robustness are fundamentally flawed, as they lead to the conclusion that no non-trivial ML classifier can be truly robust when considering all possible inputs in real-valued feature spaces.

## Executive Summary
This paper critically examines the widely accepted notion of machine learning (ML) model robustness, particularly focusing on adversarial examples and robustness certification. The authors argue that existing definitions of both local and global robustness are fundamentally flawed, as they lead to the conclusion that no non-trivial ML classifier can be truly robust when considering all possible inputs in real-valued feature spaces. The paper demonstrates that the standard experimental setups used to assess robustness are inadequate, as they often rely on random sampling which cannot capture the existence of adversarial examples that must exist for any non-trivial classifier. The authors further argue that efforts towards robustness certification are futile under the commonly used definitions.

## Method Summary
The paper uses theoretical analysis to examine the fundamental limitations of robustness definitions for ML classifiers. It constructs mathematical proofs showing that non-trivial classifiers on real-valued features cannot be globally robust under standard l_p norm definitions. The authors also implement practical experiments using pseudo-Boolean encoding and SAT solvers to verify the existence of adversarial examples in Binarized Neural Networks trained on MNIST and CIFAR-10 datasets. The methodology involves analyzing the relationship between robustness certification and formal explanations, demonstrating how non-existence of adversarial examples can be used to compute formal explanations of ML models.

## Key Results
- Proves that no non-trivial classifier defined on real-valued features can be globally robust under standard l_p norm definition
- Demonstrates that standard experimental setups relying on random sampling are inadequate for assessing robustness
- Shows that efforts towards robustness certification are futile under commonly used definitions for non-trivial classifiers
- Establishes that robustness tools can be valuable as building blocks for computing formal explanations of ML models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper proves that no non-trivial classifier defined on real-valued features can be globally robust under the standard l_p norm definition.
- Mechanism: By constructing a point z on the line segment between two points va and vb that are classified differently, the paper shows that for any epsilon > 0, there exists a point within the epsilon-ball around z where the prediction changes, violating global robustness.
- Core assumption: The classifier is non-trivial (i.e., there exist at least two points in feature space with different predictions).
- Evidence anchors:
  - [abstract] The authors argue that existing definitions of both local and global robustness are fundamentally flawed, as they lead to the conclusion that no non-trivial ML classifier can be truly robust when considering all possible inputs in real-valued feature spaces.
  - [section] Proposition 2: Any non-trivial classifier defined on real-valued features is not globally robust, independently of the value of epsilon chosen.
- Break condition: If the feature space is discrete or if the classifier is trivial (always predicts the same class).

### Mechanism 2
- Claim: The paper shows that efforts towards robustness certification are futile under commonly used definitions.
- Mechanism: Since no non-trivial classifier can be globally robust, it follows that there must exist points in feature space where the classifier is not locally robust, making it impossible to certify robustness for any non-trivial classifier.
- Core assumption: The definition of robustness certification requires guaranteeing robustness for any input in feature space.
- Evidence anchors:
  - [abstract] The authors argue that efforts towards robustness certification are futile under the commonly used definitions.
  - [section] Proposition 4: For any non-trivial classifier defined on real-valued features, robustness cannot be certified, independently of the value of epsilon chosen.
- Break condition: If the definition of robustness certification is relaxed to allow probabilistic guarantees or if the input space is restricted.

### Mechanism 3
- Claim: The paper demonstrates that the standard experimental setup used to assess robustness is inadequate.
- Mechanism: Random sampling of feature space cannot capture the existence of adversarial examples that must exist for any non-trivial classifier, leading to incorrect conclusions about local robustness.
- Core assumption: The experimental setup relies on random sampling of feature space to assess local robustness.
- Evidence anchors:
  - [abstract] The authors argue that the standard experimental setups used to assess robustness are inadequate, as they often rely on random sampling which cannot capture the existence of adversarial examples that must exist for any non-trivial classifier.
  - [section] Examples of inadequate robustness assessment: The paper discusses how VNN-COMP and other works claiming robustness certification are inaccurate due to relying on random sampling.
- Break condition: If the experimental setup uses a more exhaustive search of feature space or if the classifier is known to be robust.

## Foundational Learning

- Concept: Minkowski distance (l_p norm)
  - Why needed here: The paper uses the l_p norm to define local and global robustness, which is essential for understanding the paper's arguments.
  - Quick check question: What is the formula for the l_p norm between two points x and y in m-dimensional space?

- Concept: Adversarial examples
  - Why needed here: The paper's arguments about the impossibility of robustness hinge on the existence of adversarial examples for any non-trivial classifier.
  - Quick check question: How is an adversarial example defined in the context of local robustness?

- Concept: Logic-based explainability
  - Why needed here: The paper connects the non-existence of adversarial examples to formal explanations, which is crucial for understanding the paper's positive results.
  - Quick check question: What is the relationship between abductive explanations (AXps) and the non-existence of adversarial examples?

## Architecture Onboarding

- Component map:
  - Classifier M with class function κ
  - Feature space F = D1 × D2 × ... × Dm
  - Set of classes K = {c1, c2, ..., cK}
  - Distance metric l_p norm
  - Input constraints (unconstrained, constrained, or distribution-restricted)

- Critical path:
  1. Define a non-trivial classifier M on real-valued features
  2. Show that there exist points va and vb with different predictions
  3. Construct a point z on the line segment between va and vb
  4. Demonstrate that for any epsilon > 0, there exists a point within the epsilon-ball around z with a different prediction
  5. Conclude that M is not globally robust
  6. Argue that this implies M is not locally robust for some points and robustness cannot be certified

- Design tradeoffs:
  - Real-valued vs. discrete features: The paper's negative results hold for real-valued features but may not apply to discrete features with appropriate distance metrics.
  - Unconstrained vs. constrained inputs: The paper assumes unconstrained inputs, but the results may differ if input constraints are considered.

- Failure signatures:
  - Claiming global robustness for a non-trivial classifier defined on real-valued features
  - Attempting to certify robustness for a non-trivial classifier defined on real-valued features
  - Using random sampling to assess local robustness for a non-trivial classifier defined on real-valued features

- First 3 experiments:
  1. Implement a simple linear classifier and attempt to prove global robustness for various epsilon values.
  2. Modify the experimental setup to use a more exhaustive search of feature space instead of random sampling and observe the impact on local robustness assessment.
  3. Implement a Binarized Neural Network (BNN) and use a SAT solver to verify the existence of adversarial examples for various epsilon values, confirming the paper's results on global robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimal hitting set (MHS) duality relationship between distance-restricted AXps and CXps, and how can it be exploited to compute these explanations?
- Basis in paper: [explicit] The paper states that "MHS duality between distance-restricted AXps and CXps has been proved, which enables the development of algorithms for navigating the space of εAXps and εCXps."
- Why unresolved: The paper mentions the MHS duality relationship but does not provide a detailed explanation or proof of how it works or how it can be exploited.
- What evidence would resolve it: A formal proof of the MHS duality relationship between εAXps and εCXps, along with algorithms that demonstrate how to leverage this duality to efficiently compute these explanations.

### Open Question 2
- Question: What are the key properties of distance-restricted explanations, including worst-case size, and how do they compare to probabilistic explanations?
- Basis in paper: [explicit] The paper suggests that future work will investigate "key properties of distance-restricted explanations, including worst-case size" and proposes them as an alternative to probabilistic explanations.
- Why unresolved: The paper does not provide any details on the properties of distance-restricted explanations or compare them to probabilistic explanations.
- What evidence would resolve it: A comprehensive analysis of the properties of distance-restricted explanations, including worst-case size, computational complexity, and quality compared to probabilistic explanations.

### Open Question 3
- Question: How can automated reasoners be instrumented to account for input distributions when deciding robustness?
- Basis in paper: [inferred] The paper discusses the limitations of sampling inputs according to inferred distributions and suggests that future work may investigate how to integrate such distributions with automated reasoners for robustness.
- Why unresolved: The paper acknowledges the challenge of integrating input distributions with automated reasoners but does not provide any solutions or insights on how to address this issue.
- What evidence would resolve it: A proposed method or framework for integrating input distributions with automated reasoners when deciding robustness, along with experimental results demonstrating its effectiveness.

## Limitations
- Focuses on real-valued feature spaces, which may not generalize to discrete or structured domains with different distance metrics
- Theoretical results hinge on the assumption that classifiers are non-trivial, limiting scope of applicability
- Experimental validation restricted to Binarized Neural Networks, leaving open questions about standard continuous neural networks

## Confidence

**High Confidence**: The theoretical impossibility of global robustness for non-trivial classifiers on real-valued features (Proposition 2 and 4)

**Medium Confidence**: The argument that random sampling cannot capture adversarial examples (requires empirical validation)

**Medium Confidence**: The claim that robustness certification efforts are futile under standard definitions (logically follows from theoretical results but may not account for practical relaxations)

## Next Checks

1. **Empirical Validation**: Conduct experiments on continuous neural networks using exhaustive search methods to verify that adversarial examples are consistently found, confirming the inadequacy of random sampling approaches.

2. **Alternative Distance Metrics**: Investigate whether using alternative distance metrics (e.g., discrete metrics or tree-edit distance) on discrete feature spaces can enable non-trivial robustness guarantees.

3. **Probabilistic Robustness Certification**: Explore whether relaxing robustness certification to probabilistic guarantees (e.g., "99% of inputs are robust") can circumvent the theoretical impossibility results while maintaining practical utility.