---
ver: rpa2
title: 'Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision
  Making'
arxiv_id: '2310.03022'
source_url: https://arxiv.org/abs/2310.03022
tags:
- learning
- performance
- action
- dataset
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a mismatch between Decision Transformer's
  attention module and the local dependence pattern inherent in reinforcement learning
  trajectories. To address this, the authors propose Decision ConvFormer (DC), which
  replaces the attention module with local convolution filters tailored for state,
  action, and return tokens.
---

# Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making

## Quick Facts
- **arXiv ID**: 2310.03022
- **Source URL**: https://arxiv.org/abs/2310.03022
- **Reference count**: 30
- **Primary result**: Decision ConvFormer achieves state-of-the-art performance on offline RL benchmarks by replacing attention with local convolution filters, reducing training time by 70% on Atari while improving performance by 24-39% across domains.

## Executive Summary
Decision ConvFormer (DC) addresses a fundamental mismatch between Decision Transformer's attention mechanism and the local dependence pattern inherent in reinforcement learning trajectories. The paper proposes replacing the attention module with local convolution filters that better capture the Markovian property of RL data. This design significantly reduces model complexity while improving both performance and sample efficiency across multiple offline RL benchmarks including MuJoCo, AntMaze, and Atari. DC demonstrates superior generalization to out-of-distribution target returns and achieves substantial computational efficiency gains.

## Method Summary
Decision ConvFormer replaces the attention module in Decision Transformer with causal convolution filters. The architecture uses three separate convolution filters for state, action, and return tokens, each with a filter length of L=6 and context length of K=8 timesteps. The model processes RTG, state, and action tokens through separate embedding layers, applies the convolution module to capture local dependencies, and uses a feed-forward network for final predictions. Training uses MSE loss between predicted and actual actions with hyperparameters including batch size 64, embedding dimension 256/128, and learning rate 1e-3/1e-4 for MuJoCo/AntMaze and Atari respectively.

## Key Results
- DC achieves state-of-the-art performance across MuJoCo, AntMaze, and Atari offline RL benchmarks
- 24% improvement over Decision Transformer on AntMaze and 39% on Atari tasks
- 70% reduction in training time for Atari experiments
- Superior generalization to out-of-distribution target returns compared to DT
- Significant reduction in model parameters while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing attention with causal convolution filters captures the local dependence inherent in RL trajectories more effectively than the attention module.
- Mechanism: Convolution filters are static and designed to capture the Markovian property by considering only nearby tokens, whereas attention weights are computed dynamically for each input sequence and tend to over-parameterize the model, capturing non-Markovian dependencies.
- Core assumption: The inherent dependence in RL trajectories follows a Markovian pattern where only local context is necessary for decision-making.
- Evidence anchors:
  - [abstract] "DC employs local convolution filtering as the token mixer and can effectively capture the inherent local associations of the RL dataset."
  - [section 2] "However, unlike data sequences in NLP for which Transformer was originally developed, offline RL data has an inherent pattern of local association between adjacent timestep tokens due to the Markovian property."
  - [corpus] Weak corpus support; no direct citations to similar mechanisms in the neighbor papers.
- Break condition: If the RL dataset exhibits strong long-range dependencies or non-Markovian behavior, the fixed local convolution filters would fail to capture necessary information.

### Mechanism 2
- Claim: The use of separate convolution filters for state, action, and return tokens allows the model to capture distinct semantics of each modality.
- Mechanism: By using three distinct convolution filters (state, action, and RTG), the model can learn different association patterns for each token type, reflecting their unique roles in the decision-making process.
- Core assumption: State, action, and return tokens have different semantic meanings and require different filtering mechanisms to capture their respective dependencies.
- Evidence anchors:
  - [section 3.2] "Considering the disparity among state, action, and RTG embeddings, we use three separate convolution filters for each hidden dimension: state filter, action filter, and RTG filter, to capture the unique information inherent in each embedding."
  - [section 3.2] "The reason for adopting three distinct filters for mod (p, 3) = 1 (p: RTG position), = 2 (p: state position), or = 3 (p: action position) is to capture different semantics when the current position corresponds to RTG, state or action."
  - [corpus] No direct evidence from corpus neighbors supporting this specific mechanism.
- Break condition: If the semantic distinction between state, action, and return tokens is not significant, using separate filters may not provide additional benefits and could increase model complexity unnecessarily.

### Mechanism 3
- Claim: The reduced model complexity of DC compared to DT leads to better generalization and avoids overfitting to the training dataset.
- Mechanism: DC has significantly fewer parameters due to the use of convolution filters instead of the attention module, which reduces the risk of overfitting and improves generalization, especially for out-of-distribution target RTGs.
- Core assumption: Over-parameterization in DT leads to overfitting and poor generalization, particularly for target RTGs not seen during training.
- Evidence anchors:
  - [section 5.3] "Indeed, a recent study by Lawson & Qureshi (2023) showed that even replacing the attention parameters learned in one MuJoCo environment with those learned in another environment results in almost no performance decrease."
  - [section 5.3] "This means that DC better understands the task context and better knows how to achieve the unseen desired higher target RTG by learning from the seen dataset than DT. The superior generalization capability of DC to DT implies that the model complexity of DC is closer to the optimal complexity than that of DT indeed."
  - [corpus] No direct evidence from corpus neighbors supporting this specific mechanism.
- Break condition: If the training dataset is sufficiently large and diverse, the reduced model complexity may not provide significant benefits and could limit the model's capacity to learn complex patterns.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: Understanding the Markovian property is crucial for designing the convolution filters that capture local dependencies in RL trajectories.
  - Quick check question: What is the Markovian property and how does it apply to RL trajectories?

- Concept: Transformer Architecture
  - Why needed here: Familiarity with the standard Transformer architecture and its attention mechanism is necessary to understand the proposed replacement with convolution filters.
  - Quick check question: How does the attention mechanism in Transformer work, and what are its key components?

- Concept: Offline Reinforcement Learning
  - Why needed here: Knowledge of offline RL and its challenges, such as the reliance on static datasets and the importance of return-conditioned behavior cloning, is essential for understanding the motivation behind DC.
  - Quick check question: What are the main challenges in offline RL, and how does return-conditioned behavior cloning address them?

## Architecture Onboarding

- Component map:
  Input embedding layer -> Convolution module (state/action/RTG filters) -> Feed-forward network -> Layer normalization -> Prediction

- Critical path:
  1. Input sequence formation: Construct the input sequence from RTG, state, and action tokens.
  2. Embedding: Transform each token type using separate embedding layers.
  3. Convolution: Apply the three separate convolution filters to capture local dependencies.
  4. Feed-forward: Process the convolution output through a feed-forward network.
  5. Prediction: Generate the next action based on the processed information.

- Design tradeoffs:
  - Local vs. global dependencies: DC focuses on local dependencies, which may not capture long-range dependencies as effectively as attention mechanisms.
  - Model complexity: DC has fewer parameters than DT, which can lead to better generalization but may limit the model's capacity to learn complex patterns.
  - Static vs. dynamic filtering: DC uses static convolution filters, while DT uses dynamic attention weights, which can adapt to different input sequences but may over-parameterize the model.

- Failure signatures:
  - Poor performance on tasks requiring long-range dependencies: If the RL task has significant long-range dependencies, DC may struggle to capture them due to its focus on local information.
  - Overfitting to the training dataset: If the training dataset is small or lacks diversity, DC's reduced model complexity may not be sufficient to learn complex patterns, leading to overfitting.
  - Sensitivity to hyperparameter choices: The performance of DC may be sensitive to the choice of context length, filter size, and other hyperparameters, requiring careful tuning for optimal results.

- First 3 experiments:
  1. Ablation study on convolution filters: Remove the separate filters for state, action, and RTG tokens and use a single filter for all tokens. Compare the performance with the original DC model to assess the importance of modality-specific filtering.
  2. Context length sensitivity analysis: Vary the context length (K) and filter size (L) in DC and evaluate the impact on performance across different RL tasks. Identify the optimal combination of context length and filter size for each task.
  3. Out-of-distribution generalization test: Train DC on a dataset with a specific range of target RTGs and evaluate its performance on out-of-distribution target RTGs. Compare the generalization capability of DC with DT to assess the benefits of the reduced model complexity.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in a dedicated section. The research focuses on demonstrating the effectiveness of Decision ConvFormer through empirical results and theoretical analysis rather than posing future research directions.

## Limitations

- The paper doesn't explore alternative architectures that combine attention and convolution mechanisms, limiting understanding of whether some attention components might still be beneficial in RL contexts.
- The theoretical arguments about attention mismatch with RL's local dependencies remain largely theoretical without extensive empirical validation across diverse environments.
- The generalization claims would benefit from testing on entirely unseen domains rather than just out-of-distribution RTGs within the same domain.

## Confidence

- **High confidence**: The empirical performance claims (state-of-the-art results on MuJoCo, AntMaze, and Atari) are well-supported by the experimental results presented. The efficiency improvements (70% training time reduction) are clearly demonstrated with specific metrics.
- **Medium confidence**: The theoretical arguments about attention mismatch with RL's local dependencies are reasonable but not definitively proven. The claims about convolution being "sufficient" for decision-making are supported by strong results but could be challenged by tasks requiring longer-range dependencies.
- **Medium confidence**: The claims about superior generalization and reduced overfitting are supported by experiments but would benefit from more extensive testing across diverse environments and with different dataset sizes.

## Next Checks

1. **Long-range dependency test**: Design an RL environment with explicit long-range dependencies (e.g., requiring memory of events 50+ steps in the past) to empirically test whether DC's local filtering approach fails where attention-based methods succeed.

2. **Attention-convolution hybrid ablation**: Create a variant that combines both attention and convolution mechanisms, varying the relative weight between them to empirically determine if some attention components are still beneficial even in RL contexts.

3. **Scaling behavior analysis**: Systematically vary the dataset size and diversity to empirically determine at what point the reduced model complexity of DC becomes a limitation rather than an advantage, testing the hypothesis that DT overfits more severely with smaller datasets.