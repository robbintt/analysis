---
ver: rpa2
title: 'Weaving Pathways for Justice with GPT: LLM-driven automated drafting of interactive
  legal applications'
arxiv_id: '2312.09198'
source_url: https://arxiv.org/abs/2312.09198
tags:
- text
- form
- forms
- questions
- fields
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents three approaches to automating the completion
  of court forms using generative AI, with the goal of speeding up the authoring of
  tools to help self-represented litigants. The authors explore a generative AI approach
  using GPT-3 to iteratively prompt users, a constrained template-driven approach
  using GPT-4-turbo to generate drafts subject to human review, and a hybrid method.
---

# Weaving Pathways for Justice with GPT: LLM-driven automated drafting of interactive legal applications

## Quick Facts
- **arXiv ID**: 2312.09198
- **Source URL**: https://arxiv.org/abs/2312.09198
- **Reference count**: 17
- **Primary result**: Hybrid model of constrained automated drafting with human review is most effective for authoring guided interviews for court forms

## Executive Summary
This paper explores the use of generative AI, specifically GPT models, to automate the authoring of guided interviews for court forms. The goal is to speed up the creation of tools that help self-represented litigants navigate complex legal processes. Three approaches are tested: fully automated GPT-3 prompting, constrained template-driven GPT-4 drafting with human review, and a hybrid method combining both. The authors find that while AI can successfully identify user inputs and generate interview questions, human oversight remains essential for ensuring accuracy and usability, particularly for complex legal documents.

## Method Summary
The authors develop a system that uses GPT-3 and GPT-4-turbo to automate the creation of guided interviews for court forms using the open-source Docassemble platform. They employ the Assembly Line Weaver tool to scan court form templates and generate draft interviews in YAML format. For Word documents, GPT-4-turbo labels placeholders and generates variable names, definitions, and questions based on context. For PDF forms, OCR is used to identify fields, followed by GPT-4-turbo analysis to create interview questions and fill in fields, with human review ensuring accuracy. The approach is tested on name change forms from 12 jurisdictions.

## Key Results
- The hybrid model of constrained automated drafting with human review is most effective for authoring guided interviews
- AI successfully identified user inputs and created interview questions for 62-69% of fields in court forms
- Word documents yielded better field identification results than PDFs despite more ambiguous placeholder markers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid model of constrained automated drafting with human review is most effective for authoring guided interviews
- Mechanism: Large language model generates draft interview questions and variable labels, which human author then refines using established templates and style guides
- Core assumption: Human expertise is needed to ensure usability, legal accuracy, and appropriate question ordering that AI alone cannot reliably produce
- Evidence anchors:
  - [abstract] "We conclude that the hybrid model of constrained automated drafting with human review is best suited to the task of authoring guided interviews."
  - [section] "However, this AI cannot reliably automate the large number of more complex legal documents without human assistance. Human review and editing of these interviews remains essential."
  - [corpus] Weak evidence - corpus focuses on other LLM-driven applications rather than legal form automation specifically
- Break condition: When forms are too complex for AI to handle basic field identification and question generation, or when human review becomes more time-consuming than manual authoring

### Mechanism 2
- Claim: Word documents can be automatically labeled for variables using LLM analysis of placeholder text
- Mechanism: Python-docx extracts formatted text runs, LLM analyzes context to generate appropriate variable names and descriptions, replacing placeholders with Jinja2 variables
- Core assumption: LLM can understand contextual meaning of placeholder text better than rule-based systems
- Evidence anchors:
  - [section] "We had better success with identifying and labeling fields in Word documents than PDFs, despite the more ambiguous markers that the Word documents used to label placeholders"
  - [section] "This approach worked well for fields where the user was expected to write an answer"
  - [corpus] No direct evidence in corpus about document variable labeling
- Break condition: When placeholder text is too ambiguous or when document formatting is too complex for reliable extraction

### Mechanism 3
- Claim: PDF forms can be automatically converted to interactive interviews using OCR and LLM analysis
- Mechanism: Fill PDF fields with placeholder text, convert to images, apply OCR to get field context, use LLM to generate variable names, definitions, and questions
- Core assumption: Combining OCR with LLM context analysis can overcome PDF's stream-based format limitations
- Evidence anchors:
  - [section] "The interviews ran and filled in fields in the PDF document" and "We were able to automatically recognize and write questions for 62% to 69% of the existing fields"
  - [section] "We tested this approach with name change forms taken from 12 jurisdictions"
  - [corpus] No direct evidence in corpus about PDF form automation
- Break condition: When PDF fields are too small for OCR (like checkboxes) or when form layout prevents accurate field-context mapping

## Foundational Learning

- Concept: Docassemble platform and YAML format
  - Why needed here: The entire system builds interviews using Docassemble's YAML format, so understanding its structure is crucial
  - Quick check question: What are the key components of a Docassemble interview file in YAML format?

- Concept: Large language model prompting techniques
  - Why needed here: Different prompting strategies were tested (returning full text vs. positions) with varying success rates
  - Quick check question: Why did the initial prompting approach that returned positions fail while the approach returning full modified text succeed?

- Concept: Optical Character Recognition (OCR) limitations
  - Why needed here: OCR was critical for extracting field context from PDFs but had specific limitations with small elements
  - Quick check question: What specific type of form field was OCR unable to handle reliably and why?

## Architecture Onboarding

- Component map: PDF/Word input → Field identification → Context analysis → Variable naming → Question generation → Template integration → Human review → Final interview
- Critical path: PDF/Word input → Field identification → Context analysis → Variable naming → Question generation → Template integration → Human review → Final interview
- Design tradeoffs:
  - Speed vs. accuracy: Fully automated approach faster but less reliable than hybrid
  - Context preservation vs. token limits: Full document context improves accuracy but exceeds LLM context windows
  - Checkbox handling vs. complexity: Special handling needed for checkboxes increases complexity
- Failure signatures:
  - Low field identification rate (<60%) indicates OCR or context analysis issues
  - Incorrect variable naming suggests LLM misunderstanding of placeholder context
  - Template integration errors point to format compatibility problems
- First 3 experiments:
  1. Test Word document variable labeling with simple placeholder formats
  2. Test PDF field identification using OCR with forms containing primarily text fields
  3. Test hybrid workflow integration with Assembly Line Weaver using both PDF and Word inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the accuracy of checkbox field identification and labeling in PDF documents when using OCR and large language models?
- Basis in paper: [explicit] The paper discusses difficulties with checkbox fields, noting they were often too small to add OCR-visible text to and were only paired with related text 28% of the time.
- Why unresolved: The authors acknowledge this as a limitation but do not propose specific solutions beyond noting the need for improvement.
- What evidence would resolve it: A study comparing different OCR techniques, image preprocessing methods, or machine learning approaches specifically trained on checkbox identification could demonstrate improved accuracy rates.

### Open Question 2
- Question: What is the optimal balance between automated drafting and human review in the hybrid model to maximize efficiency while maintaining quality?
- Basis in paper: [explicit] The paper concludes that the hybrid model is best suited for authoring guided interviews but does not specify at which stages human intervention is most critical or how to determine when automated drafting is sufficient.
- Why unresolved: While the authors identify intervention points, they do not provide data on how much time is saved at each stage or what level of human review is necessary for different types of forms.
- What evidence would resolve it: Comparative studies measuring time savings and quality outcomes with different levels of human review at various stages of the automation process would help determine optimal intervention points.

### Open Question 3
- Question: How can we ensure that automatically generated questions and data types do not inadvertently exclude valid but unusual inputs (e.g., non-standard address formats or names)?
- Basis in paper: [inferred] The authors mention concerns about GPT-4 turbo being too rigid, such as assigning numeric data types to phone numbers and ZIP codes, which could exclude valid formats.
- Why unresolved: While the authors acknowledge this limitation, they do not propose specific solutions to validate diverse input formats while maintaining data integrity.
- What evidence would resolve it: Testing with diverse user populations and analyzing error rates for different data type assignments could identify patterns in exclusion and inform more inclusive validation rules.

## Limitations

- The hybrid approach requires significant human review, potentially reducing speed benefits for complex forms
- Evaluation methodology relies on self-reported usability assessments rather than comprehensive user testing with actual litigants
- Study focuses primarily on name change forms from 12 jurisdictions, limiting generalizability to other legal domains

## Confidence

- **High Confidence**: The finding that constrained automated drafting with human review outperforms fully automated approaches for guided interviews
- **Medium Confidence**: The claim that LLM-driven approaches can achieve "at least as good as the original PDF" usability
- **Medium Confidence**: The assertion that Word documents yield better results than PDFs for field identification and labeling

## Next Checks

1. Conduct controlled studies comparing the hybrid AI-assisted authoring process against traditional manual methods, measuring both authoring time and resulting form usability for actual users
2. Apply the same methodology to forms from different legal domains (divorce, housing, employment) to assess whether the 62-69% field identification rate holds across diverse form types and jurisdictions
3. Systematically test the hybrid approach on forms of increasing complexity to identify the precise threshold where human review time outweighs automation benefits, and document specific failure patterns