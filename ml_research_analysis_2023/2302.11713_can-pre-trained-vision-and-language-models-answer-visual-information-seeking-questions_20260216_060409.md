---
ver: rpa2
title: Can Pre-trained Vision and Language Models Answer Visual Information-Seeking
  Questions?
arxiv_id: '2302.11713'
source_url: https://arxiv.org/abs/2302.11713
tags:
- visual
- questions
- question
- answer
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces INFO SEEK, a new benchmark for evaluating
  pre-trained vision and language models on visual information-seeking questions.
  INFO SEEK consists of two parts: a human-written dataset (8.9K examples) and an
  automated dataset (1.3M examples) generated from Wikidata and visual entity recognition
  datasets.'
---

# Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?

## Quick Facts
- arXiv ID: 2302.11713
- Source URL: https://arxiv.org/abs/2302.11713
- Reference count: 40
- Primary result: State-of-the-art pre-trained vision and language models struggle with visual information-seeking questions, but fine-tuning and explicit entity linking significantly improve performance

## Executive Summary
This paper introduces INFO SEEK, a new benchmark for evaluating pre-trained vision and language models on visual information-seeking questions. The benchmark consists of 8.9K human-written examples and 1.3M automated examples generated from Wikidata and visual entity recognition datasets. The authors analyze various pre-trained models (PaLI, OFA, CLIP) under two evaluation protocols: No KB (end-to-end) and With KB (pipeline). Results show that while current models struggle with these tasks, fine-tuning on INFO SEEK significantly improves performance. The study also reveals that pipeline systems with explicit entity linking outperform end-to-end models, highlighting the importance of accurate visual entity recognition. Interestingly, end-to-end models show better performance on tail entities, suggesting potential for hybrid approaches.

## Method Summary
The authors evaluate pre-trained vision and language models on INFO SEEK using two protocols: No KB (end-to-end) and With KB (pipeline). For No KB, they fine-tune PaLI and OFA on the INFO SEEK training set and evaluate zero-shot and fine-tuned performance. For With KB, they use CLIP for visual entity recognition and retrieve relevant Wikipedia passages for language models (PaLM, FiD) to answer questions. The evaluation uses harmonic mean of UNSEEN QUESTION and UNSEEN ENTITY splits. The authors also analyze performance across different question types (STRING, TIME, NUMERICAL) and entity frequency (head vs tail entities).

## Key Results
- State-of-the-art pre-trained models achieve negligible performance on INFO SEEK without fine-tuning
- Fine-tuning on INFO SEEK significantly improves model performance across all architectures
- Pipeline systems with explicit entity recognition outperform end-to-end models, especially on unseen entities
- End-to-end models show better performance on tail entities, revealing potential for hybrid approaches
- Visual entity recognition accuracy (22% with CLIP) is a critical bottleneck for pipeline systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on INFO SEEK elicits knowledge from pre-trained models that was learned during pretraining but not immediately accessible in zero-shot.
- Mechanism: INFO SEEK training data contains QA pairs that require linking visual entities to knowledge base information. Fine-tuning teaches models how to activate and use pre-existing factual knowledge stored in parameters for visual info-seeking tasks.
- Core assumption: Pre-trained vision-language models contain rich factual knowledge in their parameters that can be triggered by appropriate training signals.
- Evidence anchors:
  - [abstract]: "fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training"
  - [section 5.1]: "Without fine-tuning, PaLI can produce a negligible overall performance on the INFO SEEK Wikidata... significantly worse than the fine-tuned counterpart"
  - [corpus]: Weak evidence for the mechanism itself - the paper doesn't directly measure what knowledge was present pre-training versus learned during fine-tuning

### Mechanism 2
- Claim: Pipeline systems with explicit entity recognition and KB retrieval outperform end-to-end models because they provide better access to factual knowledge.
- Mechanism: By separating visual entity recognition from language QA, pipeline systems can more reliably identify the correct entity and then use targeted KB retrieval to find relevant information, avoiding the need to memorize all knowledge in parameters.
- Core assumption: Knowledge base articles contain comprehensive information about entities that can answer INFO SEEK questions when properly retrieved.
- Evidence anchors:
  - [abstract]: "accurate visual entity recognition can be used to improve performance on InfoSeek by retrieving relevant documents"
  - [section 5.2]: "Pipeline systems outperform the best end-to-end model (i.e., PaLI) by a big margin, especially on the unseen entity split"
  - [section 5.3]: "Better entity recognition can lead to significant performance gain" - shows CLIP entity recognition at 22% accuracy is bottleneck

### Mechanism 3
- Claim: End-to-end models have unique advantage on tail entities because they can leverage learned patterns and associations rather than relying on entity-specific KB entries.
- Mechanism: Pre-trained models may have learned general patterns about how entities relate to attributes, allowing them to generalize to rare entities without needing specific KB entries for each.
- Core assumption: Pre-training on diverse web-scale data creates generalizable patterns that can be applied to rare entities.
- Evidence anchors:
  - [abstract]: "end-to-end models perform better on tail entities, revealing a promising direction for combining the advantages of both end-to-end and pipeline models"
  - [section 5.3]: "PaLI-17B outperforms the pipeline systems by a large margin on the tail entities, particularly for questions related to geographical information"
  - [corpus]: No direct evidence provided for why this occurs - the paper states the finding but doesn't explain the mechanism

## Foundational Learning

- Concept: Visual entity recognition
  - Why needed here: INFO SEEK requires linking images to specific entities in a knowledge base. Without accurate entity recognition, models cannot retrieve relevant information.
  - Quick check question: Given an image of the Eiffel Tower, can you correctly identify it as "Eiffel Tower" rather than just "tower" or "building"?

- Concept: Knowledge base retrieval
  - Why needed here: INFO SEEK questions require factual knowledge not present in images. Models need to retrieve relevant documents from sources like Wikipedia.
  - Quick check question: If asked "When was the Eiffel Tower built?", can you retrieve the correct Wikipedia page and find the construction date?

- Concept: Fine-grained VQA
  - Why needed here: INFO SEEK focuses on detailed information-seeking questions, not just visual attribute recognition. Models need to understand both visual content and factual knowledge.
  - Quick check question: Can you distinguish between a question that can be answered from image content alone versus one requiring external knowledge?

## Architecture Onboarding

- Component map: Image → Visual entity recognition → KB retrieval → Answer generation → Evaluation
- Critical path: Image → Entity recognition → KB retrieval → Answer generation → Evaluation
  The slowest component determines overall system performance.
- Design tradeoffs:
  - End-to-end vs. pipeline: End-to-end simpler but limited by parameter capacity; pipeline more complex but can access external knowledge
  - Entity recognition accuracy vs. coverage: Higher accuracy limits to known entities; broader coverage risks lower precision
  - KB size vs. retrieval speed: Larger KBs contain more knowledge but slow down retrieval
- Failure signatures:
  - Low entity recognition accuracy → pipeline performance drops sharply
  - KB retrieval returning irrelevant documents → answer accuracy suffers
  - Fine-tuning overfitting → poor generalization to unseen entities
- First 3 experiments:
  1. Evaluate CLIP entity recognition accuracy on INFO SEEK images to establish baseline
  2. Compare end-to-end model performance with and without fine-tuning on INFO SEEK
  3. Measure pipeline performance with oracle entity recognition to identify bottlenecks

## Open Questions the Paper Calls Out

Open Question 1
- Question: How can end-to-end models be improved to handle tail entities more effectively?
- Basis in paper: Explicit
- Why unresolved: The paper notes that end-to-end models like PaLI perform significantly better on tail entities compared to pipeline systems, but the exact reasons and potential improvements are not explored.
- What evidence would resolve it: Experiments comparing different model architectures or training strategies specifically focused on tail entities could provide insights.

Open Question 2
- Question: What is the optimal way to combine the strengths of end-to-end and pipeline models for visual information-seeking tasks?
- Basis in paper: Explicit
- Why unresolved: The paper identifies that pipeline systems outperform end-to-end models on head entities, while end-to-end models excel on tail entities. However, it does not explore methods to integrate these approaches.
- What evidence would resolve it: Comparative studies of hybrid models or ensemble methods could reveal the best approach to leverage both paradigms.

Open Question 3
- Question: How does the performance of models vary with different levels of entity granularity (e.g., buildings vs. specific landmarks)?
- Basis in paper: Inferred
- Why unresolved: While the paper discusses entity coverage, it does not delve into how model performance changes with the granularity of entities.
- What evidence would resolve it: Analysis of model performance across different levels of entity specificity could provide insights into model capabilities.

Open Question 4
- Question: What are the specific challenges in visual entity recognition that limit performance on visual information-seeking tasks?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that visual entity recognition is a bottleneck, but it does not explore the underlying challenges in detail.
- What evidence would resolve it: In-depth error analysis of visual entity recognition models could identify specific areas for improvement.

Open Question 5
- Question: How can pre-training data be augmented to improve the memorization of fine-grained knowledge in end-to-end models?
- Basis in paper: Inferred
- Why unresolved: The paper suggests that fine-tuning elicits knowledge from pre-trained models, but it does not explore strategies for improving pre-training to better memorize fine-grained knowledge.
- What evidence would resolve it: Experiments with different pre-training strategies or datasets could reveal effective methods for enhancing knowledge memorization.

## Limitations

- Entity recognition bottleneck: Visual entity recognition accuracy (22% with CLIP) is a critical bottleneck that limits pipeline system performance
- KB coverage constraints: Pipeline approach is limited by the scope of the knowledge base, potentially missing information available in larger KBs
- Evaluation granularity: The paper doesn't provide detailed analysis of specific failure modes within each question type category

## Confidence

- **High confidence**: Pipeline systems outperform end-to-end models on seen entities, and end-to-end models perform better on tail entities. These are directly supported by experimental results with clear metrics.
- **Medium confidence**: The mechanism explanation for why fine-tuning elicits knowledge from pretraining. While results show improved performance after fine-tuning, the paper doesn't directly measure what knowledge was present pre-training versus learned during fine-tuning.
- **Medium confidence**: The importance of explicit entity linking. While supported by results, alternative explanations aren't fully ruled out.

## Next Checks

1. **Entity recognition ablation**: Test the pipeline approach with oracle entity recognition (perfect accuracy) to quantify the exact contribution of the entity recognition bottleneck versus other pipeline components.

2. **Cross-dataset generalization**: Evaluate models fine-tuned on INFO SEEK on other visual QA benchmarks (OK-VQA, A-OKVQA) to verify that the knowledge elicited is genuinely useful beyond the specific INFO SEEK domain.

3. **Scale sensitivity analysis**: Test whether the performance gap between end-to-end and pipeline approaches changes with model scale by evaluating medium-sized models (e.g., PaLI-8B, PaLI-3B) in addition to the 17B parameter version.