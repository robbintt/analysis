---
ver: rpa2
title: Conformalized Semi-supervised Random Forest for Classification and Abnormality
  Detection
arxiv_id: '2302.02237'
source_url: https://arxiv.org/abs/2302.02237
tags:
- test
- training
- prediction
- type
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CSForest (Conformalized Semi-supervised Random
  Forest), a novel method for classification and abnormality detection under distributional
  shifts between training and test data. CSForest combines semi-supervised random
  forest ensembles with Jackknife+aB conformal prediction to construct calibrated
  set-valued predictions that include the true label with high probability while flagging
  outliers.
---

# Conformalized Semi-supervised Random Forest for Classification and Abnormality Detection

## Quick Facts
- arXiv ID: 2302.02237
- Source URL: https://arxiv.org/abs/2302.02237
- Authors: 
- Reference count: 29
- Primary result: Novel method combining semi-supervised random forests with Jackknife+aB conformal prediction for classification and outlier detection under distributional shifts

## Executive Summary
CSForest introduces a novel approach for classification and abnormality detection when training and test data exhibit distributional shifts. The method leverages semi-supervised random forest ensembles with Jackknife+aB conformal prediction to construct calibrated set-valued predictions that maintain coverage guarantees even under label shifts. By incorporating test samples into the training process and using a specialized calibration procedure, CSForest adapts to the test distribution while providing provable worst-case coverage for inlier classes.

## Method Summary
CSForest combines semi-supervised random forest ensembles with Jackknife+aB conformal prediction to address classification and outlier detection under distributional shifts. The method trains B random forest trees using both labeled training data and unlabeled test data, then constructs calibrated scores by comparing how likely a test sample belongs to each class relative to training samples from that class. A weighted objective function balances optimization for training and test performance, and the Jackknife+aB calibration procedure ensures worst-case coverage guarantees for inlier classes under generalized label shift models.

## Key Results
- CSForest achieves type II errors of 8.8% for inliers and 44% for outliers on MNIST, significantly outperforming state-of-the-art methods
- Theoretical guarantees show CSForest provides worst-case coverage (1-2α) for inlier classes under generalized label shift models
- The method demonstrates robust performance across varying sample sizes and label shift scenarios, maintaining effectiveness even with limited test data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CSForest achieves calibrated coverage guarantees even under distributional shifts by leveraging Jackknife+aB conformal prediction with semi-supervised random forest ensembles
- Mechanism: Uses unlabeled test samples to adapt the random forest classifier, then applies Jackknife+aB to construct calibrated set-valued predictions. By excluding paired training-test observations during score computation, it ensures worst-case coverage (1-2α) for inlier classes under generalized label shift models
- Core assumption: Features from each observed class remain unchanged under distributional shifts, while class proportions and outlier components can vary
- Evidence anchors:
  - [abstract]: "Theoretically, we establish CSForest to cover true labels for previously observed inlier classes under arbitrarily label-shift in the test data."
  - [section 3]: "Theorem 3.2... CSForest satisfies: P[k ∈ ˆCi(X)|Y = k] ≥ 1 − 2α, for all i ∈ Ite and k = 1, . . . , K."
  - [corpus]: Weak - no direct citations about Jackknife+aB with semi-supervised ensembles in the corpus

### Mechanism 2
- Claim: CSForest outperforms state-of-the-art methods in both classification accuracy for inliers and outlier detection power
- Mechanism: Incorporates test samples into the random forest training process and uses Jackknife+aB calibration to adapt to the test distribution more effectively than methods that only use training data or poorly utilize test data
- Core assumption: The semi-supervised approach can learn meaningful representations from the combination of labeled training and unlabeled test data
- Evidence anchors:
  - [abstract]: "Extensive experiments on synthetic data and MNIST demonstrate that CSForest significantly outperforms state-of-the-art methods like BCOPS, CRF, and DC..."
  - [section 1.1]: "CSForest achieves significantly better classification accuracy than existing classification methods while providing a provable worst-case coverage guarantee..."
  - [corpus]: Weak - no direct citations about performance comparisons in the corpus

### Mechanism 3
- Claim: CSForest provides robust performance across varying sample sizes and label shift scenarios
- Mechanism: The weighted objective function µ(x) = fte(x) + wftr(x) allows balancing between optimizing for test performance (w=0) and training performance (w large), making the method adaptable to different data regimes
- Core assumption: The weight w can be tuned to achieve optimal performance for specific application requirements
- Evidence anchors:
  - [section 3]: "CSForest considers optimizing the 'average' training and test performance, and considers µ(x) = fte(x) + wftr(x) for a weight w ≥ 0."
  - [section 4.2]: "CSForest achieves the best power for outlier detection while obtaining close to the lowest type II error for inliers as we vary the sample sizes."
  - [corpus]: Weak - no direct citations about weighted objectives in the corpus

## Foundational Learning

- Concept: Conformal prediction and coverage guarantees
  - Why needed here: CSForest relies on conformal prediction framework to construct calibrated set-valued predictions with guaranteed coverage
  - Quick check question: What is the difference between marginal coverage and conditional coverage in conformal prediction?

- Concept: Semi-supervised learning and random forests
  - Why needed here: CSForest combines labeled training data with unlabeled test data to improve classification and outlier detection
  - Quick check question: How does incorporating unlabeled test data into random forest training differ from standard semi-supervised learning approaches?

- Concept: Generalized label shift model
  - Why needed here: The theoretical guarantees of CSForest are derived under this distributional shift model
  - Quick check question: How does the generalized label shift model differ from the standard label shift model?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Semi-supervised random forest training -> Jackknife+aB calibration -> Prediction set construction

- Critical path:
  1. Train semi-supervised random forest using both training and test samples
  2. Compute ensemble predictions excluding paired observations
  3. Calibrate using Jackknife+aB to achieve coverage guarantees
  4. Construct set-valued predictions for test samples

- Design tradeoffs:
  - Using test samples in training improves adaptation but requires careful calibration to maintain coverage guarantees
  - The weight w balances between training and test optimization but requires tuning
  - Jackknife+aB provides better coverage than split conformal but increases computational cost

- Failure signatures:
  - Poor outlier detection: May indicate insufficient adaptation to test distribution
  - Coverage guarantee violation: Could suggest breakdown of generalized label shift assumption
  - High type II errors: Might indicate suboptimal choice of weight w or insufficient training data

- First 3 experiments:
  1. Reproduce the MNIST experiment from section 4.1 to verify basic functionality
  2. Test performance under varying sample sizes as in section 4.2 to assess robustness
  3. Evaluate coverage under traditional label shift (section 4.3) to check theoretical guarantees

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weight parameter w in CSForest for balancing training and test performance in different scenarios?
- Basis in paper: [explicit] The paper mentions w = 1 as the default choice but also experiments with smaller w ∈ [0,1) and varying sample sizes in Appendix C
- Why unresolved: The paper only briefly explores the effect of different w values on outlier detection performance but doesn't provide a systematic method for choosing w based on data characteristics
- What evidence would resolve it: A comprehensive study showing how w should be chosen based on factors like the proportion of outliers, sample sizes, and label shift magnitude would resolve this question

### Open Question 2
- Question: How does CSForest perform with extremely small test sample sizes, particularly when there are fewer than 5 test samples per class?
- Basis in paper: [explicit] The paper briefly mentions exploring CSForest with only 5 samples per class in the test set and shows promising results, but defers this as future research
- Why unresolved: The paper only provides limited results for very small test sample sizes and doesn't thoroughly investigate the performance limits of CSForest in this regime
- What evidence would resolve it: Extensive experiments varying test sample sizes from very small (e.g., 1-5 per class) to moderate sizes, comparing CSForest's performance to other methods, would resolve this question

### Open Question 3
- Question: How does the performance of CSForest compare to other methods when dealing with high-dimensional data beyond the MNIST example?
- Basis in paper: [inferred] The paper demonstrates CSForest's effectiveness on MNIST data but doesn't extensively test it on other high-dimensional datasets or explore how dimensionality affects performance
- Why unresolved: The paper only provides results for one specific high-dimensional dataset (MNIST) and doesn't investigate how CSForest scales or performs on other types of high-dimensional data
- What evidence would resolve it: Comprehensive experiments on various high-dimensional datasets (e.g., CIFAR, ImageNet, genomics data) comparing CSForest's performance to other methods would resolve this question

## Limitations
- The method relies heavily on the generalized label shift assumption, which may not hold in many real-world scenarios where feature distributions also shift
- The theoretical coverage guarantees are derived under idealized conditions that may not translate perfectly to finite-sample settings
- The computational complexity of Jackknife+aB calibration with semi-supervised random forests could become prohibitive for large-scale applications

## Confidence
- **High confidence**: The theoretical framework for coverage guarantees under generalized label shift models is well-established, with explicit proofs provided for the (1-2α) coverage bound
- **Medium confidence**: The empirical performance claims are based on synthetic and MNIST experiments, which provide proof of concept but may not fully capture real-world complexity
- **Low confidence**: The practical utility of the method across diverse domains remains largely unproven, as the evaluation is limited to relatively simple benchmark datasets

## Next Checks
1. Evaluate CSForest performance on more complex, real-world datasets with varying degrees of distributional shift, including cases where the generalized label shift assumption is violated
2. Benchmark the computational efficiency of CSForest with increasing numbers of trees (B) and sample sizes, comparing against alternative methods to identify practical limitations
3. Systematically investigate the impact of the weight parameter w and other hyperparameters on performance across different data regimes and distributional shift scenarios