---
ver: rpa2
title: On Separate Normalization in Self-supervised Transformers
arxiv_id: '2309.12931'
source_url: https://arxiv.org/abs/2309.12931
tags:
- normalization
- sepnorm
- sharenorm
- uniformity
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple modification to self-supervised transformers
  that employs separate normalization layers for the [CLS] symbol and the tokens.
  The motivation is that the [CLS] symbol plays a special role in representation learning
  and should be treated differently from the rest of the tokens.
---

# On Separate Normalization in Self-supervised Transformers

## Quick Facts
- arXiv ID: 2309.12931
- Source URL: https://arxiv.org/abs/2309.12931
- Reference count: 26
- One-line primary result: Average 2.7% performance improvement across image, natural language, and graph domains by using separate normalization layers for [CLS] and tokens

## Executive Summary
This paper proposes a simple yet effective modification to self-supervised transformers that employs separate normalization layers for the [CLS] symbol and other tokens. The key insight is that [CLS] plays a special role in representation learning by summarizing global context, and should therefore be treated differently from regular tokens. By using separate normalization layers, the [CLS] embeddings can better encode global contextual information and achieve more uniform distributions in anisotropic embedding spaces. The method is easy to implement and demonstrates consistent improvements across multiple domains including computer vision, natural language processing, and graph learning.

## Method Summary
The paper addresses the issue of shared normalization layers in self-supervised transformers by proposing separate normalization layers for the [CLS] symbol and other tokens. Specifically, the authors replace the shared normalization layer with distinct layers, typically using combinations of batch normalization (BN) for [CLS] and layer normalization (LN) for tokens, or vice versa depending on the domain. This modification is applied to various self-supervised transformer models including masked autoencoders (MAE) and evaluated across multiple downstream tasks using linear probing or fine-tuning approaches.

## Key Results
- Average 2.7% performance improvement across computer vision, natural language, and graph domains
- Better uniformity preservation in [CLS] embeddings without requiring explicit contrastive loss
- Improved downstream task performance with minimal additional computational overhead

## Why This Works (Mechanism)

### Mechanism 1
Separate normalization reduces dimensional collapse in [CLS] embeddings by preventing conflicting normalization statistics from tokens and [CLS]. When [CLS] and token features share normalization layers, their differing statistical distributions create conflicting update directions during backpropagation. This forces [CLS] features to compress into a lower-dimensional subspace to satisfy both token and [CLS] constraints simultaneously. Separate normalization allows each feature type to optimize its own distribution independently.

### Mechanism 2
Separate normalization enables better uniformity preservation in [CLS] embeddings without requiring explicit contrastive loss. Batch normalization on [CLS] provides an implicit negative term similar to contrastive methods by normalizing across the batch dimension. This creates natural separation between different [CLS] embeddings without requiring explicit negative sampling. Layer normalization applied to tokens maintains their feature relationships without interfering with [CLS] uniformity.

### Mechanism 3
Separate normalization allows [CLS] embeddings to better encode global contextual information by avoiding competition with token features for representational capacity. When sharing normalization, [CLS] embeddings must compete with token features for representational space within the same normalized sphere. This competition forces [CLS] to compress its information content. Separate normalization gives [CLS] exclusive access to its own representational space, allowing it to capture more global context without interference.

## Foundational Learning

- Concept: Layer normalization vs batch normalization
  - Why needed here: Understanding the fundamental differences between these normalization techniques is crucial since SepNorm specifically uses both in combination
  - Quick check question: What dimension does layer normalization normalize across versus batch normalization?

- Concept: Dimensional collapse in self-supervised learning
  - Why needed here: The paper's central claim is that separate normalization prevents dimensional collapse in [CLS] embeddings
  - Quick check question: What is dimensional collapse and why does it typically occur in self-supervised representation learning?

- Concept: Uniformity metrics in representation learning
  - Why needed here: The paper uses uniformity metrics to evaluate embedding quality and demonstrate SepNorm's effectiveness
  - Quick check question: How is the uniformity metric calculated and what does it measure about embedding distributions?

## Architecture Onboarding

- Component map: Input tokens → [CLS] token + sequence tokens → Separate normalization (BN for [CLS], LN for tokens) → Transformer layers → Final [CLS] embedding → Downstream classifier

- Critical path: Token embedding and [CLS] addition → Separate normalization application → Self-attention and feed-forward layers → Final [CLS] embedding extraction → Downstream task prediction

- Design tradeoffs:
  - Parameter count: SepNorm adds parameters (γ, β for both normalization types)
  - Training stability: SepNorm may improve stability by reducing conflicting gradients
  - Computational overhead: Minimal additional computation beyond parameter count
  - Flexibility: Can mix normalization types based on domain requirements

- Failure signatures:
  - Performance worse than baseline: Incorrect normalization configuration or implementation bug
  - No improvement despite correct implementation: [CLS] and token distributions are too similar
  - Training instability: Improper initialization or learning rate scaling issues

- First 3 experiments:
  1. Implement SepNorm with BN for [CLS] and LN for tokens on a small image classification dataset
  2. Compare uniformity metrics between shared and separate normalization configurations
  3. Test different normalization combinations (BN+BN, LN+LN, BN+LN) to identify optimal configuration for specific domain

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of normalization layer (LN vs. BN) for the [CLS] symbol affect the uniformity of the embeddings in contrastive methods? While the paper demonstrates that BN improves uniformity compared to LN, it does not provide a detailed analysis of how the choice of normalization layer affects the uniformity in contrastive methods. Experiments comparing the uniformity of [CLS] embeddings using different normalization layers (LN vs. BN) in contrastive methods, such as MAE and U-MAE, would provide insights into the impact of the choice of normalization layer on uniformity.

### Open Question 2
How does the uniformity of token embeddings affect the downstream task performance, even though only the [CLS] embeddings are used for classification? While the paper mentions that the uniformity of token embeddings also benefits the downstream tasks, it does not provide a detailed analysis of the underlying mechanisms. Experiments investigating the relationship between token uniformity and downstream task performance, such as varying the uniformity of token embeddings and measuring the impact on classification accuracy, would shed light on the role of token uniformity in improving [CLS] representations.

### Open Question 3
How does the proposed SepNorm method compare to other normalization techniques, such as Powernorm, in terms of stability and performance in transformers? While the paper focuses on the effectiveness of SepNorm compared to ShareNorm, it does not directly compare it to other normalization techniques like Powernorm. Experiments comparing the performance and stability of SepNorm with other normalization techniques, such as Powernorm, in transformers would provide insights into the relative advantages and disadvantages of different normalization approaches.

## Limitations

- The paper's claims about dimensional collapse prevention and uniformity improvement lack direct mechanistic evidence and rely on observed correlations rather than established causal relationships
- The 2.7% average improvement may not justify the additional complexity in all applications, particularly for smaller or simpler tasks
- The specific normalization combinations that work best vary by domain, suggesting the benefits may be domain-dependent rather than universally applicable

## Confidence

High confidence in the empirical findings: The paper presents consistent 2.7% average performance improvements across diverse domains with proper experimental controls and statistically significant results.

Medium confidence in the mechanism explanation: While the paper provides plausible explanations for why separate normalization works (reduced dimensional collapse, better uniformity preservation, improved global context encoding), these mechanisms are not directly validated through gradient analysis or ablation studies.

Low confidence in domain generalizability: The paper shows results on image, NLP, and graph tasks, but the optimal normalization combinations vary by domain (BN+LN for CV, LN+BN for NLP), suggesting the benefits may be domain-dependent rather than universally applicable.

## Next Checks

1. Gradient analysis validation: Track and compare gradient norms and directions for [CLS] embeddings under shared versus separate normalization configurations to directly verify the dimensional collapse prevention hypothesis.

2. Ablation study on representation capacity: Systematically vary the dimensionality of [CLS] embeddings under both normalization schemes to test whether separate normalization genuinely provides more effective representational capacity.

3. Batch size sensitivity analysis: Evaluate the uniformity preservation mechanism by testing SepNorm across different batch sizes to determine the minimum batch size required for effective implicit negative term generation from batch normalization.