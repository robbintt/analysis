---
ver: rpa2
title: 'Multimodal Brain-Computer Interface for In-Vehicle Driver Cognitive Load Measurement:
  Dataset and Baselines'
arxiv_id: '2304.04273'
source_url: https://arxiv.org/abs/2304.04273
tags:
- cognitive
- load
- driving
- data
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel multimodal cognitive load dataset
  called CL-Drive, which contains EEG, ECG, EDA, and gaze data collected from 21 participants
  while driving in a simulator under various conditions. The dataset includes frequent
  subjective cognitive load ratings every 10 seconds as ground truth.
---

# Multimodal Brain-Computer Interface for In-Vehicle Driver Cognitive Load Measurement: Dataset and Baselines

## Quick Facts
- arXiv ID: 2304.04273
- Source URL: https://arxiv.org/abs/2304.04273
- Authors: Multiple authors
- Reference count: 40
- One-line primary result: Novel multimodal cognitive load dataset with 83.67% binary classification accuracy

## Executive Summary
This paper introduces CL-Drive, a novel multimodal dataset for driver cognitive load measurement that combines EEG, ECG, EDA, and gaze data with frequent subjective ratings. The dataset contains data from 21 participants performing simulated driving tasks under various cognitive load conditions. The authors provide comprehensive benchmarking results using both classical machine learning and deep learning approaches, demonstrating that multimodal fusion significantly outperforms unimodal approaches with up to 83.67% accuracy in binary classification tasks.

## Method Summary
The study collected multimodal physiological data from 21 participants during simulated driving tasks using a validated driving simulator. Data included EEG, ECG, EDA, and gaze signals sampled at 250Hz, 512Hz, 4Hz, and 120Hz respectively. Subjective cognitive load was measured using PAAS scale every 10 seconds during 3-minute driving scenarios. The dataset was split into training (70%), validation (15%), and testing (15%) sets, with binary and ternary classification tasks performed using 10-fold cross-validation and leave-one-subject-out evaluation.

## Key Results
- Multimodal approaches combining EEG, ECG, EDA, and gaze data outperform unimodal setups
- XGB and VGG-style networks show strong performance in cognitive load classification
- Binary classification achieves up to 83.67% accuracy in 10-fold setup and 76.17% in LOSO setup
- Feature extraction yields 40 EEG features, 53 ECG features, 30 EDA features, and 32 gaze features

## Why This Works (Mechanism)

### Mechanism 1
Multimodal fusion of EEG, ECG, EDA, and gaze signals improves cognitive load classification accuracy by capturing complementary aspects of cognitive load. EEG reflects direct neural activity, while ECG and EDA capture autonomic nervous system responses, and gaze data indicates visual attention. Fusing these signals leverages diverse information sources and reduces noise impact. Core assumption: Each modality provides unique, non-redundant information. Break condition: Severe corruption or missing data in one or more modalities.

### Mechanism 2
Frequent subjective self-reported cognitive load scores every 10 seconds serve as reliable ground truth labels by providing dense, temporally aligned labels that capture dynamic cognitive load changes. This allows models to learn fine-grained patterns in physiological signals corresponding to cognitive load changes. Core assumption: Participants can accurately self-assess their cognitive load at frequent intervals. Break condition: Participant inability to provide accurate ratings due to distraction, fatigue, or misunderstanding.

### Mechanism 3
Using both classical machine learning and deep learning models trained on extracted features and raw data allows comprehensive benchmarking by leveraging different model strengths. Classical ML models excel at handcrafted features while deep learning models learn complex patterns from raw data. Core assumption: Dataset is sufficiently large and diverse for both approaches. Break condition: Dataset too small or features not informative, leading to overfitting or poor performance.

## Foundational Learning

- Concept: Understanding of cognitive load and its measurement in driving contexts
  - Why needed here: Study focuses on measuring cognitive load induced by driving tasks
  - Quick check question: What are the two main categories of cognitive load described in the literature, and how do they differ?

- Concept: Familiarity with physiological signals and their relationship to cognitive load
  - Why needed here: Study uses EEG, ECG, EDA, and gaze data to measure cognitive load
  - Quick check question: Which physiological signal directly measures neural activity in the brain, and which ones capture autonomic nervous system responses?

- Concept: Knowledge of machine learning and deep learning techniques for classification
  - Why needed here: Study uses both classical ML and deep learning models for cognitive load classification
  - Quick check question: What is the main advantage of using deep learning models over classical ML models for this task, and what is a potential drawback?

## Architecture Onboarding

- Component map: Data collection (simulator, sensors, ratings) -> Preprocessing (filtering, artifact removal, feature extraction) -> Models (ML and deep learning) -> Evaluation (10-fold CV, LOSO, binary/ternary classification)

- Critical path: Collect data from 21 participants in driving simulator -> Preprocess physiological signals and extract features -> Train and evaluate classical ML and deep learning models on features and raw data -> Compare performance of different models and modality combinations

- Design tradeoffs: Multiple modalities increase accuracy but increase data complexity and processing requirements; frequent subjective ratings provide dense labels but may be affected by participant fatigue; training on both features and raw data allows comprehensive benchmarking but increases computational cost

- Failure signatures: Poor classification accuracy may indicate data quality, feature extraction, or model selection issues; overfitting may occur with small dataset or complex models; class imbalance may affect performance in ternary classification

- First 3 experiments:
  1. Train logistic regression model on EEG features only to establish baseline performance
  2. Train XGBoost model on all extracted features (EEG, ECG, EDA, gaze) to evaluate multimodal fusion benefit
  3. Train VGG-style deep learning model on raw EEG data to compare end-to-end learning with feature-based approaches

## Open Questions the Paper Calls Out

### Open Question 1
How does cognitive load measured during simulated driving compare to real-world driving conditions? Basis: Study uses simulator and acknowledges simulation adaptation syndrome concerns. Why unresolved: Simulator may not capture real-world complexity and unpredictability. What evidence would resolve it: Comparative studies measuring cognitive load in both simulator and real-world driving using same sensors and protocols.

### Open Question 2
How do demographic factors (age, gender, driving experience) influence relationship between physiological signals and cognitive load during driving? Basis: Study collected data from 21 participants but did not perform detailed demographic analysis. Why unresolved: Dataset includes diverse participants but analysis didn't explore individual differences. What evidence would resolve it: Analysis stratified by demographic factors or targeted data collection with balanced representation.

### Open Question 3
How does frequency of subjective cognitive load ratings (every 10 seconds) affect accuracy and reliability of automated cognitive load detection models? Basis: Study collected ratings every 10 seconds but didn't explore impact of sampling rate. Why unresolved: While frequent ratings provide dense ground truth, optimal frequency for balancing data quality and practical implementation is unknown. What evidence would resolve it: Comparative analysis of model performance using different rating frequencies (5, 10, 30, or 60 seconds).

## Limitations

- Dataset size of 21 participants may limit generalizability and lead to overfitting, particularly in LOSO validation
- Simulated driving environment may not fully capture real-world driving complexity and unpredictability
- Subjective cognitive load ratings may introduce temporal misalignment with physiological signals due to potential delays in physiological responses

## Confidence

- **High Confidence**: Multimodal fusion of EEG, ECG, EDA, and gaze signals improves classification accuracy, supported by strong empirical evidence and established principles of physiological signal complementarity
- **Medium Confidence**: Reliability of subjective self-reported cognitive load ratings as ground truth labels, reasonable but dependent on participant compliance and accurate self-assessment
- **Low Confidence**: Assertion that dataset and baselines will significantly advance the field, forward-looking claim not directly validated within the study

## Next Checks

1. **Dataset Size and Diversity Validation**: Conduct power analysis to determine if 21 participants is sufficient for robust conclusions, particularly for LOSO validation; explore impact of adding more participants or using cross-dataset validation

2. **Ground Truth Label Validation**: Compare subjective cognitive load ratings with objective measures (e.g., task performance metrics) to assess label validity; investigate temporal alignment between ratings and physiological signals

3. **Real-World Applicability Check**: Test models trained on simulated driving data on small set of real-world driving data (if available) to evaluate generalizability; alternatively, simulate real-world conditions within driving simulator to assess robustness