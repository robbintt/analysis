---
ver: rpa2
title: Probabilistic Self-supervised Learning via Scoring Rules Minimization
arxiv_id: '2309.02048'
source_url: https://arxiv.org/abs/2309.02048
tags:
- scoring
- learning
- network
- rule
- probabilistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ProSMin, a novel probabilistic self-supervised\
  \ learning framework that leverages scoring rules minimization to enhance representation\
  \ quality and mitigate collapsing representations. The method employs two neural\
  \ networks\u2014an online network and a target network\u2014that learn diverse representations\
  \ through knowledge distillation."
---

# Probabilistic Self-supervised Learning via Scoring Rules Minimization

## Quick Facts
- arXiv ID: 2309.02048
- Source URL: https://arxiv.org/abs/2309.02048
- Reference count: 40
- Primary result: ProSMin achieves superior accuracy and calibration on various downstream tasks, outperforming self-supervised baselines on large-scale datasets like ImageNet-O and ImageNet-C.

## Executive Summary
ProSMin introduces a novel probabilistic self-supervised learning framework that leverages scoring rules minimization to enhance representation quality and mitigate collapsing representations. The method employs two neural networks—an online network and a target network—that learn diverse representations through knowledge distillation. By presenting input samples in two augmented formats, the online network predicts the target network's representation under different views using a custom loss function based on proper scoring rules. Theoretical analysis demonstrates the strict propriety of the modified scoring rule, validating the method's optimization process.

## Method Summary
ProSMin is a self-supervised learning framework that uses two neural networks (online and target) with ViT-S/16 backbone, projector and predictor heads. The online network samples from a multivariate normal distribution using the reparameterization trick and generates multiple augmented samples. The target network processes a second view of the input to obtain target representations. The method minimizes a scoring rule loss combining energy score and kernel score components between online samples and target representations. The target network parameters are updated via exponential moving average of the online network parameters. The framework is trained using AdamW optimizer with cosine learning rate schedule on ImageNet for 300 epochs.

## Key Results
- ProSMin achieves superior accuracy and calibration on in-distribution generalization, out-of-distribution detection, dataset corruption, low-shot learning, and transfer learning tasks
- Outperforms self-supervised baselines on large-scale datasets like ImageNet-O and ImageNet-C
- Demonstrates better calibration and accuracy compared to existing methods across multiple downstream evaluation protocols

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProSMin avoids representation collapse by enforcing diversity through probabilistic modeling
- Mechanism: The energy score term `S2(Pθ, zξ)` penalizes the model for making representations too similar to each other across the r samples, while the distillation term `S1(Pθ, zξ)` aligns predictions with the target network. The convex combination with hyperparameter λ controls the balance between diversity and alignment.
- Core assumption: The probabilistic output distributions have sufficient variance to generate diverse samples that can be meaningfully compared
- Evidence anchors:
  - "Our framework can benefit from more layers in the online network and centering on the target outputs to improve the results. Still, our method inherently can not collapse to the same representation because of its probabilistic nature."
  - "The energy score with β = 1 (L1 loss) yields the best performance among the tested scoring rules"

### Mechanism 2
- Claim: The modified scoring rule is strictly proper, ensuring convergence to the true distribution
- Mechanism: By proving that S*(Pξ, Pξ) < S*(Pθ, Pξ) for all Pθ ≠ Pξ, the optimization process is guaranteed to push the online network distribution toward the target network distribution
- Core assumption: The target network's output distribution is the "true" distribution we want to recover
- Evidence anchors:
  - "We provide proof in following Section 4.3" with the mathematical derivation showing strict propriety
  - "By utilizing a strictly proper scoring rule and retaining the ability to calculate gradients as usual (as proven in 9.5.4), we can infer that our algorithm will converge towards the desired minimum"

### Mechanism 3
- Claim: Self-distillation with EMA creates a moving target that prevents overfitting to a static distribution
- Mechanism: The target network parameters ξ are updated via exponential moving average of the online network parameters θ, creating a slowly evolving target distribution that the online network must continuously adapt to
- Core assumption: The EMA update rate (α hyperparameter) is properly balanced between stability and adaptation
- Evidence anchors:
  - "The parameters of the target network are updated through the EMA of the weights from the online network... saving the need for backpropagation and thus reducing computation time considerably"
  - "α commences from 0.9, implying a faster pace of knowledge distillation" (from ablation study)

## Foundational Learning

- Concept: Scoring rules and proper scoring rules
  - Why needed here: The entire optimization objective is based on minimizing a scoring rule between predicted and target distributions
  - Quick check question: What property must a scoring rule have to incentivize truthful probability reporting, and why is this important for representation learning?

- Concept: Reparameterization trick for gradient backpropagation through stochastic nodes
  - Why needed here: The method generates samples from the predictor's output distribution but needs to backpropagate through the mean and variance parameters
  - Quick check question: How does the reparameterization trick allow backpropagation through a sampling operation, and what are the requirements for this to work?

- Concept: Knowledge distillation and self-distillation
  - Why needed here: The target network learns from the online network through a distillation process, which is central to the two-network architecture
  - Quick check question: What is the difference between traditional knowledge distillation and self-distillation, and how does EMA play a role in the latter?

## Architecture Onboarding

- Component map: Input augmentation layer → Online network (encoder + projector + predictor) → Sample generation → Scoring rule computation → Input augmentation layer → Target network (encoder + projector) → Target representation → EMA update mechanism connecting online and target networks

- Critical path:
  1. Generate two augmented views of input
  2. Online network processes first view through predictor to get probabilistic output
  3. Generate r samples from predicted distribution using reparameterization trick
  4. Target network processes second view to get target representation
  5. Compute scoring rule loss between online samples and target representation
  6. Backpropagate through online network only
  7. Update target network via EMA

- Design tradeoffs:
  - Number of samples r: More samples provide better scoring rule estimates but increase computation
  - Embedding size: Larger embeddings capture more information but require more parameters and memory
  - λ parameter: Controls balance between diversity enforcement and alignment; needs tuning per dataset
  - EMA rate α: Faster updates provide more adaptive targets but may reduce stability

- Failure signatures:
  - Training loss plateaus at high value: Could indicate improper scoring rule implementation or collapsed variance
  - Online and target representations become identical: May indicate λ too high or EMA too fast
  - Poor downstream performance despite low training loss: Could indicate overfitting to pretext task or insufficient diversity

- First 3 experiments:
  1. Implement basic version with energy score only (λ=1) and test on CIFAR-10 to verify basic functionality
  2. Add kernel score component and tune λ parameter to find optimal balance
  3. Compare different sample sizes r (4, 8, 16) to find sweet spot between performance and computation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ProSMin vary with different values of the hyperparameter β in the energy score?
- Basis in paper: The paper mentions that β ∈ (0, 2) and provides a theoretical explanation for L1 (β = 1) and L2 (β = 2) losses, but does not extensively explore the impact of varying β within this range.
- Why unresolved: The paper only briefly mentions the theoretical implications of different β values and does not provide empirical results for a range of β values.
- What evidence would resolve it: Empirical results showing the performance of ProSMin with different values of β in the energy score, across various downstream tasks and datasets.

### Open Question 2
- Question: What is the impact of using different kernel functions in the kernel score objective function?
- Basis in paper: The paper mentions using a Gaussian kernel but does not explore the performance impact of using other kernel functions.
- Why unresolved: The paper only briefly mentions the use of a Gaussian kernel and does not compare its performance to other kernel functions.
- What evidence would resolve it: Empirical results comparing the performance of ProSMin using different kernel functions in the kernel score objective function.

### Open Question 3
- Question: How does the number of augmented samples (r) affect the performance of ProSMin?
- Basis in paper: The paper mentions that r is the number of augmentation samples multiplied by the number of samples, but does not provide a detailed analysis of how varying r affects performance.
- Why unresolved: The paper only briefly mentions the number of augmented samples and does not provide a thorough investigation of its impact on performance.
- What evidence would resolve it: Empirical results showing the performance of ProSMin with different numbers of augmented samples (r), across various downstream tasks and datasets.

### Open Question 4
- Question: What is the effect of using different backbone architectures (e.g., ViT vs. ResNet) on the performance of ProSMin?
- Basis in paper: The paper mentions using ViT and ResNet as backbone architectures but does not provide a detailed comparison of their performance.
- Why unresolved: The paper only briefly mentions the use of different backbone architectures and does not provide a thorough analysis of their impact on performance.
- What evidence would resolve it: Empirical results comparing the performance of ProSMin using different backbone architectures (e.g., ViT vs. ResNet), across various downstream tasks and datasets.

### Open Question 5
- Question: How does the performance of ProSMin compare to other self-supervised learning methods on larger-scale datasets beyond ImageNet?
- Basis in paper: The paper mentions experiments on ImageNet-O and ImageNet-C but does not explore performance on other large-scale datasets.
- Why unresolved: The paper only briefly mentions experiments on ImageNet-O and ImageNet-C and does not provide a comprehensive comparison with other self-supervised learning methods on larger-scale datasets.
- What evidence would resolve it: Empirical results comparing the performance of ProSMin to other self-supervised learning methods on larger-scale datasets beyond ImageNet.

## Limitations

- The method's dependence on proper hyperparameter tuning (λ, α, r) is not fully explored, with performance potentially sensitive to initialization and dataset characteristics
- Computational overhead of generating multiple samples (r parameter) is mentioned but not thoroughly analyzed in terms of wall-clock time or memory requirements compared to other SSL methods
- Evaluation focuses heavily on ViT architectures, leaving questions about performance on CNN backbones or smaller models

## Confidence

- **High confidence**: The basic mechanism of using scoring rules for representation learning and the mathematical proof of strict propriety (assuming correct implementation)
- **Medium confidence**: Claims about superior performance across multiple downstream tasks, as these depend on specific hyperparameter choices and implementation details
- **Low confidence**: Claims about computational efficiency and scalability, given the lack of detailed runtime analysis

## Next Checks

1. **Implementation verification**: Reproduce the scoring rule implementation and verify that the strict propriety proof holds numerically for the modified scoring rule with various λ values in the claimed optimal range (0.26-0.40).

2. **Hyperparameter sensitivity analysis**: Systematically vary λ, α, and r parameters across their reported ranges to identify performance plateaus and determine whether the reported "optimal" values are truly robust or overfitted to specific runs.

3. **Cross-architecture validation**: Test ProSMin on non-ViT architectures (e.g., ResNet-50) using the same hyperparameters to verify that performance gains are not architecture-specific and to assess generalization across backbone choices.