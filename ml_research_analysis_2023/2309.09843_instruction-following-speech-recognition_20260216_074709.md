---
ver: rpa2
title: Instruction-Following Speech Recognition
arxiv_id: '2309.09843'
source_url: https://arxiv.org/abs/2309.09843
tags:
- speech
- instructions
- skill
- training
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces instruction-following speech recognition,
  a novel approach to training end-to-end ASR models to understand and execute diverse
  free-form text instructions. The key idea is to train a Listen-Attend-Spell (LAS)
  model on Librispeech data, conditioning it to perform various ASR-related tasks
  based on text instructions, without relying on pre-trained speech or text foundation
  models.
---

# Instruction-Following Speech Recognition

## Quick Facts
- arXiv ID: 2309.09843
- Source URL: https://arxiv.org/abs/2309.09843
- Reference count: 0
- Key outcome: Train end-to-end ASR model to understand and execute diverse free-form text instructions without pre-trained components, achieving competitive Librispeech performance while adding instruction-following capability

## Executive Summary
This paper introduces instruction-following speech recognition, training a Listen-Attend-Spell (LAS) model to understand and execute diverse free-form text instructions for various ASR-related tasks. The key innovation is conditioning the model to perform tasks like speech transcription, word replacement, transcript manipulation, and summarization based on text instructions, all without relying on pre-trained speech or text foundation models. Remarkably, the model achieves competitive ASR performance (~2.4% WER on Librispeech test-other) while demonstrating the ability to interpret and execute simple instructions, including unseen ones.

## Method Summary
The method trains a Conformer-Transformer LAS model on Librispeech 960h with instruction-based training. For each training step, the model samples an instruction from 100-600 options per skill, modifies the transcript accordingly, and prepends the instruction to the target output. The decoder learns to condition on these prefix tokens during autoregressive generation. Instruction sampling uses weighted sampling (α=56 for transcription, β=4 for summarization) to maintain ASR performance while acquiring additional skills. The model is trained from scratch without pre-trained components.

## Key Results
- Instruction-following LAS model achieves 2.4% WER on Librispeech test-other, matching standard LAS performance
- Model executes unseen instructions with ~80% accuracy, demonstrating generalization beyond memorized patterns
- Instruction-following capability emerges without pre-trained speech or text foundation models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model learns to interpret free-form text instructions by seeing them as prefix tokens to the autoregressive decoder.
- Mechanism: During training, each utterance is paired with a randomly sampled instruction token sequence. The instruction is prepended to the target transcript, separated by an [EOT] token. The decoder learns to condition on these prefix tokens when generating the final output.
- Core assumption: The autoregressive decoder can attend to both the instruction prefix and the speech encoder context simultaneously via cross-attention, enabling dynamic task switching.
- Evidence anchors:
  - [abstract] "training a Listen-Attend-Spell model to understand and execute a diverse set of free-form text instructions"
  - [section 3] "During test time, instructions concatenated with an [EOT] token serve as prefixes to the LAS decoder."
  - [corpus] Weak: no direct evidence of prefix conditioning in related work; needs inference.

### Mechanism 2
- Claim: Instruction-following generalization is achieved through instruction sampling and target modification, not through memorizing specific instructions.
- Mechanism: For each training step, the model samples an instruction from a large set (100-600 per skill) and modifies the transcript accordingly. This forces the model to learn the semantic mapping between instructions and operations rather than the exact text.
- Core assumption: The variety and diversity of instructions in the training set ensure that the model learns the instruction-action mapping, not the exact instruction strings.
- Evidence anchors:
  - [section 2.2] "This approach not only ensures diverse instructions but also narrows the scope of out-of-distribution instructions during inference, compelling the model to reason over the text query rather than memorizing it directly."
  - [section 4.2] "Generalization to Unseen Instructions. The model executes unseen instructions with ~80% accuracy, providing direct evidence of its instruction understanding."
  - [corpus] Weak: related work shows LLM-based prompting but not this exact sampling approach.

### Mechanism 3
- Claim: The instruction-following capability does not degrade baseline ASR performance because the speech transcription skill is sampled with high probability during training.
- Mechanism: The training objective uses weighted sampling (α=56 for transcription, β=4 for summarization) ensuring that the majority of updates optimize for standard ASR while still learning other skills.
- Core assumption: By keeping transcription as the dominant training signal, the model maintains strong ASR capabilities while acquiring additional skills.
- Evidence anchors:
  - [section 3] "For instruction sampling, we configure α = 56 and β = 4."
  - [section 4.1] "The results suggest that instruction-based training does not degrade ASR performance"
  - [corpus] Weak: no direct evidence in related work about maintaining ASR performance during multitask training.

## Foundational Learning

- Concept: Autoregressive sequence modeling with teacher forcing
  - Why needed here: The model must learn to predict the next token given the instruction prefix and speech encoder context. Teacher forcing ensures stable training by providing the correct previous tokens during training.
  - Quick check question: What would happen if we used scheduled sampling instead of teacher forcing in this instruction-following setup?

- Concept: Cross-attention in encoder-decoder architectures
  - Why needed here: The decoder must attend to the entire speech encoder context to perform tasks like word replacement and summarization, which require understanding the full utterance.
  - Quick check question: How would the model's performance change if we used a CTC-based architecture instead of LAS for instruction-following?

- Concept: Token-level task specification through prefix conditioning
  - Why needed here: Instructions must be converted into a format the model can condition on during both training and inference. The prefix approach allows the same model to handle multiple tasks.
  - Quick check question: What are the limitations of using prefix conditioning for instruction-following compared to a separate instruction encoder?

## Architecture Onboarding

- Component map: Speech → Conformer encoder → Context vectors → Transformer decoder → Cross-attention → Next token prediction
- Critical path:
  1. Speech → Conformer encoder → Context vectors
  2. Instruction + [EOT] + target → Decoder input
  3. Cross-attention: decoder attends to encoder context
  4. Next token prediction with teacher forcing
- Design tradeoffs:
  - Autoregressive vs non-autoregressive: Autoregressive allows prefix conditioning but is slower
  - Conformer vs Transformer encoder: Conformer better for speech, but Transformer might be simpler
  - Sampling weights: Higher α maintains ASR but may limit skill acquisition
- Failure signatures:
  - Low WER but poor instruction-following: model learned ASR but not instruction semantics
  - High WER: training instability or insufficient speech modeling capacity
  - Inconsistent skill execution: insufficient instruction diversity or poor target modification
- First 3 experiments:
  1. Train baseline LAS on Librispeech without instructions, verify WER matches reported values
  2. Train with only transcription skill (α=1, β=0), verify instruction-following doesn't emerge
  3. Train with balanced sampling (α=1, β=1), observe degradation in ASR performance

## Open Questions the Paper Calls Out

- What is the precise contribution of pre-trained components (speech encoder, LLM decoder) to the instruction-following capabilities of Speech LLMs, and how much can these capabilities be replicated with a small model trained from scratch?
- How does the model's performance on instruction-following tasks generalize to out-of-distribution (OOD) instructions with novel vocabulary, sentence structures, or task types not seen during training?
- What are the precise mechanisms by which the model learns to understand and execute diverse ASR-related tasks from instructions, and how does this learning evolve over the course of training?
- How does instruction-following ASR compare to traditional ASR+LLM cascading pipelines in terms of privacy, safety, and ability to handle sensitive information?

## Limitations

- The model is trained exclusively on Librispeech, which contains only read speech from a single speaker per utterance, limiting understanding of performance in multi-speaker or conversational environments
- Evaluation focuses primarily on instruction-following accuracy and WER on Librispeech, lacking systematic evaluation of failure modes like contradictory instructions or ambiguous phrasing
- The instruction set diversity and sampling mechanism may not capture the full spectrum of natural language instructions that users might provide in real-world applications

## Confidence

- **High Confidence**: The core claim that instruction-following training does not degrade baseline ASR performance (WER ~2.4% on test-other) is well-supported by experimental results
- **Medium Confidence**: The claim of generalization to unseen instructions (~80% accuracy) is supported by evaluation data but requires further validation across diverse instruction types
- **Low Confidence**: The assertion that the model achieves "true instruction understanding" rather than pattern matching is the weakest claim, as evaluation does not systematically probe semantic comprehension

## Next Checks

- Conduct a systematic analysis of instruction complexity by categorizing instructions into simple (word-level changes) and complex (semantic transformations) tasks, then measuring performance degradation as complexity increases
- Test the model on a multi-speaker, conversational speech dataset (e.g., Switchboard) to evaluate whether instruction-following capabilities transfer beyond Librispeech's read speech domain
- Implement an adversarial instruction test suite containing contradictory instructions, ambiguous phrasing, and instructions requiring external knowledge to stress-test instruction understanding capabilities