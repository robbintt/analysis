---
ver: rpa2
title: Less is More for Long Document Summary Evaluation by LLMs
arxiv_id: '2309.07382'
source_url: https://arxiv.org/abs/2309.07382
tags:
- summary
- evaluation
- document
- source
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of expensive and less effective
  long document summary evaluation by large language models (LLMs), which struggle
  with high computational costs and the Lost-in-the-Middle problem. The proposed Extract-then-Evaluate
  method extracts key sentences from source documents and evaluates summaries based
  on this condensed version.
---

# Less is More for Long Document Summary Evaluation by LLMs

## Quick Facts
- arXiv ID: 2309.07382
- Source URL: https://arxiv.org/abs/2309.07382
- Authors: 
- Reference count: 40
- Primary result: Extract-then-Evaluate method reduces evaluation costs by up to 50% while achieving Pearson correlations up to 0.85 with human judgments

## Executive Summary
This paper addresses the challenge of evaluating long document summaries using large language models (LLMs), which suffer from high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is overlooked. The proposed Extract-then-Evaluate method extracts key sentences from source documents and evaluates summaries based on this condensed version, achieving higher correlations with human judgments while reducing costs. Experiments on four datasets (arXiv, GovReport, PubMed, SQuALITY) demonstrate that document length of 1000-2000 tokens and ROUGE-based sentence extraction provide optimal performance.

## Method Summary
The Extract-then-Evaluate approach involves three main steps: (1) sentence extraction from long source documents using one of four methods (LEAD, ROUGE, BERTScore, or NLI), (2) condensing extracted sentences to target length (1000-2000 tokens), and (3) using GPT-4 with deterministic prompts to evaluate summaries based on the condensed document. The method addresses both computational efficiency and the Lost-in-the-Middle problem by focusing LLM attention on the most relevant content. Four datasets were used for evaluation: arXiv, GovReport, PubMed, and SQuALITY, each containing document-summary pairs with human judgment scores.

## Key Results
- Evaluation costs reduced by up to 50% compared to using full documents
- Pearson correlation with human judgments reaches up to 0.85
- Optimal document length identified as 1000-2000 tokens
- ROUGE-based sentence extraction consistently outperforms other methods across all datasets and evaluation criteria

## Why This Works (Mechanism)

### Mechanism 1
Extracting key sentences before LLM evaluation reduces computational costs and improves alignment with human judgments. By condensing long documents into shorter, information-dense versions (1000-2000 tokens), LLMs can process inputs more efficiently while retaining critical information for evaluation. This works because LLMs perform better on shorter, curated inputs than on full-length documents due to reduced noise and improved focus.

### Mechanism 2
The Lost-in-the-Middle problem causes LLMs to overlook important information in the middle of long documents. When processing long sequences, LLMs allocate disproportionate attention to document beginnings and ends, causing middle content to be underrepresented in their analysis. This architectural limitation of transformer-based LLMs inherently limits their ability to maintain equal attention across extremely long sequences.

### Mechanism 3
ROUGE-based sentence extraction methods consistently outperform other approaches across different datasets and evaluation criteria. ROUGE-based methods extract sentences that maximize recall with the model-generated summary, effectively capturing content most relevant to the summary being evaluated. This works because sentences with high ROUGE overlap with the summary are more likely to contain the key information needed for accurate evaluation.

## Foundational Learning

- Concept: Pearson correlation coefficient
  - Why needed here: Used to measure alignment between automatic evaluation scores and human judgments
  - Quick check question: If evaluation metric scores increase but human scores decrease, what happens to Pearson correlation?

- Concept: Spearman's rank correlation
  - Why needed here: Provides complementary measure to Pearson by focusing on rank order rather than absolute values
  - Quick check question: When would Spearman correlation be more informative than Pearson correlation for evaluation metrics?

- Concept: Transformer attention mechanisms
  - Why needed here: Understanding how LLMs process long sequences helps explain why information loss occurs in the middle
  - Quick check question: What architectural limitation of standard transformers causes attention to concentrate at sequence boundaries?

## Architecture Onboarding

- Component map: Source document → Sentence extraction module → Condensed document → LLM evaluation prompt → Score output
- Critical path:
  1. Receive source document and summary pair
  2. Extract key sentences based on chosen method
  3. Concatenate extracted sentences to target length (1000-2000 tokens)
  4. Format prompt for LLM evaluation
  5. Submit to LLM and receive score
  6. Calculate correlation with human judgments

- Design tradeoffs:
  - Cost vs. accuracy: Shorter documents save money but may lose information
  - Extraction method vs. domain: ROUGE works well for scientific texts but may underperform on general domains
  - Fixed vs. adaptive length: Fixed lengths simplify implementation but may not optimize for all documents

- Failure signatures:
  - Poor correlation despite low cost: Extraction method is missing critical information
  - High cost with no improvement: Document length exceeds optimal range
  - Inconsistent results across runs: Temperature parameter not set to 0 in LLM evaluation

- First 3 experiments:
  1. Compare correlation at different document lengths (128, 256, 512, 1024, 2048 tokens) using ROUGE-1 extraction
  2. Test all four extraction methods at optimal length to identify best performer
  3. Validate that setting temperature=0 produces deterministic results across multiple runs

## Open Questions the Paper Calls Out

### Open Question 1
How does the Extract-then-Evaluate approach perform when using different LLM architectures (e.g., GPT-3.5, Claude, PaLM) compared to GPT-4? The study only used GPT-4 due to resource limitations and did not test the method's generalizability across different LLM architectures.

### Open Question 2
What is the optimal balance between document length and summary length for evaluation, and how does this ratio affect correlation with human judgments? While the paper establishes minimum document length requirements, it doesn't investigate how different document-to-summary length ratios impact evaluation quality or computational efficiency.

### Open Question 3
How do different sentence extraction methods (LEAD, ROUGE, BERTScore, NLI) perform when evaluated using alternative human judgment metrics beyond Pearson and Spearman correlations? The study focuses on two correlation metrics but doesn't explore how extraction methods perform when evaluated using other human judgment metrics like Cohen's kappa or qualitative assessment.

## Limitations

- The optimal document length of 1000-2000 tokens was determined on scientific and government documents and may not generalize to other domains
- The effectiveness of ROUGE-based extraction assumes substantial overlap between summaries and source sentences, which may not hold for highly abstractive summarization
- The evaluation relied exclusively on GPT-4, and results may vary with different LLM architectures or smaller models

## Confidence

**High Confidence**: The cost reduction claims are well-supported, showing up to 50% reduction in evaluation costs through the Extract-then-Evaluate approach.

**Medium Confidence**: The improvement in correlation with human judgments (up to 0.85 Pearson correlation) is convincing for the tested datasets, but generalizability to other domains remains uncertain.

**Low Confidence**: The claim that ROUGE-based methods are universally superior across all domains requires more validation beyond the scientific datasets tested.

## Next Checks

1. Cross-domain validation: Test the Extract-then-Evaluate approach on non-scientific domains (news, social media, fiction) to verify if the 1000-2000 token optimal range and ROUGE-based extraction superiority hold across different writing styles.

2. Alternative LLM evaluation: Replicate the correlation experiments using smaller, more cost-effective LLMs (e.g., GPT-3.5, LLaMA-2) to determine if performance benefits persist with constrained computational resources.

3. Abstractive summarization test: Evaluate the approach using highly abstractive summaries that contain minimal direct sentence overlap with source documents to test ROUGE-based extraction effectiveness.