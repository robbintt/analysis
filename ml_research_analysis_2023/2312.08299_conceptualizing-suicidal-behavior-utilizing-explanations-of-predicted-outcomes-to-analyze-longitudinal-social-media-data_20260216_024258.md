---
ver: rpa2
title: 'Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted Outcomes
  to Analyze Longitudinal Social Media Data'
arxiv_id: '2312.08299'
source_url: https://arxiv.org/abs/2312.08299
tags:
- social
- media
- attribution
- data
- suicidal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a novel method for detecting suicidal ideation
  in social media posts by leveraging token attribution from large language models.
  The approach uses Layer Integrated Gradients to assign attribution scores to tokens
  in Reddit posts, enabling classification without relying on LLMs during inference.
---

# Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted Outcomes to Analyze Longitudinal Social Media Data

## Quick Facts
- arXiv ID: 2312.08299
- Source URL: https://arxiv.org/abs/2312.08299
- Reference count: 36
- One-line primary result: A novel method for detecting suicidal ideation in social media posts using token attribution from fine-tuned language models, achieving high recall and F1-score when grouping risk levels.

## Executive Summary
This study introduces a novel approach for detecting suicidal ideation in social media posts by leveraging token attribution from fine-tuned language models. The method uses Layer Integrated Gradients to assign attribution scores to tokens in Reddit posts, enabling classification without relying on LLMs during inference. TF-IDF scaling is applied to enhance token attribution performance. The approach is evaluated on a longitudinal dataset, demonstrating promising results, particularly when grouping risk levels into broader categories. This method reduces computational overhead and provides interpretable results, making it a valuable tool for preliminary screening of suicide risk in social media data.

## Method Summary
The method involves fine-tuning a BERT-type model (MentalRoBERTa-base) on labeled Reddit posts from the UMD dataset, then using Layer Integrated Gradients to compute token attributions for each post. These attributions are aggregated across the dataset, with TF-IDF scaling applied to balance token frequencies. The resulting token-level scores are used to classify posts into risk categories. The approach is designed to be computationally efficient by using pre-computed attributions rather than running LLMs during inference, and it is evaluated on both cross-sectional and longitudinal datasets.

## Key Results
- The method achieved a recall of 89.01% and F1-score of 87.01% for no-risk versus any-risk classification.
- TF-IDF scaling improved classification performance, supporting the hypothesis that balancing token frequencies enhances predictive accuracy.
- Longitudinal context aggregation captured temporal patterns in user posts, improving detection of suicidal ideation compared to single-post analysis.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer Integrated Gradients enables attribution of suicidal ideation predictions to specific tokens without using LLMs during inference.
- Mechanism: Layer Integrated Gradients computes gradients with respect to the input tokens, producing attribution scores that indicate each token's contribution to the final prediction. These scores are aggregated across the dataset to identify high-risk tokens.
- Core assumption: The gradient-based attribution method faithfully represents the LLM's reasoning process and generalizes to new, unseen data.
- Evidence anchors:
  - [abstract] "we employed a model explanation method, Layer Integrated Gradients, on top of a fine-tuned state-of-the-art language model, to assign each token from Reddit users' posts an attribution score for predicting suicidal ideation."
  - [section III.B] "Using the labeled text as predicted from the language model, we are able to generate model explanations using Layer Integrated Gradients by computing each token's attribution towards the predicted label."
  - [corpus] Weak evidence; no direct citations to attribution literature in corpus.
- Break condition: If the fine-tuned LLM overfits to training data, attribution scores may not generalize and could mislead classification.

### Mechanism 2
- Claim: TF-IDF scaling improves token attribution performance by balancing frequently used terms.
- Mechanism: TF-IDF weights reduce the influence of common tokens that may dominate attribution scores, allowing rarer but more predictive tokens to have greater impact on classification.
- Core assumption: The relationship between token frequency and predictive power is inversely correlated; common tokens are less informative for distinguishing suicidal ideation.
- Evidence anchors:
  - [abstract] "We also expand upon this idea by implementing TF-IDF scaling on token level, and evaluating the effects on prediction performance."
  - [section IV.C.1] "After implementing the TF-IDF scaling, our hypothesis seems to be supported as shown by higher F1-score when using TF-IDF scaling."
  - [corpus] Weak evidence; no direct TF-IDF scaling citations in corpus.
- Break condition: If the dataset is highly imbalanced or contains domain-specific jargon, TF-IDF scaling could suppress important but frequent terms.

### Mechanism 3
- Claim: Longitudinal context aggregation improves detection of suicidal ideation by capturing temporal patterns in user posts.
- Mechanism: Concatenating posts from the same user over time creates a richer context for the LLM to identify evolving risk indicators that single posts might miss.
- Core assumption: Suicidal ideation risk indicators are expressed consistently over time and across different subreddits, making longitudinal aggregation effective.
- Evidence anchors:
  - [abstract] "Finally, we implement this novel method to predict longitudinal and long context-length... and evaluate the method's performance in predicting suicidal ideation in this case."
  - [section III.A] "The longitudinal dataset is generated by concatenating posts from each users on all subreddit they posted, ordered by timestamp."
  - [corpus] Weak evidence; corpus focuses on detection methods rather than longitudinal modeling.
- Break condition: If users' posting behavior or risk indicators change drastically over time, aggregation could dilute temporally relevant signals.

## Foundational Learning

- Concept: Token Attribution via Layer Integrated Gradients
  - Why needed here: To interpret LLM predictions and extract meaningful features for classification without running the LLM during inference.
  - Quick check question: What is the difference between vanilla gradients and integrated gradients in terms of attribution stability?

- Concept: TF-IDF Weighting
  - Why needed here: To balance the influence of common versus rare tokens in attribution scores, improving classification performance.
  - Quick check question: How does IDF penalize tokens that appear in many documents, and why is this useful for sentiment or risk detection?

- Concept: Longitudinal Data Aggregation
  - Why needed here: To capture temporal patterns and context that single posts cannot provide, improving detection accuracy for evolving risk states.
  - Quick check question: What are the risks of temporal leakage when aggregating user posts across time for model training?

## Architecture Onboarding

- Component map:
  Data ingestion -> Preprocessing (Reddit posts, risk labels) -> Fine-tuning LLM (MentalRoBERTa-base) -> Layer Integrated Gradients attribution extraction -> TF-IDF scaling -> Token aggregation -> Classification model

- Critical path: Fine-tune LLM -> Generate attributions -> Aggregate and scale -> Train classifier -> Evaluate on gold dataset

- Design tradeoffs:
  - Model size vs. inference speed (large vs. base models)
  - Attribution granularity vs. computational cost
  - Context length limits vs. information loss in sliding windows

- Failure signatures:
  - Poor validation accuracy -> overfitting or suboptimal hyperparameters
  - Low recall -> mislabeling or weak attribution signals
  - High variance in attribution scores -> unstable gradients or noisy data

- First 3 experiments:
  1. Fine-tune MentalRoBERTa-base on UMD training set and evaluate validation accuracy.
  2. Apply Layer Integrated Gradients to generate token attributions and visualize top contributing tokens.
  3. Compare classification performance with and without TF-IDF scaling on the gold dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the proposed method for detecting suicidal ideation in social media posts compared to state-of-the-art models?
- Basis in paper: [explicit] The authors state, "This study proposes a novel method for detecting suicidal ideation in social media posts by leveraging token attribution from large language models."
- Why unresolved: The authors mention that future work includes comparing with state-of-the-art models, but no such comparison is provided in the current study.
- What evidence would resolve it: A direct comparison of the proposed method's performance with that of state-of-the-art models on the same dataset and evaluation metrics.

### Open Question 2
- Question: How does the inclusion of TF-IDF scaling impact the performance of the proposed method in detecting suicidal ideation?
- Basis in paper: [explicit] The authors state, "TF-IDF scaling is applied to enhance the token attribution performance."
- Why unresolved: The authors mention that TF-IDF scaling improves the performance, but the extent of improvement is not quantified or compared to other scaling methods.
- What evidence would resolve it: A detailed analysis of the impact of TF-IDF scaling on the proposed method's performance, including comparisons with other scaling methods.

### Open Question 3
- Question: How does the proposed method perform on longitudinal social media data compared to non-longitudinal data?
- Basis in paper: [explicit] The authors state, "The method is evaluated on a longitudinal dataset, demonstrating promising results."
- Why unresolved: The authors do not provide a direct comparison of the proposed method's performance on longitudinal and non-longitudinal data.
- What evidence would resolve it: A comparison of the proposed method's performance on longitudinal and non-longitudinal social media data using the same evaluation metrics.

## Limitations
- The attribution mechanism's reliability depends heavily on the fine-tuned LLM's generalization ability, which may not extend to unseen populations.
- The method's performance on social media platforms other than Reddit remains untested, limiting generalizability.
- The binary classification approach (no-risk vs any-risk) may oversimplify the nuanced continuum of suicidal ideation risk levels.

## Confidence

**High Confidence**: The technical implementation of Layer Integrated Gradients for token attribution is well-established in the literature, and the basic methodology for computing and aggregating attribution scores is sound. The computational efficiency gains from using pre-computed attributions versus running LLMs during inference are clearly demonstrated.

**Medium Confidence**: The improvement in classification performance when applying TF-IDF scaling is supported by experimental results, but the underlying assumption about token frequency and predictive power requires further validation across diverse datasets. The longitudinal aggregation approach shows promise, but the assumption that risk indicators remain consistent over time needs more rigorous testing.

**Low Confidence**: The model's real-world applicability and generalizability beyond the specific Reddit dataset used in this study. The potential for temporal leakage in the longitudinal approach and the method's performance on imbalanced or domain-specific datasets remain significant uncertainties.

## Next Checks
1. **Cross-platform validation**: Evaluate the method on social media data from platforms other than Reddit (e.g., Twitter, specialized mental health forums) to assess generalizability across different user populations and language patterns.

2. **Temporal stability analysis**: Conduct a systematic evaluation of how attribution scores and classification performance change when posts are aggregated over different time windows, and test for temporal leakage by comparing models trained on chronological versus shuffled post sequences.

3. **Ablation study on TF-IDF scaling**: Perform a detailed ablation study comparing classification performance with various token weighting schemes (raw frequency, TF-IDF, custom domain-specific weights) to quantify the contribution of TF-IDF scaling and identify scenarios where it may be counterproductive.