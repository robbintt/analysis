---
ver: rpa2
title: Revisiting invariances and introducing priors in Gromov-Wasserstein distances
arxiv_id: '2307.10093'
source_url: https://arxiv.org/abs/2307.10093
tags:
- coot
- feature
- alignment
- distance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Augmented Gromov-Wasserstein (AGW) distance,
  which combines Gromov-Wasserstein and CO-Optimal Transport distances to better control
  isometric transformations and incorporate prior knowledge in optimal transport.
  The AGW metric allows for improved alignment of datasets by tightening the invariances
  of Gromov-Wasserstein and leveraging feature-level information.
---

# Revisiting invariances and introducing priors in Gromov-Wasserstein distances

## Quick Facts
- arXiv ID: 2307.10093
- Source URL: https://arxiv.org/abs/2307.10093
- Authors: 
- Reference count: 38
- Key outcome: This paper introduces the Augmented Gromov-Wasserstein (AGW) distance, which combines Gromov-Wasserstein and CO-Optimal Transport distances to better control isometric transformations and incorporate prior knowledge in optimal transport.

## Executive Summary
This paper introduces the Augmented Gromov-Wasserstein (AGW) distance, which combines Gromov-Wasserstein and CO-Optimal Transport distances to better control isometric transformations and incorporate prior knowledge in optimal transport. The AGW metric allows for improved alignment of datasets by tightening the invariances of Gromov-Wasserstein and leveraging feature-level information. Theoretical analysis shows AGW inherits weak invariance to translation and covers fewer isometries than Gromov-Wasserstein. Experiments demonstrate AGW's superior performance in single-cell multi-omic alignment tasks, achieving lower alignment errors compared to existing methods, and its effectiveness in heterogeneous domain adaptation for machine learning.

## Method Summary
The Augmented Gromov-Wasserstein (AGW) distance is formulated as a convex combination of the Gromov-Wasserstein (GW) loss and the CO-Optimal Transport (COOT) loss, with an interpolation parameter α controlling the balance between the two. The optimization problem is solved using a block coordinate descent algorithm that alternatively updates sample and feature couplings. AGW inherits weak invariance to translation from COOT while covering fewer isometries than GW, providing better control over transformations. The method incorporates prior knowledge on feature-level relationships through the feature coupling matrix, enabling improved alignment performance.

## Key Results
- AGW demonstrates lower alignment errors than GW and COOT in single-cell multi-omic alignment tasks
- AGW consistently outperforms baselines in heterogeneous domain adaptation for machine learning
- Theoretical analysis shows AGW inherits weak invariance to translation and covers fewer isometries than GW
- The interpolation parameter α allows for control over the level of rigidity to transformations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Augmented Gromov-Wasserstein (AGW) distance allows tighter control over isometric transformations than Gromov-Wasserstein (GW) by introducing a convex combination of GW and CO-Optimal Transport (COOT) loss terms.
- Mechanism: The AGW objective function is a weighted sum of the GW loss (which depends only on pairwise distances) and the COOT loss (which also depends on raw feature representations). By varying the interpolation parameter α, one can interpolate between GW (α=1) and COOT (α=0), thereby restricting the class of transformations that yield zero distance.
- Core assumption: The feature-level information in COOT captures transformations that GW cannot distinguish, and the convex combination smoothly interpolates between the two.
- Evidence anchors:
  - [abstract] "allows for some control over the level of rigidity to transformations"
  - [section 3.2] "one may see that the AGW problem always admits a solution"
  - [corpus] Weak: no direct evidence from corpus that interpolation parameter controls rigidity; inferred from theory.
- Break condition: If the feature representations do not capture the relevant transformations, the AGW distance may not provide meaningful restriction.

### Mechanism 2
- Claim: AGW incorporates prior knowledge on feature-level relationships between the two domains, enabling improved alignment performance.
- Mechanism: The feature coupling matrix γv in AGW can be guided or constrained using prior knowledge (e.g., known correspondences between genes in cross-species alignment). This influences the optimization of the sample coupling γs, leading to better alignments.
- Core assumption: The feature alignment information is accurate and relevant to the task, and the optimization process effectively incorporates this guidance.
- Evidence anchors:
  - [abstract] "incorporates feature alignments, enabling us to better leverage prior knowledge on the input data for improved performance"
  - [section 3.2] "combining COOT and GW distance allows us to effectively influence the optimization of γs by introducing priors on feature matchings through γv"
  - [corpus] Weak: no direct evidence from corpus that incorporating prior knowledge improves alignment; inferred from experiments.
- Break condition: If the prior knowledge is inaccurate or irrelevant, the AGW alignment may be worse than GW or COOT.

### Mechanism 3
- Claim: AGW inherits weak invariance to translation from COOT, which is desirable in practice.
- Mechanism: The proof shows that AGW is weakly invariant to translation, meaning that the optimal transport plan is preserved under translation, even though the distance itself may shift. This is important because preserving the plan is more relevant than preserving the distance in many applications.
- Core assumption: Weak invariance to translation is a desirable property for the application at hand.
- Evidence anchors:
  - [section 3.3] "AGW inherits the weak invariant to translation from COOT"
  - [section 3.3] "AGW is weakly invariant to translation"
  - [corpus] No evidence from corpus on the practical benefits of weak invariance to translation.
- Break condition: If strong invariance to translation is required, AGW may not be suitable.

## Foundational Learning

- Concept: Optimal Transport (OT) theory
  - Why needed here: AGW is based on OT, so understanding the basic concepts of OT is crucial to grasp the paper.
  - Quick check question: What is the Kantorovich formulation of the OT problem, and how does it differ from the Monge formulation?

- Concept: Gromov-Wasserstein (GW) distance
  - Why needed here: AGW builds upon GW, so understanding GW's properties, limitations, and applications is essential.
  - Quick check question: What is the main limitation of GW distance in terms of invariance to transformations, and how does AGW address this?

- Concept: CO-Optimal Transport (COOT)
  - Why needed here: AGW interpolates between GW and COOT, so understanding COOT's properties and how it differs from GW is necessary.
  - Quick check question: What is the key difference between COOT and GW in terms of the information they use for alignment, and how does this affect their invariances?

## Architecture Onboarding

- Component map: Input matrices X, Y -> Distance matrices DX, DY -> Initialize couplings γs, γv -> Block coordinate descent optimization -> Output optimal couplings γs, γv
- Critical path:
  1. Compute intra-domain distance matrices DX and DY.
  2. Initialize sample and feature couplings γs and γv.
  3. Iterate until convergence:
     a. Fix γs, optimize γv by solving a COOT problem.
     b. Fix γv, optimize γs by solving an AGW problem.
  4. Return optimal couplings γs and γv.
- Design tradeoffs:
  - Flexibility vs. control: AGW allows for more control over the invariances compared to GW, but may be less flexible in some cases.
  - Computational complexity: AGW is more computationally expensive than GW due to the additional COOT term, but can be mitigated using entropic regularization.
- Failure signatures:
  - Poor alignment quality: If the prior knowledge is inaccurate or the interpolation parameter is not well-tuned, AGW may not improve upon GW or COOT.
  - Slow convergence: If the optimization is not well-conditioned or the problem is ill-posed, the block coordinate descent algorithm may converge slowly or not at all.
- First 3 experiments:
  1. Verify the interpolation property: Check that AGW approaches GW as α→1 and COOT as α→0.
  2. Test weak invariance to translation: Verify that AGW is weakly invariant to translation by checking that the optimal transport plan is preserved under translation.
  3. Benchmark on a simple alignment task: Compare the alignment quality of AGW with GW and COOT on a simple dataset where the ground truth is known.

## Open Questions the Paper Calls Out
The paper identifies open questions about the specific isometries induced by AGW distance beyond those covered by GW and COOT, and suggests this requires further investigation. The authors note that AGW covers "much fewer isometries than GW distance" but do not provide a complete characterization of the invariant transformations.

## Limitations
- The paper does not provide empirical evidence demonstrating the practical benefits of AGW's weak invariance to translation property
- Experimental validation lacks statistical significance tests and variance reporting across multiple runs
- The paper does not discuss computational complexity scaling with dataset size or the impact of entropic regularization strength on alignment quality

## Confidence

**High Confidence**: The interpolation property between GW and COOT is mathematically well-established. The convergence proof for the block coordinate descent algorithm follows standard OT optimization frameworks.

**Medium Confidence**: The claim that AGW allows "better control over isometric transformations" is supported by theory but lacks direct empirical validation. The alignment performance improvements on single-cell data are demonstrated but without statistical rigor.

**Low Confidence**: The assertion that AGW "incorporates prior knowledge on feature-level relationships" is more aspirational than demonstrated - the experiments do not explicitly show how incorporating known correspondences improves results.

## Next Checks

1. **Statistical Significance**: Re-run single-cell alignment experiments with multiple random seeds and report p-values comparing AGW against GW and COOT baselines to establish whether performance differences are statistically significant.

2. **Transformation Coverage**: Create synthetic datasets with controlled transformations (rotations, scalings, permutations) and measure which transformations AGW can correctly align versus GW, directly testing the claim about restricted isometry coverage.

3. **Prior Knowledge Integration**: Design an experiment where partial ground-truth correspondences are available, incorporate them as priors in AGW, and measure the improvement in alignment quality compared to using no priors.