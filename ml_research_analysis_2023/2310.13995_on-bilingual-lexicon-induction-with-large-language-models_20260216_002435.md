---
ver: rpa2
title: On Bilingual Lexicon Induction with Large Language Models
arxiv_id: '2310.13995'
source_url: https://arxiv.org/abs/2310.13995
tags:
- inly
- word
- iswy
- thelx
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large multilingual language models
  (mLLMs) can be effectively prompted and fine-tuned for bilingual lexicon induction
  (BLI), a core task in multilingual NLP. The authors explore zero-shot and few-shot
  prompting with in-context examples from nearest neighbours, as well as BLI-oriented
  fine-tuning of smaller LLMs.
---

# On Bilingual Lexicon Induction with Large Language Models

## Quick Facts
- arXiv ID: 2310.13995
- Source URL: https://arxiv.org/abs/2310.13995
- Reference count: 40
- This paper demonstrates that large multilingual language models (mLLMs) can be effectively prompted and fine-tuned for bilingual lexicon induction (BLI), achieving state-of-the-art performance with few-shot in-context prompting.

## Executive Summary
This paper explores the application of large multilingual language models (mLLMs) to bilingual lexicon induction (BLI), a core task in multilingual NLP. The authors investigate zero-shot and few-shot prompting strategies using in-context examples from nearest neighbors, as well as BLI-oriented fine-tuning of smaller LLMs. Experiments with 18 open-source mLLMs ranging from 0.3B to 13B parameters on two BLI benchmarks show that few-shot prompting with in-context examples achieves state-of-the-art performance, establishing new records for many language pairs. The results demonstrate the strong BLI capabilities of mLLMs, particularly LLaMA13B, which outperforms previous state-of-the-art methods. However, the study also identifies limitations, including lower performance on lower-resource languages and fewer supported languages compared to traditional approaches.

## Method Summary
The authors evaluate bilingual lexicon induction using 18 open-source mLLMs from five families (mT5, mT0, XGLM, mGPT, LLaMA) with parameters ranging from 0.3B to 13B. They employ two prompting strategies: zero-shot prompting with hand-crafted English templates and few-shot prompting with 5 in-context examples retrieved from nearest neighbors of the source word using fastText embeddings. The method involves template search on a development set (German→French) to select optimal prompts for each model family. Inference uses beam search (size 5) with the first valid target word extracted from output. The approach is evaluated on two benchmarks: XLING (8 languages, 56 BLI directions) and PanLex-BLI (15 lower-resource languages, 210 BLI directions).

## Key Results
- Few-shot prompting with in-context examples from nearest neighbors achieves state-of-the-art BLI performance across most language pairs
- LLaMA13B sets new records for many language pairs, outperforming previous state-of-the-art methods
- Performance strongly correlates with model size within model families, with larger models showing better BLI capabilities
- Encoder-decoder models generally outperform decoder-only models in BLI tasks
- Performance degrades significantly on lower-resource languages compared to high-resource language pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based few-shot learning leverages in-context examples to align source words with target words by retrieving nearest neighbors from monolingual embedding space.
- Mechanism: The LLM receives prompts containing pairs of source and target words, where the target word is the translation of the source word. By including in-context examples retrieved from nearest neighbors, the LLM can infer translation patterns.
- Core assumption: Nearest neighbors in monolingual embedding space are good approximations for translation pairs, and the LLM can generalize from these examples.
- Evidence anchors: [abstract] "few-shot in-context prompting with a set of seed translation pairs" [section] "we propose to retrieve the nearest neighbours of a source word which we use to construct in-context samples to boost BLI performance"
- Break condition: If nearest neighbors in monolingual space are not good approximations of translation pairs, or if the LLM cannot generalize from the examples.

### Mechanism 2
- Claim: Larger LLMs exhibit stronger BLI performance due to their capacity to learn complex patterns from prompts.
- Mechanism: As the size of the LLM increases, its ability to understand and generate nuanced language improves, leading to better translation accuracy.
- Core assumption: Model size correlates with the complexity of patterns the model can learn.
- Evidence anchors: [abstract] "stronger BLI performance tends to be associated with larger model sizes" [section] "1) As expected, within the same mLLM family, larger models usually present stronger BLI capabilities"
- Break condition: If model size does not correlate with pattern learning complexity, or if larger models overfit to specific prompt patterns.

### Mechanism 3
- Claim: Encoder-decoder models outperform decoder-only models in BLI tasks due to their architecture.
- Mechanism: Encoder-decoder models can process the entire input sequence before generating the output, allowing for better context understanding and translation accuracy.
- Core assumption: The architecture of encoder-decoder models provides better context understanding than decoder-only models.
- Evidence anchors: [abstract] "We group these models into two categories; in what follows, we briefly introduce each of them" [section] "For these two encoder-decoder style mLLMs, we aim to derive prompts such that the first word of the output sequence serves as its guess for wy"
- Break condition: If decoder-only models can achieve similar or better performance with optimized prompts.

## Foundational Learning

- Concept: Cross-lingual word embeddings (CLWEs)
  - Why needed here: Understanding CLWEs is crucial as they form the basis of traditional BLI methods, against which LLM-based approaches are compared.
  - Quick check question: What is the primary method used in traditional BLI approaches to align word embeddings from different languages?

- Concept: Few-shot learning
  - Why needed here: Few-shot learning is the core methodology used to prompt LLMs for BLI, leveraging in-context examples to improve translation accuracy.
  - Quick check question: How does few-shot learning differ from zero-shot learning in the context of prompting LLMs for BLI?

- Concept: Nearest neighbor retrieval
  - Why needed here: Nearest neighbor retrieval is used to find in-context examples for few-shot prompting, based on the assumption that semantically similar words in one language translate to similar words in another.
  - Quick check question: Why is nearest neighbor retrieval used to find in-context examples for few-shot prompting in BLI tasks?

## Architecture Onboarding

- Component map: LLM (encoder-decoder or decoder-only) -> Prompt generator -> Nearest neighbor retriever -> Translation evaluator
- Critical path: The critical path is the generation of the translation, which involves creating the prompt, feeding it to the LLM, and evaluating the output.
- Design tradeoffs: Using larger LLMs improves performance but increases computational cost. Encoder-decoder models may perform better but are more complex than decoder-only models.
- Failure signatures: Poor translation accuracy, inability to handle low-resource languages, or failure to generalize from in-context examples.
- First 3 experiments:
  1. Test zero-shot prompting with a simple template on a high-resource language pair.
  2. Evaluate the impact of model size on BLI performance by comparing different LLM variants.
  3. Assess the effectiveness of in-context examples by comparing few-shot prompting with and without nearest neighbor retrieval.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing model size beyond 13B parameters consistently improve BLI performance for lower-resource languages?
- Basis in paper: [explicit] The paper notes that current mLLMs (size≤13B) still lack strong word translation capabilities for lower-resource languages and suggests that larger models (e.g., LLaMA30B) might perform better, but this was not investigated due to computational constraints.
- Why unresolved: The paper only experimented with mLLMs up to 13B parameters due to computational limitations and did not test larger models like LLaMA30B on lower-resource languages.
- What evidence would resolve it: Experiments with larger mLLMs (e.g., LLaMA30B or bigger) on lower-resource language pairs would determine if performance improves with model size.

### Open Question 2
- Question: What training objectives and strategies beyond the current pretraining objectives could improve BLI performance when fine-tuning mLLMs?
- Basis in paper: [explicit] The paper found limited gains from BLI-oriented fine-tuning with current pretraining objectives and suggests that other training objectives and strategies should be investigated for improved BLI performance with mLLMs.
- Why unresolved: The paper only explored fine-tuning with the models' own pretraining objectives and did not investigate alternative training objectives or strategies that might be more effective for BLI.
- What evidence would resolve it: Experiments with alternative fine-tuning strategies (e.g., prompt tuning, adapters, LoRA) and different training objectives specifically designed for BLI would reveal more effective approaches.

### Open Question 3
- Question: How does the quality and quantity of in-context examples impact BLI performance, and what is the optimal approach for selecting these examples?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of in-context examples from nearest neighbors over random examples, but also notes that gains saturate with higher numbers of examples (n ≥ 5) and suggests this is an area for further investigation.
- Why unresolved: While the paper shows that in-context examples improve performance, it did not extensively explore the optimal number of examples, the best methods for selecting examples, or how example quality impacts performance.
- What evidence would resolve it: Systematic experiments varying the number, quality, and selection methods for in-context examples across different language pairs would determine the optimal approach for maximizing BLI performance.

## Limitations
- Performance degrades significantly on lower-resource languages compared to high-resource language pairs, limiting practical applicability
- Template sensitivity and reliance on single-language development set for template selection may not generalize well
- Nearest neighbor retrieval quality assumptions may not hold for lower-resource languages where embedding quality is compromised
- Limited language coverage compared to traditional BLI methods, constrained by model pretraining data

## Confidence

**High Confidence**: "Few-shot prompting with in-context examples from nearest neighbors achieves state-of-the-art BLI performance" - This claim is well-supported by extensive experimental results across multiple model families and benchmarks, with clear quantitative improvements over zero-shot baselines and traditional methods.

**Medium Confidence**: "Larger LLMs exhibit stronger BLI capabilities" - While the correlation between model size and performance is demonstrated within model families, the comparison across different model architectures (encoder-decoder vs decoder-only) introduces confounding variables that make definitive causal claims difficult.

**Medium Confidence**: "LLaMA13B outperforms previous state-of-the-art methods" - The claim is supported by benchmark results, but the comparison is limited to specific language pairs and doesn't account for computational costs or the broader ecosystem of BLI approaches.

## Next Checks

**Check 1: Nearest neighbor validation study**
Systematically evaluate the quality of retrieved nearest neighbors by human annotation or extrinsic validation tasks. For a sample of source words across multiple languages, verify whether the retrieved neighbors actually correspond to valid translations or merely semantically similar concepts. This would test the foundational assumption underlying the in-context example selection method.

**Check 2: Template transfer robustness analysis**
Conduct cross-language template validation by testing whether templates optimized for high-resource language pairs maintain effectiveness when applied to low-resource pairs within the same model family. This would quantify the sensitivity of performance to template selection and identify whether the German→French development set is representative of broader multilingual BLI performance.

**Check 3: Computational cost-benefit analysis**
Measure and compare the computational resources required for LLM-based BLI (including prompt generation, nearest neighbor retrieval, and inference) versus traditional BLI methods across the full range of evaluated language pairs. This would provide a more complete picture of practical applicability beyond raw accuracy metrics.