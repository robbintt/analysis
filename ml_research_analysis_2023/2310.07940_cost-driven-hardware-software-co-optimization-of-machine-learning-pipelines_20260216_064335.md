---
ver: rpa2
title: Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines
arxiv_id: '2310.07940'
source_url: https://arxiv.org/abs/2310.07940
tags:
- size
- cost
- latency
- https
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores how to optimally co-design ML models and hardware
  for ultra-compact, cost-sensitive systems. By jointly considering quantization,
  model scaling, and multi-modality, it demonstrates that XNOR precision with small
  models is optimal for latency, while fixed-point models achieve better cost-efficiency.
---

# Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines

## Quick Facts
- arXiv ID: 2310.07940
- Source URL: https://arxiv.org/abs/2310.07940
- Reference count: 40
- This work explores how to optimally co-design ML models and hardware for ultra-compact, cost-sensitive systems. By jointly considering quantization, model scaling, and multi-modality, it demonstrates that XNOR precision with small models is optimal for latency, while fixed-point models achieve better cost-efficiency. Using a $20 ESP-EYE board, it builds a multimodal face+voice biometric authentication system, showing that fusion models reduce error rate by 6–10% over unimodal ones. The study establishes clear guidelines for balancing model complexity, sensor selection, and precision, enabling practical deployment of ML on extremely resource-constrained devices.

## Executive Summary
This paper presents a systematic approach to optimizing machine learning pipelines for ultra-compact, cost-sensitive embedded systems. By exploring the trade-offs between model precision, complexity, and multi-modality, the authors demonstrate that different quantization schemes (XNOR, fixed-point, floating-point) have distinct advantages depending on the application requirements and hardware constraints. The study uses a $20 ESP-EYE board to build a multimodal face+voice biometric authentication system, showing that fusion models reduce error rate by 6–10% over unimodal ones. The work provides clear guidelines for balancing model complexity, sensor selection, and precision, enabling practical deployment of ML on extremely resource-constrained devices.

## Method Summary
The study systematically explores hardware-software co-design for ultra-compact systems by training ResNet-style models with variable complexity, quantization (floating-point, fixed-point, XNOR with 1-3 bits), and multi-modality (face, voice, fusion) on the VoxCeleb2 dataset. The models are evaluated on the VoxCeleb1 dataset using Equal Error Rate (EER) and effective latency metrics on an ESP-EYE board. Cost estimates are derived from component pricing data from Digi-Key. The method involves setting up the ESP-EYE board with camera and microphone, training models with different configurations, and measuring EER, latency, and cost to identify optimal trade-offs between precision, modality, and complexity.

## Key Results
- XNOR precision with small models is optimal for latency in ultra-compact systems.
- Fixed-point models achieve better cost-efficiency than floating-point or XNOR models in the $10+ price range.
- Multi-modality (face + voice) reduces error rate by 6–10% over unimodal models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XNOR precision with small models is optimal for latency in ultra-compact systems.
- Mechanism: Binarized operations (XNOR) replace integer multiplications with bitwise operations, drastically reducing computation time on resource-constrained hardware.
- Core assumption: The hardware supports efficient bitwise operations and the accuracy drop from binarization is acceptable for the target application.
- Evidence anchors:
  - [abstract]: "While many ways of making models smaller and more efficient have been developed, there is a lack of understanding of which ones are best suited for particular scenarios."
  - [section 2]: "Due to their efficiency, multiple software and hardware implementations of binarized neural networks have been proposed."
  - [corpus]: Weak; no direct evidence in neighbors, but this is a well-known principle in the field.
- Break condition: If the application requires high accuracy that cannot tolerate the degradation from binarization, or if the hardware lacks efficient bitwise operation support.

### Mechanism 2
- Claim: Multi-modality (e.g., face + voice) reduces error rate by 6–10% over unimodal models.
- Mechanism: Combining independent data streams (face and voice) provides complementary information, improving robustness to anomalies in one stream.
- Core assumption: The modalities are sufficiently independent and the sensor cost is justified by the accuracy improvement.
- Evidence anchors:
  - [abstract]: "Using a $20 ESP-EYE board, it builds a multimodal face+voice biometric authentication system, showing that fusion models reduce error rate by 6–10% over unimodal ones."
  - [section 3]: "Prior works have shown the importance of multi-modality and fusion in biometric authentication, as it can improve accuracy and resilience to attacks."
  - [corpus]: Weak; no direct evidence in neighbors, but this is supported by cited works [3, 11, 39, 51, 58, 62].
- Break condition: If the modalities are highly correlated (e.g., face and voice features are similar), the benefit of fusion diminishes.

### Mechanism 3
- Claim: Fixed-point models achieve better cost-efficiency than floating-point or XNOR models in the $10+ price range.
- Mechanism: Fixed-point quantization reduces storage and memory requirements without the severe accuracy loss of XNOR, and Flash memory is relatively cheap, making it cost-effective.
- Core assumption: The application can tolerate the precision loss from fixed-point quantization and the cost savings outweigh any accuracy degradation.
- Evidence anchors:
  - [section 6.5.2]: "Generally speaking, the majority of authentication models we trained are estimated to take between 30 and 120 seconds to execute... Models that take longer than 70 seconds to run do not yield additional accuracy benefits."
  - [section 6.5.3]: "From a cost-accuracy standpoint, it is generally better to choose a high-precision model than a low-precision model. However, precision is a significant factor when taking latency of the model into account."
  - [corpus]: Weak; no direct evidence in neighbors, but this is a standard practice in edge ML.
- Break condition: If the application requires the dynamic range of floating-point or the extreme speed of XNOR, fixed-point may not be optimal.

## Foundational Learning

- Concept: Quantization-aware training (QAT)
  - Why needed here: To train models with reduced precision (fixed-point, XNOR) while minimizing accuracy loss.
  - Quick check question: What is the primary difference between post-training quantization and quantization-aware training?
- Concept: Model complexity vs. accuracy trade-off
  - Why needed here: Understanding how increasing model complexity affects error rate and cost, especially the diminishing returns beyond a certain point.
  - Quick check question: Why might increasing model complexity beyond a certain point not yield significant accuracy improvements?
- Concept: Multi-modality fusion techniques
  - Why needed here: To effectively combine different data streams (e.g., face and voice) for improved authentication accuracy.
  - Quick check question: What are some common methods for fusing multi-modal data in machine learning?

## Architecture Onboarding

- Component map:
  - Microcontroller (ESP32-S3): Dual-core processor, on-chip memory, peripherals
  - Sensors: Camera (for face), Microphone (for voice)
  - Memory: PSRAM (for activations), Flash (for weights and code)
  - Software: ESP-DL inference library (for floating/fixed-point), 3PXNet (for XNOR)
- Critical path:
  1. Sensor data acquisition (image/audio)
  2. Preprocessing (resize, normalization, feature extraction)
  3. Model inference (forward pass)
  4. Embedding comparison and decision
- Design tradeoffs:
  - Precision vs. accuracy: XNOR is fastest but least accurate; fixed-point is a balance; floating-point is most accurate but slowest and most expensive
  - Single-modality vs. multi-modality: Multi-modality improves accuracy but increases cost and complexity
  - Model complexity: More complex models are more accurate but require more resources
- Failure signatures:
  - High error rate: Model not complex enough, poor quantization, insufficient training data
  - Long latency: Model too complex, inefficient quantization, single-threaded execution
  - High cost: Overly complex model, unnecessary sensors, insufficient memory planning
- First 3 experiments:
  1. Baseline: Floating-point ResNet-6 face-only model on ESP-EYE
  2. Quantization: Fixed-point ResNet-6 face-only model, compare accuracy and latency
  3. Multi-modality: Fixed-point ResNet-6 face+voice fusion model, compare accuracy and latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal multi-modal biometric fusion strategies for different edge deployment scenarios, considering sensor cost and latency constraints?
- Basis in paper: [explicit] The paper mentions that "Several methods of enabling multimodality in a model exist, like feature concatenation, or ensemble classifiers" and explores the tradeoff between adding modalities and increasing model complexity.
- Why unresolved: The paper only explores feature concatenation and does not evaluate other fusion strategies like soft-attention fusion, compact bilinear pooling fusion, or gated multi-modal fusion mentioned in [13].
- What evidence would resolve it: Comparative analysis of different fusion strategies across various edge deployment scenarios with varying sensor costs and latency constraints.

### Open Question 2
- Question: How does the choice of neural network architecture family impact the overall system cost and performance for edge ML applications beyond the specific case study?
- Basis in paper: [explicit] The paper discusses the choice between ResNet and VGGNet architectures and mentions that "The choice of model architecture and corresponding dataflow also has an impact on peak activation memory, and thus the required PSRAM size."
- Why unresolved: The paper only explores ResNet architectures and does not comprehensively evaluate other architecture families like MobileNet or custom architectures.
- What evidence would resolve it: Systematic comparison of different architecture families across various edge ML applications, considering factors like parameter count, activation memory, and quantization sensitivity.

### Open Question 3
- Question: What are the optimal hardware-software co-design strategies for achieving the best balance between security (false acceptance rate) and user experience (effective latency) in edge ML systems?
- Basis in paper: [explicit] The paper introduces the concept of "effective latency" which combines EER and latency, and discusses the tradeoff between security and user experience.
- Why unresolved: The paper only explores a limited set of false acceptance rates (1% and 10%) and does not provide a comprehensive framework for balancing security and user experience across different application scenarios.
- What evidence would resolve it: Development of a systematic framework for optimizing hardware-software co-design strategies based on desired security levels and user experience requirements across various edge ML applications.

## Limitations
- The optimal precision choice (XNOR vs fixed-point vs floating-point) is highly dependent on specific application requirements and hardware capabilities, which may not generalize to all ultra-compact systems.
- Latency estimates are based on benchmarks that may not capture real-world execution overhead, particularly for preprocessing and sensor integration.
- Cost estimates rely on specific component pricing that may fluctuate, and the study doesn't account for potential economies of scale or alternative hardware configurations.

## Confidence
- **High confidence:** The general principle that different quantization schemes (XNOR, fixed-point, floating-point) have distinct latency-accuracy-cost trade-offs, and that these trade-offs are critical for ultra-compact systems.
- **Medium confidence:** The specific claim that XNOR precision with small models is optimal for latency, and that fixed-point models achieve better cost-efficiency than floating-point in the $10+ price range.
- **Medium confidence:** The claim that multi-modality reduces error rate by 6-10% over unimodal models.

## Next Checks
1. **Hardware independence validation:** Test the optimal model configurations (precision, complexity, multi-modality) on a different ultra-compact platform (e.g., Raspberry Pi Pico, Arduino) to assess generalizability beyond the ESP-EYE board.
2. **Real-world latency measurement:** Implement the recommended models on the ESP-EYE board with full sensor integration (camera and microphone preprocessing) and measure actual end-to-end latency, comparing it to the estimated values used in the study.
3. **Cost sensitivity analysis:** Re-evaluate the cost-accuracy trade-offs using current component pricing from Digi-Key and explore alternative hardware configurations (e.g., different memory sizes, sensors) to understand the robustness of the recommended models to cost fluctuations.