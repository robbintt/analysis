---
ver: rpa2
title: A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets
arxiv_id: '2305.18486'
source_url: https://arxiv.org/abs/2305.18486
tags:
- chatgpt
- evaluation
- datasets
- tasks
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first comprehensive evaluation of ChatGPT\
  \ on a wide range of NLP benchmark datasets, covering 140 tasks and analyzing 255K\
  \ responses. The study assesses ChatGPT\u2019s strengths and weaknesses in diverse\
  \ areas like question-answering, text summarization, commonsense reasoning, and\
  \ ethical considerations."
---

# A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets

## Quick Facts
- arXiv ID: 2305.18486
- Source URL: https://arxiv.org/abs/2305.18486
- Reference count: 40
- Key outcome: ChatGPT performs worse than SOTA fine-tuned models on average but matches human-level performance in some algorithmic tasks and demonstrates strong mathematical/coding abilities in zero-shot settings.

## Executive Summary
This paper presents the first comprehensive evaluation of ChatGPT across 140 NLP benchmark tasks, analyzing 255K responses to assess its strengths and weaknesses. The study covers diverse areas including question-answering, text summarization, commonsense reasoning, and ethical considerations. While ChatGPT shows impressive open-domain knowledge and mathematical abilities in zero-shot settings, it struggles with commonsense reasoning in certain datasets and generally underperforms compared to fine-tuned models. The evaluation also reveals ChatGPT's unique capability to identify and respond to multiple queries within a single input (PolyQuery Synthesis), highlighting both its potential and limitations for real-world applications.

## Method Summary
The study evaluates ChatGPT (gpt-3.5-turbo-0301) on 140 benchmark datasets using zero-shot prompting, generating responses via the ChatGPT API. The evaluation employs both automatic metrics (ROUGE, BLEU) and human evaluation for generative tasks, with responses compared against ground truth. The researchers analyze performance across different prompting strategies (Chain-of-Thought vs. Answer-Only) and examine reasoning capabilities, bias, and ethical considerations. A newly discovered capability called "PolyQuery Synthesis" is identified, where ChatGPT can parse and respond to multiple queries in a single input prompt.

## Key Results
- ChatGPT matches human-level performance in some algorithmic tasks despite underperforming SOTA fine-tuned models on average
- Strong mathematical and coding abilities demonstrated in zero-shot settings, particularly when using chain-of-thought prompting
- Impressive open-domain knowledge but struggles with commonsense reasoning in specific datasets (PIQA, HellaSwag, WinoGrande)
- Novel capability to identify and respond to multiple queries in single input prompts (PolyQuery Synthesis)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ChatGPT's performance degrades when faced with novel mathematical notation without chain-of-thought prompting.
- **Mechanism:** The model relies on pattern matching from pre-training data. When new notation is introduced, it attempts to map it to familiar patterns, often leading to incorrect answers if no explicit step-by-step reasoning is provided.
- **Core assumption:** ChatGPT's training data contained standard mathematical notation, but not extensive examples of redefined symbols.
- **Evidence anchors:** [abstract] "ChatGPT is prone to give incorrect answers via memorizing the original mathematical notation from its pre-training data without properly understanding the new instructions when the CoT prompts are not used."
- **Break condition:** Providing chain-of-thought prompts or fine-tuning on examples with redefined notation.

### Mechanism 2
- **Claim:** ChatGPT can identify multiple queries in a single input prompt and generate responses for each.
- **Mechanism:** The model's instruction-tuned architecture allows it to parse complex prompts and generate coherent responses for multiple sub-tasks simultaneously.
- **Core assumption:** The training data included examples of multi-query prompts, and the model learned to handle them effectively.
- **Evidence anchors:** [abstract] "We observe that ChatGPT can answer multiple arbitrary (unrelated) knowledge-based queries from a single input prompt."
- **Break condition:** Increasing the number of queries in a single prompt beyond the model's capacity.

### Mechanism 3
- **Claim:** ChatGPT's performance on commonsense reasoning tasks varies significantly across different datasets.
- **Mechanism:** The model's performance depends on the specific type of commonsense reasoning required. Some tasks may align better with the model's training data and capabilities than others.
- **Core assumption:** The training data covered a wide range of commonsense reasoning scenarios, but not all types equally.
- **Evidence anchors:** [abstract] "it shows impressive open-domain knowledge but struggles with commonsense reasoning in some datasets."
- **Break condition:** Fine-tuning on specific commonsense reasoning tasks or using specialized datasets.

## Foundational Learning

- **Concept:** Large Language Models (LLMs)
  - **Why needed here:** Understanding the architecture and training process of LLMs is crucial for interpreting ChatGPT's performance and limitations.
  - **Quick check question:** What is the key difference between fine-tuning and in-context learning in LLMs?

- **Concept:** Chain-of-Thought (CoT) Prompting
  - **Why needed here:** CoT prompting significantly impacts ChatGPT's performance on reasoning tasks, as evidenced by the varying results with and without CoT.
  - **Quick check question:** How does CoT prompting help LLMs solve complex reasoning problems?

- **Concept:** Zero-shot vs. Few-shot Learning
  - **Why needed here:** The paper evaluates ChatGPT's zero-shot performance, which is a key aspect of its capabilities and limitations.
  - **Quick check question:** What is the main difference between zero-shot and few-shot learning in the context of LLMs?

## Architecture Onboarding

- **Component map:** ChatGPT is a transformer-based LLM with an instruction-tuned architecture that combines supervised fine-tuning on human-generated prompts with reinforcement learning with human feedback (RLHF).
- **Critical path:** The critical path involves: 1) Providing a prompt, 2) Generating a response, 3) Evaluating the response (either automatically or with human intervention).
- **Design tradeoffs:** The instruction-tuned architecture allows ChatGPT to perform well on a wide range of tasks without fine-tuning, but it may struggle with novel tasks or specialized domains. RLHF helps align outputs with human preferences but may introduce biases.
- **Failure signatures:** ChatGPT may generate incorrect or nonsensical responses when faced with novel tasks, complex reasoning problems, or prompts requiring specific domain knowledge. It may also struggle with precise numerical calculations or logical deductions.
- **First 3 experiments:**
  1. Evaluate ChatGPT's performance on a simple question-answering task with and without CoT prompting to observe the impact of CoT on its reasoning ability.
  2. Test ChatGPT's ability to handle multiple queries in a single prompt by providing a complex prompt with several unrelated questions and analyzing its responses.
  3. Assess ChatGPT's performance on a commonsense reasoning task from a dataset where it previously struggled (e.g., PIQA or HellaSwag) to identify specific areas of weakness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we develop new evaluation metrics for instruction-tuned LLMs like ChatGPT that go beyond traditional metrics like ROUGE and BLEU, which may not fully capture the quality of their generated outputs?
- **Basis in paper:** [inferred] The paper mentions that ChatGPT's generated summaries are preferred by human annotators over state-of-the-art models, despite lower ROUGE scores. This suggests that current evaluation metrics may not accurately reflect the quality of instruction-tuned LLM outputs.
- **Why unresolved:** Developing new evaluation metrics that capture the nuances of instruction-tuned LLM outputs is an open research challenge. It requires a deep understanding of the strengths and limitations of these models, as well as innovative approaches to quantify their performance.
- **What evidence would resolve it:** Developing and validating new evaluation metrics that correlate well with human preferences and accurately reflect the capabilities of instruction-tuned LLMs like ChatGPT.

### Open Question 2
- **Question:** How can we mitigate the potential biases and ethical concerns associated with instruction-tuned LLMs like ChatGPT, especially when they are used in high-stakes applications like autonomous vehicles or medical diagnosis?
- **Basis in paper:** [explicit] The paper discusses the potential biases and ethical concerns of ChatGPT, such as its performance on the WinoBias dataset and its ability to generate misinformation. It also mentions the need for human evaluation to assess its responses.
- **Why unresolved:** Addressing biases and ethical concerns in instruction-tuned LLMs is a complex challenge that requires a multi-faceted approach, including careful dataset curation, bias detection and mitigation techniques, and transparent evaluation methods.
- **What evidence would resolve it:** Developing and implementing effective bias detection and mitigation techniques for instruction-tuned LLMs, as well as establishing ethical guidelines and best practices for their development and deployment.

### Open Question 3
- **Question:** How can we leverage the unique capabilities of instruction-tuned LLMs like ChatGPT, such as their ability to follow multi-query instructions (PolyQuery Synthesis), to develop more efficient and effective AI systems for real-world applications?
- **Basis in paper:** [explicit] The paper introduces the concept of PolyQuery Synthesis, a unique capability of ChatGPT to identify and respond to multiple queries in a single input prompt. It suggests that this capability could be useful in real-world scenarios with limited budgets.
- **Why unresolved:** Harnessing the full potential of instruction-tuned LLMs like ChatGPT requires further research into their unique capabilities and how they can be effectively integrated into AI systems. This includes exploring their limitations and developing techniques to optimize their performance.
- **What evidence would resolve it:** Developing and evaluating AI systems that leverage the unique capabilities of instruction-tuned LLMs like ChatGPT, demonstrating their effectiveness and efficiency in real-world applications.

## Limitations

- Evaluation focuses exclusively on zero-shot performance without exploring few-shot capabilities, potentially underestimating ChatGPT's full potential
- Reliance on automatic metrics for some tasks may not fully capture the nuances of language understanding and generation quality
- Comparison with fine-tuned models involves different evaluation paradigms (zero-shot vs. fine-tuned), requiring careful interpretation of performance gaps

## Confidence

**High Confidence:** ChatGPT's strong performance on mathematical and coding tasks in zero-shot settings, and its ability to identify multiple queries in a single input (PolyQuery Synthesis) are well-supported by consistent results across multiple datasets and tasks.

**Medium Confidence:** The claim that ChatGPT performs worse than SOTA fine-tuned models on average requires careful interpretation, as the comparison involves different evaluation paradigms (zero-shot vs. fine-tuned). The variability in commonsense reasoning performance across different datasets is also moderately supported but requires further investigation into task-specific factors.

**Low Confidence:** The mechanism explaining why ChatGPT struggles with novel mathematical notation without chain-of-thought prompting relies on indirect evidence and assumptions about the model's training data that cannot be directly verified.

## Next Checks

1. **Few-shot Performance Validation:** Replicate the evaluation using few-shot prompts for tasks where ChatGPT showed weakness in zero-shot settings to determine if performance gaps with fine-tuned models can be narrowed.

2. **Cross-dataset Consistency Check:** Conduct a deeper analysis of why ChatGPT's commonsense reasoning performance varies significantly across datasets (e.g., strong on SIQA but weak on HellaSwag) by examining the specific reasoning types required in each task.

3. **Instruction Following Robustness:** Test ChatGPT's ability to follow complex instructions with multiple constraints by designing prompts that progressively increase in complexity and analyzing where the model begins to deviate from requirements.