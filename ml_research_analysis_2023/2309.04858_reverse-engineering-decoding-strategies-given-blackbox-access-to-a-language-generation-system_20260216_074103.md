---
ver: rpa2
title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language
  Generation System
arxiv_id: '2309.04858'
source_url: https://arxiv.org/abs/2309.04858
tags:
- prompt
- prompts
- distribution
- language
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a black-box attack method to reverse-engineer
  the decoding strategy of a language model API without access to model internals
  or parameters. The attack leverages prompt engineering to induce predictable token
  distributions, then analyzes the observed outputs to distinguish between top-k and
  top-p sampling and estimate their hyperparameters.
---

# Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System

## Quick Facts
- arXiv ID: 2309.04858
- Source URL: https://arxiv.org/abs/2309.04858
- Reference count: 12
- Primary result: Black-box attack method can identify top-k vs top-p decoding and estimate hyperparameters without model access

## Executive Summary
This paper presents a novel black-box attack method that can reverse-engineer the decoding strategy of a language model API without requiring access to model internals or parameters. The attack works by carefully engineering prompts to induce predictable token distributions, then analyzing the observed outputs to distinguish between top-k and top-p sampling methods and estimate their hyperparameters. Through systematic querying of the model with crafted prompts, the method successfully identifies the decoding strategy and estimates parameters like k (number of tokens sampled) and p (probability threshold) with reasonable accuracy. The research demonstrates this approach on multiple open-source models and the proprietary ChatGPT system, highlighting that inference implementation details can be uncovered even when model details are hidden, with implications for both model auditing and generated text detection.

## Method Summary
The method consists of two main phases: first, it determines whether the model uses top-k or top-p sampling by analyzing the consistency of unique token counts across prompts with different vocabulary sizes; second, it estimates the specific hyperparameter (k or p) by repeatedly querying the model with carefully designed prompts that induce known probability distributions. For top-k estimation, the method counts unique tokens across multiple queries and uses the coupon collector problem to estimate k. For top-p estimation, it constructs prompts yielding known distributions and estimates p by comparing the observed token set to the expected distribution. The approach requires no model access beyond a black-box API that returns single tokens per prompt.

## Key Results
- Successfully distinguished between top-k and top-p sampling strategies across GPT-2, BLOOM, Pythia, and ChatGPT models
- Estimated k values within 10-15% accuracy for top-k sampling models using 100-500 queries
- Estimated p values within 0.05 of true values for top-p sampling models
- Demonstrated that decoding strategy information can be extracted even from proprietary systems like ChatGPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method can distinguish between top-k and top-p sampling by observing the number of unique tokens generated from carefully crafted prompts.
- Mechanism: By repeatedly querying the model with prompts designed to induce near-uniform distributions over a known set of tokens, the method counts how many unique tokens appear in the responses. If the number of unique tokens is consistent across different prompts with different sized vocabularies, this indicates top-p sampling. If the number of unique tokens varies significantly with the prompt's vocabulary size, this indicates top-k sampling.
- Core assumption: The prompts induce near-uniform distributions over the token sets, and the model's output distributions are well-approximated by the known distributions used for analysis.
- Evidence anchors:
  - [abstract] "Our ability to discover which decoding strategy was used has implications for detecting generated text."
  - [section 3.4] "In this paper, we set a goal of determining p to within 0.05 of the true value. We can upper bound p by constructing a prompt that yields a known, computable distribution over a set of vocab items Vm."
  - [corpus] Found 25 related papers, average neighbor FMR=0.428, average citations=0.0. Weak corpus evidence for this specific mechanism.

### Mechanism 2
- Claim: The method can estimate the hyperparameter k for top-k sampling by counting the number of unique tokens generated.
- Mechanism: For a given prompt, the method queries the model n times, each time keeping just the first token of the output. By observing the number of unique items in the set of responses, the method can trivially lower bound k. As n approaches infinity, all k allowed responses will be observed.
- Core assumption: The model uses top-k sampling, and the prompt induces a distribution where the least likely token in the top-k has a probability no more than c times less likely than if the distribution were truly uniform.
- Evidence anchors:
  - [section 3.3] "We can trivially lower bound k by observing the number of unique items in a set of responses."
  - [section 3.3] "If n â‰¥ 2cklog(ck), then with probability at least 1 - 1/ck, our prediction is exactly correct."
  - [corpus] Found 25 related papers, average neighbor FMR=0.428, average citations=0.0. Weak corpus evidence for this specific mechanism.

### Mechanism 3
- Claim: The method can estimate the hyperparameter p for top-p sampling by comparing the observed token distribution to a known distribution.
- Mechanism: The method constructs prompts that yield known distributions over a set of vocabulary items. By repeatedly sampling with the prompt and counting how many of those items are generated, the method can estimate p as the sum of the probabilities of the k most likely tokens in the known distribution, where k is the number of unique tokens observed.
- Core assumption: The method has access to an underlying distribution that approximates the model's true distribution, and the prompts induce distributions that are well-approximated by the known distributions.
- Evidence anchors:
  - [section 3.4] "We can upper bound p by constructing a prompt that yields a known, computable distribution over a set of vocab items Vm."
  - [section 4.2] "For each prompt, we need to compute a known distribution over the next word."
  - [corpus] Found 25 related papers, average neighbor FMR=0.428, average citations=0.0. Weak corpus evidence for this specific mechanism.

## Foundational Learning

- Concept: Probability distributions and sampling
  - Why needed here: The method relies on understanding how probability distributions work and how sampling from these distributions can be used to infer the underlying distribution's properties.
  - Quick check question: If a model outputs tokens with probabilities [0.5, 0.3, 0.2], what is the probability of sampling the second token?

- Concept: Coupon collector problem
  - Why needed here: The method uses the coupon collector problem to estimate the number of unique tokens that will be generated when sampling from a distribution, which is crucial for estimating the hyperparameters k and p.
  - Quick check question: If a distribution has 10 equally likely outcomes, how many samples on average are needed to see all 10 outcomes?

- Concept: Entropy and information theory
  - Why needed here: The method relies on understanding how entropy relates to the randomness of a distribution, which is important for designing prompts that induce near-uniform distributions.
  - Quick check question: What is the entropy of a distribution with 4 equally likely outcomes?

## Architecture Onboarding

- Component map: Prompt Engineering -> Model Querying -> Result Analysis -> Decoding Strategy Identification
- Critical path: Design prompts inducing known distributions -> Query model repeatedly -> Count unique tokens -> Estimate k or p -> Compare estimates across prompts to distinguish top-k vs top-p
- Design tradeoffs: The main tradeoff is between the number of queries needed and the accuracy of the estimates. More queries generally lead to more accurate estimates, but also increase the computational cost and the risk of triggering rate limits or detection mechanisms.
- Failure signatures: The method may fail if the prompts do not induce near-uniform distributions, if the model's output distributions significantly differ from the known distributions, or if the model uses a decoding strategy other than top-k or top-p.
- First 3 experiments:
  1. Estimate k for a model using top-k sampling with a known k value, using a prompt that induces a near-uniform distribution over a set of tokens.
  2. Estimate p for a model using top-p sampling with a known p value, using a prompt that yields a known distribution over a set of tokens.
  3. Distinguish between top-k and top-p sampling for a model with an unknown decoding strategy, using multiple prompts with different sized vocabularies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt engineering strategy to induce consistent distributions across different families of language models for top-p estimation?
- Basis in paper: [inferred] The paper mentions that their estimates for p are worse when there is a greater mismatch between the known distribution used for top-p estimation and the true distribution of the language model underlying the blackbox system being attacked. They suggest that future work is needed for reverse engineering fully closed models.
- Why unresolved: The paper does not provide a definitive answer on how to design prompts that consistently produce close-to-uniform distributions across different families of language models. They only mention that increased precision is achievable by using additional prompts.
- What evidence would resolve it: A comprehensive study that systematically evaluates the effectiveness of various prompt engineering strategies across different language model families would provide evidence on the optimal approach.

### Open Question 2
- Question: How does the use of temperature annealing in conjunction with top-k or top-p sampling affect the effectiveness of the reverse-engineering methods?
- Basis in paper: [explicit] The paper mentions that their methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p sampling. They state that extending their methods to support this setting should be straightforward but would require further research.
- Why unresolved: The paper does not explore the impact of temperature annealing on the reverse-engineering methods. They only provide a brief explanation of how it could be detected.
- What evidence would resolve it: Experimental results showing the performance of the reverse-engineering methods on systems that use temperature annealing in combination with top-k or top-p sampling would provide evidence on how effective the methods are in these scenarios.

### Open Question 3
- Question: How does the choice of known distribution model affect the accuracy of top-p estimation?
- Basis in paper: [explicit] The paper mentions that they experiment with using both GPT-2 Base and GPT-3 Davinci to compute the known distributions for top-p estimation. They observe that using Davinci as the known model leads to a better attack on GPT-3 models but a worse one on all other models.
- Why unresolved: The paper does not provide a definitive answer on which known distribution model is optimal for top-p estimation. They only show the performance of two specific models.
- What evidence would resolve it: A comprehensive study that evaluates the performance of different known distribution models across various language models would provide evidence on the optimal choice for top-p estimation.

## Limitations
- The method assumes the target uses only top-k or top-p sampling exclusively and may fail against hybrid or more sophisticated decoding strategies
- The approach requires multiple API queries which could be computationally expensive, trigger rate limits, or be detected by API providers
- Performance may degrade significantly for models with different tokenization schemes, vocabulary sizes, or generation behaviors not represented in the tested models

## Confidence

**High Confidence Claims:**
- The theoretical framework for distinguishing between top-k and top-p sampling based on unique token counts is sound and mathematically justified
- The core mathematical relationship between observed unique tokens and underlying hyperparameters (k and p) follows from well-established probability theory

**Medium Confidence Claims:**
- The practical implementation details for prompt engineering to induce near-uniform distributions are plausible but may require significant tuning for different models
- The reported accuracy rates for hyperparameter estimation are likely achievable on the tested models but may not generalize to all language models

**Low Confidence Claims:**
- The method's effectiveness against production-grade, proprietary models with sophisticated decoding strategies remains largely unproven
- The attack's resilience to API defenses, caching mechanisms, and detection systems is unknown

## Next Checks

1. **Cross-Architecture Validation**: Test the method on diverse model families including encoder-decoder models (e.g., T5, BART), multilingual models, and models with different tokenization schemes to assess generalizability beyond the tested GPT-2, BLOOM, and Pythia architectures.

2. **Hybrid Decoding Strategy Detection**: Evaluate whether the method can detect and characterize hybrid decoding strategies that combine top-k and top-p sampling, or more sophisticated approaches like top-k with temperature scaling or dynamic threshold adjustment.

3. **Detection and Mitigation Analysis**: Investigate whether the attack can be detected by API providers through pattern analysis of query behavior, and assess the effectiveness of potential mitigation strategies such as request randomization, output caching, or adaptive rate limiting.