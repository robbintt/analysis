---
ver: rpa2
title: 'TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models'
arxiv_id: '2311.04589'
source_url: https://arxiv.org/abs/2311.04589
tags:
- arxiv
- image
- audio
- non-textual
- multi-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TEAL, an approach to enable frozen LLMs to
  perform both understanding and generation tasks involving non-textual modalities,
  such as image and audio. The key idea is to treat the input from any modality as
  a token sequence and learn a joint embedding space for all modalities.
---

# TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models

## Quick Facts
- **arXiv ID:** 2311.04589
- **Source URL:** https://arxiv.org/abs/2311.04589
- **Reference count:** 12
- **Key outcome:** Frozen LLMs achieve 11.5 BLEU points improvement on COCO-caption and 8.5 accuracy points on ScienceQA using token-based multimodal processing.

## Executive Summary
This paper introduces TEAL (Tokenize and Embed ALL), a framework that enables frozen large language models to process non-textual modalities (images and audio) by treating all inputs as token sequences. The core innovation is a joint embedding space where different modalities are projected after tokenization, allowing frozen LLMs to process multimodal inputs autoregressively. The approach uses modality-specific tokenizers, a learnable embedding matrix, projection layers for alignment, and de-tokenizers for generation. Experiments demonstrate substantial improvements over baseline methods on image captioning, visual question answering, and speech recognition tasks while maintaining the LLM's original text capabilities.

## Method Summary
TEAL processes multimodal inputs through a two-stage supervised fine-tuning approach. First, inputs from any modality are discretized into token sequences using off-the-shelf tokenizers (BEiT-V2 for images, Whisper for audio). These token sequences are embedded into a joint embedding space via a learnable embedding matrix. Projection layers align non-textual embeddings with textual embeddings before feeding them to a frozen LLM. The LLM processes tokens autoregressively as it would with text. For generation tasks, predicted token sequences are converted back to modality-specific outputs using de-tokenizers. The method employs bias-norm tuning for parameter efficiency while freezing the LLM core.

## Key Results
- Achieves 11.5 BLEU-4 point improvement on COCO-caption task compared to VQ-GAN baseline
- Obtains 8.5 accuracy point improvement on ScienceQA multimodal question answering
- Reaches 24.22 WER on CoV oST 2 ASR task, outperforming HuBERTlarge baseline (31.77 WER)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Frozen LLMs can perform multimodal tasks without fine-tuning their core parameters by treating all modalities as token sequences.
- **Mechanism:** Inputs from any modality are discretized into token sequences using modality-specific tokenizers, then embedded into a shared space. The frozen LLM processes these tokens autoregressively, treating them identically to text tokens.
- **Core assumption:** All modalities can be represented as discrete tokens compatible with the LLM's tokenization expectations.
- **Evidence anchors:** [abstract] treats inputs as token sequences with joint embedding space; [section] describes discretization and embedding process; [corpus] shows average neighbor FMR=0.44 (weak signal).
- **Break condition:** If modality tokenization loses critical information or tokens don't align semantically with LLM vocabulary.

### Mechanism 2
- **Claim:** Joint embedding space aligns non-textual embeddings with textual embeddings for seamless LLM processing.
- **Mechanism:** Projection layers after non-textual embedding and before output layer project non-textual embeddings into textual embedding space, ensuring dimensional and semantic alignment.
- **Core assumption:** Projection layers can map non-textual embeddings to textual space without semantic loss.
- **Evidence anchors:** [section] describes projection layers' dual purposes; [section] mentions initialization with tokenizer codebooks; [corpus] weak signal on alignment effectiveness.
- **Break condition:** If projection fails to maintain semantic relationships, LLM misinterprets non-textual tokens.

### Mechanism 3
- **Claim:** Tokenizer choice critically impacts multimodal performance through semantic richness of token sequences.
- **Mechanism:** Different tokenizers (DALL-E, VQ-GAN, BEiT-V2 for images; HuBERT, Whisper for audio) produce token sequences with varying semantic information, directly affecting LLM understanding and generation capability.
- **Core assumption:** Semantic information in tokenizer is crucial for modality alignment and effective LLM processing.
- **Evidence anchors:** [section] shows BEiT-V2 achieves 11.5 BLEU improvement over VQ-GAN; [section] speculates BEiT-V2's semantic pre-training provides advantage; [corpus] weak signal.
- **Break condition:** If tokenizer fails to capture semantic information or tokens aren't LLM-compatible.

## Foundational Learning

- **Concept:** Tokenization and Discretization
  - **Why needed here:** Multimodal inputs must be converted to discrete tokens for LLM processing.
  - **Quick check question:** What's the difference between tokenization and discretization, and why are both important here?

- **Concept:** Embedding Spaces and Alignment
  - **Why needed here:** Non-textual embeddings must project into LLM's text embedding space for semantic consistency.
  - **Quick check question:** How do projection layers ensure non-textual embeddings align semantically with textual embeddings?

- **Concept:** Autoregressive Generation
  - **Why needed here:** LLM generates output tokens autoregressively; this must work seamlessly for multimodal tokens.
  - **Quick check question:** How does autoregressive generation differ when handling multimodal versus text tokens?

## Architecture Onboarding

- **Component map:** Tokenizer → Embedding Matrix → Projection Layer → Frozen LLM → Output Matrix → De-tokenizer
- **Critical path:** Input modality → Tokenizer → Joint Embedding Space → Frozen LLM → Output Layer → De-tokenizer → Output modality
- **Design tradeoffs:**
  - Tokenizer choice vs. semantic richness: More complex tokenizers capture more semantics but increase computational cost
  - Projection layer complexity vs. alignment quality: Sophisticated projections improve alignment but add parameters
  - Freezing LLM vs. fine-tuning: Freezing preserves text capabilities but may limit multimodal adaptation
- **Failure signatures:**
  - Poor multimodal task performance: Indicates embedding misalignment or inadequate tokenization
  - Text task degradation: Suggests projection layers interfere with textual processing
  - Generation artifacts: Indicates de-tokenizer or embedding alignment issues
- **First 3 experiments:**
  1. Test different tokenizers on image captioning to evaluate semantic richness impact
  2. Compare performance with and without projection layers to assess alignment necessity
  3. Evaluate audio tokenizer cluster center variation effects on ASR performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does tokenizer choice (BEiT-V2 vs VQ-GAN) impact TEAL performance on multimodal tasks?
- **Basis in paper:** [explicit] Different tokenizers yield different performance levels, but underlying reasons aren't explored.
- **Why unresolved:** Paper shows performance differences but lacks analysis of why tokenizers affect task-specific performance differently.
- **What evidence would resolve it:** Detailed ablation studies comparing multiple tokenizers across tasks with qualitative embedding analysis.

### Open Question 2
- **Question:** What are limitations of K-means clustering vs VQ-based neural codecs for audio tokenization?
- **Basis in paper:** [explicit] Mentions K-means flexibility but doesn't explore limitations compared to VQ methods.
- **Why unresolved:** Paper highlights K-means advantages without discussing potential drawbacks or quality impacts.
- **What evidence would resolve it:** Comparative studies on audio reconstruction quality and semantic fidelity between methods.

### Open Question 3
- **Question:** How does two-stage training (pre-training + fine-tuning) affect TEAL performance and efficiency?
- **Basis in paper:** [explicit] Describes two-stage process but doesn't explore alternative strategies.
- **Why unresolved:** While effective, paper doesn't investigate whether joint training or progressive fine-tuning could improve results.
- **What evidence would resolve it:** Experimental comparisons of TEAL using different training strategies across multimodal tasks.

## Limitations
- Limited generalization to arbitrary modalities beyond images and audio demonstrated
- Embedding space alignment mechanism underspecified and lacks semantic relationship validation
- Evaluation scope narrow with focus on specific tasks rather than absolute performance standards

## Confidence

**High Confidence Claims:**
- TEAL architecture is technically sound and implementable
- Approach enables frozen LLMs to process multimodal inputs without catastrophic forgetting
- Different tokenizers significantly impact performance, validating semantic tokenization importance

**Medium Confidence Claims:**
- Projection layers effectively align non-textual and textual embeddings
- Specific improvements on COCO-caption and ScienceQA are reproducible and significant
- WER of 24.22 on CoV oST 2 represents meaningful improvement over baseline

**Low Confidence Claims:**
- Approach generalizes to arbitrary modalities beyond demonstrated examples
- Semantic alignment through projection layers maintains meaningful cross-modal relationships
- Method scales effectively to complex multimodal generation tasks

## Next Checks

1. **Cross-Tokenizer Ablation Study:** Systematically evaluate TEAL's performance across 5-10 different tokenizer architectures for each modality, measuring both task performance and embedding space similarity metrics to validate semantic alignment robustness.

2. **Modality Extension Experiment:** Apply TEAL to a third modality (video or 3D point clouds) using appropriate tokenizers, evaluating both understanding and generation tasks to test modality-agnostic properties.

3. **Embedding Space Analysis:** Conduct systematic analysis of joint embedding space using nearest neighbor retrieval across modalities, semantic similarity preservation tests, and cross-modal retrieval benchmarks to quantify semantic alignment quality.