---
ver: rpa2
title: 'Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable
  Video Action Recognition'
arxiv_id: '2311.15619'
source_url: https://arxiv.org/abs/2311.15619
tags:
- video
- text
- action
- pages
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large-scale visual-language
  pre-trained models for video action recognition. Most existing methods follow an
  "adapt then align" paradigm that adapts pre-trained image encoders to model video-level
  representations and utilizes one-hot or text embedding of the action labels for
  supervision, overlooking the challenge of mapping from static images to complicated
  activity concepts.
---

# Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition

## Quick Facts
- arXiv ID: 2311.15619
- Source URL: https://arxiv.org/abs/2311.15619
- Authors: 
- Reference count: 40
- Primary result: State-of-the-art performance in fully supervised, few-shot, and zero-shot video action recognition with significantly reduced computational cost

## Executive Summary
This paper introduces a novel "Align before Adapt" (ALT) paradigm for video action recognition that leverages entity-to-region alignments to improve generalization across different learning scenarios. Instead of directly adapting image encoders to video tasks, ALT first establishes fine-grained alignments between visual regions and text entities from a pre-constructed corpus. These alignments are then used as queries in a transformer-based video adapter, allowing the model to explain actions through their underlying entities while preserving the visual-language alignment of pre-trained models. The method achieves state-of-the-art performance on Kinetics-400 in fully supervised settings (88.1% top-1 accuracy with only 4947 GFLOPs) and significantly outperforms existing approaches in few-shot and zero-shot scenarios.

## Method Summary
The ALT method follows a novel "align before adapt" paradigm that leverages entity-to-region alignments for video action recognition. The approach uses a frozen CLIP image encoder to generate region-aware embeddings, then matches these embeddings to a text corpus of action-related entities using a Gumbel-Softmax-based alignment module. The aligned entities' text embeddings are fed as queries to a transformer-based video adapter, which aggregates semantic information across video frames. This allows the model to explain actions through underlying entities (body parts, scenes, objects, motion) while maintaining computational efficiency by freezing the image encoder and only adapting a lightweight video adapter.

## Key Results
- Achieves 88.1% top-1 accuracy on Kinetics-400 with only 4947 GFLOPs in fully supervised settings
- Outperforms state-of-the-art methods by 7.1% and 9.2% in 2-shot experiments on HMDB-51 and UCF-101 respectively
- Demonstrates superior generalizability across fully supervised, few-shot, and zero-shot learning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning entity-to-region correspondences before adapting to video action recognition improves generalization, especially in few-shot and zero-shot scenarios.
- Mechanism: By matching region-aware image embeddings to a text corpus of action-related entities, the model establishes fine-grained visual-semantic correspondences. These alignments are then used as queries in a transformer-based video adapter, preserving the visual-language alignment of VLP during adaptation.
- Core assumption: Actions can be explained by underlying entities (body parts, scenes, objects, motion) that have corresponding regions in images, and these entities are reusable across different action categories.
- Evidence anchors:
  - [abstract] "This paradigm reuses the visual-language alignment of VLP during adaptation and tries to explain an action by the underlying entities."
  - [section] "Using the established alignments, we utilize the text embedding of the entities as queries in a transformer-based decoder for action recognition."
- Break condition: If the text corpus does not contain relevant entities for a given action category, or if the region-aware embeddings cannot reliably capture the corresponding visual regions, the alignment quality will degrade and hurt performance.

### Mechanism 2
- Claim: The transformer-based video adapter that uses entity text embeddings as queries can extract the most important semantics from multi-frame video embeddings.
- Mechanism: The video adapter takes the text embeddings of aligned entities as queries, and the multi-frame visual embeddings as keys and values. Through cross-attention, it aggregates entity-level information across frames to form a discriminative video representation.
- Core assumption: The most discriminative information for action recognition is captured by the aligned entities, and a transformer architecture can effectively aggregate this information across frames.
- Evidence anchors:
  - [abstract] "With the aligned entities, we feed their text embeddings to a transformer-based video adapter as the queries, which can help extract the semantics of the most important entities from a video to a vector."
  - [section] "The CA module utilizes evolved text embeddings as queries to aggregate to the visual embeddings across the frames, preserving the entity-level visual information during the adaption."
- Break condition: If the aligned entities do not capture the most important semantics for a given action, or if the transformer architecture cannot effectively aggregate the information, the video representation quality will suffer.

### Mechanism 3
- Claim: Leveraging entity-to-region alignments reduces the computational cost compared to fully adapting the image encoder for video recognition.
- Mechanism: By freezing the CLIP image encoder and only fine-tuning a lightweight video adapter, the method achieves competitive performance with significantly fewer GFLOPs compared to approaches that adapt the full image encoder.
- Core assumption: The frozen CLIP image encoder provides sufficient region-aware embeddings, and the video adapter can effectively transform these embeddings without needing to adapt the full encoder.
- Evidence anchors:
  - [abstract] "ALT demonstrates competitive performance while maintaining remarkably low computational costs. In fully supervised scenarios, it achieves 88.1% top-1 accuracy on Kinetics-400 with only 4947 GFLOPs."
  - [section] "It enhances our framework by 6.8% in top-1 accuracy on the HMDB-51 dataset under the 2-shot configuration, while reducing computational cost by 23% with the ViT-base backbone."
- Break condition: If the frozen image encoder cannot provide sufficient region-aware embeddings, or if the video adapter cannot effectively transform these embeddings, the performance will degrade and the computational savings may not be worthwhile.

## Foundational Learning

- Concept: Visual-language pre-training (VLP) and how CLIP works
  - Why needed here: The method builds upon CLIP as the backbone image encoder, so understanding its architecture and training objective is crucial for implementing the region-aware image encoder and entity-to-region alignments.
  - Quick check question: What is the main objective used to train CLIP, and how does it enable zero-shot image classification?

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: The video adapter is based on a transformer architecture, and understanding self-attention, cross-attention, and how they can be used to aggregate information across frames is essential for implementing and debugging the adapter.
  - Quick check question: How does cross-attention differ from self-attention, and in what scenarios is cross-attention particularly useful?

- Concept: Few-shot and zero-shot learning paradigms
  - Why needed here: The method is specifically designed to excel in few-shot and zero-shot scenarios, so understanding the challenges and common approaches in these settings is important for evaluating and improving the method's generalization ability.
  - Quick check question: What are the main differences between few-shot and zero-shot learning, and what are some common strategies for addressing the data scarcity in each setting?

## Architecture Onboarding

- Component map: Input video frames -> Region-aware image encoder -> Entity-to-region alignment -> Video adapter -> Video representation -> Action classification
- Critical path: Input video frames → Region-aware image encoder → Entity-to-region alignment → Video adapter → Video representation → Action classification
- Design tradeoffs:
  - Freezing the image encoder reduces computational cost but may limit the model's ability to adapt to video-specific features
  - Using a text corpus of entities improves generalization but requires careful construction and may introduce noise if the corpus is not well-curated
  - The transformer-based video adapter is powerful but adds complexity and computational overhead compared to simpler adapters
- Failure signatures:
  - Poor alignment quality: Entity-to-region alignments are noisy or inaccurate, leading to degraded video representations
  - Insufficient temporal modeling: The video adapter fails to capture important temporal dynamics in the video, hurting action recognition performance
  - Overfitting to seen classes: The model performs well on training classes but poorly on novel classes in few-shot or zero-shot settings
- First 3 experiments:
  1. Verify the region-aware image encoder produces meaningful embeddings by visualizing the merged patches and their corresponding regions
  2. Check the entity-to-region alignment quality by examining the similarity matrix and the top aligned entities for a few sample frames
  3. Test the video adapter's ability to aggregate information across frames by feeding in a simple video with a clear action and examining the resulting video representation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed ALT method's performance vary when using different types of text corpus entities (e.g., body parts, objects, scenes, motion) in isolation versus in combination?
- Basis in paper: [explicit] The paper investigates the effect of different sub-collections of the text corpus in Table C4, showing that each category is helpful, and the models with all text entities further outperformed the baseline.
- Why unresolved: The paper does not provide a detailed analysis of the individual contributions of each type of text entity or explore the potential synergistic effects of combining different types.
- What evidence would resolve it: Conducting ablation studies where each type of text entity is used in isolation and in various combinations, and comparing the performance on different datasets and learning scenarios.

### Open Question 2
- Question: What is the impact of varying the number of token reductions (r) in the image encoder on the efficiency and accuracy trade-off of the ALT method?
- Basis in paper: [explicit] The paper investigates the performance of varying r in Table C5, showing that as r increases, the computational cost decreases, but so does the accuracy.
- Why unresolved: The paper only explores a limited range of r values and does not provide insights into the optimal choice of r for different datasets or learning scenarios.
- What evidence would resolve it: Conducting extensive experiments with a wider range of r values and analyzing the efficiency-accuracy trade-off on various datasets and learning scenarios.

### Open Question 3
- Question: How does the ALT method's performance compare to other state-of-the-art methods when dealing with motion-heavy or instance-agnostic datasets like Something-Something V2?
- Basis in paper: [explicit] The paper acknowledges that existing approaches, which utilize image-language models and primarily emphasize visual semantics, do not provide satisfactory solutions in low-shot scenarios for datasets that are motion-heavy or instance-agnostic, such as Something-Something V2.
- Why unresolved: The paper does not provide a detailed comparison of the ALT method's performance with other state-of-the-art methods on motion-heavy or instance-agnostic datasets.
- What evidence would resolve it: Conducting experiments comparing the ALT method's performance with other state-of-the-art methods on motion-heavy or instance-agnostic datasets, such as Something-Something V2, and analyzing the results.

## Limitations
- The entity-to-region alignment mechanism relies heavily on the quality and coverage of the text corpus, which is not fully specified in the paper
- Performance gains in few-shot and zero-shot scenarios may be partially attributed to specific dataset splits and evaluation protocols rather than the method itself
- Computational efficiency claims are based on FLOPs rather than actual wall-clock time or memory usage, which could differ significantly in practice

## Confidence

**High Confidence**: The fully supervised results on Kinetics-400 (88.1% top-1 accuracy) are well-supported by standard evaluation protocols and comparable to existing methods. The computational efficiency claims are also relatively straightforward to verify.

**Medium Confidence**: The few-shot and zero-shot generalization results are promising but depend heavily on the specific experimental setup, including dataset splits, text corpus construction, and evaluation metrics. The paper provides limited details on these critical aspects.

**Low Confidence**: The claim that the method "explains an action by the underlying entities" is more conceptual than empirically validated. While the paper demonstrates that aligned entities contribute to performance, it does not provide a detailed analysis of which entities are most important for different action categories or how these alignments relate to human understanding of actions.

## Next Checks
1. **Text Corpus Quality Analysis**: Construct multiple text corpora with varying quality and coverage, then evaluate how ALT's performance changes. This would reveal the sensitivity of the method to corpus quality and help identify the minimum viable corpus requirements.

2. **Ablation on Alignment Components**: Systematically remove or modify components of the entity-to-region alignment pipeline (e.g., token merging strategy, similarity computation method, Gumbel-Softmax temperature) to isolate which components contribute most to performance gains.

3. **Cross-Dataset Generalization Test**: Evaluate ALT on a held-out action recognition dataset that was not used in any way during training or text corpus construction. This would provide a more rigorous test of the method's generalization ability beyond the reported few-shot and zero-shot results.