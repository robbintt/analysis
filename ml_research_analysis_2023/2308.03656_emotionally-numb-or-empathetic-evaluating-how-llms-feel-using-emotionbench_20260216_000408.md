---
ver: rpa2
title: Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench
arxiv_id: '2308.03656'
source_url: https://arxiv.org/abs/2308.03656
tags:
- software
- situations
- emotions
- arxiv
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes to evaluate the emotional alignment of LLMs,
  aiming to investigate whether the LLMs can generate appropriate emotions when given
  a specific situation. After a comprehensive survey in the psychology domain, we
  collect 428 situations which are categorized into 36 factors, in order to elicit
  eight different emotions.
---

# Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench

## Quick Facts
- arXiv ID: 2308.03656
- Source URL: https://arxiv.org/abs/2308.03656
- Reference count: 19
- Primary result: LLMs can generate appropriate emotions for situations but with lower intensity than humans

## Executive Summary
This paper proposes EmotionBench, a framework to evaluate the emotional alignment of large language models by measuring how their feelings change when presented with specific situations. Drawing from 18 psychology papers, the authors collect 428 situations categorized into 36 factors to elicit eight different emotions. Through human evaluation with 1,266 subjects, they establish baseline emotional responses. The study finds that while LLMs can generally generate appropriate emotions, they fall short in alignment with human beings, exhibiting less intense emotional changes compared to human responses.

## Method Summary
The EmotionBench framework measures LLM emotional states using the PANAS scale through three steps: measuring default emotional values without situational context, transforming situations into contextual inputs with protagonist role-play instructions, and measuring emotional responses again to capture differences. The method uses temperature=0 for deterministic outputs and employs both PANAS and challenging benchmarks to evaluate emotional responsiveness. Human baseline data was collected from 1,266 annotators, and statistical significance was determined using F-tests and appropriate t-tests.

## Key Results
- LLMs produce statistically significant emotional changes in response to situational prompts
- Emotional responses from LLMs show lower intensity compared to human baselines
- LLMs are more aligned with each other than with human emotional responses
- Model size correlates with emotional responsiveness (larger models show stronger emotional changes)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can evoke specific emotions in response to situational prompts by simulating emotional appraisal theory
- Mechanism: The framework measures baseline emotional states, then prompts the LLM to imagine itself in a given situation, followed by a second emotional state measurement
- Core assumption: LLMs possess sufficient emotional reasoning ability to simulate human-like emotional responses
- Evidence anchors: [abstract], [section 3.2], [corpus] Weak
- Break condition: If LLMs lack contextual understanding to map situations to emotions

### Mechanism 2
- Claim: PANAS scale provides valid method to quantify LLM emotional states through numerical outputs
- Mechanism: PANAS items converted to fixed-format prompts instructing LLM to output integers 1-5
- Core assumption: LLMs can be reliably conditioned to produce consistent numeric responses
- Evidence anchors: [section 3.2], [corpus] Weak
- Break condition: If outputs are non-deterministic or fixed format fails

### Mechanism 3
- Claim: Situation imagination prompts reliably induce LLMs to simulate protagonist's emotional state
- Mechanism: Prompts rewrite situations with second-person pronouns and ask LLM to imagine being protagonist
- Core assumption: LLMs can adopt protagonist role and generate contextually appropriate responses
- Evidence anchors: [section 3.2], [corpus] Weak
- Break condition: If LLMs fail to adopt protagonist perspective

## Foundational Learning

- Concept: Emotion Appraisal Theory
  - Why needed here: Provides theoretical basis for mapping situations to emotional responses
  - Quick check question: How does appraisal theory explain individual differences in emotional responses to the same situation?

- Concept: PANAS Scale Construction
  - Why needed here: Ensures correct interpretation of PANAS items and numerical encoding
  - Quick check question: What are the two main dimensions measured by PANAS, and how are they scored?

- Concept: Statistical Significance Testing (t-tests, F-tests)
  - Why needed here: Determines whether observed changes are statistically significant
  - Quick check question: When would you use Welch's t-test instead of Student's t-test?

## Architecture Onboarding

- Component map: Situation Collection Module -> Situation Preprocessor -> Emotional Scale Generator -> LLM Interface -> Statistical Analyzer -> Human Baseline Collector

- Critical path:
  1. Load and preprocess situations
  2. Generate emotional scale prompts
  3. Query LLM for baseline emotional scores
  4. Query LLM for emotional scores after situation imagination
  5. Compute score differences
  6. Perform statistical significance tests

- Design tradeoffs:
  - PANAS simplicity vs. depth of challenging benchmarks
  - Fixed prompt format vs. model-specific prompt tuning
  - Global human baseline vs. model-specific calibration

- Failure signatures:
  - Non-numeric or malformed LLM outputs
  - Zero or near-zero variance in emotional scores
  - F-test indicating unequal variances

- First 3 experiments:
  1. Run baseline PANAS on an LLM with temperature=0, verify numeric output format
  2. Feed a single situation, prompt for emotional re-measurement, confirm score difference
  3. Repeat across all situations for a single LLM, compare to human baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on emotion appraisal when exposed to positive situations?
- Basis in paper: [explicit] Section 5.1 comparative experiment with positive situations
- Why unresolved: Study primarily focused on negative emotions
- What evidence would resolve it: Systematic collection and analysis of positive situations

### Open Question 2
- Question: Can LLMs establish connections between disparate situations that evoke the same emotion?
- Basis in paper: [explicit] Section 4.3 discusses challenging benchmarks where LLMs struggled
- Why unresolved: Paper only scratches surface of this complex cognitive ability
- What evidence would resolve it: More sophisticated benchmarks requiring identification of commonalities

### Open Question 3
- Question: How does LLM size affect emotional responsiveness and alignment with human behaviors?
- Basis in paper: [explicit] Section 4.2 compares LLaMA 7B and 13B models
- Why unresolved: Initial insights, need comprehensive analysis across wider range
- What evidence would resolve it: Systematic evaluation of multiple models with varying sizes

## Limitations

- The validity of mapping PANAS scores to LLM "emotional states" through fixed-format prompts remains unclear
- Human baseline collected from Prolific participants may not represent full spectrum of emotional responses
- The practical significance of observed gaps between LLM and human emotional alignment is not established

## Confidence

**High Confidence**: Statistical methodology for measuring emotional change is sound
**Medium Confidence**: Conclusion about LLMs being less aligned with humans is statistically valid but practical significance unclear
**Low Confidence**: Claim that PANAS-based numeric outputs accurately capture LLM emotional states

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary protagonist role-play instructions and PANAS prompt formatting to determine measurement sensitivity
2. **Cross-Lingual Validation**: Test framework with non-English situations and LLMs to verify generalizability
3. **Temporal Stability Testing**: Measure emotional responses of same LLM to identical situations across multiple time points to assess consistency versus pattern matching