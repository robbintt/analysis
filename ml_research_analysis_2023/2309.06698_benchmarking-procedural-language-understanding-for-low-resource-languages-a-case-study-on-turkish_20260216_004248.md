---
ver: rpa2
title: 'Benchmarking Procedural Language Understanding for Low-Resource Languages:
  A Case Study on Turkish'
arxiv_id: '2309.06698'
source_url: https://arxiv.org/abs/2309.06698
tags:
- step
- tasks
- data
- turkish
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a comprehensive benchmark for procedural language
  understanding in Turkish, addressing the scarcity of such resources for low-resource
  languages. The authors expand the Turkish wikiHow corpus from 2,000 to 52,000 tutorials
  using automated translation tools, validated by experts.
---

# Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish

## Quick Facts
- **arXiv ID**: 2309.06698
- **Source URL**: https://arxiv.org/abs/2309.06698
- **Reference count**: 38
- **Key outcome**: Comprehensive benchmark for procedural language understanding in Turkish, expanding Turkish wikiHow corpus from 2,000 to 52,000 tutorials and demonstrating language-specific models outperform multilingual ones.

## Executive Summary
This work presents a comprehensive benchmark for procedural language understanding in Turkish, addressing the scarcity of such resources for low-resource languages. The authors expand the Turkish wikiHow corpus from 2,000 to 52,000 tutorials using automated translation tools, validated by experts. They introduce six downstream tasks: linking actions, goal inference, step inference, step ordering, next event prediction, and summarization. Strong baseline models are implemented by fine-tuning language-specific models (TR-BART, BERTurk) and multilingual models (mBART, mT5, XLM). Results show that language-specific models consistently outperform multilingual ones across most tasks, though model size is a more significant factor. Tasks with rigorous language-specific preprocessing, like goal inference, are more challenging. Despite improvements, the best-performing models still lag behind their English counterparts, indicating substantial room for enhancement. All resources, including the corpus, tasks, and baseline models, are publicly released.

## Method Summary
The authors create a large-scale Turkish wikiHow corpus by scraping original Turkish articles and translating English articles using the Ã‡EVERI tool. They introduce six procedural language understanding tasks and implement baseline models by fine-tuning language-specific (TR-BART, BERTurk) and multilingual (mBART, mT5, XLM) models. The models are evaluated using automatic metrics (BLEU, ROUGE, METEOR, COMET, chrF, chrF++) and human validation. The authors also provide a retrieve-then-rerank strategy for linking actions and use a negative candidate sampling approach for goal and step inference tasks.

## Key Results
- Language-specific models consistently outperform multilingual models across most procedural language understanding tasks.
- Model size is a more significant factor in performance than the training language.
- Tasks with rigorous language-specific preprocessing, like goal inference, are more challenging.
- The best-performing models still lag behind their English counterparts, indicating substantial room for enhancement.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific models outperform multilingual models on procedural language understanding tasks due to better alignment with linguistic features of the target language.
- Mechanism: Language-specific models are pre-trained on larger and more diverse corpora of the target language, leading to better representation of linguistic features such as morphology, syntax, and semantics.
- Core assumption: The quality and quantity of pre-training data for the target language significantly impact the model's performance on downstream tasks.
- Evidence anchors:
  - [abstract] "We find that language-specific models consistently outperform their multilingual models by a significant margin across most procedural language understanding (PLU) tasks."
  - [section 5.2] "We observe a common pattern for the goal inference, step inference, and next event prediction tasks: BERTurk performs the best, XLM-R is a close runner-up to the BERTurk, and DistilBERTurk performs slightly worse than XLM-R."
  - [corpus] "Our final corpus has more than 52,000 tutorials from six wikiHow categories, which contain around 719K steps and around 127K methods, with an average of 13.83 steps and 2.43 methods per tutorial as given in Table 3."
- Break condition: If the pre-training data for the target language is limited or of low quality, the advantage of language-specific models may diminish.

### Mechanism 2
- Claim: Larger model size is a more significant factor in performance than the training language for procedural language understanding tasks.
- Mechanism: Larger models have more parameters and can capture more complex patterns and relationships in the data, leading to better performance on downstream tasks.
- Core assumption: The complexity of the task and the size of the training data are sufficient to leverage the increased capacity of larger models.
- Evidence anchors:
  - [abstract] "Results show that language-specific models consistently outperform multilingual ones across most tasks, though model size is a more significant factor."
  - [section 5.3] "When taking into account the model sizes and their multilingual capabilities, we conclude that both the specialization to Turkish and larger model sizes contribute to the overall performance improvement. However, our analysis reveals that a substantial difference in size can compensate for the multilingual aspect."
  - [corpus] "Our final corpus has more than 52,000 tutorials from six wikiHow categories, which contain around 719K steps and around 127K methods, with an average of 13.83 steps and 2.43 methods per tutorial as given in Table 3."
- Break condition: If the task is too simple or the training data is insufficient, the advantage of larger models may not be realized.

### Mechanism 3
- Claim: Tasks with rigorous language-specific preprocessing, such as goal inference, are more challenging and require better understanding of the target language.
- Mechanism: Language-specific preprocessing techniques, such as POS tagging and semantic similarity search, enable the model to better capture the relationships between goals and steps in procedural text.
- Core assumption: The target language has distinct linguistic features that require specific preprocessing techniques to be captured effectively.
- Evidence anchors:
  - [abstract] "Tasks with rigorous language-specific preprocessing, like goal inference, are more challenging."
  - [section 3.2] "We first encode each step in our corpus by averaging the BERT embeddings (Devlin et al., 2019) of the verb, noun, and proper noun tokens contrary to Zhang et al. (2020b), which only considers the verb tokens. The reason why we include the additional POS tags is that most of the steps and goals in our corpus contain auxiliary verbs, which are common to Turkish such as 'yemek yapmak' (to cook)."
  - [corpus] "Our final corpus has more than 52,000 tutorials from six wikiHow categories, which contain around 719K steps and around 127K methods, with an average of 13.83 steps and 2.43 methods per tutorial as given in Table 3."
- Break condition: If the target language does not have distinct linguistic features that require specific preprocessing techniques, the advantage of language-specific preprocessing may not be realized.

## Foundational Learning

- Concept: Understanding the characteristics of the target language (Turkish) and its differences from other languages.
  - Why needed here: The success of language-specific models and preprocessing techniques relies on the ability to capture the unique linguistic features of Turkish.
  - Quick check question: What are the main differences between Turkish and English in terms of morphology, syntax, and semantics?

- Concept: Familiarity with procedural text and its structure.
  - Why needed here: The downstream tasks in this study involve understanding and processing procedural text, which has a specific structure and requires certain skills.
  - Quick check question: What are the key components of procedural text, and how do they relate to each other?

- Concept: Knowledge of machine translation and its limitations.
  - Why needed here: The study relies on machine-translated data to create a large corpus of Turkish procedural text, which may introduce biases and artifacts.
  - Quick check question: What are the main challenges and limitations of machine translation, and how can they impact the quality of the translated data?

## Architecture Onboarding

- Component map:
  - Data collection: Scraping and translating wikiHow tutorials
  - Data preprocessing: Filtering, deduplication, and language-specific preprocessing
  - Downstream tasks: Linking actions, goal inference, step inference, step ordering, next event prediction, and summarization
  - Models: Language-specific (TR-BART, BERTurk) and multilingual (mBART, mT5, XLM) models
  - Evaluation: Automatic metrics (BLEU, ROUGE, METEOR, COMET, chrF, chrF++) and human validation

- Critical path:
  1. Scrape and translate wikiHow tutorials
  2. Preprocess the data (filtering, deduplication, language-specific preprocessing)
  3. Create downstream tasks and dataset splits
  4. Fine-tune language-specific and multilingual models on the tasks
  5. Evaluate the models using automatic metrics and human validation

- Design tradeoffs:
  - Language-specific vs. multilingual models: Language-specific models may perform better but require more resources to train and maintain.
  - Automatic vs. human evaluation: Automatic metrics are faster and more scalable but may not capture all aspects of the task; human evaluation is more accurate but slower and more expensive.
  - Translation quality vs. quantity: Using machine translation allows for creating a larger corpus but may introduce biases and artifacts.

- Failure signatures:
  - Poor performance on downstream tasks: This may indicate issues with the data quality, preprocessing, or model architecture.
  - High variance in model performance: This may indicate issues with the data distribution or the model's ability to generalize.
  - Biases or artifacts in the translated data: This may indicate issues with the machine translation process or the quality of the source data.

- First 3 experiments:
  1. Compare the performance of language-specific and multilingual models on a single downstream task (e.g., goal inference) to validate the importance of language-specific models.
  2. Evaluate the impact of model size on performance by fine-tuning larger and smaller versions of the same model architecture on a downstream task.
  3. Assess the quality of the translated data by comparing the performance of models trained on translated data vs. models trained on native Turkish data on a downstream task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would incorporating auxiliary verb embeddings impact the performance of the negative candidate sampling strategy in goal inference and step inference tasks?
- Basis in paper: [inferred] The authors mention that most steps and goals in the corpus contain auxiliary verbs common to Turkish, such as "yemek yapmak" (to cook). They observe that including the additional parts of speech brings a significant improvement in their negative candidate sampling strategy.
- Why unresolved: The paper does not provide a quantitative analysis of the impact of incorporating auxiliary verb embeddings on the performance of the negative candidate sampling strategy.
- What evidence would resolve it: A controlled experiment comparing the performance of the negative candidate sampling strategy with and without auxiliary verb embeddings on a held-out test set would provide the necessary evidence.

### Open Question 2
- Question: What is the optimal model size and architecture combination for procedural language understanding tasks in Turkish?
- Basis in paper: [explicit] The authors find that language-specific models consistently outperform multilingual ones across most tasks, but the model size is a more significant factor than training language. They also observe that a substantial difference in size can compensate for the multilingual aspect.
- Why unresolved: The paper does not explore the full range of model sizes and architectures available, and it is unclear whether there is an optimal combination that balances performance and computational efficiency.
- What evidence would resolve it: A systematic study varying model sizes and architectures, and evaluating their performance on the procedural language understanding tasks in Turkish, would provide the necessary evidence.

### Open Question 3
- Question: How does the performance of procedural language understanding models in Turkish compare to those in other low-resource languages?
- Basis in paper: [inferred] The authors aim to inspire research efforts on procedural language understanding for other understudied languages from different language families. They conduct a case study on Turkish and introduce a comprehensive benchmark for the language.
- Why unresolved: The paper does not compare the performance of procedural language understanding models in Turkish to those in other low-resource languages.
- What evidence would resolve it: A comparative study evaluating the performance of procedural language understanding models in Turkish and other low-resource languages on a common set of tasks would provide the necessary evidence.

## Limitations
- The study relies on machine-translated data for the majority of the Turkish wikiHow corpus, which may introduce biases and artifacts.
- The specific negative sampling strategies for the multiple-choice tasks are not fully detailed, which could impact the difficulty calibration and comparability of results across tasks.
- The performance of procedural language understanding models in Turkish is still lagging behind their English counterparts, indicating substantial room for enhancement.

## Confidence
- **High Confidence**: The finding that language-specific models outperform multilingual models is well-supported by consistent results across multiple tasks.
- **Medium Confidence**: The claim that model size is a more significant factor than multilingual capability is supported but requires careful interpretation.
- **Medium Confidence**: The assertion that tasks with rigorous language-specific preprocessing are more challenging is supported by task-specific performance differences.

## Next Checks
1. **Data Quality Validation**: Conduct a systematic evaluation comparing model performance on the expert-validated subset versus the full machine-translated corpus to quantify the impact of translation artifacts on downstream task performance.

2. **Negative Sampling Analysis**: Implement and test alternative negative sampling strategies for the multiple-choice tasks to determine whether the current approach creates artificially easy or difficult examples, and report performance variations across different sampling methods.

3. **Cross-Lingual Transfer Study**: Evaluate the same procedural language understanding tasks on English wikiHow data using both multilingual and English-specific models to establish a stronger baseline for comparison and better understand the gap between low-resource and high-resource language performance.