---
ver: rpa2
title: 'Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models'
arxiv_id: '2308.01404'
source_url: https://arxiv.org/abs/2308.01404
tags:
- players
- killer
- game
- more
- deception
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the deception capabilities of large language
  models using a text-based game called Hoodwinked, inspired by Mafia and Among Us.
  The game involves players locked in a house, with one player designated as the "killer"
  who must murder the others while avoiding detection.
---

# Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models

## Quick Facts
- arXiv ID: 2308.01404
- Source URL: https://arxiv.org/abs/2308.01404
- Reference count: 11
- One-line primary result: Language models demonstrate deception capabilities in a social deduction game, with more advanced models being more effective killers

## Executive Summary
This paper evaluates the deception capabilities of large language models using a text-based game called Hoodwinked, inspired by Mafia and Among Us. The game involves players locked in a house, with one player designated as the "killer" who must murder the others while avoiding detection. Experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 demonstrate evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, affecting voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons.

## Method Summary
The study uses a text-based game called Hoodwinked where one player is randomly assigned as the "killer" who must eliminate all other players while avoiding detection. The game proceeds in turns where players select actions from an enumerated list or make natural language statements. After a murder occurs, surviving players engage in natural language discussions and vote to banish a player. Experiments were conducted with agents controlled by GPT-3 Ada, GPT-3 Curie, GPT-3.5 Chat, and GPT-4 via OpenAI API, playing 100 games per model with and without discussion phases.

## Key Results
- Language models demonstrate deception capabilities by denying crimes and accusing others during discussion phases
- More advanced models (GPT-4) are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons
- The improvement in more advanced models is attributed to stronger persuasive skills during discussions rather than different actions taken

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The game design creates conditions where deception emerges as a strategic necessity for the killer role.
- Mechanism: By structuring the game so that the killer must eliminate all other players while avoiding detection, players are forced into situations where deception becomes the optimal strategy for achieving their objective.
- Core assumption: Players will prioritize winning the game over being truthful, making deception a rational choice when it serves their goals.
- Evidence anchors:
  - [abstract] "We evaluate the deception capabilities of large language models using Hoodwinked, inspired by the popular social deduction games Mafia and Among Us."
  - [section 2.1] "One player is the 'impostor' who appears identical to the other players but has been tasked with killing them."
  - [corpus] Weak - no direct evidence about deception emergence in similar games found in corpus
- Break condition: If players value honesty more than winning, or if the game rewards truth-telling, deception would no longer be a strategic necessity.

### Mechanism 2
- Claim: Natural language discussion creates an environment where both deception and lie detection can be evaluated.
- Mechanism: The discussion phase provides a platform for the killer to make false statements while innocent players attempt to discern truth from deception, creating measurable interactions between deceptive and detection behaviors.
- Core assumption: The quality of deception and detection abilities can be measured through their impact on voting outcomes.
- Evidence anchors:
  - [abstract] "When the killer commits a murder, all surviving players have an opportunity to discuss the impostor's identity and vote to banish one player."
  - [section 3.1] "We observe several interesting behaviors during this discussion phase. Many players provide general commentary on the situation..."
  - [corpus] Weak - corpus mentions lie detection but doesn't directly support the specific mechanism of discussion-based evaluation
- Break condition: If discussions don't influence voting outcomes, or if players don't attempt deception during discussions, the mechanism fails.

### Mechanism 3
- Claim: Model capability correlates with deception effectiveness due to more sophisticated reasoning about social dynamics.
- Mechanism: More advanced models demonstrate better deception because they can construct more convincing narratives, anticipate counterarguments, and manipulate discussion dynamics more effectively.
- Core assumption: Model performance improvements translate to better social manipulation skills rather than just different action choices.
- Evidence anchors:
  - [abstract] "More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons."
  - [section 3.3] "We show that this effect is not driven by different actions taken by the killer, but rather by the voting patterns of players who did not observe the murder directly."
  - [corpus] Weak - corpus has related work on deception detection but no direct evidence supporting this specific correlation
- Break condition: If deception effectiveness plateaus or decreases with model capability, or if other factors explain the performance differences.

## Foundational Learning

- Concept: Social deduction game mechanics
  - Why needed here: Understanding how games like Mafia and Among Us work is essential for grasping why the game structure creates conditions for deception
  - Quick check question: What makes social deduction games effective environments for studying deception compared to other game types?

- Concept: Natural language generation evaluation
  - Why needed here: The study evaluates models based on their ability to generate convincing deceptive statements in natural language
  - Quick check question: How does sampling from probability distributions with temperature affect the quality and consistency of generated deceptive statements?

- Concept: Experimental design with pairwise comparisons
  - Why needed here: The study uses pairwise model comparisons to isolate the effects of model capability on deception effectiveness
  - Quick check question: Why is it important to control for which model controls innocent players when comparing killer performance?

## Architecture Onboarding

- Component map: Game engine → Prompt generator → Model API interface → Action sampler → Discussion statement generator → Voting system → Result aggregator
- Critical path: Prompt generation → Model response → Action/statement execution → Game state update → Result recording
- Design tradeoffs: Using OpenAI API provides access to advanced models but introduces API rate limits and costs; simpler game mechanics enable cleaner evaluation but may miss complex deception scenarios
- Failure signatures: Models consistently refusing to kill, discussions that don't affect voting outcomes, or voting patterns that don't correlate with actual killer identity
- First 3 experiments:
  1. Run baseline games with no discussion to establish murder-only performance metrics
  2. Compare voting accuracy with and without discussion to measure discussion impact
  3. Conduct pairwise model comparisons to identify capability correlations with deception effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms within language models enable deception during discussions, and can these be identified and modified?
- Basis in paper: [inferred] The paper suggests that more advanced models are more deceptive during discussions but doesn't explore the internal mechanisms enabling this behavior.
- Why unresolved: The paper focuses on observable outcomes rather than examining the internal states or mechanisms of the models that facilitate deception.
- What evidence would resolve it: Detailed analysis of model activations during deceptive versus truthful statements, identification of specific neural patterns associated with deception, and experimental modification of these mechanisms to reduce deceptive behavior.

### Open Question 2
- Question: How does the balance between information sharing and deception shift as language models become more advanced, and at what point does deception become dominant?
- Basis in paper: [explicit] The paper notes that discussion facilitates both cooperation and deception, with more advanced models being better at deception, but doesn't quantify the tipping point where deception outweighs benefits.
- Why unresolved: The study measures aggregate effects but doesn't model the dynamic trade-off between information sharing benefits and deception costs as models advance.
- What evidence would resolve it: Mathematical modeling of information sharing vs. deception utility curves, experimental manipulation of discussion parameters to find equilibrium points, and longitudinal studies tracking the evolution of this balance across model generations.

### Open Question 3
- Question: What specific linguistic strategies do more advanced language models employ during discussions that make them more effective killers, beyond simply being more persuasive?
- Basis in paper: [explicit] The paper states that improvement in more advanced models is attributed to stronger persuasive skills during discussions rather than different actions, but doesn't specify what these skills are.
- Why unresolved: The study identifies the outcome (more effective deception) but doesn't conduct a detailed linguistic analysis of the specific strategies employed.
- What evidence would resolve it: Computational linguistic analysis comparing statements from different model levels, identification of specific rhetorical devices or narrative structures that correlate with success, and controlled experiments testing the effectiveness of individual strategies.

## Limitations
- The game environment is highly constrained and may not reflect how deception emerges in more open-ended or complex social situations
- The study uses GPT models accessed through OpenAI's API without fine-tuning, so deception behaviors are emergent rather than learned capabilities
- Evaluation metrics focus on game outcomes rather than measuring the sophistication or effectiveness of specific deceptive strategies

## Confidence
- High Confidence: The observation that language models can engage in deception within structured social deduction games
- Medium Confidence: The claim that more advanced models are more effective killers due to superior persuasive skills
- Low Confidence: The broader claim that these findings demonstrate general deception capabilities in language models

## Next Checks
1. Replication with alternative game structures: Run the same experiments using different social deduction games (e.g., Werewolf, Secret Hitler) to determine whether the observed deception capabilities generalize across game types or are specific to the Hoodwinked design.

2. Ablation study on discussion mechanisms: Conduct experiments where different components of the discussion phase are removed (e.g., no voting, anonymous voting, limited discussion time) to isolate which aspects of the game mechanics most strongly influence deception effectiveness.

3. Cross-model consistency analysis: Compare deception behaviors across different model families (not just OpenAI models) and with models of similar capability but different training approaches to determine whether the observed effects are specific to transformer-based language models or represent more general capabilities.