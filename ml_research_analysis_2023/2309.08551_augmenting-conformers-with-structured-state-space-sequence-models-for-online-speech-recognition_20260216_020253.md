---
ver: rpa2
title: Augmenting conformers with structured state-space sequence models for online
  speech recognition
arxiv_id: '2309.08551'
source_url: https://arxiv.org/abs/2309.08551
tags:
- convolution
- ssms
- test
- online
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We explored augmenting neural encoders for online speech recognition
  with structured state-space sequence models (SSMs). We proposed two new approaches:
  stacking SSMs with convolutions (COM) and reparameterizing convolution kernels with
  SSMs (REP).'
---

# Augmenting conformers with structured state-space sequence models for online speech recognition

## Quick Facts
- **arXiv ID**: 2309.08551
- **Source URL**: https://arxiv.org/abs/2309.08551
- **Reference count**: 0
- **Primary result**: Achieved 4.01%/8.53% WER on Librispeech test clean/other sets using SSMformers

## Executive Summary
This paper explores augmenting neural encoders for online speech recognition with structured state-space sequence models (SSMs). The authors propose three approaches: drop-in replacement (DIR), stacking SSMs with convolutions (COM), and reparameterizing convolution kernels with SSMs (REP). Their best model achieves 4.01%/8.53% WER on Librispeech test sets, outperforming conformers with extensively tuned convolutions. The work demonstrates that SSMs can be effective even for local context modeling, motivating further theoretical study of their capabilities.

## Method Summary
The authors augment conformer encoders with SSMs using three approaches: DIR replaces convolutions with SSMs, COM stacks SSMs with small-kernel convolutions, and REP reparameterizes convolution kernels using SSMs. They use S4D-Real and S4D-Lin parameterizations with small state dimensions (N=2-4) and train using RNN-Transducer models on Librispeech with SpecAugment and variational noise. The models are evaluated on WER metrics using frame-sync beam search decoding.

## Key Results
- SSMformers with COM approach achieve 4.01%/8.53% WER on Librispeech test clean/other sets
- COM outperforms DIR and REP approaches consistently across settings
- Small SSMs with real-valued recurrent weights (S4D-Real) perform best
- SSMformers outperform conformers with extensively tuned convolutions

## Why This Works (Mechanism)

### Mechanism 1
Small SSMs with real-valued recurrent weights are effective at local context modeling because they can capture local patterns through discretized convolution kernels that can be truncated to finite size. The S4D-Real parameterization with negative real parts ensures stability while maintaining parameter efficiency. Evidence shows S4D-Real consistently outperforms S4D-Lin across settings.

### Mechanism 2
The COM approach works because SSMs and small-kernel convolutions complement each other by modeling different aspects of local structure. SSMs learn compact representations of convolution operations that can approximate finite-size kernels more efficiently than direct parameterization. This is evidenced by consistent WER improvements when combining SSMs with small-kernel convolutions.

### Mechanism 3
REP approach succeeds because SSMs can learn to approximate finite-size convolution kernels more efficiently than direct parameterization. The SSM's ability to generate arbitrary-length kernels through the CĀ^B recurrence allows it to discover compact representations that outperform hand-tuned finite kernels, even though SSMs are traditionally thought to excel at long-range dependencies.

## Foundational Learning

- **State-space models as parameterized RNNs**: Understanding SSMs as linear RNNs with specific weight structures helps grasp how they capture temporal dependencies differently from standard RNNs or CNNs. Quick check: How does the SSM discretization (Ā = e^{A∆}) ensure stability while maintaining expressiveness?

- **Convolution-kernel view of SSMs**: The equivalence between SSM outputs and convolutions (K̃ = [CĀB, CĀ^2B, ..., CĀ^{T-1}B]) explains why SSMs can replace convolutions and how their infinite-context capability emerges. Quick check: What mathematical property of the SSM ensures that K̃ can represent arbitrarily long kernels with fixed parameters?

- **Online vs offline ASR architectural constraints**: The distinction between unlimited left context (offline) and only left context (online) determines which SSM approaches are viable and how they should be integrated. Quick check: Why does the drop-in replacement approach provide unlimited left context while the reparameterization approach does not?

## Architecture Onboarding

- **Component map**: Input → 2D conv subsampling → Layer 1 → ... → Layer N → MHSA → Output. SSM computations occur in parallel across feature dimensions within each convolution module.

- **Critical path**: Each encoder layer contains MHSA module (unchanged) and convolution module (modified). Convolution module contains optional pre-SSM convolution (kernel size 2-16), SSM (N×N diagonal A, N×H B, H×N C, H×H D), and post-SSM batch norm/linear layer.

- **Design tradeoffs**: DIR provides unlimited context but requires careful SSM dimension tuning; COM balances local and global modeling but adds parameters; REP maintains inference efficiency but may limit SSM expressiveness.

- **Failure signatures**: WER degradation indicates mismatched kernel sizes (COM), insufficient SSM capacity (DIR/REP), or optimization difficulties with complex parameterization (S4D-Lin).

- **First 3 experiments**:
  1. Compare DIR with baseline conformer using different N values (2, 4, 8) to establish baseline SSM effectiveness.
  2. Test COM with fixed N=4 and varying pre-SSM convolution kernel sizes (2, 4, 8, 16) to find optimal local context combination.
  3. Evaluate REP with varying kernel truncation lengths (L=2, 4, 8, 16) to determine optimal finite context modeling.

## Open Questions the Paper Calls Out

- **Open Question 1**: Why do small SSMs with real-valued recurrent weights perform best in online ASR despite theoretical results suggesting larger dimensions are more effective? This contradicts theoretical expectations and previous empirical results in ASR literature using larger SSM dimensions.

- **Open Question 2**: What is the fundamental difference between SSMs and convolutions that makes SSMs effective even when forced to model local context? The paper acknowledges this as an open theoretical question but doesn't provide a definitive explanation.

- **Open Question 3**: Why does the combination (COM) approach work better than either component alone, and what is the optimal relationship between SSM and convolution parameters? While empirical results show COM outperforms other approaches, the mechanism by which SSM and convolution complement each other remains unexplained.

## Limitations
- Lack of directly comparable baselines in literature for SSM effectiveness in online ASR
- Theoretical understanding of why small SSMs work for local context modeling remains limited
- Empirical results may not generalize beyond Librispeech dataset to other ASR benchmarks

## Confidence
- **High Confidence**: Empirical observation that S4D-Real outperforms S4D-Lin and that COM approach provides consistent WER improvements over DIR
- **Medium Confidence**: Mechanism explaining why small-state SSMs work for local context - while empirical evidence supports this, theoretical justification is limited
- **Low Confidence**: Broader claim that SSMs can be effective for local context modeling in online ASR - contradicts conventional wisdom and lacks strong theoretical grounding

## Next Checks
1. **Ablation on SSM State Dimension**: Systematically test SSMformers with N=1,2,4,8,16 across all three approaches (DIR, COM, REP) on Librispeech to establish the precise relationship between SSM capacity and online ASR performance.

2. **Theoretical Analysis of Local Context Capture**: Mathematically analyze how the SSM recurrence CĀ^B generates kernels that capture local patterns, and derive conditions under which small-state SSMs approximate finite convolution kernels effectively.

3. **External Validation on Different ASR Datasets**: Evaluate the best-performing SSMformer configuration (COM with S4D-Real, N=4) on alternative speech recognition benchmarks (e.g., WSJ, Common Voice) to test generalizability beyond Librispeech.