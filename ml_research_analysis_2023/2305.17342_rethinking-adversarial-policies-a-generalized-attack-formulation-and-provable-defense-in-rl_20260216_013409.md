---
ver: rpa2
title: 'Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable
  Defense in RL'
arxiv_id: '2305.17342'
source_url: https://arxiv.org/abs/2305.17342
tags:
- adversarial
- attack
- policy
- victim
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies adversarial policies in multi-agent reinforcement
  learning, where an attacker controls an agent to act against a victim agent. Existing
  approaches assume full control of the attacker, which may not be realistic and can
  lead to detectable abnormal behaviors.
---

# Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL

## Quick Facts
- arXiv ID: 2305.17342
- Source URL: https://arxiv.org/abs/2305.17342
- Reference count: 40
- Primary result: Generalized attack framework allows partial control over agent with probability ϵπ, enabling stealthy attacks while maintaining convergence guarantees for robust victim policy training

## Executive Summary
This paper addresses adversarial policies in multi-agent reinforcement learning by proposing a generalized attack framework that enables partial control over the attacker's ability to influence the victim's state distribution. Unlike existing approaches that assume full control of the attacker, this work introduces an attack budget parameter ϵπ that regulates how much the attacker can deviate from the victim's original policy. The paper develops adversarial training algorithms with timescale separation to find the most robust victim policy, providing provable defenses with convergence guarantees. Experiments on Kuhn Poker and Robosumo demonstrate that the generalized attacks reduce state distribution shifts while achieving the same winning rate as baselines, and the adversarial training methods lead to stable learning dynamics and less exploitable victim policies.

## Method Summary
The method introduces a generalized attack formulation where the attacker controls another agent α with adversarial policy πα, parameterized by attack budget ϵπ. The attack model is (1-ϵπ)bπα + ϵππα, where b is the victim's policy and πα is the adversarial policy. The framework generalizes action adversarial RL to multi-agent settings and allows for stealthy attacks by aligning state distributions. Adversarial training algorithms with timescale separation are developed, where the attacker's policy is updated with a larger learning rate while the victim's policy is updated with a smaller learning rate. This approach ensures convergence to the most robust victim policy against adversarial attacks.

## Key Results
- Generalized attacks reduce state distribution shifts while achieving same winning rate as unconstrained attacks in Robosumo environments
- Adversarial training with timescale separation converges to less exploitable victim policies compared to single-timescale training and self-play
- The proposed methods demonstrate stable learning dynamics and provide provable defenses with convergence guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attack budget ϵπ regulates partial control over the attacker's ability to influence the victim's state distribution, enabling stealthier attacks.
- Mechanism: By controlling ϵπ, the attacker can limit the TV distance between the original and attacked state distributions, reducing detectable anomalies.
- Core assumption: The attacker's control over the victim agent is probabilistic and bounded by ϵπ.
- Evidence anchors: [abstract] mentions ϵπ parameter; [section] provides Proposition 3.2 on bounded policy discrepancy; corpus papers focus on white-box attacks not partial control.

### Mechanism 2
- Claim: Timescale separation in adversarial training ensures convergence to the most robust victim policy by decoupling the attacker's and victim's learning rates.
- Mechanism: A larger learning rate for the attacker allows it to quickly adapt to the victim's policy, while a smaller rate for the victim ensures stable convergence to the minimax robust solution.
- Core assumption: The attacker and victim can be trained independently with different learning rates without destabilizing training dynamics.
- Evidence anchors: [abstract] discusses timescale separation; [section] presents Algorithm 2; corpus papers lack explicit discussion of timescale separation in multi-agent RL.

### Mechanism 3
- Claim: The generalized attack formulation extends single-agent action adversarial RL to multi-agent settings, allowing the attacker to exploit the victim indirectly through another agent.
- Mechanism: By controlling another agent α with adversarial policy πα, the attacker can indirectly influence the victim ν's performance without directly perturbing its state or actions.
- Core assumption: The victim's performance is affected by the actions of other agents in the environment.
- Evidence anchors: [abstract] mentions generalization to multi-agent settings; [section] discusses attack model's connection to action adversarial RL; corpus papers focus on direct attacks not indirect approaches.

## Foundational Learning

- Concept: Markov games and their extension of MDPs to multi-agent settings
  - Why needed here: The paper studies adversarial attacks in multi-agent RL, requiring understanding of the game-theoretic framework of Markov games
  - Quick check question: Can you explain the difference between an MDP and a Markov game in terms of the number of agents and their objectives?

- Concept: Adversarial training and its application to robust reinforcement learning
  - Why needed here: The paper develops adversarial training algorithms with timescale separation to find the most robust victim policy
  - Quick check question: How does adversarial training in RL differ from adversarial training in supervised learning, and why is timescale separation crucial in the RL setting?

- Concept: Policy gradient methods and their convergence properties in non-convex non-concave optimization
  - Why needed here: The paper analyzes the convergence of adversarial training algorithms in the presence of non-convex non-concave objective functions
  - Quick check question: What are the challenges in analyzing the convergence of policy gradient methods in non-convex non-concave optimization problems, and how does the paper address these challenges?

## Architecture Onboarding

- Component map: Attacker(πα) -> Victim(πν) -> Environment(Markov game) -> Attacker(πα)
- Critical path: 1) Initialize policies, 2) Update attacker policy (large learning rate), 3) Update victim policy (small learning rate), 4) Repeat until convergence
- Design tradeoffs: Stealthiness vs. attack effectiveness (smaller ϵπ values reduce detectable anomalies but may weaken attack capability); learning rate selection affects convergence speed and stability
- Failure signatures: Unstable training dynamics (oscillations or divergence), ineffective attacks (failure to degrade victim performance), poor robustness (exploitability by unseen adversarial policies)
- First 3 experiments: 1) Compare state distribution shifts between constrained and unconstrained attacks, 2) Evaluate convergence and robustness of adversarial training with/without timescale separation on Kuhn Poker, 3) Test effectiveness in Robosumo against baselines like self-play and fictitious play

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed generalized attack formulation perform in multi-agent environments with more than two agents?
- Basis in paper: The paper focuses on two-player zero-sum games and mentions extension to multiple agents but lacks experimental results or theoretical analysis
- Why unresolved: The paper does not explore scalability and effectiveness in complex multi-agent environments with multiple attackers and victims
- What evidence would resolve it: Experimental results and theoretical analysis demonstrating performance in multi-agent environments with more than two agents, including impact on state distribution shifts and attack success rates

### Open Question 2
- Question: How does the choice of the attack budget ϵπ affect the trade-off between attack success rate and stealthiness in different environments?
- Basis in paper: The paper discusses the trade-off through ϵπ and provides Robosumo results for specific values but lacks systematic analysis across environments
- Why unresolved: The paper does not explore how optimal ϵπ varies across different environments and tasks
- What evidence would resolve it: Comprehensive study analyzing the relationship between ϵπ and the trade-off across various environments and tasks, identifying optimal ϵπ values for different scenarios

### Open Question 3
- Question: How does the proposed adversarial training with timescale separation perform compared to other defense strategies in terms of convergence speed and robustness against adaptive attacks?
- Basis in paper: The paper introduces the method as a provably robust defense and compares to single-timescale training, self-play, and fictitious play but lacks comparison to other defense strategies
- Why unresolved: The paper does not explore relative strengths compared to other state-of-the-art defense methods or performance against adaptive attackers
- What evidence would resolve it: Comparative studies evaluating the proposed method against other defense strategies including convergence speed, robustness against adaptive attacks, and computational efficiency

## Limitations
- The assumption that partial control via ϵπ guarantees stealthiness may not hold against sophisticated detection mechanisms
- Effectiveness of indirect attacks may be limited in environments where victim performance is not significantly influenced by other agents
- Computational complexity of adversarial training algorithms may be prohibitive in large-scale, high-dimensional environments

## Confidence
- **High**: Theoretical convergence guarantees for adversarial training with timescale separation
- **Medium**: Effectiveness of generalized attack formulation in reducing state distribution shifts
- **Medium**: Robustness of victim policy against unseen adversarial attacks

## Next Checks
1. Conduct ablation studies to quantify the impact of attack budget ϵπ on both stealthiness and attack effectiveness across different environments
2. Implement a detection mechanism to measure the victim's ability to identify and mitigate the proposed attacks under varying ϵπ values
3. Extend evaluation to more complex multi-agent environments (e.g., StarCraft II) to assess scalability and generalizability of proposed methods