---
ver: rpa2
title: 'Large Language Models in Cryptocurrency Securities Cases: Can a GPT Model
  Meaningfully Assist Lawyers?'
arxiv_id: '2308.06032'
source_url: https://arxiv.org/abs/2308.06032
tags:
- complaint
- legal
- case
- securities
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates LLMs\u2019 legal reasoning\
  \ and drafting capabilities in litigation, specifically in cryptocurrency securities\
  \ cases. The research tested GPT-3.5\u2019s ability to identify potential legal\
  \ violations from case fact patterns and compared ChatGPT-drafted complaints against\
  \ lawyer-drafted ones using mock juror assessments."
---

# Large Language Models in Cryptocurrency Securities Cases: Can a GPT Model Meaningfully Assist Lawyers?

## Quick Facts
- arXiv ID: 2308.06032
- Source URL: https://arxiv.org/abs/2308.06032
- Reference count: 21
- Key outcome: GPT-3.5 showed poor legal reasoning capabilities in identifying cryptocurrency securities violations but performed adequately at drafting complaints that convinced mock jurors as effectively as lawyer-drafted versions

## Executive Summary
This study evaluates GPT-3.5's capabilities in legal reasoning and drafting within cryptocurrency securities cases. The research found that GPT-3.5 performed poorly at identifying potential legal violations from case fact patterns, with low precision (0.333), recall (0.118), and F1-score (0.174). However, ChatGPT demonstrated adequate drafting capabilities, with mock jurors showing no statistically significant difference in their decisions based on whether complaints were AI- or lawyer-drafted. The findings suggest LLMs are not yet capable of replacing lawyers in legal reasoning tasks but could assist with drafting, potentially improving access to justice by reducing legal service costs.

## Method Summary
The study employed a mixed-methods approach using real-world cryptocurrency securities cases from class actions and SEC enforcement actions. For the legal reasoning task, researchers used zero-shot prompting with the IRAC (Issue, Rule, Application, Conclusion) framework to ask GPT-3.5 to identify potential violations from case fact patterns extracted from Law360 articles. The model's outputs were compared against actual charged violations to calculate precision, recall, and F1-score metrics. For the drafting task, ChatGPT was used to generate complaints from the same fact patterns, which were then compared against lawyer-drafted complaints using a mock juror survey. Jurors assessed whether allegations were proven and their confidence in decisions, while linguistic analysis measured concreteness differences between AI- and lawyer-drafted documents.

## Key Results
- GPT-3.5's legal reasoning performance was poor with precision of 0.333, recall of 0.118, and F1-score of 0.174
- Mock jurors found no statistically significant difference between AI- and lawyer-drafted complaints in terms of proven allegations (80% vs 88%)
- Jurors showed similar confidence levels (average 4.05 vs 4.21 on 5-point scale) for both AI- and lawyer-authored complaints
- GPT-3.5's violations suggestions tended to be correct but missed additional, correct violations (high false negatives, low false positives)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5 performs better at legal drafting than legal reasoning because it can generate coherent, context-aware text when given specific prompts, but lacks deep understanding of legal concepts needed for accurate violation identification.
- Mechanism: GPT-3.5 leverages its large-scale language modeling capabilities to produce text that mimics legal document structure and style, but its reasoning ability is limited by the quality and scope of its training data, particularly for specialized legal domains like cryptocurrency securities law.
- Core assumption: GPT-3.5's training data included sufficient legal text examples to enable competent drafting but not enough specialized knowledge to accurately identify violations.
- Evidence anchors:
  - [abstract]: "ChatGPT performed better at legal drafting, and jurors' decisions were not statistically significantly associated with the author of the document upon which they based their decisions."
  - [section]: "Overall, GPT-3.5's ability to determine potential violations of U.S. law from a given set of facts was poor... The primary reason for the low overall performance metrics was the high number of false negatives (i.e., violations it missed which were charged in the complaint)."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.325, average citations=0.0. Top related titles include "Better Call GPT, Comparing Large Language Models Against Lawyers" suggesting other research confirms GPT-3.5's drafting capabilities.
- Break condition: If GPT-3.5 is given prompts with insufficient context or specificity, its drafting quality will degrade significantly.

### Mechanism 2
- Claim: Mock jurors make decisions based on the perceived completeness and coherence of complaints rather than their actual legal merit, as evidenced by their similar decisions for both AI- and lawyer-drafted documents.
- Mechanism: Jurors rely on surface-level indicators of legal validity (clear structure, professional language, logical flow) when evaluating complaints, which GPT-3.5 can replicate effectively even without deep legal understanding.
- Core assumption: Jurors in this study were not evaluating the actual legal merits of the cases but rather the persuasiveness of the written documents.
- Evidence anchors:
  - [abstract]: "jurors' decisions were not statistically significantly associated with the author of the document upon which they based their decisions."
  - [section]: "Overwhelmingly, ChatGPT drafted convincing complaints, which performed only slightly worse than the lawyer-drafted ones in terms of convincing jurors the allegations were proven (80% of AI-drafted complaints were proven according to our jurors, compared to 88% of lawyer-drafted ones)."
  - [corpus]: Found 25 related papers but average citations=0.0 suggests limited external validation of this specific finding.
- Break condition: If jurors were instructed to evaluate legal merit rather than document persuasiveness, the difference between AI- and lawyer-drafted complaints would likely become more apparent.

### Mechanism 3
- Claim: GPT-3.5's tendency to minimize false positives (incorrectly identifying violations) while missing true violations (false negatives) makes it safer for preliminary legal work than a model that might generate incorrect legal claims.
- Mechanism: GPT-3.5's conservative approach to legal reasoning reduces the risk of generating false or misleading legal claims, which is particularly important in legal contexts where incorrect claims could constitute malpractice.
- Core assumption: In legal contexts, the cost of false positives (incorrectly identifying violations) is higher than the cost of false negatives (missing violations).
- Evidence anchors:
  - [abstract]: "GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely missed additional, correct violations)."
  - [section]: "This shows more promise in terms of LLMs' ability to conduct legal reasoning tasks on behalf of lawyers, at least in a preliminary capacity. It would be preferable for fewer, but correct, charges to be identified, as opposed to the inclusion of erroneous charges, at least from a malpractice perspective."
  - [corpus]: Found 25 related papers but average citations=0.0 suggests limited external validation of this specific finding.
- Break condition: If legal professionals prioritize comprehensive coverage over accuracy, GPT-3.5's conservative approach would be less useful.

## Foundational Learning

- Concept: Legal reasoning frameworks (like IRAC - Issue, Rule, Application, Conclusion)
  - Why needed here: The study used IRAC as a prompting technique to structure GPT-3.5's legal analysis, demonstrating the importance of structured reasoning in legal AI applications.
  - Quick check question: How would you modify an IRAC prompt to improve GPT-3.5's identification of cryptocurrency securities violations?

- Concept: Concreteness in computational linguistics
  - Why needed here: The study used concreteness scores to quantify differences between AI- and lawyer-drafted complaints, showing how linguistic analysis can reveal differences in writing style.
  - Quick check question: What concreteness score would you expect for a complaint written in highly technical legal jargon versus plain language?

- Concept: False positives vs. false negatives in legal contexts
  - Why needed here: Understanding the different costs of these error types is crucial for evaluating GPT-3.5's performance and potential applications in legal work.
  - Quick check question: In what legal scenarios might false positives be more costly than false negatives?

## Architecture Onboarding

- Component map:
  - Data collection -> Case selection and fact pattern extraction
  - Model selection -> GPT-3.5 (text-davinci-003 for reasoning, gpt-3.5-turbo-0301 for drafting)
  - Prompt engineering -> Zero-shot prompts with IRAC framework for reasoning, multi-step prompts for drafting
  - Evaluation metrics -> Precision, recall, F1-score for reasoning; mock juror decisions for drafting
  - Quality control -> Juror confidence scores, concreteness analysis, qualitative comparison

- Critical path:
  1. Select cases and extract fact patterns
  2. Design and test prompts
  3. Generate outputs and calculate reasoning metrics
  4. Draft AI complaints and prepare lawyer comparisons
  5. Conduct mock juror survey
  6. Analyze results and compare performance

- Design tradeoffs:
  - Using real cases provides realistic evaluation but limits control over variables
  - Zero-shot prompting simplifies implementation but may miss optimization opportunities
  - Mock jurors provide human evaluation but introduce variability and limited sample size

- Failure signatures:
  - Low precision in reasoning indicates GPT-3.5 is generating incorrect legal claims
  - Low recall in reasoning suggests GPT-3.5 is missing relevant violations
  - High variability in juror decisions indicates inconsistent drafting quality
  - Significant concreteness differences between AI and lawyer drafts suggest style issues

- First 3 experiments:
  1. Test different temperature settings (0.2 vs 1.0) on reasoning task to evaluate impact on precision and recall
  2. Compare zero-shot vs few-shot prompting for drafting task to assess performance improvements
  3. Analyze specific cases where GPT-3.5 performed poorly to identify patterns in prompt failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would GPT-3.5's performance change if provided with relevant legal texts in the prompt?
- Basis in paper: [inferred] from Nay et al (2023) finding that providing relevant legal texts moderately improved GPT-4 results, suggesting this could help GPT-3.5 as well.
- Why unresolved: This study used zero-shot transfer without fine-tuning or providing relevant legal texts in prompts.
- What evidence would resolve it: Experiment with few-shot prompting and providing relevant legal texts to GPT-3.5, then compare performance metrics to the current zero-shot results.

### Open Question 2
- Question: Would a more recent LLM model (trained after June 2021) perform better at identifying cryptocurrency securities violations?
- Basis in paper: [explicit] The authors note they used GPT-3.5 (trained pre-June 2021) and expect future models to improve.
- Why unresolved: This study only tested GPT-3.5; newer models like GPT-4 were not available during the study period.
- What evidence would resolve it: Test the same cases and prompts on GPT-4 or other post-June 2021 models, comparing precision, recall, and F1 scores.

### Open Question 3
- Question: Does the concreteness of source material (e.g., Law360 articles) affect the quality of AI-drafted legal complaints?
- Basis in paper: [explicit] The authors found a significant positive correlation between Law360 article concreteness and ChatGPT-generated complaint concreteness.
- Why unresolved: While correlation is established, causation and practical implications remain unclear.
- What evidence would resolve it: Systematically vary the concreteness level of source materials and measure the resulting complaint quality using both linguistic analysis and mock juror assessments.

## Limitations

- The evaluation relied on mock jurors rather than legal professionals, potentially missing nuanced quality differences between AI- and lawyer-drafted complaints
- Results are limited to cryptocurrency securities cases and may not generalize to other legal domains
- The study used zero-shot prompting without exploring few-shot or fine-tuning approaches that might improve performance

## Confidence

- **High confidence**: GPT-3.5 is not yet capable of replacing lawyers in legal reasoning tasks (supported by clear quantitative metrics showing poor precision, recall, and F1-score)
- **Medium confidence**: GPT-3.5 can draft complaints that perform similarly to lawyer-drafted versions in convincing mock jurors (based on limited sample size and non-professional evaluators)
- **Low confidence**: Broader implications for access to justice and cost reduction (not directly measured in the study)

## Next Checks

1. **Professional Validation**: Replicate the drafting evaluation with actual judges and practicing lawyers rather than mock jurors to assess whether legal professionals detect meaningful quality differences between AI- and lawyer-drafted complaints.

2. **Cross-Domain Testing**: Test GPT-3.5's reasoning capabilities across multiple legal domains (criminal, family, corporate law) to determine whether the poor performance is specific to cryptocurrency securities or represents a more general limitation.

3. **Model Comparison**: Compare GPT-3.5's performance against both junior lawyers and other AI models (GPT-4, Claude) to establish whether the observed limitations are inherent to current LLMs or specific to this model version.