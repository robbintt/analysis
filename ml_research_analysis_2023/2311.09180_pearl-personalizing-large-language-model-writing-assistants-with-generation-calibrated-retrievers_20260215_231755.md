---
ver: rpa2
title: 'Pearl: Personalizing Large Language Model Writing Assistants with Generation-Calibrated
  Retrievers'
arxiv_id: '2311.09180'
source_url: https://arxiv.org/abs/2311.09180
tags:
- text
- work
- target
- generation
- post
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PEARL, a writing assistant that personalizes
  LLM generations through retrieval augmentation. The key novelty is a generation-calibrated
  retriever trained to select historical user documents that improve generation quality.
---

# Pearl: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers

## Quick Facts
- arXiv ID: 2311.09180
- Source URL: https://arxiv.org/abs/2311.09180
- Reference count: 40
- Primary result: Generation-calibrated retrievers improve personalized text generation by aligning retrieval scores with downstream generation quality

## Executive Summary
This paper introduces PEARL, a writing assistant that personalizes LLM generations through retrieval augmentation. The key innovation is a generation-calibrated retriever trained to select historical user documents that improve generation quality. PEARL uses a KL-divergence objective that scales retrieval scores to reflect downstream generation benefits. Experiments on social media datasets demonstrate PEARL outperforms non-personalized and retrieval-augmented baselines in both automated and manual evaluations. The calibrated scores also enable performance prediction and selective revision of low-quality generations.

## Method Summary
PEARL personalizes LLM writing assistants by retrieving historical user documents that improve generation quality. The method trains a generation-calibrated retriever using KL-divergence loss with scale calibration, selects training data based on auxiliary model likelihood differences, and uses the retriever to fetch relevant documents for LLM prompting. The auxiliary model computes document relevance scores offline, which guide both training data selection and retriever calibration. The system operates in three phases: training data creation using the auxiliary model, retriever training with KL-divergence loss, and inference using retrieved documents as few-shot examples in the LLM prompt.

## Key Results
- PEARL outperforms non-personalized and retrieval-augmented baselines in automated metrics (ROUGE, BertScore) and manual evaluations
- Generation-calibrated retriever scores correlate with downstream generation quality, enabling performance prediction
- Selective revision of low-quality generations further improves overall results
- Scale calibration ensures predicted scores from the trained retriever accurately reflect the distribution of the auxiliary model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Generation-calibrated retrievers improve personalized text generation by aligning retrieval scores with downstream generation quality.
- **Mechanism**: The retriever is trained using a scale-calibrating KL-divergence objective that ensures scores reflect the benefit of documents for personalized generation.
- **Core assumption**: The auxiliary model's likelihood scores accurately represent the downstream generation benefit.
- **Evidence anchors**:
  - [abstract]: "scale-calibrating KL-divergence objective that ensures that our retriever scores remain proportional to the downstream generation quality"
  - [section 3.3]: "scale-calibrating KL-divergence loss (Yan et al., 2022), thereby optimizing it directly for the personalized generation task"
  - [corpus]: Weak - corpus shows related work on retriever calibration but no direct evidence of this specific mechanism

### Mechanism 2
- **Claim**: Selective data filtering improves retriever performance by focusing training on requests that benefit from personalization.
- **Mechanism**: Training data is filtered using auxiliary model scores to identify requests and documents that benefit from retrieval augmentation.
- **Core assumption**: Not all requests benefit equally from personalization, and filtering helps focus on beneficial cases.
- **Evidence anchors**:
  - [section 3.2]: "Identifying a subset of training requests that are likely to benefit from personalization"
  - [section 4.3]: "This step reflects the idea that not all pairs will benefit from retrieval augmentation"
  - [corpus]: Weak - corpus shows related work on training data selection but no direct evidence of this specific filtering approach

### Mechanism 3
- **Claim**: Scale calibration improves score distribution matching between auxiliary and trained retrievers.
- **Mechanism**: An anchor example with median score ensures predicted scores from the trained retriever more accurately reflect the distribution of the auxiliary model.
- **Core assumption**: The median score serves as a good reference point for scaling.
- **Evidence anchors**:
  - [section 3.3]: "setting y0 to the median value of scores from Eq (1) for positive candidate documents works well"
  - [section 4.3]: "This helps the predicted scores from fretr more accurately reflect the distribution of faux"
  - [corpus]: Weak - corpus shows related work on scale calibration but no direct evidence of this specific application

## Foundational Learning

- **KL Divergence**:
  - Why needed here: Used to train the retriever to match the score distribution of the auxiliary model
  - Quick check question: What is the difference between standard KL divergence and scale-calibrated KL divergence?

- **Crossencoder Architecture**:
  - Why needed here: Allows the retriever to jointly process requests and documents for better matching
  - Quick check question: Why might a crossencoder be preferred over a bi-encoder for this task?

- **In-context Learning**:
  - Why needed here: The retrieved documents are used as few-shot examples in the LLM prompt
  - Quick check question: How does the number of retrieved examples (k) affect the generation quality?

## Architecture Onboarding

- **Component map**: User Request → Retriever (fretr) → LLM → Output Text. Auxiliary model (faux) used offline for training data creation.
- **Critical path**: Request → Retriever → LLM → Output (generation path is the performance-critical path)
- **Design tradeoffs**: Crossencoder retriever provides better matching but is slower than bi-encoder; larger k improves personalization but increases prompt size and cost
- **Failure signatures**: Poor generation quality despite good retrieval scores might indicate mismatch between auxiliary and main LLM; low retriever scores across all documents might indicate underspecified requests
- **First 3 experiments**:
  1. Compare retrieval performance (precision@k) between PEARL and baselines
  2. Measure correlation between retriever scores and generation quality metrics
  3. Test different values of k (number of retrieved examples) to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scale-calibrated KL-divergence objective compare to other calibration methods like temperature scaling or Platt scaling in terms of improving retrieval performance and calibration?
- Basis in paper: [explicit] The paper introduces a scale-calibrating KL-divergence objective and mentions that it helps the predicted scores from the retriever more accurately reflect the distribution of the auxiliary model.
- Why unresolved: The paper only compares the scale-calibrating KL-divergence to a standard KL-divergence objective and does not compare it to other calibration methods commonly used in machine learning.
- What evidence would resolve it: A controlled experiment comparing the performance of the scale-calibrating KL-divergence objective to other calibration methods like temperature scaling or Platt scaling on the same datasets and tasks.

### Open Question 2
- Question: How does the performance of PEARL change when using different types of historical documents for personalization, such as emails, documents, or code, compared to social media posts?
- Basis in paper: [inferred] The paper mentions that PEARL could be extended to use heterogeneous sets of documents per user, but does not explore this possibility.
- Why unresolved: The paper only evaluates PEARL on social media datasets and does not investigate its performance on other types of documents that might be used for personalization.
- What evidence would resolve it: An experiment comparing the performance of PEARL on different types of historical documents, such as emails, documents, or code, to its performance on social media posts.

### Open Question 3
- Question: How does the choice of the auxiliary text generation model (faux) affect the performance of PEARL, and what are the trade-offs between using a larger model for better generation quality and a smaller model for faster training data creation?
- Basis in paper: [explicit] The paper mentions that the auxiliary model should be smaller than the main LLM to support efficient training data creation and that using a larger open model for faux will incur additional costs.
- Why unresolved: The paper does not explore the impact of using different sizes or types of auxiliary models on the performance of PEARL.
- What evidence would resolve it: An experiment comparing the performance of PEARL using different auxiliary models, such as varying sizes or architectures, to determine the optimal choice for balancing generation quality and training efficiency.

## Limitations
- Limited evaluation to social media datasets; performance on other document types unknown
- Missing implementation details for prompt templates and exact training hyperparameters
- Selective filtering might exclude valuable training examples in different contexts
- Scale calibration mechanism requires empirical validation on varied data distributions

## Confidence
- Generation-calibration effectiveness: **High** (supported by multiple experiments and metrics)
- Selective filtering benefits: **Medium** (logical but needs broader validation)
- Scale calibration mechanism: **Medium** (theoretically sound but limited empirical evidence)

## Next Checks
1. Replicate selective filtering impact: Test PEARL's performance with different filtering thresholds and on datasets where not all requests clearly benefit from personalization.

2. Validate KL-divergence calibration: Measure the correlation between calibrated retriever scores and generation quality across multiple domains and request types.

3. Test robustness to document distribution: Evaluate PEARL's performance when user document collections vary significantly in size, recency, and topical diversity.