---
ver: rpa2
title: Learning Differentiable Particle Filter on the Fly
arxiv_id: '2312.05955'
source_url: https://arxiv.org/abs/2312.05955
tags:
- particle
- online
- filters
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying differentiable
  particle filters in real-world scenarios where data arrive sequentially and distribution
  shifts are common. Traditional approaches rely on offline supervised training, which
  can lead to poor performance when test-time data differs from training data.
---

# Learning Differentiable Particle Filter on the Fly

## Quick Facts
- arXiv ID: 2312.05955
- Source URL: https://arxiv.org/abs/2312.05955
- Reference count: 26
- One-line primary result: Online learning framework for differentiable particle filters enables adaptation to distribution shifts without ground truth states, outperforming pre-trained models on sequential data.

## Executive Summary
This paper addresses the challenge of deploying differentiable particle filters in real-world scenarios where data arrive sequentially and distribution shifts are common. Traditional approaches rely on offline supervised training, which can lead to poor performance when test-time data differs from training data. The authors propose an online learning framework for differentiable particle filters (OL-DPFs) that enables parameter updates as new data arrive, without requiring ground truth state information. The method maximizes an evidence lower bound (ELBO) approximation using sliding windows of observations, allowing the model to adapt to distribution shifts.

## Method Summary
The OL-DPF framework combines normalising flows-based differentiable particle filters with an online learning algorithm that maximizes an ELBO approximation. The approach uses sliding windows of observations to update model parameters θ and ϕ every L time steps through gradient descent on the ELBO approximation computed from particle weights. This enables adaptation to distribution shifts without ground truth state information. The method employs normalising flows to construct flexible dynamic models, proposal distributions, and measurement models that can be learned online.

## Key Results
- OL-DPFs significantly outperform pre-trained DPFs when distribution shift occurs, achieving lower RMSE values
- The model successfully adapts to new data distributions, converging within 1,000-3,000 time steps depending on dimensionality
- OL-DPFs demonstrate robust online Bayesian filtering in dynamic environments without requiring labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online learning enables differentiable particle filters to adapt to distribution shifts without ground truth states by maximizing an ELBO approximation using sliding windows of observations.
- Mechanism: The model maintains time-varying parameters θS and ϕS within sliding windows of length L, updating them through gradient descent on the ELBO approximation computed from particle weights. This allows the filter to adapt to new data distributions as they arrive sequentially.
- Core assumption: The ELBO approximation using particle weights is sufficiently accurate to guide parameter updates, and the sliding window approach provides a practical balance between computational cost and adaptation speed.
- Evidence anchors:
  - [abstract] "The method maximizes an evidence lower bound (ELBO) approximation using sliding windows of observations, allowing the model to adapt to distribution shifts."
  - [section] "we propose to decompose the whole trajectory into sliding windows of length L and update θ and ϕ every L time steps."
  - [corpus] No direct corpus evidence found for the specific ELBO approximation approach used here.
- Break condition: The approximation becomes too coarse if the distribution shift is severe or if the sliding window length L is poorly chosen, leading to ineffective adaptation.

### Mechanism 2
- Claim: Normalizing flows enable flexible construction of state-space model components, allowing the differentiable particle filter to learn complex dynamics and measurement models without prior structural knowledge.
- Mechanism: Normalizing flows transform simple base distributions through invertible neural networks to create complex proposal, dynamic, and measurement models. This provides the flexibility needed for online adaptation while maintaining differentiability for gradient-based learning.
- Core assumption: Normalizing flows are sufficiently expressive to capture the true underlying distributions in the data, and the change of variable formula enables accurate computation of densities needed for particle weight updates.
- Evidence anchors:
  - [section] "we construct the proposed OL-DPF with a flexible differentiable particle filtering framework called normalising flows-based differentiable particle filters (NF-DPFs)"
  - [section] "By applying the change of variable formula, the proposal density of xt can be computed as: q (xt|xt−1, yt; ϕ) = h (ˆxt|xt−1, yt; ϕ) |det JFϕ (ˆxt; yt)|−1"
  - [corpus] No direct corpus evidence found for the specific normalizing flow implementation details.
- Break condition: If the normalizing flows are not expressive enough to capture the true data distributions, the model will fail to adapt properly despite online learning.

### Mechanism 3
- Claim: The unsupervised loss based on particle weights enables online learning without requiring ground truth state information, making the approach practical for real-world applications.
- Mechanism: By using the particle filter's own weight estimates as an unsupervised signal, the model can optimize parameters through maximum likelihood estimation without labeled data. This enables deployment in scenarios where ground truth is expensive or impossible to obtain.
- Core assumption: The particle filter's weight estimates provide a sufficiently informative signal for parameter optimization, even in the absence of ground truth.
- Evidence anchors:
  - [abstract] "The technical constraint is that there is no known ground truth state information in the online inference setting. We address this by adopting an unsupervised loss to construct the online model updating procedure"
  - [section] "In OL-DPFs, parameters of differentiable particle filters are optimised by maximising a training objective modified from the evidence lower bound (ELBO) of the marginal log-likelihood in an online, unsupervised manner."
  - [corpus] No direct corpus evidence found for the specific unsupervised loss formulation used here.
- Break condition: If the particle weights become degenerate or uninformative, the unsupervised loss may provide poor optimization signals, leading to parameter updates that degrade performance.

## Foundational Learning

- Concept: Sequential Monte Carlo methods and particle filtering fundamentals
  - Why needed here: The entire approach builds on particle filtering theory, including importance sampling, weight updates, and resampling strategies
  - Quick check question: Can you explain how particle weights are updated in a standard particle filter and why resampling is necessary?

- Concept: Normalizing flows and invertible neural networks
  - Why needed here: The method relies on normalizing flows to construct flexible, learnable components for the state-space model
  - Quick check question: How does the change of variable formula enable density computation for transformed distributions in normalizing flows?

- Concept: Evidence lower bound (ELBO) and variational inference
  - Why needed here: The online learning objective is based on maximizing an ELBO approximation, which requires understanding of variational inference principles
  - Quick check question: Why is the ELBO a lower bound on the marginal likelihood, and how does Jensen's inequality apply in this context?

## Architecture Onboarding

- Component map: Observations → Particle filtering with current parameters → Weight computation → ELBO approximation → Gradient computation → Parameter update → New particle generation with updated parameters
- Critical path: Sequential observations arrive → Particle filter processes with current parameters → Weights computed → ELBO approximation calculated → Gradients computed → Parameters updated → New particles generated
- Design tradeoffs: Sliding window length L trades off between adaptation speed and computational cost; normalizing flow architecture complexity trades off between expressiveness and training stability; particle count Np trades off between estimation accuracy and computational burden.
- Failure signatures: Degenerate particle weights (effective sample size too low), slow or failed convergence of parameters, poor performance on distribution-shifted data despite adaptation attempts.
- First 3 experiments:
  1. Implement the basic differentiable particle filter with normalizing flows on a simple linear Gaussian state-space model to verify core functionality
  2. Add the sliding window mechanism and verify that parameters can be updated online without ground truth states
  3. Test adaptation to distribution shift by training on one data distribution and evaluating on a shifted distribution with online learning enabled

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OL-DPFs compare to traditional online parameter estimation methods for particle filters when the structure of the state-space model is partially known?
- Basis in paper: [explicit] The paper mentions that traditional online parameter estimation methods are designed as nested structures for solving two layers of Bayesian filtering problems, but they assume the structure of the state-space model is known. OL-DPFs do not make this assumption.
- Why unresolved: The paper does not provide a direct comparison between OL-DPFs and traditional online parameter estimation methods for particle filters.
- What evidence would resolve it: An experiment comparing the performance of OL-DPFs with traditional online parameter estimation methods for particle filters in a scenario where the structure of the state-space model is partially known.

### Open Question 2
- Question: What is the impact of the length of sliding windows (L) on the convergence speed and accuracy of OL-DPFs?
- Basis in paper: [explicit] The paper mentions that the whole trajectory is decomposed into sliding windows of length L, and the model parameters are updated every L time steps. However, the optimal value of L is not discussed.
- Why unresolved: The paper does not provide a systematic study of the impact of L on the performance of OL-DPFs.
- What evidence would resolve it: An experiment varying the value of L and measuring the convergence speed and accuracy of OL-DPFs for different sliding window lengths.

### Open Question 3
- Question: How does the performance of OL-DPFs scale with increasing dimensionality of the state-space model?
- Basis in paper: [explicit] The paper evaluates the performance of OL-DPFs on multivariate linear Gaussian state-space models with varying dimensionalities (2, 5, and 10). The results show that the convergence speed decreases as the dimensionality increases.
- Why unresolved: The paper only tests up to 10 dimensions, and it is unclear how OL-DPFs would perform in even higher dimensional spaces.
- What evidence would resolve it: An experiment testing OL-DPFs on state-space models with even higher dimensionalities to assess the scalability of the method.

## Limitations
- Uncertainty about the effectiveness of ELBO approximation for severe distribution shifts or poorly chosen sliding window lengths
- Unknown expressiveness of normalizing flows for complex real-world data distributions
- Potential failure when particle weights become degenerate or uninformative for parameter optimization

## Confidence
- High confidence in core methodology and theoretical framework
- Medium confidence in practical effectiveness across diverse real-world scenarios
- Low confidence in scalability to very high-dimensional state spaces without modifications

## Next Checks
1. Test the OL-DPF on a real-world sequential dataset with known distribution shifts (e.g., sensor data from changing environments) to validate performance beyond synthetic examples
2. Conduct ablation studies varying the sliding window length L and particle count Np to understand their impact on adaptation speed and accuracy
3. Compare OL-DPF performance against alternative online adaptation methods like continual learning or domain adaptation techniques in the same distribution shift scenarios