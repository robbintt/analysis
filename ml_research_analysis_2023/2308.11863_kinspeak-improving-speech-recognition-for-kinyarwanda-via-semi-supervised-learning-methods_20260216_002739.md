---
ver: rpa2
title: 'KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised
  learning methods'
arxiv_id: '2308.11863'
source_url: https://arxiv.org/abs/2308.11863
tags:
- speech
- data
- kinyarwanda
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates techniques to improve automatic speech
  recognition (ASR) for Kinyarwanda, a low-resource Bantu language. The authors explore
  self-supervised pre-training, curriculum-based fine-tuning, and semi-supervised
  learning to leverage large amounts of unlabelled speech data.
---

# KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods

## Quick Facts
- arXiv ID: 2308.11863
- Source URL: https://arxiv.org/abs/2308.11863
- Reference count: 9
- Key outcome: New state-of-the-art 3.2% WER on Kinyarwanda speech recognition using syllable-based tokenization, curriculum fine-tuning, and semi-supervised learning

## Executive Summary
This paper presents KinSPEAK, a novel approach to improving automatic speech recognition (ASR) for Kinyarwanda, a low-resource Bantu language. The authors combine self-supervised pre-training, curriculum-based fine-tuning, and semi-supervised learning to leverage both labelled and unlabelled speech data. They develop a mobile application for efficient speech-text alignment and collect a high-quality studio dataset from public sources. The study demonstrates that syllable-based tokenization outperforms character-based tokenization for Kinyarwanda, achieving 3.2% WER on a new dataset and 15.9% WER on the Mozilla Common Voice benchmark, setting a new state-of-the-art for Kinyarwanda ASR.

## Method Summary
The authors employ a multi-stage approach to improve Kinyarwanda ASR: (1) self-supervised pre-training on 22,000 hours of unlabelled YouTube speech data using contrastive objectives similar to wav2vec2.0, (2) curriculum-based fine-tuning starting with 86 hours of clean studio data from JW.ORG followed by progressively adding noisier Mozilla Common Voice examples, and (3) semi-supervised learning where the model transcribes unlabelled YouTube utterances, filters them by CTC beam scores and language model probabilities, and adds the highest-quality transcriptions back to the training set for multiple generations. They also develop a mobile application for efficient speech-text alignment and compare character-based versus syllable-based tokenization, finding the latter superior for Kinyarwanda due to its morphological structure.

## Key Results
- Achieved 3.2% WER on JW.ORG test set and 15.9% WER on Mozilla Common Voice benchmark
- Syllable-based tokenization outperformed character-based tokenization for Kinyarwanda ASR
- Semi-supervised learning with 4 generations improved performance by leveraging unlabelled YouTube data
- Curriculum learning schedule with staged introduction of increasingly noisy data enhanced model robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Syllable-based tokenization outperforms character-based tokenization for Kinyarwanda ASR due to the language's morphological structure.
- Mechanism: Kinyarwanda's orthography consists only of open syllables (ending in vowels) and consonant clusters that align with elementary school teaching units. Syllable-based tokenization leverages this structure by treating consonant clusters as single tokens, reducing the vocabulary size and improving the model's ability to map speech to text.
- Core assumption: The orthographic units used in Kinyarwanda elementary education (vowels, simple consonants, and consonant clusters) are optimal for ASR tokenization.
- Evidence anchors:
  - [abstract]: "Our experiments also indicate that using syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda."
  - [section 3.4]: "In our experiments, we empirically compare the effectiveness of this syllable-based tokenization against the more common character-based tokenization."
  - [corpus]: Weak evidence - the corpus does not provide direct comparison data, only mentions the general finding.
- Break condition: If Kinyarwanda orthography were to change or if the elementary school teaching units were to differ significantly from the actual spoken language patterns, this mechanism might fail.

### Mechanism 2
- Claim: Curriculum-based fine-tuning improves ASR performance by gradually introducing harder examples.
- Mechanism: The model is first trained on clean, studio-quality data (JW.ORG dataset), then progressively introduced to noisier and more diverse examples from the Mozilla Common Voice dataset. This staged approach allows the model to learn robust representations before tackling more challenging data.
- Core assumption: Clean data provides a better foundation for learning than immediately exposing the model to noisy data.
- Evidence anchors:
  - [abstract]: "following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda."
  - [section 3.3]: "At stage 0, we train the model on JW.ORG-only training set which contains about 80 hours of data. At each subsequent stage, we double the amount of training examples by adding harder and harder examples from MCV training set."
  - [corpus]: Weak evidence - the corpus does not provide specific performance metrics for each curriculum stage.
- Break condition: If the clean data does not adequately represent the language's phonetic or phonological features, or if the noisier data is too diverse, this staged approach might not improve performance.

### Mechanism 3
- Claim: Semi-supervised learning (Semi-SL) leverages unlabelled YouTube data to improve ASR performance.
- Mechanism: The model transcribes unlabelled YouTube utterances, ranks the transcriptions using both CTC beam search scores and an external language model, and adds the top-ranked examples to the training set. This process is repeated across multiple generations, allowing the model to learn from a larger and more diverse dataset.
- Core assumption: The transcriptions generated by the initial model are sufficiently accurate to be useful for training.
- Evidence anchors:
  - [abstract]: "using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda."
  - [section 3.3]: "we experiment with semi-supervised learning (Semi-SL) by transcribing and filtering audio segments from our YouTube speech data, and then adding this new dataset to our original training dataset and resuming training."
  - [section 5.2]: "Our final model achieved 1.0% CER / 3.2% WER on JW.ORG test set and 4.8% CER / 15.9% WER on MCV test set."
- Break condition: If the initial model's transcriptions are too inaccurate, or if the unlabelled data is not representative of the language's usage, this mechanism might degrade performance.

## Foundational Learning

- Concept: Self-supervised pre-training (Self-PT) using contrastive objectives.
  - Why needed here: Kinyarwanda is a low-resource language with limited transcribed speech data. Self-PT allows the model to learn general speech representations from large amounts of unlabelled data before fine-tuning on the limited labelled data.
  - Quick check question: How does contrastive pre-training differ from supervised pre-training in terms of data requirements and learning objectives?

- Concept: Connectionist Temporal Classification (CTC) loss function.
  - Why needed here: CTC allows the model to learn a monotonic mapping between speech frames and text units without requiring explicit alignment between them. This is particularly useful for Kinyarwanda, which has a large vocabulary and complex morphology.
  - Quick check question: How does CTC handle the alignment problem between variable-length speech and text sequences?

- Concept: Conformer architecture.
  - Why needed here: Conformer combines the strengths of convolutional neural networks (CNNs) and transformers, allowing the model to capture both local and global dependencies in speech data. This is important for Kinyarwanda, which has complex phonological rules and prosody.
  - Quick check question: What are the key architectural differences between Conformer and traditional transformer models, and how do these differences benefit speech recognition tasks?

## Architecture Onboarding

- Component map: Log mel-spectrogram -> CNN sub-sampling -> Conformer layers -> Linear projection (pre-training) / CTC projection (fine-tuning) -> Predicted text sequence

- Critical path: Pre-training on YouTube data (contrastive learning) → Fine-tuning stage 1 (clean JW.ORG data) → Fine-tuning stages 2-5 (curriculum introduction of MCV data) → Semi-supervised learning (4 generations of transcribing/filtering YouTube data and adding to training)

- Design tradeoffs: Using syllable-based tokenization increases vocabulary size but reduces sequence length, potentially improving accuracy but increasing computational cost. Curriculum learning requires careful curation of training data and may not generalize well to new data distributions. Semi-supervised learning relies on the quality of initial model's transcriptions, which may introduce errors.

- Failure signatures: Overfitting to clean data if curriculum learning is not properly implemented. Degradation in performance if semi-supervised learning introduces too much noise. Inability to generalize to new speakers or accents if the training data is not diverse enough.

- First 3 experiments:
  1. Evaluate the impact of syllable-based vs. character-based tokenization on a small subset of the data.
  2. Compare the performance of the model with and without self-supervised pre-training on the clean JW.ORG dataset.
  3. Test the curriculum learning approach by training the model on increasingly noisy data and evaluating performance at each stage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific challenges in adapting the mobile application design for speech-text alignment to other low-resource languages with different orthographic systems?
- Basis in paper: [explicit] The paper describes the development of a mobile application for Kinyarwanda speech-text alignment but does not discuss its adaptability to other languages.
- Why unresolved: The paper focuses solely on Kinyarwanda and does not provide insights into how the application would perform with languages that have different orthographic or phonetic characteristics.
- What evidence would resolve it: Comparative studies testing the mobile application's effectiveness across multiple low-resource languages with varying orthographic systems.

### Open Question 2
- Question: How does the performance of syllable-based tokenization compare to character-based tokenization in languages other than Kinyarwanda?
- Basis in paper: [explicit] The paper demonstrates that syllable-based tokenization performs better for Kinyarwanda but does not explore its effectiveness in other languages.
- Why unresolved: The study is limited to Kinyarwanda, and the unique linguistic features of this language may not be representative of other languages.
- What evidence would resolve it: Empirical studies applying both tokenization methods to a diverse set of languages, particularly those with different syllable structures.

### Open Question 3
- Question: What are the long-term impacts of using semi-supervised learning on the robustness and generalization of ASR models for low-resource languages?
- Basis in paper: [explicit] The paper discusses the use of semi-supervised learning to improve ASR performance but does not address its long-term effects.
- Why unresolved: The study focuses on immediate performance improvements without considering how these methods affect model stability and adaptability over time.
- What evidence would resolve it: Longitudinal studies tracking model performance and robustness over extended periods and varying data conditions.

## Limitations

- The effectiveness of syllable-based tokenization is tightly coupled to Kinyarwanda's specific orthographic and phonological structure, limiting generalizability to other languages.
- The curriculum learning approach depends on careful curation and ordering of training data, which may not transfer well to different data distributions or languages.
- Semi-supervised learning introduces potential noise through transcription errors, but the paper does not provide detailed error analysis of these transcriptions.

## Confidence

**High Confidence:** The general finding that combining self-supervised pre-training, curriculum fine-tuning, and semi-supervised learning improves Kinyarwanda ASR performance is well-supported by the results (3.2% WER on JW.ORG, 15.9% WER on MCV). The comparative advantage of syllable-based tokenization over character-based tokenization is also clearly demonstrated.

**Medium Confidence:** The specific implementation details of the curriculum schedule and semi-supervised learning pipeline have medium confidence. While the overall approach is sound, the exact parameters (number of generations, filtering thresholds, MCV data ordering) are not fully specified, making exact reproduction challenging.

**Low Confidence:** The claim that syllable-based tokenization would necessarily benefit other low-resource languages has low confidence. The effectiveness appears tightly coupled to Kinyarwanda's specific orthographic and phonological structure, which may not transfer to languages with different morphological or orthographic systems.

## Next Checks

1. **Tokenization Generalization Test:** Implement the syllable-based tokenization approach for another Bantu language with similar orthographic structure (e.g., Kirundi or Kiswahili) and compare performance against character-based tokenization using the same model architecture and training procedure. This would test whether the tokenization advantage generalizes beyond Kinyarwanda.

2. **Curriculum Schedule Ablation:** Conduct an ablation study removing the curriculum learning component by training directly on the combined JW.ORG and MCV dataset without staged progression. Compare final performance to the curriculum approach to quantify the specific contribution of staged learning versus overall data volume.

3. **Semi-Supervised Learning Noise Analysis:** Perform detailed error analysis on the transcriptions generated during semi-supervised learning, measuring transcription accuracy at each generation and correlating this with performance improvements. This would reveal whether performance gains are due to better data diversity or simply more training examples, and establish optimal stopping criteria for the semi-supervised process.