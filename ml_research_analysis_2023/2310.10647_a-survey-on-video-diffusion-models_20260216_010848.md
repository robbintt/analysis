---
ver: rpa2
title: A Survey on Video Diffusion Models
arxiv_id: '2310.10647'
source_url: https://arxiv.org/abs/2310.10647
tags:
- video
- diffusion
- generation
- videos
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of video diffusion
  models, categorizing them into three main areas: video generation, video editing,
  and video understanding. It covers over 100 different works and provides detailed
  insights into the technical perspectives and research objectives of each category.'
---

# A Survey on Video Diffusion Models

## Quick Facts
- arXiv ID: 2310.10647
- Source URL: https://arxiv.org/abs/2310.10647
- Reference count: 40
- Key outcome: Comprehensive survey categorizing over 100 video diffusion models into generation, editing, and understanding areas

## Executive Summary
This survey provides a comprehensive overview of video diffusion models, covering more than 100 works across three main research areas: video generation, video editing, and video understanding. The paper introduces fundamental concepts of diffusion models and details the technical approaches and objectives within each category, including text-to-video generation, pose-guided video generation, and video anomaly detection. It also presents experimental results and comparisons across benchmark datasets while highlighting current challenges and future directions in the field.

## Method Summary
The survey systematically categorizes video diffusion models into three main areas: video generation (text-to-video, image-to-video, etc.), video editing (frame editing, object manipulation), and video understanding (classification, segmentation, anomaly detection). For each area, it analyzes the specific methods and techniques used, such as multi-stage training pipelines for high-resolution generation, training-free adaptation of pre-trained image models for editing, and specialized architectures for understanding tasks. The survey draws on experimental results from various benchmark datasets to evaluate model performance and identify key challenges.

## Key Results
- Video diffusion models are gradually superseding GANs and autoregressive Transformers in video generation due to better controllability and photorealism
- Multi-stage training pipelines enable high-resolution, temporally coherent video generation
- Training-free adaptation of pre-trained image diffusion models enables rapid zero-shot video editing
- Current evaluation metrics (FID, FVD, CLIPSIM) have limitations in fully capturing video generation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models supplant GANs and autoregressive Transformers in video generation due to their stronger controllability, photorealism, and diversity.
- Mechanism: The denoising process in diffusion models iteratively refines random noise into structured video content, allowing stable gradient-based training that avoids mode collapse and learns richer distributions.
- Core assumption: Large-scale paired video-text datasets exist to provide sufficient supervision for the diffusion training process.
- Evidence anchors: [abstract] "diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers" [section] "Current research on diffusion models is mostly based on three predominant formulations: denoising diffusion probabilistic models (DDPMs), score-based generative models (SGMs), and stochastic differential equations (Score SDEs)"

### Mechanism 2
- Claim: Multi-stage training pipelines (e.g., base generation → interpolation → super-resolution) enable high-resolution, temporally coherent video generation.
- Mechanism: Each stage addresses a specific bottleneck: the base model generates coarse, consistent frames; interpolation increases frame rate; super-resolution boosts spatial resolution, each stage conditioning on the previous output.
- Core assumption: The diffusion model's latent space preserves temporal relationships that can be progressively refined without introducing inconsistencies.
- Evidence anchors: [section] "Video LDM [35] trains a T2V network composed with three training stages, including key-frame T2V generation, video frame interpolation and spatial super-resolution modules." [section] "Similarly, LA VIE [138] employs a cascaded video diffusion model composed of three stages: a base T2V stage, a temporal interpolation stage, and a video super-resolution stage."

### Mechanism 3
- Claim: Training-free adaptation of pre-trained image diffusion models enables rapid zero-shot video editing by propagating frame-level edits through optical flow or cross-frame attention.
- Mechanism: Key frames are edited via the image model, then features are propagated to other frames using optical flow or attention mechanisms to enforce consistency across the sequence.
- Core assumption: The pre-trained image model's latent space is sufficiently rich and transferable to video content without catastrophic forgetting.
- Evidence anchors: [section] "TokenFlow [225] demonstrates that consistency in edited videos can be achieved by enforcing consistency in the diffusion feature space. Specifically, this is accomplished by sampling key frames, jointly editing them, and propagating the features from the key frames to all other frames based on the correspondences provided by the original video features." [section] "EVE [226] proposes two strategies to reinforce temporal consistency: Depth Map Guidance to locate spatial layouts and motion trajectories of moving objects as well as Frame-Align Attention which forces the model to place attention on both previous and current frames."

## Foundational Learning

- Concept: Forward and reverse Markov chains in DDPMs
  - Why needed here: Understanding how noise is progressively added and removed is essential to implement or modify diffusion models.
  - Quick check question: In the forward process, what distribution is used to perturb xt-1 into xt?

- Concept: Temporal attention and cross-frame attention in video diffusion
  - Why needed here: These mechanisms model motion and coherence across frames, which image models lack.
  - Quick check question: How does temporal attention differ from standard self-attention in terms of receptive field?

- Concept: Evaluation metrics (FID, FVD, CLIPSIM)
  - Why needed here: These metrics quantify fidelity, diversity, and text alignment in generated videos.
  - Quick check question: Why is FVD preferred over FID for video evaluation?

## Architecture Onboarding

- Component map: Text embedding → conditioning injection → temporal modeling → frame generation
- Critical path: Text embedding → conditioning injection → temporal modeling → frame generation
- Design tradeoffs:
  - Pixel-level vs. latent diffusion: speed vs. quality
  - Joint image-video pretraining vs. video-only: generalization vs. specialization
  - Dense vs. sparse temporal attention: coherence vs. compute cost
- Failure signatures:
  - Flickering frames: temporal inconsistency, likely due to weak motion modeling
  - Low text alignment: weak conditioning injection or poor text encoder
  - Artifacts: insufficient denoising steps or poor noise schedule
- First 3 experiments:
  1. Train a small 3D U-Net on synthetic moving MNIST to verify temporal attention integration.
  2. Implement classifier-free guidance and compare guidance scale effects on fidelity vs. diversity.
  3. Replace dense temporal attention with cross-frame attention only and measure coherence loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective strategies for improving the efficiency of video diffusion models, both in terms of training time and inference speed, while maintaining or enhancing generation quality?
- Basis in paper: [explicit] The paper discusses the heavy training costs of T2V models and mentions efforts like SimDA to mitigate training expenses, but acknowledges that both dataset magnitude and temporal complexity remain critical concerns.
- Why unresolved: While some methods have attempted to address efficiency, the paper highlights that significant improvements are still needed in both training and inference efficiency without compromising quality.
- What evidence would resolve it: Empirical studies comparing the training time, inference speed, and generation quality of different video diffusion models under various efficiency-focused strategies.

### Open Question 2
- Question: How can we develop more comprehensive and accurate evaluation benchmarks and metrics for video generation that go beyond current FVD and IS scores to capture aspects like temporal consistency, semantic alignment, and user preference?
- Basis in paper: [explicit] The paper discusses the limitations of current evaluation metrics and the reliance on user AB testing, highlighting the need for more tailored benchmarks that accurately reflect video generation quality.
- Why unresolved: Existing metrics primarily focus on distributional differences and don't fully capture the nuances of video generation quality, such as temporal consistency and semantic alignment.
- What evidence would resolve it: Development and validation of new evaluation metrics that better capture the key aspects of video generation quality, along with comparative studies demonstrating their effectiveness.

### Open Question 3
- Question: What are the key architectural innovations and training techniques needed to overcome the model incapacity issues observed in video editing, such as maintaining temporal consistency when replacing objects with significantly different attributes?
- Basis in paper: [explicit] The paper discusses limitations in video editing methods, including temporal consistency failures when replacing human figures with animals and difficulties in maintaining structural and temporal consistency when injecting extra objects.
- Why unresolved: Current methods struggle with complex editing tasks that require significant changes to video content while preserving temporal coherence and structural integrity.
- What evidence would resolve it: Successful demonstrations of video editing methods that can handle complex object replacements and content modifications while maintaining high temporal consistency and structural integrity.

## Limitations
- The survey may not capture the most recent advances in video diffusion models due to the rapid evolution of the field
- Evaluation of video generation models remains challenging due to lack of standardized benchmarks and high computational costs
- Categorization of methods into three main areas may oversimplify the increasingly overlapping nature of research directions

## Confidence
- **High confidence**: The fundamental mechanisms of diffusion models (noise schedule, denoising process, temporal attention) are well-established and consistently reported across multiple works.
- **Medium confidence**: Claims about diffusion models superseding GANs and Transformers are supported by current trends but may shift as new architectures emerge.
- **Medium confidence**: The effectiveness of multi-stage training pipelines is demonstrated across several works, though the optimal stage configurations remain task-dependent.

## Next Checks
1. Implement and compare at least three representative video diffusion models on standard datasets (UCF-101, HMDB51) to verify claimed performance improvements.
2. Systematically remove temporal attention mechanisms from a baseline model to quantify their contribution to video coherence.
3. Evaluate models trained on one domain (e.g., human action videos) on out-of-distribution content (e.g., aerial footage) to assess robustness.