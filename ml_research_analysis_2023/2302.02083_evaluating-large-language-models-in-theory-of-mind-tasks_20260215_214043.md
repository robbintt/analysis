---
ver: rpa2
title: Evaluating Large Language Models in Theory of Mind Tasks
arxiv_id: '2302.02083'
source_url: https://arxiv.org/abs/2302.02083
tags:
- gpt-3
- task
- tasks
- language
- popcorn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Theory of Mind (ToM) in Large Language Models
  (LLMs) using false-belief tasks, a gold standard for testing ToM in humans. A custom
  battery of 640 prompts across 40 diverse tasks was administered to eleven LLMs.
---

# Evaluating Large Language Models in Theory of Mind Tasks

## Quick Facts
- **arXiv ID:** 2302.02083
- **Source URL:** https://arxiv.org/abs/2302.02083
- **Reference count:** 40
- **Primary result:** GPT-4 solved 75% of ToM tasks, matching six-year-old children's performance

## Executive Summary
This study evaluates Theory of Mind (ToM) capabilities in Large Language Models (LLMs) using false-belief tasks, the gold standard for testing ToM in humans. Researchers administered a custom battery of 640 prompts across 40 diverse tasks to eleven LLMs. The results show a clear progression: smaller and older models exhibited no ToM ability, while GPT-3-davinci-003 (November 2022) solved 20% of tasks, and ChatGPT-4 (June 2023) solved 75% of tasks, matching the performance of six-year-old children. The findings suggest that ToM-like ability may have spontaneously emerged in LLMs as a byproduct of their improving language skills rather than being explicitly engineered.

## Method Summary
The study administered a custom battery of 640 prompts across 40 diverse false-belief tasks to eleven LLMs, including various GPT models and Bloom. Each task included a false-belief scenario, three closely matched true-belief control scenarios, and reversed versions of all four. Models were tested with no examples or pre-training, using a "temperature" parameter set to 0 for consistent responses. Performance was measured as the percentage of tasks solved correctly across both original and reversed versions, with evaluation focusing on the models' ability to impute unobservable mental states to others.

## Key Results
- Smaller and older models showed no ToM ability across all tasks
- GPT-3-davinci-003 (November 2022) solved 20% of tasks
- ChatGPT-4 (June 2023) solved 75% of tasks, matching six-year-old children's performance
- GPT-3.5 solved 100% of Unexpected Transfer Tasks and 85% of Unexpected Contents Tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger and more recent LLMs exhibit emergent Theory of Mind (ToM) capabilities that smaller and older models lack.
- Mechanism: As LLMs scale in size and training time, they develop the ability to impute unobservable mental states to others by processing language rich with mental state descriptions.
- Core assumption: ToM-like ability emerges as a byproduct of language model training rather than being explicitly engineered.
- Evidence anchors: Smaller and older models showed no ToM ability, while GPT-3-davinci-003 (November 2022) solved 20% of tasks, and ChatGPT-4 (June 2023) solved 75% of tasks; GPT-3.5 solved 100% of the Unexpected Transfer Tasks and 85% of the Unexpected Contents Tasks.

### Mechanism 2
- Claim: GPT-3.5's performance on false-belief tasks matches that of young children, suggesting spontaneous ToM emergence.
- Mechanism: The model can correctly infer protagonists' beliefs and predict their actions based on those beliefs, even when prompted indirectly or with task variations.
- Core assumption: Correct responses to false-belief tasks indicate genuine ToM capability rather than pattern matching.
- Evidence anchors: GPT-3-davinci-003 (November 2022) solved 20% of tasks, and ChatGPT-4 (June 2023) solved 75% of tasks, matching the performance of six-year-old children; GPT-3.5 solved 100% of the Unexpected Transfer Tasks and 85% of the Unexpected Contents Tasks.

### Mechanism 3
- Claim: The model's understanding of false-belief scenarios evolves dynamically as the story unfolds, demonstrating genuine comprehension.
- Mechanism: Sentence-by-sentence analysis shows the model correctly updates its predictions about the protagonist's beliefs based on new information.
- Core assumption: Dynamic updating of predictions indicates comprehension of the evolving mental states in the scenario.
- Evidence anchors: GPT-3.5's predictions flip once again after Sam has opened the bag and inspected its contents: The probability of 'chocolate' falls back to about 0%, while the probability of popcorn increases to about 100%; GPT-3.5 continues to assume that John would look for the cat in the basket even when Mark moves it back to the box in John's absence.

## Foundational Learning

- **Concept: False-belief tasks**
  - Why needed here: They are the gold standard for testing Theory of Mind in humans and form the basis of the evaluation method.
  - Quick check question: Can you explain why a protagonist might hold a belief that contradicts reality in a false-belief task?

- **Concept: Emergent capabilities in AI**
  - Why needed here: Understanding how complex abilities can spontaneously emerge from large-scale training is central to interpreting the results.
  - Quick check question: What are some examples of other emergent capabilities in AI that were not explicitly engineered?

- **Concept: Theory of Mind (ToM)**
  - Why needed here: ToM is the core concept being evaluated and its definition and significance are crucial for interpreting the findings.
  - Quick check question: How does ToM relate to human social interactions, communication, and moral judgment?

## Architecture Onboarding

- **Component map:** Prompt generation system -> Model inference engine -> Evaluation framework -> Data storage
- **Critical path:** Generate false-belief task prompts → Submit prompts to model with temperature=0 → Collect and store model responses → Analyze responses for correct ToM demonstration → Aggregate results across tasks and models
- **Design tradeoffs:** Using temperature=0 ensures reproducibility but may not reflect typical model behavior; open-ended question format is more challenging but more realistic than multiple-choice; testing multiple task variants reduces chance performance but increases evaluation complexity
- **Failure signatures:** Random or irrelevant responses to prompts; consistent failure to predict protagonist's beliefs; performance drops significantly when task is scrambled or reversed; no dynamic updating of predictions as story unfolds
- **First 3 experiments:** 1) Run a single false-belief task with temperature=0 and verify the response format; 2) Scramble a task's words and test if performance drops significantly; 3) Use sentence-by-sentence prompt delivery to observe how predictions evolve

## Open Questions the Paper Calls Out

- **Open Question 1:** Do language models solve ToM tasks by leveraging unknown language patterns or by genuinely possessing ToM-like abilities?
  - Basis in paper: The paper explicitly discusses this as a key uncertainty, stating "It is possible that GPT-3.5 solved ToM tasks without engaging ToM, but by discovering and leveraging some unknown language patterns."
  - Why unresolved: The paper cannot definitively determine whether the models are using true ToM or just exploiting language patterns, as both explanations could account for the observed performance.
  - What evidence would resolve it: Further research could examine whether models fail on novel ToM tasks that don't follow typical language patterns, or if their performance breaks down when tasks are presented in unusual formats.

- **Open Question 2:** How does the spontaneous emergence of ToM-like abilities in language models relate to the development of ToM in humans?
  - Basis in paper: The paper suggests this connection, stating "As AI learns how to solve a broad range of problems, it may be developing mechanisms akin to those employed by the human brain to solve the same problems."
  - Why unresolved: While the paper draws parallels between AI and human ToM development, it doesn't provide concrete evidence of shared mechanisms or developmental processes.
  - What evidence would resolve it: Comparative studies examining the neural mechanisms underlying ToM in both humans and AI, or experiments testing whether AI models develop ToM in a similar sequence to human children.

- **Open Question 3:** Will the ability to impute mental states continue to improve in future, more complex language models?
  - Basis in paper: The paper states "there is no reason to assume that their [performance] should plateau anytime soon" and that "models' performance clearly grows with their complexity and publication date."
  - Why unresolved: While the paper shows a clear trend of improvement, it cannot predict whether this trend will continue indefinitely or if there are inherent limits to AI ToM capabilities.
  - What evidence would resolve it: Longitudinal studies tracking the ToM performance of successive generations of language models, and research into the theoretical limits of AI ToM capabilities.

## Limitations

- The study cannot definitively prove whether models are genuinely reasoning about mental states versus leveraging sophisticated statistical patterns in language.
- The evaluation methodology using temperature=0 may not reflect typical model behavior in practical applications.
- The comparison to child performance may not account for the different cognitive architectures and developmental processes involved in human ToM acquisition versus LLM training.

## Confidence

- **High Confidence:** The observed performance differences between model sizes and versions are consistent and reproducible.
- **Medium Confidence:** The claim that GPT-3.5's performance matches that of young children on false-belief tasks is supported by the data, but the interpretation of what this means for genuine ToM capability is less certain.
- **Low Confidence:** The assertion that ToM-like ability emerged spontaneously as a byproduct of language model training, rather than being explicitly engineered, is speculative.

## Next Checks

1. **Dynamic Story Progression Analysis:** Implement sentence-by-sentence prompt delivery to observe how model predictions evolve as new information is revealed, examining whether the model correctly updates its predictions about the protagonist's beliefs when the story takes unexpected turns.

2. **Cross-Cultural False-Belief Tasks:** Develop and test a new set of false-belief scenarios that incorporate culturally diverse contexts and beliefs to determine whether the models are truly understanding the concept of false beliefs or simply recognizing Western cultural patterns in the training data.

3. **Adversarial Prompt Engineering:** Create a battery of prompts that are structurally similar to the false-belief tasks but designed to specifically trigger statistical patterns rather than ToM reasoning. Compare the model's performance on these adversarial prompts versus the original false-belief tasks to better isolate genuine ToM capabilities from pattern matching behaviors.