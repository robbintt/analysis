---
ver: rpa2
title: Measuring the Robustness of NLP Models to Domain Shifts
arxiv_id: '2306.00168'
source_url: https://arxiv.org/abs/2306.00168
tags:
- uni00000003
- uni00000037
- uni00000011
- uni00000013
- uni00000055
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Domain Robustness (DR) challenge in NLP,
  where model performance degrades when applied to new domains. The authors introduce
  a new DR benchmark covering 7 diverse NLP tasks (classification, QA, generation)
  across multiple domains.
---

# Measuring the Robustness of NLP Models to Domain Shifts

## Quick Facts
- arXiv ID: 2306.00168
- Source URL: https://arxiv.org/abs/2306.00168
- Reference count: 40
- One-line primary result: Both fine-tuned and few-shot learning models suffer from domain robustness challenges, though few-shot models are more robust; Target Drop is a better predictor of performance degradation than Source Drop.

## Executive Summary
This paper addresses the Domain Robustness (DR) challenge in NLP, where model performance degrades when applied to new domains. The authors introduce a new DR benchmark covering 7 diverse NLP tasks (classification, QA, generation) across multiple domains. They propose two metrics for characterizing DR: Source Drop (SD), measuring performance degradation relative to source in-domain performance, and Target Drop (TD), measuring degradation relative to target in-domain performance. Through extensive experiments with 14,000+ domain shifts across fine-tuned and few-shot learning models, they find that both model types suffer from DR, though few-shot models are more robust. They demonstrate that SD and TD capture different aspects of the DR challenge, with TD being a better predictor of average performance degradation.

## Method Summary
The study constructs a Domain Robustness Benchmark with 7 NLP tasks across 4-6 domains each, using curated datasets from various sources. For fine-tuned models, they experiment with 21 models of varying sizes from BERT, RoBERTa, DeBERTa, T5, and BART families, using AdamW optimizer with hyperparameter tuning on source domain validation sets. For few-shot learning, they test LLMs including OPT, GPT-JT, GPT-3 variants, and GPT-4 with demonstrations selected by similarity using SBERT embeddings. They evaluate both Source Drop (SD) and Target Drop (TD) metrics using F1 score and BertScore as evaluation metrics across all domain shifts.

## Key Results
- Both fine-tuned and few-shot learning models suffer from domain robustness challenges, though the impact is weaker in few-shot learning
- Target Drop (TD) is a better predictor of cross-domain performance than Source Drop (SD) due to smaller variance and stronger correlation with target in-domain performance
- Increasing model size improves in-domain and cross-domain performance for classification tasks but only reduces the performance gap (improves domain robustness) for classification and QA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Target Drop (TD) is a better predictor of cross-domain performance than Source Drop (SD) because it captures degradation relative to what could have been achieved with target-domain training data.
- Mechanism: The study shows that the variance of TD is consistently smaller than the variance of SD across tasks, and the correlation between ST (cross-domain performance) and TT (target in-domain performance) is much stronger than the correlation with SS (source in-domain performance). This means TT better approximates ST, making TD a more reliable indicator of average performance degradation.
- Core assumption: The target in-domain performance (TT) is a more accurate reference point for measuring degradation than the source in-domain performance (SS) because it reflects the achievable performance in the target domain.
- Evidence anchors:
  - [abstract] "We found that in significant proportions of domain shifts, either SD or TD is positive, but not both, emphasizing the importance of considering both measures as diagnostic tools."
  - [section] "The correlation between ST and TT is much stronger than the correlation with SS, and for most fine-tuning tasks, it is above 0.8. This result can explain why the variance and the Worst drops of the SD are larger than the ones of the TD."
- Break condition: If the correlation between ST and SS becomes stronger than the correlation between ST and TT, or if the variance of SD becomes smaller than the variance of TD, the mechanism would break.

### Mechanism 2
- Claim: Few-shot learning models are more robust to domain shifts than fine-tuned models because they lack strong anchoring to the source distribution.
- Mechanism: The study finds that while both model types suffer from domain robustness challenges, the impact is weaker in few-shot learning. This is because few-shot models rely on a small number of demonstrations rather than extensive training on source domain data, making them less anchored to the source distribution and more adaptable to new domains.
- Core assumption: The absence of extensive training data that anchors the model to the source distribution makes few-shot models more flexible and adaptable to domain shifts.
- Evidence anchors:
  - [abstract] "Our experimental results demonstrate the persistent existence of the DR challenge in both fine-tuning and few-shot learning models, though it is less pronounced in the latter."
  - [section] "Notice that the four scenarios are prevalent in all of the few-shot learning tasks, and in that case, we may say that the effect of the domain shift is weak."
- Break condition: If few-shot models were found to perform worse than fine-tuned models on domain shifts, or if the performance gap between few-shot and fine-tuned models on domain shifts increased significantly, the mechanism would break.

### Mechanism 3
- Claim: Increasing model size improves in-domain and cross-domain performance, but only for classification tasks does it reduce the performance gap (improve domain robustness).
- Mechanism: The study shows that for classification and QA tasks, larger models not only improve average in-domain and cross-domain performance but also reduce the Average Drop and Average WorstTD. However, for generation tasks, the trend is less clear and varies between tasks.
- Core assumption: Larger models have more capacity to learn robust representations that generalize better across domains, but this effect is more pronounced for tasks that are less complex or have clearer boundaries between classes.
- Evidence anchors:
  - [section] "Regarding the performance gap (i.e., the Average Drop), the general trend for classification and QA tasks is that larger models improve the Average Drop and the Average WorstTD and TD."
  - [section] "For all tasks and models, the average in-domain performance is higher than the average cross-domain performance, except for the QA and QG tasks, where the difference between the two is minor."
- Break condition: If increasing model size was found to increase the performance gap for any task, or if smaller models were found to outperform larger models on domain robustness for any task, the mechanism would break.

## Foundational Learning

- Concept: Domain Robustness (DR)
  - Why needed here: The entire study is focused on understanding and measuring DR, which is the challenge of maintaining performance when models are applied to new domains without labeled data.
  - Quick check question: What is the difference between Source Drop (SD) and Target Drop (TD) in measuring DR?

- Concept: Domain Shift
  - Why needed here: The study investigates how performance changes when the distribution of the data shifts from the source domain to the target domain, which is the core of the DR challenge.
  - Quick check question: What are the three types of domain shifts mentioned in the paper (covariate shift, prior shift, concept shift)?

- Concept: Fine-tuning vs Few-shot Learning
  - Why needed here: The study compares the DR of fine-tuned models (trained extensively on source domain data) and few-shot learning models (relying on a small number of demonstrations), which have different levels of anchoring to the source distribution.
  - Quick check question: How does the level of anchoring to the source distribution differ between fine-tuned and few-shot learning models, and how does this affect their DR?

## Architecture Onboarding

- Component map: DR benchmark (7 tasks × 4-6 domains) → SD and TD metrics → Fine-tuned models (21 models) and Few-shot LLMs (GPT-3, GPT-4) → Experimental evaluation → Analysis of DR patterns
- Critical path: 1) Construct DR benchmark with diverse NLP tasks and domains, 2) Define SD and TD metrics, 3) Conduct extensive experiments with fine-tuned and few-shot learning models, 4) Analyze results to understand DR challenge and compare model types.
- Design tradeoffs: The choice of tasks and domains in the benchmark affects the generalizability of the findings. Using natural topic shifts simplifies analysis but may not capture all types of domain shifts. The decision to measure both SD and TD adds complexity but provides a more complete picture of DR.
- Failure signatures: If the variance of SD is consistently smaller than the variance of TD, or if the correlation between ST and SS is consistently stronger than the correlation between ST and TT, it would indicate a failure in the mechanism. If few-shot models consistently outperform fine-tuned models on domain robustness, it would challenge the assumption that fine-tuning is still the de-facto standard.
- First 3 experiments:
  1. Measure SD and TD for a simple classification task (e.g., sentiment analysis) with two domains to understand the basic mechanics of the metrics.
  2. Compare the DR of a small fine-tuned model and a few-shot model on the same task to observe the difference in robustness.
  3. Test the effect of increasing model size on the DR of a classification task to validate the relationship between model size and domain robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Target Drop (TD) compare to the Source Drop (SD) in predicting cross-domain performance degradation across different NLP tasks?
- Basis in paper: [explicit] The authors propose TD as a complementary metric to SD, finding that TD often has lower variance and better approximates the Average Drop.
- Why unresolved: The paper provides initial evidence but doesn't conduct a systematic comparative analysis of SD and TD's predictive power across all tasks and models.
- What evidence would resolve it: A comprehensive study measuring and comparing the correlation between SD, TD, and actual cross-domain performance degradation across all 7 tasks and both fine-tuned and few-shot models.

### Open Question 2
- Question: What is the optimal number of demonstrations for few-shot learning to minimize the performance gap between in-domain and cross-domain tasks?
- Basis in paper: [explicit] The paper shows that increasing demonstrations improves performance but doesn't necessarily minimize the gap, with the NLI task showing decreased performance with more demonstrations.
- Why unresolved: The analysis only goes up to 4 demonstrations due to token limitations, and the relationship between demonstration count and performance gap varies by task.
- What evidence would resolve it: Experiments testing a wider range of demonstration counts (up to the token limit) across all tasks to identify optimal demonstration numbers for minimizing the performance gap.

### Open Question 3
- Question: How does model size affect the Domain Robustness challenge differently for fine-tuned versus few-shot learning models?
- Basis in paper: [explicit] The paper finds that larger models improve both in-domain and cross-domain performance for fine-tuning, but the effect on the performance gap is less clear for few-shot models.
- Why unresolved: The analysis doesn't systematically compare the effect of model size on the DR challenge across all tasks and both learning paradigms.
- What evidence would resolve it: A controlled experiment varying model size while holding other factors constant, measuring both SD and TD for each model size across all tasks and learning paradigms.

## Limitations
- The benchmark focuses on English-language tasks, limiting generalizability to other languages and cultural contexts
- The study uses natural topic shifts which may not capture all types of domain shifts (covariate, prior, or concept shifts)
- The selection of tasks and domains may not fully represent all real-world domain shift scenarios

## Confidence

- High confidence: Domain robustness remains a significant challenge for both fine-tuned and few-shot learning models, supported by extensive experimental results across 14,000+ domain shifts
- Medium confidence: Target Drop (TD) is a better predictor of cross-domain performance than Source Drop (SD), as this relies on correlation patterns that may vary with different task selections
- Medium confidence: Few-shot learning models are more robust to domain shifts, as this is based on comparative performance but the effect size may vary depending on the specific tasks and domains tested

## Next Checks

1. Test the proposed metrics (SD and TD) on a different set of tasks and domains, particularly focusing on non-topic-based domain shifts (e.g., style shifts, temporal shifts) to verify the generalizability of the findings.

2. Conduct ablation studies on the few-shot learning mechanism by varying the number of demonstrations and their selection criteria to better understand the factors contributing to improved domain robustness.

3. Evaluate the proposed metrics on multilingual datasets to assess whether the observed patterns hold across different languages and cultural contexts, addressing the current limitation to English-language tasks.