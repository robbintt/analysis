---
ver: rpa2
title: Safety Aware Autonomous Path Planning Using Model Predictive Reinforcement
  Learning for Inland Waterways
arxiv_id: '2311.09878'
source_url: https://arxiv.org/abs/2311.09878
tags:
- path
- planning
- ship
- frenet
- frame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel path planning approach for autonomous
  shipping in inland waterways using Model Predictive Reinforcement Learning (MPRL).
  The approach calculates a series of waypoints for vessels to follow using an occupancy
  grid map representation of the environment.
---

# Safety Aware Autonomous Path Planning Using Model Predictive Reinforcement Learning for Inland Waterways

## Quick Facts
- arXiv ID: 2311.09878
- Source URL: https://arxiv.org/abs/2311.09878
- Reference count: 26
- Key outcome: MPRL outperforms PPO and Frenet frame baselines, safely navigating to goal in both test scenarios while baselines fail

## Executive Summary
This paper introduces a novel path planning approach for autonomous shipping in inland waterways using Model Predictive Reinforcement Learning (MPRL). The method calculates waypoints for vessels using an occupancy grid map representation and employs a trained PPO agent to simulate trajectories and select the optimal one based on expected return. The approach is evaluated against Frenet frame navigation and pure PPO-based navigation on two test scenarios, demonstrating superior performance and safety by maintaining greater distances from obstacles.

## Method Summary
The method combines Model Predictive Control with Reinforcement Learning, specifically using a trained PPO agent to simulate multiple candidate trajectories and select the one with the highest expected return. The environment is represented as an occupancy grid map, and the MPRL system generates waypoints by simulating trajectories with varied action parameters, evaluating their returns using n-step bootstrapping, and selecting the optimal trajectory. Failure detection is implemented by checking for collisions with the first waypoint, triggering human intervention if necessary.

## Key Results
- MPRL safely navigates to goal in both test scenarios while PPO-based approach fails in both and Frenet frame fails in corner scenario
- MPRL trajectories maintain greater distances from obstacles compared to Frenet frame navigation
- CDF analysis shows MPRL stays further from obstacles (only 8% of trajectory within 4m of obstacles vs higher percentages for baselines)

## Why This Works (Mechanism)

### Mechanism 1
MPRL outperforms baseline methods by using PPO agent to simulate multiple candidate trajectories and selecting the one with highest expected return, rather than relying on a single policy output. For each planning step, MPRL generates multiple candidate trajectories by varying the action and change-in-action parameters. Each trajectory is simulated using the PPO agent's learned policy, and the expected return is computed using n-step bootstrapping. The trajectory with highest return is selected and converted to waypoints. This assumes the PPO agent has been adequately trained to predict reward outcomes for different trajectory choices.

### Mechanism 2
MPRL's failure detection works by checking if the first waypoint in the selected trajectory causes a collision, enabling safe handover to human operator. After selecting the best trajectory, MPRL checks if the transition to the first waypoint results in a collision. If so, it triggers a failure mode and requests human intervention. This assumes collision detection in the simulation environment accurately reflects real-world collision risk.

### Mechanism 3
MPRL generates safer trajectories than Frenet frame by maintaining greater distance from obstacles, as shown by cumulative distribution function analysis. MPRL's reward function penalizes trajectories that come too close to obstacles, and the trajectory selection process favors paths with higher clearance. This is quantitatively demonstrated by the CDF showing MPRL trajectories are farther from obstacles than Frenet frame trajectories. This assumes the reward function adequately captures safety objectives and the PPO agent learns to optimize for them.

## Foundational Learning

- **Reinforcement Learning with Proximal Policy Optimization (PPO)**: PPO is the underlying algorithm used to train the agent that simulates trajectories in MPRL and provides the baseline comparison. *Quick check: What is the key innovation of PPO compared to vanilla policy gradient methods, and why is it important for stable training?*

- **Model Predictive Control (MPC)**: MPRL extends MPC concepts by using RL to evaluate candidate trajectories rather than a manually engineered cost function. *Quick check: How does MPRL's use of RL for trajectory evaluation differ from traditional MPC's use of a cost function?*

- **Occupancy Grid Maps**: The environment representation using occupancy grid maps allows MPRL to handle arbitrary obstacle shapes and waterway configurations. *Quick check: What are the advantages and disadvantages of using occupancy grid maps versus other environment representations like point clouds or polygon meshes?*

## Architecture Onboarding

- **Component map**: User provides goal → Global planner generates waypoints → Local planner (MPRL/PPO/Frenet) generates local trajectory → Ship control follows trajectory → Sensor inputs and map fusion are simulated in experiments
- **Critical path**: Global planning → Local planning → Ship control → Collision detection → Failure detection → Human intervention (if needed)
- **Design tradeoffs**: MPRL trades computational complexity (simulating multiple trajectories) for improved safety and adaptability compared to baselines. Waypoint-based control enables human monitoring but requires additional path following algorithms
- **Failure signatures**: PPO baseline fails to reach goal in both scenarios. Frenet frame fails in corner scenario. MPRL failure mode triggers when first waypoint causes collision
- **First 3 experiments**:
  1. Replicate scenario 1 (straight path with obstacles) to verify MPRL safety advantage over Frenet frame
  2. Replicate scenario 2 (corner with obstacles) to verify MPRL can handle complex scenarios where baselines fail
  3. Test MPRL in a new scenario with narrow passages to evaluate failure detection and handover mechanism

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations
- Simulation-only evaluation may not capture real-world complexities like sensor noise, weather effects, and dynamic obstacles
- Limited baseline comparisons (only PPO and Frenet frame) potentially missing other relevant approaches
- Failure detection mechanism relies on single collision check for first waypoint, which may not capture all failure modes in complex scenarios

## Confidence
- **High**: The paper clearly explains the MPRL mechanism and provides quantitative comparison through CDF analysis
- **Medium**: Experimental results support claims within tested scenarios, but lack of hyperparameter details and limited baseline comparisons reduce generalizability
- **Medium**: Safety advantage demonstrated is promising but needs validation through real-world testing

## Next Checks
1. **Real-world testing**: Implement MPRL on an actual autonomous vessel in controlled waterway conditions to validate simulation results and assess real-world performance under varying environmental conditions

2. **Comparative analysis with additional baselines**: Compare MPRL against other relevant path planning methods such as sampling-based planners (RRT*, PRM) and learning-based approaches (DRL without MPC) to establish its relative performance across different metrics

3. **Robustness evaluation**: Test MPRL under perturbed conditions including sensor noise, dynamic obstacles, and varying ship dynamics to assess its adaptability and failure detection mechanisms in challenging scenarios