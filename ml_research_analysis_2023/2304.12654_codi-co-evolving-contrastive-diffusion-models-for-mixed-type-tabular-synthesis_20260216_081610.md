---
ver: rpa2
title: 'CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis'
arxiv_id: '2304.12654'
source_url: https://arxiv.org/abs/2304.12654
tags:
- data
- diffusion
- discrete
- continuous
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoDi, a co-evolving contrastive diffusion
  model for synthesizing mixed-type tabular data. The method addresses the challenge
  of modeling discrete variables in tabular data by using two separate diffusion models
  for continuous and discrete variables, which are conditioned on each other during
  training.
---

# CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis

## Quick Facts
- arXiv ID: 2304.12654
- Source URL: https://arxiv.org/abs/2304.12654
- Reference count: 16
- Primary result: CoDi outperforms 8 baselines on 11 datasets with F1 scores up to 0.95 and coverage scores up to 0.93

## Executive Summary
This paper introduces CoDi, a novel approach for synthesizing mixed-type tabular data using co-evolving diffusion models with contrastive learning. The method addresses the challenge of modeling discrete variables in tabular data by using two separate diffusion models for continuous and discrete variables, which are conditioned on each other during training. To further improve the connection between the two models, CoDi employs a contrastive learning approach with a novel negative sampling method that focuses on permuting continuous and discrete variable pairs.

## Method Summary
CoDi synthesizes mixed-type tabular data by training two separate diffusion models—one for continuous variables and one for discrete variables—that condition on each other's outputs during both training and sampling. The continuous diffusion model processes variables in their native continuous space, while the discrete diffusion model handles discrete variables directly in discrete space, avoiding the information loss from one-hot encoding. The models are trained jointly with a contrastive learning objective that uses negative sampling by permuting variable pairs to strengthen the correlation modeling between continuous and discrete variables.

## Key Results
- CoDi achieves F1 scores up to 0.95 and coverage scores up to 0.93 on 11 real-world datasets
- Sampling time is faster than previous state-of-the-art methods
- Outperforms 8 baseline methods across multiple evaluation metrics including F1, AUROC, R2, and RMSE
- The contrastive learning component with negative sampling improves correlation preservation between variable types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separate diffusion models for continuous and discrete variables outperform joint modeling in mixed-type tabular synthesis.
- Mechanism: Continuous and discrete variables are processed in their respective natural spaces—continuous values remain continuous and discrete values remain discrete—avoiding suboptimal approximations like one-hot encoding followed by continuous sampling.
- Core assumption: The correlation structure between continuous and discrete variables can be preserved when each is modeled in its native space and later conditioned on the other.
- Evidence anchors:
  - [abstract] "However, there still exists a difficulty in modeling discrete variables (columns) of tabular data."
  - [section] "We claim that discrete variables should be treated in discrete spaces, unlike the existing methods that process them in continuous spaces along with continuous variables."
  - [corpus] Weak; no direct empirical comparison found in corpus neighbors, but aligns with motivation in "Continuous Diffusion for Mixed-Type Tabular Data".
- Break condition: If the conditional dependency between continuous and discrete variables is too complex to be captured by the conditioning scheme, performance may degrade.

### Mechanism 2
- Claim: Co-evolving conditional diffusion models improve data fidelity by allowing each model to condition on the other's output during training and sampling.
- Mechanism: At each time step, the continuous diffusion model conditions on the current discrete sample and vice versa; this mutual conditioning enforces alignment of the two variable types in the learned joint distribution.
- Core assumption: The forward and reverse Markov transitions of the two diffusion models can be synchronized so that conditioning on the other variable type does not break the diffusion dynamics.
- Evidence anchors:
  - [section] "To synthesize samples from the space to which each type belongs, we train two diffusion models for the two variable types separately — however, the two diffusion models read conditions from each other since their diffusion/denoising processes are intercorrelated."
  - [section] Proposition 3.1 formalizes the forward and reverse processes with cross-conditioning.
  - [corpus] Weak; corpus neighbors discuss heterogeneous imputation but not co-evolutionary conditioning.
- Break condition: If the conditioning introduces unstable gradients or numerical instability in the denoising network, training may fail.

### Mechanism 3
- Claim: Contrastive learning with negative sampling that permutes continuous-discrete pairs improves the binding between the two diffusion models.
- Mechanism: For each anchor continuous sample, a positive sample is generated conditioned on its correct discrete counterpart; a negative sample is generated conditioned on a mismatched discrete counterpart; contrastive loss encourages the model to distinguish correct from incorrect pairings.
- Core assumption: The mismatch between continuous and discrete parts in negative samples is sufficient to guide the model toward preserving correct inter-variable correlations.
- Evidence anchors:
  - [section] "For the negative sample, we generateˆxC− 0 with negative condition xD− 0, which is an inappropriate counterpart for xC 0."
  - [section] "We randomly shuffle the rows of discrete variables altogether, maintaining the pair...Method 3 retains the latter, which can result in the outcome."
  - [corpus] Weak; no direct contrastive learning approach for mixed-type tabular data found in corpus neighbors.
- Break condition: If the negative sampling does not create sufficiently distinct negatives, the contrastive loss may provide negligible signal.

## Foundational Learning

- Concept: Diffusion probabilistic models
  - Why needed here: The paper builds two diffusion models, one for continuous and one for discrete variables; understanding the forward/reverse Markov processes is essential.
  - Quick check question: What is the role of the variance schedule βt in the continuous diffusion process?

- Concept: Conditional generation in diffusion models
  - Why needed here: The co-evolving models require conditioning one on the output of the other; this is a generalization of standard conditional diffusion.
  - Quick check question: How does conditioning on a discrete variable differ from conditioning on a continuous variable in a diffusion model?

- Concept: Contrastive learning with triplet loss
  - Why needed here: The negative sampling method relies on contrastive objectives to strengthen the binding between continuous and discrete models.
  - Quick check question: What is the effect of the margin m in the triplet loss on the separation of positive and negative pairs?

## Architecture Onboarding

- Component map: Continuous diffusion model -> Discrete diffusion model -> Contrastive learning module with negative sampling
- Critical path: (1) Sample t, (2) Forward diffuse continuous and discrete variables separately, (3) Condition each on the other's diffused state, (4) Compute diffusion losses, (5) Generate positive/negative samples for contrastive learning, (6) Update both models jointly
- Design tradeoffs: Separate models reduce input dimensionality and parameter count but add complexity of synchronization; contrastive learning adds training time but improves correlation modeling
- Failure signatures: (1) Poor alignment between continuous and discrete outputs, (2) Unstable training due to conditioning loops, (3) Ineffective contrastive learning if negatives are not distinct
- First 3 experiments:
  1. Train only the continuous diffusion model on mixed-type data (discrete variables one-hot encoded) and compare to CoDi.
  2. Remove contrastive learning and evaluate if co-evolutionary conditioning alone suffices.
  3. Vary the negative sampling method (e.g., Method 1 vs Method 3) and measure impact on sampling quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed negative sampling method perform compared to alternative negative sampling strategies in tabular data synthesis?
- Basis in paper: [explicit] The paper mentions comparing three different negative sampling methods in Section 4.4, showing that Method 3 (randomly shuffling all discrete variables while maintaining their internal pairs) performs best.
- Why unresolved: While the paper compares these three methods, it doesn't explore other potential negative sampling strategies that might be more effective for tabular data.
- What evidence would resolve it: Comparative experiments using other negative sampling strategies (e.g., based on domain knowledge, learned embeddings, or different perturbation schemes) would help determine if the proposed method is optimal.

### Open Question 2
- Question: How sensitive is CoDi's performance to the choice of hyperparameters, particularly the learning rate, embedding dimensions, and contrastive learning coefficients?
- Basis in paper: [explicit] The paper mentions that hyperparameters were tuned for each dataset and diffusion model (continuous and discrete) separately in Appendix D.2, but doesn't provide a systematic sensitivity analysis.
- Why unresolved: The paper doesn't explore how variations in these hyperparameters affect the model's performance or whether there are regions of hyperparameter space that yield similar or better results.
- What evidence would resolve it: Comprehensive ablation studies and sensitivity analyses varying these hyperparameters across multiple orders of magnitude would reveal their impact on performance.

### Open Question 3
- Question: Can the co-evolving conditional diffusion model framework be extended to handle other types of mixed data beyond continuous and discrete variables, such as ordinal or categorical variables with hierarchical relationships?
- Basis in paper: [inferred] The paper focuses on continuous and discrete variables, but the framework of having two diffusion models conditioned on each other could potentially be extended to other data types.
- Why unresolved: The paper doesn't explore whether the co-evolving conditional framework would be effective for other data types or how it would need to be modified to handle them.
- What evidence would resolve it: Experiments applying CoDi or a modified version to datasets with ordinal or hierarchical categorical variables, along with theoretical analysis of how the framework would need to adapt, would address this question.

## Limitations

- The mechanistic justification for design choices relies on limited ablation studies
- The negative sampling method is described conceptually but not exhaustively validated
- Computational complexity analysis is not detailed, particularly for large-scale datasets

## Confidence

- Confidence level: Medium
- Strong empirical results on 11 datasets but limited mechanistic justification
- No comprehensive ablation studies to isolate component contributions
- Sampling time comparison favorable but lacks dataset complexity breakdown

## Next Checks

1. Perform comprehensive ablation studies removing each component (separate models, co-evolutionary conditioning, contrastive learning) to quantify their individual contributions to performance gains.

2. Test the robustness of the negative sampling method by comparing Method 3 with alternative strategies (e.g., random shuffling vs. nearest-neighbor-based negatives) on a subset of datasets.

3. Evaluate the model's ability to capture complex dependencies by measuring correlation preservation between continuous and discrete variables before and after synthesis across all 11 datasets.