---
ver: rpa2
title: Towards Making the Most of ChatGPT for Machine Translation
arxiv_id: '2303.13780'
source_url: https://arxiv.org/abs/2303.13780
tags:
- chatgpt
- translation
- performance
- prompts
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatGPT exhibits strong translation capabilities for high-resource
  languages but underperforms on complex tasks like low-resource and distant-language-pairs
  translation. This work explores ways to further enhance ChatGPT's translation ability
  by adjusting temperature, task information, and domain information.
---

# Towards Making the Most of ChatGPT for Machine Translation

## Quick Facts
- arXiv ID: 2303.13780
- Source URL: https://arxiv.org/abs/2303.13780
- Authors: 
- Reference count: 7
- ChatGPT shows strong translation for high-resource languages but struggles with low-resource and distant-language-pairs translation.

## Executive Summary
This paper investigates ChatGPT's machine translation capabilities and proposes methods to enhance its performance through prompt engineering. The study finds that ChatGPT's translation quality depends significantly on temperature settings, with lower temperatures yielding better results. By introducing Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP), the authors demonstrate improvements in translation performance, particularly for complex tasks. The research also reveals that ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially mitigated through the proposed prompts.

## Method Summary
The study evaluates ChatGPT's translation performance using gpt-3.5-turbo-0301 across multiple language pairs and domains. The researchers systematically vary temperature settings from 0 to 1 to assess its impact on translation quality. They propose two prompt engineering strategies: Task-Specific Prompts (TSP) that prepend "You are a machine translation system" to bridge the task gap, and Domain-Specific Prompts (DSP) that introduce domain navigation information to leverage generalization ability. The evaluation uses COMET-20 as the primary metric alongside BLEU and ChrF scores on Flores-200 and WMT19 test sets. The study also explores few-shot in-context learning and chain-of-thought prompting strategies to understand their effects on translation behavior.

## Key Results
- ChatGPT's translation performance degrades significantly as temperature increases from 0 to 1
- Task-Specific Prompts improve performance in complex translation tasks by emphasizing task information
- Domain-Specific Prompts with correct domain information consistently improve performance, while incorrect domain information causes significant degradation
- ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by the proposed prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temperature setting directly influences ChatGPT's translation quality by controlling output diversity.
- Mechanism: Lower temperatures reduce stochasticity in the sampling process, leading to more deterministic and stable translations that are less prone to hallucination and better aligned with reference translations.
- Core assumption: Translation is a high-certainty task where variability in output is more harmful than beneficial.
- Evidence anchors:
  - [abstract] "ChatGPT's performance depends largely on temperature, with lower temperatures generally yielding better results"
  - [section 3.1] "as the temperature rises, there is a clear degradation both in COMET and BLEU scores"
- Break condition: If task uncertainty increases (e.g., creative translation or highly ambiguous inputs), higher temperatures might be beneficial for exploring diverse interpretations.

### Mechanism 2
- Claim: Providing explicit task information via prompts bridges the gap between ChatGPT's conversational design and translation requirements.
- Mechanism: By prepending task-specific instructions ("You are a machine translation system"), the model receives clear guidance on expected behavior, reducing the likelihood of generating conversational or hallucinated content.
- Core assumption: The model's fine-tuning on conversational data creates a default behavior that is not optimal for translation tasks without explicit task framing.
- Evidence anchors:
  - [abstract] "Emphasizing the task information in prompts can improve ChatGPT's performance, particularly in complex translation tasks"
  - [section 3.2] "we propose Task-Specific Prompts (TSP) to further emphasize the task information to bridge the task gap"
- Break condition: If the prompt becomes too restrictive or verbose, it might interfere with the model's natural language understanding capabilities.

### Mechanism 3
- Claim: Domain-specific information in prompts enables ChatGPT to leverage its generalization ability for improved performance in specialized translation tasks.
- Mechanism: By specifying the domain (e.g., biomedical, news), the model can better contextualize vocabulary and phrasing expectations, leading to more accurate translations within that domain.
- Core assumption: The model has learned domain-relevant patterns during pretraining that can be activated through targeted prompts.
- Evidence anchors:
  - [abstract] "Introducing correct domain information consistently improves performance, while incorrect domain information leads to significant degradation"
  - [section 3.3] "we propose Domain-Specific Prompts (DSP) to introduce the domain navigation information to elicit ChatGPT's generalization ability"
- Break condition: If the domain information is incorrect or misleading, performance degrades significantly, indicating the importance of accurate domain specification.

## Foundational Learning

- Concept: Temperature scaling in language models
  - Why needed here: Understanding how temperature affects sampling behavior is crucial for optimizing translation quality
  - Quick check question: What happens to the probability distribution over tokens when temperature is set to 0 versus 1?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The effectiveness of translation depends heavily on how instructions are framed and what examples are provided
  - Quick check question: How does adding task-specific instructions to a prompt change the model's output distribution?

- Concept: Domain adaptation without fine-tuning
  - Why needed here: Leveraging domain information through prompts rather than fine-tuning is a key efficiency advantage
  - Quick check question: What mechanisms allow a general-purpose model to adapt to specific domains through prompt engineering?

## Architecture Onboarding

- Component map: Prompt generation -> API call (gpt-3.5-turbo-0301) -> Response processing -> Evaluation (COMET/BLEU/ChrF) -> Analysis
- Critical path: Prompt generation → API call → Response processing → Evaluation → Analysis
- Design tradeoffs: Lower temperatures improve translation quality but reduce diversity; task-specific prompts improve accuracy but may constrain creativity; domain prompts improve specialized performance but require accurate domain specification
- Failure signatures: Hallucinations in non-English-centric tasks, significant performance degradation with incorrect domain information, and word-by-word translation behavior with CoT prompting
- First 3 experiments:
  1. Test temperature sensitivity by running translations at T=0, 0.2, 0.4, 0.6, 0.8, 1.0 and measuring COMET/BLEU scores
  2. Compare TSP effectiveness by running with and without task-specific instructions across multiple language pairs
  3. Validate DSP impact by testing correct vs. incorrect domain prompts on WMT19 Bio and News datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ChatGPT vary when translating between language pairs with significantly different resource levels or language distances?
- Basis in paper: [explicit] The paper discusses that ChatGPT's performance largely depends on temperature, especially in difficult languages, and that the impact of temperature is relatively small when translating to high-resource languages like German, while for complex languages like Chinese, it has a large degradation in performance when the temperature changes from 0 to 1.
- Why unresolved: The paper does not provide a detailed analysis of how ChatGPT's performance varies across different language pairs with varying resource levels and language distances.
- What evidence would resolve it: Conducting experiments with a diverse set of language pairs, including high-resource, low-resource, and distant language pairs, and analyzing the performance of ChatGPT across these pairs would provide insights into how its performance varies.

### Open Question 2
- Question: What are the underlying reasons for ChatGPT's tendency to generate hallucinations when translating non-English-centric language pairs, and how can this issue be effectively mitigated?
- Basis in paper: [explicit] The paper highlights that ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by the proposed prompts but still need to be highlighted for the MT/NLP community.
- Why unresolved: The paper does not provide a detailed explanation of the underlying reasons for this behavior or effective strategies to mitigate it.
- What evidence would resolve it: Investigating the causes of hallucinations, such as the model's training data or the prompt design, and developing targeted strategies to reduce or eliminate hallucinations in non-English-centric translations would address this question.

### Open Question 3
- Question: How do different in-context learning strategies, particularly those inspired by example-based and statistical machine translation, affect ChatGPT's performance on machine translation tasks?
- Basis in paper: [inferred] The paper explores the effects of advanced in-context learning strategies, including few-shot in-context learning and chain-of-thought prompting, and finds that few-shot ICL can improve ChatGPT's performance, while CoT leads to word-by-word translation behavior and significant degradation.
- Why unresolved: The paper does not provide a comprehensive analysis of various in-context learning strategies and their impact on ChatGPT's translation performance.
- What evidence would resolve it: Conducting experiments with different in-context learning strategies, such as example-based and statistical MT-inspired approaches, and evaluating their impact on ChatGPT's translation performance would provide insights into the effectiveness of these strategies.

## Limitations
- Results may not generalize to newer models like GPT-4
- Hallucination problem in non-English-centric tasks is only partially addressed
- Temperature-prompt interaction effects were not fully explored
- Study focuses on specific language pairs and domains, limiting generalizability

## Confidence
- **High confidence:** Temperature effects on translation quality (supported by systematic variation and clear degradation patterns)
- **Medium confidence:** Task-Specific Prompts effectiveness (demonstrated improvements but limited to specific task framing)
- **Medium confidence:** Domain-Specific Prompts impact (shows consistent improvements but domain accuracy is critical)
- **Low confidence:** Chain-of-thought prompting degradation (based on limited observations, mechanism not fully explained)

## Next Checks
1. **Temperature-Strategy Interaction Test:** Run experiments combining TSP/DSP prompts with different temperature settings (0, 0.2, 0.4, 0.6, 0.8, 1.0) to determine if prompt strategies amplify or mitigate temperature effects across various language pairs.

2. **Model Generalization Test:** Replicate the temperature and prompt experiments using GPT-4 to assess whether the observed patterns hold across different model versions, particularly for the hallucination problem in non-English-centric tasks.

3. **Hallucination Root Cause Analysis:** Conduct controlled experiments isolating whether hallucinations stem from the model's default behavior, prompt ambiguity, or specific linguistic characteristics of source languages, using both English-centric and non-English-centric language pairs with and without post-processing.