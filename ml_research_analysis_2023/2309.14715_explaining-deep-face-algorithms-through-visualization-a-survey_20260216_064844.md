---
ver: rpa2
title: 'Explaining Deep Face Algorithms through Visualization: A Survey'
arxiv_id: '2309.14715'
source_url: https://arxiv.org/abs/2309.14715
tags:
- face
- image
- saliency
- features
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work surveys explainability methods for deep face models,
  focusing on visualization techniques. It reviews saliency maps, feature visualization,
  and feature inversion methods, highlighting their application to face tasks and
  identifying challenges.
---

# Explaining Deep Face Algorithms through Visualization: A Survey

## Quick Facts
- arXiv ID: 2309.14715
- Source URL: https://arxiv.org/abs/2309.14715
- Reference count: 40
- Key outcome: Survey of explainability methods for deep face models, with user study showing saliency maps preferred by novices while feature visualization favored by experts familiar with explainability methods

## Executive Summary
This survey comprehensively reviews visualization-based explainability methods for deep face models, focusing on saliency maps, feature visualization, and feature inversion techniques. The authors highlight that generic explainability methods designed for natural images often fail on face images due to domain-specific characteristics. Through a user study with practitioners, they find that explanation preferences vary based on user expertise - saliency maps are generally preferred for their simplicity and direct relevance to face tasks, while feature visualization appeals more to those with explainability experience. The paper emphasizes the need for face-specific evaluation protocols and user-centered design in developing explainable AI for face processing tasks.

## Method Summary
The survey systematically categorizes existing visualization methods for deep face models into three main types: saliency maps (including occlusion-based and gradient-based methods adapted for facial structure), feature visualization (using activation maximization to reveal hierarchical learning patterns), and feature inversion (reconstructing images from deep features with TV regularization). The authors apply these methods to various face tasks including recognition, pose estimation, and emotion recognition using VGG16-based networks. A key contribution is the user study comparing explanation methods, where practitioners evaluated different visualization types based on interpretability and utility. The methodology combines theoretical analysis of face-specific challenges with empirical evaluation through practitioner feedback.

## Key Results
- Saliency maps respecting facial structure outperform generic methods for face-specific attribution
- Deep face models exhibit hierarchical learning patterns: early layers learn simple patterns, middle layers learn facial parts, deeper layers encode identity-specific geometry
- User study reveals expertise-dependent preferences: novices prefer saliency maps while experts favor feature visualization
- Face-specific evaluation protocols are needed to properly assess explainability method effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Saliency maps work better on faces when they respect facial structure rather than treating the face as a generic object
- Mechanism: Face-specific saliency methods incorporate anatomical priors (e.g., landmark-based occlusions, canonical face alignment) to guide perturbation and attribution, which prevents the algorithm from highlighting irrelevant background or uniform face regions
- Core assumption: Faces have a shared structure (nose center, mouth below, eyes above) that can be exploited to improve attribution fidelity
- Evidence anchors: [abstract] "Most current explainability methods are designed for object recognition and cannot directly be applied to the face domain due to their differences from the natural image domain"; [section 2.4] "Respecting the facial structure is critical in adapting generic saliency algorithms to face-specific ones"

### Mechanism 2
- Claim: Feature visualization of deep face models reveals hierarchical learning: early layers learn simple patterns, middle layers learn facial parts, and deeper layers encode identity-specific geometry
- Mechanism: By maximizing activations of filters/neurons, we obtain visualizations that show what each layer "looks for." For faces, this translates into interpretable facial components (eyes, nose shapes) at higher layers
- Core assumption: Convolutional filters form a meaningful basis for face representation, and each layer's filters specialize progressively toward task-relevant features
- Evidence anchors: [abstract] "We review existing face explainability works and reveal valuable insights into the structure and hierarchy of face networks"; [section 3.2] "deep face models learn hierarchical features from face images. The initial layers learn simple patterns... higher layers learn complex patterns composed of these simple patterns"

### Mechanism 3
- Claim: User comprehension of face explanations is higher when explanations match the user's expertise level (saliency maps for novices, feature visualization for experts)
- Mechanism: Different explanation types provide different abstraction levels. Saliency maps give an immediate, task-focused view; feature visualization requires understanding of model internals. Matching explanation complexity to user knowledge improves trust and utility
- Core assumption: The gap between "practitioners" (who develop explainability) and "consumers" (who use it) creates a usability mismatch
- Evidence anchors: [abstract] "determine the design considerations for practical face visualizations accessible to AI practitioners by conducting a user study"; [section 7] "most respondents preferred saliency maps, except those who have used or implemented explainability algorithms"

## Foundational Learning

- Concept: Facial landmark-based occlusion for saliency
  - Why needed here: Standard occlusion slides a window over the image; for faces, systematic occlusions at landmark-defined regions preserve the facial structure and yield more interpretable attributions
  - Quick check question: What facial region would you occlude to test if a gender classifier relies on hair?

- Concept: Activation maximization for feature visualization
  - Why needed here: This optimization finds the input image that maximally activates a neuron/filter; for faces, it reveals what facial features each neuron encodes
  - Quick check question: If a neuron in layer 12 responds to "eyes," what would its activation maximization visualization look like?

- Concept: Feature inversion and its regularization
  - Why needed here: Inverting deep features back to image space shows what information the model preserves at each layer and helps diagnose misclassifications
  - Quick check question: Why might a face recognition model's last-layer inversion emphasize eye shape over skin texture?

## Architecture Onboarding

- Component map: Input (face images) -> Models (VGG16-based face networks) -> Explainers (occlusion saliency, TV-regularized inversion, activation maximization) -> Evaluation (user survey, quantitative metrics)
- Critical path: 1. Load pre-trained face model 2. Apply occlusion at landmark-based regions 3. Generate saliency heatmap 4. Compute feature inversion with TV regularization 5. Perform activation maximization per layer 6. Aggregate results and conduct user survey
- Design tradeoffs:
  - Saliency vs. feature visualization: Saliency is easier to interpret but less detailed; visualization is more informative but requires expertise
  - Regularization strength in inversion: Higher TV regularization yields cleaner images but may lose fine-grained identity cues
  - Landmark detection accuracy: Errors propagate into occlusion-based saliency, reducing fidelity
- Failure signatures:
  - Saliency maps highlighting uniform face regions → landmark detection failure or model reliance on non-facial cues
  - Feature inversion producing random noise → inadequate regularization or too-deep layer inversion
  - User survey showing no preference → explanations too abstract or not task-relevant
- First 3 experiments:
  1. Run occlusion saliency on a gender classifier; verify that occluding hair reduces confidence more than occluding eyes
  2. Perform feature inversion from the last conv layer; check if reconstructed eyes and nose shapes are recognizable
  3. Apply activation maximization to a randomly chosen filter in conv4; confirm it responds to a facial part (e.g., eyebrow curvature)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do saliency maps perform when applied to facial expressions that involve conflicting emotions, such as a smile with crying eyes?
- Basis in paper: [explicit] The paper discusses the challenges of applying general saliency algorithms to face images, particularly in cases of conflicting emotions (e.g., smiling lips with crying eyes). Figure 1 illustrates the limitations of GradCAM in highlighting relevant features for each class in such cases
- Why unresolved: The paper highlights the difficulty of saliency maps in capturing nuanced emotional expressions but does not provide a comprehensive evaluation or comparison of different saliency methods specifically for conflicting emotions
- What evidence would resolve it: A systematic evaluation of various saliency methods on a dataset of faces with conflicting emotions, comparing their ability to highlight relevant features for each emotion class

### Open Question 2
- Question: How do feature visualization methods differ in their ability to capture task-specific information for face models trained on different tasks (e.g., recognition, pose estimation, emotion recognition)?
- Basis in paper: [explicit] The paper discusses the application of feature visualization methods to face models and highlights the hierarchical patterns learned by filters in convolutional layers. Figure 10 shows that different face networks learn different features based on their tasks, with recognition networks focusing on eye and nose shapes, pose networks on 3D shape, and emotion networks on eyebrow and mouth curves
- Why unresolved: The paper provides qualitative insights into the differences between feature visualizations for different tasks but does not quantify the task-specific information captured by each method or compare their effectiveness
- What evidence would resolve it: A quantitative analysis of the task-specific information captured by different feature visualization methods, comparing their ability to highlight discriminative features for each task

### Open Question 3
- Question: How do human interpretations of facial concepts align with the internal representations learned by deep face models?
- Basis in paper: [explicit] The paper discusses the challenges of aligning deep face features with human-understandable concepts. Figure 16 shows potential definitions for facial concepts, including facial features, head pose, emotions, and facial action units. The paper also mentions the work of Yin et al. [20], which aims to train filters to represent specific face features regardless of identity or pose
- Why unresolved: The paper provides insights into the potential concepts for face images and the efforts to align filters with these concepts but does not evaluate the alignment between human interpretations and model representations
- What evidence would resolve it: A user study comparing human interpretations of facial concepts with the internal representations learned by deep face models, using methods such as concept activation vectors (TCAV) or automatic concept-based evaluation (ACE)

## Limitations

- Face-specific methods may not generalize to non-frontal or occluded face scenarios
- User study methodology and sample size not fully detailed, limiting generalizability
- Lack of quantitative validation for face-specific explainability method effectiveness

## Confidence

- Face-specific saliency methods: Medium - supported by theoretical reasoning but limited empirical validation
- Hierarchical feature learning in face models: Medium - based on established CNN principles but not face-specific validation
- User preference findings: Medium - survey methodology not fully detailed, sample size unclear

## Next Checks

1. Implement landmark-based occlusion on a model trained with non-frontal faces to test structure assumption validity
2. Conduct ablation studies on TV regularization strength in feature inversion to find optimal balance
3. Replicate the user study with a larger, more diverse sample including both face and general ML practitioners