---
ver: rpa2
title: 'Deep Active Learning in the Presence of Label Noise: A Survey'
arxiv_id: '2302.11075'
source_url: https://arxiv.org/abs/2302.11075
tags:
- learning
- label
- noise
- active
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The survey reviews deep active learning (DAL) in the presence of
  label noise, highlighting unique approaches, their strengths, and weaknesses. With
  the recent success of vision transformers in image classification tasks, the paper
  provides a brief overview and considers how transformer layers and attention mechanisms
  can enhance diversity, importance, and uncertainty-based selection in queries sent
  to an oracle for labeling.
---

# Deep Active Learning in the Presence of Label Noise: A Survey

## Quick Facts
- arXiv ID: 2302.11075
- Source URL: https://arxiv.org/abs/2302.11075
- Reference count: 31
- Key outcome: Survey reviews DAL approaches with label noise, highlights strengths/weaknesses, and proposes exploring ViTs and contrastive learning for better sample selection

## Executive Summary
This survey examines deep active learning methods when training data contains label noise, focusing on how to effectively select high-value samples for labeling despite imperfect annotations. The paper categorizes active learning strategies (uncertainty, diversity, importance-based) and analyzes their robustness to label noise, while proposing vision transformers and contrastive learning as promising directions for improving sample selection. It identifies key challenges including computational efficiency, lack of standardized benchmarks, and the need for better noise-handling mechanisms in active learning pipelines.

## Method Summary
The survey synthesizes existing DAL approaches by examining pool-based methods that select samples based on uncertainty, diversity, or importance metrics from unlabeled data pools. It analyzes noise-handling techniques including denoising layers, filtering mechanisms, and dual-oracle systems. The proposed methodology involves iterative active learning cycles where a model queries an oracle for labels, with various selection strategies and noise mitigation approaches evaluated on standard image classification datasets with artificially injected label noise at different rates (10%, 30%, 60%).

## Key Results
- Deep active learning can maintain effectiveness even with significant label noise (up to 60%) through appropriate selection strategies
- Denoising layers and filtering approaches show promise in mitigating the impact of noisy labels during active learning
- Vision transformers and contrastive learning methods represent promising directions for improving sample selection quality in noisy label scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep active learning with label noise improves classification accuracy by iteratively selecting high-uncertainty, diverse samples for labeling, which counteracts the model's overfitting to noisy labels.
- Mechanism: The algorithm maintains an unlabeled pool and selects samples based on uncertainty scores (e.g., entropy) and diversity metrics (e.g., core-set). These selected samples are sent to an oracle for labeling. The newly labeled samples are added to the training set, and the model is retrained. This process repeats, progressively refining the model's understanding of the true data distribution despite label noise.
- Core assumption: The oracle's label accuracy is higher than the initial noisy labels, and the uncertainty and diversity metrics effectively identify samples that, when correctly labeled, will improve the model's generalization.
- Evidence anchors:
  - [abstract]: "Deep active learning has emerged as a powerful tool for training deep learning models within a predefined labeling budget."
  - [section]: "Pool-based methods select samples for labeling from an unlabeled pool, based on either the uncertainty of the currently trained model on samples U n, the diversity of samples in the labeled set Lm used to train the current model or a combination of both."
  - [corpus]: Weak - no direct evidence of active learning with label noise in the corpus.
- Break condition: If the oracle consistently provides incorrect labels or the uncertainty/diversity metrics fail to identify informative samples, the active learning process will not improve and may even degrade performance.

### Mechanism 2
- Claim: Denoising layers in neural networks can mitigate the impact of label noise during training by learning to map noisy labels to their correct counterparts.
- Mechanism: A denoising layer is added to the neural network architecture. This layer takes the softmax output of the classifier and learns a mapping to a probability distribution over the true labels. During training, the network is optimized to minimize the difference between the denoised output and the true labels. During testing, the penultimate layer's output is used for prediction, bypassing the denoising layer.
- Core assumption: The denoising layer can learn an effective mapping from noisy labels to true labels, and this mapping generalizes to unseen data.
- Evidence anchors:
  - [abstract]: "We further propose exploring contrastive learning methods to derive good image representations that can aid in selecting high-value samples for labeling in an active learning setting."
  - [section]: "In Gupta et al. [2020], Gupta et al. propose the use of standard sample diversity and importance query policies, supplemented by the model's confidence scores on samples. They use a denoising layer to their network."
  - [corpus]: Weak - no direct evidence of denoising layers in the corpus.
- Break condition: If the noise distribution is too complex or the denoising layer is not expressive enough, it may fail to learn an effective mapping, leading to poor performance.

### Mechanism 3
- Claim: Contrastive learning can improve the quality of image representations used for active learning by learning invariant features that are robust to label noise.
- Mechanism: Contrastive learning is used to pre-train a neural network on the unlabeled dataset. This involves creating positive pairs of similar images and negative pairs of dissimilar images. The network learns to map similar images to similar representations and dissimilar images to dissimilar representations. These learned representations are then used in the active learning process to select diverse and informative samples.
- Core assumption: Contrastive learning can learn meaningful representations that are robust to label noise, and these representations are useful for active learning.
- Evidence anchors:
  - [abstract]: "We further propose exploring contrastive learning methods to derive good image representations that can aid in selecting high-value samples for labeling in an active learning setting."
  - [section]: "More recent literature blends pool and density-based methods to take advantage of each approach's benefits. These methods thus lead to efficient and robust models trained on core-sets containing diverse samples that maximize the margins between object classes."
  - [corpus]: Weak - no direct evidence of contrastive learning in the corpus.
- Break condition: If the contrastive learning objective is not well-suited to the data or the learned representations are not discriminative enough, the active learning process may not benefit.

## Foundational Learning

- Concept: Convolutional Neural Networks (CNNs)
  - Why needed here: CNNs are the dominant approach for image classification tasks, and understanding their architecture and operation is crucial for designing active learning algorithms that work with image data.
  - Quick check question: What are the key advantages of CNNs over fully connected networks for image classification?

- Concept: Vision Transformers (ViTs)
  - Why needed here: ViTs are a newer architecture that has shown promising results in image classification, and exploring their potential for active learning with label noise is a key research direction.
  - Quick check question: How do ViTs differ from CNNs in terms of input encoding and feature learning?

- Concept: Label Noise
  - Why needed here: Understanding the different types and sources of label noise is essential for designing robust active learning algorithms that can handle noisy labels.
  - Quick check question: What are the main challenges in training deep learning models on datasets with label noise?

## Architecture Onboarding

- Component map: Data (Unlabeled pool, labeled set, test set) -> Model (Neural network, denoising layer) -> Selection (Uncertainty metric, diversity metric) -> Oracle (Human annotator or automated labeling system) -> Labeled set -> Retrain Model -> Repeat

- Critical path: Data -> Model -> Selection -> Oracle -> Labeled set -> Retrain Model -> Repeat

- Design tradeoffs:
  - Model complexity vs. training time: More complex models may achieve better performance but require more computational resources.
  - Selection strategy: Balancing uncertainty and diversity is crucial for effective active learning.
  - Oracle quality: The accuracy of the oracle directly impacts the effectiveness of the active learning process.

- Failure signatures:
  - Poor performance on the test set: Indicates that the active learning process is not effectively improving the model's generalization.
  - High variance in performance across different runs: Suggests that the selection strategy is not robust to noise.
  - Slow convergence: May indicate that the selection strategy is not effectively identifying informative samples.

- First 3 experiments:
  1. Implement a basic active learning algorithm with uncertainty sampling on a clean dataset (e.g., MNIST) to verify the core functionality.
  2. Add label noise to the dataset and evaluate the performance of the active learning algorithm with and without a denoising layer.
  3. Replace the CNN with a ViT and compare the performance of the active learning algorithm on the same noisy dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different label noise distributions (class-dependent vs class-independent) impact the effectiveness of DAL algorithms?
- Basis in paper: [explicit] The paper discusses different types of label noise and their impact on DAL, mentioning class-dependent and class-independent label noise.
- Why unresolved: The paper mentions these types of noise but does not provide a comprehensive comparison of their impact on DAL algorithm performance.
- What evidence would resolve it: Experiments comparing DAL algorithm performance on datasets with different label noise distributions, specifically analyzing the impact of class-dependent vs class-independent noise.

### Open Question 2
- Question: What is the impact of oracle abstention on the generalization bounds of DAL algorithms in the presence of label noise?
- Basis in paper: [explicit] The paper mentions the work of Amin et al. [2021] which investigates the generalization bounds of DAL and abstention learning.
- Why unresolved: While the paper mentions this work, it does not provide a detailed analysis of how oracle abstention specifically impacts generalization bounds in noisy label settings.
- What evidence would resolve it: Theoretical analysis and empirical experiments demonstrating the relationship between oracle abstention rates, label noise, and the generalization bounds of DAL algorithms.

### Open Question 3
- Question: How can contrastive learning methods be effectively integrated into DAL algorithms to improve sample selection in the presence of label noise?
- Basis in paper: [explicit] The paper proposes exploring contrastive learning methods to derive good image representations for sample selection in DAL.
- Why unresolved: The paper suggests this as a future research direction but does not provide concrete methods or results for integrating contrastive learning into DAL algorithms.
- What evidence would resolve it: Development and evaluation of specific DAL algorithms that incorporate contrastive learning for sample selection, demonstrating improved performance on noisy label datasets.

## Limitations

- Lack of standardized benchmark datasets and evaluation protocols specifically designed for deep active learning with label noise
- Computational efficiency and scalability challenges, particularly for vision transformer-based approaches
- Limited systematic comparison of different noise-handling mechanisms (denoising layers, filtering, dual-oracle systems) under identical conditions

## Confidence

- High confidence: The characterization of active learning mechanisms (uncertainty sampling, diversity selection, core-set methods) is well-established in the literature and accurately represented.
- Medium confidence: Claims about denoising layers and contrastive learning applications are supported by citations but lack detailed implementation specifics that would enable direct replication.
- Low confidence: The survey's predictions about vision transformer performance in active learning with noisy labels remain largely theoretical, as few published results exist to validate these claims.

## Next Checks

1. Implement a standardized benchmark suite with consistent noise injection protocols (e.g., 10%, 30%, 60% symmetric and asymmetric noise) across the mentioned datasets to enable fair method comparisons.
2. Conduct ablation studies comparing denoising layer architectures, contrastive learning pretraining approaches, and selection strategy combinations under controlled noise conditions.
3. Evaluate computational cost scaling for vision transformer-based active learning methods versus CNN baselines across different dataset sizes and labeling budgets.