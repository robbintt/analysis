---
ver: rpa2
title: Graph Elicitation for Guiding Multi-Step Reasoning in Large Language Models
arxiv_id: '2311.09762'
source_url: https://arxiv.org/abs/2311.09762
tags:
- question
- reasoning
- answer
- triplets
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a graph-guided reasoning approach for multi-step
  reasoning in large language models. It addresses the limitations of conventional
  chain-of-thought prompting, such as generating irrelevant or redundant sub-questions,
  by extracting knowledge triplets from the question and using them to guide sub-question
  generation.
---

# Graph Elicitation for Guiding Multi-Step Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2311.09762
- Source URL: https://arxiv.org/abs/2311.09762
- Reference count: 22
- Primary result: Graph-guided reasoning outperforms standard chain-of-thought prompting on multi-hop question answering datasets

## Executive Summary
This paper introduces a graph-guided reasoning approach that addresses the limitations of conventional chain-of-thought prompting for multi-step reasoning tasks. By extracting knowledge triplets from questions to form a graph representation, the method guides sub-question generation and verifies rationales against this graph structure. The approach shows significant improvements over existing prompting methods on three multi-hop question answering datasets, with particular benefits when combined with retrieval augmentation.

## Method Summary
The method extracts knowledge triplets from questions to construct a graph representation, then generates sub-questions based on these triplets. For each sub-question, it generates answers with optional knowledge retrieval, and verifies the generated rationales against the question graph by comparing extracted triplets. This process iterates until an answer is reached or a stopping criterion is met. The verification step filters out irrelevant or redundant rationales, ensuring the reasoning path stays focused on the original question's requirements.

## Key Results
- Graph-guided reasoning outperforms chain-of-thought, self-consistency, and self-ask methods on three multi-hop QA datasets
- The method achieves higher exact match (EM) and F1 scores compared to baseline prompting approaches
- Retrieval augmentation further improves performance in open-domain settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-guided reasoning reduces irrelevant rationale generation by verifying intermediate steps against the question graph.
- Mechanism: Each generated rationale is converted to triplets and checked for matching relations and entities with the question graph before acceptance. This filters out hallucinated or irrelevant intermediate steps early.
- Core assumption: LLMs can reliably convert intermediate rationales into accurate knowledge triplets, and the question graph sufficiently constrains the valid answer space.

### Mechanism 2
- Claim: Using knowledge triplets for subquestion generation ensures all relevant information is covered without redundancy.
- Mechanism: The question graph is built from extracted triplets, and subquestions are generated for each triplet. This guarantees coverage of the logical steps needed to answer the question.
- Core assumption: Triplets extracted from the question are complete and correctly represent the reasoning steps needed to answer the question.

### Mechanism 3
- Claim: Retrieval augmentation using subquestions as queries provides more targeted knowledge than using the original question.
- Mechanism: For each subquestion, relevant knowledge documents are retrieved and used to generate rationales, addressing the LLM's knowledge gaps and reducing hallucination.
- Core assumption: Subquestions are simpler and more focused, making retrieval results more relevant than retrieving for the full complex question.

## Foundational Learning

- Concept: Knowledge graph triplet representation and extraction
  - Why needed here: The entire method relies on representing questions and rationales as structured triplets for comparison and verification.
  - Quick check question: Given the question "When did the director of film X die?", what would the extracted triplets look like?

- Concept: Retrieval-augmented generation fundamentals
  - Why needed here: The method retrieves external knowledge for each subquestion to improve answer accuracy.
  - Quick check question: What are the key differences between using the original question vs. subquestions as retrieval queries?

- Concept: Chain-of-thought prompting mechanics
  - Why needed here: The method builds on CoT prompting but adds graph-guided verification steps.
  - Quick check question: How does CoT prompting differ from standard prompting in terms of reasoning path generation?

## Architecture Onboarding

- Component map: Question graph construction → Intermediate subquestion generation → Answer generation (with optional retrieval) → Rationale verification → Repeat or final answer
- Critical path: Triplet extraction → Subquestion generation → Rationale generation → Verification → Answer generation
- Design tradeoffs:
  - Graph-guided vs. pure CoT: Better accuracy but higher complexity and computational cost
  - Subquestion-based retrieval vs. question-based: More targeted but requires well-formed subquestions
  - Confidence threshold: Too high may skip useful retrievals, too low may include noise
- Failure signatures:
  - No progress after several iterations: Likely verification is too strict or triplet extraction is failing
  - Low retrieval relevance: Subquestions may be poorly formed or too complex
  - High variance across runs: Stochastic decoding in subquestion generation may need tuning
- First 3 experiments:
  1. Test triplet extraction quality on a small set of questions with ground truth triplets
  2. Verify that subquestion generation covers all triplets without redundancy
  3. Check retrieval relevance scores when using subquestions vs. original questions as queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the graph-guided reasoning method scale with increasingly complex multi-hop questions that require more than 4 reasoning steps?
- Basis in paper: The paper mentions that the MuSiQue dataset includes 2 to 4 hop questions, but does not explore performance on questions requiring more reasoning steps.
- Why unresolved: The experiments only cover datasets with up to 4-hop questions, leaving the method's effectiveness on more complex reasoning tasks unexplored.

### Open Question 2
- Question: What is the impact of different knowledge retrieval methods (e.g., dense retrieval vs. BM25) on the performance of the graph-guided reasoning approach in open-domain settings?
- Basis in paper: The paper uses BM25 for retrieval but mentions that advancements in retrieval methods could help improve performance.
- Why unresolved: The paper does not compare different retrieval methods or explore their impact on the overall reasoning performance.

### Open Question 3
- Question: How does the graph-guided reasoning method perform when applied to non-factoid questions or open-ended reasoning tasks beyond multi-hop question answering?
- Basis in paper: The method is evaluated only on multi-hop question answering datasets, but the authors mention that the approach could be generalized to other types of questions.
- Why unresolved: The current experiments do not test the method's effectiveness on non-factoid or open-ended reasoning tasks.

## Limitations
- The method's effectiveness heavily depends on the quality of triplet extraction from both questions and rationales.
- Performance on complex questions with nested or implicit reasoning remains unclear.
- The retrieval-augmented variant's improvements may diminish on datasets where relevant documents are sparse or noisy.

## Confidence
- **High confidence**: Graph-guided reasoning outperforms standard chain-of-thought prompting on benchmark datasets.
- **Medium confidence**: The verification mechanism effectively reduces irrelevant rationale generation, but edge cases with complex implicit reasoning are not fully explored.
- **Medium confidence**: Retrieval augmentation improves performance, but the relative gains depend heavily on retrieval quality and dataset characteristics.

## Next Checks
1. Test triplet extraction robustness on questions with implicit knowledge or complex reasoning patterns.
2. Vary the triplet matching threshold to find optimal balance between filtering noise and preserving valid reasoning steps.
3. Compare retrieval result quality when using subquestions vs. original questions as queries across different dataset domains.