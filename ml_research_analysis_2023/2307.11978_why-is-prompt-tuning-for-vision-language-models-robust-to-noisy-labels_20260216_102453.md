---
ver: rpa2
title: Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?
arxiv_id: '2307.11978'
source_url: https://arxiv.org/abs/2307.11978
tags:
- prompt
- tuning
- noise
- clip
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Vision-language models like CLIP adapt to new tasks via prompt
  tuning, which we find is highly robust to noisy labels. This robustness stems from
  two factors: the fixed classname tokens provide strong regularization, and the pre-trained
  image-text embeddings offer robust priors.'
---

# Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?

## Quick Facts
- arXiv ID: 2307.11978
- Source URL: https://arxiv.org/abs/2307.11978
- Authors: 
- Reference count: 40
- Vision-language models like CLIP adapt to new tasks via prompt tuning, which we find is highly robust to noisy labels.

## Executive Summary
This paper investigates why prompt tuning for vision-language models like CLIP is remarkably robust to label noise. The authors identify two key mechanisms: fixed class tokens provide strong regularization during optimization, and pre-trained image-text embeddings offer robust semantic priors. They demonstrate that this robustness enables a novel unsupervised prompt tuning approach where CLIP's own noisy zero-shot predictions are used to tune its prompt, significantly improving accuracy without labeled data.

## Method Summary
The method involves adapting CLIP to new image classification tasks through prompt tuning, where only a small set of learnable tokens (16 per class) are optimized while the pre-trained visual and text encoders remain frozen. The approach is tested across 8 datasets with varying noise rates (0-50%) injected into the training labels. Two loss functions are compared: standard cross-entropy and generalized cross-entropy (GCE). The key insight is that fixed class tokens in the text encoder provide regularization that suppresses noisy gradients during training.

## Key Results
- Prompt tuning achieves superior robustness to label noise compared to linear probing and full fine-tuning
- Combining prompt tuning with GCE loss further improves robustness, especially at high noise rates
- CLIP's zero-shot predictions can be used to generate pseudo-labels for unsupervised prompt tuning, improving accuracy significantly
- The robustness holds across multiple datasets and noise types (random and confusion noise)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fixed classname tokens provide strong regularization to the optimization process, reducing gradients induced by noisy samples.
- Mechanism: The pre-trained text encoder uses the classname tokens to encode relationships between visual concepts, creating a structured label space. During prompt tuning, this structure acts as a prior that regularizes the optimization, making it less sensitive to noisy labels.
- Core assumption: The pre-trained text encoder has learned meaningful relationships between class names that can be leveraged during prompt tuning.
- Evidence anchors:
  - [abstract] "the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples"
  - [section 4.3] "Table 2 shows the analysis on four dataset for different noise levels. Compared to prompt-tuning, which optimizes only learnable tokens shared across all classes, both CLS-Tuning and Full-Prompt-Tuning models struggle at high noise rate."
  - [corpus] "Noisy labels that can degrade prompt learning performance" - indicates noisy labels are a known problem but doesn't specifically support this mechanism
- Break condition: If the pre-trained text encoder's class embeddings are poor quality or not semantically meaningful, the regularization effect would be weak.

### Mechanism 2
- Claim: The pre-trained image-text embedding provides robust priors for image classification, compensating for degraded data structure due to label noise.
- Mechanism: CLIP's pre-trained embedding space encodes rich semantic relationships learned from diverse web data. When label noise corrupts the data structure, these priors help maintain classification performance by providing a stable reference.
- Core assumption: The pre-trained embedding space captures generalizable semantic relationships that remain useful even when labels are corrupted.
- Evidence anchors:
  - [abstract] "the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification"
  - [section 4.2] "We observe that this robustness stems from the combination between Prompt Tuning and GCE, and not from GCE alone" - suggests the embedding itself provides inherent robustness
  - [corpus] Weak - mentions noisy labels as a problem but doesn't specifically support this mechanism
- Break condition: If the downstream task domain is very different from CLIP's pre-training data, the priors may not generalize well.

### Mechanism 3
- Claim: Prompt tuning can suppress gradients from noisy samples while aggregating gradients from clean samples.
- Mechanism: By learning only shared prompt tokens (not class-specific ones), prompt tuning creates a constrained optimization problem that naturally focuses on clean samples. This reduces the impact of noisy gradients during training.
- Core assumption: The optimization constraints in prompt tuning create selective gradient aggregation.
- Evidence anchors:
  - [section 4.4] "prompt tuning displays significantly lower ratios than linear probing" when measuring noisy-to-clean gradient norm ratio
  - [section 4.2] "This property likely arises from the highly constrained prompt tuning optimization, which restricting the model to fit the noisy labels"
  - [corpus] "Noisy labels that can degrade prompt learning performance" - indicates noisy labels are problematic but doesn't specifically support this mechanism
- Break condition: If the noise rate is extremely high (>50%), the clean sample gradient signal may be too weak to dominate.

## Foundational Learning

- Concept: Cross-entropy loss and its relationship to gradient magnitude
  - Why needed here: Understanding why prompt tuning is robust requires knowing how different loss functions handle noisy labels and their gradient properties
  - Quick check question: Why does cross-entropy loss tend to overfit to noisy labels in deep networks?

- Concept: Pre-trained vision-language embeddings and semantic relationships
  - Why needed here: The robustness mechanism relies on CLIP's learned semantic relationships between concepts
  - Quick check question: How does CLIP's text encoder encode relationships between class names like "dog" and "cat"?

- Concept: Gradient-based optimization and regularization effects
  - Why needed here: Understanding how fixed tokens provide regularization requires knowledge of how gradients flow through different components
  - Quick check question: What happens to the gradient magnitude when you fix certain parameters during optimization?

## Architecture Onboarding

- Component map: Visual encoder (ResNet/ViT) -> Learnable prompt tokens -> Text encoder (Transformer) -> Classname tokens -> Similarity computation -> Loss function
- Critical path: Image → Visual encoder → Embedding → Similarity computation → Class posterior → Loss → Gradient update (only prompt tokens)
- Design tradeoffs:
  - More learnable tokens → more capacity but less robustness
  - GCE loss parameter q → trade-off between robustness and performance
  - Fixed vs learned classname tokens → regularization vs flexibility
- Failure signatures:
  - High noisy-to-clean gradient ratio → optimization dominated by noise
  - Performance degradation with context length → overfitting to noise
  - Poor results with confusion noise → structure in noise overwhelms priors
- First 3 experiments:
  1. Compare noisy-to-clean gradient ratios between prompt tuning and linear probing on a simple dataset
  2. Test robustness with different numbers of learnable tokens (Ctx-0, Ctx-1, Ctx-2, etc.)
  3. Evaluate performance with random vs confusion label noise at 50% rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal context length for prompt tuning in different datasets and noise levels?
- Basis in paper: [explicit] The paper mentions that the optimal context length is dataset-dependent and all context lengths achieve superior performance compared to traditional linear probing.
- Why unresolved: The paper only provides a brief mention of context length and does not provide a detailed analysis of the optimal context length for different datasets and noise levels.
- What evidence would resolve it: A comprehensive study that tests different context lengths on various datasets and noise levels, and identifies the optimal context length for each case.

### Open Question 2
- Question: How does the performance of prompt tuning change when using different backbone architectures?
- Basis in paper: [explicit] The paper mentions that the noise robustness of prompt tuning is backbone-agnostic and that both ResNet-50 and ViT-B/32 achieve competitive performance at high noise rates.
- Why unresolved: The paper only compares two backbone architectures and does not provide a detailed analysis of how the performance of prompt tuning changes when using different backbone architectures.
- What evidence would resolve it: A comprehensive study that tests prompt tuning with various backbone architectures on different datasets and noise levels, and compares their performance.

### Open Question 3
- Question: How does the performance of prompt tuning change when using different types of label noise?
- Basis in paper: [explicit] The paper mentions that prompt tuning is robust to different types of label noise, including random noise and confusion noise.
- Why unresolved: The paper only provides a brief mention of different types of label noise and does not provide a detailed analysis of how the performance of prompt tuning changes when using different types of label noise.
- What evidence would resolve it: A comprehensive study that tests prompt tuning with various types of label noise on different datasets and noise levels, and compares their performance.

## Limitations

- Empirical evidence is based on synthetic noise rather than realistic label noise distributions found in real-world datasets
- Findings may not generalize to vision-language models beyond CLIP or domains with different pre-training data distributions
- Theoretical explanation for fixed tokens providing regularization is intuitive but not rigorously proven
- Unsupervised prompt tuning relies on CLIP's zero-shot performance being good enough, which may not hold for out-of-distribution domains

## Confidence

**High Confidence Claims:**
- Prompt tuning with fixed class tokens shows better robustness to random label noise than full fine-tuning or linear probing (supported by direct experimental comparison)
- The combination of prompt tuning with GCE loss provides the best robustness (supported by ablation studies)
- CLIP's zero-shot predictions can be used for unsupervised prompt tuning (demonstrated with clear accuracy improvements)

**Medium Confidence Claims:**
- Fixed class tokens provide regularization through pre-trained semantic relationships (supported by ablation but mechanism not rigorously proven)
- Pre-trained image-text embeddings provide robust priors (supported by cross-dataset experiments but theoretical basis weak)
- Prompt tuning naturally suppresses noisy gradients while aggregating clean ones (gradient analysis supports this but alternative explanations possible)

**Low Confidence Claims:**
- These mechanisms will generalize to other vision-language models beyond CLIP
- The findings apply equally to non-random noise patterns (confusion, structured noise)
- The same robustness properties hold at noise rates above 50%

## Next Checks

1. **Generalization to Real-World Noise**: Test the robustness claims on datasets with realistic label noise (e.g., WebVision, Clothing1M) rather than synthetic random noise to validate if the findings hold in practical scenarios.

2. **Theoretical Analysis of Regularization**: Conduct a rigorous mathematical analysis of how fixed class tokens affect the loss landscape and gradient dynamics during optimization, moving beyond empirical correlation to establish causal mechanisms.

3. **Cross-Model Validation**: Apply the same experimental protocol to other vision-language models (e.g., ALIGN, BASIC, OpenCLIP) to determine if the robustness properties are specific to CLIP's architecture or generalizable to the prompt tuning paradigm.