---
ver: rpa2
title: Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty
  for machine learning regression ?
arxiv_id: '2310.11978'
source_url: https://arxiv.org/abs/2310.11978
tags:
- scaling
- isotonic
- calibration
- regression
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Binwise Variance Scaling (BVS) as a post hoc
  recalibration method for machine learning regression uncertainties, testing its
  ability to improve both consistency (calibration conditional on uncertainty) and
  adaptivity (calibration conditional on input features). The method uses binning
  schemes based on either uncertainty values or input features to estimate local scaling
  factors for prediction uncertainties.
---

# Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?

## Quick Facts
- arXiv ID: 2310.11978
- Source URL: https://arxiv.org/abs/2310.11978
- Authors: 
- Reference count: 29
- This study evaluates Binwise Variance Scaling (BVS) as a post hoc recalibration method for machine learning regression uncertainties, testing its ability to improve both consistency (calibration conditional on uncertainty) and adaptivity (calibration conditional on input features).

## Executive Summary
This study evaluates Binwise Variance Scaling (BVS) as a post hoc recalibration method for machine learning regression uncertainties, testing its ability to improve both consistency (calibration conditional on uncertainty) and adaptivity (calibration conditional on input features). Applied to a QM9 dataset of atomization energies, BVS was compared with isotonic regression across multiple bin numbers and loss functions. Results showed that while BVS could improve consistency on the training set, this improvement did not fully transfer to the test set. More critically, BVS failed to achieve both consistency and adaptivity simultaneously. When optimized for adaptivity, consistency was lost, and when optimized for consistency, adaptivity remained poor. Alternative binning schemes and loss functions showed some promise but ultimately could not overcome these fundamental limitations. The study concludes that simple post hoc methods like BVS are unlikely to achieve the dual goals of consistency and adaptivity required for reliable individual predictions, suggesting that more sophisticated approaches targeting these properties during model training may be necessary.

## Method Summary
The study evaluates Binwise Variance Scaling (BVS) as a post hoc recalibration method for machine learning regression uncertainties, testing its ability to improve both consistency (calibration conditional on uncertainty) and adaptivity (calibration conditional on input features). The method uses binning schemes based on either uncertainty values or input features to estimate local scaling factors for prediction uncertainties. Applied to a QM9 dataset of atomization energies, BVS was compared with isotonic regression across multiple bin numbers and loss functions. Results showed that while BVS could improve consistency on the training set, this improvement did not fully transfer to the test set. More critically, BVS failed to achieve both consistency and adaptivity simultaneously.

## Key Results
- BVS improved consistency on training sets but showed poor transfer to test sets
- When optimized for adaptivity, consistency was lost; when optimized for consistency, adaptivity remained poor
- Alternative binning schemes and loss functions showed some promise but could not overcome fundamental limitations
- Simple post hoc methods like BVS are unlikely to achieve both consistency and adaptivity required for reliable individual predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Binwise Variance Scaling (BVS) improves consistency by locally rescaling uncertainties within bins where z-scores deviate from 1.
- Mechanism: The method splits the dataset into equal-size bins based on uncertainty values, computes local z-scores mean squares (ZMS), and applies scaling factors \( s_i = v_i^{-1/2} \) to adjust uncertainties so that ZMS ≈ 1 within each bin.
- Core assumption: Local calibration within bins implies global consistency; errors and uncertainties are sufficiently correlated within bins.
- Evidence anchors:
  - [abstract] "BVS uses binning schemes based on either uncertainty values or input features to estimate local scaling factors for prediction uncertainties."
  - [section II.A] "The local calibration within bin i can be assessed by the z-scores mean square (ZMS)... A corrective scaling factor is thoroughly obtained as \( s_i = v_i^{-1/2} \) to be applied to uncertainties falling within the limits of the corresponding bin."
- Break condition: If the correlation between errors and uncertainties is weak or non-monotonic across bins, local scaling will fail to generalize.

### Mechanism 2
- Claim: Using input-feature-based binning can improve adaptivity by rescaling uncertainties conditional on feature values.
- Mechanism: Instead of binning by uncertainty, the method bins by input features (e.g., molecular mass X1), then applies local scaling factors within each feature bin to adjust uncertainties so that calibration holds conditional on that feature.
- Core assumption: Errors depend systematically on input features; hence, local scaling within feature bins can correct adaptivity.
- Evidence anchors:
  - [abstract] "The method uses binning schemes based on either uncertainty values or input features to estimate local scaling factors."
  - [section II.D] "For a correct estimation of fv,x, it is important to ensure a good balance between the bin size... and the number of bins."
- Break condition: If feature bins are too small or if features are weakly correlated with errors, scaling factors will be unreliable or overfit.

### Mechanism 3
- Claim: Alternative loss functions (Scon, Sada, Stot) guide BVS optimization toward consistency, adaptivity, or both.
- Mechanism: The study defines loss functions combining Scal (average calibration), Su (consistency), and SXi (adaptivity) scores, then optimizes BVS scaling factors to minimize these losses.
- Core assumption: The chosen loss function can steer the optimization to balance consistency and adaptivity.
- Evidence anchors:
  - [section II.B] "consistency can be targeted by minimizing Scon = Scal + Su while adaptivity can be targeted with Sada = Scal + ∑ SXi."
  - [section III.C.1] "Optimization of scaling factors provided by Eq. 2 was attempted for u- and X1-based binning schemes and for the NLL, Stot, Scon and Sada loss functions."
- Break condition: If the loss function overweights one objective, the other may be neglected, leading to imbalance.

## Foundational Learning

- Concept: Z-scores and their mean squares (ZMS) as calibration metrics.
  - Why needed here: BVS relies on ZMS = 1 for perfect local calibration; understanding ZMS is essential to interpret scaling corrections.
  - Quick check question: If ZMS = 2 in a bin, what scaling factor is applied to uncertainties in that bin?
    - Answer: \( s_i = 2^{-1/2} \approx 0.707 \), reducing uncertainties by ~30%.

- Concept: Confidence intervals for validation of calibration (fv metric).
  - Why needed here: fv measures the fraction of bins whose ZMS confidence intervals contain the target value; it validates whether BVS achieves consistency/adaptivity.
  - Quick check question: What fv value indicates good calibration?
    - Answer: fv ≈ 0.95, meaning ~95% of bins have CIs containing the target.

- Concept: Overparameterization risk in binning.
  - Why needed here: Too many bins can overfit the training data and hurt generalization to test data; the study explores bin numbers from 2 to 80.
  - Quick check question: What happens to BVS performance as bin number increases beyond ~40?
    - Answer: Average calibration (Scal) degrades, and overfitting risk increases.

## Architecture Onboarding

- Component map: Input data (E, u, X) -> Binning module -> Scaling factor estimator -> Calibration validator -> Optimizer
- Critical path:
  1. Sort data by binning variable (u or X)
  2. Partition into NB bins
  3. Compute local ZMS and scaling factors
  4. Apply scaling to uncertainties
  5. Evaluate calibration scores on train/test sets
  6. Iterate with different NB and loss functions
- Design tradeoffs:
  - Few bins → poor local correction but better generalization
  - Many bins → better local fit but risk overfitting and poor average calibration
  - u-based binning → targets consistency
  - X-based binning → targets adaptivity but may not transfer well
- Failure signatures:
  - Scal increases with bin number → overfitting
  - fv,x << 0.95 for consistency or adaptivity → calibration failure
  - Transfer gap: training set fv close to 0.95 but test set fv much lower → overfitting
- First 3 experiments:
  1. Run BVS with NB=15 (Frenkel & Goldberger's choice) on QM9, compare fv,u to isotonic regression
  2. Test X1-based binning with NB=20, evaluate fv,X1 on training and test sets
  3. Optimize BVS with Sada loss and NB=80, check if fv,X1 improves while fv,u remains acceptable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions (dataset characteristics, model architectures, binning strategies) can post hoc methods like BVS achieve both consistency and adaptivity simultaneously?
- Basis in paper: [explicit] The paper concludes that "it is unlikely that simple post hoc recalibration methods such as isotonic regression or BVS will be able to ensure both the consistency and adaptivity levels required for reliable individual predictions"
- Why unresolved: The paper tested BVS on a specific QM9 dataset with certain binning strategies and loss functions, but didn't exhaustively explore all possible conditions that might enable dual calibration
- What evidence would resolve it: Systematic testing of BVS across diverse datasets with varying feature correlations, different molecular properties, and alternative binning schemes beyond the tested X1 and u variables

### Open Question 2
- Question: What is the theoretical relationship between the number of bins in BVS and the achievable calibration performance trade-off between consistency and adaptivity?
- Basis in paper: [explicit] The paper observes that "Increasing the bin number to 40 and 80 enables an improvement in all scores at the learning stage and some differentiation between the loss functions, notably when considering adaptivity scores. However, gains in adaptivity are contrasted by losses in consistency"
- Why unresolved: While the paper tests different bin numbers (20, 40, 80), it doesn't establish a mathematical framework for understanding how bin number affects the consistency-adaptivity trade-off
- What evidence would resolve it: A theoretical analysis deriving the optimal bin number as a function of dataset size and feature correlation structure, validated through extensive empirical testing

### Open Question 3
- Question: Can alternative binning variables beyond uncertainty values and molecular mass (X1) enable BVS to achieve both consistency and adaptivity simultaneously?
- Basis in paper: [explicit] The paper tested "alternative binning schemes" including X1-based binning and 2D binning (u, X1), but found these "do not bring any notable improvement to the results presented below"
- Why unresolved: The paper only tested a limited set of binning variables and didn't explore the full space of possible input features or combinations that might correlate with both errors and uncertainty
- What evidence would resolve it: Systematic testing of BVS with various molecular descriptors, topological features, or learned embeddings as binning variables, comparing performance across diverse molecular property prediction tasks

## Limitations
- The study focuses on a single dataset (QM9 atomization energies) with specific prediction models, limiting generalizability
- Only two input features (molecular mass and heteroatom fraction) were tested for adaptivity, potentially missing other important correlates
- The binning approach assumes monotonic relationships between errors and binning variables, which may not hold for all ML regression tasks

## Confidence
- High confidence: The empirical results are well-documented with clear performance metrics and show consistent patterns across training and test sets
- Medium confidence: The mechanistic explanation that simple post-hoc binning methods are inherently limited in achieving dual calibration properties
- Low confidence: The absolute necessity of sophisticated training-time approaches, as the study does not comprehensively test hybrid methods or alternative calibration frameworks

## Next Checks
1. Test BVS on diverse datasets (e.g., material properties, physical simulations) to assess generalizability of the consistency-adaptivity trade-off
2. Experiment with adaptive binning schemes that detect and handle non-monotonic error distributions within bins
3. Compare BVS against calibration methods that incorporate feature importance weighting during the scaling factor optimization