---
ver: rpa2
title: Machine Reading Comprehension using Case-based Reasoning
arxiv_id: '2305.14815'
source_url: https://arxiv.org/abs/2305.14815
tags:
- answer
- question
- cbr-mrc
- questions
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CBR-MRC, a semi-parametric method for machine
  reading comprehension that explicitly reasons over a set of retrieved cases from
  a nonparametric memory. The core idea is that contextualized answers to similar
  questions share semantic similarities.
---

# Machine Reading Comprehension using Case-based Reasoning

## Quick Facts
- arXiv ID: 2305.14815
- Source URL: https://arxiv.org/abs/2305.14815
- Reference count: 25
- Key outcome: CBR-MRC achieves 11.5 EM improvement on NaturalQuestions and 8.4 EM on NewsQA compared to strong baselines

## Executive Summary
This paper introduces CBR-MRC, a semi-parametric machine reading comprehension method that uses case-based reasoning to retrieve and compare with similar past cases. The core insight is that contextualized answers to similar questions share semantic similarities, allowing the model to predict answers by selecting spans in the test context most similar to retrieved case answers. The approach provides interpretable predictions by attributing them to specific evidence cases and demonstrates robustness to lexical diversity in contexts, outperforming fully-parametric methods.

## Method Summary
CBR-MRC operates by first encoding a test question and retrieving similar cases from a casebase using question embeddings. It then extracts candidate answer spans from the test context and compares these spans with contextualized answer representations from the retrieved cases using inner products. The candidate span with the highest aggregated similarity score is selected as the predicted answer. The model is trained using a contrastive loss that maximizes similarity between answer span embeddings of similar questions while minimizing similarity for non-answer spans.

## Key Results
- Achieves 11.5 EM improvement on NaturalQuestions compared to strong baselines
- Outperforms fully-parametric methods by 8.4 EM on NewsQA
- Demonstrates superior ability to identify correct supporting evidence with higher span-EM scores
- Shows robustness to lexical diversity in contexts where fully-parametric methods degrade

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CBR-MRC leverages the hypothesis that contextualized answers to similar questions share semantic similarities, enabling retrieval of semantically similar cases to predict answers.
- Mechanism: The model retrieves similar cases from a casebase using question embeddings, then predicts the answer by selecting the span in the test context most similar to the contextualized representations of answers in the retrieved cases.
- Core assumption: Contextualized answers to similar questions share representational similarities in dense embeddings obtained from language models.
- Break condition: If the question embeddings do not capture the latent relations accurately, the similarity function may retrieve irrelevant cases, leading to incorrect predictions.

### Mechanism 2
- Claim: CBR-MRC's semi-parametric nature allows it to attribute predictions to specific evidence cases, making it interpretable and debuggable.
- Mechanism: The model explicitly reasons over a set of retrieved cases from a nonparametric memory, and the prediction is based on the most similar span in the test context to the contextualized representations of answers in the retrieved cases.
- Core assumption: The semi-parametric nature of CBR-MRC allows for transparent inference procedures that can clearly attribute predictions to specific instances used in the decision-making process.
- Break condition: If the retrieved cases are not relevant or the contextualized representations are not accurate, the attribution of predictions to specific evidence cases may be misleading.

### Mechanism 3
- Claim: CBR-MRC is robust to lexical diversity in contexts, outperforming fully-parametric methods.
- Mechanism: The model performs answer extraction by explicitly comparing span similarities of the target context with the context spans in the filtered casebase, allowing it to handle varying levels of lexical diversity in contexts for the same latent relations.
- Core assumption: As lexical diversity increases, fully-parametric models must rely on learned parameters to handle all variations, while CBR-MRC can partially shift this burden to test time by grounding itself using retrieved cases.
- Break condition: If the retrieved cases do not cover the full range of lexical diversity or the comparison mechanism is not robust, the model may still struggle with highly diverse contexts.

## Foundational Learning

- Concept: Latent relations between questions and answers
  - Why needed here: CBR-MRC relies on the overlap of latent relations between questions to learn a similarity function and retrieve similar cases.
  - Quick check question: How does CBR-MRC identify the latent relations between questions and answers without explicit annotations?

- Concept: Contextualized embeddings and semantic similarity
  - Why needed here: CBR-MRC uses contextualized embeddings of answers from retrieved cases to find semantic overlap within the target context.
  - Quick check question: What role do contextualized embeddings play in CBR-MRC's prediction mechanism?

- Concept: Case-based reasoning and semi-parametric approaches
  - Why needed here: CBR-MRC adapts case-based reasoning from classical AI and uses a semi-parametric approach to allow for transparent inference procedures and robustness to lexical diversity.
  - Quick check question: How does CBR-MRC's semi-parametric nature contribute to its interpretability and robustness compared to fully-parametric methods?

## Architecture Onboarding

- Component map: Casebase -> Case retrieval (BERT-based encoder) -> Case reuse (dense retriever) -> Contrastive loss fine-tuning -> Answer prediction
- Critical path:
  1. Encode the test question and retrieve similar cases from the casebase.
  2. Extract candidate answer spans from the test context.
  3. Compare the candidate spans with answer representations from the retrieved cases using inner products.
  4. Select the candidate span with the highest aggregated similarity score as the predicted answer.
- Design tradeoffs:
  - Tradeoff between the number of retrieved cases (k) and performance: Increasing k may enrich the set of contexts for referencing but can also introduce irrelevant cases that degrade performance.
  - Tradeoff between interpretability and performance: The semi-parametric nature of CBR-MRC allows for transparent inference procedures but may require more computational resources compared to fully-parametric methods.
- Failure signatures:
  - Incorrect predictions due to irrelevant cases being retrieved or contextualized representations not accurately capturing the latent relations.
  - Poor performance on questions with high lexical diversity in contexts if the retrieved cases do not cover the full range of variations.
  - Overfitting to the answer tokens instead of focusing on getting the correct answer, leading to lower F1 scores.
- First 3 experiments:
  1. Evaluate the performance of CBR-MRC on a held-out test set to measure its accuracy in extracting answers and identifying supporting evidence.
  2. Analyze the model's robustness to lexical diversity by clustering questions based on latent relations and computing n-gram statistics across passage contexts.
  3. Test the model's ability to transfer to new domains with limited labeled data by evaluating its performance on out-of-domain datasets like BioASQ and RelEx.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CBR-MRC's performance scale with the size of the casebase, and what is the optimal balance between casebase size and retrieval efficiency?
- Basis in paper: [inferred] The paper mentions that CBR-MRC's casebase is typically set to the full training data and may be continually augmented. It also discusses the effect of retrieval quantity on performance, finding that too many retrievals can introduce noise.
- Why unresolved: The paper does not provide an analysis of how CBR-MRC's performance scales with different casebase sizes or investigate the trade-off between casebase size and retrieval efficiency.
- What evidence would resolve it: Conducting experiments with varying casebase sizes and analyzing the impact on performance, retrieval time, and memory usage would help determine the optimal casebase size for CBR-MRC.

### Open Question 2
- Question: How does CBR-MRC handle questions with multiple correct answers or ambiguous contexts, and can it be extended to provide a ranked list of potential answers?
- Basis in paper: [explicit] The paper mentions that CBR-MRC predicts a single answer span for each question, and it reports exact match (EM) and F1 scores as evaluation metrics. However, it does not discuss how the model handles questions with multiple correct answers or ambiguous contexts.
- Why unresolved: The paper focuses on CBR-MRC's ability to predict a single answer span and does not explore its performance in handling questions with multiple correct answers or ambiguous contexts.
- What evidence would resolve it: Investigating CBR-MRC's performance on questions with multiple correct answers or ambiguous contexts and evaluating its ability to provide a ranked list of potential answers would help understand its limitations and potential extensions.

### Open Question 3
- Question: How does CBR-MRC's performance compare to other interpretable machine reading comprehension methods, such as those based on attention mechanisms or graph networks?
- Basis in paper: [inferred] The paper discusses the interpretability of CBR-MRC by attributing predictions to specific evidence cases. It also mentions that previous methods have used attention mechanisms and graph networks to capture attended parts of text or relations.
- Why unresolved: The paper does not provide a direct comparison between CBR-MRC and other interpretable machine reading comprehension methods, such as those based on attention mechanisms or graph networks.
- What evidence would resolve it: Conducting experiments comparing CBR-MRC's performance and interpretability with other interpretable machine reading comprehension methods would help determine its strengths and weaknesses relative to alternative approaches.

## Limitations
- Performance heavily depends on the quality of question and answer embeddings, which may not capture true latent relations
- Casebase construction and maintenance are underspecified, yet critical for retrieval quality
- The boundaries of CBR-MRC's robustness to lexical diversity are not clearly defined, with potential thresholds beyond which the approach struggles

## Confidence
- **High Confidence**: The semi-parametric design allowing interpretable predictions is well-supported by the methodology description and aligns with established case-based reasoning principles.
- **Medium Confidence**: The performance improvements on benchmark datasets are demonstrated, but the extent to which these generalize to more diverse or challenging question types remains uncertain.
- **Medium Confidence**: The claimed robustness to lexical diversity is supported by experiments, but the specific mechanisms and boundaries of this robustness need further validation.

## Next Checks
1. **Ablation Study on Embedding Quality**: Systematically degrade the quality of question and answer embeddings (e.g., using smaller models, fewer layers) to quantify how sensitive CBR-MRC's performance is to embedding quality, establishing the true limits of the approach.

2. **Casebase Size and Diversity Analysis**: Conduct experiments varying the size and diversity of the casebase to determine the minimum requirements for effective retrieval, and identify the point at which adding more cases yields diminishing returns or even degrades performance.

3. **Out-of-Distribution Generalization Test**: Evaluate CBR-MRC on questions that fall outside the semantic distribution of the training casebase (e.g., entirely new question types or domains) to assess whether the method can truly generalize beyond its training distribution or if it's primarily memorizing patterns from similar cases.