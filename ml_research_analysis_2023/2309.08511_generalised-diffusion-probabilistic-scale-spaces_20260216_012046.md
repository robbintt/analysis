---
ver: rpa2
title: Generalised Diffusion Probabilistic Scale-Spaces
arxiv_id: '2309.08511'
source_url: https://arxiv.org/abs/2309.08511
tags:
- diffusion
- image
- osmosis
- process
- probabilistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a generalised scale-space theory for diffusion
  probabilistic models, bridging theoretical foundations with practical applications.
  It extends the scale-space framework to probability distributions, demonstrating
  that probabilistic diffusion models fulfill key architectural properties such as
  semigroup, Lyapunov sequences, and permutation invariance.
---

# Generalised Diffusion Probabilistic Scale-Spaces

## Quick Facts
- arXiv ID: 2309.08511
- Source URL: https://arxiv.org/abs/2309.08511
- Reference count: 40
- Primary result: Establishes theoretical scale-space properties for diffusion probabilistic models and connects them to classical image filtering methods

## Executive Summary
This work introduces a generalised scale-space theory for diffusion probabilistic models (DPMs), bridging theoretical foundations with practical applications. The authors extend classical scale-space concepts to probability distributions, demonstrating that DPMs fulfill key architectural properties including semigroup behavior, Lyapunov sequences via entropy increase, and permutation invariance. They establish conceptual and empirical connections between DPMs and classical image filtering techniques like homogeneous diffusion and osmosis filtering, showing that recent advances in diffusion models approximate these traditional methods. Experimental comparisons validate these theoretical findings, particularly showing that blurring diffusion models closely resemble osmosis filtering trajectories.

## Method Summary
The authors establish scale-space properties for DPMs by analyzing the forward diffusion process through Markov transition probabilities. They prove semigroup properties by showing recursive construction of coarser scales from finer ones, demonstrate Lyapunov sequences through monotonic entropy increase under specific parameter conditions, and verify permutation invariance of transition probabilities. The work connects DPMs to classical filters by deriving frequency-domain formulations and comparing trajectories in both spatial and DCT domains. Experimental validation uses the BSDS500 dataset to compare image variance evolution and FID scores across five models: DPM, inverse heat dissipation, homogeneous diffusion, blurring diffusion, and probabilistic osmosis.

## Key Results
- DPM forward processes form semigroups, enabling recursive scale construction from initial distributions
- Differential entropy increases monotonically during diffusion under specific αiβi parameter conditions
- Blurring diffusion models approximate osmosis filtering through frequency-dependent transitions in the DCT domain
- Experimental validation shows blurring diffusion closely resembles osmosis filtering trajectories on BSDS500 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The forward diffusion process in DPM forms a semigroup, enabling recursive construction of coarser scales from finer ones
- Mechanism: Each time step's distribution can be reached either directly from the initial distribution in i steps or via intermediate scales, due to the Markov property
- Core assumption: The transition probabilities follow a Markov process where each state depends only on the previous state
- Evidence anchors:
  - [section]: "The distribution p(ui) at scale i can be reached equivalently in i steps from p(u0) or in ℓ steps from p(ui−ℓ)"
  - [abstract]: "It extends the scale-space framework to probability distributions, demonstrating that probabilistic diffusion models fulfill key architectural properties such as semigroup"
  - [corpus]: Weak evidence - no direct mentions of semigroup properties in related papers
- Break condition: If transition probabilities violate the Markov property (state depends on more than just previous state)

### Mechanism 2
- Claim: Entropy increases monotonically during the forward diffusion process, quantifying information reduction
- Mechanism: The differential entropy of the probability distribution increases with each diffusion step under specific conditions on αi and βi parameters
- Core assumption: Gaussian noise addition preserves the Markov property and entropy increase conditions are met
- Evidence anchors:
  - [section]: "The differential entropy Hp(Ui) increases with ti under the assumption αiβi ≥ (2πe)−n/2"
  - [abstract]: "demonstrating that probabilistic diffusion models fulfill key architectural properties such as...Lyapunov sequences"
  - [corpus]: Weak evidence - no direct mentions of entropy-based Lyapunov sequences in related papers
- Break condition: If noise parameters αi or βi violate the entropy preservation conditions

### Mechanism 3
- Claim: Blurring diffusion models approximate osmosis filtering through deterministic blur combined with noise
- Mechanism: The DCT-domain formulation of blurring diffusion creates frequency-dependent transitions that closely match osmosis trajectories
- Core assumption: The diffusion operator preserves average grey value while the drift field drives convergence to noise
- Evidence anchors:
  - [section]: "blurring diffusion closely resembles osmosis filtering" and "Using such an approximation instead of directly applying 2-D osmosis is convenient"
  - [abstract]: "Moreover, we show conceptual and empirical connections to diffusion and osmosis filters"
  - [corpus]: Weak evidence - no direct mentions of osmosis filtering connections in related papers
- Break condition: If frequency-dependent parameters don't match between models or if the drift field isn't properly constructed

## Foundational Learning

- Concept: Markov processes and transition probabilities
  - Why needed here: The entire scale-space theory relies on the Markov property for the forward diffusion process
  - Quick check question: Can you explain why each diffusion step depends only on the previous state, not the entire history?

- Concept: Scale-space theory and Lyapunov sequences
  - Why needed here: The work generalizes classical scale-space concepts to probability distributions using entropy as a measure of simplification
  - Quick check question: How does increasing differential entropy relate to the concept of information reduction in traditional scale-spaces?

- Concept: Fokker-Planck equation and drift-diffusion physics
  - Why needed here: Understanding the physical motivation connects DPM to osmosis filtering and classical image processing
  - Quick check question: What role does the drift vector field play in the connection between DPM and osmosis filters?

## Architecture Onboarding

- Component map: Forward process (Markov chain with Gaussian transitions) -> Scale-space evolution (probability distributions) -> Entropy-based Lyapunov sequences -> Connection to classical filters (diffusion, osmosis)
- Critical path: Define transition probabilities -> Establish semigroup property -> Prove entropy increase -> Compare to classical scale-spaces
- Design tradeoffs: Noise-only vs. blur+noise forward processes (simplicity vs. better image quality), frequency-dependent vs. uniform parameters (accuracy vs. computational efficiency)
- Failure signatures: Violation of Markov property breaks semigroup property; incorrect noise parameters break entropy increase; poor parameter choices lead to bad FID scores
- First 3 experiments:
  1. Implement forward DPM with varying noise schedules and measure entropy increase to verify Lyapunov property
  2. Compare blurring diffusion trajectories to homogeneous diffusion and osmosis in spatial domain to validate visual similarity
  3. Compute FID scores between intermediate distributions of different models to quantify approximation quality to osmosis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the scale-space properties of probabilistic diffusion models be leveraged to improve existing deep learning architectures?
- Basis in paper: [explicit] The paper establishes scale-space properties for probabilistic diffusion models, including semigroup, Lyapunov sequences, and permutation invariance, and suggests potential applications.
- Why unresolved: While the paper identifies these properties, it does not provide concrete methods for integrating them into existing deep learning models or quantify their impact on performance.
- What evidence would resolve it: Experimental results comparing deep learning models with and without the incorporation of scale-space properties, demonstrating improvements in tasks like image generation or classification.

### Open Question 2
- Question: What are the specific mechanisms by which the choice of forward process degradation (e.g., noise vs. blur) affects the quality and diversity of generated images in diffusion models?
- Basis in paper: [explicit] The paper mentions that the choice of degradation in the forward process significantly impacts generative performance and that blurring diffusion models approximate osmosis filtering.
- Why unresolved: The paper provides theoretical connections but does not experimentally isolate and quantify the effects of different degradation mechanisms on image quality metrics or diversity measures.
- What evidence would resolve it: Controlled experiments varying the forward process degradation type and measuring its impact on established image generation benchmarks like FID or IS.

### Open Question 3
- Question: Can the probabilistic osmosis process be directly implemented as a deep learning model, and how would it compare to existing diffusion models in terms of training efficiency and sample quality?
- Basis in paper: [inferred] The paper proposes a probabilistic osmosis process as a theoretical alternative to existing diffusion models and notes its close resemblance to blurring diffusion models.
- Why unresolved: The paper does not explore the practical implementation of probabilistic osmosis as a deep learning model or benchmark it against current state-of-the-art diffusion models.
- What evidence would resolve it: Implementation of a probabilistic osmosis-based diffusion model and comparison of its training dynamics, sample quality, and computational efficiency against leading diffusion models on standard datasets.

## Limitations

- Limited quantitative validation of scale-space properties beyond theoretical proofs
- DCT implementation details for inverse heat dissipation and blurring diffusion are not fully specified
- Connection to classical filters relies primarily on visual comparisons rather than comprehensive metrics
- Experimental scope restricted to BSDS500 dataset without broader generalization testing

## Confidence

- High: Semigroup property of forward diffusion process (well-established Markov chain theory)
- Medium: Entropy-based Lyapunov sequences (requires specific parameter conditions)
- Low: Empirical equivalence to classical diffusion filters (limited quantitative validation)

## Next Checks

1. Implement a controlled experiment varying αi and βi parameters systematically to verify the entropy increase conditions for Lyapunov sequences
2. Conduct quantitative comparison of frequency spectra between blurring diffusion and osmosis filtering to validate the claimed approximation quality
3. Perform ablation studies on the DCT implementation details to identify critical factors for matching classical filter behavior