---
ver: rpa2
title: 'Tokenization Preference for Human and Machine Learning Model: An Annotation
  Study'
arxiv_id: '2304.10813'
source_url: https://arxiv.org/abs/2304.10813
tags:
- tokenization
- annotation
- machine
- human
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the effect of tokenization methods on the performance
  of humans and machine learning models in a Japanese commonsense question answering
  task. Six different tokenizers are used to tokenize the questions, and the accuracy
  of human annotators and ML models (Bag-of-Words and BiLSTM) are measured.
---

# Tokenization Preference for Human and Machine Learning Model: An Annotation Study

## Quick Facts
- arXiv ID: 2304.10813
- Source URL: https://arxiv.org/abs/2304.10813
- Reference count: 12
- Primary result: BPE and Unigram tokenizers based on language models perform best for both humans and machines in Japanese QA tasks

## Executive Summary
This study investigates how different tokenization methods affect performance for both humans and machine learning models in Japanese commonsense question answering. The researchers compared six tokenization approaches (MeCab, Unigram, BPE, MaxMatch, OpTok, Random) across multiple dimensions including accuracy, appropriateness ranking, and response time. The key finding is that tokenization methods using language models (BPE and Unigram) provide the best balance between human readability and machine learning performance, challenging the assumption that human-friendly tokenization always benefits ML models. The study reveals that human preferences for tokenization do not necessarily align with optimal machine learning performance.

## Method Summary
The researchers used the JCommonsenseQA dataset from JGLUE and Japanese Wikipedia dump for tokenizer training. Six different tokenization methods were applied to the question texts, and human annotators evaluated both the appropriateness of each tokenization and answered questions with shuffled tokens. Machine learning models (Bag-of-Words and BiLSTM) were trained and evaluated on the tokenized data. Performance metrics included human accuracy, response time, appropriateness ranking, and ML accuracy, allowing for direct comparison between human and machine preferences for different tokenization approaches.

## Key Results
- BPE and Unigram tokenizers, which use language models, were most appropriate for humans and led to highest ML model performance
- MeCab, while most appropriate for humans, resulted in second-lowest ML performance
- Random tokenization was most difficult for both humans and machines
- Tokenization length and entropy significantly affected human response time, with longer sequences requiring more time to process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BPE and Unigram tokenizers produce tokenization that is more readable and performant for both humans and machines
- Mechanism: These tokenizers use statistical language models to learn tokenization that reflects natural language usage patterns, creating token boundaries that align with human intuition while providing consistent tokens for ML models
- Core assumption: Language model statistical patterns align with human linguistic intuitions and ML model preferences
- Evidence anchors:
  - [abstract]: "BPE and Unigram, which are based on language models, are more appropriate for humans and also lead to higher performance for machine learning models"
  - [section]: "OpTok and Unigram are the second and third appropriate tokenization"
  - [corpus]: Weak evidence - corpus analysis doesn't directly compare BPE/Unigram performance

### Mechanism 2
- Claim: Tokenization length and entropy affect human readability and response time
- Mechanism: Shorter tokenization sequences with lower entropy are easier for humans to process and reconstruct meaning from, especially when token order is shuffled
- Core assumption: Human cognitive processing load is influenced by number of tokens and their predictability
- Evidence anchors:
  - [section]: "Response time for MeCab tokenization is not short... yields longer sequences"
  - [section]: "Response time for OpTok was the shortest while tokenization length of OpTok is relatively long"
  - [corpus]: Weak evidence - corpus analysis doesn't directly measure human response time

### Mechanism 3
- Claim: ML model performance is not strongly correlated with human-perceived tokenization appropriateness
- Mechanism: ML models learn to map token sequences to answers based on statistical patterns in training data, which may not align with human linguistic intuitions about appropriate word boundaries
- Core assumption: ML models can learn effective representations from tokenization that humans consider inappropriate
- Evidence anchors:
  - [abstract]: "Result also implies that existing methods using language models for tokenization could be a good compromise both for human and ML models"
  - [section]: "Performance of QA models with MeCab tokenization is the second lowest result... while MeCab is the most appropriate tokenization for humans"
  - [corpus]: Weak evidence - corpus analysis doesn't directly compare ML performance with human annotation scores

## Foundational Learning

- Concept: Word segmentation in languages without explicit word boundaries
  - Why needed here: Japanese text lacks whitespace between words, making tokenization methods crucial for both human readability and machine processing
  - Quick check question: How does the lack of whitespace in Japanese text affect the number of possible tokenization options compared to English?

- Concept: Subword tokenization methods (BPE, Unigram, WordPiece)
  - Why needed here: These methods learn tokenization from data rather than relying on predefined dictionaries, potentially creating more effective token boundaries for both humans and machines
  - Quick check question: What is the key difference between dictionary-based tokenization (like MeCab) and subword tokenization methods in terms of how they determine token boundaries?

- Concept: Evaluation metrics for tokenization quality
  - Why needed here: The study uses multiple metrics including accuracy, appropriateness ranking, and response time to evaluate tokenization from different perspectives
  - Quick check question: Why might a tokenization method that scores highly on human appropriateness ranking not necessarily lead to the best machine learning model performance?

## Architecture Onboarding

- Component map: Data preprocessing -> Six tokenization methods -> ML model training -> Human annotation -> Performance comparison
- Critical path: Tokenization → QA model training → Human annotation → Performance comparison
- Design tradeoffs: Simple QA models isolate tokenization effects but may not reflect real-world performance; human annotation introduces potential bias
- Failure signatures: Inconsistent human annotation standards, ML models not sensitive to tokenization quality, tokenization methods not covering full vocabulary
- First 3 experiments:
  1. Replicate the study with a different QA dataset to verify generalizability of findings
  2. Test the effect of tokenization on more complex QA models (e.g., BERT, RoBERTa) to see if trends hold with larger models
  3. Conduct a controlled human study with eye-tracking to measure cognitive load and reading patterns for different tokenization methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the order of tokenization affect human comprehension and response time in a Bag-of-Words setting?
- Basis in paper: [explicit] The paper mentions that the question text is shuffled in the Bag-of-Words setting to compare human and machine performance
- Why unresolved: The paper does not provide a detailed analysis of the impact of token order on human comprehension and response time
- What evidence would resolve it: Conducting an experiment where the token order is varied and measuring human response time and accuracy

### Open Question 2
- Question: How do large language models with different tokenization methods perform on downstream tasks compared to humans?
- Basis in paper: [explicit] The paper suggests that comparing large language models with different tokenization methods could provide more practical insights
- Why unresolved: The paper only compares simple QA models and does not explore the performance of large language models with different tokenization methods
- What evidence would resolve it: Evaluating the performance of large language models with different tokenization methods on downstream tasks and comparing them to human performance

### Open Question 3
- Question: How can the annotation format be improved to obtain more significant differences in human accuracy among different tokenization methods?
- Basis in paper: [explicit] The paper mentions that the dataset is easy to solve even with shuffled tokens, leading to small differences in human accuracy among tokenization methods
- Why unresolved: The paper does not propose specific improvements to the annotation format to elicit more significant differences in human accuracy
- What evidence would resolve it: Conducting experiments with improved annotation formats, such as using a flash card manner or eye-tracking systems, to measure human performance more accurately

## Limitations

- The study focuses on a single Japanese QA dataset which may not generalize to other tasks or domains
- Simple Bag-of-Words and BiLSTM models may not reflect real-world performance with more sophisticated architectures
- Human annotation introduces potential biases through annotation tool design and specific instructions
- The shuffling task used to measure response time may not accurately reflect natural reading patterns

## Confidence

**High Confidence**: BPE and Unigram tokenizers perform well for both humans and machines is supported by multiple metrics and consistent with theoretical advantages of subword tokenization methods.

**Medium Confidence**: Human-perceived tokenization appropriateness does not strongly correlate with ML performance is plausible but may be influenced by specific task and model architectures.

**Medium Confidence**: Tokenization length and entropy significantly affect human response time is supported by data but the mechanism is not fully explained.

## Next Checks

1. Replicate the analysis on a different Japanese NLP task (e.g., sentiment analysis or named entity recognition) to verify whether tokenization preferences observed in QA tasks extend to other domains.

2. Test the effect of tokenization on more sophisticated models like BERT or RoBERTa to determine if observed trends hold when using transformer-based architectures.

3. Conduct a follow-up human study with eye-tracking and cognitive load measurements to validate the relationship between tokenization length, entropy, and processing difficulty.