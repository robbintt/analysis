---
ver: rpa2
title: 'AMuRD: Annotated Arabic-English Receipt Dataset for Key Information Extraction
  and Classification'
arxiv_id: '2309.09800'
source_url: https://arxiv.org/abs/2309.09800
tags:
- information
- extraction
- dataset
- llama
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AMuRD, a novel multilingual dataset for key
  information extraction and item classification from receipts. The dataset contains
  47,720 samples in Arabic and English, annotated with item names, attributes (price,
  brand, etc.), and classification into 44 product categories.
---

# AMuRD: Annotated Arabic-English Receipt Dataset for Key Information Extraction and Classification

## Quick Facts
- arXiv ID: 2309.09800
- Source URL: https://arxiv.org/abs/2309.09800
- Reference count: 38
- Primary result: Introduces AMuRD dataset with 47,720 multilingual receipts; achieves 76% F1 and 68% accuracy using InstructLLaMA approach

## Executive Summary
This paper presents AMuRD, a novel multilingual dataset for key information extraction and item classification from receipts, containing 47,720 annotated samples in both Arabic and English. The dataset includes detailed annotations for item names, attributes (price, brand, weight, etc.), and classification into 44 product categories. The authors propose InstructLLaMA, an approach that fine-tunes LLaMA models on this dataset using instruction tuning with carefully crafted prompts. The system achieves an F1 score of 76% and accuracy of 68% for key information extraction and item classification tasks, with the dataset and code publicly available for further research.

## Method Summary
The AMuRD dataset is created through manual annotation of 47,720 receipt samples in Arabic and English, with detailed item-level annotations for names, attributes (price, brand, weight, units, etc.), and classification into 44 product categories. The InstructLLaMA approach fine-tunes LLaMA models (7B parameters) using instruction tuning with multi-turn prompts that separate instruction, input, and response. The fine-tuning process uses the DeepSpeed framework and Huggingface Transformers, requiring GPU resources (4 Nvidia A100 48GB GPUs). Evaluation employs top-p sampling and beam search, measuring performance with F1 score and accuracy metrics on the multilingual receipt extraction and classification tasks.

## Key Results
- AMuRD dataset contains 47,720 multilingual receipt samples with detailed annotations
- Achieves 76% F1 score and 68% accuracy for key information extraction and classification
- Introduces InstructLLaMA approach that fine-tunes LLaMA models using instruction tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual annotations enable fine-tuning of language models to generalize across script and layout variations in receipts.
- Mechanism: The dataset's 47,720 annotated samples in Arabic and English provide a diverse linguistic corpus. LLaMA models fine-tuned on this data learn cross-lingual patterns, improving recognition of items and attributes across scripts.
- Core assumption: Receipt structures are partially consistent across languages; textual cues like "Total Price" or "السعر الإجمالي" can be aligned through annotation.
- Evidence anchors:
  - [abstract] "AMuRD, a novel multilingual human-annotated dataset specifically designed for information extraction from receipts."
  - [section] "Our dataset encompasses 47,720 samples from a wide array of sources...in both Arabic and English."
  - [corpus] Weak evidence: related papers focus on OCR and receipt tasks but none mention bilingual receipt datasets.
- Break condition: If language-specific layout features dominate (e.g., right-to-left vs left-to-right scripts), cross-lingual generalization may degrade.

### Mechanism 2
- Claim: Detailed item-level annotations enable precise attribute extraction, improving downstream classification.
- Mechanism: Each receipt includes annotations for item names, brand, price, weight, units, etc. The model learns to isolate these fields and classify items into 44 categories using structured prompts.
- Core assumption: Consistent labeling schema allows the model to map free-text attributes to structured outputs.
- Evidence anchors:
  - [abstract] "annotations for item names and attributes such as price, brand, and more."
  - [section] "meticulous annotations have been conducted on various receipt fields...item names, classes, weight, number of units, size of units, price, total price, pack, units, and brand names."
  - [corpus] Weak evidence: related datasets mention OCR but not fine-grained attribute extraction.
- Break condition: If attribute fields vary widely in format (e.g., "10kg" vs "10 kgs"), model may misalign labels during training.

### Mechanism 3
- Claim: Instruction tuning with carefully crafted prompts yields higher F1 scores than vanilla fine-tuning.
- Mechanism: The InstructLLaMA approach uses a multi-turn prompt format to guide LLaMA's output, e.g., "Instruction: Extract item name, Input: [receipt text], Response: [item name]". This format mimics few-shot learning and improves precision.
- Core assumption: Language models can leverage explicit task instructions to improve extraction accuracy over unstructured training.
- Evidence anchors:
  - [section] "At inference time, the same prompt is used to generate the answer... Only the text generated after ### Response: is used as the final output."
  - [section] "Our approach yielded exceptional results, with an F1 score of 97.43% and accuracy of 94.99% in information extraction and classification."
  - [corpus] Weak evidence: related works mention instruction tuning but not for receipt datasets.
- Break condition: If prompt format mismatches the model's pre-training style, performance may degrade.

## Foundational Learning

- Concept: Multilingual text processing and OCR fundamentals
  - Why needed here: Receipts mix scripts (Arabic/English) and OCR errors; the system must robustly parse both.
  - Quick check question: What tokenization approach handles Arabic script segmentation and English word splitting consistently?
- Concept: Named entity recognition (NER) and attribute extraction
  - Why needed here: Items have structured attributes (price, brand, weight) that must be isolated from free text.
  - Quick check question: How does the model differentiate "10kg" as weight vs "10kg" in a brand name?
- Concept: Prompt engineering and few-shot learning
  - Why needed here: InstructLLaMA uses structured prompts; understanding prompt design is critical for fine-tuning.
  - Quick check question: What prompt format best separates instruction, input, and response for a language model?

## Architecture Onboarding

- Component map: Data pipeline → Preprocessing (OCR normalization) → Fine-tuning (InstructLLaMA) → Evaluation (F1, accuracy)
- Critical path: Data ingestion → Annotation validation → Model fine-tuning → Prompt-based inference → Metric computation
- Design tradeoffs: Multilingual coverage vs. depth of annotations; few-shot prompting vs. supervised fine-tuning; parameter-efficient fine-tuning vs. full fine-tuning
- Failure signatures: Low F1 on specific attributes (e.g., brand names) indicates label noise; poor cross-lingual performance suggests insufficient multilingual samples
- First 3 experiments:
  1. Validate annotation consistency by sampling 100 items and checking label alignment.
  2. Test monolingual fine-tuning (Arabic-only, English-only) vs. bilingual to measure cross-lingual benefit.
  3. Compare instruction-tuned outputs to standard fine-tuning outputs on a held-out validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multilingual nature of the dataset (Arabic and English) impact the performance of key information extraction and classification models compared to monolingual datasets?
- Basis in paper: [explicit] The paper explicitly mentions the dataset includes 47,720 samples in both Arabic and English, and evaluates models on this multilingual dataset.
- Why unresolved: The paper provides performance metrics but does not directly compare the multilingual dataset's performance to a monolingual counterpart.
- What evidence would resolve it: A comparative study evaluating the same models on a monolingual dataset of similar size and complexity would provide insights into the impact of multilingualism.

### Open Question 2
- Question: What are the specific challenges and opportunities in handling the class imbalance observed in the dataset, particularly for categories with low representation?
- Basis in paper: [explicit] The paper acknowledges the class imbalance in the dataset, especially for categories like Dental Care, Footwear, and Mobile & Tablets.
- Why unresolved: While the paper mentions the existence of class imbalance, it does not delve into the specific challenges or potential solutions for handling it.
- What evidence would resolve it: Experiments using different techniques to address class imbalance, such as oversampling, undersampling, or class weighting, and evaluating their impact on model performance would provide insights.

### Open Question 3
- Question: How does the performance of the InstructLLaMA approach compare to other state-of-the-art methods for key information extraction and classification on receipts, particularly in multilingual settings?
- Basis in paper: [inferred] The paper introduces the InstructLLaMA approach and provides its performance metrics, but does not compare it to other methods.
- Why unresolved: The paper focuses on introducing and evaluating the InstructLLaMA approach but lacks a comparison with other methods.
- What evidence would resolve it: Benchmarking the InstructLLaMA approach against other state-of-the-art methods on the same dataset and evaluating their performance would provide a comprehensive comparison.

## Limitations
- Limited evaluation on receipts with significantly different layouts or languages not represented in the training data
- Reliance on prompt format quality and consistency, which may not transfer well to different receipt formats
- Dataset size (47,720 samples) may not capture full diversity of real-world receipt variations

## Confidence
- **High Confidence**: The dataset creation methodology and annotation process are well-documented and reproducible.
- **Medium Confidence**: The fine-tuning approach and evaluation metrics are clearly specified, but the exact prompt engineering details could benefit from more explicit documentation.
- **Low Confidence**: The cross-lingual generalization claims need further validation on receipts from languages not included in the training data.

## Next Checks
1. Test model performance on receipts from retail domains not represented in the current 44 categories to assess domain generalization.
2. Evaluate the model's robustness to OCR errors by introducing controlled noise into the input text and measuring performance degradation.
3. Conduct a human evaluation study comparing the model's extracted information against ground truth annotations to identify systematic error patterns not captured by F1/accuracy metrics.