---
ver: rpa2
title: On Suppressing Range of Adaptive Stepsizes of Adam to Improve Generalisation
  Performance
arxiv_id: '2302.01029'
source_url: https://arxiv.org/abs/2302.01029
tags:
- adam
- set-adam
- adaptive
- training
- stepsizes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of improving generalization performance
  in deep neural networks by modifying the adaptive learning rates of the Adam optimizer.
  The core idea is to suppress the range of adaptive learning rates by performing
  three consecutive operations on the second momentum: down-scaling, epsilon-embedding,
  and down-translating.'
---

# On Suppressing Range of Adaptive Stepsizes of Adam to Improve Generalisation Performance

## Quick Facts
- arXiv ID: 2302.01029
- Source URL: https://arxiv.org/abs/2302.01029
- Reference count: 40
- Primary result: SET-Adam improves generalization performance by suppressing range of adaptive stepsizes, achieving 69.19% validation accuracy on transformers compared to 66.90% for AdaBelief

## Executive Summary
This paper addresses the problem of improving generalization performance in deep neural networks by modifying the adaptive learning rates of the Adam optimizer. The authors propose SET-Adam, which suppresses the range of adaptive stepsizes through three consecutive operations on the second momentum: down-scaling, epsilon-embedding, and down-translating. These operations are designed to reduce variance of adaptive learning rates and mimic the behavior of SGD with momentum. SET-Adam is evaluated across multiple deep learning tasks including NLP, image classification, and image generation, demonstrating consistent improvements over eight other adaptive optimizers.

## Method Summary
SET-Adam modifies the Adam optimizer by performing three operations on the second momentum (vt) at each iteration: (1) Down-scaling - layerwise scaling based on angles between layerwise subvectors and all-one vectors using cos² scaling; (2) Epsilon-embedding - placing epsilon inside the square root operation to suppress stepsize range via Taylor expansion effects; (3) Down-translating - subtracting a scalar value to uplift adaptive stepsizes and avoid extreme small values. These operations collectively reduce the variance of adaptive learning rates, making them more similar to SGD with momentum behavior.

## Key Results
- SET-Adam achieves 69.19% validation accuracy on transformer tasks, outperforming AdaBelief's 66.90%
- SET-Adam consumes 12-20% additional time per epoch compared to Adam, which is considered a reasonable trade-off
- SET-Adam consistently outperforms eight other adaptive optimizers across NLP, image classification, and image generation tasks
- The method uses only one additional parameter (τ = 0.5) that was not tuned per task for simplicity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The down-scaling operation reduces the range of adaptive stepsizes by exploiting layerwise gradient statistics.
- Mechanism: At each iteration, the second momentum vector vt is processed layerwise. For each layer, the subvector vl,t is down-scaled by a factor cos²(∠vl,t 1l), where the angle is computed between vl,t and the all-one vector 1l of the same dimension. Subvectors with larger angles (indicating higher variance across coordinates) are down-scaled more aggressively.
- Core assumption: Layerwise subvectors of vt have different angle distributions relative to their all-one vectors, and these angles correlate with the need to reduce stepsize range.
- Evidence anchors:
  - [abstract] "We suppress the range of the adaptive stepsizes of Adam by exploiting the layerwise gradient statistics."
  - [section] "The down-scaling operation on vt is performed layerwise by making use of the angles between the layerwise subvectors of vt and the corresponding all-one subvectors."
  - [corpus] No direct evidence found in corpus; this is a novel mechanism specific to SET-Adam.
- Break condition: If layerwise gradient statistics are homogeneous across layers, the angle-based down-scaling would provide no benefit.

### Mechanism 2
- Claim: The epsilon-embedding operation suppresses the range of adaptive stepsizes by placing epsilon inside the sqrt operation.
- Mechanism: Instead of using √vt/ (1-β2) + ε, the method uses √vt/ (1-β2) + ε, placing epsilon inside the square root. By Taylor expansion, this penalizes large stepsizes more when vt elements are small and reduces extreme small stepsizes when vt elements are large.
- Core assumption: The placement of epsilon relative to the square root operation affects the variance of the resulting stepsizes in a predictable way.
- Evidence anchors:
  - [section] "We show via a Taylor expansion that putting the epsilon parameter inside the sqrt operation essentially suppresses the range of the adaptive stepsizes."
  - [section] "By Taylor approximation that putting the epsilon parameter inside the sqrt operation of (3) in Adam helps to suppress the range of adaptive stepsizes."
  - [corpus] No direct evidence found in corpus; this is a novel insight about epsilon placement.
- Break condition: If the Taylor approximation breaks down (e.g., with very large vt values), the suppression effect may not hold.

### Mechanism 3
- Claim: The down-translating operation uplifts adaptive stepsizes to avoid extreme small values.
- Mechanism: After epsilon-embedding, a scalar value τ(dl·min(wl,t[i])) is subtracted from wl,t, where dl is the layer dimension and min(wl,t[i]) is the minimum element in wl,t. This uplifts the resulting stepsizes to prevent them from becoming too small.
- Core assumption: After epsilon-embedding, some elements of wl,t may be too close to the lower bound √ε, requiring uplift to maintain effective learning.
- Evidence anchors:
  - [section] "The down-translating operation subtracts a scalar value from wl,t for the lth neural layer, where the scalar is computed as a function of wl,t. It is designed to uplift the resulting adaptive stepsizes to avoid extreme small ones."
  - [section] "By inspection of the behaviors of SET-Adam in Figs. 1 and 4, it can be seen that the down-translating operation manages to uplift the resulting adaptive stepsizes of SET-Adam in Fig. 1, which implicitly avoids extreme small adaptive stepsizes as expected."
  - [corpus] No direct evidence found in corpus; this mechanism appears specific to SET-Adam.
- Break condition: If the parameter τ is set too high, it could push stepsizes above desired ranges or cause instability.

## Foundational Learning

- Concept: Layerwise gradient statistics heterogeneity
  - Why needed here: SET-Adam exploits the fact that gradient statistics vary across different neural layers to perform layerwise processing of the second momentum. Understanding this heterogeneity is crucial for appreciating why layerwise operations are beneficial.
  - Quick check question: Why would gradient statistics typically be heterogeneous across different neural layers in a deep network?

- Concept: Adaptive stepsize variance and generalization
  - Why needed here: The paper's core hypothesis is that reducing the variance of adaptive stepsizes improves generalization performance. Understanding the relationship between stepsize variance and optimization dynamics is essential.
  - Quick check question: How does reducing the variance of adaptive stepsizes relate to the convergence behavior of SGD with momentum?

- Concept: Taylor expansion approximation for function analysis
  - Why needed here: The paper uses Taylor expansion to analyze how placing epsilon inside vs. outside the sqrt operation affects stepsize ranges. This mathematical tool is key to understanding the epsilon-embedding mechanism.
  - Quick check question: What does a Taylor expansion of √a + x around x = 0 tell us about the behavior of the function for small values of x?

## Architecture Onboarding

- Component map: Main optimizer loop -> Down-scaling module -> Epsilon-embedding module -> Down-translating module -> Modified second momentum used in Adam update
- Critical path:
  1. Compute gradients gt
  2. Update first momentum mt (unchanged from Adam)
  3. Update second momentum vt (unchanged from Adam)
  4. Apply down-scaling operation layerwise
  5. Apply epsilon-embedding operation
  6. Apply down-translating operation
  7. Use modified vt in parameter update
  8. Output new parameters θt
- Design tradeoffs:
  - Computational overhead: SET-Adam adds 12-20% time per epoch compared to Adam due to the three additional operations
  - Parameter sensitivity: Only one additional parameter (τ) needs tuning, with τ=0.5 working well empirically
  - Memory usage: Similar to Adam, as operations are performed in-place on layerwise subvectors
  - Convergence stability: The three operations are designed to avoid extreme stepsizes, potentially improving stability
- Failure signatures:
  - Training instability: If τ is set too high, stepsizes may become too large
  - No performance improvement: If layerwise gradient statistics are homogeneous, down-scaling provides no benefit
  - Convergence slowdown: If epsilon-embedding is too aggressive, it may overly suppress necessary stepsize adaptation
- First 3 experiments:
  1. Implement SET-Adam on a simple CNN (e.g., VGG11) on CIFAR-10 with default Adam parameters, comparing validation accuracy curves
  2. Visualize layerwise average stepsizes across training epochs to verify the down-scaling effect is working as intended
  3. Test SET-Adam with τ=0 (disabling down-translating) to isolate the contribution of the first two operations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SET-Adam vary when the parameter τ is tuned for each specific DNN task rather than fixed at 0.5?
- Basis in paper: [explicit] The paper states "the additional parameter τ in SET-Adam was set to τ = 0.5, and not tunned for each DNN task for simplicity."
- Why unresolved: The paper chose to use a fixed τ value for all experiments, which may not be optimal for all tasks. The impact of tuning τ for each task is unknown.
- What evidence would resolve it: Conducting experiments with different τ values for each DNN task and comparing the results to the fixed τ = 0.5 case would provide insights into the importance of task-specific tuning.

### Open Question 2
- Question: How does SET-Adam perform on tasks outside of NLP, image classification, and image generation, such as reinforcement learning or graph neural networks?
- Basis in paper: [inferred] The paper demonstrates SET-Adam's effectiveness on three specific types of DNN tasks but does not explore other domains where adaptive optimizers are commonly used.
- Why unresolved: The paper focuses on a limited set of tasks, leaving the generalizability of SET-Adam to other domains unexplored.
- What evidence would resolve it: Evaluating SET-Adam on a diverse range of tasks, including reinforcement learning and graph neural networks, would provide insights into its broader applicability.

### Open Question 3
- Question: What is the impact of the down-scaling operation's cosine power (currently n=2) on SET-Adam's performance?
- Basis in paper: [explicit] The paper mentions considering higher-order cases (γl,t = cos^n(∠vl,t 1l), n = 4) but found n = 4 too aggressive and computationally expensive.
- Why unresolved: The paper only explores the n=2 case for the down-scaling operation, leaving the impact of different powers unexplored.
- What evidence would resolve it: Conducting experiments with varying cosine powers for the down-scaling operation and analyzing the impact on performance would provide insights into the optimal choice of n.

## Limitations
- The core mechanism relies on the assumption that layerwise gradient statistics exhibit sufficient heterogeneity, which lacks direct experimental validation
- The Taylor expansion analysis supporting the epsilon-embedding mechanism lacks rigorous error bounds for extreme cases
- The paper lacks ablation studies isolating the contribution of each of the three operations

## Confidence
- **High confidence**: The empirical performance improvements of SET-Adam over competing optimizers (69.19% vs 66.90% validation accuracy) are well-documented with multiple benchmarks
- **Medium confidence**: The theoretical justification for the three operations (down-scaling, epsilon-embedding, down-translating) is provided but lacks rigorous mathematical proofs or extensive empirical validation of the underlying assumptions
- **Low confidence**: The paper does not provide sufficient evidence that the layerwise gradient statistics are sufficiently heterogeneous across different architectures to justify the complexity of the proposed method

## Next Checks
1. Perform an ablation study by training with SET-Adam variants that disable each of the three operations individually to quantify their individual contributions to performance gains
2. Conduct statistical analysis of gradient angle distributions across different neural network layers to verify the fundamental assumption underlying the down-scaling operation
3. Test SET-Adam's performance on additional architectures (e.g., Vision Transformers, GPT-style models) and tasks to assess generalizability beyond the evaluated benchmarks