---
ver: rpa2
title: 'Change is Hard: A Closer Look at Subpopulation Shift'
arxiv_id: '2302.12254'
source_url: https://arxiv.org/abs/2302.12254
tags:
- worst
- shift
- accuracy
- subpopulation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a comprehensive benchmark for subpopulation
  shift, a common issue in machine learning where models perform poorly on underrepresented
  subgroups in the training data. The authors propose a unified framework to characterize
  and quantify different types of subpopulation shifts, including spurious correlations,
  attribute imbalance, class imbalance, and attribute generalization.
---

# Change is Hard: A Closer Look at Subpopulation Shift

## Quick Facts
- arXiv ID: 2302.12254
- Source URL: https://arxiv.org/abs/2302.12254
- Reference count: 40
- Key outcome: Comprehensive benchmark shows existing algorithms only improve subgroup robustness for certain shift types, with simple worst-class accuracy serving as effective model selection criterion

## Executive Summary
This paper presents a comprehensive benchmark for evaluating subpopulation shift in machine learning, where models perform poorly on underrepresented subgroups. The authors propose a unified framework that decomposes subpopulation shifts into four basic types: spurious correlations, attribute imbalance, class imbalance, and attribute generalization. Through extensive experiments on 12 real-world datasets spanning vision, language, and healthcare domains, they evaluate 20 state-of-the-art algorithms, training over 10,000 models to understand when and how these algorithms succeed or fail.

The primary findings reveal that existing algorithms only reliably improve subgroup robustness for spurious correlations and class imbalance, while showing limited effectiveness for attribute imbalance and generalization. Surprisingly, a simple model selection criterion based on worst-class accuracy consistently outperforms more complex approaches, even without access to group information. The study also identifies a fundamental tradeoff between worst-group accuracy and worst-case precision, highlighting the importance of careful metric selection when evaluating subpopulation shift algorithms.

## Method Summary
The benchmark evaluates 20 state-of-the-art subpopulation shift algorithms across 12 real-world datasets using a unified framework that characterizes four types of shifts. Each dataset is split into training, validation, and test sets with consistent attribute availability settings. The framework models inputs as combinations of core features (label-specific) and attributes (potentially biased), allowing isolation of different shift mechanisms. Model selection is performed using worst-group accuracy when group information is available, or worst-class accuracy as a proxy when it is not. The evaluation covers multiple domains including vision (Waterbirds, CelebA), language (CivilComments, MultiNLI), and healthcare (MIMIC-CXR, CheXpert).

## Key Results
- Existing algorithms improve subgroup robustness only for spurious correlations and class imbalance, not attribute imbalance or generalization
- Simple worst-class accuracy model selection outperforms complex approaches without requiring group information
- Fundamental tradeoff exists between worst-group accuracy and worst-case precision across all shift types
- Larger, more diverse pretraining datasets significantly improve worst-group accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subpopulation shift can be decomposed into four basic types: spurious correlations, attribute imbalance, class imbalance, and attribute generalization
- Mechanism: The framework models each input as generated from core features (label-specific) and attributes (potentially biased). This decomposition allows isolating how attribute and class terms in Bayes' theorem influence classification under different shifts
- Core assumption: Attributes are conditionally independent given core features, and attributes either are label-dependent (biased) or label-independent (benign)
- Evidence anchors:
  - [abstract] "We first propose a unified framework that dissects and explains common shifts in subgroups"
  - [section 3.1] "we view each input x as being fully described or generated from a set of underlying core features xcore and a list of attributes a"
  - [corpus] Weak - corpus neighbors do not directly confirm this decomposition framework
- Break condition: If attributes are not conditionally independent or if the core features cannot be separated from attributes, the decomposition fails

### Mechanism 2
- Claim: Model selection based on worst-class accuracy is surprisingly effective even without group annotations
- Mechanism: When validation attributes are unknown, worst-class accuracy approximates worst-group accuracy because groups are defined by attribute-label combinations. This simple metric consistently selects models with high worst-group performance
- Core assumption: The relationship between class performance and subgroup performance is stable enough that worst-class accuracy serves as a proxy for worst-group accuracy
- Evidence anchors:
  - [abstract] "we find that a simple selection criterion based on worst-class accuracy is surprisingly effective even without any group information"
  - [section 5.4] "Simply stopping when the worst-class accuracy reaches a maxima achieves the best worst-group accuracy on average"
  - [corpus] Weak - no corpus evidence directly supports this model selection finding
- Break condition: If the correlation between class and subgroup performance breaks down in certain datasets or if worst-class accuracy is dominated by majority classes

### Mechanism 3
- Claim: There is a fundamental tradeoff between worst-group accuracy and worst-case precision
- Mechanism: Optimizing for worst-group accuracy (recall) can reduce worst-case precision because improving recall often involves lowering classification thresholds or including more uncertain samples, which increases false positives
- Core assumption: Worst-group accuracy and worst-case precision are not aligned objectives and can be inversely correlated
- Evidence anchors:
  - [abstract] "we demonstrate the fundamental tradeoff between WGA and other important metrics"
  - [section 5.5] "we observe a strong negative linear correlation, indicating an intrinsic tradeoff between WGA and worst-case precision"
  - [corpus] Weak - corpus neighbors do not discuss this specific tradeoff between WGA and precision
- Break condition: If the relationship between recall and precision is not inverse for certain datasets or if different decision thresholds break this pattern

## Foundational Learning

- Concept: Conditional independence of attributes given core features
  - Why needed here: The framework assumes attributes are conditionally independent given core features to decompose the classification model into manageable terms
  - Quick check question: If two attributes are spuriously correlated with the label through different mechanisms, are they still conditionally independent given the true label?

- Concept: Distributionally robust optimization
  - Why needed here: Many algorithms in the benchmark use this technique to optimize for worst-case performance across groups
  - Quick check question: How does group DRO differ from standard empirical risk minimization when minimizing the maximum loss across groups?

- Concept: Precision-recall tradeoff
  - Why needed here: Understanding this tradeoff is crucial for interpreting the fundamental tradeoff between worst-group accuracy and worst-case precision
  - Quick check question: If a classifier's threshold is lowered to improve recall, what typically happens to precision?

## Architecture Onboarding

- Component map: 20 algorithms -> 12 datasets (vision/language/healthcare) -> 8 evaluation metrics (accuracy, precision, F1, calibration, etc.) -> 3 attribute availability settings
- Critical path: 1) Define subpopulation shift framework, 2) Curate datasets with different shift types, 3) Implement 20 algorithms, 4) Run comprehensive experiments, 5) Analyze results across settings
- Design tradeoffs: Comprehensive coverage vs. computational cost (10,000+ models trained), algorithm diversity vs. implementation complexity, real-world datasets vs. controlled synthetic data
- Failure signatures: Inconsistent worst-group improvements across shift types indicates framework limitations, poor model selection when using wrong metrics, or algorithms failing on certain shift types
- First 3 experiments:
  1. Run ERM baseline on Waterbirds to establish spurious correlation performance
  2. Test GroupDRO on CelebA to verify subgroup robustness with known attributes
  3. Evaluate worst-class accuracy model selection on CivilComments without validation attributes

## Open Questions the Paper Calls Out

- Question: How do mislabeling or noise in attribute and label annotations affect the performance of subpopulation shift algorithms?
- Basis in paper: [inferred] The authors acknowledge that real-world data can have mislabeling in both attributes and labels but do not consider this effect in their benchmark
- Why unresolved: The paper focuses on idealized, clean datasets and does not evaluate the robustness of algorithms to label or attribute noise
- What evidence would resolve it: Experiments comparing algorithm performance on datasets with varying levels of label/attribute noise, or synthetic datasets with controlled noise levels

- Question: What is the impact of multiple spurious attributes on model performance, and how does mitigating one spurious correlation affect reliance on others?
- Basis in paper: [explicit] The authors note that prior work has shown reducing reliance on one spurious attribute can increase reliance on another, but they only consider a single attribute in their benchmark
- Why unresolved: The benchmark and analysis are limited to scenarios with a single spurious attribute, leaving the multi-attribute case unexplored
- What evidence would resolve it: Experiments on datasets with multiple spurious attributes, measuring changes in reliance on different attributes when mitigating specific correlations

- Question: How does the size and diversity of pretraining datasets affect worst-group accuracy across different types of subpopulation shifts?
- Basis in paper: [explicit] The authors observe that larger and more diverse pretraining datasets (e.g., SWAG vs. ImageNet-1K) lead to significant improvements in worst-group accuracy, but do not analyze this across different shift types
- Why unresolved: The analysis of pretraining dataset impact is limited to overall performance gains without examining differential effects on spurious correlations, attribute imbalance, class imbalance, and attribute generalization
- What evidence would resolve it: Systematic experiments varying pretraining dataset size and diversity, with performance metrics broken down by shift type

## Limitations

- Framework assumes conditional independence of attributes given core features, which may not hold in complex real-world scenarios
- Computational cost of evaluating 20 algorithms across 12 datasets limits scalability to larger benchmarks
- Many findings rely on correlation rather than causal mechanisms, particularly the effectiveness of worst-class accuracy as a proxy

## Confidence

- **High Confidence:** The decomposition framework and dataset curation methodology are well-established. The observation that worst-class accuracy serves as an effective proxy has strong empirical support across multiple datasets.
- **Medium Confidence:** The fundamental tradeoff between worst-group accuracy and worst-case precision is observed consistently, but the relationship may vary with different decision thresholds or dataset characteristics.
- **Low Confidence:** The conditional independence assumption underlying the framework may break down in practice, and some algorithms show inconsistent performance across different shift types without clear explanations.

## Next Checks

1. Test worst-class accuracy model selection on datasets with more than two classes to verify generalizability beyond binary classification
2. Evaluate whether the worst-group vs worst-precision tradeoff holds when using different decision thresholds (not just default 0.5)
3. Investigate the conditional independence assumption by analyzing real-world datasets where attributes may be correlated through mechanisms beyond the label