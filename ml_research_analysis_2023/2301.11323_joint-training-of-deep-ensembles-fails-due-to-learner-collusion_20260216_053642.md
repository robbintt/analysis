---
ver: rpa2
title: Joint Training of Deep Ensembles Fails Due to Learner Collusion
arxiv_id: '2301.11323'
source_url: https://arxiv.org/abs/2301.11323
tags:
- training
- ensemble
- joint
- loss
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the underperformance of joint training in
  deep ensemble models compared to independent training. The authors propose that
  joint training leads to "learner collusion," where base learners artificially inflate
  diversity by biasing predictions in opposing directions that cancel in aggregate.
---

# Joint Training of Deep Ensembles Fails Due to Learner Collusion

## Quick Facts
- arXiv ID: 2301.11323
- Source URL: https://arxiv.org/abs/2301.11323
- Reference count: 40
- Primary result: Joint training of deep ensembles fails because base learners collude to artificially inflate diversity, leading to poor generalization

## Executive Summary
This paper investigates why joint training of deep ensemble models consistently underperforms compared to independent training. The authors identify a phenomenon called "learner collusion" where base learners coordinate to bias their predictions in opposing directions that cancel in aggregate, creating apparent diversity without genuine improvement. They introduce an augmented objective with a diversity weighting parameter β to smoothly interpolate between independent (β=0) and joint (β=1) training. Extensive experiments across various architectures and datasets demonstrate that joint training consistently performs worst due to poor generalization from learner collusion, while intermediate β values often achieve better performance.

## Method Summary
The authors compare independent training (each ensemble member trained separately) with joint training (ensemble loss optimized directly). They introduce an augmented objective that interpolates between these extremes using a diversity weighting parameter β. For classification, they use probability averaging for ensemble aggregation and cross-entropy loss. For regression, they use score averaging and mean squared error. The diversity term measures the difference between ensemble loss and average individual loss. They conduct extensive experiments on CIFAR-10/CIFAR-100 with ResNet architectures, Tiny ImageNet with ViT and SimpleViT, and UCI tabular datasets.

## Key Results
- Joint training (β=1) consistently underperforms independent training (β=0) across all tested architectures and datasets
- Intermediate β values (0 < β < 1) often achieve better performance than both extremes
- Diversity values explode in joint training due to learner collusion, creating pseudo-diversity that fails to generalize
- Debiasing techniques can reduce diversity in jointly trained models, confirming the collusion mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of deep ensembles fails because base learners engage in "learner collusion" to artificially inflate diversity
- Mechanism: When optimizing the ensemble loss, the diversity term DIV can be trivially maximized by base learners biasing their predictions in opposing directions that cancel in aggregate
- Core assumption: The additional diversity requirement in joint training creates an adversarial incentive for base learners to appear diverse without genuinely improving ensemble performance
- Evidence anchors:
  - [abstract] "base learners collude to artificially inflate their apparent diversity" and "pseudo-diversity fails to generalize beyond the training data"
  - [section] "we discover that joint optimization results in a phenomenon in which base learners collude to artificially inflate their apparent diversity"
  - [corpus] Weak - corpus papers focus on ensemble methods but don't specifically address learner collusion phenomenon

### Mechanism 2
- Claim: The joint training objective creates a dual optimization problem that base learners can exploit
- Mechanism: The ensemble loss decomposes into individual errors and diversity terms, allowing base learners to minimize joint loss by maximizing diversity through coordinated prediction shifts rather than genuine diversity
- Core assumption: Base learners can adjust their predictions in ways that appear diverse to the diversity metric while maintaining similar functional behavior
- Evidence anchors:
  - [abstract] "joint optimization results in a phenomenon in which base learners collude to artificially inflate their apparent diversity"
  - [section] "the additional requirement for diversity in the training objective can be trivially achieved if multiple learners collude to bias their predictive distributions in opposing directions"
  - [corpus] Weak - corpus lacks direct evidence of this dual optimization exploitation

### Mechanism 3
- Claim: Learner collusion leads to poor generalization because the apparent diversity doesn't reflect genuine functional diversity
- Mechanism: Base learners that collude to appear diverse create predictions that cancel in aggregate but are individually poor performers, leading to models that overfit to training data
- Core assumption: Models that achieve diversity through coordinated bias rather than genuine functional differences will fail to generalize
- Evidence anchors:
  - [abstract] "This pseudo-diversity fails to generalize beyond the training data, causing a larger generalization gap"
  - [section] "although training loss can be driven down by learner collusion, this does not generalize beyond the training set, resulting in a larger generalization gap than independent training"
  - [corpus] Weak - corpus papers don't specifically address generalization gap differences from collusion

## Foundational Learning

- Concept: Ensemble diversity decomposition
  - Why needed here: Understanding that ensemble loss can be decomposed into individual errors and diversity terms is crucial for grasping why joint training fails
  - Quick check question: Can you express the ensemble loss as the difference between weighted average individual losses and ensemble loss?

- Concept: Gradient analysis of ensemble averaging methods
  - Why needed here: Different averaging methods (probability vs score) affect how gradients flow to individual learners, which impacts whether they can coordinate
  - Quick check question: Do you understand why probability averaging results in variable gradients while score averaging results in constant gradients for all learners?

- Concept: Generalization gap analysis
  - Why needed here: Comparing training and test loss curves helps identify whether models are overfitting, which is key to understanding learner collusion effects
  - Quick check question: Can you calculate and interpret the generalization gap for an ensemble model?

## Architecture Onboarding

- Component map:
  - Base learners (M neural networks) -> Ensemble aggregation function (weighted average) -> Loss function (cross-entropy) -> Diversity term (difference between individual and ensemble loss) -> Beta parameter (controls interpolation)

- Critical path:
  1. Initialize M base learners with different random seeds
  2. Forward pass through all learners
  3. Compute ensemble prediction
  4. Calculate loss (individual + diversity terms)
  5. Backpropagate gradients to all learners
  6. Update parameters
  7. Repeat until convergence

- Design tradeoffs:
  - Beta = 0 (independent): No coordination, potential for redundancy
  - Beta = 1 (joint): Full coordination, risk of collusion
  - Beta in (0,1): Balance between coordination and independence
  - Ensemble size (M): Larger ensembles more vulnerable to collusion

- Failure signatures:
  - Large diversity values relative to individual errors
  - High test set diversity that doesn't translate to better performance
  - Large increase in error when randomly dropping base learners at test time
  - Significant gap between training and test loss curves

- First 3 experiments:
  1. Train ensembles with beta = 0, 0.5, 1.0 and compare test loss
  2. Plot diversity vs beta to observe explosion at beta = 1
  3. Apply debiasing to jointly trained models and measure diversity reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on diversity weighting parameter β that prevents pathological behavior in deep ensembles?
- Basis in paper: [explicit] The paper proves in Theorem 5.1 that β > 1 leads to unbounded loss, establishing an upper bound on β
- Why unresolved: While the paper establishes the upper bound, it doesn't explore the precise lower bound or the optimal β value that balances diversity and performance
- What evidence would resolve it: Empirical studies systematically testing different β values across various architectures and datasets to find the optimal trade-off point

### Open Question 2
- Question: How does learner collusion manifest differently across various ensemble architectures (CNNs, ResNets, Vision Transformers)?
- Basis in paper: [explicit] The paper demonstrates learner collusion across different architectures but doesn't provide a detailed comparative analysis
- Why unresolved: The paper shows consistent poor performance with joint training but doesn't deeply investigate architecture-specific mechanisms of collusion
- What evidence would resolve it: Detailed architectural analysis of how gradient flows and parameter updates differ across architectures during joint training

### Open Question 3
- Question: Can regularization techniques specifically designed to prevent learner collusion improve joint training performance?
- Basis in paper: [inferred] The paper identifies learner collusion as the root cause of joint training failure but doesn't explore mitigation strategies
- Why unresolved: The paper focuses on diagnosing the problem rather than proposing solutions to prevent or mitigate learner collusion
- What evidence would resolve it: Experiments testing various regularization approaches (e.g., diversity penalties, adversarial training, ensemble-aware regularization) to prevent collusion while maintaining joint training benefits

## Limitations

- The analysis focuses primarily on cross-entropy loss and classification tasks, with limited investigation of regression or other loss functions
- The study doesn't explore whether debiasing techniques could mitigate learner collusion while preserving diversity benefits
- The paper doesn't provide a detailed comparative analysis of how learner collusion manifests differently across various ensemble architectures

## Confidence

- High confidence: Joint training with β=1 consistently underperforms independent training across multiple architectures and datasets
- Medium confidence: Learner collusion is the primary mechanism causing joint training failure; the phenomenon is robust but may manifest differently across architectures
- Medium confidence: Intermediate β values (0 < β < 1) provide practical optimization sweet spots, though optimal values vary by task

## Next Checks

1. Test the learner collusion hypothesis on architectures with different aggregation functions (beyond probability averaging) to verify the mechanism is universal
2. Evaluate the augmented objective with varying β on larger-scale datasets (e.g., ImageNet) to confirm scalability of the findings
3. Investigate whether debiasing techniques can effectively mitigate learner collusion without sacrificing diversity gains in jointly trained ensembles