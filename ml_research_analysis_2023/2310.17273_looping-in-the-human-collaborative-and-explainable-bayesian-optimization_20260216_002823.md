---
ver: rpa2
title: Looping in the Human Collaborative and Explainable Bayesian Optimization
arxiv_id: '2310.17273'
source_url: https://arxiv.org/abs/2310.17273
tags:
- bayesian
- human
- optimization
- coexbo
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a collaborative and explainable Bayesian optimization
  (CoExBO) framework that addresses the opacity and lack of user trust in traditional
  BO methods. CoExBO integrates human insights through preference learning and provides
  explanations using Shapley values to foster trust and understanding.
---

# Looping in the Human Collaborative and Explainable Bayesian Optimization

## Quick Facts
- **arXiv ID**: 2310.17273
- **Source URL**: https://arxiv.org/abs/2310.17273
- **Reference count**: 40
- **Primary result**: CoExBO achieves up to 12.5x time-to-accuracy speedup compared to conventional BO methods while integrating human insights through preference learning and explanations.

## Executive Summary
This paper introduces CoExBO, a collaborative and explainable Bayesian optimization framework that addresses the opacity and lack of user trust in traditional BO methods. CoExBO integrates human insights through preference learning and provides explanations using Shapley values to foster trust and understanding. The framework also offers a no-harm guarantee, ensuring convergence even with adversarial human interventions. Experiments on synthetic and real-world tasks, including lithium-ion battery design, demonstrate substantial improvements in convergence speed and user trust.

## Method Summary
CoExBO integrates human preferences into Bayesian optimization by presenting users with candidate pairs and learning their preferences via a Gaussian Process preference model. This implicitly captures expert knowledge through relative comparisons rather than absolute values. After generating candidate pairs, CoExBO uses Shapley values to explain which features contributed most to each candidate's selection. The acquisition function gradually shifts from human-informed to standard Bayesian optimization as more data is collected, providing a no-harm guarantee that ensures convergence even with incorrect human preferences.

## Key Results
- CoExBO achieves up to 12.5x time-to-accuracy speedup compared to conventional BO methods
- The framework demonstrates improved user trust and understanding through Shapley value explanations
- Provides a no-harm guarantee ensuring convergence even with adversarial human interventions
- Successfully applied to high-dimensional material design and lithium-ion battery electrolyte optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CoExBO uses preference learning to integrate human insights without requiring explicit knowledge models.
- **Mechanism**: Instead of asking users to specify a prior over the input space, CoExBO presents pairs of candidates and learns preferences via a Gaussian Process preference model. This implicitly captures expert knowledge through relative comparisons rather than absolute values.
- **Core assumption**: Humans can effectively judge which of two candidates is better, even if they cannot specify the optimal location directly.
- **Evidence anchors**:
  - [abstract] "Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization"
  - [section 3.1] "CoExBO deviates from conventional methods that require users to input an exact knowledge model. Instead, it presents users with candidate pairs, empowering them to select the perceived optimal one"
- **Break condition**: If users cannot consistently distinguish between candidate pairs, or if their preferences are highly inconsistent, the preference model becomes unreliable and provides little useful guidance.

### Mechanism 2
- **Claim**: CoExBO provides explanations through Shapley values to build user trust and understanding.
- **Mechanism**: After generating candidate pairs, CoExBO uses Shapley values to explain which features contributed most to each candidate's selection. This helps users understand the optimization process and verify that it aligns with their expertise.
- **Core assumption**: Shapley values provide meaningful and interpretable explanations for users, and users can understand and act on this information.
- **Evidence anchors**:
  - [abstract] "CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization"
  - [section 3.3] "To foster trust in the black-box optimizer among users, we employ Shapley values... to provide feature attributions for the acquisitions and the surrogate model"
- **Break condition**: If the feature space is too high-dimensional or if the Shapley value explanations are too complex for users to interpret meaningfully, the explanations may not effectively build trust.

### Mechanism 3
- **Claim**: CoExBO offers a "no-harm guarantee" that ensures convergence even with adversarial human interventions.
- **Mechanism**: The acquisition function gradually shifts from human-informed to standard Bayesian optimization as more data is collected. Even with incorrect human preferences, the influence of human input decays over time, ensuring asymptotic convergence to standard BO performance.
- **Core assumption**: The decay rate of human influence is appropriately balanced to maintain no-harm guarantee while still leveraging useful human knowledge when available.
- **Evidence anchors**:
  - [abstract] "CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to a vanilla Bayesian optimization"
  - [section 3.2] "As more experimental data accumulates and refines the surrogate model, the influence of human input gradually diminishes. Theorem 2 proves this offers a no-harm guarantee"
- **Break condition**: If the decay rate is too slow, incorrect human preferences could significantly impact convergence. If too fast, useful human knowledge may be underutilized.

## Foundational Learning

- **Gaussian Processes**: 
  - Why needed here: CoExBO uses GPs as the surrogate model for the objective function and for the preference model. Understanding GP theory is essential for implementing and tuning the algorithm.
  - Quick check question: What are the two key components of a Gaussian Process model, and how do they determine the predictions?

- **Bayesian Optimization**: 
  - Why needed here: CoExBO is an extension of BO, so understanding the basic BO framework (acquisition functions, exploration-exploitation tradeoff) is crucial.
  - Quick check question: What is the role of the acquisition function in Bayesian optimization, and how does it balance exploration and exploitation?

- **Preference Learning**:
  - Why needed here: CoExBO's core innovation is learning from pairwise comparisons rather than direct function evaluations. Understanding preference learning methods is essential for implementing the human-in-the-loop component.
  - Quick check question: How does preference learning differ from standard supervised learning, and what are common approaches for modeling preferences?

## Architecture Onboarding

- **Component map**: Gaussian Process surrogate model -> Preference learning model -> Acquisition function generator -> Shapley value explainer -> Human interaction interface
- **Critical path**: (1) Generate candidate pair using acquisition function, (2) Present candidates to user, (3) Collect user preference, (4) Update preference model, (5) Update acquisition function, (6) Iterate
- **Design tradeoffs**: The key tradeoff is between leveraging human knowledge (which can accelerate convergence if accurate) and maintaining robustness to human errors (which requires careful decay scheduling). Another tradeoff is between explanation complexity and interpretability.
- **Failure signatures**: If the preference model fails to learn meaningful patterns from user comparisons, the acquisition function will not benefit from human input. If Shapley value explanations are not aligned with user expectations, trust may not be built. If the decay rate is misconfigured, convergence guarantees may be violated.
- **First 3 experiments**:
  1. Implement the basic BO framework with standard acquisition function to establish baseline performance
  2. Add preference learning component and test on synthetic preference data to verify it can learn meaningful patterns
  3. Integrate preference model into acquisition function and test on simple synthetic optimization problems with simulated human preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CoExBO's performance compare to other preference-based BO methods in terms of convergence speed and accuracy?
- Basis in paper: [explicit] The paper mentions that CoExBO consistently outperforms four baselines except for the Rosenbrock function.
- Why unresolved: The paper only compares CoExBO to a few baselines and does not provide a comprehensive comparison to all preference-based BO methods.
- What evidence would resolve it: A comprehensive comparison of CoExBO to a wide range of preference-based BO methods on various benchmark functions.

### Open Question 2
- Question: How does the explainability feature in CoExBO impact the user's understanding of the optimization process and their trust in the algorithm?
- Basis in paper: [explicit] The paper states that CoExBO explains its candidate selection every iteration to foster trust and empowers users with a clearer grasp of the optimization.
- Why unresolved: The paper does not provide empirical evidence on the impact of the explainability feature on user understanding and trust.
- What evidence would resolve it: A user study evaluating the impact of the explainability feature on user understanding and trust in CoExBO.

### Open Question 3
- Question: How does the no-harm guarantee in CoExBO ensure convergence even with adversarial human interventions?
- Basis in paper: [explicit] The paper states that CoExBO offers a no-harm guarantee, allowing users to make mistakes, and even with extreme adversarial interventions, the algorithm converges asymptotically to a vanilla Bayesian optimization.
- Why unresolved: The paper does not provide a detailed explanation of how the no-harm guarantee works in practice.
- What evidence would resolve it: A theoretical proof or empirical evidence demonstrating the convergence of CoExBO under various adversarial human interventions.

## Limitations
- The paper lacks details on the human-in-the-loop interaction protocol, including how preferences are elicited and how frequently updates occur
- Hyperparameter settings for the Gaussian process and Bayesian quadrature are not specified
- The scalability of Shapley value explanations for high-dimensional feature spaces is not addressed

## Confidence

- **High confidence**: The core mechanism of preference learning integration and the no-harm guarantee through decay scheduling
- **Medium confidence**: The effectiveness of Shapley value explanations for building user trust, as the paper assumes users can interpret these but doesn't validate this assumption
- **Low confidence**: The claim of 12.5x speedup, as the comparison baseline and experimental conditions are not fully specified

## Next Checks

1. **Test preference model learning**: Generate synthetic preference data with known patterns and verify CoExBO's preference model can learn these patterns effectively
2. **Validate Shapley explanations**: Conduct user studies to assess whether Shapley value explanations are interpretable and actually build trust as claimed
3. **Verify no-harm guarantee**: Test CoExBO with adversarial preference inputs to confirm it converges to standard BO performance as the no-harm guarantee suggests