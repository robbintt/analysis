---
ver: rpa2
title: 'LANISTR: Multimodal Learning from Structured and Unstructured Data'
arxiv_id: '2305.16556'
source_url: https://arxiv.org/abs/2305.16556
tags:
- data
- learning
- multimodal
- image
- lanistr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LANISTR, a new attention-based framework
  for multimodal learning from structured and unstructured data. LANISTR uses a novel
  multimodal fusion module with a similarity-based multimodal masking loss to learn
  cross-modal relations from large-scale multimodal data with missing modalities.
---

# LANISTR: Multimodal Learning from Structured and Unstructured Data

## Quick Facts
- arXiv ID: 2305.16556
- Source URL: https://arxiv.org/abs/2305.16556
- Authors: 
- Reference count: 40
- Primary result: 6.6% and 14% improvements in AUROC and accuracy on MIMIC-IV and Amazon Product Review datasets when fine-tuned with 0.1% and 0.01% labeled data respectively

## Executive Summary
LANISTR introduces a novel attention-based framework for multimodal learning that can handle missing modalities in both training and inference. The system combines unimodal masking pretraining with a similarity-based multimodal masking objective to learn rich cross-modal representations from large-scale unlabeled data. By using a Perceiver IO-style fusion module, LANISTR achieves linear computational complexity while maintaining strong performance on downstream tasks with minimal labeled data.

## Method Summary
LANISTR uses separate encoders for text (BERT), image (ViT), and structured data (TabNet or Transformer), followed by a multimodal fusion encoder that employs cross-attention in a latent space. The model is pretrained using unimodal masking losses (MLM, MIM, FM, TM) and a novel similarity-based multimodal masking loss that maximizes the cosine similarity between masked and non-masked input embeddings. This pretraining enables the model to learn cross-modal relationships even when modalities are missing. For downstream tasks, LANISTR is fine-tuned with a classification head using the labeled data.

## Key Results
- Achieves 85.89% AUROC on MIMIC-IV in-hospital mortality prediction (6.6% improvement over baselines)
- Achieves 85.93% accuracy on Amazon Product Review star rating prediction (14% improvement over baselines)
- Maintains strong performance even with high ratios of missing modalities during test time
- Outperforms state-of-the-art alternatives when fine-tuned with only 0.1% and 0.01% of labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LANISTR's similarity-based multimodal masking loss enables learning cross-modal relations from large-scale multimodal data with missing modalities during training and test time.
- Mechanism: The model creates masked views of the data triplet (I, L, T) and minimizes the negative cosine similarity between the embeddings generated by the masked and non-masked inputs, encouraging the model to learn cross-modal relations such that the cosine similarity between the embeddings of a masked and a non-masked data is maximized.
- Core assumption: The masked and non-masked versions of the same data share the same semantic content, so maximizing their similarity encourages learning of cross-modal relationships.
- Evidence anchors:
  - [abstract]: "We introduce a new similarity-based multimodal masking loss that enables it to learn cross-modal relations from large-scale multimodal data with missing modalities."
  - [section 3.2.2]: "Assume the input data samples are in the form X = ( I, L, T ) where I, L, and T represent image, language, and time series/tabular modality inputs. We create masked views of the data triplet denoted as ˆX = ( ˆI, ˆL, ˆT ) by randomly masking a portion of the input... We define z2 = f ( ˆX) minimize the negative cosine similarity between e1 and z2"
- Break condition: If the semantic content changes significantly between masked and non-masked versions, or if the masking destroys critical cross-modal relationships, the similarity maximization could lead to degenerate representations.

### Mechanism 2
- Claim: The Perceiver IO-style fusion module in LANISTR provides efficient cross-modal fusion with linear complexity, addressing the computational bottleneck of traditional attention mechanisms.
- Mechanism: Instead of using the full input to generate queries and keys at every layer (quadratic complexity), LANISTR maps inputs to a latent space whose size is independent of the input and/or output size, processes the latent representations through cross-attention, then decodes back to outputs.
- Core assumption: The latent space can capture the essential cross-modal relationships without requiring quadratic attention over the full input dimensions.
- Evidence anchors:
  - [section 3.1]: "Inspired by recent success in reducing the cost and memory of Transformers to linear scale [28], we decode the cross-modal relationship using a cross-attention mechanism... we adopt Perceiver IO-style module [28] to fuse the modalities together without using the full input to generate queries and keys at every layer"
  - [abstract]: "LANISTR leverages unimodal masking pretraining while encompassing cross-modal relationships through a similarity-based multimodal masking objective"
- Break condition: If the latent space becomes too small to capture necessary cross-modal interactions, or if the mapping to/from latent space loses critical information, the fusion quality would degrade.

### Mechanism 3
- Claim: Pretraining LANISTR on large-scale unlabeled data with missing modalities improves downstream task performance when fine-tuned on small labeled datasets.
- Mechanism: The model learns rich unimodal and cross-modal representations from abundant unlabeled data with missing modalities, then transfers this knowledge to downstream tasks with limited labeled data through fine-tuning.
- Core assumption: The representations learned from unlabeled data with missing modalities are transferable and generalize well to downstream tasks, even when those tasks have different data distributions.
- Evidence anchors:
  - [abstract]: "Experiments on two real-world datasets, MIMIC-IV and Amazon Product Review, demonstrate significant improvements over state-of-the-art alternatives, achieving 6.6% and 14% increases in AUROC and accuracy, respectively, when fine-tuned with only 0.1% and 0.01% of labeled data"
  - [section 5.1]: "LANISTR without pretraining achieves 80.82% which is only slightly better than LateFusion while pretraining with unlabeled data improves the performance to 85.89% of AUROC which is a significant increase"
- Break condition: If the pretraining data distribution is too different from the downstream task, or if the downstream task requires specific fine-grained patterns not captured in pretraining, the transfer benefit could be minimal or negative.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: LANISTR uses MLM as one of its unimodal masking losses to pretrain the text encoder, allowing it to learn from non-parallel text data
  - Quick check question: What percentage of tokens are typically masked during MLM pretraining in BERT-like models?

- Concept: Cross-modal attention mechanisms
  - Why needed here: LANISTR's fusion module uses cross-attention in the latent space to learn relationships between different modalities efficiently
  - Quick check question: How does cross-attention differ from self-attention in terms of the query/key/value sources?

- Concept: Multimodal learning with missing modalities
  - Why needed here: LANISTR is specifically designed to handle real-world scenarios where some samples are missing one or more modalities
  - Quick check question: What are the main challenges in training multimodal models when some samples lack certain modalities?

## Architecture Onboarding

- Component map: Text encoder (BERT) -> Image encoder (ViT) -> Structured data encoder (TabNet/Transformer) -> Projection layers -> Concatenation -> Multimodal fusion encoder (Perceiver IO-style) -> Projector -> Output embeddings -> Classification head
- Critical path: Input → modality-specific encoders → projection layers → concatenation → multimodal fusion encoder → projector → output embeddings → loss computation (unimodal + multimodal masking) → backprop
- Design tradeoffs: The Perceiver IO fusion provides linear complexity but may lose some fine-grained cross-modal interactions compared to full attention. The masking-based pretraining enables learning from missing modalities but requires careful hyperparameter tuning. Separate encoders for each modality provide specialization but increase model complexity.
- Failure signatures: Poor cross-modal fusion indicated by unimodal-only performance matching or exceeding multimodal performance; overfitting to specific modality combinations when downstream tasks have different missing modality patterns; instability during pretraining if masking ratios are inappropriate.
- First 3 experiments:
  1. Run LANISTR without pretraining on a small labeled dataset to establish baseline performance
  2. Evaluate each unimodal masking loss independently to understand their individual contributions
  3. Test LANISTR with only the similarity-based multimodal masking loss (no unimodal masking) to validate the importance of learning unimodal representations first

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LANISTR's performance scale with the size of the pretraining dataset, especially in cases where the pretraining data is significantly larger than the downstream task data?
- Basis in paper: [explicit] The paper mentions that pretraining becomes effective when using 75% of the total available unlabeled data, which is 583 times more than the labeled data.
- Why unresolved: The paper only shows the performance at 75% of the pretraining data size. It is unclear how the performance scales beyond this point or how much improvement can be achieved with even larger pretraining datasets.
- What evidence would resolve it: Experiments showing LANISTR's performance on downstream tasks with varying sizes of pretraining datasets, particularly focusing on scenarios where the pretraining data is orders of magnitude larger than the labeled data.

### Open Question 2
- Question: Can LANISTR be effectively extended to handle datasets with four or more modalities simultaneously, and what are the challenges associated with such an extension?
- Basis in paper: [inferred] The paper mentions that LANISTR was evaluated on datasets with three modalities and acknowledges that the framework can be extended to four modalities, but it does not explore this scenario.
- Why unresolved: The paper does not provide any experimental results or analysis on the performance of LANISTR when dealing with four or more modalities. It is unclear whether the current architecture and pretraining strategies can scale effectively to such cases.
- What evidence would resolve it: Experiments comparing LANISTR's performance on downstream tasks using datasets with three, four, and potentially more modalities, along with an analysis of the challenges and limitations encountered during the extension process.

### Open Question 3
- Question: Is there a way to automatically determine the importance of different modalities in a dataset prior to fine-tuning, to optimize the use of LANISTR for specific tasks?
- Basis in paper: [explicit] The paper mentions that they found using all four modalities in MIMIC-IV (image+text+time series+tabular) yielded similar performance to using three modalities (image+text+time series), but they did not have an automated method to determine this beforehand.
- Why unresolved: The paper does not propose or evaluate any method for automatically assessing the importance of different modalities in a dataset. It is unclear how to determine which modalities are most relevant for a given task without extensive experimentation.
- What evidence would resolve it: Development and evaluation of a method that can automatically assess the importance of different modalities in a dataset, potentially through techniques like feature importance analysis, ablation studies, or unsupervised learning approaches. This method should be able to predict the optimal subset of modalities to use for a given task, reducing the need for extensive experimentation.

## Limitations
- Computational complexity analysis focuses on theoretical linear scaling but lacks concrete runtime comparisons with baselines
- Results primarily demonstrated on two specific datasets (MIMIC-IV and Amazon Review) with similar modality combinations
- Uncertainty around optimal masking ratios and loss weight hyperparameters for different datasets
- Limited exploration of extending to four or more modalities beyond theoretical acknowledgment

## Confidence

- High confidence in the overall architecture design and pretraining/fine-tuning methodology
- Medium confidence in the specific implementation details of the multimodal masking loss
- Medium confidence in the generalizability of results across different datasets and missing modality patterns

## Next Checks

1. Test LANISTR on additional multimodal datasets with different missing modality distributions to validate robustness
2. Perform ablation studies varying the masking ratios and loss weights to optimize pretraining
3. Conduct runtime and memory usage comparisons between LANISTR and baseline methods to verify computational efficiency claims