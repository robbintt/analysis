---
ver: rpa2
title: Combining Reconstruction and Contrastive Methods for Multimodal Representations
  in RL
arxiv_id: '2302.05342'
source_url: https://arxiv.org/abs/2302.05342
tags:
- learning
- contrastive
- joint
- representations
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses representation learning for reinforcement
  learning (RL) when multiple sensor modalities are available, such as proprioception
  and egocentric vision. The authors propose Contrastive Reconstructive Aggregated
  representation Learning (CoRAL), a framework that learns a joint representation
  of all sensors using Recurrent State Space Models (RSSMs) and allows choosing the
  most appropriate self-supervised loss (reconstruction or contrastive) for each modality.
---

# Combining Reconstruction and Contrastive Methods for Multimodal Representations in RL

## Quick Facts
- **arXiv ID**: 2302.05342
- **Source URL**: https://arxiv.org/abs/2302.05342
- **Reference count**: 40
- **Primary result**: Combining reconstruction and contrastive methods for multimodal representation learning significantly improves RL performance on tasks with visual distractions and occlusions.

## Executive Summary
This paper addresses representation learning for reinforcement learning (RL) when multiple sensor modalities are available, such as proprioception and egocentric vision. The authors propose Contrastive Reconstructive Aggregated representation Learning (CoRAL), a framework that learns a joint representation of all sensors using Recurrent State Space Models (RSSMs) and allows choosing the most appropriate self-supervised loss (reconstruction or contrastive) for each modality. They evaluate CoRAL on tasks with images containing distractions or occlusions, a new locomotion suite, and a challenging manipulation suite with realistic distractions. Results show that learning a multimodal representation by combining contrastive and reconstruction-based losses significantly improves performance compared to more naive approaches and other recent baselines, solving tasks that are out of reach for image-only methods.

## Method Summary
The method uses RSSMs to learn a joint representation of multiple sensor modalities (images and proprioception) for RL. It combines reconstruction-based and contrastive approaches for training, selecting the most appropriate method for each sensor modality. The RSSM architecture includes separate encoders for each modality, a deterministic path using GRU, and a variational distribution. Reconstruction is used for proprioception while contrastive methods (variational or predictive coding) are used for images. The learned representation is then used as input to a Soft Actor-Critic (SAC) policy. The approach is evaluated on DeepMind Control Suite tasks, a custom Locomotion Suite, and a Box Pushing task with realistic distractions.

## Key Results
- CoRAL solves tasks that are out of reach for image-only methods by effectively handling visual distractions and occlusions
- Joint representations outperform concatenating independent image representations with proprioception
- Predictive coding approaches perform better than variational approaches when dynamics learning is crucial (e.g., with occlusions)
- Learning a multimodal representation by combining contrastive and reconstruction-based losses significantly improves performance compared to more naive approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using reconstruction for proprioception and contrastive methods for images allows each modality to be treated according to its information density characteristics.
- Mechanism: Reconstruction forces the model to capture all details of an observation, which is beneficial for proprioception (noise-free, low-dimensional) but problematic for images with distractions. Contrastive methods can ignore irrelevant details in images while still learning useful representations.
- Core assumption: The information density and noise characteristics of proprioception differ fundamentally from those of images, making different self-supervised losses optimal for each modality.
- Evidence anchors:
  - [abstract]: "different self-supervised loss functions have distinct advantages and limitations depending on the information density of the underlying sensor modality"
  - [section]: "Reconstruction is a powerful tool if observations contain only task-relevant or static elements. Yet, it can fail to learn good representations if observations are noisy or include distracting elements"
  - [corpus]: Weak evidence - the corpus neighbors focus on general multimodal representation learning but don't specifically address the reconstruction vs. contrastive choice for different modalities
- Break condition: If proprioception contains significant noise or if images are mostly task-relevant without distractions, this mechanism may break down.

### Mechanism 2
- Claim: Joint representations outperform concatenating independent image representations with proprioception by allowing cross-modal information flow during training.
- Mechanism: When learning representations jointly, proprioception can guide the image representation learning process, helping it focus on relevant aspects. This is more effective than learning image representations independently and then concatenating them with proprioception.
- Core assumption: The representation learning process benefits from having multiple modalities available simultaneously rather than learning them in isolation.
- Evidence anchors:
  - [abstract]: "learning a multimodal representation by combining contrastive and reconstruction-based losses can significantly improve performance and solve tasks that are out of reach for more naive representation learning approaches"
  - [section]: "learning joint representations of all sensors is beneficial in either case. Doing so outperforms both exclusively using images and concatenating proprioception with independently learned image representations"
  - [corpus]: Weak evidence - corpus neighbors discuss multimodal learning but don't specifically compare joint vs. concatenated representations
- Break condition: If the modalities are completely independent with no complementary information, joint learning may not provide benefits.

### Mechanism 3
- Claim: The choice between variational and predictive coding approaches for RSSMs depends on the task characteristics, with predictive coding performing better when dynamics learning is more important.
- Mechanism: Variational approaches focus on reconstructing observations, while predictive coding explicitly learns to predict future states. When occlusions or other challenges require strong dynamics modeling, predictive coding's explicit ahead prediction loss is advantageous.
- Core assumption: Different RSSM training paradigms have different strengths, and the optimal choice depends on the specific task requirements.
- Evidence anchors:
  - [section]: "While results increase particularly in settings where a combination of images and proprioception is necessary for decision-making, we also observe significant performance improvements when all information is available through images. These findings show that including proprioception can indeed guide representation learning algorithms to find better representations for RL."
  - [section]: "It appears that CPC-based approaches with their explicit ahead prediction loss are better equipped to learn dynamics in this setting"
  - [corpus]: Weak evidence - corpus neighbors don't specifically compare variational vs. predictive coding approaches
- Break condition: If the task doesn't require strong dynamics modeling or if the prediction loss introduces instability.

## Foundational Learning

- Concept: Recurrent State Space Models (RSSMs)
  - Why needed here: RSSMs provide a framework for learning temporal representations that accumulate information across multiple sensors and time steps, which is essential for multimodal RL
  - Quick check question: What are the two components of the latent state in RSSMs, and how do they differ in their update mechanisms?

- Concept: Variational Inference vs. Contrastive Learning
  - Why needed here: The paper compares reconstruction-based (variational) and contrastive approaches for training RSSMs, requiring understanding of both methods
  - Quick check question: How does the InfoNCE estimator in contrastive learning differ from the variational lower bound in terms of what they optimize?

- Concept: Mutual Information Estimation
  - Why needed here: Contrastive approaches rely on estimating mutual information between observations and latent states, which is crucial for understanding how these methods work
  - Quick check question: What is the relationship between maximizing mutual information and learning representations that are invariant to task-irrelevant information?

## Architecture Onboarding

- Component map: Sensor observations → Encoders → RSSM → Variational posterior → Policy
- Critical path: Sensor observations → Encoders → RSSM → Variational posterior → Policy
- Design tradeoffs:
  - Reconstruction vs. contrastive: Reconstruction provides stronger learning signals but is susceptible to distractions; contrastive methods are more robust to distractions but may miss some details
  - Joint vs. concatenated representations: Joint learning allows cross-modal guidance but is more complex; concatenation is simpler but may miss complementary information
  - Variational vs. predictive coding: Variational focuses on reconstruction quality; predictive coding emphasizes dynamics learning
- Failure signatures:
  - Poor performance on tasks with distractions when using reconstruction-only approaches
  - Representation collapse when using contrastive methods without proper regularization
  - No improvement over concatenation when modalities are truly independent
- First 3 experiments:
  1. Implement basic RSSM with reconstruction loss on proprioception only to verify proprioceptive state estimation
  2. Add image encoder and implement contrastive loss for images while keeping reconstruction for proprioception
  3. Compare joint representation learning against concatenation baseline on a simple multimodal task (e.g., standard DMC task)

## Open Questions the Paper Calls Out
1. How does CoRAL's performance compare when pre-training representations on out-of-domain data before fine-tuning on RL tasks?
2. What is the theoretical justification for why proprioceptive signals help guide representation learning in high-dimensional visual inputs?
3. How does the choice of loss function (reconstruction vs contrastive) interact with different types of visual distractions beyond what was tested?

## Limitations
- Performance improvements depend heavily on the presence of distracting elements in observations
- Computational overhead of joint multimodal learning versus simpler concatenation approaches is not fully characterized
- Results are demonstrated on custom locomotion and manipulation suites that may not generalize to broader RL applications

## Confidence

- **High Confidence**: The fundamental claim that joint multimodal representations outperform concatenated independent representations is well-supported by ablation studies across multiple task suites.
- **Medium Confidence**: The superiority of combining reconstruction for proprioception with contrastive methods for images is demonstrated, though the optimal choice may depend on specific task characteristics.
- **Medium Confidence**: The advantage of predictive coding over variational approaches for tasks with occlusions/distractions is shown but may not generalize to all settings.

## Next Checks
1. Evaluate CoRAL's performance on additional task suites with varying levels of observation noise and distraction to determine the conditions under which the method provides the most benefit.
2. Measure and compare the sample efficiency and computational overhead of joint multimodal learning versus simpler concatenation approaches across different task complexities.
3. Test whether representations learned with CoRAL can be effectively transferred to related but distinct tasks, examining the generalization capabilities of the learned multimodal representations.