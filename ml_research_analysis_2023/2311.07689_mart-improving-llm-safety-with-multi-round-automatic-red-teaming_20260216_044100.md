---
ver: rpa2
title: 'MART: Improving LLM Safety with Multi-round Automatic Red-Teaming'
arxiv_id: '2311.07689'
source_url: https://arxiv.org/abs/2311.07689
tags:
- safety
- adversarial
- prompts
- prompt
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MART addresses the problem of scaling LLM safety improvements by
  proposing an iterative adversarial training framework where an adversarial LLM generates
  challenging prompts that elicit unsafe responses from a target LLM, while the target
  LLM is fine-tuned with safety-aligned data from these adversarial prompts. The method
  incorporates both automatic adversarial prompt writing and safe response generation
  in an iterative manner, where the adversarial LLM crafts better attacks on the updated
  target LLM, while the target LLM improves through safety fine-tuning.
---

# MART: Improving LLM Safety with Multi-round Automatic Red-Teaming

## Quick Facts
- **arXiv ID**: 2311.07689
- **Source URL**: https://arxiv.org/abs/2311.07689
- **Authors**: [Not specified in source]
- **Reference count**: 8
- **One-line primary result**: MART reduces violation rates by up to 84.7% through iterative adversarial training while maintaining helpfulness

## Executive Summary
MART introduces a novel iterative adversarial training framework that improves LLM safety by having two LLMs compete against each other. The adversarial LLM generates challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned using safety-aligned data from these interactions. After 4 rounds of this process, violation rates can be reduced by up to 84.7% compared to models with limited safety alignment, achieving performance comparable to models trained with extensive manual adversarial prompt writing.

## Method Summary
MART operates through an iterative cycle where an adversarial LLM (M_adv) generates prompts similar to previously successful attacks, which are then used to elicit responses from the target LLM (M_tgt). Both models are fine-tuned based on reward model feedback: M_adv is updated with successful adversarial prompts, while M_tgt is fine-tuned on responses that score high on both safety and helpfulness metrics. The process uses context distillation at iteration 1 and rejection sampling at iteration 3, with data selection based on safety (S_s > 0.8) and helpfulness (S_h > 0.4) thresholds.

## Key Results
- Violation rate reduced by up to 84.7% after 4 rounds of MART on adversarial prompt benchmarks
- Performance comparable to LLMs with extensive manual adversarial prompt writing
- Model helpfulness on non-adversarial prompts remains stable throughout iterations

## Why This Works (Mechanism)

### Mechanism 1
MART achieves iterative safety improvement through adversarial competition between two LLMs. The adversarial LLM generates prompts that elicit unsafe responses from the target LLM, which is then fine-tuned on these adversarial prompts using safety-aligned data from its own responses. This cycle repeats, with the adversarial LLM crafting better attacks on the updated target LLM while the target LLM improves through safety fine-tuning. Core assumption: Both models can effectively learn from their interactions without manual intervention. Evidence anchors: [abstract] and [section 2.4]. Break condition: If either model cannot generate effective new prompts or learn from interactions.

### Mechanism 2
MART maintains helpfulness while improving safety through selective fine-tuning. The target LLM is fine-tuned only on responses that score high on both safety and helpfulness metrics, preventing over-conservative responses that would degrade helpfulness. Core assumption: Safety and helpfulness are separable dimensions that can be optimized independently. Evidence anchors: [abstract] and [section 2.3]. Break condition: If selection criteria become too strict, training data becomes insufficient and the target model may overfit.

### Mechanism 3
MART generalizes to out-of-domain adversarial prompts through its iterative adversarial training approach. By continuously generating new adversarial prompts that target current vulnerabilities, MART creates a diverse training distribution that captures various attack patterns. Core assumption: The adversarial model can generate diverse prompts that generalize beyond specific patterns in seed data. Evidence anchors: [section 3.2] and [section 2.2]. Break condition: If adversarial model's prompt generation becomes too narrow or repetitive.

## Foundational Learning

- **Concept**: Supervised fine-tuning (SFT) with safety alignment data
  - Why needed here: Target LLM needs to learn from its own responses to safety-aligned prompts
  - Quick check question: What is the difference between standard SFT and safety alignment SFT used in MART?

- **Concept**: Reward modeling for automatic evaluation
  - Why needed here: Manual evaluation is too costly for iterative red-teaming; reward models provide scalable feedback
  - Quick check question: How do safety and helpfulness reward models differ in their evaluation criteria?

- **Concept**: Adversarial training in machine learning
  - Why needed here: Iterative adversarial process between two LLMs drives continuous improvement
  - Quick check question: What is the key difference between MART's adversarial training and traditional adversarial training?

## Architecture Onboarding

- **Component map**: 
  - Adversarial LLM (Madv) -> Target LLM (Mtgt) -> Safety Reward Model (S_s) -> Data Selection -> Training Pipeline
  - Helpfulness Reward Model (S_h) -> Data Selection -> Training Pipeline

- **Critical path**:
  1. Adversarial LLM generates prompts from previous successful attacks
  2. Target LLM generates responses to these prompts
  3. Reward models evaluate responses
  4. Data selection filters safe and helpful responses
  5. Both models are fine-tuned on their respective data
  6. Cycle repeats

- **Design tradeoffs**:
  - Safety vs. helpfulness: Stricter safety thresholds may lead to overly conservative responses
  - Data quantity vs. quality: Relaxing thresholds increases data but may reduce quality
  - Iteration count vs. diminishing returns: More iterations may yield diminishing safety improvements

- **Failure signatures**:
  - Safety improvements plateau while violation rates remain high
  - Helpfulness scores decrease significantly across iterations
  - Adversarial prompts become repetitive or ineffective
  - Reward model scores diverge from human preferences

- **First 3 experiments**:
  1. Run a single iteration with a small seed dataset to verify the basic adversarial generation and fine-tuning loop works
  2. Compare violation rates on in-distribution and out-of-distribution test sets after 2-3 iterations
  3. Test the impact of different data selection thresholds on the balance between safety improvement and helpfulness maintenance

## Open Questions the Paper Calls Out

- **Open Question 1**: How does MART perform when applied to dialogue scenarios with multi-turn conversations between the adversarial and target models? The paper mentions this as a possible future direction but does not explore it. What evidence would resolve it: Experiments comparing MART's performance in single-turn versus multi-turn dialogue scenarios.

- **Open Question 2**: What is the impact of integrating reinforcement learning techniques with MART's framework? The paper leaves the integration of other techniques for future exploration. What evidence would resolve it: Comparative studies showing the effectiveness of reinforcement learning-enhanced MART versus the current implementation.

- **Open Question 3**: How does the quantity and quality of safety data affect MART's performance in improving model safety? While the paper provides some insights into the trade-off between data quantity and quality, it does not fully explore the optimal balance. What evidence would resolve it: Detailed analysis of MART's performance with varying amounts of safety data.

## Limitations

- Reward model architectures and training procedures are not fully specified, which are critical components of the MART framework
- Generalizability to real-world scenarios beyond controlled benchmark settings remains unclear
- The effectiveness on truly novel attack patterns is based on limited evidence from one specific benchmark

## Confidence

- **High confidence**: The iterative adversarial training mechanism is clearly described and logically sound, with well-documented empirical results
- **Medium confidence**: The claim that MART maintains stable helpfulness while improving safety is supported by presented data, but long-term stability across more iterations is not demonstrated
- **Low confidence**: The claim about MART's effectiveness on out-of-domain adversarial prompts is based on limited evidence from one specific benchmark

## Next Checks

1. Conduct human evaluation on a held-out test set to verify that reward model-based safety improvements align with actual human safety judgments
2. Extend MART iterations beyond 4 rounds to assess whether safety improvements continue to increase or if diminishing returns set in
3. Apply MART-trained safety improvements to different base LLM architectures to test whether learned safety behaviors transfer beyond the specific model used in experiments