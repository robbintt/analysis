---
ver: rpa2
title: Is ChatGPT A Good Keyphrase Generator? A Preliminary Study
arxiv_id: '2303.13001'
source_url: https://arxiv.org/abs/2303.13001
tags:
- chatgpt
- keyphrase
- generation
- keyphrases
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a preliminary study of ChatGPT as a keyphrase
  generator, evaluating its performance on six benchmark datasets across various aspects
  including keyphrase generation prompts, diversity, multi-domain generation, and
  long document understanding. The authors design six candidate prompts to analyze
  ChatGPT's understanding of keywords vs.
---

# Is ChatGPT A Good Keyphrase Generator? A Preliminary Study

## Quick Facts
- arXiv ID: 2303.13001
- Source URL: https://arxiv.org/abs/2303.13001
- Reference count: 6
- Key outcome: ChatGPT shows strong performance on present keyphrase generation, outperforming several state-of-the-art models, but struggles with absent keyphrase generation

## Executive Summary
This paper provides a preliminary study of ChatGPT as a keyphrase generator, evaluating its performance on six benchmark datasets across various aspects including keyphrase generation prompts, diversity, multi-domain generation, and long document understanding. The authors design six candidate prompts to analyze ChatGPT's understanding of keywords vs. keyphrases, "extract" vs. "generate", and present vs. absent keyphrases. Results show ChatGPT achieves strong performance on present keyphrase generation, outperforming several state-of-the-art models. It also demonstrates excellent adaptability to multi-domain data and ability to generate diverse keyphrases with low duplication. However, ChatGPT struggles with absent keyphrase generation, indicating room for improvement.

## Method Summary
The study uses six benchmark datasets (KP20k, INSPEC, NUS, KRAPIVIN, SEMEVAL2010, OPEN KP) and six candidate prompts (TP1-TP6) based on OpenAI's provided prompt. The authors preprocess the datasets by tokenizing, lowercasing, replacing digits, and removing duplicates. They use each prompt to generate keyphrases from the documents in the datasets using ChatGPT, then calculate F1@5 and F1@M scores, average numbers of unique present and absent keyphrases, and average duplication ratios for each dataset and prompt combination.

## Key Results
- ChatGPT achieves strong performance on present keyphrase generation, outperforming several state-of-the-art models
- ChatGPT demonstrates excellent adaptability to multi-domain data
- ChatGPT can generate diverse keyphrases with low duplication, but struggles with absent keyphrase generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT achieves strong performance on present keyphrase generation compared to state-of-the-art models
- Mechanism: ChatGPT leverages its large-scale pre-training and instruction-following capabilities to effectively extract keyphrases that are explicitly mentioned in the source text
- Core assumption: The model has sufficient capacity to encode and retrieve relevant phrases from the input document when prompted appropriately
- Evidence anchors:
  - [abstract] "Results show ChatGPT achieves strong performance on present keyphrase generation, outperforming several state-of-the-art models"
  - [section 2.2] "we find that ChatGPT performs exceptionally well on all six candidate prompts, with minor performance differences observed across the datasets"
  - [corpus] Weak evidence - only one related paper directly addresses this comparison

### Mechanism 2
- Claim: ChatGPT demonstrates excellent adaptability to multi-domain data
- Mechanism: The model's broad pretraining corpus enables it to generalize across different document domains without requiring domain-specific fine-tuning
- Core assumption: The pretraining data sufficiently covers the domains present in the evaluation datasets
- Evidence anchors:
  - [abstract] "It also demonstrates excellent adaptability to multi-domain data"
  - [section 2.3] "The results show that ChatGPT is highly adaptable to multi-domain data and performs better than the state-of-the-art keyphrase extraction baselines"
  - [corpus] Moderate evidence - related papers discuss cross-domain robustness but not specifically for ChatGPT

### Mechanism 3
- Claim: ChatGPT can generate diverse keyphrases with low duplication
- Mechanism: The model's generative capabilities allow it to produce multiple distinct phrases that cover different aspects of the document content
- Core assumption: The sampling strategy and lack of explicit diversity constraints don't lead to repetitive outputs
- Evidence anchors:
  - [abstract] "It also demonstrates excellent adaptability to multi-domain data and ability to generate diverse keyphrases with low duplication"
  - [section 2.4] "ChatGPT generates more unique keyphrases than all baselines by a large margin and achieves a significantly lower duplication ratio without additional hyper-parameter settings"
  - [corpus] Moderate evidence - diversity in keyphrase generation is mentioned but not specifically for ChatGPT

## Foundational Learning

- Concept: Prompt engineering and its impact on LLM performance
  - Why needed here: The study shows that different prompt formulations (TP1-TP6) significantly affect ChatGPT's keyphrase generation performance
  - Quick check question: What specific elements in a prompt (e.g., "extract" vs "generate", "keyword" vs "keyphrase") most influence the quality of generated keyphrases?

- Concept: Evaluation metrics for keyphrase generation
  - Why needed here: Understanding F1@5 and F1@M metrics is crucial for interpreting the reported results and comparing with baselines
  - Quick check question: How do macro-averaged F1 scores differ from micro-averaged scores, and why are they appropriate for keyphrase generation evaluation?

- Concept: Zero-shot vs few-shot learning with LLMs
  - Why needed here: The study uses zero-shot prompting, but the authors mention few-shot prompting as a potential future direction
  - Quick check question: What are the trade-offs between zero-shot and few-shot prompting in terms of performance and practical deployment?

## Architecture Onboarding

- Component map: Document input -> Prompt formulation -> ChatGPT API call -> Keyphrase extraction -> Post-processing (stemming, duplicate removal) -> Metric calculation
- Critical path: Document input -> Prompt formulation -> ChatGPT API call -> Keyphrase extraction -> Post-processing (stemming, duplicate removal) -> Metric calculation
- Design tradeoffs: Zero-shot prompting offers flexibility and no training data requirements but may underperform specialized fine-tuned models on absent keyphrase generation
- Failure signatures: Poor performance on absent keyphrases, high variance across different prompt formulations, potential token limit issues with long documents
- First 3 experiments:
  1. Test all six candidate prompts (TP1-TP6) on a single document to observe prompt sensitivity
  2. Compare present vs absent keyphrase generation performance on a document with both types
  3. Evaluate performance degradation as document length approaches ChatGPT's token limit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ChatGPT for keyphrase generation change with different hyperparameter settings, such as temperature, frequency penalty, and presence penalty?
- Basis in paper: [explicit] The paper states that "we do not consider setting different hyper-parameters in this report, which affected the keyphrase generation performance of ChatGPT" and suggests that "we will further consider the corresponding hyper-parameters of ChatGPT to verify its performance on the keyphrase generation task and give a more detailed analysis."
- Why unresolved: The paper did not experiment with different hyperparameter settings for ChatGPT, which could significantly impact its performance on keyphrase generation tasks.
- What evidence would resolve it: Systematic experiments varying ChatGPT's hyperparameters (temperature, frequency penalty, presence penalty, etc.) while keeping the prompt constant, comparing performance metrics across the different settings.

### Open Question 2
- Question: Can few-shot prompting improve ChatGPT's performance on keyphrase generation tasks, particularly for absent keyphrase generation?
- Basis in paper: [explicit] The paper mentions that "few-shot prompting" is a limitation of the current study and suggests that "we believe it is possible to enhance the performance of large language models in the keyphrase generation task through similar methods."
- Why unresolved: The paper did not explore the use of few-shot prompting to improve ChatGPT's performance on keyphrase generation tasks, which could potentially enhance its ability to generate absent keyphrases.
- What evidence would resolve it: Experiments comparing ChatGPT's performance with and without few-shot prompting on keyphrase generation tasks, particularly focusing on absent keyphrase generation.

### Open Question 3
- Question: How do semantic-based evaluation metrics compare to exact match metrics for assessing ChatGPT's keyphrase generation performance?
- Basis in paper: [explicit] The paper states that "a semantic-based evaluation metric may be more suitable to measure the performance of ChatGPT on the keyphrase generation task" and suggests that "human evaluation can provide more insights for comparing ChatGPT with keyphrase generation baselines."
- Why unresolved: The paper used exact match metrics (F1@5 and F1@M) for evaluation, which may not fully capture the semantic similarity between predicted and gold keyphrases.
- What evidence would resolve it: Comparative studies using both exact match metrics and semantic-based metrics (such as BERTScore or ROUGE-L) to evaluate ChatGPT's keyphrase generation performance, along with human evaluation studies to validate the results.

## Limitations
- The study relies on zero-shot prompting without fine-tuning, limiting performance on absent keyphrase generation
- The exact prompt formulations (TP1-TP6) are only described in Table 2 without full text, making exact replication challenging
- The evaluation uses macro-averaged F1 scores which may not capture the full performance picture, particularly for rare keyphrases

## Confidence
- High Confidence: ChatGPT's strong performance on present keyphrase generation compared to state-of-the-art models
- Medium Confidence: Claims about ChatGPT's adaptability to multi-domain data and diversity in generated keyphrases
- Low Confidence: The scalability and practical deployment considerations, which are not explicitly addressed in the study

## Next Checks
1. Test the six candidate prompts (TP1-TP6) on a small set of documents to verify their formulations match the descriptions and assess prompt sensitivity.
2. Compare ChatGPT's performance on present vs absent keyphrases using a single document with both types to validate the claimed limitation.
3. Evaluate performance degradation when document length approaches ChatGPT's token limit to assess scalability constraints.