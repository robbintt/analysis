---
ver: rpa2
title: Exploring Large Language Models for Ontology Alignment
arxiv_id: '2309.07172'
source_url: https://arxiv.org/abs/2309.07172
tags:
- concept
- ontology
- mappings
- concepts
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates zero-shot Large Language Models (LLMs) for
  ontology alignment, focusing on concept equivalence matching across ontologies.
  We evaluate Flan-T5-XXL and GPT-3.5-turbo on challenging subsets from OAEI Bio-ML
  datasets, incorporating concept labels and structural contexts through carefully
  designed prompts.
---

# Exploring Large Language Models for Ontology Alignment

## Quick Facts
- arXiv ID: 2309.07172
- Source URL: https://arxiv.org/abs/2309.07172
- Reference count: 8
- Primary result: Flan-T5-XXL with threshold-based filtering achieved F-scores of 0.721 and 0.346 on NCIT-DOID and SNOMED-FMA datasets respectively

## Executive Summary
This work investigates zero-shot Large Language Models (LLMs) for ontology alignment, focusing on concept equivalence matching across ontologies. The study evaluates Flan-T5-XXL and GPT-3.5-turbo on challenging subsets from OAEI Bio-ML datasets, incorporating concept labels and structural contexts through carefully designed prompts. Results show that while LLMs can achieve competitive performance with threshold-based filtering, they still fall short of fine-tuned models like BERTMap, highlighting the need for improved prompt design and structural context integration strategies.

## Method Summary
The study uses Flan-T5-XXL and GPT-3.5-turbo in a zero-shot setting to perform ontology alignment on two challenging Bio-ML dataset subsets (NCIT-DOID and SNOMED-FMA), each containing 10,000 concept pairs. The approach involves prompt-based concept identification where the LLM classifies whether source and target concepts are equivalent, using generation probabilities of "Yes" tokens as classification scores. Various settings are tested including vanilla zero-shot, threshold-based filtering, parent/child context incorporation, and combined approaches.

## Key Results
- Flan-T5-XXL with threshold-based filtering achieved F-scores of 0.721 (NCIT-DOID) and 0.346 (SNOMED-FMA)
- Outperformed BERTMapLt but fell short of BERTMap's performance
- Incorporating parent/child contexts did not enhance matching results
- Computational costs limited evaluation to challenging subsets rather than full datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot LLM performance in ontology alignment depends critically on prompt design that incorporates both concept labels and hierarchical structural contexts.
- Mechanism: The LLM uses the provided prompt to understand task requirements and available information, then generates a classification score based on how well it can match the concepts using both lexical and structural features.
- Core assumption: LLMs can effectively leverage hierarchical information (parent/child concepts) when explicitly included in prompts.
- Evidence anchors: [section] "For simplicity, we use the generation probability of the "Yes" token as the classification score"; [section] "We assess its performance factoring in the use of concept labels, score thresholding, and structural contexts"; [corpus] Weak evidence - related papers focus on complex alignments but don't provide direct experimental validation
- Break condition: When hierarchical contexts provide conflicting information or when the LLM cannot properly interpret the structural relationships, leading to degraded performance compared to simpler lexical matching.

### Mechanism 2
- Claim: Threshold-based filtering improves precision at the cost of recall in LLM-based ontology alignment.
- Mechanism: By setting a score threshold, the system filters out lower-confidence mappings, reducing false positives while potentially missing some true matches.
- Core assumption: There exists an optimal threshold that balances precision and recall for the specific ontology domain.
- Evidence anchors: [section] "Using a threshold enhances precision but reduces recall"; [section] "We also consider candidate target concepts with the "No" answer as well as their "No" scores, placing them after the candidate target concepts with the "Yes" answer in an ascending order"; [corpus] No direct evidence found in related papers about threshold effects in ontology alignment
- Break condition: When the optimal threshold varies significantly across different concept pairs, making a single threshold ineffective for the entire dataset.

### Mechanism 3
- Claim: Zero-shot LLM performance is inherently limited compared to fine-tuned models like BERTMap for ontology alignment tasks.
- Mechanism: Pre-trained LLMs without domain-specific fine-tuning lack the specialized knowledge and training that BERTMap has accumulated for ontology matching.
- Core assumption: Domain-specific fine-tuning provides significant performance advantages over zero-shot approaches.
- Evidence anchors: [section] "Flan-T5-XXL with threshold-based filtering achieved F-scores of 0.721 and 0.346 on NCIT-DOID and SNOMED-FMA datasets respectively, outperforming BERTMapLt but falling short of BERTMap's performance"; [section] "Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design"; [corpus] Related work shows LLMs are being explored for complex ontology alignment, suggesting ongoing research into their capabilities
- Break condition: When the domain knowledge gap is too large for zero-shot approaches to bridge, even with optimal prompt design and context incorporation.

## Foundational Learning

- Concept: Zero-shot learning in LLMs
  - Why needed here: Understanding that the LLMs are used without any fine-tuning specific to ontology alignment tasks
  - Quick check question: What distinguishes zero-shot from few-shot learning in the context of LLM applications?

- Concept: Ontology alignment evaluation metrics
  - Why needed here: The work uses precision, recall, F-score, Hits@1, MRR, and rejection rate to evaluate performance
  - Quick check question: How does Hits@1 differ from MRR in evaluating ranking-based ontology alignment systems?

- Concept: Structural context in ontologies
  - Why needed here: The work incorporates parent/child concept relationships to improve alignment accuracy
  - Quick check question: Why might hierarchical information in ontologies be more informative than simple string matching for certain concept pairs?

## Architecture Onboarding

- Component map: Prompt generation (template-based) -> LLM inference (Flan-T5-XXL or GPT-3.5-turbo) -> Score thresholding -> Evaluation against ground truth mappings
- Critical path: Prompt generation → LLM inference → Score thresholding → Ranking evaluation
- Design tradeoffs: Zero-shot approaches avoid fine-tuning costs but sacrifice performance compared to fine-tuned models; including structural contexts increases prompt complexity and may confuse the LLM; thresholding improves precision but reduces recall
- Failure signatures: Poor performance when concept labels are semantically similar but structurally distant; when hierarchical contexts provide conflicting information; or when the LLM cannot properly interpret the prompt format
- First 3 experiments:
  1. Test vanilla zero-shot performance with basic prompt (concept labels only) on a small subset to establish baseline
  2. Add threshold-based filtering to assess precision-recall tradeoff on the same subset
  3. Incorporate parent/child contexts to evaluate whether structural information improves alignment accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt design for LLMs in ontology alignment tasks?
- Basis in paper: [explicit] The paper explicitly states that "results suggest that LLM-based OM systems hold the potential to outperform existing ones, but require efforts in prompt design and exploration of optimal presentation methods for ontology contexts."
- Why unresolved: The paper found that different prompt variations (with/without parent/child contexts, with/without thresholding) yielded inconsistent results across datasets, suggesting that prompt design remains an open challenge requiring systematic exploration.
- What evidence would resolve it: Systematic ablation studies comparing different prompt architectures, template variations, and context presentation methods across multiple ontology alignment datasets would identify optimal prompt designs.

### Open Question 2
- Question: How can ontology structural contexts be effectively incorporated into LLM-based ontology alignment?
- Basis in paper: [explicit] The paper notes that "incorporating parent/child contexts does not enhance matching results – this underscores the need for a more in-depth examination of strategies for leveraging ontology contexts."
- Why unresolved: Despite attempting to include hierarchical relationships in prompts, the paper found no performance improvement, indicating that current approaches to incorporating structural context are inadequate.
- What evidence would resolve it: Development and evaluation of novel methods for representing ontology structures (e.g., graph-based representations, hierarchical embeddings, or context-aware prompting) that demonstrably improve alignment performance.

### Open Question 3
- Question: What is the most efficient approach for tuning LLMs for ontology alignment tasks?
- Basis in paper: [explicit] The paper mentions that "future studies include... investigating efficient few-shot tuning" and notes that LLMs are "highly computationally expensive."
- Why unresolved: The paper only explored zero-shot performance, while the computational cost of LLMs makes it challenging to explore fine-tuning approaches systematically.
- What evidence would resolve it: Comparative studies evaluating different few-shot tuning strategies (parameter-efficient fine-tuning, prompt tuning, etc.) against zero-shot performance across multiple ontology alignment benchmarks, measuring both effectiveness and computational efficiency.

## Limitations

- Computational costs restricted evaluation to challenging subsets rather than full Bio-ML datasets
- Zero-shot approach significantly underperforms compared to fine-tuned models like BERTMap
- Effectiveness of prompt design remains uncertain, with inconsistent results across different variations

## Confidence

- High Confidence: The claim that Flan-T5-XXL with threshold-based filtering achieved specific F-scores (0.721 and 0.346) on the tested datasets
- Medium Confidence: The assertion that threshold-based filtering improves precision at the cost of recall
- Low Confidence: The broader claim that LLMs have potential to outperform existing ontology alignment systems given careful framework design

## Next Checks

1. **Prompt Design Optimization**: Systematically test alternative prompt templates and formulations to determine if performance improvements are achievable through better prompt engineering, particularly for incorporating structural contexts.

2. **Dataset Scaling Validation**: Evaluate the same LLM approaches on progressively larger subsets of the Bio-ML datasets to assess whether performance scales proportionally or degrades with increased dataset complexity and size.

3. **Few-Shot Fine-Tuning Comparison**: Implement and compare few-shot fine-tuning approaches against the zero-shot baseline to quantify the performance gap and determine the minimum fine-tuning requirements for competitive results.