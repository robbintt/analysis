---
ver: rpa2
title: 'LeTI: Learning to Generate from Textual Interactions'
arxiv_id: '2305.10314'
source_url: https://arxiv.org/abs/2305.10314
tags:
- leti
- feedback
- code
- textual
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LETI introduces a new LM fine-tuning paradigm that leverages textual
  feedback from automatic evaluators to improve code generation performance. The method
  iteratively fine-tunes the model on a concatenation of natural language instructions,
  LM-generated programs, and textual feedback from a Python interpreter.
---

# LeTI: Learning to Generate from Textual Interactions

## Quick Facts
- arXiv ID: 2305.10314
- Source URL: https://arxiv.org/abs/2305.10314
- Reference count: 40
- Improves code generation performance on MBPP dataset by up to 63.2% compared to pre-trained model

## Executive Summary
LETI (Learning to Generate from Textual Interactions) introduces a new fine-tuning paradigm that leverages textual feedback from automatic evaluators to improve code generation performance. The method iteratively fine-tunes pre-trained language models on a concatenation of natural language instructions, LM-generated programs, and textual feedback from a Python interpreter. By prepending binary reward tokens to differentiate correct and buggy solutions, LETI achieves significant improvements on code generation benchmarks without requiring ground-truth outputs for training.

## Method Summary
LETI fine-tunes pre-trained CodeGen models through iterative feedback-conditioned fine-tuning (FCFT). For each training problem, it generates multiple candidate solutions, evaluates them using a Python interpreter to obtain textual error feedback, and fine-tunes the model on sequences containing reward tokens, instructions, solutions, and feedback. The process interleaves FCFT with continued pre-training on raw code data to prevent catastrophic forgetting. The method achieves improved code generation performance on MBPP and generalizes to unseen problems in HumanEval.

## Key Results
- Achieves up to 63.2% improvement in pass@k metric on MBPP dataset compared to pre-trained model
- Requires fewer than half the gradient steps compared to standard fine-tuning methods
- Generalizes to unseen problems in HumanEval dataset and shows effectiveness on natural language tasks like event argument extraction

## Why This Works (Mechanism)

### Mechanism 1
Iterative fine-tuning on LM-generated solutions with feedback creates a feedback loop that improves code generation quality over successive iterations. LETI repeatedly samples solutions, evaluates them using a Python interpreter to obtain textual feedback (error messages and stack traces), and fine-tunes the model on these error-conditioned sequences. This allows the model to learn to avoid the same types of errors in future generations. Core assumption: Python interpreter provides sufficiently informative and generalizable feedback. Evidence: Fine-tuning on concatenated sequences of instruction, solution, and feedback improves pass@k scores over iterations.

### Mechanism 2
Prepending a binary reward token (good/bad) conditions the model to generate higher-quality solutions when prompted with the good token. By prepending <|good|> or <|bad|> to fine-tuning sequences, LETI aligns correct and buggy solutions with their respective tokens. This allows the model to learn to generate better solutions when conditioned on <|good|> in future iterations. Core assumption: Model can effectively partition its probability space to associate reward tokens with solution quality. Evidence: Binary tokens differentiate correct from buggy solutions during fine-tuning.

### Mechanism 3
Regularization via continued pre-training on raw code data prevents catastrophic forgetting of the model's original capabilities during fine-tuning. LETI interleaves feedback-conditioned fine-tuning with LM objective optimization on a pre-training dataset. This helps maintain the model's performance on tasks it was originally trained for (e.g., reasoning, chain-of-thought). Core assumption: Pre-training dataset is sufficiently large and diverse to preserve general language understanding. Evidence: Ablation study shows regularization is essential to maintain LM's original capability on untrained tasks.

## Foundational Learning

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: LETI is conceptually similar to RLHF but uses textual feedback from an automatic evaluator instead of human feedback. Understanding RLHF helps grasp how LETI uses feedback to guide the model.
  - Quick check question: What is the main difference between RLHF and LETI in terms of the source of feedback?

- **Concept:** Feedback-Conditioned Fine-Tuning (FCFT)
  - Why needed here: FCFT is the core training method used in LETI. It fine-tunes the model on sequences that include the natural language instruction, the generated solution, and the textual feedback.
  - Quick check question: How does FCFT differ from standard supervised fine-tuning on input-output pairs?

- **Concept:** Pass@k Metric
  - Why needed here: Pass@k is the evaluation metric used to assess code generation performance in LETI. It measures the percentage of problems for which at least one of k generated solutions passes all test cases.
  - Quick check question: Why is pass@k a more suitable metric than exact match accuracy for code generation tasks?

## Architecture Onboarding

- **Component map:** Pre-trained LM -> Solution Evaluator (Python interpreter) -> FCFT Trainer -> Pre-training Dataset
- **Critical path:** Generate solutions → Evaluate with Python interpreter → Fine-tune on feedback-conditioned sequences (FCFT) → Regularize with continued pre-training → Repeat for multiple iterations
- **Design tradeoffs:** Exploration vs computational cost (number of solutions per problem), richness of feedback (textual vs binary) vs model complexity, regularization strength vs adaptation speed
- **Failure signatures:** No improvement in pass@k over iterations, degradation in performance on out-of-domain tasks, generation of syntactically incorrect code
- **First 3 experiments:** 1) Implement minimal LETI with small LM and binary feedback mechanism, 2) Evaluate impact of different numbers of solutions per problem on performance, 3) Compare effectiveness of textual vs binary feedback in improving pass@k

## Open Questions the Paper Calls Out

### Open Question 1
How does LETI performance scale with model size beyond 2B parameters? The paper suggests larger models might show more significant improvements but did not experiment beyond 2B due to computational constraints. What evidence would resolve it: Empirical results on 6B and 16B parameter models comparing iteration efficiency and final pass@k scores.

### Open Question 2
Can textual feedback from sources other than Python interpreters be equally effective for code generation? The study focused solely on Python interpreter feedback without exploring other automatic or human feedback sources. What evidence would resolve it: Comparative experiments using feedback from different sources (static code analyzers, human reviewers) and measuring impact on performance.

### Open Question 3
How does the choice of solution evaluator implementation affect optimization of task-specific metrics in NLP tasks? The paper demonstrates the effect but does not systematically explore different evaluator designs. What evidence would resolve it: Systematic experiments varying evaluator implementation and measuring resulting changes in metrics like precision, recall, and F1 scores.

## Limitations

- Effectiveness depends on quality and informativeness of Python interpreter feedback, which may be too generic for complex error correction
- Binary reward token conditioning mechanism effectiveness not thoroughly evaluated for generalization
- Regularization through continued pre-training may not prevent catastrophic forgetting across all out-of-domain tasks

## Confidence

- **High Confidence:** MBPP and HumanEval experimental results are well-supported with empirical evidence showing up to 63.2% improvement
- **Medium Confidence:** Claims about textual feedback improving sample efficiency have some support but lack detailed quantification
- **Low Confidence:** Claims about regularization preventing catastrophic forgetting based on limited ablation study

## Next Checks

1. Evaluate the informativeness of textual feedback by conducting human evaluation to assess whether Python interpreter feedback is sufficiently informative for error correction across different problem types.

2. Analyze the impact of reward token conditioning by investigating whether the model truly learns to associate tokens with solution quality or simply memorizes token-solution pairs, potentially testing with more nuanced reward systems.

3. Test the robustness of regularization by evaluating LETI with and without continued pre-training on a wider range of out-of-domain tasks (summarization, question answering) to assess catastrophic forgetting prevention.