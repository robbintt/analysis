---
ver: rpa2
title: 'Impossible Distillation: from Low-Quality Model to High-Quality Dataset &
  Model for Summarization and Paraphrasing'
arxiv_id: '2305.16635'
source_url: https://arxiv.org/abs/2305.16635
tags:
- dataset
- pairs
- generation
- summarization
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Impossible Distillation, a novel framework
  for distilling a high-quality dataset and model for summarization and paraphrasing
  from a low-quality teacher model that itself cannot reliably perform these tasks.
  The key idea is to leverage the paraphrastic proximity intrinsic to pre-trained
  language models (LMs) and search for paraphrases in the LM distribution using constrained
  decoding and task-specific filters.
---

# Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing

## Quick Facts
- **arXiv ID:** 2305.16635
- **Source URL:** https://arxiv.org/abs/2305.16635
- **Reference count:** 40
- **Key outcome:** Distills a high-quality dataset and model for summarization and paraphrasing from a low-quality teacher model that cannot reliably perform these tasks

## Executive Summary
This paper introduces Impossible Distillation, a novel framework that leverages the paraphrastic proximity intrinsic to pre-trained language models to generate high-quality datasets for summarization and paraphrasing, even when the source model cannot perform these tasks reliably. The approach uses constrained decoding with task-specific filters to search for paraphrases in the LM distribution, then trains a student model on this generated dataset and amplifies its capability through self-distillation. Experiments show that a 770M parameter model distilled using this method consistently outperforms strong baselines including models distilled from ChatGPT and sometimes even ChatGPT itself, while the LM-generated dataset exhibits higher diversity and fidelity than up to 13x larger human-authored datasets.

## Method Summary
Impossible Distillation is a two-stage framework that first generates candidate paraphrase and summary pairs from a pre-trained language model using constrained decoding (Neurologic algorithm with lexical constraints for sequential generation and nucleus sampling for parallel generation), then filters these candidates using task-specific filters (NLI-based entailment, length, and diversity filters) to create an initial dataset. A student model is trained on this dataset, and then self-distillation is performed where the trained model generates new candidate pairs that are filtered and used to train an amplified task model. The method introduces controllability by quantizing generated pairs into discrete bins based on controllable attributes like length and lexical constraints, allowing for controllable text generation.

## Key Results
- A 770M parameter model distilled using Impossible Distillation consistently outperforms strong baselines including models distilled from ChatGPT and sometimes even ChatGPT itself
- The LM-generated dataset (DIMSUM+) exhibits higher diversity and fidelity than up to 13x larger human-authored datasets
- The framework enables controllable text generation through quantization of generated pairs into discrete bins based on controllable attributes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Paraphrastic proximity intrinsic to pre-trained LMs creates high-density subspaces where paraphrases naturally cluster, enabling efficient generation without external supervision.
- **Mechanism:** Constrained decoding (Neurologic algorithm) searches within these proximal subspaces to generate valid paraphrase pairs, reducing the search space from the entire LM distribution to a paraphrastically coherent region.
- **Core assumption:** Paraphrases occupy proximal subspaces in the LM distribution even when the LM itself cannot reliably perform the task.
- **Evidence anchors:**
  - [abstract] "We hypothesize and verify the paraphrastic proximity intrinsic to pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in the LM distribution."
  - [section 2.1.1] "Inspired by an empirical observation that good summaries and paraphrases tend to preserve salient keywords in the original sentence"
  - [corpus] Weak - the corpus search found no direct evidence for this specific mechanism; relies on paper's claim
- **Break condition:** If paraphrases don't naturally cluster in LM distributions, or if constrained decoding cannot effectively navigate to these subspaces, the generation efficiency collapses.

### Mechanism 2
- **Claim:** Sequential generation with lexical constraints preserves salient spans of input text in surface form, while parallel generation with stochastic decoding provides abstractive diversity, together enriching dataset coverage.
- **Mechanism:** Two complementary generation strategies (sequential with keyword constraints vs. parallel with low-temperature sampling) capture different paraphrasing styles and summarization strategies.
- **Core assumption:** Good summaries/paraphrases can be generated through both extractive (keyword-preserving) and abstractive (stochastic sampling) approaches.
- **Evidence anchors:**
  - [section 2.1.1] "Inspired by an empirical observation that good summaries and paraphrases tend to preserve salient keywords in the original sentence"
  - [section 2.1.1] "while parallel generation results in a diverse, abstractive set of pairs"
  - [corpus] Weak - no corpus evidence directly supports this dual-generation mechanism claim
- **Break condition:** If one generation strategy dominates or fails to produce valid pairs, dataset diversity and quality suffer.

### Mechanism 3
- **Claim:** Self-distillation amplifies task capability by having the initial task model generate candidate pairs, then filtering and training on its own high-quality generations.
- **Mechanism:** Task model M1 generates pairs using its learned distribution, which are filtered and used to train M2, creating an iterative capability amplification loop.
- **Core assumption:** Initial task model can generate sufficiently high-quality candidates for effective self-distillation.
- **Evidence anchors:**
  - [abstract] "By training a student model on the generated dataset and amplifying its capability through self-distillation"
  - [section 2.2] "Using the same filters as the previous stage, we filter the high-quality pairs into D1. Finally, we fine-tune M1 on D1, yielding the end-stage model M2."
  - [corpus] Weak - no corpus evidence supports this specific self-distillation mechanism
- **Break condition:** If M1 cannot generate candidates that pass quality filters, or if self-distillation causes collapse into repetitive patterns.

## Foundational Learning

- **Concept:** Paraphrase generation and summarization task definitions
  - Why needed here: The entire framework depends on correctly defining what constitutes valid paraphrases and summaries to construct appropriate filters
  - Quick check question: Can you articulate the difference between bidirectional entailment for paraphrases versus unidirectional entailment for summaries?
- **Concept:** Knowledge distillation and student-teacher model relationships
  - Why needed here: The framework is fundamentally built on distilling task knowledge from an initial LM to a task-specific student model
  - Quick check question: What distinguishes supervised distillation from the proposed decoding-guided and self-distillation approaches?
- **Concept:** Constrained decoding algorithms and lexical constraint satisfaction
  - Why needed here: Neurologic decoding is central to the sequential generation process for preserving keywords while maintaining grammatical coherence
  - Quick check question: How does Neurologic decoding differ from standard beam search when handling lexical constraints?

## Architecture Onboarding

- **Component map:** Teacher LM (GPT-2/CTRL/BioGPT) → Constrained generation → Candidate pair pool → Task-specific filters → Initial dataset D0 → Student model M0 → Task model M1 → Self-distillation generation → Filtered pairs D1 → Amplified model M2
- **Critical path:** Teacher LM → Constrained generation → Filtering → Training → Self-distillation → Final evaluation
- **Design tradeoffs:**
  - Generation diversity vs. filter precision: More candidates increase diversity but require stricter filters
  - Filter strictness vs. dataset size: Tighter filters produce higher quality but smaller datasets
  - Controllability quantization vs. model complexity: More control codes enable finer control but increase training complexity
- **Failure signatures:**
  - Low sample efficiency in constrained generation indicates poor paraphrase proximity
  - High false positive rate in filtering suggests filters are too lenient
  - Poor transfer performance indicates insufficient domain coverage in generated data
- **First 3 experiments:**
  1. Measure sample efficiency and ROUGE-L for sequential vs. parallel generation on a small seed set
  2. Test filter thresholds on validation pairs to find optimal precision-recall tradeoff
  3. Compare M1 performance with/without quantization to verify controllability benefit

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the diversity of the LM-generated dataset (DIMSUM+) compare to human-authored datasets in terms of summarization strategy (abstractiveness and compression ratio)?
- **Basis in paper:** [explicit] The paper states that DIMSUM+ exhibits higher diversity and fidelity than up to 13 times larger human-authored datasets.
- **Why unresolved:** The paper provides qualitative evidence (e.g., Figure 4) but does not provide quantitative metrics to directly compare the diversity of summarization strategies between DIMSUM+ and human-authored datasets.
- **What evidence would resolve it:** A quantitative comparison of the distribution of abstractiveness and compression ratio between DIMSUM+ and human-authored datasets, such as Gigaword, would provide a clearer understanding of the diversity of the LM-generated dataset.

### Open Question 2
- **Question:** Can IMPOSSIBLE DISTILLATION be effectively applied to tasks beyond summarization and paraphrasing, such as translation or longer-form text generation?
- **Basis in paper:** [inferred] The paper mentions that IMPOSSIBLE DISTILLATION could be adapted for a broader range of tasks by redefining the pair generation constraints and task-specific filters.
- **Why unresolved:** The paper focuses on summarization and paraphrasing, and does not provide experimental results or analysis for other tasks like translation or longer-form text generation.
- **What evidence would resolve it:** Experimental results demonstrating the effectiveness of IMPOSSIBLE DISTILLATION for tasks such as translation or paragraph-level summarization would validate its applicability beyond the scope of the current study.

### Open Question 3
- **Question:** How does the quality of the task-specific filters (e.g., NLI model, lexical constraints) impact the overall performance of IMPOSSIBLE DISTILLATION?
- **Basis in paper:** [explicit] The paper mentions that the filters are frozen throughout the distillation pipeline and may not always be accessible for wider range of tasks.
- **Why unresolved:** The paper does not provide an analysis of how the quality or choice of filters affects the performance of the distilled model or the quality of the generated dataset.
- **What evidence would resolve it:** An ablation study or sensitivity analysis exploring the impact of different filter choices (e.g., NLI models, lexical constraint algorithms) on the performance of IMPOSSIBLE DISTILLATION would shed light on the importance of filter quality.

## Limitations

- The core mechanism relies on the unproven assumption that paraphrases naturally cluster in LM distributions, which is not rigorously justified
- The method depends heavily on the quality and accessibility of task-specific filters, which may not generalize to all tasks
- The framework's effectiveness for tasks beyond summarization and paraphrasing remains untested

## Confidence

- **High Confidence:** The empirical results showing superior performance on standard benchmarks (ROUGE scores, BERTScore) and the demonstration that a 770M model can outperform much larger models including ChatGPT
- **Medium Confidence:** The proposed mechanisms for why paraphrastic proximity enables effective distillation, as these are plausible but not rigorously proven
- **Low Confidence:** The claim about dataset diversity being higher than human-authored datasets, as diversity metrics can be gamed and the comparison methodology is not fully transparent

## Next Checks

1. Test the paraphrastic proximity hypothesis directly by measuring cosine similarity in embedding space between paraphrases vs. non-paraphrases across multiple LM architectures to verify if paraphrases cluster more closely than random sentence pairs
2. Conduct ablation studies varying the generation temperature and constraint strength to determine the optimal tradeoff between diversity and quality in the candidate generation phase
3. Perform human evaluation studies comparing Impossible Distillation outputs with outputs from larger models on tasks where the method claims to outperform, to verify if automatic metrics align with human judgments