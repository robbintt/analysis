---
ver: rpa2
title: Personality Traits in Large Language Models
arxiv_id: '2307.00184'
source_url: https://arxiv.org/abs/2307.00184
tags:
- personality
- validity
- llms
- ipip-neo
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a novel methodology for administering validated
  psychometric personality tests to large language models (LLMs) and establishing
  the construct validity of the resulting scores. The approach involves prompting
  LLMs with personality survey items and simulated human personas to generate responses,
  then analyzing the responses using psychometric reliability and validity metrics.
---

# Personality Traits in Large Language Models

## Quick Facts
- arXiv ID: 2307.00184
- Source URL: https://arxiv.org/abs/2307.00184
- Reference count: 40
- Key outcome: This paper develops a novel methodology for administering validated psychometric personality tests to large language models (LLMs) and establishing the construct validity of the resulting scores.

## Executive Summary
This paper presents a comprehensive method for administering validated psychometric tests to large language models (LLMs) to quantify, analyze, and shape personality traits exhibited in their text outputs. The approach involves prompting LLMs with personality survey items and simulated human personas to generate responses, then analyzing the responses using psychometric reliability and validity metrics. The authors apply this methodology to 18 different LLMs, demonstrating that personality traits simulated in LLM outputs can be reliable and valid, especially for larger and instruction fine-tuned models. They also show that personality in LLM outputs can be shaped along desired dimensions by prompting with trait-related adjectives at different intensity levels.

## Method Summary
The authors develop a method for administering validated psychometric personality tests to LLMs by leveraging their ability to complete prompts. They use controlled variations in prompting, including persona descriptions, item preambles, and postambles, to simulate population variance in LLM responses. The LLM responses are then scored using scoring mode, and statistical analyses are performed to assess the reliability (Cronbach's alpha, Guttman's lambda 6, McDonald's omega) and validity (convergent, discriminant, criterion) of the resulting personality profiles. The method is applied to 18 LLMs of varying sizes and fine-tuning levels, using the IPIP-NEO and BFI personality inventories, as well as external validity measures such as PANAS, BPAQ, SSCS, and PVQ-RR.

## Key Results
- The methodology establishes construct validity for LLM personality by applying psychometric principles to LLM responses, with stronger evidence of reliability and validity for larger and instruction fine-tuned models.
- Personality in LLM outputs can be reliably shaped through prompt engineering using trait-relevant adjectives at different intensity levels, achieving granular control over personality trait levels.
- The lexical hypothesis holds for LLMs, with trait-relevant adjectives in prompts effectively influencing LLM personality trait levels.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The methodology establishes construct validity for LLM personality by applying psychometric principles to LLM responses.
- Mechanism: The method simulates population variance in LLM responses through controlled prompting (descriptive personas, item preambles, item postambles) allowing statistical relationships between personality and external correlates to be assessed.
- Core assumption: LLM responses to survey items are independent events when each item is scored separately.
- Evidence anchors:
  - [abstract] "present a comprehensive method for administering validated psychometric tests and quantifying, analyzing, and shaping personality traits exhibited in text generated from widely-used LLMs"
  - [section] "To administer a psychometric test to LLMs, we leverage their ability to complete a prompt. In our context, a given prompt instructs a given LLM to rate an item (i.e., a descriptive statement; e.g., "I am the life of the party.") from a psychometric test using a standardized response scale."
- Break condition: If LLM responses show strong dependencies between items or if the variance simulation through prompting fails to create meaningful variation across prompts.

### Mechanism 2
- Claim: LLM personality traits can be reliably shaped through prompt engineering using trait-relevant adjectives at different intensity levels.
- Mechanism: The method leverages the lexical hypothesis, expecting that LLMs are most responsive to prompts containing trait-relevant language, and uses linguistic qualifiers (e.g., "extremely," "very," "a bit") to achieve granular control over personality trait levels.
- Core assumption: Trait-relevant adjectives and linguistic qualifiers in prompts can reliably influence LLM personality trait levels.
- Evidence anchors:
  - [abstract] "personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles"
  - [section] "To achieve more precise control of personality levels, we hypothesize that the linguistic qualifiers often used in Likert-type response scales...are useful for setting up a target level of each adjective."
- Break condition: If LLM responses do not show consistent changes in personality trait levels when prompted with different intensity levels of trait adjectives.

### Mechanism 3
- Claim: Larger and instruction fine tuned LLMs produce more reliable and valid personality profiles.
- Mechanism: The method shows that as model size increases and with instruction fine tuning, internal consistency, convergent validity, and criterion validity of LLM-simulated personality test scores improve.
- Core assumption: Model size and instruction fine tuning directly impact the ability of LLMs to generate human-aligned personality responses.
- Evidence anchors:
  - [abstract] "evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models"
  - [section] "At 62B parameters, the base PaLM model showed nearly uniform personality score distribution...Instruction-tuned variants, Flan-PaLM and Flan-PaLMChilla, showed more normal distributions of personality, with lower kurtosis."
- Break condition: If personality reliability and validity do not improve with model size or if instruction fine tuning does not enhance the alignment with human personality profiles.

## Foundational Learning

- Concept: Psychometric test administration and validation
  - Why needed here: The methodology relies on administering validated psychometric tests to LLMs and establishing their construct validity using established psychometric metrics.
  - Quick check question: What are the key subtypes of construct validity in psychometrics, and how are they assessed?
- Concept: Large language model prompting and inference modes
  - Why needed here: The methodology uses different prompting techniques and LLM inference modes (generative and scoring) to administer psychometric tests and shape personality traits.
  - Quick check question: How do generative and scoring modes differ in LLM inference, and when would each be used in the context of administering psychometric tests?
- Concept: Statistical analysis of reliability and validity
  - Why needed here: The methodology requires computing reliability metrics (Cronbach's alpha, Guttman's lambda 6, McDonald's omega) and validity correlations (convergent, discriminant, criterion) to establish the construct validity of LLM personality profiles.
  - Quick check question: What are the differences between Cronbach's alpha, Guttman's lambda 6, and McDonald's omega in assessing internal consistency, and when would each be used?

## Architecture Onboarding

- Component map: Prompt generation (with personas, preambles, postambles) -> LLM inference (generative or scoring mode) -> Response scoring -> Statistical analysis (reliability and validity metrics) -> Validation results -> Personality shaping (if applicable)
- Critical path: Prompt generation → LLM inference → Response scoring → Statistical analysis → Validation results → Personality shaping (if applicable)
- Design tradeoffs: Using scoring mode for response evaluation ensures independence between items but may limit the richness of generated text. Using descriptive personas introduces variance but may not fully capture the complexity of human personality. Larger models provide better reliability and validity but require more computational resources.
- Failure signatures: If LLM responses show low internal consistency (Cronbach's alpha < 0.70), poor convergent validity (correlations with redundant measures < 0.60), or lack criterion validity (no correlation with theoretically related external measures), the methodology may not be establishing valid personality constructs.
- First 3 experiments:
  1. Administer IPIP-NEO and BFI personality tests to a base PaLM 62B model using different prompt variations (personas, preambles, postambles) and assess internal consistency and unidimensionality.
  2. Compare the reliability and validity of personality profiles generated by base PaLM 62B, Flan-PaLM 62B, and Flan-PaLMChilla 62B models to evaluate the impact of instruction fine tuning.
  3. Use trait adjectives and intensity levels to shape personality in Flan-PaLM 540B and assess the effectiveness of the shaping method by computing correlations between prompted levels and observed IPIP-NEO scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the psychometric properties of personality traits in LLMs vary across different model families beyond the PaLM family explored in this study?
- Basis in paper: [explicit] The authors state that their methodology for administering psychometric surveys is applicable to any decoder-only architecture model, not just PaLM.
- Why unresolved: The study only evaluated personality traits in PaLM family models. Other popular decoder-only models like GPT were not tested.
- What evidence would resolve it: Administering the same psychometric surveys to a diverse set of decoder-only models (e.g., GPT, LLaMA) and comparing their reliability, validity, and malleability results.

### Open Question 2
- Question: To what extent do personality traits in LLM-generated text generalize across different languages and cultures?
- Basis in paper: [explicit] The authors acknowledge that their study only considered English and did not make cultural considerations beyond the applied psychometrics. Most psychometric tests have been validated in cross-cultural research.
- Why unresolved: The study only evaluated personality traits in English. Cultural differences in personality expression and interpretation were not explored.
- What evidence would resolve it: Administering the same psychometric surveys to LLMs in different languages and cultures, and comparing the results to the English findings.

### Open Question 3
- Question: How do personality traits in LLM-generated text change over time as the model is fine-tuned on new data?
- Basis in paper: [inferred] The authors demonstrate that personality traits in LLMs can be shaped through prompting, but do not explore how continuous fine-tuning on new data affects these traits.
- Why unresolved: The study only evaluated personality traits in a static model. The dynamic nature of personality in LLMs was not investigated.
- What evidence would resolve it: Continuously fine-tuning an LLM on new data and periodically administering psychometric surveys to track changes in personality trait scores over time.

## Limitations

- **Sampling bias from prompt design**: The study relies on controlled prompt variations to simulate population variance in LLM responses, which may not fully capture the complexity and diversity of human personality.
- **Reliability of scoring mode implementation**: The exact implementation details of the scoring mode and how responses are mapped to Likert scale options are unclear, introducing uncertainty about the reliability and consistency of the personality trait measurements.
- **Limited corpus evidence**: The related work corpus search yielded a relatively weak evidence base, with few supporting citations from related work, suggesting that the approach may be novel but untested in the broader literature.

## Confidence

**High confidence**: The general methodology of administering psychometric tests to LLMs and analyzing the responses using reliability and validity metrics is sound and well-established in the field of psychometrics.

**Medium confidence**: The claim that larger and instruction fine-tuned LLMs produce more reliable and valid personality profiles is supported by the experimental results but relies on the specific models and tests used in the study.

**Low confidence**: The claim that personality in LLM outputs can be reliably shaped through prompt engineering using trait-relevant adjectives at different intensity levels is based on the lexical hypothesis and may depend on the LLM's training data and architecture.

## Next Checks

1. **Prompt template validation**: Conduct a sensitivity analysis by varying the prompt templates and persona descriptions used in the experiments. Compare the reliability and validity metrics across different prompt variations to assess the robustness of the approach and identify optimal prompt designs.

2. **Scoring mode verification**: Implement the scoring mode for response evaluation using a transparent and reproducible method. Compare the results with alternative scoring approaches to verify the reliability and consistency of the personality trait measurements.

3. **Generalizability assessment**: Apply the methodology to a diverse set of LLMs, personality inventories, and external validity measures not used in the original study. Evaluate the reliability and validity of the resulting personality profiles to assess the generalizability of the approach across different models, tests, and domains.