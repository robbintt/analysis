---
ver: rpa2
title: Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel's
  Spectrum
arxiv_id: '2307.14531'
source_url: https://arxiv.org/abs/2307.14531
tags:
- neural
- kernel
- networks
- bias
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Modified Spectrum Kernels (MSKs) to manipulate
  the spectral bias of wide neural networks, enabling accelerated convergence of gradient
  descent. MSKs allow approximating kernel matrices with desired eigenvalues, even
  when closed-form solutions are unknown.
---

# Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel's Spectrum

## Quick Facts
- arXiv ID: 2307.14531
- Source URL: https://arxiv.org/abs/2307.14531
- Authors: 
- Reference count: 40
- Primary result: Introduces Modified Spectrum Kernels (MSKs) to accelerate gradient descent convergence for wide neural networks by manipulating spectral bias, achieving polynomial or exponential speedups without altering final predictions.

## Executive Summary
This paper introduces Modified Spectrum Kernels (MSKs) to control the spectral bias of wide neural networks during training. By leveraging the Neural Tangent Kernel (NTK) framework, the authors propose a preconditioned gradient descent method that modifies the training dynamics to accelerate convergence. The approach allows approximating kernel matrices with desired eigenvalues even when closed-form solutions are unknown, enabling faster learning of high-frequency functions. Critically, the method maintains consistency - the preconditioning does not change the final network prediction on test points compared to standard gradient descent.

## Method Summary
The method constructs a preconditioner S by modifying the top k eigenvalues of the NTK matrix K using a Lipschitz continuous function g, while preserving the eigenvectors. Preconditioned gradient descent then follows linear dynamics characterized by KS instead of K, accelerating convergence in specific eigen-directions. The MSK framework approximates target kernels by applying g to the eigenvalues of an existing kernel. The approach is computationally efficient, requiring only the top k eigenvalues and eigenvectors of the NTK matrix. The method is validated on synthetic data where it demonstrates significant acceleration in learning high-frequency functions compared to standard gradient descent.

## Key Results
- Preconditioned gradient descent accelerates convergence by modifying spectral bias without changing final predictions
- MSKs enable approximating kernel matrices with desired eigenvalues even when closed-form solutions are unknown
- Experiments show polynomial or exponential speedups in learning high-frequency functions on synthetic data

## Why This Works (Mechanism)

### Mechanism 1
MSK construction manipulates the spectrum of an existing kernel by applying a function g to its eigenvalues while keeping eigenfunctions unchanged. This enables construction of new kernels with nearly arbitrary spectral properties when closed-form solutions are unknown. The method assumes eigenfunctions can be evaluated at data points and g is Lipschitz continuous.

### Mechanism 2
Preconditioned gradient descent uses a preconditioning matrix S that shares eigenvectors with the NTK matrix K but has modified eigenvalues. This changes the linear dynamics from being governed by K to being governed by KS, allowing control over convergence speed in each eigen-direction. The approach assumes the network operates in the NTK regime and S is positive definite.

### Mechanism 3
The method is consistent - it does not change final network predictions compared to standard gradient descent. PGD optimizes a preconditioned loss equivalent to kernel ridge regression with modified regularization. As regularization approaches zero, the solution converges to the same minimizer as standard gradient descent. This assumes sufficient training width and iterations.

## Foundational Learning

- Neural Tangent Kernel (NTK) and its relation to wide neural networks: Understanding NTK is crucial to grasp how PGD modifies training dynamics. Quick check: What is the NTK matrix and how does it characterize training dynamics of wide neural networks?

- Kernel ridge regression and its solution: Familiarity with KRR is necessary to understand theoretical results. Quick check: How is the solution to kernel ridge regression expressed in terms of the kernel matrix and its eigenvalues/eigenvectors?

- Eigenvalue decomposition and spectral analysis: Essential for following mathematical arguments about spectral properties. Quick check: How does eigenvalue decomposition of a kernel matrix relate to its spectral properties and representable functions?

## Architecture Onboarding

- Component map: MSK construction -> PGD algorithm -> Consistency analysis
- Critical path: 1) Construct MSK by applying g to NTK eigenvalues, 2) Build preconditioning matrix S from top k eigenvalues/eigenvectors, 3) Implement PGD with S, 4) Verify consistency by comparing predictions
- Design tradeoffs: Choosing g affects convergence speed per eigen-direction; selecting k balances efficiency and manipulation; learning rate must be smaller than 2/(λmin(KS) + λmax(KS))
- Failure signatures: Slow convergence from poor S choice or insufficient width; inconsistent predictions from insufficient training; numerical instability from ill-conditioned K
- First 3 experiments: 1) Validate MSK approximation by comparing MSK and target kernel matrices, 2) Test PGD acceleration on synthetic data with known spectral properties, 3) Verify consistency by comparing PGD and standard GD predictions on test points

## Open Questions the Paper Calls Out

### Open Question 1
How does MSK perform on real-world datasets beyond synthetic data? The authors demonstrate on synthetic data but mention future work on real-world scenarios. Empirical validation on benchmark datasets like CIFAR or regression tasks would verify practical effectiveness.

### Open Question 2
What is the theoretical relationship between the number of modified eigenvalues (k) and convergence speedup? The authors suggest k may be chosen small for efficiency but provide no theoretical guidelines. A theoretical analysis deriving this relationship would be valuable.

### Open Question 3
How does MSK compare to other preconditioning techniques in terms of generalization performance? The paper focuses on convergence speed but doesn't explore how preconditioning affects generalization. Empirical studies comparing test error would address this trade-off.

## Limitations

- Theoretical claims rely heavily on NTK regime assumptions, potentially limiting practical applicability to moderately sized networks
- Empirical validation is currently limited to synthetic data, leaving real-world performance unverified
- Critical design choices (eigenvalue modification function g and number of eigenvalues k) are not thoroughly explored
- Computational overhead of computing and storing NTK matrix may be prohibitive for large datasets

## Confidence

- MSK Approximation: High
- PGD Acceleration: Medium
- Consistency: High

## Next Checks

1. **Scalability Assessment**: Evaluate the method on larger, real-world datasets to assess computational feasibility and practical performance beyond synthetic settings.

2. **NTK Regime Validation**: Systematically test the method's performance as a function of network width to quantify the departure from NTK assumptions.

3. **Hyperparameter Sensitivity**: Conduct ablation studies varying the eigenvalue modification function g and the number of eigenvalues k to understand their impact on convergence and generalization.