---
ver: rpa2
title: Can Programming Languages Boost Each Other via Instruction Tuning?
arxiv_id: '2308.16824'
source_url: https://arxiv.org/abs/2308.16824
tags:
- code
- programming
- languages
- language
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report explores whether programming languages can boost each
  other during the instruction fine-tuning phase of code large language models. The
  authors craft training data for eight popular programming languages (Python, JavaScript,
  TypeScript, C, C++, Java, Go, HTML) and fine-tune StarCoder on each language separately.
---

# Can Programming Languages Boost Each Other via Instruction Tuning?

## Quick Facts
- arXiv ID: 2308.16824
- Source URL: https://arxiv.org/abs/2308.16824
- Reference count: 6
- Key outcome: Instruction tuning on one programming language can significantly improve code generation performance in other languages, with improvement magnitude related to language similarity

## Executive Summary
This paper investigates whether programming languages can mutually enhance each other's code generation performance during instruction fine-tuning of code large language models. The authors fine-tune StarCoder on eight popular programming languages separately and evaluate each model's performance across all languages. Surprisingly, they find that instruction tuning on one language can significantly improve performance in other languages, even when those languages are quite different (e.g., HTML improving Java performance by 15.24% pass@1). The results suggest that instruction tuning unlocks the model's inherent capabilities for understanding programming concepts and following instructions, rather than just incorporating new language-specific knowledge.

## Method Summary
The authors fine-tune StarCoder 7B and 15B base models on instruction-answer pairs for eight programming languages: Python, JavaScript, TypeScript, C, C++, Java, Go, and HTML. Each language has approximately 9K instruction-answer pairs generated through in-depth and in-breadth evolution processes. The fine-tuning uses DeepSpeed with fp16, batch size of 2 per GPU, learning rate of 2e-5 with cosine annealing, gradient accumulation steps of 4, and warmup steps of 30. They also create a multilingual training set of 9K instances covering all 8 languages (~1.2K instances per language) and evaluate all models on HumanEval-X and newly created HumanEval-C and HumanEval-TypeScript benchmarks.

## Key Results
- CodeM-Python 15B trained on Python increased Java performance by 17.95% absolute pass@1 on HumanEval-X
- CodeM-HTML 7B trained on HTML corpus improved Java by 15.24% absolute pass@1, demonstrating cross-language transfer even between very different languages
- Multilingual instruction tuning (CodeM-Mixed) outperformed monolingual models on their respective target languages, even surpassing CodeM-Python on HumanEval-Python and CodeM-Java on HumanEval-Java
- C and C++ showed stronger mutual improvement than cross-language pairs, as did JavaScript and TypeScript, suggesting language similarity affects transfer effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning unlocks inherent cross-language understanding rather than just learning new language-specific knowledge.
- Mechanism: During instruction tuning, the model learns to follow instructions and understand programming concepts at a higher level of abstraction. This abstract understanding transfers across languages because many programming concepts are fundamentally similar regardless of syntax.
- Core assumption: Programming languages share common underlying concepts and problem-solving patterns that can be abstracted during instruction tuning.
- Evidence anchors:
  - [abstract] "Our findings suggest that instruction tuning unlocks the model's inherent potential, such as natural or programming language understanding and following-instruction capabilities, rather than merely incorporating new knowledge."
  - [section] "Drawing upon these observations, we conjecture that the improvement in multilingual code generation performance is predominantly due to instruction tuning unlocking the model's inherent potential, such as natural or programming language understanding and following-instruction capabilities, rather than merely incorporating new knowledge."
- Break condition: If languages are too dissimilar in syntax and semantics (e.g., HTML vs. C++), the cross-language transfer would be minimal or nonexistent.

### Mechanism 2
- Claim: Language similarity determines the degree of cross-language improvement during instruction tuning.
- Mechanism: Programming languages that share similar design principles, syntax patterns, or paradigms transfer knowledge more effectively during instruction tuning. The model leverages commonalities between languages to improve performance.
- Core assumption: Language similarity correlates with shared programming concepts and patterns that can be transferred during instruction tuning.
- Evidence anchors:
  - [abstract] "we found that the improvement margin is related to the language similarity between them, with C and C++ boosting each other more significantly, as do JavaScript and TypeScript."
  - [section] "More surprisingly, CODE M-HTML 7B trained on the markup language HTML also can achieve an absolute 15.24% pass@1 improvement in Java."
- Break condition: If languages have completely different paradigms or syntax with no shared concepts, the improvement would be negligible regardless of instruction tuning.

### Mechanism 3
- Claim: Multilingual training data in instruction tuning can improve overall performance without harming language-specific capabilities.
- Mechanism: Exposure to multiple languages during instruction tuning helps the model develop more robust language-agnostic instruction-following capabilities, while also learning to switch between language contexts effectively.
- Core assumption: Multilingual instruction tuning enhances the model's ability to generalize across languages rather than confusing it with conflicting syntax rules.
- Evidence anchors:
  - [abstract] "we further construct a 9K multilingual training set covering 8 programming languages. Although each language comprises only a small amount (~1.2K) of training instances, experimental findings suggest that CODE M-Mixed excels in all languages, even surpassing CODE M-Python on HumanEval-Python and CODE M-Java on HumanEval-Java."
- Break condition: If the multilingual training set is too small or poorly balanced, it might not provide enough signal for effective cross-language learning.

## Foundational Learning

- Concept: Language similarity and transfer learning
  - Why needed here: Understanding how programming languages relate to each other is crucial for grasping why instruction tuning on one language can improve performance in others
  - Quick check question: Why do you think C and C++ would boost each other more than Java and HTML during instruction tuning?

- Concept: Instruction tuning methodology
  - Why needed here: The core mechanism relies on instruction tuning rather than traditional pretraining or fine-tuning approaches
  - Quick check question: What's the key difference between instruction tuning and standard fine-tuning approaches?

- Concept: Cross-lingual transfer in NLP
  - Why needed here: The concept extends established NLP principles about cross-lingual transfer to the programming language domain
  - Quick check question: How might the principles of cross-lingual transfer in natural languages apply to programming languages?

## Architecture Onboarding

- Component map: StarCoder base model → Instruction tuning on monolingual/multilingual corpora → Evaluation across multiple programming languages
- Critical path: Data preparation → Instruction tuning training → Model evaluation → Analysis of cross-language improvements
- Design tradeoffs: Monolingual vs. multilingual training data; model scale (7B vs 15B); evaluation benchmarks across languages
- Failure signatures: No cross-language improvement; degradation in target language performance; training instability during instruction tuning
- First 3 experiments:
  1. Train StarCoder 7B on Python corpus and evaluate on HumanEval-X across all languages to establish baseline cross-language improvements
  2. Train StarCoder 7B on HTML corpus and evaluate on HumanEval-X to test the surprising HTML-to-Java improvement finding
  3. Train StarCoder 7B on mixed multilingual corpus and compare performance against monolingual models to validate multilingual training benefits

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the findings and discussion, several natural open questions emerge:

- How does the similarity between programming languages affect the degree of mutual improvement during instruction tuning?
- What is the optimal ratio of monolingual to multilingual training data for maximizing code generation performance across multiple languages?
- Can instruction tuning on one programming language transfer to understanding natural language instructions in code-related tasks?

## Limitations
- Limited to 8 programming languages, which may not represent the full diversity of programming paradigms
- The instruction-answer pairs are synthetically generated, which may not capture the full complexity of real-world coding scenarios
- Only tests one base model architecture (StarCoder), so results may not generalize to other code LLMs

## Confidence
- High: Core finding that programming languages can boost each other during instruction tuning
- Medium: Claim that language similarity determines improvement margins
- Low: Finding that multilingual instruction tuning can improve performance across all languages

## Next Checks
1. **Cross-Architecture Validation**: Test the instruction tuning approach on different base models (e.g., CodeLlama, CodeT5) to determine if the cross-language improvement effect is model-specific or generalizable across code LLMs.

2. **Language Pair Analysis**: Conduct systematic experiments varying the similarity between source and target languages (e.g., Python→JavaScript, C→Go, HTML→CSS) to better understand the relationship between language similarity and transfer effectiveness.

3. **Ablation Study on Training Data**: Perform experiments with varying amounts of monolingual vs. multilingual training data to determine the minimum effective training corpus size and optimal language mixing ratios for maximizing cross-language improvements.