---
ver: rpa2
title: 'ASPEST: Bridging the Gap Between Active Learning and Selective Prediction'
arxiv_id: '2304.03870'
source_url: https://arxiv.org/abs/2304.03870
tags:
- learning
- aspest
- training
- active
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces active selective prediction, a new paradigm
  combining selective prediction and active learning to improve accuracy and coverage
  under distribution shift while reducing human labeling cost. The proposed ASPEST
  method uses ensembles of model snapshots and self-training with aggregated outputs
  as pseudo labels.
---

# ASPEST: Bridging the Gap Between Active Learning and Selective Prediction

## Quick Facts
- **arXiv ID**: 2304.03870
- **Source URL**: https://arxiv.org/abs/2304.03870
- **Reference count**: 40
- **Key outcome**: ASPEST achieves up to 88.84% AUC on MNIST→SVHN with labeling budget of 100, significantly outperforming prior methods on image, text, and structured datasets.

## Executive Summary
ASPEST introduces active selective prediction, a novel paradigm that combines selective prediction and active learning to improve accuracy and coverage under distribution shift while reducing human labeling cost. The method uses ensembles of model snapshots during training and self-training with aggregated outputs as pseudo labels. By addressing the challenges of over-fitting and over-confidence that arise when fine-tuning on small amounts of labeled test data, ASPEST demonstrates substantial improvements across diverse benchmarks including image, text, and structured datasets.

## Method Summary
ASPEST trains N models with different random seeds and uses checkpoint ensembles to select informative test samples based on ensemble margin. The method fine-tunes on selected labeled data and performs self-training using ensemble-averaged softmax outputs as pseudo labels. This approach addresses the fundamental challenges in active selective prediction by improving confidence calibration and reducing over-confidence under distribution shift. The framework integrates active learning acquisition functions with selective prediction thresholds to optimize the coverage-accuracy tradeoff.

## Key Results
- Achieves 88.84% AUC on MNIST→SVHN with labeling budget of 100
- Consistently improves accuracy and coverage across image, text, and structured datasets
- Outperforms prior methods on DomainNet (R→C, R→P, R→S) benchmarks
- Demonstrates effectiveness of checkpoint ensembling and self-training components

## Why This Works (Mechanism)

### Mechanism 1
Ensembling checkpoints during training reduces over-confident wrong predictions and improves calibration under distribution shift. Multiple checkpoints capture different generalization levels, and averaging their softmax outputs produces more calibrated uncertainty estimates than any single checkpoint.

### Mechanism 2
Self-training with ensemble-averaged softmax outputs as pseudo-labels improves selective prediction performance. The ensemble's averaged outputs are more accurate and less noisy than individual predictions, making them better pseudo-labels that boost both accuracy and coverage.

### Mechanism 3
Selecting test samples with lowest ensemble margin for labeling maximizes active learning efficiency. Samples with low ensemble margin are near the decision boundary where the ensemble is most uncertain, making them most informative for improving both model accuracy and ensemble confidence calibration.

## Foundational Learning

- **Concept**: Selective prediction and coverage-accuracy tradeoff
  - Why needed here: Core to understanding the problem ASPEST solves - balancing when to defer to humans vs. making predictions
  - Quick check question: What happens to coverage when you increase the threshold τ in selective prediction?

- **Concept**: Distribution shift and its impact on model confidence
  - Why needed here: Explains why standard selective prediction fails under distribution shift and why ASPEST is needed
  - Quick check question: How does distribution shift typically affect model confidence calibration?

- **Concept**: Active learning acquisition functions and their selection criteria
  - Why needed here: Critical for understanding how ASPEST selects samples for labeling differently than standard active learning
  - Quick check question: What is the difference between margin-based and entropy-based acquisition functions?

## Architecture Onboarding

- **Component map**: Model snapshots → Ensemble aggregation → Sample selection (margin-based) → Fine-tuning on labeled data → Self-training with pseudo-labels → Updated ensemble
- **Critical path**: Ensemble construction → Sample selection → Fine-tuning → Self-training → Performance evaluation
- **Design tradeoffs**: Computational cost of maintaining multiple checkpoints vs. performance gain; pseudo-label quality vs. self-training effectiveness
- **Failure signatures**: Overfitting on small labeled sets; poor ensemble calibration; ineffective sample selection leading to label budget waste
- **First 3 experiments**:
  1. Verify that checkpoint ensemble improves calibration over single model on shifted data
  2. Test margin-based sample selection vs. random selection for active learning efficiency
  3. Evaluate self-training with ensemble outputs vs. predicted labels on selective prediction metrics

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of checkpoint ensembles compare to other ensemble methods like Bayesian model averaging or knowledge distillation-based ensembles under distribution shift? The paper demonstrates checkpoint ensembles improve performance but doesn't compare against alternative ensemble techniques.

### Open Question 2
What is the optimal threshold η for self-training in ASPEST, and how does it vary across different datasets and domain shifts? The paper mentions performance is not highly sensitive to η but uses fixed values across datasets without systematic study.

### Open Question 3
How does ASPEST perform in online learning scenarios where data arrives sequentially rather than in batches? The paper assumes access to all unlabeled test data upfront, which may not be realistic in many applications.

## Limitations
- Relies heavily on synthetic distribution shifts rather than real-world domain adaptation scenarios
- Doesn't systematically analyze hyperparameter sensitivity across datasets
- Requires training multiple models with different random seeds, creating computational overhead

## Confidence

- **High confidence**: Claims about ASPEST improving selective prediction metrics over baselines on tested datasets
- **Medium confidence**: Claims about specific mechanisms (checkpoint ensembling, self-training) being primary drivers of performance improvement
- **Low confidence**: Claims about ASPEST's effectiveness on real-world distribution shifts beyond synthetic scenarios tested

## Next Checks

1. **Real-world domain adaptation validation**: Test ASPEST on established domain adaptation benchmarks (e.g., Office-31, DomainNet with natural splits) to verify performance on realistic distribution shifts.

2. **Hyperparameter sensitivity analysis**: Systematically vary key hyperparameters (ensemble size, self-training iterations, confidence thresholds) across different datasets to quantify method's robustness.

3. **Computational efficiency benchmarking**: Measure wall-clock time and memory requirements of ASPEST versus baseline methods across different model sizes to quantify practical deployment costs.