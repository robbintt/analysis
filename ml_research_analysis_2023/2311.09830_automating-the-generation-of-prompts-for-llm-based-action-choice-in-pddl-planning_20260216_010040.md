---
ver: rpa2
title: Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning
arxiv_id: '2311.09830'
source_url: https://arxiv.org/abs/2311.09830
tags:
- planning
- domain
- pddl
- plan
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work automates the generation of natural language prompts
  for LLM-based planning from PDDL domain specifications, removing the need for manual
  prompt engineering. Using an LLM, it converts PDDL predicates and actions into natural
  language templates, which are then composed into domain descriptions and problem
  instances.
---

# Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning

## Quick Facts
- **arXiv ID**: 2311.09830
- **Source URL**: https://arxiv.org/abs/2311.09830
- **Reference count**: 6
- **One-line primary result**: Automatic generation of natural language prompts from PDDL achieves planning performance comparable to manually crafted prompts.

## Executive Summary
This work automates the generation of natural language prompts for LLM-based planning from PDDL domain specifications, eliminating the need for manual prompt engineering. The system uses an LLM to convert PDDL predicates and actions into natural language templates, which are then composed into domain descriptions and problem instances. Experiments on 12 PDDL domains show that automatically generated prompts perform comparably to manually written ones from prior work. ReAct planning with GPT-4 achieved the best accuracy across domains, while interactive methods recovered better from prediction errors. The approach enables systematic evaluation of LLM planning capabilities and reveals that external domain knowledge in manual prompts—rather than the encoding method itself—can boost LLM performance.

## Method Summary
The system automates NL prompt generation through a two-step process: (1) LLM-based generation of NL prompts from PDDL using few-shot examples, and (2) LLM planning with four approaches (Basic, Chain-of-Thought, Act, ReAct) evaluated on GPT-4. The method processes 12 PDDL domains with 21 problems per domain selected for solvability and optimal plan length between 3 and 20. Few-shot examples are generated from optimal plans, and NL domain descriptions are composed from templates generated by an LLM. The planning approaches are evaluated using accuracy (Acc) of reaching goal state, average length factor (LF) comparing predicted plan length to optimal, and Acc0 (accuracy without mistakes) for interactive approaches.

## Key Results
- Automatically generated NL prompts achieve performance comparable to manually crafted prompts
- ReAct planning with GPT-4 achieved the highest accuracy across domains
- Interactive methods (Act, ReAct) recovered better from prediction errors than non-interactive approaches
- Plan length and initial state predicate complexity correlate with LLM planning difficulty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automatically generated natural language prompts from PDDL achieve performance comparable to manually crafted ones.
- Mechanism: The system leverages an LLM to convert PDDL predicates and actions into NL templates, preserving the semantic relationships and types. These templates are then composed into full domain descriptions that retain the structural and logical integrity of the original PDDL, allowing LLMs to perform planning without needing manually engineered prompts.
- Core assumption: The LLM can accurately interpret PDDL syntax and semantics to produce high-quality NL encodings that maintain the domain's logical structure and action semantics.
- Evidence anchors:
  - [abstract] "automatically generated NL prompts result in similar LLM-planning performance as the previous manually generated ones"
  - [section 3.1] "We use an LLM-based approach to generate these templates... The upper part of Table 1 shows the natural language templates that get generated by the LLM (O:) for predicates from the logistics domain"
- Break condition: If the LLM fails to preserve semantic relationships or type information during template generation, the resulting NL prompts will not capture the domain logic accurately, degrading LLM planning performance.

### Mechanism 2
- Claim: Interactive LLM planning with environmental feedback (ReAct) outperforms non-interactive and CoT approaches.
- Mechanism: ReAct combines reasoning steps with real-time environment interaction. The LLM generates actions and receives observations about their executability, allowing it to recover from prediction errors by adjusting subsequent actions based on feedback. This iterative correction enables solving problems that non-interactive approaches cannot.
- Core assumption: The environment can provide meaningful, actionable feedback that allows the LLM to identify and correct mistakes during planning.
- Evidence anchors:
  - [abstract] "Interactive methods recovered better from prediction errors"
  - [section 4.2.3] "Based on the information provided by VAL, the domain engine generates an observation for the P-LLM... If the predicted action is not executable the observation informs about this and states which preconditions are not satisfied"
- Break condition: If the environment feedback is ambiguous, delayed, or uninformative, the LLM cannot effectively recover from errors, reducing the advantage of interactive planning.

### Mechanism 3
- Claim: The difficulty of LLM planning correlates with plan length and complexity of initial state predicates.
- Mechanism: Longer optimal plans and more complex initial state predicates increase the cognitive load on the LLM, making it harder to generate correct action sequences. Domains requiring more steps or involving intricate state conditions (e.g., spatial reasoning, dead states) exceed the LLM's planning capacity.
- Core assumption: LLM performance degrades with increased planning horizon and state complexity due to limitations in compositional reasoning and generalization from short few-shot examples.
- Evidence anchors:
  - [section 5.2] "Longer optimal plans and more complex initial state predicates increase the cognitive load on the LLM... Domains requiring more steps or involving intricate state conditions... exceed the LLM's planning capacity"
- Break condition: If the LLM can effectively decompose long-horizon tasks or if the domain's logical structure is simple despite longer plans, the correlation between plan length and difficulty may not hold.

## Foundational Learning

- Concept: Planning Domain Definition Language (PDDL) syntax and semantics
  - Why needed here: The entire system depends on correctly interpreting PDDL domain and problem files to generate accurate NL prompts. Understanding PDDL's structure (predicates, actions, types, preconditions, effects) is essential for both the automatic conversion process and evaluating LLM planning performance.
  - Quick check question: What is the difference between typed and untyped PDDL domains, and how does this affect object naming in the automatic NL generation?

- Concept: Prompt engineering and few-shot learning in LLMs
  - Why needed here: The system uses few-shot examples to guide the LLM in converting PDDL to NL and in generating plans. Understanding how to construct effective few-shot prompts and how LLMs generalize from them is critical for both the conversion process and the planning approaches (Basic, CoT, Act, ReAct).
  - Quick check question: How does the choice of few-shot examples affect the LLM's ability to solve longer or more complex planning problems?

- Concept: Classical planning evaluation metrics (accuracy, plan length factor, optimal plans)
  - Why needed here: The system evaluates LLM planning performance using metrics like accuracy (whether the goal is reached) and length factor (ratio of generated plan length to optimal). Understanding these metrics is necessary to interpret experimental results and compare LLM performance to symbolic planners.
  - Quick check question: Why might an LLM generate a correct but suboptimal plan, and how is this captured by the length factor metric?

## Architecture Onboarding

- Component map: PDDL input files -> APB-LLM -> NL templates -> NL domain descriptions -> P-LLM prompt (with few-shot examples) -> P-LLM generates plan -> T-LLM translates plan to PDDL -> Domain engine + VAL validates plan
- Critical path: 1. PDDL → NL templates (APB-LLM) 2. NL templates → NL domain description 3. NL domain + problem + few-shot examples → P-LLM prompt 4. P-LLM generates plan 5. T-LLM translates plan to PDDL 6. Domain engine validates plan
- Design tradeoffs:
  - Manual vs. automatic NL encodings: Manual encodings can include external domain knowledge but require expert effort; automatic encodings are scalable and unbiased but may miss implicit domain insights.
  - Interactive vs. non-interactive planning: Interactive methods allow error recovery but increase cost and latency; non-interactive methods are faster but cannot correct mistakes.
  - Few-shot example length: Short examples are cheaper but may not generalize to longer problems; longer examples improve generalization but increase prompt size and cost.
- Failure signatures:
  - APB-LLM generates incorrect NL templates (semantic errors in predicate/action descriptions)
  - P-LLM fails to generate correct plans (low accuracy, high length factor)
  - T-LLM mistranslates NL actions to PDDL (invalid actions, syntax errors)
  - Domain engine feedback is uninformative (ambiguous or missing error messages)
  - Few-shot examples are unrepresentative (do not cover domain complexity)
- First 3 experiments:
  1. Run APB-LLM on a simple typed domain (e.g., Blocksworld) and verify that the generated NL predicates and actions preserve the original PDDL semantics.
  2. Use the generated NL domain with the Basic planning approach on a simple problem and check if the P-LLM generates a correct plan (accuracy > 0.8).
  3. Compare the accuracy and length factor of the Basic approach using automatically generated NL domain vs. manually crafted domain on the same problem to verify comparable performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM planners scale with plan length and problem complexity across different PDDL domains?
- Basis in paper: [explicit] The paper discusses that longer plans and more complex domains like Depot, Floortile, Goldminer, and Rovers are more difficult for LLM planners, but does not provide a systematic analysis of how performance scales with these factors.
- Why unresolved: The paper provides examples of domains that are more challenging but does not offer a detailed quantitative analysis of the relationship between plan length, problem complexity, and LLM planner performance.
- What evidence would resolve it: A comprehensive study measuring LLM planner performance across a range of plan lengths and problem complexities, with statistical analysis of the impact of these factors on success rates and plan quality.

### Open Question 2
- Question: What are the specific characteristics of PDDL domains that make them particularly challenging for LLM planners, beyond plan length and complexity?
- Basis in paper: [inferred] The paper identifies some domain-specific challenges, such as the Floortile domain's complex spatial relations and the Movie Seq domain's temporal ordering requirements, but does not provide a general framework for identifying challenging domain characteristics.
- Why unresolved: The paper offers examples of challenging domains but does not establish a comprehensive framework for understanding the types of domain features that pose difficulties for LLM planners.
- What evidence would resolve it: A systematic analysis of various PDDL domains, identifying common features that correlate with poor LLM planner performance, potentially leading to a predictive model of domain difficulty.

### Open Question 3
- Question: How can few-shot examples be optimally selected and structured to improve LLM planner performance on complex planning tasks?
- Basis in paper: [explicit] The paper mentions that the selection of few-shot examples affects LLM planning accuracy and references Levy et al.'s work on diverse few-shot examples, but does not explore this in detail for planning tasks.
- Why unresolved: The paper acknowledges the importance of few-shot examples but does not investigate methods for selecting or structuring them to maximize LLM planner performance.
- What evidence would resolve it: An empirical study comparing different methods of selecting and structuring few-shot examples for LLM planning, with quantitative analysis of their impact on planner performance across various domains.

## Limitations
- The semantic preservation of PDDL-to-NL conversion lacks independent validation beyond planning performance metrics
- Comparison to manual prompts focuses on performance parity rather than exploring whether manual prompts enable solving more complex problems
- The study does not provide a comprehensive framework for understanding domain characteristics that pose challenges for LLM planners

## Confidence
- **High Confidence**: The experimental methodology for comparing planning approaches (Basic, CoT, Act, ReAct) is well-specified and the results showing ReAct's superior accuracy are directly supported by the data.
- **Medium Confidence**: The claim about automatic NL prompts matching manual prompt performance is supported by experimental results but lacks independent validation of the semantic preservation mechanism.
- **Medium Confidence**: The correlation between plan length/complexity and LLM planning difficulty is observed but not deeply analyzed or explained mechanistically.

## Next Checks
1. **Semantic Validation**: Extract a subset of automatically generated NL predicates and actions, then manually verify whether their logical relationships and type constraints match the original PDDL specifications.
2. **Cross-Domain Generalization**: Test the automatic NL generation system on a new PDDL domain not included in the original 12, evaluating whether performance remains consistent with the reported results.
3. **Error Analysis on Manual Prompts**: Identify specific domains where manual prompts historically outperformed automatic ones, then analyze what domain knowledge or encoding choices in the manual prompts enabled this performance difference.