---
ver: rpa2
title: How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study
  on Learning with Representations
arxiv_id: '2310.10616'
source_url: https://arxiv.org/abs/2310.10616
tags:
- transformer
- layer
- linear
- representation
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how transformers perform in-context learning
  (ICL) on problems with a compositional structure where labels depend on inputs through
  a fixed representation function composed with a varying linear function. The authors
  construct transformers that implement optimal ICL algorithms by first computing
  the representations and then performing linear ICL on the transformed data.
---

# How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations

## Quick Facts
- arXiv ID: 2310.10616
- Source URL: https://arxiv.org/abs/2310.10616
- Authors: 
- Reference count: 40
- Primary result: Transformers can implement near-optimal in-context learning with compositional structure by computing fixed representations in lower layers and performing linear ICL in upper layers

## Executive Summary
This paper investigates how transformers perform in-context learning (ICL) on problems with compositional structure where labels depend on inputs through a fixed representation function composed with a varying linear function. The authors construct transformers that implement optimal ICL algorithms by first computing the representations and then performing linear ICL on the transformed data. Through theoretical analysis and empirical validation, they show that trained transformers consistently achieve near-optimal ICL performance and exhibit mechanisms aligning with the theory, such as copying representations and performing linear ICL on upper layers. The findings provide insight into how transformers may perform ICL in more realistic scenarios with compositional structure.

## Method Summary
The paper studies in-context learning on synthetic regression problems where labels depend on inputs through a fixed representation function composed with a varying linear function. The authors train small GPT-2-style transformers (12 layers, 8 heads, Dhid=256) on these ICL data distributions using Adam optimizer with learning rate 1e-4 for 300K steps. They evaluate performance using mean squared error and investigate mechanisms through linear probing experiments that measure how well intermediate representations contain quantities of interest. A novel "pasting" experiment tests whether upper layers can perform linear ICL independently. Theoretical results provide bounds on excess risk for the optimal algorithm.

## Key Results
- Trained transformers achieve near-optimal ICL risk on synthetic tasks with compositional structure
- Lower layers compute the fixed representation function while upper layers perform linear ICL on transformed data
- Linear probing reveals clear mechanisms: representations appear in x-tokens then are copied to y-tokens, with optimal predictions computed in upper layers
- Pasting experiment confirms upper layers can perform linear ICL independently when given properly formatted inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers implement ICL with representations by first computing a fixed representation function, then performing linear ICL on transformed data
- Mechanism: Lower layers transform input through representation function and prepare data in specific format, while upper layers perform linear ICL on transformed dataset
- Core assumption: The representation function Φ⋆ is fixed across instances, allowing transformer to learn it during pretraining
- Evidence anchors:
  - [abstract] "By construction, the optimal ICL algorithm first transforms the inputs by the representation function, and then performs linear ICL on top of the transformed dataset"
  - [section] "Our high-level finding is that the lower layers transforms the data by the representation and prepares it into a certain format, and the upper layers perform linear ICL on top of the transformed data"
  - [corpus] Weak - corpus neighbors discuss linear functions and dynamical systems but don't directly address the two-module mechanism
- Break condition: If representation function changes between instances, or lower layers fail to compute representation accurately

### Mechanism 2
- Claim: Transformer uses copying mechanism to move computed representations from x-tokens to y-tokens
- Mechanism: After computing Φ⋆(xi) at xi token, transformer copies this representation to following yi token, creating format [Φ⋆(xi), yi] needed for linear ICL
- Core assumption: Positional encoding allows transformer to distinguish between x and y tokens and perform targeted copying
- Evidence anchors:
  - [abstract] "exhibit the desired dissection where lower layers transforms the dataset and upper layers perform linear ICL"
  - [section] "each Φ⋆(xi) appears both in the i-th x-token and is also copied into the succeeding y token"
  - [corpus] Weak - corpus neighbors don't discuss copying mechanisms specifically
- Break condition: If copying is lossy or fails to preserve representation information

### Mechanism 3
- Claim: Upper module of transformer can perform linear ICL independently when provided with properly formatted inputs
- Mechanism: Upper layers implement gradient descent-based linear regression algorithms that learn linear relationships from in-context examples
- Core assumption: Upper module has sufficient capacity to implement iterative optimization algorithms like gradient descent
- Evidence anchors:
  - [abstract] "Through extensive probing and a new pasting experiment, we further reveal several mechanisms within the trained transformers...linear ICL capability of the upper layers alone"
  - [section] "the upper module of the transformer can indeed perform nearly optimal linear ICL without representation when we use the one-layer transformer embedding"
  - [corpus] Moderate - corpus neighbors discuss transformers implementing gradient descent for linear ICL
- Break condition: If upper module lacks capacity to implement iterative optimization, or input format is incompatible

## Foundational Learning

- Concept: Representation learning
  - Why needed here: Entire paper studies how transformers learn to compute and use fixed representation functions for ICL
  - Quick check question: What is the difference between learning a representation function and learning the linear coefficients in the ICL setting?

- Concept: Linear regression and ridge regression
  - Why needed here: Upper module performs linear ICL, essentially ridge regression on transformed data
  - Quick check question: How does ridge regression differ from ordinary least squares, and why is it preferred in this setting?

- Concept: Transformer attention mechanisms
  - Why needed here: Copying and transformation mechanisms rely on attention heads with specific weight configurations
  - Quick check question: How does a single attention head implement the copying behavior described in Lemma B.1?

## Architecture Onboarding

- Component map: Input → lower layers (representation computation) → intermediate format [Φ⋆(xi), yi] → upper layers (linear ICL) → predictions
- Critical path: Input → lower layers (representation computation) → intermediate format [Φ⋆(xi), yi] → upper layers (linear ICL) → predictions
- Design tradeoffs: Using ReLU activation instead of softmax for better theoretical guarantees vs. potential performance impact; predicting at every token vs. just the last token
- Failure signatures: Poor ICL performance when noise level is high; probing errors that don't show expected pattern of decreasing then increasing for representations; inability to copy representations accurately
- First 3 experiments:
  1. Train transformer on linear ICL without representations to establish baseline performance
  2. Train transformer on ICL with representations and measure probing errors for Φ⋆(xi) at different layers
  3. Perform pasting experiment to test if upper layers can perform linear ICL independently

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do transformers implement the linear ICL module after computing representations? What specific mechanisms are used?
- Basis in paper: [explicit] Paper identifies evidence for upper layers performing linear ICL on transformed data, but doesn't fully characterize specific mechanisms
- Why unresolved: Paper uses linear probing to identify high-level mechanisms, but doesn't characterize specific computational steps within linear ICL module
- What evidence would resolve it: Further experiments and theoretical analysis could shed light on specific mechanisms used by upper layers, such as whether they use gradient descent, ridge regression, or other methods

### Open Question 2
- Question: How do transformers handle case of multiple possible representation functions? What is mechanism for selecting appropriate representation?
- Basis in paper: [explicit] Paper studies mixture of multiple representations and identifies evidence for "post-ICL representation selection mechanism" in some settings
- Why unresolved: Paper conjectures two possible mechanisms for algorithm selection (concurrent-ICL or post-ICL) but doesn't conclusively determine which one is used or how it works
- What evidence would resolve it: Further experiments could determine whether transformers use concurrent-ICL or post-ICL selection, and theoretical analysis could shed light on how selection mechanism works

### Open Question 3
- Question: How do transformers learn the representation function from raw input data? What is mechanism for learning the fixed representation?
- Basis in paper: [inferred] Paper assumes representation function is fixed and doesn't study how transformers learn it from raw input data
- Why unresolved: Paper focuses on ICL capabilities of transformers given fixed representation, but doesn't study how representation itself is learned
- What evidence would resolve it: Experiments could study how transformers learn representation function from raw input data, and theoretical analysis could characterize mechanism for learning fixed representation

## Limitations
- Theoretical assumptions about noise levels and bounds may not hold in more complex settings
- Empirical validation limited to synthetic data with shallow MLPs as representation functions
- Probing experiments measure linear decodability, which may not capture all relevant information in representations

## Confidence
- **High**: Theoretical results on optimal ICL algorithms and their implementation bounds
- **High**: Transformers achieving near-optimal ICL risk on synthetic tasks
- **Medium**: Specific mechanisms (copying, upper-layer linear ICL) observed through probing experiments
- **Medium**: Pasting experiment showing upper layers can perform linear ICL independently

## Next Checks
1. **Mechanism robustness check**: Test whether copying mechanism and upper-layer linear ICL persist when Φ⋆ is replaced with more complex representation function (e.g., deeper MLPs or convolutional networks) that may require more sophisticated transformation mechanisms.

2. **Cross-architecture comparison**: Apply same theoretical framework and probing methodology to other transformer variants (e.g., Longformer, Performer) or attention mechanisms to determine if two-module structure is universal or specific to standard transformers.

3. **Real-world compositional task validation**: Construct or identify real datasets with clear compositional structure (e.g., scientific datasets where features must be transformed before linear prediction) and test whether trained transformers exhibit similar lower-layer representation computation and upper-layer linear ICL patterns.