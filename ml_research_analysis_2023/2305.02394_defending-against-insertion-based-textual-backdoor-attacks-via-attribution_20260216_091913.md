---
ver: rpa2
title: Defending against Insertion-based Textual Backdoor Attacks via Attribution
arxiv_id: '2305.02394'
source_url: https://arxiv.org/abs/2305.02394
tags:
- defense
- trigger
- attack
- attacks
- backdoor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes AttDef, an attribution-based defense method
  against two insertion-based textual backdoor attacks: BadNL and InSent. The key
  idea is to detect triggers based on attribution scores, as tokens with higher attribution
  scores are more likely to be triggers.'
---

# Defending against Insertion-based Textual Backdoor Attacks via Attribution

## Quick Facts
- arXiv ID: 2305.02394
- Source URL: https://arxiv.org/abs/2305.02394
- Reference count: 23
- Primary result: AttDef achieves 79.97% average attack mitigation accuracy for pre-training attacks

## Executive Summary
This paper introduces AttDef, an attribution-based defense method against insertion-based textual backdoor attacks. The approach identifies trigger tokens using attribution scores, then masks them to restore original model predictions. By leveraging an external ELECTRA model for poison sample detection, AttDef achieves state-of-the-art performance against BadNL and InSent attacks on four benchmark datasets. The method demonstrates significantly faster inference times compared to baseline approaches while maintaining high clean accuracy.

## Method Summary
AttDef defends against insertion-based textual backdoor attacks by detecting and masking trigger words based on their attribution scores. The method uses an external ELECTRA model to filter potential benign samples, then computes attribution scores for remaining samples using partial layer-wise relevance propagation. Trigger tokens with high attribution scores are masked before input reaches the poisoned model. The defense operates in three stages: poison sample discrimination, trigger detection via attribution scores, and trigger masking to restore original predictions.

## Key Results
- Achieves 79.97% average attack mitigation accuracy for pre-training attacks
- Reduces attack success rate from 95.35% to 20.38% on average
- Maintains clean accuracy with only 2% degradation on average
- 3.13 times faster inference than ONION baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attribution scores can identify trigger tokens
- Mechanism: Tokens with high attribution scores contribute most to model's false predictions when triggers are inserted
- Core assumption: Trigger words flip model predictions and therefore have higher attribution scores than benign words
- Evidence anchors: Abstract states tokens with larger attribution scores are potential triggers; section 3.4 explains trigger words play important roles when they flip predictions
- Break condition: When triggers are short or context-dependent, attribution scores may not clearly distinguish them from benign words

### Mechanism 2
- Claim: ELECTRA can distinguish poisoned from clean samples
- Mechanism: ELECTRA uses replaced token detection to identify tokens that don't fit context, treating them as potential triggers
- Core assumption: Poisoned samples contain tokens that are contextually unusual compared to clean samples
- Evidence anchors: Section 3.3 adopts ELECTRA as poison sample discriminator; section 6 shows ELECTRA performs best on SST-2 dataset
- Break condition: When poisoned samples use common words or when clean samples contain contextually unusual tokens

### Mechanism 3
- Claim: Masking triggers restores original predictions
- Mechanism: By masking identified trigger tokens, the model receives input closer to its original training distribution
- Core assumption: Removing trigger tokens eliminates the backdoor effect without significantly harming benign input
- Evidence anchors: Section 3.5 describes masking words that appear in training data triggers; section 5.1 reports achieving state-of-the-art performance
- Break condition: When triggers are semantically important to sentence meaning, masking them degrades clean accuracy

## Foundational Learning

- Concept: Partial Layerwise Relevance Propagation
  - Why needed here: Calculates word-wise attribution scores by decomposing model predictions through gradient backpropagation
  - Quick check question: What is the primary purpose of using LRP in this defense method?

- Concept: Replaced Token Detection
  - Why needed here: ELECTRA uses this task to identify tokens that don't fit context, which can indicate poisoned samples
  - Quick check question: How does ELECTRA's replaced token detection task relate to trigger detection?

- Concept: Perplexity-based outlier detection
  - Why needed here: ONION baseline uses perplexity differences to identify outlier words that could be triggers
  - Quick check question: What metric does ONION use to identify potential trigger words?

## Architecture Onboarding

- Component map: Poison Sample Discriminator (ELECTRA) → Attribution-based Trigger Detector (LRP) → Mask Sanitization → Poisoned Model
- Critical path: Test input → ELECTRA filtering → Attribution score calculation → Trigger masking → Model prediction
- Design tradeoffs: High attribution threshold reduces false positives but may miss triggers; ELECTRA filtering speeds processing but may miss some poisoned samples
- Failure signatures: High clean accuracy degradation indicates excessive masking; low attack mitigation indicates missed triggers
- First 3 experiments:
  1. Test ELECTRA's accuracy on clean vs poisoned samples from validation set
  2. Vary attribution score threshold to find optimal balance between CACC and ASR reduction
  3. Compare defense effectiveness on single-trigger vs multi-trigger poisoned samples

## Open Questions the Paper Calls Out

- How does the performance of AttDef vary with different attribution score calculation methods beyond gradient-based partial LRP?
- How effective is AttDef against input-dependent dynamic backdoor attacks?
- How does the performance of AttDef change when applied to informal language datasets beyond OLID?

## Limitations

- ELECTRA's effectiveness varies significantly across datasets, performing poorly on informal language text like OLID
- Attribution score-based trigger detection may fail for context-dependent triggers or when multiple triggers are used simultaneously
- The defense is only effective against static insertion-based trigger backdoor attacks, not input-dependent dynamic attacks

## Confidence

- High Confidence: The claim that AttDef achieves state-of-the-art performance with 79.97% attack mitigation accuracy for pre-training attacks
- Medium Confidence: The claim that ELECTRA can effectively distinguish poisoned from clean samples
- Medium Confidence: The claim that masking identified triggers restores original predictions

## Next Checks

1. Test AttDef's performance on additional datasets with different linguistic characteristics (e.g., biomedical text, legal documents) to evaluate generalizability beyond the four benchmark datasets.

2. Conduct ablation studies to quantify the individual contributions of ELECTRA filtering versus attribution-based trigger detection to the overall defense performance.

3. Evaluate the defense against adaptive attacks where triggers are designed to evade attribution-based detection by having low attribution scores or being contextually integrated into the sentence structure.