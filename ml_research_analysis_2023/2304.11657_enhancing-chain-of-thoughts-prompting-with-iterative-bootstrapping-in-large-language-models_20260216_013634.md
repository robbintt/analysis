---
ver: rpa2
title: Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large
  Language Models
arxiv_id: '2304.11657'
source_url: https://arxiv.org/abs/2304.11657
tags:
- reasoning
- answer
- final
- iter-cot
- reasoningprocess
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Iter-CoT improves reasoning in large language models by iteratively
  correcting and summarizing generated reasoning chains. It addresses the problem
  of error-prone intermediate reasoning steps by using weak bootstrapping (correctness
  of answers only) and strong bootstrapping (human annotation of errors).
---

# Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models

## Quick Facts
- arXiv ID: 2304.11657
- Source URL: https://arxiv.org/abs/2304.11657
- Reference count: 40
- Key outcome: Iter-CoT improves reasoning performance by iteratively correcting reasoning chains, achieving up to 8.17% improvement over previous methods across eleven reasoning datasets

## Executive Summary
Iter-CoT introduces an iterative bootstrapping approach to enhance Chain-of-Thought (CoT) prompting by enabling large language models to autonomously correct errors in reasoning chains. The method combines weak bootstrapping (using answer correctness) and strong bootstrapping (with human error annotations) to select challenging yet solvable exemplars for demonstrations. By iteratively refining reasoning chains through Revise-Prompt and generating comprehensive summaries with Summary-Prompt, Iter-CoT achieves state-of-the-art performance on arithmetic, commonsense, and symbolic reasoning tasks, demonstrating the effectiveness of self-correction mechanisms in prompting large language models.

## Method Summary
Iter-CoT improves reasoning by iteratively correcting and summarizing generated reasoning chains through weak and strong bootstrapping. The approach first generates initial reasoning chains using Zero-Shot-CoT, then applies Revise-Prompt iteratively to correct errors until reaching correct answers. Summary-Prompt is then used to generate final comprehensive reasoning chains. The method selects exemplars based on iteration count, using questions requiring multiple corrections as demonstrations. Weak bootstrapping uses answer correctness for corrections, while strong bootstrapping employs human annotations of specific errors. For inference, the approach samples demonstrations from selected exemplars to guide reasoning on new problems.

## Key Results
- Achieves up to 8.17% improvement over previous state-of-the-art methods on reasoning tasks
- Iter-CoT(S) with strong bootstrapping shows 7.2% improvement compared to manual correction
- Performance improves from 54.7% to 76.6% after 4 iterations on GSM8K dataset
- Outperforms both few-shot CoT and Zero-Shot-CoT across arithmetic, commonsense, and symbolic reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iter-CoT improves reasoning performance by iteratively correcting and summarizing reasoning chains through weak and strong bootstrapping
- Mechanism: The model generates initial reasoning chains, identifies errors, and uses Revise-Prompt to iteratively correct these errors until a correct answer is reached. Then Summary-Prompt is used to generate a final comprehensive reasoning chain
- Core assumption: Large language models can self-correct errors when provided with supervisory information about answer correctness and explicit error annotations
- Evidence anchors:
  - [abstract] "By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains"
  - [section] "We utilize the Revise-Prompt for each erroneous instance, instructing the LLM of its incorrectness and allowing it to generate a new response"
  - [corpus] Weak - found 25 related papers but none directly address self-correction mechanisms through iterative bootstrapping
- Break condition: If the model cannot identify its own errors or gets stuck in incorrect reasoning loops, the bootstrapping process fails to improve performance

### Mechanism 2
- Claim: Iter-CoT selects challenging yet answerable questions as exemplars by leveraging the iterative correction process
- Mechanism: Questions requiring multiple correction iterations are deemed more difficult. The approach uses these questions as demonstrations, avoiding overly simple or complex examples that hurt performance
- Core assumption: Questions requiring more bootstrapping iterations represent appropriate difficulty levels for demonstrations
- Evidence anchors:
  - [section] "We observe that the performance of the LLMs is further improved (increasing from 4089(54.7%) to 4898(59.1%) after the first iteration and continuing to improve with subsequent iterations, ultimately reaching a peak of 5726(76.6%)"
  - [section] "Our findings suggest that guiding LLMs with more challenging exemplars can improve overall performance"
  - [corpus] Weak - found papers on CoT prompting but none specifically address exemplar difficulty selection through iterative bootstrapping
- Break condition: If iteration count doesn't correlate with question difficulty or if the model cannot solve questions even after multiple corrections

### Mechanism 3
- Claim: Strong bootstrapping with human error annotation provides more effective demonstrations than weak bootstrapping alone
- Mechanism: Human annotators identify specific errors in reasoning chains, providing detailed correction guidance that enables the model to generate more accurate demonstrations
- Core assumption: Human-provided error annotations offer richer supervisory information than answer correctness alone
- Evidence anchors:
  - [section] "The strong bootstrapping method empowers us to select more challenging examples for demonstration, enhancing the ability to generalize to both simple and complex questions"
  - [section] "Iter-CoT(S) yields even greater improvement (7.2%)" compared to manual correction
  - [corpus] Weak - no direct corpus evidence found on human-annotated error correction for CoT demonstrations
- Break condition: If human annotations are too time-consuming or don't provide additional benefit over automated corrections

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Iter-CoT builds upon CoT by adding iterative correction and selection mechanisms
  - Quick check question: How does CoT differ from standard prompting approaches in terms of intermediate reasoning steps?

- Concept: Bootstrapping in machine learning
  - Why needed here: The iterative correction process relies on bootstrapping principles to improve model performance
  - Quick check question: What distinguishes weak bootstrapping (answer correctness only) from strong bootstrapping (error annotations)?

- Concept: Exemplar selection and difficulty calibration
  - Why needed here: Iter-CoT's effectiveness depends on selecting appropriate demonstration examples
  - Quick check question: Why might questions requiring multiple correction iterations be better exemplars than either very simple or very difficult questions?

## Architecture Onboarding

- Component map:
  Data pipeline: Train/test split, exemplar generation
  Core module: Iter-CoT with weak/strong bootstrapping
  Prompt templates: Revise-Prompt, Summary-Prompt
  Inference pipeline: ICL with sampled demonstrations
  Evaluation: Accuracy metrics across reasoning tasks

- Critical path:
  1. Generate initial reasoning chains using Zero-Shot-CoT
  2. Identify errors and apply Revise-Prompt iteratively
  3. Generate final reasoning chains using Summary-Prompt
  4. Select exemplars based on iteration count
  5. Sample demonstrations for inference
  6. Evaluate performance

- Design tradeoffs:
  - Iteration count vs. computation cost
  - Weak vs. strong bootstrapping resource requirements
  - Exemplar difficulty vs. generalization ability
  - Manual annotation cost vs. performance gain

- Failure signatures:
  - Performance degradation with increased iterations
  - Inability to correct certain error types
  - Overfitting to specific exemplar difficulty levels
  - Inconsistent performance across reasoning tasks

- First 3 experiments:
  1. Ablation study: Compare performance with 0, 1, 2, 3, 4, 5+ iterations
  2. Cross-dataset generalization: Test exemplar selection on held-out datasets
  3. Human annotation impact: Compare weak vs. strong bootstrapping performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Iter-CoT's performance vary with different levels of bootstrapping iterations across diverse reasoning tasks?
- Basis in paper: [explicit] The paper discusses iterations in weak bootstrapping and their impact on performance in Section 4.5.1, showing improved performance up to 4 iterations on GSM8K dataset
- Why unresolved: The paper only evaluates iterations on GSM8K dataset. Performance trends across other arithmetic, commonsense, and symbolic reasoning tasks with varying iteration counts remain unexplored
- What evidence would resolve it: Systematic experiments showing performance curves across 1-6 iterations for each of the eleven datasets, identifying optimal iteration counts per task type

### Open Question 2
- Question: What is the comparative effectiveness of human annotation in strong bootstrapping versus automated self-correction in weak bootstrapping?
- Basis in paper: [explicit] The paper introduces both weak (answer correctness only) and strong (human-annotated error correction) bootstrapping methods in Section 3.2, noting differences in effectiveness across task types
- Why unresolved: While the paper reports performance differences, it doesn't quantify the trade-off between human effort in strong bootstrapping versus computational efficiency of weak bootstrapping, nor does it establish when each method is preferable
- What evidence would resolve it: A cost-benefit analysis comparing accuracy gains against annotation time/human effort for strong bootstrapping versus computational costs for weak bootstrapping across all datasets

### Open Question 3
- Question: How does Iter-CoT's exemplar selection mechanism compare to alternative difficulty assessment methods?
- Basis in paper: [explicit] The paper claims Iter-CoT selects "challenging yet answerable questions" using a criterion based on LLM solvability, contrasting it with hop-based criteria in Appendix A.4
- Why unresolved: The paper mentions comparable yet distinguishable results between its criterion and hop-based methods but doesn't provide detailed comparative analysis or explain why one might be preferable in different scenarios
- What evidence would resolve it: Head-to-head comparisons showing exemplar selection overlap, performance differences, and computational efficiency between Iter-CoT's criterion and hop-based methods across all eleven datasets

## Limitations
- Performance depends on model's ability to self-correct, which may not generalize across all reasoning domains
- Strong bootstrapping requires substantial human annotation effort, limiting scalability
- Exemplar selection mechanism assumes consistent relationship between difficulty and correction iterations that may not hold for all problem types

## Confidence

- **High Confidence**: The general framework of iterative correction and exemplar selection shows consistent performance improvements across multiple reasoning tasks
- **Medium Confidence**: The specific mechanism of using iteration count as a proxy for difficulty level in exemplar selection
- **Medium Confidence**: The relative performance gains between weak and strong bootstrapping approaches

## Next Checks

1. **Robustness to error detection failures**: Test the system's performance when the model fails to identify errors in its own reasoning chains, particularly for complex multi-step problems where error detection may be challenging

2. **Cross-domain generalization**: Evaluate the approach on reasoning tasks outside the training distribution, including domains not represented in the eleven datasets, to assess the generalizability of the iterative bootstrapping mechanism

3. **Human annotation efficiency**: Measure the time and expertise required for strong bootstrapping annotations, and quantify the marginal benefit of human error annotations versus automated correction methods to determine cost-effectiveness