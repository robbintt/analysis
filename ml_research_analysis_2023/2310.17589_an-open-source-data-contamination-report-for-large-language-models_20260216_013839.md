---
ver: rpa2
title: An Open Source Data Contamination Report for Large Language Models
arxiv_id: '2310.17589'
source_url: https://arxiv.org/abs/2310.17589
tags:
- contamination
- data
- benchmarks
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive data contamination analysis
  of over 15 popular large language models across six major multiple-choice QA benchmarks.
  The authors introduce an open-source pipeline to enable the community to perform
  contamination analysis on custom data and models.
---

# An Open Source Data Contamination Report for Large Language Models

## Quick Facts
- arXiv ID: 2310.17589
- Source URL: https://arxiv.org/abs/2310.17589
- Reference count: 7
- One-line primary result: Introduces open-source pipeline for analyzing data contamination in LLMs across six major benchmarks, revealing contamination levels from 1% to 45%

## Executive Summary
This paper presents a comprehensive analysis of data contamination in large language models across six major multiple-choice QA benchmarks. The authors introduce an open-source pipeline that enables systematic detection of test examples present in training data, revealing contamination levels ranging from 1% to 45%. The study shows that contamination significantly impacts model evaluation, with performance boosts of up to 14% on some benchmarks, while demonstrating that larger models benefit more from contamination than smaller ones. The work provides critical insights for improving the reliability of LLM evaluation.

## Method Summary
The contamination analysis pipeline combines search engine queries with Common Crawl indexing to detect overlap between test examples and training data. The method involves verbalizing benchmark questions with correct answers, querying these verbatim through Bing Search API, and computing METEOR scores to identify contamination with a 0.75 threshold. The study evaluates Llama series models (7B, 13B, 30B, 65B, 70B) on six benchmarks (WinoGrande, AI2_ARC, CommonsenseQA, HellaSwag, MMLU, C-Eval), splitting each into clean and contaminated subsets to measure performance differences using the OpenCompass platform.

## Key Results
- Contamination levels vary widely from 1% to 45% across benchmarks
- Performance gains of up to 14% and 7% observed on contaminated C-Eval and Hellaswag benchmarks
- Larger models benefit more from contamination than smaller models
- Contamination increases rapidly over time across benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contamination analysis reveals significant data leakage in popular LLMs, with performance inflation of up to 14% on contaminated benchmarks.
- Mechanism: By comparing model performance on clean vs. contaminated subsets, the extent of data leakage can be quantified and its impact on evaluation scores measured.
- Core assumption: Test examples from benchmarks are present in the training data of LLMs.
- Evidence anchors:
  - [abstract] "Our experiments reveal varying contamination levels ranging from 1% to 45% across benchmarks, with contamination increasing rapidly over time."
  - [section 6] "On average, most models exhibit 1-6% higher accuracy on contaminated subsets versus clean ones."
  - [corpus] Corpus shows related work on contamination detection methods, supporting the core premise of data leakage.
- Break condition: If test examples are not present in training data, contamination analysis would not reveal significant performance differences.

### Mechanism 2
- Claim: Contamination analysis methodology using search engines and Common Crawl can effectively identify overlapping between test sets and training data.
- Mechanism: By querying test examples verbatim and checking for matches in Common Crawl, contamination can be detected and quantified.
- Core assumption: Common Crawl contains the training data used for LLMs and can be queried for contamination analysis.
- Evidence anchors:
  - [section 4] "Therefore, we try to find overlap between benchmarks and training data in two steps: First, 1) we use the Bing Search API to check if verbatim test examples appear online, which likely indicates inclusion in Common Crawl."
  - [section 5] "Our analysis reveals varying levels of data contamination across six multi-choice QA benchmarks, as shown in Table 2."
  - [corpus] Corpus neighbors include papers on contamination detection methods, validating the approach.
- Break condition: If training data is not accessible via search engines or not present in Common Crawl, the contamination analysis methodology would fail.

### Mechanism 3
- Claim: Contamination analysis can guide the development of mitigation strategies to improve the reliability of LLM evaluation.
- Mechanism: By identifying the extent and impact of contamination, the effectiveness of different mitigation approaches can be assessed.
- Core assumption: Understanding contamination is necessary for developing effective mitigation strategies.
- Evidence anchors:
  - [section 7.1] "Several techniques have been proposed to mitigate data contamination issue in language model evaluation previously. Our findings provide some novel insights on the effectiveness of these approaches."
  - [section 7] "Our findings emphasise contamination as a widespread issue that undermines benchmark credibility."
  - [corpus] Corpus neighbors discuss various contamination mitigation techniques, supporting the claim.
- Break condition: If contamination is not a significant issue or mitigation strategies are not effective, contamination analysis would not guide their development.

## Foundational Learning

- Concept: Data contamination in LLM evaluation
  - Why needed here: Understanding the problem of data contamination is essential for conducting effective contamination analysis and developing mitigation strategies.
  - Quick check question: What is data contamination in the context of LLM evaluation?

- Concept: Contamination detection methods
  - Why needed here: Familiarity with different contamination detection methods is necessary for selecting and implementing the appropriate approach for a given analysis.
  - Quick check question: What are the main approaches used to detect data contamination in LLM evaluation?

- Concept: Evaluation benchmark construction
  - Why needed here: Knowledge of how benchmarks are constructed and the potential sources of contamination is important for interpreting contamination analysis results and guiding mitigation efforts.
  - Quick check question: How are popular LLM evaluation benchmarks typically constructed, and what are the potential sources of contamination?

## Architecture Onboarding

- Component map: Data collection (test sets and training data) -> Contamination detection (search engine queries and Common Crawl indexing) -> Performance analysis (model evaluation on clean vs. contaminated subsets) -> Results interpretation (quantifying contamination levels and impact on evaluation scores)
- Critical path: Collect test sets and training data → Detect contamination using search engine queries and Common Crawl indexing → Evaluate model performance on clean and contaminated subsets → Analyze results and quantify contamination levels and impact
- Design tradeoffs: The main design tradeoff is between the comprehensiveness of contamination analysis (e.g., using more test sets and training data) and the computational resources required. Using search engines and Common Crawl indexing can reduce the computational burden but may introduce limitations (e.g., query length restrictions).
- Failure signatures: Failure signatures of the contamination analysis pipeline include: inability to detect contamination despite its presence (false negatives), detection of contamination that is not actually present (false positives), and inaccurate quantification of contamination levels or impact on evaluation scores.
- First 3 experiments:
  1. Evaluate a simple LLM on a single test set using the contamination analysis pipeline to validate the methodology and identify potential issues.
  2. Conduct contamination analysis on a larger set of LLMs and test sets to assess the generalizability of the results and identify patterns or trends.
  3. Compare the contamination analysis results with those obtained using alternative methods (e.g., manual inspection of training data) to validate the accuracy and reliability of the approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific percentage threshold of text overlap between evaluation examples and training data should be considered as significant data contamination?
- Basis in paper: [explicit] The paper states that contamination effects begin impacting performance when overlap surpasses 60% sequence similarity.
- Why unresolved: The paper provides evidence that substantial verbatim overlap is needed before contamination provides advantages, but doesn't specify a definitive threshold for what constitutes significant contamination.
- What evidence would resolve it: Empirical studies examining model performance across a range of contamination levels to determine the point at which contamination significantly impacts evaluation results.

### Open Question 2
- Question: How can we effectively measure and mitigate data contamination in reading comprehension benchmarks, given the challenges of including full questions and contexts?
- Basis in paper: [inferred] The paper discusses the limitations of current approaches in handling long passages and suggests that further improvements would be necessary to extend the methodology to reading comprehension benchmarks.
- Why unresolved: The paper acknowledges the challenges but doesn't provide a comprehensive solution for addressing contamination in reading comprehension tasks.
- What evidence would resolve it: Development and validation of new methods specifically designed to handle long passages and accurately detect contamination in reading comprehension benchmarks.

### Open Question 3
- Question: To what extent does the rapid proliferation of questions and answers on the internet contribute to the increasing risk of data contamination over time?
- Basis in paper: [explicit] The paper finds that benchmarks often spread over time, increasing the risk of data contamination, and provides an example showing a significant increase in contamination levels in the latest Common Crawl dumps compared to previous ones.
- Why unresolved: While the paper demonstrates the trend of increasing contamination, it doesn't quantify the specific contribution of internet proliferation to this increase.
- What evidence would resolve it: Longitudinal studies tracking the spread of benchmark content across the internet and correlating this with changes in contamination levels over time.

## Limitations

- Bing Search API query length limitations prevent analysis of reading comprehension benchmarks with longer passages
- METEOR-based contamination threshold of 0.75 may miss subtler forms of contamination
- Analysis focuses on specific Llama models and six benchmarks, potentially limiting generalizability

## Confidence

**High Confidence**: Observed contamination levels (1-45%) across tested benchmarks are reliable based on systematic search-based detection with METEOR scoring.

**Medium Confidence**: Performance differences (1-14% accuracy gains) between clean and contaminated subsets are reasonably robust, though exact magnitude may vary.

**Low Confidence**: Specific contamination detection thresholds and exact relationship between contamination levels and performance impacts are less certain.

## Next Checks

1. Implement an alternative contamination detection method (such as N-gram overlap analysis) to cross-validate the search-based approach and assess false positive/negative rates.

2. Replicate the contamination analysis on a broader range of model families beyond Llama (including proprietary models) to test the robustness of observed contamination patterns and performance impacts.

3. Conduct a longitudinal study tracking contamination levels and model performance over extended time periods, using more granular temporal resolution to better understand the rate of contamination increase.