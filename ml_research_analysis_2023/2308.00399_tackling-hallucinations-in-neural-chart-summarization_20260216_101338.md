---
ver: rpa2
title: Tackling Hallucinations in Neural Chart Summarization
arxiv_id: '2308.00399'
source_url: https://arxiv.org/abs/2308.00399
tags:
- chart
- hallucinations
- data
- input
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucinations in neural chart summarization
  models. The authors analyze hallucinations in two chart summarization datasets and
  find that extrinsic hallucinations are caused by ungrounded information in training
  summaries, while intrinsic hallucinations stem from input formatting issues.
---

# Tackling Hallucinations in Neural Chart Summarization

## Quick Facts
- arXiv ID: 2308.00399
- Source URL: https://arxiv.org/abs/2308.00399
- Authors: 
- Reference count: 24
- Primary result: NLI-based filtering significantly reduces hallucinations in chart summarization while maintaining or improving automatic metric scores.

## Executive Summary
This paper addresses hallucinations in neural chart summarization by analyzing their root causes and proposing targeted solutions. The authors identify two types of hallucinations: extrinsic (ungrounded information from training data) and intrinsic (incorrect value generation from input formatting issues). They propose modifying input formats to include chart titles and adjacent x-y pairs, and using natural language inference to filter ungrounded sentences from training data. Human evaluation demonstrates significant hallucination reduction with the NLI-filtered model, while automatic metrics show mixed but generally positive results.

## Method Summary
The authors propose a two-pronged approach to reduce hallucinations in chart summarization. First, they modify the input format to include chart titles, axis labels, and adjacent x-y pairs to address intrinsic hallucinations caused by long-distance dependencies. Second, they apply natural language inference (NLI) filtering to remove ungrounded sentences from training summaries, addressing extrinsic hallucinations. They fine-tune T5 models on both original and NLI-filtered datasets, evaluating performance using automatic metrics (BLEU, ROUGE, NUBIA) and human evaluation across five dimensions including hallucination detection.

## Key Results
- NLI-filtered model shows significant reduction in hallucinations compared to baseline in human evaluation
- BLEU score improves from 27.5 to 28.8 with NLI filtering
- Logical agreement increases while contradiction decreases in NUBIA scores
- Automatic metrics show mixed results with improvements in some areas and mixed changes in others

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extrinsic hallucinations are caused by ungrounded information in training summaries.
- **Mechanism:** When training data contains sentences not entailed by the input chart, the model learns to generate similar ungrounded content, leading to extrinsic hallucinations.
- **Core assumption:** The model learns to reproduce patterns from training data, including ungrounded sentences.
- **Evidence anchors:**
  - [abstract] "Our analysis shows that the target side of chart summarization training datasets often contains additional information, leading to hallucinations."
  - [section] "20 out of 50 summaries contained ungrounded information" in the c2t-small dataset.
  - [corpus] Weak evidence - related papers focus on hallucination reduction but don't specifically address training data grounding issues.
- **Break condition:** If training data is cleaned or filtered to remove ungrounded sentences, extrinsic hallucinations should decrease.

### Mechanism 2
- **Claim:** Intrinsic hallucinations stem from input formatting issues with long-distance dependencies.
- **Mechanism:** When x and y values are not adjacent in the linearized input, the model struggles to learn pairwise relationships, leading to contradictory or incorrect value generation.
- **Core assumption:** The model's ability to learn pairwise relationships depends on the proximity of related values in the input sequence.
- **Evidence anchors:**
  - [abstract] "We also found that shortening long-distance dependencies in the input sequence and adding chart-related information like title and legends improves the overall performance."
  - [section] "The distance between each x and its corresponding y value is large, and we speculate the model faces difficulty when learning pairwise relationships between x and y, leading to intrinsic hallucination."
  - [corpus] Moderate evidence - ChartAdapter and AltChart papers address chart understanding but focus on vision-language models rather than sequence formatting.
- **Break condition:** If input formatting is improved to reduce long-distance dependencies, intrinsic hallucinations should decrease.

### Mechanism 3
- **Claim:** Natural Language Inference (NLI) can effectively filter ungrounded sentences from training summaries.
- **Mechanism:** By using NLI to determine if each sentence in a summary is entailed by the chart data, ungrounded sentences can be removed, resulting in cleaner training data.
- **Core assumption:** NLI models can accurately determine entailment between chart data and summary sentences.
- **Evidence anchors:**
  - [abstract] "We propose a natural language inference (NLI) based method to preprocess the training data and show through human evaluation that our method significantly reduces hallucinations."
  - [section] "If the sentence gets an entailment score above the threshold of 0.3, we keep it, otherwise we discard the sentence."
  - [corpus] Strong evidence - AlignScore paper directly addresses NLI-based factual consistency evaluation.
- **Break condition:** If NLI filtering threshold is set too high or low, it may either remove too many valid sentences or fail to remove ungrounded ones.

## Foundational Learning

- **Concept: Data-to-text generation**
  - Why needed here: Chart summarization is a specific instance of data-to-text generation where chart data must be converted to natural language summaries.
  - Quick check question: What distinguishes chart summarization from other data-to-text tasks?

- **Concept: Hallucination types in NLG**
  - Why needed here: Understanding intrinsic vs. extrinsic hallucinations is crucial for diagnosing and addressing the specific problems in chart summarization outputs.
  - Quick check question: How would you distinguish between intrinsic and extrinsic hallucinations in a generated summary?

- **Concept: Input linearization for sequence models**
  - Why needed here: Proper linearization of chart data (including titles, labels, and adjacent x-y pairs) is essential for reducing model confusion and improving output quality.
  - Quick check question: Why might adjacent x-y pairs be more effective than separate x and y value lists in the input?

## Architecture Onboarding

- **Component map:** Chart parsing -> Linearization -> NLI filtering -> T5 fine-tuning -> Evaluation
- **Critical path:**
  1. Parse chart data and create linearized input
  2. Apply NLI filtering to training summaries
  3. Finetune T5 model on cleaned data
  4. Generate and evaluate summaries
- **Design tradeoffs:**
  - Using NLI filtering vs. other hallucination reduction methods
  - Including more chart context (titles, labels) vs. longer input sequences
  - Balancing automatic metrics with human evaluation
- **Failure signatures:**
  - High NUBIA contradiction scores indicate intrinsic hallucinations
  - Presence of entities or facts not in chart data indicates extrinsic hallucinations
  - Low BLEU/ROUGE scores may indicate over-filtering of training data
- **First 3 experiments:**
  1. Compare T5 finetuned on original vs. NLI-filtered data using human evaluation for hallucination detection
  2. Test different input linearizations (with/without titles, adjacent vs. separate x-y pairs)
  3. Evaluate impact of NLI filtering threshold on output quality and hallucination rates

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The NLI filtering approach relies heavily on the quality and appropriateness of the entailment model for chart data
- The evaluation primarily focuses on hallucination reduction without thoroughly examining potential loss of meaningful content through aggressive filtering
- The input formatting improvements may not generalize well to all chart types, particularly those with complex multi-series data

## Confidence
- **High confidence:** The identification of extrinsic hallucinations stemming from ungrounded training data, supported by clear empirical evidence showing 40% of summaries containing ungrounded information.
- **Medium confidence:** The effectiveness of NLI filtering in reducing hallucinations, though the threshold selection (0.3) appears somewhat arbitrary and may need task-specific tuning.
- **Medium confidence:** The impact of input formatting on intrinsic hallucinations, with the mechanism plausible but requiring more extensive validation across diverse chart types.

## Next Checks
1. **Threshold sensitivity analysis:** Systematically evaluate the impact of different NLI filtering thresholds (0.1, 0.3, 0.5, 0.7) on hallucination rates versus information retention, to identify optimal tradeoff points.
2. **Cross-dataset generalization:** Apply the proposed methods to chart summarization datasets from different domains (e.g., scientific papers, business reports) to assess robustness and identify domain-specific limitations.
3. **Long-term consistency evaluation:** Track hallucination rates and content quality across multiple generations from the same model to assess whether improvements are consistent or if the model exhibits unstable behavior over time.