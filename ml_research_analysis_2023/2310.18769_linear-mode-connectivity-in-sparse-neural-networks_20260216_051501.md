---
ver: rpa2
title: Linear Mode Connectivity in Sparse Neural Networks
arxiv_id: '2310.18769'
source_url: https://arxiv.org/abs/2310.18769
tags:
- data
- synthetic
- subnetworks
- pruning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the training dynamics and stability of sparse
  neural networks when using synthetic data (specifically, distilled data) for pruning.
  The authors find that synthetic subnetworks, created by pruning with distilled data,
  exhibit greater stability to SGD noise on real data compared to both the dense model
  and subnetworks found using traditional iterative magnitude pruning (IMP) with real
  data.
---

# Linear Mode Connectivity in Sparse Neural Networks

## Quick Facts
- arXiv ID: 2310.18769
- Source URL: https://arxiv.org/abs/2310.18769
- Reference count: 28
- One-line primary result: Synthetic subnetworks pruned with distilled data show greater stability to SGD noise than both dense models and IMP subnetworks, using up to 150x less training data.

## Executive Summary
This paper investigates the training dynamics and stability of sparse neural networks when using synthetic data (specifically, distilled data) for pruning. The authors find that synthetic subnetworks, created by pruning with distilled data, exhibit greater stability to SGD noise on real data compared to both the dense model and subnetworks found using traditional iterative magnitude pruning (IMP) with real data. This stability is demonstrated through linear mode connectivity analysis, loss landscape visualizations, and measurements of the Hessian's diagonal. The authors show that synthetic subnetworks can match the performance of IMP subnetworks while using up to 150x less training data, suggesting that using synthetic data for pruning can lead to more stable and efficient sparse neural networks.

## Method Summary
The paper proposes using distilled data (a compressed representation of the training data) in iterative magnitude pruning to identify important weights for creating sparse subnetworks. The method involves: (1) generating distilled data from the original training set using Information-intensive Dataset Condensation (IDC), (2) using the distilled data in IMP to identify important weights, (3) pruning the lowest magnitude weights to create a sparse subnetwork, and (4) training the sparse subnetwork on real data. Stability is assessed by interpolating between two trained models on different data orderings and analyzing loss behavior, along with loss landscape visualizations and Hessian diagonal measurements.

## Key Results
- Synthetic subnetworks exhibit full linear mode connectivity in simpler scenarios (ConvNet-3 on CIFAR-10 & ResNet-10 on ImageNet-10)
- Synthetic subnetworks show greater stability to SGD noise on real data compared to dense models and IMP subnetworks
- Synthetic subnetworks can match IMP performance while using up to 150x less training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic subnetworks created with distilled data exhibit greater stability to SGD noise on real data than subnetworks created with traditional IMP.
- Mechanism: The distilled data used in IMP acts as a compressed, generalized representation of the real data, leading to a smoother loss landscape and reduced sensitivity to training order.
- Core assumption: Dataset distillation effectively captures the essential information needed for the task while minimizing irrelevant, data-specific details.
- Evidence anchors:
  - [abstract]: "synthetic subnetworks, created by pruning with distilled data, exhibit greater stability to SGD noise on real data compared to both the dense model and subnetworks found using traditional iterative magnitude pruning (IMP) with real data."
  - [section]: "We see that in simpler scenarios with ConvNet-3 on CIFAR-10 & ResNet-10 on ImageNet-10, we exhibit full linear mode connectivity."
  - [corpus]: Weak evidence. Related works focus on pruning and sparse networks but do not specifically address stability with synthetic data.
- Break condition: If the dataset distillation method fails to capture the essential information for the task, or if the synthetic data introduces significant noise, the stability advantage may be lost.

### Mechanism 2
- Claim: The stability of synthetic subnetworks leads to linear mode connectivity, where models trained on different data orderings converge to the same minima.
- Mechanism: The smoother loss landscape of synthetic subnetworks allows for a continuous path between models trained on different data orderings, without encountering a barrier in the loss.
- Core assumption: Linear mode connectivity is a desirable property, indicating that the model is not overly sensitive to the specific ordering of the training data.
- Evidence anchors:
  - [abstract]: "That is, synthetically chosen subnetworks often train to the same minima, or exhibit linear mode connectivity."
  - [section]: "If the loss does not increase during interpolation, then this implies they exist in the same minima or at least the same flat basin."
  - [corpus]: Weak evidence. Related works mention linear mode connectivity but do not connect it to the use of synthetic data.
- Break condition: If the loss landscape is inherently non-convex or if the model architecture is highly sensitive to initialization, linear mode connectivity may not be achievable.

### Mechanism 3
- Claim: The use of synthetic data for pruning can lead to significant reductions in the amount of training data required.
- Mechanism: By using a compressed representation of the data (the distilled dataset), the model can identify important weights with fewer training examples.
- Core assumption: The distilled dataset effectively summarizes the information content of the full training set.
- Evidence anchors:
  - [abstract]: "synthetic subnetworks can match the performance of IMP subnetworks while using up to 150x less training data."
  - [section]: "Using distilled data only to choose our sparsity mask allows us to better understand the architectural relationship of this data."
  - [corpus]: Weak evidence. Related works focus on pruning efficiency but do not specifically address the use of synthetic data for data reduction.
- Break condition: If the distilled dataset is too small or if it fails to capture the diversity of the full training set, the model may not be able to identify the important weights accurately.

## Foundational Learning

- Concept: Linear Mode Connectivity (LMC)
  - Why needed here: LMC is used to assess the stability of the synthetic subnetworks to SGD noise and determine if models trained on different data orderings converge to the same minima.
  - Quick check question: What does it mean if the loss increases during interpolation between two trained models?

- Concept: Dataset Distillation
  - Why needed here: Dataset distillation is the key technique used to create the synthetic data that is used for pruning, leading to more stable and efficient sparse networks.
  - Quick check question: How does dataset distillation differ from traditional data augmentation techniques?

- Concept: Information Bottleneck
  - Why needed here: The Information Bottleneck framework is used to analyze the information flow from the distilled data to the target task, providing insights into why the synthetic subnetworks are more stable.
  - Quick check question: How does the Information Bottleneck framework relate to the concept of generalization in machine learning?

## Architecture Onboarding

- Component map:
  Dataset Distillation -> Iterative Magnitude Pruning (IMP) -> Sparse Subnetwork -> Linear Interpolation -> Loss Landscape Visualization

- Critical path:
  1. Generate distilled data from the original training set.
  2. Use the distilled data in IMP to identify important weights.
  3. Prune the lowest magnitude weights to create a sparse subnetwork.
  4. Train the sparse subnetwork on the real data.
  5. Assess the stability of the synthetic subnetwork using linear interpolation and loss landscape visualization.

- Design tradeoffs:
  - Accuracy vs. Efficiency: Using distilled data for pruning can lead to significant reductions in training data requirements but may slightly impact the final accuracy.
  - Stability vs. Trainability: Synthetic subnetworks are more stable but may have a slightly higher training loss compared to IMP subnetworks.

- Failure signatures:
  - If the distilled data is not representative of the original training set, the synthetic subnetworks may not perform well.
  - If the pruning process is too aggressive, the sparse subnetwork may lose important information and fail to converge.

- First 3 experiments:
  1. Generate distilled data using a simple dataset distillation method and compare the stability of the resulting synthetic subnetworks to IMP subnetworks.
  2. Vary the compression ratio of the distilled data and assess the impact on the performance and stability of the synthetic subnetworks.
  3. Apply the synthetic pruning method to different model architectures and datasets to evaluate its generalizability.

## Open Questions the Paper Calls Out
- Question: How does the stability of synthetic subnetworks vary across different dataset distillation methods beyond IDC?
- Question: What is the relationship between the compression ratio in dataset distillation and the generalization performance of synthetic subnetworks?
- Question: Can synthetic subnetworks maintain their stability and performance on larger, more complex datasets and architectures beyond ResNet-18 and CIFAR-100?

## Limitations
- The paper's experiments are constrained to smaller models and datasets due to computational limitations of dataset distillation.
- The claimed 150x reduction in training data requirements is based on specific experimental conditions and may not generalize across different model architectures, datasets, or dataset distillation methods.
- The relationship between linear mode connectivity and actual generalization performance needs clearer validation.

## Confidence

- **High Confidence**: The core observation that synthetic subnetworks exhibit greater stability to SGD noise on real data compared to dense models and IMP subnetworks.
- **Medium Confidence**: The mechanism by which distilled data leads to smoother loss landscapes is plausible but not fully established.
- **Low Confidence**: The claimed 150x reduction in training data requirements is based on specific experimental conditions and may not generalize.

## Next Checks
1. Evaluate synthetic and IMP subnetworks on out-of-distribution data or through cross-dataset transfer to verify that stability translates to better generalization.
2. Systematically vary the dataset distillation method to isolate whether the stability advantage comes from the distillation process itself or other factors in the experimental setup.
3. Test the synthetic pruning approach on larger-scale models (e.g., ResNet-50 on full ImageNet) and more complex tasks to assess whether the stability benefits and data efficiency gains persist at scale.