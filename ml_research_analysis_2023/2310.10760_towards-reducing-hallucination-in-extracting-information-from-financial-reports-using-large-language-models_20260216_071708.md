---
ver: rpa2
title: Towards reducing hallucination in extracting information from financial reports
  using Large Language Models
arxiv_id: '2310.10760'
source_url: https://arxiv.org/abs/2310.10760
tags:
- information
- llms
- arxiv
- metadata
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of extracting information from
  earnings call transcripts using large language models (LLMs), with a focus on reducing
  hallucination. The authors propose a retrieval-augmented LLM approach that leverages
  metadata to improve the accuracy of information extraction.
---

# Towards reducing hallucination in extracting information from financial reports using Large Language Models

## Quick Facts
- arXiv ID: 2310.10760
- Source URL: https://arxiv.org/abs/2310.10760
- Reference count: 25
- Key outcome: Metadata filtering improves LLM answer quality from BERTScore 0.59 to 0.60 when extracting information from earnings call transcripts

## Executive Summary
This work addresses hallucination in LLM-based information extraction from earnings call transcripts by combining retrieval-augmented generation with metadata filtering. The authors propose chunking documents, embedding them with metadata, and using company-specific filtering during retrieval to ground responses in verifiable content. Their approach shows modest improvements in answer quality metrics when compared to baseline LLM usage without metadata constraints.

## Method Summary
The authors implement a retrieval-augmented LLM pipeline that processes earnings call transcripts by chunking them into manageable segments, embedding these chunks with metadata (company, sector, date), and storing them in a vector database. For each query, they filter documents by company metadata, retrieve similar chunks using MMR similarity search, and generate answers with various LLMs (PaLM2, BLOOMZ-7b1, Pythia-1.4b, flan-t5-base, Llama2-7b). The system is evaluated on 50 Nifty 50 company transcripts with 400 manually curated question-answer pairs using multiple similarity metrics.

## Key Results
- PaLM2 with metadata achieves BERTScore of 0.60 versus 0.59 without metadata
- Metadata filtering significantly improves answer quality compared to LLMs without metadata
- The approach shows consistent improvements across multiple LLM architectures tested
- Retrieval-augmented generation with metadata grounding reduces hallucination in financial report extraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metadata filtering reduces cross-document hallucination by restricting the retrieval pool to the target company's documents.
- Mechanism: During retrieval, document metadata (e.g., company name, sector) is used to filter the candidate documents before computing similarity scores. This ensures that only relevant document chunks are retrieved for a given query.
- Core assumption: Metadata accurately reflects the content of the document and can be used as a reliable filter for query-document relevance.
- Evidence anchors:
  - [abstract] "reducing hallucination by combining retrieval-augmented generation technique as well as metadata"
  - [section 3.4] "we must specify the company documents we intend to examine. We employ document metadata as a filter to refine the document selection and extract relevant answers tailored to the user’s query."
  - [corpus] Weak evidence: no explicit mention of metadata filtering in corpus papers; assumed from this work.
- Break condition: If metadata is missing, incorrect, or ambiguous (e.g., multiple companies with similar names), the filter may exclude relevant documents or include irrelevant ones.

### Mechanism 2
- Claim: Chunking enables processing of long financial reports by fitting them into LLM context windows.
- Mechanism: Earnings reports are split into smaller text segments (chunks) using a recursive character-based splitter, ensuring each chunk is within the model's context limit (e.g., 1024 tokens). These chunks are embedded and stored for retrieval.
- Core assumption: Splitting preserves semantic coherence and that important context is not lost between chunks.
- Evidence anchors:
  - [section 3.3] "Chunking entails breaking down the document into smaller, more manageable segments that comfortably fit within the context window of the expansive language model"
  - [section 3.3] "we deploy RecursiveCharacterTextSplitter, which operates by segmenting extensive text into chunks of a designated size"
  - [corpus] No direct evidence; this is a common preprocessing step in LLM applications.
- Break condition: If chunks are too small, important context may be split across chunks; if too large, they may exceed context limits.

### Mechanism 3
- Claim: Retrieval-augmented generation grounds LLM responses in verifiable external content, reducing hallucination.
- Mechanism: The LLM retrieves relevant document chunks using similarity search (e.g., MMR) and uses them as context to generate answers, rather than relying solely on its parametric memory.
- Core assumption: Retrieved content is relevant and accurate, and the LLM can effectively integrate it into coherent answers.
- Evidence anchors:
  - [abstract] "reducing hallucination by combining retrieval-augmented generation technique"
  - [section 3.2] "Retrieval-augmented generation [...] enhances the capabilities of LLMs by integrating retrieval systems into their architecture"
  - [corpus] Weak evidence: some related papers mention retrieval augmentation but not specifically for hallucination reduction.
- Break condition: If retrieval fails to find relevant content or retrieves misleading information, the generated answer may still be hallucinated.

## Foundational Learning

- Concept: Tokenization and context windows
  - Why needed here: LLMs process text as tokens and have fixed context limits; understanding these limits is crucial for chunking strategy.
  - Quick check question: If a model has a 4096-token context window, approximately how many words can it process at once?

- Concept: Embeddings and similarity search
  - Why needed here: Document chunks are converted to embeddings for efficient similarity-based retrieval during Q&A.
  - Quick check question: What is the purpose of converting text chunks into embeddings before storing them in a vector database?

- Concept: Evaluation metrics for text generation
  - Why needed here: Multiple metrics (BERTScore, BARTScore, Jaro similarity, LC subsequence) are used to assess answer quality against ground truth.
  - Quick check question: Which metric would be most appropriate for comparing semantic similarity between two sentences?

## Architecture Onboarding

- Component map:
  Data collection → Web scraping of earnings reports
  Preprocessing → Chunking + metadata tagging
  Embedding generation → LLM-based embeddings stored in vector DB
  Retrieval → Metadata filtering + MMR similarity search
  Generation → Retrieval-augmented LLM answering queries
  Evaluation → Multiple similarity metrics vs ground truth

- Critical path:
  1. Collect and preprocess documents (chunk + embed)
  2. Store in vector DB with metadata
  3. For each query: filter by metadata, retrieve similar chunks, generate answer
  4. Evaluate answer quality

- Design tradeoffs:
  - Chunk size vs context limit: Smaller chunks reduce context loss but may lose coherence; larger chunks risk truncation.
  - Metadata granularity: More detailed metadata improves filtering but increases storage and complexity.
  - Number of retrieved chunks: More chunks provide richer context but may introduce noise.

- Failure signatures:
  - Answers unrelated to query → Metadata filtering or retrieval failing
  - Incomplete answers → Chunking breaking important context
  - Hallucinated content → Retrieval not grounding responses adequately

- First 3 experiments:
  1. Compare BERTScore with and without metadata filtering on a subset of 50 questions.
  2. Vary chunk size (512, 1024, 2048 tokens) and measure retrieval accuracy and answer coherence.
  3. Test different numbers of retrieved chunks (5, 10, 20) and evaluate impact on hallucination and relevance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the use of metadata in retrieval-augmented LLMs compare to other methods of reducing hallucination, such as fine-tuning or prompt engineering?
- Basis in paper: [explicit] The paper mentions that using metadata significantly improves the quality of generated answers compared to using LLMs without metadata, but it does not compare this approach to other methods of reducing hallucination.
- Why unresolved: The paper focuses on the effectiveness of using metadata and does not explore other methods of reducing hallucination in LLMs.
- What evidence would resolve it: A comparative study of the effectiveness of using metadata, fine-tuning, and prompt engineering in reducing hallucination in LLMs for information extraction from financial reports.

### Open Question 2
- Question: How does the performance of the retrieval-augmented LLM approach with metadata compare to traditional information extraction methods, such as rule-based approaches or OCR techniques?
- Basis in paper: [explicit] The paper mentions that traditional methods like keyword matching and rule-based approaches often result in limited accuracy and scalability, and OCR techniques encounter difficulties in accurately processing unstructured transcript text. However, it does not provide a direct comparison of the proposed approach to these traditional methods.
- Why unresolved: The paper focuses on the advantages of the proposed approach but does not provide a comprehensive comparison to traditional information extraction methods.
- What evidence would resolve it: A study comparing the performance of the retrieval-augmented LLM approach with metadata to traditional information extraction methods in terms of accuracy, scalability, and ability to handle unstructured text.

### Open Question 3
- Question: How does the choice of LLM architecture (e.g., encoder-decoder vs. decoder-only) impact the performance of the retrieval-augmented approach with metadata in information extraction from financial reports?
- Basis in paper: [inferred] The paper experiments with various LLM architectures, including PaLM2, BLOOMZ-7b1, Pythia-1.4b, flan-t5-base, and Llama2-7b, but it does not analyze the impact of these architectural differences on the performance of the retrieval-augmented approach with metadata.
- Why unresolved: The paper presents results for different LLM architectures but does not provide an in-depth analysis of how the choice of architecture affects the performance of the proposed approach.
- What evidence would resolve it: A study investigating the impact of different LLM architectures on the performance of the retrieval-augmented approach with metadata in information extraction from financial reports, considering factors such as semantic similarity-based metrics and pairwise alignment-based similarity metrics.

## Limitations
- Limited evaluation scope with only 50 Nifty 50 companies and 400 manually curated question-answer pairs
- Metadata filtering effectiveness depends heavily on metadata quality and completeness, which is not empirically validated
- Modest improvement (BERTScore 0.59 to 0.60) suggests the approach may not be transformative for hallucination reduction

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Metadata filtering reduces hallucination | Medium confidence |
| Results generalize to other financial documents | Low confidence |
| Chunking approach preserves semantic coherence | Medium confidence |

## Next Checks

1. **Ablation study on metadata filtering**: Run the same evaluation pipeline without metadata filtering on a subset of questions to quantify the specific contribution of metadata to hallucination reduction versus other factors.

2. **Cross-company contamination test**: Design queries that could potentially retrieve documents from multiple companies and measure how often metadata filtering prevents cross-company hallucination versus how often it might exclude relevant context.

3. **Metric sensitivity analysis**: Compare the four evaluation metrics against human judgments of hallucination on a small sample to determine which metric most accurately captures hallucination reduction versus other aspects of answer quality.