---
ver: rpa2
title: Internally Rewarded Reinforcement Learning
arxiv_id: '2302.00270'
source_url: https://arxiv.org/abs/2302.00270
tags:
- reward
- learning
- function
- noise
- clipped
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies a class of reinforcement learning problems where
  reward signals are generated by an internal discriminator model that is jointly
  optimized with the policy. This interdependence leads to an unstable learning process
  due to noisy rewards from an immature discriminator impeding policy learning, and
  vice versa.
---

# Internally Rewarded Reinforcement Learning

## Quick Facts
- arXiv ID: 2302.00270
- Source URL: https://arxiv.org/abs/2302.00270
- Reference count: 39
- One-line primary result: Clipped linear reward function stabilizes internally rewarded reinforcement learning by reducing reward noise variance

## Executive Summary
This paper addresses the instability inherent in internally rewarded reinforcement learning (IRRL), where reward signals are generated by an internal discriminator model that is jointly optimized with the policy. The interdependence between policy and discriminator creates a feedback loop where noisy rewards from an immature discriminator impede policy learning, and vice versa. The authors propose a clipped linear reward function that consistently stabilizes training by reducing the variance of reward noise. Their theoretical analysis demonstrates that this approach effectively addresses the noise amplification problem while maintaining sufficient signal for policy improvement.

## Method Summary
The authors formulate IRRL as a Markov Decision Process where rewards are generated by an internal discriminator model estimating the posterior probability of labels given trajectories. They propose a clipped linear reward function that bounds the reward signal to reduce variance from discriminator noise. The method is evaluated across three diverse tasks: hard attention for digit recognition using cluttered MNIST, unsupervised skill discovery in a four-room environment, and robotic object counting under occlusion scenarios. Training alternates between policy optimization and discriminator updates using the proposed reward function, with comparisons against accuracy-based and logarithmic reward functions.

## Key Results
- Clipped linear reward consistently stabilizes training across all three diverse IRRL tasks
- Linear reward shows lower and more stable variance compared to logarithmic reward functions
- The proposed method achieves faster convergence and higher performance than baseline reward functions
- Experimental results validate the theoretical analysis of noise reduction effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clipped linear reward reduces the variance of reward noise by bounding the reward signal
- Mechanism: The linear reward function transforms the posterior probability into a bounded range [0, 1] by clipping it to be at least the prior probability, preventing extreme reward values that amplify noise
- Core assumption: The discriminator noise has zero mean and is symmetric around the true posterior
- Evidence anchors: [abstract] "Experimental results show that the proposed reward function can consistently stabilize the training process by reducing the impact of reward noise"; [section 4.3] "the variance of rlin is low and stable compared with the variance of logarithmic reward rlog"
- Break condition: If the prior p(y) is poorly estimated or changes dramatically during training, the clipping threshold becomes ineffective

### Mechanism 2
- Claim: Linear reward maintains information about relative intensities in the reward signal while being robust to noise
- Mechanism: Linear transformation preserves proportional differences while keeping rewards bounded, making the signal more stable during early training when discriminators are uncertain
- Core assumption: The transformation preserves enough information about the underlying mutual information to guide policy learning
- Evidence anchors: [abstract] "propose a simple clipped linear reward function that consistently stabilizes training by reducing the variance of reward noise"; [section 4.3] "the variance of rlin is low and stable compared with the variance of logarithmic reward rlog"
- Break condition: If the relationship between reward magnitude and policy improvement becomes too attenuated, learning may slow down

### Mechanism 3
- Claim: Clipped linear reward aligns the optimization objective with χ²-divergence rather than KL-divergence
- Mechanism: Using linear reward instead of logarithmic reward effectively maximizes χ²-mutual information rather than Shannon's mutual information, providing better robustness to label noise and uncertain discriminators
- Core assumption: χ²-divergence provides better noise robustness than KL-divergence for the given problem structure
- Evidence anchors: [section 6.1] "the linear reward function has specific meanings from an information-theoretic perspective... derived from the optimization objective of maximizing the χ²-divergence"; [section 4.3] "The proposed clipped linear reward has a similar shape to the rectified linear unit (ReLU)"
- Break condition: If the prior distribution p(y) is highly imbalanced or multimodal, χ²-divergence may introduce its own optimization challenges

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: The paper explicitly formulates IRRL as an MDP with modified reward structure
  - Quick check question: What are the key differences between standard MDP rewards and IRRL rewards?

- Concept: Mutual information maximization
  - Why needed here: The reward functions are designed to maximize mutual information between trajectories and labels
  - Quick check question: How does maximizing mutual information relate to the discriminator's uncertainty?

- Concept: f-divergence measures
  - Why needed here: The linear reward is connected to χ²-divergence as an alternative to KL-divergence
  - Quick check question: What properties of χ²-divergence make it more suitable for noisy environments?

## Architecture Onboarding

- Component map: Policy network -> Discriminator network -> Reward function -> Training loop
- Critical path: Policy generates trajectory τ from state → Discriminator computes qφ(y|τ) for target label y → Reward function transforms qφ(y|τ) into reward r → Policy updates using reward signal → Discriminator updates using trajectory-label pairs
- Design tradeoffs: Linear vs logarithmic transformation (linear provides better noise robustness but may lose some information-theoretic properties); Clipping threshold (must balance between noise reduction and information preservation); Prior estimation (quality of p(y) estimate affects clipping effectiveness)
- Failure signatures: High variance in reward signal despite clipping; Policy collapse to trivial solutions (always same trajectory); Discriminator overfitting to current policy's trajectory distribution
- First 3 experiments: 1) Compare linear vs logarithmic reward on simple synthetic task with known noise characteristics; 2) Test clipping sensitivity by varying p(y) estimation quality; 3) Validate mutual information maximization by measuring discriminator uncertainty reduction over training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of f-divergence measure affect the stability and performance of IRRL beyond the χ²-divergence used in the linear reward?
- Basis in paper: [explicit] The paper mentions that the linear reward can be derived from maximizing the χ²-divergence and suggests investigating other f-mutual information measures
- Why unresolved: The paper only analyzes the χ²-divergence and does not explore other f-divergence measures or compare their effects on IRRL stability and performance
- What evidence would resolve it: Experiments comparing the performance and stability of IRRL using different f-divergence measures (e.g., KL divergence, Jensen-Shannon divergence) would provide evidence on the optimal choice for IRRL

### Open Question 2
- Question: Can the clipped linear reward function be extended to handle regression-based critics in IRRL?
- Basis in paper: [inferred] The paper focuses on classification-based critics and mentions that guidance for designing reward functions for both classification and regression cases would be significant
- Why unresolved: The paper does not provide a theoretical analysis or empirical results on using the clipped linear reward function with regression-based critics
- What evidence would resolve it: Developing a theoretical framework for extending the clipped linear reward to regression tasks and conducting experiments comparing its performance with other reward functions in regression-based IRRL tasks would provide evidence

### Open Question 3
- Question: How can the impact of insufficient observations be explicitly addressed in IRRL to further improve stability and performance?
- Basis in paper: [explicit] The paper acknowledges that insufficient observations are a challenge in IRRL but does not explicitly address this issue in their method
- Why unresolved: The proposed clipped linear reward function focuses on reducing the impact of reward noise but does not consider the issue of insufficient observations, which can also lead to unstable training
- What evidence would resolve it: Developing a method that explicitly assesses the sufficiency of observations and incorporating it into the reward function or training process, followed by experiments demonstrating improved stability and performance, would provide evidence

## Limitations
- Theoretical analysis assumes discriminator noise is zero-mean and symmetric, which may not hold when discriminators overfit to current policies
- Focus on variance reduction doesn't fully account for potential bias introduced by clipping mechanism
- Paper doesn't address scenarios where prior p(y) estimates change significantly during training

## Confidence
- High confidence: The empirical superiority of clipped linear reward over baselines across diverse tasks
- Medium confidence: The theoretical analysis linking linear reward to χ²-divergence and its noise robustness properties
- Medium confidence: The claim that linear reward preserves sufficient information for effective policy learning

## Next Checks
1. Test the robustness of clipped linear reward when discriminator noise becomes asymmetric or biased
2. Evaluate performance degradation when prior p(y) estimates are systematically incorrect
3. Compare learning curves when using different divergence measures (KL vs χ²) in the same experimental setup