---
ver: rpa2
title: Improving Fairness using Vision-Language Driven Image Augmentation
arxiv_id: '2311.01573'
source_url: https://arxiv.org/abs/2311.01573
tags:
- bias
- protected
- images
- dataset
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to mitigate bias in deep learning
  models, particularly in the facial domain, by using a pre-trained diffusion model
  (DiffAE) to generate images that exemplify under-represented protected characteristics
  (e.g., age and skin color). The method, called Vision-Language Bias Control (VLBC),
  learns interpretable paths in the semantic space of DiffAE, guided by contrastive
  text dipoles, to edit these protected characteristics.
---

# Improving Fairness using Vision-Language Driven Image Augmentation

## Quick Facts
- arXiv ID: 2311.01573
- Source URL: https://arxiv.org/abs/2311.01573
- Authors: 
- Reference count: 40
- Primary result: Method improves fairness metrics on CelebA-HQ and UTKFace datasets by generating synthetic images to augment under-represented protected characteristics

## Executive Summary
This paper introduces Vision-Language Bias Control (VLBC), a method that uses a pre-trained diffusion model (DiffAE) to generate and manipulate images to mitigate bias in deep learning models, particularly for facial attribute classification. The approach learns interpretable paths in DiffAE's semantic space using contrastive text dipoles (ContraCLIP) to edit protected characteristics like age and skin color while preserving downstream task labels. The manipulated images are then used to augment datasets, making them fairer for training. Experiments demonstrate improvements in overall accuracy, fairness metrics, and performance against state-of-the-art methods, with the capability to control bias both toward and away from specific attributes.

## Method Summary
VLBC addresses bias by generating synthetic facial images that exemplify under-represented protected characteristics (e.g., darker skin tones, older age groups). The method first identifies minority protected characteristic classes in the real dataset, then uses DiffAE to generate synthetic images which are pseudo-labeled by a pre-trained attribute network. ContraCLIP learns interpretable edit paths in DiffAE's semantic space using contrastive text prompts (e.g., "young face" vs. "old face"). These paths are applied to selected synthetic images to manipulate the desired protected characteristics, and the augmented dataset is used to train downstream classifiers. The approach is evaluated on CelebA-HQ and UTKFace datasets using fairness metrics including difference in accuracy and disparity of equal opportunity.

## Key Results
- VLBC improves overall accuracy and fairness metrics (difference in accuracy, disparity of equal opportunity) on CelebA-HQ and UTKFace datasets
- Method outperforms state-of-the-art bias mitigation techniques in several settings
- Achieves controllable bias manipulation, capable of both reducing and increasing bias toward specific attributes
- Demonstrates effectiveness for both age and skin color as protected characteristics

## Why This Works (Mechanism)

### Mechanism 1
Diffusion models provide semantically meaningful latent spaces that can be traversed to edit protected characteristics while preserving downstream task labels. DiffAE encodes images into a semantic space where attribute-specific paths can be learned via contrastive text guidance, enabling controlled editing of protected characteristics without altering the target classification label.

### Mechanism 2
Augmenting minority protected characteristic classes with synthetic images reduces bias by balancing training data. The method identifies under-represented protected characteristics, generates synthetic images via DiffAE, manipulates them to exemplify the minority class, and adds them to the training set while preserving original labels.

### Mechanism 3
Using natural language contrastive dipoles to guide edits enables interpretable, controllable bias manipulation. ContraCLIP learns non-linear paths in DiffAE's semantic space using pairs of contrasting text prompts, serving as interpretable edit directions that can be applied with controllable strength.

## Foundational Learning

- Concept: Diffusion models and semantic editing
  - Why needed here: DiffAE's semantic space is the foundation for controllable attribute editing; understanding how diffusion models encode and decode images is critical for applying edits.
  - Quick check question: How does DiffAE's encoder-decoder structure enable traversal in semantic space without corrupting image content?

- Concept: Fairness metrics and bias quantification
  - Why needed here: The method evaluates bias via difference in accuracy across protected characteristics; understanding these metrics is essential for interpreting results and tuning augmentation.
  - Quick check question: What does a negative difference in accuracy indicate about model bias toward a protected characteristic?

- Concept: Contrastive learning and text-image alignment
  - Why needed here: ContraCLIP uses CLIP-based text prompts to define edit directions; understanding contrastive learning is key to grasping how natural language drives image manipulation.
  - Quick check question: How does a contrastive loss between text embeddings guide the discovery of meaningful edit paths in image space?

## Architecture Onboarding

- Component map: Data selection -> Synthetic generation -> Pseudo-labeling -> Image selection -> Augmentation -> Training
- Critical path: Real dataset → bias analysis → synthetic image generation → pseudo-labeling → augmentation → training
- Design tradeoffs:
  - Generation bias vs. augmentation effectiveness: Synthetic images inherit biases from DiffAE's training data, potentially limiting augmentation impact
  - Traversal length vs. edit strength: Longer traversals increase manipulation but risk unnatural artifacts
  - Pseudo-label accuracy vs. augmentation quality: Incorrect pseudo-labels can lead to inappropriate augmentations
- Failure signatures:
  - No improvement in fairness metrics despite augmentation
  - Degradation in overall accuracy or f1-score
  - Augmentation produces unrealistic or semantically inconsistent images
- First 3 experiments:
  1. Run bias analysis on a simple dataset (e.g., CelebA subset) to identify under-represented protected characteristics
  2. Generate a small synthetic set and manually inspect pseudo-labels for accuracy
  3. Apply a single ContraCLIP+DiffAE edit path to a few synthetic images and verify visual plausibility and attribute preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the VLBC method in reducing bias across diverse downstream tasks beyond binary classification?
- Basis in paper: [explicit] The paper states that VLBC is tested on several downstream tasks of CelebA-HQ and UTKFace with age and skin color as protected characteristics, showing competitive or better performance in mitigating bias. However, the paper focuses primarily on binary classification tasks.
- Why unresolved: The paper does not explore the effectiveness of VLBC on non-binary classification tasks or other types of downstream tasks such as object detection or segmentation.
- What evidence would resolve it: Conducting experiments on a wider range of downstream tasks, including non-binary classification, object detection, and segmentation, and comparing the performance of VLBC with other bias mitigation methods would provide evidence.

### Open Question 2
- Question: How does the quality and diversity of the synthetic dataset (Xs) generated by DiffAE impact the effectiveness of VLBC in mitigating bias?
- Basis in paper: [explicit] The paper mentions that the synthetic dataset (Xs) is generated using DiffAE and pseudo-labeled by a pre-trained network on 41 attributes. However, it does not explore the impact of the quality and diversity of this synthetic dataset on the bias mitigation performance.
- Why unresolved: The paper does not provide an analysis of how variations in the quality and diversity of the synthetic dataset affect the bias mitigation results.
- What evidence would resolve it: Conducting experiments with synthetic datasets of varying quality and diversity, and analyzing the impact on the bias mitigation performance of VLBC, would provide evidence.

### Open Question 3
- Question: How does the choice of text prompts used to define the protected characteristics in ContraCLIP affect the manipulation results and bias mitigation performance?
- Basis in paper: [explicit] The paper mentions that text prompts are used to define the protected characteristics in ContraCLIP, and provides examples of text prompts for age and skin color. However, it does not explore the impact of different text prompts on the manipulation results and bias mitigation performance.
- Why unresolved: The paper does not provide an analysis of how variations in the choice of text prompts affect the manipulation results and bias mitigation performance.
- What evidence would resolve it: Conducting experiments with different text prompts for defining the protected characteristics, and analyzing the impact on the manipulation results and bias mitigation performance, would provide evidence.

## Limitations
- Method relies heavily on the quality and disentanglement of DiffAE's semantic space, which may not generalize across all domains
- Synthetic data generation introduces potential bias that may propagate through augmentation, limiting effectiveness when DiffAE itself is biased
- The approach requires careful selection of text prompts and manipulation parameters, which may not scale well to many protected characteristics

## Confidence
- High confidence: The augmentation framework and evaluation metrics are well-established in fairness literature
- Medium confidence: The effectiveness of diffusion model-based semantic editing for controlled attribute manipulation
- Medium confidence: The claim that natural language contrastive dipoles provide interpretable control

## Next Checks
1. Conduct an ablation study comparing ContraCLIP-guided edits against baseline edits using random latent vectors or simple linear interpolations
2. Systematically evaluate how attribute manipulation affects downstream classification accuracy on a held-out validation set
3. Analyze the distribution of protected characteristics in the synthetic dataset before augmentation to quantify bias in the DiffAE model itself