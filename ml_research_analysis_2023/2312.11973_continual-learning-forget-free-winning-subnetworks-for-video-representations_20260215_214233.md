---
ver: rpa2
title: 'Continual Learning: Forget-free Winning Subnetworks for Video Representations'
arxiv_id: '2312.11973'
source_url: https://arxiv.org/abs/2312.11973
tags:
- learning
- continual
- video
- neural
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continual learning, specifically
  catastrophic forgetting, where a model's performance on previous tasks degrades
  when learning new ones. The authors propose a novel method called Fourier Subneural
  Operator (FSO) that operates in Fourier space to adaptively and compactly encode
  videos, discovering reusable subnetworks with diverse bandwidths.
---

# Continual Learning: Forget-free Winning Subnetworks for Video Representations

## Quick Facts
- arXiv ID: 2312.11973
- Source URL: https://arxiv.org/abs/2312.11973
- Authors: 
- Reference count: 40
- Primary result: Fourier Subneural Operator (FSO) improves continual learning performance across task incremental learning, video incremental learning, and few-shot class incremental learning scenarios.

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by proposing a novel method called Fourier Subneural Operator (FSO) that operates in Fourier space to adaptively encode videos through specialized subnetworks. The approach combines Winning Subnetworks (WSN) with FSO to discover reusable subnetworks with diverse bandwidths, preventing catastrophic forgetting while maintaining efficiency. The method is evaluated across three continual learning scenarios: Task Incremental Learning (TIL), Video Incremental Learning (VIL), and Few-shot Class Incremental Learning (FSCIL), showing significant performance improvements over existing baselines.

## Method Summary
The proposed method combines Winning Subnetworks (WSN) with Fourier Subneural Operator (FSO) to address catastrophic forgetting in continual learning. WSN uses task-specific binary masks learned via weight scores to selectively freeze weights from previous tasks while updating only non-used weights for new tasks. FSO operates in Fourier space by decomposing video signals into sine and cosine components, enabling efficient video representation through specialized subnetworks. For few-shot scenarios, the method extends to SoftNet using soft masks with values in [0,1] to prevent overfitting. The approach is validated across multiple datasets including CIFAR-100, TinyImageNet, and UVG video sequences.

## Key Results
- In CIFAR-100 Superclass, WSN+FSO achieves 61.70% accuracy with zero backward transfer, outperforming other baselines
- In video incremental learning, WSN+FSO shows improved PSNR scores and outperforms EWC, iCaRL, and ESMER on UVG17 dataset
- The method significantly boosts higher-layer performance in TIL and FSCIL, and lower-layer performance in VIL scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fourier Subneural Operator (FSO) decomposes video signals into sine and cosine components to adaptively select bandwidth-specific subnetworks, enabling efficient video representation without catastrophic forgetting.
- Mechanism: FSO operates in Fourier space by parameterizing a periodic subnetwork function using real and imaginary parts separately. It breaks down a neural implicit representation into its sine and cosine components, then selectively identifies the most effective Lottery tickets for representing complex periodic signals. This allows the network to capture different frequency components of the video stream with specialized subnetworks.
- Core assumption: Neural implicit representations of videos can be effectively decomposed into frequency components, and different frequency bands benefit from specialized subnetworks rather than a single dense network.
- Evidence anchors:
  - [abstract] "Fourier Subneural Operator (FSO), which breaks down a neural implicit representation into its sine and cosine components (real and imaginary parts) and then selectively identifies the most effective Lottery tickets for representing complex periodic signals."
  - [section 3.1] "Following the previous definition of Fourier convolutional operator [47], we adapt and redefine this definition to better fit the needs of the NIR framework."
  - [corpus] Weak evidence - only 0 citations and no related work directly addressing Fourier decomposition for video representations.
- Break condition: If the frequency decomposition fails to capture essential temporal or spatial patterns in the video, or if the real and imaginary component separation introduces significant information loss.

### Mechanism 2
- Claim: Winning Subnetworks (WSN) prevent catastrophic forgetting by freezing previously learned subnetworks while selectively updating only non-used weights for new tasks.
- Mechanism: WSN jointly learns model weights and task-adaptive binary masks. During training on new sessions, it accumulates binary masks (Ms-1 = ∨s-1 i=1 mi) to identify weights used in previous tasks. The weight update equation θ* ← θ* - η(∂L/∂θ* ⊙ (1 - Ms-1)) effectively freezes weights allocated to previous sessions while only updating weights not selected before. This selective updating prevents interference between tasks.
- Core assumption: Catastrophic forgetting can be prevented by maintaining frozen subnetworks for past tasks while allowing new tasks to utilize the remaining capacity.
- Evidence anchors:
  - [abstract] "It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) and Task-agnostic Incremental Learning (TaIL) scenarios."
  - [section 3.2.1] "To solve the first problem, we selectively update the weights by allowing updates only on the weights not selected in the previous sessions."
  - [corpus] Moderate evidence - related works on Piggyback, PackNet, and CLNP show similar mask-based approaches but with different selection strategies.
- Break condition: If the capacity constraint (c%) is too restrictive, causing insufficient parameters for new tasks, or if task similarity causes interference despite the masking mechanism.

### Mechanism 3
- Claim: Soft-Subnetworks (SoftNet) address overfitting in few-shot scenarios by using soft masks with values in [0,1] instead of binary masks, allowing smoother transitions and better generalization with limited samples.
- Mechanism: SoftNet divides the dense neural network into major and minor subnetworks (msoft = mmajor ⊕ mminor). The major subnetwork (mmajor) maintains base session knowledge with binary weights (m = 1), while the minor subnetwork (mminor ~ U(0,1)) acquires new session knowledge with soft weights. During incremental learning, only minor parameters are updated (θ* ← θ* - β(∂L/∂θ* ⊙ mminor)), regularizing the learning process.
- Core assumption: Soft masks with values between 0 and 1 provide better regularization than binary masks when training on limited data, preventing overfitting while maintaining knowledge transfer.
- Evidence anchors:
  - [abstract] "To mitigate overfitting in Few-Shot Class Incremental Learning (FSCIL), we have developed WSN variants referred to as the Soft subnetwork (SoftNet)."
  - [section 3.2.2] "The defined Soft-SubNetwork (SoftNet) follows as: msoft = mmajor ⊕ mminor, where mmajor is a binary mask and mminor ~ U(0,1)"
  - [corpus] Strong evidence - Regularized Lottery Ticket Hypothesis (RLTH) cited as foundation, with SoftNet directly building on this work.
- Break condition: If the soft mask introduces too much noise or if the uniform distribution for minor weights doesn't adequately capture task-specific importance.

## Foundational Learning

- Concept: Lottery Ticket Hypothesis (LTH)
  - Why needed here: LTH provides the theoretical foundation that sparse subnetworks can match the performance of dense networks, justifying the search for winning subnetworks in continual learning scenarios.
  - Quick check question: Can you explain why finding a "winning ticket" subnetwork is theoretically possible within a randomly initialized dense network?

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding catastrophic forgetting is essential because the primary motivation for WSN, SoftNet, and FSO is to prevent performance degradation on previous tasks when learning new ones.
  - Quick check question: What distinguishes catastrophic forgetting from catastrophic interference in the context of continual learning?

- Concept: Neural Implicit Representations (NIR)
  - Why needed here: NIR is the framework used for video representation in this work, where videos are parameterized as continuous, differentiable functions rather than discrete pixel arrays.
  - Quick check question: How does a neural implicit representation of video differ from traditional frame-by-frame processing approaches?

## Architecture Onboarding

- Component map: Backbone network (CNN/ResNet/NeRV) -> Weight score parameters (ρ) -> Binary/Soft masks (m) -> FSO modules -> Loss functions
- Critical path:
  1. Initialize backbone and weight scores
  2. For each task/session:
     - Compute mask based on top-c% weight scores
     - Accumulate masks for previous tasks
     - Update weights with frozen mask protection
     - Update weight scores using straight-through estimator
  3. For FSO integration: merge lower layer outputs through Fourier decomposition
- Design tradeoffs:
  - Sparsity level (c%) vs. task performance: Higher sparsity reduces capacity but improves efficiency
  - Real vs. imaginary components in FSO: Using both provides better performance but doubles parameter count
  - Binary vs. soft masks: Binary masks are simpler but may overfit; soft masks generalize better but are more complex
- Failure signatures:
  - Accuracy plateaus early: Likely insufficient capacity (c% too low)
  - Sudden performance drops on old tasks: Mask accumulation or freezing mechanism failure
  - Slow convergence: Learning rate too low or weight score updates ineffective
  - Poor video quality: FSO parameters not properly tuned or frequency decomposition inadequate
- First 3 experiments:
  1. Baseline comparison: Implement WSN on CIFAR-100 Superclass with c=50% and measure ACC/BWT against standard CL baselines
  2. FSO ablation: Compare WSN vs. WSN+FSO on UVG17 with varying FSO sparsity levels (0.5% to 50%)
  3. SoftNet validation: Test SoftNet on CIFAR-100 FSCIL with 5-way 5-shot tasks, comparing against binary WSN and state-of-the-art FSCIL methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Fourier Subneural Operator (FSO) specifically improve performance in lower-layer operations for Video Incremental Learning (VIL) compared to conventional methods?
- Basis in paper: [explicit] The paper mentions that FSO enhances lower-layer performance in VIL but does not provide detailed comparative analysis or quantitative evidence.
- Why unresolved: The paper does not provide specific metrics or detailed analysis comparing FSO's performance in lower layers with other methods.
- What evidence would resolve it: Detailed experimental results showing FSO's performance in lower layers compared to conventional methods, including specific metrics like PSNR or accuracy.

### Open Question 2
- Question: What are the computational trade-offs of using FSO in terms of memory and processing power compared to traditional CNN architectures?
- Basis in paper: [inferred] The paper discusses the benefits of FSO in enhancing performance but does not address the computational cost or memory requirements.
- Why unresolved: The paper does not provide information on the computational efficiency or resource requirements of implementing FSO.
- What evidence would resolve it: Comparative analysis of memory usage and processing time between FSO and traditional architectures, including benchmarks on different hardware setups.

### Open Question 3
- Question: How does the FSO handle varying bandwidths in different video sequences, and what impact does this have on the generalizability of the model?
- Basis in paper: [explicit] The paper states that FSO discovers reusable subnetworks with diverse bandwidths but does not elaborate on how it adapts to different bandwidths.
- Why unresolved: The paper does not provide details on the adaptability of FSO to different bandwidths or its impact on model generalizability.
- What evidence would resolve it: Experimental results demonstrating FSO's performance across different video sequences with varying bandwidths, including analysis of model adaptability and generalizability.

## Limitations

- The Fourier decomposition approach (FSO) lacks sufficient theoretical analysis and ablation studies to fully understand when and why it outperforms standard approaches, particularly regarding the trade-offs between frequency decomposition and information preservation.
- The performance improvements shown in experiments may be influenced by specific hyperparameter choices rather than fundamental advantages of the approach, requiring more extensive validation across diverse datasets.
- The scalability of the approach to larger-scale video datasets and more complex task scenarios remains unproven, limiting confidence in real-world applicability.

## Confidence

- High Confidence: The core mechanism of Winning Subnetworks (WSN) with task-specific binary masks and frozen weight updates is well-established and clearly explained, with strong theoretical grounding in the Lottery Ticket Hypothesis.
- Medium Confidence: The SoftNet extension for FSCIL shows reasonable theoretical motivation, but the practical benefits of soft masks over binary masks in few-shot scenarios need more extensive validation across diverse datasets.
- Low Confidence: The Fourier decomposition approach (FSO) lacks sufficient theoretical analysis and ablation studies to fully understand when and why it outperforms standard approaches, particularly regarding the trade-offs between frequency decomposition and information preservation.

## Next Checks

1. Conduct ablation studies on FSO by systematically varying the real/imaginary component usage and measuring the information loss through reconstruction quality metrics on held-out validation sets.

2. Test the scalability of the approach by implementing WSN+FSO on larger-scale video datasets (e.g., Kinetics or Something-Something) to verify that the performance gains hold with increased task complexity and dataset size.

3. Evaluate the robustness of SoftNet by testing across different few-shot task configurations (varying ways and shots) and comparing against established few-shot learning methods to isolate the contribution of soft masks versus other components.