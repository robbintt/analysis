---
ver: rpa2
title: 'FP8-BERT: Post-Training Quantization for Transformer'
arxiv_id: '2312.05725'
source_url: https://arxiv.org/abs/2312.05725
tags:
- quantization
- int8
- accuracy
- range
- post-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FP8-BERT, a post-training quantization method
  for Transformer models using the FP8 (8-bit floating point) format instead of traditional
  INT8. The authors argue that INT8 quantization suffers from significant accuracy
  loss for Transformers due to outliers in the weight distribution, while FP8's non-uniform
  representation better handles these outliers.
---

# FP8-BERT: Post-Training Quantization for Transformer

## Quick Facts
- arXiv ID: 2312.05725
- Source URL: https://arxiv.org/abs/2312.05725
- Authors: 
- Reference count: 3
- One-line primary result: FP8 quantization preserves BERT accuracy in post-training settings, outperforming INT8 quantization

## Executive Summary
This paper proposes FP8-BERT, a post-training quantization method for Transformer models using the FP8 (8-bit floating point) format instead of traditional INT8. The authors argue that INT8 quantization suffers from significant accuracy loss for Transformers due to outliers in the weight distribution, while FP8's non-uniform representation better handles these outliers. They evaluate FP8 quantization on BERT variants across GLUE and SQuAD v1.1 datasets, showing that FP8 achieves accuracy close to full-precision models, significantly outperforming INT8 quantization in post-training settings.

## Method Summary
The method uses FP8 quantization with E4M3 encoding for GEMM operations while keeping sensitive operators (LayerNorm, Gelu, Softmax) in BF16 precision. The approach employs symmetric channelwise quantization for weights and layerwise quantization for activations with static scaling factors determined by min/max calibration. The implementation uses a custom C++ library for FP8 format conversion, and evaluation is performed on BERT-base, BERT-large models across GLUE and SQuAD v1.1 datasets.

## Key Results
- FP8 quantization achieves accuracy close to full-precision models
- FP8 significantly outperforms INT8 quantization in post-training settings
- Simple calibration with min/max values is sufficient for FP8 quantization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FP8 quantization preserves accuracy in post-training settings because its non-uniform representation handles outliers better than INT8's uniform quantization.
- Mechanism: In transformer models, weight and activation distributions contain outliers that are poorly represented by uniformly spaced INT8 levels. FP8's floating-point format provides denser spacing near zero and sparser spacing for large values, allowing better representation of these outliers.
- Core assumption: Transformer weight distributions have significant outliers that degrade INT8 quantization accuracy.
- Evidence anchors:
  - [abstract] "the previous 8-bit quantization strategy based on INT8 data format either suffers from the degradation of accuracy in a Post-Training Quantization (PTQ) fashion"
  - [section] "we find that there are many outliers in BERT, which is unfriendly for the uniform quantization strategy with INT8"
  - [corpus] Weak evidence - only general references to quantization without specific outlier analysis
- Break condition: If transformer weight distributions are approximately uniform or have limited dynamic range, FP8 advantage diminishes.

### Mechanism 2
- Claim: FP8 achieves accuracy close to full-precision models because it maintains sufficient dynamic range and precision for sensitive operations.
- Mechanism: The FP8 format (specifically E4M3 encoding used here) provides a larger dynamic range than INT8 while maintaining 8-bit storage, allowing sensitive operations like LayerNorm and activation functions to retain precision.
- Core assumption: Sensitive operations in transformers are more sensitive to quantization error than standard matrix multiplications.
- Evidence anchors:
  - [section] "we use the half-precision (BF16) to calculate the rest operators (such as LayerNorm, Gelu, and Softmax)" and "we accept half-precision format (BF16) for these special operators"
  - [abstract] "PTQ with FP8 can significantly improve the accuracy upon that with INT8, to the extent of the full-precision model"
  - [corpus] Weak evidence - mentions FP8 formats but no direct comparison of dynamic range effects
- Break condition: If sensitive operations have different quantization requirements or if mixed-precision becomes too complex to manage.

### Mechanism 3
- Claim: Post-training quantization with FP8 works without significant accuracy loss because the calibration process effectively captures the statistical properties of the model.
- Mechanism: By using static channelwise quantization for weights and layerwise quantization for activations, combined with simple min/max calibration, the quantization process adapts to the model's statistical distribution without requiring expensive retraining.
- Core assumption: Static calibration with min/max values is sufficient to capture the distribution characteristics needed for accurate quantization.
- Evidence anchors:
  - [section] "we adopt the most straightforward method: min/max signal" and "we choose the basic Layerwise Quantization strategy"
  - [abstract] "without significant loss of accuracy, with a simple calibration and format conversion process"
  - [corpus] No direct evidence - only general mentions of calibration methods
- Break condition: If model distributions change significantly during inference or if more sophisticated calibration methods are required for specific tasks.

## Foundational Learning

- Concept: Floating-point representation and IEEE 754 standard
  - Why needed here: Understanding FP8 requires knowledge of how floating-point numbers represent values using sign, exponent, and mantissa fields
  - Quick check question: How does the non-uniform spacing in floating-point representation help handle outliers compared to uniform integer quantization?

- Concept: Quantization-aware training vs post-training quantization
  - Why needed here: The paper contrasts PTQ (used here) with QAT, which requires retraining; understanding this distinction is crucial for evaluating the method's practical value
  - Quick check question: What are the main advantages and disadvantages of post-training quantization compared to quantization-aware training?

- Concept: Distribution analysis and outlier detection
  - Why needed here: The paper's core argument relies on identifying that transformer models have outliers; understanding statistical distribution properties is essential
  - Quick check question: How would you empirically verify whether a model's weight distribution contains significant outliers that would benefit from non-uniform quantization?

## Architecture Onboarding

- Component map: FP32 model -> calibration phase (min/max collection) -> format conversion (FP32 to FP8 using custom C++ library) -> inference with mixed precision (FP8 for GEMM, BF16 for sensitive ops)
- Critical path: Calibration -> quantization -> inference; the calibration step determines scaling factors that affect all subsequent quantization quality
- Design tradeoffs: Simplicity vs accuracy (PTQ vs QAT), mixed precision complexity vs pure quantization, E4M3 vs E5M2 encoding choice
- Failure signatures: Significant accuracy drop indicates poor calibration, wrong encoding choice, or sensitivity of specific operations to quantization
- First 3 experiments:
  1. Run baseline FP32 model on GLUE/SQuAD to establish accuracy targets
  2. Implement and test FP8 quantization with E4M3 encoding on BERT-base to verify accuracy preservation
  3. Compare INT8 vs FP8 quantization results on the same model and datasets to quantify improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do FP8 quantized models perform on hardware platforms with native FP8 support compared to simulated FP8 implementations?
- Basis in paper: [explicit] The paper mentions evaluating FP8 results on Moffett's Photon chip with native FP8 support but doesn't provide detailed performance comparisons
- Why unresolved: The paper primarily focuses on simulated FP8 results using C++ libraries, with only brief mention of native hardware evaluation
- What evidence would resolve it: Detailed benchmarking results comparing FP8 performance between simulation and native hardware execution, including speedups and accuracy variations

### Open Question 2
- Question: What is the optimal calibration strategy for clipping range selection in FP8 quantization that maximizes accuracy across different model architectures?
- Basis in paper: [explicit] The paper uses the "min/max signal" method for calibration but acknowledges other methods exist (KL divergence minimization, etc.)
- Why unresolved: The paper adopts a simple calibration method without exploring more sophisticated alternatives or comparing their effectiveness
- What evidence would resolve it: Systematic comparison of different clipping range calibration methods across multiple model architectures and datasets, measuring both accuracy and calibration complexity

### Open Question 3
- Question: How does FP8 quantization affect models trained on different types of data distributions beyond GLUE, SQuAD, and image classification tasks?
- Basis in paper: [explicit] The paper's experiments are limited to NLP tasks and image classification, leaving questions about FP8's effectiveness on other data types
- Why unresolved: The paper doesn't explore FP8 quantization on time-series data, audio processing, or multimodal models
- What evidence would resolve it: Comprehensive evaluation of FP8 quantization across diverse data domains including speech recognition, time-series forecasting, and multimodal models

## Limitations

- Limited experimental validation for core claims about outlier handling
- Simple min/max calibration without comparison to more sophisticated methods
- Lack of comprehensive ablation studies to isolate contributions of different components

## Confidence

**High Confidence**: The general observation that INT8 quantization can degrade transformer accuracy in post-training settings is well-established in the literature. The theoretical advantage of FP8's non-uniform representation for handling outliers is mathematically sound.

**Medium Confidence**: The claim that FP8 specifically achieves accuracy close to full-precision models is supported by the theoretical arguments but lacks comprehensive experimental validation across diverse transformer architectures and tasks. The absence of error analysis or distribution plots reduces confidence in the practical significance of the improvements.

**Low Confidence**: The assertion that simple min/max calibration is sufficient for optimal FP8 quantization without any evidence of calibration quality or comparison to alternative methods. The paper doesn't address whether FP8 introduces any new failure modes or sensitivity to specific input distributions.

## Next Checks

1. **Distribution Analysis Validation**: Quantitatively measure and visualize the weight and activation distributions of BERT and other transformers to verify the presence and magnitude of outliers. Compare the quantization error distribution under INT8 vs FP8 to demonstrate where FP8 provides specific advantages.

2. **Ablation Study on Mixed Precision**: Systematically evaluate the impact of keeping different operations in BF16 versus quantizing them to FP8. Test scenarios where LayerNorm, GELU, and Softmax are also quantized to isolate the contribution of the mixed-precision strategy to overall accuracy.

3. **Calibration Quality Assessment**: Implement and compare multiple calibration strategies (min/max, percentile clipping, entropy-based) for FP8 quantization to determine whether the simple approach used in the paper is optimal or whether more sophisticated methods could yield further accuracy improvements.