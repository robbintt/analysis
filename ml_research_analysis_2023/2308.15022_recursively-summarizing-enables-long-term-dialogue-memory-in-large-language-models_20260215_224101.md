---
ver: rpa2
title: Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language
  Models
arxiv_id: '2308.15022'
source_url: https://arxiv.org/abs/2308.15022
tags:
- memory
- dialogue
- context
- response
- long-term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to enhance long-term dialogue memory
  in large language models (LLMs) by recursively summarizing conversation contexts.
  The approach uses an LLM to first generate a short summary from a small dialogue
  context, then recursively updates this summary by combining it with subsequent contexts
  to maintain a coherent memory.
---

# Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models

## Quick Facts
- arXiv ID: 2308.15022
- Source URL: https://arxiv.org/abs/2308.15022
- Reference count: 3
- Key outcome: Recursive summarization method improves long-term dialogue memory in LLMs, showing better response consistency and fluency on Multi-Session Chat dataset

## Executive Summary
This paper introduces a method to enhance long-term dialogue memory in large language models (LLMs) by recursively summarizing conversation contexts. The approach uses an LLM to first generate a short summary from a small dialogue context, then recursively updates this summary by combining it with subsequent contexts to maintain a coherent memory. Experiments on the Multi-Session Chat dataset using ChatGPT and text-davinci-003 show that this method significantly improves response consistency and fluency compared to baselines that use either full context or partial context.

## Method Summary
The proposed method addresses the challenge of maintaining coherent long-term dialogue memory in LLMs by using recursive summarization. At each session, the LLM generates a memory summary from the dialogue context, which is then recursively updated with subsequent contexts. This memory is used by a response generator to produce more consistent and contextually relevant responses. The approach is evaluated on the Multi-Session Chat dataset and demonstrates improvements in response quality across different LLMs, with further enhancements possible through minimal labeled data using in-context learning.

## Key Results
- Recursive summarization achieves better response consistency and fluency compared to full-context and partial-context baselines
- The method demonstrates robustness across different LLMs (ChatGPT and text-davinci-003)
- In-context learning with minimal labeled data can further improve memory prediction accuracy
- The approach faces challenges with hallucination in generated summaries

## Why This Works (Mechanism)

### Mechanism 1
Recursive summarization allows LLMs to compress long dialogue contexts into a fixed-size memory that can be updated incrementally. At each step, the LLM receives the previous memory and a short dialogue context, then produces a new memory that incorporates both. This is repeated across turns, enabling memory updates without requiring full context access.

### Mechanism 2
Using a compressed memory instead of the full dialogue history improves response consistency and fluency. The response generator receives only the latest memory and current context, reducing noise from irrelevant past turns and focusing on salient information.

### Mechanism 3
The approach is robust across different LLMs and can be improved with minimal labeled data via in-context learning. Because the method relies on prompting rather than fine-tuning, it can be applied to any LLM that supports context concatenation. Adding a few labeled examples to the prompt further boosts memory accuracy.

## Foundational Learning

- **Context window limitations of LLMs**: Understanding the size constraints guides prompt design. *Quick check*: If an LLM has a 4096-token limit and your current session plus full history is 6000 tokens, how many tokens must be compressed into memory?

- **Recursive state update**: Each new memory is a function of the previous memory and new context; modeling this as a recurrence is key to understanding the algorithm's flow. *Quick check*: Given memory M3 from session 3, and new context C4 from session 4, write the operation that produces M4.

- **Prompt engineering for memory tasks**: The quality of generated memory depends heavily on how the prompt frames the summarization task; knowing how to phrase instructions affects outcomes. *Quick check*: What is the difference between prompting "summarize this conversation" vs "update memory with this conversation" in terms of expected output?

## Architecture Onboarding

- **Component map**: Memory Management module -> Response Generator module -> Orchestrator
- **Critical path**: 
  1. On session end: invoke Memory Management with (prev memory, session context)
  2. Store updated memory
  3. For next user turn: invoke Response Generator with (current context, latest memory)
- **Design tradeoffs**:
  - Memory length vs. information loss: longer memory preserves more detail but risks exceeding LLM input limits
  - Update frequency vs. API cost: updating after every turn is accurate but expensive; batching at session end reduces cost but may miss mid-session continuity
  - Prompt verbosity vs. clarity: detailed instructions improve output quality but consume tokens
- **Failure signatures**:
  - Response drifts from topic → likely memory lost key context
  - Memory size grows beyond LLM capacity → truncation or failure to update
  - Responses become repetitive → memory over-summarized, losing distinguishing details
- **First 3 experiments**:
  1. Measure memory size growth over sessions to find truncation threshold
  2. Ablate memory vs. full context in Response Generator to quantify consistency gains
  3. Vary memory update frequency (per turn vs. per session) to balance cost and accuracy

## Open Questions the Paper Calls Out

- How does the proposed method scale to extremely long dialogues, such as those with hundreds of turns, and what are the limitations in terms of context length and memory management?
- How does the quality of the generated memory affect the overall performance of the dialogue system, and what are the potential trade-offs between memory accuracy and response relevance?
- How does the proposed method compare to other long-term memory techniques, such as retrieval-augmented methods or memory-augmented neural networks, in terms of efficiency and effectiveness?
- What are the potential applications of the proposed method beyond open-domain dialogue systems, such as in task-oriented dialogue or story generation, and how would the method need to be adapted for these applications?

## Limitations
- The method may struggle with extremely long contexts if the LLM's input capacity is exceeded
- Hallucination in generated summaries can lead to incorrect or nonfactual information being incorporated into responses
- The approach requires careful prompt engineering to balance memory accuracy and response relevance

## Confidence

- **High confidence**: The core mechanism of recursive summarization for memory compression is well-supported by the experimental results on the MSC dataset, showing consistent improvements in BLEU and F1 scores compared to baselines.
- **Medium confidence**: Claims about robustness across different LLMs are supported by ablation studies but limited to two specific models and may not generalize broadly.
- **Low confidence**: The paper's claims about minimal labeled data requirements for in-context learning improvements lack quantitative details about sample sizes and performance gains.

## Next Checks

1. **Memory size validation**: Measure actual memory token growth across multiple sessions and identify the exact truncation point where performance degrades, testing the claim that memory can be maintained within LLM input constraints.

2. **Cross-LLM generalization**: Evaluate the method on at least three additional LLM architectures (including smaller models) to validate the claimed robustness beyond ChatGPT and text-davinci-003.

3. **Hallucination impact quantification**: Conduct a systematic analysis comparing predicted summaries against gold summaries to measure hallucination frequency and correlate these errors with response quality degradation.