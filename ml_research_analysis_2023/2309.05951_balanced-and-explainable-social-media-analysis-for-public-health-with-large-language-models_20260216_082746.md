---
ver: rpa2
title: Balanced and Explainable Social Media Analysis for Public Health with Large
  Language Models
arxiv_id: '2309.05951'
source_url: https://arxiv.org/abs/2309.05951
tags:
- task
- label
- llms
- bert
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ALEX, a method to improve public health analysis
  on social media by addressing data imbalance and leveraging large language models
  (LLMs). The authors employ data augmentation and weighted loss fine-tuning to balance
  training, followed by LLM-based post-evaluation to correct BERT predictions.
---

# Balanced and Explainable Social Media Analysis for Public Health with Large Language Models

## Quick Facts
- arXiv ID: 2309.05951
- Source URL: https://arxiv.org/abs/2309.05951
- Authors: 
- Reference count: 40
- Key outcome: ALEX achieves first place in two tasks and a top result in another at the SMM4H 2023 competition, demonstrating effectiveness in social media analysis for public health

## Executive Summary
This paper addresses the challenge of analyzing imbalanced social media data for public health applications by proposing ALEX, a framework that combines data augmentation, weighted loss fine-tuning, and LLM-based post-evaluation. The method leverages TextAttack for minority class augmentation, BERT fine-tuning with class-weighted loss, and GPT-3.5 to verify and correct predictions through few-shot prompting. ALEX achieves state-of-the-art results on three tasks at the SMM4H 2023 competition, demonstrating its effectiveness for public health text classification where minority classes often carry the most analytical value.

## Method Summary
ALEX combines three key components: (1) data augmentation using TextAttack to generate minority class samples, (2) weighted loss fine-tuning of BERTweet-Large to address class imbalance during training, and (3) LLM-based post-evaluation using GPT-3.5 to verify and correct BERT predictions through few-shot prompting. The framework addresses both the data imbalance problem through augmentation and resampling, and the verification problem through LLM explanations. For Tasks 1 and 4, LLM converts "False" Label 1 predictions to Label 0, while for Task 2, it converts "False" Label 0 to the majority class.

## Key Results
- Achieved first place in Task 2 (micro-averaged F1) and Task 4 (F1 for Label 1) at SMM4H 2023
- Second place in Task 1 (F1 for Label 1) at SMM4H 2023
- Demonstrated significant improvements in minority class F1 scores across all tasks
- Showed clear class separation in embedding space visualizations for balanced training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation combined with weighted loss fine-tuning improves BERT performance on imbalanced public health datasets.
- Mechanism: TextAttack augmentation increases minority class samples, resampling balances class distribution, weighted loss adjusts loss contribution per class based on importance, forcing model to focus on critical categories.
- Core assumption: Minority classes in social media public health data contain high analytical value (e.g., positive COVID cases, diagnosed social anxiety).
- Evidence anchors:
  - [abstract] "The data imbalance issue can be overcome by sophisticated data augmentation methods for social media datasets."
  - [section 4.3] "In social media analysis for public health, there is usually a tendency to place importance on specific classes in text classification."
  - [corpus] Weak - no direct citation but consistent with known NLP practices for class imbalance.
- Break condition: If augmentation introduces noise that overwhelms signal, or if class importance weighting is incorrectly assigned, performance may degrade.

### Mechanism 2
- Claim: LLM-based post-evaluation corrects BERT predictions without retraining, leveraging LLM's language understanding.
- Mechanism: BERT predictions are concatenated with original text and fed to LLM with task-specific instructions and examples. LLM outputs "True"/"False" with reasoning. False predictions are corrected based on LLM judgment.
- Core assumption: LLMs can accurately identify logical inconsistencies in BERT predictions when given proper prompt context.
- Evidence anchors:
  - [abstract] "an LLMs explanation mechanism is proposed by prompting an LLM with the predicted results from BERT models."
  - [section 4.4] "By constructing BERT's prediction into a prompt, LLMs can seek logical proofs for the label within the original input."
  - [corpus] Moderate - aligns with recent few-shot prompting literature but specific application novel.
- Break condition: If LLM hallucinations dominate or prompt design fails, corrections become unreliable.

### Mechanism 3
- Claim: Balanced embeddings from fine-tuned BERT improve class separability in latent space.
- Mechanism: Balanced training leads to clearer separation of class clusters in [CLS] token embeddings, improving downstream classification.
- Core assumption: Better class separation in embedding space directly correlates with improved classification accuracy.
- Evidence anchors:
  - [section 5.4] "Visualisation for the effectiveness of the balanced training is shown... it is obvious that for Task 2 and Task 4, the balanced training can successfully separate different labels' texts in different colours."
  - [section 5.4] "for text classification tasks that suffer from low accuracy or have low inter-class distance between each class, the balanced training method in ALEX can be proven to be effective."
  - [corpus] Weak - t-SNE visualization is qualitative, no quantitative metric provided.
- Break condition: If embedding space visualization doesn't translate to performance gains, or if task has naturally high inter-class similarity.

## Foundational Learning

- Concept: Class imbalance and its impact on model performance
  - Why needed here: Social media public health datasets often have skewed class distributions where minority classes are more valuable for analysis
  - Quick check question: What metric should you prioritize when minority class is more important than majority class?

- Concept: Data augmentation techniques for text
  - Why needed here: TextAttack framework is used to generate synthetic minority samples while maintaining semantic meaning
  - Quick check question: How does TextAttack differ from simple synonym replacement augmentation?

- Concept: Prompt engineering for few-shot learning with LLMs
  - Why needed here: Effective prompts are critical for LLM to correctly evaluate BERT predictions
  - Quick check question: What are the key components of a prompt that guides LLM to act as an explainable verifier?

## Architecture Onboarding

- Component map: TextAttack augmentation → Resampling → BERT fine-tuning (weighted loss) → BERT prediction → LLM explanation/correction → Final output
- Critical path: The LLM correction step is critical - without it, model performance drops significantly on Tasks 2 and 4
- Design tradeoffs: LLM correction adds inference cost but avoids retraining expensive models; balanced training adds preprocessing time but improves accuracy
- Failure signatures: If F1 for minority class doesn't improve after balanced training, check augmentation quality; if LLM corrections are inconsistent, check prompt design
- First 3 experiments:
  1. Run baseline BERT with imbalanced training to establish performance floor
  2. Apply only data augmentation without weighted loss to isolate augmentation effect
  3. Apply only weighted loss without augmentation to isolate loss weighting effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the limitations of using LLMs for explanation and correction be overcome to improve their effectiveness in public health text classification tasks?
- Basis in paper: [explicit] The paper discusses the effectiveness of LLMs for explanation and correction but also highlights limitations such as hallucinations and the impact of repetitive information.
- Why unresolved: The paper identifies the issues but does not provide a comprehensive solution to overcome these limitations.
- What evidence would resolve it: Research demonstrating effective methods to mitigate LLM hallucinations and improve their ability to handle repetitive information in prompts.

### Open Question 2
- Question: What are the optimal strategies for balancing training data in public health text classification tasks, considering the trade-off between data augmentation and weighted loss fine-tuning?
- Basis in paper: [explicit] The paper compares different strategies for balanced training but does not provide a definitive answer on the optimal approach.
- Why unresolved: The paper shows that both augmentation and weighted loss fine-tuning can be effective, but the optimal combination and hyperparameters are not determined.
- What evidence would resolve it: Experimental results comparing various combinations of augmentation and weighted loss fine-tuning on multiple public health datasets.

### Open Question 3
- Question: How can the performance of BERT-based models be improved for public health text classification tasks on social media, considering the limitations of token size and imbalanced data?
- Basis in paper: [explicit] The paper proposes ALEX to address these limitations but does not explore other potential solutions.
- Why unresolved: The paper focuses on ALEX but does not investigate alternative approaches to improve BERT-based models for these specific tasks.
- What evidence would resolve it: Research comparing ALEX with other methods designed to overcome the limitations of BERT-based models in public health text classification.

## Limitations

- LLM-based post-evaluation introduces uncertainty due to black-box model judgments without rigorous validation of correction logic
- Balanced training improvements lack quantitative validation beyond qualitative t-SNE visualizations
- Method doesn't address potential biases introduced by augmentation or scalability to multi-class problems

## Confidence

**High Confidence Claims:**
- ALEX achieved first place in Task 2 and Task 4, and second place in Task 1 at SMM4H 2023
- Data augmentation combined with weighted loss fine-tuning improves BERT performance on imbalanced datasets
- The framework is effective for text classification tasks with low inter-class distance between classes

**Medium Confidence Claims:**
- LLM-based post-evaluation consistently corrects BERT predictions without introducing significant errors
- Balanced embeddings from fine-tuned BERT improve class separability in latent space
- The method is particularly useful for public health analysis where minority classes are more valuable

**Low Confidence Claims:**
- The exact mechanism by which LLM corrections identify and fix incorrect BERT predictions
- Whether the improvements generalize beyond the specific SMM4H 2023 tasks
- The optimal balance between augmentation quantity and quality

## Next Checks

1. **Prompt Engineering Validation**: Systematically test different prompt structures and examples for the LLM correction step to determine which prompt designs yield the most consistent and accurate corrections. Compare automated prompt generation against the manual examples described in the paper.

2. **Ablation Study on Balanced Training**: Conduct a controlled experiment that isolates the effects of data augmentation versus weighted loss fine-tuning by running four conditions: baseline imbalanced, augmentation only, weighted loss only, and both combined. Measure not just final accuracy but also training dynamics and class-wise performance.

3. **Cross-Dataset Generalization Test**: Apply ALEX to at least two additional imbalanced text classification datasets from different domains (e.g., sentiment analysis with rare extreme opinions, fraud detection with few positive cases) to assess whether the method's effectiveness extends beyond public health social media data.