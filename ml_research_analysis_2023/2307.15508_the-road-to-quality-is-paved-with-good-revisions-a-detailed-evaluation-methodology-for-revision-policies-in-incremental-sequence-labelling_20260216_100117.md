---
ver: rpa2
title: 'The Road to Quality is Paved with Good Revisions: A Detailed Evaluation Methodology
  for Revision Policies in Incremental Sequence Labelling'
arxiv_id: '2307.15508'
source_url: https://arxiv.org/abs/2307.15508
tags:
- revisions
- incremental
- edits
- revision
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an evaluation methodology for revision policies
  in incremental sequence labelling, addressing the gap in evaluating how models revise
  their outputs as new input arrives. The authors formalise revisions and edits, characterise
  their types, and propose specialised metrics such as rate of revision, revision
  pertinence, and appropriateness to assess policy quality.
---

# The Road to Quality is Paved with Good Revisions: A Detailed Evaluation Methodology for Revision Policies in Incremental Sequence Labelling

## Quick Facts
- **arXiv ID**: 2307.15508
- **Source URL**: https://arxiv.org/abs/2307.15508
- **Reference count**: 26
- **Primary result**: Introduces evaluation methodology for revision policies in incremental sequence labelling, applied to NER, POS tagging, and slot filling tasks

## Executive Summary
This paper addresses the gap in evaluating revision policies for incremental sequence labelling by proposing a comprehensive evaluation methodology. The authors formalise revisions and edits, characterise their types, and propose specialised metrics such as rate of revision, revision pertinence, and appropriateness to assess policy quality. The methodology is applied to profile three Transformer-based models on NER, POS tagging, and slot filling tasks, comparing a restart-incremental approach with two TAPIR variants. Results show TAPIR reduces recomputations by 75–90% and revisions by 2–3x, though revisions often occur late in sequences, affecting prefix correctness.

## Method Summary
The paper formalises incremental sequence labelling with revisions as a process where models produce output prefixes that extend with each new input token, optionally revising past predictions. The methodology introduces specialised metrics including rate of revision, recomputation, and active recomputation, plus pertinence and appropriateness measures that evaluate whether revisions are effective and convenient. Three Transformer-based models are evaluated: a restart-incremental approach that fully recomputes outputs when revisions occur, and two TAPIR variants with adaptive recomputation policies that decide when to recompute based on the state of the incremental chart. The framework is applied to Named Entity Recognition, Part-of-Speech tagging, and slot filling tasks.

## Key Results
- TAPIR reduces recomputations by 75–90% and revisions by 2–3x compared to restart-incremental approach
- Revision pertinence (R-Pertinence) and appropriateness (R-Appropriateness) metrics help identify ineffective revisions
- TAPIR's adaptive policy significantly improves computational efficiency while maintaining output quality
- Revisions tend to occur late in sequences, affecting prefix correctness but potentially increasing effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The methodology enables detailed analysis of when, why, and how revisions happen by formalising revisions and edits and proposing specialised metrics.
- Mechanism: By defining revisions as changes to output prefixes beyond the mandatory addition of a new label, and edits as changes to individual labels, the methodology allows precise tracking of model behavior over time.
- Core assumption: Incremental sequence labelling tasks can be modelled as a sequence of output prefixes, each extending the previous one with a new label and optionally revising past labels.
- Evidence anchors:
  - [abstract] "formalise revisions and edits, characterise their types, and propose specialised metrics such as rate of revision, revision pertinence, and appropriateness to assess policy quality."
  - [section 4.2] "An edit occurs at time t for label i if lt_i ≠ lt-1_i, meaning that the model's prediction for wi's label changed. A revision occurs when, apart from the compulsory addition, a prefix changes at time t in relation to the previous prefix..."
  - [corpus] Weak: No direct corpus evidence found for the formalisation of revisions and edits.
- Break condition: If the model does not extend output with a new label at each time step, the formalisation breaks down.

### Mechanism 2
- Claim: TAPIR's adaptive recomputation policy reduces computational overhead by 75-90% and revisions by 2-3x compared to restart-incremental approach.
- Mechanism: TAPIR decides when to recompute based on the state of the incremental chart, avoiding unnecessary full recomputations that do not lead to revisions.
- Core assumption: Not all recomputations result in revisions, and a smart policy can reduce the number of recomputations while maintaining or improving output quality.
- Evidence anchors:
  - [abstract] "Results show TAPIR reduces recomputations by 75–90% and revisions by 2–3x, but revisions often occur late in the sequence, affecting prefix correctness."
  - [section 5] "Table 5 shows that the recomputation policy implemented in TAPIR reduces the number of restarts to between 10% and 25% in comparison to the restart incremental approach..."
  - [corpus] Weak: No direct corpus evidence found for the specific performance gains of TAPIR.
- Break condition: If the policy becomes too conservative and misses necessary revisions, output quality may suffer.

### Mechanism 3
- Claim: The evaluation methodology provides a framework for comparing different revision policies and guiding the design of better ones.
- Mechanism: By characterising edits and revisions along multiple dimensions (e.g., effectiveness, convenience, range) and proposing metrics like R-Pertinence and A-Appropriateness, the methodology enables nuanced comparison of policies.
- Core assumption: Different revision policies can be meaningfully compared along the proposed dimensions and metrics.
- Evidence anchors:
  - [abstract] "The methodology enables detailed analysis of when, why, and how revisions happen, guiding the design of better revision policies."
  - [section 4.2] "In this section, we propose a detailed characterisation for the types of edits and revisions based on ten dimensions, summarised in Table 3..."
  - [corpus] Weak: No direct corpus evidence found for the effectiveness of the evaluation methodology in guiding policy design.
- Break condition: If the proposed dimensions and metrics do not capture the most important aspects of revision policies, the methodology may not be useful for comparison and design.

## Foundational Learning

- Concept: Incremental sequence labelling
  - Why needed here: Understanding the incremental nature of the task is crucial for grasping the need for and design of revision policies.
  - Quick check question: What is the difference between incremental and non-incremental sequence labelling?

- Concept: Revision policies
  - Why needed here: Revision policies govern when and how models revise their outputs as new input arrives, which is the focus of the paper.
  - Quick check question: What are the three types of incremental processors discussed in the paper, and how do they differ in terms of revisions?

- Concept: Evaluation metrics
  - Why needed here: The paper proposes specialised metrics for evaluating revision policies, which are key to understanding the methodology.
  - Quick check question: What are some of the proposed metrics for evaluating revision policies, and what do they measure?

## Architecture Onboarding

- Component map: Input tokens -> Incremental model (Transformer) -> Revision policy (TAPIR variants) -> Output prefixes -> Evaluation framework (metrics) -> Gold standard

- Critical path:
  1. Model processes incoming input tokens incrementally, producing output prefixes
  2. Revision policy decides when and how to revise output based on the state of the incremental chart
  3. Evaluation framework measures the quality of the revisions using proposed metrics
  4. Results are analysed to guide the design of better revision policies

- Design tradeoffs:
  - Strict monotonicity vs. ability to revise: Monotonic models are stable but cannot recover from mistakes; models with revision policies can correct errors but may be less stable
  - Computational efficiency vs. output quality: Aggressive revision policies may improve output quality but at a higher computational cost
  - Timeliness of revisions vs. effectiveness: Early revisions may be less effective due to lack of context, while late revisions may delay the availability of correct outputs

- Failure signatures:
  - High rate of ineffective revisions (revising correct prefixes)
  - Low R-Appropriateness (not revising incorrect prefixes)
  - High number of recomputations that do not lead to revisions
  - Revisions occurring too late in the sequence, affecting prefix correctness

- First 3 experiments:
  1. Implement a simple revision policy (e.g., restart-incremental) and evaluate its performance using the proposed metrics
  2. Implement a more sophisticated revision policy (e.g., TAPIR) and compare its performance to the simple policy
  3. Experiment with different thresholds for triggering revisions and observe their impact on computational efficiency and output quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do revision policies perform when evaluated with genuinely incremental gold standards containing locally valid hypotheses?
- Basis in paper: [explicit] The authors note that incremental gold standards are usually unavailable and that using non-incremental gold standards unfairly expects models to commit to final decisions without observing disambiguating input
- Why unresolved: The paper uses non-incremental gold standards for evaluation and acknowledges this limitation, but doesn't explore how results would differ with incremental gold standards
- What evidence would resolve it: Comparative evaluation results using both non-incremental and incremental gold standards on the same models/tasks

### Open Question 2
- Question: What is the optimal balance between revision timeliness (early corrections) and revision effectiveness (correcting with more context)?
- Basis in paper: [inferred] The paper observes that TAPIR tends to have more revisions toward the end of sentences, which can make them more effective but delays final decisions
- Why unresolved: The paper identifies this trade-off but doesn't empirically determine what timing yields optimal overall performance
- What evidence would resolve it: Experiments varying revision timing thresholds and measuring resulting quality metrics across multiple tasks

### Open Question 3
- Question: How do different linguistic phenomena (like BIO labeling schemes or disfluencies) affect the effectiveness of revision policies?
- Basis in paper: [explicit] The authors mention that connectedness is particularly relevant for BIO labeling schemes and note that slot filling showed more final revisions than other tasks
- Why unresolved: The paper provides preliminary observations about task-specific differences but doesn't systematically analyze how linguistic features impact revision quality
- What evidence would resolve it: Detailed analysis correlating specific linguistic features with revision metrics across multiple language tasks

## Limitations

- The methodology relies heavily on the availability of incremental gold standards, which are often unavailable in practice
- The evaluation framework may not fully capture user experience perspectives on revision quality
- The claimed computational efficiency gains depend on specific hyperparameter settings that aren't fully detailed
- The sample size of three tasks limits generalizability to other domains

## Confidence

- **High confidence**: Formal framework for defining revisions and edits is clearly specified with mathematical notation
- **Medium confidence**: Practical utility of the evaluation methodology, as it's applied to three tasks but the sample size is limited
- **Medium confidence**: Claimed computational efficiency gains, as the exact implementation details of TAPIR's adaptive policy are not fully specified

## Next Checks

1. Test the evaluation framework on a task without an incremental gold standard to assess how metric scores change when using non-incremental gold standards as fallback
2. Conduct ablation studies on the TAPIR policy thresholds (τ values) to determine sensitivity and identify optimal settings across different task domains
3. Perform a user study evaluating whether the characterized revision types (effectiveness, convenience, range) align with human judgments of revision quality in dialogue systems