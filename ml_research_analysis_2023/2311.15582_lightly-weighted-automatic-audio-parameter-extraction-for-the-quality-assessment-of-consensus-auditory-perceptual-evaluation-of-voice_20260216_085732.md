---
ver: rpa2
title: Lightly Weighted Automatic Audio Parameter Extraction for the Quality Assessment
  of Consensus Auditory-Perceptual Evaluation of Voice
arxiv_id: '2311.15582'
source_url: https://arxiv.org/abs/2311.15582
tags:
- voice
- audio
- feature
- jitter
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the problem of inconsistent and difficult-to-standardize
  clinical voice quality assessment by proposing a lightly weighted automatic audio
  parameter extraction approach. The method utilizes age, sex, and five audio parameters:
  jitter, absolute jitter, shimmer, harmonic-to-noise ratio (HNR), and zero crossing,
  employing classical machine learning techniques.'
---

# Lightly Weighted Automatic Audio Parameter Extraction for the Quality Assessment of Consensus Auditory-Perceptual Evaluation of Voice

## Quick Facts
- arXiv ID: 2311.15582
- Source URL: https://arxiv.org/abs/2311.15582
- Reference count: 25
- Primary result: Proposed classical ML approach with 5 audio parameters achieves RMSE 14-18, outperforming pre-trained models on CAPE-V roughness and strain prediction

## Executive Summary
This study addresses the challenge of inconsistent clinical voice quality assessment by proposing a lightweight automatic audio parameter extraction method for CAPE-V score prediction. The approach uses age, sex, and five interpretable audio parameters (jitter, absolute jitter, shimmer, HNR, and zero crossing) with classical machine learning algorithms. The method demonstrates performance comparable to state-of-the-art approaches while excelling specifically in roughness and strain attribute prediction. The study highlights the effectiveness of simple, interpretable features over complex pre-trained models for perceptual voice quality assessment tasks.

## Method Summary
The study employs classical machine learning methods (Random Forest, SVM, and KNN) to predict CAPE-V scores using age, sex, and five audio parameters extracted from the PVQD dataset. Audio samples are preprocessed by downsampling to 8 kHz and normalized to uniform lengths. Audio parameters are extracted using Surfboard 0.2.0, and models are trained using scikit-learn 1.3.0 and Pytorch 2.0.1. Data augmentation is performed to increase training data size. The proposed approach is compared against pre-trained models (Wav2vec2, Hubert, WavLM, Whisper) and evaluated using RMSE and Pearson correlation metrics.

## Key Results
- RMSE values of 14-18 achieved, comparable to state-of-the-art methods
- Outperforms pre-trained models on CAPE-V score prediction
- Jitter and HNR identified as most important features for roughness and strain attributes
- Classical ML methods demonstrate superior capability with small sample sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jitter and HNR parameters directly capture the acoustic irregularity and noise content that raters perceive as roughness and strain in CAPE-V scoring.
- Mechanism: These parameters measure vocal fold vibration variability (jitter) and harmonic-to-noise ratio (HNR), which mathematically encode the same perceptual features that clinicians use to assign roughness and strain scores.
- Core assumption: Clinicians' perceptual ratings of roughness and strain are fundamentally based on detecting irregular voicing and noise components in the signal.
- Evidence anchors:
  - [abstract] "Audio parameters such as jitter and the HNR are proven to be suitable for characterizing voice quality attributes, such as roughness and strain."
  - [section] "Jitter represents the absence of precise control over the vocal cord, which corresponds with the irregularity in the voicing source, and the HNR denotes the fraction of noise in the emitting sound, which serves as the main characteristic of roughness or strain of sound."
  - [corpus] Weak evidence - corpus neighbors focus on pre-trained models and clinical applications, but do not directly address the specific mechanism of jitter/HNR for roughness/strain characterization.
- Break condition: If the perceptual basis of roughness and strain ratings does not fundamentally rely on detecting irregular voicing and noise, then these parameters would not correlate with human ratings.

### Mechanism 2
- Claim: Classical machine learning methods outperform deep learning approaches when training data is limited and the feature space is interpretable.
- Mechanism: RF, SVM, and KNN can effectively learn decision boundaries in low-dimensional spaces with clear physical meaning, while deep models require larger datasets and may lose interpretability.
- Core assumption: The relationship between audio parameters and CAPE-V scores is sufficiently linear or has clear thresholds that can be captured by classical algorithms.
- Evidence anchors:
  - [abstract] "The proposed method utilizes age, sex, and five audio parameters... A classical machine learning approach is employed."
  - [section] "Classical machine learning methods had demonstrated superior capability in small sample size and efficacy in identifying specific critical thresholds within calculated parameters."
  - [corpus] Weak evidence - corpus neighbors discuss pre-trained models but do not directly compare classical ML performance on small datasets.
- Break condition: If the CAPE-V scoring relationship is highly non-linear or requires complex feature interactions that classical methods cannot capture, then deep learning would outperform despite smaller datasets.

### Mechanism 3
- Claim: Pre-trained models are less effective for voice quality assessment because they are optimized for speech recognition tasks rather than perceptual quality evaluation.
- Mechanism: ASR-focused pretraining learns representations that emphasize content intelligibility over acoustic quality characteristics, making them suboptimal for capturing perceptual roughness and strain attributes.
- Core assumption: The latent representations learned by ASR models prioritize phonetic content and speaker characteristics over noise and irregularity features important for quality assessment.
- Evidence anchors:
  - [abstract] "Conversely, pre-trained models exhibit limitations in effectively addressing noise-related scorings."
  - [section] "The implemented pre-trained model was originally designed for ASR task, which relies on noise robustness to achieve improved performance. This decreased the performance in the perception of the roughness attribute of the voice."
  - [corpus] Moderate evidence - corpus includes "Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations" which suggests exploration of combining traditional and foundation model approaches.
- Break condition: If pre-trained models could be fine-tuned specifically for perceptual quality tasks rather than repurposed from ASR, or if the ASR optimization incidentally captures quality-relevant features.

## Foundational Learning

- Concept: CAPE-V scale and perceptual voice quality assessment
  - Why needed here: Understanding the clinical context and scoring criteria is essential for interpreting model outputs and feature importance
  - Quick check question: What are the six attributes assessed in CAPE-V and what scale is used for each?

- Concept: Audio signal processing fundamentals (jitter, shimmer, HNR, zero-crossing)
  - Why needed here: These parameters form the core feature set and their mathematical definitions determine what aspects of voice quality they capture
  - Quick check question: How does zero-crossing rate relate to perceived noise level in a voice signal?

- Concept: Classical machine learning algorithms (RF, SVM, KNN)
  - Why needed here: These are the primary modeling approaches used and understanding their mechanisms explains the performance differences
  - Quick check question: What is the key difference between how RF and SVM handle non-linear decision boundaries?

## Architecture Onboarding

- Component map: Feature extraction (audio parameters) → Classical ML models (RF/SVM/KNN) or Fine-tuned pre-trained models → CAPE-V score prediction → Evaluation (RMSE, correlation)
- Critical path: Feature extraction → Model training → Prediction → Evaluation
- Design tradeoffs: Simple interpretable features vs. complex learned representations; classical ML vs. deep learning; computational efficiency vs. potential accuracy
- Failure signatures: High RMSE values, low correlation with ground truth, poor performance on specific attributes (especially roughness), overfitting with small datasets
- First 3 experiments:
  1. Train RF model with all five audio parameters and compare RMSE to baseline (simple mean prediction)
  2. Train SVM model with jitter and HNR only to test their individual importance
  3. Fine-tune Wav2vec2 on augmented dataset and compare performance to RF baseline

## Open Questions the Paper Calls Out

- How can we improve the training of fine-tuned layers in pre-trained models to overcome the limitations of small sample sizes in voice quality assessment?
- What architectural modifications to Whisper could potentially improve its performance in CAPE-V score prediction compared to other pre-trained models?
- How can we better characterize and utilize sex and age as features in voice quality assessment models to enhance prediction accuracy?

## Limitations

- Limited dataset size (296 samples) creates potential overfitting risks
- Comparative analysis with pre-trained models lacks comprehensive ablation studies
- Focus on sustained vowels and sentence reading may limit generalizability to diverse speech contexts

## Confidence

- High confidence: Classical ML methods with interpretable audio features effectively predict roughness and strain scores
- Medium confidence: Classical methods outperform pre-trained models, given limited comparative details
- Low confidence: Generalizability to other voice quality assessment contexts beyond PVQD dataset

## Next Checks

1. Conduct ablation studies removing individual audio parameters to quantify their independent contribution to model performance
2. Test model performance on a separate validation set with different speakers and recording conditions
3. Compare results with a fine-tuned pre-trained model specifically optimized for perceptual quality assessment rather than ASR