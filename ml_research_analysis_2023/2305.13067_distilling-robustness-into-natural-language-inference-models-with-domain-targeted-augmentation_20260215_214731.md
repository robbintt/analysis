---
ver: rpa2
title: Distilling Robustness into Natural Language Inference Models with Domain-Targeted
  Augmentation
arxiv_id: '2305.13067'
source_url: https://arxiv.org/abs/2305.13067
tags:
- data
- teacher
- distillation
- examples
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates knowledge distillation methods for natural
  language inference, focusing on improving model robustness to out-of-distribution
  data. The authors propose two complementary approaches: generating domain-targeted
  unlabeled examples to augment distillation, and upsampling minority examples that
  counter common spurious patterns during training.'
---

# Distilling Robustness into Natural Language Inference Models with Domain-Targeted Augmentation

## Quick Facts
- arXiv ID: 2305.13067
- Source URL: https://arxiv.org/abs/2305.13067
- Reference count: 19
- Primary result: Domain-targeted augmentation and Distilled Minority Upsampling significantly improve NLI model robustness to out-of-distribution data

## Executive Summary
This paper addresses the challenge of improving natural language inference model robustness to out-of-distribution data through knowledge distillation. The authors propose two complementary approaches: generating domain-targeted unlabeled examples to augment distillation, and upsampling minority examples that counter spurious patterns during training. They introduce Distilled Minority Upsampling (DMU) to identify and upsample such minority examples. Experiments on MNLI show these methods significantly outperform previous robustness solutions, with domain-targeted augmentation improving generalization to both target and non-target domains, and DMU substantially improving performance on adversarial SNLI-hard examples.

## Method Summary
The method combines knowledge distillation with domain-targeted data augmentation and minority upsampling. A teacher model provides soft predictions to a student model, which is trained not only on original labeled data but also on unlabeled examples generated to mimic target domains. The Distilled Minority Upsampling (DMU) component identifies minority examples that counter spurious patterns by examining student misclassifications during distillation, then upsamples these challenging examples to provide additional supervision. When using ensemble teachers, predictions are averaged to create more stable target distributions, reducing overfitting to individual teacher idiosyncrasies.

## Key Results
- Domain-targeted augmentation improves MNLI generalization to both target and non-target domains
- DMU substantially improves performance on adversarial SNLI-hard examples
- Methods are effective at improving robustness on HANS adversarial dataset despite using far fewer augmented examples than previous work
- Ensemble teachers combined with domain-targeted augmentation further improve performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-targeted augmentation improves student model robustness to out-of-distribution data by encouraging behavior alignment on generated examples from target domains.
- Mechanism: Generating unlabeled examples mimicking target domains and using them only for knowledge distillation (not supervised training) forces the student to internalize teacher behavior patterns without being misled by potentially incorrect synthetic labels.
- Core assumption: Generated data captures distributional characteristics of target domains sufficiently to improve generalization, even without supervision.
- Evidence anchors:
  - [abstract]: "The domain-targeted augmentation improves generalization to both target and non-target domains."
  - [section 2.2]: Describes generating examples using GPT-3 with high-level domain descriptions, ensuring variety across classes without providing real examples.
  - [corpus]: Weak corpus support - related papers focus on vision-language or synthetic augmentation but not NLP NLI domain-targeted approaches.
- Break condition: If generated data poorly represents target domain or introduces artifacts that mislead the student model.

### Mechanism 2
- Claim: Distilled Minority Upsampling (DMU) improves robustness by identifying and upsampling minority examples that counter spurious patterns during distillation.
- Mechanism: Upsampling examples misclassified by the student (not teacher) during distillation, optionally using ensemble models to identify these examples, provides additional supervision on challenging minority cases.
- Core assumption: Student-misclassified examples are more informative for robustness than teacher-misclassified ones in a distillation context.
- Evidence anchors:
  - [abstract]: "DMU substantially improves performance on adversarial SNLI-hard examples."
  - [section 2.4]: Contrasts DMU with JTT, emphasizing student-based identification and combining distillation with minority supervision.
  - [section 4.2]: Shows DMU with ensemble identification improves SNLI-hard performance significantly.
- Break condition: If student misclassifications are dominated by noise or the student is too weak to identify meaningful minority patterns.

### Mechanism 3
- Claim: Ensemble teacher models improve robustness when combined with domain-targeted augmentation, especially when teacher predictions have high variance.
- Mechanism: Averaging predictions from multiple teacher models creates more stable target distributions, reducing overfitting to individual teacher idiosyncrasies and improving out-of-distribution generalization.
- Core assumption: Ensemble averaging reduces variance in teacher predictions more than it reduces signal, particularly beneficial when single teacher models show high OOD variance.
- Evidence anchors:
  - [section 2.1]: Describes ensemble distillation loss and motivation about higher OOD prediction variance.
  - [section 4.3]: Shows BERT ensemble + domain-targeted augmentation improves MNLI performance beyond single teacher.
  - [corpus]: Limited support - most related work uses single teachers; ensemble KD is less explored in NLP.
- Break condition: If teacher models are too similar, ensemble provides no benefit; if variance is too high, ensemble averaging may dilute useful signals.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Core technique transferring teacher model knowledge to smaller student models while preserving performance.
  - Quick check question: Why use soft teacher predictions instead of hard labels during distillation?

- Concept: Out-of-Distribution (OOD) Generalization
  - Why needed here: The primary evaluation metric measuring model performance on unseen domains/datasets.
  - Quick check question: How does OOD performance differ from in-distribution performance, and why is it important?

- Concept: Spurious Correlations and Minority Examples
  - Why needed here: Understanding how models exploit dataset artifacts and how to mitigate this for robustness.
  - Quick check question: What are examples of spurious patterns in NLI datasets, and how do they affect model generalization?

## Architecture Onboarding

- Component map:
  Teacher model(s) -> Data generator (GPT-3) -> Minority identifier (DMU) -> Student model -> Evaluation

- Critical path:
  1. Generate unlabeled domain-targeted examples
  2. Identify minority examples (DMU)
  3. Combine original data, generated data, and upsampled minority examples
  4. Perform knowledge distillation with ensemble teachers if applicable
  5. Evaluate on OOD datasets (MNLI, HANS, SNLI-hard)

- Design tradeoffs:
  - Using unlabeled generated data avoids label noise but loses supervised signal
  - Student-based vs teacher-based minority identification affects which examples get emphasized
  - Ensemble teachers improve stability but increase computational cost

- Failure signatures:
  - Poor OOD performance despite good in-distribution scores
  - Generated data introduces artifacts that degrade student performance
  - DMU upsampling leads to overfitting on minority examples

- First 3 experiments:
  1. Baseline: Standard knowledge distillation without augmentation
  2. Domain-targeted augmentation only: Generate and distill on unlabeled examples
  3. DMU only: Identify and upsample minority examples during standard distillation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper limit on the effectiveness of domain-targeted data augmentation for improving robustness to out-of-distribution data?
- Basis in paper: [inferred] The paper shows that domain-targeted data augmentation improves performance on both target and non-target domains, but does not explore the limits of this improvement.
- Why unresolved: The experiments focus on a specific number of augmented examples (around 5k) and do not explore whether increasing the number of examples would lead to diminishing returns or continued improvement.
- What evidence would resolve it: Systematic experiments varying the number of augmented examples from very small to very large, to determine the point at which additional examples no longer improve robustness.

### Open Question 2
- Question: How does the performance of Distilled Minority Upsampling (DMU) compare to other methods for identifying and upsampling minority examples?
- Basis in paper: [explicit] The paper introduces DMU and shows its effectiveness on SNLI-hard, but does not compare it to other methods for identifying and upsampling minority examples.
- Why unresolved: The experiments only compare DMU to a JTT baseline, but there may be other methods that could perform better or worse.
- What evidence would resolve it: Experiments comparing DMU to other methods for identifying and upsampling minority examples, such as focal loss or counterfactual data augmentation.

### Open Question 3
- Question: How does the use of ensembles of teacher models affect the performance of domain-targeted data augmentation and DMU?
- Basis in paper: [explicit] The paper shows that using ensembles of teacher models can improve the performance of both domain-targeted data augmentation and DMU, but does not explore the optimal number or composition of teacher models.
- Why unresolved: The experiments use a fixed number of teacher models (8 for DMU, 7 for self-distillation) and do not explore whether using more or fewer models, or models with different architectures, would lead to better performance.
- What evidence would resolve it: Systematic experiments varying the number and composition of teacher models to determine the optimal configuration for improving robustness.

## Limitations
- Generated data quality depends heavily on GPT-3's ability to produce meaningful examples that capture domain characteristics without introducing artifacts
- DMU effectiveness hinges on student model capability to identify genuinely informative minority examples rather than noise
- Ensemble teacher approach increases computational overhead and may not scale well to larger model families

## Confidence

- High confidence: The core observation that domain-targeted augmentation improves OOD generalization across multiple datasets is well-supported by experimental results. The mechanism connecting generated examples to robustness gains is theoretically sound.
- Medium confidence: The DMU approach shows strong empirical results on SNLI-hard, but the specific criteria for minority example identification and the generalizability across different dataset characteristics could benefit from further exploration.
- Medium confidence: Ensemble teacher improvements are demonstrated but the magnitude of benefit relative to increased computational cost is not fully characterized.

## Next Checks
1. **Generated Data Quality Analysis**: Systematically evaluate the quality and diversity of GPT-3 generated examples by having human annotators assess whether generated examples capture target domain characteristics and avoid introducing spurious patterns that could mislead the student model.

2. **DMU Robustness to Student Model Quality**: Test DMU performance across different student model capacities (small vs large) to determine whether the method's effectiveness depends on having sufficiently capable student models to identify meaningful minority examples.

3. **Computational Cost-Benefit Analysis**: Quantify the computational overhead of ensemble teachers and compare the performance gains against alternative approaches like temperature scaling or label smoothing to determine if the ensemble approach provides sufficient benefit relative to its cost.