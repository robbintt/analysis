---
ver: rpa2
title: Input Reconstruction Attack against Vertical Federated Large Language Models
arxiv_id: '2311.07585'
source_url: https://arxiv.org/abs/2311.07585
tags:
- input
- embeddings
- reconstruction
- user
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that in vertical federated learning (VFL)
  for large language models (LLMs), user inputs can be easily reconstructed from intermediate
  embeddings. The authors propose a white-box attack that reconstructs word embeddings
  layer-by-layer using gradient descent, then maps them to tokens via similarity matching.
---

# Input Reconstruction Attack against Vertical Federated Large Language Models

## Quick Facts
- arXiv ID: 2311.07585
- Source URL: https://arxiv.org/abs/2311.07585
- Reference count: 23
- Primary result: Input reconstruction attack achieves complete recovery in one second even with strong Gaussian noise added to embeddings

## Executive Summary
This paper demonstrates a critical vulnerability in vertical federated learning (VFL) for large language models (LLMs), where user inputs can be completely reconstructed from intermediate embeddings exchanged between parties. The authors propose a white-box attack that uses gradient descent to reconstruct word embeddings layer-by-layer, then maps them to tokens via similarity matching. Experiments show that even with Gaussian noise added to embeddings as a defense, input reconstruction remains highly effective while model utility collapses, making noise-based defenses impractical. The attack works efficiently for inputs up to 1,500 tokens and requires only commercial GPU resources.

## Method Summary
The attack works in two steps: first, it reconstructs word embeddings by optimizing them through gradient descent to minimize the difference between reconstructed and actual hidden embeddings; second, it maps these reconstructed embeddings to actual tokens by finding the most similar words in the vocabulary. The attack exploits the fact that in LLMs, hidden embeddings maintain the same size as the input across layers, making the optimization problem well-posed. The evaluation uses ChatGLM6B model with SQuAD and FindSum datasets, measuring reconstruction accuracy using BLEU-4 scores and testing noise defenses with varying standard deviations.

## Key Results
- Complete input reconstruction achieved in approximately one second using commercial GPU
- BLEU-4 scores above 0.98 for various input lengths up to 1,500 tokens
- Gaussian noise addition fails as defense - model performance degrades severely before reconstruction accuracy drops
- Attack remains effective even with noise standard deviation up to 50, with complete failure only at std. 150

## Why This Works (Mechanism)

### Mechanism 1
- Claim: White-box reconstruction is effective because the adversary has full access to the bottom model and can directly optimize embeddings to match intermediate outputs
- Mechanism: The attack performs gradient descent on word embeddings to minimize the difference between reconstructed and actual hidden embeddings, then maps these to tokens via similarity matching
- Core assumption: The hidden embeddings and model parameters are differentiable and accessible to the adversary
- Evidence anchors:
  - [abstract] "We propose a white-box attack that reconstructs word embeddings layer-by-layer using gradient descent, then maps them to tokens via similarity matching"
  - [section] "We first reconstruct the word embeddings in the first layer, then we compute the similarity between the reconstructed word embeddings and the actual word embeddings in the bottom model, and find the most similar words"
  - [corpus] Weak evidence - related papers focus on feature reconstruction attacks but don't directly address LLM-specific white-box reconstruction
- Break condition: If the hidden embeddings are not differentiable or the adversary cannot access model parameters, the gradient-based optimization becomes infeasible

### Mechanism 2
- Claim: Noise addition fails as a defense because model utility degrades before input reconstruction accuracy drops significantly
- Mechanism: Gaussian noise added to hidden embeddings disrupts the model's ability to make accurate predictions, but the reconstruction attack can still recover inputs with high accuracy
- Core assumption: The reconstruction attack is robust to noise in the hidden embeddings
- Evidence anchors:
  - [abstract] "Experiments show that even with a commercial GPU, the input sentence can be reconstructed in only one second. We also investigated adding noise in the hidden embeddings before sending it to the model provider in order to enhance input privacy. However, experiment results show that such defense is impractical, since the model performance is also severely downgraded"
  - [section] "For noise std. around 50, the attack performance maintains at around 1.0 (almost complete reconstruction of the input text). Until the noise std. reaches to 150, will the attack performance become zero"
  - [corpus] Moderate evidence - related papers discuss noise addition but focus on different types of attacks and defenses
- Break condition: If noise levels are increased beyond the point where model utility is completely destroyed, reconstruction accuracy may drop significantly

### Mechanism 3
- Claim: Long texts can be reconstructed efficiently because the number of equations matches the number of variables in the hidden embeddings
- Mechanism: The hidden embeddings have the same size as the input, allowing for a well-posed optimization problem that can be solved using gradient descent
- Core assumption: The size of hidden embeddings remains constant across layers and matches the input size
- Evidence anchors:
  - [section] "For large language models, all the hidden embeddings have the same size, which indicates that in (3), the number of variables is the same as the number of equations"
  - [section] "The Bleu scores for different text lengths are all above 0.98, and no decreasing trends are observed for long texts"
  - [corpus] Weak evidence - related papers don't specifically address the relationship between embedding size and reconstruction efficiency in LLMs
- Break condition: If the hidden embeddings are downsampled or compressed layer-by-layer, the number of variables may become less than the number of equations, making reconstruction more difficult

## Foundational Learning

- Concept: Gradient descent optimization
  - Why needed here: The attack uses gradient descent to minimize the difference between reconstructed and actual hidden embeddings
  - Quick check question: How does gradient descent update the word embeddings to minimize the reconstruction error?

- Concept: Embedding similarity matching
  - Why needed here: After reconstructing word embeddings, the attack finds the most similar tokens in the vocabulary
  - Quick check question: How is the similarity between reconstructed embeddings and vocabulary embeddings computed?

- Concept: Federated learning architecture
  - Why needed here: Understanding the split learning setup is crucial for identifying the attack surface
  - Quick check question: In split learning, what information is exchanged between the user and model provider during inference?

## Architecture Onboarding

- Component map: User side (bottom model: embedding layers + early transformer layers) -> Communication (hidden embeddings) -> Model provider side (top model: later transformer layers + linear layer)
- Critical path:
  1. User feeds input to bottom model
  2. Bottom model generates hidden embeddings
  3. Hidden embeddings sent to model provider
  4. Top model processes embeddings and generates output
  5. Output sent back to user
- Design tradeoffs:
  - Privacy vs. utility: Adding noise for privacy degrades model performance
  - Complexity vs. security: Redesigning LLM backbone for privacy increases implementation complexity
  - Communication vs. security: Training in split manner avoids white-box attacks but requires trusted parties
- Failure signatures:
  - High BLEU scores indicate successful reconstruction
  - Sharp drop in model performance with added noise
  - Reconstruction accuracy remains high even with moderate noise levels
- First 3 experiments:
  1. Implement the reconstruction attack on a small LLM and verify input recovery
  2. Test noise addition as a defense and measure impact on reconstruction accuracy and model performance
  3. Experiment with different split points in the model and observe changes in attack effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the input reconstruction attack be effectively mitigated by modifying the LLM backbone architecture, such as reducing hidden embedding sizes layer-by-layer or gradually dropping early token embeddings?
- Basis in paper: [explicit] The paper suggests this as a possible defense in Section 6.1, noting that "shrinking the size of hidden embeddings layer by layer, or gradually dropping embeddings of early tokens" could help, but acknowledges that verification would be expensive.
- Why unresolved: The paper does not provide experimental evidence or analysis of these architectural modifications, leaving their effectiveness untested.
- What evidence would resolve it: Empirical studies comparing reconstruction success rates before and after implementing these architectural changes, along with analysis of the trade-offs in model performance and computational cost.

### Open Question 2
- Question: How does the input reconstruction attack performance vary across different LLM architectures (e.g., GPT, BERT, PaLM) and tokenization methods?
- Basis in paper: [inferred] The experiments were conducted only on ChatGLM6B, but the attack methodology is architecture-agnostic. The paper does not explore variations in attack effectiveness across different model types or tokenization schemes.
- Why unresolved: The paper's scope was limited to a single model, leaving questions about generalizability to other architectures unanswered.
- What evidence would resolve it: Comparative experiments applying the same reconstruction attack to multiple LLM architectures and tokenization methods, measuring attack success rates and computational requirements for each.

### Open Question 3
- Question: Are there alternative noise-adding strategies or differential privacy mechanisms that could effectively protect input privacy without severely degrading model performance?
- Basis in paper: [explicit] Section 5.1 demonstrates that Gaussian noise fails because model utility collapses before privacy is achieved, but does not explore other noise mechanisms or privacy-preserving techniques.
- Why unresolved: The paper only tested Gaussian noise and did not investigate other potential defenses like Laplace noise, gradient clipping, or advanced differential privacy frameworks.
- What evidence would resolve it: Systematic evaluation of various noise mechanisms and differential privacy techniques, measuring both reconstruction success rates and model utility across different parameter settings.

## Limitations

- Attack methodology may be specific to architectures with consistent embedding sizes across layers
- Defense exploration limited to Gaussian noise, missing other privacy-preserving approaches
- Dataset evaluation focused primarily on question-answering tasks with structured inputs

## Confidence

- High Confidence in the core attack mechanism - The white-box reconstruction approach using gradient descent and similarity matching is technically sound and well-supported by experimental evidence
- Medium Confidence in the noise defense evaluation - While experimental results clearly show noise degrades model utility before protecting privacy, exploration of defense space is limited
- Low Confidence in generalizability across all LLM architectures - Assumptions about embedding size consistency may not hold for all models, particularly those with architectural variations

## Next Checks

1. Cross-architecture validation: Implement the attack on different LLM architectures (BERT, RoBERTa, OPT) to verify if reconstruction efficiency and accuracy remain consistent when hidden embeddings change size across layers.

2. Defense mechanism exploration: Systematically evaluate alternative privacy-preserving approaches including differential privacy with adaptive noise mechanisms, split learning with secure aggregation, and homomorphic encryption for hidden embeddings.

3. Adversary capability calibration: Test the attack under different adversary assumptions (black-box access to bottom model, partial parameter knowledge) to understand the minimum capabilities required for successful reconstruction.