---
ver: rpa2
title: 'SimQ-NAS: Simultaneous Quantization Policy and Neural Architecture Search'
arxiv_id: '2312.13301'
source_url: https://arxiv.org/abs/2312.13301
tags:
- search
- quantization
- architecture
- policy
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for simultaneous quantization policy
  and neural architecture search (NAS) called SimQ-NAS. The key idea is to leverage
  multi-objective evolutionary search algorithms paired with lightly trained predictors
  to efficiently search for both sub-network architecture and corresponding quantization
  policy across different hardware platforms.
---

# SimQ-NAS: Simultaneous Quantization Policy and Neural Architecture Search

## Quick Facts
- arXiv ID: 2312.13301
- Source URL: https://arxiv.org/abs/2312.13301
- Reference count: 29
- Primary result: SimQ-NAS achieves up to 4.80x latency and 3.44x model size improvement without accuracy degradation compared to INT8 baselines

## Executive Summary
This paper introduces SimQ-NAS, a method for simultaneous neural architecture search (NAS) and quantization policy optimization. The approach leverages multi-objective evolutionary search algorithms with lightweight predictors to efficiently explore the joint space of sub-network architectures and quantization policies across different hardware platforms. By using InstaTune-generated super-networks and post-training static quantization, the method achieves significant improvements in latency and model size while maintaining accuracy on transformer and convolutional models.

## Method Summary
SimQ-NAS performs joint optimization of neural architecture and quantization policy using LINAS, a multi-objective evolutionary search algorithm. The method creates elastic super-networks through InstaTune or conventional methods, then searches for optimal sub-networks and quantization policies using lightweight predictors (ridge regression or SVR). Post-training static quantization (PTQ static) with mixed INT8 and FP32 precision is applied to discovered sub-networks. The search targets accuracy, latency, and model size objectives across hardware platforms including Intel Xeon processors.

## Key Results
- Achieves up to 4.80x improvement in latency over fully quantized INT8 baselines
- Achieves up to 3.44x improvement in model size while maintaining accuracy
- Outperforms sequential optimization approaches by finding configurations where architectural and quantization choices complement each other
- Successfully optimizes diverse model architectures including ViT, BERT, BEiT-3, and ResNet50

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simultaneous optimization of architecture and quantization policy outperforms sequential approaches by exploring a joint search space that captures interactions between structural and numerical precision choices.
- Mechanism: The LINAS algorithm iteratively refines sub-network predictions using lightweight predictors, enabling efficient traversal of a combined architecture-quantization search space without the computational cost of fully training super-networks.
- Core assumption: Joint optimization can find configurations where structural changes complement quantization choices, leading to better Pareto-optimal trade-offs than optimizing each independently.
- Evidence anchors:
  - [abstract] "by using multi-objective search algorithms paired with lightly trained predictors, we can efficiently search for both the sub-network architecture and the corresponding quantization policy"
  - [section] "Our proposed approach performs a joint architecture and quantization policy search to simultaneously optimize for network architecture and a corresponding quantization policy"
  - [corpus] Weak evidence - no direct citations found in neighbor papers, though related works exist on joint NAS+quantization
- Break condition: If the predictor accuracy degrades significantly with larger search spaces, or if the joint space becomes too sparse to yield meaningful Pareto improvements.

### Mechanism 2
- Claim: Using InstaTune-generated super-networks reduces the computational overhead compared to training super-networks from scratch, making the overall search process more efficient.
- Mechanism: InstaTune leverages pre-trained models and creates elastic super-networks during downstream fine-tuning, avoiding the expensive progressive shrinking required for traditional super-network training.
- Core assumption: Pre-trained models contain sufficient representational capacity that can be efficiently adapted through elastic fine-tuning to create suitable super-networks for downstream tasks.
- Evidence anchors:
  - [section] "Techniques like InstaTune[9] leverage off-the-shelf pre-trained models and creates the elastic super-network during the downstream fine-tuning stage and improves the overall efficiency."
  - [corpus] Weak evidence - InstaTune paper cited but not directly available in neighbors, though the concept aligns with established transfer learning principles
- Break condition: If the downstream task requires architectural modifications too extensive for fine-tuning to capture, or if the pre-trained model's inductive biases conflict with optimal sub-network structures.

### Mechanism 3
- Claim: Post-training static quantization (PTQ static) enables efficient deployment of searched sub-networks without requiring retraining with quantization-aware techniques.
- Mechanism: PTQ static quantizes weights and activations using calibration data to determine optimal quantization parameters, avoiding the computational expense of QAT while still achieving good performance with mixed precision (INT8 and FP32).
- Core assumption: The searched sub-networks can maintain accuracy when quantized using PTQ static, particularly with mixed precision allowing critical layers to remain in FP32.
- Evidence anchors:
  - [section] "We then perform post-training static quantization (PTQ static) for sub-networks with the selected mixed precision quantization policy"
  - [corpus] Weak evidence - no direct citations found in neighbor papers, though PTQ is a well-established technique
- Break condition: If the calibration dataset is not representative of deployment conditions, or if the accuracy degradation from PTQ exceeds acceptable thresholds for the application.

## Foundational Learning

- Concept: Multi-objective evolutionary search algorithms
  - Why needed here: Required to navigate the complex Pareto frontier between accuracy, latency, and model size in the joint search space
  - Quick check question: What distinguishes NSGA-II from random search in handling multiple conflicting objectives?

- Concept: Super-network architecture and weight sharing
  - Why needed here: Understanding how elastic super-networks enable efficient sub-network extraction without retraining
  - Quick check question: How does progressive shrinking differ from the elastic fine-tuning approach used by InstaTune?

- Concept: Post-training quantization vs quantization-aware training
  - Why needed here: Critical for understanding why PTQ static was chosen over QAT and its implications for deployment
  - Quick check question: What are the trade-offs between calibration-based PTQ and training-based QAT in terms of accuracy and computational cost?

## Architecture Onboarding

- Component map: Pre-trained models → InstaTune super-network generation → LINAS search with predictors → PTQ static quantization → Deployed sub-networks
- Critical path: The most time-consuming step is typically super-network generation, followed by search iterations. For a new engineer, understanding the data dependencies between these stages is crucial for debugging and optimization.
- Design tradeoffs: The choice of using only INT8 and FP32 precision limits the search space but ensures hardware compatibility. Using more sophisticated predictors could improve search efficiency but would increase computational overhead. The decision to use PTQ static rather than QAT trades potential accuracy for deployment efficiency.
- Failure signatures: Poor predictor accuracy leading to suboptimal sub-networks, super-network generation failures due to incompatible pre-trained model architectures, or quantization calibration producing inadequate results for certain layers.
- First 3 experiments:
  1. Verify super-network generation works correctly by extracting a simple sub-network and checking accuracy on a validation set
  2. Run a small-scale search with a reduced architecture search space to validate the LINAS implementation and predictor accuracy
  3. Test PTQ static quantization on a known sub-network to ensure the calibration process produces acceptable accuracy degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SimQ-NAS scale with increasing model size and complexity, particularly for very large transformer models?
- Basis in paper: [inferred] The paper demonstrates results on a range of models including ViT, BERT, BEiT-3, and ResNet50, but does not explore extremely large transformer models. The search space sizes mentioned (up to 1050) suggest computational challenges with larger models.
- Why unresolved: The paper does not investigate the scalability of the method to very large transformer models, which are becoming increasingly common in modern deep learning applications.
- What evidence would resolve it: Experiments demonstrating the performance and efficiency of SimQ-NAS on larger transformer models (e.g., GPT-3, GPT-4 sized models) would provide insights into the scalability of the approach.

### Open Question 2
- Question: What is the impact of using different search algorithms on the performance of SimQ-NAS, and how does LINAS compare to other multi-objective optimization methods?
- Basis in paper: [explicit] The paper mentions using LINAS search algorithm and compares it to NSGA-2 and Random Search for BERT SST2, showing LINAS's efficacy. However, a comprehensive comparison with other state-of-the-art search algorithms is not provided.
- Why unresolved: The paper only briefly mentions LINAS's performance compared to a few other algorithms, without a thorough comparison to a wider range of multi-objective optimization methods.
- What evidence would resolve it: A comprehensive benchmarking study comparing LINAS to other advanced multi-objective optimization algorithms (e.g., SPEA2, MOEA/D) on various model architectures and tasks would clarify the relative strengths and weaknesses of different search approaches.

### Open Question 3
- Question: How does the performance of SimQ-NAS vary across different hardware platforms and what are the implications for deployment in diverse computing environments?
- Basis in paper: [explicit] The paper mentions that latency measurements were performed on different Intel Xeon platforms (8280, 8360Y, 8380, and 8480), but does not extensively explore performance variations across a wide range of hardware.
- Why unresolved: While the paper acknowledges the use of different hardware platforms, it does not provide a comprehensive analysis of how the method's performance scales or adapts to diverse computing environments, which is crucial for real-world deployment.
- What evidence would resolve it: Extensive experiments evaluating SimQ-NAS performance across various hardware platforms (e.g., GPUs, TPUs, edge devices) and different computing environments would provide insights into its adaptability and potential deployment scenarios.

## Limitations

- Computational efficiency gains rely heavily on lightweight predictor accuracy, which is not thoroughly validated
- The claimed benefits of joint optimization over sequential approaches need more empirical ablation studies
- InstaTune-based super-network generation may not capture all architectural variations needed for certain tasks

## Confidence

- **High confidence**: The experimental methodology and evaluation framework are sound, with clear baselines and metrics
- **Medium confidence**: The claimed efficiency improvements are supported by results, though the predictor-based approach needs more rigorous validation
- **Low confidence**: The assertion that joint optimization consistently outperforms sequential approaches across all scenarios lacks sufficient ablation studies

## Next Checks

1. Conduct ablation studies comparing joint vs. sequential optimization to isolate the contribution of simultaneous search
2. Evaluate predictor accuracy and its correlation with final sub-network performance across different search space sizes
3. Test the approach on additional model architectures and tasks to assess generalizability beyond the current experimental scope