---
ver: rpa2
title: 'Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality
  and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image
  Diffusion'
arxiv_id: '2307.05564'
source_url: https://arxiv.org/abs/2307.05564
tags:
- image
- text
- clip
- images
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address Visual Word Sense Disambiguation (VWSD) by augmenting
  CLIP with additional context via key-to-text generation and exploring cross-lingual
  text embeddings, and by generating multiple candidate images via Stable Diffusion
  sampling. Augmenting CLIP with key-to-text slightly improves performance, but cross-lingual
  embeddings help when ensembled with Base-CLIP.
---

# Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion

## Quick Facts
- arXiv ID: 2307.05564
- Source URL: https://arxiv.org/abs/2307.05564
- Reference count: 5
- Best ensemble achieves 63.93% hit rate on test data

## Executive Summary
This paper addresses Visual Word Sense Disambiguation (VWSD) by augmenting CLIP with additional context via key-to-text generation and exploring cross-lingual text embeddings, and by generating multiple candidate images via Stable Diffusion sampling. The authors find that while individual augmentations don't outperform Base-CLIP, ensembling multiple approaches achieves superior results. Their best ensemble of Base-CLIP, Chinese translation, and key-to-text achieves 63.93% hit rate on test data, outperforming the baseline.

## Method Summary
The approach uses zero-shot methods with CLIP and Stable Diffusion to tackle VWSD. Augment-CLIP augments text prompts with additional context via key-to-text systems (three variants) and explores CLIP models in other languages (Chinese). SD Sampling generates 50 images from Stable Diffusion for each phrase and matches candidate images to these outputs using CLIP similarity. Models are ensembled by averaging probabilities from underlying models. The pipeline processes full phrases, generates augmented versions or multiple images, computes CLIP embeddings, matches to candidate images, and produces final predictions through ensemble voting.

## Key Results
- Individual augmentations (key-to-text, translation, SD sampling) don't outperform Base-CLIP alone
- Best ensemble achieves 63.93% hit rate on test data, outperforming baseline
- Cross-lingual translation provides valuable complementarity despite poor individual performance
- Stable Diffusion sampling provides pictorial diversity but struggles with style/lighting mismatches

## Why This Works (Mechanism)

### Mechanism 1: Key-to-text augmentation
Augmenting CLIP with key-to-text completion improves performance by adding compositional context that CLIP's training data lacks. Large language models generate descriptive sentences containing the target phrase, providing compositional context that extends the meaning of the target word beyond its isolated form. The additional context from key-to-text completion meaningfully extends the semantic meaning of the target word rather than diluting it. Break condition: When generated context misinterprets the phrase or adds irrelevant information.

### Mechanism 2: Cross-lingual translation
Cross-lingual translation can resolve ambiguity by mapping an ambiguous word in one language to an unambiguous word in another language. Translating the full phrase to Chinese leverages that language's CLIP model to capture meaning without the ambiguity present in the source language. Some words that are ambiguous in English become unambiguous when translated to Chinese, and the Chinese CLIP model can better capture this unambiguous meaning. Break condition: When translation process is poor or target word remains equally ambiguous in target language.

### Mechanism 3: Stable Diffusion sampling
Generating multiple images via Stable Diffusion provides pictorial diversity that captures different semantic interpretations of the full phrase. Text-to-image diffusion models sample multiple visual representations of the phrase, increasing the probability that at least one matches the intended meaning captured in the gold image. The diversity in Stable Diffusion outputs includes at least one image that correctly represents the target word's meaning in context, and this can be matched to the gold image. Break condition: When outputs consistently misinterpret technical terms or distance metric fails to match any generated image to gold image.

## Foundational Learning

- Concept: Compositionality in language
  - Why needed here: The paper identifies that CLIP struggles with compositionality - understanding how word meanings change when combined with other words (e.g., "baby powder" vs "milk powder").
  - Quick check question: What is the difference between "baby powder" and "milk powder" that demonstrates compositionality?

- Concept: Zero-shot learning
  - Why needed here: The approach uses pre-trained models (CLIP, Stable Diffusion) without fine-tuning on the specific VWSD task, demonstrating zero-shot capabilities.
  - Quick check question: Why is it significant that the models are used without fine-tuning on the training data?

- Concept: Ensemble methods in machine learning
  - Why needed here: The paper shows that individual augmentations don't outperform Base-CLIP alone, but ensembling multiple approaches (Base-CLIP + Chinese translation + key-to-text) achieves the best results.
  - Quick check question: How does ensembling different models help when individual models have complementary strengths and weaknesses?

## Architecture Onboarding

- Component map: Text prompt → Augmentation (key-to-text or translation) → CLIP embedding → Image matching → Ensemble scoring → Final prediction
- Critical path: Text prompt → Augmentation → CLIP embedding → Image matching → Ensemble scoring → Final prediction
- Design tradeoffs: Augmenting prompts adds context but risks introducing noise; cross-lingual approach leverages different language models but depends on translation quality; Stable Diffusion sampling provides diversity but requires effective distance metrics; ensemble methods improve robustness but increase computational complexity
- Failure signatures: Key-to-text generates irrelevant or misleading context; translation produces incorrect or non-equivalent phrases; Stable Diffusion outputs are stylistically mismatched with candidate images; ensemble averaging dilutes strong individual predictions
- First 3 experiments: 1) Test Base-CLIP with simple phrase matching to establish baseline performance; 2) Implement key-to-text augmentation with one LLM and evaluate improvement over baseline; 3) Add cross-lingual translation to the pipeline and measure ensemble performance against standalone models

## Open Questions the Paper Calls Out

### Open Question 1
How do different Base-CLIP model architectures affect performance on the Visual Word Sense Disambiguation task? The paper notes that ViT-B/32 outperformed the larger ViT-L/14 on both trial and test data, which is unexpected given the larger model's increased training and data. The paper does not investigate the reasons behind this performance discrepancy between the two CLIP model architectures. A detailed analysis comparing the performance of various Base-CLIP model architectures on the task, including a breakdown of their strengths and weaknesses, would help identify the factors contributing to their performance differences.

### Open Question 2
How can the quality of key-to-text systems be improved to better augment Base-CLIP for the Visual Word Sense Disambiguation task? The paper mentions that Augment-CLIP through key-to-text can improve performance when the additional context correctly extends the meaning of the target word, but it does not explore ways to enhance the quality of the key-to-text systems. The paper focuses on demonstrating the potential benefits of key-to-text augmentation rather than optimizing the key-to-text systems themselves. An evaluation of different key-to-text systems, along with techniques to improve their output quality, would provide insights into how to enhance the performance of Augment-CLIP through key-to-text.

### Open Question 3
How can the Stable Diffusion Sampling system be improved to better match the generated images with the gold images in the Visual Word Sense Disambiguation task? The paper discusses the pictorial diversity provided by the Stable Diffusion Sampling system but notes that the distance metric used to match generated images with gold images often fails due to style, lighting, or material coincidences. The paper does not explore alternative distance metrics or techniques to improve the matching process between generated and gold images. An investigation into different distance metrics and matching techniques, along with their impact on the performance of the Stable Diffusion Sampling system, would help identify potential improvements for better alignment between generated and gold images.

## Limitations

- Key-to-text augmentation shows only marginal improvements over Base-CLIP, with most variants performing worse than baseline
- Cross-lingual translation shows mixed results and frequently suffers from poor translation quality
- Stable Diffusion sampling struggles to match gold images due to style and lighting mismatches with technical/scientific domain

## Confidence

**High Confidence**: The ensemble approach combining Base-CLIP, Chinese translation, and key-to-text achieving 63.93% hit rate is well-supported by experimental results.

**Medium Confidence**: The observation that individual augmentations don't outperform Base-CLIP alone but show complementarity when ensembled is supported, though could use more analysis on why specific combinations work better.

**Low Confidence**: The specific mechanisms by which key-to-text generation improves compositionality capture and how cross-lingual translation resolves ambiguity are asserted but lack strong empirical validation. The effectiveness of Stable Diffusion sampling is limited by domain mismatch issues that aren't fully addressed.

## Next Checks

1. **Translation Quality Analysis**: Implement round-trip translation verification for the Chinese augmentation pipeline to quantify translation accuracy and its correlation with performance improvements.

2. **Domain Adaptation for SD Sampling**: Test Stable Diffusion sampling on a subset of technical/scientific phrases with manually curated prompts that emphasize style and lighting consistency with gold images.

3. **Ablation Study on Ensemble Components**: Systematically remove each component (Base-CLIP, translation, key-to-text) from the best ensemble to quantify the marginal contribution of each augmentation method.