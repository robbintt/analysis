---
ver: rpa2
title: 'Once Upon a $\textit{Time}$ in $\textit{Graph}$: Relative-Time Pretraining
  for Complex Temporal Reasoning'
arxiv_id: '2310.14709'
source_url: https://arxiv.org/abs/2310.14709
tags:
- time
- temporal
- question
- graph
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of modeling complex temporal
  dependencies in language models for time-sensitive question answering. The authors
  propose a novel graph-based approach, REMEMO, which explicitly models relative time
  relations between temporally-scoped sentences.
---

# Once Upon a $\textit{Time}$ in $\textit{Graph}$: Relative-Time Pretraining for Complex Temporal Reasoning

## Quick Facts
- arXiv ID: 2310.14709
- Source URL: https://arxiv.org/abs/2310.14709
- Reference count: 40
- Key outcome: Graph-based REMEMO outperforms T5 on temporal QA tasks requiring complex event-event temporal reasoning

## Executive Summary
This paper addresses the challenge of modeling complex temporal dependencies in language models for time-sensitive question answering. The authors propose REMEMO (Relative Time Modeling), a novel graph-based pre-training approach that explicitly models relative time relations between temporally-scoped sentences. By treating sentences as nodes in a fully-connected digraph where edges represent temporal relations (Earlier, Later, Contemporary), REMEMO captures temporal dependencies beyond simple time-token co-occurrence. The model is pre-trained jointly on language modeling and a novel time relation classification objective, achieving significant improvements over baseline T5 models on seven temporal QA datasets.

## Method Summary
REMEMO pre-trains a T5 encoder jointly with Language Modeling (LM) and Time Relation Classification (TRC) objectives. The process involves extracting time expressions from text using a RoBERTa-base model fine-tuned on TimeBank, normalizing these into comparable time-span tags via a rule-based script, and forming contexts where temporally-scoped sentences are marked with [TIME] tokens. During pre-training, the model learns both general language patterns (LM objective) and temporal relations between sentence pairs (TRC objective). The graph density parameter controls computational efficiency by varying the proportion of temporal relation pairs used in training. After pre-training, the model is fine-tuned on downstream temporal QA datasets.

## Key Results
- REMEMO outperforms baseline T5 on multiple temporal question answering datasets under various settings
- REMEMO shows particular effectiveness on tasks requiring reasoning over complex event-event temporal relations
- The model demonstrates stronger performance on harder questions requiring long-range temporal dependencies
- Joint pre-training with LM and TRC objectives provides consistent benefits across different temporal QA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based modeling of relative time relations enables explicit capture of complex temporal dependencies between events
- Mechanism: By treating temporally-scoped sentences as nodes in a fully-connected digraph, where edges represent temporal relations (Earlier, Later, Contemporary), the model learns structured temporal reasoning beyond simple time-token co-occurrence
- Core assumption: Temporal relations between events can be reliably extracted from explicit time expressions and normalized into comparable time-span tags
- Evidence anchors:
  - [abstract] "Inspired by the graph view, we propose RemeMo (Relative Time Modeling), which explicitly connects all temporally-scoped facts by modeling the time relations between any two sentences."
  - [section 3.2] "Inspired by this graph view, we present Relative-Time Modeling (REME MO): a novel time-aware pre-training framework that exploits relative positions on the time-axis to model complex temporal dependencies."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.37, average citations=0.0. Top related titles: Joint Multi-Facts Reasoning Network For Complex Temporal Question Answering Over Knowledge Graph
- Break condition: If time expressions are ambiguous, implicit, or lack sufficient granularity for reliable normalization into time-span tags

### Mechanism 2
- Claim: Joint pre-training with Language Modeling and Time Relation Classification objectives enables both contextual understanding and temporal reasoning
- Mechanism: The LM objective captures general language patterns while the TRC objective learns to classify temporal relations between sentence pairs, creating a dual representation of both semantic content and temporal structure
- Core assumption: Temporal relation classification can be effectively modeled as an edge classification task on the sentence graph, using concatenated sentence representations
- Evidence anchors:
  - [abstract] "Experimental results show that REME MO outperforms the baseline T5 on multiple temporal question answering datasets under various settings."
  - [section 3.2] "REME MO is pre-trained jointly with the Language Modeling (LM) objective and the proposed TRC objective."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.37, average citations=0.0. Top related titles: Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation
- Break condition: If temporal relations become too complex for the simple three-way classification (Earlier, Later, Contemporary) to capture

### Mechanism 3
- Claim: Graph density control in the TRC objective allows exploration of the trade-off between comprehensive temporal modeling and computational efficiency
- Mechanism: By varying the proportion of temporal relation pairs used in training (from all pairs to sparse subsets), the model can be tuned for different performance-efficiency trade-offs
- Core assumption: Temporal dependencies can be effectively learned even with reduced graph density, as long as key temporal connections are preserved
- Evidence anchors:
  - [section 3.3] "As shown in Figures 1b and 2, given n graph nodes (i.e., temporally-scoped sentences) in the digraph, there would be n(n − 1) ≃ O (n2) directed edges."
  - [section 4.5] "Table 6 shows the ablation results on graph density of the TRC objective ( § 3.3)."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.37, average citations=0.0. Top related titles: Inductive Graph Unlearning
- Break condition: If graph density becomes too sparse, critical temporal dependencies may be missed, degrading reasoning performance

## Foundational Learning

- Concept: Time expression normalization and time-span tag creation
  - Why needed here: The TRC objective requires comparable time representations to determine temporal relations between sentences
  - Quick check question: Given the sentence "The conference was held in June 2023 and concluded on July 15, 2023," what would be the time-span tag after normalization?

- Concept: Graph representation of temporal relations
  - Why needed here: The graph view provides a systematic framework for modeling complex temporal dependencies that go beyond simple time-token associations
  - Quick check question: In a graph with three temporally-scoped sentences A, B, and C, where A is earlier than B and B is earlier than C, what is the temporal relation between A and C?

- Concept: Edge classification in graph neural networks
  - Why needed here: The TRC objective effectively learns to classify temporal relations as edges between sentence nodes, requiring understanding of how graph edge classification works
  - Quick check question: If sentence A has time-span [2023-01-01, 2023-06-01) and sentence B has time-span [2023-06-01, 2023-12-01), what would be their temporal relation classification?

## Architecture Onboarding

- Component map: Time identification model (RoBERTa-base fine-tuned on TimeBank) -> Time normalization script (Filannino 2020) -> T5 encoder for LM and TRC objectives -> MLP classifier for temporal relation prediction -> Pre-training pipeline combining LM and TRC losses

- Critical path: 1. Input document → Sentence tokenization 2. Time identification → Extract time expressions 3. Time normalization → Create time-span tags 4. Context formation → Insert [TIME] tokens 5. Pre-training → Joint LM and TRC objectives 6. Fine-tuning → Downstream temporal QA tasks

- Design tradeoffs:
  - Full graph density (D=1.0) vs. sparse sampling for efficiency
  - Simple three-way classification vs. more granular temporal relations
  - Time-span merging strategy vs. granularity preservation
  - Single-context vs. multi-context fine-tuning approaches

- Failure signatures:
  - Poor performance on datasets requiring "event-event" temporal reasoning
  - Significant performance drop with longer context lengths
  - Inconsistent results across different graph density settings
  - Time normalization failures leading to incorrect temporal relation predictions

- First 3 experiments:
  1. Ablation study: Train REMEMO without the TRC objective to measure its contribution
  2. Graph density sweep: Test different D values (1.0, log n/n, 1/n) on a single dataset
  3. Context length analysis: Evaluate performance with varying context lengths (500, 1000, 1500 chars)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of REMEMO scale with the number of temporal relations modeled in the graph?
- Basis in paper: [explicit] The paper discusses the impact of graph density on model performance, but does not explore the scaling of performance with the number of temporal relations
- Why unresolved: The paper only investigates the effect of graph density by modifying the number of edges in the graph, but does not examine how the performance changes as the number of temporal relations increases
- What evidence would resolve it: Experiments varying the number of temporal relations in the graph and measuring the corresponding performance changes

### Open Question 2
- Question: How does the performance of REMEMO compare to other temporal reasoning methods that do not use a graph-based approach?
- Basis in paper: [inferred] The paper compares REMEMO to the baseline T5 model, but does not explore its performance against other temporal reasoning methods that do not use a graph-based approach
- Why unresolved: The paper focuses on comparing REMEMO to the baseline T5 model and does not provide a comprehensive comparison with other temporal reasoning methods
- What evidence would resolve it: Experiments comparing REMEMO to other temporal reasoning methods that do not use a graph-based approach

### Open Question 3
- Question: How does the performance of REMEMO change when applied to languages other than English?
- Basis in paper: [inferred] The paper does not discuss the application of REMEMO to languages other than English
- Why unresolved: The paper focuses on the English language and does not explore the performance of REMEMO on other languages
- What evidence would resolve it: Experiments applying REMEMO to other languages and measuring the corresponding performance changes

## Limitations

- The quality and granularity of time expression extraction and normalization is critical for TRC objective effectiveness but not thoroughly evaluated
- The graph-based approach's scalability with longer documents and more complex temporal relations remains unclear due to quadratic computational complexity
- Effectiveness of different graph density settings is not fully validated with systematic testing across the full range of options

## Confidence

**High Confidence:** The core hypothesis that explicitly modeling relative time relations between sentences improves temporal reasoning performance is well-supported by the experimental results across seven temporal QA datasets.

**Medium Confidence:** The claim that REMEMO is particularly effective at modeling long-range temporal dependencies is supported by the analysis on question difficulty, but could benefit from more direct evaluation.

**Low Confidence:** The effectiveness of different graph density settings is not fully validated, with only one comparison shown and unclear optimal settings.

## Next Checks

1. **Time Extraction Pipeline Robustness:** Conduct a manual evaluation of the time expression extraction and normalization pipeline on a sample of documents, measuring precision, recall, and identifying common failure modes.

2. **Graph Density Sensitivity Analysis:** Systematically test REMEMO across the full range of graph density settings (D=1.0, log n/n, 1/n, and intermediate values) on multiple temporal QA datasets.

3. **Long-Range Dependency Benchmark:** Create a synthetic benchmark with documents containing events spanning increasingly distant time periods and evaluate REMEMO's ability to correctly answer questions requiring reasoning across these long temporal gaps.