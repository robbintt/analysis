---
ver: rpa2
title: The Expressive Power of Transformers with Chain of Thought
arxiv_id: '2310.07923'
source_url: https://arxiv.org/abs/2310.07923
tags:
- steps
- transformer
- decoding
- intermediate
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the reasoning power of decoder-only transformers
  that can generate intermediate tokens (e.g., chain of thought) before producing
  a final answer. It establishes that such intermediate generation fundamentally extends
  computational power, but the degree of extension depends critically on the number
  of intermediate steps.
---

# The Expressive Power of Transformers with Chain of Thought

## Quick Facts
- arXiv ID: 2310.07923
- Source URL: https://arxiv.org/abs/2310.07923
- Reference count: 12
- Key outcome: Transformers with O(log n) intermediate steps remain in log-space (L), O(n) steps enable regular language recognition, and polynomial steps achieve P-completeness

## Executive Summary
This paper establishes a precise relationship between the number of intermediate steps a decoder-only transformer can generate (such as in chain-of-thought reasoning) and its computational power. The authors prove that logarithmic intermediate steps keep transformers within log-space, linear steps enable recognition of all regular languages, and polynomial steps achieve the full power of polynomial-time computation. The proofs introduce a novel "layer-norm hash" technique that enables Turing machine simulations in decoder-only transformers without external memory, providing new insights into how transformers can leverage intermediate reasoning steps to expand their computational capabilities.

## Method Summary
The paper uses formal language theory and computational complexity to analyze transformers with intermediate generation capabilities. The authors prove upper and lower bounds by constructing theoretical models that simulate automata and Turing machines using the transformer's attention mechanism. The key innovation is the layer-norm hash technique, which enables key-value retrieval across different transformer columns without external memory by leveraging properties of layer normalization. The analysis assumes saturated attention and log-precision arithmetic, and relates transformer capabilities with t(n) intermediate steps to complexity classes like TIME(t(n)), SPACE(t(n)), and language families including regular, context-free, and context-sensitive languages.

## Key Results
- Transformers with O(log n) intermediate steps remain within log-space (L), unable to solve NL- or P-complete problems
- Transformers with O(n) intermediate steps can recognize all regular languages and are equivalent to context-sensitive languages
- Transformers with polynomially many intermediate steps achieve P-completeness, precisely matching the power of polynomial-time computation

## Why This Works (Mechanism)

### Mechanism 1: Layer-norm hash for cross-column retrieval
- Claim: Layer-norm hash enables cross-column retrieval in transformers without external memory
- Mechanism: Uses layer normalization to project scalar values into unit vectors where inner product equals 1 iff the underlying scalars are equal
- Core assumption: The transformer uses log-precision arithmetic and saturated attention
- Evidence anchors:
  - [abstract]: "The proofs introduce a 'layer-norm hash' technique for implementing Turing machine simulations in decoder-only transformers without external memory"
  - [section]: "The layer-norm hash helps by representing qi/i and kj/j such that hard attention retrieves the value at j where qi = kj"
  - [corpus]: Found 25 related papers, none directly address the layer-norm hash mechanism specifically
- Break condition: If layer normalization is removed or if attention patterns cannot achieve hard attention

### Mechanism 2: Polynomial steps for P-completeness
- Claim: Polynomial intermediate steps make transformers equivalent to P
- Mechanism: Simulates t(n) Turing machine steps using t(n) intermediate decoding steps via layer-norm hash for tape management
- Core assumption: Standard complexity conjectures (P ≠ L, P ≠ NL, etc.)
- Evidence anchors:
  - [abstract]: "transformers with polynomially many intermediate steps are precisely equivalent to the class P"
  - [section]: "TIME(t(n)) ⊆ CoT(t(n)) ⊆ SPACE(t(n) + log n) ⊆ ˜TIME(t(n)2 + n2)"
  - [corpus]: Weak evidence - corpus contains related work on transformer expressiveness but not specific to polynomial step equivalence
- Break condition: If the layer-norm hash cannot scale to polynomially many steps or if precision requirements exceed log-precision

### Mechanism 3: Linear steps for regular language recognition
- Claim: Linear intermediate steps enable recognition of all regular languages
- Mechanism: Simulates finite automata by using one decoding step per automaton transition
- Core assumption: Start-separable positional encodings
- Evidence anchors:
  - [abstract]: "linear number of decoding steps, assuming projected pre-norm (a slight generalization of standard pre-norm), adds a clear new ability (under standard complexity conjectures): recognizing all regular languages"
  - [section]: "With a linear number of intermediate steps, we show that transformers are precisely equivalent to the class P"
  - [corpus]: Weak evidence - related work exists on transformer expressiveness but not specific to regular language recognition via linear steps
- Break condition: If the automaton cannot be simulated in exactly linear steps or if positional encodings are not start-separable

## Foundational Learning

- Concept: Layer normalization and its properties
  - Why needed here: Core to the layer-norm hash mechanism that enables cross-column retrieval
  - Quick check question: How does layer normalization transform a vector and why does this create useful properties for the hash?

- Concept: Complexity classes (P, L, NL, TC0, context-sensitive languages)
  - Why needed here: Framework for understanding what transformers can and cannot compute
  - Quick check question: What is the relationship between L and context-sensitive languages and why does this matter for linear steps?

- Concept: Finite automata and Turing machines
  - Why needed here: Lower bounds rely on simulating these models with intermediate steps
  - Quick check question: How many steps does a Turing machine need to recognize a context-free language versus a regular language?

## Architecture Onboarding

- Component map: Input tokens -> embeddings -> multi-head self-attention -> activation blocks -> output generation -> intermediate steps -> final answer
- Critical path: Input tokens → embeddings → multi-head self-attention → activation blocks → output generation → intermediate steps → final answer
- Design tradeoffs: More intermediate steps increase computational power but also increase inference time and memory requirements
- Failure signatures: Inability to recognize regular languages with logarithmic steps, context-sensitive language upper bound with linear steps
- First 3 experiments:
  1. Test regular language recognition with 1+i positional encodings and verify it works
  2. Measure performance degradation when layer normalization is removed
  3. Test whether transformers with n steps can simulate counter machines (NC1-complete)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can transformers with logarithmic intermediate steps solve any problems that standard transformers without intermediate steps cannot?
- Basis in paper: [explicit] The paper states that transformers with O(log n) intermediate steps remain within log-space (L) and cannot solve NL- or P-complete problems, just like transformers with no intermediate decoding.
- Why unresolved: While the paper shows this limitation, it does not provide examples of problems that are solvable with logarithmic steps but not with standard transformers. The paper focuses more on linear and polynomial steps.
- What evidence would resolve it: A concrete example of a problem that is solvable with logarithmic steps but not with standard transformers would resolve this question.

### Open Question 2
- Question: How does the layer-norm hash technique compare to other methods for implementing Turing machine simulations in decoder-only transformers?
- Basis in paper: [explicit] The paper introduces the layer-norm hash as a key technique for simulating Turing machines in decoder-only transformers without external memory or extra positional encodings.
- Why unresolved: The paper does not compare the layer-norm hash technique to other methods, such as those requiring external memory or encoder-decoder models. A comparative analysis is missing.
- What evidence would resolve it: A comparative study of different techniques for simulating Turing machines in decoder-only transformers would provide evidence to resolve this question.

### Open Question 3
- Question: Can transformers with polynomial intermediate steps learn to use their chain of thought effectively, given the limitations of the language modeling training objective?
- Basis in paper: [explicit] The paper mentions that the language modeling training objective does not allow the model to condition on its own output, which may make it difficult for transformer LMs to learn how to reliably reason with generated tokens if a chain of thought is added post hoc.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis on whether transformers with polynomial steps can overcome this limitation and learn to use their chain of thought effectively.
- What evidence would resolve it: Empirical studies on the effectiveness of transformers with polynomial steps in utilizing their chain of thought, possibly with different fine-tuning methods, would resolve this question.

## Limitations
- Results depend heavily on theoretical complexity conjectures rather than empirical validation
- Layer-norm hash technique may face practical implementation challenges due to numerical precision and achieving saturated attention
- Focus on formal language recognition may limit direct applicability to real-world reasoning tasks

## Confidence
- High confidence: Upper bound results (transformers with t(n) steps cannot exceed SPACE(t(n) + log n))
- Medium confidence: Lower bound results (transformers with polynomial steps can achieve P)
- Medium confidence: Linear step results (regular language recognition)
- Medium confidence: Logarithmic step results (L-completeness)

## Next Checks
1. Implement the layer-norm hash mechanism in a standard transformer architecture and test its ability to perform cross-column retrieval on synthetic tasks that require exact matching
2. Analyze the numerical stability of the layer-norm hash under finite precision arithmetic and determine the minimum precision requirements for reliable operation across polynomially many steps
3. Design a benchmark suite of reasoning problems that map to the theoretical language classes (regular, context-free, context-sensitive) and test whether transformer models with varying numbers of intermediate steps can solve them according to the predicted complexity bounds