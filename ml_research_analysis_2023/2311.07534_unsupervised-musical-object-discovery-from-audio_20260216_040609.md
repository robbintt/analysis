---
ver: rpa2
title: Unsupervised Musical Object Discovery from Audio
arxiv_id: '2311.07534'
source_url: https://arxiv.org/abs/2311.07534
tags:
- note
- musicslots
- learning
- music
- unsupervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MusicSlots, an unsupervised object-centric
  learning method for decomposing polyphonic audio spectrograms into constituent note
  spectrograms. The key insight is that standard softmax normalization of alpha masks
  used in visual object-centric models is ill-suited for audio due to lack of occlusion
  and opacity.
---

# Unsupervised Musical Object Discovery from Audio

## Quick Facts
- arXiv ID: 2311.07534
- Source URL: https://arxiv.org/abs/2311.07534
- Authors: 
- Reference count: 40
- Primary result: MusicSlots achieves note MSE 13.47 and mIoU 0.91 on Bach Chorales for unsupervised note discovery, outperforming established baselines on supervised note property prediction tasks.

## Executive Summary
This paper introduces MusicSlots, an unsupervised object-centric learning method for decomposing polyphonic audio spectrograms into constituent note spectrograms. The key insight is that standard softmax normalization of alpha masks used in visual object-centric models is ill-suited for audio due to lack of occlusion and opacity. MusicSlots adapts SlotAttention to audio by removing softmax normalization from alpha masks. The authors introduce a multi-object music dataset and show that MusicSlots achieves good performance on unsupervised note discovery and outperforms several established baselines on supervised note property prediction tasks, demonstrating better generalization to unseen note combinations.

## Method Summary
MusicSlots is an unsupervised object-centric learning method that decomposes polyphonic audio spectrograms into constituent note spectrograms using a modified SlotAttention architecture. The model takes mel-spectrograms as input and uses a CNN encoder to extract features, which are then processed by a SlotAttention module that iteratively updates slot representations. Each slot independently generates a note spectrogram and alpha mask through a broadcast decoder. The key innovation is removing softmax normalization from alpha masks, allowing multiple notes to contribute additively to the same spectral location. The model is trained using MSE loss and evaluated on both unsupervised note discovery metrics (note MSE, mIoU) and supervised note property prediction (classification accuracy for MIDI pitch values and instrument types).

## Key Results
- MusicSlots achieves note MSE 13.47 and mIoU 0.91 on Bach Chorales for unsupervised note discovery
- Outperforms LCM and TR baselines on supervised note property prediction tasks
- Demonstrates better generalization to unseen note combinations compared to supervised CNN baselines
- Training on multi-instrument datasets improves decomposition quality compared to single-instrument training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing softmax normalization from alpha masks enables MusicSlots to model additive auditory compositions rather than enforcing exclusive object ownership.
- Mechanism: In audio, overlapping frequencies from different notes combine additively. Softmax normalization forces each pixel/spectral bin to belong to exactly one slot, which contradicts the physical reality of audio mixtures. By replacing softmax with sigmoid or no normalization, the model can assign fractional contributions from multiple notes to the same spectral location.
- Core assumption: Auditory objects do not occlude each other; their power spectra combine additively rather than one masking another.
- Evidence anchors:
  - [abstract] "Since concepts of opacity and occlusion in vision have no auditory analogues, the softmax normalization of alpha masks in the decoders of visual object-centric models is not well-suited for decomposing audio objects."
  - [section 2] "For auditory objects, as illustrated in Figure 1 notions of occlusion and opacity are invalid which means that its feasible for one or more notes (slots) to contribute to the power at a spatial location (frequency bin and time) in the spectrogram."
- Break condition: If the model is applied to audio data with significant harmonic interference where notes create non-linear interactions beyond simple addition.

### Mechanism 2
- Claim: SlotAttention with GRU updates and implicit differentiation enables iterative refinement of slot representations for complex audio scenes.
- Mechanism: The slot attention module iteratively updates slot representations by competing for input features through key-value attention. The GRU provides temporal coherence across iterations, while implicit differentiation allows end-to-end training despite the discrete assignment problem inherent in slot matching.
- Core assumption: Iterative refinement can progressively improve slot-object assignments without explicit supervision.
- Evidence anchors:
  - [section 2] "The slot attention module learns to map a set of N = h · w input features onto a set of K slots using an iterative attention mechanism... We use a recurrent network (specifically a GRU [29, 30]) and residual MLP [19]) to update the slots with weighted values as inputs and slots at t − 1 as hidden states of the RNN."
  - [section 2] "Further, our MusicSlots model also adopts recent improvements to SlotAttention such as implicit differentiation [31]."
- Break condition: When the number of overlapping notes exceeds the number of slots, leading to under-segmentation.

### Mechanism 3
- Claim: Training on multi-instrument datasets improves note discovery by providing richer feature diversity and preventing slot collapse.
- Mechanism: When different instruments play the same note, their spectral characteristics differ due to timbre. This diversity helps slots learn more discriminative features and prevents them from collapsing to represent only pitch without instrument identity.
- Core assumption: Timbral differences between instruments provide useful supervisory signal for slot separation even in unsupervised settings.
- Evidence anchors:
  - [section 4] "Further, we observe that training on multi-instrument chord datasets is beneficial for better decomposition quality (compare with single-instrument in Appendix B)."
  - [section 3] "In the the following paragraphs we describe the details of the pipeline to generate our multi-object music datasets starting with the MIDI tokens of chords and finally getting chord/note-level spectrograms and binary masks."
- Break condition: When instrument timbral differences are too subtle to provide meaningful separation cues.

## Foundational Learning

- Concept: Mel-spectrogram representation
  - Why needed here: Provides a time-frequency representation that captures the harmonic structure of musical notes while reducing dimensionality compared to raw waveforms.
  - Quick check question: Why do we use mel-scale instead of linear frequency scale for musical audio analysis?

- Concept: Additive synthesis and spectral mixing
  - Why needed here: Understanding that audio mixtures combine additively at the spectral level is crucial for grasping why softmax normalization fails.
  - Quick check question: What happens to the power spectrum when two pure tones are played simultaneously?

- Concept: Hungarian algorithm for bipartite matching
  - Why needed here: Used to match predicted note slots with ground truth notes for evaluation when order is arbitrary.
  - Quick check question: How does the Hungarian algorithm find the optimal assignment between two sets of objects?

## Architecture Onboarding

- Component map: Input mel-spectrogram -> CNN Encoder -> Positional Embeddings -> SlotAttention Module -> Broadcast Decoder -> Alpha Composition -> MSE Loss

- Critical path: Input → CNN Encoder → Positional Embeddings → SlotAttention → Broadcast Decoder → Alpha Composition → MSE Loss

- Design tradeoffs:
  - Fixed vs. variable number of slots (fixed at K=7)
  - Stride length in CNN (ablation shows stride (1,2) improves performance)
  - Alpha mask normalization (sigmoid vs. none vs. softmax)
  - Implicit differentiation vs. explicit matching

- Failure signatures:
  - Oversegmentation: One note split across multiple slots
  - Undersegmentation: Multiple notes combined into one slot
  - Background leakage: Silence incorrectly assigned to note slots
  - Frequency bin artifacts: Gaps in overlapping regions (especially with softmax)

- First 3 experiments:
  1. Train with softmax alpha masks vs. sigmoid vs. no alpha masks on JSB-multi to verify the core mechanism claim
  2. Vary the number of slots K from 4 to 10 to find optimal setting for 3-4 note chords
  3. Test single-instrument vs. multi-instrument training to validate the dataset diversity benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of note discovery performance when using MusicSlots without alpha masks on complex polyphonic audio signals with many overlapping notes?
- Basis in paper: [inferred] The paper shows good performance (note MSE 13.47, mIoO 0.91) on Bach Chorales with up to 4 notes, but does not explore performance on more complex audio with many overlapping notes
- Why unresolved: The paper only tests on datasets with up to 4-note chords and does not explore the performance limits on more complex polyphonic audio
- What evidence would resolve it: Systematic evaluation of MusicSlots on audio datasets with increasing numbers of overlapping notes (5+, 10+, 20+ notes) to determine when performance degrades significantly

### Open Question 2
- Question: How does MusicSlots performance compare to supervised music transcription models on the same datasets when both are evaluated on the note property prediction task?
- Basis in paper: [explicit] The paper compares MusicSlots to supervised CNN and baseline autoencoders but does not directly compare to state-of-the-art supervised music transcription models
- Why unresolved: The paper only compares against simpler baseline models, not modern supervised music transcription systems that may use larger models or different architectures
- What evidence would resolve it: Direct comparison of MusicSlots against leading supervised music transcription models (like deep convolutional networks or transformer-based models) on the same note property prediction task

### Open Question 3
- Question: What are the fundamental differences in learned representations between MusicSlots and traditional autoencoder models when both are trained on the same datasets?
- Basis in paper: [explicit] The paper shows MusicSlots outperforms traditional autoencoders on note property prediction but does not analyze the qualitative differences in what each model learns
- Why unresolved: The paper focuses on quantitative performance metrics but does not provide analysis of what specific features or patterns each model learns to represent in its latent space
- What evidence would resolve it: Detailed analysis of the learned latent representations, including visualization of what each model captures in different parts of the latent space and how these relate to musical properties

### Open Question 4
- Question: How robust is MusicSlots to different types of audio degradation such as noise, reverberation, or compression artifacts?
- Basis in paper: [inferred] The paper uses clean synthetic audio datasets but does not test the model's robustness to real-world audio degradation
- Why unresolved: All experiments are conducted on clean synthetic spectrograms without any added noise or degradation, leaving questions about real-world applicability
- What evidence would resolve it: Systematic evaluation of MusicSlots performance on the same tasks with varying levels of added noise, reverberation, or compression artifacts to measure degradation in performance

## Limitations
- Evaluation relies on synthetic datasets generated from MIDI files rather than real-world polyphonic recordings
- Performance metrics measure spectral reconstruction quality but don't directly assess musical perceptibility
- Comparison with baselines is limited to specific datasets and may not generalize across diverse musical styles

## Confidence
- **High**: The core mechanism of removing softmax normalization from alpha masks for audio is well-supported by both theoretical reasoning and empirical ablation studies showing performance degradation with softmax.
- **Medium**: The benefits of multi-instrument training are demonstrated empirically but lack theoretical justification for why timbral diversity improves slot separation in unsupervised settings.
- **Low**: The superiority of MusicSlots over LCM and TR for supervised note property prediction is demonstrated but the differences are relatively modest and may be dataset-specific.

## Next Checks
1. Test MusicSlots on real-world polyphonic recordings (e.g., live performances) rather than synthetic MIDI-derived spectrograms to assess robustness to recording artifacts and performance variations.
2. Conduct perceptual studies with musicians to evaluate whether the unsupervised decompositions capture musically meaningful note separations beyond spectral MSE metrics.
3. Evaluate the model's ability to handle temporal note transitions and polyphony with varying note densities to identify failure modes beyond the 3-4 note chords used in evaluation.