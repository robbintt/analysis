---
ver: rpa2
title: 'Scaling Clinical Trial Matching Using Large Language Models: A Case Study
  in Oncology'
arxiv_id: '2308.02180'
source_url: https://arxiv.org/abs/2308.02180
tags:
- trial
- clinical
- criteria
- matching
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) such as GPT-4 can be applied to structure
  clinical trial eligibility criteria and extract complex matching logic, achieving
  strong performance in histology and biomarker extraction. Out-of-the-box GPT-4 substantially
  outperforms prior baselines like Criteria2Query and strong biomedical entity extraction
  systems, even in the zero-shot setting.
---

# Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology

## Quick Facts
- arXiv ID: 2308.02180
- Source URL: https://arxiv.org/abs/2308.02180
- Authors: [not provided]
- Reference count: 33
- Key outcome: GPT-4 outperforms prior baselines in structuring clinical trial eligibility criteria and achieves competitive end-to-end matching performance against expert systems

## Executive Summary
This paper demonstrates that large language models, particularly GPT-4, can effectively structure complex clinical trial eligibility criteria and extract matching logic for oncology trial matching without extensive domain-specific training. The approach shows strong performance in histology and biomarker extraction, outperforming established baselines like Criteria2Query and biomedical entity extraction systems in zero-shot settings. GPT-4 demonstrates competitiveness with expert systems in end-to-end trial matching when compared to historical enrollment data, suggesting practical utility for triaging patient-trial candidates with appropriate human oversight.

## Method Summary
The study applies GPT-4 to structure clinical trial eligibility criteria from XML files obtained from ClinicalTrials.gov, converting free-text criteria into structured JSON with disjunctive normal form logic. The system processes oncology trials with treatment as the primary purpose and uses oncology experts to manually structure a test set for evaluation. Performance is measured through precision, recall, and F1 scores for entity extraction and complete match logic, with end-to-end matching evaluated against historical enrollment data. The approach compares GPT-4's performance against baselines including Criteria2Query and biomedical entity extraction tools, examining both zero-shot and few-shot (3-shot) performance.

## Key Results
- GPT-4 achieves strong performance in histology and biomarker extraction from trial eligibility criteria
- Out-of-the-box GPT-4 substantially outperforms prior baselines like Criteria2Query even in zero-shot settings
- GPT-4 demonstrates competitive end-to-end trial matching performance, recovering 76.8% of gold patient-trial pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can directly parse and structure complex nested trial eligibility criteria into disjunctive normal form without extensive domain-specific training.
- Mechanism: The model uses its emergent in-context learning ability to map free-text eligibility criteria into structured logical representations (e.g., JSON with AND/OR/NOT logic).
- Core assumption: The prompt format and example demonstrations are sufficient for GPT-4 to understand the semantic parsing task without fine-tuning.
- Evidence anchors:
  - [abstract] "out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT)"
  - [section] "Out of box and with no more than three examples, cutting-edge LLMs, such as GPT-4, can already structure elaborate trial eligibility criteria and extract complex matching logic"
  - [corpus] Weak: No specific mechanism cited in related papers for zero-shot semantic parsing of clinical criteria; mostly empirical performance reports.
- Break condition: If criteria contain ambiguous or context-dependent phrasing that requires deep medical domain knowledge beyond surface patterns, the zero-shot structure may fail.

### Mechanism 2
- Claim: GPT-4 can match structured patient records against trial criteria with competitive recall and precision, outperforming rule-based systems in zero-shot settings.
- Mechanism: GPT-4 reasons over structured patient attributes and trial criteria, applying hierarchical medical ontologies to assess match compatibility.
- Core assumption: The structured patient data is sufficiently normalized and complete to enable accurate matching.
- Evidence anchors:
  - [abstract] "GPT-4 also demonstrates competitiveness in end-to-end trial matching when compared to expert systems"
  - [section] "Out of box, GPT-4 can already performs competitively, recovering 76.8% of gold patient-trial pairs"
  - [corpus] Weak: Related work focuses on end-to-end models or data augmentation but not direct zero-shot matching with LLMs; this appears to be a novel application.
- Break condition: If patient records are incomplete or require integration from scattered free-text notes exceeding LLM context limits.

### Mechanism 3
- Claim: In-context learning with few examples improves GPT-4's accuracy in structuring complex trial criteria and matching logic.
- Mechanism: Demonstrations in the prompt guide GPT-4 to better normalize terminology and handle nested logic forms.
- Core assumption: The model can generalize from the provided examples to unseen trial formats and criteria structures.
- Evidence anchors:
  - [section] "GPT-4 (3-shot) outperforms GPT-4 (zero-shot) in all scenarios, indicating that in this more challenging setting, in-context learning indeed plays a positive role"
  - [section] "In-context learning helped substantially for biomarker extraction, but didn't matter much for histology"
  - [corpus] Weak: Few related papers discuss in-context learning for clinical trial matching; mostly focused on supervised fine-tuning.
- Break condition: If the prompt examples are not representative of the diversity of trial formats, leading to overfitting to specific patterns.

## Foundational Learning

- Concept: Disjunctive Normal Form (DNF) logical expressions
  - Why needed here: Eligibility criteria are normalized into DNF to standardize matching logic for comparison and evaluation.
  - Quick check question: How would you represent "A and (B or C)" in DNF?

- Concept: Hierarchical medical ontologies (e.g., NCI Thesaurus, OncoTree)
  - Why needed here: Used to normalize trial criteria entities and patient attributes to enable accurate matching.
  - Quick check question: Why is it necessary to map both trial and patient terms to the same ontology?

- Concept: In-context learning vs. fine-tuning
  - Why needed here: Understanding when few-shot demonstrations are sufficient vs. when domain-specific fine-tuning is needed.
  - Quick check question: What are the trade-offs between using in-context learning and fine-tuning for a new medical NLP task?

## Architecture Onboarding

- Component map:
  Trial XML Ingestion -> GPT-4 Prompt Engine -> Structured JSON Output (DNF logic)
  Patient Record Ingestion -> NLP Pipeline (entity extraction) -> Structured JSON
  Matching Engine -> Ontology Mapping -> Eligibility Decision
  Evaluation Pipeline -> Gold Label Comparison -> Performance Metrics

- Critical path:
  Trial structuring (GPT-4) -> Ontology normalization -> Patient structuring (NLP pipeline) -> Hierarchical matching -> Eligibility output

- Design tradeoffs:
  - Zero-shot vs. few-shot: Faster deployment vs. better accuracy for complex criteria
  - GPT-4 context vs. patient record length: May need chunking or summarization for long records
  - Rule-based matching vs. LLM matching: Interpretability and auditability vs. flexibility

- Failure signatures:
  - High false negatives in biomarker matching: Likely due to incomplete ontology mapping
  - Inconsistent cohort assignment: Prompt may need more explicit cohort extraction instructions
  - Context limit exceeded errors: Patient records too long for single LLM call

- First 3 experiments:
  1. Compare zero-shot vs. 3-shot GPT-4 performance on a subset of trial criteria with known ground truth
  2. Measure impact of different prompt templates on the consistency of JSON output format
  3. Test hierarchical matching accuracy when trial criteria use broader ontology terms than patient data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GPT-4's performance in structuring clinical trial eligibility criteria compare to the expert system when evaluated on a broader range of diseases beyond oncology?
- Basis in paper: [explicit] The paper focuses on oncology trials and mentions the expert system is "specifically tailored for oncology."
- Why unresolved: The study is limited to oncology trials, so generalizability to other disease areas is untested.
- What evidence would resolve it: Evaluating GPT-4's structuring performance on eligibility criteria from non-oncology trials and comparing against the expert system's performance in those domains.

### Open Question 2
- Question: What is the optimal balance between context length and accuracy when structuring patient information from longitudinal medical records for clinical trial matching?
- Basis in paper: [inferred] The paper notes that "naively concatenating all potentially relevant notes will almost always exceed even the largest context size available for GPT-4: 32K tokens."
- Why unresolved: The paper uses structured patient information from state-of-the-art extraction systems rather than directly processing raw medical records with GPT-4.
- What evidence would resolve it: Systematic evaluation of GPT-4's structuring accuracy using different context lengths and sampling strategies on raw patient medical records.

### Open Question 3
- Question: How can in-context learning be optimized for complex nested logical expressions in clinical trial eligibility criteria?
- Basis in paper: [explicit] The paper notes that "in-context learning helped substantially for biomarker extraction, but didn't matter much for histology" and shows improved performance with 3-shot examples for complex DNF evaluation.
- Why unresolved: The paper only tests up to 3-shot examples and doesn't explore more sophisticated prompt engineering techniques.
- What evidence would resolve it: Comparative evaluation of GPT-4's performance with varying numbers of examples and different prompt structures for increasingly complex logical expressions.

## Limitations

- The evaluation methodology relies on historical enrollment data as a proxy for correct matching, which may not capture the full complexity of clinical decision-making
- GPT-4's decision-making process is opaque, making it difficult to audit and validate clinical trial matching recommendations
- The study focuses exclusively on oncology trials, limiting generalizability to other disease areas

## Confidence

- High Confidence: The entity extraction performance for histology and biomarkers, with F1 scores exceeding 90% in many cases. This is supported by direct comparison to established biomedical entity extraction tools and clear evaluation metrics.
- Medium Confidence: The end-to-end trial matching performance, particularly the 76.8% recall against historical enrollment data. While this demonstrates competitiveness with expert systems, the gold standard itself may have limitations.
- Low Confidence: The generalizability of few-shot learning across the full diversity of oncology trial formats. The paper shows improvement with examples, but doesn't fully characterize the space of trial structures where this approach may fail.

## Next Checks

1. **Context Window Stress Test**: Evaluate GPT-4's performance on patient records that require multiple context windows, measuring accuracy degradation when records must be split and processed in segments.

2. **Rare Biomarker Coverage**: Create a test suite of trials with rare or novel biomarkers not well-represented in training data, measuring GPT-4's ability to correctly identify and structure these criteria.

3. **Clinical Outcome Validation**: Partner with oncology centers to validate GPT-4's trial matching recommendations against actual patient enrollment outcomes over a 6-month period, moving beyond historical data to prospective validation.