---
ver: rpa2
title: Framework for Question-Answering in Sanskrit through Automated Construction
  of Knowledge Graphs
arxiv_id: '2310.07848'
source_url: https://arxiv.org/abs/2310.07848
tags:
- 'false'
- loka
- word
- words
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a framework for building knowledge graphs\
  \ (KGs) from Sanskrit texts and using them for factoid question answering in Sanskrit.\
  \ The authors implement the system on human relationships from the epics Mahabharata\
  \ and Ramayana, and synonymous relationships from the technical Ayurveda text Bh\u0101\
  vaprak\u0101\u015Ba nigha\u1E47\u1E6Du."
---

# Framework for Question-Answering in Sanskrit through Automated Construction of Knowledge Graphs

## Quick Facts
- arXiv ID: 2310.07848
- Source URL: https://arxiv.org/abs/2310.07848
- Reference count: 3
- System correctly answers ~50% of factoid questions from Sanskrit epics

## Executive Summary
This paper presents a framework for building knowledge graphs (KGs) from Sanskrit texts and using them for factoid question answering in Sanskrit. The authors implement the system on human relationships from the epics Mahabharata and Ramayana, and synonymous relationships from the technical Ayurveda text Bhāvaprakāśa nighaṇṭu. They extract triplets from the text using morphological analysis and grammar rules, then store them in an RDF knowledge graph. For question answering, they parse queries into triplets, enhance them with inverse and derived relationships, and execute SPARQL queries against the KG. Experiments show the system correctly answers about 50% of factoid questions from the epics, with analysis of failure modes. The work demonstrates the feasibility of automated KG construction and QA for Sanskrit, despite challenges from the language's morphological richness and lack of NLP tools.

## Method Summary
The framework extracts triplets from Sanskrit texts using morphological analysis and grammar rules, stores them in an RDF knowledge graph, and answers questions by parsing queries into triplets and executing SPARQL queries. The system handles inverse and derived relationships through rule-based mappings and decomposition. It was tested on human relationships from Mahabharata and Ramayana, and synonymous relationships from Bhāvaprakāśa nighaṇṭu.

## Key Results
- Correctly answers approximately 50% of factoid questions from Sanskrit epics
- Demonstrates automated KG construction from Sanskrit texts is feasible
- Identifies error sources including morphological analysis errors, text errors, and query errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic triplets extracted from Sanskrit texts can be stored as RDF triples and queried via SPARQL
- Mechanism: Text preprocessing splits compound words, morphological analysis identifies grammatical case/number/gender, relationship words are mapped to predicates, and noun phrases in genitive case become subjects with matching nominative phrases as objects
- Core assumption: Genitive case reliably indicates the "subject" of a relationship and nominative case reliably indicates the "object"
- Evidence anchors:
  - [abstract] "extract triplets from the text using morphological analysis and grammar rules"
  - [section 3.3] "a noun that exhibits the same case, number and gender as the predicate word"
  - [corpus] Weak - no direct neighbor corpus showing this triplet extraction pattern works reliably in Sanskrit
- Break condition: If genitive case does not consistently encode the subject role, or if objects are not reliably in matching nominative case

### Mechanism 2
- Claim: Inverse relationships (e.g., "has-son" vs "has-father") can be added to the KG to answer reversed queries
- Mechanism: A mapping table stores inverse relationships; when a base triplet is added, its inverse is generated and inserted
- Core assumption: The inverse mapping is one-to-one and context-independent; gender and number information is sufficient to disambiguate
- Evidence anchors:
  - [section 2.3] "we maintain a map of such inverse relationship rules"
  - [section 3.4] "we use the gender information of the subject and the object to disambiguate"
  - [corpus] Weak - corpus shows only general inverse relationship concepts, no explicit Sanskrit-specific evidence
- Break condition: If inverse mapping is not bijective or gender/number alone cannot disambiguate

### Mechanism 3
- Claim: Complex relationships can be decomposed into chains of base relationships to enable multi-hop queries
- Mechanism: A rule set defines how to split composite predicates into component base predicates, generating intermediate nodes and new triplets
- Core assumption: The decomposition rules are complete and correct for the domain; SPARQL path queries are not needed
- Evidence anchors:
  - [section 3.4] "derived relations could be broken into their component base parts"
  - [section 4.2] "each complex relation is broken into its constituent parts"
  - [corpus] Weak - corpus lists knowledge graph composition but no explicit decomposition rules for Sanskrit
- Break condition: If decomposition rules miss valid combinations or introduce false paths

## Foundational Learning

- Concept: Sanskrit morphological analysis (prātipadika, dhātu, vibhakti, liṅga, vacana)
  - Why needed here: To split sandhi, identify word roots, and determine grammatical roles for triplet extraction
  - Quick check question: Given "अजुर्न्य पुत्रः अभिमन्युः", what is the prātipadika and vibhakti of "अजुर्न्य"?
- Concept: Knowledge graph structure (nodes, edges, RDF, SPARQL)
  - Why needed here: To store extracted relationships and query them efficiently
  - Quick check question: How would you represent "arjuna has-son abhimanyu" in RDF triple format?
- Concept: Rule-based NLP vs. statistical/ML approaches
  - Why needed here: The system relies entirely on grammar rules; understanding limitations is critical
  - Quick check question: What failure modes would a rule-based system have that an ML model might mitigate?

## Architecture Onboarding

- Component map: Text preprocessing -> Sandhi splitter -> Morphological analyzer -> Relationship identifier -> Triplet extractor -> KG builder -> Query parser -> Query enhancer -> SPARQL executor
- Critical path: Text preprocessing -> Triplet extraction -> KG storage -> Query parsing -> Query execution
- Design tradeoffs:
  - Rule-based extraction is deterministic but brittle; no error recovery
  - Fixed context window (3 śloka) balances recall vs. precision
  - Manual synonym lists avoid ambiguity but require maintenance
- Failure signatures:
  - Incorrect morphological analysis -> wrong grammatical tags -> wrong triplet formation
  - Missing inverse/derived relationship mapping -> unanswerable queries
  - Oversplit sandhi -> wrong word boundaries -> lost semantic units
- First 3 experiments:
  1. Run sandhi splitter and morphological analyzer on a small corpus; manually verify output correctness
  2. Feed preprocessed text into triplet extractor; compare extracted triplets to ground truth for known sentences
  3. Query a small KG with both direct and inverse relationships; verify answer accuracy for simple questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of word analysis in Sanskrit be improved to reduce errors in triplet extraction?
- Basis in paper: [explicit] The paper mentions that "AnalysisError" is a significant source of errors, where words are analyzed incorrectly by tools like The Sanskrit Heritage Parser
- Why unresolved: Current NLP tools for Sanskrit are not as advanced as those for English, leading to frequent analysis errors
- What evidence would resolve it: Improved NLP tools or dictionaries that can correctly analyze words in various contexts would reduce these errors

### Open Question 2
- Question: Can the framework be extended to handle more complex relationships, such as recursive relationships like "has-ancestor"?
- Basis in paper: [explicit] The paper notes that recursive relationships like "has-ancestor" and "has-descendant" are not handled in the current work
- Why unresolved: The current framework does not support querying relationships with arbitrary depth
- What evidence would resolve it: Implementing algorithms to handle recursive queries and testing them on a dataset with such relationships would demonstrate the feasibility

### Open Question 3
- Question: How can the system be improved to better handle synonyms and pronouns in the text?
- Basis in paper: [explicit] The paper highlights errors related to synonyms and pronouns, such as "TextError" and issues with connecting pronouns to their subjects
- Why unresolved: The current system lacks mechanisms to accurately identify and connect synonyms and pronouns
- What evidence would resolve it: Developing and integrating methods to recognize synonyms and pronouns, and testing their effectiveness on texts with these elements would improve the system

## Limitations

- Reliance on fixed grammatical patterns (genitive for subject, nominative for object) may not generalize across all Sanskrit text types
- Performance drops significantly for complex queries (50% overall, lower for multi-hop)
- Inverse and derived relationship mappings are hand-crafted without systematic validation

## Confidence

- **High Confidence**: The general approach of using morphological analysis and grammar rules for triplet extraction is well-established
- **Medium Confidence**: The 50% accuracy figure is reported but lacks detailed breakdown by query type
- **Low Confidence**: The completeness and correctness of inverse/derived relationship mappings cannot be verified from the paper alone

## Next Checks

1. Test the sandhi splitter and morphological analyzer on a held-out Sanskrit corpus with manually verified outputs to establish baseline accuracy
2. Evaluate the triplet extraction pipeline on sentences with known relationships to measure precision and recall
3. Conduct error analysis on failed questions to categorize failure modes (morphological errors, missing relationships, incorrect case assignment)