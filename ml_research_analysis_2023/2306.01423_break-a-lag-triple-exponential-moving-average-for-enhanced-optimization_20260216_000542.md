---
ver: rpa2
title: 'Break a Lag: Triple Exponential Moving Average for Enhanced Optimization'
arxiv_id: '2306.01423'
source_url: https://arxiv.org/abs/2306.01423
tags:
- fame
- adam
- tema
- optimizers
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Fast-Adaptive Moment Estimation (FAME), a novel
  optimizer that uses Triple Exponential Moving Average (TEMA) to improve tracking
  of gradient trends and reduce lag in adaptive optimization methods. The core idea
  is to recursively correct for lag in gradient moment estimates by combining first-,
  second-, and third-order EMA terms, as opposed to the standard EMA used in optimizers
  like Adam.
---

# Break a Lag: Triple Exponential Moving Average for Enhanced Optimization

## Quick Facts
- arXiv ID: 2306.01423
- Source URL: https://arxiv.org/abs/2306.01423
- Reference count: 22
- Key outcome: FAME improves accuracy by 1.6-3.5% on CIFAR-100 and achieves 56.9% mAP@0.5 on MS-COCO compared to AdamW

## Executive Summary
This paper introduces Fast-Adaptive Moment Estimation (FAME), a novel optimizer that uses Triple Exponential Moving Average (TEMA) to improve tracking of gradient trends and reduce lag in adaptive optimization methods. FAME replaces the standard EMA used in optimizers like Adam with a recursive lag-correction mechanism combining first-, second-, and third-order EMA terms. The method was extensively evaluated across five public benchmarks, 14 architectures, and three computer vision tasks, consistently outperforming leading optimizers with modest computational overhead.

## Method Summary
FAME is based on the Triple Exponential Moving Average (TEMA) technique, which recursively corrects for lag in gradient moment estimates by combining three EMA terms as 3EMA1 - 3EMA2 + EMA3. The optimizer maintains first-, second-, and third-order EMA estimates for both gradient and squared gradient moments, using these to compute adaptive learning rates. FAME was tested with fixed hyperparameters (α=0.001, β1=0.9, β2=0.999, β3=0.3, β4=0.5, β5=0.8) across CIFAR-10/100, PASCAL-VOC, MS-COCO, and Cityscapes datasets with architectures including ResNet, MobileNet, EfficientNet, and YOLOv5.

## Key Results
- Achieved 56.9% mAP@0.5 on MS-COCO vs 44.6% for AdamW
- Improved CIFAR-100 accuracy by 1.6-3.5% over AdamW, AdaBound, and AdaHessian
- Demonstrated superior robustness with smoother convergence curves and lower sensitivity to initialization
- Modest computational overhead: 2× memory and 5% additional time compared to Adam

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TEMA reduces lag in gradient moment estimation by recursively correcting for past lag terms.
- Mechanism: TEMA applies EMA three times (EMA1, EMA2, EMA3) and combines them as 3EMA1 - 3EMA2 + EMA3 to cancel out lag while preserving smoothness.
- Core assumption: Higher-order EMA corrections can effectively estimate and remove lag without excessive noise amplification.
- Evidence anchors: [abstract] "Triple Exponential Moving Average (TEMA) to improve tracking of gradient trends and reduce lag in adaptive optimization methods"
- Break condition: If the gradient sequence is too noisy, higher-order terms may amplify noise rather than reduce lag, degrading performance.

### Mechanism 2
- Claim: FAME achieves faster convergence and better accuracy by using TEMA for both first and second moment estimates.
- Mechanism: Standard optimizers use EMA for both moments; FAME replaces EMA with TEMA for both, reducing bias in moment estimates.
- Core assumption: TEMA provides better moment estimates than EMA, leading to improved parameter updates.
- Evidence anchors: [abstract] "FAME was extensively evaluated... It consistently outperformed leading optimizers: for example, on MS-COCO it achieved 56.9% mAP@0.5 (vs. 44.6% for AdamW)"
- Break condition: If the optimization landscape is too flat or the gradients are too small, the benefits of TEMA may be negligible compared to increased computational cost.

### Mechanism 3
- Claim: FAME is more robust to initialization and dataset/task variations compared to other optimizers.
- Mechanism: By reducing lag and providing better moment estimates, FAME maintains consistent performance across different initializations and tasks.
- Core assumption: Reduced lag leads to more stable optimization trajectories, improving robustness.
- Evidence anchors: [abstract] "FAME demonstrated superior robustness, smoother convergence curves, and lower sensitivity to initialization"
- Break condition: If the task requires very specific learning rate schedules or if the model architecture has unique optimization requirements, FAME's general robustness may not be sufficient.

## Foundational Learning

- Concept: Exponential Moving Average (EMA)
  - Why needed here: EMA is the building block for TEMA and is used in most modern optimizers to smooth gradients.
  - Quick check question: How does EMA differ from simple moving average in terms of weight assignment to past data points?

- Concept: Gradient moment estimation
  - Why needed here: Moment estimation is crucial for adaptive learning rates in optimizers like Adam and FAME.
  - Quick check question: What is the difference between first-order and second-order moment estimates in optimization?

- Concept: Trend identification in time series
  - Why needed here: TEMA was originally developed for trend identification in financial data and is adapted here for gradient trend tracking.
  - Quick check question: How does lag in trend identification affect the performance of optimization algorithms?

## Architecture Onboarding

- Component map:
  - TEMA module: Calculates 3EMA1 - 3EMA2 + EMA3 for both first and second moments
  - Moment tracking: Maintains mt, vt, dmt, dvt, tmt, tvt variables
  - Parameter update: Uses FAME moments to compute weight updates

- Critical path:
  1. Initialize moment variables to zero
  2. Compute gradient gt at each step
  3. Update first-order EMA (mt, vt) using β1, β2
  4. Update second-order EMA (dmt, dvt) using β3, β5
  5. Update third-order EMA (tmt, tvt) using β4, β5
  6. Compute FAME moments using TEMA formula
  7. Update parameters using FAME moments

- Design tradeoffs:
  - Higher accuracy and robustness vs. increased memory (2×) and computational time (5%)
  - Better trend tracking vs. potential noise amplification in very noisy gradients
  - General applicability vs. potential suboptimal performance on specific tasks requiring custom optimization

- Failure signatures:
  - If the model fails to converge, check if β parameters are too aggressive (too low values)
  - If training is unstable, verify that gradient clipping is applied before FAME updates
  - If memory usage is critical, consider reducing batch size or using gradient accumulation

- First 3 experiments:
  1. Implement FAME with default hyperparameters (α=0.001, β1=0.9, β2=0.999, β3=0.3, β4=0.5, β5=0.8) and compare convergence curves with Adam on CIFAR-10 using ResNet-18
  2. Test sensitivity to β parameters by varying β3, β4, β5 in [0.2, 0.9] increments and measuring accuracy changes
  3. Evaluate robustness by training the same model with different random initializations and comparing standard deviation of final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FAME's performance scale with increasingly complex architectures, such as large-scale Vision Transformers or multi-modal models?
- Basis in paper: [explicit] The paper states FAME holds "immense potential in generating significantly improved pre-trained models, particularly for computationally intensive models like visual Transformers," but does not provide experimental results on such architectures.
- Why unresolved: The experiments focused on 14 architectures, primarily CNNs and lightweight models, without evaluating very deep or complex architectures like large-scale Transformers.
- What evidence would resolve it: Direct experimental comparison of FAME against leading optimizers on large-scale Vision Transformers (e.g., ViT-L/16, Swin-L) and multi-modal models (e.g., CLIP) across diverse tasks.

### Open Question 2
- Question: What is the optimal order of EMA for different types of gradient dynamics, and how can this be determined adaptively during training?
- Basis in paper: [explicit] The paper discusses that "Higher-order KEMAs sacrifice more smoothness in exchange to more aggressive lag reduction" and that "the appropriate order highly depends on the problem at hand," but uses only TEMA (third-order) empirically.
- Why unresolved: The paper chose TEMA empirically without exploring whether lower or higher orders might be better for specific tasks or gradient characteristics, and no adaptive mechanism for order selection is proposed.
- What evidence would resolve it: Systematic ablation studies across diverse tasks showing performance of EMA, DEMA, TEMA, and higher orders, plus development of an adaptive order selection mechanism based on gradient statistics.

### Open Question 3
- Question: How does FAME's reduced sensitivity to initialization translate to improved generalization on unseen data, and what are the theoretical underpinnings?
- Basis in paper: [explicit] The paper shows FAME is "more robust and less sensitive to the initial points" in simulated data and has "lower standard deviation across different weight initializations," but does not analyze generalization gap or provide theoretical justification.
- Why unresolved: While empirical robustness is demonstrated, the paper does not connect initialization sensitivity to generalization performance or provide theoretical analysis of why TEMA reduces this sensitivity.
- What evidence would resolve it: Theoretical analysis linking TEMA's lag reduction properties to generalization bounds, plus extensive experiments measuring test performance variance across multiple initialization seeds and datasets.

## Limitations
- The paper does not address potential noise amplification when gradients are extremely volatile, which could negate TEMA benefits
- No comparison with recent adaptive optimizers beyond the 2020 timeframe
- Limited discussion of how TEMA performs on extremely deep networks where gradient explosion/vanishing is already a concern

## Confidence
- **High confidence**: TEMA is a well-established technique for reducing lag in time series analysis, and the mathematical formulation (3EMA1 - 3EMA2 + EMA3) is correctly stated and traceable to its financial origins.
- **Medium confidence**: The paper reports extensive testing across multiple benchmarks and architectures, showing consistent improvements over baselines. However, the lack of ablation studies on TEMA vs EMA alone makes it difficult to isolate the specific contribution of TEMA to the observed gains.
- **Low confidence**: While results are impressive on computer vision tasks, there is no evidence of performance on other domains (NLP, reinforcement learning, etc.). The 5% computational overhead and 2× memory increase may be prohibitive for large-scale models.

## Next Checks
1. Implement FAME with the specified beta parameters and test on CIFAR-10 with ResNet-18, comparing convergence curves and final accuracy against Adam with identical initialization
2. Conduct an ablation study replacing TEMA with standard EMA in FAME to quantify the exact contribution of higher-order terms to performance gains
3. Test FAME on a non-vision task (e.g., language modeling on IMDB dataset) to assess cross-domain applicability and identify any domain-specific limitations