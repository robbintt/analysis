---
ver: rpa2
title: Coordinating Distributed Example Orders for Provably Accelerated Training
arxiv_id: '2302.00845'
source_url: https://arxiv.org/abs/2302.00845
tags:
- learning
- random
- d-grab
- distributed
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles distributed training of machine learning models
  with a novel data ordering method. It introduces Coordinated Distributed Gradient
  Balancing (CD-GraB), which extends the original Gradient Balancing (GraB) to distributed
  settings.
---

# Coordinating Distributed Example Orders for Provably Accelerated Training

## Quick Facts
- **arXiv ID**: 2302.00845
- **Source URL**: https://arxiv.org/abs/2302.00845
- **Reference count**: 40
- **One-line result**: Introduces CD-GraB, a distributed data ordering method that achieves linear speedup over centralized GraB with convergence rate O((mnT)^(-2/3)) for smooth non-convex objectives.

## Executive Summary
This paper tackles the challenge of distributed training of machine learning models by introducing Coordinated Distributed Gradient Balancing (CD-GraB). The key innovation is extending the Gradient Balancing (GraB) method to distributed settings through a novel PairBalance subroutine. Unlike previous approaches that rely on stale gradient means, CD-GraB balances gradients through pairwise comparisons without data movement, enabling large learning rates and robust performance. The algorithm achieves provable convergence improvements over distributed random reshuffling, with theoretical guarantees showing O((mnT)^(-2/3)) convergence for smooth non-convex objectives and Õ((mnT)^(-2)) under Polyak-Lojasiewicz conditions.

## Method Summary
CD-GraB implements distributed data ordering through a central server coordinating PairBalance operations across workers. Each worker maintains local data and computes gradients, while the server aggregates and balances these gradients using the PairBalance subroutine. The method avoids the need for stale gradient means by pairing gradients (xi,2k-1 - xi,2k) across workers, making the ordering invariant to global mean shifts. This enables efficient large learning rate usage and maintains bounded herding objectives even in distributed settings. The approach achieves linear speedup over centralized GraB with minimal communication overhead through an all-gather pattern for PairBalance coordination.

## Key Results
- CD-GraB achieves O((mnT)^(-2/3)) convergence rate for smooth non-convex objectives, outperforming distributed random reshuffling
- Under PL conditions, CD-GraB achieves Õ((mnT)^(-2)) convergence rate
- Empirical results show superior performance over naive parallel GraB and distributed random reshuffling on GLUE, CIFAR-10, and WikiText-2 tasks
- The method maintains linear speedup scaling with the number of workers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PairBalance reduces gradient discrepancy without needing stale gradient means, enabling large learning rates in distributed settings.
- **Mechanism**: The algorithm pairs gradients (xi,2k-1 - xi,2k) across workers, making the ordering invariant to global mean shifts.
- **Core assumption**: Gradients within each pair have bounded deviation from their mean and gradients are Lipschitz smooth.
- **Break condition**: If the pairwise gradient differences are large or highly heterogeneous across workers, the balancing bound degrades.

### Mechanism 2
- **Claim**: Distributed ordering via PairBalance yields linear speedup in convergence over centralized GraB.
- **Mechanism**: Workers collaboratively permute their local data such that the global ordering minimizes the parallel herding objective.
- **Core assumption**: Smoothness and bounded heterogeneity hold, and number of examples per worker is even.
- **Break condition**: If the number of workers n is very small or heterogeneity is too large, the linear speedup benefit diminishes.

### Mechanism 3
- **Claim**: The parallel herding bound remains O(1) even with decentralized data, preserving fast convergence.
- **Mechanism**: By pairing gradients locally and using Algorithm 1 on the paired sequence, the herding objective per worker stays bounded.
- **Core assumption**: The pairwise gradient differences remain bounded.
- **Break condition**: If pairwise differences become too large, the bound in Lemma 1 no longer holds.

## Foundational Learning

- **Concept**: Herding and balancing framework
  - **Why needed here**: Provides the theoretical foundation for ordering examples to minimize gradient discrepancy.
  - **Quick check question**: How does balancing reduce the herding objective by a constant factor each round?

- **Concept**: Polyak-Lojasiewicz (PL) condition
  - **Why needed here**: Enables stronger convergence rate O((mnT)^(-2)) for strongly convex-like objectives.
  - **Quick check question**: What is the relationship between the PL constant μ and the convergence rate?

- **Concept**: Smoothness and bounded gradient variance
  - **Why needed here**: Guarantees the gradient updates stay controlled and bounded, critical for the convergence proofs.
  - **Quick check question**: How does the Lipschitz constant L influence the learning rate bound in Theorem 1?

## Architecture Onboarding

- **Component map**: Workers -> Gradient computation -> Server aggregation -> PairBalance -> New permutation broadcast -> Worker update
- **Critical path**: Worker → gradient computation → server aggregation → PairBalance → new permutation broadcast → worker update
- **Design tradeoffs**:
  - All-gather vs. all-reduce: D-GraB uses all-gather for PairBalance, slightly higher communication cost but negligible overhead
  - Blocking vs. non-blocking sorter: Current implementation blocks; could parallelize sorter and SGD for speedup
- **Failure signatures**:
  - Slow convergence: Likely due to poor initial permutation or high gradient heterogeneity
  - Communication bottleneck: PairBalance may become a synchronization point under high latency
- **First 3 experiments**:
  1. Run D-GraB vs D-RR on a small convex task (e.g., logistic regression) and measure training loss curves
  2. Test PairBalance in isolation on zero-centered random vectors to verify herding bound reduction
  3. Vary number of workers (4, 16, 64) and observe speedup scaling on CIFAR-10

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations

- The convergence guarantees rely on pairwise gradient difference bounds that may not hold for highly heterogeneous data distributions across workers
- The "negligible" communication overhead claim for PairBalance coordination is not empirically validated and could become a bottleneck at scale
- The stronger convergence rates under PL conditions assume the loss satisfies this property, which may not hold for all deep learning tasks

## Confidence

- **Medium** confidence for convergence rate claims due to reliance on pairwise gradient difference bounds
- **Low** confidence for communication overhead analysis as it lacks empirical validation
- **Medium** confidence for PL condition applicability as it assumes a property that may not hold for all tasks

## Next Checks

1. **Scalability test**: Measure actual communication overhead and wall-clock time for PairBalance coordination across 4, 16, and 64 workers on CIFAR-10 to validate the "negligible overhead" claim.

2. **Heterogeneity robustness**: Implement a version with imbalanced data distribution and measure degradation in convergence rate compared to theoretical predictions.

3. **Generalization to non-PL objectives**: Run CD-GraB on tasks known to violate PL conditions and verify whether the O((mnT)^(-2/3)) rate holds or degrades.