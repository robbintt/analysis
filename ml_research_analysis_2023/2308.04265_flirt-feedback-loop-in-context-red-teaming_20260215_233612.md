---
ver: rpa2
title: 'FLIRT: Feedback Loop In-context Red Teaming'
arxiv_id: '2308.04265'
source_url: https://arxiv.org/abs/2308.04265
tags:
- prompts
- attack
- diversity
- prompt
- text-to-image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLIRT is a novel and efficient feedback loop in-context red teaming
  framework that evaluates black-box models and exposes vulnerabilities to unsafe
  content generation. It uses in-context learning in a feedback loop to iteratively
  update prompts based on model feedback, without requiring extensive data or expensive
  fine-tuning.
---

# FLIRT: Feedback Loop In-context Red Teaming

## Quick Facts
- arXiv ID: 2308.04265
- Source URL: https://arxiv.org/abs/2308.04265
- Reference count: 40
- Primary result: FLIRT achieves up to 90.8% attack success rate on Stable Diffusion models using in-context learning feedback loops

## Executive Summary
FLIRT introduces a novel feedback loop framework for automatic red teaming of generative models using in-context learning. The approach iteratively updates prompt exemplars based on safety classifier feedback, enabling efficient discovery of vulnerabilities without requiring extensive data or fine-tuning. Experiments demonstrate FLIRT's effectiveness against both text-to-image and text-to-text models, significantly outperforming baseline approaches in generating unsafe content.

## Method Summary
FLIRT operates through an iterative feedback loop where a red language model generates adversarial prompts using in-context exemplars, the target model produces outputs, safety classifiers evaluate these outputs, and positive feedback (unsafe content) triggers exemplar updates using one of four attack strategies. The framework employs automatic safety classification for efficiency and offers control over prompt diversity and toxicity through weighted scoring objectives. Different attack strategies (FIFO, LIFO, Scoring, Scoring-LIFO) provide complementary approaches to prompt generation and exemplar selection.

## Key Results
- Achieves 90.8% attack success rate against Stable Diffusion v1-4 variants, outperforming baseline approaches
- Generates 45.5% unsafe content rate against text-to-text models, significantly higher than previous methods
- Demonstrates effective transferability of adversarial prompts across different model variants
- Successfully controls diversity-toxicity trade-off through parameter tuning

## Why This Works (Mechanism)

### Mechanism 1
FLIRT's feedback loop iteratively improves prompt effectiveness by updating in-context exemplars based on safety classifier feedback. The red language model generates prompts, the target model produces outputs, safety classifiers evaluate outputs, and positive feedback triggers exemplar updates using attack strategies. Core assumption: safety classifier feedback is accurate enough to guide prompt generation toward unsafe outputs. Break condition: classifier accuracy degrades below threshold needed to distinguish safe from unsafe outputs.

### Mechanism 2
Scoring attack strategy optimizes for multiple objectives simultaneously through weighted combination. The red language model calculates objective scores for each potential exemplar list configuration and selects the configuration maximizing the weighted sum of attack effectiveness, diversity, and low-toxicity objectives. Core assumption: the scoring function accurately captures trade-offs between different objectives. Break condition: optimization problem becomes intractable or objectives conflict irreconcilably.

### Mechanism 3
Different attack strategies provide complementary benefits for adversarial prompt generation. FIFO, LIFO, Scoring, and Scoring-LIFO strategies each preserve different aspects of the prompt generation process - FIFO ensures all prompts get updated, LIFO preserves initial intent, Scoring optimizes objectives, and Scoring-LIFO combines both approaches. Core assumption: different strategies are appropriate for different scenarios. Break condition: one strategy consistently outperforms others across all scenarios.

## Foundational Learning

- **Concept**: In-context learning
  - Why needed here: FLIRT relies on the red LM's ability to learn from demonstrations in the prompt without fine-tuning
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of data requirements and model adaptation speed?

- **Concept**: Safety classification and evaluation
  - Why needed here: FLIRT depends on safety classifiers to provide feedback for the iterative learning loop
  - Quick check question: What are the potential failure modes when using automated classifiers for safety evaluation in adversarial settings?

- **Concept**: Optimization with multiple objectives
  - Why needed here: The scoring attack strategy requires balancing attack effectiveness, diversity, and toxicity through weighted objectives
  - Quick check question: How do you determine appropriate weights for competing objectives in a multi-objective optimization problem?

## Architecture Onboarding

- **Component map**: Red language model (GPT-Neo/BLOOM) -> Target model (text-to-image/text-to-text) -> Safety classifiers (NudeNet, Q16, TOXIGEN) -> Feedback loop controller -> Attack strategy selector

- **Critical path**: 1) Initialize with seed prompts 2) Generate adversarial prompt via in-context learning 3) Run target model with generated prompt 4) Evaluate output with safety classifiers 5) Update exemplars based on feedback using selected attack strategy 6) Repeat for specified iterations

- **Design tradeoffs**: Prompt diversity vs. attack effectiveness (scoring vs. FIFO/LIFO), classifier accuracy vs. automation (human vs. automated evaluation), number of iterations vs. computational cost, model size vs. inference speed

- **Failure signatures**: Stagnant prompt generation (all exemplars converge to similar patterns), low attack success rate despite many iterations, classifier feedback not improving prompt quality, transferability failures across different target models

- **First 3 experiments**: 1) Run baseline comparison with Stochastic Few Shot (SFS) approach using same seed prompts 2) Test all four attack strategies (FIFO, LIFO, Scoring, Scoring-LIFO) on vanilla Stable Diffusion 3) Validate transferability by testing prompts generated from one model on different variants

## Open Questions the Paper Calls Out

### Open Question 1
How do different scoring objectives (attack effectiveness, diversity, low-toxicity) interact and affect the overall performance of FLIRT? The paper only explores a limited set of objective combinations and does not provide comprehensive analysis of how different objective weights impact performance. Experiments varying the weights of different objectives and analyzing their impact would resolve this question.

### Open Question 2
How does the choice of the red language model (e.g., GPT-Neo, BLOOM) affect the performance of FLIRT? The paper only compares two language models and does not explore a wider range of models or investigate underlying reasons for observed similarities. Experiments using a diverse set of language models would provide insights into factors contributing to effective red teaming.

### Open Question 3
How robust is FLIRT to noise and imperfections in the safety classifiers used for evaluation? The ablation studies only explore a limited range of noise levels and do not investigate impact of different types of noise or potential for more sophisticated adversarial attacks on classifiers. Experiments exploring wider range of noise levels and types would provide insights into FLIRT's robustness in real-world scenarios.

## Limitations
- Reliance on automated safety classifiers introduces potential accuracy and robustness issues against adversarial examples
- Attack strategy effectiveness lacks theoretical grounding for optimal selection conditions
- Transferability of generated prompts may not generalize to all types of generative models or safety mechanisms

## Confidence
- **High confidence**: Core FLIRT framework architecture and iterative feedback loop mechanism are well-specified and logically sound
- **Medium confidence**: Attack effectiveness metrics and diversity measurements are clearly defined but practical impact requires further validation
- **Low confidence**: Scoring algorithm's optimization process and specific weight parameters (λ1, λ2) lack sufficient detail for full reproducibility

## Next Checks
1. **Classifier Robustness Test**: Evaluate safety classifier accuracy on benchmark set of known adversarial prompts to establish baseline reliability before running FLIRT iterations
2. **Strategy Comparative Analysis**: Systematically compare all four attack strategies across multiple target models with controlled seed prompts to identify conditions where each strategy performs optimally
3. **Transferability Validation**: Test generated prompts against wider range of model variants and safety mechanisms beyond Stable Diffusion family to assess generalization capabilities