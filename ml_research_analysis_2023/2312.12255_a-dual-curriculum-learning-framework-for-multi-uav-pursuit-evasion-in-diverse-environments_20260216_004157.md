---
ver: rpa2
title: A Dual Curriculum Learning Framework for Multi-UAV Pursuit-Evasion in Diverse
  Environments
arxiv_id: '2312.12255'
source_url: https://arxiv.org/abs/2312.12255
tags:
- task
- capture
- evader
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a dual curriculum learning framework, named
  DualCL, to address multi-UAV pursuit-evasion tasks in diverse environments. The
  method employs an Intrinsic Parameter Curriculum Proposer to progressively suggest
  intrinsic parameters from easy to hard, and an External Environment Generator to
  explore unresolved scenarios and generate appropriate training distributions of
  external environment parameters.
---

# A Dual Curriculum Learning Framework for Multi-UAV Pursuit-Evasion in Diverse Environments

## Quick Facts
- arXiv ID: 2312.12255
- Source URL: https://arxiv.org/abs/2312.12255
- Reference count: 32
- Primary result: Dual curriculum learning framework achieves over 90% capture rate and 27.5% reduction in capture timesteps in multi-UAV pursuit-evasion tasks.

## Executive Summary
This paper introduces DualCL, a dual curriculum learning framework for multi-UAV pursuit-evasion in diverse environments. The method employs an Intrinsic Parameter Curriculum Proposer to progressively suggest intrinsic parameters from easy to hard, and an External Environment Generator to explore unresolved scenarios and generate appropriate training distributions of external environment parameters. Experimental results show that DualCL significantly outperforms baseline methods, achieving over 90% capture rate and reducing capture timestep by at least 27.5% in training scenarios while demonstrating superior zero-shot generalization ability in unseen environments.

## Method Summary
The DualCL framework addresses multi-UAV pursuit-evasion through a curriculum learning approach that adaptively adjusts task difficulty. It uses a Task Evaluator to assess task success rates and select moderately difficult tasks for a curriculum archive, while a Task Sampler progressively samples from this archive with decaying probability. The framework trains policies using MAPPO on a dynamically constructed task distribution that balances exploration and exploitation. The method is evaluated in both 2D (MPE) and 3D (Omni) environments with varying scene sizes, obstacle counts, and speed ratios.

## Key Results
- Achieves over 90% capture rate across training scenarios
- Reduces capture timestep by at least 27.5% compared to baselines
- Demonstrates best zero-shot generalization ability in unseen environments
- Shows transferability from simulation to real-world environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual curriculum learning improves policy performance by adaptively adjusting task difficulty.
- Mechanism: The Task Evaluator module evaluates task success rates and selects tasks with moderate difficulty to construct a curriculum archive. The Task Sampler then samples from this archive with high probability while also sampling from the full task pool to maintain exploration.
- Core assumption: Tasks with moderate success rates (between σmin and σmax) provide the optimal learning gradient for the current policy.
- Evidence anchors:
  - [abstract] "The method employs an Intrinsic Parameter Curriculum Proposer to progressively suggest intrinsic parameters from easy to hard"
  - [section] "The Task Evaluator performs an evaluation of all tasks in the task pool using the current policy πθ and then selects a series of tasks with a moderate level of complexity, which are then integrated into the curriculum learning archive"
  - [corpus] Weak evidence - no direct citation found in corpus
- Break condition: If the evaluation metric becomes too noisy or the policy cannot improve on moderate tasks, the curriculum may stall or diverge.

### Mechanism 2
- Claim: Task sampling probability decay ensures global exploration while focusing on learnable tasks.
- Mechanism: The sampling probability p starts high for curriculum archive tasks and decays over time using coefficient α, allowing more exploration from the full task pool as training progresses.
- Core assumption: Early training benefits from focused difficulty while later training needs broader exploration to avoid forgetting and overfitting.
- Evidence anchors:
  - [abstract] "The External Environment Generator to explore unresolved scenarios and generate appropriate training distributions"
  - [section] "The sampling probability p undergoes a decay process as the number of training episodes progresses, and we use the hyperparameter α to control the decay rate"
  - [corpus] No direct evidence found in corpus
- Break condition: If α is too large, the method may degenerate to random sampling too quickly, losing curriculum benefits.

### Mechanism 3
- Claim: DualCL achieves superior zero-shot generalization by training across diverse task conditions.
- Mechanism: By maintaining a curriculum archive that covers tasks of varying difficulty and sampling from both archive and full task pool, the policy learns robust strategies applicable to unseen environments.
- Core assumption: Exposure to a broad distribution of tasks during training leads to policies that generalize better to new scenarios.
- Evidence anchors:
  - [abstract] "it exhibits the best zero-shot generalization ability in unseen environments"
  - [section] "Our method outperforms all baselines with a clear margin and achieves close to 100% capture rates in both 2-dimensional and 3-dimensional scenarios"
  - [corpus] No direct evidence found in corpus
- Break condition: If the task pool doesn't sufficiently cover the space of possible environments, generalization may fail.

## Foundational Learning

- Concept: Curriculum Learning
  - Why needed here: Multi-agent pursuit involves a vast state and action space that makes uniform training inefficient and prone to failure on complex scenarios
  - Quick check question: What is the key difference between uniform sampling and curriculum learning in reinforcement learning?

- Concept: Multi-Agent Reinforcement Learning
  - Why needed here: The problem requires coordination among multiple pursuers to capture a faster evader, necessitating policies that learn cooperative behaviors
  - Quick check question: How does parameter sharing among homogeneous agents simplify the learning problem in multi-agent systems?

- Concept: Automatic Task Difficulty Evaluation
  - Why needed here: Manually designing curricula is infeasible for the large task space defined by scene size, obstacle number, and speed ratios
  - Quick check question: What metric does the Task Evaluator use to determine task difficulty?

## Architecture Onboarding

- Component map:
  - TaskFlex Solver (main framework)
    - Task Evaluator (difficulty assessment)
    - Task Sampler (training distribution construction)
    - TaskFlex Policy (MARL backbone)
  - Simulation environments (for policy training)

- Critical path:
  1. Initialize task pool Qpool with diverse parameters
  2. Task Evaluator assesses all tasks using current policy
  3. Task Sampler constructs Qtrain from Qcur and Qpool
  4. Policy training via MARL (MAPPO) on Qtrain
  5. Repeat with updated policy

- Design tradeoffs:
  - Higher sampling probability from Qcur accelerates learning but may cause overfitting
  - Larger task pool increases coverage but requires more evaluation overhead
  - Decay coefficient α balances exploration vs. exploitation throughout training

- Failure signatures:
  - Policy performance plateaus despite continued training
  - High variance in capture rates across task conditions
  - Coverage metric remains low even after extensive training

- First 3 experiments:
  1. Test curriculum learning vs. uniform sampling on 2D scenario with fixed parameters
  2. Evaluate sensitivity to sampling probability p by training with p=0.3, 0.6, 0.9
  3. Measure zero-shot transfer by testing on environments not present in training task pool

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the decay coefficient α affect the performance of TFS in different environments and task conditions?
- Basis in paper: [explicit] The paper mentions that the decay coefficient α should be chosen carefully and suggests different values for 2-D (α = 2.0) and 3-D (α = 0.0) environments.
- Why unresolved: The paper does not provide a comprehensive analysis of how α affects performance across various environments and task conditions. It only mentions a general principle that α should not be too large.
- What evidence would resolve it: Experiments varying α across different environments and task conditions, with detailed analysis of the resulting performance metrics, would provide insights into the optimal choice of α for each scenario.

### Open Question 2
- Question: How does TFS perform when applied to real-world environments, and what are the challenges in transferring the learned policies from simulation to reality?
- Basis in paper: [explicit] The paper mentions that the method demonstrates transferability from simulation to real-world environments, but does not provide detailed results or analysis of this transfer.
- Why unresolved: The paper does not provide a thorough analysis of the performance of TFS in real-world environments or discuss the challenges faced during the transfer from simulation to reality.
- What evidence would resolve it: Conducting experiments in real-world environments and comparing the performance of TFS with its simulation counterpart would provide insights into the effectiveness of the transfer. Additionally, analyzing the challenges faced during the transfer and proposing solutions would be valuable.

### Open Question 3
- Question: How does the TaskFlex Solver handle scenarios with a large number of agents or complex interactions among agents?
- Basis in paper: [inferred] The paper focuses on multi-agent pursuit-evasion problems with a fixed number of agents (three pursuers) and does not discuss scenarios with a larger number of agents or complex interactions.
- Why unresolved: The paper does not provide information on how TFS performs when applied to scenarios with a larger number of agents or complex interactions among agents.
- What evidence would resolve it: Experiments with a varying number of agents and scenarios with complex interactions among agents would demonstrate the scalability and adaptability of TFS to different multi-agent settings.

## Limitations
- Computational overhead of evaluating all tasks in the task pool with the current policy may become prohibitive as the task pool grows
- Assumes the curriculum archive can adequately represent the space of learnable tasks, which may not hold for highly complex environments
- Theoretical guarantees of convergence and robustness across completely different domain types (simulation to real-world) remain unproven

## Confidence
- **High confidence**: The empirical results showing improved capture rates and reduced timesteps over baseline methods
- **Medium confidence**: The claim of superior zero-shot generalization, as this depends heavily on the composition of the task pool and evaluation methodology
- **Low confidence**: The theoretical guarantees of convergence and the robustness of the curriculum across completely different domain types (e.g., from simulation to real-world deployment)

## Next Checks
1. **Ablation study**: Remove either the Intrinsic Parameter Curriculum Proposer or External Environment Generator to quantify their individual contributions to performance gains.

2. **Task pool sensitivity**: Systematically vary the size and diversity of the task pool to determine how coverage and performance scale with training set size.

3. **Cross-domain transfer**: Test the policy trained in simulation on real-world captured environments to validate the claimed transferability, measuring performance degradation across the sim-to-real gap.