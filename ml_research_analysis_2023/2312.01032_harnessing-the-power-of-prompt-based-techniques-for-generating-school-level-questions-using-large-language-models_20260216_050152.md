---
ver: rpa2
title: Harnessing the Power of Prompt-based Techniques for Generating School-Level
  Questions using Large Language Models
arxiv_id: '2312.01032'
source_url: https://arxiv.org/abs/2312.01032
tags:
- prompt
- questions
- question
- short
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents EduProbe, a dataset for school-level educational
  question generation from NCERT textbooks, annotated as quadruples of context, long
  prompt, short prompt, and question. The research explores prompt-based techniques
  using fine-tuned transformer models (PEGASUS, T5, MBART, BART) and pre-trained general-purpose
  LLMs (Text-Davinci-003, GPT-3.5-Turbo) to generate descriptive and reasoning-based
  questions.
---

# Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models

## Quick Facts
- **arXiv ID**: 2312.01032
- **Source URL**: https://arxiv.org/abs/2312.01032
- **Reference count**: 40
- **Primary result**: EduProbe dataset with 3,502 quadruples shows T5 with long prompts outperforms other models in automated metrics, while Text-Davinci-003 excels in human evaluation criteria

## Executive Summary
This study presents EduProbe, a novel dataset for school-level educational question generation from NCERT textbooks, annotated as quadruples of context, long prompt, short prompt, and question. The research explores prompt-based techniques using fine-tuned transformer models (PEGASUS, T5, MBART, BART) and pre-trained general-purpose LLMs (Text-Davinci-003, GPT-3.5-Turbo) to generate descriptive and reasoning-based questions. Results demonstrate that T5 with long prompts achieves the best performance in automated metrics (ROUGE, BLEU, BERTScore), while Text-Davinci-003 performs best in human evaluation criteria (grammaticality, appropriateness, relevance). The study reveals that current models still fall short of human-level performance in educational question generation, highlighting the need for further research in this domain.

## Method Summary
The study develops EduProbe, a dataset of 3,502 quadruples from NCERT textbooks covering History, Geography, Economics, Environmental Studies, and Science (grades 6-12). Researchers fine-tune transformer-based LLMs (PEGASUS, T5, MBART, BART) using three prompt settings (with long prompt, with short prompt, without prompt) and evaluate them alongside pre-trained general-purpose LLMs (Text-Davinci-003, GPT-3.5-Turbo). The evaluation employs both automated metrics (ROUGE-2, ROUGE-L, METEOR, CHrF, BLEU, BERTScore) and human evaluation criteria (grammaticality, appropriateness, relevance, complexity, novelty). The dataset creation involves context selection from textbooks, question generation using various models and prompts, and comprehensive evaluation to determine optimal model-prompt combinations.

## Key Results
- T5 with long prompts outperforms all other fine-tuned models in automated metrics (ROUGE, BLEU, BERTScore)
- Text-Davinci-003 achieves superior performance in human evaluation criteria (grammaticality, appropriateness, relevance)
- Long prompts provide more detailed context cues that enable better alignment with reference gold standards
- Both automated and human evaluations indicate current models fall short of human-level performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: T5 with long prompts outperforms other models in automated metrics
- **Mechanism**: Long prompts provide more detailed context cues, enabling T5 to generate questions that better align with the reference gold standard in terms of n-gram and semantic overlap
- **Core assumption**: Longer, more descriptive prompts contain richer information that helps the model generate higher-quality questions
- **Evidence anchors**: [abstract] "T5 (with long prompt) outperforms all other models, but still falls short of the human baseline." [section 8.2] "T5 performs the best across all metrics in the long prompt setting."
- **Break condition**: If long prompts contain irrelevant or misleading information, they could degrade question quality rather than improve it

### Mechanism 2
- **Claim**: Pre-trained general-purpose LLMs (Text-Davinci-003, ChatGPT) excel in human evaluation criteria
- **Mechanism**: These models, trained on diverse web-scale data, have strong natural language generation capabilities that produce questions with better grammaticality, appropriateness, and relevance
- **Core assumption**: General-purpose LLMs have learned robust linguistic patterns that transfer well to educational question generation
- **Evidence anchors**: [abstract] "Text-Davinci-003 achieves the best performance in human evaluation criteria (grammaticality, appropriateness, relevance)." [section 9] "Davinci demonstrates superior performance compared to other models in human evaluation in different prompt settings."
- **Break condition**: If the evaluation criteria heavily favor fluency over content relevance, general-purpose models might score higher without truly generating better educational questions

### Mechanism 3
- **Claim**: Prompt-based techniques generate diverse questions from a single context
- **Mechanism**: Different prompt types (long, short, none) guide the model to focus on different aspects of the context, producing varied question types
- **Core assumption**: The prompt acts as a steering mechanism, directing the model's attention to specific information within the context
- **Evidence anchors**: [abstract] "Our work has the following three key differences... (ii) We explore different types of prompt-based techniques... to provide the QG models additional guidance on what information to emphasize more when generating questions..." [section 10] "We observe different sets of results under different prompt settings and a broad and diverse range of questions generated from the same context..."
- **Break condition**: If the context is too simple or the prompts too similar, the technique might not produce genuinely diverse questions

## Foundational Learning

- **Concept**: Transformer architecture
  - Why needed here: Understanding how encoder-decoder models like T5, BART, and Pegasus work is crucial for grasping why certain models perform better in QG tasks
  - Quick check question: What is the key difference between encoder-only (BERT) and encoder-decoder (T5) transformer architectures, and why is this important for generation tasks?

- **Concept**: Prompt engineering
  - Why needed here: The study heavily relies on different prompt strategies to guide question generation, making it essential to understand how prompt design affects model output
  - Quick check question: How do long prompts differ from short prompts in terms of information content, and why might one be more effective than the other for educational question generation?

- **Concept**: Evaluation metrics in NLP
  - Why needed here: The paper uses both automated metrics (ROUGE, BLEU, BERTScore) and human evaluation, requiring an understanding of their strengths and limitations
  - Quick check question: What are the main limitations of using BLEU score for evaluating question generation, and why might human evaluation be necessary?

## Architecture Onboarding

- **Component map**: Dataset curation pipeline → Model training → Evaluation framework
- **Critical path**: 1) Context selection from NCERT textbooks → 2) Question generation using fine-tuned models with different prompts → 3) Evaluation using automated and human metrics → 4) Analysis of results to determine best-performing model and prompt combination
- **Design tradeoffs**: Fine-tuning vs. few-shot prompting (fine-tuning provides better performance but requires more data/computation); Automated vs. human evaluation (automated metrics are faster but may not capture nuanced quality aspects)
- **Failure signatures**: Poor performance in automated metrics but good human evaluation (indicates fluent but less relevant questions); Good automated metrics but poor human evaluation (suggests too closely mimicking reference questions without true understanding)
- **First 3 experiments**: 1) Run T5 with long prompt on small subset to verify it outperforms other fine-tuned models; 2) Compare Davinci and ChatGPT outputs with and without prompts; 3) Generate questions using same context with different prompt types to analyze diversity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the precise cognitive mechanisms that enable pre-trained general-purpose LLMs like Text-Davinci-003 to generate novel and complex questions that outperform human-level complexity in certain contexts?
- **Basis in paper**: [explicit] The paper notes that "Davinci and ChatGPT can generate complex questions in the without prompt setting" and that "The amount of cognitive effort required to answer a question generated by Davinci and ChatGPT in the without prompt setting is significantly higher than the human baseline."
- **Why unresolved**: While the paper observes this phenomenon, it does not investigate the underlying cognitive or algorithmic reasons for this superior performance in complexity generation
- **What evidence would resolve it**: Comparative studies examining the reasoning patterns, prompt-response dynamics, and question complexity metrics between human-generated and LLM-generated questions

### Open Question 2
- **Question**: How can automated evaluation metrics be improved to better capture the quality of deep, reasoning-based questions in educational contexts?
- **Basis in paper**: [explicit] The paper states that "automated metrics have their own limitations, in terms of evaluating deep questions" and that current metrics "cannot fully capture the quality of generated questions."
- **Why unresolved**: The paper acknowledges the limitations of current automated metrics but does not propose or test alternative evaluation frameworks specifically designed for educational question generation
- **What evidence would resolve it**: Development and validation of new automated metrics that incorporate educational principles, reasoning depth, and pedagogical value

### Open Question 3
- **Question**: What are the optimal prompt structures and content for maximizing the quality and diversity of generated educational questions across different subject areas?
- **Basis in paper**: [explicit] The paper explores "different types of prompt-based techniques (e.g., long prompt, short prompt, and without prompt)" and notes that "prompts definitely help to vary the quality of the generated questions."
- **Why unresolved**: While the paper experiments with different prompt types, it does not systematically analyze which prompt structures are most effective for different subjects or question types
- **What evidence would resolve it**: Large-scale experiments comparing various prompt formulations across subjects, combined with analysis of the relationship between prompt characteristics and question quality metrics

## Limitations

- Dataset limited to NCERT textbooks from grades 6-12 across five subjects, limiting generalizability to other educational contexts
- Manual annotation process for 3,502 quadruples introduces potential subjectivity in prompt design and question quality assessment
- Automated metrics (ROUGE, BLEU, BERTScore) may not fully capture pedagogical quality of educational questions

## Confidence

- **High confidence**: T5 with long prompts outperforming other fine-tuned models in automated metrics (supported by multiple evaluation measures showing consistent superiority)
- **Medium confidence**: Text-Davinci-003 achieving best human evaluation performance (based on single evaluation round with limited annotator pool)
- **Low confidence**: General superiority of pre-trained LLMs over fine-tuned models (comparisons limited to two specific models, GPT-3.5-Turbo performance not explicitly detailed in human evaluation)

## Next Checks

1. **Replication with diverse educational datasets**: Test the best-performing models (T5 with long prompts, Text-Davinci-003) on question generation tasks using different textbook corpora or subject domains not represented in EduProbe to verify generalizability

2. **Extended human evaluation**: Conduct additional rounds of human evaluation with larger, more diverse annotator pools and include inter-annotator agreement metrics to strengthen confidence in human evaluation results

3. **Longitudinal performance analysis**: Evaluate model performance across different grade levels and subjects within EduProbe to identify whether certain models excel for specific educational contexts or question types