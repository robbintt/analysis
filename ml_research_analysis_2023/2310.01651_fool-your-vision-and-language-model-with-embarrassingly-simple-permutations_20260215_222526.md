---
ver: rpa2
title: Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations
arxiv_id: '2310.01651'
source_url: https://arxiv.org/abs/2310.01651
tags:
- permutation
- language
- answer
- adversarial
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a surprising and severe vulnerability in\
  \ large language and vision-language models (LLMs/VLLMs) when answering multiple-choice\
  \ questions: even simple permutations of answer choices can cause dramatic drops\
  \ in accuracy\u2014often below random-chance levels. The authors demonstrate that\
  \ this brittleness persists across a wide range of popular models (Llama2, Vicuna,\
  \ GPT-3.5-turbo, LLaVA, InstructBLIP, etc.) and datasets, even when incorrect options\
  \ are removed to simplify the task."
---

# Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations

## Quick Facts
- arXiv ID: 2310.01651
- Source URL: https://arxiv.org/abs/2310.01651
- Reference count: 12
- Key outcome: Simple permutations of answer choices in MCQA tasks cause dramatic accuracy drops in LLMs and VLLMs, often below random-chance levels, revealing a severe robustness vulnerability.

## Executive Summary
This paper exposes a fundamental vulnerability in large language and vision-language models: they fail catastrophically when answer choices in multiple-choice questions are permuted, even when the correct answer remains unchanged. The authors demonstrate that this brittleness affects a wide range of popular models (Llama2, Vicuna, GPT-3.5-turbo, LLaVA, InstructBLIP, etc.) across multiple datasets, with accuracy often dropping below random-chance levels. The vulnerability persists even when incorrect options are removed to simplify the task, and common mitigation strategies like majority voting or contextual calibration fail to adequately restore performance. These findings suggest that current MCQA benchmarks may be misleading indicators of model robustness, raising significant concerns about deploying these models in practical applications where answer ordering might vary.

## Method Summary
The authors systematically tested LLM and VLLM robustness to answer option permutations by evaluating model accuracy on standard MCQA datasets before and after generating all possible permutations of answer choices. For each question, they measured accuracy using the original answer order, then iteratively applied permutations until the first incorrect prediction was found. This "Cheap Permutation Testing" approach revealed the severity and consistency of the vulnerability across different model families and datasets. They also tested answer set pruning (removing distractor options) to determine if the vulnerability stemmed from complex interactions with incorrect answers, and evaluated common calibration techniques to assess potential mitigation strategies.

## Key Results
- Accuracy drops dramatically below random-chance levels when answer options are permuted, even for models that perform well on standard MCQA benchmarks
- The vulnerability persists across diverse model architectures (Llama2, Vicuna, GPT-3.5-turbo, LLaVA, InstructBLIP) and datasets (MMLU, ARC-c, BoolQ, ScienceQA, etc.)
- Removing distractor options does not resolve the vulnerability, indicating it's not simply a position bias issue
- Common mitigation strategies like majority voting and contextual calibration fail to adequately restore performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models fail to learn invariant representations for multiple-choice options.
- Mechanism: During training, models may learn to rely on positional cues or relative ordering of answer choices rather than the semantic content itself. When answer options are permuted, these learned positional dependencies are violated, causing the model to mispredict even when the correct answer remains unchanged.
- Core assumption: The model's training data did not include sufficient examples of permuted answer options, so the model never learned to treat the answer choices as an unordered set.
- Evidence anchors:
  - [abstract] "even simple permutations of answer choices can cause dramatic drops in accuracy—often below random-chance levels"
  - [section] "models should ideally be as invariant to prompt permutation as humans are"
  - [corpus] "Cheap Permutation Testing" suggests permutation-based approaches can reveal model brittleness
- Break condition: If models are trained or fine-tuned with explicit permutation augmentation during MCQA task preparation, they may learn to be invariant to option ordering.

### Mechanism 2
- Claim: The vulnerability stems from the interaction between distractors and the correct answer, not just position bias.
- Mechanism: The model may learn to identify the correct answer by evaluating its relationship to the distractors (e.g., semantic dissimilarity, contextual fit). When distractors are permuted, this learned relationship is disrupted, leading to incorrect predictions even when the correct answer's position hasn't changed.
- Core assumption: Models are not just picking the "best" answer but are using a more complex decision boundary that involves all answer choices simultaneously.
- Evidence anchors:
  - [section] "it is not just the true answer, and the location of the true answer (position bias), but also the pattern of the distractor answers around the true answer... that determine model success or failure"
  - [section] "models apparently rely on the relationships between choices, including the distractors"
  - [corpus] "A Survey and Analysis of Evolutionary Operators for Permutations" indicates permutation can significantly affect model behavior
- Break condition: If distractor options are made semantically irrelevant or if the model is trained to focus only on the question and each option independently, this vulnerability may be reduced.

### Mechanism 3
- Claim: Existing calibration and majority voting defenses are insufficient because they don't address the fundamental permutation invariance problem.
- Mechanism: Calibration techniques adjust for prior biases but don't teach the model to treat answer sets as unordered. Majority voting over all permutations is computationally expensive and still fails when most permutations lead to incorrect answers, indicating the underlying model architecture isn't equipped to handle permutation.
- Core assumption: The model's architecture and training procedure create a fundamental limitation that simple post-hoc fixes cannot overcome.
- Evidence anchors:
  - [abstract] "Common mitigation strategies like majority voting or contextual calibration fail to adequately restore performance"
  - [section] "calibration techniques such as calibrate-before-use (Zhao et al., 2021) fail to alleviate this problem effectively"
  - [corpus] "A Unified Analysis of Stochastic Gradient Descent with Arbitrary Data Permutations" suggests permutation issues can be fundamental to optimization
- Break condition: If the model architecture is modified to explicitly encode permutation invariance (e.g., using set transformers or permutation-equivariant layers), these post-hoc fixes may become unnecessary.

## Foundational Learning

- Concept: Permutation invariance
  - Why needed here: The vulnerability exists because models are not permutation invariant with respect to answer choices, treating ordered sequences as semantically meaningful when they shouldn't be.
  - Quick check question: What would happen to a truly permutation-invariant model if you randomly reordered the answer choices in a multiple-choice question?

- Concept: Set representation learning
  - Why needed here: Multiple-choice questions are fundamentally about selecting from a set of options, but current models may be treating them as ordered sequences. Understanding how to represent and process unordered sets is crucial for building robust MCQA systems.
  - Quick check question: How would you modify a transformer architecture to process a set of answer choices as an unordered collection rather than a sequence?

- Concept: In-context learning limitations
  - Why needed here: The vulnerability appears specifically in in-context learning scenarios where models must adapt to new question-answer formats without parameter updates. Understanding these limitations helps explain why even sophisticated models fail at this seemingly simple task.
  - Quick check question: Why might a model that performs well on standard MCQA benchmarks still fail when the answer options are permuted?

## Architecture Onboarding

- Component map: Input processing -> Context embedding -> Option scoring -> Prediction -> Post-processing
- Critical path: Question and options → Context embedding → Option representation → Scoring → Prediction
- Design tradeoffs: Position-based scoring is simple but not permutation invariant; set-based representations require more complex architectures but provide robustness; training with permutations increases computational cost but improves generalization
- Failure signatures: Accuracy drops significantly below random chance when options are permuted; performance varies dramatically with different permutations of the same question; calibration techniques fail to restore performance; majority voting over all permutations is computationally prohibitive
- First 3 experiments:
  1. Measure accuracy degradation when answer options are permuted for a baseline model on a standard MCQA dataset
  2. Test whether removing distractor options improves robustness (answer set pruning experiment)
  3. Implement and evaluate a simple permutation augmentation strategy during training to see if it improves robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training strategies or architectures could lead to intrinsic robustness to adversarial permutations in LLMs and VLMs?
- Basis in paper: [inferred] The paper concludes by highlighting the need for future work to develop training strategies and/or architectures that lead to intrinsic robustness to adversarial attacks.
- Why unresolved: While the paper identifies the vulnerability and its extent, it does not propose specific solutions or architectures that could mitigate this issue.
- What evidence would resolve it: Developing and testing new training methods or architectural modifications that improve model robustness to permutations, and comparing their performance against current models.

### Open Question 2
- Question: How does the vulnerability to adversarial permutations in MCQA tasks affect the trustworthiness of LLMs and VLMs in practical applications beyond benchmarks?
- Basis in paper: [inferred] The paper raises concerns about the widespread practice of evaluating and deploying these models based on MCQA tasks, urging caution in interpreting high benchmark scores as evidence of robust capabilities.
- Why unresolved: The paper does not provide empirical evidence or case studies showing how this vulnerability impacts real-world applications.
- What evidence would resolve it: Conducting studies that apply these models to practical scenarios and measuring the impact of permutation vulnerability on their performance and reliability.

### Open Question 3
- Question: What are the specific mechanisms by which distractor options influence model predictions in MCQA tasks, beyond simple position bias?
- Basis in paper: [explicit] The paper demonstrates that the vulnerability to permutations is not explainable by position bias alone and suggests that models rely on the relationships between choices, including distractors.
- Why unresolved: The paper identifies the complexity of the issue but does not delve into the underlying mechanisms or patterns that cause models to be influenced by distractor options.
- What evidence would resolve it: Analyzing model behavior and internal representations when exposed to different permutations of distractor options to uncover the specific patterns or features that influence predictions.

## Limitations

- The exact prompt templates and decoding configurations used for each model are not specified, which could significantly impact vulnerability measurements
- The paper does not provide exhaustive testing of all possible defenses or architectural modifications that might address the permutation invariance problem
- The relative contributions of position bias, distractor relationships, and other factors on permutation vulnerability are not fully disentangled

## Confidence

- Core findings (High): The vulnerability to answer option permutations is well-demonstrated across multiple models and datasets
- Proposed mechanisms (Medium): While the positional dependency explanation is compelling, the relative contributions of different factors is not fully disentangled
- Calibration/majority voting claims (Medium): These methods may still provide some benefit that wasn't captured in the experiments

## Next Checks

1. Test whether models trained with permutation augmentation during fine-tuning show improved robustness to answer option reordering
2. Implement and evaluate set-based transformer architectures that explicitly encode permutation invariance for MCQA tasks
3. Conduct ablation studies to separate the effects of position bias, distractor relationships, and other factors on permutation vulnerability