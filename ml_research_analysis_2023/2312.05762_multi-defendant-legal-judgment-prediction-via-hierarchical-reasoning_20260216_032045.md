---
ver: rpa2
title: Multi-Defendant Legal Judgment Prediction via Hierarchical Reasoning
arxiv_id: '2312.05762'
source_url: https://arxiv.org/abs/2312.05762
tags:
- defendant
- multi-defendant
- judgment
- reasoning
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-defendant Legal Judgment
  Prediction (LJP), where multiple defendants in a criminal case have complex interactions
  that existing single-defendant methods struggle to handle. The authors propose a
  Hierarchical Reasoning Network (HRN) that formalizes the judgment process as hierarchical
  reasoning chains to distinguish judgment results among defendants.
---

# Multi-Defendant Legal Judgment Prediction via Hierarchical Reasoning

## Quick Facts
- arXiv ID: 2312.05762
- Source URL: https://arxiv.org/abs/2312.05762
- Reference count: 10
- Primary result: HRN significantly outperforms state-of-the-art baselines across all LJP subtasks, achieving improvements of up to 20 percentage points in accuracy and F1 scores

## Executive Summary
This paper addresses the challenge of multi-defendant Legal Judgment Prediction (LJP), where existing single-defendant methods struggle to distinguish between defendants with similar fact descriptions. The authors propose a Hierarchical Reasoning Network (HRN) that formalizes the judgment process as hierarchical reasoning chains to predict law articles, charges, and terms of penalty for each defendant. The method uses a Sequence-to-Sequence generation framework with mT5 and Fusion-in-Decoder to handle long fact descriptions. Experiments on the newly introduced MultiLJP dataset show HRN achieves significant improvements over baselines, with ablation studies confirming the importance of both hierarchical reasoning levels and intermediate tasks.

## Method Summary
The method converts multi-defendant legal judgment prediction into a sequence-to-sequence generation task using mT5 as the language model. The hierarchical reasoning process consists of two levels: first-level reasoning identifies criminal relationships between defendants and sentencing circumstances for each defendant, while second-level reasoning performs forward prediction and backward verification of law articles, charges, and terms of penalty. Fusion-in-Decoder handles long fact descriptions by splitting them into multiple paragraphs. The model is trained using cross-entropy loss with task-specific weights to balance intermediate and final prediction objectives.

## Key Results
- HRN significantly outperforms state-of-the-art single-defendant baselines across all LJP subtasks
- Hierarchical reasoning contributes up to 20 percentage points improvement in accuracy and F1 scores
- Both first-level (criminal relationships, sentencing circumstances) and second-level (forward/backward prediction) reasoning components are essential for optimal performance
- The MultiLJP dataset enables comprehensive evaluation of multi-defendant LJP with 23,717 annotated cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical reasoning chains enable the model to distinguish judgment results among defendants by explicitly modeling intermediate tasks (criminal relationships and sentencing circumstances) before predicting final outcomes.
- Mechanism: The method uses a two-level reasoning process where the first level identifies relationships and circumstances affecting each defendant, then the second level performs forward prediction and backward verification of final judgment results.
- Core assumption: The intermediate reasoning steps contain sufficient information to disambiguate between defendants with otherwise similar fact descriptions.
- Evidence anchors: [abstract] "We formalize the multi-defendant judgment process as hierarchical reasoning chains and introduce a multi-defendant LJP method, named Hierarchical Reasoning Network (HRN), which follows the hierarchical reasoning chains to determine criminal relationships, sentencing circumstances, law articles, charges, and terms of penalty for each defendant."

### Mechanism 2
- Claim: Sequence-to-Sequence generation framework with mT5 enables effective modeling of hierarchical reasoning chains for multi-defendant LJP.
- Mechanism: The method converts reasoning chains into Seq2Seq generation tasks where the model generates label sequences conditioned on fact descriptions, defendant names, and task descriptions.
- Core assumption: Seq2Seq generation with language models can effectively capture the sequential dependencies in legal reasoning processes.
- Evidence anchors: [abstract] "we convert these reasoning chains into Sequence-to-Sequence (Seq2Seq) generation tasks and apply the mT5 (Xue et al., 2021) to model them."

### Mechanism 3
- Claim: Backward verification process improves prediction accuracy by allowing the model to validate forward predictions against the fact description.
- Mechanism: After forward prediction of law articles, charges, and terms of penalty, the model performs backward verification in reverse order, checking each prediction against the fact description and earlier predictions to ensure consistency.
- Core assumption: Legal reasoning is inherently bidirectional, with conclusions often informing the interpretation of evidence and vice versa.
- Evidence anchors: [abstract] "the second-level reasoning chain predicts and verifies the law articles, charges, and terms of penalty for each defendant, using a forward prediction process and a backward verification process, respectively."

## Foundational Learning

- Concept: Hierarchical reasoning and multi-step inference
  - Why needed here: Multi-defendant cases require distinguishing between defendants with similar fact descriptions, which necessitates explicit intermediate reasoning steps rather than direct prediction.
  - Quick check question: Can you explain why simply concatenating defendant names with fact descriptions would fail to distinguish between defendants in multi-defendant cases?

- Concept: Sequence-to-Sequence generation and language model fine-tuning
  - Why needed here: The method converts complex reasoning tasks into generation problems that can be handled by pre-trained language models, requiring understanding of how to structure generation tasks and fine-tune models for specific outputs.
  - Quick check question: How does the Fusion-in-Decoder approach handle long fact descriptions that exceed the model's token limit?

- Concept: Multi-task learning and loss function design
  - Why needed here: The model predicts multiple intermediate and final tasks simultaneously, requiring careful design of loss functions that balance different objectives.
  - Quick check question: How would you modify the loss function if you wanted to give more weight to term of penalty prediction compared to other tasks?

## Architecture Onboarding

- Component map: Fact description splitting -> mT5 encoder -> mT5 decoder -> Criminal relationship prediction -> Sentencing circumstance prediction -> Forward prediction -> Backward verification -> Output processing
- Critical path:
  1. Split fact description into paragraphs
  2. Encode paragraphs with defendant name and task description
  3. Generate criminal relationships for all defendants
  4. Generate sentencing circumstances for each defendant
  5. Forward prediction of law articles, charges, terms of penalty
  6. Backward verification in reverse order
  7. Select highest confidence prediction chain
- Design tradeoffs:
  - Hierarchical vs flat architecture: Hierarchical reasoning provides better disambiguation but adds complexity
  - Generation vs classification: Generation allows more flexible outputs but may be less precise than classification
  - Forward-only vs bidirectional: Adding backward verification improves accuracy but increases computational cost
- Failure signatures:
  - Poor criminal relationship prediction leading to incorrect sentencing circumstances
  - Inconsistent predictions between forward and backward processes
  - Failure to handle long fact descriptions properly
  - Overfitting to training data patterns
- First 3 experiments:
  1. Compare HRN with and without backward verification on MultiLJP validation set to quantify its contribution
  2. Test different loss function weightings (Î» values) to optimize the trade-off between intermediate and final tasks
  3. Evaluate performance on cases with varying numbers of defendants to identify scaling limitations

## Open Questions the Paper Calls Out

- Question: How would incorporating criminal relationships and sentencing circumstances as explicit intermediate reasoning steps impact the performance of single-defendant LJP models?
  - Basis in paper: [explicit] The paper shows that modeling these intermediate steps significantly improves multi-defendant LJP performance, but doesn't explore their impact on single-defendant cases.
  - Why unresolved: The paper focuses exclusively on multi-defendant scenarios and doesn't test whether these intermediate reasoning steps would benefit single-defendant prediction.
  - What evidence would resolve it: Experimental results comparing single-defendant LJP models with and without criminal relationship and sentencing circumstance prediction as intermediate steps.

- Question: What is the maximum number of defendants that the HRN model can effectively handle before performance degrades significantly?
  - Basis in paper: [inferred] The dataset statistics show cases with up to 4+ defendants, but the paper doesn't systematically test model performance across varying numbers of defendants.
  - Why unresolved: The paper reports overall performance metrics but doesn't analyze how model accuracy changes as the number of defendants increases.
  - What evidence would resolve it: Performance metrics broken down by defendant count categories (2, 3, 4, 5+ defendants) and identification of the point where accuracy begins to decline sharply.

- Question: How transferable is the hierarchical reasoning approach to other legal domains or jurisdictions beyond Chinese criminal law?
  - Basis in paper: [inferred] The model is trained and tested on Chinese criminal cases, but the paper doesn't discuss its applicability to other legal systems or types of law.
  - Why unresolved: The paper demonstrates effectiveness within a specific domain but doesn't explore whether the hierarchical reasoning framework generalizes to other legal contexts.
  - What evidence would resolve it: Cross-jurisdiction experiments testing the model on criminal cases from other countries or civil law cases within the same jurisdiction.

## Limitations
- The method's performance relies heavily on the quality and comprehensiveness of the MultiLJP dataset, which may not represent all types of multi-defendant cases
- The hierarchical reasoning approach adds computational complexity and may not scale well to cases with very large numbers of defendants
- The model's predictions depend on the specific legal framework of Chinese criminal law and may not generalize to other jurisdictions

## Confidence
- Confidence in hierarchical reasoning mechanism: Medium - ablation studies show improvements but baselines are not well-matched
- Confidence in Seq2Seq generation approach: Medium - technically sound but implementation details are not fully specified
- Confidence in backward verification contribution: Medium - method is logical but contribution needs independent validation

## Next Checks
1. Evaluate HRN on multi-defendant cases from different legal domains (civil, administrative) to test generalization beyond criminal law
2. Compare HRN against other multi-defendant LJP methods (once available) rather than single-defendant baselines
3. Conduct human evaluation of intermediate predictions (criminal relationships and sentencing circumstances) to verify their legal coherence and usefulness for disambiguation