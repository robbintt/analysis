---
ver: rpa2
title: Blockwise Parallel Transformer for Large Context Models
arxiv_id: '2305.19370'
source_url: https://arxiv.org/abs/2305.19370
tags:
- arxiv
- memory
- attention
- blockwise
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Blockwise Parallel Transformer (BPT), a method
  to reduce memory requirements in Transformers by leveraging blockwise computation
  of self-attention and fusing it with feedforward networks. The key idea is to process
  input sequences in blocks, computing self-attention and feedforward networks for
  each block separately, thereby avoiding the need to materialize the full attention
  matrix and reducing memory usage.
---

# Blockwise Parallel Transformer for Large Context Models

## Quick Facts
- arXiv ID: 2305.19370
- Source URL: https://arxiv.org/abs/2305.19370
- Reference count: 40
- Primary result: Achieves up to 4x memory reduction compared to state-of-the-art memory-efficient Transformers

## Executive Summary
Blockwise Parallel Transformer (BPT) introduces a method to significantly reduce memory requirements in Transformer models by leveraging blockwise computation of self-attention and fusing it with feedforward networks. The key innovation is processing input sequences in blocks, computing self-attention and feedforward networks separately for each block, which avoids materializing the full attention matrix. This approach enables training on sequences up to 32 times longer than vanilla Transformers and 2-4 times longer than previous memory-efficient methods, while achieving up to 4x memory reduction.

## Method Summary
BPT processes input sequences by splitting them into blocks and computing self-attention blockwise. For each query block, attention is computed iteratively with each key-value block, with normalization statistics tracked and combined to produce global softmax outputs. The feedforward network is fused into the blockwise loop, applied immediately to partial attention results and added via residual connection. This eliminates the need to store full intermediate activations and leverages faster SRAM memory to reduce communication costs and increase throughput.

## Key Results
- Enables training on sequences up to 32× longer than vanilla Transformers
- Achieves 2-4× longer sequence training compared to previous memory-efficient methods
- Demonstrates up to 4× memory reduction versus state-of-the-art approaches

## Why This Works (Mechanism)

### Mechanism 1
Blockwise computation reduces memory by avoiding full softmax matrix materialization. Instead of computing QK^T for the full sequence (O(N^2) memory), the input is split into blocks. For each query block Qi, attention is computed with each key-value block (Kj, Vj), and normalization statistics are tracked and combined to produce the global softmax output. This yields O(N) memory.

### Mechanism 2
Fusing feedforward computation into the blockwise loop eliminates the need to store full intermediate activations. While iterating over key-value blocks for a given query block, the feedforward network is applied immediately to the partial attention result and added via residual connection. This means the full sequence-level feedforward never materializes in memory.

### Mechanism 3
Hardware speed differences (SRAM vs HBM) can be exploited to improve throughput. Blockwise computation fits within faster SRAM, reducing data movement and communication overhead compared to full-sequence processing that spills into slower HBM.

## Foundational Learning

- **Softmax normalization and scaling properties**: Blockwise attention relies on exact reconstruction of global softmax via scaling corrections. Quick check: Given blockwise max scores and correction terms, can you reconstruct the global softmax without materializing the full QK^T matrix?

- **Residual connections and gradient flow**: Fusing feedforward into blockwise loops must preserve gradient correctness across block boundaries. Quick check: In a blockwise loop, does adding the residual before or after feedforward affect gradient propagation?

- **Memory hierarchy in GPUs/TPUs**: Performance gains rely on fitting blocks into faster SRAM to reduce memory traffic. Quick check: If a block size is too large for SRAM, what is the fallback memory tier, and how does it affect latency?

## Architecture Onboarding

- **Component map**: Input sequence → block splitter (query, key-value) → for each query block: iterate over key-value blocks → compute attention with scaling correction → apply feedforward → add residual → combine scaled block outputs → final sequence output

- **Critical path**: Outer loop: query block projection; Inner loop: key/value projection, attention, feedforward, residual; Synchronization: wait for all inner-loop iterations before moving to next query block

- **Design tradeoffs**: Block size vs memory (larger blocks reduce loop overhead but increase SRAM pressure); Precision vs speed (blockwise softmax scaling can introduce numerical error); Throughput vs latency (blockwise parallelism can improve throughput but may increase per-sample latency)

- **Failure signatures**: Out-of-memory (block size too large for available SRAM/HBM); Training instability (incorrect scaling corrections or gradient mismatches); Slow performance (blocks too small, leading to excessive loop overhead)

- **First 3 experiments**: 1) Run with block size = 128, verify memory usage drops compared to full-sequence baseline; 2) Increase block size incrementally until OOM occurs, find maximum stable block size; 3) Profile SRAM vs HBM traffic to confirm memory hierarchy benefits

## Open Questions the Paper Calls Out

### Open Question 1
How does block size selection affect the performance and memory usage of BPT? The paper mentions grid search over block sizes [16, 64, 128, 512, 1024, 2048, 4096] but doesn't provide specific details on how block size affects performance and memory usage. A detailed analysis would help understand the impact of block size selection.

### Open Question 2
How does BPT compare to other memory-efficient methods in terms of training time and computational efficiency? While the paper mentions competitive throughput and surpassing vanilla transformers in speed, it lacks direct comparison with other memory-efficient methods in training time and computational efficiency.

### Open Question 3
Can BPT be applied to other domains or tasks beyond language modeling and reinforcement learning? The paper evaluates BPT only on language modeling and reinforcement learning tasks without discussing potential applicability to other domains or tasks.

## Limitations

- Blockwise softmax scaling mechanism lacks explicit empirical validation and numerical stability analysis
- Feedforward fusion assumes gradient preservation without ablation studies comparing to sequential baselines
- Hardware-specific performance claims (SRAM benefits) lack empirical profiling data on actual memory traffic

## Confidence

**High Confidence**: Core claim that BPT enables longer sequence training with reduced memory is well-supported by direct experimental comparison showing 4x memory reduction and 32x sequence length extension.

**Medium Confidence**: Claims about 2-4x longer sequences than previous methods are supported but vary across model scales, suggesting benefits are not uniform.

**Low Confidence**: Hardware-specific performance claims lack empirical validation; blockwise softmax scaling mechanism has no explicit error analysis; feedforward fusion's impact on model accuracy is stated but not verified through ablation.

## Next Checks

1. **Numerical Stability Validation**: Implement blockwise softmax with varying block sizes and measure L2 distance between blockwise-reconstructed softmax and ground-truth full softmax across diverse token distributions and sequence lengths.

2. **Feedforward Fusion Ablation**: Compare three variants - full-sequence feedforward baseline, blockwise feedforward with residual fusion, and blockwise feedforward without fusion - to isolate contribution of fusion versus memory reduction.

3. **Hardware Profiling Study**: Instrument BPT implementation to track memory traffic between SRAM and HBM, measuring peak SRAM utilization, total HBM volume, achieved bandwidth, and relative latency to validate hardware speed benefits.