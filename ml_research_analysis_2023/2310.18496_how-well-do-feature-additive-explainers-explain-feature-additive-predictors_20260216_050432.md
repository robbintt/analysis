---
ver: rpa2
title: How Well Do Feature-Additive Explainers Explain Feature-Additive Predictors?
arxiv_id: '2310.18496'
source_url: https://arxiv.org/abs/2310.18496
tags:
- feature
- effects
- explanations
- explanation
- explainers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for evaluating the fidelity of post-hoc
  feature-additive explanation methods by comparing them to analytically derived ground
  truth from feature-additive models. The key idea is to use the MatchEffects algorithm
  to align effects between the model and explainer, enabling direct comparison of
  contributions.
---

# How Well Do Feature-Additive Explainers Explain Feature-Additive Predictors?

## Quick Facts
- arXiv ID: 2310.18496
- Source URL: https://arxiv.org/abs/2310.18496
- Reference count: 40
- Key outcome: Post-hoc explainers cannot reliably explain feature-additive predictors, especially when models involve interactions between features.

## Executive Summary
This paper proposes a method for evaluating the fidelity of post-hoc feature-additive explanation methods by comparing them to analytically derived ground truth from feature-additive models. The key innovation is the MatchEffects algorithm, which aligns effects between the model and explainer to enable direct comparison of contributions. The paper evaluates LIME, SHAP, SHAPR, MAPLE, and PDP on thousands of synthetic models and several real-world datasets. Results show that while SHAP outperforms other explainers, all methods begin to fail as dimensionality, interaction effects, and interaction order increase.

## Method Summary
The paper evaluates post-hoc explainers by generating synthetic models with controlled properties (dimensionality, interaction order, nonlinearity) and training feature-additive models on real-world datasets. It uses the MatchEffects algorithm to align model effects with explainer effects, then applies equivalence relations to enable direct comparison. Fidelity is measured using Euclidean and cosine distances between aligned contributions, with MaIoU quantifying the goodness of effect matching. The evaluation framework enables systematic assessment of explainer performance across different model complexities.

## Key Results
- SHAP consistently outperforms LIME, SHAPR, MAPLE, and PDP in explaining feature-additive predictors
- All explainers show declining performance as dimensionality, interaction effects, and interaction order increase
- The MatchEffects algorithm successfully aligns effects between models and explainers for comparison
- Real-world dataset results mirror synthetic findings, validating the evaluation approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MatchEffects algorithm enables fair comparison between model effects and explainer effects by finding the smallest common interaction subsets.
- Mechanism: Constructs a bipartite graph where model effects and explainer effects are vertices, with edges connecting effects sharing features. Connected components identify groups of effects with inter-dependencies, allowing direct comparison of contributions within matched groups.
- Core assumption: Effects with shared features have meaningful relationships that can be compared through their combined contributions.
- Evidence anchors:
  - [abstract]: "To facilitate evaluation, we propose an algorithm,MatchEffects, that directly maps any model with any amount of additive structure to feature-additive post hoc explanations."
  - [section]: "To achieve this matching, we consider allDj and ˆDfk to be the left- and right-hand vertices, respectively, of an undirected bipartite graph."
  - [corpus]: Weak - corpus papers discuss explanation evaluation but don't specifically mention bipartite graph matching approaches.
- Break condition: When the bipartite graph becomes fully connected (all effects relate to all other effects), making MaIoU uninformative and comparisons meaningless.

### Mechanism 2
- Claim: The MaIoU metric quantifies the goodness of effect matching by averaging intersection-over-union scores across matched effect pairs.
- Mechanism: For each connected component found by MatchEffects, computes IoU between all effect pairs, then averages these to produce component-level scores. Final MaIoU is the mean of all component scores.
- Core assumption: IoU between feature subsets provides meaningful measure of effect similarity.
- Evidence anchors:
  - [abstract]: "We construct a test bed for the evaluation of feature-additive post hoc explanations against ground truth derived analytically from feature-additive models."
  - [section]: "For an edge{Dj, ˆDk}∈Ec, the intersection-over-union (IoU), also known as the Jaccard index, is calculated betweenDj and ˆDk."
  - [corpus]: Weak - corpus papers mention evaluation metrics but don't specifically discuss IoU-based matching quality measures.
- Break condition: When explainers produce effects that are either too sparse (low coverage) or too dense (high coverage), making IoU scores uninformative.

### Mechanism 3
- Claim: Feature-additive models provide analytically derivable ground truth that enables direct evaluation of explainer fidelity.
- Mechanism: Models with additive structure have known feature contributions by definition. Post-hoc explainers can be evaluated by comparing their output to these analytically derived ground truth contributions.
- Core assumption: Additive structure allows for exact decomposition of model predictions into feature contributions.
- Evidence anchors:
  - [abstract]: "We ask, 'can popular feature-additive explainers (e.g., LIME, SHAP, SHAPR, MAPLE, and PDP) explain feature-additive predictors?'"
  - [section]: "We propose to evaluate feature-additive explainers by comparing their explanations to the ground truth explanations from feature-additive white box models."
  - [corpus]: Weak - corpus papers discuss evaluation but don't specifically mention using analytically derived ground truth from additive models.
- Break condition: When models contain non-additive components or complex interactions that cannot be decomposed into simple feature contributions.

## Foundational Learning

- Concept: Feature-additive model formulation
  - Why needed here: Understanding how models decompose into feature contributions is essential for evaluating explainer fidelity against ground truth
  - Quick check question: How does the model formulation allow for both main effects and interaction effects while maintaining additive structure?

- Concept: Shapley values and feature importance
  - Why needed here: SHAP and related methods use Shapley value theory to attribute contributions; understanding this helps interpret evaluation results
  - Quick check question: What is the relationship between Shapley values and feature contributions in additive models?

- Concept: Graph theory and connected components
  - Why needed here: MatchEffects relies on finding connected components in bipartite graphs to group related effects
  - Quick check question: How does the connected components algorithm ensure all inter-dependent effects are grouped together?

## Architecture Onboarding

- Component map: Model generation (synthetic/real) -> Data preparation -> Explainer execution -> MatchEffects algorithm -> Equivalence relations application -> Fidelity metric computation (MaIoU, cosine/Euclidean distance) -> Result aggregation and visualization

- Critical path: Model → Data → Explainer → MatchEffects → Equivalence relations → Fidelity metrics → Analysis

- Design tradeoffs: 
  - Exact matching vs. approximate matching of effects
  - Computational efficiency vs. matching completeness
  - Feature coverage vs. human interpretability

- Failure signatures:
  - Explainer timeouts or resource exhaustion
  - Invalid perturbations leading to NaN values
  - Poor MaIoU scores indicating mismatch between model and explainer effects
  - High fidelity scores despite poor explanation quality (false positives)

- First 3 experiments:
  1. Generate simple synthetic model with known additive structure and test basic MatchEffects functionality
  2. Run all explainers on a simple linear model to verify equivalence relations work correctly
  3. Create model with clear interaction effects and verify MatchEffects properly groups them for comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed evaluation methods perform when applied to models with higher-order interactions (order > 3) or more complex feature dependencies?
- Basis in paper: [explicit] The paper evaluates explainers on models with interaction effects up to order 3 and mentions that typical NNs contain a greater number of interaction effects and higher order of interactions.
- Why unresolved: The paper does not test models with interaction effects beyond order 3, leaving uncertainty about the scalability of the evaluation methods to more complex models.
- What evidence would resolve it: Testing the evaluation methods on models with interaction effects of order > 3 and more complex feature dependencies, and comparing the results to those obtained with lower-order interactions.

### Open Question 2
- Question: Can the MatchEffects algorithm be extended to handle cases where the explainer and model produce fundamentally different types of explanations, such as saliency maps versus feature-additive contributions?
- Basis in paper: [explicit] The paper introduces the MatchEffects algorithm to align effects between the model and explainer, but it assumes both produce feature-additive contributions.
- Why unresolved: The algorithm's current formulation is limited to feature-additive explanations, and its applicability to other explanation types is unclear.
- What evidence would resolve it: Developing and testing an extension of the MatchEffects algorithm that can handle different types of explanations, such as saliency maps, and evaluating its performance on a variety of models and explainers.

### Open Question 3
- Question: How do the proposed evaluation methods account for the potential presence of redundant or irrelevant features in the input data?
- Basis in paper: [inferred] The paper generates synthetic models with dummy features, but it does not explicitly address how the evaluation methods handle redundant or irrelevant features in real-world datasets.
- Why unresolved: The presence of redundant or irrelevant features can affect the performance of explainers and the accuracy of the evaluation, but the paper does not discuss this issue.
- What evidence would resolve it: Evaluating the proposed methods on real-world datasets with known redundant or irrelevant features, and analyzing how the presence of these features affects the evaluation results.

## Limitations

- The evaluation methodology depends critically on the MatchEffects algorithm's ability to correctly identify and group related effects, but this algorithm is not fully validated against ground truth for complex interaction patterns.
- Synthetic model generation uses random non-additive operators whose specific characteristics and distributions are not fully specified, potentially affecting result generalizability.
- The evaluation focuses specifically on feature-additive models, limiting applicability to non-additive or more complex model classes.

## Confidence

- **High confidence**: SHAP consistently outperforms other explainers across both synthetic and real-world datasets; all explainers show declining performance with increasing dimensionality and interaction complexity.
- **Medium confidence**: The MatchEffects algorithm provides meaningful effect alignment for evaluating explainer fidelity; equivalence relations enable fair comparison between model and explainer contributions.
- **Low confidence**: The specific quantitative thresholds (e.g., when MaIoU becomes uninformative) are not fully validated across diverse model types and datasets.

## Next Checks

1. **Algorithm validation**: Test MatchEffects on synthetic models with known, complex interaction patterns to verify correct grouping of inter-dependent effects.
2. **Robustness analysis**: Vary the specific operators and their weights in synthetic model generation to assess sensitivity of results to model complexity characteristics.
3. **Generalizability test**: Apply the evaluation framework to non-additive model classes (e.g., tree ensembles with complex interactions) to assess limitations of the feature-additive approach.