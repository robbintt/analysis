---
ver: rpa2
title: Task-Specific Skill Localization in Fine-tuned Language Models
arxiv_id: '2302.06600'
source_url: https://arxiv.org/abs/2302.06600
tags:
- grafting
- tasks
- task
- parameters
- regions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies where task-specific skills acquired via fine-tuning
  reside in large language models. It introduces model grafting: a small subset of
  parameters (as small as 0.01% of the model) can be identified such that transferring
  their values from the fine-tuned model to the pre-trained model recovers nearly
  the same performance as the full fine-tuned model.'
---

# Task-Specific Skill Localization in Fine-tuned Language Models

## Quick Facts
- arXiv ID: 2302.06600
- Source URL: https://arxiv.org/abs/2302.06600
- Reference count: 40
- Primary result: Task-specific skills in fine-tuned models localize to sparse parameter subsets (as small as 0.01%), enabling parameter-efficient skill transfer without retraining

## Executive Summary
This paper introduces model grafting, a method to identify sparse parameter subsets in fine-tuned language models that capture task-specific skills. By optimizing to find the smallest region whose transfer from fine-tuned to pre-trained models recovers most of the fine-tuned performance, the authors demonstrate that as little as 0.01% of parameters can capture >95% of task performance. The approach provides a post-hoc, parameter-efficient way to understand fine-tuned models and offers benefits including improved calibration (40-90% reduction in ECE) and better out-of-distribution generalization.

## Method Summary
The method fine-tunes a pre-trained model on a downstream task, then identifies a sparse grafting region by optimizing for the smallest parameter subset whose values from the fine-tuned model, when transferred to the pre-trained model, recover most of the fine-tuned performance. The optimization starts with parameters showing the largest movement during fine-tuning and iteratively refines the region using gradient-based optimization. The resulting grafted model achieves performance comparable to the full fine-tuned model while being more parameter-efficient and better calibrated.

## Key Results
- Sparse grafting regions as small as 0.01% of parameters can recover >95% of fine-tuned model performance
- Grafted models show 40-90% reduction in Expected Calibration Error (ECE) compared to fine-tuned models
- Skills for different tasks in multi-task settings localize in nearly disjoint regions, with overlap correlating to task similarity
- Grafted models often outperform fine-tuned models on out-of-distribution data while maintaining comparable in-distribution performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific skills in fine-tuned models are localized in sparse parameter subsets
- Mechanism: A small fraction of parameters (0.01%) can be identified such that transferring their values from the fine-tuned model to the pre-trained model recovers most of the fine-tuned performance without further training
- Core assumption: The fine-tuned model's skill is compressible into a sparse set of parameters that capture the core task-relevant knowledge
- Evidence anchors:
  - [abstract] "a very small subset of parameters (âˆ¼ 0.01% of model parameters) responsible for (> 95%) of the model's performance"
  - [section] "a tiny subset of parameters whose values are sufficient to solve the task"
  - [corpus] "Sparse Pre-training and Dense Fine-tuning for Large Language Models" - weak evidence, no direct discussion of sparse localization
- Break condition: If the task requires very distributed computation or the fine-tuning changes too many parameters, the localization might fail

### Mechanism 2
- Claim: Grafted models have better calibration and out-of-distribution generalization
- Mechanism: The sparse grafting regions capture "core" skills while avoiding overfitting to dataset-specific idiosyncrasies
- Core assumption: Sparsity in the grafting region correlates with better generalization and calibration
- Evidence anchors:
  - [abstract] "grafted models are better calibrated (40-90% reduction in ECE) and often have better out-of-distribution generalization"
  - [section] "grafted models may suffer a little on ID performance but match or significantly outperform vanilla fine-tuning on OOD"
  - [corpus] "Outlier Dimensions Encode Task-Specific Knowledge" - weak evidence, discusses outlier dimensions but not calibration benefits
- Break condition: If the dataset is very small or the task is highly complex, the calibration benefit might diminish

### Mechanism 3
- Claim: In multi-task models, skills for different tasks localize in nearly disjoint regions
- Mechanism: When fine-tuning on multiple tasks together, the model learns to localize skills for each task in separate parameter regions, with overlap indicating task similarity
- Core assumption: The model naturally organizes task-specific knowledge into separate regions when trained on multiple tasks
- Evidence anchors:
  - [abstract] "In multi-task settings, skills for different tasks localize in nearly disjoint regions, with overlap correlating with task similarity"
  - [section] "skills from different tasks localize in somewhat disjoint regions, where the degree of overlap between regions for two tasks seems to correlate with their intuitive similarity"
  - [corpus] "TaSL: Continual Dialog State Tracking via Task Skill Localization and Consolidation" - weak evidence, focuses on continual learning rather than multi-task localization
- Break condition: If tasks are very similar or share many underlying skills, the regions might overlap significantly

## Foundational Learning

- Concept: Fine-tuning and parameter-efficient fine-tuning
  - Why needed here: The paper compares standard fine-tuning with parameter-efficient methods like BitFit and lottery tickets
  - Quick check question: What's the main difference between standard fine-tuning and parameter-efficient fine-tuning?

- Concept: Model calibration and ECE metric
  - Why needed here: The paper measures calibration error using ECE and shows grafted models have better calibration
  - Quick check question: How does ECE measure calibration error in classification models?

- Concept: Out-of-distribution generalization
  - Why needed here: The paper tests how well models generalize to data distributions different from training
  - Quick check question: What's the difference between in-distribution and out-of-distribution performance?

## Architecture Onboarding

- Component map:
  Pre-trained model -> Fine-tuned model -> Optimization for grafting region -> Grafted model -> Evaluation

- Critical path:
  1. Fine-tune pre-trained model on downstream task
  2. Identify sparse grafting region using optimization
  3. Create grafted model by replacing pre-trained parameters with fine-tuned ones in the region
  4. Evaluate performance, calibration, and OOD generalization

- Design tradeoffs:
  - Sparsity level vs. performance recovery
  - Grafting without retraining vs. re-training the region
  - Single-task vs. multi-task localization

- Failure signatures:
  - Low sparsity levels fail to recover performance
  - Calibration benefits disappear with re-training
  - OOD generalization doesn't improve for similar tasks

- First 3 experiments:
  1. Fine-tune RoBERTa on SST-2, identify grafting region, measure performance recovery
  2. Compare calibration (ECE) between fine-tuned and grafted models
  3. Test OOD performance by evaluating on IMDb when trained on SST-2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we develop a method to find a single model that performs well on any subset of tasks without training separate models for each subset?
- Basis in paper: [inferred] The paper discusses finding task-specific regions and their unions, but notes that training models for all 2^m subsets of tasks would be impractical
- Why unresolved: The paper suggests this as an interesting direction for future work but doesn't provide a concrete solution
- What evidence would resolve it: A proposed algorithm that can efficiently find a single model capable of performing well on any given subset of tasks, along with experimental validation showing its effectiveness compared to training separate models

### Open Question 2
- Question: Can we decompose the identified skills into finer sub-skills to gain a deeper understanding of the model's capabilities?
- Basis in paper: [explicit] The paper mentions that understanding how to decompose identified skills into finer skills is a potential future direction
- Why unresolved: The paper only demonstrates skill localization at a coarse level and doesn't explore the possibility of further decomposition
- What evidence would resolve it: A method for decomposing skills into sub-skills, along with analysis showing that these sub-skills correspond to meaningful linguistic or semantic units

### Open Question 3
- Question: Can we leverage skill localization for federated learning to enable more efficient and private model training across multiple devices?
- Basis in paper: [inferred] The paper mentions federated learning as a potential application of skill localization but doesn't explore it further
- Why unresolved: The paper doesn't discuss how skill localization could be applied in a federated learning setting or what benefits it might provide
- What evidence would resolve it: An experimental study comparing federated learning with and without skill localization, showing improvements in communication efficiency, privacy preservation, or model performance

## Limitations

- Sparse localization universality: Results primarily demonstrated on sentiment classification and NLI tasks; effectiveness on generation, structured prediction, and other diverse NLP tasks remains uncertain
- Optimization stability: Fixed hyperparameters (learning rate 1e-7, 100 steps) may not generalize across different model sizes and tasks
- Cross-model applicability: Results mainly shown on RoBERTa-base and GPT-2; effectiveness on larger models (GPT-3, LLaMA) or different architectures (T5, BERT) requires validation

## Confidence

- High Confidence: The core claim that task-specific skills can be localized in sparse parameter subsets is well-supported by experimental results showing >95% performance recovery with <0.01% parameters
- Medium Confidence: Claims about improved calibration (40-90% ECE reduction) and OOD generalization are supported but could benefit from more diverse task evaluations and statistical significance testing
- Medium Confidence: Multi-task localization results showing disjoint regions with overlap correlating to task similarity are intriguing but based on a limited set of task pairs; broader task diversity would strengthen this claim

## Next Checks

1. Cross-task generalization test: Apply the grafting method to a diverse set of 10+ NLP tasks including generation, structured prediction, and QA to evaluate the universality of skill localization across different task types

2. Hyperparameter sensitivity analysis: Systematically vary the optimization learning rate (1e-6 to 1e-8) and step count (50-200) to establish robust hyperparameter settings and identify failure modes

3. Larger model evaluation: Apply the grafting approach to GPT-3-175B and LLaMA-65B models to test whether the sparse localization phenomenon scales to frontier models and to compare sparsity requirements across model scales