---
ver: rpa2
title: 'The Scenario Refiner: Grounding subjects in images at the morphological level'
arxiv_id: '2309.11252'
source_url: https://arxiv.org/abs/2309.11252
tags:
- noun
- verb
- human
- visual
- derived
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines whether vision-and-language models capture\
  \ morphological distinctions between agentive nouns (e.g., \u201Crunner\u201D) and\
  \ their corresponding verbs (e.g., \u201Crunning\u201D). The authors created a dataset\
  \ of 108 images, each paired with both a verb and derived noun caption, and collected\
  \ human judgments on Likert scales."
---

# The Scenario Refiner: Grounding subjects in images at the morphological level

## Quick Facts
- arXiv ID: 2309.11252
- Source URL: https://arxiv.org/abs/2309.11252
- Authors: 
- Reference count: 9
- Primary result: Humans prefer verb captions over agentive nouns in 91.7% of cases, while vision-language models show a significant bias toward nouns

## Executive Summary
This paper investigates whether vision-and-language models can capture morphological distinctions between agentive nouns (e.g., "runner") and their corresponding verbs (e.g., "running"). The authors created a dataset of 108 images with paired verb and noun captions and collected human judgments on Likert scales. Testing three models (CLIP, ViLT, and LXMERT) revealed that while humans strongly prefer verb captions, models systematically favor agentive nouns, showing a significant grammatical bias. The correlation between human and model judgments was moderate to weak overall, with ViLT showing the strongest alignment.

## Method Summary
The study employed a controlled experimental design using 108 images, each paired with both a verb and derived noun caption. Human participants rated the captions on seven-point Likert scales, providing preference data. Three vision-language models were then evaluated using zero-shot testing: CLIP with different visual backbones, ViLT (single-stream architecture), and LXMERT (dual-stream with object detector). Model performance was assessed by computing image-text alignment scores and comparing them to human judgments using Pearson correlation coefficients.

## Key Results
- Humans preferred verb captions over agentive nouns in 91.7% of cases
- Models showed systematic bias toward agentive nouns (48-53% of cases)
- ViLT demonstrated the strongest correlation with human judgments among the tested models
- CLIP performance varied significantly with visual backbone choice, with ViT-L/14 showing best alignment to human preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Verb-based captions align better with human perception of dynamic action scenes, while agentive nouns require additional visual cues (tools, outfits, environment).
- Mechanism: Human cognition associates agentive nouns with stereotypical roles, requiring contextual cues beyond the basic action; models without explicit morphological grounding default to action-centric interpretations.
- Core assumption: Human preference for verbs over nouns reflects deeper grounding in real-world scene interpretation rather than linguistic form alone.
- Evidence anchors:
  - [abstract] "humans prefer verb captions 91.7% of the time, while models prefer agentive nouns in 48–53% of cases, revealing a significant grammatical bias in models"
  - [section 4.1] "Participants prefer the derived noun only for a few instances that had additional characteristics elicited by visual elements, or by the kind of action performed by the human subjects in the images"
  - [corpus] Weak: no direct corpus citations for morphological grounding; evidence is drawn from experimental design
- Break condition: If visual cues are ambiguous or absent, both humans and models may default to action-based interpretation, reducing the morphological contrast.

### Mechanism 2
- Claim: CLIP's performance varies with visual backbone; ViT-L/14 aligns more closely with human judgments than ViT-B/32 or RN50.
- Mechanism: Different visual encoders capture different levels of scene detail; ViT-L/14's higher resolution and attention pooling better preserve contextual cues needed for morphological distinction.
- Core assumption: The choice of visual backbone significantly impacts the model's ability to extract fine-grained visual features relevant to morphological grounding.
- Evidence anchors:
  - [section 4.1] "The performance of CLIP seems to depend on the visual backbone. Of the three versions, ViT-L/14 displays the greatest similarity to human judgments"
  - [section 2.1] CLIP employs ResNet50 with attention pooling and ViT with modified layer normalization
  - [corpus] Weak: no corpus citations for visual backbone comparisons; inference from experimental results
- Break condition: If the dataset images are too small or low-resolution, even high-capacity backbones may fail to capture necessary details.

### Mechanism 3
- Claim: ViLT's single-stream architecture correlates better with human judgments in some domains, suggesting simpler modality interaction may better preserve morphological distinctions.
- Mechanism: By processing visual and textual features together from the start, ViLT may avoid over-specialization that dual-stream models develop during pretraining.
- Core assumption: Early fusion of modalities preserves cross-modal alignment cues that dual-stream models may lose through separate processing.
- Evidence anchors:
  - [section 4.2] "ViLT emerges as the most correlated model with human judgement in the verbal evaluation, but it exhibits the least correlation in the evaluation of the derived noun"
  - [section 2.1] ViLT is a single-stream model processing concatenated visual and textual features directly
  - [corpus] Weak: no corpus citations for single- vs dual-stream comparisons; inference from correlation results
- Break condition: If pretraining objectives strongly favor one modality, single-stream models may still develop biases despite early fusion.

## Foundational Learning

- Concept: Morphological derivation (verb → agentive noun) and its semantic/pragmatic implications
  - Why needed here: The study hinges on understanding how morphological changes alter meaning and visual expectations
  - Quick check question: What additional visual information would you expect to see in an image captioned "baker" versus "baking"?

- Concept: Cross-modal alignment and zero-shot evaluation in vision-language models
  - Why needed here: The methodology relies on comparing model image-text alignment scores to human Likert judgments
  - Quick check question: How would you interpret a CLIP model assigning higher cosine similarity to a verb caption than a noun caption for the same image?

- Concept: Likert scale data analysis and correlation statistics
  - Why needed here: Human judgments are collected on 7-point scales and compared to model outputs using Pearson correlation
  - Quick check question: If human judgments for a verb caption average 6.5 with SD 0.8, what does this tell you about consensus?

## Architecture Onboarding

- Component map: CLIP (dual encoder with visual backbone) -> ViLT (single-stream pixel-level) -> LXMERT (dual-stream with object detector)
- Critical path: Load dataset → generate noun/verb caption pairs → compute model alignment scores → compare to human Likert ratings → calculate correlations
- Design tradeoffs: Single-stream (ViLT) may preserve morphological nuances but lacks pretraining efficiency; dual-stream (LXMERT) has rich object features but may overfit to pretraining tasks; dual encoder (CLIP) is scalable but backbone-dependent
- Failure signatures: Low correlation between human and model judgments; systematic preference for nouns over verbs (or vice versa); domain-specific performance drops
- First 3 experiments:
  1. Replicate correlation analysis with additional visual backbones for CLIP
  2. Test ViLT with different pretraining objectives to see if morphological grounding improves
  3. Conduct ablation study removing object detection from LXMERT to isolate its effect on morphological understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different visual backbones (ResNet50 vs Vision Transformer) impact the morphological grounding ability of CLIP models, and which architectural choice is optimal for capturing derivationally related verb-noun distinctions?
- Basis in paper: [explicit] The paper explicitly compares three versions of CLIP with different visual backbones (ResNet50, ViT-B/32, ViT-L/14) and observes that ViT-L/14 shows the greatest similarity to human judgments.
- Why unresolved: The paper only provides preliminary observations about backbone performance differences but doesn't conduct a systematic analysis of how architectural choices affect morphological grounding capabilities.
- What evidence would resolve it: A controlled experiment varying only the visual backbone while keeping all other factors constant, measuring performance on the morphological distinction task across multiple model versions.

### Open Question 2
- Question: Do vision-language models exhibit similar morphological biases when tested on other derivational contrasts beyond the -er suffix agentive nouns (e.g., nominalizations, deadjectival nouns, verbal participles)?
- Basis in paper: [explicit] The authors state "Our methodology, developed on one specific morphological contrast, can be further extended for testing models on capturing other nuanced language features."
- Why unresolved: The paper only tests one specific morphological contrast (-er suffix), limiting generalizability of findings to other morphological phenomena.
- What evidence would resolve it: Applying the same methodology to other morphological contrasts (e.g., -ion nominalizations, -ive adjectives, -ing participles) and comparing model performance across these different morphological categories.

### Open Question 3
- Question: What specific training data characteristics (e.g., frequency of agentive vs. verbal descriptions in image captions) might explain the observed grammatical bias in vision-language models?
- Basis in paper: [inferred] The models show a bias toward agentive nouns, suggesting their training data may have certain distributional properties that favor this pattern.
- Why unresolved: The paper doesn't analyze the training data characteristics of the models or investigate whether distributional differences in training corpora explain the observed biases.
- What evidence would resolve it: Corpus analysis of the training data for these models, examining the frequency and distribution of agentive nominal versus verbal descriptions in image-text pairs, and correlating these statistics with model performance.

## Limitations
- Small dataset size (108 images) may not capture full variability of morphological distinctions
- Human judgment collection via Amazon Mechanical Turk may introduce cultural and linguistic biases
- Zero-shot evaluation without fine-tuning limits understanding of potential improvements

## Confidence
- High confidence: Human preference for verb captions over agentive nouns (91.7%)
- Medium confidence: Models systematically prefer agentive nouns
- Medium confidence: Comparative performance of different model architectures

## Next Checks
1. Expand dataset to include diverse cultural contexts and varied morphological transformations
2. Conduct fine-tuning experiments on models using morphologically annotated data
3. Test additional model architectures with explicit morphological reasoning capabilities