---
ver: rpa2
title: 'IruMozhi: Automatically classifying diglossia in Tamil'
arxiv_id: '2311.07804'
source_url: https://arxiv.org/abs/2311.07804
tags:
- tamil
- spoken
- literary
- language
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce IruMozhi, a human-annotated dataset of parallel text
  in Literary and Spoken Tamil. Using this data, we train classifiers to distinguish
  between the two registers.
---

# IruMozhi: Automatically classifying diglossia in Tamil

## Quick Facts
- arXiv ID: 2311.07804
- Source URL: https://arxiv.org/abs/2311.07804
- Reference count: 18
- Key outcome: Models can reliably classify Tamil diglossic register, but don't generalize well to unseen Literary Tamil text

## Executive Summary
This paper introduces IruMozhi, a human-annotated dataset of parallel text in Literary and Spoken Tamil. Using this data, the authors train classifiers to distinguish between the two registers and audit existing Tamil NLP datasets to assess their register composition. They find that while current models can classify diglossia reliably, there's a significant imbalance in available datasets, with most containing primarily Spoken Tamil. The work highlights the need for more balanced datasets covering both registers of Tamil to improve NLP system performance across all domains.

## Method Summary
The authors created IruMozhi by collecting parallel sentences from Wikipedia, annotating them for register (Literary vs Spoken Tamil), and augmenting with orthographic variants. They trained both Naïve Bayes classifiers and XLM-R transformer models on this data, then used these models to audit existing Tamil datasets including pretraining corpora like CC-100. The approach combines linguistic analysis of diglossic features with machine learning techniques for text classification.

## Key Results
- Classification models achieve high accuracy on IruMozhi test set (specific numbers not provided in abstract)
- Most available Tamil NLP datasets contain primarily Spoken Tamil
- Pretraining corpora like CC-100 contain more Literary Tamil than expected
- Models trained only on IruMozhi don't generalize well to unseen Literary Tamil text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diglossia classification works because literary and spoken Tamil exhibit consistent morphological and phonological differences that can be captured by n-gram models.
- Mechanism: The models detect systematic orthographic and phonetic transformations between registers, such as vowel raising, word-final consonant changes, and lexical substitutions.
- Core assumption: These linguistic differences are stable enough across different domains and annotators to be learned by statistical models.
- Evidence anchors:
  - [abstract] "Using this data, we train classifiers to distinguish between the two registers."
  - [section] "Vowels, both short monophthongs and diphthongs, are regularly raised in the word-final position... Word-final /u/ (with the exception of names) is shortened to [W]"
  - [corpus] Weak - the corpus provides examples but doesn't explicitly validate stability across domains
- Break condition: If regional dialectal variation overwhelms register-specific patterns, or if annotators introduce inconsistent transformations

### Mechanism 2
- Claim: XLM-R's success indicates that pretraining on CC-100 included sufficient spoken Tamil data to learn register distinctions.
- Mechanism: The multilingual transformer model captured cross-register patterns during pretraining, allowing finetuning to quickly adapt to the classification task.
- Core assumption: The CC-100 corpus contains enough representative spoken Tamil data for the model to learn meaningful register distinctions
- Evidence anchors:
  - [abstract] "We use these models to gauge the availability of pretraining data in Spoken Tamil"
  - [section] "testing the first 50k lines, we find a surprisingly high portion of Spoken Tamil in the CC-100 ta_rom split"
  - [corpus] Weak - corpus doesn't provide details on CC-100 composition or spoken Tamil prevalence
- Break condition: If CC-100 contains mostly literary Tamil or if register distinctions are too subtle for transformer models to capture

### Mechanism 3
- Claim: Data augmentation through orthographic variant generation prevents overfitting to a single transliteration standard.
- Mechanism: By creating multiple orthographic representations of the same linguistic content, models learn to focus on register-specific patterns rather than specific transliteration choices.
- Core assumption: Orthographic variants preserve the underlying register distinctions while providing diversity in surface forms
- Evidence anchors:
  - [section] "we design rules to augment all our data with orthographic variants, resulting in 6,224 Spoken Tamil and 2,410 Literary Tamil sentences"
  - [section] "One issue is that IruMozhi uses automatically converted Literary Tamil"
  - [corpus] Weak - corpus doesn't validate whether augmentation preserves register information or introduce noise
- Break condition: If augmentation rules introduce register-ambiguous variants or if the original data has too little diversity

## Foundational Learning

- Concept: Diglossia and register variation
  - Why needed here: Understanding the linguistic phenomenon being modeled is crucial for interpreting results and designing appropriate evaluation
  - Quick check question: What distinguishes literary from spoken Tamil beyond just formality level?

- Concept: Feature engineering for text classification
  - Why needed here: The choice between character vs word n-grams and Gaussian vs Multinomial Naïve Bayes directly impacts model performance
  - Quick check question: Why might character n-grams be more effective than word n-grams for this task?

- Concept: Cross-lingual representation learning
  - Why needed here: Understanding how XLM-R captures language-specific patterns helps explain why pretraining data matters
  - Quick check question: How does multilingual pretraining help with language-specific tasks like register classification?

## Architecture Onboarding

- Component map: Data collection → Transliteration → Annotation → Augmentation → Model training → Auditing → Evaluation
- Critical path: Parallel corpus creation → Model training on IruMozhi → Auditing existing datasets → Generalization testing
- Design tradeoffs: 
  - Orthographic standardization vs. linguistic authenticity
  - Domain-specific vs. general register models
  - Model complexity vs. interpretability
- Failure signatures:
  - Overfitting to specific annotators' styles
  - Poor generalization to new domains
  - Inability to handle code-switching
- First 3 experiments:
  1. Train Naïve Bayes with different n-gram configurations on IruMozhi and evaluate on held-out test
  2. Fine-tune XLM-R on IruMozhi and test generalization to Dakshina
  3. Audit a new Tamil dataset using trained models to estimate register composition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do models trained on IruMozhi generalize to other dialects of Spoken Tamil beyond the central Tamil Nadu variety?
- Basis in paper: [inferred] The paper mentions that Spoken Tamil has dialectal variation by geography and caste, and the annotators are from Salem, Tamil Nadu. However, the evaluation only uses models trained on IruMozhi to audit existing datasets without explicitly testing generalization to other dialects.
- Why unresolved: The paper does not conduct experiments to evaluate model performance on Spoken Tamil from different regions or social groups.
- What evidence would resolve it: Experiments testing IruMozhi-trained models on Spoken Tamil data from other Tamil Nadu regions or different social groups would show how well the models generalize beyond the central Tamil Nadu dialect.

### Open Question 2
- Question: What is the impact of using automatically transliterated Literary Tamil versus human-transliterated Literary Tamil on model performance and generalization?
- Basis in paper: [explicit] The paper notes that IruMozhi uses automatically transliterated Literary Tamil and uses the Dakshina dataset (with human-transliterated Literary Tamil) to measure generalization. However, it does not directly compare model performance on automatically vs. human-transliterated Literary Tamil.
- Why unresolved: The paper does not conduct controlled experiments comparing model performance when trained on automatically vs. human-transliterated Literary Tamil.
- What evidence would resolve it: Experiments training models on both automatically and human-transliterated Literary Tamil and comparing their performance on various test sets would reveal the impact of transliteration method on model behavior.

### Open Question 3
- Question: How effective are style transfer models in converting between Literary and Spoken Tamil, and what linguistic features do they capture or miss?
- Basis in paper: [explicit] The paper mentions plans to train style transfer models for the two varieties as future work but does not present any results or analysis of such models.
- Why unresolved: The paper only introduces the IruMozhi dataset and classification models, without developing or evaluating style transfer models.
- What evidence would resolve it: Developing and evaluating style transfer models using IruMozhi, with analysis of which linguistic features (phonological, morphological, lexical) are successfully transferred and which are not, would answer this question.

## Limitations
- Data Representation Issues: IruMozhi contains only 499 parallel sentences and relies on automatic conversion for Literary Tamil
- Generalization Concerns: Models trained on IruMozhi don't generalize well to unseen Literary Tamil text
- Pretraining Data Uncertainty: Analysis of CC-100's composition relies on indirect inference rather than direct analysis

## Confidence

**High Confidence**: The core finding that current NLP systems can reliably classify diglossic register in Tamil when trained on appropriate data.

**Medium Confidence**: The audit results showing that most available Tamil NLP datasets contain primarily Spoken Tamil.

**Low Confidence**: The claim about CC-100 containing significant Spoken Tamil data, based on model inference rather than direct corpus analysis.

## Next Checks

1. Test the trained models on a diverse set of Tamil texts from different domains (literature, news, social media, formal documents) to assess whether the register classification generalizes beyond Wikipedia content.

2. Calculate inter-annotator agreement scores for the IruMozhi dataset to quantify the consistency of register annotations and identify potential sources of model confusion.

3. Conduct a manual analysis of the CC-100 Tamil split to directly verify the distribution of Literary vs Spoken Tamil, rather than relying solely on model-based inference.