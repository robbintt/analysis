---
ver: rpa2
title: 'ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language
  Models'
arxiv_id: '2308.14353'
source_url: https://arxiv.org/abs/2308.14353
tags:
- evaluation
- llms
- ability
- wang
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ZhuJiu is a comprehensive Chinese benchmark for evaluating large
  language models across seven dimensions (knowledge, Chinese-specific, language,
  reasoning, refusal, safety, robustness) using 51 tasks and three evaluation methods
  (Metrics, Scoring, Comparative). To address data leakage, 37 evaluation datasets
  were constructed using ChatGPT.
---

# ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models

## Quick Facts
- **arXiv ID:** 2308.14353
- **Source URL:** https://arxiv.org/abs/2308.14353
- **Authors:** Multiple researchers from Chinese institutions
- **Reference count:** 6
- **Primary result:** ChatGLM2 achieved highest overall score of 91.1, with GPT-3.5-turbo scoring 93.2 (reference only)

## Executive Summary
ZhuJiu is a comprehensive Chinese benchmark designed to evaluate large language models across seven distinct ability dimensions using 51 tasks and three complementary evaluation methods. To address data leakage concerns common in LLM evaluation, the benchmark includes 37 newly constructed datasets alongside 14 public datasets. The evaluation covers knowledge, Chinese-specific, language, reasoning, refusal, safety, and robustness abilities, providing a holistic assessment of model capabilities. ChatGLM2 achieved the highest overall score of 91.1, while the evaluation platform offers public access with visualization, model arena participation, and submission features.

## Method Summary
ZhuJiu employs a three-method evaluation approach: Metrics Evaluation based on HELM framework, Scoring Evaluation using ChatGPT for scoring, and Comparative Evaluation through human-based model arena. The benchmark evaluates 10 mainstream LLMs across 7 ability dimensions (knowledge, Chinese-specific, language, reasoning, refusal, safety, robustness) using 51 tasks. To prevent data leakage, 37 evaluation datasets were specifically constructed using ChatGPT with seed data and manual review. The overall assessment produces ten-point scores with detailed breakdowns across dimensions and methods.

## Key Results
- ChatGLM2 achieved the highest overall score of 91.1 among evaluated models
- GPT-3.5-turbo scored 93.2 but is marked as reference only
- Model size strongly influences performance across all ability dimensions
- Knowledge ability remains a key challenge for Chinese LLMs, with no model achieving perfect scores
- Lower ability limits constrain overall model potential regardless of size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dimensional ability coverage improves evaluation comprehensiveness by reducing blind spots in single-dimension benchmarks.
- Mechanism: Evaluating across 7 distinct ability dimensions (knowledge, Chinese-specific, language, reasoning, refusal, safety, robustness) ensures that different aspects of LLM capabilities are measured, preventing overemphasis on any single dimension.
- Core assumption: Different LLM tasks require different abilities, and no single dimension captures overall capability.
- Evidence anchors:
  - [abstract] "comprehensive and systematic" evaluation across "7 ability dimensions covering 51 tasks"
  - [section 2.3] "We will comprehensively evaluate the model from seven ability dimensions and 3 assessment methods"
  - [corpus] Weak evidence - corpus shows related Chinese benchmarks but doesn't directly confirm this mechanism
- Break condition: If one dimension is disproportionately easier/harder than others, scores may not reflect true overall capability.

### Mechanism 2
- Claim: Multi-faceted evaluation methods collaboration increases evaluation reliability through complementary strengths.
- Mechanism: Using Metrics Evaluation, Scoring Evaluation, and Comparative Evaluation together compensates for individual method limitations, creating more robust results than any single method alone.
- Core assumption: Different evaluation methods capture different aspects of model performance and have complementary weaknesses.
- Evidence anchors:
  - [abstract] "3 different yet complementary evaluation methods to comprehensively evaluate LLMs"
  - [section 2.1] "we employ a collaborative evaluation approach that utilizes 3 types of evaluation methods"
  - [corpus] No direct corpus evidence supporting this specific claim about method complementarity
- Break condition: If methods are highly correlated or redundant, adding more methods won't improve reliability.

### Mechanism 3
- Claim: Constructing 37 evaluation datasets prevents data leakage and ensures fair evaluation.
- Mechanism: By generating evaluation data specifically for each task rather than reusing public datasets, the benchmark avoids the problem of models being trained on evaluation data.
- Core assumption: Public datasets may be contaminated with training data, making evaluations unfair.
- Evidence anchors:
  - [abstract] "To avoid data leakage, we construct evaluation data specifically for 37 tasks"
  - [section 2.2.2] "we construct corresponding evaluation datasets for 37 specific tasks"
  - [corpus] Weak evidence - corpus shows related Chinese benchmarks but doesn't confirm data leakage prevention
- Break condition: If the constructed datasets are too different from real-world data, evaluations may not be representative.

## Foundational Learning

- Concept: Multi-dimensional evaluation frameworks
  - Why needed here: Understanding how to structure comprehensive evaluations across different capability dimensions
  - Quick check question: What are the advantages and disadvantages of evaluating across multiple dimensions versus focusing deeply on one?

- Concept: Evaluation method complementarity
  - Why needed here: Knowing how different evaluation approaches can work together to provide more reliable results
  - Quick check question: How might metrics evaluation, scoring evaluation, and comparative evaluation each capture different aspects of model performance?

- Concept: Data leakage prevention in LLM evaluation
  - Why needed here: Understanding why using only public datasets can lead to inflated scores and how to construct fair evaluation data
  - Quick check question: What are the risks of evaluating LLMs on datasets that might have been used during training?

## Architecture Onboarding

- Component map: 7 ability dimensions × 3 evaluation methods = 21 evaluation components, plus 51 datasets (14 public + 37 constructed) and an online platform for visualization and model submission
- Critical path: Dataset construction → Evaluation method implementation → Score aggregation → Platform deployment → Results visualization
- Design tradeoffs: More comprehensive evaluation vs. increased complexity; more datasets vs. construction effort; public vs. constructed data
- Failure signatures: Inconsistent scores across evaluation methods; high correlation between different dimensions; platform performance issues with large-scale evaluations
- First 3 experiments:
  1. Test single dimension evaluation with all three methods to verify consistency
  2. Compare scores using only public datasets vs. constructed datasets to validate leakage prevention
  3. Run small-scale evaluation on 2-3 models to test platform functionality and score calculation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ZhuJiu's knowledge evaluation framework compare to existing benchmarks like KOLA in terms of comprehensiveness and granularity?
- Basis in paper: [explicit] The paper states that ZhuJiu evaluates knowledge ability across four perspectives (world knowledge, commonsense knowledge, linguistic knowledge, concept) with specific properties (accuracy, robustness, completeness, timeliness), while claiming it is broader than KOLA.
- Why unresolved: The paper mentions the comparison but doesn't provide a detailed quantitative or qualitative analysis of how ZhuJiu's framework differs from or improves upon KOLA in specific aspects.
- What evidence would resolve it: A side-by-side comparison of evaluation dimensions, properties, and dataset coverage between ZhuJiu and KOLA, along with empirical results showing ZhuJiu's effectiveness in detecting knowledge-related issues that KOLA might miss.

### Open Question 2
- Question: What is the exact methodology used to construct the 37 evaluation datasets to prevent data leakage, and how was ChatGPT's role in this process controlled to avoid introducing new biases?
- Basis in paper: [explicit] The paper states that 37 datasets were constructed using ChatGPT with seed data and manual review, but doesn't detail the specific construction process or bias mitigation strategies.
- Why unresolved: The methodology is mentioned but not fully described, leaving questions about the potential for ChatGPT to introduce its own biases or knowledge into the evaluation datasets.
- What evidence would resolve it: A detailed description of the dataset construction pipeline, including seed selection criteria, prompt generation process, manual review procedures, and validation checks to ensure the constructed datasets are both challenging and unbiased.

### Open Question 3
- Question: How does the three-method evaluation approach (Metrics, Scoring, Comparative) in ZhuJiu correlate with each other, and what is the empirical evidence that this combination provides more reliable results than single-method evaluations?
- Basis in paper: [explicit] The paper claims that using three complementary evaluation methods ensures authoritative and accurate results, but doesn't provide correlation analysis or comparative studies with single-method approaches.
- Why unresolved: While the theoretical advantage of multi-method evaluation is stated, there's no empirical validation showing that this approach actually produces more reliable or consistent results compared to traditional single-method benchmarks.
- What evidence would resolve it: Statistical correlation analysis between the three evaluation methods across all tasks, along with controlled experiments comparing ZhuJiu's results with equivalent single-method evaluations on the same models.

## Limitations

- Constructed datasets' quality and diversity remain unverified, as only the methodology is described but not the actual content or validation results
- GPT-3.5-turbo results (93.2) are marked as reference only, making direct comparisons with ChatGLM2's 91.1 score potentially misleading
- The correlation between different ability dimensions is not analyzed, raising questions about whether the multi-dimensional approach provides truly independent assessment
- The human evaluation component's scale and potential biases are not specified, limiting understanding of the comparative evaluation method's reliability

## Confidence

- High confidence: The benchmark successfully implements multi-dimensional evaluation across 7 ability dimensions with 3 complementary methods
- Medium confidence: The constructed datasets effectively prevent data leakage and provide fair evaluation
- Medium confidence: Model size strongly influences performance and lower ability limits constrain overall potential
- Low confidence: The overall benchmark represents the most comprehensive Chinese evaluation framework currently available

## Next Checks

1. Analyze correlation matrices between the 7 ability dimensions to verify independence of assessment
2. Conduct ablation studies comparing scores using only public datasets versus the full set including constructed datasets
3. Perform inter-rater reliability analysis on the human evaluation component to quantify consistency and potential biases