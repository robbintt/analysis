---
ver: rpa2
title: Protein Design with Guided Discrete Diffusion
arxiv_id: '2305.20009'
source_url: https://arxiv.org/abs/2305.20009
tags:
- diffusion
- sequence
- sampling
- design
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NOS, a gradient-guided sampling method for
  discrete diffusion models that operates directly in sequence space for protein design.
  By following gradients in the hidden states of the denoising network, NOS enables
  controllable sampling from discrete diffusion models without the need for structure-based
  inverse folding.
---

# Protein Design with Guided Discrete Diffusion

## Quick Facts
- arXiv ID: 2305.20009
- Source URL: https://arxiv.org/abs/2305.20009
- Reference count: 40
- Primary result: NOS enables controllable sampling from discrete diffusion models without inverse folding, achieving 97% expression rates and 25% binding rates in antibody design experiments.

## Executive Summary
This paper introduces NOS (diffusioN Optimized Sampling), a gradient-guided sampling method for discrete diffusion models that operates directly in sequence space for protein design. By following gradients in the hidden states of the denoising network, NOS enables controllable sampling from discrete diffusion models without the need for structure-based inverse folding. The method is combined with a Bayesian optimization procedure (LaMBO-2) to enable multi-objective sequence design with edit-based constraints using saliency maps. Experimental results show that NOS and LaMBO-2 achieve high expression rates (97%) and binding rates (25%) in exploratory in vitro experiments for optimizing antibody designs.

## Method Summary
The method combines NOS (diffusioN Optimized Sampling) with LaMBO-2 (a Bayesian optimization variant) to optimize protein sequences for multiple objectives including expression yield and binding affinity. NOS applies gradient guidance in the continuous hidden states of a discrete diffusion model rather than in the discrete sequence space itself, enabling optimization without violating sequence constraints. LaMBO-2 extends Bayesian optimization to handle multi-objective problems by using saliency maps to select edit positions based on their impact on the value function, and replaces deep kernel Gaussian processes with ensemble-based uncertainty quantification. The method is evaluated on antibody design tasks using experimental validation of expression rates and binding affinities.

## Key Results
- NOS enables controllable sampling from discrete diffusion models without requiring structure-based inverse folding
- LaMBO-2 achieves 97% expression rates and 25% binding rates in exploratory in vitro experiments for antibody design
- Saliency-based edit position selection dramatically improves efficiency over random or uniform selection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** NOS improves guided sampling in discrete diffusion models by optimizing in hidden states rather than direct sequence space.
- **Mechanism:** NOS applies Langevin dynamics in the continuous hidden state space of the denoising network, allowing gradient-based optimization without violating discrete sequence constraints. This approach alternates between corruption, denoising, and gradient-guided refinement steps.
- **Core assumption:** The hidden states of the denoising network provide a meaningful continuous representation that captures dependencies between sequence positions.
- **Evidence anchors:**
  - [abstract] "NOS enables controllable sampling from discrete diffusion models without the need for structure-based inverse folding."
  - [section] "NOS generates sequences with both high likelihood and desirable qualities by taking many alternating steps between corruption, denoising, and control in the continuous latent space."
  - [corpus] "Recent studies have demonstrated the strong empirical performance of diffusion models on discrete sequences across domains from natural language to biological sequence generation."

### Mechanism 2
- **Claim:** LaMBO-2's saliency-based edit position selection dramatically improves efficiency over random or uniform selection.
- **Mechanism:** Saliency maps identify which sequence positions most influence the value function by computing gradients of the value function with respect to hidden states. Edit positions are sampled according to these saliency scores, concentrating computational resources on the most impactful positions.
- **Core assumption:** Gradient-based feature attribution reliably identifies functionally important positions in protein sequences.
- **Evidence anchors:**
  - [section] "We propose to automatically choose edit positions by computing the gradient of the value function with respect to h0 to determine which positions affect the value estimate the most."
  - [section] "In the few edit regime we find that while both interventions improve the seed's value, selecting positions using saliency has a much larger effect than guidance."
  - [corpus] "Recent advancements in conditional diffusion models have shown impressive empirical performance in protein generation tasks."

### Mechanism 3
- **Claim:** Combining NOS with LaMBO's Bayesian acquisition function enables effective multi-objective optimization in sequence space.
- **Mechanism:** The value function is constructed as an expectation of the hypervolume improvement utility over a posterior distribution of objective values, allowing the method to balance multiple competing objectives while maintaining sequence naturalness.
- **Core assumption:** The Bayesian acquisition framework can effectively navigate the discrete sequence space while accounting for uncertainty in objective measurements.
- **Evidence anchors:**
  - [abstract] "The resulting method, LaMBO-2, enables discrete diffusions and stronger performance with limited edits through a novel application of saliency maps."
  - [section] "LaMBO-2 replaces the guided MLM sampler with NOS, selects edit positions based on value saliency, and replaces the deep kernel Gaussian process with ensemble-based uncertainty quantification."
  - [corpus] "We combine NOS with saliency-based edits to create LaMBO-2, a more powerful variant of the original LaMBO algorithm."

## Foundational Learning

- **Concept:** Discrete diffusion models and their reverse process
  - Why needed here: NOS builds upon discrete diffusion models as the generative foundation for protein design.
  - Quick check question: What distinguishes continuous noise diffusion from categorical noise diffusion in the context of sequence generation?

- **Concept:** Gradient-based optimization in discrete spaces
  - Why needed here: NOS uses gradient guidance through hidden states to optimize discrete sequences, circumventing the challenges of direct gradient descent on discrete variables.
  - Quick check question: How does NOS maintain sequence naturalness while optimizing for desired attributes?

- **Concept:** Bayesian optimization for multi-objective problems
  - Why needed here: LaMBO-2 extends Bayesian optimization to handle multiple competing objectives (expression yield, binding affinity) while maintaining uncertainty quantification.
  - Quick check question: What role does the hypervolume improvement utility play in multi-objective Bayesian optimization?

## Architecture Onboarding

- **Component map:** Encoder network -> Discrete diffusion head -> Discriminative heads -> Value function -> Saliency map generator -> Langevin dynamics optimizer
- **Critical path:** 1. Encode sequence to hidden states, 2. Apply corruption and denoising steps, 3. Perform gradient-guided refinement using value function, 4. Decode to discrete sequence, 5. Evaluate using saliency-based position selection
- **Design tradeoffs:** Shared encoder vs. separate encoders for generative and discriminative tasks, Ensemble-based uncertainty vs. deep kernel Gaussian process, Continuous hidden state optimization vs. direct discrete optimization, Saliency-based position selection vs. uniform or expert-defined positions
- **Failure signatures:** Poor sequence recovery in unguided CDR infilling (indicates diffusion model issues), Low expression rates in wetlab validation (indicates value function misalignment), Saliency maps highlighting irrelevant positions (indicates feature attribution problems), Inability to balance competing objectives (indicates acquisition function issues)
- **First 3 experiments:** 1. Implement NOS sampling on a simple categorical diffusion model and verify improved trade-offs between likelihood and objective value, 2. Train saliency maps on a toy value function and verify they identify relevant positions, 3. Combine NOS with simple acquisition function (e.g., expected improvement) and test on a single-objective task before scaling to multi-objective

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several remain unresolved including how NOS performance compares to other gradient-guided sampling methods across different objectives, the impact of guidance layer choice on performance, and how saliency-based position selection compares to other methods across various protein design tasks.

## Limitations
- The paper reports promising in vitro validation rates but provides limited details on experimental protocols and sample sizes, making it difficult to assess statistical significance and reproducibility
- Computational resources required for NOS sampling and LaMBO-2 optimization are not fully characterized, raising questions about scalability
- The method's performance on protein design tasks beyond antibodies is not explored

## Confidence
- **High confidence:** The theoretical foundation of NOS as gradient-guided sampling in discrete diffusion models, and its distinction from previous methods that rely on inverse folding or direct sequence space optimization
- **Medium confidence:** The effectiveness of saliency-based edit position selection in LaMBO-2, as supported by both synthetic experiments and limited wetlab validation, but with uncertainty about generalization to different protein design tasks
- **Medium confidence:** The overall performance claims for LaMBO-2 in antibody design, given the promising but preliminary nature of the experimental validation

## Next Checks
1. Conduct a larger-scale wetlab validation study with a more diverse set of target antigens and a larger sample size to confirm the reported expression and binding rates
2. Perform ablation studies comparing NOS with alternative gradient-guided sampling methods and saliency-based edit selection with random or expert-defined position selection across multiple protein design tasks
3. Characterize the computational complexity and runtime of NOS and LaMBO-2 on different hardware configurations to assess scalability and identify potential bottlenecks