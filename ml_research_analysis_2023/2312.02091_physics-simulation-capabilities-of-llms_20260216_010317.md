---
ver: rpa2
title: Physics simulation capabilities of LLMs
arxiv_id: '2312.02091'
source_url: https://arxiv.org/abs/2312.02091
tags:
- code
- problem
- potential
- physics
- orbit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models on PhD-level to research-level
  computational physics problems. The authors contribute original and challenging
  problems in celestial mechanics, stellar physics, 1D fluid dynamics, and non-linear
  dynamics.
---

# Physics simulation capabilities of LLMs

## Quick Facts
- arXiv ID: 2312.02091
- Source URL: https://arxiv.org/abs/2312.02091
- Authors: Unknown
- Reference count: 40
- One-line primary result: GPT-4 fails most PhD-level computational physics problems but about 40% of solutions could plausibly pass; physics and coding errors are most common.

## Executive Summary
This paper evaluates large language models on PhD-level to research-level computational physics problems across celestial mechanics, stellar physics, 1D fluid dynamics, and non-linear dynamics. Using well-documented open-source packages (REBOUND, MESA, Dedalus, SciPy), the authors design original challenging problems to test the LLM's physics world models rather than simple code lookup capabilities. GPT-4 demonstrates limited success, failing most problems but producing about 40% of solutions that could plausibly pass, with physics and coding errors being the most prevalent failure modes. The study identifies specific weaknesses including poor handling of physical units, tendency to hallucinate plausible sub-modules, and inability to reliably define steady-state or stopping conditions.

## Method Summary
The study generates solutions using GPT-4 (version gpt-4-0314) with zero-shot prompting using simple academic-style problem statements. The authors design ~50 original problems across four physics domains, conditioning the LLM generation on well-documented packages (REBOUND for celestial mechanics, MESA for stellar physics, Dedalus for 1D fluid dynamics, SciPy for non-linear dynamics). Solutions are evaluated line-by-line using four metrics: coding correctness, physics reasoning correctness, necessity, and sufficiency. Additionally, an educational Pass-Fail metric assesses whether solutions capture the salient physical ingredients. Domain experts evaluate the code without execution, categorizing errors into coding, physics, unnecessary, and insufficient types.

## Key Results
- GPT-4 fails most PhD-level computational physics problems, with approximately 40% of solutions passing plausibility thresholds
- Physics errors and coding errors are the most common failure modes across all problem domains
- LLMs demonstrate poor handling of physical units, tendency to hallucinate plausible sub-modules, and inability to define steady-state or stopping conditions reliably

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning LLM generation on well-documented, widely-used physics packages improves code quality and reliability.
- Mechanism: The LLM leverages extensive online documentation and code examples from these packages during pre-training, allowing it to generate code that aligns with established physics simulation practices.
- Core assumption: The LLM has been exposed to sufficient training data containing examples of these specific physics packages in use.
- Evidence anchors:
  - [abstract]: "We condition LLM generation on the use of well-documented and widely-used packages to elicit coding capabilities in the physics and astrophysics domains."
  - [section]: "Given the strong coding abilities of LLMs, but their (currently) limited abilities in physics, this approach allows us to anchor the generation task in our problems on a specific code base, potentially surfacing shortcomings in the physics world models learned by LLMs."
  - [corpus]: No direct corpus evidence available.
- Break condition: If the LLM has not been exposed to sufficient training data containing examples of the specific physics packages used, or if the packages are not well-documented or widely-used.

### Mechanism 2
- Claim: The use of multiple evaluation metrics (coding, physics, necessity, sufficiency) provides a more comprehensive assessment of LLM-generated solutions.
- Mechanism: By evaluating code lines along these four criteria, the authors can identify and categorize different types of errors, providing insights into the LLM's strengths and weaknesses in computational physics.
- Core assumption: The evaluation criteria are well-defined and consistently applied by the evaluators.
- Evidence anchors:
  - [abstract]: "We evaluate LLM performance on several soft metrics: counts of lines that contain different types of errors (coding, physics, necessity and sufficiency) as well as a more 'educational' Pass-Fail metric focused on capturing the salient physical ingredients of the problem at hand."
  - [section]: "Our code evaluation relies on four metrics applied to each line of code generated: Is the code correct? Is the physics reasoning behind the code solution correct? Is the code line necessary? Is the code line sufficient?"
  - [corpus]: No direct corpus evidence available.
- Break condition: If the evaluation criteria are not well-defined or consistently applied, or if the chosen metrics do not adequately capture the LLM's performance in computational physics.

### Mechanism 3
- Claim: The design of original, challenging problems that require generalization beyond standard textbook cases elicits the LLM's physics world model capabilities.
- Mechanism: By posing problems that are not commonly encountered in the scientific literature, the authors test the LLM's ability to apply physical laws and principles to novel situations, revealing its capacity for physics reasoning and generalization.
- Core assumption: The original problems are sufficiently challenging and require the LLM to go beyond simple pattern matching or code memorization.
- Evidence anchors:
  - [abstract]: "We contribute âˆ¼ 50 original and challenging problems in celestial mechanics, stellar physics, 1D fluid dynamics, and non-linear dynamics."
  - [section]: "To minimize the risk of such data contamination, we avoid standard problems and instead contribute original problems crafted specifically for this work. In other words, our problems are designed to elicit some level of physics generalization capabilities from the LLMs."
  - [corpus]: No direct corpus evidence available.
- Break condition: If the original problems are not sufficiently challenging or do not require significant generalization, the LLM may perform well without truly demonstrating its physics world model capabilities.

## Foundational Learning

- Concept: Physics simulation capabilities
  - Why needed here: The paper evaluates LLMs on their ability to generate code for complex physics simulations, requiring an understanding of the physics involved and the ability to translate that understanding into working code.
  - Quick check question: What are the key components of a physics simulation, and how do they relate to the physics principles being modeled?
- Concept: Code generation and evaluation
  - Why needed here: The paper focuses on the LLM's ability to generate code solutions and the evaluation of those solutions using various metrics, requiring an understanding of code structure, syntax, and the ability to identify and categorize errors.
  - Quick check question: What are the common types of errors that can occur in code, and how can they be systematically identified and classified?
- Concept: Physics world models
  - Why needed here: The paper aims to assess the LLM's understanding of physics principles and its ability to apply them to novel situations, requiring an understanding of how physical laws and concepts are represented and reasoned about in AI systems.
  - Quick check question: How do AI systems represent and reason about physical concepts, and what are the challenges in developing accurate and generalizable physics world models?

## Architecture Onboarding

- Component map: LLM (GPT4) -> Physics packages (REBOUND, MESA, Dedalus, SciPy) -> Problem set -> Evaluation metrics (coding, physics, necessity, sufficiency) -> Error categorization
- Critical path: 1. Define problem set and evaluation criteria 2. Generate LLM prompts using simple prompt strategy 3. Generate code solutions using GPT4 4. Evaluate code solutions using four metrics and categorize errors 5. Analyze results and identify failure modes
- Design tradeoffs:
  - Using well-documented, widely-used physics packages vs. more specialized or novel packages
  - Designing original, challenging problems vs. using standard textbook problems
  - Using a simple prompt strategy vs. more complex, problem-specific prompts
  - Focusing on code lines vs. considering code comments and additional text
- Failure signatures:
  - Poor handling of physical units and code versioning
  - Tendency to hallucinate plausible sub-modules and functions
  - Inability to define steady-state or stopping conditions reliably
  - Lack of physical justification for global run parameters
- First 3 experiments:
  1. Generate code solutions for a simple, well-defined problem using a single physics package, and evaluate the results using the four metrics
  2. Generate code solutions for a more complex problem that requires the integration of multiple physics packages, and evaluate the results using the four metrics
  3. Generate code solutions for an original, challenging problem that requires significant generalization, and evaluate the results using the four metrics and the error categorization scheme

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would performance scale with larger/more diverse evaluation sets?
- Basis in paper: [inferred] The authors acknowledge their evaluation set is "more illustrative than representative" due to limited time commitment and inherent challenges in generating original problems in specialized domains.
- Why unresolved: The paper uses only ~50 problems across 4 physics domains. A larger, more diverse set would provide more reliable statistics and potentially reveal additional failure modes.
- What evidence would resolve it: Conducting evaluations with 200+ problems across 8+ physics domains, with multiple independent evaluators per problem.

### Open Question 2
- Question: Would fine-tuning on numerical simulation tasks improve performance?
- Basis in paper: [explicit] "If training datasets, or reward functions for reinforcement learning, can be constructed on numerical simulation tasks, it would then be particularly interesting to fine-tune LLMs..."
- Why unresolved: The study only evaluates zero-shot performance. Fine-tuning could potentially address some of the identified failure modes like poor physical unit handling and lack of physical justification for global run parameters.
- What evidence would resolve it: Fine-tuning experiments comparing zero-shot vs fine-tuned performance on the same evaluation set, measuring improvement in physics/coding error rates.

### Open Question 3
- Question: Are LLM physics capabilities correlated with code base availability in pretraining data?
- Basis in paper: [explicit] "It may be interesting to correlate the performance of LLMs with the extent to which open-source packages and code examples are available for pre-training."
- Why unresolved: The paper only tests LLMs on well-documented, widely-used packages. Performance on obscure/undocumented packages remains unknown.
- What evidence would resolve it: Systematic testing across code bases varying in popularity/documentation, measuring performance correlation with GitHub stars, documentation quality, and pretraining dataset inclusion.

## Limitations

- The evaluation framework relies heavily on expert judgment for Pass/Fail grading, introducing potential subjectivity
- The study does not execute the generated code, so runtime errors, numerical instabilities, or integration failures remain undetected
- Zero-shot generation with simple prompts may not fully explore the LLMs' capabilities, as more sophisticated prompting strategies could potentially improve performance

## Confidence

- Medium Confidence: Claims about overall LLM performance on physics simulation problems (approximately 40% plausibility rate, failure mode identification)
- Medium Confidence: Claims about specific failure modes (physical units handling, submodule hallucination, stopping condition definition)
- Low Confidence: Claims about the superiority of conditioning on well-documented packages versus alternative approaches

## Next Checks

1. Execute the generated code solutions to identify runtime errors, numerical instabilities, or integration failures that may not be apparent from static code analysis, providing a more complete picture of LLM performance.

2. Test whether more sophisticated prompting strategies (few-shot examples, chain-of-thought reasoning, problem decomposition) can improve the quality and correctness of generated physics code compared to the simple academic-style prompts used in this study.

3. Have multiple independent domain experts evaluate a subset of solutions using the same criteria to quantify inter-rater reliability and assess the subjectivity in the Pass/Fail and error categorization metrics.