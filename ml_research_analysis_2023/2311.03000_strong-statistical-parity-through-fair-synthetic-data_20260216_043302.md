---
ver: rpa2
title: Strong statistical parity through fair synthetic data
arxiv_id: '2311.03000'
source_url: https://arxiv.org/abs/2311.03000
tags:
- data
- synthetic
- fairness
- fair
- synfair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to generate fair synthetic data that
  achieves strong statistical parity across all thresholds, addressing the challenge
  of ensuring fairness in downstream models trained on biased real-world data. The
  core idea is to adjust the learned probability distributions of the synthetic data
  generator to equalize them across sensitive attributes, either by integrating the
  adjustment into the sampling process or as a post-processing step.
---

# Strong statistical parity through fair synthetic data

## Quick Facts
- arXiv ID: 2311.03000
- Source URL: https://arxiv.org/abs/2311.03000
- Reference count: 20
- Primary result: Achieves strong statistical parity (SPD < 0.1 across all thresholds) in synthetic data while maintaining high accuracy

## Executive Summary
This paper proposes a method to generate fair synthetic data that achieves strong statistical parity across all classification thresholds. The approach adjusts learned probability distributions in synthetic data generators to equalize outcomes across sensitive attributes, either through integrated sampling or post-processing. Experiments on Adult and Dutch census datasets demonstrate significant reduction in statistical parity difference while maintaining high accuracy, with downstream models trained on fair synthetic data producing fair predictions across arbitrary thresholds.

## Method Summary
The method generates fair synthetic data by aligning the conditional probability distributions of privileged and unprivileged groups. Using a synthetic data generator (MOSTLY AI), synthetic datasets are created from biased real-world data. A post-processing classifier (LightGBM) learns the target probabilities from synthetic data, then a linear transformation function aligns the unprivileged group's distribution with the privileged group's distribution through quantile matching. The fairness adjustment strength is controlled by parameter λ, allowing flexibility in the fairness-utility tradeoff. Downstream models (AutoGluon with CatBoost, LightGBM, RandomForest, XGBoost) are trained on the fair synthetic data and evaluated for statistical parity across all thresholds.

## Key Results
- Reduces statistical parity difference (SPD) to below 0.1 across all thresholds for both Adult and Dutch census datasets
- Maintains high accuracy with maximum 5 percentage point drop in AUC-ROC compared to non-fair synthetic data
- Downstream classifiers trained on fair synthetic data achieve fair predictions across arbitrary thresholds
- Provides flexibility to tune fairness-utility tradeoff through λ parameter without retraining synthetic data generator

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adjusting learned probability distributions across sensitive attributes equalizes outcomes across all thresholds
- Mechanism: Linear transformation f maps quantiles of unprivileged group's P(Y|X,S=s2) to match privileged group's P(Y|X,S=s1) distribution
- Core assumption: Synthetic data generator accurately preserves original data distributions
- Evidence anchors:
  - [abstract]: "By equalizing the learned target probability distributions...downstream model trained on such synthetic data provides fair predictions across all thresholds"
  - [section]: "We align both distributions by learning a linear function f which transforms...quantiles of P(Y|X,S=s2) to match...quantiles of P(Y|X,S=s1)"
- Break condition: If synthetic data generator fails to preserve original distributions or downstream classifier cannot accurately predict target variable

### Mechanism 2
- Claim: Post-processing flexibility allows tuning fairness-utility tradeoff without retraining generator
- Mechanism: Parameter λ controls convex combination between adjusted P*(Y=1|X,S=s2) and original P(Y=1|X,S=s2) probabilities
- Core assumption: Synthetic data generator's output probabilities are stable and accurate enough for meaningful post-processing
- Evidence anchors:
  - [abstract]: "This fairness adjustment can be either directly integrated into the sampling process...or added as a post-processing step"
  - [section]: "For balancing the strength of the fairness correction, we introduce the parameter λ...forming the convex combination"
- Break condition: If synthetic data generator produces probabilities with high variance or systematic bias

### Mechanism 3
- Claim: Strong statistical parity in synthetic data ensures fair predictions in downstream models
- Mechanism: Creating synthetic data where P(Z≥t|S=s1) = P(Z≥t|S=s2) for all thresholds transfers fairness property to downstream classifiers
- Core assumption: Downstream model can accurately learn from synthetic data and its performance is representative of real data
- Evidence anchors:
  - [section]: "Our empirical results demonstrate that our approach effectively achieves these objectives...reduce the statistical parity difference to levels well below 0.1"
  - [section]: "For both data sets in the synfair_1.0 case, we successfully reduce the initial SPD to a level well below 0.1 across all models"
- Break condition: If synthetic data distribution significantly diverges from real data or downstream model overfits to synthetic characteristics

## Foundational Learning

- Concept: Wasserstein distance as a measure of distribution similarity
  - Why needed here: Theoretical grounding in minimizing Wasserstein distance between probability distributions of privileged and unprivileged groups
  - Quick check question: How does minimizing Wasserstein distance between P(Z|X,S=s1) and P(Z|X,S=s2) relate to achieving statistical parity?

- Concept: Cumulative distribution functions (CDFs) and quantile matching
  - Why needed here: Implementation uses quantile matching to align probability distributions by transforming quantiles of one distribution to match another
  - Quick check question: Why does matching quantiles of P(Y|X,S) distributions ensure equal positive rates across thresholds?

- Concept: Statistical parity difference (SPD) as a fairness metric
  - Why needed here: SPD is the primary metric used to evaluate whether fair synthetic data achieves the desired fairness property across all thresholds
  - Quick check question: How does SPD=0 across all thresholds ensure "strong statistical parity" as defined in the paper?

## Architecture Onboarding

- Component map: Synthetic data generator -> Post-processing classifier -> Fairness adjustment module -> Downstream AutoGluon models -> Evaluation pipeline
- Critical path: Synthetic data generation → Post-processing classifier training → Fairness adjustment application → Downstream model training → Fairness evaluation
- Design tradeoffs: Pre-processing vs in-processing vs post-processing fairness approaches; computational cost of multiple λ values vs flexibility; synthetic data quality vs fairness correction strength
- Failure signatures: High variance in SPD across thresholds; large performance drops (AUC-ROC) with fairness adjustment; downstream models failing to achieve fair predictions despite fair synthetic training data
- First 3 experiments:
  1. Generate synthetic data with no fairness adjustment (λ=0) and measure baseline SPD and AUC-ROC
  2. Apply fairness adjustment with λ=1 and verify SPD reduction while monitoring performance impact
  3. Test intermediate λ values (0.25, 0.5, 0.75) to find optimal fairness-utility tradeoff for a specific use case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on datasets with more than two sensitive attributes?
- Basis in paper: [inferred] The paper focuses on binary classification with two protected groups but doesn't explore scenarios with multiple sensitive attributes
- Why unresolved: Current experiments limited to datasets with only one sensitive attribute (gender)
- What evidence would resolve it: Experiments on datasets with multiple sensitive attributes showing performance and challenges

### Open Question 2
- Question: What is the impact of the proposed method on fairness metrics other than statistical parity?
- Basis in paper: [inferred] Method focuses on statistical parity but doesn't explore effects on other fairness definitions like equal opportunity or disparate impact
- Why unresolved: Method is designed for statistical parity, effects on other metrics are unknown
- What evidence would resolve it: Experiments measuring various fairness metrics (equal opportunity, disparate impact, etc.) on generated fair synthetic data and downstream models

### Open Question 3
- Question: How does the proposed method handle imbalanced datasets where the positive class is rare?
- Basis in paper: [inferred] Paper doesn't explicitly address class imbalance, which could be significant for fairness-aware methods
- Why unresolved: Datasets used have reasonable balance between positive and negative classes
- What evidence would resolve it: Experiments on imbalanced datasets showing method's effectiveness in achieving fairness while maintaining reasonable accuracy

## Limitations

- Assumes synthetic data generator sufficiently preserves original data properties; if generator introduces artifacts, fairness adjustment may not transfer effectively
- Linear quantile transformation assumes monotonic relationships between group distributions, which may not hold for all datasets
- Limited experimental validation to two datasets with gender as sole sensitive attribute, raising questions about generalization to more complex scenarios

## Confidence

- **High Confidence**: Theoretical foundation linking Wasserstein distance minimization to statistical parity is well-established; quantile-matching approach for distribution alignment is mathematically sound
- **Medium Confidence**: Experimental results show consistent SPD reduction below 0.1 across thresholds, but performance impact (±5 percentage points AUC-ROC) may be dataset-dependent; limited to two datasets with gender as sole sensitive attribute
- **Low Confidence**: Generalization to datasets with multiple sensitive attributes, complex interactions, or different data modalities (e.g., text, images) remains unverified

## Next Checks

1. **Distribution Fidelity Test**: Compare marginal and joint distributions between original and synthetic data using statistical tests (Kolmogorov-Smirnov, chi-squared) to ensure generator preserves data characteristics before applying fairness adjustments
2. **Multi-Attribute Fairness**: Extend experiments to datasets with multiple sensitive attributes (e.g., race, age) and verify whether linear quantile transformation scales or requires modification
3. **Cross-Dataset Generalization**: Apply method to datasets from different domains (e.g., healthcare, finance) with varying levels of bias and complexity to assess robustness and identify failure modes