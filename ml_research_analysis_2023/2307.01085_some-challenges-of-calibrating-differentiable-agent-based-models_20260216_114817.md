---
ver: rpa2
title: Some challenges of calibrating differentiable agent-based models
arxiv_id: '2307.01085'
source_url: https://arxiv.org/abs/2307.01085
tags:
- gradient
- abms
- flow
- parameters
- differentiable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the challenges of applying automatic differentiation
  (AD) to agent-based models (ABMs). ABMs are complex systems composed of many interacting
  agents, but their discrete nature and intractable likelihood functions make calibration
  difficult.
---

# Some challenges of calibrating differentiable agent-based models

## Quick Facts
- arXiv ID: 2307.01085
- Source URL: https://arxiv.org/abs/2307.01085
- Reference count: 27
- Primary result: Demonstrates that modifications like finite gradient horizons and hybrid forward/reverse-mode AD enable successful gradient-based calibration of large-scale ABMs with millions of agents

## Executive Summary
This paper investigates the challenges of applying automatic differentiation (AD) to agent-based models (ABMs), which are complex systems with discrete randomness and intractable likelihood functions. The authors propose several modifications to standard AD techniques, including a finite gradient horizon to reduce Monte Carlo variance, hybrid forward/reverse-mode AD to balance memory and computation, and the Gumbel-Softmax trick to differentiate through discrete randomness. Experiments on Brock & Hommes and JUNE models show that these modifications enable successful gradient-based calibration even for large-scale models with millions of agents.

## Method Summary
The paper proposes a Generalised Variational Inference (GVI) framework using normalizing flows to calibrate differentiable ABMs. The method employs AdamW optimizer with learning rate 10^-3 and Monte Carlo gradient estimation with 5 samples. Key innovations include: (1) a finite gradient horizon H that truncates computation paths to reduce variance, (2) hybrid forward/reverse-mode AD where forward-mode computes the ABM Jacobian and reverse-mode computes gradients through the flow, and (3) the Gumbel-Softmax trick for differentiating through discrete randomness. The approach is validated on Brock & Hommes (1,000 agents) and JUNE (8.6 million agents) models using synthetic and real-world data.

## Key Results
- Finite gradient horizon significantly improves calibration by reducing Monte Carlo gradient variance
- Hybrid forward/reverse-mode AD enables scalable gradient computation for large-scale ABMs with millions of agents
- The proposed differentiable JUNE model achieves lower losses than its non-differentiable counterpart
- Gradient-based calibration successfully recovers ground-truth parameters in Brock & Hommes experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a finite gradient horizon reduces Monte Carlo gradient variance at the cost of introducing some bias.
- Mechanism: By truncating the computation graph to only include paths of length ≤ H+1 (where H is the gradient horizon), the number of contributing paths to the gradient estimate is reduced, thereby lowering variance. The retained paths still capture direct dependencies and short-range indirect dependencies.
- Core assumption: The bias introduced by truncating paths is outweighed by the reduction in variance, making the gradient estimate more usable for optimization.
- Evidence anchors:
  - [abstract]: "modifications to vanilla AD can become necessary" and mention of "finite gradient horizon" as a solution.
  - [section]: "We posit that this is a manifestation of a bias-variance trade-off in the Monte Carlo gradient estimation step: pruning a subset of paths in the computation graph with the use of a finite gradient horizon may introduce some bias in, but can significantly reduce the variance of, Monte Carlo estimates of the gradient."
  - [corpus]: Weak - no direct evidence in related papers.

### Mechanism 2
- Claim: Hybrid forward/reverse-mode AD enables scalable gradient computation for large-scale ABMs by balancing memory usage and computational cost.
- Mechanism: Forward-mode AD is used to compute the Jacobian of ABM outputs with respect to ABM parameters, which is memory-efficient but computationally expensive (scales with number of parameters). Reverse-mode AD is used to compute gradients of the loss with respect to the flow parameters, which is memory-intensive but computationally efficient (scales with number of outputs). The hybrid approach combines the strengths of both.
- Core assumption: The memory savings from using forward-mode AD for the ABM Jacobian outweigh the additional computational cost, making the hybrid approach scalable for large-scale ABMs.
- Evidence anchors:
  - [abstract]: "modifications like a finite gradient horizon and hybrid forward/reverse-mode AD can enable successful gradient-based calibration, even for large-scale models with millions of agents."
  - [section]: "We implement a hybrid AD technique: we use FMAD to obtain the Jacobian Jθ of the ABM outputs with respect to the ABM parameters, and combine it with RMAD through the flow qϕ."
  - [corpus]: Weak - no direct evidence in related papers.

### Mechanism 3
- Claim: The Gumbel-Softmax reparameterization trick enables differentiation through discrete randomness in ABMs, allowing for gradient-based calibration.
- Mechanism: The Gumbel-Softmax trick approximates discrete random variables with continuous, differentiable variables, allowing gradients to flow through the sampling process. This enables the use of gradient-based optimization methods for calibrating ABMs with discrete components.
- Core assumption: The continuous approximation provided by Gumbel-Softmax is sufficiently accurate for the purposes of gradient-based optimization, even though it introduces some bias.
- Evidence anchors:
  - [abstract]: "modifications like a finite gradient horizon and hybrid forward/reverse-mode AD can enable successful gradient-based calibration, even for large-scale models with millions of agents."
  - [section]: "The GRADABM-JUNE model (Quera-Bofarull, 2023) is a differentiable implementation of JUNE which employs the GS reparameterisation trick to differentiate through discrete randomness."
  - [corpus]: Weak - no direct evidence in related papers.

## Foundational Learning

- Concept: Automatic Differentiation (AD)
  - Why needed here: AD is the core technique used to compute gradients of the ABM outputs with respect to the model parameters, enabling gradient-based calibration.
  - Quick check question: What is the key difference between forward-mode AD and reverse-mode AD in terms of computational complexity?

- Concept: Reparameterization Trick
  - Why needed here: The reparameterization trick is used to express the gradients of the loss with respect to the flow parameters as an expectation over the noise distribution, enabling the use of Monte Carlo gradient estimates.
  - Quick check question: How does the reparameterization trick allow for the computation of gradients through stochastic nodes?

- Concept: Monte Carlo Gradient Estimation
  - Why needed here: Monte Carlo methods are used to estimate the gradients of the loss with respect to the flow parameters, as the exact gradients are intractable for large-scale ABMs.
  - Quick check question: What is the trade-off between bias and variance in Monte Carlo gradient estimates, and how does it affect the choice of gradient horizon?

## Architecture Onboarding

- Component map: Normalizing flow -> ABM simulator -> Loss function -> AD engine -> Optimizer
- Critical path:
  1. Sample parameters from the prior/normalizing flow
  2. Run the ABM simulator with the sampled parameters
  3. Compute the loss between the ABM output and the observed data
  4. Compute gradients of the loss with respect to the flow parameters using AD
  5. Update the flow parameters using the optimizer
- Design tradeoffs:
  - Memory vs. computation: Hybrid forward/reverse-mode AD trades memory for computation, enabling scalability for large-scale ABMs.
  - Bias vs. variance: Finite gradient horizon introduces bias but reduces variance in Monte Carlo gradient estimates.
  - Accuracy vs. tractability: Gumbel-Softmax approximation enables differentiation through discrete randomness but introduces some bias.
- Failure signatures:
  - High variance in Monte Carlo gradient estimates (may indicate need for larger gradient horizon or more samples)
  - Slow convergence of the normalizing flow (may indicate need for different architecture or learning rate)
  - Memory errors when running large-scale ABMs (may indicate need for more efficient AD implementation)
- First 3 experiments:
  1. Run the ABM simulator with a small number of agents and time steps to verify that the implementation is correct and produces sensible output.
  2. Implement the hybrid forward/reverse-mode AD and verify that it produces gradients with the expected magnitude and direction.
  3. Calibrate a simple ABM with a small number of parameters using the proposed method and compare the results to a baseline method (e.g., grid search).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop unbiased and low-variance gradient estimation methods for differentiable ABMs with discrete randomness beyond the Gumbel-Softmax trick?
- Basis in paper: [explicit] The paper mentions that the Gumbel-Softmax trick does not guarantee unbiased or low-variance gradients, and developing unbiased and lower variance methods such as StochasticAD is an active field of research.
- Why unresolved: The paper only discusses the Gumbel-Softmax trick as a current solution, but notes its limitations and the need for better methods.
- What evidence would resolve it: Experiments comparing the performance of different gradient estimation methods (e.g. Gumbel-Softmax, StochasticAD, other unbiased estimators) on calibrating various ABMs with discrete randomness.

### Open Question 2
- Question: How does the choice of gradient horizon H affect the bias-variance tradeoff and calibration performance for different ABMs and loss functions?
- Basis in paper: [explicit] The paper demonstrates that a finite gradient horizon can dramatically improve gradient-assisted training, but the optimal horizon value depends on the specific model and task.
- Why unresolved: The experiments only show results for one ABM and loss function, so the generalizability to other models is unknown.
- What evidence would resolve it: A systematic study varying the gradient horizon H and evaluating calibration performance across a range of ABMs, loss functions, and data regimes.

### Open Question 3
- Question: What is the optimal strategy for combining forward-mode and reverse-mode AD to balance memory usage and computational cost for large-scale ABMs?
- Basis in paper: [inferred] The paper presents a hybrid AD technique for the JUNE model, but does not explore alternative strategies or provide guidelines for when to use each mode.
- Why unresolved: The experiments only consider one hybrid approach, and the trade-offs may depend on factors like model size, number of parameters, and available computational resources.
- What evidence would resolve it: A comparison of different AD strategies (e.g. pure forward, pure reverse, hybrid with different splitting points) on a range of large-scale ABMs, evaluating memory usage, computational time, and calibration performance.

## Limitations

- The finite gradient horizon approach lacks theoretical guarantees about the bias-variance trade-off for different ABM structures
- The computational overhead of forward-mode AD for high-dimensional parameter spaces remains unclear
- The Gumbel-Softmax reparameterization introduces approximation bias whose impact on calibration quality is not fully characterized

## Confidence

- High: The fundamental challenge of intractable likelihoods in ABMs is well-established, and differentiable ABMs provide a viable solution path
- Medium: The hybrid AD approach and finite gradient horizon show empirical success, but lack comprehensive theoretical analysis
- Medium: The core methodology is sound, but the specific implementation details and hyperparameter choices may not generalize across all ABM types

## Next Checks

1. Systematically measure the bias introduced by finite gradient horizons across different ABM structures and determine if there are predictable patterns in when truncation becomes problematic.

2. Test the hybrid AD approach on ABMs with increasing parameter dimensions (beyond the current millions of agents) to identify the breaking point where forward-mode AD computational costs become prohibitive.

3. Compare the Gumbel-Softmax approach against other discrete reparameterization techniques (e.g., straight-through estimators, RELAX) to assess whether the observed performance is specific to the chosen method or representative of the broader approach.