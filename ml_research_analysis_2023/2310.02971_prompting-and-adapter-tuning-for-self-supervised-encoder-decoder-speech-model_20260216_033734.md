---
ver: rpa2
title: Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model
arxiv_id: '2310.02971'
source_url: https://arxiv.org/abs/2310.02971
tags:
- speech
- prompting
- tuning
- adapter
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first application of prompting and adapter
  tuning to encoder-decoder SSL speech models, specifically on Wav2Seq. We show that
  prompting outperforms previous speech prompting methods on sequence generation tasks,
  achieving 53% relative improvement in WER for ASR and 27% in F1 score for slot filling.
---

# Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model

## Quick Facts
- arXiv ID: 2310.02971
- Source URL: https://arxiv.org/abs/2310.02971
- Reference count: 0
- Key outcome: This paper presents the first application of prompting and adapter tuning to encoder-decoder SSL speech models, specifically on Wav2Seq. We show that prompting outperforms previous speech prompting methods on sequence generation tasks, achieving 53% relative improvement in WER for ASR and 27% in F1 score for slot filling. Prompting also competes with fine-tuning in low-resource scenarios. In cross-lingual ASR across 7 languages, prompting and adapter tuning outperform conventional fine-tuning when using limited trainable parameters. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.

## Executive Summary
This paper introduces prompting and adapter tuning techniques for encoder-decoder self-supervised learning (SSL) speech models, specifically targeting Wav2Seq architecture. The authors demonstrate that these parameter-efficient methods achieve state-of-the-art performance on sequence generation tasks including automatic speech recognition (ASR) and slot filling, while using significantly fewer trainable parameters than conventional fine-tuning. The work provides the first comprehensive evaluation of prompting in encoder-decoder architectures across multiple languages and resource scenarios.

## Method Summary
The paper applies two parameter-efficient transfer learning methods to Wav2Seq: prompting and adapter tuning. Prompting involves prepending trainable prompt vectors to each Transformer layer's input, while adapter tuning inserts bottleneck adapters with down-projection, activation, and up-projection layers. Both methods are evaluated against conventional fine-tuning across three scenarios: high-resource ASR on LibriSpeech-100h, low-resource ASR on 10h subsets, and cross-lingual ASR across 7 languages using the Multilingual LibriSpeech dataset. The methods are also tested on slot filling using the Audio SNIPS dataset, with performance measured by WER, F1 score, and accuracy metrics.

## Key Results
- Prompting achieves 53% relative improvement in WER compared to SpeechPrompt on LibriSpeech-100h
- Prompting outperforms adapter tuning in low-resource scenarios but adapter tuning performs better with full datasets
- In cross-lingual ASR across 7 languages, prompting and adapter tuning outperform fine-tuning when using only 20% of trainable parameters (4.3M vs 19.7M)
- Prompt placement at last encoder layers yields optimal performance, with E(ℓ3) and E(ℓ6) configurations achieving lowest WER

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting and adapter tuning can match fine-tuning performance in cross-lingual ASR when training parameters are limited.
- Mechanism: Both techniques modify only a small fraction of model parameters (2.5M-6.1M vs 19.7M for fine-tuning), preserving most pre-trained knowledge while adapting task-specific behavior.
- Core assumption: The pre-trained encoder-decoder model contains generalizable speech representations that can be efficiently adapted without full fine-tuning.
- Evidence anchors:
  - [abstract] "When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages."
  - [section] "When the prompt length is 120, Wav2Seq-Prompt outperforms Wav2Seq-FT (n = 2) on 6 out of 7 languages, while using only about 20% (4.3M out of 19.7M) of the trainable parameters."
  - [corpus] Weak evidence - no directly comparable cross-lingual prompting studies found in corpus.
- Break condition: If the pre-trained model's representations are too language-specific or the target language is too distant from pre-training language, adaptation may fail even with prompting/adapters.

### Mechanism 2
- Claim: Prompting outperforms adapter tuning in low-resource scenarios but underperforms in full-dataset scenarios.
- Mechanism: Prompting adds minimal architectural changes (prompt vectors) that don't disrupt pre-trained knowledge, while adapters introduce new transformation layers that may overfit small datasets.
- Core assumption: The pre-trained model's knowledge is more stable when accessed through simple modifications rather than through new transformation layers.
- Evidence anchors:
  - [abstract] "Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning."
  - [section] "In the low-resource scenario, the prompting method consistently outperforms the adapter-based method across all languages."
  - [section] "However, in the full-dataset scenario, the adapter method surpasses the performance of the prompting method."
- Break condition: If the dataset is large enough to support full adapter training, adapters can learn more effective task-specific transformations than simple prompts.

### Mechanism 3
- Claim: Prompt placement at decoder layers, particularly last layers, yields better performance than encoder-only prompting.
- Mechanism: The decoder is responsible for generating output sequences, so prompting at decoder layers directly influences the generation process more effectively than encoder-only modifications.
- Core assumption: The decoder's attention mechanisms and generation process are more sensitive to prompt influence than the encoder's representation extraction.
- Evidence anchors:
  - [section] "We find that incorporating the prompts at the last layers of the encoder yields the best effectiveness... adding prompts at the last 3 encoder layers (E(ℓ3)) or the last 6 encoder layers (E(ℓ6)) achieves the lowest WER compared to other methods with the same number of trainable parameters."
  - [section] "For example, adding prompts at the last 3 encoder layers (E(ℓ3)) or the last 6 encoder layers (E(ℓ6)) achieves the lowest WER compared to other methods with the same number of trainable parameters."
  - [corpus] No direct corpus evidence found for prompt placement strategies in encoder-decoder speech models.
- Break condition: If the target task requires more encoder-side adaptation (e.g., complex feature extraction), encoder-focused prompting might be more effective.

## Foundational Learning

- Concept: Self-supervised learning in speech models
  - Why needed here: The entire approach builds on pre-trained SSL models (Wav2Seq) that learn from unlabeled data, which is the foundation for both prompting and adapter tuning effectiveness.
  - Quick check question: What distinguishes encoder-only, decoder-only, and encoder-decoder SSL speech models in terms of their capabilities and limitations?

- Concept: Parameter-efficient transfer learning
  - Why needed here: Prompting and adapter tuning are parameter-efficient alternatives to full fine-tuning, which is the core contribution and motivation for this work.
  - Quick check question: How do the number of trainable parameters in prompting/adapter tuning compare to full fine-tuning, and why does this matter for computational efficiency?

- Concept: Cross-lingual transfer learning
  - Why needed here: The paper extensively evaluates cross-lingual ASR performance, which tests the generalizability of prompting and adapter tuning across different languages.
  - Quick check question: What challenges arise when transferring a model pre-trained on English to perform ASR on 7 other languages, and how do prompting and adapter tuning address these challenges?

## Architecture Onboarding

- Component map: Wav2Seq backbone: CNN encoder → Transformer encoder → Transformer decoder → output; Prompting: trainable prompt vectors inserted at input of each Transformer layer (both encoder and decoder); Adapter tuning: bottleneck adapters inserted in each Transformer layer with down-projection, activation, and up-projection; Cross-lingual setup: pre-trained on English LibriSpeech-960, fine-tuned/adapted on 7 target languages

- Critical path: Speech input → CNN encoder feature extraction → Transformer encoder contextualization → Transformer decoder generation → output sequence (with either prompting or adapter modifications)

- Design tradeoffs:
  - Prompting vs. adapter tuning: Prompting preserves more original model structure but may have limited capacity; adapters add transformation capacity but may disrupt pre-trained knowledge
  - Prompt length: Longer prompts provide more capacity but increase parameter count and may lead to overfitting in low-resource scenarios
  - Adapter bottleneck dimension: Larger bottlenecks provide more capacity but increase parameters and training time

- Failure signatures:
  - Poor cross-lingual performance: May indicate insufficient language similarity or need for more adaptation parameters
  - Overfitting in low-resource scenarios: May indicate too many trainable parameters relative to available data
  - Degraded performance compared to baseline: May indicate prompt/adapter placement issues or insufficient capacity

- First 3 experiments:
  1. Replicate Wav2Seq prompting results on a single language (e.g., German) with varying prompt lengths to establish baseline effectiveness
  2. Compare prompt placement strategies (encoder-only vs. decoder-only vs. both) on the same language to optimize prompt positioning
  3. Test adapter tuning with varying bottleneck dimensions on the same language to establish adapter baseline and compare with prompting results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for why prompting consistently outperforms adapter tuning in low-resource scenarios but adapter tuning performs better with more training data?
- Basis in paper: [explicit] The authors observe that "in the low-resource scenario, the prompting method consistently outperforms the adapter-based method across all languages" but "in the full-dataset scenario, the adapter method surpasses the performance of the prompting method"
- Why unresolved: The paper notes this phenomenon but does not provide a theoretical explanation for why the relative performance changes with dataset size.
- What evidence would resolve it: Controlled experiments varying both dataset size and parameter count systematically, along with analysis of how each method preserves or disrupts the pre-trained model's internal representations during adaptation.

### Open Question 2
- Question: Why does placing prompts at the last layers of the encoder yield better performance than placing them at earlier layers or the decoder?
- Basis in paper: [explicit] The authors find that "incorporating the prompts at the last layers of the encoder yields the best effectiveness" in their German ASR experiments, but do not explain why.
- Why unresolved: The paper identifies this pattern but does not investigate the underlying reasons for the observed performance differences across different prompt placements.
- What evidence would resolve it: Detailed analysis of the attention patterns and information flow when prompts are placed at different layers, potentially using visualization techniques to understand how prompt placement affects the model's processing.

### Open Question 3
- Question: What is the fundamental difference in how encoder-decoder models versus decoder-only models process prompts for sequence generation tasks?
- Basis in paper: [explicit] The authors note that "Wav2Seq-Prompt exhibits significant improvements over SpeechPrompt" for sequence generation tasks and attribute this to "pseudo speech recognition pre-training task and its encoder-decoder architecture" compared to GSLM's speech continuation task.
- Why unresolved: While the paper demonstrates superior performance, it does not deeply analyze the architectural or training differences that make encoder-decoder models more effective with prompts for sequence generation.
- What evidence would resolve it: Comparative studies of how prompts affect the hidden representations in encoder-decoder versus decoder-only architectures, including analysis of the pre-training objectives' impact on prompt responsiveness.

## Limitations
- Limited empirical validation with systematic ablation studies on prompt length and adapter bottleneck dimensions
- Cross-lingual performance analysis lacks investigation of linguistic similarity effects and script differences
- Implementation details missing for critical components including prompt initialization and adapter activation functions

## Confidence

- **High confidence**: The fundamental observation that prompting and adapter tuning can achieve competitive results with fewer trainable parameters is well-supported. The parameter efficiency claims are verifiable and the core methodology is clearly described.

- **Medium confidence**: The claim that prompting outperforms adapter tuning in low-resource scenarios is supported by results but lacks mechanistic explanation. The paper shows this pattern but doesn't investigate why the simpler prompting approach works better with limited data.

- **Low confidence**: The assertion that prompt placement at decoder layers is universally optimal is based on limited experimentation. The paper suggests this but doesn't provide comprehensive ablation studies across different task types or language families.

## Next Checks

1. **Ablation study on prompt length**: Systematically vary prompt length (30, 60, 90, 120, 180) on a single language (e.g., German) to establish the relationship between prompt capacity and performance, particularly examining overfitting in low-resource scenarios.

2. **Cross-lingual performance analysis**: Conduct detailed analysis of which language pairs show the largest performance gaps between prompting and fine-tuning. Calculate linguistic distance metrics (phonological, orthographic) between English and target languages to identify correlation with adaptation effectiveness.

3. **Adapter vs prompt capacity comparison**: Design experiments that match the number of trainable parameters between prompting and adapter tuning while varying their distribution. For instance, compare 120 prompt vectors against adapters with equivalent total parameters but different bottleneck dimensions to isolate the architectural contribution beyond parameter count.