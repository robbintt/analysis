---
ver: rpa2
title: Character-level NMT and language similarity
arxiv_id: '2308.04398'
source_url: https://arxiv.org/abs/2308.04398
tags:
- character-level
- translation
- char
- training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Character-level neural machine translation models trained on Czech-Slovak
  data outperform subword-level models on automatic metrics. Finetuning subword-level
  models to character-level can match or surpass subword-level performance for less
  similar language pairs like Czech-Croatian.
---

# Character-level NMT and language similarity

## Quick Facts
- arXiv ID: 2308.04398
- Source URL: https://arxiv.org/abs/2308.04398
- Reference count: 3
- Character-level models trained on Czech-Slovak data outperform subword-level models on automatic metrics.

## Executive Summary
This paper investigates character-level neural machine translation (NMT) performance across five language pairs varying in similarity to Czech. The authors find that character-level models significantly outperform subword-level models for closely related languages like Czech-Slovak, while showing mixed results for less related pairs. They demonstrate that finetuning subword-level models to character-level can recover or surpass subword-level performance for less similar languages. The study also examines the impact of model depth, finding that deeper transformers are better suited for character-level translation due to their ability to handle longer sequences.

## Method Summary
The authors trained bilingual Transformer models using the MarianNMT framework on MultiParaCrawl data for Czech-to-target language translation (Croatian, German, Hungarian, Slovak, Spanish). They varied vocabulary sizes (4k, 32k subword tokens, character-level) and model depths (6-6, 16-6, 16-16 encoder-decoder layers). Training used Adam optimizer with early stopping, and evaluation metrics included BLEU, chrF (Î²=2), and COMET scores. A key experimental condition involved finetuning subword-level models by continuing training on character-segmented data.

## Key Results
- Character-level models outperform subword-level models for closely related Czech-Slovak translation pairs
- Finetuning subword-level models to character-level closes or surpasses the performance gap for less related languages
- Model depth improvements are observed but do not consistently outperform baseline Transformer-base models for character-level translation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character-level models perform better for closely related languages because they capture fine-grained morphological similarities without losing intra-word structure.
- Mechanism: Character-level processing preserves morphological and orthographic overlap between languages, allowing the model to directly align shared character sequences and inflections that subword models might split or obscure.
- Core assumption: Morphological similarity is best encoded at the character level, and character overlap is a strong signal for relatedness.
- Evidence anchors:
  - [abstract]: "character-level models outperform subword-level models in terms of automated evaluation scores only in closely related Czech-Slovak translation pair."
  - [section 3.1]: "We use chrF score, traditionally used to compute translation quality, as a language similarity metric... We hypothesize that character-level similarity is an important aspect for our experiments."
- Break condition: If languages are not morphologically similar (e.g., Hungarian vs. Czech), the fine-grained character overlap no longer provides meaningful signal, and subword models that capture semantic chunks become more effective.

### Mechanism 2
- Claim: Finetuning from subword-level to character-level leverages learned semantic representations while adapting to character-level granularity, improving translation quality for less related languages.
- Mechanism: Pre-trained subword models learn robust semantic and syntactic patterns; switching to character-level input allows the model to exploit these patterns while handling morphological variations at a finer level, avoiding the need to learn everything from scratch.
- Core assumption: Semantic and syntactic knowledge is largely preserved when changing from subword to character segmentation.
- Evidence anchors:
  - [abstract]: "We confirm previous findings that it is possible to close the gap by finetuning the already trained subword-level models to character-level."
  - [section 4.2]: "Starting from the last checkpoint of the subword-level training, we switched the dataset to a character-split one... We see that in cases where training a char-level model from scratch didn't perform well compared to a subword-level one, finetuning from subword-level helps to attain the quality of the subword-level and even surpass it in some cases."
- Break condition: If the semantic representations learned at subword level are not transferable to character-level input (e.g., due to drastic vocabulary differences), finetuning may not recover the subword-level performance.

### Mechanism 3
- Claim: Deeper Transformer models are better suited for character-level translation because they can handle the increased sequence length and capture long-range dependencies more effectively.
- Mechanism: The quadratic complexity of attention in Transformers is mitigated by deeper layers that can better model dependencies across longer character sequences, compensating for the longer input length inherent in character-level processing.
- Core assumption: Increased depth allows the model to manage longer sequences without performance degradation.
- Evidence anchors:
  - [abstract]: "We trained bilingual Transformer translation models... We vary the training dataset size, vocabulary size and model depth and study the effects."
  - [section 4.3]: "Previous work suggests that character-level processing in Transformers requires the use of deeper models to reach the same performance as subword-level processing... We observe improvements in character-level translation compared to subword-level models of the same depth, but not compared to the Transformer-base models."
- Break condition: If the dataset is too small or hyperparameters are non-optimal, deeper models may overfit or fail to outperform shallower models, as seen in the experiments.

## Foundational Learning

- Concept: Subword segmentation and its impact on translation quality
  - Why needed here: Understanding how subword segmentation affects model performance is crucial for comparing it with character-level approaches and interpreting the results.
  - Quick check question: How does subword segmentation with SentencePiece affect the vocabulary size and the ability of the model to generalize across morphologically similar languages?

- Concept: Language similarity metrics and their relevance to NMT
  - Why needed here: The study uses chrF score as a language similarity metric, which is essential for understanding why character-level models perform better for closely related languages.
  - Quick check question: Why might chrF score be a more appropriate metric for measuring language similarity in the context of character-level NMT than other similarity measures like lexical similarity?

- Concept: Transfer learning and finetuning in neural networks
  - Why needed here: The finetuning approach relies on leveraging pre-trained subword-level models to improve character-level translation, making it essential to understand how knowledge transfer works in neural networks.
  - Quick check question: What are the potential benefits and risks of finetuning a subword-level model to character-level input in terms of model performance and generalization?

## Architecture Onboarding

- Component map: Input preprocessing (SentencePiece tokenization) -> Transformer model (configurable layers) -> MarianNMT training -> Evaluation (BLEU/chrF/COMET)
- Critical path: 1. Data preparation: Download and preprocess MultiParaCrawl data, apply SentencePiece segmentation 2. Model training: Train baseline Transformer models with varying vocab sizes and depths 3. Finetuning: Apply finetuning from subword to character level where beneficial 4. Evaluation: Compute BLEU, chrF, and COMET scores on validation and test sets
- Design tradeoffs:
  - Vocabulary size: Smaller vocabularies (4k) may lead to less sparsity but coarser segmentation; larger vocabularies (32k) capture more semantic units but may be less effective for morphologically similar languages.
  - Model depth: Deeper models handle character-level input better but require more data and computational resources; shallower models are faster but may underperform on character-level tasks.
  - Training data size: Larger datasets improve subword-level performance but may reduce the relative benefit of character-level processing.
- Failure signatures:
  - Character-level models underperforming on less related languages: Indicates that fine-grained character overlap is not providing meaningful signal.
  - Finetuning not improving performance: Suggests that semantic representations from subword-level training are not transferable to character-level input.
  - Deeper models not outperforming baseline: Points to issues with dataset size, hyperparameter choices, or overfitting.
- First 3 experiments:
  1. Train baseline Transformer models (6 encoder, 6 decoder) with 4k and 32k subword vocabularies and character-level segmentation on a small dataset (50k sentences) for Czech-Slovak translation.
  2. Train deeper Transformer models (16 encoder, 16 decoder) with character-level segmentation on the same dataset to evaluate the impact of model depth.
  3. Apply finetuning from 4k subword-level models to character-level on Czech-Croatian translation to assess the effectiveness of transfer learning for less related languages.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise language similarity thresholds that determine when character-level models outperform subword-level models?
- Basis in paper: [explicit] The paper shows character-level models outperform subword-level models for Czech-Slovak (most similar) but mixed results for other pairs; suggests language similarity is a key factor.
- Why unresolved: The paper only measures similarity using chrF and LexSim scores but doesn't establish a quantitative threshold for performance differences.
- What evidence would resolve it: Empirical experiments systematically varying language similarity metrics and showing performance crossover points between character and subword models.

### Open Question 2
- Question: Does increasing model depth beyond 16 layers consistently improve character-level translation performance?
- Basis in paper: [inferred] The paper experiments with 16-encoder/6-decoder and 16-encoder/16-decoder models but finds limited improvements over base models.
- Why unresolved: The experiments only go up to 16 layers; it's unclear if deeper models might help with larger datasets or different architectures.
- What evidence would resolve it: Training character-level models with 20-32 layers on large datasets and comparing performance to subword models.

### Open Question 3
- Question: How does character-level translation robustness to domain shift compare to subword-level models?
- Basis in paper: [explicit] The introduction mentions assumptions about character-level robustness to domain shift but states there are no conclusive proofs.
- Why unresolved: The experiments use only MultiParaCrawl and FLORES-200 datasets without domain variation.
- What evidence would resolve it: Systematic evaluation of character-level vs subword models across multiple domains with varying vocabulary and style.

## Limitations

- The paper lacks direct corpus evidence to support core mechanisms, relying on theoretical assumptions rather than empirical validation
- Only five language pairs from the Indo-European family are examined, limiting generalizability to typologically diverse languages
- No variance analysis or multiple runs are reported, making it difficult to assess statistical significance of observed differences

## Confidence

**High confidence**: The finding that character-level models outperform subword-level models for closely related languages (Czech-Slovak) is well-supported by the experimental results, though the mechanism explanation relies on theoretical assumptions rather than direct evidence.

**Medium confidence**: The finetuning approach showing improvements for less related languages is supported by experimental results, but the lack of ablation studies and statistical significance measures reduces confidence in the generality of this finding.

**Low confidence**: The hypothesis that deeper models inherently handle character-level processing better is weakly supported, as the experimental results show inconsistent improvements and the authors themselves note mixed outcomes.

## Next Checks

1. **Replication with statistical rigor**: Run each experimental condition multiple times (minimum 5 runs) to establish confidence intervals and determine statistical significance of observed differences between model variants.

2. **Cross-linguistic generalization**: Extend experiments to include language pairs from different language families (e.g., Slavic vs. Uralic, or Indo-European vs. Turkic) to test whether the proposed mechanisms hold across typologically diverse languages.

3. **Ablation studies for finetuning**: Compare finetuning against (a) training from scratch with the same total compute budget, (b) continued training on the same subword segmentation, and (c) finetuning with different learning rates to isolate the specific benefit of character-level adaptation.