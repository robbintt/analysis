---
ver: rpa2
title: 'NumHG: A Dataset for Number-Focused Headline Generation'
arxiv_id: '2309.01455'
source_url: https://arxiv.org/abs/2309.01455
tags:
- headline
- generation
- numhg
- dataset
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NumHG, a dataset for number-focused headline
  generation. The dataset contains 27,746 news articles with numerals in the headline.
---

# NumHG: A Dataset for Number-Focused Headline Generation

## Quick Facts
- arXiv ID: 2309.01455
- Source URL: https://arxiv.org/abs/2309.01455
- Reference count: 19
- Primary result: Dataset contains 27,746 news articles with single numerals in headlines, annotated with operations needed to generate correct numerals

## Executive Summary
This paper introduces NumHG, a dataset designed to advance research in number-focused headline generation. The dataset contains 27,746 news articles where headlines include exactly one numeral, with each article annotated to identify the operations required to generate that numeral (such as Copy, Trans, Span, Round, Paraphrase, Add, Subtract, Multiply, and Divide). The authors evaluated five baseline models (BART, T5, Pegasus, S1/e1/c1/s1/o1/n1, and BRIO) on this dataset and found that while these models perform well on traditional metrics like ROUGE, they struggle significantly with generating accurate numerals in headlines. Human evaluation confirmed this gap, showing that BRIO achieved the highest scores for numeral accuracy, reasonableness, and readability, but still had room for improvement. The authors conclude that NumHG can drive progress in number-focused headline generation and stimulate further research in numeral-focused text generation.

## Method Summary
The authors collected 27,746 news articles with single numerals in headlines from multiple sources. Each article was annotated with the specific operations needed to generate the correct numeral in the headline, using nine categories: Copy, Trans, Span, Round, Paraphrase, Add, Subtract, Multiply, and Divide. Five baseline models (BART, T5, Pegasus, S1/e1/c1/s1/o1/n1, and BRIO) were fine-tuned on the dataset using 5-fold cross-validation with specific hyperparameters (Adam optimizer, learning rate 5e-5, batch size 16, beam search with beam size 4). Models were evaluated using both automated metrics (ROUGE-1/2/L, BERTScore, MoverScore) and human evaluation focusing on numeral accuracy, reasonableness, and readability.

## Key Results
- Models achieve high ROUGE scores but perform significantly worse on numeral accuracy
- BRIO model achieved the highest scores in human evaluation across numeral accuracy, reasonableness, and readability
- Human evaluation revealed a substantial gap between automated metric performance and actual numeral accuracy
- The dataset enables fine-grained analysis of numeral handling operations in headline generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NumHG dataset enables targeted evaluation of numeral accuracy in headline generation
- Mechanism: By providing fine-grained annotations of operations needed to generate numerals (Copy, Trans, Span, Round, Paraphrase, Add, Subtract, Multiply, Divide), the dataset allows models to be evaluated specifically on their ability to handle numerical content, rather than just overall text quality
- Core assumption: The annotated operations accurately capture all the ways numerals can be derived or transformed in headline generation
- Evidence anchors:
  - [abstract] "We identify the lack of datasets providing fine-grained annotations for accurate numeral generation as a major roadblock"
  - [section 3.1] "For accurate numeral generation in headlines, the model may need to manipulate the numerals in the article body or perform basic calculations"
  - [corpus] Weak evidence - no direct comparison with other datasets on numeral handling
- Break condition: If real-world headline generation requires operations not captured in the current annotation scheme, or if the frequency distribution of operations doesn't reflect actual journalistic practice

### Mechanism 2
- Claim: Human evaluation reveals that models perform significantly worse on numeral accuracy than traditional metrics suggest
- Mechanism: Human evaluators assess headlines across three dimensions (numerical accuracy, reasonableness, and readability), providing a more nuanced view of model performance than automated metrics like ROUGE
- Core assumption: Human evaluators can reliably judge the correctness of numerals in context
- Evidence anchors:
  - [abstract] "Our study reveals a need for improvement in numerical accuracy, demonstrating the potential of the NumHG dataset to drive progress in number-focused headline generation"
  - [section 4.3] "Human evaluation results. As illustrated in Table 5, BRIO excels in Numeral Accuracy, Reasonableness, and Readability in human evaluations"
  - [corpus] Weak evidence - human evaluation methodology not detailed in corpus neighbors
- Break condition: If human evaluators show low inter-rater reliability or if their judgments don't correlate with actual reader comprehension

### Mechanism 3
- Claim: The dataset's structure (single numeral per headline) simplifies the evaluation of numeral handling
- Mechanism: By focusing on headlines with only one numeral, the dataset creates a controlled environment where models can be evaluated on their ability to generate a specific number rather than juggling multiple numerical references
- Core assumption: Single-numeral headlines represent a significant enough subset of real-world headlines to be meaningful for model development
- Evidence anchors:
  - [section 3.1] "As a further restriction, NumHG is centered on headlines featuring only a single number, leading us to exclude articles with more than one numeral in the headline"
  - [corpus] Weak evidence - no analysis of single vs. multi-numeral headline distribution in real news
- Break condition: If single-numeral headlines are too rare in practice to justify this constraint, or if models trained on single-numeral headlines fail to generalize to multi-numeral scenarios

## Foundational Learning

- Concept: Abstractive summarization vs. extractive summarization
  - Why needed here: Headline generation is described as an abstractive task, requiring creation of new text rather than selection of existing sentences
  - Quick check question: What is the key difference between abstractive and extractive summarization approaches?

- Concept: ROUGE metric limitations
  - Why needed here: The paper notes that while ROUGE scores are good, they don't capture numeral accuracy issues
  - Quick check question: Why might ROUGE scores be insufficient for evaluating headline generation models that must handle numerals correctly?

- Concept: Fine-grained annotation schemes
  - Why needed here: The dataset uses detailed operation annotations (Copy, Trans, Add, etc.) to track how numerals are derived
  - Quick check question: How do fine-grained annotations differ from simple binary labels in dataset construction?

## Architecture Onboarding

- Component map:
  Data pipeline: News article collection → Filtering (single numeral headlines) → Annotation (operation labeling) → Dataset creation
  Model evaluation: Baseline models (BART, T5, Pegasus, S1/e1/c1/s1/o1/n1, BRIO) → Automated metrics (ROUGE, BERTScore, MoverScore) → Human evaluation
  Quality control: Automated validation → Human validation → Consensus building

- Critical path:
  1. Data collection and filtering to ensure single-numeral headlines
  2. Annotator task design and quality control
  3. Baseline model evaluation with both automated and human metrics
  4. Analysis of results to identify improvement opportunities

- Design tradeoffs:
  - Single numeral constraint vs. real-world complexity
  - Manual annotation cost vs. dataset quality
  - Automated vs. human evaluation coverage
  - Model complexity vs. training efficiency

- Failure signatures:
  - High variance in human evaluations indicating unclear guidelines
  - Automated metrics showing high scores while human evaluation shows low numeral accuracy
  - Models performing well on ROUGE but failing on specific numeral operations
  - Annotator disagreement rates exceeding acceptable thresholds

- First 3 experiments:
  1. Evaluate baseline models on NumHG and compare automated vs. human evaluation results
  2. Analyze the distribution of operations in the dataset to identify which numeral handling strategies are most common
  3. Test whether models trained on NumHG show improved numeral accuracy on real-world headlines with multiple numerals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can numeral accuracy in headline generation be improved beyond current baseline models?
- Basis in paper: [explicit] The paper identifies that current models like BART, T5, Pegasus, S1/e1/c1/s1/o1/n1, and BRIO struggle with generating accurate numerals in headlines despite good performance on traditional metrics like ROUGE. Human evaluation showed that even the best model (BRIO) had room for improvement in numeral accuracy.
- Why unresolved: While the paper introduces the NumHG dataset and evaluates existing models, it does not propose or test specific methods to improve numeral accuracy in headline generation. The authors only mention future plans to inject numerical reasoning schemes into generation models.
- What evidence would resolve it: Comparative evaluation of models enhanced with numerical reasoning capabilities against the baseline models on the NumHG dataset, showing significant improvement in numeral accuracy while maintaining or improving other metrics like readability and reasonableness.

### Open Question 2
- Question: How does the presence of unanswerable questions in the dataset affect model performance and evaluation?
- Basis in paper: [explicit] The paper mentions that NumHG incorporates unanswerable questions, with annotators asked to provide a rationale for their unanswerability. However, the evaluation metrics used (ROUGE, BERTScore, MoverScore, human evaluation) do not explicitly account for or evaluate model performance on unanswerable questions.
- Why unresolved: The paper does not analyze how models handle unanswerable questions or how the presence of these questions affects overall model performance and evaluation. It's unclear if models should be penalized for attempting to answer unanswerable questions or if they should learn to identify and skip such questions.
- What evidence would resolve it: Analysis of model performance specifically on answerable vs. unanswerable questions in NumHG, including metrics that account for the ability to correctly identify unanswerable questions. This could involve modifying evaluation metrics or introducing new ones that consider question-answerability.

### Open Question 3
- Question: What is the optimal balance between numeral accuracy and other headline quality metrics (reasonableness and readability) in number-focused headline generation?
- Basis in paper: [inferred] The paper evaluates models on three criteria: numeral accuracy, reasonableness, and readability. However, it does not explore the trade-offs between these metrics or determine an optimal balance. For example, a model might achieve perfect numeral accuracy but produce headlines that are less readable or reasonable.
- Why unresolved: The paper does not provide guidance on how to weigh these different aspects of headline quality or explore scenarios where there might be conflicts between numeral accuracy and other quality metrics. It's unclear what the ideal performance profile would look like for a number-focused headline generation model.
- What evidence would resolve it: A comprehensive analysis of the relationship between numeral accuracy, reasonableness, and readability across different models on NumHG. This could include correlation studies, trade-off analyses, and potentially the development of a composite metric that balances these aspects according to user preferences or task requirements.

## Limitations
- Dataset restriction to single-numeral headlines may not reflect real-world headline complexity
- Focus on English news articles limits applicability to other languages and domains
- Human evaluation methodology lacks detailed documentation of inter-rater reliability measures

## Confidence
- **High confidence**: The dataset creation methodology and basic statistics (27,746 articles, 9 operation categories) are well-documented and reproducible
- **Medium confidence**: The finding that models perform significantly worse on numeral accuracy than suggested by ROUGE scores is supported by the data, though the exact magnitude of this gap could vary with different evaluation protocols
- **Low confidence**: The claim that BRIO is "the most effective model" for this task is based on limited comparison with only four other baselines, and the specific advantages of BRIO for numeral handling are not fully explained

## Next Checks
1. **Cross-lingual validation**: Test whether models trained on NumHG can effectively handle numeral generation in headlines from non-English news sources, or evaluate the dataset's potential for multilingual adaptation

2. **Multi-numeral extension**: Create a subset of NumHG containing headlines with multiple numerals and evaluate whether models trained on the single-numeral dataset can generalize to this more complex scenario

3. **Real-world deployment test**: Apply the best-performing models to a sample of actual news articles with diverse numeral usage patterns, measuring both numeral accuracy and overall headline quality as judged by professional editors or journalists