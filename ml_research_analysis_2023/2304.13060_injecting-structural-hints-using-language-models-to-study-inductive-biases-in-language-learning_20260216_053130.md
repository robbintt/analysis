---
ver: rpa2
title: 'Injecting structural hints: Using language models to study inductive biases
  in language learning'
arxiv_id: '2304.13060'
source_url: https://arxiv.org/abs/2304.13060
tags:
- language
- inductive
- bias
- learning
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the role of inductive biases in language learning
  by using transformer language models pretrained on formally structured data. The
  authors investigate whether recursive processing, non-context-free dependencies,
  or Zipfian vocabulary distributions serve as effective structural biases for learning
  natural language.
---

# Injecting structural hints: Using language models to study inductive biases in language learning

## Quick Facts
- arXiv ID: 2304.13060
- Source URL: https://arxiv.org/abs/2304.13060
- Reference count: 12
- Key outcome: Pretraining on synthetic formal languages with structural biases improves English language learning; non-context-free relationships outperform recursive ones, and Zipfian vocabulary distribution provides independent benefit.

## Executive Summary
This paper investigates inductive biases in language learning by pretraining transformer models on synthetic formal languages with different structural properties, then fine-tuning on natural English. The authors test whether recursive processing, non-context-free dependencies, or Zipfian vocabulary distributions serve as effective structural biases. Results show that any structure in pretraining helps, with non-context-free token-token relationships (CROSS) outperforming recursive ones (NEST), and Zipfian vocabulary distributions improving learning independently of grammatical structure. This controlled experimental approach provides insights into structural biases that are difficult to obtain from human studies.

## Method Summary
The authors pretrain GPT-2-small models (12 layers, 12 heads, hidden size 768) on synthetic corpora (1 billion tokens each) from formal languages NEST, CROSS, RAND, and REP, all with 500-token vocabulary. Models are trained for 10,000 steps (batch size 512, 1,000 warmup steps). After pretraining, new embedding matrices are initialized by sampling from the pretrained embeddings to map from 500 to 50,257 subwords, and models are fine-tuned on Wikitext-103 for 500 steps (batch size 256). Perplexity on the test set measures learning success.

## Key Results
- Any structural pretraining (NEST, CROSS, REP) improves perplexity over unstructured RAND baseline
- Non-context-free relationships (CROSS) outperform recursive structures (NEST) for English learning
- Zipfian vocabulary distribution provides independent benefit across all structural conditions
- Structural biases learned during pretraining transfer across different vocabulary spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining on synthetic formal languages with specific structural biases can successfully transfer structural knowledge to natural language learning.
- Mechanism: The model learns abstract structural patterns during pretraining that generalize to English syntax and semantics, even when the vocabulary and surface form differ between pretraining and fine-tuning languages.
- Core assumption: Structural information learned during pretraining is encoded in a way that transfers across different vocabulary spaces and modalities.
- Evidence anchors:
  - [abstract] "Using pretraining to instill different inductive biases in a model, we ask two questions: How do inductive biases towards recursive processing or context-sensitive token-token interactions influence language learning?"
  - [section] "We demonstrate that vocabulary distribution interacts with structural inductive biases to make learners more predisposed to language learning."
- Break condition: The transfer fails if the structural patterns are too specific to the synthetic language or if the embedding space cannot bridge the vocabulary gap.

### Mechanism 2
- Claim: Non-context-free relationships (crossing dependencies) provide stronger inductive biases for language learning than purely recursive structures.
- Mechanism: Models pretrained on CROSS language (allowing crossing dependencies) show better perplexity on English than those pretrained on NEST (recursive only), suggesting that modeling arbitrary token-token interactions is more beneficial than hierarchical recursion alone.
- Core assumption: Natural language contains more crossing dependencies than purely nested recursive structures, making non-context-free biases more applicable.
- Evidence anchors:
  - [abstract] "We show that non-context-free relationships form the best inductive biases."
  - [section] "When assessing the learnability of a human language like English... an inductive learning bias of structural recursion is less helpful than a non-context-free learning bias of crossing token-token connections."
- Break condition: If the model's attention mechanism cannot effectively model crossing dependencies, the advantage disappears.

### Mechanism 3
- Claim: A Zipfian vocabulary distribution acts as a structural bias that improves language learning independently of grammatical structure.
- Mechanism: Models pretrained with Zipfian-distributed vocabulary show better perplexity than those with uniform distribution, suggesting that learning to prioritize frequent tokens provides an advantage.
- Core assumption: The human language Zipfian distribution provides cognitive advantages that can be captured by pretraining.
- Evidence anchors:
  - [abstract] "We demonstrate that vocabulary distribution interacts with structural inductive biases to make learners more predisposed to language learning."
  - [section] "We also show that a Zipfian vocabulary distribution forms a good inductive bias independently from grammatical structure."
- Break condition: If the fine-tuning vocabulary is too dissimilar from natural language distributions, the Zipfian bias may not transfer effectively.

## Foundational Learning

- Concept: Formal language theory and context-free vs context-sensitive grammars
  - Why needed here: The paper directly manipulates these concepts through synthetic languages (NEST, CROSS, RAND, REP) to test different structural biases
  - Quick check question: What is the key difference between context-free and context-sensitive grammars that makes CROSS non-context-free?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how transformers model token relationships is crucial for interpreting why different pretraining structures transfer differently
  - Quick check question: How does the transformer attention mechanism enable modeling of crossing dependencies that CFGs cannot capture?

- Concept: Vocabulary distribution and Zipf's law
  - Why needed here: The paper explicitly manipulates vocabulary distributions (Zipfian vs uniform) to test their effect as structural biases
  - Quick check question: What is Zipf's law and why might it provide an inductive bias for language learning?

## Architecture Onboarding

- Component map: GPT-2-small model with 12 transformer layers, 12 attention heads per layer, hidden size of 768, max sequence length of 512. Pretraining on synthetic formal languages (NEST, CROSS, RAND, REP) with vocabulary size 500, then fine-tuning on English Wikitext-103 with vocabulary size 50,257.
- Critical path: Initialize GPT-2 → Pretrain on formal language (1B tokens, 10k steps) → Initialize new embedding matrix for English → Fine-tune on Wikitext-103 (500 steps) → Measure perplexity on test set.
- Design tradeoffs: Small vocabulary size (500) in pretraining vs large English vocabulary (50,257) creates vocabulary gap but strengthens claims about structural transfer; using synthetic data allows precise control but may not capture all aspects of natural language structure.
- Failure signatures: Poor perplexity after fine-tuning suggests either the pretraining structure didn't transfer effectively or the embedding matrix initialization failed to bridge the vocabulary gap.
- First 3 experiments:
  1. Compare NEST vs CROSS pretraining: Expect CROSS to show better perplexity due to non-context-free bias.
  2. Test Zipfian vs uniform vocabulary distribution: Expect Zipfian to improve performance across all structural conditions.
  3. Compare RAND vs REP baselines: Expect REP to show modest improvement over RAND due to minimal structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the non-context-free bias in CROSS outperform recursive biases for learning other natural languages beyond English?
- Basis in paper: [explicit] The authors compare NEST (recursive, context-free) and CROSS (non-context-free, crossing dependencies) in pretraining for English, finding CROSS superior.
- Why unresolved: The study only tests on English Wikitext-103. Cross-linguistic variation in dependency structures and recursion depth could alter the relative effectiveness of these biases.
- What evidence would resolve it: Systematically pretraining models on CROSS vs NEST and fine-tuning on typologically diverse languages (e.g., German, Japanese, Arabic) while measuring perplexity or downstream task performance.

### Open Question 2
- Question: How does the interaction between Zipfian vocabulary distribution and grammatical structure vary with corpus size and domain?
- Basis in paper: [inferred] The authors show Zipfian pretraining helps across all tested grammatical structures, but the interaction with domain-specific distributions (e.g., legal vs. conversational text) is unexamined.
- Why unresolved: The experiments use a single large corpus (Wikitext-103) and fixed pretraining vocabulary size, leaving open how these factors modulate the Zipfian benefit.
- What evidence would resolve it: Vary pretraining corpus size, fine-tuning domain (news, social media, technical), and measure how Zipfian vs. uniform pretraining affects learning efficiency and final performance.

### Open Question 3
- Question: Are there diminishing returns or ceiling effects for structural bias strength in pretraining?
- Basis in paper: [inferred] The authors find even minimal structure (REP) helps over RAND, but don't test whether increasingly complex formal languages (e.g., mildly context-sensitive) continue to improve learning.
- Why unresolved: Only four formal language families are tested; the space of possible structural biases is much larger.
- What evidence would resolve it: Design pretraining languages with progressively richer dependency structures (e.g., tree-adjoining grammars) and measure the point at which additional complexity no longer yields performance gains on fine-tuning tasks.

## Limitations

- The synthetic formal languages may not fully capture the complexity and nuance of natural language structure, limiting generalizability of findings.
- The vocabulary gap between pretraining (500 tokens) and fine-tuning (50,257 subwords) creates uncertainty about whether structural transfer would work with more overlapping vocabularies.
- The controlled experimental design, while powerful for isolating specific factors, may miss important interactions between multiple structural biases that occur in natural language learning.

## Confidence

**High Confidence**: The claim that pretraining on structured synthetic languages improves natural language learning performance has strong experimental support from the perplexity measurements across all structured conditions (NEST, CROSS, REP) versus unstructured RAND baseline.

**Medium Confidence**: The specific claim that non-context-free relationships (CROSS) provide stronger inductive biases than recursive structures (NEST) is supported by experimental data but relies on the assumption that the synthetic languages accurately model the relevant structural differences in natural language.

**Medium Confidence**: The assertion that Zipfian vocabulary distribution acts as an independent structural bias is well-supported experimentally but the mechanism by which this bias transfers to natural language learning could benefit from further investigation.

## Next Checks

1. **Structural Transfer Analysis**: Conduct ablation studies where specific structural features (recursion depth, crossing frequency) are systematically varied in the synthetic languages to map the relationship between pretraining structure and transfer effectiveness.

2. **Vocabulary Gap Investigation**: Test whether the structural transfer advantage persists when pretraining vocabulary size is increased toward the fine-tuning vocabulary size, or when using vocabulary overlap between pretraining and fine-tuning phases.

3. **Attention Pattern Analysis**: Examine attention patterns in the fine-tuned models to verify that the claimed structural biases (recursion, crossing dependencies, frequency prioritization) are actually being utilized during English language processing.