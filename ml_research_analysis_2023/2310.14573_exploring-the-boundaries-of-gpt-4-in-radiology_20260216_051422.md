---
ver: rpa2
title: Exploring the Boundaries of GPT-4 in Radiology
arxiv_id: '2310.14573'
source_url: https://arxiv.org/abs/2310.14573
tags:
- gpt-4
- radiology
- sentence
- cases
- findings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides a systematic evaluation of GPT-4\u2019s ability\
  \ to understand and generate radiology reports. It compares GPT-4 against state-of-the-art\
  \ (SOTA) radiology-specific models across seven tasks, including sentence similarity\
  \ classification, natural language inference, disease classification, entity extraction,\
  \ disease progression classification, and findings summarisation."
---

# Exploring the Boundaries of GPT-4 in Radiology

## Quick Facts
- **arXiv ID**: 2310.14573
- **Source URL**: https://arxiv.org/abs/2310.14573
- **Reference count**: 12
- **Key outcome**: GPT-4 matches or exceeds state-of-the-art radiology models across seven tasks, with strong performance in sentence-level semantics and domain knowledge understanding.

## Executive Summary
This paper provides a systematic evaluation of GPT-4's ability to process and understand radiology reports across seven key tasks. The authors compare GPT-4 against specialized radiology models using zero-shot, few-shot, and similarity-based prompting strategies. GPT-4 demonstrates strong performance in most tasks, particularly in understanding sentence-level semantics and domain-specific terminology. Through extensive error analysis with a board-certified radiologist, the study reveals that GPT-4 possesses substantial radiology knowledge, with most errors stemming from ambiguous cases or label noise rather than genuine model limitations. The research establishes GPT-4 as a promising foundation model for radiology report processing while highlighting the importance of human-in-the-loop approaches for safety-critical applications.

## Method Summary
The study evaluates GPT-4's performance across seven radiology tasks using public datasets including MIMIC-CXR, Open-i, MS-CXR-T, RadNLI, Chest ImaGenome, and RadGraph. The authors implement zero-shot, few-shot, and similarity-based example selection prompting strategies through an ImpressionGPT framework. GPT-4 is compared against state-of-the-art radiology-specific models across tasks including sentence similarity classification, natural language inference, disease classification, entity extraction, disease progression classification, and findings summarisation. The evaluation employs accuracy, F1, RougeL, and CheXbert metrics, with extensive error analysis conducted by a board-certified radiologist.

## Key Results
- GPT-4 matches or exceeds SOTA radiology models in most tasks, achieving new SOTA on RadNLI with 10% improvement in macro F1
- GPT-4 demonstrates strong understanding of radiology domain terminology and sentence-level semantics
- Error analysis reveals most mistakes are due to ambiguous cases or label noise rather than genuine knowledge gaps
- Prompting strategies like few-shot examples and self-consistency significantly improve performance on schema-specific tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-4 can generalize radiology knowledge from diverse contexts without task-specific training.
- **Mechanism**: GPT-4 leverages its broad pretraining to understand domain-specific terminology and inference patterns in radiology reports, enabling zero-shot or few-shot performance on tasks like NLI and disease classification.
- **Core assumption**: The model's pretraining corpus includes sufficient radiology-related text to support inference without fine-tuning.
- **Evidence anchors**: GPT-4 outperforms or matches SOTA radiology-specific models in most tasks, with particularly strong performance in understanding sentence-level semantics; GPT-4 + CoT achieves new SOTA on RadNLI, outperforming DoT5 by 10% in macro F1.
- **Break condition**: If the radiology domain contains highly specialized or rare terminology not present in pretraining data, performance may degrade.

### Mechanism 2
- **Claim**: Few-shot and similarity-based example selection improve GPT-4's performance on schema-specific tasks.
- **Mechanism**: Providing in-context examples helps GPT-4 learn the specific schema or style required for tasks like entity extraction and findings summarisation, where explicit instructions are insufficient.
- **Core assumption**: GPT-4 can infer task requirements from examples and apply them consistently to new inputs.
- **Evidence anchors**: GPT-4 with 200 selected examples achieves overall on-par performance with RadGraph benchmark; GPT-4 with example selection can match supervised baselines.
- **Break condition**: If examples are inconsistent or ambiguous, GPT-4 may learn incorrect patterns or fail to generalize.

### Mechanism 3
- **Claim**: Self-consistency and deferring from uncertain cases enhance GPT-4's reliability in ambiguous tasks.
- **Mechanism**: Running multiple inference passes and using majority voting reduces errors from inconsistent outputs; deferring allows GPT-4 to avoid making incorrect predictions on uncertain cases.
- **Core assumption**: GPT-4's outputs are sufficiently consistent across runs for self-consistency to be effective.
- **Evidence anchors**: For disease classification, GPT-4 achieves very strong performance on those cases for which it is not uncertain; self-consistency can correct most of the model mistakes that occur in minority runs.
- **Break condition**: If GPT-4's outputs are highly inconsistent or the majority vote is wrong, self-consistency may not improve reliability.

## Foundational Learning

- **Concept**: Natural Language Inference (NLI)
  - Why needed here: NLI tasks require understanding logical relationships between premise and hypothesis sentences, which is central to many radiology report analysis tasks.
  - Quick check question: Can you explain the difference between entailment, contradiction, and neutral in NLI?

- **Concept**: Entity Extraction and Schema Learning
  - Why needed here: Radiology reports often require extracting structured information (entities) following specific schemas, which GPT-4 must learn from examples.
  - Quick check question: How would you define the difference between observation and anatomy entities in a radiology report?

- **Concept**: Prompt Engineering and In-Context Learning
  - Why needed here: GPT-4's performance depends heavily on how tasks are framed in prompts and the examples provided in context.
  - Quick check question: What is the difference between zero-shot, few-shot, and chain-of-thought prompting?

## Architecture Onboarding

- **Component map**: Radiology report text -> Prompt generator (with examples) -> GPT-4 model -> Post-processor (structured results) -> Evaluator (metrics/radiologist assessment)

- **Critical path**: 
  1. Load radiology report and task configuration
  2. Generate prompt with appropriate examples
  3. Send prompt to GPT-4 API
  4. Process GPT-4 output
  5. Evaluate results (metrics or qualitative analysis)

- **Design tradeoffs**:
  - Zero-shot vs. few-shot: Zero-shot is faster but may underperform on schema-specific tasks; few-shot improves accuracy but increases cost and complexity.
  - Self-consistency vs. single run: Self-consistency improves reliability but doubles API calls and latency.
  - Example selection vs. random: Similarity-based selection may improve performance but requires additional embedding computation.

- **Failure signatures**:
  - Low performance on schema-specific tasks with zero-shot prompting
  - Inconsistent outputs across runs indicating need for self-consistency
  - Errors on ambiguous cases suggesting need for deferring or human review

- **First 3 experiments**:
  1. Test zero-shot performance on sentence similarity tasks to establish baseline
  2. Add 10 random few-shot examples and measure improvement on entity extraction
  3. Implement self-consistency on NLI task and compare to single-run results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically improve GPT-4's ability to surface nuanced domain knowledge in complex contexts?
- Basis in paper: [inferred] The paper notes that GPT-4 occasionally makes mistakes in complex contexts that require nuanced domain knowledge, such as differentiating between "hyperinflated" and "well-expanded" lungs. These errors occur even when GPT-4 can correctly answer simpler, isolated questions about the same topic.
- Why unresolved: The paper suggests that these mistakes are not due to a fundamental lack of knowledge, but rather an inability to surface the correct knowledge in all contexts with all various prompts. The authors acknowledge that future prompting strategies might help with these cases, but do not explore this further.
- What evidence would resolve it: Systematic experimentation with various prompting strategies (e.g., chain-of-thought, self-critique, tree of thoughts) specifically designed to elicit nuanced domain knowledge in complex contexts. Comparison of GPT-4's performance on these tasks with and without these strategies.

### Open Question 2
- Question: How can we design evaluation benchmarks that better account for the inherent ambiguity and label noise in radiology tasks?
- Basis in paper: [explicit] The paper's error analysis reveals that many errors in tasks like disease classification and entity extraction are due to ambiguous cases or label noise, rather than genuine model mistakes. The authors call for more quality control when creating evaluation benchmarks in the future.
- Why unresolved: The paper does not propose specific methods for addressing this issue. It highlights the problem but does not offer a solution for creating more reliable benchmarks that account for these factors.
- What evidence would resolve it: Development and validation of new evaluation benchmarks that incorporate explicit uncertainty labels or multiple expert annotations to better capture the inherent ambiguity in radiology tasks. Comparison of model performance on these new benchmarks versus traditional ones.

### Open Question 3
- Question: What is the optimal approach for combining GPT-4 with task-specific supervised models in radiology applications?
- Basis in paper: [inferred] The paper suggests that for "learn-by-example" tasks like entity extraction and findings summarization, GPT-4 requires significant example-based prompting to match the performance of supervised baselines. The authors propose exploring ways to combine GPT-4 and supervised models, treating the latter as plugins.
- Why unresolved: The paper does not explore this combination approach in detail. It only mentions it as a potential direction for future research.
- What evidence would resolve it: Empirical comparison of different approaches for combining GPT-4 with supervised models (e.g., using supervised models as plugins, ensemble methods, or hybrid architectures) across various radiology tasks. Analysis of the trade-offs between performance, computational cost, and ease of use for each approach.

## Limitations
- Evaluation relies entirely on public datasets that may not represent full diversity of real-world radiology practice
- Study focuses on English-language chest X-ray reports from specific institutions, limiting generalizability to other modalities and anatomical regions
- Human evaluation represents a single expert perspective rather than systematic multi-rater assessment across different clinical backgrounds

## Confidence

**High confidence**: GPT-4's strong performance on sentence similarity and NLI tasks, and its ability to understand radiology domain terminology

**Medium confidence**: GPT-4's competitive performance on entity extraction and disease classification when provided with appropriate examples

**Medium confidence**: The effectiveness of prompting strategies (few-shot, self-consistency, deferring) in improving performance and reliability

## Next Checks
1. Test GPT-4 on radiology reports from different institutions and imaging modalities (CT, MRI, ultrasound) to assess domain generalization
2. Conduct multi-rater human evaluation with radiologists from different institutions and experience levels to validate error analysis findings
3. Evaluate GPT-4's performance on clinical decision support tasks that require integration of findings across multiple reports or longitudinal patient data