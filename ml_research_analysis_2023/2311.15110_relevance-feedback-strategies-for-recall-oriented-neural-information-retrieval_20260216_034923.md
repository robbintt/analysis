---
ver: rpa2
title: Relevance feedback strategies for recall-oriented neural information retrieval
arxiv_id: '2311.15110'
source_url: https://arxiv.org/abs/2311.15110
tags:
- feedback
- relevance
- similarity
- document
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing review effort in
  high-recall information retrieval tasks while avoiding false negatives. The authors
  propose an iterative relevance feedback approach that combines BERT-based dense-vector
  search with cumulative vector operations (summing) on user-selected documents.
---

# Relevance feedback strategies for recall-oriented neural information retrieval

## Quick Facts
- arXiv ID: 2311.15110
- Source URL: https://arxiv.org/abs/2311.15110
- Reference count: 29
- Primary result: Iterative relevance feedback with cumulative vector summation reduces review effort by 17.85% to 59.04% compared to no feedback while achieving 80% recall

## Executive Summary
This paper addresses the challenge of reducing review effort in high-recall information retrieval tasks while avoiding false negatives. The authors propose an iterative relevance feedback approach that combines BERT-based dense-vector search with cumulative vector operations (summing) on user-selected documents. Experiments using the RCV-1 v2 dataset show the approach significantly reduces review effort compared to no feedback, with cumulative summing of selected document embeddings delivering the best results.

## Method Summary
The method implements iterative relevance feedback through cumulative vector summation of selected document embeddings. Users query a document collection, receive ranked results, and select relevant documents. The system then sums the BERT embeddings of selected documents with the query embedding to create an updated query vector, which is used to re-rank the entire collection. The approach supports both TF-IDF and BERT-based similarity methods, operates at the paragraph level for querying but document level for ranking, and uses all-mpnet-base-v2 model with HNSW vector search.

## Key Results
- Cumulative vector summation feedback reduces review effort by 17.85% to 59.04% compared to no feedback
- First-paragraph querying slightly outperforms random paragraph selection
- Cumulative feedback methods outperform keyword expansion and average vector operations
- Each feedback iteration involves reviewing 10 paragraphs to achieve 80% recall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cumulative vector summation of selected document embeddings improves recall without increasing false negatives
- Mechanism: When users select relevant documents, their BERT embeddings are summed with the query embedding, creating a new query vector that represents both the original query and the user's feedback. This updated vector is then used to re-rank the document collection.
- Core assumption: The semantic direction of the query can be improved by adding vectors from documents that contain the desired information, without losing the original query intent.
- Evidence anchors:
  - [abstract]: "relevance feedback is based on cumulatively summing the queried and selected embeddings"
  - [section]: "vector-based pseudo-relevance feedback... the queried vector can be summed with the selected search results"
  - [corpus]: Weak - no direct corpus evidence found for this specific cumulative summing approach
- Break condition: If the sum operation causes the query vector to drift too far from the original intent, potentially retrieving irrelevant documents or missing relevant ones that don't align with the accumulated feedback.

### Mechanism 2
- Claim: Paragraph-level query granularity combined with document-level ranking improves recall in dense retrieval
- Mechanism: The system queries at the paragraph level but ranks at the document level, capturing relevant information that might be isolated to specific paragraphs while maintaining the document context for final ranking decisions.
- Core assumption: Relevant information is not uniformly distributed across documents, and querying at finer granularity captures this while document-level ranking maintains coherence.
- Evidence anchors:
  - [section]: "relevant information can be exclusive to a specific part of a document" and "querying the first paragraph of a document slightly outperforms querying random paragraphs"
  - [section]: "experiments on the paragraph level... only consider documents" for ranking
  - [corpus]: Weak - corpus doesn't directly address this specific paragraph-to-document ranking approach
- Break condition: If documents have uniform information distribution or if paragraph boundaries don't align with topical boundaries, the advantage may disappear.

### Mechanism 3
- Claim: First-paragraph querying outperforms random paragraph selection due to higher prevalence of relevant terms
- Mechanism: The first paragraph typically contains the most important information and keywords, making it a more effective query representation than randomly selected paragraphs.
- Core assumption: Documents follow a structure where the first paragraph contains the most salient information about the document's topic.
- Evidence anchors:
  - [section]: "querying the first paragraph gives better results that querying a random paragraph" and "prior research found that the effectiveness of 'query by document' approaches depends on the prevalence of relevant terms in the queried text"
  - [section]: "it's probable that the first paragraphs in the RCV-1 v2 dataset outperform random paragraphs because they have a higher prevalence of relevant terms"
  - [corpus]: Weak - no corpus evidence specifically supporting first-paragraph advantage
- Break condition: If documents have different structures (e.g., conclusions-first formats) or if the first paragraph is less representative than other paragraphs.

## Foundational Learning

- Concept: Vector space models and cosine similarity
  - Why needed here: The entire feedback mechanism relies on understanding how document and query vectors can be compared and combined using cosine similarity to measure semantic similarity.
  - Quick check question: What range of values can cosine similarity produce, and what do the extremes represent?

- Concept: BERT embeddings and contextual word representations
  - Why needed here: The method uses SBERT embeddings, which capture context-sensitive meaning, and understanding how these differ from traditional word embeddings is crucial for grasping why the approach works.
  - Quick check question: How do BERT embeddings differ from static word embeddings like Word2Vec in handling polysemy (words with multiple meanings)?

- Concept: Information retrieval evaluation metrics (recall, precision, F1)
  - Why needed here: The paper evaluates success using recall (ability to find all relevant documents) and precision (ability to avoid irrelevant documents), which are fundamental metrics in information retrieval.
  - Quick check question: In a high-recall scenario like legal search, which metric is prioritized and why?

## Architecture Onboarding

- Component map: Text similarity module (TF-IDF or BERT-based) -> Document ranking module (document-level or paragraph-based) -> Feedback processing module (vector operations like sum, average, or keyword expansion)
- Critical path: Query document → Retrieve similar documents → User selects relevant documents → Process feedback (sum vectors) → Re-rank documents → Present updated results
- Design tradeoffs: BERT-based methods provide better semantic understanding but require more computational resources than TF-IDF; paragraph-level querying increases granularity but adds complexity to document ranking.
- Failure signatures: Poor performance when the first paragraph doesn't represent the document well; when user feedback is inconsistent or contradictory; when the corpus has uniform document structure.
- First 3 experiments:
  1. Test baseline TF-IDF with no feedback on document level to establish performance floor
  2. Test SBERT-based dense retrieval with first-paragraph querying on document level
  3. Test cumulative vector summation feedback on the best-performing similarity method from step 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do paragraph-based document rankings perform when extended to datasets with significantly more documents and topics?
- Basis in paper: [explicit] The paper notes that their dataset was limited to 300 articles per topic and suggests future work could run methods on larger datasets to research scalability.
- Why unresolved: The experiments were conducted on a relatively small dataset (300 articles per topic), and the paper explicitly suggests that findings might not apply to larger, real-world information retrieval scenarios.
- What evidence would resolve it: Experiments on larger datasets with thousands or millions of documents would show whether the paragraph-based document ranking methods maintain their effectiveness or if they degrade with scale.

### Open Question 2
- Question: Does the effectiveness of querying the first paragraph vary across different document types and domains beyond news articles?
- Basis in paper: [explicit] The paper found that querying the first paragraph slightly outperforms querying random paragraphs for news articles and notes this property could be exclusive to the news articles used in their research.
- Why unresolved: The experiments only used news articles from the RCV-1 v2 dataset, and the paper acknowledges this finding might not apply to data from other domains.
- What evidence would resolve it: Testing the first-paragraph vs. random-paragraph querying strategy across different document types (legal documents, scientific papers, technical manuals, etc.) would determine if this pattern holds or varies by domain.

### Open Question 3
- Question: What is the optimal balance between cumulative feedback and computational overhead when implementing these methods in production systems?
- Basis in paper: [inferred] The paper notes that cumulative feedback methods reduce review effort the most but also shows that feedback amplification adds computational latency, suggesting a trade-off exists.
- Why unresolved: While the paper identifies that cumulative methods work best, it doesn't explore the trade-off between maximal effectiveness and acceptable computational costs in real-world implementations.
- What evidence would resolve it: Experiments varying the number of iterations, feedback amplification levels, and computing resources would identify optimal configurations that balance recall improvement with practical computational constraints.

## Limitations
- Experiments conducted on single newswire dataset (RCV-1 v2) limiting generalizability
- 80% recall target may not be sufficient for critical applications requiring near-perfect recall
- Cumulative vector summation assumes consistent feedback, but contradictory user input could degrade performance
- BERT computational costs may become prohibitive for very large document collections

## Confidence

- High confidence in the effectiveness of cumulative vector summation for improving recall
- Medium confidence in the generalizability across domains and document types
- Medium confidence in the computational efficiency claims, given the single dataset context

## Next Checks

1. Test the cumulative vector summation approach on domain-specific corpora (legal, medical) to assess generalizability beyond news articles
2. Conduct user studies to measure actual review time savings and evaluate the quality of feedback in realistic scenarios
3. Benchmark the computational overhead of iterative BERT-based feedback against traditional keyword expansion methods for large-scale document collections