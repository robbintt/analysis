---
ver: rpa2
title: Grokking as a First Order Phase Transition in Two Layer Networks
arxiv_id: '2310.03789'
source_url: https://arxiv.org/abs/2310.03789
tags:
- action
- learning
- phase
- grokking
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes Grokking through the lens of phase transitions
  in two-layer networks, applying adaptive kernel theory to teacher-student models
  with cubic-polynomial and modular addition tasks. The core method involves mean-field
  decoupling of weight modes and saddle-point analysis of the resulting non-linear
  actions, mapping Grokking to first-order phase transitions with three learning phases
  (GFL, GMFL-I, GMFL-II).
---

# Grokking as a First Order Phase Transition in Two Layer Networks

## Quick Facts
- arXiv ID: 2310.03789
- Source URL: https://arxiv.org/abs/2310.03789
- Reference count: 40
- Primary result: Grokking manifests as a first-order phase transition in two-layer networks, with three learning phases (GFL, GMFL-I, GMFL-II) characterized by saddle-point analysis of weight distributions.

## Executive Summary
This paper analyzes Grokking through the lens of phase transitions in two-layer networks, applying adaptive kernel theory to teacher-student models with cubic-polynomial and modular addition tasks. The core method involves mean-field decoupling of weight modes and saddle-point analysis of the resulting non-linear actions, mapping Grokking to first-order phase transitions with three learning phases. Analytical predictions match experiments, showing that Grokking corresponds to a transition from Gaussian feature learning to mixed Gaussian phases. The approach successfully predicts the onset of Grokking and demonstrates that feature learning can reduce sample complexity compared to infinite-width GP limits.

## Method Summary
The paper analyzes Grokking using adaptive kernel theory, mapping it to first-order phase transitions in two-layer networks. The method involves mean-field decoupling of weight modes and saddle-point analysis of the resulting non-linear actions. For teacher-student models, the action is reduced from d-coupled modes to a single variable problem by projecting onto the teacher direction. For modular addition, symmetries reduce the high-dimensional weight space to dependence on a single overlap variable. This enables analytical tractability and prediction of the three learning phases: Gaussian feature learning (GFL), mixed Gaussian phase I (GMFL-I), and mixed Gaussian phase II (GMFL-II).

## Key Results
- Grokking manifests as a first-order phase transition with three distinct learning phases in two-layer networks
- Feature learning reduces sample complexity by learning simpler components first and applying them to complex targets
- Analytical predictions of phase transitions and sample complexity match experimental results for both cubic-polynomial and modular addition tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grokking manifests as a first-order phase transition in the weight distribution of two-layer networks.
- Mechanism: The paper maps Grokking to the appearance of multiple degenerate saddle points in the action governing weight probabilities. Initially, only a single saddle at w=0 dominates (GFL phase). As effective data increases, new saddles with |w|>0 become equally probable, marking the transition to a mixed Gaussian phase (GMFL-I). Eventually, only the |w|>0 saddles remain (GMFL-II), corresponding to successful generalization.
- Core assumption: The action for weight distributions can be approximated using saddle-point methods when d≫1, allowing the identification of phase transitions through degenerate minima.
- Evidence anchors:
  - [abstract] "we show that after Grokking, the state of the DNN is analogous to the mixed phase following a first-order phase transition"
  - [section 2.4] "A first-order phase transition then amounts to a point where two distinct saddles become degenerate (in action value)"
  - [corpus] Weak evidence - neighboring papers mention phase transitions but don't specifically address the saddle-point degeneracy mechanism described here

### Mechanism 2
- Claim: Feature learning reduces sample complexity compared to infinite-width GP limits by learning features from simpler components and applying them to complex ones.
- Mechanism: The finite network learns linear components of the target first, then uses these to learn non-linear components more efficiently than an infinite-width GP which requires O(d³) examples. This "assisted learning" mechanism is enabled by the adaptive kernel framework.
- Core assumption: The complex prior induced by a finite DNN can leverage learned linear features to learn non-linear features more efficiently than a uniform GP prior.
- Evidence anchors:
  - [abstract] "the state of the DNN generates useful internal representations of the teacher that are sharply distinct from those before the transition"
  - [section 3.1.2] "Here this occurs two orders of magnitudes earlier (n=6000). This occurs because the complex prior induced by a finite DNN learns the features from the readily accessible linear components of the target and applies them to the non-linear ones"
  - [corpus] No direct evidence in neighbors - this is a novel theoretical contribution of the paper

### Mechanism 3
- Claim: The teacher-student and modular addition models can be reduced to low-dimensional problems through symmetry arguments and mean-field decoupling.
- Mechanism: For the teacher-student model, the action is reduced from d-coupled modes to a single variable problem by projecting onto the teacher direction. For modular addition, symmetries reduce the high-dimensional weight space to a dependence on a single overlap variable 'a'. This enables analytical tractability.
- Core assumption: The symmetries of the task allow for dimensional reduction, and the dominant fluctuations occur along specific directions (teacher direction or specific Fourier modes).
- Evidence anchors:
  - [section 2.4] "Conveniently we find that as one scales up d, N, n in an appropriate manner, S[w] has the form dŜ[w] where d≫1 and Ŝ[w] has O(1) coefficients"
  - [section 3.2.1] "As a result, one finds that the target is always an eigenvector of the kernel and t̄_p^mn = a y_p^mn"
  - [corpus] No direct evidence in neighbors - this is a methodological innovation specific to this paper

## Foundational Learning

- Concept: Saddle-point approximation and phase transitions in statistical physics
  - Why needed here: The paper relies on identifying phase transitions through saddle-point analysis of the action governing weight distributions
  - Quick check question: What physical quantity corresponds to the "action" in this context, and how do degenerate minima signal a first-order phase transition?

- Concept: Mean-field theory and symmetry breaking
  - Why needed here: The dimensional reduction relies on mean-field decoupling of weight modes, and the phase transition involves symmetry breaking in the weight distribution
  - Quick check question: How does the mean-field assumption justify replacing sums over weight components with their average values?

- Concept: Kernel methods and Gaussian processes
  - Why needed here: The paper builds on adaptive kernel theory and compares finite networks to infinite-width GP limits
  - Quick check question: What distinguishes feature learning in finite networks from the behavior of their infinite-width GP counterparts?

## Architecture Onboarding

- Component map: Two-layer network with Erf/square activations -> Mean-field decoupling -> Saddle-point analysis -> Phase transition identification -> Feature learning characterization
- Critical path: Training → weight distribution analysis → saddle-point calculation → phase transition identification → feature learning characterization
- Design tradeoffs:
  - Two-layer architecture enables analytical tractability but limits applicability to deeper networks
  - Erf/square activations are chosen for mathematical convenience rather than performance
  - Mean-field scaling (χ) controls feature learning strength but requires careful tuning
- Failure signatures:
  - No phase transition observed (weights remain Gaussian throughout)
  - Multiple saddle points appear but don't lead to improved generalization
  - Analytical predictions deviate significantly from experiments (finite-size effects)
- First 3 experiments:
  1. Train the teacher-student model with varying noise levels and verify the three-phase behavior in weight distributions
  2. Plot the overlap of network output with linear vs cubic components to identify the Grokking transition
  3. Test the modular addition model and verify the predicted phase transition in the eigenvalue λ controlling generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample complexity advantage of finite-width networks over infinite-width GPs scale with problem dimensionality d?
- Basis in paper: Explicit - the paper claims "an FCN NNGP kernel induces a uniform prior over cubic polynomials" requiring n ∝ d³ examples, while their finite-width model learns the same features at n ∝ d.
- Why unresolved: The authors only provide a conjecture supported by analytical arguments but no rigorous proof. They note that establishing this requires showing that n and n/σ² can be kept in the same order of d, which remains unproven.
- What evidence would resolve it: A formal proof demonstrating that the sample complexity of finite-width networks scales as O(d) rather than O(d³) for learning cubic features, potentially using corrections to the equivalence kernel approximation.

### Open Question 2
- Question: How would breaking the reflection symmetry in the teacher-student model affect the phase transition and Grokking behavior?
- Basis in paper: Explicit - the paper states "due to our choice of activation function, weights acquire a reflection symmetry. Hence the above is actually a first-order symmetry-breaking transition. Breaking this symmetry, by adding say a weak bias, would make one of the new saddles more favourable."
- Why unresolved: The authors only mention this possibility but do not explore it experimentally or analytically. The implications for the phase diagram and Grokking dynamics remain unknown.
- What evidence would resolve it: Numerical experiments comparing Grokking behavior with and without bias terms, showing how the critical noise levels and weight distributions change when reflection symmetry is broken.

### Open Question 3
- Question: Can the GMFL framework be extended to deeper networks and what new phenomena might emerge?
- Basis in paper: Explicit - "As our results utilize a rather general formalism...we believe they generalize to deep networks and varying architecture."
- Why unresolved: The current analysis is limited to two-layer networks. While the authors suggest generalization to deeper architectures, they do not provide evidence or analysis of how multiple hidden layers would affect the phase transition structure.
- What evidence would resolve it: Application of the GMFL framework to networks with 3+ layers, showing whether additional saddle points emerge and how the learning phases evolve with depth.

## Limitations
- Analysis relies on mean-field approximations and saddle-point methods, which may break down for smaller system sizes
- The claim that feature learning reduces sample complexity is only demonstrated for specific target structures with decomposable linear and non-linear components
- Dimensional reduction techniques used for analytical tractability may not generalize to more complex tasks or deeper architectures

## Confidence
- Phase transition interpretation of Grokking: Medium
- Feature learning reducing sample complexity: Medium  
- Dimensional reduction through symmetry: Medium
- Applicability to real-world datasets: Low

## Next Checks
1. Test the phase transition framework on deeper networks (3+ layers) to assess scalability of the mean-field approach
2. Apply the method to real-world datasets with structured targets to verify the feature learning mechanism
3. Perform finite-size scaling analysis to determine the minimum system size required for the phase transition interpretation to hold