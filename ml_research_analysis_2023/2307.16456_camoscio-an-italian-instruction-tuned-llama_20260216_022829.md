---
ver: rpa2
title: 'Camoscio: an Italian Instruction-tuned LLaMA'
arxiv_id: '2307.16456'
source_url: https://arxiv.org/abs/2307.16456
tags:
- language
- italian
- dataset
- text
- camoscio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Camoscio is an instruction-tuned language model for the Italian
  language, derived from LLaMA 7B. It is finetuned using LoRA on a dataset of translated
  English instruction prompts, with the goal of enabling zero-shot task performance
  in Italian.
---

# Camoscio: an Italian Instruction-tuned LLaMA

## Quick Facts
- arXiv ID: 2307.16456
- Source URL: https://arxiv.org/abs/2307.16456
- Reference count: 24
- Key outcome: Italian instruction-tuned model based on LLaMA 7B showing competitive zero-shot performance on summarization, QA, and style transfer tasks

## Executive Summary
Camoscio is an Italian instruction-tuned language model derived from LLaMA 7B, designed to perform zero-shot tasks in Italian. The model is finetuned using LoRA on a dataset of translated English instruction prompts from Stanford Alpaca. Camoscio demonstrates competitive performance compared to models specifically trained for downstream tasks, achieving strong results on NewsSum-IT summarization, SQuAD-IT question answering, and XFORMAL-IT formality style transfer. The model and dataset are publicly available for further research.

## Method Summary
The authors translated the Stanford Alpaca instruction dataset to Italian using ChatGPT and finetuned LLaMA 7B with LoRA adapters. The model was trained with the standard next-token prediction objective using prompt templates in Italian. Evaluation was performed on three downstream tasks (NewsSum-IT, SQuAD-IT, XFORMAL-IT) using ROUGE, BERTScore, and Exact Match metrics. Zero-shot evaluation was conducted by directly applying the finetuned model to these tasks without additional training.

## Key Results
- Achieved R1=0.250, R2=0.104, RL=0.174 on NewsSum-IT summarization, comparable to mT5 and IT5 baselines
- Showed competitive zero-shot performance on SQuAD-IT and XFORMAL-IT tasks compared to task-specific models
- Successfully generated responses for long input documents while maintaining relevance and coherence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot task performance in Italian competes favorably with models specifically finetuned for those tasks
- Mechanism: Camoscio leverages the instruction-tuning dataset from Stanford Alpaca, translated to Italian using ChatGPT, to adapt the LLaMA 7B model to follow prompts in Italian. This enables the model to perform zero-shot on downstream tasks without task-specific training.
- Core assumption: Translating the instruction dataset preserves the task semantics and the model can generalize to unseen Italian tasks
- Evidence anchors:
  - [abstract] "Results indicate that the modelâ€™s zero-shot performance on various downstream tasks in Italian competes favorably with existing models specifically finetuned for those tasks."
  - [section 4.2] "We can observe that the zero-shot performance of Camoscio in both tasks is competitive with trained models."
- Break condition: If the translation quality is poor or the model fails to generalize beyond the translated instruction patterns

### Mechanism 2
- Claim: LoRA parameter-efficient finetuning enables training on standard desktop hardware
- Mechanism: LoRA reduces the number of trainable parameters by learning low-rank updates to the weight matrices, allowing the full LLaMA 7B model to be finetuned with limited computational resources
- Core assumption: The low-rank approximation captures the necessary task-relevant changes while keeping the computational footprint small
- Evidence anchors:
  - [abstract] "Specifically, we finetuned the smallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts translated to Italian via ChatGPT."
  - [section 3.2] "The model is trained with supervision with the standard objective to predict the next token given the previous ones."
- Break condition: If the low-rank approximation is insufficient to capture the task-specific adaptations, leading to poor performance

### Mechanism 3
- Claim: Exact Match via ChatGPT (EM-GPT) metric provides a better estimate of zero-shot performance than traditional exact match
- Mechanism: EM-GPT uses an external LM (ChatGPT) to judge whether the answer provided by a model is correct given the question and ground truth, accounting for variations in wording
- Core assumption: The external LM can accurately assess the semantic correctness of the answer despite differences in phrasing
- Evidence anchors:
  - [section 4.1] "To estimate quantitatively the actual performance of Camoscio we used instead Exact Match via ChatGPT."
  - [section 4.2] "These results also show that the EM-GPT metric of trained models correlates well with the existing EM metric, even though it is a little bit lower."
- Break condition: If the external LM's judgments are biased or inconsistent, leading to unreliable performance estimates

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: Enables the model to follow natural language instructions and perform zero-shot on downstream tasks
  - Quick check question: What is the main difference between standard language model pretraining and instruction tuning?

- Concept: Parameter-efficient finetuning (PEFT)
  - Why needed here: Allows the full LLaMA 7B model to be finetuned on limited computational resources using techniques like LoRA
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full finetuning?

- Concept: Zero-shot evaluation
  - Why needed here: Assesses the model's ability to perform tasks without any task-specific training, using only the instruction-tuning dataset
  - Quick check question: What is the key difference between zero-shot and few-shot evaluation?

## Architecture Onboarding

- Component map: LLaMA 7B base model -> LoRA adapters -> Instruction-tuning dataset (translated from Stanford Alpaca) -> Zero-shot evaluation on downstream tasks
- Critical path: Dataset translation -> LoRA adapter configuration -> Finetuning with LoRA -> Zero-shot evaluation
- Design tradeoffs: Full finetuning vs. LoRA (performance vs. computational efficiency), dataset size vs. quality (broad coverage vs. high-quality translations)
- Failure signatures: Poor performance on downstream tasks, failure to generate responses, factual errors or hallucinations in generated text
- First 3 experiments:
  1. Finetune LLaMA 7B with LoRA on a small subset of the translated instruction dataset and evaluate on a simple downstream task
  2. Compare the performance of Camoscio with and without LoRA adapters on a downstream task
  3. Evaluate the quality of the translated instruction dataset by manually checking a sample of translations and their impact on model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would Camoscio perform on other Italian language tasks not included in the evaluation, such as sentiment analysis or machine translation?
- Basis in paper: [inferred] The paper mentions that it is unclear whether the competitive zero-shot performance observed on summarization, question answering, and style transfer tasks extends to other tasks, especially those outside the training distribution of the instruction-tuning dataset.
- Why unresolved: The paper does not provide evaluation results for other Italian language tasks, and the model's performance on these tasks is unknown.
- What evidence would resolve it: Evaluating Camoscio on a diverse set of Italian language tasks, including sentiment analysis, machine translation, and others, would provide evidence of its generalization capabilities.

### Open Question 2
- Question: How does the quality of the Italian instruction-tuning dataset, translated from Stanford Alpaca, impact Camoscio's performance?
- Basis in paper: [explicit] The paper acknowledges that the translation of the Stanford Alpaca dataset to Italian using ChatGPT is not always perfect and may introduce noise into the instruction-tuning dataset.
- Why unresolved: The impact of the dataset quality on Camoscio's performance is not quantified or analyzed in the paper.
- What evidence would resolve it: Comparing Camoscio's performance on a manually curated high-quality Italian instruction-tuning dataset versus the translated dataset would reveal the impact of dataset quality on the model's performance.

### Open Question 3
- Question: What are the potential biases in Camoscio, and how do they affect its outputs?
- Basis in paper: [explicit] The paper mentions that Camoscio, like other language models, suffers from common problems such as hallucinations, factual errors, and several kinds of biases.
- Why unresolved: The paper does not provide a detailed analysis of the specific biases present in Camoscio or their impact on the model's outputs.
- What evidence would resolve it: Conducting a thorough bias analysis of Camoscio, including evaluating its outputs for gender, racial, and other biases, would provide insights into the model's potential biases and their effects.

## Limitations

- Translation quality of the instruction dataset may impact model performance, but no quantitative analysis was provided
- Limited comparison with task-specific Italian models that were specifically finetuned for the evaluated tasks
- Reliance on ChatGPT for both dataset translation and evaluation introduces potential bias and consistency issues

## Confidence

- **High Confidence**: The claim that LoRA enables parameter-efficient finetuning on standard desktop hardware is well-supported by the literature and the described experimental setup.
- **Medium Confidence**: The claim of competitive zero-shot performance is supported by the evaluation results, but the lack of detailed comparison with task-specific models and the potential impact of translation quality on performance introduce some uncertainty.
- **Low Confidence**: The claim that EM-GPT provides a better estimate of zero-shot performance than traditional exact match is based on the authors' assertion and a single reference to a previous study.

## Next Checks

1. Conduct a manual evaluation of a sample of translated instructions from the Stanford Alpaca dataset to assess translation quality and identify semantic drift or errors.
2. Evaluate Camoscio's performance on downstream tasks against Italian language models specifically finetuned for these tasks to better understand its competitive stance.
3. Compare the reliability and consistency of EM-GPT with traditional exact match metrics by evaluating a diverse set of model outputs using both metrics and analyzing the correlation and potential biases in the EM-GPT judgments.