---
ver: rpa2
title: 'Differentially Private Natural Language Models: Recent Advances and Future
  Directions'
arxiv_id: '2301.09112'
source_url: https://arxiv.org/abs/2301.09112
tags:
- privacy
- data
- private
- which
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides the first systematic survey of recent advances
  in differentially private (DP) natural language processing (NLP). It highlights
  key differences between DP-NLP and standard DP deep learning, focusing on privacy
  notions (e.g., selective DP, metric DP) and privacy levels (word vs.
---

# Differentially Private Natural Language Models: Recent Advances and Future Directions

## Quick Facts
- **arXiv ID**: 2301.09112
- **Source URL**: https://arxiv.org/abs/2301.09112
- **Reference count**: 40
- **Primary result**: First systematic survey of DP-NLP, categorizing methods and highlighting challenges in large-scale training, sentence-level privacy, and private inference.

## Executive Summary
This paper provides the first comprehensive survey of differentially private (DP) natural language processing (NLP), synthesizing recent advances in applying DP to language models. The authors systematically categorize DP-NLP methods into gradient perturbation (DP-SGD, DP-Adam) and embedding vector perturbation (private embeddings, metric DP), highlighting key differences from standard DP deep learning. The survey covers applications ranging from pre-trained models like BERT and GPT-2 to federated learning and synthetic data generation, while identifying major open challenges in efficiency, privacy granularity, and inference.

## Method Summary
The paper surveys two main categories of differentially private NLP methods: gradient perturbation approaches that modify standard optimizers by adding calibrated noise to clipped gradients during training, and embedding vector perturbation methods that privatize word or sentence embeddings before model training. Gradient perturbation methods (DP-SGD, DP-Adam) work by clipping per-example gradients, summing them, and adding Gaussian noise scaled to privacy parameters ε and δ. Embedding perturbation approaches use distance metrics in embedding space to define adjacency relations and apply noise while preserving semantic similarity. The paper discusses privacy accounting through composition theorems and amplification via subsampling, while highlighting the computational challenges of applying these methods to large pre-trained models.

## Key Results
- DP-NLP methods are categorized into gradient perturbation (DP-SGD, DP-Adam) and embedding vector perturbation (private embeddings, metric DP mechanisms)
- Privacy notions differ from standard DP, with selective DP, metric DP, and UMLDP tailored to text data characteristics
- Applications span pre-trained model fine-tuning, federated learning, and synthetic data generation with varying privacy levels (word, sentence, user)
- Major challenges identified include large-scale training efficiency, sentence-level privacy protection, and private inference mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Gradient Perturbation with Bounded Sensitivity
Adding calibrated Gaussian noise to clipped gradients ensures differential privacy during training. During each SGD iteration, compute