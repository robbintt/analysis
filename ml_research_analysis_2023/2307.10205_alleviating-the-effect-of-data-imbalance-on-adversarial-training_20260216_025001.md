---
ver: rpa2
title: Alleviating the Effect of Data Imbalance on Adversarial Training
arxiv_id: '2307.10205'
source_url: https://arxiv.org/abs/2307.10205
tags:
- reat
- training
- classes
- tail
- pgd-at
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of adversarial training on
  long-tailed datasets, where the model''s robustness and accuracy on tail data are
  low due to uneven adversarial examples and unbalanced feature embedding space. The
  authors propose a new adversarial training framework called REAT, which consists
  of two components: (1) a new training strategy inspired by the effective number
  to guide the model to generate more balanced and informative adversarial examples,
  and (2) a carefully constructed penalty function to force a satisfactory feature
  space.'
---

# Alleviating the Effect of Data Imbalance on Adversarial Training

## Quick Facts
- arXiv ID: 2307.10205
- Source URL: https://arxiv.org/abs/2307.10205
- Reference count: 40
- Primary result: Proposes REAT framework combining RBL and TAIL to improve adversarial robustness on long-tailed datasets

## Executive Summary
This paper addresses the challenge of adversarial training on long-tailed datasets where tail classes suffer from poor robustness and accuracy due to imbalanced adversarial examples and feature space allocation. The authors propose REAT (Re-balanced Adversarial Training), a framework that combines two components: a re-balanced loss for generating more uniform adversarial examples using effective number weighting, and a tail-sample-mining-based feature margin regularization to expand feature space for tail classes. Evaluation on CIFAR-10-LT and CIFAR-100-LT datasets demonstrates state-of-the-art results across multiple attack methods.

## Method Summary
REAT combines two novel components with base adversarial training: (1) Re-Balanced Loss (RBL) that generates adversarial examples with balanced predicted labels across classes using adaptive effective number weighting, and (2) Tail-sample-mining-based feature margin regularization (TAIL) that expands feature space allocation for tail classes through KL divergence-based regularization. The framework integrates with standard PGD-based adversarial training and long-tailed recognition losses to improve both robustness and clean accuracy on tail classes.

## Key Results
- Achieves state-of-the-art robustness on CIFAR-10-LT and CIFAR-100-LT with ResNet-18 and WideResNet-28-10
- Outperforms existing methods including RoBal across multiple attack types (PGD-20/100, CW-100, AutoAttack)
- Improves both clean accuracy and robust accuracy on tail classes while maintaining overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: REAT improves robustness on tail classes by re-balancing adversarial example (AE) generation using an adaptive effective number weighting scheme.
- Mechanism: The model generates AEs such that their predicted labels are distributed more uniformly across all classes, counteracting the bias toward head classes in long-tailed data. The effective number from class-balanced learning is generalized to AE generation, assigning dynamic weights inversely proportional to the effective number of AEs misclassified into each class.
- Core assumption: The distribution of AE labels in one epoch is approximately stable and predictable from the previous epoch, allowing effective number calculation to guide balanced AE generation.
- Evidence anchors:
  - [abstract] "REAT consists of two components: (1) a new training strategy inspired by the term effective number to guide the model to generate more balanced and informative AEs"
  - [section 3.1] "We generalize the definition of effective number and extend it from normal training to AE generation"
  - [corpus] "Long-tailed Medical Diagnosis with Relation-aware Representation Learning..." (weak corpus evidence for AE rebalancing)
- Break condition: If AE label distributions shift unpredictably between epochs, the effective number calculation fails to guide balanced AE generation.

### Mechanism 2
- Claim: REAT balances the feature embedding space by expanding the feature space allocated to tail classes via a regularization term.
- Mechanism: A tail-sample-mining-based feature margin regularization (TAIL) approach computes Kullback-Leibler divergence between feature distributions of tail and other classes, applying larger weights to tail samples to increase their influence on the embedding space.
- Core assumption: Tail classes benefit from increased feature space allocation and that distributional differences can be effectively captured by KL divergence with weighted contributions.
- Evidence anchors:
  - [abstract] "(2) a carefully constructed penalty function to force a satisfactory feature space"
  - [section 3.2] "we propose a regularization term to increase the area of features from tail classes"
  - [corpus] "TAET: Two-Stage Adversarial Equalization Training on Long-Tailed Distributions" (weak corpus evidence for feature regularization)
- Break condition: If the feature space expansion for tail classes does not translate into improved classification accuracy, the regularization term is ineffective.

### Mechanism 3
- Claim: REAT achieves state-of-the-art robustness by combining re-balanced AE generation and feature space regularization, outperforming methods like RoBal that rely on gradient obfuscation.
- Mechanism: REAT integrates RBL for AE generation and TAIL for feature alignment, ensuring both balanced AE labels and equitable feature space allocation. This dual approach addresses both causes of tail class underperformance.
- Core assumption: Combining RBL and TAIL yields multiplicative benefits rather than just additive improvements.
- Evidence anchors:
  - [abstract] "REAT can effectively enhance the model's robustness and preserve the model's clean accuracy"
  - [section 4.3] "Our REAT (i.e., combining RBL and TAIL) achieves the best results under various attacks"
  - [corpus] "BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition" (weak corpus evidence for combined approach)
- Break condition: If either RBL or TAIL component degrades under certain dataset or model configurations, the combined performance advantage is lost.

## Foundational Learning

- Concept: Adversarial training and AE generation
  - Why needed here: REAT builds on adversarial training by modifying AE generation to address long-tailed dataset challenges.
  - Quick check question: How does standard adversarial training generate AEs, and why do they become biased toward head classes in long-tailed datasets?

- Concept: Effective number in class-balanced learning
  - Why needed here: REAT generalizes the effective number concept from class-balanced learning to AE generation to guide balanced AE creation.
  - Quick check question: What is the mathematical definition of effective number, and how does it help in re-weighting classes?

- Concept: Feature embedding space and regularization
  - Why needed here: REAT uses a regularization term to balance the feature embedding space between head and tail classes.
  - Quick check question: How does an unbalanced feature embedding space affect classification performance, especially for tail classes?

## Architecture Onboarding

- Component map: RBL -> TAIL -> Integration layer -> Base adversarial training
- Critical path:
  1. Generate AEs using PGD with RBL-modified loss
  2. Train model on AEs with combined loss (long-tailed recognition loss + TAIL regularization)
  3. Evaluate robustness and accuracy on test set
- Design tradeoffs:
  - RBL vs. other re-weighting strategies: RBL uses dynamic effective number calculation based on AE labels, potentially more effective than static re-weighting
  - TAIL vs. classifier modification: TAIL modifies feature space allocation rather than classifier structure, avoiding gradient obfuscation issues
- Failure signatures:
  - AE label distribution remains heavily skewed toward head classes despite RBL
  - Feature embedding space remains unbalanced despite TAIL regularization
  - Model performance degrades compared to baseline adversarial training
- First 3 experiments:
  1. Compare AE label distributions with and without RBL on CIFAR-10-LT to verify balanced generation
  2. Visualize feature embedding space before and after TAIL regularization using t-SNE
  3. Evaluate model robustness and accuracy on tail classes with REAT vs. baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effective number calculation for adversarial example generation differ from the standard long-tailed recognition scenario, and why is this difference necessary?
- Basis in paper: [explicit] The paper states "We generalize the definition of effective number to the AE generation process" and explains that their β is class-related to assign finer convergence parameters for each class, which is different from the calculation in [6].
- Why unresolved: The paper mentions the difference but doesn't provide detailed mathematical derivation or experimental comparison showing why the class-related β is necessary for AE generation versus standard recognition.
- What evidence would resolve it: A detailed mathematical analysis showing the derivation of the modified effective number formula, and controlled experiments comparing the standard vs modified versions on AE generation effectiveness.

### Open Question 2
- Question: What is the exact mechanism by which the Tail-sample-mining-based feature margin regularization (TAIL) improves robustness, and can this be quantified at the feature level?
- Basis in paper: [explicit] The paper describes TAIL as maximizing distributional differences between tail and other classes using Kullback-Leibler divergence, but doesn't provide quantitative analysis of how this affects feature space separability.
- Why unresolved: While the paper explains the conceptual approach and shows overall robustness improvements, it lacks detailed analysis of how individual feature dimensions are affected or how the regularization specifically enhances class separability.
- What evidence would resolve it: Feature space analysis showing pre/post TAIL feature distributions, quantification of class separability metrics (like inter-class distance vs intra-class variance), and ablation studies showing the impact of different components of TAIL.

### Open Question 3
- Question: How does the performance of REAT scale with dataset complexity and number of classes, particularly for very long-tailed distributions with extreme imbalance ratios?
- Basis in paper: [inferred] The paper tests on CIFAR-10-LT and CIFAR-100-LT with UR up to 100, but doesn't explore more extreme scenarios or larger datasets.
- Why unresolved: The experimental results are limited to specific datasets and moderate imbalance levels, leaving questions about REAT's effectiveness on more challenging long-tailed scenarios.
- What evidence would resolve it: Experiments on datasets with higher class counts and more extreme imbalance ratios, analysis of performance degradation patterns as UR increases, and comparison with other methods on these more challenging scenarios.

## Limitations
- Implementation ambiguity: Lacks detailed pseudocode and precise hyperparameter settings for RBL and TAIL components
- Limited evaluation scope: Only tested on CIFAR-10-LT and CIFAR-100-LT datasets with specific model architectures
- No adaptive attack testing: Does not evaluate against attacks specifically designed to target REAT's unique components

## Confidence
- High Confidence: The core observation that adversarial examples in long-tailed datasets are imbalanced and that this affects tail class performance
- Medium Confidence: The effectiveness of the REAT framework in improving robustness and accuracy on tail classes
- Low Confidence: The claim that REAT "significantly outperforms existing solutions" without detailed ablation studies

## Next Checks
1. Conduct ablation studies isolating RBL and TAIL components to quantify their individual contributions
2. Test REAT against adaptive attacks specifically designed to exploit the framework's unique characteristics
3. Evaluate REAT on additional long-tailed datasets (e.g., ImageNet-LT) and with different model architectures to assess generalizability