---
ver: rpa2
title: 'Adversarial Attacks and Defenses in Large Language Models: Old and New Threats'
arxiv_id: '2310.19737'
source_url: https://arxiv.org/abs/2310.19737
tags:
- attack
- adversarial
- attacks
- defense
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies embedding space attacks as a viable threat
  model for open-source Large Language Models (LLMs), demonstrating that these models
  can be easily triggered into generating malicious content. The authors show that
  embedding space attacks are significantly more efficient than recent discrete space
  attacks, requiring on average 8.8 forward/backward passes to achieve a 100% trigger
  response rate against Llama2-7b-chat on the adversarial behavior dataset.
---

# Adversarial Attacks and Defenses in Large Language Models: Old and New Threats

## Quick Facts
- arXiv ID: 2310.19737
- Source URL: https://arxiv.org/abs/2310.19737
- Reference count: 14
- Key outcome: Embedding space attacks achieve 100% trigger response rate against Llama2-7b-chat with only 8.8 forward/backward passes on average

## Executive Summary
This paper identifies embedding space attacks as a viable threat model for open-source Large Language Models, demonstrating that these models can be easily triggered into generating malicious content. The authors show that embedding space attacks are significantly more efficient than recent discrete space attacks, requiring on average 8.8 forward/backward passes to achieve a 100% trigger response rate. They also illustrate the importance of evaluation guidelines by circumventing a recently proposed defense using a simple handcrafted instruction. The paper provides LLM-specific prerequisites for accurate defense evaluations to reduce errors and overestimation of robustness in the context of an impending adversarial arms race.

## Method Summary
The paper implements embedding space attacks by optimizing continuous token embeddings rather than discrete token sequences. Using a simple unconstrained sign gradient-descent optimizer, the attack searches for adversarial perturbations by performing signed gradient descent without projection, updating the adversarial perturbation in every attack iteration. The method is tested on Llama2-7b-chat using the adversarial behavior dataset, aiming to maximize the probability of affirmative responses to harmful requests. The attack exploits the autoregressive property of LLMs where an initial affirmative response tends to lead to continued generation of harmful content.

## Key Results
- Embedding space attacks achieve 100% trigger response rate against Llama2-7b-chat on the adversarial behavior dataset
- The attack requires only 8.8 forward/backward passes on average, making it orders of magnitude more efficient than discrete token attacks
- A recently proposed defense with high certified robustness guarantees was circumvented by simple changes to the threat model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding space attacks are orders of magnitude more efficient than discrete token attacks
- Mechanism: By optimizing over continuous token embeddings rather than discrete token sequences, the attacker can directly manipulate the model's internal representation without the constraints of the discrete vocabulary space. This allows gradient-based optimization to find adversarial perturbations much faster
- Core assumption: The embedding space is sufficiently rich to represent adversarial examples that trigger harmful responses, and the optimization landscape is smooth enough for gradient descent to find effective perturbations
- Evidence anchors:
  - [abstract]: "embedding space attacks are significantly more efficient than recent discrete space attacks, requiring on average 8.8 forward/backward passes to achieve a 100% trigger response rate"
  - [section 3.2]: "with 8.8 forward/backward passes on average, we achieve 100% trigger response rate against Llama2-7b-chat"
- Break condition: If the embedding space is sufficiently regularized or if the model's embedding representations are adversarially trained to be robust to small perturbations

### Mechanism 2
- Claim: Open-source LLMs can be triggered into generating malicious content through embedding space attacks
- Mechanism: The attacker optimizes the embedding vectors of input tokens to maximize the probability of the model generating an affirmative response to harmful requests. Once the model enters an "affirmative response mode," it tends to continue generating related harmful content
- Core assumption: LLMs have an autoregressive property where an initial affirmative response to a harmful prompt leads to continued generation of harmful content
- Evidence anchors:
  - [abstract]: "demonstrate that embedding space attacks are a viable threat model in open-source LLMs that is not discussed in the current research landscape"
  - [section 3.2]: "we find that this simple attack works quite well in practice; with 8.8 forward/backward passes on average, we achieve 100% trigger response rate"
- Break condition: If the model has strong content filtering mechanisms or if the autoregressive generation is interrupted by safety mechanisms

### Mechanism 3
- Claim: Simple changes to the threat model can circumvent recently proposed defenses
- Mechanism: The defense assumes that the attacker must use a predefined harmful instruction that can be detected by a surrogate model. By removing the instruction or replacing harmful words with benign alternatives, the defense can be bypassed while still achieving the adversarial goal
- Core assumption: The defense's threat model is too narrow and assumes constraints on the attacker that don't hold in practice
- Evidence anchors:
  - [section 3.3]: "We found a defense that was published only a month later and reports a high certified robustness guarantee against adversarial prompting... Our experiment serves as an example that the same pattern of seemingly promising defenses that are broken by later evaluations likely will appear again"
- Break condition: If the defense adopts a more comprehensive threat model that accounts for instruction-less attacks or uses more sophisticated content analysis

## Foundational Learning

- Concept: Adversarial examples in machine learning
  - Why needed here: The paper builds on the established concept of adversarial examples but applies it to the LLM domain, showing how these attacks can be extended to natural language processing
  - Quick check question: What is the key difference between adversarial examples in computer vision versus natural language processing?

- Concept: Threat modeling in adversarial machine learning
  - Why needed here: The paper emphasizes the importance of precise threat model definitions for accurate robustness evaluation, showing how different threat model assumptions lead to different defense effectiveness
  - Quick check question: How does the choice of threat model affect the evaluation of defense robustness?

- Concept: Autoregressive language model behavior
  - Why needed here: The paper exploits the property that once an LLM starts generating harmful content, it tends to continue in that direction, making embedding space attacks particularly effective
  - Quick check question: What property of autoregressive language models makes them vulnerable to certain types of attacks?

## Architecture Onboarding

- Component map: Tokenizer -> Embedding layer -> Transformer blocks -> Output layer -> Safety alignment
- Critical path: 1. Tokenize input text 2. Convert tokens to embeddings 3. Process through transformer layers 4. Generate output tokens autoregressively 5. Apply safety filtering (if present)
- Design tradeoffs: Open vs. closed source (open-source allows direct embedding manipulation but may have weaker safety measures); Efficiency vs. robustness (more efficient attacks may be more effective but require more computational resources); Safety vs. utility (stronger safety measures may reduce harmful outputs but also limit model utility)
- Failure signatures: Model generating harmful content despite safety measures; Attack optimization converging quickly (8.8 iterations on average); Safety mechanisms being bypassed through embedding manipulation
- First 3 experiments: 1. Replicate the embedding attack on Llama2-7b-chat with the adversarial behavior dataset 2. Test the same attack on different open-source models to verify generalizability 3. Evaluate the effectiveness of existing safety measures against embedding space attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can embedding space attacks be mitigated in open-source LLMs through modifications to the model architecture or training process?
- Basis in paper: explicit
- Why unresolved: The paper demonstrates that embedding space attacks are highly effective against open-source LLMs like Llama2-7b-chat, requiring only 8.8 forward/backward passes on average to achieve a 100% trigger response rate. However, it does not explore potential architectural or training modifications to mitigate these attacks.
- What evidence would resolve it: Experiments showing that specific architectural changes (e.g., different embedding schemes, regularization techniques) or training modifications (e.g., adversarial training on embedding space perturbations) can significantly reduce the success rate of embedding space attacks.

### Open Question 2
- Question: How do embedding space attacks transfer between different open-source LLM models with varying architectures and training data?
- Basis in paper: inferred
- Why unresolved: The paper demonstrates the effectiveness of embedding space attacks on Llama2-7b-chat but does not investigate whether these attacks transfer to other open-source LLM models. Transferability is a crucial factor in understanding the broader impact of this threat model.
- What evidence would resolve it: Experiments showing the success rate of embedding space attacks when transferred from one open-source LLM to another, across a range of model architectures and training datasets.

### Open Question 3
- Question: What is the relationship between the size and complexity of open-source LLMs and their vulnerability to embedding space attacks?
- Basis in paper: inferred
- Why unresolved: The paper uses Llama2-7b-chat (7 billion parameters) as an example but does not explore how the model's size or complexity affects its susceptibility to embedding space attacks. Understanding this relationship could inform model design and deployment decisions.
- What evidence would resolve it: Experiments comparing the success rates of embedding space attacks on open-source LLMs of varying sizes (e.g., 1 billion, 7 billion, 70 billion parameters) and architectural complexities (e.g., different attention mechanisms, layer configurations).

### Open Question 4
- Question: Can embedding space attacks be detected and blocked in real-time by monitoring the embedding space during inference?
- Basis in paper: inferred
- Why unresolved: The paper demonstrates the effectiveness of embedding space attacks but does not explore potential detection and mitigation strategies. Real-time monitoring of the embedding space during inference could potentially identify and block adversarial perturbations.
- What evidence would resolve it: Experiments showing that monitoring the embedding space for out-of-distribution patterns or anomalies can effectively detect and block embedding space attacks in real-time, with minimal impact on model performance for legitimate inputs.

## Limitations
- Evaluation based on a single model (Llama2-7b-chat) and specific adversarial behavior dataset, limiting generalizability
- No ablation studies provided to show sensitivity to hyperparameters or attack iterations
- Defense circumvention demonstration limited to only one recently published defense

## Confidence

High confidence: The efficiency claim for embedding space attacks (8.8 forward/backward passes for 100% success rate) is well-supported by the experimental results presented for the specific model and dataset combination tested.

Medium confidence: The claim that embedding space attacks represent a "viable threat model" for open-source LLMs is supported by the results but would benefit from broader testing across multiple models and threat scenarios.

Medium confidence: The assertion that simple threat model changes can circumvent defenses is demonstrated for one specific case, but may not generalize to all defense approaches.

## Next Checks

1. Replicate the embedding space attack on at least three different open-source LLMs (e.g., LLaMA-2, Mistral, Vicuna) to assess generalizability of the attack efficiency results across model architectures and training approaches.

2. Conduct ablation studies varying the step size (Î±) and number of attack iterations to understand the sensitivity of the attack success rate to these hyperparameters, and determine if the reported 8.8 average passes is robust across parameter settings.

3. Test the same defense circumvention approach against a broader set of published defenses (minimum 5) to evaluate whether the threat model vulnerability is a systematic issue or specific to the single defense examined.