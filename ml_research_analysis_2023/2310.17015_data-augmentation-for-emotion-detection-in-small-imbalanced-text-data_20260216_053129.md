---
ver: rpa2
title: Data Augmentation for Emotion Detection in Small Imbalanced Text Data
arxiv_id: '2310.17015'
source_url: https://arxiv.org/abs/2310.17015
tags:
- data
- augmentation
- emotion
- datasets
- records
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work explores the use of data augmentation to improve emotion\
  \ detection in small, imbalanced text datasets. Four augmentation methods\u2014\
  Easy Data Augmentation (EDA), static embeddings, contextual embeddings, and ProtAugment\u2014\
  were applied to three emotion-labeled datasets of varying sizes, emotion categories,\
  \ and distributions."
---

# Data Augmentation for Emotion Detection in Small Imbalanced Text Data

## Quick Facts
- arXiv ID: 2310.17015
- Source URL: https://arxiv.org/abs/2310.17015
- Reference count: 36
- This work explores the use of data augmentation to improve emotion detection in small, imbalanced text datasets.

## Executive Summary
This work explores the use of data augmentation to improve emotion detection in small, imbalanced text datasets. Four augmentation methods—Easy Data Augmentation (EDA), static embeddings, contextual embeddings, and ProtAugment—were applied to three emotion-labeled datasets of varying sizes, emotion categories, and distributions. Results showed significant improvements in macro-averaged F1 scores compared to non-augmented training, with the best method varying by dataset: EDA excelled on essay-style data, while contextualized embeddings performed best on tweet-based data. A case study using ChatGPT for paraphrasing demonstrated higher lexical diversity and strong performance. External data augmentation with GoEmotions also yielded improvements, especially for datasets aligned with Ekman's emotion taxonomy. These findings highlight the potential of data augmentation, particularly for small, imbalanced emotion detection tasks.

## Method Summary
The study applied four data augmentation methods (EDA, static embeddings, contextual embeddings, ProtAugment) to three emotion-labeled datasets (COVID-19 survey data, EmoEvent-EN tweets, WASSAA-21 essays). For each method, 5 augmented samples were generated per original record and concatenated with the original training data. RoBERTa models were trained on the augmented datasets and evaluated using macro-averaged F1 scores. A case study with ChatGPT paraphrasing and external augmentation using GoEmotions were also conducted to assess lexical diversity and cross-dataset performance.

## Key Results
- Data augmentation significantly improved macro-averaged F1 scores across all three datasets compared to non-augmented training.
- The most effective augmentation method varied by dataset type: EDA worked best for essay-style data (WASSAA-21), while contextualized embeddings excelled for tweet-based data (EmoEvent-EN).
- ChatGPT-based paraphrasing showed higher lexical diversity than simpler techniques but at significantly higher computational cost.
- External augmentation with GoEmotions yielded improvements, particularly for datasets aligned with Ekman's emotion taxonomy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation improves classification by increasing lexical diversity in training data.
- Mechanism: Paraphrasing methods (EDA, ProtAugment, ChatGPT) replace words and restructure sentences to generate new examples with similar meaning but different phrasing.
- Core assumption: The classifier benefits from seeing varied expressions of the same emotion label.
- Evidence anchors:
  - [abstract] "Results show the promising potential of these methods" and "case study using ChatGPT for paraphrasing demonstrated higher lexical diversity and strong performance"
  - [section] "Paraphrasing with chatGPT...was much more lexically diverse than simpler techniques (EDA), at the cost of much higher runtime and complexity"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.437, average citations=0.2. Top related titles: Speech and Text-Based Emotion Recognizer, VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings, Deep Imbalanced Learning for Multimodal Emotion Recognition in Conversations.
- Break condition: If paraphrasing generates text that changes emotional meaning or introduces noise, performance may degrade.

### Mechanism 2
- Claim: Contextualized embeddings augment data by replacing words based on surrounding context.
- Mechanism: BERT-based augmentation predicts similar words conditioned on the full sentence, maintaining semantic coherence.
- Core assumption: Context-aware word replacements preserve emotional content while adding diversity.
- Evidence anchors:
  - [abstract] "contextualized embeddings performed best on tweet-based data"
  - [section] "contextualized embeddings...did the best for tweet-based data (EmoEvent)"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.437, average citations=0.2.
- Break condition: If context window is too small or domain-specific slang is replaced with general terms, emotion cues may be lost.

### Mechanism 3
- Claim: External data augmentation supplements training data when emotion taxonomies align.
- Mechanism: GoEmotions dataset (Ekman-labeled) is merged with target datasets to increase sample count for common emotions.
- Core assumption: Shared emotion categories between datasets enable meaningful knowledge transfer.
- Evidence anchors:
  - [abstract] "External data augmentation with GoEmotions also yielded improvements, especially for datasets aligned with Ekman's emotion taxonomy"
  - [section] "W ASSA-21's emotions are more representative of the Ekman taxonomy [2], therefore better suited to the additional records from GoEmotions"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.437, average citations=0.2.
- Break condition: If target dataset has unique emotion categories not present in external data, augmentation may not help.

## Foundational Learning

- Concept: Macro-averaged F1 score
  - Why needed here: Provides balanced evaluation across imbalanced emotion classes
  - Quick check question: Why might accuracy be misleading on imbalanced emotion datasets?
- Concept: BLEU score for augmentation evaluation
  - Why needed here: Lower BLEU indicates more lexical diversity in augmented text
  - Quick check question: What does a BLEU score of 20 indicate compared to 70?
- Concept: Stratified train-test split
  - Why needed here: Maintains class distribution proportions between train and test sets
  - Quick check question: What could go wrong if we use random split on highly imbalanced data?

## Architecture Onboarding

- Component map: Data preprocessing → Augmentation module → Feature extraction → RoBERTa classifier → Evaluation
- Critical path: Augmentation output → RoBERTa input features → Prediction accuracy
- Design tradeoffs: EDA (fast, simple) vs. BERT embeddings (slow, context-aware) vs. ChatGPT (powerful, expensive)
- Failure signatures: High BLEU scores indicate weak augmentation; low F1 despite high accuracy suggests class imbalance issues
- First 3 experiments:
  1. Run EDA augmentation on COVID-19 dataset and measure F1-macro improvement
  2. Compare BLEU scores across all augmentation methods on W ASSA-21
  3. Test external GoEmotions augmentation on COVID-19 vs W ASSA-21

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of data augmentation vary across different emotion taxonomies (e.g., Ekman vs. non-Ekman taxonomies) in small, imbalanced datasets?
- Basis in paper: [explicit] The paper mentions that different taxonomies exist (e.g., Ekman, Plutchik) and that datasets may not follow any standard taxonomy. The case study with GoEmotions, which follows Ekman's taxonomy, showed better results for datasets aligned with it.
- Why unresolved: The paper only compared one non-Ekman dataset (COVID-19) and one Ekman-based dataset (W ASSA-21) in detail, leaving uncertainty about how augmentation generalizes across various taxonomies.
- What evidence would resolve it: Systematic experiments applying the same augmentation methods to datasets from multiple emotion taxonomies, measuring performance differences to identify which methods work best for each taxonomy.

### Open Question 2
- Question: How does the lexical diversity introduced by different augmentation methods (e.g., EDA, contextual embeddings, chatGPT) quantitatively affect emotion detection performance in small, imbalanced datasets?
- Basis in paper: [explicit] The paper uses BLEU scores to measure lexical diversity, noting that lower BLEU (higher diversity) correlated with better performance for some datasets, but not all. ChatGPT showed much lower BLEU than EDA, suggesting higher diversity, but the relationship to performance was not fully explored.
- Why unresolved: The paper provides BLEU scores and performance metrics but does not analyze the direct relationship between lexical diversity and classification accuracy across all datasets and augmentation methods.
- What evidence would resolve it: A correlation analysis between BLEU scores and F1-macro scores across all datasets and augmentation methods, or controlled experiments varying the degree of lexical diversity to observe its impact on performance.

### Open Question 3
- Question: What is the optimal balance between the complexity of augmentation methods (e.g., runtime, GPU requirements) and their performance gains for emotion detection in small, imbalanced datasets?
- Basis in paper: [explicit] The paper notes that EDA is simple and fast with no GPU requirement, while methods like ProtAugmenter and contextual embeddings are computationally expensive. It also observes that EDA performed best for some datasets despite lower lexical diversity, suggesting a trade-off between complexity and effectiveness.
- Why unresolved: The paper compares performance and runtime but does not systematically evaluate the trade-off between computational cost and performance improvement, nor does it suggest when simpler methods might be preferable.
- What evidence would resolve it: A cost-benefit analysis quantifying performance gains per unit of computational cost (e.g., F1 improvement per minute of runtime) for each augmentation method across datasets, helping to identify when simpler methods are sufficient.

## Limitations

- Limited evaluation of augmentation methods and base classifier (only 4 augmentation methods and RoBERTa tested)
- Small sample size of datasets (only 3 datasets with different text styles and emotion distributions)
- Augmentation ratio (5×) was arbitrarily chosen rather than optimized through systematic hyperparameter tuning
- ChatGPT evaluation only tested one prompt without systematic prompt engineering
- External augmentation assumes compatibility between emotion taxonomies without quantifying semantic overlap

## Confidence

**High confidence**: Data augmentation improves macro-F1 scores compared to non-augmented training. This claim is supported by consistent improvements across all three datasets and all four augmentation methods tested.

**Medium confidence**: The effectiveness of augmentation methods varies by dataset characteristics (essay-style vs. tweet-style). While patterns are observed, the sample size of three datasets is too small to establish definitive correlations between text style and optimal augmentation method.

**Medium confidence**: External data augmentation with GoEmotions is most effective for datasets aligned with Ekman's taxonomy. This conclusion is based on observed improvements but lacks quantitative analysis of taxonomy overlap or systematic testing across multiple external datasets.

**Low confidence**: ChatGPT-based augmentation is superior to simpler techniques. While lexical diversity was higher, runtime costs were significantly greater, and no cost-benefit analysis was provided to justify this tradeoff.

## Next Checks

1. **Ablation study on augmentation ratio**: Systematically vary the augmentation factor (2×, 5×, 10×, 20×) for each method to identify optimal ratios for different dataset types and quantify the point of diminishing returns.

2. **Taxonomy alignment quantification**: Develop and apply a metric to measure semantic overlap between target dataset emotions and external dataset emotions (like GoEmotions), then correlate this alignment score with observed performance improvements.

3. **Prompt engineering for ChatGPT augmentation**: Test multiple prompt variants for ChatGPT paraphrasing, including different temperature settings, instruction specificity, and emotion-preserving constraints to identify optimal configurations.