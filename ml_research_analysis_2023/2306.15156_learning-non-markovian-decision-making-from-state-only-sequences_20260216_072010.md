---
ver: rpa2
title: Learning non-Markovian Decision-Making from State-only Sequences
arxiv_id: '2306.15156'
source_url: https://arxiv.org/abs/2306.15156
tags:
- learning
- policy
- transition
- state
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles imitation learning from state-only demonstrations,
  relaxing the Markov assumption to handle non-Markovian domains. The proposed approach,
  Latent-action non-Markov Decision Process (LanMDP), models the joint state-action
  distribution as a Boltzmann policy (energy-based prior) coupled with a Markovian
  transition.
---

# Learning non-Markovian Decision-Making from State-only Sequences

## Quick Facts
- arXiv ID: 2306.15156
- Source URL: https://arxiv.org/abs/2306.15156
- Reference count: 40
- Primary result: LanMDP achieves competitive performance with state-action baselines and surpasses other state-only methods on MuJoCo tasks

## Executive Summary
This paper addresses imitation learning from state-only demonstrations in non-Markovian domains. The proposed Latent-action non-Markov Decision Process (LanMDP) models joint state-action distributions using an energy-based policy prior coupled with a Markovian transition model. Learning is achieved through maximum likelihood estimation with short-run MCMC for prior sampling and importance sampling for posterior approximation, avoiding backpropagation through non-injective dynamics. The method demonstrates competitive performance with state-action baselines and superior results compared to other state-only methods on standard MuJoCo benchmarks.

## Method Summary
LanMDP learns from state-only demonstrations by modeling the joint state-action distribution as a Boltzmann policy (energy-based prior) with a Markovian transition. The policy is trained via maximum likelihood estimation using short-run MCMC to sample from the prior and importance sampling to approximate the posterior without backpropagating through the transition model. The learned model supports both model-free policy execution (prior sampling) and model-based planning (posterior sampling). A transition model is trained using self-interaction data to enable importance sampling for posterior approximation.

## Key Results
- LanMDP achieves performance competitive with state-action baselines (BC, GAIL) on MuJoCo suite
- LanMDP surpasses other state-only methods (BCO, GAIFO) in data efficiency and generalizability
- Synthetic cubic curve planning experiments demonstrate that non-Markovian policies are necessary to capture underlying dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The learned energy-based policy enables multi-modal action sampling, capturing uncertainty when contexts are incomplete
- Mechanism: When context length is insufficient, the energy-based prior allows multiple action modes to emerge during sampling, compensating for missing higher-order derivative information
- Core assumption: The energy function can encode uncertainty distributions over actions that reflect incomplete state information
- Evidence anchors:
  - [abstract]: "where the policy is an energy-based prior in the latent space of the state transition generator"
  - [section]: "the LanMDP policy with length context 2 leverages its energy-based multi-modality to capture the uncertainty induced by marginalizing part of the necessary contexts"
  - [corpus]: Weak - no direct corpus evidence on multi-modality compensation
- Break condition: If the energy function cannot represent multi-modal distributions or if sampling becomes too degenerate in high dimensions

### Mechanism 2
- Claim: Importance sampling with learned transition uncertainty weighting enables effective posterior sampling without backpropagating through non-injective dynamics
- Mechanism: By reweighting prior samples using transition model uncertainty estimates, the method approximates the true posterior without requiring gradients through the transition model
- Core assumption: The transition model's uncertainty estimates are reliable enough to serve as importance weights
- Evidence anchors:
  - [section]: "we develop an alternative posterior sampling method with importance sampling to overcome this challenge"
  - [section]: "Let cpat; s0:t`1q “ Epαpat|s0:tqrβpst`1|st, atqs, posterior sampling...can be realized by adjusting importance weights"
  - [corpus]: Weak - no corpus evidence on this specific importance sampling approach
- Break condition: If the transition model is highly uncertain or multi-modal, making importance weights unreliable

### Mechanism 3
- Claim: The maximum likelihood estimation objective induces a sequential decision-making problem equivalent to maximum entropy reinforcement learning
- Mechanism: Through the construction in Theorem 1, the MLE objective can be shown to maximize the same expected return as a carefully constructed RL problem with rewards derived from the energy function
- Core assumption: The ground-truth conditional state distribution p˚pst`1|s0:tq is accessible and the transition is known
- Evidence anchors:
  - [section]: "We show that the prior sampling at each step can indeed lead to optimal expected returns"
  - [section]: "Almost surprisingly, we find that the entire family of maximum entropy reinforcement learning...naturally emerges"
  - [corpus]: Weak - no corpus evidence on this specific connection between MLE and max-entropy RL
- Break condition: If the assumptions about known transition and accessible ground-truth distribution are violated

## Foundational Learning

- Concept: Maximum Likelihood Estimation with latent variables
  - Why needed here: The model learns from state-only sequences where actions are latent, requiring marginalization over action sequences during training
  - Quick check question: What is the main computational challenge when applying MLE to models with latent variables?

- Concept: Energy-based models and sampling
  - Why needed here: The policy is represented as an energy-based distribution requiring sampling techniques like Langevin dynamics for training and inference
  - Quick check question: How does the normalizing constant in energy-based models affect gradient computation?

- Concept: Importance sampling
  - Why needed here: Used to approximate posterior distributions without backpropagating through the transition model
  - Quick check question: What condition must hold for importance sampling to provide an unbiased estimate?

## Architecture Onboarding

- Component map:
  - State sequences from buffer -> Policy model (energy-based MLP) -> Prior samples (Langevin dynamics)
  - State sequences from buffer -> Transition model (deterministic MLP + noise) -> Importance weights
  - Prior samples + Importance weights -> Policy gradient update
  - Policy samples + Transition model -> Self-interaction data -> Transition model update

- Critical path:
  1. Sample demonstrations from buffer
  2. Generate prior samples via Langevin dynamics
  3. Compute importance weights using transition model
  4. Update policy via gradient from Eq. (19)
  5. Generate self-interaction data
  6. Update transition model via gradient from Eq. (9)

- Design tradeoffs:
  - Sampling vs. analytical methods: Sampling is necessary for continuous actions but introduces variance
  - Importance sampling vs. MCMC: Importance sampling avoids backprop through dynamics but requires good uncertainty estimates
  - Context length: Longer contexts increase expressivity but require more training data and computation

- Failure signatures:
  - Poor acceptance rates in curve planning indicate inability to capture non-Markovian patterns
  - High variance in importance weights suggests unreliable transition uncertainty estimates
  - Divergence in policy gradients may indicate sampling issues or poor initialization

- First 3 experiments:
  1. Verify the cubic curve planning experiment with context length 1 vs 4 to demonstrate non-Markovian necessity
  2. Test importance sampling vs. MCMC posterior sampling on a simple environment
  3. Validate the transition model by comparing self-interaction trajectories to demonstrations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of context length in LanMDP affect its performance on tasks with varying levels of non-Markovianity, and is there an optimal context length that generalizes well across different environments?
- Basis in paper: [explicit] The paper discusses the impact of context length on the cubic curve planning task and mentions varying context lengths (1, 2, 4, 6) but does not provide a systematic analysis of how context length affects performance across diverse environments
- Why unresolved: The experiments focus on a specific task and do not explore a wide range of environments with different degrees of non-Markovianity to establish a clear relationship between context length and performance
- What evidence would resolve it: Conducting experiments on a diverse set of tasks with varying levels of non-Markovianity, systematically varying the context length, and analyzing the performance trends to identify an optimal context length that generalizes well

### Open Question 2
- Question: Can the energy-based policy in LanMDP be extended to handle continuous action spaces more efficiently, reducing the computational overhead associated with sampling methods like MCMC?
- Basis in paper: [explicit] The paper mentions that the computational cost of sampling methods like MCMC can be significant for high-dimensional action spaces and suggests that action quantization could be a potential solution
- Why unresolved: The paper does not explore or evaluate the effectiveness of action quantization or other methods to improve the efficiency of sampling in continuous action spaces
- What evidence would resolve it: Implementing and testing alternative sampling methods, such as action quantization or variational inference, and comparing their computational efficiency and performance to the current MCMC-based approach in LanMDP

### Open Question 3
- Question: How does the performance of LanMDP compare to state-of-the-art model-based reinforcement learning methods when applied to non-Markovian domains, and what are the trade-offs between model-based and model-free approaches in such settings?
- Basis in paper: [explicit] The paper demonstrates that LanMDP achieves competitive performance compared to state-action baselines and state-only methods in MuJoCo tasks, but does not directly compare it to model-based reinforcement learning methods
- Why unresolved: The experiments focus on imitation learning and do not explore the potential of LanMDP in a reinforcement learning setting or compare it to model-based RL methods that could handle non-Markovian domains
- What evidence would resolve it: Implementing and testing LanMDP in a reinforcement learning setting, comparing its performance to model-based RL methods like MBPO or PETS on non-Markovian tasks, and analyzing the trade-offs between model-based and model-free approaches in terms of sample efficiency, computational cost, and performance

## Limitations

- Theoretical claims rely on strong assumptions about known transitions and accessible ground-truth distributions that may not hold in practice
- Computational overhead of MCMC sampling may be prohibitive for real-time applications and high-dimensional action spaces
- Experimental evaluation lacks systematic testing of robustness to varying degrees of non-Markovianity and partial observability

## Confidence

- Mechanism 3 (MLE to max-entropy RL connection): Low confidence - relies on assumptions about known transitions and accessible distributions
- Mechanism 2 (Importance sampling with transition uncertainty): Medium confidence - lacks empirical validation of uncertainty estimate reliability
- Mechanism 1 (Multi-modality compensation): Medium confidence - primarily supported by single cubic curve experiment

## Next Checks

1. **Ablation study on context length**: Systematically vary context length in MuJoCo environments to empirically demonstrate the trade-off between expressivity and sample efficiency, and validate that longer contexts improve performance in truly non-Markovian tasks

2. **Importance weight reliability analysis**: Measure the variance and distribution of importance weights across different environments and compare performance with and without the transition uncertainty weighting to quantify its contribution

3. **Real-world deployment test**: Evaluate LanMDP on a physical robot manipulation task with partial observability to assess whether the theoretical advantages translate to practical settings with sensor noise and latency