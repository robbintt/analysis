---
ver: rpa2
title: Cross-Domain Policy Adaptation via Value-Guided Data Filtering
arxiv_id: '2305.17625'
source_url: https://arxiv.org/abs/2305.17625
tags:
- domain
- target
- dynamics
- source
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of policy generalization across
  domains with dynamics mismatch, particularly in robotics. The authors propose Value-Guided
  Data Filtering (VGDF), an algorithm that selectively shares transitions from a source
  domain to a target domain based on the proximity of paired value targets across
  the two domains.
---

# Cross-Domain Policy Adaptation via Value-Guided Data Filtering

## Quick Facts
- arXiv ID: 2305.17625
- Source URL: https://arxiv.org/abs/2305.17625
- Authors: 
- Reference count: 40
- Primary result: VGDF outperforms prior methods like DARC and GARAT in cross-domain policy adaptation by selectively sharing transitions based on value consistency across domains

## Executive Summary
This paper addresses the challenge of policy generalization across domains with dynamics mismatch, particularly in robotics. The authors propose Value-Guided Data Filtering (VGDF), an algorithm that selectively shares transitions from a source domain to a target domain based on the proximity of paired value targets across the two domains. The key insight is that transitions with significant dynamics discrepancy but consistent value targets can be equivalent for policy adaptation. Experiments on various environments with kinematic and morphology shifts show that VGDF outperforms prior methods like DARC and GARAT. The method provides a simple yet effective approach to dynamics adaptation by focusing on value consistency rather than just dynamics discrepancy.

## Method Summary
VGDF uses a learned target domain dynamics model to generate paired transitions and selectively shares those with similar value targets. The algorithm collects source domain transitions using an exploration policy, generates fictitious target domain states using a dynamics ensemble, computes Fictitious Value Proximity (FVP) using value functions, and applies rejection sampling to select high FVP transitions. These filtered transitions are then used to update value functions and policies alongside limited target domain interactions. The method extends SAC with data filtering based on value consistency rather than just dynamics discrepancy.

## Key Results
- VGDF outperforms prior methods like DARC and GARAT on environments with kinematic and morphology shifts
- Ablation studies validate the effectiveness of both data selection and exploration components
- The method extends to offline source data scenarios
- FVP provides an effective metric for quantifying dynamics shifts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Transitions with significant dynamics discrepancy but consistent value targets are equivalent for policy adaptation
- **Mechanism**: By selecting source domain transitions where the paired value targets across domains are close, the method filters out transitions that would mislead the policy due to dynamics mismatch
- **Core assumption**: Value consistency across domains implies policy effectiveness even when dynamics differ
- **Evidence anchors**:
  - [abstract] "transitions with significant dynamics discrepancy but consistent value targets can be equivalent for policy adaptation"
  - [section 4.2] "we claim that the transitions with significant dynamics mismatch can be equivalent concerning the value estimations that evaluate the long-term behaviors"
- **Break condition**: If the learned value function cannot accurately estimate long-term returns in the target domain, the filtering becomes ineffective

### Mechanism 2
- **Claim**: Rejection sampling based on fictitious value proximity (FVP) selects transitions that generalize well to the target domain
- **Mechanism**: The method generates an ensemble of fictitious next states using learned dynamics models, estimates their values, and selects transitions where the source domain value is likely under the target domain value distribution
- **Core assumption**: The ensemble dynamics model can generate realistic next states that help estimate value proximity
- **Evidence anchors**:
  - [section 5.1] "we utilize the rejection sampling to select fixed percentage data (i.e., 25%) with the highest likelihood from a batch of source domain transitions"
  - [section 6.4] "we propose quantifying the dynamics shifts via the proposed Fictitious Value Proximity (FVP)"
- **Break condition**: If the dynamics model ensemble is highly inaccurate, FVP estimates will be unreliable

### Mechanism 3
- **Claim**: Optimistic exploration policy πE helps collect high-value source domain transitions for the target domain
- **Mechanism**: A separate exploration policy maximizes the upper confidence bound of Q-values, encouraging exploration of source domain states that might have high value in the target domain
- **Core assumption**: The upper confidence bound estimate captures uncertainty that leads to valuable exploration
- **Evidence anchors**:
  - [section 5.1] "we introduce an exploration policy πE that maximizes the approximate upper confidence bound of the Q-value"
  - [section 6.2] "Removing the optimistic exploration technique results in performance degradation in three out of four environments"
- **Break condition**: If the confidence bounds are poorly estimated, exploration may be inefficient or miss important regions

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs) and their components (states, actions, transition probabilities, rewards, discount factor)
  - **Why needed here**: The paper frames the cross-domain adaptation problem as learning policies across two MDPs with different dynamics but shared structure
  - **Quick check question**: What are the key components that define an MDP and how do they differ between source and target domains in this work?

- **Concept**: Value functions and Bellman equations for policy evaluation
  - **Why needed here**: The method relies on estimating value functions to compute FVP and select transitions, requiring understanding of how value functions capture long-term return
  - **Quick check question**: How does the value function V(s) relate to the expected return from state s under policy π?

- **Concept**: Rejection sampling and importance weighting for data selection
  - **Why needed here**: The algorithm uses rejection sampling based on FVP likelihood to filter source domain transitions, requiring understanding of these statistical techniques
  - **Quick check question**: What is the difference between rejection sampling and importance weighting, and when might one be preferred over the other?

## Architecture Onboarding

- **Component map**: Source domain interaction module -> Dynamics model ensemble -> Value function network -> Data filtering module -> Policy network -> Target domain interaction module

- **Critical path**: 
  1. Collect source domain transition (ssrc, asrc, rsrc, s'src)
  2. Generate fictitious target domain states using dynamics ensemble
  3. Compute FVP using value function and fictitious states
  4. Apply rejection sampling to select high FVP transitions
  5. Update value function with filtered source domain data and target domain data
  6. Update policies (exploration and main)

- **Design tradeoffs**:
  - Ensemble size vs. computational cost: Larger ensembles capture more uncertainty but are slower
  - Data selection ratio vs. sample efficiency: Higher ratios keep more data but may include less relevant transitions
  - Fixed temperature coefficient vs. adaptive entropy: Fixed is simpler but may not adapt well to different environments

- **Failure signatures**:
  - Poor performance despite filtering: Indicates dynamics models or value functions are inaccurate
  - Slow learning: May indicate exploration policy isn't finding valuable regions
  - Instability: Could result from poor FVP estimates or inappropriate selection ratios

- **First 3 experiments**:
  1. Grid world with simple kinematic shift: Validate basic mechanism works on controlled environment
  2. HalfCheetah with broken back thigh: Test on realistic robotic environment with kinematic mismatch
  3. Ant with short feet: Test on morphology mismatch scenario to verify generalization across shift types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ensemble size of the dynamics model affect the trade-off between computational cost and performance?
- Basis in paper: [explicit] The paper investigates the effect of ensemble size (M = 3, 5, 7) on performance in section F.2, showing that smaller ensembles can achieve competitive asymptotic performance.
- Why unresolved: While the paper demonstrates that smaller ensembles can achieve similar performance, it doesn't explore the computational cost implications or identify an optimal ensemble size for balancing performance and efficiency.
- What evidence would resolve it: A detailed analysis comparing the training time, inference time, and memory usage for different ensemble sizes, along with their impact on performance, would help identify the optimal trade-off.

### Open Question 2
- Question: How does the data selection ratio (ξ%) affect the performance of VGDF in different environments with varying degrees of dynamics mismatch?
- Basis in paper: [explicit] The paper investigates the effect of data selection ratio (ξ% = 10%, 25%, 50%, 75%) in section 6.2, showing robust performance within a specific range (ξ% ≤ 50%).
- Why unresolved: The paper doesn't explore how the optimal data selection ratio varies with the degree of dynamics mismatch or how it interacts with other hyperparameters like ensemble size.
- What evidence would resolve it: Experiments systematically varying the data selection ratio across environments with different dynamics mismatch levels, while controlling for other hyperparameters, would reveal the relationship between ξ% and performance.

### Open Question 3
- Question: Can the Fictitious Value Proximity (FVP) be used to quantify the degree of dynamics mismatch in a more general way?
- Basis in paper: [explicit] The paper suggests that FVP could be used to quantify domain differences in section 6.4, but the analysis is limited to specific environments and doesn't establish a general framework.
- Why unresolved: The paper doesn't provide a theoretical foundation for using FVP as a general measure of dynamics mismatch or explore its limitations and potential biases.
- What evidence would resolve it: A theoretical analysis of the properties of FVP as a measure of dynamics mismatch, along with empirical validation across a wider range of environments and dynamics shifts, would establish its general applicability.

## Limitations

- The method's performance depends heavily on the quality of learned dynamics models and value functions, which may be challenging to train accurately for complex robotic environments
- The fixed selection ratio (ξ%=25%) and ensemble size (M=7) were chosen empirically without systematic analysis of optimal hyperparameters
- The approach requires access to both source and target domain interactions, limiting applicability in scenarios where source domain data is unavailable

## Confidence

- **High Confidence**: The core mechanism of value-guided data filtering is well-supported by theoretical justification and empirical results across four distinct environments
- **Medium Confidence**: The claim that value consistency across domains implies policy effectiveness despite dynamics differences is supported by experimental results but would benefit from additional theoretical analysis
- **Low Confidence**: The robustness of the method to different ensemble sizes and selection ratios remains uncertain due to limited hyperparameter analysis

## Next Checks

1. **Dynamics Model Sensitivity**: Systematically vary ensemble size M and evaluate how performance scales, particularly focusing on the minimum ensemble size needed for reliable FVP estimates
2. **Hyperparameter Robustness**: Test the method's sensitivity to the selection ratio ξ% across different environments and determine if adaptive selection ratios might improve performance
3. **Theoretical Bounds**: Derive formal bounds on how value consistency relates to policy performance in the target domain to strengthen the theoretical foundation of the approach