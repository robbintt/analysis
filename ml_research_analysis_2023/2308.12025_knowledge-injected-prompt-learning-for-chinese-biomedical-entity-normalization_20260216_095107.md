---
ver: rpa2
title: Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization
arxiv_id: '2308.12025'
source_url: https://arxiv.org/abs/2308.12025
tags:
- entity
- knowledge
- learning
- medical
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses Chinese biomedical entity normalization (BEN),
  aiming to align non-standard medical entities to standardized terms. The proposed
  method, PL-Knowledge, leverages prompt learning with knowledge injection from an
  external medical knowledge base.
---

# Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization

## Quick Facts
- arXiv ID: 2308.12025
- Source URL: https://arxiv.org/abs/2308.12025
- Authors: 
- Reference count: 33
- Key outcome: Knowledge-injected prompt learning achieves 12.96% higher accuracy in few-shot scenarios and 0.94% in full-scale cases for Chinese biomedical entity normalization.

## Executive Summary
This paper addresses the challenging task of Chinese biomedical entity normalization (BEN), where non-standard medical entity mentions must be aligned to standardized terms. The proposed PL-Knowledge method leverages prompt learning with knowledge injection from the CMeKG medical knowledge graph to enhance entity disambiguation and matching. By encoding medical knowledge items and incorporating them into tailor-made prompt templates, the approach significantly outperforms baseline methods, particularly in few-shot scenarios where labeled data is scarce. The method demonstrates 12.96% accuracy improvement in few-shot cases and 0.94% in full-scale settings on the CHIP 2019 benchmark dataset.

## Method Summary
The PL-Knowledge method follows a systematic approach: first, candidate standard entities are matched to original entities using Jaccard similarity; second, relevant knowledge items are extracted from the CMeKG knowledge graph using longest overlap matching; third, knowledge items and relationships are encoded using embedding layers and BiGRU networks for contextual refinement; fourth, knowledge embeddings are injected into prompt templates through placeholder substitution; finally, the model predicts using MC-BERT with a verbalizer mapping [MASK] token predictions to binary labels. The approach supports three template types (manual, mixed with soft tokens, and knowledge-injected) and evaluates performance across different sample sizes.

## Key Results
- Achieves 12.96% higher accuracy than baselines in few-shot scenarios (64-shot training)
- Shows 0.94% improvement over baseline methods in full-scale training settings
- Mixed templates with soft tokens perform best in moderate-sample scenarios, while manual templates excel in extremely low-data regimes
- Knowledge injection becomes less effective when training data is extremely scarce (e.g., 16-shot cases)

## Why This Works (Mechanism)

### Mechanism 1
Knowledge injection via external medical knowledge graph improves entity normalization accuracy in few-shot scenarios. The method encodes medical knowledge items (diseases, surgeries, locations) from CMeKG and injects them into prompt templates, supplementing entity representations with domain-specific context. Core assumption: medical entities contain latent relationships captured by external knowledge. Break condition: if knowledge graph lacks relevant items, injection provides no signal.

### Mechanism 2
Prompt learning with tailored templates outperforms traditional fine-tuning in few-shot Chinese BEN. The method converts classification into cloze-style tasks using handcrafted prompt templates, reducing labeled data requirements. Core assumption: pre-trained language models can perform few-shot classification when guided by well-designed prompts. Break condition: poor template design leads to incorrect semantic relationships.

### Mechanism 3
Mixed templates with soft tokens provide better performance than purely manual or fully soft templates in moderate-sample scenarios. Soft tokens are learnable embeddings initialized with meaningful vectors or random values, allowing template structure adaptation during training. Core assumption: soft token flexibility enables learning task-specific prompt structures. Break condition: in extremely low-data regimes, soft tokens may cause overfitting, making manual templates more reliable.

## Foundational Learning

- Concept: Prompt learning and template design
  - Why needed here: Convert classification problem into natural language cloze task to leverage pre-trained language models without extensive fine-tuning
  - Quick check question: What is the role of the verbalizer in prompt learning, and how does it map model outputs to classification labels?

- Concept: External knowledge graph utilization
  - Why needed here: Chinese biomedical entities often have ambiguous or non-standard forms; external knowledge provides structured context to disambiguate and align entities
  - Quick check question: How does the longest overlap matching method extract relevant knowledge items for a given entity pair?

- Concept: Few-shot learning challenges
  - Why needed here: Chinese biomedical entity normalization suffers from limited annotated data; method must generalize effectively from small sample sizes
  - Quick check question: Why does knowledge injection become less effective when training data is extremely scarce (e.g., 16-shot)?

## Architecture Onboarding

- Component map: Preprocessing & candidate matching -> Knowledge extraction -> Knowledge encoding -> Prompt template construction -> Prediction output
- Critical path: 1) Input original and candidate entities 2) Match candidate entities via text similarity 3) Extract and encode relevant knowledge items 4) Inject knowledge embeddings into prompt template 5) Feed into MC-BERT and predict via verbalizer
- Design tradeoffs: Manual templates are stable but inflexible; mixed templates adapt but risk overfitting in low-data regimes; knowledge injection improves accuracy but adds computational overhead and depends on KG quality
- Failure signatures: Low accuracy in 16-shot cases indicates insufficient data to leverage knowledge injection; performance drops when knowledge graph lacks relevant items; poor template design leads to ambiguous predictions
- First 3 experiments: 1) Compare manual vs mixed vs knowledge-injected templates on 64-shot subset 2) Evaluate knowledge injection with Word2Vec vs random initialization 3) Test model performance on full dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed knowledge injection approach be extended to handle more complex knowledge structures beyond simple entity-relation pairs?
- Basis in paper: [explicit] The paper mentions using CMeKG but only extracts simple entity-relation pairs for knowledge injection
- Why unresolved: The paper only demonstrates knowledge injection using simple entity-relation pairs without exploring more complex structures like nested relationships or temporal information
- What evidence would resolve it: Experiments comparing performance of knowledge injection using simple entity-relation pairs versus more complex knowledge structures on the same BEN task

### Open Question 2
- Question: What is the optimal balance between manual template design and automated template learning for Chinese biomedical entity normalization?
- Basis in paper: [explicit] The paper compares manual, mixed, and knowledge-injected templates but doesn't explore the full spectrum between fully manual and fully automated design
- Why unresolved: The paper only tests a limited set of template designs, leaving open how much template design should be manual versus automated for optimal performance
- What evidence would resolve it: A systematic study varying the degree of template automation and measuring performance on the BEN task

### Open Question 3
- Question: How does the performance of the proposed method scale with increasing knowledge base size and diversity?
- Basis in paper: [inferred] The paper uses a specific knowledge base (CMeKG) but doesn't explore how performance changes with larger or more diverse knowledge bases
- Why unresolved: The paper doesn't provide insights into how model performance might change with much larger or more diverse knowledge bases
- What evidence would resolve it: Experiments systematically varying the size and diversity of the knowledge base while measuring BEN performance

## Limitations
- Evaluation scope limited to single Chinese BEN dataset without external validation
- Knowledge injection effectiveness depends heavily on CMeKG coverage and quality
- Template design process lacks detailed justification for specific token sequences
- Computational overhead of knowledge encoding and injection not discussed

## Confidence

- **High confidence**: Few-shot performance improvements (12.96% accuracy gain) well-supported by experimental results and align with established prompt learning principles
- **Medium confidence**: Mixed templates outperforming manual templates in moderate-sample scenarios supported by internal comparisons but lacks broader validation
- **Low confidence**: Scalability claims to full-scale scenarios based on single 0.94% improvement without analysis of diminishing returns or computational efficiency

## Next Checks
1. **Knowledge Graph Dependency Test**: Remove or partially mask knowledge items from CMeKG and measure performance degradation to quantify model's dependence on knowledge coverage
2. **Cross-Dataset Generalization**: Evaluate model on different Chinese biomedical entity normalization dataset or related medical entity linking task to assess transfer beyond CHIP 2019 benchmark
3. **Template Robustness Analysis**: Systematically vary template designs and measure performance sensitivity to identify optimal configurations and failure conditions