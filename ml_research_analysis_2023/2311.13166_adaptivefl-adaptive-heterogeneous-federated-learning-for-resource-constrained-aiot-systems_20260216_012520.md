---
ver: rpa2
title: 'AdaptiveFL: Adaptive Heterogeneous Federated Learning for Resource-Constrained
  AIoT Systems'
arxiv_id: '2311.13166'
source_url: https://arxiv.org/abs/2311.13166
tags:
- uni00000013
- uni00000048
- uni00000046
- adaptivefl
- heterogeneous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AdaptiveFL, a novel federated learning approach
  for resource-constrained AIoT systems. AdaptiveFL employs a fine-grained width-wise
  model pruning strategy to generate heterogeneous models for diverse AIoT devices.
---

# AdaptiveFL: Adaptive Heterogeneous Federated Learning for Resource-Constrained AIoT Systems

## Quick Facts
- arXiv ID: 2311.13166
- Source URL: https://arxiv.org/abs/2311.13166
- Reference count: 16
- This paper proposes AdaptiveFL, a novel federated learning approach for resource-constrained AIoT systems.

## Executive Summary
AdaptiveFL introduces a fine-grained width-wise model pruning strategy and reinforcement learning-based client selection to address heterogeneous AIoT device constraints in federated learning. The approach generates multiple model sizes from a single global model and dynamically assigns them to devices based on available resources. Experimental results demonstrate up to 16.83% accuracy improvement over state-of-the-art methods in both IID and non-IID scenarios, effectively addressing low classification performance due to device heterogeneity.

## Method Summary
AdaptiveFL implements fine-grained width-wise pruning controlled by width pruning ratio (rw) and starting layer index (I) to create heterogeneous model sizes. An RL-based client selection mechanism uses curiosity and resource tables to match models with suitable devices, avoiding failed training attempts. The system aggregates parameters across different model sizes by their corresponding indices in the original full model. Training employs SGD optimizer with learning rate 0.01, momentum 0.5, batch size 50, and local epoch 5 across 100-180 clients using CIFAR-10, CIFAR-100, and FEMNIST datasets.

## Key Results
- Achieves up to 16.83% inference accuracy improvement over state-of-the-art methods
- Effective in both IID and non-IID data distribution scenarios
- Successfully addresses low classification performance in heterogeneous AIoT environments

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained width-wise pruning improves inference performance by preserving shallow layers while pruning deep layers. The method prunes parameters from deep layers using a width pruning ratio while leaving shallow layers intact, allowing heterogeneous models to share more generalized parameters. This works because shallow layers contain more generalized features that should be preserved, while deep layers can be pruned with minimal performance impact.

### Mechanism 2
Reinforcement learning-based client selection reduces communication overhead by matching models to suitable devices. The RL system uses curiosity and resource tables to select devices that can successfully train specific model sizes, avoiding failed training attempts. This is effective because device training success correlates with historical training scores and model size compatibility.

### Mechanism 3
Heterogeneous model aggregation preserves performance by sharing parameters across different model sizes. Since all submodels are pruned from the same full global model, the cloud server can aggregate parameters by their corresponding indices in the full model. This works because parameters pruned from the same original model maintain semantic correspondence across different model sizes.

## Foundational Learning

- Concept: Federated Learning fundamentals (FedAvg algorithm)
  - Why needed here: AdaptiveFL builds upon standard FedAvg but modifies the aggregation process for heterogeneous models
  - Quick check question: How does FedAvg differ from standard centralized training, and what aggregation formula does it use?

- Concept: Model pruning techniques and their impact on performance
  - Why needed here: The paper's core innovation relies on understanding which layers can be pruned with minimal accuracy loss
  - Quick check question: What is the relationship between layer depth and parameter importance according to [9], and how does this justify the pruning strategy?

- Concept: Reinforcement Learning basics (Markov Decision Process formulation)
  - Why needed here: The client selection mechanism is formulated as an MDP with states, actions, and rewards
  - Quick check question: How does the MDP formulation in section 3.3 map to the client selection problem, and what are the state and action spaces?

## Architecture Onboarding

- Component map: Cloud server (model pruning, RL-based client selection, model aggregation modules) -> AIoT devices (model receiving, resource-aware pruning, local training, result uploading) -> Model pool (heterogeneous submodels at different size levels) -> RL tables (curiosity and resource tracking historical training success)

- Critical path: Model pruning → RL-based client selection → Local training → Model uploading → Model aggregation → Updated global model

- Design tradeoffs:
  - Fine-grained vs. coarse-grained pruning: Fine-grained provides better performance but requires more complex pruning logic
  - RL exploration vs. exploitation: Balancing curiosity rewards with resource-based selection to avoid getting stuck in local optima
  - Model size granularity: More model sizes provide better fit to devices but increase complexity of the model pool

- Failure signatures:
  - High communication waste rate indicates poor model-to-device matching
  - Low test accuracy despite successful training suggests pruning strategy is removing critical parameters
  - High variance in submodel accuracy indicates aggregation is not properly preserving knowledge transfer

- First 3 experiments:
  1. Test fine-grained vs. coarse-grained pruning on a single device type with controlled resource constraints to validate the pruning strategy
  2. Validate RL client selection by comparing success rates with random selection on a small, controlled device pool
  3. Test model aggregation across heterogeneous sizes on a synthetic dataset where ground truth parameter importance is known

## Open Questions the Paper Calls Out

### Open Question 1
How does the fine-grained width-wise model pruning strategy compare to other pruning techniques (e.g., depth-wise, unstructured) in terms of computational efficiency and performance for heterogeneous AIoT devices?

### Open Question 2
How does the RL-based client selection strategy perform under different levels of device heterogeneity and resource constraints?

### Open Question 3
How does the AdaptiveFL approach scale with an increasing number of AIoT devices and varying data distributions?

## Limitations

- Implementation details of the Split function and reward calculation remain unspecified, creating potential reproducibility challenges
- Core assumptions about layer depth and parameter importance rely on cited prior work rather than independent validation
- Limited evaluation on extremely heterogeneous device environments beyond those represented in experiments

## Confidence

- High confidence in basic federated learning framework and model aggregation approach
- Medium confidence in width-wise pruning mechanism, supported by theoretical justification but limited empirical validation
- Medium confidence in RL-based client selection, with theoretical soundness but potential sensitivity to hyperparameter tuning
- Low confidence in generalization to extremely heterogeneous device environments not represented in evaluation

## Next Checks

1. Ablation study on pruning strategy: Compare width-wise pruning against random pruning and structured pruning on a single model type to isolate the impact of the fine-grained approach

2. RL reward function sensitivity: Systematically vary the curiosity vs. resource reward weighting to determine optimal balance and identify failure modes

3. Cross-dataset robustness test: Evaluate AdaptiveFL on a significantly different dataset (e.g., medical imaging or sensor data) to assess generalization beyond computer vision tasks