---
ver: rpa2
title: Anomaly detection with semi-supervised classification based on risk estimators
arxiv_id: '2309.00379'
source_url: https://arxiv.org/abs/2309.00379
tags:
- methods
- have
- loss
- risk
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses anomaly detection under semi-supervised settings
  where unlabeled data may contain both normal and anomalous instances. It proposes
  two risk-based classification methods: a shallow model using an unbiased risk estimator
  and a deep model using a nonnegative (biased) risk estimator.'
---

# Anomaly detection with semi-supervised classification based on risk estimators

## Quick Facts
- arXiv ID: 2309.00379
- Source URL: https://arxiv.org/abs/2309.00379
- Authors: 
- Reference count: 40
- Key outcome: Risk-based semi-supervised AD methods significantly outperform OC-SVM and PU baselines on 26 datasets, with competitive deep learning performance

## Executive Summary
This paper addresses the problem of anomaly detection in semi-supervised settings where unlabeled data may contain both normal and anomalous instances. The authors propose two novel classification-based methods using risk estimators: a shallow model with an unbiased risk estimator and a deep model with a nonnegative (biased) risk estimator. These methods leverage labeled normal data, labeled anomalous data, and unlabeled data to improve detection performance. Theoretical analysis establishes estimation error and excess risk bounds for both approaches, while extensive experiments demonstrate significant improvements over existing baselines on benchmark datasets.

## Method Summary
The proposed methods formulate semi-supervised anomaly detection as a classification problem using risk minimization. The shallow model uses a linear decision function with a feature map and optimizes an unbiased risk estimator that incorporates both labeled and unlabeled data. The deep model extends this to neural networks with a nonnegative risk estimator. Both methods address the challenge that unlabeled data may contain anomalies by modeling the mixture distribution of the data. Regularization techniques ensure nonnegativity of the empirical risk in the shallow model, preventing overfitting issues identified in prior PU learning work. The methods are evaluated using AUC on multiple benchmark datasets, comparing against one-class SVM, semi-supervised OC-SVM, and PU learning baselines.

## Key Results
- Shallow risk-based method achieves significant improvements over OC-SVM and PU learning baselines on 26 benchmark datasets
- Deep risk-based method shows competitive performance against deep semi-supervised AD methods on MNIST, Fashion-MNIST, and CIFAR-10
- The nonnegative risk estimator effectively prevents negative empirical risks and overfitting in both shallow and deep models
- Performance is robust to changes in parameters a and πe_p, though manual tuning is still required for optimal results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The risk-based approach overcomes the unrealistic assumption that unlabeled data contains only normal instances by directly modeling the mixture distribution of labeled and unlabeled data.
- Mechanism: The paper introduces two risk estimators (unbiased and nonnegative) that incorporate both labeled normal, labeled anomalous, and unlabeled data. The nonnegative estimator uses a max{0, ...} term to ensure nonnegativity, preventing overfitting issues identified in prior work.
- Core assumption: The unlabeled data follows a mixture distribution p(x) = π_p p_p(x) + π_n p_n(x), where π_p and π_n are the class prior probabilities.
- Evidence anchors:
  - [abstract] "We propose two novel classification-based anomaly detection methods. Firstly, we introduce a semi-supervised shallow anomaly detection method based on an unbiased risk estimator."
  - [section] "In order to improve anomaly detection performance, we focus on the semi-supervised setting where a negative dataset is also available."
  - [corpus] Weak evidence - no directly related work on this specific mechanism found in corpus.
- Break condition: If the mixture assumption is violated (e.g., unlabeled data distribution differs significantly from the assumed form), the risk estimators may become biased and performance degrades.

### Mechanism 2
- Claim: The theoretical analysis provides estimation error bounds and excess risk bounds for both risk minimizers, establishing their statistical learning properties.
- Mechanism: Using Rademacher complexity analysis and McDiarmid's inequality, the paper derives bounds on the difference between empirical risk minimizers and true risk minimizers for both the unbiased (ˆR²_s) and nonnegative (ˆR¹_s) estimators.
- Core assumption: The function class G is bounded (||g||_∞ ≤ C_g) and the loss function is Lipschitz continuous over the bounded domain.
- Evidence anchors:
  - [section] "We additionally establish estimation error bounds and excess risk bounds for the two risk minimizers, building upon the theoretical findings presented in Kiryo et al. (2017); Niu et al. (2016)."
  - [section] "Theorem 2 (Estimation error bound for ˆg1)... For any δ > 0, the following inequality hold with probability at least 1-δ"
  - [corpus] Weak evidence - corpus contains general risk bounds work but not specific to this AD formulation.
- Break condition: If the Lipschitz continuity assumption fails or the function class is too complex, the derived bounds may not hold, leading to overfitting.

### Mechanism 3
- Claim: The proposed regularization techniques ensure nonnegativity of the empirical risk in the shallow model, preventing the negative empirical risk problem identified in PU learning.
- Mechanism: Theorem 1 provides specific conditions on the loss function (boundedness and linear-odd properties) and regularization parameters that guarantee the empirical risk remains nonnegative, avoiding the overfitting issues that occur when negative empirical risks are minimized.
- Core assumption: The loss function satisfies the conditions ℓ(t,-1) - ℓ(t,+1) ≥ -b₁|t| and ℓ(t,-1) ≥ b₂(b₃ - |t|) for some positive constants b₁, b₂, b₃.
- Evidence anchors:
  - [section] "We develop methods to select suitable regularization that ensures the nonnegativity of the empirical risk in the proposed shallow AD method."
  - [section] "Theorem 1 Suppose there exist positive constants b₁, b₂ and b₃ such that..." with specific loss function examples in Table 1.
  - [corpus] Weak evidence - corpus contains general regularization work but not this specific nonnegativity guarantee.
- Break condition: If the loss function doesn't satisfy the boundedness conditions or if regularization is insufficient, the empirical risk can become negative, leading to severe overfitting.

## Foundational Learning

- Concept: Risk minimization in statistical learning theory
  - Why needed here: The entire approach is based on minimizing risk estimators derived from the classification risk, so understanding the relationship between empirical risk, true risk, and generalization bounds is essential.
  - Quick check question: What is the difference between empirical risk and true risk, and why do we need generalization bounds?

- Concept: Rademacher complexity and generalization bounds
  - Why needed here: The theoretical analysis relies on Rademacher complexity to bound the estimation error of the risk minimizers, which is a standard tool in statistical learning theory.
  - Quick check question: How does Rademacher complexity relate to the capacity of a function class and its generalization ability?

- Concept: Positive-unlabeled (PU) learning framework
  - Why needed here: The paper builds upon PU learning techniques but extends them to the semi-supervised setting with both positive and negative labeled data available.
  - Quick check question: What is the key difference between PU learning and the semi-supervised setting considered in this paper?

## Architecture Onboarding

- Component map: Data pipeline (labeled normal, labeled anomalous, unlabeled) -> Risk estimator computation -> Optimization engine -> Model class (shallow/deep) -> Evaluation module (AUC)
- Critical path: Data preprocessing → Risk estimator computation → Optimization → Model evaluation
- Design tradeoffs: Shallow vs deep models (expressiveness vs computational cost), unbiased vs nonnegative estimators (bias-variance tradeoff), regularization strength (overfitting vs underfitting)
- Failure signatures: Negative empirical risk values, poor AUC on validation data, overfitting to training data
- First 3 experiments:
  1. Verify the nonnegativity of the empirical risk with different regularization strengths on synthetic data
  2. Compare the unbiased vs nonnegative estimators on a benchmark dataset with known class priors
  3. Test the sensitivity of AUC to the mixture parameter a and class prior estimates π̂_p, π̂_n

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of risk-based anomaly detection methods vary when the unlabeled data contains anomalies from multiple classes versus a single class?
  - Basis in paper: The authors note that in their deep rAD experiments, "the anomalous data in our generation process can originate from more than one of the nine classes (unlike in the setup of deep SAD where the anomaly is only from one of the nine classes)" and observe that "the performance of deep SAD degrades over the performance reported in Ruff et al. (2020). The degradation is more severe for CIFAR-10."
  - Why unresolved: The paper only compares performance when anomalies come from multiple classes versus a single class for deep SAD, not for their proposed deep rAD method. The impact on rAD performance remains unexplored.
  - What evidence would resolve it: Systematic experiments comparing rAD performance when anomalies come from 1, 2, 5, and all 9 classes, measuring AUC across these settings.

- **Open Question 2**: What is the theoretical relationship between the estimation error bounds established for the risk minimizers and the actual anomaly detection performance in practice?
  - Basis in paper: The authors establish "estimation error bounds and excess risk bounds for the two risk minimizers" in Section 4, but acknowledge in Section 7 that "although the risk bounds are established for the proposed risk minimizers in Section 4, we still need the assumption that πp and πn are known in advance, which is a limitation."
  - Why unresolved: The paper provides theoretical bounds but doesn't empirically validate how these bounds correlate with actual AUC performance or investigate how estimation errors in class priors affect real-world detection accuracy.
  - What evidence would resolve it: Empirical studies measuring the gap between theoretical bounds and actual AUC scores across datasets, and experiments varying the accuracy of πp/πn estimates to quantify their impact on detection performance.

- **Open Question 3**: Can the optimal combination of (a, πe_p) for risk-based anomaly detection be learned from the data rather than requiring manual tuning?
  - Basis in paper: The authors state in Section 7 that "Although the experiments have shown that our rAD methods are quite robust to the changes of the parameters a and πe_p, we still have to tune them to obtain the best AD performance" and propose in the conclusion that "One possible research direction is to develop a method that can learn the best combination of (a, πe_p) from the available data."
  - Why unresolved: The paper demonstrates robustness to parameter choices but doesn't provide a systematic method for automatic parameter selection, leaving practitioners to rely on grid search or domain knowledge.
  - What evidence would resolve it: Development and validation of a meta-learning approach or cross-validation strategy that automatically selects optimal (a, πe_p) values, with experiments showing improved or comparable performance to manual tuning across diverse datasets.

## Limitations

- The theoretical analysis relies on bounded function classes and Lipschitz continuous losses, which may not hold for deep neural networks in practice
- Empirical evaluation for deep methods is limited to only 3 datasets (MNIST, Fashion-MNIST, CIFAR-10), with mixed performance relative to deep SAD
- The assumption that unlabeled data may contain anomalies introduces additional complexity in estimating class priors that could affect performance

## Confidence

- **High confidence**: The core mechanism of using nonnegative risk estimators to prevent overfitting in semi-supervised AD, supported by both theory (Theorem 1) and empirical results (significant improvements over baselines on 26 datasets)
- **Medium confidence**: The theoretical bounds for estimation error and excess risk, as they depend on assumptions that may be violated in practice (e.g., bounded function classes for deep models)
- **Medium confidence**: The deep rAD method's competitive performance, as it's tested on only 3 datasets and the results show mixed performance relative to deep SAD

## Next Checks

1. Test robustness to prior estimation: Systematically vary the estimates of class priors π̂_p and π̂_n to assess how sensitive the methods are to prior misspecification, particularly when the true unlabeled data contains a mixture of normal and anomalous instances.

2. Evaluate on multi-class anomaly scenarios: Extend the deep rAD evaluation to settings where anomalies come from multiple classes simultaneously, to better understand the limitations mentioned in the paper regarding this scenario.

3. Verify nonnegativity guarantee: Implement the regularization selection from Theorem 1(iii) and verify empirically that the shallow rAD method maintains nonnegative empirical risk across different loss functions and datasets, as this is crucial for preventing overfitting.