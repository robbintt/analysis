---
ver: rpa2
title: Spectral State Space Models
arxiv_id: '2312.06837'
source_url: https://arxiv.org/abs/2312.06837
tags:
- sequence
- spectral
- learning
- system
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new architecture for sequence modeling based
  on spectral filtering techniques for learning linear dynamical systems. The key
  idea is to use spectral filters as fixed convolutional filters in a state space
  model, which gives provable robustness properties without requiring learning of
  the filters.
---

# Spectral State Space Models

## Quick Facts
- arXiv ID: 2312.06837
- Source URL: https://arxiv.org/abs/2312.06837
- Reference count: 40
- Primary result: Spectral state space models (STUs) can efficiently learn marginally stable linear dynamical systems where traditional SSMs fail, achieving exponential decay in error as filter count increases.

## Executive Summary
This paper introduces spectral state space models (STUs) that use fixed spectral filters as convolutional filters in a state space model architecture. By leveraging spectral filtering techniques, STUs can efficiently learn marginally stable linear dynamical systems where traditional SSMs struggle due to training instability. The approach provides provable robustness properties that are independent of the underlying system's spectrum and dimensionality, addressing key limitations of existing sequence modeling methods.

## Method Summary
The STU architecture projects input sequences onto spectral filters (eigenvectors of a Hankel matrix) and combines this with a small auto-regressive component for stability. The spectral filters are fixed and chosen based on theoretical properties of the Hankel matrix, avoiding the need to learn unstable system matrices directly. The model is trained using mini-batch gradient descent with l2 loss, and is evaluated on synthetic marginally stable linear dynamical systems. The key innovation is using spectral filtering to approximate the system dynamics rather than directly parameterizing system matrices.

## Key Results
- STUs achieve exponential decay in reconstruction error as the number of spectral filters increases
- STUs train significantly more stably than Linear Recurrent Units (LRU) on marginally stable systems
- STUs demonstrate provable robustness properties independent of the spectrum of underlying dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: STUs can efficiently learn marginally stable LDS where traditional SSMs fail
- Mechanism: Fixed spectral filters avoid learning unstable system matrices directly, providing provable robustness
- Core assumption: Spectral filtering can approximate marginally stable LDS with symmetric matrices
- Evidence anchors:
  - [abstract]: "Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem."
  - [section 3]: "The main advantage of spectral filtering is that for some special types of linear dynamical systems, notably those which have symmetric matrices A, the effective memory required to represent an observation in the spectral basis is independent of δ!"

### Mechanism 2
- Claim: STUs achieve exponential decay in reconstruction error as filter count increases
- Mechanism: Spectral filters are eigenvectors of Hankel matrices with exponentially decaying eigenvalues
- Core assumption: Hankel matrix spectrum decays exponentially to enable accurate approximation with finite filters
- Evidence anchors:
  - [abstract]: "The STU is also shown to achieve exponential decay in error as the number of filters increases, matching theoretical predictions."
  - [section 3]: "Lemma A.4 (Lemma E.3 [HSZ17]). Let σj be the top jth eigenvalue of Z. Then we have that σj ≤ min(3/4, Γc−j/log(L)) where c = eπ2/4 and Γ ≤ 10−6 is an absolute constant."

### Mechanism 3
- Claim: STUs are more stable to train than traditional SSMs on marginally stable systems
- Mechanism: Spectral parameterization provides convex learning landscape vs non-convex system matrix optimization
- Core assumption: Convex parameterization of spectral filters leads to more stable optimization
- Evidence anchors:
  - [section 5.1]: "Curiously we observe that for the LRU training plateaus completely for the first 50% of training highlighting the difficulty of optimization via a non-convex landscape."
  - [section 5.1]: "We observe that the STU is significantly more efficient at learning the LDS as opposed to LRU... the STU has a stable optimization trajectory and the loss decreases continuously."

## Foundational Learning

- Concept: Linear dynamical systems (LDS)
  - Why needed here: STUs are built upon LDS theory, understanding LDS is crucial for grasping STU motivation
  - Quick check question: What are the four system matrices (A, B, C, D) in an LDS, and what role does each play in the system's evolution?

- Concept: Spectral filtering
  - Why needed here: Spectral filtering enables STUs to efficiently learn long-range dependencies without instability
  - Quick check question: How do spectral filters differ from standard convolutional filters, and why are they particularly suited for learning LDS?

- Concept: Hankel matrices and their spectral properties
  - Why needed here: Spectral filters are eigenvectors of Hankel matrices, understanding their properties is essential
  - Quick check question: What is the structure of a Hankel matrix, and why do its eigenvalues decay exponentially in STUs?

## Architecture Onboarding

- Component map: Input featurization -> Spectral component + Auto-regressive component -> Output
- Critical path: Input → Featurization → Spectral component + Auto-regressive component → Output
- Design tradeoffs:
  - Number of filters (K): More filters allow better approximation but increase computational cost
  - Choice of Hankel matrix: Different constructions may lead to different spectral properties and performance
  - Balance between auto-regressive and spectral components: Auto-regressive provides stability but may limit long-range memory
- Failure signatures:
  - Poor performance on long-range dependency tasks: May indicate insufficient filters or inappropriate Hankel matrix
  - Training instability: May suggest issues with auto-regressive component or initialization
  - Overfitting: May occur if filter count is too large relative to dataset size
- First 3 experiments:
  1. Synthetic LDS learning: Train STU on synthetically generated marginally stable LDS and compare to traditional SSM
  2. Ablation study on K: Vary filter count and measure impact on reconstruction error and computational cost
  3. Real-world sequence modeling: Apply STU to language modeling or time series forecasting and compare to baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between STU's exponential error decay rate and filter count K, and how does this compare to theoretical predictions?
- Basis in paper: [explicit] Paper shows empirical exponential decay but doesn't provide rigorous mathematical proof or closed-form expression
- Why unresolved: While empirical evidence exists, formal mathematical proof demonstrating decay rate and comparison with theoretical predictions is lacking
- What evidence would resolve it: Formal mathematical proof of exponential decay rate with respect to K and comparison with theoretical predictions

### Open Question 2
- Question: How does STU's performance on marginally stable LDS compare to S4 or DSS in terms of sample complexity and computational efficiency?
- Basis in paper: [inferred] Paper compares STU to LRU but doesn't directly compare to other state-of-the-art methods
- Why unresolved: Paper focuses on demonstrating advantages over LRU without comprehensive comparison to other methods
- What evidence would resolve it: Experimental results comparing STU to S4, DSS, and others on marginally stable LDS tasks with sample complexity and computational efficiency metrics

### Open Question 3
- Question: What are STU's limitations for handling non-symmetric or non-normal system matrices, and how can these be addressed?
- Basis in paper: [explicit] Paper states STU is designed for symmetric matrices but doesn't discuss limitations for non-symmetric or non-normal matrices
- Why unresolved: Paper lacks detailed analysis of performance on non-symmetric/non-normal matrices, leaving applicability questions open
- What evidence would resolve it: Experimental results and theoretical analysis of STU performance on non-symmetric and non-normal matrices with potential modifications to address limitations

## Limitations

- Limited evaluation to synthetic data without testing on real-world marginally stable systems
- Architecture sensitivity to hyperparameter choices (K, Hankel matrix construction) not thoroughly explored
- Performance on non-symmetric or non-normal system matrices not characterized

## Confidence

- Theoretical foundation connecting spectral filtering to Hankel matrices: **High**
- Efficient learning of marginally stable systems: **Medium**
- Exponential error decay property: **Medium**
- Generalization beyond specific architecture choices: **Low**

## Next Checks

1. Test STU performance on marginally stable LDS with added observation noise to evaluate robustness under realistic conditions
2. Apply STUs to real-world datasets with known marginally stable dynamics to validate claims beyond synthetic data
3. Systematically vary filter count K and test different Hankel matrix constructions to determine sensitivity to architectural choices