---
ver: rpa2
title: 'Optimizing with Low Budgets: a Comparison on the Black-box Optimization Benchmarking
  Suite and OpenAI Gym'
arxiv_id: '2310.00077'
source_url: https://arxiv.org/abs/2310.00077
tags:
- optimization
- budget
- bbob
- algorithms
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares Bayesian optimization (BO) methods commonly
  used in machine learning with classical black-box optimization heuristics on two
  benchmark suites: the BBOB functions and the OpenAI Gym reinforcement learning environment.
  The study focuses on low-budget scenarios where the number of function evaluations
  is limited, as is often the case in expensive optimization problems.'
---

# Optimizing with Low Budgets: a Comparison on the Black-box Optimization Benchmarking Suite and OpenAI Gym

## Quick Facts
- arXiv ID: 2310.00077
- Source URL: https://arxiv.org/abs/2310.00077
- Reference count: 40
- BO-based optimizers perform well on low-budget optimization tasks but are outperformed by classical heuristics as budgets increase

## Executive Summary
This paper presents a comprehensive comparison of Bayesian optimization (BO) methods with classical black-box optimization heuristics on two benchmark suites: the BBOB functions and OpenAI Gym reinforcement learning environments. The study focuses on low-budget scenarios where function evaluations are limited, which is typical for expensive optimization problems. The authors evaluate multiple state-of-the-art BO algorithms including Turbo, AX, SMAC, HyperOpt, and Optuna, along with classical baselines like CMA-ES, Cobyla, PSO, and NGOpt. Results show that BO-based methods excel when budgets are constrained but become less competitive as evaluation budgets increase, while classical heuristics scale better to higher budgets.

## Method Summary
The study uses the Nevergrad platform to evaluate optimization algorithms on two benchmark suites: the BBOB benchmark with 24 functions across various dimensions (2-40D), and OpenAI Gym environments with neural network policies. The authors test BO-based methods (Turbo, AX, SMAC, HyperOpt, Optuna, LA-MCTS) against classical heuristics (CMA-ES, Cobyla, PSO, NGOpt) under low-budget conditions ranging from 10 to 3200 function evaluations. Performance is measured using empirical cumulative distribution functions of runtimes to reach optimal objective values, winning rates, and average loss across multiple random instances.

## Key Results
- BO-based optimizers achieve superior performance on both BBOB and OpenAI Gym benchmarks when evaluation budgets are limited
- As budgets increase, CMA-ES and differential evolution algorithms become more competitive and often outperform BO methods
- Proper initialization scaling is critical for performance, particularly in the OpenAI Gym environment with unbounded domains
- BO methods incur significantly higher computational costs compared to classical heuristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian optimization (BO) methods are effective for low-budget black-box optimization problems due to their ability to model the objective function as a random process and use acquisition functions to balance exploration and exploitation.
- Mechanism: BO treats the objective function as a random function, places a prior over it (typically a Gaussian process), and uses the posterior mean and uncertainty to guide the search for promising solutions through an acquisition function.
- Core assumption: The objective function is expensive to evaluate, making it worthwhile to invest computational resources in building a surrogate model to guide the search.
- Evidence anchors:
  - [abstract] "BO-based algorithms are popular in the ML community, as they are used for hyperparameter optimization and more generally for algorithm configuration. However, their efficiency decreases as the dimensionality of the problem and the budget of evaluations increase."
  - [section] "BO [14], [33] is a sequential design strategy targeting global optimization of black-box functions that do not assume any functional forms. It is particularly advantageous for problems where the objective function is difficult to evaluate, is a black box with some known structure, relies upon less than 20 dimensions, and where no information about sensitivity and derivatives is available."
  - [corpus] Evidence is weak; the corpus papers do not directly address the mechanisms of BO for low-budget optimization.
- Break Condition: When the dimensionality of the problem or the budget of evaluations becomes large, the computational cost of BO increases significantly, making it less efficient than other methods.

### Mechanism 2
- Claim: Monte Carlo Tree Search (MCTS) can be adapted for black-box optimization by recursively learning space partitions and using BO within selected regions to improve performance.
- Mechanism: MCTS progressively learns and generalizes promising regions in the problem space by recursively partitioning, allowing solvers like BO to access these regions and improve their performance.
- Core assumption: The problem space can be effectively partitioned into promising and unpromising regions, and BO can benefit from focusing on the promising regions.
- Evidence anchors:
  - [abstract] "Monte Carlo Tree Search (MCTS) [19] is a solver that migrated from trees of bandits for games and control, including alpha-zero [39], [40], to applications in BBO [18], [41]. Its evolved version, LA-MCTS [18], progressively learns and generalizes promising regions in the problem space by recursively partitioning so that solvers like BO can access these regions to improve their performance."
  - [section] "MCTS uses the Monte Carlo simulation to accumulate value estimates that lead to highly rewarding trajectories in the search tree. In other words, MCTS pays more attention to promising nodes (i.e., subregions of the search space), in order to avoid the need to brute force all possibilities."
  - [corpus] Evidence is weak; the corpus papers do not directly address the mechanisms of MCTS for low-budget optimization.
- Break Condition: When the problem space cannot be effectively partitioned or when the computational cost of partitioning becomes prohibitive.

### Mechanism 3
- Claim: Classical black-box optimization algorithms, such as CMA-ES and Cobyla, can perform well on both BBOB and OpenAI Gym benchmarks, especially when the budget of evaluations is limited.
- Mechanism: These algorithms use heuristics or mathematical programming techniques to search for optimal solutions without relying on a surrogate model, making them computationally efficient and effective for low-budget problems.
- Core assumption: The problem structure allows for effective search strategies without the need for a surrogate model, and the computational cost of the algorithm is low enough to be feasible within the given budget.
- Evidence anchors:
  - [abstract] "Our results confirm that BO-based optimizers perform well on both benchmarks when budgets are limited, albeit with a higher computational cost, while they are often outperformed by algorithms from other families when the evaluation budget becomes larger."
  - [section] "As baselines commonly used in the broader BBO context we consider CMA, which stands for CMA-ES, a well-known evolution strategy [27], Cobyla, a tool from mathematical programming [23], particle swarm optimization (PSO [50]), and NGOpt from Nevergrad [5], a wizard that combines many classical algorithms in various ways [51]."
  - [corpus] Evidence is weak; the corpus papers do not directly address the mechanisms of classical BBO algorithms for low-budget optimization.
- Break Condition: When the problem dimensionality increases or the budget of evaluations becomes large, the performance of these algorithms may deteriorate compared to more sophisticated methods.

## Foundational Learning

- Concept: Gaussian Processes
  - Why needed here: Gaussian processes are used as a prior distribution over functions in Bayesian optimization, allowing for the modeling of the objective function as a random process.
  - Quick check question: What is the role of the kernel function in a Gaussian process, and how does it affect the modeling of the objective function?

- Concept: Monte Carlo Tree Search
  - Why needed here: MCTS is adapted for black-box optimization by recursively learning space partitions and using BO within selected regions to improve performance.
  - Quick check question: How does MCTS balance exploration and exploitation in the search tree, and what are the key components of the UCB policy used for node selection?

- Concept: Evolutionary Strategies
  - Why needed here: Evolutionary strategies, such as CMA-ES, are used as classical black-box optimization algorithms that can perform well on both BBOB and OpenAI Gym benchmarks.
  - Quick check question: How does CMA-ES adapt the covariance matrix of the search distribution to improve convergence, and what are the key differences between CMA-ES and other evolutionary strategies?

## Architecture Onboarding

- Component map: BBOB benchmark suite -> optimization algorithms -> performance metrics -> comparison analysis
- Critical path: Run optimization algorithms on benchmark problems → collect performance data → analyze ECDFs and winning rates → draw conclusions about algorithm effectiveness
- Design tradeoffs: BO methods offer better sample efficiency but higher computational overhead; classical heuristics are computationally cheaper but may require more evaluations
- Failure signatures: Poor initialization scaling, overfitting to specific problem instances, low statistical significance in performance differences
- First 3 experiments:
  1. Run the BBOB benchmark suite with a low budget (10D function evaluations) and compare the performance of BO-based methods against classical heuristics like CMA-ES and Cobyla.
  2. Run the OpenAI Gym benchmark with tiny and bigger neural nets (scaling factor 1 and 3) for budgets 25, 50, 100, 200, 400, 800, 1600, 3200 function evaluations.
  3. Analyze the scaling behavior of the algorithms by increasing the dimensionality of the problems and the budget of evaluations, and identify the point at which BO methods become less competitive compared to classical heuristics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performances of BO-based algorithms scale with increasing problem dimensionality beyond 40 dimensions?
- Basis in paper: [explicit] The paper notes that BO-based algorithms incur higher computational costs and their performance decreases with increasing dimensionality, but only tests up to 40 dimensions.
- Why unresolved: The paper only tests up to 40 dimensions due to computational constraints, leaving the performance of BO-based algorithms on higher-dimensional problems unclear.
- What evidence would resolve it: Conducting experiments with BO-based algorithms on problems with dimensions greater than 40 to compare their performance and computational costs.

### Open Question 2
- Question: How does the initialization scaling affect the performance of optimization algorithms on OpenAI Gym problems with unbounded domains?
- Basis in paper: [explicit] The paper mentions that the OpenAI Gym benchmark is sensitive to variable scaling and that the scale of algorithms is particularly critical for these problems.
- Why unresolved: The paper optimizes the scaling for Bayesian Optimization methods but does not explore how different initialization scalings affect other algorithms' performance.
- What evidence would resolve it: Systematically varying the initialization scaling for different algorithms on OpenAI Gym problems and measuring their performance.

### Open Question 3
- Question: What is the impact of different budget sizes on the relative performance of BO-based algorithms versus classical heuristics in the OpenAI Gym environment?
- Basis in paper: [explicit] The paper shows that BO-based methods perform well for small budgets but are outperformed by other algorithms as the budget increases.
- Why unresolved: The paper only tests budgets up to 3200 evaluations, leaving the long-term trend unclear.
- What evidence would resolve it: Extending the experiments to even larger budgets to observe the crossover point where classical heuristics consistently outperform BO-based methods.

## Limitations

- Computational cost analysis for BO methods is not fully quantified, making it difficult to assess practical tradeoffs between performance and resource requirements
- Limited evaluation of higher-dimensional problems (only up to 10D in some experiments) restricts generalizability to real-world applications
- Results show significant variability in performance across different BBOB functions, but the paper doesn't systematically analyze which problem characteristics favor which algorithm families

## Confidence

- High confidence: BO methods perform well on low-budget optimization tasks across both BBOB and OpenAI Gym benchmarks
- Medium confidence: CMA-ES and classical heuristics become more competitive as budget increases, though the exact crossover point varies by problem
- Low confidence: The claim about BO methods' performance deterioration with increasing dimensionality is based on limited experimental evidence

## Next Checks

1. Implement systematic scaling analysis by running experiments at 5D, 10D, 20D, and 40D for all algorithms to identify the exact budget and dimensionality thresholds where BO methods become less competitive
2. Measure and report wall-clock time for all algorithms to quantify the computational overhead of BO methods beyond function evaluations
3. Conduct ablation studies removing problem-specific hyperparameter tuning to assess how much performance variance is due to overfitting versus algorithm choice