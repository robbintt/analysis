---
ver: rpa2
title: 'LLM-Based Agent Society Investigation: Collaboration and Confrontation in
  Avalon Gameplay'
arxiv_id: '2310.14985'
source_url: https://arxiv.org/abs/2310.14985
tags:
- agents
- players
- game
- avalon
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the social behaviors of large language
  model (LLM)-based agents in the Avalon communication game, a strategic game requiring
  collaboration and deception. The authors propose a novel multi-agent framework that
  guides agents through gameplay using system prompts and a modular design inspired
  by human reasoning, including memory storage, analysis, planning, action, and response
  generation.
---

# LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay

## Quick Facts
- **arXiv ID**: 2310.14985
- **Source URL**: https://arxiv.org/abs/2310.14985
- **Reference count**: 40
- **Primary result**: Proposed framework achieves 70% win rate as evil side and 100% as good side in Avalon, with agents exhibiting emergent social behaviors like deception and high persuasion.

## Executive Summary
This study investigates how large language model (LLM)-based agents behave in the social deduction game Avalon, focusing on their ability to collaborate and deceive. The authors propose a modular multi-agent framework that guides agents through gameplay using system prompts and structured reasoning modules (memory, analysis, planning, action, response). Experiments show the framework achieves strong win rates and reveals sophisticated social behaviors, including high leadership approval, effective persuasion, and spontaneous deception tactics like assuming alternate identities. The work demonstrates that LLM agents can navigate complex social interactions and highlights the importance of analyzing adversaries for strategic success.

## Method Summary
The study uses ChatGPT as the LLM backend to implement a multi-agent framework for Avalon gameplay. The framework consists of six modules: setup (role assignment), memory storage and summarization, analysis and planning, action and response generation, and experience learning. Agents are guided by system prompts tailored to each role and game state. Experiments involve 40 consecutive Avalon games (20 with agents as evil side, 20 as good side) against a competitive baseline adapted from Werewolf AI agents. Performance is measured through gameplay outcomes (win rates, quest engagement, failure votes) and social behaviors (leadership approval, self-recommendation success, deception detection, teamwork, and confrontation rates).

## Key Results
- Framework achieves 70% win rate when playing as evil side and 100% win rate as good side in Avalon.
- High leadership approval rates (>80%) and strong persuasion skills (high self-recommendation success) observed.
- Agents spontaneously exhibit deception tactics, such as assuming alternate identities (e.g., Morgana assuming loyal servant identity 35% of the time).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework's modular design mirrors human reasoning and enables dynamic strategy adaptation.
- Mechanism: Memory stores conversation history, analysis infers opponent roles, planning formulates strategies, action decides moves, and response generates natural language explanations. This decomposition allows agents to process incomplete information and adjust tactics.
- Core assumption: LLMs can simulate human-like strategic reasoning when guided by structured prompts and memory management.
- Evidence anchors: [abstract] mentions modular components including memory, analysis, planning, action, and response. [section 4] provides detailed module descriptions.
- Break condition: If memory exceeds LLM input limits or analysis planning fails to adapt, the modular approach collapses into reactive behavior.

### Mechanism 2
- Claim: Agents spontaneously exhibit deception by assuming alternate identities without explicit instructions.
- Mechanism: In the first round, agents in roles like Morgana and Assassin sometimes choose to conceal or assume other identities (e.g., Morgana assuming a loyal servant identity 35% of the time). This behavior emerges from the response generation module, which allows agents to "assume the identity of the opposing faction."
- Core assumption: LLMs can generate deceptive strategies autonomously when the response generation module is not constrained to truthfulness.
- Evidence anchors: [abstract] mentions spontaneous deception tactics. [section 6.3] discusses deception rates and agent behavior in Morgana and Assassin roles.
- Break condition: If the response generation module is constrained to truthfulness or if deception is explicitly forbidden, this spontaneous behavior would not emerge.

### Mechanism 3
- Claim: The experience learning module allows agents to improve strategies over time by learning from both their own and others' gameplay.
- Mechanism: The framework includes two learning modes: self-role strategy learning (generating suggestions for the agent's own role) and other-role strategy learning (analyzing opponents' strategies). These suggestions are incorporated into future gameplay instructions, enabling agents to refine their tactics.
- Core assumption: LLMs can effectively extract and apply strategic insights from game logs to improve future performance.
- Evidence anchors: [abstract] mentions the framework can learn from experience. [section 4.5] describes the experience learning module and its two-step process.
- Break condition: If the learning process fails to generalize from past games or if the suggestions become too specific to individual players, the learning mechanism would lose effectiveness.

## Foundational Learning

- **Concept**: Social deduction games (e.g., Avalon)
  - Why needed here: Understanding the game mechanics and social dynamics is crucial for designing agents that can navigate the complex interactions in Avalon.
  - Quick check question: What are the key differences between Avalon and other social deduction games like Werewolf?

- **Concept**: Multi-agent systems
  - Why needed here: The framework relies on multiple LLM agents interacting and collaborating (or competing) within the game environment.
  - Quick check question: How does the framework facilitate communication and coordination between agents?

- **Concept**: Large language model prompting
  - Why needed here: The framework uses carefully crafted prompts to guide LLM agents through each module, ensuring they perform the desired actions and generate appropriate responses.
  - Quick check question: What are the key components of the system prompts used to assign roles and guide gameplay?

## Architecture Onboarding

- **Component map**: Setup -> Memory Storage/Summarization -> Analysis/Planning -> Action/Response Generation -> Experience Learning
- **Critical path**: The critical path for agent decision-making is: memory â†’ analysis â†’ planning â†’ action â†’ response. Each step depends on the output of the previous one.
- **Design tradeoffs**: The modular design allows for flexibility and adaptability but also introduces complexity in coordinating the modules. The use of LLM for each module provides powerful reasoning capabilities but also requires careful prompt engineering and memory management.
- **Failure signatures**: Common failure modes include: memory overflow leading to loss of context, analysis errors resulting in incorrect role inference, planning failures causing suboptimal strategies, and response generation issues leading to nonsensical or inconsistent behavior.
- **First 3 experiments**:
  1. Test each module in isolation with simple prompts to ensure they perform their intended functions.
  2. Run a full game with all modules enabled and observe the agent's behavior and performance.
  3. Conduct an ablation study by removing one module at a time to assess its impact on the overall framework.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different LLM architectures (e.g., transformer variants, attention mechanisms) impact the emergence of specific social behaviors (deception, persuasion, leadership) in multi-agent Avalon gameplay?
  - Basis in paper: [inferred] The paper uses a fixed architecture (gpt-3.5-turbo-16k) but does not explore how architectural variations might influence the observed social behaviors.
  - Why unresolved: The study focuses on a single LLM architecture and does not investigate the impact of architectural differences on agent behavior.
  - What evidence would resolve it: Comparative experiments using different LLM architectures with identical prompting and evaluation protocols to measure differences in social behavior emergence.

- **Open Question 2**: What is the relationship between the complexity of system prompts and the sophistication of social strategies employed by LLM agents in Avalon?
  - Basis in paper: [explicit] The paper mentions "Abstracted Strategy Sð‘ð‘– for gameplay" as a component of system prompts, but does not explore how varying prompt complexity affects agent strategies.
  - Why unresolved: The study uses predefined prompts but does not systematically vary prompt complexity to assess its impact on agent behavior.
  - What evidence would resolve it: Controlled experiments varying prompt complexity while keeping other factors constant, measuring changes in agent strategy sophistication and success rates.

- **Open Question 3**: How does the inclusion of explicit ethical constraints in system prompts affect the emergence of deceptive behaviors in LLM agents during Avalon gameplay?
  - Basis in paper: [inferred] The paper observes spontaneous deception (e.g., Morgana assuming alternate identities) but does not investigate whether explicit ethical constraints would alter this behavior.
  - Why unresolved: The study does not explore the effect of ethical constraints on agent behavior, leaving open the question of whether deception is an emergent property or a result of prompt design.
  - What evidence would resolve it: Experiments comparing agent behavior with and without explicit ethical constraints in prompts, measuring changes in deceptive behavior frequency and success rates.

## Limitations
- Baseline comparison lacks detailed methodology and performance metrics, limiting assessment of claimed improvements.
- Generalization of spontaneous deception across different game setups and agent populations is unclear.
- Effectiveness of the experience learning module in consistently improving agent strategies over time is not empirically validated.

## Confidence

- **High Confidence**: The framework's modular design and basic functionality in guiding LLM agents through Avalon gameplay. Reported winning rates (70% as evil, 100% as good) are based on conducted experiments.
- **Medium Confidence**: Analysis of social behaviors such as leadership approval rates (>80%) and self-recommendation success. These metrics are derived from experiment data but may be influenced by specific agent configurations and game dynamics.
- **Low Confidence**: Spontaneous emergence of deception tactics and effectiveness of the experience learning module in consistently improving agent strategies over time. These claims are supported by observations but lack rigorous quantitative validation.

## Next Checks
1. **Baseline Replication**: Obtain and replicate the exact methodology and performance metrics of the competitive baseline used in the study to ensure a fair and accurate comparison of the proposed framework's improvements.
2. **Deception Generalization Test**: Conduct experiments with varied agent populations and game configurations to assess whether the spontaneous deception tactics observed in Morgana and Assassin roles generalize to other roles and scenarios.
3. **Learning Effectiveness Quantification**: Design a longitudinal study to track the performance of agents over multiple games, quantifying the impact of the experience learning module on strategic improvements and consistency of outcomes.