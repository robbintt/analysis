---
ver: rpa2
title: Accommodating Missing Modalities in Time-Continuous Multimodal Emotion Recognition
arxiv_id: '2311.10119'
source_url: https://arxiv.org/abs/2311.10119
tags:
- modalities
- missing
- when
- multimodal
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multimodal emotion recognition
  when some modalities are missing. The authors propose a Transformer-based architecture
  that uses cross-attention to weigh the importance of different modalities and an
  auto-regressive approach to incorporate past predictions.
---

# Accommodating Missing Modalities in Time-Continuous Multimodal Emotion Recognition

## Quick Facts
- arXiv ID: 2311.10119
- Source URL: https://arxiv.org/abs/2311.10119
- Authors: [Not specified in input]
- Reference count: 40
- Key outcome: 37% improvement in CCC for arousal and 30% improvement for valence prediction using optimized training strategy for missing modalities

## Executive Summary
This paper addresses the challenge of multimodal emotion recognition when some modalities are missing, proposing a Transformer-based architecture with cross-attention for modality weighting and an auto-regressive decoder for incorporating past predictions. The authors develop a training strategy that randomly hides important modalities during training to improve robustness to missing data. Experimental results on the Ulm-TSST dataset demonstrate significant improvements in concordance correlation coefficient (CCC) compared to a late-fusion baseline, particularly when modalities are missing.

## Method Summary
The authors propose a MultiModal Transformer Encoder (MMTE) that processes individual modalities with Temporal Convolutional Networks (TCNs), adds positional and modality encodings, then applies bidirectional attention. An Auto-regressive MultiModal Transformer Decoder (AMMTD) uses cross-attention to weigh modality importance and self-attention to incorporate past predictions. The Emotion Regression Network (ERN) produces final valence/arousal values. A key innovation is the training strategy that identifies important modalities and randomly hides them during training with specific probabilities to improve robustness when those modalities are missing in practice.

## Key Results
- 37% improvement in CCC for arousal prediction compared to late-fusion baseline
- 30% improvement in CCC for valence prediction with optimized training strategy
- Training with hidden important modalities improves performance even when all modalities are present

## Why This Works (Mechanism)

### Mechanism 1
Cross-attention in the decoder enables modality weighting rather than temporal weighting. The decoder's cross-attention module attends only to encoder outputs at the current time-step across modalities, forcing the model to learn importance weights for each modality independently of temporal dependencies.

### Mechanism 2
Auto-regressive decoder with past predictions improves time-continuous emotion recognition. The decoder uses previous predictions as input features, allowing the model to incorporate temporal context from predicted values rather than raw input features.

### Mechanism 3
Training with hidden important modalities improves robustness to missing modalities. During training, the model randomly hides identified important modalities with certain probabilities, forcing it to learn from less informative modalities and become less reliant on any single modality.

## Foundational Learning

- Concept: Cross-attention vs self-attention
  - Why needed here: Cross-attention allows the model to weigh information from different modalities, while self-attention handles temporal dependencies within each modality.
  - Quick check question: What is the difference between cross-attention and self-attention in transformer architectures?

- Concept: Auto-regressive prediction
  - Why needed here: Emotion recognition in time-continuous settings requires incorporating past predictions to maintain temporal coherence.
  - Quick check question: Why might using past predictions as input be better than using raw features for time-continuous tasks?

- Concept: Concordance correlation coefficient (CCC)
  - Why needed here: CCC measures agreement between predicted and ground truth values, accounting for both correlation and bias, making it suitable for emotion regression tasks.
  - Quick check question: How does CCC differ from simple correlation coefficient in evaluating regression performance?

## Architecture Onboarding

- Component map: Raw features → TCNs → MMTE → AMMTD (auto-regressive) → ERN → predictions
- Critical path: Raw features pass through modality-specific TCNs, then the MMTE encoder, followed by the auto-regressive AMMTD decoder, and finally the ERN regression layer
- Design tradeoffs: Using cross-attention for modality weighting vs temporal weighting, auto-regressive approach vs direct prediction, training with hidden modalities vs training with all modalities present
- Failure signatures: Poor performance when a modality is missing, temporal discontinuities in predictions, modality imbalance in predictions
- First 3 experiments:
  1. Test with all modalities present, no missing data, to establish baseline performance
  2. Test with each individual modality removed to identify most important modalities
  3. Test with optimized training strategy (hiding important modalities) to verify robustness improvements

## Open Questions the Paper Calls Out

### Open Question 1
How do the proposed models perform on datasets with more diverse emotional expressions compared to the Ulm-TSST dataset used in this study? The paper focuses on a single dataset with specific types of emotional expressions but does not compare performance on other datasets with different emotional contexts.

### Open Question 2
What is the impact of using different types of features on the model's performance and robustness to missing modalities? The paper uses specific features (eGeMAPS for audio, FAU for video, concatenated physiological signals) but does not explore the impact of using alternative features.

### Open Question 3
How does the model's performance scale with an increasing number of modalities? The paper uses three modalities but does not explore scenarios with more modalities.

### Open Question 4
What is the effect of the optimized training strategy on model performance in real-world scenarios where modalities are missing unpredictably? The paper's experiments are controlled and do not reflect the unpredictability of real-world scenarios.

## Limitations

- Experimental validation limited to a single dataset (Ulm-TSST) with small sample sizes (35/10/10 train/validation/test split)
- Hyperparameter optimization for modality hiding probabilities performed empirically without reporting search process or ranges
- No confidence intervals or statistical significance tests reported for performance improvements
- Evaluation focuses on CCC without comprehensive analysis of failure cases and error patterns

## Confidence

**High Confidence:** Architectural design choices (TCNs for feature extraction, Transformer encoder-decoder structure, CCC as loss function) are well-established and properly implemented. Ablation studies showing modality importance are straightforward to verify.

**Medium Confidence:** 37% and 30% CCC improvements reported but without confidence intervals or statistical significance testing. Robustness claims to missing modalities are supported but could benefit from more systematic evaluation.

**Low Confidence:** Claim that optimized training strategy is superior to standard training lacks direct comparative evidence. Paper mentions comparing against late-fusion baseline but doesn't clearly show performance against same architecture trained with all modalities always present.

## Next Checks

**Validation Check 1:** Reproduce baseline late-fusion model and compare performance when trained with all modalities always present versus trained with optimized hiding strategy to validate whether hiding strategy provides meaningful improvements.

**Validation Check 2:** Systematically evaluate model performance across different missing modality patterns (only audio missing, only video missing, physiological signals missing, combinations thereof) to understand which types of missing data the model handles well and which remain challenging.

**Validation Check 3:** Test model architecture on at least one additional multimodal emotion recognition dataset (e.g., RECOLA or DEAP) to assess generalizability beyond Ulm-TSST dataset, including both full modality performance and missing modality scenarios.