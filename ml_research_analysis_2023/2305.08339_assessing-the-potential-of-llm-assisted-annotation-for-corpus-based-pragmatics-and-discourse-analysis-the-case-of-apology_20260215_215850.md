---
ver: rpa2
title: 'Assessing the potential of LLM-assisted annotation for corpus-based pragmatics
  and discourse analysis: The case of apology'
arxiv_id: '2305.08339'
source_url: https://arxiv.org/abs/2305.08339
tags:
- annotation
- bing
- apology
- chatbot
- apologising
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the potential of Large Language Models (LLMs)
  in assisting pragma-discursive corpus annotation, focusing on the speech act of
  apology. It compared GPT-3.5 (ChatGPT), GPT-4 (Bing chatbot), and human annotation
  performance.
---

# Assessing the potential of LLM-assisted annotation for corpus-based pragmatics and discourse analysis: The case of apology

## Quick Facts
- arXiv ID: 2305.08339
- Source URL: https://arxiv.org/abs/2305.08339
- Reference count: 8
- GPT-4 achieved F1 scores of 99.95% for APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER in apology annotation

## Executive Summary
This study evaluates the potential of Large Language Models (LLMs) for pragma-discursive corpus annotation, focusing on the speech act of apology. The researchers compared GPT-3.5 (ChatGPT), GPT-4 (Bing chatbot), and human annotation performance on 5,539 instances containing "sorry" from the Spoken BNC2014 corpus. GPT-4 significantly outperformed GPT-3.5 and achieved F1 scores ranging from 89.74% to 99.95% across different functional elements, demonstrating strong semantic and pragmatic understanding despite being slightly less accurate than human annotation overall.

## Method Summary
The study used few-shot prompting with carefully curated exemplars to annotate apology speech acts in the Spoken BNC2014 corpus. Researchers extracted 5,539 instances containing "sorry" and tested both GPT-3.5 and GPT-4 using the same prompt format with 15-20 exemplar annotations. Performance was evaluated by comparing LLM outputs to human annotations on a subset of 1,000 instances, measuring precision, recall, and F1 scores for five functional elements: APOLOGISING, REASON, APOLOGISER, APOLOGISEE, and INTENSIFIER.

## Key Results
- GPT-4 achieved significantly higher F1 scores than GPT-3.5 across all annotation categories
- GPT-4's performance was close to human annotation, with only slightly lower overall accuracy (99.2% vs 99.8%)
- The study demonstrated that LLMs can effectively handle the semantic and pragmatic aspects of language in corpus annotation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's superior performance in pragmatic annotation stems from its ability to infer context and intent beyond surface lexical forms.
- Mechanism: GPT-4 leverages deep contextual embeddings to distinguish pragmatic functions (e.g., apology vs. sympathy) where surface forms are ambiguous.
- Core assumption: The LLM's training data included sufficient pragmatic contexts to model nuanced language use.
- Evidence anchors:
  - [abstract] GPT-4 significantly outperformed GPT-3.5 with F1 scores of 99.95% for APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER.
  - [section] The study notes that GPT-4 demonstrated strong capacities in understanding both the semantic and pragmatic aspects of language.
- Break condition: If the pragmatic function relies heavily on cultural or domain-specific knowledge absent from training data.

### Mechanism 2
- Claim: Few-shot prompting with carefully curated exemplars enables LLMs to generalize annotation rules effectively.
- Mechanism: The prompt provides diverse examples of functional elements, allowing the model to infer patterns and apply them to new instances.
- Core assumption: The exemplar selection covers the variability in how functional elements are realized.
- Evidence anchors:
  - [section] The study describes using few-shot prompting with exemplars selected based on frequent clusters and tested for accuracy.
  - [section] The prompt design emphasized conciseness, semantic clarity, and explicit tagging to enhance performance.
- Break condition: If the prompt exemplars are too narrow or contain inconsistencies, leading to pattern overfitting or confusion.

### Mechanism 3
- Claim: GPT-4's architecture allows it to handle nested and overlapping functional elements more accurately than GPT-3.5.
- Mechanism: The model's attention mechanisms and contextual understanding enable precise boundary detection and hierarchical tagging.
- Core assumption: The architecture supports multi-label classification within a single utterance.
- Evidence anchors:
  - [section] GPT-4 achieved high F1 scores across multiple tags (APOLOGISING, REASON, APOLOGISER, APOLOGISEE, INTENSIFIER) in the same annotation task.
  - [section] The study highlights GPT-4's ability to correctly annotate complex instances with multiple functional elements.
- Break condition: If the input length exceeds model token limits or if elements are highly interdependent.

## Foundational Learning

- Concept: Pragmatic annotation
  - Why needed here: Understanding the distinction between semantic and pragmatic features is crucial for evaluating LLM performance in this task.
  - Quick check question: Can you explain why automatic tagging of pragmatic phenomena is more challenging than part-of-speech tagging?

- Concept: Few-shot learning
  - Why needed here: The study relies on few-shot prompting to adapt the LLM to the specific annotation task without fine-tuning.
  - Quick check question: How does few-shot learning differ from zero-shot and supervised learning in the context of LLM prompting?

- Concept: Local grammar analysis
  - Why needed here: The study uses local grammar as a framework for identifying functional elements of speech acts, which informs the annotation schema.
  - Quick check question: What are the key analytical procedures in local grammar analysis, and how do they apply to speech act annotation?

## Architecture Onboarding

- Component map: Corpus preprocessing (extraction of sorry instances) -> Prompt engineering (exemplar selection and formatting) -> LLM inference (annotation generation) -> Evaluation (precision, recall, F1 calculation)
- Critical path: Prompt design -> Exemplar testing -> Annotation generation -> Result validation
- Design tradeoffs: Using few-shot prompting avoids fine-tuning costs but may limit performance on highly novel cases; prompt conciseness improves accuracy but may omit edge cases
- Failure signatures: Inconsistent tagging across similar instances, failure to detect pragmatic intent, or incorrect element boundaries
- First 3 experiments:
  1. Test prompt with a small set of diverse exemplars and measure annotation accuracy.
  2. Vary exemplar order and formatting to identify optimal prompt structure.
  3. Compare GPT-3.5 and GPT-4 outputs on the same sample to quantify performance differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-assisted annotation compare to other semi-automated annotation approaches like Bootstrapping or Prodigy?
- Basis in paper: [explicit] The paper discusses Bootstrapping and Prodigy as semi-supervised annotation methods in NLP, but does not directly compare their performance to LLM-assisted annotation
- Why unresolved: The study focuses on comparing LLMs to human annotation rather than comparing different semi-automated approaches
- What evidence would resolve it: Direct comparison studies using the same datasets and annotation tasks with Bootstrapping, Prodigy, and LLM-assisted methods

### Open Question 2
- Question: What are the specific limitations of current prompting strategies for pragmatic annotation tasks beyond the speech act of apology?
- Basis in paper: [explicit] The paper discusses prompting strategies for apology annotation but notes that "the inner workings of the 'black box' of LLMs are still obscure"
- Why unresolved: The study only examines one specific speech act (apology) and doesn't explore limitations for other pragmatic phenomena
- What evidence would resolve it: Comparative studies testing prompting strategies across multiple pragmatic annotation tasks (e.g., requests, complaints, gratitude)

### Open Question 3
- Question: What are the minimum requirements for corpus size and linguistic complexity to make LLM-assisted annotation cost-effective compared to human annotation?
- Basis in paper: [inferred] The paper discusses the potential of LLMs to make annotation "more efficient, scalable and accessible" but doesn't provide cost-effectiveness thresholds
- Why unresolved: The study doesn't analyze the economic trade-offs between human and LLM annotation across different corpus characteristics
- What evidence would resolve it: Cost-benefit analyses comparing human and LLM annotation across corpora of varying sizes and linguistic complexities

## Limitations
- The study focused exclusively on instances containing "sorry" from a single corpus, limiting generalizability to other speech acts or domains
- Unknown exact prompt formatting details and exemplar selection criteria may affect reproducibility
- The performance metrics are based on a relatively small human-annotated validation set (1,000 instances)

## Confidence
- High: GPT-4's superior performance compared to GPT-3.5 for pragmatic annotation tasks
- Medium: Generalizability of results beyond the specific domain of apologies
- Medium: Comparison with human annotation accuracy, though GPT-4 achieved slightly lower overall accuracy (99.2% vs 99.8%)

## Next Checks
1. Test GPT-4's performance on annotating different speech acts (e.g., requests, complaints) to assess generalizability beyond apologies.
2. Evaluate annotation accuracy on corpora from different English-speaking regions or cultures to identify potential bias in pragmatic interpretation.
3. Systematically examine instances where GPT-4 achieved lower F1 scores to identify specific patterns of failure and refine prompt engineering strategies.