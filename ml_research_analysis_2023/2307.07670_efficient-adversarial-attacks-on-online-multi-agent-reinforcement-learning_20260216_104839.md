---
ver: rpa2
title: Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning
arxiv_id: '2307.07670'
source_url: https://arxiv.org/abs/2307.07670
tags:
- attack
- policy
- reward
- attacker
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies poisoning attacks on multi-agent reinforcement
  learning (MARL). An attacker can manipulate actions and rewards to guide agents
  to a target policy or maximize rewards under an attacker-chosen reward function.
---

# Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.07670
- Source URL: https://arxiv.org/abs/2307.07670
- Authors: 
- Reference count: 40
- Key outcome: Mixed attack strategies can efficiently force MARL agents to follow target policies with sublinear cost and loss.

## Executive Summary
This paper investigates poisoning attacks on multi-agent reinforcement learning (MARL) systems, where an attacker can manipulate both actions and rewards to guide agents toward a target policy or maximize rewards under an attacker-chosen reward function. The authors demonstrate that neither action-only nor reward-only attacks are universally effective, establishing fundamental limitations for each approach. They introduce a mixed attack strategy that combines both action and reward poisoning, showing it can efficiently force agents to follow target policies in gray-box settings. For black-box settings where the target policy is unknown, they propose an approximate mixed attack strategy that first explores to identify an optimal policy before applying the attack.

## Method Summary
The paper studies adversarial attacks on MARL agents in Markov games, where an attacker can manipulate both actions and rewards. Three attack settings are considered: white-box (full environment information), gray-box (knows target policy but not environment), and black-box (knows neither). The authors prove that action-only or reward-only attacks have fundamental limitations and cannot always efficiently force target policies. They introduce mixed attack strategies that combine action and reward poisoning, which are shown to efficiently force agents to follow target policies in gray-box settings with sublinear cost and loss. For black-box settings, an approximate mixed attack strategy is proposed that first explores to identify an optimal policy before applying the attack. The main results are theoretical bounds on attack cost and loss, showing that mixed attack strategies can achieve sublinear scaling.

## Key Results
- Mixed attack strategies can efficiently force MARL agents to follow target policies with sublinear cost and loss in gray-box settings
- Action-only and reward-only attacks have fundamental limitations and cannot always efficiently force target policies
- Approximate mixed attack strategies work in black-box settings by combining exploration with targeted manipulation
- The attacker can efficiently attack MARL agents with sublinear cost and loss even when the target policy is unknown

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mixed attack strategies can efficiently force MARL agents to follow a target policy with sublinear cost and loss.
- **Mechanism:** The attacker uses both action and reward poisoning to ensure that the target policy becomes the unique Nash equilibrium in the agents' observed environment. By manipulating actions when agents deviate from the target policy and nullifying rewards for non-target actions, the attacker ensures that following the target policy is always optimal from each agent's perspective.
- **Core assumption:** The target policy has positive rewards under the attacker's reward function (Rmin > 0), and the attacker can manipulate both actions and rewards.
- **Evidence anchors:**
  - [abstract] "A mixed attack strategy using both action and reward poisoning is introduced, which is shown to efficiently force agents to follow a target policy in gray-box settings."
  - [section 4] "If the attacker follows the mixed attack strategy the best response of each agent i towards any product policy π−i is π†i. The optimal policy π† is the unique {NE, CE, CCE}."
- **Break condition:** If the target policy yields zero rewards under the attacker's reward function, or if the attacker can only manipulate one of action or reward.

### Mechanism 2
- **Claim:** Action poisoning only attacks have fundamental limitations and cannot always efficiently force target policies.
- **Mechanism:** Action poisoning alone cannot create the necessary reward structure to make the target policy uniquely optimal. Even with full knowledge of the environment, there exist Markov games where no action-only strategy can achieve sublinear loss and cost.
- **Core assumption:** The post-attack environment must maintain reward boundaries within [0,1] to avoid detection.
- **Evidence anchors:**
  - [section 3.1] "There exists a target policy π† and a MG such that no action poisoning Markov attack strategy alone can efficiently and successfully attack MARL agents."
  - [section 3.1] "There exists a MG and a target policy π† such that no reward poisoning Markov attack strategy alone can efficiently and successfully attack MARL agents."
- **Break condition:** If the attacker can manipulate rewards beyond the [0,1] boundary, making detection irrelevant.

### Mechanism 3
- **Claim:** Approximate mixed attack strategies work in black-box settings by combining exploration with targeted manipulation.
- **Mechanism:** The attacker first explores to identify an approximate optimal policy through optimistic value function estimation, then switches to a mixed attack strategy targeting that policy. This two-phase approach achieves sublinear regret despite not knowing the target policy initially.
- **Core assumption:** The optimal policy can be identified within sublinear time through exploration, and the environment is stationary during the attack phase.
- **Evidence anchors:**
  - [section 5] "The attacker will receive the attacker reward r†. Since the optimal (target) policy that maximizes the attacker's reward is unknown, the attacker needs to explore the environment to obtain the optimal policy."
  - [section 5] "We build a confidence bound to show the value function difference between π∗ and π† in the following lemma."
- **Break condition:** If the optimal policy changes during exploration, or if exploration requires super-linear time.

## Foundational Learning

- **Concept: Markov Games and Nash Equilibrium**
  - Why needed here: The attack strategies rely on forcing the agents to learn policies that form specific equilibria in the modified game.
  - Quick check question: What distinguishes a Nash equilibrium from a correlated equilibrium in the context of multi-agent reinforcement learning?

- **Concept: Sublinear Regret Bounds**
  - Why needed here: The efficiency of attack strategies is measured by whether their cost and loss scale sublinearly with time steps.
  - Quick check question: How does a regret bound of O(√T) differ from O(T) in terms of attack efficiency?

- **Concept: Optimistic Value Function Estimation**
  - Why needed here: The black-box attack strategy uses upper confidence bounds to balance exploration and exploitation when identifying target policies.
  - Quick check question: What role does the bonus term B(N) = (H√S+1)√(log(2AHτ/δ)/(2N)) play in the exploration phase?

## Architecture Onboarding

- **Component map:** Attacker module -> Environment wrapper -> Agent learning module -> Evaluation module
- **Critical path:** Observe agent actions and rewards → Decide whether to poison (action, reward, or both) → Apply manipulation and update internal estimates → Evaluate whether target policy is being followed → Adjust strategy based on observed outcomes
- **Design tradeoffs:**
  - Action vs reward poisoning: Action poisoning is more detectable but can be more direct; reward poisoning is stealthier but may require more precise calibration
  - Exploration vs exploitation in black-box settings: Longer exploration improves target policy accuracy but increases initial cost
  - Granularity of manipulation: Fine-grained attacks (state-action specific) are more effective but require more information
- **Failure signatures:**
  - Linear growth in attack cost or loss indicates the attack strategy is not efficient
  - Agents learning policies that are not equilibria in the post-attack environment
  - High variance in estimated value functions during exploration phase
- **First 3 experiments:**
  1. Implement the mixed attack strategy on a simple 2x2 Markov game and verify sublinear cost/loss scaling
  2. Test the η-gap attack on a game where Condition 2 holds, comparing against baseline no-attack performance
  3. Evaluate the approximate mixed attack on V-learning agents in a black-box setting with varying exploration times τ

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the necessary and sufficient conditions for an action poisoning only attack strategy to be efficient and successful in forcing MARL agents to follow a target policy in general Markov games?
- Basis in paper: [explicit] The paper shows that there exist some MGs where no action poisoning only Markov attack strategy can be efficient and successful (Theorem 1), but also provides sufficient conditions (Condition 1) under which d-portion attack can be efficient and successful.
- Why unresolved: The paper only provides sufficient conditions, not necessary ones. The space of MGs where action poisoning fails is not fully characterized.
- What evidence would resolve it: Characterizing the exact class of MGs and target policies for which action poisoning is provably inefficient, or proving a necessary condition that holds for all such cases.

### Open Question 2
- Question: Can the approximate mixed attack strategy be extended to more complex MARL algorithms beyond V-learning while maintaining similar sublinear cost and loss bounds?
- Basis in paper: [inferred] The paper analyzes the effectiveness of approximate mixed attack on V-learning agents, but doesn't explore other algorithms like Q-learning, Nash Q-learning, or policy gradient methods.
- Why unresolved: The analysis relies on specific properties of V-learning (e.g., the adversarial bandit update structure) that may not hold for other algorithms.
- What evidence would resolve it: Theoretical analysis showing the attack strategy's performance on other MARL algorithms, or empirical evaluation across multiple algorithm types.

### Open Question 3
- Question: How does the performance of the mixed attack strategy scale with the number of agents m in large-scale multi-agent systems?
- Basis in paper: [inferred] The analysis shows the attack cost and loss scale as O(m) in terms of the number of agents (e.g., Theorems 7 and 8), but doesn't examine how this scales in practice for large m.
- Why unresolved: The theoretical bounds may not reflect practical scaling behavior, and the attack may become less effective as m increases due to coordination complexity.
- What evidence would resolve it: Empirical studies varying the number of agents in different game types, and theoretical analysis of the attack's effectiveness in high-dimensional action spaces.

## Limitations
- Theoretical guarantees rely heavily on specific conditions (Condition 1 and 2) that may not hold in many practical scenarios
- Gray-box attack assumes knowledge of the target policy, which may not be realistic in many applications
- Black-box approach's effectiveness depends on the exploration phase correctly identifying an optimal policy within sublinear time

## Confidence
- **High confidence**: The theoretical framework for mixed attack strategies in gray-box settings (Mechanism 1) is well-established with formal proofs
- **Medium confidence**: The impossibility results for action-only and reward-only attacks (Mechanism 2) are theoretically sound but may have limited practical relevance
- **Low confidence**: The black-box attack strategy's practical effectiveness (Mechanism 3) depends on empirical factors not fully explored in the paper

## Next Checks
1. **Condition verification**: Systematically test whether Conditions 1 and 2 hold across a diverse set of Markov games to understand the practical applicability of the attack strategies
2. **Target policy identification**: Evaluate the black-box attack's exploration phase in environments with multiple optimal policies to assess how well it identifies targetable policies
3. **Robustness testing**: Test the attack strategies against agents using different MARL algorithms (beyond V-learning) and with varying levels of robustness to reward manipulation