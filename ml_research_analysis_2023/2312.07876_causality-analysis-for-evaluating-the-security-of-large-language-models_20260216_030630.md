---
ver: rpa2
title: Causality Analysis for Evaluating the Security of Large Language Models
arxiv_id: '2312.07876'
source_url: https://arxiv.org/abs/2312.07876
tags:
- uni00000013
- uni00000011
- uni00000003
- neuron
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework called CASPER for conducting lightweight
  causality analysis of LLMs at token, layer, and neuron level. The authors applied
  CASPER to analyze open-source LLMs like Llama2 and Vicuna and made several interesting
  discoveries.
---

# Causality Analysis for Evaluating the Security of Large Language Models

## Quick Facts
- **arXiv ID**: 2312.07876
- **Source URL**: https://arxiv.org/abs/2312.07876
- **Reference count**: 37
- **Key outcome**: Introduces CASPER framework for lightweight causality analysis of LLMs, discovering that RLHF causes overfitting to harmful prompts and identifying a mysterious neuron with disproportionate causal effect on model output

## Executive Summary
This paper introduces CASPER, a framework for conducting lightweight causality analysis of Large Language Models (LLMs) at token, layer, and neuron levels. The authors applied CASPER to analyze open-source LLMs like Llama2 and Vicuna, revealing that RLHF fine-tuning leads to overfitting in specific layers that makes models vulnerable to adversarial attacks. They demonstrated this vulnerability using an emoji-based attack that achieves 100% success rate on Trojan Detection Competition tasks. Additionally, they discovered a mysterious neuron (neuron 2100) in both Llama2 and Vicuna that has an unreasonably high causal effect on model output, which can be exploited to completely cripple the LLM through targeted attacks.

## Method Summary
The paper introduces the Casper framework for causality analysis of LLMs using Causal Mediation Analysis (CMA) to measure Average Indirect Effects (AIE) at different levels. The method involves normal execution and intervened execution where specific layers or neurons are short-circuited or set to zero. Kurtosis statistics are used to identify layers with unusually high or low causal effects. The framework is applied to analyze token-level, layer-level, and neuron-level causal effects across benign, harmful, and adversarial prompts. The study focuses on open-source models including Llama2 and Vicuna, examining how RLHF fine-tuning affects the distribution of causal effects across layers.

## Key Results
- RLHF fine-tuning causes overfitting in specific layers, making models vulnerable to adversarial prompts that avoid these overfitted patterns
- Emoji-based adversarial attacks achieve 100% success rate on red-teaming tasks by bypassing overfitted safety layers
- Neuron 2100 in both Llama2 and Vicuna has an unreasonably high causal effect on output, enabling complete model manipulation through targeted attacks
- The mysterious neuron's existence is transferable across different models, suggesting a fundamental architectural feature

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF causes overfitting to harmful prompts in specific layers
- Mechanism: RLHF fine-tunes the model with human feedback on harmful prompts, causing certain layers to become overly specialized at detecting and blocking those specific prompts
- Core assumption: The safety mechanism is primarily based on pattern matching rather than semantic understanding
- Evidence anchors:
  - [abstract]: "we show that RLHF has the effect of overfitting a model to harmful prompts"
  - [section 4.1]: "Kurtosis score is significantly higher for harmful prompts, and layer 3 consistently emerges as the most influential layer"
  - [corpus]: Weak - related papers discuss Trojan detection but not the overfitting mechanism specifically
- Break condition: If safety mechanisms are based on semantic understanding rather than pattern matching, this mechanism would fail

### Mechanism 2
- Claim: Neuron 2100 acts as a "Trojan" neuron that controls model output
- Mechanism: This single neuron has disproportionate causal effect on model output, and manipulating its value can completely change the model's behavior
- Core assumption: A single neuron can have such a large impact on model behavior
- Evidence anchors:
  - [abstract]: "existence of one mysterious neuron in both Llama2 and Vicuna that has an unreasonably high causal effect on the output"
  - [section 6.1]: "neuron 2100 (of layer-1, layer-2) stands out with an AIE close to 1"
  - [corpus]: Weak - corpus mentions Trojan neurons but not this specific phenomenon
- Break condition: If neuron values are highly redundant or distributed across the network, this mechanism would fail

### Mechanism 3
- Claim: Emoji attack works by bypassing overfitting layers
- Mechanism: Translating harmful prompts to emojis creates prompts that don't match the patterns the overfitted layers are looking for
- Core assumption: Overfitted layers are looking for specific text patterns rather than semantic meaning
- Evidence anchors:
  - [section 5]: "adversarial prompts are effective perhaps because they are able to successfully avoid those overfitted harmful prompts"
  - [section 5]: "causality analysis of the emoji prompts shows an AIE distribution similar to that of the benign inputs"
  - [corpus]: Weak - corpus doesn't discuss emoji-based attacks specifically
- Break condition: If safety mechanisms are based on semantic understanding, this mechanism would fail

## Foundational Learning

- Concept: Causal mediation analysis
  - Why needed here: The framework uses CMA to measure causal effects by comparing normal and intervened executions
  - Quick check question: What is the difference between measuring ACE and using CMA for causal analysis?

- Concept: Kurtosis statistic
  - Why needed here: Used to measure whether certain layers have unusually high or low causal effects
  - Quick check question: What does a high kurtosis score indicate about the distribution of AIE values?

- Concept: Structural Causal Models (SCM)
  - Why needed here: LLMs are modeled as SCMs to enable causal analysis
  - Quick check question: How does modeling a neural network as an SCM enable causal analysis?

## Architecture Onboarding

- Component map: Casper framework -> LLM -> CMA analysis -> AIE calculation -> Neuron intervention module -> Emoji translation module
- Critical path: Input prompt → LLM → Measure logits → Intervene (short-circuit layer or set neuron to 0) → Measure new logits → Calculate AIE difference → Repeat for all layers/neurons
- Design tradeoffs:
  - Speed vs accuracy: CMA is chosen over ACE for computational efficiency
  - Granularity vs complexity: Neuron-level analysis is more detailed but computationally expensive
  - Transferability vs specificity: Emoji attacks are transferable but may be less effective than tailored attacks
- Failure signatures:
  - High kurtosis across all prompt types: Indicates no special overfitting
  - Uniform AIE values across neurons: Suggests no single influential neuron
  - Emoji attacks fail: Indicates semantic-based safety mechanisms
- First 3 experiments:
  1. Run layer-based causality analysis on a simple LLM with benign/harmful prompts to observe overfitting patterns
  2. Test the emoji attack on a model with known overfitted safety mechanisms
  3. Conduct neuron-based analysis to identify any high-AIE neurons and test their impact by intervention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the existence of neuron 2100 in Llama2 and Vicuna models?
- Basis in paper: [explicit] The paper mentions that the existence of neuron 2100 is uncertain and suggests it might be related to a natural Trojan neuron
- Why unresolved: The paper does not provide a clear explanation for why neuron 2100 exists and its specific role in the model
- What evidence would resolve it: Further analysis and experiments to understand the training process and the specific function of neuron 2100 could provide insights into its existence

### Open Question 2
- Question: How does the safety mechanism of LLMs rely on overfitting, and can this be mitigated?
- Basis in paper: [explicit] The paper discusses how RLHF leads to overfitting of certain layers, making the model vulnerable to adversarial prompts
- Why unresolved: The paper suggests that the current safety mechanisms are brittle and may not be based on a deep understanding of ethical considerations
- What evidence would resolve it: Research into developing more robust safety mechanisms that do not rely on overfitting could address this issue

### Open Question 3
- Question: What are the implications of the transferability of the Trojan neuron attack?
- Basis in paper: [explicit] The paper mentions that the attack suffixes generated by targeting neuron 2100 exhibit strong transferability across different models
- Why unresolved: The paper does not fully explore the implications of this transferability for the security of LLMs
- What evidence would resolve it: Further studies on the transferability of such attacks across various models and contexts could provide insights into their broader impact

## Limitations
- The mysterious neuron phenomenon appears in only Llama2 and Vicuna, with unknown generalizability to other models
- The emoji attack's practical effectiveness may be limited by content moderation systems or models specifically trained on emoji-based prompts
- The assumption that safety mechanisms rely on pattern matching rather than semantic understanding may not hold for more sophisticated safety training approaches

## Confidence
- **High Confidence**: The methodology for conducting causality analysis is well-established and the framework implementation appears sound
- **Medium Confidence**: The discovery of the mysterious neuron and the effectiveness of the emoji attack are supported by experiments on the tested models
- **Low Confidence**: The claim that RLHF specifically causes overfitting to harmful prompts in layer 3 requires more extensive validation across different RLHF implementations

## Next Checks
1. Test the emoji attack methodology on models with different safety training approaches (e.g., constitutional AI, debate-based fine-tuning) to verify whether the attack succeeds when semantic understanding is prioritized over pattern matching
2. Conduct a systematic search across multiple model families and sizes to determine how common the mysterious neuron phenomenon is, and whether it represents a fundamental architectural feature or an artifact of specific training procedures
3. Design experiments to distinguish between pattern-matching and semantic-based safety mechanisms by testing whether models can recognize harmful intent in semantically equivalent prompts that differ in surface form (including emoji, synonyms, paraphrasing)