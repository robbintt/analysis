---
ver: rpa2
title: 'One is More: Diverse Perspectives within a Single Network for Efficient DRL'
arxiv_id: '2310.14009'
source_url: https://arxiv.org/abs/2310.14009
tags:
- omnet
- network
- learning
- value
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OMNet, which leverages multiple subnetworks
  within a single neural network to enable diverse outputs without increasing parameters
  or computational overhead. OMNet is integrated with SAC and evaluated on continuous
  control tasks, demonstrating improved sampling efficiency and computational efficiency.
---

# One is More: Diverse Perspectives within a Single Network for Efficient DRL

## Quick Facts
- **arXiv ID**: 2310.14009
- **Source URL**: https://arxiv.org/abs/2310.14009
- **Reference count**: 18
- **Key outcome**: OMNet leverages multiple subnetworks within a single neural network to improve sampling efficiency and computational efficiency in continuous control tasks when integrated with SAC.

## Executive Summary
This paper introduces OMNet, a method that enables diverse value estimates without increasing parameters by sharing a single weight matrix across multiple binary masks. By sampling different actor subnetworks per episode, OMNet generates more diverse trajectories that improve early exploration. The approach maintains small value estimation bias by averaging across subnetworks' estimates and demonstrates better generalization under noisy environments. OMNet achieves an effective balance between performance and computational cost compared to traditional ensemble methods.

## Method Summary
OMNet integrates with SAC by maintaining a set of fixed binary masks that create multiple subnetworks from a single weight tensor. During training, only one subnetwork is updated per step while the actor loss averages over all subnetworks. Value targets are computed using the minimum of two randomly chosen subnetwork estimates, mimicking Clipped Double Q-learning within a single network. Each episode uses a different actor subnetwork for decision-making, enhancing exploration through diverse trajectory sampling while sharing learned features across subnetworks.

## Key Results
- OMNet achieves improved sampling efficiency compared to vanilla SAC across MuJoCo continuous control tasks
- Value estimation bias remains small due to minimum-subnetwork target selection within a single network
- Enhanced exploration leads to faster state space coverage in 2D maze environments
- Better generalization performance under noisy environments while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: OMNet enables diverse value estimates without increasing parameters by sharing a single weight matrix across multiple binary masks.
- **Mechanism**: Each mask selects a different subnetwork from the same weight tensor; training updates only one mask's corresponding subnetwork per step, so gradients propagate through overlapping parameters, preserving shared knowledge while allowing independent output diversity.
- **Core assumption**: The binary masks remain fixed during training, and the overlap in parameters between subnetworks provides sufficient regularization to prevent overfitting.
- **Evidence anchors**:
  - [abstract] "leverages multiple subnetworks within a single neural network to enable diverse outputs without increasing parameters or computational overhead."
  - [section 3.1] "maintains a set of fixed masks during training, i.e., M = {m1, m2, · · · , mN }, where N is the number of masks."
- **Break condition**: If masks are too sparse, subnetworks become disjoint and lose shared learning benefits; if masks overlap too heavily, diversity diminishes.

### Mechanism 2
- **Claim**: Sampling from different actor subnetworks within OMNet yields more diverse trajectories, improving early exploration.
- **Mechanism**: Each episode uses a different actor subnetwork (chosen uniformly at random), so the policy behavior varies across episodes while still sharing the same learned features, exposing the replay buffer to a wider range of state-action pairs.
- **Core assumption**: Diversity in early-stage trajectories is more valuable than consistency, and the shared representation still captures relevant features for all subnetworks.
- **Evidence anchors**:
  - [section 3.3] "for each episode, we randomly select one subnetwork from the actor to make decisions, and we maintain the use of the same subnetwork throughout each episode."
  - [section 4.3] Visual evidence in 2D maze showing faster coverage of the state space when using OMNet actor.
- **Break condition**: If the environment is deterministic and fully observable, the benefit of policy diversity may be negligible.

### Mechanism 3
- **Claim**: Using OMNet as a value network reduces overestimation bias by averaging across subnetworks' estimates.
- **Mechanism**: TD targets are computed using the minimum of two randomly chosen subnetwork estimates, which acts like Clipped Double Q-learning but within a single network, lowering variance and bias.
- **Core assumption**: The subnetwork estimates are sufficiently independent given their different masks, so taking the minimum reduces optimistic bias.
- **Evidence anchors**:
  - [section 3.2] "calculate the target Q value by taking the minimum value estimate from two different subnetworks."
  - [section 4.2] Figure 4 showing OMNet maintains small value bias compared to baselines.
- **Break condition**: If masks are too similar, subnetwork estimates will be correlated and the bias reduction effect will be minimal.

## Foundational Learning

- **Concept**: Bernoulli mask sampling (sparsity control)
  - **Why needed here**: Determines the overlap and diversity between subnetworks; sparsity S controls how many parameters are active in each subnetwork.
  - **Quick check question**: If S = 0.5 and we have N = 5 subnetworks, what is the expected number of parameters active in a randomly chosen subnetwork relative to the full network?
    - **Answer**: 50% of parameters are active.

- **Concept**: Ensemble averaging vs. random subnetwork selection during training
  - **Why needed here**: OMNet updates only one subnetwork per iteration (randomly chosen), unlike traditional ensembles that update all members; this reduces compute but requires careful loss formulation.
  - **Quick check question**: In OMNet's critic loss, why do we average over all subnetworks when computing the actor loss but only update one subnetwork per step?
    - **Answer**: To keep the actor's gradient signal stable while still allowing each subnetwork to learn independently.

- **Concept**: Clipped Double Q-learning adaptation to subnetworks
  - **Why needed here**: OMNet mimics the overestimation reduction of TD3/SAC by selecting the minimum of two subnetwork Q-values, but within a single network.
  - **Quick check question**: How does OMNet's target Q computation differ from REDQ's?
    - **Answer**: REDQ uses 10 separate networks and a random subset for the target; OMNet uses two random subnetworks from one shared weight matrix.

## Architecture Onboarding

- **Component map**: Base weight tensor θ (shared) -> Fixed binary mask set M = {m₁,…,m_N} -> Subnetworks θᵢ = θ ⊙ mᵢ (virtual, no extra storage) -> SAC-style actor and critic heads on top of subnetworks
- **Critical path**:
  1. Sample mask index i for training
  2. Forward pass through θᵢ
  3. Compute loss using random subnetwork(s) for target
  4. Backpropagate only through θᵢ
  5. Repeat for actor update (average over all subnetworks)
- **Design tradeoffs**:
  - More subnetworks → higher diversity but risk of redundancy if masks too similar
  - Higher sparsity → more diverse subnetworks but less shared learning
  - Fixed masks → no mask-update overhead, but less adaptivity
- **Failure signatures**:
  - Performance collapses if all subnetworks converge to identical outputs (masks too overlapping)
  - Instability if sparsity too high (few active parameters per subnetwork)
  - No improvement over baseline if masks too similar or N too small
- **First 3 experiments**:
  1. Run OMNet with N=2, S=0.5 on Hopper-v4 and compare to vanilla SAC
  2. Visualize visitation maps in 2D maze with and without OMNet actor
  3. Measure value bias curves over training steps for OMNet vs. single-network SAC

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal sparsity level and number of subnetworks for OMNet to achieve the best performance across various tasks?
- **Basis in paper**: [explicit] The authors conduct an ablation study to analyze the sensitivity of OMNet to sparsity and the number of subnetworks. They observe that OMNet consistently achieves favorable performance within a wide range of sparsity levels and exhibits strong stability across various subnetwork counts. However, they also note that increasing the number of subnetworks to 8 leads to a slight drop in performance compared to using 5 subnetworks.
- **Why unresolved**: The authors do not provide a definitive answer to what the optimal sparsity level and number of subnetworks are. They only demonstrate that OMNet is robust to hyperparameter choices and performs well within a certain range.
- **What evidence would resolve it**: Systematic experiments varying sparsity levels and the number of subnetworks across a wide range of tasks would provide insights into the optimal configuration for different scenarios.

### Open Question 2
- **Question**: How does the performance of OMNet compare to ensemble methods that maintain multiple distinct networks?
- **Basis in paper**: [explicit] The authors compare OMNet to REDQ, which uses 10 value networks, and DroQ, which uses 2 value networks. They demonstrate that OMNet achieves comparable performance to REDQ and DroQ while using fewer parameters and lower computational overhead.
- **Why unresolved**: The comparison is limited to specific ensemble methods (REDQ and DroQ). It remains unclear how OMNet would perform against other ensemble approaches or how the performance gap varies across different tasks and environments.
- **What evidence would resolve it**: Extensive experiments comparing OMNet to a broader range of ensemble methods across diverse tasks and environments would provide a more comprehensive understanding of its relative performance.

### Open Question 3
- **Question**: What are the theoretical guarantees of OMNet in terms of convergence and generalization?
- **Basis in paper**: [inferred] The authors provide empirical evidence of OMNet's effectiveness and efficiency but do not delve into theoretical analysis. They mention that OMNet maintains small value estimation bias and enhances exploration, but they do not provide formal proofs or bounds.
- **Why unresolved**: Theoretical analysis is crucial for understanding the limitations and potential of OMNet. Without formal guarantees, it is difficult to assess its applicability to more complex and diverse scenarios.
- **What evidence would resolve it**: Theoretical analysis establishing convergence properties, generalization bounds, and sample complexity of OMNet would provide a deeper understanding of its capabilities and limitations.

## Limitations
- Fixed mask assumption may limit long-term performance ceiling compared to adaptive methods
- Computational savings are modest since forward passes still compute the full weight matrix
- Limited theoretical analysis of convergence properties and generalization bounds

## Confidence
- **High confidence**: OMNet reduces value estimation bias through minimum-subnetwork target selection
- **Medium confidence**: Diversity benefits in early exploration translate to sustained performance gains
- **Medium confidence**: Computational efficiency claims hold for the tested network scales

## Next Checks
1. Test OMNet with dynamic mask adaptation during training to assess whether fixed masks limit performance ceiling
2. Evaluate the scaling behavior on larger network architectures (e.g., 1M+ parameters) to verify computational efficiency claims
3. Compare OMNet against modern ensemble methods like Muesli and Actor-Advisor to establish relative sample efficiency improvements