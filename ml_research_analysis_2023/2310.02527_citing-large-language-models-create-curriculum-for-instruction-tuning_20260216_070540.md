---
ver: rpa2
title: 'CITING: Large Language Models Create Curriculum for Instruction Tuning'
arxiv_id: '2310.02527'
source_url: https://arxiv.org/abs/2310.02527
tags:
- instruction
- response
- citing
- criteria
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a method called CITING to improve instruction
  tuning of large language models (LLMs) by having a teacher LLM create a curriculum
  of criteria for evaluating responses and teach the student LLM to revise its answers
  accordingly. The key steps are: 1) The teacher LLM crafts rubrics for evaluating
  answers to different types of questions, 2) The student LLM generates an initial
  response, 3) The teacher LLM revises the response based on the rubrics, 4) The student
  LLM learns from the revision through fine-tuning.'
---

# CITING: Large Language Models Create Curriculum for Instruction Tuning

## Quick Facts
- arXiv ID: 2310.02527
- Source URL: https://arxiv.org/abs/2310.02527
- Authors: Ruochen Wang, Liang Pang, Yuchen Hao, Yunfeng Zhang, Nan Duan, Ming Zhou, Tianyu Liu, Yanru Qu
- Reference count: 16
- Primary result: CITING achieves 79.4% win rate over SFT, 73.4% over RLHF, 78.1% over RRHF, and 76.3% over RAFT on instruction tuning tasks

## Executive Summary
CITING is a method for improving instruction tuning of large language models by having a teacher LLM create evaluation rubrics and teach the student LLM to self-correct based on those rubrics. The approach involves categorizing instructions, generating evaluation criteria, and iteratively refining student responses through AI feedback. Experiments on four datasets show CITING significantly outperforms state-of-the-art baselines like SFT, RLHF, RRHF, and RAFT, with strong performance in few-shot and zero-shot settings.

## Method Summary
CITING employs a teacher LLM to create a curriculum for instruction tuning through two main steps: (1) crafting rubrics for evaluating answers corresponding to various types of questions, and (2) having the student LLM learn to follow these rubrics and perform self-correction based on revisions made by the teacher. The method uses LLaMA-7B as the backbone model, fine-tuned with supervised learning on the Alpaca dataset, then iteratively refined using 1000 sampled instructions where GPT-3.5 revises initial responses based on quality criteria.

## Key Results
- CITING achieves 79.4% win rate over SFT, 73.4% over RLHF, 78.1% over RRHF, and 76.3% over RAFT on articulation, depth, and comprehensiveness metrics
- Strong performance demonstrated in few-shot and zero-shot settings across multiple datasets
- Outperforms state-of-the-art baselines on commonsense reasoning, world knowledge, and reading reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The teacher LLM's ability to create evaluation rubrics for different instruction types improves student response quality.
- Mechanism: By categorizing instructions and generating specific evaluation criteria, the teacher provides structured feedback that guides the student's self-correction process.
- Core assumption: The teacher LLM can effectively categorize instructions and generate meaningful evaluation criteria that generalize to unseen instructions.
- Evidence anchors:
  - [abstract]: "we employ a teacher LLM to create a curriculum for instruction tuning, which we refer to as Curriculum Instruction TunING (CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics for evaluating the answers corresponding to various types of questions, and (2) the student LLM learns to follow the rubrics and perform self-correction from the revision made by the teacher."
  - [section]: "Rubric Design by Teacher LLM. It first employs a teacher LLM to categorize all instructions into multiple types and design the rubric for each type."
  - [corpus]: Weak evidence - corpus provides related papers but no direct evidence of rubric effectiveness.

### Mechanism 2
- Claim: Iterative self-correction based on teacher feedback improves student LLM performance over multiple rounds.
- Mechanism: The student generates an initial response, the teacher revises it based on rubrics, and the student learns from this revision through fine-tuning. This process is iterated.
- Core assumption: Each iteration of feedback and fine-tuning leads to meaningful improvement in the student's ability to self-correct.
- Evidence anchors:
  - [abstract]: "The student LLM learns to follow the rubrics and perform self-correction from the revision made by the teacher. We further iteratively carry out it to embody the procedure of CITING."
  - [section]: "Curriculum Instruction Tuning for Student LLM. The student LLM learns to revise its initial response from teacher LLM based on the criteria from the first part via instruction turning."
  - [corpus]: Weak evidence - corpus contains related curriculum learning papers but no direct evidence of iterative improvement in this specific context.

### Mechanism 3
- Claim: Using AI feedback (LAIF) can match or exceed human feedback in aligning LLMs with human preferences.
- Mechanism: The teacher LLM acts as an AI evaluator and reviser, providing feedback that guides the student's learning process without human intervention.
- Core assumption: AI-generated feedback can effectively capture human preferences and guide model alignment.
- Evidence anchors:
  - [abstract]: "we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs."
  - [section]: "Alpaca World Reading Reasoning Dataset 0.0 0.2 0.4 0.6 0.8 Win Rate CITING RLHF Figure 2: CITING vs. RLHF on four tasks. In this paper, we argue that it is crucial to exploit the generation capability of the teacher LLM to make LAIF excel."
  - [corpus]: Moderate evidence - corpus includes "GLIDE-RL: Grounded Language Instruction through DEmonstration in RL" which uses RL for language instruction, suggesting AI feedback approaches are viable.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT provides the initial alignment of the student LLM to instruction-following before applying the more complex CITING method.
  - Quick check question: What is the purpose of SFT in the CITING pipeline, and how does it differ from CITING?

- Concept: Curriculum Learning
  - Why needed here: CITING uses a curriculum-based approach where the teacher LLM provides increasingly refined feedback over multiple iterations.
  - Quick check question: How does the iterative feedback process in CITING relate to curriculum learning principles?

- Concept: Instruction Tuning
  - Why needed here: The entire CITING method is designed to improve instruction tuning of LLMs, making them better at following and responding to instructions.
  - Quick check question: What is the difference between instruction tuning and general language model training?

## Architecture Onboarding

- Component map: Teacher LLM -> Rubric Creation -> Instruction Classification -> Student LLM -> Initial Response Generation -> Teacher LLM Revision -> Student LLM Fine-tuning

- Critical path:
  1. Teacher LLM creates rubrics for instruction types
  2. New instructions are classified using similarity matching
  3. Student LLM generates initial response
  4. Teacher LLM revises response based on rubrics
  5. Student LLM fine-tuned on revised responses
  6. Process iterates for multiple rounds

- Design tradeoffs:
  - Using AI feedback vs. human feedback: Faster and cheaper but potentially less nuanced
  - Number of iterations: More iterations may improve performance but risk catastrophic forgetting
  - Rubric granularity: More specific rubrics may provide better guidance but require more teacher LLM effort

- Failure signatures:
  - Student model shows no improvement or degradation after iterations
  - Rubrics fail to generalize to new instruction types
  - Teacher LLM revisions don't meaningfully improve initial responses

- First 3 experiments:
  1. Test rubric creation and classification on a small set of instructions to ensure the teacher LLM can effectively categorize and generate criteria.
  2. Run a single iteration of the feedback loop to verify the student learns from teacher revisions.
  3. Test the iterative process for 2-3 rounds to observe improvement trends and check for catastrophic forgetting.

## Open Questions the Paper Calls Out

Open Question 1
- Question: What is the impact of the number of iterations in the curriculum instruction tuning process on the student LLM's performance?
- Basis in paper: Inferred from the results shown in Table 2, where the performance of CITING decreases as the number of instruction tuning rounds increases.
- Why unresolved: The paper does not provide a detailed analysis of why the performance decreases with more iterations or what the optimal number of iterations might be.
- What evidence would resolve it: A more comprehensive study that explores the performance of CITING at different iteration levels, including an analysis of why performance might degrade and what the optimal number of iterations is.

Open Question 2
- Question: How does CITING perform on tasks that require a deep understanding of complex topics or extensive knowledge bases?
- Basis in paper: Inferred from the paper's claim that CITING performs best on the Commonsense Reasoning dataset and the discussion on the importance of step-by-step reasoning and revision.
- Why unresolved: The paper does not provide specific examples or results on tasks that require deep understanding of complex topics or extensive knowledge bases.
- What evidence would resolve it: Experiments on datasets that require deep understanding of complex topics or extensive knowledge bases, along with a comparison of CITING's performance to other methods.

Open Question 3
- Question: How does CITING handle the potential issue of catastrophic forgetting, where the model might forget previously learned knowledge as it is fine-tuned?
- Basis in paper: Inferred from the observation in the ablation study that repeated instruction tuning can lead to catastrophic forgetting, causing the model to forget knowledge from the earliest supervised fine-tuning (SFT).
- Why unresolved: The paper does not provide a solution or strategy to mitigate the issue of catastrophic forgetting.
- What evidence would resolve it: Experiments that test CITING's performance on tasks that require knowledge from the earliest SFT after multiple rounds of curriculum instruction tuning, or a proposed strategy to mitigate catastrophic forgetting.

## Limitations

- The evaluation relies heavily on GPT-4 scoring for "articulate, in-depth, and comprehensive" responses, which may introduce bias since GPT-4 is used both as evaluator and teacher model in some experiments.
- The study uses a relatively small curriculum of 1000 instructions for fine-tuning, which may not capture the full diversity of real-world instruction types.
- The paper doesn't provide extensive ablation studies on the impact of iteration count or rubric granularity on final performance.

## Confidence

The claim that CITING significantly outperforms state-of-the-art baselines on instruction tuning has **Medium confidence** due to reliance on GPT-4 scoring and limited curriculum size. The mechanism of using AI feedback to improve instruction tuning is supported by **High confidence** based on consistent experimental results. However, the claim that this approach can fully replace human feedback has **Low confidence** given the limited evaluation scope and lack of comparison to human-annotated datasets.

## Next Checks

1. **Ablation study on iteration count**: Run experiments with varying numbers of CITING iterations (1, 2, 3, 5) to identify the optimal iteration count and check for diminishing returns or catastrophic forgetting.

2. **Human evaluation validation**: Conduct a human evaluation study comparing CITING-generated responses against those from SFT, RLHF, and RRHF baselines to verify that GPT-4 scoring aligns with human preferences.

3. **Curriculum diversity stress test**: Test CITING's performance when the curriculum contains instruction types that are underrepresented in the training data to assess the rubric generalization capability.