---
ver: rpa2
title: 'EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models with
  Semi-structured Data'
arxiv_id: '2312.15696'
source_url: https://arxiv.org/abs/2312.15696
tags:
- data
- pre-training
- tasks
- continual
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates domain-specific continual pre-training
  of large language models (LLMs) for e-commerce applications. The authors propose
  a data mixing strategy that leverages semi-structured e-commerce data by connecting
  related product information from different sources and concatenating them into training
  samples.
---

# EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models with Semi-structured Data

## Quick Facts
- arXiv ID: 2312.15696
- Source URL: https://arxiv.org/abs/2312.15696
- Authors: 
- Reference count: 18
- Key outcome: Continual pre-training with mixed semi-structured e-commerce data improves domain-specific performance while maintaining general NLP capabilities

## Executive Summary
This paper investigates domain-specific continual pre-training of large language models for e-commerce applications. The authors propose a data mixing strategy that leverages semi-structured e-commerce data by connecting related product information from different sources and concatenating them into training samples. They construct EcomGPT-CT models based on BLOOM and evaluate them on two benchmarks: EcomICL (few-shot in-context learning) and EcomSFT (zero-shot after instruction tuning). Experimental results show that continual pre-training with a mix of general and e-commerce data improves performance on e-commerce tasks, with the proposed data mixing strategy outperforming separate sampling from different sources.

## Method Summary
The approach involves continual pre-training of BLOOM models using a balanced mix of general text data and semi-structured e-commerce data. The data mixing strategy creates connected clusters of product-related information from multiple sources (titles, properties, reviews, descriptions) and concatenates them into single training samples. The continual pre-training uses an auto-regressive language modeling objective with a 2:1 ratio of general to e-commerce tokens. The models are evaluated on EcomICL (few-shot in-context learning) and EcomSFT (zero-shot after instruction tuning) benchmarks, as well as general NLP tasks to assess knowledge retention.

## Key Results
- Continual pre-training with mixed data improves e-commerce task performance by up to 4.85% accuracy on EcomICL and 4.36% on EcomSFT
- The data mixing strategy outperforms independent sampling from different sources, particularly for tasks requiring domain knowledge
- Models maintain general NLP capabilities while enhancing domain-specific performance, with balanced data ratio (2:1 general to e-commerce) proving effective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixing semi-structured e-commerce data from different sources into single training samples improves domain-specific performance more than independent sampling.
- Mechanism: The data mixing strategy creates connected clusters of product-related information from multiple sources and concatenates them, allowing the model to learn intrinsic relationships between different types of e-commerce data within a single context window.
- Core assumption: The model can effectively learn from the concatenated heterogeneous data format and benefit from seeing related information together rather than separately.
- Evidence anchors:
  - [section] "as opposed to sampling from different data sources independently, employing our designed data mixing strategy, which amalgamates data from various sources into a single training sample, enables the model to learn the intrinsic connections between different types of data more effectively"
  - [abstract] "we design a mixing strategy among different data sources to better leverage E-commercial semi-structured data"

### Mechanism 2
- Claim: Continual pre-training with a balanced mix of general and domain-specific data preserves general NLP capabilities while enhancing domain performance.
- Mechanism: By maintaining a 2:1 ratio of general to e-commerce tokens, the model receives sufficient domain-specific knowledge injection without losing its general world knowledge and language understanding capabilities acquired during initial pre-training.
- Core assumption: The model has sufficient capacity to retain both general and domain-specific knowledge simultaneously, and the continual pre-training objective allows for knowledge transfer without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "integrating general pre-training data with domain-specific data from e-commerce strengthens the model's domain adaptability without significantly compromising its performance on general NLP benchmarks"
  - [section] "we also incorporate general text data into the training datasets" and "it is imperative to maintain a balance in the token count between general and domain-specific data"

### Mechanism 3
- Claim: The effectiveness of continual pre-training varies significantly by task type, with the largest improvements for tasks requiring domain knowledge or having unique data formats.
- Mechanism: Tasks that heavily depend on domain-specific knowledge or have data formats substantially different from general text benefit most because the model learns to handle these specific patterns during continual pre-training.
- Core assumption: Different NLP tasks have varying dependencies on domain knowledge and data format familiarity, and the model can leverage these during continual pre-training.
- Evidence anchors:
  - [section] "tasks that are highly dependent on domain knowledge or exhibit unique data formats demonstrate greater performance discrepancies before and after continual pre-training"
  - [section] "For tasks that heavily rely on domain knowledge (e.g., Product Classification (PDC) and Title Generation (TIG)) or exhibit substantial differences in data format compared to general text (e.g., Description Summary (DES) and Short Title Generation (STG)), continual pre-training significantly enhance model performance"

## Foundational Learning

- Concept: Masked Language Modeling (MLM) and auto-regressive language modeling
  - Why needed here: Understanding the pre-training objectives used in both initial LLM training and continual pre-training phases
  - Quick check question: What is the key difference between MLM and auto-regressive language modeling objectives?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: To understand why maintaining general capabilities while adapting to domain-specific tasks is challenging and requires careful data balancing
  - Quick check question: What strategies can be used to mitigate catastrophic forgetting during continual pre-training?

- Concept: Semi-structured data representation and processing
  - Why needed here: The paper's core innovation involves transforming semi-structured e-commerce data (tables, product attributes) into usable training samples
  - Quick check question: How does the paper transform semi-structured data into text sequences suitable for LLM training?

## Architecture Onboarding

- Component map: Data preprocessing pipeline (cleaning, deduplication, transformation of semi-structured data) -> Data mixing strategy module (graph-based clustering and concatenation) -> Continual pre-training module (using BLOOM architecture with modified data) -> Evaluation framework (EcomICL and EcomSFT benchmarks)
- Critical path: Data preprocessing → Data mixing → Continual pre-training → Evaluation
- Design tradeoffs: Using semi-structured data provides more domain-specific knowledge but requires complex preprocessing; mixing general and domain data maintains capabilities but requires careful balancing; graph-based clustering is more sophisticated but computationally expensive
- Failure signatures: Poor performance on general tasks indicates catastrophic forgetting; poor performance on domain tasks indicates insufficient domain adaptation; unstable training suggests data mixing issues
- First 3 experiments:
  1. Baseline: Run continual pre-training with only general data to establish performance baseline
  2. Domain-only: Run continual pre-training with only e-commerce data to measure maximum domain adaptation potential
  3. Mixed strategy comparison: Compare the proposed data mixing strategy against simple concatenation or independent sampling approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the data mixing strategy's performance compare to other domain adaptation methods like domain-adaptive pretraining or task-adaptive pretraining?
- Basis in paper: [explicit] The authors compare their data mixing strategy to separate sampling from different sources, but don't compare to other domain adaptation methods
- Why unresolved: The paper focuses on comparing their data mixing strategy to a baseline of separate sampling, but doesn't explore how it stacks up against other established domain adaptation techniques
- What evidence would resolve it: Empirical comparison of the data mixing strategy's performance against domain-adaptive pretraining and task-adaptive pretraining on the same benchmarks

### Open Question 2
- Question: What is the optimal ratio of general to domain-specific data for continual pretraining?
- Basis in paper: [explicit] The authors mention they set a 2:1 ratio of general to domain-specific data due to hardware limitations and initial explorations, but don't conduct detailed experiments on different ratios
- Why unresolved: The paper acknowledges the importance of balancing general and domain-specific data but doesn't explore how different ratios impact model performance
- What evidence would resolve it: Systematic experiments varying the ratio of general to domain-specific data and measuring the impact on model performance across different tasks

### Open Question 3
- Question: How does continual pretraining affect the model's ability to retain world knowledge and perform complex reasoning tasks?
- Basis in paper: [inferred] The authors mention preserving the model's generalization capabilities but don't explicitly test its reasoning or knowledge retention abilities after continual pretraining
- Why unresolved: The paper focuses on domain-specific task performance but doesn't investigate potential trade-offs in general knowledge and reasoning capabilities
- What evidence would resolve it: Comprehensive evaluation of the model's performance on general knowledge benchmarks and complex reasoning tasks before and after continual pretraining

## Limitations

- Evaluation relies on proprietary datasets from Amazon and Taobao, making independent validation difficult
- The study focuses on two specific model sizes (3B and 7B parameters) without exploring whether results generalize to smaller or larger architectures
- The continual pre-training process shows significant computational overhead (200k steps) without clear guidance on optimal stopping criteria

## Confidence

**High Confidence**: The claim that continual pre-training with mixed general and domain-specific data improves e-commerce task performance is well-supported by experimental results showing consistent improvements across multiple benchmarks.

**Medium Confidence**: The claim about the superiority of the proposed data mixing strategy over independent sampling is moderately supported but lacks detailed ablation studies.

**Low Confidence**: The claim about maintaining general NLP capabilities while enhancing domain performance has limited evidence, with sparse evaluation on general tasks.

## Next Checks

1. **Ablation Study**: Conduct a systematic ablation study of the data mixing strategy components (graph-based clustering, filtering thresholds, concatenation methods) to quantify the contribution of each element to performance gains.

2. **General Capability Monitoring**: Implement continuous evaluation on a comprehensive suite of general NLP benchmarks throughout the continual pre-training process to detect any gradual degradation in general capabilities that might not be apparent in final evaluation.

3. **Scaling Analysis**: Test the proposed approach across a wider range of model sizes (from 1B to 13B parameters) to determine whether the data mixing strategy and performance improvements scale proportionally with model capacity.