---
ver: rpa2
title: Metastatic Breast Cancer Prognostication Through Multimodal Integration of
  Dimensionality Reduction Algorithms and Classification Algorithms
arxiv_id: '2309.10324'
source_url: https://arxiv.org/abs/2309.10324
tags:
- algorithm
- data
- components
- cancer
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of accurately detecting metastatic
  cancer, which is responsible for 90% of cancer-related deaths. The research introduces
  a novel approach using Machine Learning (ML) preprocessing and classification algorithms.
---

# Metastatic Breast Cancer Prognostication Through Multodal Integration of Dimensionality Reduction Algorithms and Classification Algorithms

## Quick Facts
- arXiv ID: 2309.10324
- Source URL: https://arxiv.org/abs/2309.10324
- Reference count: 0
- Primary result: 71.14% accuracy using PCA + Genetic Algorithm + k-nearest neighbors pipeline

## Executive Summary
This study addresses the critical challenge of accurately detecting metastatic cancer, which accounts for 90% of cancer-related deaths. The research introduces a novel machine learning approach that combines dimensionality reduction with classification algorithms to improve detection accuracy. By preprocessing pathology image data using Principal Component Analysis (PCA) and a Genetic Algorithm, followed by classification with k-nearest neighbors, logistic regression, and decision tree classifiers, the method achieves 71.14% accuracy - significantly outperforming manual pathologist methods which average 40% accuracy.

## Method Summary
The method processes 32x32 pixel pathology images from the PatchCamelyon dataset through a multi-stage pipeline. First, PCA reduces the high-dimensional image data to 250 principal components to manage computational complexity and reduce overfitting. A genetic algorithm then optimizes the selection of these components by searching for subsets that maximize classifier performance. Finally, the optimized feature set is classified using k-nearest neighbors, logistic regression, or decision tree algorithms. The optimal configuration uses 29 generations of the genetic algorithm with 29 population size and 25% mutation rate, achieving the highest accuracy with k-nearest neighbors classification.

## Key Results
- Achieved 71.14% classification accuracy using PCA + Genetic Algorithm + k-nearest neighbors pipeline
- k-nearest neighbors outperformed logistic regression (71%) and decision tree (64%) classifiers
- PCA with 250 components provided optimal balance between dimensionality reduction and information retention
- Genetic algorithm improved accuracy for decision tree and k-NN classifiers but showed mixed results for logistic regression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining PCA and a genetic algorithm for preprocessing yields higher classification accuracy than using either alone.
- Mechanism: PCA reduces dimensionality by projecting high-dimensional image data onto orthogonal components that preserve maximum variance. The genetic algorithm then optimizes the subset of these components by selecting combinations that maximize classifier performance, reducing noise and overfitting.
- Core assumption: The variance-preserving nature of PCA components aligns with discriminative features for cancer detection, and the genetic algorithm can efficiently search this reduced space.
- Evidence anchors:
  - [abstract] "The highest accuracy of 71.14% was achieved using a pipeline combining PCA, the genetic algorithm, and k-nearest neighbors."
  - [section] "The combination of the principal component analysis algorithm and the genetic algorithm improved the overall accuracy when combined with the classification algorithms."
  - [corpus] Weak evidence; corpus papers focus on different methods (deep learning, transfer learning) rather than PCA+GA pipelines.
- Break condition: If PCA components do not align with discriminative features, or if the genetic algorithm's stochastic search fails to converge to a useful subset, accuracy gains may disappear.

### Mechanism 2
- Claim: K-nearest neighbors outperforms logistic regression and decision tree classifiers in this pipeline despite being simpler.
- Mechanism: KNN classifies based on similarity in the reduced feature space without assuming linear relationships. This is advantageous when the relationship between pathology image features and cancer labels is non-linear, as logistic regression assumes, and when the dataset size is manageable for KNN's distance calculations.
- Core assumption: The reduced feature space from PCA+GA preserves discriminative information, and the dataset size allows KNN to compute distances efficiently.
- Evidence anchors:
  - [abstract] "The highest accuracy of 71.14% was produced by the ML pipeline comprising of PCA, the genetic algorithm, and the k-nearest neighbors algorithm."
  - [section] "The k-nearest neighbors algorithm had the highest accuracy and logistic regression had the lowest accuracy... This was unexpected, considering that logistic regression usually had the highest accuracy when testing all the other parameters in the algorithm."
  - [corpus] Weak evidence; corpus does not compare KNN directly with logistic regression in similar settings.
- Break condition: If the dataset grows large or high-dimensional, KNN's computational cost and memory usage become prohibitive, reducing its practicality.

### Mechanism 3
- Claim: Preprocessing with PCA reduces overfitting and improves generalization compared to raw pixel input.
- Mechanism: Raw pathology images have >27,000 dimensions. PCA compresses these into ~250 orthogonal components, reducing noise and the curse of dimensionality, allowing classifiers to learn robust patterns without memorizing noise.
- Core assumption: The first 250 principal components capture sufficient discriminative variance for cancer detection, and further dimensionality reduction does not discard critical information.
- Evidence anchors:
  - [section] "The optimal number of components for principal component analysis in the study was found to be 250 principal components... The reason that higher numbers of principal components had lower accuracies is because of overfitting."
  - [section] "Principal Component Analysis (PCA) is an unsupervised Machine Learning (ML) algorithm... PCA improves the data's readability, reduces the memory needed to store the data, improves the speed, and overall increases the efficiency and usability of the dataset."
  - [corpus] No direct evidence; corpus focuses on other methods.
- Break condition: If the number of principal components is too low, discriminative information is lost; if too high, overfitting returns.

## Foundational Learning

- Concept: Principal Component Analysis (PCA) and its role in dimensionality reduction
  - Why needed here: Raw pathology images are high-dimensional (>27,000 pixels), making direct classification computationally expensive and prone to overfitting. PCA compresses data while preserving variance, enabling efficient learning.
  - Quick check question: What is the primary criterion PCA uses to select principal components?

- Concept: Genetic algorithms for feature selection
  - Why needed here: After PCA, the feature space is still large. The genetic algorithm searches for optimal subsets of PCA components that maximize classifier accuracy, automating feature selection beyond manual tuning.
  - Quick check question: How does a genetic algorithm decide which chromosomes (feature subsets) survive to the next generation?

- Concept: K-nearest neighbors (KNN) classification mechanics
  - Why needed here: KNN classifies by similarity in the reduced feature space without assuming linearity, which is beneficial for non-linear relationships in pathology data. It also requires no training phase, simplifying the pipeline.
  - Quick check question: What metric does KNN use to measure similarity between samples?

## Architecture Onboarding

- Component map:
  - Input: Raw pathology images (32x32 pixels, RGB) → Flatten → PCA (250 comps) → Genetic algorithm → KNN → Classification

- Critical path:
  - Image → Flatten → PCA (250 comps) → Genetic algorithm → KNN → Classification

- Design tradeoffs:
  - PCA vs raw pixels: PCA reduces dimensionality and noise but may lose some fine-grained detail.
  - Genetic algorithm vs manual feature selection: GA automates search but is computationally expensive and stochastic.
  - KNN vs logistic regression: KNN is simpler and handles non-linearity better but scales poorly with large datasets.

- Failure signatures:
  - Low accuracy (<50%): Indicates poor feature selection or misalignment between PCA components and discriminative features.
  - High variance across runs: Suggests genetic algorithm instability or insufficient generations.
  - Long runtime: Implies too many principal components or inefficient genetic algorithm parameters.

- First 3 experiments:
  1. Run PCA with varying numbers of components (e.g., 50, 100, 250, 500) and measure classifier accuracy to find optimal dimensionality.
  2. Test KNN with different k values (1, 5, 10, 20, 50) on PCA-reduced data to tune the classifier.
  3. Compare classification accuracy with and without the genetic algorithm to quantify its contribution to performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why did the k-nearest neighbors algorithm outperform logistic regression and decision tree classifiers despite its simplicity and assumptions about continuous data?
- Basis in paper: [explicit] The paper states that k-nearest neighbors achieved the highest accuracy of 71.14% compared to logistic regression (71%) and decision tree (64%), and notes this result was unexpected given k-NN's limitations with binary classification.
- Why unresolved: The paper does not provide a definitive explanation for this counterintuitive result, only speculating that k-NN's simplicity may have prevented overfitting and that it can handle non-linear relationships better than logistic regression.
- What evidence would resolve it: Comparative analysis of overfitting metrics (training vs testing accuracy) across all three algorithms, or sensitivity analysis showing how each algorithm performs on data with different underlying mathematical relationships (linear vs non-linear).

### Open Question 2
- Question: How does dataset size affect the performance of preprocessing algorithms like PCA and genetic algorithms in detecting metastatic cancer?
- Basis in paper: [explicit] The paper tested PCA with different dataset sizes (from 50,000 to 195 images) and found that larger datasets resulted in higher accuracies, with principal components becoming more generic and less specific.
- Why unresolved: While the paper demonstrates a correlation between dataset size and accuracy, it does not establish causation or determine the optimal dataset size for balancing accuracy with computational efficiency.
- What evidence would resolve it: Systematic testing of dataset sizes across a broader range, including very small and very large datasets, with analysis of computational time and memory usage to identify the point of diminishing returns.

### Open Question 3
- Question: Why did the genetic algorithm improve accuracy for decision tree and k-nearest neighbors classifiers but not for logistic regression, despite logistic regression being used as the fitness function?
- Basis in paper: [explicit] The paper states that the genetic algorithm significantly improved accuracy for decision tree and k-NN classifiers, but had mixed results for logistic regression, with accuracy sometimes being slightly higher without the genetic algorithm.
- Why unresolved: The paper suggests that the genetic algorithm adapted its chromosomes to support logistic regression, potentially leading to inaccuracy in other classifiers, but does not explore this mechanism in detail or test alternative fitness functions.
- What evidence would resolve it: Testing the genetic algorithm with different fitness functions (e.g., using k-NN or decision tree accuracy as the fitness function) and comparing the results across all three classifiers to determine if the choice of fitness function influences the genetic algorithm's effectiveness.

## Limitations

- Dataset size is relatively small (220k training images) compared to deep learning requirements
- Method requires significant computational resources, particularly for genetic algorithm optimization
- Performance comparison against modern deep learning approaches is not directly addressed

## Confidence

- High confidence in methodology and reported results within study constraints
- Medium confidence in generalizability to other cancer types or datasets
- Low confidence in comparison to state-of-the-art deep learning approaches

## Next Checks

1. Conduct repeated experiments with different random seeds to assess variance in genetic algorithm results
2. Compare the PCA+GA+KNN pipeline performance against deep learning baselines on the same dataset
3. Test the pipeline on external breast cancer datasets to evaluate generalization capability