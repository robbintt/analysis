---
ver: rpa2
title: Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural
  Proverbs and Sayings
arxiv_id: '2309.08591'
source_url: https://arxiv.org/abs/2309.08591
tags:
- proverbs
- proverb
- language
- languages
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether multilingual large language models
  (mLLMs) can reason about cultural common ground through proverbs and sayings across
  six languages. The authors create MAPS, a dataset of proverbs with conversational
  contexts and inference tasks, and evaluate a wide range of mLLMs on memorization
  and reasoning tasks.
---

# Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings

## Quick Facts
- arXiv ID: 2309.08591
- Source URL: https://arxiv.org/abs/2309.08591
- Reference count: 37
- Multilingual LLMs show cultural gaps when reasoning about proverbs across six languages

## Executive Summary
This paper investigates whether multilingual large language models can reason about cultural common ground through proverbs and sayings across six languages. The authors create MAPS, a dataset of proverbs with conversational contexts and inference tasks, and evaluate a wide range of mLLMs on memorization and reasoning tasks. Results show that while mLLMs have some knowledge of proverbs, memorization does not necessarily indicate reasoning ability. The models struggle with figurative proverbs and show performance gaps across languages, indicating cultural gaps in understanding. The authors conclude that further research is needed to improve the cultural-diversity of mLLMs and include cultural priors in machine translation models.

## Method Summary
The study evaluates multilingual LLMs on a custom dataset called MAPS containing 2,313 proverbs across six languages (English, German, Russian, Bengali, Chinese, Indonesian) with conversational contexts and inference tasks. Models are assessed using zero-shot prompting with English templates for both memorization (completing proverbs by masking last word) and reasoning (choosing correct interpretation in context) tasks. The evaluation includes masked language models (XLM-R, mT0) and causal LMs (BLOOMZ, XGLM, Llama-2), focusing on identifying culture gaps when models reason about proverbs translated from other languages.

## Key Results
- mLLMs show better performance on high-resource languages (English, Chinese) compared to low-resource languages (Indonesian, Bengali, Russian)
- Memorization of proverbs does not translate to reasoning ability in conversational contexts
- Models struggle significantly with figurative proverbs across all languages, revealing culture gaps in understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual LLMs possess varying levels of knowledge about proverbs, but this knowledge is unevenly distributed across languages.
- Mechanism: The models have been trained on corpora that contain more data from certain languages (e.g., English, Chinese) than others, leading to better memorization and understanding of proverbs in those languages.
- Core assumption: The distribution of proverbs in the training data mirrors the performance disparities observed in the study.
- Evidence anchors:
  - [abstract] "Results show that while mLLMs have some knowledge of proverbs, memorization does not necessarily indicate reasoning ability."
  - [section 5.1] "All models exhibit disparities among the memorization across all languages, and are particularly bad for Indonesian, Bengali, and Russian (i.e. lower-resource languages)."
  - [corpus] FMR score for related papers is moderate (0.415 avg), suggesting limited prior research on multilingual proverb reasoning, which aligns with the novelty of this work.
- Break condition: If the training data distribution changes significantly or if models are trained on more balanced multilingual corpora, the performance disparities might diminish.

### Mechanism 2
- Claim: Memorization of proverbs does not equate to the ability to reason with them in context.
- Mechanism: While models can recall proverbs, understanding their figurative meaning and applying them appropriately in conversational contexts requires additional reasoning capabilities that go beyond simple memorization.
- Core assumption: The models have not been explicitly trained to understand the nuances of figurative language and cultural context.
- Evidence anchors:
  - [abstract] "memorizing proverbs does not mean understanding them within a conversational context."
  - [section 5.2] "Memorization doesn't indicate the ability to reason with proverbs."
  - [corpus] The benchmark MABL focuses on metaphors, not proverbs, indicating a gap in research on proverb reasoning.
- Break condition: If models are trained with explicit reasoning tasks or fine-tuned on culturally diverse datasets, their reasoning abilities might improve.

### Mechanism 3
- Claim: There are significant "culture gaps" in mLLMs when reasoning about proverbs and sayings translated from other languages.
- Mechanism: Machine translation systems do not adequately handle cultural context, leading to errors in translation that affect the models' ability to reason correctly.
- Core assumption: The translation systems used are not culturally aware and fail to preserve the nuances of proverbs.
- Evidence anchors:
  - [abstract] "there is a 'culture gap' in mLLMs when reasoning about proverbs and sayings translated from other languages."
  - [section 5.3] "Machine Translation (MT)...do not handle cultural context (i.e. proverbs) well, such as incomplete translations or wrong translations."
  - [corpus] The related work on cross-lingual benchmarks (M5) suggests that multilingual models still struggle with cultural diversity, supporting the existence of culture gaps.
- Break condition: If translation systems incorporate cultural priors or if models are trained on culturally diverse multilingual data, the culture gaps might be reduced.

## Foundational Learning

- Concept: Figurative language understanding
  - Why needed here: Proverbs often have figurative meanings that differ from their literal interpretations, and models need to understand these nuances to reason correctly.
  - Quick check question: Can the model distinguish between the literal and figurative meanings of a proverb like "The apple doesn't fall far from the tree"?

- Concept: Cross-lingual transfer learning
  - Why needed here: The study evaluates models' ability to reason about proverbs in languages they may not have been explicitly trained on, requiring effective cross-lingual transfer.
  - Quick check question: Does the model's performance on proverbs in a target language improve when it has been trained on proverbs in a related language?

- Concept: Cultural context awareness
  - Why needed here: Understanding proverbs requires knowledge of the cultural context in which they are used, which may vary across languages and regions.
  - Quick check question: Can the model correctly interpret a proverb that relies on culturally specific knowledge, such as the significance of certain animals or foods?

## Architecture Onboarding

- Component map: Multilingual LLMs (mT0, BLOOMZ, XGLM) -> MAPS dataset (proverbs + contexts) -> Zero-shot evaluation templates -> Memorization and reasoning task outputs
- Critical path: Data collection (proverbs and contexts) -> Model evaluation (memorization and reasoning tasks) -> Analysis of results to identify culture gaps
- Design tradeoffs: Balancing the dataset across languages is challenging due to resource availability, and the evaluation focuses on zero-shot performance to assess inherent model capabilities
- Failure signatures: Poor performance on lower-resource languages, inability to reason with figurative proverbs, and significant drops in performance when asked to pick wrong answers
- First 3 experiments:
  1. Evaluate model memorization of proverbs by masking the last word and checking if the model can complete it
  2. Assess reasoning ability by prompting the model to choose the correct interpretation of a proverb in a given context
  3. Investigate culture gaps by translating proverbs to another language and evaluating model performance on the translated data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ability of mLLMs to reason with figurative proverbs compare to their ability to reason with literal proverbs, and what factors contribute to this difference?
- Basis in paper: [explicit] The paper discusses that mLLMs perform worse on figurative proverbs compared to non-figurative ones across multiple languages.
- Why unresolved: The paper does not delve into the specific factors that make figurative proverbs more challenging for mLLMs to understand, such as the complexity of the underlying metaphor or the cultural context required for interpretation.
- What evidence would resolve it: A detailed analysis of the performance of mLLMs on different types of figurative proverbs, along with an investigation into the linguistic and cultural features that contribute to their difficulty, would provide insights into this question.

### Open Question 2
- Question: To what extent does the memorization of proverbs by mLLMs contribute to their ability to reason with proverbs in conversational contexts, and how does this relationship vary across different languages and proverb types?
- Basis in paper: [explicit] The paper finds that memorization of proverbs does not necessarily indicate better reasoning ability with proverbs in contextual settings, and the benefit of memorization is inconsistent across languages.
- Why unresolved: The paper does not explore the specific mechanisms through which memorization may or may not aid reasoning, nor does it examine how this relationship differs for various types of proverbs or across languages with different resource levels.
- What evidence would resolve it: A comprehensive study analyzing the correlation between memorization and reasoning performance for different categories of proverbs, across multiple languages and model sizes, would shed light on this question.

### Open Question 3
- Question: How do culture gaps in mLLMs manifest in cross-lingual communication, and what strategies can be employed to mitigate these gaps and improve cultural awareness in mLLMs?
- Basis in paper: [explicit] The paper identifies culture gaps in mLLMs when reasoning with proverbs translated from other languages and suggests that additional research is needed to improve cultural-awareness and the inclusion of cultural priors in mLLMs.
- Why unresolved: The paper does not provide a detailed analysis of the specific types of culture gaps that exist or propose concrete strategies for addressing them, such as incorporating cultural knowledge during pre-training or fine-tuning.
- What evidence would resolve it: A thorough investigation into the nature and extent of culture gaps in mLLMs, along with experiments testing various approaches to incorporating cultural knowledge and mitigating these gaps, would help answer this question.

## Limitations

- Dataset Representativeness: The MAPS dataset may not fully capture the diversity within each language's proverb usage or account for regional variations in meaning.
- Translation Quality: The study does not quantitatively measure the impact of translation quality on model performance, potentially conflating translation errors with model limitations.
- Zero-shot Evaluation: Exclusive use of zero-shot prompting without few-shot examples limits understanding of how much performance could improve with minimal task-specific fine-tuning.

## Confidence

**High Confidence**: 
- Models show better performance on high-resource languages (English, Chinese) compared to low-resource languages (Indonesian, Bengali, Russian)
- Memorization ability does not translate to reasoning ability
- Models struggle with figurative proverbs across all languages

**Medium Confidence**:
- Culture gaps exist when reasoning about translated proverbs
- Machine translation systems inadequately handle cultural context
- Performance disparities correlate with training data distribution

**Low Confidence**:
- Exact magnitude of culture gaps attributable to translation vs. model limitations
- Generalizability of findings to non-proverb figurative language
- Impact of prompt template variations on evaluation results

## Next Checks

1. **Translation Impact Analysis**: Create a controlled experiment where proverbs are translated by professional translators rather than automated systems, then compare model performance to isolate translation effects from model limitations.

2. **Few-shot Evaluation**: Repeat the reasoning task with 5-shot examples per language to establish upper bounds on model performance and determine whether zero-shot limitations explain the observed culture gaps.

3. **Cross-lingual Transfer Study**: Design experiments to test whether models trained on proverbs in one language can transfer this knowledge to reasoning about proverbs in related languages (e.g., Russian to Bengali), controlling for script and linguistic similarity.