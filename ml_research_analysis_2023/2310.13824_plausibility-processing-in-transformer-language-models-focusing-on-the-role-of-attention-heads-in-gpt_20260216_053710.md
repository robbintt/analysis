---
ver: rpa2
title: 'Plausibility Processing in Transformer Language Models: Focusing on the Role
  of Attention Heads in GPT'
arxiv_id: '2310.13824'
source_url: https://arxiv.org/abs/2310.13824
tags:
- attention
- heads
- plausibility
- gpt2
- processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how Transformer language models process semantic
  plausibility in noun-verb relations by analyzing individual attention heads in GPT-2.
  It finds that GPT-2 shows greater similarity to human plausibility processing patterns
  than other models like BERT, RoBERTa, and ALBERT.
---

# Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT

## Quick Facts
- arXiv ID: 2310.13824
- Source URL: https://arxiv.org/abs/2310.13824
- Reference count: 8
- Primary result: GPT-2 shows greater similarity to human plausibility processing patterns than BERT, RoBERTa, and ALBERT

## Executive Summary
This paper investigates how Transformer language models process semantic plausibility in noun-verb relations by analyzing individual attention heads in GPT-2. The study finds that GPT-2 exhibits stronger alignment with human plausibility processing patterns compared to other transformer models. Through systematic analysis of attention head behavior, the research identifies 18 specific heads that detect plausible noun-verb relationships, demonstrating that semantic processing occurs across multiple layers rather than being concentrated in specific regions. The work highlights that while these heads collectively contribute to plausibility processing, their individual contributions vary significantly, with one head (0,10) accounting for most of GPT-2's plausibility effects.

## Method Summary
The study analyzes GPT-2's attention heads to identify which ones detect semantic plausibility between nouns and verbs. Using experimental materials from Cunnings and Sturt (2018) with 32 sentence sets varying noun-verb plausibility, the researchers compute surprisal (negative log probability) at verbs and measure attention head accuracy in selecting plausible nouns over implausible ones. They systematically prune identified plausibility-processing heads and measure changes in surprisal patterns to establish causal effects. The analysis compares GPT-2's behavior with other transformer models (BERT, RoBERTa, ALBERT) and examines how attention distributions correlate with human processing patterns.

## Key Results
- GPT-2 demonstrates greater similarity to human plausibility processing patterns than BERT, RoBERTa, and ALBERT
- 18 attention heads in GPT-2 detect plausible noun-verb relationships, distributed across all 12 layers
- Individual attention head performance in detecting plausibility doesn't correlate with their causal contribution to model behavior
- One head (0,10) accounts for most of GPT-2's plausibility effects despite modest individual performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention heads in GPT-2 detect semantic plausibility between nouns and verbs independently of syntactic dependency relations.
- Mechanism: Individual attention heads can focus on the semantic relationship between words (e.g., "plate" and "shattered") regardless of whether they are in a syntactic dependency. The head's attention distribution reflects the semantic plausibility rather than grammatical structure.
- Core assumption: Attention patterns can encode semantic relationships that go beyond syntactic dependencies.
- Evidence anchors:
  - [abstract] "it was found that: i) GPT2 has a number of attention heads that detect plausible noun-verb relationships"
  - [section 4] "I consider attention heads are able to process plausible relationships between nouns and verbs when their accuracy in identifying appropriate nouns surpasses the chance level"
  - [corpus] Weak - the corpus mentions attention heads and transformers but doesn't directly address semantic plausibility detection
- Break condition: If attention patterns were found to only encode syntactic dependencies and not semantic relationships, this mechanism would break.

### Mechanism 2
- Claim: GPT-2's decoder-only architecture contributes to its similarity with human plausibility processing patterns.
- Mechanism: GPT-2 processes sentences incrementally, constructing meaning of each word based only on preceding context, similar to how humans process language. This contrasts with bidirectional models that integrate both preceding and following words.
- Core assumption: Incremental processing is more psychologically plausible for simulating human language comprehension.
- Evidence anchors:
  - [section 3.4] "I assume that the GPT2's similarity to humans arises from the psychological plausibility of its decoder-only architecture"
  - [section 3] "it processes sentences incrementally much like the way humans process sentences"
  - [corpus] Weak - the corpus mentions transformer architectures but doesn't directly address incremental vs. bidirectional processing differences
- Break condition: If bidirectional models were found to be equally or more similar to human processing patterns, this mechanism would break.

### Mechanism 3
- Claim: Individual attention head performance in detecting plausibility doesn't correlate with its causal contribution to model behavior.
- Mechanism: An attention head's ability to identify plausible noun-verb pairs (measured by accuracy) doesn't predict how much removing that head affects the model's overall plausibility sensitivity. Some heads with modest performance can have large causal effects.
- Core assumption: Performance metrics at the head level don't necessarily translate to model-level effects due to information flow and integration across layers.
- Evidence anchors:
  - [abstract] "attention heads' individual performance in detecting plausibility does not necessarily correlate with how much they contribute to GPT2's plausibility processing ability"
  - [section 5.4] "Interestingly, the head (0, 10) did not achieve noteworthy performance in detecting plausible nouns over implausible nouns in Section 4"
  - [corpus] Weak - the corpus mentions attention heads but doesn't specifically address the disconnect between head performance and causal contribution
- Break condition: If head performance metrics were found to reliably predict their causal effects on model behavior, this mechanism would break.

## Foundational Learning

- Concept: Causal mediation analysis in neural networks
  - Why needed here: To understand how individual attention heads contribute to the model's overall plausibility processing ability, not just whether they can detect plausibility
  - Quick check question: What's the difference between measuring an attention head's accuracy at a task versus its causal effect on model behavior?

- Concept: Incremental vs. bidirectional language model processing
  - Why needed here: To understand why GPT-2 shows more human-like plausibility processing compared to BERT, RoBERTa, and ALBERT
  - Quick check question: How does processing each word based only on preceding context differ from integrating both preceding and following context?

- Concept: Attention head specialization in transformer architectures
  - Why needed here: To understand how different attention heads can serve different functions (syntactic vs. semantic processing) and how they're distributed across layers
  - Quick check question: Why might semantic processing heads be more diffusely distributed across layers compared to syntactic processing heads?

## Architecture Onboarding

- Component map:
  - Input embeddings → 12-layer transformer blocks → Output logits
  - Each block contains: multi-head self-attention, layer normalization, feed-forward network, residual connections
  - 12 attention heads per layer (144 total), each with independent parameters

- Critical path:
  - Token embeddings → Bottom layers (syntactic processing) → Middle layers (semantic processing) → Top layers (task-specific representations) → Output

- Design tradeoffs:
  - Decoder-only (GPT-2) vs. encoder-only (BERT) vs. encoder-decoder (T5) architectures for different processing characteristics
  - Number of attention heads per layer (12) vs. computational cost
  - Depth (12 layers) vs. ability to capture complex semantic relationships

- Failure signatures:
  - Removing plausibility-processing heads causes loss of plausibility effects without affecting other language tasks
  - Randomly removing heads maintains some plausibility effects but reduces overall performance
  - Certain heads (like 0,10) cause disproportionate effects when removed

- First 3 experiments:
  1. Measure surprisal differences between plausible and implausible conditions with full model
  2. Remove plausibility-processing heads (0,1), (0,5), (0,10) and measure changes in surprisal patterns
  3. Gradually prune attention heads in order of their plausibility detection accuracy and track changes in model behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do attention heads specialized for semantic plausibility interact differently with syntactic dependency heads compared to other attention heads?
- Basis in paper: [inferred] The paper notes that plausibility processing heads are "diffusely distributed" across layers and affect both syntactic dependents and distractors, suggesting potential interactions with syntactic processing heads.
- Why unresolved: The paper focuses on causal effects of plausibility heads in isolation but doesn't examine their interactions with syntactic heads or other specialized heads.
- What evidence would resolve it: Experiments testing surprisal changes when pruning plausibility heads in combination with syntactic heads, or analyzing attention patterns when both types are active simultaneously.

### Open Question 2
- Question: How do the 18 plausibility-processing attention heads encode semantic features to distinguish plausible from implausible noun-verb relations?
- Basis in paper: [explicit] The paper finds 18 heads that detect plausibility but doesn't explain the mechanism by which they encode semantic features like [+shatterable].
- Why unresolved: The analysis only measures head performance through accuracy metrics without examining the internal representations or feature extraction processes.
- What evidence would resolve it: Probing experiments that extract attention patterns for specific semantic features, or ablation studies showing which token-level information each head relies on.

### Open Question 3
- Question: Would the plausibility processing patterns differ in autoregressive models with different architectural choices (e.g., decoder-only vs encoder-decoder)?
- Basis in paper: [explicit] The paper attributes GPT-2's superior performance to its "decoder-only architecture" that processes incrementally like humans.
- Why unresolved: The study only compares GPT-2 with encoder-based models (BERT, RoBERTa, ALBERT) without testing other autoregressive architectures like decoder-encoder hybrids.
- What evidence would resolve it: Testing plausibility processing in models like BART or T5, or comparing GPT-2 with models using different positional encoding schemes.

## Limitations

- Only three plausibility-processing heads (0,1), (0,5), and (0,10) were systematically tested for causal effects, leaving 15 heads unverified
- The study doesn't explain the mechanism by which plausibility-processing heads encode semantic features
- Claims about decoder-only architecture being psychologically plausible lack direct experimental validation

## Confidence

- **High confidence**: GPT-2 shows greater similarity to human plausibility processing than BERT, RoBERTa, and ALBERT; attention heads can detect semantic plausibility independently of syntax
- **Medium confidence**: The identified 18 heads collectively contribute to plausibility processing; individual head performance doesn't predict causal contribution
- **Low confidence**: Claims about GPT-2's decoder-only architecture being psychologically plausible; specific causal effects of all 18 identified heads

## Next Checks

1. **Systematic ablation study**: Perform controlled pruning of each of the 18 identified plausibility-processing heads individually and in combinations to verify their collective and individual causal contributions to model behavior.

2. **Cross-linguistic validation**: Test whether the same attention head mechanisms for plausibility processing generalize to other languages and typologically diverse sentence structures beyond the English materials used.

3. **Temporal dynamics analysis**: Track how surprisal differences between plausible and implausible conditions evolve across processing time (using models like HuBERT or similar) to determine if the plausibility effects observed are truly incremental or reflect post-hoc integration.