---
ver: rpa2
title: Computational Basis of LLM's Decision Making in Social Simulation
arxiv_id: '2504.11671'
source_url: https://arxiv.org/abs/2504.11671
tags:
- social
- steering
- page
- https
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a method for probing, quantifying, and manipulating
  how social concepts are encoded within transformer-based LLMs. The approach extracts
  vectors representing variable variations (e.g., male-to-female, age ranges) from
  the model's internal residual stream, orthogonalizes them to isolate unique effects,
  and injects controlled perturbations to steer decision-making.
---

# Computational Basis of LLM's Decision Making in Social Simulation

## Quick Facts
- **arXiv ID:** 2504.11671
- **Source URL:** https://arxiv.org/abs/2504.11671
- **Reference count:** 27
- **Primary result:** Introduces a method to extract, orthogonalize, and inject steering vectors representing social concepts (gender, age, framing) in transformer residual streams, successfully manipulating LLM decisions in a Dictator Game with high precision.

## Executive Summary
This study develops a computational framework for probing and controlling how social concepts are encoded within transformer-based LLMs. The approach extracts vectors representing variable variations from the model's internal residual stream, orthogonalizes them to isolate unique effects, and injects controlled perturbations to steer decision-making. Applied to a Dictator Game experiment, the method successfully identified internal representations of social factors and manipulated model decisions with high precision, demonstrating that social meanings can be treated as measurable, manipulable directions in LLM activation space.

## Method Summary
The method involves four key steps: (1) extracting "vectors of variable variation" by computing mean residual stream differences across controlled variable conditions, (2) orthogonalizing these vectors to isolate unique variable contributions, (3) projecting partial vectors onto a decision direction vector, and (4) injecting scaled projections into residual streams during inference to produce controlled behavioral shifts. The approach was tested on Llama-3.1-8B-Instruct using a Dictator Game task where participants allocate $20, with success measured through logistic regression coefficient changes and logic-check pass rates.

## Key Results
- 320 out of 1,891 steering manipulations (16.92%) significantly influenced the likelihood of transferring money
- 98.44% of successful manipulations showed the expected positive effect
- 88.75% of successful manipulations demonstrated orthogonal effects, supporting variable independence

## Why This Works (Mechanism)

### Mechanism 1: Social Variables Encode as Linear Directions in Residual Streams
- Claim: Social concepts manifest as approximately linear vectors in the residual stream that can be extracted via experimental contrasts.
- Evidence: [abstract] "We extract 'vectors of variable variations' (e.g., 'male' to 'female') from the LLM's internal state." [Section 1.3.2] "By taking the difference of these means, we obtain the vector of age variation."
- Break condition: If variable representations are non-linear or entangled, extracted vectors may not capture meaningful directions.

### Mechanism 2: Orthogonalization Isolates Unique Variable Contributions
- Claim: Subtracting overlapping vector components via projection removes confounds, yielding "partial steering vectors" that represent each variable's independent effect.
- Evidence: [abstract] "By extracting and orthogonalizing 'vectors of variable variations'... the approach enables precise control." [Section 3.4] "88.75% of successful manipulations showed orthogonal effects."
- Break condition: If social variables are fundamentally non-orthogonal, orthogonalization may remove meaningful signal.

### Mechanism 3: Decision-Space Projection Enables Targeted Behavioral Steering
- Claim: Projecting partial IV vectors onto the DV direction and injecting scaled versions into residual streams produces controlled shifts in model outputs.
- Evidence: [Section 3.3.2] "320 of 1,891 manipulations (16.92%) produced statistically significant coefficient changes for Female; 98.44% of these were positive."
- Break condition: If large α values destabilize representations or gradient propagation dilutes directional effects.

## Foundational Learning

- Concept: Residual Stream Architecture
  - Why needed here: The entire method depends on understanding how transformer layers pass information through additive residual connections.
  - Quick check question: Can you explain why a vector injected at layer 5 would influence the final output more than one injected at layer 30?

- Concept: Cosine Similarity vs. Dot Product
  - Why needed here: The paper emphasizes dissociation between directional alignment and magnitude when measuring variable influence on decisions.
  - Quick check question: If two vectors have cosine similarity of 0.9 but a small dot product, what does that tell you about their relationship?

- Concept: Orthogonalization via Projection
  - Why needed here: Removing shared variance between correlated variables is central to isolating causal effects.
  - Quick check question: Given vectors v_gender and v_age, how would you compute the component of v_gender that is independent of age?

## Architecture Onboarding

- Component map: Tokenizer -> Embedding Layer (4,096-dim) -> 32 Decoder Layers (Self-Attention + MLP + Residual Add) -> LM Head (logits over 128K tokens)
- Critical path: 1. Run baseline trials with randomized IV combinations; collect residual streams at all layers 2. Compute mean residual streams per IV condition; subtract to get raw steering vectors 3. Orthogonalize raw vectors to obtain partial steering vectors 4. Extract DV steering vector by contrasting high-D vs. low-D trials 5. Project partial IV vectors onto DV direction 6. Inject α × projection into target layer during inference; measure output shift
- Design tradeoffs:
  - Injection coefficient α: Larger |α| produces stronger effects but risks output incoherence (paper suggests |α| < 15 for stability; tested up to 30)
  - Layer selection ℓ: Early layers (1-10) allow amplified effects via residual propagation; later layers (20-31) offer more localized control
  - Injecting full partial vector vs. projection only: Projection sacrifices non-decision-aligned effects for precision
- Failure signatures:
  - Model collapse: Generated text becomes incoherent or repetitive → |α| too large
  - No behavioral change despite injection → Vector may be poorly estimated; try different layers or re-estimate with more trials
  - Unexpected changes in non-target variables → Orthogonalization incomplete; check partial vector correlations
  - Low logic-check pass rate → Prompt design issue or model capability limitation
- First 3 experiments:
  1. Replicate a single IV steering (e.g., Female) at layer 15 with α = 10; verify regression coefficient changes in expected direction using 500 trials.
  2. Test orthogonality by steering Gender while measuring coefficients for Age and Give-Take; confirm no significant spillover.
  3. Sweep α ∈ {-20, -10, 0, 10, 20} at layers 5, 15, 25 to map dose-response and layer sensitivity for a target variable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can activation-based steering interventions that align LLM behavior with human benchmarks be shown to produce externally valid simulations of human social behavior?
- Basis in paper: [explicit] "We do not claim external sociological validity; linking these internal directions to human cognition and behaviors is a separate empirical question."
- Why unresolved: The study deliberately focused on internal representational validity and controlled steering within activation space.
- What evidence would resolve it: Systematic human-LLM comparison studies using identical experimental paradigms.

### Open Question 2
- Question: How do steering vector magnitudes, sign stability, and layer sensitivity vary across different model families and architectures?
- Basis in paper: [explicit] Section 1.2.5 states these characteristics are empirical and may vary with model family and data.
- Why unresolved: The study only tested Llama3.1-8B; architectural differences alter subspace structure in ways not yet characterized.
- What evidence would resolve it: Cross-model replication using identical extraction and injection protocols.

### Open Question 3
- Question: Why does the relationship between injection coefficient and resulting regression coefficient remain weak and unpredictable?
- Basis in paper: [explicit] "Theoretically, we would expect a stronger positive correlation... but the observed correlation is minuscule (|r| < 0.05)."
- Why unresolved: The additive residual-stream assumption may not fully capture how steering vectors propagate through attention and MLP sublayers.
- What evidence would resolve it: Mechanistic analysis of how injected vectors are transformed across layers.

## Limitations

- Model and Task Specificity: All results are derived from Llama-3.1-8B-Instruct on a single Dictator Game scenario.
- Residual Stream Access Assumptions: The method assumes residual streams can be reliably extracted and manipulated at arbitrary layers without architectural constraints.
- Activation Geometry Validity: The approach presumes social variables form coherent, classifiable directions in activation space that may not reflect non-linear, context-dependent social concepts.

## Confidence

**High Confidence**: The method can extract and inject vectors that statistically shift LLM decisions in the controlled Dictator Game; 98.44% of successful manipulations showed expected positive effects; orthogonalization reduces variable correlation in practice.

**Medium Confidence**: Social variables encode as approximately linear directions in residual streams; orthogonalization isolates unique variable contributions; projection onto decision direction enables targeted behavioral steering.

**Low Confidence**: Method generalizes to other social simulation tasks or model architectures; extracted vectors represent true causal social representations; approach scales to high-dimensional social concept spaces.

## Next Checks

1. **Cross-Task Transfer**: Apply the steering method to a different social simulation (e.g., Trust Game) using the same model. Compare vector extraction quality and steering effectiveness to test generalizability of the linear representation assumption.

2. **Ablation Study on Orthogonalization**: Run parallel experiments with and without orthogonalization across all variable combinations. Measure both steering precision and collateral effects to quantify the actual benefit of mathematical disentanglement.

3. **Layer-wise Steering Analysis**: Systematically test injection at all 31 layers with varying α values for a single variable. Map the dose-response relationship to identify whether early layers amplify effects through residual propagation as hypothesized.