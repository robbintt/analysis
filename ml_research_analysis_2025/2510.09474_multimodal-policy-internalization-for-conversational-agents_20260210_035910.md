---
ver: rpa2
title: Multimodal Policy Internalization for Conversational Agents
arxiv_id: '2510.09474'
source_url: https://arxiv.org/abs/2510.09474
tags:
- condition
- policy
- check
- image
- continue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Multimodal Policy Internalization (MPI),\
  \ a new task for embedding reasoning-intensive multimodal policies into large multimodal\
  \ model parameters to improve policy compliance without in-context prompts. The\
  \ authors construct two datasets\u2014ClevrPolicy for synthetic decision-making\
  \ and GTAPolicy for real-world tool-use\u2014and propose a three-stage training\
  \ framework, TriMPI, which combines visually-masked continual pretraining, chain-of-thought\
  \ supervised finetuning, and a novel PolicyRollout reinforcement learning algorithm."
---

# Multimodal Policy Internalization for Conversational Agents

## Quick Facts
- arXiv ID: 2510.09474
- Source URL: https://arxiv.org/abs/2510.09474
- Reference count: 40
- Primary result: Up to 70.7% accuracy gain on synthetic decision-making and 79.4% on real-world tool-use tasks

## Executive Summary
This paper introduces Multimodal Policy Internalization (MPI), a framework for embedding complex multimodal policies directly into large multimodal model parameters, eliminating the need for in-context policy prompts at inference time. The authors construct two datasets—ClevrPolicy for synthetic decision-making and GTAPolicy for real-world tool-use—and propose a three-stage training framework called TriMPI. Experiments demonstrate substantial accuracy improvements (70.7% and 79.4% absolute gains) over baseline methods while reducing inference token usage by 93.9%.

## Method Summary
TriMPI employs a three-stage training approach on Qwen2.5-VL-7B. First, Visually-Masked Continual Pretraining (VM-CPT) injects policy knowledge by performing next-token prediction on policy-text and multimodal policy documents while masking visual tokens. Second, Chain-of-Thought Supervised Finetuning (CoT-SFT) with LoRA adapters trains the model to generate reasoning before answers. Third, PolicyRollout reinforcement learning extends GRPO/DAPO by generating both policy-aware and standard rollouts, computing advantages from the combined set while applying gradients only to the no-policy path. This enables inference without providing the policy, achieving significant accuracy improvements across both ClevrPolicy and GTAPolicy benchmarks.

## Key Results
- TriMPI achieves 70.7% absolute accuracy gain on ClevrPolicy compared to SFT baseline
- TriMPI achieves 79.4% absolute accuracy gain on GTAPolicy compared to SFT baseline
- Policy internalization reduces prompt token usage by 93.9% while maintaining performance
- RL stage uniquely leverages non-CoT data, achieving 47.05% accuracy vs. 22.25% for SFT on ClevrPolicy-T

## Why This Works (Mechanism)

### Mechanism 1: Visually-Masked Continual Pretraining (VM-CPT)
Pretraining on policy text while masking visual tokens injects policy knowledge before behavior learning. The model performs next-token prediction on concatenated sequences but computes loss only on non-visual tokens, allowing standard language modeling objectives to operate on multimodal policy documents without requiring novel pretraining objectives.

### Mechanism 2: PolicyRollout Augments Exploration with Policy-Grounded Responses
Adding policy-conditioned responses to RL rollouts improves exploration quality without train-inference mismatch. During rollouts, the model generates both standard and policy-aware responses, using the combined set for advantage estimation while applying gradients only to the no-policy path.

### Mechanism 3: RL Stage Enables Non-CoT Data Leverage and Reasoning Generalization
RL with verifiable rewards extracts learning signal from non-CoT data that SFT cannot utilize effectively. GRPO/DAPO algorithms generate multiple rollouts and reinforce high-reward trajectories based on answer correctness, allowing learning from abundant non-CoT examples without requiring reasoning annotations.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**: TriMPI's RL stage builds directly on GRPO; PolicyRollout modifies GRPO's rollout generation. Without understanding baseline GRPO, the PolicyRollout modification will be opaque.
  - Quick check: Can you explain why GRPO uses group-based advantage estimation rather than a learned value function?

- **Catastrophic Forgetting in Continual Learning**: The paper explicitly evaluates robustness to forgetting on MMMU-Pro and MMLU-Pro benchmarks. Policy internalization must not degrade general reasoning.
  - Quick check: Why might small-domain finetuning cause more forgetting than large-domain finetuning?

- **Chain-of-Thought Supervision vs. Outcome-Only RL**: The paper compares CoT-SFT (learning reasoning trajectories) vs. RL (learning from outcomes). Understanding when each is necessary prevents redundant experimentation.
  - Quick check: Given limited annotation budget, when would you prioritize CoT data generation vs. expanding non-CoT data for RL?

## Architecture Onboarding

- **Component map**: Input: (Query Q, Image I, Policy P) → VM-CPT → CoT-SFT → RL with PolicyRollout → Final internalized model
- **Critical path**: CoT data quality determines SFT initialization quality, which affects RL exploration; VM-CPT must precede SFT; PolicyRollout matters most for complex policies
- **Design tradeoffs**: GRPO vs. DAPO (DAPO converges faster on data-rich ClevrPolicy but overfits on data-scarce GTAPolicy); LoRA vs. full finetuning (SFT uses LoRA to preserve general capabilities); Forward vs. Reverse CoT generation
- **Failure signatures**: Near-random performance after SFT indicates train-inference mismatch; high RL reward but low eval accuracy suggests PolicyRollout gradients not transferring; catastrophic forgetting on general benchmarks indicates overfitting to small dataset
- **First 3 experiments**: 1) Baseline validation: Run CoT-SFT on ClevrPolicy-T to verify ~49% accuracy; 2) Ablation checkpoint: Train VM-CPT + CoT-SFT (no RL) to verify 22-27% accuracy range; 3) PolicyRollout sanity check: Compare GRPO vs. PoRo-GRPO on N=4 policy complexity expecting 10-15% gap

## Open Questions the Paper Calls Out

- **Scaling to larger, diverse datasets**: Does TriMPI maintain performance gains when scaling to significantly larger, more diverse real-world datasets beyond synthetic ClevrPolicy and limited GTAPolicy benchmarks?
- **Sophisticated continual pretraining**: Can more sophisticated continual pretraining methods outperform the simple visual masking (VM-CPT) used in this study for embedding visual policy components?
- **Heterogeneous task mixtures**: How can training frameworks be adapted to effectively internalize mixtures of tasks with highly divergent response formats (e.g., natural language vs. structured tool calls)?

## Limitations

- Performance evaluation limited to synthetic and small real-world datasets, leaving scalability to massive, open-domain datasets unproven
- VM-CPT masks visual tokens during initial policy knowledge injection, potentially losing information from policy images
- RL stage effectiveness varies substantially between datasets, suggesting sensitivity to dataset characteristics

## Confidence

**High Confidence**: VM-CPT consistently improves performance across all ablations and datasets; Three-stage framework provides substantial gains over SFT-only baselines; General architecture and training pipeline are well-specified

**Medium Confidence**: PolicyRollout's mechanism of transferring policy-aware knowledge to no-policy conditions; RL stage's specific contribution to non-CoT data leverage; Catastrophic forgetting robustness claims

**Low Confidence**: Exact conditions for optimal GRPO vs. DAPO selection; Generalization guarantees for unseen policy types; Long-term retention of internalized policies under continued training

## Next Checks

1. **PolicyRollout Transfer Validation**: Design an experiment where policy-aware rollouts exploit policy-specific shortcuts unavailable during inference. Measure whether high rollout rewards correlate with high inference accuracy.

2. **Visual Token Masking Analysis**: Conduct controlled experiments on ClevrPolicy-M where visual tokens are progressively unmasked during VM-CPT. Compare performance against fully masked and fully unmasked conditions.

3. **Generalization Stress Test**: Create a variant of ClevrPolicy with novel policy patterns not seen during training. Evaluate TriMPI's performance against baselines to assess true policy internalization versus memorization.