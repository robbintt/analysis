---
ver: rpa2
title: Normality Calibration in Semi-supervised Graph Anomaly Detection
arxiv_id: '2510.02014'
source_url: https://arxiv.org/abs/2510.02014
tags:
- anomaly
- graphnc
- teacher
- normal
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphNC addresses the limitation of semi-supervised graph anomaly
  detection (GAD) methods, which often overfit to labeled normal nodes, leading to
  high false positive rates. The proposed GraphNC framework calibrates normality learned
  from a teacher model (a pre-trained semi-supervised GAD model) in both anomaly score
  and representation spaces.
---

# Normality Calibration in Semi-supervised Graph Anomaly Detection

## Quick Facts
- arXiv ID: 2510.02014
- Source URL: https://arxiv.org/abs/2510.02014
- Reference count: 40
- Key outcome: GraphNC improves semi-supervised GAD by 7.1% AUROC and 31.5% AUPRC over GGAD baseline

## Executive Summary
GraphNC addresses overfitting to labeled normal nodes in semi-supervised graph anomaly detection by calibrating normality learned from a pre-trained teacher model. The framework uses ScoreDA to align student anomaly scores with teacher distribution and NormReg to regularize normality through perturbation-based consistency on labeled nodes. This dual approach consistently improves three types of teacher models across six benchmark datasets, significantly reducing false positives while maintaining high detection performance.

## Method Summary
GraphNC is a teacher-student framework where a student GNN learns from a frozen pre-trained teacher model. It consists of two components: ScoreDA minimizes MSE between student and teacher anomaly scores to align distributions, and NormReg applies consistency regularization on labeled normal nodes by enforcing representation similarity between original and masked inputs. The student model uses a 2-layer GNN backbone with MLP scorer, trained with Adam optimizer at learning rate 5e-3 or 5e-4 depending on dataset, with mask ratio ω=0.3 and α=0.01 controlling regularization strength.

## Key Results
- Achieves 7.1% average AUROC improvement over GGAD baseline
- Achieves 31.5% average AUPRC improvement over GGAD baseline
- Consistent performance gains across six benchmark datasets with different teacher models

## Why This Works (Mechanism)

### Mechanism 1: Score Distribution Alignment (ScoreDA)
ScoreDA minimizes MSE between student and teacher anomaly scores, leveraging the teacher's superior separation capability to push normal and abnormal scores toward opposite tails. The core assumption is that the teacher correctly characterizes most normal nodes. Break condition: If teacher is highly inaccurate, ScoreDA may propagate noise and degrade performance.

### Mechanism 2: Perturbation-based Normality Regularization (NormReg)
NormReg enforces consistency between original and augmented representations of labeled normal nodes using attribute masking. This minimizes representation distance for labeled normal nodes, creating a more compact normal cluster and reducing false positives. Core assumption: Masking preserves normal node semantics. Break condition: If mask ratio is too high, it may destroy critical feature information.

### Mechanism 3: Noise-Tolerant Knowledge Transfer
The synergy between ScoreDA and NormReg allows the student to correct teacher inaccuracies. ScoreDA transfers general score structure while NormReg uses labeled normal nodes as ground truth anchors to override incorrect teacher scores. Core assumption: Labeled set V_l is clean and representative. Break condition: If V_l contains anomalies, NormReg will reinforce false negatives.

## Foundational Learning

- **Concept: Semi-Supervised Graph Anomaly Detection (GAD)**
  - Why needed: Operates with small verified normal node sets (V_l) and large unlabeled sets (V_u), distinct from fully supervised/unsupervised settings
  - Quick check: Can you distinguish between one-class classification loss vs reconstruction loss used in baseline methods?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - Why needed: GraphNC is fundamentally a distillation framework where student learns from teacher's soft anomaly scores
  - Quick check: How does MSE loss between teacher/student scores differ from training with hard binary labels?

- **Concept: Consistency Regularization**
  - Why needed: NormReg relies on semi-supervised learning principle that predictions should remain consistent under input perturbation
  - Quick check: Why is consistency enforced only on labeled nodes (V_l) rather than unlabeled nodes?

## Architecture Onboarding

- **Component map:** Input Graph G → Teacher Model (frozen) → Student Model (2-layer GNN + MLP) → Augmenter (mask 30%) → Loss Aggregator (ScoreDA + NormReg)

- **Critical path:**
  1. Pre-train Teacher on V_l and freeze
  2. Forward pass Input through Student → Y^S
  3. Forward pass Masked Input through Student → H̃^S
  4. Compute L_ScoreDA on all nodes (V)
  5. Compute L_NormReg on labeled nodes (V_l)
  6. Backpropagate L_GraphNC = L_ScoreDA + α·L_NormReg to update Student only

- **Design tradeoffs:**
  - Teacher Choice: Stronger teachers yield better results but framework robust to weaker ones due to NormReg
  - Alpha (α): Controls regularization strength; excessive α may compress representation space too much
  - Mask Ratio (ω): Fixed at 0.3 but different datasets may benefit from different levels

- **Failure signatures:**
  - High FPR: ScoreDA overfitting to noisy teacher; increase α or check V_l quality
  - Low separation: Student failing to converge to teacher; check learning rate or teacher quality

- **First 3 experiments:**
  1. Teacher Ablation: Run GraphNC with GGAD, DOMINANT, OCGNN on Amazon dataset
  2. Component Isolation: Compare OT+ScoreDA vs OT+ScoreDA+NormReg on FPR reduction
  3. Noise Robustness: Degrade teacher predictions and measure GraphNC recovery via NormReg

## Open Questions the Paper Calls Out

- **Future work will explore other supplementary supervision or expert knowledge to reduce its reliance on the teacher model.**
- **How does the presence of label noise (anomalies within V_l) impact NormReg efficacy?**
- **Can the optimal mask ratio ω be determined adaptively based on graph topology rather than fixed across datasets?**

## Limitations
- Performance heavily relies on teacher model quality and limited labeled data quality
- Fixed mask ratio (ω=0.3) may not be optimal across all graph types with varying attribute distributions
- Core mechanism effectiveness depends on clean labeled normal set V_l being representative of true normality

## Confidence

- **High confidence:** ScoreDA's mechanism of aligning student-teacher score distributions for separation
- **Medium confidence:** NormReg's effectiveness in reducing false positives through consistency regularization  
- **Medium confidence:** Overall framework's performance improvements over GGAD baseline
- **Low confidence:** Exact reproducibility without teacher model code and complete dataset specifications

## Next Checks

1. **Teacher degradation test:** Systematically degrade teacher predictions and measure if GraphNC maintains/improves performance relative to degraded teacher, quantifying gap between OT+ScoreDA and OT+ScoreDA+NormReg

2. **Component isolation experiment:** Run controlled experiments on Amazon dataset comparing OT, OT+ScoreDA only, and full GraphNC, measuring FPR reduction specifically attributable to NormReg

3. **Mask ratio sensitivity analysis:** Test GraphNC across six datasets with varying mask ratios (ω ∈ {0.1, 0.3, 0.5, 0.7}) to identify optimal perturbation strength for each dataset type and validate ω=0.3 choice