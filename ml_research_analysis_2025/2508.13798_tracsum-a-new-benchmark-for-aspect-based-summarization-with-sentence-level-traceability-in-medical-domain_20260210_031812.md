---
ver: rpa2
title: 'TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level
  Traceability in Medical Domain'
arxiv_id: '2508.13798'
source_url: https://arxiv.org/abs/2508.13798
tags:
- summary
- evaluation
- summarization
- aspect
- citations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TracSum, a new benchmark for aspect-based
  summarization in the medical domain with sentence-level traceability. The benchmark
  addresses the problem of factual accuracy in medical summaries by requiring systems
  to generate structured summaries paired with citations to source sentences.
---

# TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain

## Quick Facts
- arXiv ID: 2508.13798
- Source URL: https://arxiv.org/abs/2508.13798
- Reference count: 40
- Key outcome: Introduces TracSum benchmark for aspect-based medical summarization with sentence-level citations, demonstrating improved factual accuracy through explicit sentence tracking before summarization.

## Executive Summary
This paper introduces TracSum, a new benchmark for aspect-based summarization in the medical domain with sentence-level traceability. The benchmark addresses the problem of factual accuracy in medical summaries by requiring systems to generate structured summaries paired with citations to source sentences. The dataset comprises 3.5K summary-citation pairs annotated from 500 medical abstracts across seven medical aspects. A fine-grained evaluation framework measures completeness and consistency using claim and citation recall/precision. Experiments with multiple LLMs show that explicitly tracking relevant sentences before summarization improves accuracy, while incorporating full context further enhances completeness. The proposed TRACK-THEN-SUM pipeline achieves strong performance, demonstrating TracSum's effectiveness as a benchmark for traceable, aspect-based summarization.

## Method Summary
The TracSum benchmark introduces a traceable aspect-based summarization task where systems must generate medical summaries paired with sentence-level citations. The approach uses a TRACK-THEN-SUM pipeline with two components: a Tracker (T) that identifies relevant sentences for each aspect using a binary classifier on sentence-aspect pairs, and a Summarizer (S) that generates aspect-specific summaries conditioned on cited sentences. Both components are fine-tuned from LLaMA-3.1-8B-Instruct using QLoRA with specified hyperparameters. The evaluation framework decomposes summaries into subclaims using Mistral Large and checks entailment with the TRUE model to compute four metrics: Claim Recall, Citation Recall, Claim Precision, and Citation Precision.

## Key Results
- TRACK-THEN-SUM pipeline outperforms standard methods, with the best configuration (TTS⊕f.) achieving 72.23% Claim Recall and 70.07% Citation Recall
- Explicitly tracking relevant sentences before summarization (TTS) improves factual accuracy compared to context-only approaches
- Incorporating full context (TTS⊕f.) further enhances completeness but requires careful citation management
- GPT-4o shows substantially stronger alignment with human judgments (ρ = 0.80) compared to the TRUE model (ρ = 0.61)

## Why This Works (Mechanism)
The TRACK-THEN-SUM pipeline works by first identifying relevant evidence sentences through the Tracker component, which creates a focused set of source material for the Summarizer. This two-stage approach prevents the model from having to sift through irrelevant information during summarization, reducing hallucination and improving factual consistency. The sentence-level citation requirement forces the system to ground each claim in specific source text, creating a verifiable link between generated content and original evidence. By decomposing summaries into subclaims and checking entailment with the TRUE model, the evaluation framework provides fine-grained measurement of both completeness (what was claimed) and consistency (what was supported by citations).

## Foundational Learning

**Aspect-based summarization** - Extracting information relevant to specific predefined aspects from source documents
Why needed: Medical abstracts contain diverse information across different aspects (aims, participants, outcomes) that require separate treatment
Quick check: Verify the seven aspects (A, I, O, P, M, D, S) cover all major components of medical abstracts

**Sentence-level traceability** - Linking each claim in the summary to specific source sentences via citations
Why needed: Ensures factual accuracy and enables verification of generated content
Quick check: Confirm each summary subclaim has corresponding citation from the source abstract

**Entailment-based evaluation** - Using NLI models to verify whether summary claims are supported by cited evidence
Why needed: Provides automated, fine-grained measurement of factual consistency
Quick check: Test TRUE model on sample entailment pairs to verify expected performance

**QLoRA fine-tuning** - Quantized Low-Rank Adaptation for efficient LLM adaptation
Why needed: Enables fine-tuning large models on limited hardware resources
Quick check: Verify memory usage stays within hardware constraints during training

## Architecture Onboarding

**Component map**: Abstract -> Tracker T (sentence-aspect pairs) -> Cited sentences -> Summarizer S (conditioned on aspect + cited sentences) -> Structured summary + citations -> Evaluation (decomposition + entailment)

**Critical path**: The TRACK-THEN-SUM pipeline is the core workflow: Tracker identifies relevant sentences, Summarizer generates aspect-specific content conditioned on those sentences, evaluation decomposes and verifies claims against citations.

**Design tradeoffs**: Sentence tracking improves factual accuracy but adds complexity; full context incorporation enhances completeness but risks citation precision; automated evaluation is efficient but shows gap vs human judgment.

**Failure signatures**: Low citation recall on Outcomes and Intervention aspects (more relevant sentences per abstract); claim recall discrepancy between automatic (TRUE) and human evaluation (~10%); hallucination on TTS without full context.

**First experiments**: 1) Train Tracker T and measure per-aspect precision/recall to identify low-performing aspects; 2) Evaluate Summarizer S with and without cited sentences to measure tracking benefit; 3) Compare TTS vs TTS⊕f. outputs to diagnose context impact on completeness vs precision.

## Open Questions the Paper Calls Out

**Open Question 1**: Does LLM-assisted dataset generation introduce residual biases that persist even after human annotation and revision? The authors note that using Mistral Large for the initial dataset draft "may also introduce model-specific biases" despite mitigation strategies like manual revision. It is unclear if human revision eliminates only factual errors while retaining the LLM's specific phrasing preferences or citation patterns, potentially skewing the benchmark's evaluation of other models.

**Open Question 2**: How does the Track-Then-Sum pipeline perform when applied to full-text clinical trial reports instead of just abstracts? The study is constrained to "abstracts" to ensure public accessibility, but the authors acknowledge that clinical decision-making relies heavily on full-text articles. The sentence-level tracking mechanism may face challenges with document length, context window limits, or dispersed evidence in full texts that are not present in concise abstracts.

**Open Question 3**: Can open-source entailment evaluators be optimized to match or exceed the human alignment of proprietary models like GPT-4o on this specific task? The authors find GPT-4o shows "substantially stronger alignment with human judgments" (ρ = 0.80) compared to the TRUE model or Mistral-Large. The superior performance of proprietary models suggests domain-specific reasoning capabilities that current open-source evaluators lack, creating a reliance on closed-source systems for accurate benchmarking.

## Limitations
- Dataset accessibility: The 3.5K annotated summary-citation pairs are not publicly available, requiring author contact or full re-annotation to reproduce
- Narrow domain focus: Limited to melanoma clinical trials, which may not generalize to other medical specialties or abstract types
- Evaluation framework dependency: Relies on external models (Mistral Large, TRUE) with underspecified prompt formats and integration details

## Confidence
- **High Confidence**: TRACK-THEN-SUM pipeline architecture and QLoRA fine-tuning procedure are clearly specified and reproducible
- **Medium Confidence**: Evaluation methodology using CLR/CIR/CLP/CIP metrics is well-defined, but decomposition prompts and TRUE model integration are underspecified
- **Low Confidence**: Generalization claims to other medical domains are speculative given the narrow melanoma clinical trial focus

## Next Checks
1. **Dataset Verification**: Request or reconstruct the TracSum dataset by sampling 500 melanoma RCT abstracts from PubMed using the specified filters and annotate with 2 annotators per instance following the Table 1 aspects and Appendix A guidelines to verify the 3.5K summary-citation pair count and distribution.

2. **Evaluation Pipeline Validation**: Implement the full evaluation pipeline using Mistral Large for summary decomposition and TRUE for entailment checking, then validate on a subset of summaries using GPT-4o as entailment evaluator to measure the human vs. automatic evaluation gap (ρ should improve from 0.61 to 0.80 as reported).

3. **Tracker Threshold Analysis**: Experiment with tracker T thresholds beyond 0.5 (e.g., 0.3, 0.7) and aspect-specific positive weights during training to diagnose the reported failure mode where aspects O (Outcomes) and I (Intervention) show lower citation recall despite containing more relevant sentences per abstract.