---
ver: rpa2
title: 'PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases'
arxiv_id: '2509.25238'
source_url: https://arxiv.org/abs/2509.25238
tags:
- gid00001
- recovery
- gid00068
- paladin
- gid00083
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PALADIN is a training framework that equips language agents with
  execution-level robustness by combining systematic failure injection with taxonomy-guided
  recovery supervision. The method trains on 50,000+ recovery-annotated trajectories
  constructed from an enhanced ToolBench dataset, then uses retrieval-based exemplar
  matching to guide recovery during inference.
---

# PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases

## Quick Facts
- **arXiv ID**: 2509.25238
- **Source URL**: https://arxiv.org/abs/2509.25238
- **Reference count**: 40
- **Key outcome**: PALADIN achieves 89.68% Recovery Rate (+57% relative), 78.38% Task Success Rate (+5.55% relative), and 82.55% Catastrophic Success Rate (+9.25% relative) over baseline ToolBench agents.

## Executive Summary
PALADIN addresses the critical weakness of tool-augmented LLM agents—failure to recover from runtime tool errors like timeouts, malformed outputs, and API failures. Rather than hallucinating success or deadlocking, PALADIN trains agents to diagnose, retry, and replan when tools fail. The system combines systematic failure injection with taxonomy-guided recovery supervision, achieving dramatic robustness gains while maintaining base tool competence through LoRA-based fine-tuning.

## Method Summary
PALADIN uses LoRA fine-tuning on a base LLM to inject recovery competence while preserving existing tool-use capabilities. The training corpus contains 50,000+ recovery-annotated trajectories constructed via systematic failure injection into ToolBench datasets. Recovery strategies are generated by GPT-5 Teacher for truncated traces at failure points. At inference, runtime errors trigger retrieval-based exemplar matching to curated recovery actions aligned with ToolScan's taxonomy, guiding the agent through diagnosis→retry→replan sequences.

## Key Results
- Achieves 89.68% Recovery Rate (+57% relative) compared to baseline ToolBench agents
- Maintains 78.38% Task Success Rate (+5.55% relative) while reducing catastrophic failures
- Outperforms strongest baseline CRITIC by +13.3% across all metrics
- Generalizes to unseen tool APIs with 95.2% recovery performance retention

## Why This Works (Mechanism)

### Mechanism 1: Failure-Injected Training with Recovery Supervision
Systematic exposure to failure trajectories paired with expert-annotated recovery actions teaches agents transferable error-handling behaviors. Training trajectories are constructed by injecting controlled failures (timeouts, malformed outputs, API errors) at specific execution points, truncating traces at failure, and using GPT-5 Teacher to generate multi-turn `Recovery:` segments with explicit corrective actions. The training objective $L_{PALADIN} = L_{SFT} + \lambda L_{rec}$ reinforces tokens following recovery tags, creating supervised signal for diagnosis→retry→replan sequences.

### Mechanism 2: Taxonomy-Aligned Retrieval at Inference
Matching runtime failures to curated exemplar recovery strategies provides inference-time guidance that amplifies robustness beyond training distribution. When error $e_t$ is detected, PALADIN retrieves the most similar exemplar $f_{ref} = \arg\min_{f_i \in E} d(f_{obs}, f_i)$ from a bank of 55+ failure-recovery pairs aligned to ToolScan taxonomy. The corresponding recovery action $r_i$ guides trajectory correction, bridging training-to-deployment gaps by grounding novel errors in seen patterns.

### Mechanism 3: LoRA-Based Recovery Injection While Preserving Base Competence
Low-rank adaptation enables recovery skill acquisition without catastrophic forgetting of pre-trained tool-use capabilities. LoRA adapters (rank 16, α=32) inject recovery-specific weights into attention and MLP projections while freezing base model. Training corpus is 80% failure-rich, 20% clean trajectories to maintain baseline competence. This dual-objective approach yields recovery gains without sacrificing task success on non-failure cases.

## Foundational Learning

- **Tool-augmented agent trajectories**: Why needed? PALADIN operates on sequences of (state, action) pairs where actions include tool calls; understanding failure propagation requires trajectory-level reasoning, not single-call fixes. Quick check: Can you explain why a single malformed JSON response might cascade into task abandonment in a multi-turn agent?

- **Causal language modeling with supervised fine-tuning (SFT)**: Why needed? PALADIN's training objective extends standard next-token prediction with recovery-specific loss; understanding how $L_{rec}$ weights corrective tokens is essential. Quick check: How does adding $\lambda L_{rec}$ for tokens after `Recovery:` tags change what the model learns versus vanilla SFT?

- **Nearest-neighbor retrieval for exemplar-based reasoning**: Why needed? Inference-time recovery depends on matching error signatures to a curated bank; understanding similarity metrics and retrieval latency tradeoffs is practical knowledge. Quick check: What failure modes might arise if your similarity metric matches on surface features (error codes) rather than semantic error class?

## Architecture Onboarding

- **Component map**: ToolBench -> Trace Parser -> Failure Injector -> GPT-5 Teacher (recovery annotation) -> Serialized trajectories -> LoRA fine-tuning -> Error detector -> Retrieval over 55+ exemplar bank -> Recovery action execution -> Resume trajectory -> PaladinEval simulator -> Deterministic failure injection -> GPT-5 grader -> TSR, RR, CSR, ES metrics

- **Critical path**: 
  1. Construct recovery dictionary (55+ exemplars aligned to ToolScan taxonomy)
  2. Generate 50K annotated trajectories (inject failures → truncate → Teacher repair)
  3. Fine-tune with LoRA (rank 16, α=32) on mixed 80/20 failure/clean corpus
  4. Deploy with inference-time retrieval matching observed errors to exemplars

- **Design tradeoffs**: 
  - Recovery vs. efficiency: Retry-heavy strategies improve CSR/RR but reduce ES (r = -0.72 correlation); PALADIN trades ~1 additional step for 3× recovery improvement
  - Training diversity vs. domain specificity: 50K trajectories span diverse tools but may underfit niche APIs; retrieval mitigates but does not eliminate this gap
  - LoRA rank vs. capacity: Rank 16 balances parameter efficiency with recovery pattern expressiveness; higher rank may improve complex recovery but risks overfitting

- **Failure signatures**: 
  - Hallucinated success: Agent reports task completion despite tool failure (CSR captures this; PALADIN reduces from ~70% to ~20%)
  - Deadlock loops: Repeated retry of identical failed action; recovery annotation teaches retry→tool-switch→terminate escalation
  - Retrieval mismatch: Similarity metric returns irrelevant exemplar; manifests as inappropriate recovery action (e.g., authentication retry for timeout error)

- **First 3 experiments**: 
  1. Baseline comparison: Run vanilla ToolBench agent and CRITIC on PaladinEval with identical failure injection; measure RR gap to quantify recovery-only gains
  2. Retrieval ablation: Disable exemplar matching at inference; compare RR/TSR/CSR drops (expect 20-30 point degradation per paper)
  3. Out-of-domain generalization: Test on held-out tool APIs not in training set; verify 95%+ recovery rate retention (paper claims 95.2%)

## Open Questions the Paper Calls Out
- **Adaptive controllers for retry intensity**: Future work should explore adaptive controllers that modulate retry intensity based on task difficulty or model confidence. Unresolved because PALADIN currently applies uniform retry strategies regardless of failure context or task complexity.

- **Integration with production error logs**: Future work should explore integration with error logs from production environments. Unresolved because all training and evaluation uses simulated failure injection aligned to ToolScan's seven error classes, while real-world deployment may encounter failure modes outside this taxonomy.

- **GPT-5 bias in annotation and grading**: The paper uses GPT-5 API as both the teacher generating recovery annotations and the automated grader computing metrics, without analyzing potential alignment between generation and evaluation biases. Unresolved because if GPT-5's recovery preferences and grading criteria share systematic patterns, reported gains may partially reflect evaluation alignment rather than genuine robustness improvements.

## Limitations
- Reliance on GPT-5 for both recovery annotation and automated grading may introduce circular bias in reported improvements.
- The curated bank of 55+ failure-recovery exemplars lacks specified selection criteria and coverage of real-world failure diversity.
- The 95.2% generalization claim assumes the retrieval mechanism's similarity metric effectively maps novel errors to known recovery strategies, which requires validation in production settings.

## Confidence
- **High confidence**: The relative improvements over baselines (CRITIC +13.3%, +57% RR, +5.55% TSR) are robust within the controlled PaladinEval environment, given the systematic failure injection and deterministic evaluation framework.
- **Medium confidence**: Generalization to unseen tool APIs (95.2% recovery rate) assumes the retrieval mechanism's similarity metric effectively maps novel errors to known recovery strategies—this requires validation in production settings with adversarial or distribution-shifted failures.
- **Low confidence**: The paper does not specify λ in the training objective $L_{PALADIN} = L_{SFT} + \lambda L_{rec}$, nor does it justify the choice of rank 16 for LoRA adapters. These hyperparameters could significantly impact the balance between recovery competence and base tool proficiency.

## Next Checks
1. **Retrieval robustness under semantic drift**: Test PALADIN on errors that are semantically similar but syntactically distinct from the exemplar bank (e.g., "connection timeout" vs. "server not responding"). Measure whether the similarity metric $d$ captures functional equivalence or fails due to surface-level mismatches.

2. **Out-of-domain API stress test**: Deploy PALADIN on a held-out tool API suite with novel failure modes (e.g., authentication revocation, quota exhaustion) not covered by ToolScan's 7 error classes. Track RR/TSR/CSR degradation to quantify taxonomy-recall limitations.

3. **Hyperparameter sensitivity analysis**: Sweep λ (e.g., 0.1, 1.0, 10.0) and LoRA rank (8, 16, 32) to identify the Pareto frontier between recovery gains and base competence retention. Monitor ES to ensure efficiency is not sacrificed for robustness.