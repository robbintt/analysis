---
ver: rpa2
title: 'DermETAS-SNA LLM: A Dermatology Focused Evolutionary Transformer Architecture
  Search with StackNet Augmented LLM Assistant'
arxiv_id: '2512.08998'
source_url: https://arxiv.org/abs/2512.08998
tags:
- architecture
- assistant
- fold
- skin
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DermETAS-SNA LLM, a dermatology-focused AI
  assistant that integrates evolutionary transformer architecture search with a stackNet
  ensemble and retrieval-augmented generation. The system addresses limitations in
  existing dermatological AI by optimizing Vision Transformer architectures through
  evolutionary search on the SKINCON dataset and fine-tuning class-specific binary
  classifiers for 23 skin disease categories.
---

# DermETAS-SNA LLM: A Dermatology Focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM Assistant

## Quick Facts
- arXiv ID: 2512.08998
- Source URL: https://arxiv.org/abs/2512.08998
- Reference count: 32
- Primary result: 16.06% F1 improvement over baseline (56.30% vs 48.51%)

## Executive Summary
DermETAS-SNA LLM integrates evolutionary transformer architecture search with a stackNet ensemble and retrieval-augmented generation to create a dermatology-focused AI assistant. The system optimizes Vision Transformer architectures through evolutionary search on the SKINCON dataset, then fine-tunes class-specific binary classifiers for 23 skin disease categories. A stackNet architecture combines multiple binary classifiers with deep features and probability statistics, while a DERM-RAG pipeline uses Google Gemini 2.5 Pro to generate clinically grounded diagnostic explanations. Domain expert assessment by eight licensed medical doctors showed a 92% agreement rate with the system's clinical responses.

## Method Summary
The system employs ETAS to optimize ViT architectures on SKINCON dataset, then fine-tunes 23 binary classifiers per disease category using both Full Unfreezing and Gradual Unfreezing strategies. A stackNet meta-classifier combines probability outputs, multi-scale ResNet-50 features, and probability statistics using a 1D CNN with Focal Loss. The DERM-RAG pipeline retrieves from a medical textbook corpus using Qwen2-1.5B embeddings, QdrantDB vector storage, and Cohere re-ranking before generating explanations with Gemini 2.5 Pro.

## Key Results
- Achieved F1-score of 56.30% across 23 disease categories, a 16.06% improvement over SkinGPT-4
- Domain expert assessment showed 92% agreement rate with clinical responses
- Binary classifier approach outperformed single multi-class model in handling class imbalance
- DERM-RAG pipeline reduced hallucination compared to non-retrieval LLM responses

## Why This Works (Mechanism)

### Mechanism 1
Evolutionary search discovers ViT architectures better-suited to dermatological feature extraction than fixed architectures. Genetic algorithm iteratively refines ViT hyperparameters using crossover, mutation, and selection with fitness based on average F1 across 5-fold CV on SKINCON. The search space includes 6-12 layers, 8/16 attention heads, MLP dimensions of 2048/3072/4096, and dropout rates of 0.1-0.3.

### Mechanism 2
Class-specific binary classifiers followed by a meta-classifier reduce confusion from class imbalance and inter-class similarity. Each binary classifier learns to distinguish one disease vs. all others using balanced one-vs-all datasets. The meta-classifier combines probability vectors, multi-scale deep features from ResNet-50, and probability statistics using a 1D CNN trained with Focal Loss.

### Mechanism 3
RAG with domain-curated medical corpus grounds LLM responses, reducing hallucination and improving clinical relevance. Medical textbooks are chunked, embedded, and stored in vector database. At inference, predicted diagnosis and detected features construct queries for retrieval, re-ranking, and synthesis by Gemini 2.5 Pro.

## Foundational Learning

- **Vision Transformer mechanics and patch-based attention**: Essential for understanding ETAS search space and fitness signals. Can you explain how a 16×16 patch embedding with 768-dim representation feeds into a 12-layer transformer with 16 attention heads?
- **Genetic Algorithm operators and fitness landscapes**: Critical for understanding ETAS loop mechanics. Given population size 5 and mutation probability 0.2, what is the expected number of mutated individuals per generation?
- **Ensemble learning and stacking meta-classifiers**: Required to understand StackNet's two-stage design. Why might concatenating probability vectors, deep features, and statistics provide complementary signals to a meta-classifier?

## Architecture Onboarding

- **Component map**: Input Image → ETAS-optimized ViT backbone → 23 Binary Classifier heads → ResNet-50 → Multi-scale features → Feature fusion F(x) = P ⊕ D_multi ⊕ S → Meta-classifier (1D CNN + Focal Loss) → Predicted class + features → Query construction → DERM-RAG: Query → Qwen2-1.5B embedding → QdrantDB retrieval → Cohere rerank → Gemini 2.5 Pro → Response

- **Critical path**: ETAS fitness evaluation dominates compute; binary classifier fine-tuning per class requires 23× training; meta-classifier training on fused features; RAG retrieval latency at inference

- **Design tradeoffs**: Population size vs. search thoroughness; binary vs. multi-class classification efficiency; RAG grounding vs. latency overhead

- **Failure signatures**: ETAS convergence failure (fitness plateaus); binary classifier collapse (50% accuracy); meta-classifier overfitting (high train/low val); RAG hallucination despite retrieval

- **First 3 experiments**: 1) Ablate ETAS: Use fixed ViT and compare binary classifier F1 distributions; 2) Meta-classifier feature ablation: Train with subsets of F(x) to measure incremental gains; 3) RAG retrieval quality audit: Manually label top-5 retrieved chunks for relevance and correlate with expert agreement

## Open Questions the Paper Calls Out

- How does DermETAS-SNA generalize to independent dermatology datasets such as HAM10000 and ISIC across diverse skin tones and imaging conditions? The current evaluation uses only stratified splits of DermNet; the ETAS-optimized ViT and StackNet ensemble have not been validated on external datasets.

- Can expanding the RAG knowledge base to include MedlinePlus, UMLS, and UpToDate significantly reduce hallucination rates and improve clinical accuracy? Current DERM-RAG uses five textbooks; domain expert evaluation shows 6.0% neutral and 2.1% disagreement rates, suggesting room for improvement in coverage.

- What architectural or data augmentation strategies can improve classification performance for visually similar conditions like Psoriasis (44.52% F1) and Warts (55.74% F1)? The binary classifier strategy does not explicitly model inter-class visual similarity, and no targeted data augmentation was explored for these underperforming categories.

## Limitations
- Reported F1 improvement lacks statistical significance testing across multiple random seeds
- ETAS population size (5) and generation count (20) may be insufficient for global search space exploration
- Binary classifier approach requires 23× training time and may produce correlated errors the meta-classifier cannot resolve

## Confidence
- **High confidence**: ETAS framework implementation and general StackNet architecture design
- **Medium confidence**: Reported performance metrics and clinical expert agreement rates
- **Low confidence**: Causal attribution that ETAS specifically caused the F1 improvement rather than other design choices

## Next Checks
1. Run ETAS with larger populations (10-20) and additional generations (30-50) to verify fitness convergence and architecture stability
2. Perform ablation studies comparing binary classifiers with direct 23-class multi-label classification using identical ViT backbone
3. Conduct systematic RAG retrieval quality assessment with human relevance judgments on held-out queries to establish correlation between retrieval precision and expert agreement