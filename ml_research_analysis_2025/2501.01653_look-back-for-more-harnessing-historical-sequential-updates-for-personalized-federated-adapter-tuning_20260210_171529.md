---
ver: rpa2
title: 'Look Back for More: Harnessing Historical Sequential Updates for Personalized
  Federated Adapter Tuning'
arxiv_id: '2501.01653'
source_url: https://arxiv.org/abs/2501.01653
tags:
- clients
- learning
- personalized
- adapter
- pfedseq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes pFedSeq, a personalized federated learning
  framework that leverages past sequential adapter updates from clients to generate
  more robust personalized adapters for large foundation models. The key innovation
  is using a sequential learner (based on Selective State Space Models) to process
  sequences of past adapter updates from all clients, capturing cross-client and cross-step
  relationships to produce better personalized calibrations.
---

# Look Back for More: Harnessing Historical Sequential Updates for Personalized Federated Adapter Tuning

## Quick Facts
- arXiv ID: 2501.01653
- Source URL: https://arxiv.org/abs/2501.01653
- Reference count: 15
- Primary result: pFedSeq outperforms 10 SOTA personalized FL methods by up to 5.39% accuracy on heterogeneous datasets

## Executive Summary
This paper introduces pFedSeq, a personalized federated learning framework that leverages historical adapter update sequences from clients to generate more robust personalized models. The key innovation is using a Sequential Learner (based on Selective State Space Models) to process sequences of past adapter updates, capturing cross-client and cross-step relationships to produce better personalized calibrations. Experiments on four benchmark datasets show pFedSeq achieves up to 5.39% higher accuracy than state-of-the-art methods, particularly excelling on datasets with domain discrepancies.

## Method Summary
pFedSeq operates by having clients train LoRA adapters on their local data while the server maintains a history of all adapter updates. The server trains a Sequential Learner (a 2-layer Mamba model) to process sequences of past updates from all clients and generate personalized calibrations. During warm-up rounds, only global adapters are used. After warm-up, clients receive personalized adapters computed as the sum of the global adapter and calibration from the Sequential Learner. The server trains the Sequential Learner using received updates as gradient proxies, approximating the direction of clients' local loss gradients without accessing raw data.

## Key Results
- pFedSeq achieves up to 5.39% higher accuracy than 10 state-of-the-art personalized FL methods
- Performance gains are particularly significant on datasets with domain discrepancies (DomainNet, Omniglot)
- The framework shows consistent improvements across all four benchmark datasets (CIFAR-100, Tiny-ImageNet, DomainNet, Omniglot)
- Ablation studies confirm the importance of both historical information (sequence length L) and the Selective SSM architecture

## Why This Works (Mechanism)

### Mechanism 1: Momentum-like Smoothing via Historical Trajectories
Utilizing a sequence of past updates (L > 1) produces more robust personalized adapters than relying solely on the latest round, acting as a stability mechanism similar to momentum in optimization. The Sequential Learner processes a window of historical updates (Δ_{t-L+1:t}), filtering out noisy or transient deviations and identifying consistent trends in the optimization path.

### Mechanism 2: Selective Cross-Client Relation Modeling
The Selective State Space Model (SSM) architecture allows the server to dynamically select which cross-client interactions are relevant for personalization at each step. Unlike standard RNNs, the Selective SSM makes its parameters dependent on the input, allowing it to effectively "attend" to relations where Client j's history informs Client i's current calibration while ignoring irrelevant or conflicting cross-client signals.

### Mechanism 3: Gradient Proxy for Server-Side Optimization
The server can train the Sequential Learner without accessing client data by treating received adapter updates (Δ) as proxies for the gradients of clients' local losses. The server optimizes the Sequential Learner parameters using the update rule (Δψ)_t = Σ(∇_ψ ξ_{t-1})^T Δ_t, aligning its generation of personalized calibrations with the direction that minimizes clients' local objectives.

## Foundational Learning

- **Concept: Adapter Tuning (LoRA)**
  - Why needed here: The paper operates on "Federated Adapter Tuning" where the heavy backbone (ViT) is frozen. You must understand that communication and computation are restricted to the small LoRA matrices (A, B), not the full model.
  - Quick check question: Can you explain why the update dimension D is treated as the "batch dimension" in the Sequential Learner input?

- **Concept: State Space Models (SSMs) vs. Transformers/RNNs**
  - Why needed here: The core architecture is a Selective SSM (Mamba). You need to understand that this is a recurrent model (efficient inference) but trained with parallel scan (efficient training), and crucially, its dynamics are *input-dependent* (selective).
  - Quick check question: How does the "selection mechanism" in Mamba differ from a standard LSTM cell regarding how it handles input sequences?

- **Concept: Personalized Federated Learning (PFL)**
  - Why needed here: You must distinguish between learning a single global model (FedAvg) vs. generating distinct models for each client. pFedSeq is a "Hypernetwork-based" approach to PFL.
  - Quick check question: In pFedSeq, is the goal to output a single aggregated model or a set of calibrations ξ_i tailored to each client i?

## Architecture Onboarding

- **Component map:** Local Clients (frozen Backbone + LoRA Adapter + Head) -> Server Buffer (history of updates Δ) -> Sequential Learner (2-layer Mamba) -> Aggregator (FedAvg) -> Personalized Adapter (θ_i = θ_global + ξ_i)

- **Critical path:**
  1. Warm-up (Rounds 1...W): Server runs standard FedAvg, updates Sequential Learner using incoming Δ but discards output calibrations ξ, sending only θ_g to clients
  2. Inference (Round >W): Server inputs buffered update sequence Δ_{t-L+1:t} into trained Learner to get ξ, final personalized model θ_i = θ_g + ξ_i
  3. Optimization: Server updates Learner weights ψ using just-received Δ_t as gradient proxy for previous output ξ_{t-1}

- **Design tradeoffs:**
  - Sequence Length (L): Longer L improves performance (up to a plateau) but increases memory linearly (activations). Paper found L=10 to 20 to be the sweet spot
  - Warm-up (W): Essential for stability. Skipping warm-up causes abrupt drop as untrained learner outputs random calibrations
  - Batching Strategy: Parameters (D) are treated as the batch dimension. If D is huge, you may need sub-batching, though LoRA keeps D small

- **Failure signatures:**
  - Instability after Warm-up: Learning curves drop immediately after round W. Diagnosis: Learner learning rate too high or warm-up W insufficient
  - Stagnation: Performance matches FedAvg. Diagnosis: Sequence length L might be set to 1 (accidentally disabling history) or calibration magnitude ξ is vanishing
  - OOM (Out of Memory): SSM states or input buffer grows too large. Diagnosis: Check expanded state dimension M or sequence length L scaling

- **First 3 experiments:**
  1. Sanity Check (Ablation L): Run pFedSeq with L=1 vs L=10 on CIFAR-100. Verify that removing history degrades accuracy to confirm the core hypothesis
  2. Architecture Validation: Swap the Mamba learner for an MLP (Variant D) or LSTM (Variant E). Expect lower accuracy or training instability to validate the choice of Selective SSM
  3. Hyperparameter Sensitivity: Vary the warm-up rounds W ∈ {5, 10, 20} on DomainNet. Verify that performance stabilizes as W increases, confirming the need for server-side pre-training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can pFedSeq effectively generalize to federated fine-tuning of Large Language Models (LLMs) given the differences in adapter dynamics between NLP and vision tasks?
- Basis: The paper exclusively evaluates the framework on computer vision benchmarks (CIFAR-100, Tiny-ImageNet, DomainNet, Omniglot) using a ViT backbone, despite LoRA and adapter tuning originating in the NLP domain
- Why unresolved: The sequential dependencies in text data and the behavior of adapters in LLMs may differ significantly from the image classification setting tested
- Evidence: Performance benchmarks on standard federated NLP datasets (e.g., Shakespeare, StackOverflow) using LLM backbones

### Open Question 2
- Question: How does pFedSeq scale in terms of server-side memory and latency when deployed in cross-device settings with thousands of clients?
- Basis: The experimental setup is limited to a small number of clients (N ∈ {6, 10, 20}), and the complexity analysis notes that memory consumption increases linearly with the number of clients N
- Why unresolved: Processing the sequence input Δ ∈ ℝ^{D × N × L} in a single batch may become computationally prohibitive for the server as N grows large
- Evidence: System profiling of server throughput and memory usage in simulations with N > 100 clients

### Open Question 3
- Question: How does the accuracy of the "update-as-gradient-proxy" approximation degrade as the number of local training epochs increases?
- Basis: The method approximates the local gradient ∇L using the adapter update Δ, which the authors state is "equivalent to replacing a single gradient step," implying a simplification of the client's local optimization trajectory
- Why unresolved: If clients perform many local steps (large E), the accumulated update may diverge significantly from the instantaneous gradient direction, potentially introducing noise into the sequential learner's training
- Evidence: Sensitivity analysis of model convergence and accuracy when varying local epochs E

## Limitations
- Empirical gains are benchmarked on synthetic heterogeneous data splits rather than real-world federated datasets with naturally occurring client drift
- The paper does not report memory or computational overhead of the Sequential Learner relative to the standard FedAvg baseline
- LoRA configuration (rank, alpha scaling, targeted layers) is not explicitly specified, creating ambiguity for exact reproduction

## Confidence
- **High confidence**: The mechanism that historical trajectories provide momentum-like smoothing for personalization stability, supported by ablation showing performance drops when L=1
- **Medium confidence**: The claim that Selective SSM provides superior cross-client relation modeling compared to LSTM/MLP, as the evidence relies on controlled experiments but lacks real-world federated learning benchmarks
- **Low confidence**: The scalability claims for production deployment, as computational overhead and memory usage are not reported

## Next Checks
1. **Domain Drift Test**: Implement a synthetic federated learning scenario with concept drift (gradually changing client data distributions) to verify pFedSeq maintains performance when historical updates become outdated, testing the "momentum-like smoothing" mechanism's break condition
2. **Resource Overhead Benchmark**: Measure and compare GPU memory usage and training time per round between pFedSeq and FedAvg across different client counts to quantify the practical deployment costs of the Sequential Learner
3. **Real-World Dataset Validation**: Apply pFedSeq to a federated learning dataset with naturally occurring heterogeneity (e.g., LEAF or FedScale benchmarks) rather than synthetic Dirichlet splits to assess generalization beyond controlled experimental conditions