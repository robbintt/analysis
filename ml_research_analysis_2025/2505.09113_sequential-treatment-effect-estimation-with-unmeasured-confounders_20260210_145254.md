---
ver: rpa2
title: Sequential Treatment Effect Estimation with Unmeasured Confounders
arxiv_id: '2505.09113'
source_url: https://arxiv.org/abs/2505.09113
tags:
- treatment
- time
- confounders
- sequential
- unmeasured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of estimating cumulative treatment
  effects in sequential decision-making scenarios where unmeasured confounders influence
  both treatments and outcomes. The authors propose a novel Decomposing Sequential
  Instrumental Variable framework for Counterfactual Regression (DSIV-CFR) that leverages
  negative control assumptions to recover latent instrumental variables from observed
  covariates.
---

# Sequential Treatment Effect Estimation with Unmeasured Confounders

## Quick Facts
- **arXiv ID:** 2505.09113
- **Source URL:** https://arxiv.org/abs/2505.09113
- **Reference count:** 32
- **One-line primary result:** DSIV-CFR achieves significantly lower MSE in one-step-ahead outcome prediction and approaches oracle performance in multi-step sequential treatment decision making under unmeasured confounding.

## Executive Summary
This paper addresses the challenge of estimating cumulative treatment effects in sequential decision-making scenarios where unmeasured confounders influence both treatments and outcomes. The authors propose a novel Decomposing Sequential Instrumental Variable framework for Counterfactual Regression (DSIV-CFR) that leverages negative control assumptions to recover latent instrumental variables from observed covariates. The method employs transformers to model long sequential dependencies and learns representations of instrumental variables and confounders through mutual information constraints. Experimental results demonstrate DSIV-CFR's superior performance across four datasets, achieving significantly lower mean squared error in one-step-ahead outcome prediction compared to state-of-the-art baselines.

## Method Summary
DSIV-CFR is a two-module architecture that estimates counterfactual outcomes in sequential decision-making with unmeasured confounders. The framework treats instrument variables as special negative control exposures and prior outcomes as negative control outcomes, enabling IV recovery through conditional independence. A transformer backbone encodes sequential history into embeddings, which are then disentangled into instrumental variable and confounder representations using mutual information constraints. A generalized method of moments (GMM) framework with adversarial weighting estimates counterfactual outcomes while mitigating bias from unmeasured confounders. The method alternates between training the counterfactual regressor, variational parameters for MI estimation, and the bridge function for GMM weights.

## Key Results
- DSIV-CFR achieves significantly lower mean squared error in one-step-ahead outcome prediction compared to state-of-the-art baselines across synthetic, tumor growth, cryptocurrency, and MIMIC-III datasets
- In multi-step sequential treatment decision making, DSIV-CFR approaches oracle outcomes and outperforms competing methods
- Hyperparameter analysis confirms the effectiveness of both mutual information constraints for IV decomposition and adversarial function learning in the GMM framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recovering latent instrumental variables from observed covariates enables unbiased treatment effect estimation under unmeasured confounding.
- Mechanism: The framework treats instrument variables (IVs) as special negative control exposures (NCE) and prior outcomes as negative control outcomes (NCO). Under Assumption 3.7, this allows identification of Z from X through conditional independence: Z⊥⊥Y | {A, Ā, C, Y_prev} (Theorem 3.8). The IV affects treatment but remains independent of unmeasured confounders U.
- Core assumption: Negative control assumption (3.7) requires proportional effects of U on NCE/A and NCO/Y, plus the additive noise model (3.4) where E[U|X]=0.
- Evidence anchors: [abstract] "an instrumental variable (IV) is a special negative control exposure, while the previous outcome serves as a negative control outcome"; [section 3] Theorem 3.8 proof shows how Y_prev acts as proxy for U, enabling IV identification without observing U
- Break condition: If U effects on Y_prev and Y are not proportional, or if E[U|X]≠0, identification fails.

### Mechanism 2
- Claim: Mutual information constraints can disentangle IV representations from confounder representations in the latent space.
- Mechanism: Contrastive log-ratio upper bounds (CLUB) enforce: (1) IV relevance to treatment (L_ZA), (2) IV exclusion from outcome given conditioning set (L_ZY with RBF-kernel weights), (3) confounder influence on both A and Y (L_CA, L_CY), and (4) IV-confounder independence (L_ZC). Equation 15 aggregates these.
- Core assumption: IV and confounder representations are separable; the mapping X→{Z,C} is static (footnote 1) though authors claim dynamic applicability.
- Evidence anchors: [section 4.1] Equations 8-14 define the full MI loss with variational approximations; [section 5.3.2] Hyperparameter analysis (Figure 4) shows α=0.1 (MI weight) improves over α=0 (ablated)
- Break condition: If IV and confounders are not conditionally independent given observations, decomposition yields entangled (invalid) representations.

### Mechanism 3
- Claim: GMM with adversarial weighting provides consistent counterfactual estimation by constructing moment conditions from IV representations.
- Mechanism: The bridge function f learns sample weights M based on {Ā, C, Z}. The adversarial loss L_adv (Eq. 18) maximizes weighted prediction error while the predictor h minimizes it—this min-max game enforces the GMM moment condition. Combined with MSE loss (Eq. 17), this biases the estimator toward satisfying the IV orthogonality constraint.
- Core assumption: The treatment effect function h(·) is time-invariant (Assumption 3.5); sufficient overlap exists (3.2).
- Evidence anchors: [section 4.2] Equation 5 derives the inverse problem form; GMM is the "standard approach"; [section 5.3.2] β=0.1 (adversarial weight) outperforms β=0 (ablated), confirming GMM contribution
- Break condition: If treatment assignment is deterministic given IV (no overlap), the conditional distribution F(A|·) collapses and GMM fails.

## Foundational Learning

- **Instrumental Variables**: Why needed here: Core to recovering causal effects when U exists; must understand relevance, exclusion, and exogeneity conditions. Quick check question: Given a proposed IV, can you state the three standard validity conditions and which one the negative control framework relaxes?
- **Negative Controls**: Why needed here: Enables IV recovery without pre-specified instruments; NCE and NCO serve as U proxies. Quick check question: In a medical time series, what makes a variable a valid NCO but not necessarily an IV?
- **Generalized Method of Moments**: Why needed here: Provides the estimation framework linking IV representations to outcome predictions via moment conditions. Quick check question: How does GMM differ from two-stage least squares when moment conditions are overidentified?

## Architecture Onboarding

- Component map: Input Ĥ → ψ(Ĥ) → {ϕ_Z, ϕ_C} → {L_MI (train θ), L_adv (train f)} → h predicts Ŷ → L_MSE + αL_MI + βL_adv
- Critical path: Input Ĥ → ψ(Ĥ) → {ϕ_Z, ϕ_C} → {L_MI (train θ), L_adv (train f)} → h predicts Ŷ → L_MSE + αL_MI + βL_adv
- Design tradeoffs: Higher α/β improves bias reduction but may conflict with MSE objective (Figure 4 shows degradation at extreme values); Transformer backbone captures long dependencies but increases training time ~2-3x vs LSTM baselines (Table 4); RBF kernel width σ fixed at 1; may need tuning for different data scales
- Failure signatures: MSE not decreasing while L_adv increases: GMM moment condition unsatisfiable (check overlap); L_ZC not converging: IV-confounder decomposition failing (check Assumption 3.4); Validation error much lower than training: Potential data leakage in sequential splits
- First 3 experiments: 1) Ablation with α=0, β=0: Establish baseline without MI/GMM; expect higher MSE than full model; 2) Synthetic data with known U: Compare recovered Z to ground truth IV via correlation; validates decomposition; 3) Short sequence (T<20) test: Transformers may underperform RNNs; identify minimum viable sequence length

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the framework be adapted to provide consistent estimates when unmeasured confounders are serially correlated, violating the assumption that $E[U_t | \bar{X}_{t-1}] = 0$? The Impact Statement identifies this as a limitation: "The limitation of our method lies in the constraints mentioned in Assumption 3.4. If $E[U|X] \neq 0$, the estimate of potential outcome will be biased." The current method relies on the additive noise model where error terms are independent; the authors concede the framework is currently insufficient if this condition is not met.
- **Open Question 2**: How can the multi-step decision-making module be extended to predict future outcomes without requiring pre-observed future covariates for instrument decomposition? Appendix C notes that for multi-step extensions, "covariates for decomposition... must be observable... If we were to predict $X$ on our own... it would not include the IVs." The current implementation for multi-step planning relies on observing future covariates to extract instruments, preventing true long-term forecasting where future contexts are unknown.
- **Open Question 3**: To what extent does the violation of the proportional confounding effect assumption (Assumption 3.7) degrade the performance of the DSIV-CFR estimator? The method relies on Assumption 3.7 (Negative Control), requiring that the effect of unmeasured confounders on negative controls and outcomes is proportional. The paper does not analyze sensitivity to this assumption. While the assumption is necessary for identification, real-world noise often violates strict proportionality, and the model's robustness to such violations is unstated.

## Limitations
- The framework's reliance on the negative control assumption (Assumption 3.7) and additive noise model (Assumption 3.4) may not hold in many real-world scenarios where confounding effects are not proportional
- The assumption that IV representations can be statically mapped from observed covariates may not generalize well to truly dynamic settings where IV relevance evolves over time
- The requirement for observed covariates at all decision points prevents true long-term forecasting where future contexts are unknown

## Confidence
- **High confidence**: The mechanism for IV identification through negative controls (Mechanism 1) and the GMM-based estimation framework (Mechanism 3) are well-established approaches with strong theoretical foundations
- **Medium confidence**: The mutual information-based disentanglement approach (Mechanism 2) shows empirical promise but lacks direct corpus validation and may struggle when IV and confounder representations are inherently entangled
- **Low confidence**: The claim that the method works for "dynamic" sequential decision-making is limited by the static mapping assumption and the requirement for observed covariates at all decision points

## Next Checks
1. **Assumption robustness testing**: Systematically vary the proportionality of confounding effects in synthetic data to quantify performance degradation when negative control assumptions are violated
2. **Alternative IV decomposition**: Compare the mutual information approach against simpler methods like supervised IV classification or factor analysis to isolate the contribution of the specific MI decomposition technique
3. **True dynamic extension**: Modify the framework to handle cases where IV relevance changes over time, potentially through time-varying MI constraints or recurrent representation heads, and evaluate on synthetic data with evolving IV relationships