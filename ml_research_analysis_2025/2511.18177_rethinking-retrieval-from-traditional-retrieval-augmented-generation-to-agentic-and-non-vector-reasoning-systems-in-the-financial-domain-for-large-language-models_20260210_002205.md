---
ver: rpa2
title: 'Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic
  and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models'
arxiv_id: '2511.18177'
source_url: https://arxiv.org/abs/2511.18177
tags:
- retrieval
- latency
- generation
- systems
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic evaluation comparing vector-based
  agentic RAG systems with hybrid search and metadata filtering against hierarchical
  node-based reasoning systems for financial document question answering. The study
  evaluates two enhancement techniques - cross-encoder reranking and small-to-big
  chunk retrieval - applied to the vector-based architecture.
---

# Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models

## Quick Facts
- arXiv ID: 2511.18177
- Source URL: https://arxiv.org/abs/2511.18177
- Reference count: 6
- Primary result: Vector-based agentic RAG achieves 68% win rate over hierarchical systems with comparable latency (5.2 vs 5.98s) on 150-question SEC filings benchmark

## Executive Summary
This paper presents the first systematic evaluation comparing vector-based agentic RAG systems with hierarchical node-based reasoning for financial document question answering. The study evaluates two enhancement techniques - cross-encoder reranking and small-to-big chunk retrieval - applied to the vector-based architecture. Across 1,200 SEC filings (10-K, 10-Q, 8-K) on a 150-question benchmark, vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 vs 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5, improving from 0.160 to 0.750 with perfect Recall@5 (1.00). Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency.

## Method Summary
The study compares two retrieval architectures on 1,200 SEC filings (10-K, 10-Q, 8-K) from Fortune 500 companies (2020-2025), averaging 73,175 tokens per document. The vector-based agentic RAG uses 512-token chunks with 50-token overlap, OpenAI text-embedding-ada-002, Azure AI Search with hybrid semantic+lexical search, metadata filtering, and GPT-4o generation. The hierarchical node-based system preprocesses documents into structured tables of contents with LLM-generated section summaries. Cross-encoder reranking uses Cohere rerank-english-v3.0 with parameter sweeps (k_initial, k_final), while small-to-big retrieval expands target chunks with ±2 neighbors. Answer quality is evaluated using LLM-as-a-judge (Claude 4.5 Sonnet) on 6 criteria and pairwise win rates.

## Key Results
- Vector-based agentic RAG achieves 68% win rate over hierarchical node-based systems (5.2s vs 5.98s latency)
- Cross-encoder reranking improves MRR@5 from 0.160 to 0.750 (59% absolute improvement) with perfect Recall@5 at optimal (10,5) parameters
- Small-to-big retrieval achieves 65% win rate over baseline chunking with only 0.2s additional latency
- Hierarchical node-based system fails to answer 2 questions and provides 2 incorrect responses due to context window limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-encoder reranking improves retrieval precision by jointly encoding query-chunk pairs for fine-grained relevance scoring.
- Mechanism: Initial vector search retrieves k_initial chunks using bi-encoder embeddings (fast but approximate). A cross-encoder then processes each query-chunk pair together, enabling attention between query tokens and chunk tokens. Top k_final chunks are selected based on cross-encoder scores.
- Core assumption: The computational overhead of cross-encoder scoring is justified by improved ranking quality for financial queries.
- Evidence anchors:
  - [abstract] "Cross-encoder reranking improved Mean Reciprocal Rank from 0.160 to 0.750 (59% absolute improvement) with perfect Recall@5 at optimal parameters"
  - [section] Table 4 shows optimal (k_initial=10, k_final=5) achieves MRR@5 of 0.750 vs baseline 0.160, with all reranking settings achieving perfect Recall@5 (1.00) vs baseline 0.50
  - [corpus] Limited corpus validation—neighbor papers discuss RAG enhancements but do not specifically replicate cross-encoder results in financial domains
- Break condition: When k_initial is too large (≥50), latency increases with diminishing MRR gains; when k_final approaches k_initial, reranking benefit diminishes.

### Mechanism 2
- Claim: Small-to-big retrieval improves answer quality by expanding target chunks with neighboring context while preserving retrieval precision.
- Mechanism: Vector search identifies the most relevant chunk. Instead of returning only that chunk, the system retrieves adjacent chunks (idx-2, idx-1, idx+1, idx+2) and concatenates them, providing surrounding context that may span chunk boundaries critical for multi-hop reasoning.
- Core assumption: Neighboring chunks contain relevant contextual information that improves answer generation without introducing excessive distractor content.
- Evidence anchors:
  - [abstract] "Small-to-big retrieval achieved a 65% win rate over baseline chunking with only 0.2 seconds additional latency"
  - [section] Section 4.5 reports 65% win rate, $0.000078 per-query cost, and asynchronous implementation (0.17s) outperforming synchronous (0.34s)
  - [corpus] No direct corpus validation found for small-to-big technique in financial domains
- Break condition: For needle-in-haystack queries where answers are isolated facts, expanded context provides limited benefit and may introduce noise.

### Mechanism 3
- Claim: Vector-based hybrid search outperforms hierarchical traversal because semantic matching is more effective than LLM-based table-of-contents navigation for complex financial queries.
- Mechanism: Hybrid search combines semantic vector similarity with lexical BM25 matching, enabling retrieval based on meaning and exact term overlap. Hierarchical systems require LLMs to select relevant nodes from a structured table-of-contents, which creates a bottleneck when the LLM struggles to map query intent to section titles.
- Core assumption: Financial queries often require specific factual retrieval rather than broad document summarization.
- Evidence anchors:
  - [abstract] "Vector-based agentic RAG system with hybrid search and metadata filtering achieved a 68% win rate over hierarchical node-based systems"
  - [section] Section 5.1 states the node-based system "faced context window limitations, failing to answer 2 questions and providing 2 incorrect responses" while "vector-based system successfully retrieved relevant context for all questions"
  - [corpus] Neighbor paper "Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation" supports agentic RAG efficacy in fintech but does not directly compare with hierarchical approaches
- Break condition: Hierarchical traversal may excel for broad summarization queries; the study's question set emphasized specific factual retrieval.

## Foundational Learning

- Concept: **Mean Reciprocal Rank (MRR)**
  - Why needed here: Primary metric for evaluating retrieval ranking quality; measures how early the first relevant chunk appears in results.
  - Quick check question: If relevant chunks appear at positions 1, 3, and 5 for three queries, what is the MRR?

- Concept: **Cross-encoder vs. Bi-encoder**
  - Why needed here: Explains why reranking works—bi-encoders encode query and document separately (fast, approximate), cross-encoders process them jointly (slow, accurate).
  - Quick check question: Which encoding approach is better for initial retrieval over 1M documents, and which for reranking 10 candidates?

- Concept: **Recall@k**
  - Why needed here: Measures coverage—whether relevant chunks appear anywhere in top-k results, regardless of rank.
  - Quick check question: If 5 relevant chunks exist and 3 appear in top-5 results, what is Recall@5?

## Architecture Onboarding

- Component map:
  - Document Processing Pipeline -> Vector Embedding -> Hybrid Search -> Reranking (optional) -> Context Expansion (optional) -> GPT-4o Generation -> Answer

- Critical path: Query → LLM agent formulates search query → Hybrid retrieval → Reranking (optional) → Context expansion (optional) → GPT-4o generation → Answer

- Design tradeoffs:
  - Reranking (10,5): +1.8s latency for +59% MRR improvement
  - Small-to-big: +0.2s latency for 65% win rate improvement
  - Hierarchical preprocessing: $30.62/10-K vs. vector embedding costs; higher upfront, lower runtime
  - Summary inclusion in nodes: 6.3× preprocessing cost increase

- Failure signatures:
  - Hierarchical node-based: LLM fails to select correct table-of-contents nodes; context window overflow causing failed/incorrect answers
  - Large k_initial (≥50): Latency increases with diminishing MRR returns
  - Needle-in-haystack queries with small-to-big: Expanded context may introduce noise without benefit

- First 3 experiments:
  1. **Baseline replication**: Implement vector-based agentic RAG with hybrid search on a 20-question subset; measure MRR@5, Recall@5, and latency without any enhancements.
  2. **Reranking parameter sweep**: Test (k_initial, k_final) pairs: (10,5), (20,10), (50,10); plot MRR vs. latency to identify optimal operating point for your latency budget.
  3. **Small-to-big window sizing**: Compare 1-neighbor vs. 2-neighbor expansion; evaluate win rate on multi-hop vs. single-hop questions to determine when expansion provides value.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would a hybrid architecture combining vector search with hierarchical reasoning perform, where vector embeddings narrow the initial scope of candidate nodes before LLM traversal?
- Basis in paper: [explicit] "A promising direction for future work would be to combine vector search with hierarchical reasoning by using vector embeddings to narrow the initial scope of candidate nodes before LLM traversal, potentially addressing the table-of-contents selection bottleneck while preserving the structured reasoning benefits of hierarchical approaches."
- Why unresolved: The study only compared vector-based and hierarchical systems independently; no hybrid approach was tested.
- What evidence would resolve it: Head-to-head comparison of pure vector, pure hierarchical, and hybrid architectures on the same benchmark measuring MRR, win rate, latency, and cost.

### Open Question 2
- Question: How do alternative reranking architectures (LLM-based rerankers, domain-specific financial rerankers) compare to Cohere rerank-english-v3.0 in accuracy-latency tradeoffs for financial document retrieval?
- Basis in paper: [explicit] "Our evaluation focused on Cohere rerank-english-v3.0; comparing alternative reranking architectures including LLM-based rerankers and domain-specific financial rerankers would provide additional insights into the accuracy-latency tradeoffs for financial document retrieval."
- Why unresolved: Only one reranker model was evaluated; alternative architectures remain untested in this domain.
- What evidence would resolve it: Systematic comparison of multiple reranker types on financial QA with controlled latency budgets and accuracy metrics.

### Open Question 3
- Question: How do different expansion window sizes in small-to-big retrieval affect answer quality and context distraction during generation?
- Basis in paper: [explicit] "Our implementation expanded each target chunk with its immediate neighbors (one preceding and one following chunk). Future work can explore how different expansion window sizes effect answer quality and context distraction during generation."
- Why unresolved: Only one expansion configuration (±1 chunk) was tested; optimal window size remains unknown.
- What evidence would resolve it: Ablation study varying window sizes (e.g., ±1, ±2, ±3 chunks) measuring answer quality, retrieval latency, and "lost in the middle" effects.

## Limitations
- LLM-as-a-judge reliability: The study relies on Claude 4.5 Sonnet for pairwise win rate evaluation across 6 criteria, but LLM judgments can be inconsistent and may not perfectly align with human preferences.
- Financial domain specificity: Results are demonstrated only on SEC filings (10-K, 10-Q, 8-K) from Fortune 500 companies, limiting generalizability to other financial document types.
- Parameter sensitivity: Optimal (k_initial=10, k_final=5) configuration is based on limited testing and may vary with different query distributions.

## Confidence
- High confidence: Vector-based RAG outperforming hierarchical node-based systems (68% win rate, 5.2s vs 5.98s latency)
- Medium confidence: Cross-encoder reranking achieving 59% MRR improvement (0.160→0.750)
- Medium confidence: Small-to-big retrieval 65% win rate

## Next Checks
1. **Human evaluation validation**: Replicate the win rate results with human judges evaluating 50 random question-answer pairs from the benchmark to verify LLM-as-a-judge alignment and identify any systematic scoring biases.
2. **Cross-domain generalization**: Apply the vector-based RAG architecture with optimal enhancements to a different financial document corpus (e.g., earnings call transcripts or annual reports from non-US markets) to test performance consistency.
3. **Cost sensitivity analysis**: Calculate total operational costs for each architecture under realistic query volumes, including embedding storage, retrieval API calls, and reranking expenses, to identify true production deployment thresholds.