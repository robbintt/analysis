---
ver: rpa2
title: Synthetic Data Generation by Supervised Neural Gas Network for Physiological
  Emotion Recognition Data
arxiv_id: '2501.16353'
source_url: https://arxiv.org/abs/2501.16353
tags:
- data
- synthetic
- emotion
- recognition
- signals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of data scarcity in emotion
  recognition using physiological signals by introducing a Supervised Neural Gas (SNG)
  network for synthetic data generation. The SNG network efficiently organizes input
  data based on topological and feature-space proximity, enabling the generation of
  real-world-like synthetic datasets that preserve intrinsic patterns of physiological
  emotion data.
---

# Synthetic Data Generation by Supervised Neural Gas Network for Physiological Emotion Recognition Data

## Quick Facts
- arXiv ID: 2501.16353
- Source URL: https://arxiv.org/abs/2501.16353
- Reference count: 40
- Primary result: SNG achieves 97.37% accuracy on ESD dataset with efficient synthetic data generation for physiological emotion recognition

## Executive Summary
This study addresses data scarcity in emotion recognition using physiological signals by introducing a Supervised Neural Gas (SNG) network for synthetic data generation. SNG organizes input data based on topological and feature-space proximity, generating real-world-like synthetic datasets that preserve intrinsic patterns of physiological emotion data. Experiments on EEG and ECG/GSR datasets demonstrate SNG outperforms most evaluated models, achieving high accuracy (up to 97.37% on ESD dataset) and significant improvements in processing time.

## Method Summary
The method implements Supervised Neural Gas (SNG) training for 100 iterations with 10 neurons per class, noise_level=0.1, and batch_size=32. The network uses learning rate η(t) and neighborhood range λ(t) that decay over time. After training, synthetic samples are generated by sampling prototype weights and adding Gaussian noise. The approach is evaluated on two datasets: Brainwave EEG (1550 samples, 5 frequency bands, 4 classes) and ESD (253 samples, 14 features, 4 emotion classes) using XGBoost classifier with 70/30 train-test split over 5 runs.

## Key Results
- SNG achieves 97.37% accuracy on ESD dataset, outperforming most baseline models
- Mean squared error between original and synthetic samples: 0.059 (EEG) and 0.101 (ESD)
- Significant improvements in processing time compared to traditional generative models

## Why This Works (Mechanism)

### Mechanism 1: Competitive Topology Fitting with Distance-Ranked Updates
SNG organizes neurons to reflect underlying data topology by updating neurons proportionally to their rank-distance from input samples. For each input vector x, all neurons are ranked by distance, and updates apply larger adjustments to closer neurons via the neighborhood function h_λ(t,k) = e^(-k/λ(t)). This creates a "soft" winner-take-most dynamic where nearby neurons migrate toward input clusters while preserving relative topological relationships. The assumption is that physiological emotion data manifold is locally smooth and can be approximated by finite prototype vectors.

### Mechanism 2: Class-Conditioned Prototype Specialization
Training separate neuron pools per class enables synthetic generation with precise label control without requiring a separate classifier. Weights w_c,i(t) are indexed by both neuron i and class c. During training, only neurons belonging to the input's true class receive updates. At generation time, selecting a class c and sampling from its neurons guarantees label correctness by construction. The assumption is that class boundaries in physiological feature space are sufficiently separable.

### Mechanism 3: Controlled Noise Injection for Sample Diversity
Adding Gaussian noise to learned prototypes generates diverse synthetic samples while maintaining statistical fidelity to original distribution. After training, each neuron weight w_c,i becomes a prototype. Synthetic samples are generated as x_synthetic = w_c,i + N(0, σ²) where σ = noise_level (default 0.1). This injects local variability around each prototype. The assumption is that the true data distribution around each prototype is approximately Gaussian.

## Foundational Learning

- Concept: **Competitive Learning / Vector Quantization**
  - Why needed here: SNG is fundamentally a competitive learning system where neurons "compete" to represent input patterns. Understanding winner selection and neighborhood updates is prerequisite to grasping why SNG organizes topology rather than just minimizing reconstruction error.
  - Quick check question: If you double the number of neurons but keep the same training data, what happens to the average distance between each data point and its nearest neuron?

- Concept: **Neighborhood Functions in Self-Organizing Systems**
  - Why needed here: The λ(t) parameter controls how many neurons around the winner get updated. This is what preserves topology—the "gas" property. Without this, the network collapses to k-means-like behavior.
  - Quick check question: Why does λ(t) decrease over time rather than stay constant?

- Concept: **Prototype-Based Generation vs. Generative Models**
  - Why needed here: Unlike VAEs or GANs that learn explicit density functions or implicit sampling distributions, SNG generates data by perturbing stored prototypes. This is computationally efficient but has different failure modes.
  - Quick check question: What type of data distribution would be impossible for prototype+perturbation to model accurately?

## Architecture Onboarding

- Component map: Input layer -> Competition layer (class-partitioned neurons) -> Training loop -> Generation module
- Critical path: 1) Initialize neurons per class (random or k-means seeding) 2) For each training iteration: present input, compute distances to all neurons in that class, rank neurons, update via Equation 3 with decaying η(t) and λ(t) 3) After training: to generate n synthetic samples for class c, randomly select neurons from class c and apply Equation 7
- Design tradeoffs:
  - More neurons → better coverage of within-class variation, but higher memory/compute and risk of overfitting to noise
  - Higher noise_level → more diverse synthetic samples, but higher MSE and potential label corruption at class boundaries
  - Longer training (more iterations) → better convergence, but paper shows convergence by ~80 iterations for small datasets
- Failure signatures:
  - High MSE with low classification accuracy on synthetic data: noise_level too high or insufficient neurons
  - Low MSE but poor downstream classifier performance: prototypes converged to outliers, not representative samples
  - Class imbalance in synthetic output: training data imbalance propagated (SNG does not rebalance automatically)
  - Training loss plateaus early: λ(t) decays too fast, freezing topology prematurely
- First 3 experiments:
  1. Reproduce the MSE results on Brain Wave EEG (target: ~0.059) with 10 neurons/class, noise_level=0.1, 100 iterations. If MSE > 0.15, debug distance calculations or neighborhood function.
  2. Ablation on neuron count: Run with [5, 10, 20, 50] neurons per class. Plot MSE vs. neuron count and downstream classifier accuracy. Expect diminishing returns past 10-20.
  3. Noise sensitivity test: Vary noise_level from [0.01, 0.05, 0.1, 0.2, 0.5]. Plot MSE and synthetic-only classifier accuracy. Identify the knee point where accuracy degrades despite higher diversity.

## Open Questions the Paper Calls Out
- Can SNG be effectively adapted for synthetic data generation in non-physiological modalities, such as images and body motion data?
- Does integrating SNG with deep generative architectures (e.g., Transformers, C-GANs) enhance the robustness and accuracy of synthetic data generation?
- How does the fixed hyperparameter configuration impact SNG performance when applied to significantly larger or higher-dimensional datasets?

## Limitations
- The Gaussian noise assumption for sample diversity lacks validation in physiological signal synthesis
- Single-prototype-per-neuron approach may struggle with highly multimodal class distributions
- SNG doesn't address class imbalance in synthetic generation, potentially perpetuating training data biases

## Confidence
- **Medium confidence** in core mechanism claims due to lack of direct empirical validation in the corpus
- Medium confidence in experimental results as they are internally consistent
- Low confidence in scalability assumptions for larger datasets

## Next Checks
1. Verify the Gaussian noise assumption by analyzing local distribution characteristics of physiological signals and testing alternative perturbation strategies
2. Conduct ablation studies comparing class-conditioned vs. joint-training approaches for prototype specialization
3. Evaluate SNG's performance on datasets with known class overlap to test robustness at class boundaries