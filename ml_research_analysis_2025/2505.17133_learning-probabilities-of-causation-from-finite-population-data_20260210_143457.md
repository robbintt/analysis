---
ver: rpa2
title: Learning Probabilities of Causation from Finite Population Data
arxiv_id: '2505.17133'
source_url: https://arxiv.org/abs/2505.17133
tags:
- value
- 'true'
- range
- predicted
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of estimating probabilities of
  causation for subpopulations with insufficient data by leveraging machine learning.
  The core method involves training models on subpopulations with known probabilities
  to predict these values for unknown subgroups.
---

# Learning Probabilities of Causation from Finite Population Data

## Quick Facts
- **arXiv ID:** 2505.17133
- **Source URL:** https://arxiv.org/abs/2505.17133
- **Reference count:** 40
- **Primary result:** MLPs with Mish activation achieve MAE ≈ 0.02 for predicting PNS bounds on synthetic data.

## Executive Summary
This paper proposes using machine learning to estimate probabilities of causation (PNS, PN, PS) for subpopulations with insufficient data by leveraging insights from subpopulations with sufficient data. The authors train models on synthetic data generated from four different Structural Causal Models (SCMs) and demonstrate that an MLP with Mish activation consistently outperforms other models like SVM, Random Forest, and GBDT. The approach addresses the fundamental challenge that calculating PNS typically requires thousands of samples per subpopulation, which is often unavailable in real-world scenarios.

## Method Summary
The method treats PNS bound estimation as a supervised regression task, training ML models on subpopulations with sufficient data to predict bounds for those without. The authors generate synthetic data from four SCMs (Confounder, Covariate, Direct, Mediator), compute analytical ground truth bounds using Tian & Pearl's formulas, and filter for subpopulations with >1,300 samples to form a training set of ~2,000 examples. They train MLPs with Mish activation (architecture: 15→64→32→16→1) using MSE loss and Adam optimizer, comparing performance against baseline models.

## Key Results
- MLP with Mish activation achieves the best performance across all SCMs with MAE ≈ 0.02 for most cases
- The Direct SCM is trivially learned (MAE ~0.001), while the Mediator SCM is most challenging (MAE ~0.058)
- Mish outperforms ReLU and LeakyReLU specifically for predicting lower bounds due to better handling of zero-value distributions
- Knowledge transfer occurs across different SCMs, though performance varies with causal complexity

## Why This Works (Mechanism)

### Mechanism 1
Standard ML models can approximate the complex, non-linear mappings between population covariates (Z) and causal bounds (PNS/PN/PS) without explicitly knowing the SCM equations. The authors treat PNS estimation as supervised regression, training models on sufficient subpopulations to learn the functional relationship f(Z) → PNS bound implied by the SCM.

### Mechanism 2
The Mish activation function (x · tanh(ln(1+e^x))) outperforms ReLU and LeakyReLU for predicting PNS lower bounds because it handles the high frequency of zero-values and preserves negative pre-activations better. Unlike ReLU which maps negative inputs to zero, Mish is non-monotonic and smooth, allowing gradients to flow through negative regions more effectively.

### Mechanism 3
Knowledge transfer occurs across different SCMs, allowing models trained on simpler structures to perform well on more complex ones, though with varying success. The MLP captures the consequences of these structures on data distributions, performing excellently on simple "Direct" SCM but struggling with complex "Mediator" SCM involving multiple causal paths.

## Foundational Learning

- **Concept:** Probabilities of Causation (PNS, PN, PS)
  - **Why needed:** These are the targets of prediction, capturing counterfactuals rather than simple correlations. PNS measures the probability that Y would occur if X did, and Y would not occur if X did not.
  - **Quick check:** Can you explain why calculating PNS generally requires both experimental (P(Y|do(X))) and observational (P(Y|X)) data, rather than just one?

- **Concept:** Tian & Pearl's Bounds
  - **Why needed:** PNS is often unidentifiable without strong assumptions, so the paper predicts the lower and upper bounds derived by Tian and Pearl rather than a single point estimate.
  - **Quick check:** If a model predicts a lower bound of 0.1 and an upper bound of 0.9, how should a decision-maker interpret this compared to a prediction of [0.4, 0.5]?

- **Concept:** Structural Causal Models (SCMs) & Data Sparsity
  - **Why needed:** The core problem is data sparsity in subpopulations defined by covariates Z. Understanding this helps explain why direct calculation fails and why ML works by learning across groups.
  - **Quick check:** Why is generating 50 million samples necessary to get only ~2,000 training examples for the "sufficient" subpopulation groups?

## Architecture Onboarding

- **Component map:** Input Layer (15 binary features Z₁...Z₁₅) → MLP Core (15→64→32→16→1) → Mish activation (hidden) → Sigmoid output → Lower/Upper Bound prediction

- **Critical path:**
  1. Data Gen: Define SCM → Generate 50M synthetic samples → Compute "Informer" (Ground Truth) PNS bounds
  2. Filtering: Select subpopulations with >1,300 samples to form "Sufficient" Training Set (N≈2000)
  3. Training: Train MLP (Mish) on sparse training set using MSE or Smooth L1 loss
  4. Inference: Predict PNS bounds for all 32,768 possible subpopulations and validate against "Informer" data

- **Design tradeoffs:**
  - MLP vs. Transformers: Transformers offer no significant gain for this tabular, fixed-length input
  - Synthetic vs. Real: Synthetic data allows ground-truth verification but limits real-world transferability

- **Failure signatures:**
  - High MAE on Mediator SCM: Check if causal structure involves complex mediation paths
  - Negative Lower Bounds: If using ReLU, lower bounds might suffer from gradient loss

- **First 3 experiments:**
  1. Activation Ablation: Train MLP using ReLU, LeakyReLU, and Mish on "Confounder" dataset
  2. Data Scarcity Stress Test: Reduce training subpopulation count from 2,000 to 500
  3. Generalization Check: Train on "Confounder" SCM data and test on "Outcome-Covariate" SCM data

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the pipeline maintain high accuracy on real-world datasets with noise, missingness, or unobserved confounding? The authors acknowledge their study is limited to synthetic data and bridging this gap requires real-world datasets.

- **Open Question 2:** Does model complexity need to increase with causal structure complexity? The paper questions whether more sophisticated models are necessary for denser or higher-dimensional causal mechanisms.

- **Open Question 3:** What is the minimal training set size required to reliably predict PNS bounds for decision-critical subpopulations? The authors identify determining minimal training sets for key subpopulations as a promising research direction.

## Limitations
- Results rely entirely on synthetic data, leaving real-world performance untested
- Method assumes sufficient training examples exist for some subpopulations
- Computational cost of generating 50M samples and calculating analytical ground truth bounds is significant

## Confidence

- **High Confidence:** MLP with Mish activation outperforms other architectures on synthetic datasets for PNS prediction
- **Medium Confidence:** The mechanism by which ML models learn causal bounds is well-explained, but generalizability to real-world data remains unproven
- **Low Confidence:** The assumption that Mish's mathematical properties are the sole reason for superior performance on lower bounds

## Next Checks
1. Apply trained models to a public dataset with known causal structure (e.g., IHDP) to assess performance degradation or stability
2. Conduct a more granular study comparing Mish to other smooth, non-monotonic activations (e.g., Swish, SiLU)
3. Systematically vary the complexity of synthetic SCMs to map the precise limits of the model's predictive capability