---
ver: rpa2
title: Performance Gap in Entity Knowledge Extraction Across Modalities in Vision
  Language Models
arxiv_id: '2412.14133'
source_url: https://arxiv.org/abs/2412.14133
tags:
- image
- visual
- entity
- layers
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates a significant performance gap in vision-language
  models (VLMs) when extracting factual knowledge about entities presented visually
  versus textually. The study introduces PopVQA, a dataset enabling isolation of entity
  recognition from knowledge extraction, revealing an 18% accuracy drop in some models
  for visual inputs.
---

# Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models

## Quick Facts
- arXiv ID: 2412.14133
- Source URL: https://arxiv.org/abs/2412.14133
- Authors: Ido Cohen; Daniela Gottesman; Mor Geva; Raja Giryes
- Reference count: 40
- Primary result: 18% accuracy drop for visual vs. textual entity knowledge extraction in some VLMs

## Executive Summary
This work investigates a significant performance gap in vision-language models (VLMs) when extracting factual knowledge about entities presented visually versus textually. The study introduces PopVQA, a dataset enabling isolation of entity recognition from knowledge extraction, revealing an 18% accuracy drop in some models for visual inputs. Mechanistic interpretability tools show that meaningful information flow from image tokens occurs primarily in deeper layers, with critical image processing concentrated in the middle layers. This leaves fewer layers for subsequent reasoning, suggesting inefficiency in how VLMs utilize their layers for reasoning. The findings highlight a gap between visual and textual information processing, offering insights into improving VLMs' ability to leverage internal knowledge when presented with visual inputs.

## Method Summary
The paper benchmarks the performance gap in VLMs when extracting factual knowledge about entities presented visually versus textually. PopVQA dataset is used, and VLMs including LLaVA-1.5-7B, LLaVA-MORE-8B, LLaVA-1.6-34B, and Qwen2-VL are evaluated. The methodology involves: (1) identifying entities the model can recognize visually, (2) comparing accuracy on factual questions between visual and textual prompts, (3) using activation patching to swap image token hidden states between entity forward passes at different layers to find critical processing layers, and (4) forward patching to freeze image token states from early layers through layer 20 to test if earlier processing suffices.

## Key Results
- VLMs show an 18% accuracy drop when extracting knowledge about entities presented visually versus textually
- Critical image processing concentrated in middle layers (15-24), with entity information propagating to query tokens late
- Deeper models (LLaVA34B) and unfrozen visual backbones (Qwen2-VL) show smaller accuracy gaps
- Models often output entity names instead of requested facts, suggesting incomplete reasoning propagation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Image tokens do not meaningfully propagate entity information to query and generation tokens until middle-to-deep layers, creating a late-binding bottleneck.
- **Mechanism:** The vision encoder produces visual features that are projected into the LLM embedding space, but cross-attention transfer of entity identity to later token positions concentrates in layers 15-24 (for LLaVA7B, critical transition at layer 20; for LM-SigLIP, layer 17; for LM-CLIP, layer 24). Cross-patching image hidden states before these layers causes the model to predict the injected entity; patching after reliably predicts the original entity.
- **Core assumption:** The cross-patching switch point reflects when entity identification has been processed and propagated, not merely when patching artifacts become visible.
- **Evidence anchors:** "meaningful information flow from image tokens occurs primarily in deeper layers, with critical image processing concentrated in the middle layers"; Cross-patching experiments show entity prediction switches from injected to original at layer 20 (LLaVA7B), with 100% injected prediction up to layer 14 even across entity types; Kaduri et al. (2024, concurrent) find middle layers critical for cross-modal transfer; Neo et al. (2025) find similar attention-layer roles for visual attributes.
- **Break condition:** If early-fusion models (shared embedding spaces from layer 0) show no accuracy gap between visual and textual entity queries, the late-binding explanation would be insufficient.

### Mechanism 2
- **Claim:** Entity identification and knowledge extraction compete for layer budget, with identification consuming middle layers and leaving fewer layers for factual reasoning.
- **Mechanism:** The model performs a two-hop operation: (1) recognize entity from image tokens, (2) retrieve parametric knowledge about that entity. When identification dominates until layer ~20, only ~12 layers remain for reasoning. The paper shows models often output the entity name itself instead of the requested relation (e.g., naming the subject when asked for spouse), suggesting incomplete reasoning propagation.
- **Core assumption:** Layer count proxies for computational capacity allocated to each subtask; more layers for reasoning would improve factual extraction.
- **Evidence anchors:** "the model investing too many layers in identifying the subject visually, leaving fewer layers for the second hop of factual reasoning"; LLaVA34B (deeper model) shows lower drop (12.1%) than LLaVA7B (17.7%); Qwen2-VL (unfrozen visual backbone) shows lowest drop (4.3%); Biran et al. (2024, cited in paper) document similar late-hopping limitations in multi-hop textual queries in LLMs.
- **Break condition:** If patching that forces early identification (freezing image tokens from layers 0-4 through layer 20) eliminates the accuracy gap, the competition-for-layers hypothesis would need revision.

### Mechanism 3
- **Claim:** Early availability of entity identity in image token representations does not guarantee early propagation to query tokens via attention.
- **Mechanism:** Forward activation patching shows LLaVA-MORE-8B variants can identify ~30-50% of entities even when image tokens are frozen from layer 0, implying the vision encoder + projection already encodes sufficient identity signal. However, entities with early-identifiable representations still show equivalent or larger accuracy drops, suggesting attention mechanisms do not route this information to query tokens until middle layers.
- **Core assumption:** Early identification in frozen-patch experiments reflects information present in visual hidden states, not information that reaches query positions.
- **Evidence anchors:** Forward patching from early layers retains 30-50% identification; Table 2 shows early-id entities have higher accuracy but also higher drop (18.2% vs 15.4% for LM-CLIP); "better knowledge of an entity (higher accuracy) leads to early recognition, and not the other way around"; Geva et al. (2023, cited) show similar late subject-to-query information flow in text-only factual recall.
- **Break condition:** If attention-head-level interventions that force early cross-modal attention eliminate the gap without degrading other capabilities, the propagation-timing constraint would be identified as causal.

## Foundational Learning

- **Concept: Mechanistic Interpretability (Activation Patching, Attention Knockout)**
  - **Why needed here:** The paper's causal claims depend on intervention-based methods that alter hidden states mid-inference to trace information flow.
  - **Quick check question:** Can you explain why patching image hidden states from one entity's forward pass into another's at layer 15 vs. layer 25 yields different prediction outcomes?

- **Concept: Two-Hop Reasoning in Transformers**
  - **Why needed here:** The visual entity knowledge task decomposes into identification + extraction, analogous to multi-hop reasoning chains studied in LLMs.
  - **Quick check question:** For the query "Who is the spouse of the subject in this image?", what are the two hops and where might each fail?

- **Concept: Vision-Language Model Architecture (Encoder-Projector-LLM)**
  - **Why needed here:** The paper's notation and experiments assume familiarity with how visual features flow through g → W → f and where interventions occur.
  - **Quick check question:** In LLaVA-style architectures, at what point do image tokens and text tokens first share a representation space?

## Architecture Onboarding

- **Component map:** Vision encoder (g) -> Projection layer (W) -> LLM (f) -> Generated tokens
- **Critical path:** 1. Image → vision encoder → projection → image tokens at layer 0; 2. Middle layers (15-24): Cross-modal attention concentrates here; entity identity propagates to query/generation tokens; 3. Late layers (25+): Text-dominated reasoning; factual knowledge extraction; 4. If identification completes late, reasoning budget is compressed
- **Design tradeoffs:** Deeper models (LLaVA34B) reduce accuracy gap but increase compute; Unfrozen visual backbones (Qwen2-VL) improve alignment but require more training compute; Early-fusion architectures could parallelize identification and reasoning but may lose modality-specific preprocessing
- **Failure signatures:** Model outputs entity name when asked for related fact (e.g., "Who is the spouse?" → "Robin Williams"); Visual accuracy substantially below text accuracy for known entities; Attention knockout in layers 15-20 causes complete prediction collapse
- **First 3 experiments:** 1. Reproduce cross-patching experiment (Section 5) on your VLM: patch image hidden states from entity A into entity B's forward pass at each layer, plot prediction switch point; 2. Run forward activation patching (Section 6): freeze image tokens from layer k through layer 20 for k ∈ {0, 5, 10, 15}, measure identification accuracy; 3. Benchmark visual vs. textual entity knowledge extraction on PopVQA subset: filter entities your model identifies correctly, then compare accuracy on factual questions with entity given as image vs. text

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do early-fusion or native VLM architectures exhibit the same late information flow bottleneck from image tokens as observed in projection-based models like LLaVA?
- Basis in paper: The Conclusion states, "this behavior may be mitigated to some extent by using early-fusion models... We leave this exploration for future work," and the Limitations section suggests studying a "larger variety of VLM types."
- Why unresolved: The mechanistic interpretability analysis was restricted to LLaVA-style models due to their canonical architecture and ease of intervention.
- What evidence would resolve it: Applying activation patching and attention knockout techniques to early-fusion models to determine if critical image processing shifts to earlier layers.

### Open Question 2
- Question: Can specific training objectives or architectural modifications successfully decouple entity identification from knowledge extraction to reduce the performance gap?
- Basis in paper: The Conclusion proposes "adjusting training regimes to better separate or balance identification and knowledge extraction steps, as well as designing architectural enhancements that reduce entanglement."
- Why unresolved: This work focused on diagnosing the layer-usage inefficiency rather than implementing or validating specific fixes.
- What evidence would resolve it: A training regime that forces earlier entity representation convergence, resulting in a statistically significant reduction in the 18% accuracy drop.

### Open Question 3
- Question: To what extent do visual confounders (e.g., contextual cues like national jerseys) versus pure entity recognition contribute to the observed accuracy variance?
- Basis in paper: The Limitations section notes the study "does not go into disentangling the effects of such confounders on the observed gap," despite acknowledging they can steer the model.
- Why unresolved: The dataset (PopVQA) includes natural images where visual context provides inference shortcuts, but these were not controlled for in the isolation experiments.
- What evidence would resolve it: An ablation study using synthetic or masked images where contextual confounders are systematically removed to isolate pure recognition performance.

## Limitations

- The causal role of middle layers (15-24) in the accuracy gap is inferred from patching experiments but not directly validated through architectural ablation studies
- The analysis focuses on entity identification as a bottleneck, but does not rule out that visual reasoning might require different computational primitives than textual reasoning
- The PopVQA dataset, while enabling controlled experiments, covers a specific domain (celebrity entities) and may not generalize to other knowledge domains or entity types

## Confidence

- **High Confidence:** The empirical finding of an 18% accuracy gap in some models is directly measured and reproducible. The dataset construction and basic gap measurement methodology are well-specified.
- **Medium Confidence:** The mechanistic claims about late information flow and layer competition are supported by patching experiments but rely on interpretations of intervention effects. The two-hop reasoning model is plausible but not exhaustively validated.
- **Low Confidence:** Claims about why early identification in frozen-patch experiments does not translate to early query token propagation depend on assumptions about attention mechanisms that are not fully explored.

## Next Checks

1. **Architectural Ablation:** Train or modify a VLM to perform entity identification in early layers (layers 0-4) and freeze those representations. Measure whether this eliminates the accuracy gap without degrading other visual tasks.
2. **Cross-Domain Generalization:** Apply the PopVQA methodology to a dataset of non-celebrity entities (e.g., scientific concepts, historical events) to test if the accuracy gap and its mechanistic causes generalize beyond the current domain.
3. **Attention Head Analysis:** Use attention visualization tools to identify which specific attention heads in layers 15-24 are responsible for cross-modal entity propagation. Knockout experiments on these heads could validate their causal role.