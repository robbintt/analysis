---
ver: rpa2
title: 'Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking
  Strategic Agent Behaviours under Realistically Simulated Market Conditions'
arxiv_id: '2507.02698'
source_url: https://arxiv.org/abs/2507.02698
tags:
- pricing
- agents
- agent
- price
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks multi-agent reinforcement learning (MARL)
  algorithms for dynamic pricing in supply chains, addressing the limitations of static,
  rule-based pricing systems that fail to model interdependent agent behaviors. The
  research compares MADDPG, MADQN, and QMIX against rule-based baselines within a
  simulated environment informed by real e-commerce transaction data and a LightGBM
  demand prediction model.
---

# Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions

## Quick Facts
- arXiv ID: 2507.02698
- Source URL: https://arxiv.org/abs/2507.02698
- Authors: Thomas Hazenberg; Yao Ma; Seyed Sahand Mohammadi Ziabari; Marijn van Rijswijk
- Reference count: 40
- Primary result: MARL algorithms (MADDPG, MADQN, QMIX) produce emergent strategic behaviors in supply chain pricing that outperform static rule-based systems in revenue but with varying stability and fairness trade-offs

## Executive Summary
This study benchmarks multi-agent reinforcement learning (MARL) algorithms for dynamic pricing in supply chains, addressing the limitations of static, rule-based pricing systems that fail to model interdependent agent behaviors. The research compares MADDPG, MADQN, and QMIX against rule-based baselines within a simulated environment informed by real e-commerce transaction data and a LightGBM demand prediction model. Results show that rule-based agents achieve near-perfect fairness (Jain's Index: 0.9896) and highest price stability (volatility: 0.024), but lack competitive dynamics. MADQN exhibits aggressive pricing with highest volatility and lowest fairness (0.5844), while MADDPG offers a balanced approach with moderate market competition (share volatility: 9.5 pp), relatively high fairness (0.8819), and stable pricing. The findings suggest MARL introduces emergent strategic behaviors not captured by static pricing rules, offering potential for more adaptive and responsive dynamic pricing strategies in supply chain environments.

## Method Summary
The research employs a simulated market environment with 4 competing agents, each managing 5 products, using real B2B e-commerce transaction data to train a LightGBM demand prediction model. Three MARL algorithms (MADDPG, MADQN, QMIX) are compared against rule-based baselines, with agents submitting prices weekly and receiving rewards based on revenue and stability penalties. The environment features demand prediction, revenue calculation, and state observation updates, while MARL agents use experience replay with recency-biased sampling. Experiments run over 30 episodes of 104 weeks each, with performance evaluated across metrics including revenue, fairness (Jain's Index), price stability (volatility), and market share dynamics.

## Key Results
- Rule-based agents achieve near-perfect fairness (Jain's Index: 0.9896) and highest price stability (volatility: 0.024), but lowest revenue potential
- MADQN exhibits aggressive pricing with highest volatility (0.085) and lowest fairness (0.5844), achieving maximum revenue in inelastic markets
- MADDPG provides balanced approach with moderate market competition (share volatility: 9.5 pp), relatively high fairness (0.8819), and stable pricing (volatility: 0.052)
- QMIX shows coordinated behavior in cooperative settings, with performance aligning with cooperative MARL literature

## Why This Works (Mechanism)

### Mechanism 1: Centralized Critic with Decentralized Actor (MADDPG)
- Claim: MADDPG enables stable learning in non-stationary multi-agent environments by using centralized critics during training while maintaining decentralized execution.
- Mechanism: Each agent has a local actor network mapping observations to continuous pricing actions. During training, centralized critics access joint state-action information from all agents to compute more accurate gradients. During execution, agents act independently using only local observations, encoding competitive dynamics via price ratios, demand trends, and market share metrics in state representations.
- Core assumption: The joint state-action space provides sufficient information for stable gradient estimation despite non-stationary opponent policies.
- Evidence anchors:
  - [abstract]: "MADDPG provides a more balanced approach, supporting market competition (share volatility: 9.5 pp) while maintaining relatively high fairness (0.8819) and stable pricing"
  - [section 3.5.1]: "While action decisions are decentralized, centralized critics leverage joint state-action information during training to improve learning stability across agents"
  - [corpus]: Related MARL pricing work (arxiv 2501.08234) similarly employs non-zero-sum Markov games for competing railway operators
- Break condition: When agents cannot share information during training due to privacy constraints, or when the joint action space becomes computationally intractable with many agents (scalability limit noted in section 2.3.1).

### Mechanism 2: Independent Q-Learning with Discrete Actions (MADQN)
- Claim: Decentralized learning without coordination mechanisms leads to aggressive, revenue-maximizing but destabilizing strategies.
- Mechanism: Each MADQN agent independently learns discrete pricing actions (-10% to +10%) using only local observations and rewards. Without centralized coordination or opponent modeling, agents converge toward self-interested strategies that aggressively exploit market conditions. The ε-greedy exploration (decaying from 1.0 to 0.05) encourages initial exploration but converges to exploitation of discovered advantageous strategies.
- Core assumption: Agents can achieve high individual rewards without explicit coordination mechanisms.
- Evidence anchors:
  - [abstract]: "MADQN exhibits aggressive pricing behaviour, with the highest volatility and the lowest fairness (0.5844)"
  - [section 3.5.2]: "MADQN agents learn independently, relying solely on local observations and rewards. This decentralization encourages self-interested behaviour"
  - [section 4]: "MADQN agents demonstrate the most aggressive pricing, exhibiting the highest adjustment magnitude (0.0358) and frequency (0.763)"
  - [corpus]: Limited direct corpus evidence on MADQN for pricing; related work focuses on coordinated approaches rather than independent learners
- Break condition: When market requires price stability, regulatory compliance, or when aggressive strategies trigger customer backlash. Also breaks in elastic markets where demand drops sharply with price increases.

### Mechanism 3: Value Factorization with Monotonicity Constraints (QMIX)
- Claim: Enforcing monotonicity between individual Q-values and joint Q-values enables coordinated multi-agent strategies while preserving decentralized execution.
- Mechanism: Each agent maintains local Q-networks for action evaluation, while a centralized mixing network aggregates per-agent utilities into a global action-value estimate. The mixing network, conditioned on global state, enforces that local Q-value improvements always benefit the joint objective through monotonicity constraints. This prevents destructive competition while allowing coordinated strategies where inter-product pricing gives better returns than independent optimization.
- Core assumption: The joint optimal policy can be decomposed into monotonic local policies without significant loss.
- Evidence anchors:
  - [abstract]: Results summary indicates QMIX performs well in cooperative settings
  - [section 3.5.3]: "QMIX's advantage lies in representing joint action-values which is not just a sum of individual functionalities, allowing it to learn policies where inter-product pricing coordination gives better returns"
  - [section 5.1]: "QMIX was better at promoting coordination among agents. Its behaviour aligns with findings in the cooperative MARL literature"
  - [corpus]: Graph-Attentive MAPPO paper (arxiv 2511.00039) explores related coordination mechanisms for retail pricing
- Break condition: When agent objectives are fundamentally adversarial rather than cooperative, or when the monotonicity constraint proves too restrictive for optimal policies in complex competitive markets.

## Foundational Learning

- **Markov Decision Processes (MDPs) vs. Markov Games**
  - Why needed here: Single-agent RL assumes stationary environments (transition probabilities fixed), but multi-agent pricing involves non-stationary opponents whose policies change during learning. Understanding this distinction explains why standard Q-learning fails in MARL and why CTDE architectures emerged.
  - Quick check question: Why does an environment become non-stationary when multiple learning agents interact simultaneously, and what architectural pattern addresses this?

- **Experience Replay and Recency-Biased Sampling**
  - Why needed here: All three MARL algorithms use replay buffers, but the paper introduces "recency-biased sampling" as a custom contribution. Understanding why decorrelating samples matters—and why recency bias might help in non-stationary markets—helps evaluate architectural modifications.
  - Quick check question: Why does storing and sampling past experiences improve learning stability, and what tradeoff does recency bias introduce?

- **Exploration-Exploitation in Non-Stationary Environments**
  - Why needed here: The paper uses different exploration strategies (Gaussian noise for MADDPG, ε-greedy for MADQN/QMIX), all decaying over episodes. In multi-agent settings, exploration by one agent changes the environment for others, making convergence harder and requiring careful decay scheduling.
  - Quick check question: How does Agent A's exploration affect Agent B's learning, and why might this require different exploration strategies than single-agent RL?

## Architecture Onboarding

- **Component map:**
  Demand Model (LightGBM) -> Simulation Environment -> Agent Pool (4 agents) -> State Representation -> Evaluation Pipeline

- **Critical path:**
  1. Preprocess Online Retail II dataset → train LightGBM demand model with early stopping
  2. Initialize 4 agents with identical product portfolios and cost structures
  3. For each of 30 episodes (104 weeks each):
     - Agents observe state → select prices (continuous for MADDPG, discrete ±10% for MADQN/QMIX)
     - Environment predicts demand via LightGBM, computes rewards (revenue + stability penalty)
     - MARL agents store transitions in replay buffer, periodically sample and update networks
  4. Aggregate metrics across evaluation window, compute confidence intervals over 8 independent runs

- **Design tradeoffs:**
  - **Continuous vs. discrete actions**: MADDPG (continuous) allows fine-grained adjustments but requires noise-based exploration; MADQN/QMIX (discrete ±10% buckets) simplify exploration but limit granularity
  - **Coordination vs. independence**: QMIX and MADDPG enable coordination through centralized components (mixing network, critics) during training; MADQN is fully independent—faster training but more volatile outcomes
  - **Stability vs. adaptability**: Rule-based offers maximum stability (volatility 0.024) but no adaptation; MADQN offers maximum adaptability (adjustment frequency 0.763) but high volatility (0.085); MADDPG balances both (volatility 0.052, frequency 0.423)

- **Failure signatures:**
  - **Price wars**: Rapidly cascading price drops across agents, visible as synchronized negative price momentum; indicates over-aggressive competitive strategies without coordination
  - **Revenue collapse despite high prices**: MADQN raising prices to extreme levels when demand is inelastic (ε = -0.072 in this dataset); will fail catastrophically in elastic markets where demand responds to price
  - **Non-convergence**: Q-values or actor losses oscillating without stabilization across episodes; often indicates learning rate too high, insufficient exploration decay, or replay buffer issues
  - **Single-agent dominance**: One agent capturing >80% market share consistently; suggests initialization bias, unfair feature access, or broken competitive dynamics

- **First 3 experiments:**
  1. **Baseline validation**: Replicate 4x Rule-Based vs 4x MADDPG comparison. Verify MADDPG achieves higher revenue (~£90K vs ~£23K), moderate volatility (~0.052), fairness ~0.88. This validates your environment setup and demand model implementation.
  2. **Elasticity sensitivity test**: Modify LightGBM predictions or use synthetic demand with elastic response (ε ≈ -1.0 to -2.0). Re-run 4x MADQN. Hypothesis: aggressive pricing should become less dominant as demand becomes price-sensitive. This tests generalization beyond the inelastic giftware context.
  3. **Hybrid stability boundary**: Run 1x MADDPG + 3x Rule-Based configuration. Verify the paper's finding that a single MARL agent cannot disrupt a stable rule-based market (total revenue modestly rises but market share remains balanced). This establishes deployment safety bounds for introducing MARL into existing rule-based systems.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do MARL agent strategies and performance trade-offs change when applied to markets with highly elastic demand rather than the inelastic giftware market studied?
- Basis in paper: [explicit] The authors state that the near-zero elasticity ($\varepsilon = -0.072$) limits generalizability and that "future research should focus on... products with elastic demand" to evaluate responsiveness in sensitive markets (Section 5.3.1, Section 6).
- Why unresolved: The current inelastic environment allowed MADQN agents to raise prices without significant demand loss, potentially masking flaws in aggressive strategies that would fail in elastic markets.
- What evidence would resolve it: Benchmarking results from simulations using datasets with high price elasticity (e.g., travel or groceries) to observe if aggressive agents still dominate or if stability-focused agents become optimal.

### Open Question 2
- Question: How do supply limits, inventory constraints, and fulfillment delays influence the convergence and stability of MARL pricing agents?
- Basis in paper: [explicit] The authors note the simulation lacked operational constraints and suggest "Future work should explore how such constraints shape agent behaviour" (Section 5.3.4).
- Why unresolved: The current environment assumed infinite availability and immediate feedback, ignoring real-world friction that necessitates inventory-sensitive pricing decisions.
- What evidence would resolve it: Experiments incorporating stock-out scenarios and lead times into the environment, analyzing whether agents like MADDPG adapt to maintain stability better than the reactive MADQN.

### Open Question 3
- Question: Can graph-based or hierarchical MARL architectures maintain the observed balance between revenue and fairness when scaling to large, multi-tier supply chains?
- Basis in paper: [explicit] The authors conclude that "Future research should... explore scalable MARL architectures, such as graph-based or hierarchical methods" to address computational complexity in larger systems (Section 6).
- Why unresolved: Standard CTDE methods face stability and scalability issues as agent counts increase, limiting their applicability to complex enterprise ERP environments.
- What evidence would resolve it: Performance benchmarks of advanced architectures in simulations with significantly more agents and product tiers, comparing their coordination efficiency and computational cost against the current baselines.

## Limitations

- The study relies on historical B2B transaction data with inelastic demand characteristics (ε = -0.072), limiting generalizability to markets with price-sensitive consumers
- The simulation environment remains a closed system that cannot capture all real-world supply chain complexities, regulatory constraints, or competitive behaviors outside the controlled agent pool
- The LightGBM demand model's R² = 0.74 suggests moderate predictive accuracy, with potential overfitting risks that could affect policy robustness

## Confidence

- **High confidence**: Rule-based agents achieving near-perfect fairness (Jain's Index: 0.9896) and highest price stability (volatility: 0.024)
- **Medium confidence**: MADDPG's balanced approach with moderate market competition (share volatility: 9.5 pp), relatively high fairness (0.8819), and stable pricing
- **Medium confidence**: MADQN's aggressive pricing with highest volatility and lowest fairness (0.5844)
- **Low confidence**: Generalization of MARL advantages beyond the specific e-commerce context and demand elasticity profile tested

## Next Checks

1. **Elasticity sensitivity validation**: Re-run all MARL algorithms with demand models calibrated for price-elastic markets (ε ≈ -1.0 to -2.0) to verify whether MADQN's aggressive strategy becomes detrimental and whether MADDPG's balanced approach proves more robust across market types

2. **Dynamic market structure robustness**: Introduce market entrants/exits and portfolio changes mid-simulation to test whether MARL agents maintain performance advantages when competitive dynamics shift, as rule-based systems cannot adapt

3. **Transfer learning assessment**: Train MARL agents on one product category dataset, then evaluate performance when deployed to a different category with distinct demand patterns to quantify generalization capabilities beyond the training distribution