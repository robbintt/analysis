---
ver: rpa2
title: Does Model Size Matter? A Comparison of Small and Large Language Models for
  Requirements Classification
arxiv_id: '2510.21443'
source_url: https://arxiv.org/abs/2510.21443
tags:
- llms
- slms
- requirements
- performance
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares small (SLM) and large (LLM) language models\
  \ for requirements classification, addressing privacy, cost, and deployability concerns\
  \ with LLMs. Eight models\u2014five SLMs (7-8B parameters) and three LLMs (1-2T\
  \ parameters)\u2014were evaluated on three datasets using F1 score, precision, and\
  \ recall."
---

# Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification

## Quick Facts
- arXiv ID: 2510.21443
- Source URL: https://arxiv.org/abs/2510.21443
- Reference count: 19
- Key outcome: Small language models nearly match large language models in requirements classification performance, with only a 2% lower average F1 score that is not statistically significant.

## Executive Summary
This study evaluates whether model size matters for requirements classification by comparing five small language models (7-8B parameters) against three large language models (1-2T parameters) across three datasets. Using Chain-of-Thought plus Few-Shot prompting, the evaluation found that small models achieve performance within 2% of large models on average F1 score, with this difference being statistically non-significant. The research demonstrates that small models offer significant advantages in privacy, cost, and local deployment while maintaining competitive accuracy for requirements classification tasks.

## Method Summary
The study evaluated eight language models (five SLMs: 7-8B parameters; three LLMs: 1-2T parameters) on three requirements classification datasets using Chain-of-Thought plus Few-Shot prompting with four examples per class. SLMs were run locally on RTX 4090 GPU with temperature=0 and 3-run majority voting, while LLMs were accessed via API. Performance was measured using macro-averaged precision, recall, and F1 score, with statistical analysis using Scheirer-Ray-Hare test to assess significance.

## Key Results
- Small language models achieved an average F1 score only 2% lower than large language models across three datasets
- The 2% performance difference was not statistically significant (p = 0.296)
- Dataset characteristics had a stronger impact on performance than model size, with effect size η²H = 0.63 for datasets versus η²H = 0.04 for model type
- Small models even outperformed large models in recall on the PROMISE Reclass dataset despite being up to 300 times smaller

## Why This Works (Mechanism)

### Mechanism 1: Task Complexity Saturation at Lower Model Capacity
Requirements classification involves binary and multi-class classification within a constrained label space (e.g., FR/NFR, Sec/NSec). The 7-8B parameter models capture sufficient semantic distinctions for these pattern-matching tasks, yielding diminishing returns from additional parameters. This suggests classification tasks saturate in performance at model sizes well below trillion-parameter scales.

### Mechanism 2: Prompting Strategy Compensation
Chain-of-Thought combined with Few-Shot prompting narrows the performance gap by externalizing context that larger models might infer internally. This reduces the burden on model parameters to encode task-specific patterns, allowing smaller models to perform competitively when given appropriate guidance through explicit reasoning scaffolds and labeled examples.

### Mechanism 3: Dataset Characteristics as Primary Performance Driver
Statistical analysis showed dataset effect size (η²H = 0.63, large) vastly exceeded model type effect size (η²H = 0.04, small). Well-defined classification schemas with clear boundaries reduce ambiguity, allowing smaller models to achieve near-ceiling performance regardless of size.

## Foundational Learning

- **Concept: Statistical Significance vs. Practical Significance**
  - Why needed here: The 2% F1 difference was not statistically significant (p = 0.296), but the study acknowledges potential Type II error due to small sample size. Understanding this distinction is critical for interpreting whether SLMs are truly "equivalent" or if the study lacked power to detect real differences.
  - Quick check question: If a follow-up study with 50 models found the same 2% gap but with p < 0.05, would your recommendation change?

- **Concept: Macro-Averaging for Multi-Class Imbalance**
  - Why needed here: The datasets have class imbalances (e.g., PROMISE: 255 FR vs. 370 NFR). Macro-averaged F1 treats each class equally rather than weighting by frequency, which affects how performance should be interpreted for rare classes.
  - Quick check question: If security requirements are 10x rarer than non-security, would macro-F1 over- or under-represent performance on the security class compared to weighted-F1?

- **Concept: Data Leakage in LLM Evaluation**
  - Why needed here: The paper notes that "some datasets may have been included in the pre-training corpora of the evaluated LLMs, potentially inflating their performance." This is a fundamental validity concern when comparing models trained on unknown data.
  - Quick check question: If PROMISE was in GPT-5's training data but not in Llama-3's, how would this bias the comparison?

## Architecture Onboarding

- **Component map:** Requirement text + class definitions + few-shot examples → prompt template → inference layer (local SLM or API LLM) → 3 runs with majority voting → final label → macro-precision, macro-recall, macro-F1 computation

- **Critical path:** 1) Prompt construction (CoT + 4 examples per class) → highest impact on SLM performance; 2) Temperature = 0 for reproducibility → deterministic outputs required for statistical testing; 3) Majority voting across 3 runs → mitigates output variability

- **Design tradeoffs:** Local SLM deployment offers privacy, control, and no API costs but requires GPU infrastructure and takes longer (400s avg vs. 138-300s for LLMs). API-based LLM provides speed and potential accuracy but involves data exposure, reproducibility issues, and scaling costs. The study used one prompt for consistency, though production systems may benefit from per-model prompt optimization.

- **Failure signatures:** SLM recall spikes (0.96 on PROMISE Reclass) with precision drops (0.55-0.58) indicate over-generation of positive class; calibrate classification threshold. High variance across 3 runs suggests temperature not properly fixed or model non-determinism. Performance drop on PROMISE Reclass vs. other datasets indicates multi-label or ambiguous class definitions require prompt refinement.

- **First 3 experiments:** 1) Baseline replication: Run the provided replication package on your hardware with Llama-3-8B on all three datasets; verify F1 within ±0.02 of reported values. 2) Prompt sensitivity test: Vary the number of few-shot examples (0, 2, 4, 8) and measure F1 impact; identify minimum examples needed for acceptable performance. 3) Domain adaptation: Apply the pipeline to your own proprietary requirements dataset (10-50 samples with gold labels); compare SLM vs. LLM and assess whether the 2% gap holds in your context.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can SLMs match LLMs in quality when generating explanations for requirements classification decisions?
  - Basis in paper: The authors state future research will focus on generating "explanations that justify classifications" to leverage generative capabilities beyond simple label prediction.
  - Why unresolved: The current study only evaluated classification accuracy (F1, Precision, Recall) and did not assess the models' ability to produce reasoning or rationale.
  - What evidence would resolve it: A comparative study measuring the quality, faithfulness, and utility of explanations generated by SLMs versus LLMs for specific requirements.

- **Open Question 2:** Does the observed performance parity between SLMs and LLMs persist in generative RE tasks like traceability or model generation?
  - Basis in paper: The roadmap section highlights the need to extend evaluation to tasks requiring artifact generation, such as traceability and model generation, to identify hybrid pipelines.
  - Why unresolved: This study was restricted to binary classification tasks; generative tasks may utilize the larger parameter space of LLMs differently.
  - What evidence would resolve it: Benchmarking both model types on generative tasks (e.g., generating UML diagrams or traceability links) using appropriate similarity or correctness metrics.

- **Open Question 3:** Would a larger sample size of models reveal a statistically significant performance difference that this study missed?
  - Basis in paper: The authors note the limited sample size (8 models) increases the risk of Type II errors, potentially obscuring genuine performance differences despite the consistent 2% LLM advantage.
  - Why unresolved: The statistical power was limited by the number of models evaluated, making the non-significant result potentially unstable.
  - What evidence would resolve it: Repeating the statistical analysis with a broader set of models (e.g., 20+ SLMs and LLMs) to determine if the F1 difference becomes significant.

## Limitations
- Study conclusions based on three public datasets that may not generalize to all requirements classification contexts
- Single prompt strategy may underrepresent potential of both SLMs and LLMs
- Data leakage from pre-training remains a concern for LLM performance on PROMISE datasets
- Study did not evaluate finetuning approaches that could narrow or reverse observed performance gap
- Hardware constraints limited evaluation to SLMs with ≤8B parameters, leaving open question about mid-size models (15-40B)

## Confidence
- **High confidence**: SLMs can achieve near-LLM performance on requirements classification (supported by statistically non-significant F1 difference and effect size analysis)
- **Medium confidence**: Dataset characteristics are more influential than model size (robust ANOVA results, but limited to three datasets)
- **Medium confidence**: CoT+Few-Shot prompting effectively compensates for smaller model capacity (prior work cited, but single prompt used)
- **Low confidence**: The 2% performance gap is definitively due to model size rather than other factors (cannot separate from potential data leakage or prompt effects)

## Next Checks
1. **Prompt Sensitivity Analysis**: Systematically vary few-shot examples (0, 2, 4, 8 per class) and prompt phrasing across all 8 models to quantify how much performance depends on prompt engineering versus model capacity.

2. **Cross-Domain Validation**: Apply the exact pipeline to a fourth, independently sourced requirements dataset from a different domain (e.g., medical device requirements or aerospace) to test generalizability beyond the three evaluated datasets.

3. **Mid-Size Model Benchmark**: Evaluate 15-40B parameter models to identify the inflection point where additional parameters stop providing meaningful returns for requirements classification tasks.