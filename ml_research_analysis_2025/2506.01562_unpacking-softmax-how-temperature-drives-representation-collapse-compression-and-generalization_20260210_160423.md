---
ver: rpa2
title: 'Unpacking Softmax: How Temperature Drives Representation Collapse, Compression,
  and Generalization'
arxiv_id: '2506.01562'
source_url: https://arxiv.org/abs/2506.01562
tags:
- rank
- softmax
- temperature
- training
- collapse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how softmax temperature influences representation\
  \ learning in deep networks. It introduces rank-deficit bias, showing that high-temperature\
  \ training leads to solutions with significantly lower rank than the number of classes\u2014\
  contrasting with Neural Collapse predictions."
---

# Unpacking Softmax: How Temperature Drives Representation Collapse, Compression, and Generalization

## Quick Facts
- **arXiv ID**: 2506.01562
- **Source URL**: https://arxiv.org/abs/2506.01562
- **Reference count**: 40
- **Primary result**: High softmax temperature triggers representation collapse through logits norm growth, creating a fundamental trade-off between OOD generalization and OOD detection

## Executive Summary
This paper reveals how softmax temperature fundamentally shapes representation learning in deep networks through a phenomenon called rank-deficit bias. When training with high temperature, networks consistently converge to solutions with significantly lower rank than the number of classes—contradicting Neural Collapse predictions of full-rank (C−1) solutions. This behavior emerges from logits norm growth, where networks must increase the norm of their outputs to escape the symmetric loss landscape created by high temperature. The study demonstrates that while low-rank, compressed representations improve out-of-distribution detection through the NECO method, they simultaneously harm generalization to unseen distributions. These findings provide practical tools for controlling model behavior through temperature and architectural choices.

## Method Summary
The study investigates softmax temperature effects through systematic experiments across multiple architectures (ResNet, VGG, ViT, MLP) and datasets (CIFAR-10/100, ImageNet variants). Networks are trained with varying softmax temperatures (T ∈ {1, 10, 50, 100, 1000}) while monitoring numerical rank of pre-softmax logits, effective depth of representation learning, and OOD performance. Linear probes evaluate per-layer feature quality, and the NECO method measures OOD detection capability. Initialization scale and normalization placement are also studied as implicit temperature regulators. Key metrics include solution rank (SR), effective depth (κ), OOD generalization loss (ρ), and detection AUROC/FPR.

## Key Results
- High softmax temperature consistently produces rank-deficit solutions (significantly lower rank than class count), violating Neural Collapse predictions
- Logits norm growth is the primary driver of post-softmax rank inflation, even when pre-softmax rank remains low
- A fundamental trade-off exists: compressed representations (low effective depth) improve OOD detection but harm OOD generalization
- Initialization scale and normalization layers implicitly regulate logits norm, pre-conditioning or preventing representation collapse
- The NECO method benefits from rank-deficit solutions due to improved OrthoDev convergence

## Why This Works (Mechanism)

### Mechanism 1
High softmax temperature flattens softmax outputs toward uniform distributions, creating a symmetric loss landscape where each sample incurs nearly identical cross-entropy loss (~ln(1/n)). To minimize loss, networks must break this symmetry by increasing logits norms through singular vector alignment. When alignment occurs across multiple layers, the top singular value of final logits becomes the product of each layer's top singular values—creating exponential growth that dominates the spectrum and collapses numerical rank.

### Mechanism 2
Softmax amplifies post-softmax rank independently of pre-softmax rank through norm-driven column diversification. As logits norm increases, softmax outputs become sharper, reducing inter-column similarity and expanding the singular value spectrum. This means rank(M) ≪ c is possible while rank(softmax(M)) remains full—theoretical minimum rank for full post-softmax output is 2, though empirical solutions remain far from this limit.

### Mechanism 3
Low logits norm creates compressed representations that improve OOD detection but harm OOD generalization—a fundamental trade-off. Compressed representations force earlier layers to achieve near-final accuracy, reducing model capacity for handling distribution shift at deeper layers. Simultaneously, rank-deficit solutions achieve lower OrthoDev values (OOD representations become more orthogonal to class means), which directly improves NECO-based OOD detection.

## Foundational Learning

- **Softmax Temperature**: Central control variable affecting logits norm, representation rank, and OOD behavior. Not just an inference-time calibration tool. *Quick check: If temperature T=100, what happens to softmax output entropy compared to T=1?*

- **Numerical Rank via SVD**: The paper's core metric for measuring representation collapse. Differs from exact rank by thresholding singular values. *Quick check: How would numerical rank change if the top singular value grows 100× while others remain constant?*

- **Neural Collapse (NC)**: Prior framework predicting full-rank (C−1) solutions at convergence. Paper shows rank-deficit bias violates NC predictions. *Quick check: What rank does NC predict for a 100-class classification problem at terminal phase?*

## Architecture Onboarding

- **Component map**: Input → Backbone (ResNet/VGG/ViT/MLP) → Penultimate Layer (A_L) → Classifier Weights (W_L) → Logits Matrix (M = W_L A_L) → Softmax(·/T) → Predictions
- **Critical path**: Initialization sets initial logits norm → training with high T forces ||M|| growth via alignment → alignment cascade → exponential top singular value growth → rank collapse → compressed representations → OOD trade-off emerges
- **Design tradeoffs**:
  | Goal | Configuration | Side Effect |
  |------|---------------|-------------|
  | Better OOD detection | High T or low init ||M|| | Worse OOD generalization |
  | Better OOD generalization | Low T or high init ||M|| | Larger effective depth, less compression |
  | Prevent collapse | Normalization after logits | Loses explicit norm control |
  | Maximize compression | Match ||M||_init to desired effective depth | Careful hyperparameter search needed |
- **Failure signatures**:
  - Training stalls with high T: Loss symmetry not broken; increase learning rate or reduce T
  - VGG collapses at baseline: Architecture inherently produces low ||M||_init; apply low-T training or add normalization
  - OOD detection degrades despite high T: Check if ||M|| actually grew during training; may need longer training or different init
- **First 3 experiments**:
  1. Temperature sweep on ResNet-18/CIFAR-100: Train with T ∈ {1, 10, 50, 100}, measure SR, κ, and ρ. Verify rank-deficit bias and trade-offs emerge as predicted.