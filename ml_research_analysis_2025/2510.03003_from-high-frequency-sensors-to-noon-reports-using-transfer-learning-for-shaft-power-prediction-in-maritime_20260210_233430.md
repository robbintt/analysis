---
ver: rpa2
title: 'From high-frequency sensors to noon reports: Using transfer learning for shaft
  power prediction in maritime'
arxiv_id: '2510.03003'
source_url: https://arxiv.org/abs/2510.03003
tags:
- data
- power
- noon
- reports
- vessels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting shaft power in
  maritime vessels using low-frequency noon reports when high-frequency sensor data
  is unavailable or costly. The authors propose a transfer learning approach where
  a neural network is first trained on high-frequency sensor data and then fine-tuned
  using low-frequency noon reports from other vessels.
---

# From high-frequency sensors to noon reports: Using transfer learning for shaft power prediction in maritime

## Quick Facts
- arXiv ID: 2510.03003
- Source URL: https://arxiv.org/abs/2510.03003
- Authors: Akriti Sharma; Dogan Altan; Dusica Marijan; Arnbjørn Maressa
- Reference count: 31
- Primary result: Transfer learning reduces MAPE by 10.6% for sister vessels, 3.6% for similar vessels, and 5.3% for different vessels compared to models trained solely on noon reports

## Executive Summary
This paper addresses the challenge of predicting shaft power in maritime vessels using low-frequency noon reports when high-frequency sensor data is unavailable or costly. The authors propose a transfer learning approach where a neural network is first trained on high-frequency sensor data and then fine-tuned using low-frequency noon reports from other vessels. The method is tested on sister, similar, and different vessel types. Results show that transfer learning reduces mean absolute percentage error (MAPE) by 10.6% for sister vessels, 3.6% for similar vessels, and 5.3% for different vessels compared to models trained solely on noon reports. The approach effectively bridges the performance gap between sensor data and noon reports, providing accurate shaft power predictions and capturing consumption trends over time, making it a viable solution for fuel efficiency planning and predictive maintenance in maritime operations.

## Method Summary
The method involves pre-training a neural network on high-frequency sensor data (15-minute intervals) from a reference vessel (S_V1), then fine-tuning it on low-frequency noon reports from target vessels. The neural network architecture consists of an input layer with 7 features (speed, RPM, draft, wave/swell height/direction), three dense hidden layers (128, 64, 32 neurons with ReLU activation), and a single output neuron. During transfer learning, the first three layers are frozen while only the output layer is fine-tuned using a reduced learning rate. The model is trained on sensor data using MAE loss, Adam optimizer (LR=1e-3), and batch size of 32, then fine-tuned on noon reports with LR=1e-4 and batch size of 16. Data fusion with Copernicus (CMEMS) weather data is performed using trilinear interpolation to align meteorological data with vessel sensor timestamps.

## Key Results
- Transfer learning reduced MAPE by 10.6% for sister vessels, 3.6% for similar vessels, and 5.3% for different vessels compared to models trained only on noon reports
- The approach significantly improved R² scores, with S_V3 improving from -0.32 to 0.35
- Visual analysis showed the transfer learning model tracked actual power consumption trends more accurately than baseline models
- The method effectively bridges the performance gap between high-frequency sensor data and low-frequency noon reports

## Why This Works (Mechanism)

### Mechanism 1
High-frequency sensor data pre-training establishes a generalized physical mapping of speed and resistance that prevents overfitting to sparse, low-frequency noon reports. The neural network learns robust feature extractors from dense sensor data. By freezing these layers during fine-tuning, the model retains the high-resolution understanding of how inputs relate to power, while the final tunable layer adjusts for statistical differences inherent in daily noon reports.

### Mechanism 2
Transfer learning corrects for data scarcity and "concept drift" in target vessels by anchoring predictions to a historically stable source distribution. Noon reports suffer from low sample volume and human error. By initializing weights from a data-rich source, the model starts with a strong prior. Fine-tuning on limited target data adjusts this prior rather than attempting to learn the complex power function from scratch, resulting in better temporal generalization.

### Mechanism 3
Architectural freezing acts as a regularizer against the noise and "reporting lag" in manual noon reports. Noon reports are manually generated daily averages prone to human error. Training a deep network on such sparse, noisy data usually leads to high variance. Freezing the majority of the network limits the capacity for the noise to distort the learned physical relationships, forcing the model to only adjust the final scaling layer.

## Foundational Learning

- **Concept: Transfer Learning (Domain Adaptation)**
  - Why needed: The core innovation is applying knowledge from a data-rich domain (sensors) to a data-poor domain (reports).
  - Quick check: If you fine-tuned all layers instead of freezing the first three, would you expect better or worse performance on the "Different Vessel" category?

- **Concept: Data Fusion (Meteorological & AIS)**
  - Why needed: Sensor data lacks reliable weather labels; the authors fuse external data (CMEMS) to align sensor features with report features.
  - Quick check: Why is trilinear interpolation necessary when mapping CMEMS data to the vessel's sensor log?

- **Concept: Temporal Validation**
  - Why needed: The authors reject random shuffling in favor of training on past data to predict the future, simulating real-world deployment.
  - Quick check: Why does a random train/test split typically inflate performance metrics for time-series problems like vessel fuel consumption?

## Architecture Onboarding

- **Component map:** Input(7 features) -> Dense(128, ReLU) -> Dense(64, ReLU) -> Dense(32, ReLU) -> Dense(1, Linear)
- **Critical path:**
  1. Train Source Model on S_V1 sensor data -> Save weights
  2. Load Source weights into Target Model
  3. Freeze layers 1-3
  4. Train Target Model on Noon Reports from target vessel using reduced LR and batch size
- **Design tradeoffs:** Freezing layers protects against overfitting to sparse noon reports but caps maximum adaptability to different vessels. Using 7 common features enables transfer but may discard useful sensor-specific data.
- **Failure signatures:** Negative R² indicates models failed to capture variance and were worse than mean baseline. High Std Dev of MAPE across runs suggests sparse noon report training is unstable.
- **First 3 experiments:**
  1. Train model from scratch on S_V1 sensor data to establish upper bound of performance
  2. Train a model only on S_V2 noon reports to establish lower bound
  3. Compare "Freeze 1-3" vs. "Freeze None" vs. "Freeze All" when transferring from S_V1 to S_V2

## Open Questions the Paper Calls Out

- **Generalizability to different vessel types:** The study is limited to general cargo and container ships; future work should test the approach on tankers and bulk carriers to address scalability issues.
- **Model retraining frequency:** It remains unclear how long the model can make accurate predictions without retraining, and how prediction error evolves over time.
- **Comparison with data fusion strategy:** The performance of data fusion between meteorological data and noon reports should be compared to the transfer learning-based approach.

## Limitations
- The approach shows limited effectiveness for vessels with fundamentally different operational profiles (only 5.3% MAPE improvement for different vessels)
- Reliance on manual noon reports introduces inherent data quality limitations that transfer learning cannot fully overcome
- Frozen architecture may limit adaptability when vessels operate in regimes substantially different from the source vessel

## Confidence

- **High Confidence:** Transfer learning reduces MAPE compared to training from scratch on noon reports (supported by quantitative results across all vessel categories)
- **Medium Confidence:** Frozen layers prevent overfitting to sparse data (mechanistically sound but not explicitly tested against full fine-tuning)
- **Medium Confidence:** Generalizability to vessels with substantially different operational profiles (limited by modest improvement on "Different Vessels")

## Next Checks

1. Conduct an ablation study comparing "Freeze 1-3" vs. "Freeze None" vs. "Freeze All" when transferring from S_V1 to S_V2 to confirm partial freezing is optimal
2. Test model performance when source vessel operates in substantially different environment (e.g., Arctic vs. tropical waters) to validate physical law invariance assumption
3. Implement robustness test by introducing synthetic noise to noon reports at varying levels to quantify frozen architecture's effectiveness as regularizer against reporting errors