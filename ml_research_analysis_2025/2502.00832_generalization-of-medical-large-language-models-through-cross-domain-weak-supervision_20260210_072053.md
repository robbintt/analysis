---
ver: rpa2
title: Generalization of Medical Large Language Models through Cross-Domain Weak Supervision
arxiv_id: '2502.00832'
source_url: https://arxiv.org/abs/2502.00832
tags:
- medical
- icft
- arxiv
- https
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ICFT, an Incremental Curriculum-Based Fine-Tuning
  framework for medical large language models. ICFT addresses the challenge of adapting
  general-purpose LLMs to the medical domain by progressively transitioning from general
  linguistic knowledge to domain-specific expertise.
---

# Generalization of Medical Large Language Models through Cross-Domain Weak Supervision

## Quick Facts
- **arXiv ID**: 2502.00832
- **Source URL**: https://arxiv.org/abs/2502.00832
- **Reference count**: 37
- **Primary result**: ICFT framework achieves up to 2.58 points improvement in ROUGE-1, 6.61% higher win rate in response generation, and reduces factual errors by 36.9% compared to state-of-the-art baselines.

## Executive Summary
This paper introduces ICFT, an Incremental Curriculum-Based Fine-Tuning framework for medical large language models. ICFT addresses the challenge of adapting general-purpose LLMs to the medical domain by progressively transitioning from general linguistic knowledge to domain-specific expertise. The framework integrates curriculum learning, dual-stage memory coordination, and parameter-efficient fine-tuning (LoRA) to mitigate catastrophic forgetting and optimize computational efficiency. Experiments on medical NLP tasks including question answering, preference classification, and response generation show that ICFT consistently outperforms state-of-the-art baselines.

## Method Summary
ICFT is a three-stage framework: (1) Medical Knowledge Injection using a domain adapter with down-projection and up-projection matrices, (2) Dual-Stage Memory Coordination with Short-Term Memory (STM) and Long-Term Memory (LTM) using attention-based retrieval, and (3) LoRA fine-tuning with only 0.55% of parameters updated. The framework progressively transitions from general medical texts to specialized clinical data, using consistency loss to preserve knowledge and frequency-based thresholds to manage memory storage. Training occurs on medical datasets like HealthCareMagic and iCliniq, with evaluation on question answering, classification, and generation tasks.

## Key Results
- Achieves up to 2.58 points improvement in ROUGE-1 for medical question answering
- 6.61% higher win rate in medical response generation compared to baselines
- Reduces factual errors by 36.9% while training only 36M parameters (0.55% of total)
- Human evaluations confirm superior performance in accuracy, relevance, and fluency
- Strong generalization to unseen data while maintaining parameter efficiency

## Why This Works (Mechanism)

### Mechanism 1: Curriculum-Based Progressive Knowledge Injection
- Claim: Incrementally transitioning from general to domain-specific data reduces catastrophic forgetting compared to direct fine-tuning.
- Mechanism: The domain adapter projects input through Wdown (down-projection) and Wup (up-projection) matrices, with consistency loss L_consistency enforcing knowledge preservation between original and adapted parameters.
- Core assumption: Medical knowledge can be decomposed into difficulty levels that map to a learnable curriculum order.
- Evidence anchors:
  - [abstract] "progressive transition from general linguistic knowledge to strong domain-specific expertise"
  - [section 3.1] Eq. 3 defines consistency loss as L2 distance between Φ(xi; W) and Φ(xi; W + ΔW_adapter)
  - [corpus] Neighbor work "Incentivizing Strong Reasoning from Weak Supervision" supports weak-to-strong generalization premise
- Break condition: If curriculum ordering does not correlate with task complexity (e.g., rare diseases appearing early), performance may degrade rather than improve.

### Mechanism 2: Dual-Stage Memory Coordination (STM + LTM)
- Claim: Separating recent interactions from high-frequency domain knowledge improves retrieval relevance and reduces context loss.
- Mechanism: STM (capacity K) stores last K dialogue rounds; LTM stores items where freq(k) ≥ θ. Retrieval uses attention: a_STM = softmax(q^T D_STM) and a_LTM = softmax(q^T D_LTM), combined as z = W_STM a_STM + W_LTM a_LTM.
- Core assumption: Access frequency correlates with knowledge importance; attention weights meaningfully rank relevance.
- Evidence anchors:
  - [section 3.2] Eqs. 4-8 define STM update, LTM threshold condition, and combined retrieval
  - [section 4.7, Table 6] ICFT achieves 91.38% STM and 93.12% LTM retrieval accuracy vs. DPeM's 88.65%/90.21%
  - [corpus] No direct corpus evidence for dual-stage memory in medical LLMs; related work focuses on single-stage retrieval
- Break condition: If θ is set too low, LTM overflows with noisy entries; if K is too small, STM loses multi-turn context needed for clinical dialogue.

### Mechanism 3: Parameter-Efficient LoRA Adaptation
- Claim: Updating only 0.55% of parameters via low-rank decomposition preserves generalization while reducing computational cost.
- Mechanism: LoRA adds trainable matrices A ∈ R^(d×r) and B ∈ R^(r×k) such that W' = W + AB, with regularization λ||A||²_F + λ||B||²_F.
- Core assumption: Task-specific adaptations live in a low-rank subspace; rank r captures sufficient expressivity.
- Evidence anchors:
  - [section 3.3] Eq. 9-10 define LoRA update and fine-tuning loss
  - [section 4.5, Table 4] ICFT trains 36M parameters (0.55%) vs. SFT's 6,500M (100%)
  - [corpus] "MING-MOE" (neighbor paper) also uses low-rank adapter experts for medical multi-task learning, supporting PEFT viability
- Break condition: If r is underspecified for complex medical reasoning tasks, model capacity bottlenecks emerge; if λ is too high, adaptation underfits.

## Foundational Learning

- **Curriculum Learning**
  - Why needed here: ICFT's core hypothesis depends on ordering training samples by complexity to enable weak-to-strong knowledge transition.
  - Quick check question: Can you explain why training on simple general medical text before complex case studies might reduce forgetting?

- **Attention-Based Retrieval Mechanisms**
  - Why needed here: Memory coordination uses softmax attention over stored embeddings to retrieve relevant context.
  - Quick check question: Given query q and memory matrix D, how does softmax(q^T D) produce a weighted retrieval?

- **Low-Rank Matrix Factorization (LoRA/PEFT)**
  - Why needed here: Understanding why W' = W + AB with r ≪ d enables efficient fine-tuning without full weight updates.
  - Quick check question: If d=4096 and r=8, how many trainable parameters does LoRA add per weight matrix?

## Architecture Onboarding

- **Component map:** Base LLM Φ (frozen backbone) -> Domain Adapter (Wdown, Wup) -> Medical Knowledge Injection -> Dual-Stage Memory (STM buffer, LTM store) -> Retrieval augmentation -> LoRA Modules (A, B matrices) -> Task-specific adaptation head

- **Critical path:** Medical Knowledge Injection (Stage 1) -> Memory Coordination Setup (Stage 2) -> LoRA Fine-Tuning (Stage 3). Each stage builds on the previous; skipping Stage 1 risks poor domain grounding.

- **Design tradeoffs:**
  - Higher STM capacity K improves multi-turn coherence but increases memory footprint and retrieval latency.
  - Lower rank r reduces parameters but may underfit specialized medical subdomains (e.g., radiology vs. psychiatry).
  - Threshold θ controls LTM selectivity; too aggressive filtering loses rare but critical knowledge.

- **Failure signatures:**
  - Catastrophic forgetting: Sudden drop in general NLP task performance -> check L_consistency weighting.
  - Repetitive responses: High distinct-n drop -> STM may be dominating retrieval; rebalance W_STM vs. W_LTM.
  - Factual errors on rare diseases: Low LTM hit rate -> θ may be too high or curriculum skipped rare cases.

- **First 3 experiments:**
  1. **Ablation without Memory Coordination:** Disable STM/LTM, run on HealthCareMagic QA, expect ~1.72 ROUGE-1 drop per Table 2.
  2. **Vary LoRA rank r:** Test r ∈ {4, 8, 16, 32} on preference classification; plot accuracy vs. parameter count to find inflection point.
  3. **Curriculum order swap:** Train on complex case studies first, then general text; measure factual error rate increase to validate curriculum hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the ICFT framework be effectively extended to process and integrate multi-modal medical data, such as combining textual clinical notes with medical imaging?
- Basis in paper: [explicit] The conclusion explicitly states, "Future work will focus on extending ICFT to multi-modal medical data, such as integrating textual and imaging information."
- Why unresolved: The current framework architecture, specifically the domain adapter and memory coordination modules, is designed exclusively for text-based large language models.
- What evidence would resolve it: Demonstrating the framework's performance on multi-modal benchmarks (e.g., medical VQA) where the model successfully aligns visual features with the textual knowledge stored in the Dual-Stage Memory.

### Open Question 2
- Question: Is the ICFT framework transferable to other high-stakes domains (e.g., legal or financial) without fundamental architectural changes to the curriculum learning strategy?
- Basis in paper: [explicit] The conclusion identifies "exploring its application in other high-stakes domains" as a direction for future research.
- Why unresolved: The curriculum strategy is tailored for the "weak to strong" transition of medical knowledge; it is unclear if the complexity pacing suits the knowledge structures of other specialized fields.
- What evidence would resolve it: Successful application of the identical ICFT pipeline on domain-specific datasets from non-medical fields, showing comparable improvements over standard fine-tuning baselines.

### Open Question 3
- Question: To what degree does the model retain its foundational general linguistic and reasoning capabilities on non-medical benchmarks after undergoing the ICFT process?
- Basis in paper: [inferred] The paper claims the method "mitigate[s] catastrophic forgetting" and preserves "general linguistic abilities," but experimental validation is restricted entirely to medical tasks (QA, Classification).
- Why unresolved: Without evaluation on general NLP benchmarks (e.g., MMLU, HellaSwag), it remains unproven whether the specialized curriculum preserves the model's broad competence or merely shifts its proficiency entirely to the medical domain.
- What evidence would resolve it: A comparative study of model performance on general-purpose reasoning benchmarks before and after ICFT training.

## Limitations

- Critical hyperparameters (LoRA rank, STM capacity, LTM threshold, learning rates) are not specified, preventing direct reproduction
- Curriculum scheduling methodology (how complexity levels are defined and transitioned) lacks detailed specification
- No evaluation on general NLP benchmarks to verify preservation of foundational linguistic capabilities
- Base model architecture specification is ambiguous (mentions GPT-3.5 and LLaMA without explicit selection)

## Confidence

- **High confidence**: Parameter efficiency claims (0.55% trainable parameters) and retrieval accuracy improvements over DPeM (91.38% vs 88.65% STM, 93.12% vs 90.21% LTM) - these are direct measurements from Table 6 with clear methodology.
- **Medium confidence**: Curriculum-based catastrophic forgetting mitigation - the mechanism is theoretically sound but depends heavily on proper curriculum ordering, which is not fully specified in the paper.
- **Low confidence**: Human evaluation results and win rate improvements - these depend on subjective judgment and small sample sizes (Table 5 shows win rates based on limited pairwise comparisons).

## Next Checks

1. **Curriculum Order Validation**: Systematically test different curriculum orderings (general→specialized vs. specialized→general) on the same model and dataset to quantify the actual impact of progressive knowledge injection versus simple fine-tuning.

2. **Memory Component Ablation**: Run controlled experiments disabling STM vs. LTM separately to isolate their individual contributions to the reported 6.61% win rate improvement in response generation.

3. **Hyperparameter Sensitivity Analysis**: Vary LoRA rank (r=4, 8, 16, 32) and STM capacity (K=32, 64, 128) while measuring both performance and parameter efficiency to identify optimal tradeoffs and verify claims about 0.55% parameter usage.