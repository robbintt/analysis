---
ver: rpa2
title: Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer's
  Disease Classification
arxiv_id: '2512.16964'
source_url: https://arxiv.org/abs/2512.16964
tags:
- alzheimer
- disease
- classification
- vision
- dementia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses Alzheimer\u2019s disease classification from\
  \ MRI scans using a four-class setup (non-demented, mild dementia, moderate dementia,\
  \ very mild dementia). The proposed PseudoColorViT-Alz method enhances grayscale\
  \ MRI images with pseudo-color mapping using the jet colormap, then applies Vision\
  \ Transformer-based classification to leverage global contextual features."
---

# Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer's Disease Classification

## Quick Facts
- **arXiv ID**: 2512.16964
- **Source URL**: https://arxiv.org/abs/2512.16964
- **Reference count**: 40
- **Primary result**: 99.79% accuracy and 100% AUC on 4-class Alzheimer's disease classification using pseudo-color MRI with Vision Transformers

## Executive Summary
This paper presents PseudoColorViT-Alz, a method for multiclass Alzheimer's disease classification from MRI scans using pseudo-color mapping and Vision Transformers. The approach converts grayscale MRI images to RGB using the jet colormap before applying a ViT-base-patch16-224 model. On the OASIS-1 dataset, the method achieves state-of-the-art performance with 99.79% accuracy and 100% AUC, outperforming recent CNN-based methods. The key innovation is leveraging pseudo-color transformations to enhance subtle intensity variations while utilizing the global feature learning capabilities of Vision Transformers.

## Method Summary
The method processes grayscale MRI scans through a pseudo-color transformation using the jet colormap, converting single-channel images to RGB format. These enhanced images are resized to 224×224 and normalized to [0,1] before being fed into a ViT-base-patch16-224 model pretrained on ImageNet. The classification head is replaced with a 4-class softmax layer for Alzheimer's disease stages. The model is fine-tuned using Adam optimizer with learning rate 1e-4, batch size 32, and categorical cross-entropy loss, with early stopping based on test accuracy after a maximum of 50 epochs.

## Key Results
- Achieved 99.79% accuracy and 100% AUC on 4-class Alzheimer's disease classification
- Outperformed recent CNN-based methods (96.1%–99.68% accuracy) from 2024–2025
- Demonstrated that colormap-enhanced Vision Transformers significantly improve multiclass Alzheimer's disease classification

## Why This Works (Mechanism)

### Mechanism 1
Pseudo-color mapping amplifies subtle intensity variations in grayscale MRI, improving feature discrimination for the downstream classifier. The jet colormap transforms single-channel grayscale intensities into a three-channel RGB representation where intensity gradients map to distinct color transitions. This non-linear mapping enhances contrast between tissue boundaries that appear nearly homogeneous in raw grayscale. Core assumption: The colormap preserves diagnostically relevant anatomical structure while amplifying discriminative signals. Evidence anchors: [abstract] "pseudo-color representations...amplifies anatomical texture and contrast cues"; [section 3.1] "enhances subtle intensity variations and anatomical structures relevant to neurodegenerative progression." Break condition: If colormap choice introduces spurious color boundaries unrelated to pathology, the mechanism may be colormap-specific.

### Mechanism 2
Vision Transformers' global self-attention captures long-range spatial dependencies across brain regions that CNNs' local receptive fields miss. ViT partitions images into patches and computes self-attention across all patch pairs, enabling direct modeling of relationships between anatomically distant regions (e.g., hippocampus and cortical areas) without progressive dilation through convolutional layers. Core assumption: Alzheimer's pathology involves spatially distributed changes where cross-regional relationships are more discriminative than local patterns. Evidence anchors: [abstract] "global feature learning capabilities of Vision Transformers"; [section 1] "CNNs typically focus on local patterns and often struggle to capture long-range spatial relationships." Break condition: If performance gains disappear when using randomly initialized ViT, global attention may not be the primary driver.

### Mechanism 3
Pseudo-color transformation improves compatibility with ImageNet-pretrained ViT weights, enabling more effective transfer learning. Standard ViTs expect 3-channel RGB inputs matching their pretraining distribution. Pseudo-color produces 3-channel inputs with structured color distributions, potentially activating pretrained color-processing pathways better than simple grayscale channel replication. Core assumption: Pretrained color feature detectors transfer meaningfully to pseudo-colored medical images despite domain shift. Evidence anchors: [section 1] "most ViTs are pretrained on large-scale RGB natural image datasets. Existing adaptations...often fail to fully exploit structural and textural information"; [section 6] "This enhanced representation aligns more effectively with Vision Transformers pretrained on natural RGB images." Break condition: If ablation shows grayscale replication + ViT matches pseudo-color performance, the pretrained weight alignment mechanism is weakened.

## Foundational Learning

- Concept: **Vision Transformer patch embedding and self-attention**
  - Why needed here: Understanding how ViT processes pseudo-colored MRI requires knowing how images become patch sequences and how attention computes global relationships.
  - Quick check question: Given a 224×224 image with 16×16 patches, how many patch tokens does ViT process (excluding CLS token)?

- Concept: **Transfer learning with domain adaptation**
  - Why needed here: The method relies on ImageNet-pretrained weights transferring to medical imaging; understanding domain shift helps assess mechanism validity.
  - Quick check question: Why might RGB natural image features transfer poorly to grayscale medical images, and how does pseudo-coloring address this?

- Concept: **Multi-class evaluation metrics (macro-averaged, OvR AUC)**
  - Why needed here: Paper reports 100% AUC with 99.79% accuracy on imbalanced 4-class data; understanding metric computation prevents misinterpretation.
  - Quick check question: If moderate dementia has only 488 samples vs. 5000+ for other classes, how could macro-averaged metrics differ from accuracy?

## Architecture Onboarding

- Component map:
  Grayscale MRI (H×W) → Resize to 224×224 → Normalize to [0,255] → Apply jet colormap → RGB tensor (3×224×224) → Normalize to [0,1] → ViT-Base-Patch16 → Linear(768→4) + Softmax

- Critical path:
  1. Colormap choice determines input distribution—verify preprocessing preserves anatomical structure
  2. Fine-tuning hyperparameters (lr=1e-4, patience=2) control overfitting risk on limited moderate dementia class
  3. Classification head replacement must initialize properly to avoid gradient issues

- Design tradeoffs:
  - Jet colormap selected without ablation; other colormaps (viridis, hot) untested
  - No data augmentation used—relies entirely on pretrained features; may limit robustness
  - Early stopping patience=2 is aggressive; may underutilize training data
  - Moderate dementia class (488 samples) severely underrepresented; model may bias toward larger classes

- Failure signatures:
  - If moderate dementia recall is significantly lower than other classes, class imbalance is unaddressed
  - If validation accuracy oscillates or plateaus early, learning rate or patience may need adjustment
  - If colormap visualization shows artifacts at intensity boundaries, preprocessing may introduce spurious features

- First 3 experiments:
  1. Ablation: Train with grayscale channel replication vs. pseudo-color on same ViT architecture to isolate colormap contribution.
  2. Colormap sweep: Test alternative colormaps (viridis, plasma, coolwarm) to assess whether jet is optimal or arbitrary.
  3. Class-balanced evaluation: Report per-class precision/recall and confusion matrix analysis focusing on moderate dementia performance given severe imbalance (488 vs. 5000+ samples).

## Open Questions the Paper Calls Out

### Open Question 1
How does PseudoColorViT-Alz generalize to independent Alzheimer's disease datasets such as ADNI or OASIS-3?
- Basis in paper: [explicit] The Discussion states: "further validation on additional datasets such as ADNI or OASIS-3 would strengthen generalizability claims."
- Why unresolved: The study only evaluated on OASIS-1, which has consistent acquisition protocols. Multi-center datasets have different scanner types, imaging parameters, and population demographics that may affect transferability.
- What evidence would resolve it: Cross-dataset validation experiments showing performance metrics (accuracy, AUC) when the OASIS-1 trained model is tested on ADNI or OASIS-3 without retraining.

### Open Question 2
Which colormap optimizes classification performance and clinical interpretability for MRI-based AD detection?
- Basis in paper: [explicit] The Discussion notes: "the choice of colormap may influence performance and interpretability, suggesting an avenue for future optimization." The Conclusion also mentions "exploring adaptive colormap strategies."
- Why unresolved: Only the jet colormap was tested; other colormaps (hot, viridis, plasma, custom mappings) may differentially enhance intensity variations relevant to neurodegenerative patterns.
- What evidence would resolve it: Systematic ablation study comparing multiple colormaps on the same dataset with matched training conditions, including clinician assessment of interpretability.

### Open Question 3
Does early stopping based on test set accuracy introduce optimistic bias in reported performance metrics?
- Basis in paper: [inferred] The methodology states early stopping uses "patience = 2 based on test accuracy," which technically allows model selection using the evaluation set, potentially inflating the 99.79% accuracy.
- Why unresolved: Standard practice uses a separate validation set for early stopping decisions; using the test set for this purpose may lead to indirect overfitting.
- What evidence would resolve it: Re-experimentation with proper three-way split (train/validation/test) or k-fold cross-validation, comparing final test accuracy against the current methodology.

### Open Question 4
How does the model perform on the severely underrepresented moderate dementia class compared to other classes?
- Basis in paper: [inferred] The dataset contains only 488 moderate dementia samples versus 5000+ for other classes, yet the paper reports no per-class sensitivity/specificity analysis beyond the confusion matrix.
- Why unresolved: High overall accuracy can mask poor performance on minority classes; balanced accuracy or per-class F1 scores were not reported.
- What evidence would resolve it: Class-specific performance metrics (precision, recall, F1) extracted from the confusion matrix, or stratified analysis showing the model's discriminative ability for moderate dementia specifically.

## Limitations
- Severe class imbalance (488 moderate dementia vs. 5000+ other classes) with no explicit class-balancing techniques reported
- Early stopping based on test accuracy may cause data leakage and inflated performance metrics
- Only one colormap (jet) tested without ablation against alternatives
- No independent dataset validation to establish generalizability across different acquisition protocols

## Confidence
- **High confidence**: The general architecture (ViT-base-patch16-224 with pseudo-color preprocessing) is technically sound and reproducible.
- **Medium confidence**: The accuracy/AUC improvements over recent methods are real but likely inflated due to data leakage in early stopping.
- **Low confidence**: The claimed 100% AUC and the specific mechanism of colormap-enhanced transfer learning require independent validation.

## Next Checks
1. Ablation study: Train identical ViT architecture with grayscale channel replication vs. jet pseudo-color on the same data split to isolate colormap contribution.
2. Class-balanced evaluation: Report per-class precision, recall, and confusion matrix, focusing on moderate dementia performance given the 488 vs. 5000+ sample disparity.
3. Colormap sweep: Test alternative colormaps (viridis, plasma, coolwarm) to determine if jet is optimal or if pseudo-coloring itself drives improvements.