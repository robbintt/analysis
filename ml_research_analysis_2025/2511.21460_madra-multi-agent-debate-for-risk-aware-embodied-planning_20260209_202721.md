---
ver: rpa2
title: 'MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning'
arxiv_id: '2511.21460'
source_url: https://arxiv.org/abs/2511.21460
tags:
- agent
- risk
- agents
- task
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of ensuring safety in embodied
  AI agents performing household tasks, where dangerous instructions pose significant
  risks. The proposed method, MADRA, introduces a training-free Multi-Agent Debate
  Risk Assessment framework that leverages collective reasoning to enhance safety
  awareness without sacrificing task performance.
---

# MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning

## Quick Facts
- arXiv ID: 2511.21460
- Source URL: https://arxiv.org/abs/2511.21460
- Reference count: 40
- One-line result: Multi-agent debate achieves >90% unsafe task rejection while reducing safe-task over-rejection to <8%

## Executive Summary
MADRA addresses the critical challenge of ensuring safety in embodied AI agents performing household tasks by introducing a training-free Multi-Agent Debate Risk Assessment framework. The system employs multiple LLM-based agents to collectively debate the safety of instructions, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA achieves over 90% rejection of unsafe tasks while maintaining low rejection of safe tasks, outperforming existing methods in both safety and execution efficiency.

## Method Summary
MADRA is a training-free multi-agent debate framework that uses k=3 debating agents plus 1 Critical Agent to assess instruction safety through structured debate. The Critical Agent scores each response on four dimensions (Logical Soundness 30%, Risk Identification 30%, Evidence Quality 30%, Clarity 10%) and guides iterative revisions. Final decisions use consensus or majority voting. The system integrates with hierarchical planning (High-level → Low-level planner with RAG-based memory) and includes a self-evolution mechanism for failure analysis and replanning (max 3 iterations). Tested models include GPT-3.5, GPT-4o, Deepseek-v3, Llama-3-70B, and Qwen-max on SafeAware-VH benchmark (800 instructions across 10 risk categories).

## Key Results
- MADRA achieves >90% rejection of unsafe tasks while reducing safe-task over-rejection from 33.6% (Safety CoT) to 7.9%
- Four-dimensional scoring framework reduces false rejections by ~5% compared to single-agent approaches
- Self-evolution mechanism improves task success rates by up to 10% across iterations

## Why This Works (Mechanism)

### Mechanism 1: Structured Multi-Agent Debate Reduces Single-LLM Over-Rejection
Multiple agents independently assess risk, then a Critical Agent scores reasoning quality across four dimensions. Agents revise positions based on scored feedback, and final decisions use consensus or majority voting—diluting individual biases. Core assumption: agent diversity and structured scoring will converge toward better calibrations than single-agent prompting.

### Mechanism 2: Four-Dimensional Scoring Constrains Over-Interpretation
The Critical Agent penalizes "over-interpretation" and "imagined virtual scenarios," scoring on Logical Soundness, Risk Identification, Evidence Quality, and Clarity. Scoring anchors reasoning to explicit task semantics. Core assumption: LLMs can reliably identify over-interpretation when explicitly instructed to evaluate it.

### Mechanism 3: Hierarchical Self-Evolution Enables Continuous Improvement
Failed actions trigger Self-Evolution Agent diagnosis across Action Semantics, Object States, and Preconditions. Insights feed back to High-Level Planner for revised strategies, and successful executions add to memory database. Core assumption: Failure diagnoses are accurate and don't compound errors through recursive hallucination.

## Foundational Learning

- **Concept: Over-Rejection vs. True Rejection Trade-off**
  - Why needed here: Safety systems that maximize rejection of unsafe tasks often incorrectly reject safe tasks too. MADRA explicitly optimizes this trade-off.
  - Quick check question: Given a safety system with 90% unsafe-task rejection and 30% safe-task rejection, is it usable? What if safe-task rejection were 5% instead?

- **Concept: Consensus vs. Majority Voting Fallback**
  - Why needed here: MADRA uses consensus for confidence, but falls back to voting when agents disagree—balancing reliability with throughput.
  - Quick check question: Three agents vote [Safe, Unsafe, Unsafe]. What is the final decision? What if they all agree on Unsafe?

- **Concept: Training-Free vs. Preference Alignment**
  - Why needed here: MADRA deliberately avoids fine-tuning (which requires compute and only works for open-source models). Understanding this constraint clarifies design choices.
  - Quick check question: Why might a training-free approach be preferred for proprietary models like GPT-4o?

## Architecture Onboarding

- **Component map**: MADRA Safety Module: k Debate Agents + 1 Critical Agent → Consensus/Majority Decision → Planning Pipeline: Memory Enhancement (RAG) → High-Level Planner → Low-Level Planner → Execution → Feedback Loop: Execution Failure → Self-Evolution Agent → Revised Plans → Retry

- **Critical path**: Input instruction → MADRA risk assessment (if Unsafe, reject immediately) → Memory retrieval → Hierarchical planning → Execution → On failure, self-evolution and replan

- **Design tradeoffs**: More agents (k=5) improve unsafe detection but increase safe-task rejection variability; k=3 is the balance point. Stronger Critical Agent (GPT-4o/GPT-3.5) yields better calibration; weaker models (Llama3) cause over-rejection. More debate rounds improve convergence but increase latency; 95% consensus reached within 3 rounds.

- **Failure signatures**: High safe-task rejection (>30%) → Critical Agent may be too weak or evaluation criteria misapplied. Consensus never reached after 3 rounds → Agents may have incompatible priors; check prompt consistency. Self-evolution degrades after 3 iterations → Overthinking triggers hallucination; cap iterations.

- **First 3 experiments**: 1) Baseline calibration: Run single-agent Safety CoT on SafeAware-VH subset; measure safe/unsafe rejection rates as reference. 2) Agent count sweep: Test k=1,2,3,4,5 debate agents with GPT-3.5 as Critical Agent; plot safe vs. unsafe rejection trade-off. 3) Critical Agent ablation: Fix k=3, swap Critical Agent (GPT-4o vs. GPT-3.5 vs. Llama3); observe safe-task rejection changes.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the integration of multi-modal visual perception affect the risk assessment accuracy of MADRA compared to the current semantic-only approach? (Current framework "focuses on semantic planning without visual integration" and lists developing "end-to-end vision-action models" as future work.)

- **Open Question 2**: How robust is the multi-agent consensus mechanism against "edge-case" scenarios involving ambiguous or adversarial instruction structures? (Paper identifies need to "augment the framework with... edge-case scenarios to enhance robustness" as future research.)

- **Open Question 3**: To what extent does the "simulation-to-reality gap" affect the safety guarantees of the framework when deployed on physical robotic hardware? (Paper explicitly notes the lack of visual integration "creat[es] a simulation-to-reality gap.")

## Limitations

- The SafeAware-VH benchmark dataset is not publicly available, preventing independent verification of performance claims
- Exact prompt templates appear truncated in the paper, making faithful reproduction difficult
- Implementation details like LLM API parameters and memory retrieval thresholds are unspecified
- All experiments conducted in simulated environments, leaving real-world deployment effects unverified

## Confidence

- **High confidence**: Multi-agent debate mechanism's core concept is sound and well-supported by ablation studies
- **Medium confidence**: Four-dimensional scoring framework appears novel and effective, though weak corpus evidence exists
- **Medium confidence**: Self-evolution mechanism shows measurable improvements, but claims about iteration-3 degradation need validation
- **Low confidence**: Absolute performance numbers cannot be verified without the dataset

## Next Checks

1. **Dataset reproduction**: Construct a proxy dataset of 800 household instructions with 10 risk categories and evaluate MADRA vs. Safety CoT baselines to verify the claimed reduction in safe-task over-rejection

2. **Critical Agent strength ablation**: Systematically test MADRA with GPT-4o, GPT-3.5, and weaker models as Critical Agent to validate the claim that agent capability directly impacts over-rejection rates

3. **Debate round convergence**: Measure consensus achievement rates across 1-5 debate rounds to confirm the claimed 95% convergence within 3 rounds and identify the iteration-3 degradation threshold