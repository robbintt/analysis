---
ver: rpa2
title: The Effectiveness of Approximate Regularized Replay for Efficient Supervised
  Fine-Tuning of Large Language Models
arxiv_id: '2512.22337'
source_url: https://arxiv.org/abs/2512.22337
tags:
- arxiv
- learning
- fine-tuning
- forgetting
- matthew
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that standard LoRA fine-tuning can cause catastrophic
  forgetting even on small datasets, with larger models forgetting more. The authors
  address this by combining KL regularization and approximate replay using open web
  data.
---

# The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models

## Quick Facts
- arXiv ID: 2512.22337
- Source URL: https://arxiv.org/abs/2512.22337
- Authors: Matthew Riemer; Erik Miehling; Miao Liu; Djallel Bouneffouf; Murray Campbell
- Reference count: 40
- One-line primary result: Approximate replay alone reduces forgetting by ~3× without harming task performance; combining with KL regularization achieves up to 7× less forgetting with minimal compute overhead.

## Executive Summary
This paper demonstrates that standard LoRA fine-tuning causes catastrophic forgetting even on small datasets, with larger models exhibiting more severe forgetting than smaller ones. The authors propose combining KL regularization and approximate replay using open web data to stabilize learning while maintaining task performance. Their experiments on Qwen models show that KL regularization acts as a Bayesian prior constraining output drift, while replay approximates the pre-training distribution through interleaved next-token prediction. The combined approach achieves up to 7× reduction in forgetting while preserving task-specific plasticity.

## Method Summary
The method combines KL divergence regularization with approximate replay during LoRA fine-tuning. KL regularization penalizes divergence between fine-tuned and base model outputs, functioning as a Bayesian prior. Approximate replay interleaves next-token prediction on OpenWebText data to approximate pre-training distribution. The training objective combines standard cross-entropy loss, KL divergence penalty with coefficient β, and replay loss. The approach uses LoRA adapters (r=32, α=64) on key/value/output matrices, with replay rates ρ∈{0, 1, 3, 7} and KL coefficients β∈{0, 0.001, 0.01, 0.1}.

## Key Results
- Standard LoRA fine-tuning causes catastrophic forgetting (F=15.4) even on small datasets
- Approximate replay alone reduces forgetting by ~3× without hurting task performance
- KL coefficient β=0.01 nearly eliminates forgetting while retaining plasticity
- Combined approach achieves up to 7× reduction in forgetting with minimal compute overhead
- Larger models forget more than smaller models during fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** KL regularization constrains output drift from the base model, functioning as a Bayesian prior that preserves general capabilities.
- **Mechanism:** The KL divergence penalty $-\beta D_{KL}(\pi_{\theta+\phi}||\pi_\theta)$ is mathematically equivalent to a Gaussian prior over weights centered on the base model (via Fisher information approximation). This biases optimization toward solutions that maintain the base model's output distribution while adapting to new tasks.
- **Core assumption:** The base model's output distribution encodes valuable general knowledge that should not be overwritten.
- **Evidence anchors:**
  - [abstract] "penalizes KL divergence with respect to the initial model"
  - [Section 2.4.1] "From Bayesian perspective, learning can be treated as Maximum A Posteriori (MAP) estimation... the negative KL term is equivalent to the log-prior"
  - [corpus] Limited direct corpus evidence on KL regularization for supervised fine-tuning; most related work focuses on RL settings.
- **Break condition:** If β is set too high (e.g., 0.1), plasticity is suppressed—the model cannot adapt to new tasks.

### Mechanism 2
- **Claim:** Approximate replay stabilizes learning by interleaving next-token prediction on open web data, approximating the pre-training distribution.
- **Mechanism:** Fine-tuning on only task-specific data creates biased optimization toward $d_{current}$. Replay samples from a similar-but-different web corpus (OpenWebText) approximate the steady-state distribution $d_{future}$, preventing the model from overfitting to the narrow fine-tuning distribution.
- **Core assumption:** Open web data is sufficiently similar to the original pre-training data to serve as an effective proxy, despite not being identical.
- **Evidence anchors:**
  - [abstract] "interleaves in data for next token prediction from a different, yet similar, open access corpus"
  - [Section 2.3.2] "the experience replay buffer eventually converges to the steady-state distribution"
  - [corpus] GeRe (FMR 0.585) validates replay effectiveness for LLM anti-forgetting, though focuses on general samples rather than web corpus specifically.
- **Break condition:** If replay data distribution differs substantially from pre-training distribution (domain mismatch), stabilization weakens.

### Mechanism 3
- **Claim:** The combination of KL regularization and approximate replay provides complementary constraints—KL constrains outputs while replay stabilizes internal representations.
- **Mechanism:** KL regularization operates on the output distribution level, preventing the model from generating outputs far from the base model. Replay operates at the data/optimization level, ensuring gradients from diverse examples prevent representation collapse. Together they address both output-space and weight-space drift.
- **Core assumption:** The two mechanisms do not interfere negatively and provide additive or multiplicative benefits.
- **Evidence anchors:**
  - [Section 4] "The best results come from combining approximate replay with KL divergence regularization... up to 7× reduction in forgetting"
  - [Table 1] Shows β=0.001 + ρ=3X achieves 2.0 average forgetting vs 15.4 for baseline (7.7× reduction)
  - [corpus] MURR (FMR 0.538) explores regularized replay for document streams but in retrieval context, not LLM fine-tuning.
- **Break condition:** If compute budget is extremely constrained, high replay rates may be impractical; β alone provides a lighter-weight alternative.

## Foundational Learning

- **Concept: Catastrophic Forgetting / Stability-Plasticity Dilemma**
  - Why needed here: The entire paper frames LoRA fine-tuning as a continual learning problem where optimizing on new tasks degrades prior capabilities.
  - Quick check question: Can you explain why updating on task A might harm performance on task B, even with LoRA modifying <1% of parameters?

- **Concept: KL Divergence**
  - Why needed here: The primary regularization mechanism penalizes how much the fine-tuned model's output distribution diverges from the base model.
  - Quick check question: If two distributions have KL divergence of 0, what does that mean? If it's very large?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper specifically addresses forgetting in LoRA fine-tuning; understanding that LoRA can affect all parameters despite small parameter count is critical.
  - Quick check question: If LoRA only learns 0.5% of parameters, why can it still overwrite knowledge in the remaining 99.5%?

## Architecture Onboarding

- **Component map:** Base LLM (frozen parameters θ) -> LoRA adapters (trainable parameters φ) -> KL divergence module -> Replay data loader -> Training loop

- **Critical path:**
  1. Load base model and attach LoRA adapters to key/value/output matrices
  2. Forward pass on SFT batch → compute cross-entropy loss
  3. Forward pass on same batch through frozen base model → compute KL divergence
  4. Sample replay batch from OpenWebText → compute next-token prediction loss
  5. Combine: L_total = L_SFT + β × KL + L_replay
  6. Backprop through LoRA parameters only

- **Design tradeoffs:**
  - β=0.01: Near-zero forgetting, moderate plasticity reduction (~1.8% vs 2.5% gain)
  - β=0.001: Same plasticity as baseline, ~2× less forgetting
  - ρ=1X: Most economical; ρ=3X: Best results; ρ=7X: Diminishing returns
  - Compute overhead: (ρ+1)× for replay; 1.5× for KL (negligible with LoRA)

- **Failure signatures:**
  - Forgetting still high: β too low (≤0.0001) or replay data mismatched
  - No task adaptation: β too high (≥0.1) or LoRA rank too small
  - Training unstable: Learning rate too high or replay distribution too narrow

- **First 3 experiments:**
  1. **Baseline validation:** Standard LoRA fine-tuning (ρ=0, β=0) on a single task → confirm catastrophic forgetting (expect ~15-25% degradation on general benchmarks)
  2. **Ablation—KL only:** β=0.01, ρ=0 → measure plasticity vs forgetting tradeoff
  3. **Combined method:** β=0.001, ρ=1X → target configuration achieving ~2× forgetting reduction with preserved plasticity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does approximate regularized replay perform with Mixture of Experts (MoE) architectures, given that router behavior may confound stability dynamics during fine-tuning?
- Basis in paper: [explicit] "We avoid the use of MoE models in our experiments to avoid this potential conflating factor in the results, but believe this is a promising direction for improving fine-tuning of LLMs while preventing forgetting."
- Why unresolved: MoE routers may not be trained with gradient interference in mind, potentially affecting how replay and KL regularization interact with expert routing decisions during continual learning.
- What evidence would resolve it: Experiments applying approximate regularized replay to MoE LLMs (e.g., Mixtral) with analysis of router stability and expert-specific forgetting patterns.

### Open Question 2
- Question: Why do larger LLMs exhibit more severe catastrophic forgetting during fine-tuning, contradicting prior findings that scale reduces forgetting?
- Basis in paper: [explicit] "An interesting aspect of our results is that we find catastrophic forgetting in fine-tuning to be even worse for bigger models than it is for smaller models. This directly contradicts findings in the work of Ramasesh et al. [78] suggesting that larger models experience less forgetting than small models."
- Why unresolved: The discrepancy may relate to differences between fine-tuning regimes (small datasets, few steps) versus continual pre-training (large datasets), but the mechanistic explanation remains unclear.
- What evidence would resolve it: Systematic studies isolating dataset size, training duration, and model scale as factors, potentially with internal activation analysis comparing representation drift across scales.

### Open Question 3
- Question: How does the choice of approximate replay corpus affect forgetting mitigation, beyond the OpenWebText corpus tested?
- Basis in paper: [inferred] The paper uses OpenWebText as a proxy for pre-training data, noting it is "different, yet similar" to actual pre-training corpora. The sensitivity of results to corpus selection remains unexplored.
- Why unresolved: Domain mismatch between the replay corpus and actual pre-training data could reduce effectiveness; optimal corpus characteristics (domain coverage, size, quality) are not characterized.
- What evidence would resolve it: Ablation experiments using different open corpora (e.g., C4, Pile subsets) with quantification of distributional similarity to base model pre-training data and correlation with forgetting metrics.

## Limitations
- Only tested on five small instruction-tuning tasks rather than longer task sequences or more diverse domains
- KL regularization assumes base model outputs encode valuable general knowledge, which may not hold for all models
- OpenWebText may not accurately approximate actual pre-training data composition
- Fixed hyperparameters across all model sizes may not be optimal for each scale

## Confidence
**High Confidence:** The baseline finding that LoRA causes catastrophic forgetting is well-established through direct comparison (F=15.4 vs F=2.0 for best method). The general effectiveness of KL regularization in reducing forgetting is demonstrated across multiple settings.

**Medium Confidence:** The claim that β=0.01 "nearly eliminates forgetting" while maintaining plasticity is supported but shows some variability across tasks. The superiority of combined KL+replay over individual methods is shown but with overlapping error bars in some cases.

**Low Confidence:** The assertion that approximate replay alone achieves "up to 3× reduction in forgetting" without hurting task performance needs more validation across diverse task types and longer training horizons. The assumption that OpenWebText sufficiently approximates pre-training data remains unverified.

## Next Checks
1. **Sequence Length Test:** Evaluate forgetting over 10+ sequential tasks to assess whether KL+replay maintains effectiveness in true continual learning scenarios, measuring both task-specific and general capabilities.

2. **Domain Mismatch Analysis:** Test replay effectiveness using domains far from web text (e.g., medical, legal) to quantify how distribution shift affects stabilization, measuring FMR degradation as function of domain distance.

3. **Hyperparameter Sensitivity Study:** Systematically vary β and ρ across different model scales (1.5B, 7B, 14B) to identify optimal configurations per scale, measuring the forgetting-plasticity tradeoff curve for each combination.