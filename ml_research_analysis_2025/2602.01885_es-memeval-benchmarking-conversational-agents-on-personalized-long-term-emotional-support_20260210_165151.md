---
ver: rpa2
title: 'ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional
  Support'
arxiv_id: '2602.01885'
source_url: https://arxiv.org/abs/2602.01885
tags:
- user
- long-term
- support
- emotional
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ES-MemEval, a benchmark for evaluating long-term
  memory capabilities of conversational agents in personalized emotional support scenarios.
  The authors construct EvoEmo, a multi-session dataset featuring evolving user states
  and implicit, fragmented disclosures across 18 virtual users.
---

# ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support

## Quick Facts
- arXiv ID: 2602.01885
- Source URL: https://arxiv.org/abs/2602.01885
- Reference count: 40
- Introduces ES-MemEval benchmark and EvoEmo dataset for evaluating long-term memory in personalized emotional support dialogues

## Executive Summary
ES-MemEval introduces a benchmark for evaluating long-term memory capabilities of conversational agents in personalized emotional support scenarios. The authors construct EvoEmo, a multi-session dataset featuring evolving user states and implicit, fragmented disclosures across 18 virtual users. ES-MemEval systematically assesses five core memory abilities—information extraction, temporal reasoning, conflict detection, abstention, and user modeling—through question answering, summarization, and dialogue generation tasks. Experiments reveal that explicit long-term memory reduces hallucinations and improves personalization, while retrieval-augmented generation enhances factual consistency but struggles with temporal dynamics and evolving user states.

## Method Summary
The authors construct EvoEmo, a multi-session emotional support dialogue dataset with 18 virtual users across 401 sessions spanning 14.9 months average. ES-MemEval evaluates five core memory capabilities through three tasks: question answering (1,209 samples), summarization (125 cases), and dialogue generation (34 scenarios). The benchmark tests open-source models (Ministral-8B, Phi-3-Medium-128k, Mistral-Small-24B), commercial models (GPT-3.5-turbo, GPT-4o), and retrieval-augmented generation (RAG) with bge-m3 embeddings and FAISS indexing. Evaluation uses F1, BERTScore, LLM-as-Judge, ROUGE, and event-level metrics.

## Key Results
- RAG improves factual consistency but struggles with temporal dynamics and evolving user states
- Session-level retrieval granularity outperforms turn-level and round-level approaches
- Smaller models (8B) degrade significantly with extended contexts despite nominal 128K token support
- GPT-4o achieves 66.7% abstention rate, dropping to 12.7% with RAG due to overconfidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented memory improves factual consistency but struggles with temporal dynamics
- Mechanism: RAG systems use dense retrieval (bge-m3) to fetch top-k relevant session contexts from a FAISS index, which explicitly grounds model responses in user history rather than relying on parametric memory alone
- Core assumption: Retrieved contexts contain sufficient signal for personalization; ranking quality directly impacts downstream reasoning
- Evidence anchors:
  - [abstract]: "RAG enhances factual consistency but struggles with temporal dynamics and evolving user states"
  - [section 6.1.1]: "RAG enhances factual recall but does not uniformly benefit all capabilities... model performance in user modeling and temporal reasoning remains suboptimal, as F1 scores seldom exceed 20.0"
  - [corpus]: RGMem (arXiv:2510.16392) notes existing RAG approaches "primarily operate at fixed granularities and lack mechanisms for dynamic memory evolution"
- Break condition: When user state evolves rapidly between sessions; when relevant information is sparse across many sessions; when retrieval ranking degrades (NDCG@k remains moderate at 59-63%)

### Mechanism 2
- Claim: Session-level retrieval granularity outperforms turn-level and round-level for emotional support dialogues
- Mechanism: Session-level units preserve conversational coherence and emotional context that spans multiple turns; fragmented disclosures become meaningful only when aggregated
- Core assumption: Emotional support requires understanding user state trajectories, not isolated facts
- Evidence anchors:
  - [abstract]: User information is "dispersed, implicit, and continuously evolving"
  - [section 6.1.2]: "Session-level retrieval is more effective for long-term emotional support scenarios, where relevant information is sparsely distributed and only becomes meaningful when aggregated across multiple conversational turns"
  - [corpus]: Memoria (arXiv:2512.12686) proposes "progressive memory compression" to handle granularity tradeoffs
- Break condition: When sessions contain substantial irrelevant content; when redundancy from full sessions exceeds model processing capacity

### Mechanism 3
- Claim: Smaller long-context models degrade with extended input contexts despite nominal context window claims
- Mechanism: Attention mechanisms distribute computational capacity across all tokens; longer sequences dilute signal-to-noise ratio, causing smaller models to lose relevant information
- Core assumption: Effective context length ≠ claimed context window; model capacity scales with parameter count
- Evidence anchors:
  - [section 6.1.3]: "Although both models nominally support up to 128K tokens, their effective performance substantially deteriorates as input context length increases... Mistral-8B achieves optimal performance at 2K tokens, while Mistral-24B peaks at 8K"
  - [corpus]: Weak direct corpus support for this specific degradation pattern
- Break condition: When using larger models (>24B parameters); when retrieval pre-filters context to relevant segments

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architecture for long-term memory; understanding retrieval granularity and ranking metrics (Recall@k, NDCG@k) is essential for optimizing personalization
  - Quick check question: Given a 15K-token user history, would you retrieve top-4 sessions or top-20 turns, and why?

- Concept: **Temporal Reasoning in Dialogue**
  - Why needed here: EvoEmo requires tracking event sequences, causal dependencies, and state evolution across 14.9 months average span
  - Quick check question: If a user mentions "I'm feeling better about work now" in Session 15, what prior events must the system retrieve to contextualize this?

- Concept: **Conflict Detection and Abstention**
  - Why needed here: Two of five core memory capabilities; models must identify contradictions in user disclosures and withhold responses when information is insufficient
  - Quick check question: A user states "I'm single" in Session 3 but mentions "my partner" in Session 7—should the model flag this, and how?

## Architecture Onboarding

- Component map:
  EvoEmo Dataset -> ES-MemEval Benchmark -> Memory Store (FAISS + bge-m3) -> Retrieval Module -> LLM Generator -> Evaluation Suite

- Critical path:
  1. Load user profile + event timeline from EvoEmo
  2. Retrieve relevant sessions using dense retrieval (session-level, k=4)
  3. Construct prompt with retrieved context + current query
  4. Generate response with target LLM
  5. Evaluate using task-specific metrics (QA: F1/BERTScore/LLM-judge; Summarization: ROUGE/event-F1; Dialogue: observation recall/weighted accuracy)

- Design tradeoffs:
  - Session-level vs turn-level retrieval: Session preserves coherence but risks redundancy; turn-level precision improves but loses context
  - Full-history vs RAG: Full-history ensures completeness but degrades smaller models; RAG improves factual grounding but struggles with temporal reasoning
  - Commercial vs open-source: GPT-4o superior in abstention (66.7% vs ~15%) but RAG narrows gap (Mistral-24B + RAG: 1.27 LLM-score vs GPT-4o + RAG: 1.33)

- Failure signatures:
  - Hallucination without explicit memory (No-Mem condition: models "tend to hallucinate plausible user experiences")
  - Temporal reasoning degradation (F1 <20% even with RAG)
  - Context length degradation for models <24B (Mistral-8B optimal at 2K, degrades beyond)
  - Abstention failure with RAG (GPT-4o abstention drops from 66.7% to 12.7% with RAG—overconfidence from retrieved content)

- First 3 experiments:
  1. **Baseline RAG comparison**: Test Mistral-8B, Mistral-24B, and GPT-4o with session-level retrieval (k=4) on QA task across all five capabilities; expect RAG to improve IE but struggle on TR/UM
  2. **Context length ablation**: Evaluate Mistral-24B at 2K, 8K, 20K context windows with and without RAG; expect performance degradation without RAG at 20K, mitigation with RAG
  3. **Granularity study**: Compare session-level vs round-level vs turn-level retrieval for Mistral-24B on dialogue generation task; measure observation recall and weighted accuracy; expect session-level (k=4) to outperform

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can retrieval-augmented generation (RAG) mechanisms be calibrated to better handle temporal dynamics and evolving user states rather than merely retrieving static facts?
- Basis in paper: [explicit] The authors observe that while RAG improves factual consistency, it "struggles with temporal dynamics and evolving user states," explicitly calling for "retrieval-aware calibration" in the conclusion.
- Why unresolved: Current retrieval methods (e.g., BGE-M3) rely on semantic similarity, often failing to prioritize the recency or causal progression of events required for user modeling.
- What evidence would resolve it: A retrieval mechanism that incorporates temporal weighting or causal ordering, resulting in significantly higher scores in the "Temporal Reasoning" and "User Modeling" categories of the benchmark.

### Open Question 2
- Question: Do the findings regarding hallucination and memory utility in the synthetic EvoEmo dataset generalize to real-world, human-to-human emotional support interactions?
- Basis in paper: [explicit] In the Limitations section, the authors acknowledge the dataset is synthetic and "may still diverge from real-world conversational dynamics," identifying this as a key future direction.
- Why unresolved: GPT-4o generated the dialogues based on profiles; real human trauma disclosures may be more ambiguous, implicit, or linguistically distinct than synthetic data.
- What evidence would resolve it: A comparative study evaluating the same LLM agents on a dataset of genuine, longitudinal counseling sessions with human annotators.

### Open Question 3
- Question: Can alternative memory granularities, such as compressed contexts or atomic user observations, outperform the session-level retrieval found optimal in this study?
- Basis in paper: [explicit] The authors note they "did not explore... finer-grained memory units (e.g., user observations... or compressed contexts)" in their experiments, listing this as a limitation.
- Why unresolved: While session-level retrieval performed best in the current setup, it introduces noise; compressed or atomic units might reduce context length while retaining critical details.
- What evidence would resolve it: Benchmark results showing that extracting specific "user observations" or "dialogue summaries" as retrieval units yields higher Recall@k and LLM-as-Judge scores than full-session retrieval.

## Limitations

- Dataset generalization: EvoEmo's 18 virtual users may not capture full diversity of real-world emotional support scenarios
- Temporal reasoning gap: Persistent low F1 scores (<20%) indicate fundamental architectural limitations in tracking evolving user states
- Context window degradation: Smaller models exhibit substantial performance degradation beyond 2K tokens despite nominal 128K token support

## Confidence

**High Confidence Claims**:
- Retrieval-augmented memory consistently improves factual recall and reduces hallucinations across all model sizes
- Session-level retrieval granularity outperforms finer-grained approaches for emotional support dialogues
- Smaller models (<24B parameters) exhibit substantial performance degradation with extended contexts

**Medium Confidence Claims**:
- RAG struggles specifically with temporal dynamics and evolving user states
- RAG's impact on commercial models differs qualitatively from open-source models (improved factual consistency but reduced abstention)
- The five-core capability framework comprehensively captures long-term memory challenges

**Low Confidence Claims**:
- EvoEmo represents the "first" long-term emotional support dialogue dataset (though related work exists)
- The specific parameter configurations (k=4 retrieval, session-level granularity) are optimal across all scenarios

## Next Checks

1. **Cross-Dataset Validation**: Test the same RAG architecture and evaluation framework on real-world emotional support datasets to assess ecological validity and generalization beyond synthetic EvoEmo users.

2. **Temporal Reasoning Architecture Study**: Design and evaluate specialized temporal reasoning modules (e.g., temporal attention mechanisms, event sequence modeling) specifically for tracking user state evolution across sessions.

3. **Commercial Model Internal Analysis**: Investigate why RAG reduces abstention rates in GPT-4o—conduct ablation studies on retrieved content influence, temperature sensitivity, and confidence calibration mechanisms.