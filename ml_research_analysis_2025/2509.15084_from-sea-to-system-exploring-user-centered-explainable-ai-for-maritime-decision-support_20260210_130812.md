---
ver: rpa2
title: 'From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision
  Support'
arxiv_id: '2509.15084'
source_url: https://arxiv.org/abs/2509.15084
tags:
- maritime
- trust
- systems
- survey
- explainable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support

## Quick Facts
- arXiv ID: 2509.15084
- Source URL: https://arxiv.org/abs/2509.15084
- Authors: Doreen Jirak; Pieter Maes; Armeen Saroukanoff; Dirk van Rooy
- Reference count: 17
- Key outcome: None (ongoing study)

## Executive Summary
This paper presents a conceptual framework for evaluating user-centered Explainable AI (XAI) in maritime decision support systems. The authors propose a survey-based experiment to assess how transparency features affect trust, usability, and adoption willingness among maritime professionals. The study addresses the gap between AI performance and user trust in high-stakes navigational contexts, proposing domain-specific scenarios drawn from real collision avoidance situations.

## Method Summary
The study employs a within-subject experimental design comparing baseline (user-only) and XAI (AI decision + significant features) conditions across maritime radar-based scenarios. Participants complete pre-questionnaires on technology disposition and trust propensity, engage with simulated radar tasks depicting collision avoidance scenarios, and provide post-questionnaire responses on satisfaction, trust, and openness to adoption. The methodology acknowledges the absence of real system interaction and adapts XAI-specific metrics from Hoffman et al. [6] rather than standard usability scales.

## Key Results
- Data collection is ongoing; no empirical results presented yet
- Framework established for maritime-specific XAI evaluation
- Identified critical design considerations for XAI in high-stakes operational contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XAI features may improve user trust and willingness to adopt AI-assisted systems in maritime operations, conditional on proper calibration to user cognitive resources.
- Mechan: Transparency features (showing decision rationale + significant features) → reduced black-box opacity → improved cognitive trust (perceived competence, predictability) and affective trust (emotional comfort, alignment) → increased adoption willingness.
- Core assumption: Maritime professionals will actually engage with and value explanations; current assumption based on general XAI literature, not yet validated in maritime domain.
- Evidence anchors:
  - [abstract] "trust in AI depends not only on performance but also on transparency and interpretability"
  - [section] Page 3: trust encompasses "cognitive trust (e.g., perceived competence, reliability, and predictability)" and "affective trust (e.g., feelings of safety, alignment with human intentions)"
  - [corpus] Van de Merwe et al. (neighbor ref [14]) showed agent transparency can improve situational awareness but is "a function of traffic complexity and richness of the agent's reasoning"
- Break condition: If explanations add cognitive load without actionable insight, trust may decrease rather than increase (over-reliance prevention noted in Buçinca et al. [3]).

### Mechanism 2
- Claim: Domain-specific scenario framing (COLREG-aligned radar tasks) is necessary for valid XAI evaluation in maritime contexts.
- Mechan: Realistic radar-based collision avoidance scenarios → activates domain expertise and professional mental models → enables meaningful assessment of whether XAI aligns with operational routines and regulations → produces ecologically valid trust/usability judgments.
- Core assumption: Paper's scenario design accurately represents real maritime decision contexts; not yet empirically verified.
- Evidence anchors:
  - [section] Page 4: "scenarios drawn from daily maritime routines on a vessel, e.g., collision avoidance" using "radar images selected based on critical time steps, for instance, when COLREG rules do not apply"
  - [section] Page 2: maritime decisions shaped by "technical constraints but also by weather conditions, team dynamics, and longstanding seafaring practices"
  - [corpus] Madsen et al. [10] found agent transparency "did not improve all SAW levels uniformly and that information display needs calibration to the human cognitive resources"
- Break condition: If scenarios are too simplified or fail to capture dynamic team workflows, results won't generalize to actual bridge operations.

### Mechanism 3
- Claim: Survey-based experimental comparison (baseline vs. XAI condition) can capture differential effects of explainability on perceived usefulness and trust.
- Mechan: Within-subject design with pre-questionnaire (disposition to technology, propensity to trust) → scenario exposure (baseline action suggestion first, then maritime assistant output with explanation) → post-questionnaire (satisfaction, trust, openness) → isolates XAI's marginal contribution.
- Core assumption: Self-reported trust and satisfaction correlate with actual behavioral adoption in real operations.
- Evidence anchors:
  - [section] Page 4: Figure 1 shows experimental flow; "after obtaining the sailor's responses about their action suggestion (baseline condition), we present the output of a 'maritime assistant' displaying its decision and the most significant feature"
  - [section] Page 3: "xAI metrics like suggested in [6] apply and can be merged together with individual, scenario-tailored items"
  - [corpus] Hoffman et al. [6] provides validated measures for "explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance"
- Break condition: If participants give socially desirable responses or lack engagement with hypothetical scenarios, measured effects won't reflect real-world behavior.

## Foundational Learning

- Concept: Trust calibration in automation (Mayer's integrative model; cognitive vs. affective trust)
  - Why needed here: Paper assumes trust is multi-dimensional and develops over time; understanding this is prerequisite to interpreting survey results and designing explanations that address both competence perceptions and emotional comfort.
  - Quick check question: Can you distinguish between "I trust this system because it's predictable" (cognitive) and "I feel safe using this system" (affective)?

- Concept: Situation Awareness (SAW) levels (Endsley's three levels: perception, comprehension, projection)
  - Why needed here: Neighbor research [10, 14] shows XAI transparency affects SAW levels unevenly; maritime decision support must account for how explanations support or overload each level.
  - Quick check question: When reviewing a radar display with AI suggestions, which SAW level is primarily supported by showing "significant features" vs. showing predicted vessel trajectories?

- Concept: COLREGs (International Regulations for Preventing Collisions at Sea)
  - Why needed here: Scenario design hinges on situations where COLREGs apply vs. don't apply; evaluating whether AI "interprets these rules and adapts in real time" requires basic rule familiarity.
  - Quick check question: In a head-on collision scenario, which COLREG rule governs the required action, and would an AI explanation need to reference this explicitly?

## Architecture Onboarding

- Component map:
  - Pre-questionnaire → disposition to technology, propensity to trust (RQ1)
  - Scenario stimuli → radar images at critical decision points (collision avoidance, COLREG edge cases)
  - Baseline condition → participant action suggestion (no AI)
  - XAI condition → maritime assistant displays decision + significant features
  - Post-questionnaire → trust, satisfaction, openness, perceived usefulness (RQ2, RQ3)
  - Demographics → rank, years at sea (collected post-survey to avoid bias)
  - Open questions → concerns, expectations about AI integration

- Critical path: Scenario presentation quality determines validity. Radar images must be realistic, time-step selection must capture genuine decision moments, and XAI explanations must show meaningful features—not just confidence scores.

- Design tradeoffs:
  - No real system interaction → can't use standard SUS/usability scales; must adapt XAI-specific metrics (Hoffman et al. [6])
  - Hypothetical scenarios → ecological validity vs. experimental control
  - Demographics after survey → avoids priming bias but limits pre-stratification
  - Open questions added → qualitative depth but increases analysis complexity

- Failure signatures:
  - Participants treat survey as abstract exercise, not reflecting real operational concerns
  - XAI explanations too generic (e.g., "high confidence") to be informative
  - Trust measures don't differentiate cognitive vs. affective dimensions
  - Scenario complexity too low/high relative to participant expertise (captain vs. cadet)
  - Recruitment bias toward tech-enthusiastic seafarers (self-selection)

- First 3 experiments:
  1. Pilot test scenarios with 5-10 maritime professionals to verify ecological validity (realism of radar images, appropriateness of decision points, clarity of XAI feature highlighting) before full deployment.
  2. Manipulation check: Does the XAI condition actually increase perceived transparency? Compare post-scenario ratings between participants who saw explanations vs. those who only saw AI decisions without rationale.
  3. Subgroup analysis plan: Pre-register analysis comparing trust effects across experience levels (junior vs. senior officers) and roles (navigators vs. engineers) to identify whether XAI design needs differ by user type.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the inclusion of explainability features (XAI) affect user satisfaction, trust, and willingness to adopt AI-assisted systems in maritime navigation?
- Basis in paper: [explicit] The authors list this as Research Question 2 (RQ2) in Section 3 (Survey Design).
- Why unresolved: The paper describes the data collection as "ongoing" and focuses on the conceptual framework rather than presenting results.
- What evidence would resolve it: Statistical analysis of post-questionnaire data comparing the baseline condition against the XAI "maritime assistant" condition.

### Open Question 2
- Question: How do explainability preferences vary across different maritime roles (e.g., navigators vs. engineers) and levels of AI familiarity?
- Basis in paper: [explicit] The conclusion states the authors "anticipate expanding this research to explore how explainability preferences vary across roles... and levels of AI familiarity" (Page 4).
- Why unresolved: This is identified as a future expansion of the current work, distinct from the immediate survey objectives.
- What evidence would resolve it: A comparative study analyzing survey responses segmented by specific job functions and prior experience with autonomous systems.

### Open Question 3
- Question: Can XAI metrics be effectively merged with scenario-tailored items to measure usability in the absence of real-time interaction?
- Basis in paper: [inferred] The authors note that "standard scales for system usability do not apply due to absence of real interactions" and propose merging XAI metrics with custom items (Page 4).
- Why unresolved: The paper provides the design rationale but does not yet validate if this hybrid approach successfully captures usability in a simulated environment.
- What evidence would resolve it: Validation of the survey instrument showing that the tailored items correlate with observed user performance or qualitative feedback.

### Open Question 4
- Question: Does agent transparency improve situational awareness uniformly, or is it dependent on traffic complexity and the richness of the agent's reasoning?
- Basis in paper: [inferred] The literature review cites conflicting findings (Merwe et al.) suggesting transparency's effect on situational awareness is variable, establishing the gap this study addresses (Page 3).
- Why unresolved: The paper highlights this ambiguity in the field as motivation, though the current study focuses on trust and usability.
- What evidence would resolve it: Experimental data correlating the complexity of the radar scenarios with self-reported or measured situational awareness scores.

## Limitations

- No empirical results yet; study design presented but not validated
- Self-reported measures may not correlate with actual behavioral adoption
- Hypothetical scenarios may lack ecological validity compared to real bridge operations

## Confidence

High confidence in methodology soundness and theoretical framework alignment. Medium confidence in ecological validity assumptions. Low confidence in generalizability without empirical validation.

## Next Checks

1. Pilot study with 5-10 maritime professionals to validate scenario realism and XAI feature clarity
2. Manipulation check comparing transparency perception between baseline and XAI conditions
3. Pre-registered subgroup analysis plan for experience-level and role-based differences in XAI preferences