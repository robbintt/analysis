---
ver: rpa2
title: 'DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature
  Grids'
arxiv_id: '2601.10715'
source_url: https://arxiv.org/abs/2601.10715
tags:
- grid
- feature
- neural
- equation
- interpolation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new neural differential equation solver\
  \ called \u2202\u221E-Grid that uses a differentiable grid-based representation\
  \ to solve PDEs more efficiently than existing methods. Traditional neural solvers\
  \ like Siren are accurate but slow, while grid-based methods like Instant-NGP train\
  \ faster but cannot compute higher-order derivatives needed for solving differential\
  \ equations."
---

# DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids

## Quick Facts
- arXiv ID: 2601.10715
- Source URL: https://arxiv.org/abs/2601.10715
- Reference count: 28
- Primary result: Achieves 5-20× speedup over coordinate-based MLP methods for solving differential equations while maintaining accuracy

## Executive Summary
This paper introduces ∂∞-Grid, a neural differential equation solver that uses differentiable feature grids with Gaussian RBF interpolation to solve PDEs more efficiently than existing methods. Traditional neural solvers like Siren are accurate but slow, while grid-based methods like Instant-NGP train faster but cannot compute higher-order derivatives needed for solving differential equations. The authors address this by combining feature grids with smooth RBF interpolation, which is infinitely differentiable, and introduce a multi-resolution design to capture high-frequency signals and improve gradient propagation. Experiments on tasks such as image reconstruction, Helmholtz equations, and cloth simulation show that ∂∞-Grid achieves 5-20× speedups over coordinate-based MLP methods while maintaining accuracy.

## Method Summary
∂∞-Grid uses multi-resolution feature grids with Gaussian RBF interpolation to enable efficient neural PDE solving. The method stores learnable features at grid nodes, uses RBF interpolation (φ(r) = exp(-(εr)²)) to compute smooth interpolation weights that support higher-order derivatives, and employs multi-scale grids to accelerate global gradient propagation. A small decoder (linear layer or MLP) maps interpolated features to output values. The model is trained by minimizing PDE residuals as loss functions without ground-truth field supervision, using autograd to compute derivatives through both the decoder and RBF interpolation weights.

## Key Results
- Achieves 5-20× training speedup over coordinate-based MLP methods like Siren
- Successfully solves challenging PDEs including Kirchhoff-Love plate equations and Eikonal equations
- Maintains accuracy while being significantly faster - image reconstruction from 10 minutes to 25 seconds
- Outperforms both traditional neural solvers and hybrid grid methods on multiple PDE benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Gaussian RBF interpolation enables computation of higher-order derivatives
- Core assumption: PDEs require second-order or higher derivatives computed through interpolation
- Evidence: Linear interpolation is only C⁰ at boundaries and C¹ in interiors, causing second derivatives to vanish; Gaussian RBFs are infinitely differentiable
- Break condition: If PDE only requires first-order derivatives, linear interpolation may suffice

### Mechanism 2: Multi-resolution grids accelerate global gradient propagation
- Core assumption: Solution fields contain separable multi-scale structure with low-frequency global components and high-frequency local details
- Evidence: Single-resolution grids require many optimization steps for global gradient propagation due to small neighborhoods; multi-resolution provides coarse levels with larger receptive fields
- Break condition: If solution is primarily high-frequency with minimal global structure

### Mechanism 3: Feature grid encoding localizes parameter updates
- Core assumption: Physical fields exhibit spatial locality where nearby points have correlated values
- Evidence: Coordinate-based MLPs require backpropagation through all weights for every point; feature grids update only neighboring parameters
- Break condition: If field lacks spatial locality or has discontinuous structure

## Foundational Learning

- **Radial Basis Function (RBF) Interpolation**
  - Why needed: Understanding how Gaussian RBFs provide smooth, infinitely differentiable interpolation weights
  - Quick check: Explain why φ(r) = exp(-(εr)²) has continuous derivatives of all orders, and what happens to the effective neighborhood as ε decreases

- **Physics-Informed Loss Formulation**
  - Why needed: Method trains by minimizing PDE residuals as loss functions without ground-truth supervision
  - Quick check: For PDE F(x, u, ∇u, ∇²u) = 0, what does loss L = ∫_Ω |F(·)|² dx optimize?

- **Automatic Differentiation Through Interpolation**
  - Why needed: Method relies on autograd to compute ∇u, ∇²u through both decoder and RBF interpolation weights
  - Quick check: Can autograd compute second derivatives through RBF interpolation step?

## Architecture Onboarding

- Component map:
Input coordinates x ∈ R^d → Multi-resolution feature grids {F^s} → RBF interpolation per scale → Concatenate multi-scale features → Decoder d(·; Θ) → Output field u(x) → PDE loss via autograd

- Critical path:
1. Stratified sampling of coordinates across domain with boundary/initial condition points
2. Precompute neighborhood indices N_ρ(x) for all samples
3. For each scale, gather features from (2ρ)^d neighboring grid nodes and compute RBF weights
4. Concatenate multi-scale features, pass through decoder
5. Apply boundary constraints via distance weighting
6. Compute PDE residual derivatives via autograd, accumulate loss
7. Backpropagate to update grid features and decoder weights

- Design tradeoffs:
  - Shape parameter ε: Smaller ε → smoother interpolation, larger neighborhood, higher compute cost
  - Number of scales S: More scales improve global gradient flow but increase parameters
  - Grid resolution N_max: Higher resolution captures finer details but increases memory quadratically
  - Decoder complexity: Linear layer sufficient for simple fields; small MLP with tanh for complex mappings

- Failure signatures:
  - Visible discontinuities at grid cell boundaries → ε too large or ρ too small
  - Slow convergence with poor global structure → Missing coarse grid levels
  - High-frequency artifacts → Grid resolution too low for target frequency content
  - Excessive smoothing → RBF bandwidth too wide
  - Boundary errors → Runge's phenomenon inherent to RBF methods

- First 3 experiments:
  1. Image reconstruction from gradient field (Poisson equation) - supervise with ∇u, compare convergence speed against Siren
  2. Helmholtz equation with single point source - verify complex field reconstruction, compare against Hankel function solution
  3. Ablation: single-resolution vs multi-resolution - run identical PDE with S=1 and S=4, plot PSNR/convergence over time

## Open Questions the Paper Calls Out

### Open Question 1: High-dimensional scalability
- Question: Can planar projection strategies effectively mitigate the curse of dimensionality in high-dimensional domains?
- Basis: Appendix I suggests exploring planar projection strategies to improve scalability in high-dimensional domains
- Evidence needed: Successful application to 4D+ spatio-temporal problems showing reduced memory footprints and convergence times comparable to lower-dimensional cases

### Open Question 2: Comparison with traditional solvers
- Question: How does computational efficiency compare to traditional numerical solvers like multi-grid methods?
- Basis: Appendix I notes the method might not be as efficient as multi-grid methods, requiring further investigation
- Evidence needed: Benchmarks against multi-grid solvers on standard PDEs measuring wall-clock time and FLOPS to convergence

### Open Question 3: Mitigating boundary errors
- Question: Can boundary errors caused by Runge's phenomenon be mitigated to improve reconstruction accuracy?
- Basis: Appendix I identifies boundary errors (Runge's phenomenon) as a well-studied drawback of RBF methods
- Evidence needed: Comparative analysis showing reduced MSE specifically at domain boundaries using adaptive kernel shapes or boundary-specific loss terms

## Limitations
- No rigorous proof that linear interpolation fundamentally cannot support second-order derivatives
- Method's performance on highly anisotropic or discontinuous fields (shocks, interfaces) is untested
- Grid memory scaling (O(N^d)) for high-dimensional problems is acknowledged but not quantitatively analyzed
- No theoretical convergence guarantees or error bounds for RBF-grid approximation

## Confidence

- RBF interpolation enabling higher-order derivatives: **Medium** - Mechanism sound but claim lacks rigorous proof
- Multi-resolution grids accelerating global gradient propagation: **Medium** - Supported by ablation in image reconstruction but not systematically tested across all PDE tasks
- 5-20× speedup over MLPs: **High** - Well-supported by Table 1 with clear baselines and consistent timing measurements
- General applicability to challenging PDEs: **High** - Successful results across diverse problem types with quantitative metrics

## Next Checks

1. **Ablation study**: Replace Gaussian RBF with linear interpolation while keeping multi-resolution design. Measure if second-order derivatives become zero or numerically unstable, confirming the critical role of RBF smoothness.

2. **Boundary behavior test**: Solve Poisson equation on domains with sharp corners or discontinuous boundary conditions. Visualize solution quality near boundaries to detect RBF-induced smoothing artifacts or Runge's phenomenon.

3. **Dimensional scaling experiment**: Fix total parameters and vary dimensionality (2D→3D→4D) while measuring accuracy and memory usage. Quantify how RBF-grid scaling compares to MLP coordinate-based approaches in high dimensions.