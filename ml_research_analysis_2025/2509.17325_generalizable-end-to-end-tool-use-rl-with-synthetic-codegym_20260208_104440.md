---
ver: rpa2
title: Generalizable End-to-End Tool-Use RL with Synthetic CodeGym
arxiv_id: '2509.17325'
source_url: https://arxiv.org/abs/2509.17325
tags:
- training
- environment
- arxiv
- codegym
- environments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeGym addresses the limited generalization of LLM agents trained
  on static trajectories by synthesizing diverse, verifiable multi-turn tool-use environments
  from coding problems. The method abstracts reusable functions from coding solutions
  into callable tools, enabling RL agents to explore varied workflows interactively
  rather than relying on static demonstrations.
---

# Generalizable End-to-End Tool-Use RL with Synthetic CodeGym

## Quick Facts
- arXiv ID: 2509.17325
- Source URL: https://arxiv.org/abs/2509.17325
- Reference count: 40
- Qwen2.5-32B-Instruct trained on CodeGym achieves 8.7 absolute accuracy gain on τ-bench benchmark

## Executive Summary
CodeGym addresses the limited generalization of LLM agents trained on static trajectories by synthesizing diverse, verifiable multi-turn tool-use environments from coding problems. The method abstracts reusable functions from coding solutions into callable tools, enabling RL agents to explore varied workflows interactively rather than relying on static demonstrations. Across multiple model sizes, training in CodeGym consistently improves out-of-distribution generalization, with Qwen2.5-32B-Instruct achieving an absolute accuracy gain of 8.7 points on the τ-bench benchmark.

## Method Summary
CodeGym synthesizes tool-use RL environments from coding problems by extracting atomic functions from solutions and rewriting them as callable tools in interactive POMDP environments. The framework uses LLM-based synthesis to generate Gymnasium environments with unit-test verification, applies quality filtering for complexity and difficulty, and trains agents via distributed GRPO. The pipeline transforms static coding problems into verifiable, multi-turn workflows where agents must compose tool sequences to solve tasks, with binary rewards from unit test outcomes providing stable RL signals.

## Key Results
- Qwen2.5-32B-Instruct trained on CodeGym achieves 8.7 absolute accuracy gain on τ-bench OOD benchmark
- RL training consistently outperforms SFT on OOD tasks across model sizes (7B, 32B, 72B)
- Quality filtering (10-256 tool calls, ≤25% pass rate) improves training efficiency without reducing task diversity

## Why This Works (Mechanism)

### Mechanism 1: Synthetic-to-Real Transfer via Code Structure
Training on tool-use environments derived from coding problems improves generalization to unseen real-world agent workflows because code execution logic mirrors real-world workflow patterns (conditional branching, iteration, state management). The verifiable binary reward from unit tests provides stable RL signal for learning compositional strategies.

### Mechanism 2: RL Exploration Over Static Imitation
Reinforcement learning with active exploration generalizes better than supervised fine-tuning because RL exposes agents to a wider trajectory distribution through environment feedback on both successes and failures. This iterative policy refinement enables more robust strategies than static trajectory imitation.

### Mechanism 3: Quality Filtering for Efficient RL
Filtering environments by tool-use complexity and difficulty improves training efficiency by focusing compute on non-trivial, learnable tasks. Environments with 10-256 tool calls and ≤25% pass rate with base models ensure resources target challenging but solvable instances.

## Foundational Learning

- **POMDP (Partially Observable Markov Decision Process)**: CodeGym environments are formalized as POMDPs with state, action, transition, reward, and observation functions. Quick check: Can you explain why partial observability matters for preventing shortcut solutions?

- **RL with Verifiable Rewards (RLVR)**: The training loop relies on binary rewards from unit test outcomes. Quick check: How would training change if rewards were dense instead of sparse binary?

- **GRPO (Group Relative Policy Optimization)**: The paper uses GRPO for RL training, a variant of PPO designed for stability. Quick check: What is the role of the clip ratio (0.2 in this setup) in preventing policy degradation?

## Architecture Onboarding

- **Component map**: Gym Synthesis -> Gym Verification -> Quality Filtering -> RL Training Pipeline
- **Critical path**: Collect coding problems → Synthesize environments via LLM → Verify solvability → Filter environments → Train agent with GRPO → Evaluate on benchmarks
- **Design tradeoffs**: Scalability vs. quality (large-scale synthesis risks noise), Short-CoT vs. long-CoT models (reasoning bypass), Exploration vs. exploitation (high temperature encourages exploration)
- **Failure signatures**: Environment crashes (malformed tool calls), Policy collapse (repetitive tool calls), Overfitting (training reward plateaus)
- **First 3 experiments**: 1) Baseline replication on Qwen2.5-32B-Instruct, 2) Ablation on filtering effectiveness, 3) SFT vs. RL comparison on generalization

## Open Questions the Paper Calls Out

- **Theoretical guarantees against reasoning shortcuts**: Developing methods to synthesize environments with theoretical guarantees that prevent LLMs from exploiting reasoning shortcuts remains an important direction for future work.

- **Combining reasoning with tool use**: Further investigation of combining reasoning objectives with CodeGym training may reveal complementary benefits for both reasoning accuracy and interactive tool use performance.

- **Mapping deterministic logic to stochastic workflows**: The deterministic logic of coding problems may not fully map to the stochastic or ambiguous nature of real-world workflows.

## Limitations

- Limited theoretical justification for transfer assumptions between code logic and real-world workflows
- No rigorous ablation studies on domain-specific transfer across different workflow types
- Quality filtering criteria are empirically derived without theoretical grounding

## Confidence

- **High Confidence**: In-domain training improvements and OOD benchmark results on τ-bench and ALFWorld
- **Medium Confidence**: RL vs. SFT superiority and quality filtering benefits
- **Low Confidence**: Transfer mechanism assumptions and scalability claims

## Next Checks

1. Replicate the full pipeline with alternative LLM models (e.g., GPT-4) to test prompt dependency and model generality
2. Conduct systematic ablation studies varying complexity filters to identify optimal settings
3. Test transfer to completely different domains (e.g., creative writing, medical diagnosis) to validate structural similarity assumptions