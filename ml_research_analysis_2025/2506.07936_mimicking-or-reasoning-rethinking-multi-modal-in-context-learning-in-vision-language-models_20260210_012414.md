---
ver: rpa2
title: 'Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language
  Models'
arxiv_id: '2506.07936'
source_url: https://arxiv.org/abs/2506.07936
tags:
- reasoning
- answer
- arxiv
- shot
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-examines the assumption that vision-language models
  (VLMs) can perform genuine multimodal in-context learning (MM-ICL). Under various
  controlled conditions, the authors find that VLMs often fail to learn meaningfully
  from demonstrations, relying instead on shallow heuristics such as copying or majority
  voting.
---

# Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models

## Quick Facts
- arXiv ID: 2506.07936
- Source URL: https://arxiv.org/abs/2506.07936
- Reference count: 40
- Primary result: VLMs often rely on shallow heuristics rather than genuine task understanding in multi-modal in-context learning

## Executive Summary
This paper re-examines the assumption that vision-language models (VLMs) can perform genuine multimodal in-context learning (MM-ICL). Under various controlled conditions, the authors find that VLMs often fail to learn meaningfully from demonstrations, relying instead on shallow heuristics such as copying or majority voting. To strengthen the test, they propose a new MM-ICL with Reasoning pipeline that augments demonstrations with rationales, but even with this enhancement, models show limited sensitivity to shot count, retrieval method, and rationale quality. The results suggest that current VLMs lack true MM-ICL capabilities and do not effectively leverage demonstration-level information as intended.

## Method Summary
The study evaluates MM-ICL through four protocols that vary demonstration format (answer-only vs answer+reasoning) and expected output format alignment. Support sets are selected via multimodal retrieval or random sampling, with in-distribution (ID) and out-of-distribution (OOD) splits to distinguish memorization from generalization. The approach generates pseudo-reasoning for demonstrations using model outputs, optionally filtering incorrect samples or injecting ground-truth rationales. Experiments span perception datasets (TextVQA, OK-VQA) and reasoning datasets (ScienceQA, A-OKVQA, M3CoT) across base models (Qwen2.5-VL, InternVL2.5, Llama-3.2V) and reasoning variants (VLM-R1, VL-Rethinker, InternVL-MPO, LLaVA-CoT).

## Key Results
- VLMs show minimal performance improvement with more demonstrations, often plateauing or degrading beyond 2-4 shots
- Input-based retrieval consistently underperforms random selection for reasoning models due to incoherent rationale structures
- Improving rationale quality through filtering or ground-truth injection does not consistently improve performance
- Format consistency between demonstrations and expected output provides moderate gains for reasoning models

## Why This Works (Mechanism)

### Mechanism 1: Format Consistency Aligns Context with Expected Output
- Claim: When demonstration format matches the model's expected output format, VLM reasoners show improved performance, especially at higher shot counts.
- Mechanism: Consistent formatting (rationale + answer in demos, rationale + answer expected) reduces cognitive interference from format mismatch, allowing the model to focus on content rather than structural adaptation.
- Core assumption: VLM reasoners trained to generate structured reasoning outputs benefit from seeing the same structure in demonstrations.
- Evidence anchors:
  - [abstract] "We propose a new MM-ICL with Reasoning pipeline that augments each demonstration with a generated rationale alongside the answer."
  - [section 6.1, Table 2] Consistent formatting outperforms inconsistent formatting across models and datasets, with gains up to +8.71 on M3CoT at 8-shot.
  - [corpus] Related work (Rethinking Invariance in In-context Learning) shows ICL sensitivity to prompt ordering; this paper extends to format consistency for reasoning.

### Mechanism 2: Input-Based Retrieval Harms Reasoning Model Coherence
- Claim: Multimodal retrievers that select demonstrations based on input similarity benefit base models but harm reasoning models.
- Mechanism: Base models copy answer patterns from similar inputs; reasoning models receive coherent inputs but incoherent reasoning paths across demonstrations, creating confusion rather than learning signal.
- Core assumption: Reasoning paths are not predictable from input similarity—similar questions can have vastly different rationale structures.
- Evidence anchors:
  - [section 6.3] "MM-retriever consistently underperforms compared to random selection, especially on reasoning-intensive datasets" for reasoning models.
  - [section 6.3] "Even when the input similarity is high, the corresponding rationales can be diverse in content, structure, and logic chain."
  - [corpus] Weak direct evidence in corpus for this specific retrieval-reasoning interaction; primarily established by this paper.

### Mechanism 3: Shallow Heuristic Dominance Over True Learning
- Claim: Current VLMs exploit surface-level patterns (answer format, majority voting, copying) rather than internalizing task strategies from demonstration content.
- Mechanism: Models optimize for pattern completion based on shallow correlations rather than reasoning chain comprehension, limiting generalization from demonstrations.
- Core assumption: If VLMs were learning task strategies, performance would improve with: (a) more demonstrations, (b) higher-quality rationales, (c) relevant retrieval.
- Evidence anchors:
  - [abstract] "VLMs often rely on shallow heuristics—such as copying or majority voting—rather than true task understanding."
  - [section 6.2, Table 5] "Improving rationale quality, either by filtering out incorrect support samples or injecting ground truth rationale, does not consistently lead to improved performance."
  - [corpus] Multiple related papers (What do VLMs see in context, BiasICL) question ICL effectiveness in VLMs, supporting the shallow heuristics concern.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: The paper's central question is whether VLMs truly perform ICL or merely pattern-match; understanding ICL's definition is prerequisite to evaluating this claim.
  - Quick check question: Why would adding more demonstrations potentially degrade rather than improve performance if a model is doing genuine ICL?

- **Concept: Distribution Shift (In-Distribution vs Out-of-Distribution Support Sets)**
  - Why needed here: The paper uses ID vs OOD settings to distinguish memorization (performance boost only in ID) from generalization (performance preserved in OOD).
  - Quick check question: If a VLM's performance improves with ID demonstrations but degrades with OOD demonstrations of the same task type, what does this suggest about its learning mechanism?

- **Concept: Chain-of-Thought (CoT) Reasoning in Demonstrations**
  - Why needed here: The paper tests whether enriching demonstrations with explicit reasoning steps enables deeper learning; understanding CoT's role in prompting is essential.
  - Quick check question: How does providing step-by-step reasoning in demonstrations differ pedagogically from providing only final answers?

## Architecture Onboarding

- **Component map:**
  - Qwen2.5-VL -> VL-R1 -> InternVL2.5 -> InternVL-MPO -> Llama-3.2V -> LLaVA-CoT -> Gemini-2.0-Flash

- **Critical path:**
  1. Support set selection (ID vs OOD, retrieval method, shot count)
  2. Demonstration content preparation (answer-only vs answer+reasoning)
  3. Format alignment verification (consistent vs inconsistent with model training)
  4. Context construction and query execution
  5. Response evaluation (exact match for perception, LLM judge for reasoning)

- **Design tradeoffs:**
  - **Pseudo vs Gold reasoning:** Pseudo-reasoning maintains realistic conditions but may include errors; gold reasoning provides quality but requires dataset annotations.
  - **Retrieval vs Random:** Retrieval benefits base models (pattern copying) but harms reasoning models (incoherent reasoning paths).
  - **Filtering vs Coverage:** Filtering incorrect demonstrations removes noise but reduces diversity, which the paper suggests is more important for VLMs.

- **Failure signatures:**
  - Zero-shot consistently outperforming few-shot across datasets
  - Performance plateauing or degrading beyond 2-4 shots
  - No improvement when ground-truth rationales replace pseudo-reasoning
  - Large ID-OOD gaps that persist even with consistent formatting

- **First 3 experiments:**
  1. **Format consistency validation:** Run Protocol 2 (answer-only demos) vs Protocol 4 (reasoning+answer demos) on a single VLM reasoner (e.g., VL-Rethinker-7B) across 1/2/4/8 shots on ScienceQA to confirm Table 2 patterns.
  2. **Retrieval-reasoning interaction test:** Compare MM-retriever vs random selection on a base model (Llama-3.2V-11B) and its reasoning variant (LLaVA-CoT) on M3CoT to verify the cross-over effect.
  3. **Rationale quality sensitivity probe:** Run baseline → +filter → +ground-truth rationale → +both on InternVL2.5-8B-MPO across reasoning datasets to confirm insensitivity to demonstration quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reasoning-aware retrieval strategies that consider rationale similarity (not just input similarity) improve MM-ICL performance for reasoning models?
- Basis in paper: [explicit] Authors note MM-retriever underperforms random selection for reasoning models because it selects based on input similarity without accounting for reasoning path relevance.
- Why unresolved: Current retrievers ignore whether retrieved rationales share consistent or relevant reasoning trajectories with the query.
- What evidence would resolve it: Develop a retriever that scores demonstrations by reasoning path similarity and show consistent improvement over input-only retrieval on reasoning benchmarks.

### Open Question 2
- Question: Would interleaved-modal chain-of-thought (integrating visual and textual reasoning steps) enhance demonstration learning in VLMs?
- Basis in paper: [explicit] Listed as a future direction: "reasoning with multiple modalities (interleaved-modal CoT)."
- Why unresolved: Current MM-ICL provides reasoning only in text, potentially limiting cross-modal reasoning transfer.
- What evidence would resolve it: Construct demonstrations with visual reasoning steps interleaved with text and compare performance against text-only rationales.

### Open Question 3
- Question: Why does filtering incorrect support samples sometimes degrade MM-ICL performance—does diversity outweigh correctness?
- Basis in paper: [inferred] Table 5 shows filtering incorrect samples slightly degrades performance in several cases; authors hypothesize reduced diversity may be responsible.
- Why unresolved: The trade-off between support set correctness and diversity remains unclear.
- What evidence would resolve it: Systematic ablation varying both correctness ratios and diversity metrics in support sets to isolate each factor's contribution.

## Limitations

- Experimental scope limited to specific VLM families (Qwen2.5-VL, InternVL2.5, Llama-3.2V) limiting generalizability
- Reliance on synthetic pseudo-reasoning introduces potential confounding factors despite ground-truth ablation
- LLM judge evaluation framework adds model-dependent variability to performance measurements

## Confidence

- **High confidence:** Format consistency improves reasoning model performance when aligned with expected output format (Section 6.1, Table 2)
- **Medium confidence:** Input-based retrieval harms reasoning model coherence due to diverse rationale structures for similar inputs (Section 6.3)
- **Medium confidence:** VLMs primarily exploit shallow heuristics rather than genuine task understanding (Section 6.2, Table 5)

## Next Checks

1. Replicate the ID vs OOD performance gap analysis across a broader set of VLMs (including GPT-4V, Claude-3-Vision) to test the robustness of the shallow heuristics hypothesis
2. Conduct human evaluation of pseudo-reasoning quality and its correlation with downstream performance to validate the synthetic rationale generation pipeline
3. Test the format consistency hypothesis with additional reasoning datasets and cross-task generalization scenarios to determine if improvements transfer beyond the studied domains