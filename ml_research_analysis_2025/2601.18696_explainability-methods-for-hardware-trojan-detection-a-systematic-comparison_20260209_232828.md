---
ver: rpa2
title: 'Explainability Methods for Hardware Trojan Detection: A Systematic Comparison'
arxiv_id: '2601.18696'
source_url: https://arxiv.org/abs/2601.18696
tags:
- trojan
- feature
- methods
- hardware
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically compares explainability methods for hardware
  trojan detection, evaluating domain-aware property analysis, model-agnostic case-based
  reasoning, and feature attribution techniques (LIME, SHAP, gradient). The study
  finds that property-based analysis provides circuit-specific interpretations aligned
  with engineering expertise (e.g., "high fanin complexity near outputs indicates
  potential triggers"), while case-based reasoning achieves 97.4% correspondence between
  predictions and training exemplars.
---

# Explainability Methods for Hardware Trojan Detection: A Systematic Comparison

## Quick Facts
- arXiv ID: 2601.18696
- Source URL: https://arxiv.org/abs/2601.18696
- Reference count: 40
- Primary result: XGBoost achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold improvement over prior work, with domain-aware and case-based explainability methods offering complementary advantages over generic feature rankings.

## Executive Summary
This paper systematically evaluates explainability methods for hardware trojan detection, comparing domain-aware property analysis, model-agnostic case-based reasoning, and feature attribution techniques (LIME, SHAP, gradient). The study finds that property-based analysis provides circuit-specific interpretations aligned with engineering expertise, while case-based reasoning achieves 97.4% correspondence between predictions and training exemplars. The research demonstrates that domain-aware and case-based approaches offer complementary advantages for hardware security applications compared to generic feature rankings.

## Method Summary
The study evaluates three categories of explainability methods for hardware trojan detection: (1) domain-aware property analysis using 31 XGBoost classifiers trained on different feature combinations from 5 base circuit features, (2) case-based reasoning using k-NN retrieval with 97.4% correspondence between predictions and training exemplars, and (3) model-agnostic feature attribution (LIME, SHAP, gradient). The XGBoost classifier is trained on Trust-Hub benchmark data with extreme class imbalance (275:1 benign-to-trojan ratio) using cost-sensitive learning and a 0.99 prediction threshold. The 11,392-sample test set includes 30 circuits from RS232 and ISCAS benchmarks, with five structural features extracted per gate.

## Key Results
- XGBoost achieves 46.15% precision and 52.17% recall, a 9-fold improvement over prior work
- Property-based analysis provides circuit-specific interpretations like "high fanin complexity near outputs indicates potential triggers"
- Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars
- Gradient attribution runs 481× faster than SHAP but provides similar domain-opaque insights
- LIME and SHAP show strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level interpretability

## Why This Works (Mechanism)

### Mechanism 1: Domain-Aware Property Ensemble Voting
Property-based analysis provides explanations through circuit concepts that align with hardware engineering expertise. 31 XGBoost classifiers, each trained on different feature combinations derived from 5 base circuit features, vote on trojan classification using weighted aggregation with per-class effectiveness metrics. This produces explanations like "high fanin complexity near primary output indicates potential rare-event trigger circuits." Core assumption: hardware trojans exhibit recognizable structural signatures requiring high fanin complexity near outputs for payload propagation. Break condition: Low-dimensional feature spaces limit discriminative power through naive combinatorics.

### Mechanism 2: Case-Based k-NN Precedent Retrieval
Case-based reasoning provides model-agnostic justifications grounded in training exemplars without requiring domain-specific interpretation. For each test gate, retrieve k=5 nearest neighbors from training data using Euclidean distance in 5-dimensional feature space. Predictions via distance-weighted voting; correspondence metric quantifies agreement between classifier prediction and neighbor consensus. Core assumption: similar circuit structures share trojan/benign classification. Break condition: Low correspondence (<70%) signals ambiguity warranting manual review; fails when training lacks diverse trojan exemplars.

### Mechanism 3: Model-Agnostic Feature Attribution (SHAP/LIME)
LIME and SHAP provide mathematically consistent feature attributions with strong inter-method correlation but lack circuit-level interpretability. LIME perturbs features locally and fits linear models; SHAP computes Shapley values via tree structure analysis. Both output importance scores without domain translation. Core assumption: feature importance rankings reveal detection logic regardless of domain context. Break condition: Domain-opaque outputs require engineers to interpret importance scores without circuit design guidance.

## Foundational Learning

- Concept: **Hardware Trojan Taxonomy (Trigger + Payload)**
  - Why needed here: All detection features and explanations reference this structure—triggers as rare-event combinational logic, payloads as signal paths to observable outputs.
  - Quick check question: Can you explain why high LGFi (logic gate fanin) near primary outputs indicates potential trojan triggers rather than normal circuit complexity?

- Concept: **Class Imbalance in Security Classification**
  - Why needed here: Dataset has 275:1 benign-to-trojan ratio; standard accuracy is misleading. XGBoost's `scale_pos_weight` parameter implements cost-sensitive learning.
  - Quick check question: Why does 99.6% accuracy with 0% trojan recall represent a trivial classifier failure rather than success?

- Concept: **Explainability Taxonomy (Domain-Aware vs. Model-Agnostic vs. Inherently Interpretable)**
  - Why needed here: The paper's core contribution is comparing these three categories—understanding their tradeoffs is essential for method selection.
  - Quick check question: What information does "LGFi=12, PO=1 matches rare-event trigger signatures" (property-based) provide that "LGFi importance=0.73" (SHAP) does not?

## Architecture Onboarding

- Component map: CircuitGraph + NetworkX parse Verilog netlists → 5 structural features per gate → XGBoost classification → Post-hoc explanation generation (parallel paths for 5 methods)
- Critical path: Netlist parsing → Feature extraction → XGBoost classification → Post-hoc explanation generation (parallel paths for 5 methods)
- Design tradeoffs:
  - Precision vs. Recall: Threshold 0.99 achieves 46.15% precision / 52.17% recall vs. baseline 0.5 threshold at 10.17% precision / 76.09% recall
  - Speed vs. Rigor: Gradient attribution 481× faster than SHAP but lacks theoretical guarantees
  - Domain alignment vs. Generality: Property-based aligned with engineering expertise but underperforms (2.00% precision); case-based achieves both (46.15% precision, 97.4% correspondence)
- Failure signatures:
  - High recall, low precision (>70% recall, <15% precision) → threshold too low, excessive false positives overwhelming engineers
  - Low correspondence (<70%) in case-based → ambiguous feature space or insufficient training diversity
  - Property ensemble precision collapse → naive combinatorics insufficient in low-dimensional settings; need ratio/interaction features
- First 3 experiments:
  1. Train XGBoost on 5-feature dataset with `scale_pos_weight=275`, evaluate precision/recall at thresholds 0.5 and 0.99 to confirm 9× precision improvement claim.
  2. Compute Spearman correlation between LIME and SHAP feature rankings on 100-gate sample; verify ρ≈0.94.
  3. For 50 test gates, retrieve k=5 neighbors and compute correspondence; confirm average >95% for high-confidence predictions.

## Open Questions the Paper Calls Out

### Open Question 1
Can domain-aware feature engineering techniques, such as ratio-based properties (e.g., LGFi/PO) or circuit-relative percentiles, significantly improve the precision of property-based explainability methods in low-dimensional feature spaces? The authors state that the property-based method's low precision (2.00%) reflects a limitation in low-dimensional settings and explicitly propose "ratio-based properties," "interaction terms," and "circuit-relative percentile features" as three directions for future improvement.

### Open Question 2
How do domain-aware property analysis and case-based reasoning compare in terms of decision-making effectiveness and trust when evaluated by practicing hardware security engineers rather than quantitative metrics? The authors identify the lack of "human-subjects studies with practicing hardware security engineers" as a limitation, explicitly calling for future work to "quantify actionability, trust, and decision-making effectiveness across explainability methods."

### Open Question 3
Do the comparative advantages of domain-aware explainability methods hold for sequential trigger mechanisms and analog/mixed-signal trojans? The authors note the evaluation "focuses exclusively on digital combinational trojans... excluding analog/mixed-signal trojans, sequential trigger mechanisms, and system-level threats."

## Limitations
- Results are based on Trust-Hub benchmark with extreme class imbalance (275:1 benign-to-trojan), which may not generalize to balanced or real-world scenarios
- Critical implementation details (EPARS weighting, exact feature combinations for 31 classifiers) remain unclear without access to the codebase
- Property-based explanations assume hardware engineers can validate "high fanin complexity near outputs" as trojan signatures—validation across diverse engineering teams is limited

## Confidence

- **High confidence**: Detection performance metrics (precision=46.15%, recall=52.17%, FPR=0.25%) and statistical significance (McNemar tests, 95% CIs) are well-supported by the experimental design
- **Medium confidence**: The 9-fold precision improvement claim is credible but depends on baseline selection and threshold optimization procedures that could vary
- **Medium confidence**: Explainability method comparisons (correlation coefficients, correspondence metrics) are methodologically sound but assume the Trust-Hub dataset represents realistic trojan scenarios

## Next Checks

1. Test the same explainability methods on a different hardware trojan dataset with different circuit characteristics to assess generalizability beyond Trust-Hub
2. Conduct user studies with hardware security engineers to validate whether property-based explanations actually improve detection accuracy or debugging efficiency compared to generic feature attributions
3. Systematically vary the trojan-to-benign ratio and threshold settings to identify conditions where explainability degrades or becomes misleading, particularly for case-based reasoning when training data lacks diverse trojan exemplars