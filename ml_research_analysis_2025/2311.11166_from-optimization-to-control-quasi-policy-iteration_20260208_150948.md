---
ver: rpa2
title: 'From Optimization to Control: Quasi Policy Iteration'
arxiv_id: '2311.11166'
source_url: https://arxiv.org/abs/2311.11166
tags:
- prior
- algorithm
- update
- algorithms
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes quasi-policy iteration (QPI), a novel model-based
  reinforcement learning algorithm inspired by quasi-Newton methods from convex optimization.
  QPI introduces an efficient approximation of the Hessian matrix in the policy iteration
  algorithm by exploiting two linear structural constraints specific to Markov decision
  processes (MDPs): the row-stochasticity of transition matrices and the piecewise-affine
  nature of the Bellman operator.'
---

# From Optimization to Control: Quasi Policy Iteration

## Quick Facts
- **arXiv ID**: 2311.11166
- **Source URL**: https://arxiv.org/abs/2311.11166
- **Reference count**: 40
- **Primary result**: Proposes QPI algorithm achieving PI-like convergence with VI computational complexity via structural constraint-based Hessian approximation

## Executive Summary
This paper introduces Quasi Policy Iteration (QPI), a model-based reinforcement learning algorithm that accelerates policy iteration by approximating the Hessian matrix through two structural constraints specific to Markov decision processes. The algorithm exploits the row-stochasticity of transition matrices and the piecewise-affine nature of the Bellman operator to achieve a closed-form rank-one update with O(n²m) complexity per iteration. QPI exhibits empirical convergence behavior similar to quasi-Newton methods while maintaining the computational efficiency of value iteration. The authors also extend this approach to a model-free setting (QPL) that maintains comparable complexity to standard Q-learning while showing competitive performance.

## Method Summary
QPI approximates the Hessian matrix in policy iteration by solving a constrained least-squares problem that enforces two exact linear constraints: transition matrices must be row-stochastic (P·1 = 1) and must satisfy the Bellman operator structure (P·v_k = γ^(-1)(T_k - c_k)). This yields a closed-form rank-one update for the transition matrix approximation and corresponding gain matrix via the Woodbury formula. The algorithm includes a safeguarding mechanism that falls back to value iteration when the Bellman error exceeds a threshold, ensuring global linear convergence. In the model-free setting, QPL extends this approach by maintaining estimates of the gain matrix and incorporating projection to handle sampling noise, with updates similar to Q-learning but incorporating the quasi-Newton approximation.

## Key Results
- QPI achieves PI-like quadratic convergence empirically while maintaining VI's O(n²m) per-iteration complexity
- Performance is largely independent of the discount factor γ, unlike standard VI which slows dramatically for γ → 1
- QPL in the model-free setting shows competitive performance with established algorithms (QL, SQL, ZQL) while maintaining similar computational complexity
- Structured priors can significantly accelerate convergence for certain MDPs but may deteriorate performance for others, particularly in the model-free setting

## Why This Works (Mechanism)

### Mechanism 1: Structural Constraint-Based Hessian Approximation
QPI achieves efficient Hessian approximation by exploiting two exact linear constraints specific to MDPs: row-stochasticity (P·1 = 1) and Bellman operator structure (P·v_k = γ^(-1)(T_k - c_k)). This yields a closed-form rank-one update with O(n²m) complexity. The approximation reduces to the prior when the constraints become linearly dependent (v_k = ρ·1).

### Mechanism 2: Safeguarding Ensures Contraction with Guaranteed Linear Rate
Embedding QPI within a VI-based safeguard guarantees global linear convergence at rate γ while preserving empirical acceleration. The safeguard activates when θ_{k+1} > γ^{k+1} θ_0, falling back to VI updates. This ensures θ_k ≤ γ^k θ_0 for all k while allowing QPI to exhibit quasi-Newton-like convergence empirically.

### Mechanism 3: Secant Conditions Enable Local Superlinear Convergence
Augmenting the approximation with secant-type constraints (γP(v_k - v_{k-1}) = T_k - T_{k-1}) when π_{v_k} = π_{v_{k-1}} guarantees local superlinear convergence. This makes the approximation a rank-three update satisfying the Dennis-Moré condition, leveraging the strong semismoothness of the Bellman residual map.

## Foundational Learning

- **Quasi-Newton Methods and Hessian Approximation**: Understanding how rank-j updates approximate Hessians via the Woodbury formula is essential for grasping why QPI achieves O(n²m) complexity. *Quick check*: Given Ĥ_k = H_prior + (B - H_prior R)(R^⊤ R)^(-1) R^⊤, what is the rank of the update term?

- **Bellman Operators as Fixed-Point Maps**: The core innovation exploits structural properties of T—specifically, that T(v) = c_π_v + γP_π_v v is piecewise-affine. *Quick check*: If T is a γ-contraction in ∞-norm, what does this imply about ||v_k - T(v_k)||_∞ as k → ∞?

- **Markov Decision Process Formulation (Tabular Case)**: The algorithm is derived for finite state-action spaces with explicit transition matrices P ∈ R^(n×n). *Quick check*: In the model-free QPL update, why must the projection Π_M(p_k) be applied to the extra term p_k = (Ĝ_k - I)(T̂_k - q_k)?

## Architecture Onboarding

- **Component map**: Prior selection -> Constraint formation -> Gain matrix computation -> Update application -> Safeguard/backtracking
- **Critical path**: 1) Compute T_k = T(v_k) via min over actions: O(n²m); 2) Extract c_k and compute constraint vectors: O(n); 3) Solve for Ĝ_k via closed-form rank-one update: O(n²); 4) Apply update and check safeguard condition: O(n²m)
- **Design tradeoffs**: 
  - Prior selection: uniform (simplifies computation), recursive (mimics Broyden QNM but requires projection), structured (improves some MDPs, degrades others)
  - Safeguarding vs backtracking: safeguarding simpler but may over-trigger; backtracking allows more aggressive steps with guaranteed contraction
  - Standard QPI vs modified: secant-enhanced variant adds rank-three updates and conditional logic for faster local convergence
- **Failure signatures**: 
  - Frequent safeguard activation for absorbing-state MDPs at high γ
  - Model-free QPL underperformance on structured MDPs
  - Constraint linear dependence when v_k = ρ·1
- **First 3 experiments**:
  1. Reproduce Garnet MDP benchmark: generate n=50, m=5, nb=10; compare QPI to VI, PI, AVI, NVI for γ ∈ {0.9, 0.99, 0.999}
  2. Ablation on prior selection: compare uniform, recursive, and structured priors on Healthcare MDP
  3. Model-free QPL sanity check: implement synchronous QPL on Garnet MDP; compare to QL, SQL, ZQL over K=10⁴ iterations

## Open Questions the Paper Calls Out

### Open Question 1
Can the convergence rate of the basic QPI algorithm be theoretically improved from linear to superlinear or quadratic without requiring explicit secant-type constraints? The authors note the "lack of a theoretical guarantee for the empirically observed improvement" and state establishing a local superlinear rate without secant conditions is an "interesting result" yet to be achieved.

### Open Question 2
How can non-negativity or sparsity constraints be incorporated into the transition matrix approximation while maintaining computational efficiency? Section 3.2.4 identifies that adding such constraints currently undermines efficiency because the optimization problem no longer admits a low-rank, closed-form solution.

### Open Question 3
Why does the use of structured priors improve performance in model-based QPI but cause performance deterioration in the model-free QPL algorithm on structured MDPs? The paper reports "contradictory results" where structured priors significantly improved model-based QPI for Healthcare MDP but deteriorated model-free QPL performance for Graph MDP.

## Limitations

- Performance gains rely on structural constraints that may not hold when transition kernels are unknown or poorly estimated from data
- Model-free QPL extension requires careful stabilization through projection, potentially limiting practical applicability
- Empirical results depend on specific problem structures that may not generalize to all MDPs
- Frequent safeguard activation can degrade QPI to VI-like performance, particularly for absorbing-state MDPs at high discount factors

## Confidence

- **High confidence**: O(n²m) computational complexity claim, linear convergence rate under safeguarding
- **Medium confidence**: Empirical acceleration benefits (quasi-Newton-like behavior), local superlinear convergence in modified version
- **Low confidence**: Model-free QPL performance on structured MDPs (e.g., Graph MDP results)

## Next Checks

1. **Stress test on absorbing-state MDPs**: Systematically evaluate QPI performance on MDPs with absorbing states across discount factors (0.9 to 0.999) to quantify safeguard activation frequency and resulting performance degradation.

2. **Prior sensitivity analysis**: Conduct controlled experiments varying prior choice (uniform, recursive, structured) across multiple MDP families to quantify impact on convergence speed and identify problem characteristics favoring each prior type.

3. **Model-free extension robustness**: Implement QPL with adaptive learning rate schedules and compare performance against stabilized variants (e.g., proximal regularization) on structured MDPs where current approach underperforms.