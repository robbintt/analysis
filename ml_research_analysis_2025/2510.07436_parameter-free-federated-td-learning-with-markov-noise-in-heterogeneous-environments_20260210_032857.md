---
ver: rpa2
title: Parameter-Free Federated TD Learning with Markov Noise in Heterogeneous Environments
arxiv_id: '2510.07436'
source_url: https://arxiv.org/abs/2510.07436
tags:
- follows
- lemma
- where
- learning
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving parameter-free
  optimal convergence rates in federated temporal difference (TD) learning for Markovian
  data in heterogeneous environments. While federated learning can accelerate reinforcement
  learning by distributing exploration and training across multiple agents, existing
  methods achieving optimal convergence rates rely on unknown problem parameters,
  particularly in the presence of Markovian noise.
---

# Parameter-Free Federated TD Learning with Markov Noise in Heterogeneous Environments

## Quick Facts
- arXiv ID: 2510.07436
- Source URL: https://arxiv.org/abs/2510.07436
- Reference count: 40
- One-line result: Achieves optimal $\tilde{O}(1/(NT))$ convergence for federated TD learning without requiring problem-specific step sizes

## Executive Summary
This paper addresses the challenge of achieving parameter-free optimal convergence rates in federated temporal difference (TD) learning for Markovian data in heterogeneous environments. The authors propose a two-timescale Federated Temporal Difference (FTD) learning algorithm with Polyak-Ruppert averaging that provably attains the optimal $\tilde{O}(1/(NT))$ convergence rate in both average-reward and discounted settings without requiring problem-specific step sizes. The key innovation is updating the value function estimate on a faster timescale using a universal step size while updating the average reward estimate on a slower timescale, combined with Polyak-Ruppert averaging.

## Method Summary
The method implements two-timescale TD learning where value function updates use step size $\beta_t = (t+1)^{-\beta}$ with $\beta \in (1/2, 1)$, while average reward updates use $(t+1)^{-1}$. The algorithm employs Polyak-Ruppert averaging for the value function parameter and federated aggregation of local TD errors across heterogeneous agents. The approach works for both average-reward and discounted settings, with convergence proofs showing optimal rates despite the semi-contraction nature of the average-reward Bellman operator and the presence of Markovian noise.

## Key Results
- Proposes parameter-free FTD algorithm achieving optimal $\tilde{O}(1/(NT))$ convergence rate
- Applies to both average-reward and discounted TD learning settings
- Heterogeneity gap scales as $O((\varepsilon_p + \varepsilon_r)^2)$ and vanishes as environment differences decrease
- Numerical simulations confirm theoretical findings and demonstrate performance comparable to parameter-dependent methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Polyak-Ruppert averaging enables optimal convergence with universal step sizes, removing the need for problem-specific parameters like minimum eigenvalues of transition matrices.
- Mechanism: The algorithm runs TD updates on a fast timescale with step size $\beta_t = (t+1)^{-\beta}$ where $\beta \in (1/2, 1)$, while maintaining a running average $\bar{\theta}_{t+1} = \bar{\theta}_t + \frac{1}{t+1}[\theta_t - \bar{\theta}_t]$ on a slow timescale. This averaging smooths the noisy iterates toward the optimal solution asymptotically.
- Core assumption: The underlying stochastic approximation process converges in expectation; the averaging kernel has sufficient "memory" to dampen variance from Markovian noise.
- Evidence anchors: [abstract] "proposing a two-timescale Federated Temporal Difference (FTD) learning with Polyak-Ruppert averaging... provably attains the optimal $\tilde{O}(1/NT)$ rate... without requiring problem-specific step sizes"

### Mechanism 2
- Claim: Two-timescale separation allows independent analysis of value function and average reward estimates, enabling convergence proof despite the average-reward Bellman operator being only a semi-contraction.
- Mechanism: Value function updates with $\beta_t$ (faster decay) effectively "see" a quasi-static average reward estimate $r_t$ which evolves with $(t+1)^{-1}$ (slower decay). This timescale separation justifies treating $r_t$ as approximately constant during value function analysis, then showing $r_t$ converges independently.
- Core assumption: Assumption A4 ensures the feature matrix column space excludes the all-ones vector, guaranteeing unique fixed points for both estimates; ergodicity (A1) ensures mixing.
- Evidence anchors: [section I] "the average-reward Bellman operator is not a contraction in the standard norm, but only in a semi-norm"

### Mechanism 3
- Claim: Federated aggregation with local TD errors achieves $O(1/N)$ variance reduction while heterogeneity introduces a bounded gap that vanishes as environment differences decrease.
- Mechanism: Each agent $i$ computes local TD error $\delta^i_{t+1}$ using its local transition; the server aggregates $\theta_{t+1} = \theta_t + \frac{\beta_t}{N} \sum_i \delta^i_{t+1}$. The heterogeneity gap $H_\theta(\varepsilon_p, \varepsilon_r) = O((\varepsilon_p + \varepsilon_r)^2)$ arises from MDP differences but does not grow with iterations.
- Core assumption: Assumption A2 bounds transition heterogeneity ($|P^\mu_i - P^\mu_j| \leq \varepsilon_p P^\mu_i$) and reward heterogeneity ($\|R^i - R^j\| \leq \varepsilon_r$).
- Evidence anchors: [section III, Theorem III.2] Heterogeneity gap $H_\theta(\varepsilon_p, \varepsilon_r)$ "go to 0 as $\max\{\varepsilon_p, \varepsilon_r\} \to 0$"

## Foundational Learning

- Concept: **Markov Decision Process (MDP) and Policy Evaluation**
  - Why needed here: The paper solves policy evaluation (estimating value functions for a fixed policy $\mu$) under both average-reward and discounted criteria. Understanding Bellman operators $\mathcal{T}^\mu_i$ is essential to grasp why convergence is non-trivial.
  - Quick check question: Given a policy $\mu$ and transition matrix $P^\mu$, can you write the Bellman equation for the differential value function $V^\mu$ under the average-reward criterion?

- Concept: **Stochastic Approximation and ODE Method**
  - Why needed here: TD learning is a stochastic approximation algorithm. The convergence analysis relies on treating iterates as discretizations of an underlying ODE; PR-averaging exploits this connection to achieve optimal rates.
  - Quick check question: In stochastic approximation, why does a decreasing step size $\alpha_t = 1/t^\beta$ with $\beta \in (1/2, 1)$ balance bias-variance tradeoff differently than $\beta = 1$?

- Concept: **Linear Function Approximation in RL**
  - Why needed here: The value function is approximated as $V \approx \Phi\theta$ where $\Phi \in \mathbb{R}^{|S| \times d}$ with $d \ll |S|$. The matrices $A_i = \Phi^\top D^\mu_i (I - P^\mu_i)\Phi$ and their positive definiteness (from A4) are central to convergence proofs.
  - Quick check question: Why does Assumption A4 require that the all-ones vector $\mathbf{1}$ not lie in the column space of $\Phi$ for average-reward TD learning?

## Architecture Onboarding

- Component map:
  - Client nodes (N agents) -> Central server -> Client nodes
  - Each agent holds local MDP, samples trajectories, computes local TD error
  - Server aggregates TD errors, updates global parameters, broadcasts back

- Critical path:
  1. Initialize $\bar{\theta}_0 = \theta_0$, $r^i_0 = r_0$ for all $i$
  2. Each iteration: agents sample locally → compute $\delta^i_{t+1}$ → transmit to server
  3. Server updates $\theta_{t+1}$, $\bar{\theta}_{t+1}$, $r_{t+1}$ → broadcast back
  4. Output: $\bar{\theta}_T$ as the parameter-free optimal estimate after $T$ iterations

- Design tradeoffs:
  - **Communication frequency**: Every-iteration communication (as in algorithm) vs. periodic; more frequent communication accelerates convergence but increases bandwidth
  - **Step size exponent $\beta$**: Smaller $\beta$ (closer to 0.5) yields faster transient but larger constants; $\beta$ closer to 1 reduces variance but slows initial progress
  - **Local vs. global averaging**: Current design averages TD errors; could alternatively average $\theta$ directly, but theoretical guarantees differ

- Failure signatures:
  - Convergence stalls at high plateau: Likely heterogeneity bound violated; check $\varepsilon_p, \varepsilon_r$ against actual MDP differences
  - Oscillating or diverging estimates: Step size too large or $\beta \leq 1/2$; verify $\beta \in (1/2, 1)$
  - Average reward estimate diverges: Feature matrix may contain all-ones vector, violating A4; check $\Phi$ construction
  - No linear speedup with more agents: Agents may not be sampling independently (correlated trajectories); verify each agent has separate MDP instance

- First 3 experiments:
  1. **Baseline convergence validation**: Implement AvgFedTD(0) with $N \in \{2, 5, 10, 20\}$ agents, $|S| = |A| = 100$, $d = 21$, $\beta = 0.6$; plot $\mathbb{E}\|\bar{\theta}_t - \theta^*_1\|^2$ vs. $t$ to confirm $\tilde{O}(1/(NT))$ scaling
  2. **Step size sensitivity**: Sweep $\beta \in \{0.51, 0.6, 0.8, 0.95\}$ with fixed $N=10$; observe rate-constant tradeoff; verify that $\beta \to 0.5$ approaches the conjectured optimal constant but may increase variance
  3. **Heterogeneity stress test**: Fix $N=5$, vary $\varepsilon_r \in \{0.5, 1, 5, 10\}$ and $\varepsilon_p \in \{0.2, 0.4, 0.6, 0.8\}$ separately; measure final error plateau to validate that heterogeneity gap scales as $O((\varepsilon_p + \varepsilon_r)^2)$; check if error curves remain parallel vs. shifted

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed two-timescale parameter-free approach be extended to asynchronous federated Q-learning with PR-averaging in the average-reward setting?
- Basis in paper: [explicit] The authors state in Section VI: "For future work, we plan to extend these techniques to asynchronous federated Q-learning with PR-averaging in the average-reward setting."
- Why unresolved: A key challenge identified is that Assumption A4 (the all-ones vector not being in the column space of the feature matrix) cannot be guaranteed in Q-learning. Without this, the associated Bellman operator no longer admits a unique fixed point.
- What evidence would resolve it: A theoretical framework proving finite-time convergence for federated Q-learning that handles the potential lack of a unique fixed point, or an algorithmic modification that ensures the condition holds.

### Open Question 2
- Question: Does using a constant step-size ($\beta=0$) achieve the optimal convergence rate while remaining parameter-free?
- Basis in paper: [explicit] Section V notes that simulation results lead to the conjecture that the optimal rate is achieved at $\beta=0$, but the authors state, "it is unclear if this choice of constant will be parameter free."
- Why unresolved: The current theoretical analysis relies on a specific range for the step-size exponent ($\beta \in (1/2, 1)$). Extending this to a constant step-size (which corresponds to $\beta=0$) requires a different theoretical treatment to ensure the "parameter-free" property is maintained.
- What evidence would resolve it: A finite-time analysis proving that a universal constant step-size yields the optimal $\tilde{O}(1/(NT))$ rate without dependence on unknown problem parameters like minimum eigenvalues.

### Open Question 3
- Question: Can the proposed federated TD learning algorithms be modified to maintain convergence guarantees in the presence of a small subset of adversarial workers?
- Basis in paper: [explicit] Section VI lists "federated RL with a small subset of adversarial workers" as an "exciting direction" for future work.
- Why unresolved: The current convergence proofs rely on the aggregation of updates from cooperative agents. Malicious updates from adversarial workers could disrupt the averaging process or exploit the Markovian noise to destabilize the learning dynamics.
- What evidence would resolve it: Robust aggregation rules (e.g., Byzantine-resilient methods) integrated into the FTD framework with accompanying theoretical bounds on the convergence rate and error floors under adversarial conditions.

## Limitations
- Heterogeneity gap scales as $O((\varepsilon_p + \varepsilon_r)^2)$, which may be significant for highly heterogeneous environments
- Requirement that feature matrix column space excludes the all-ones vector (Assumption A4) is restrictive
- Convergence analysis assumes agents sample independently from their respective MDPs

## Confidence
- **High**: Core theoretical claims for average-reward setting with established stochastic approximation techniques
- **Medium**: Discounted setting analysis with appropriate modifications for heterogeneity
- **Low**: Conjecture about constant step-size achieving optimal rate without parameter dependence

## Next Checks
1. Implement the algorithm with varying $\beta$ values to empirically verify the tradeoff between convergence rate constant and variance reduction
2. Conduct experiments with different heterogeneity levels to quantify how the gap term affects practical performance across diverse environments
3. Test the algorithm with feature matrices that potentially violate Assumption A4 to understand the impact on convergence and identify practical feature construction guidelines