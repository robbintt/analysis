---
ver: rpa2
title: Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement
  Learning
arxiv_id: '2510.03181'
source_url: https://arxiv.org/abs/2510.03181
tags:
- state
- regret
- learning
- self
- transition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses non-stationary reinforcement learning under
  distribution shifts, where transition functions change at specific episodes (finite-horizon)
  or time steps (infinite-horizon). The authors propose DQUCB, a shift-aware Q-learning
  UCB algorithm that uses a transition density function to detect distribution shifts
  and adjust exploration rates accordingly.
---

# Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.03181
- Source URL: https://arxiv.org/abs/2510.03181
- Reference count: 40
- Primary result: DQUCB achieves better regret bounds than non-shift-aware QUCB in non-stationary settings, with strong empirical performance on GridWorld, FrozenLake, and COVID-19 patient allocation tasks.

## Executive Summary
This paper addresses the challenge of non-stationary reinforcement learning where transition functions change at specific episodes or time steps. The authors propose DQUCB, a shift-aware Q-learning algorithm that uses transition density estimation to detect distribution shifts and dynamically adjust exploration rates. By scaling the UCB bonus inversely with transition likelihood, DQUCB effectively resets exploration when shifts occur while maintaining efficient exploitation in stationary periods. Theoretically, DQUCB achieves better regret bounds than standard QUCB, and empirically it outperforms both non-shift-aware baselines and model-based approaches across multiple benchmark environments including a real-world COVID-19 patient allocation task.

## Method Summary
DQUCB modifies standard Q-learning with UCB by incorporating a density estimator that tracks the likelihood of observed transitions. The algorithm maintains a sliding window of recent transitions and uses Kernel Density Estimation (KDE) to compute p(s'|s,a). The UCB bonus is scaled by the inverse of this likelihood: b_t = c · (p(s'|s,a) · sqrt(H³·ι/t))⁻¹. When the environment is stable, high likelihood reduces exploration; when a shift occurs, low likelihood increases exploration, forcing the agent to re-explore. The density model is updated incrementally using the n most recent (s', s, a) tuples, allowing real-time shift detection without full model re-training.

## Key Results
- DQUCB achieves better regret bounds than standard QUCB in non-stationary settings, avoiding linear regret growth after distribution shifts
- On GridWorld and FrozenLake tasks, DQUCB significantly outperforms QUCB and model-based baselines (UCBVI, UCBMQ) in both regret and computational efficiency
- The method demonstrates strong real-world performance on COVID-19 patient hospital allocation, achieving better adaptation when shifts occur
- Theoretical analysis proves DQUCB's regret is bounded even across multiple distribution shifts, while standard QUCB suffers linear regret post-shift

## Why This Works (Mechanism)

### Mechanism 1: Inverse Likelihood Scaling
The algorithm stabilizes exploration by scaling the UCB bonus inversely with transition likelihood. Standard QUCB uses b_t ∝ √(1/t), while DQUCB uses b_t ∝ (1/p(s'|s,a))√(1/t). When transitions are likely under the learned density (stable environment), p is high, shrinking the bonus for exploitation. During a shift, p→0, causing the bonus to spike and forcing re-exploration. Core assumption: the density estimator can distinguish old P from new distribution P̄ via likelihood ratios. Break condition: density estimator fails to differentiate distributions or has high estimation error ε.

### Mechanism 2: Density-Based OOD Detection
The method detects shifts by treating transition likelihood as an out-of-distribution score. The density function p(s'|s,a;θ) is trained on a rolling window of recent transitions. If the environment shifts to P̄ while θ represents P, the likelihood ratio P/P̄ drops, signaling misalignment. Core assumption: shifts are detectable within window size n. Break condition: rolling window n is too large, mixing old and new distributions and blurring the OOD signal.

### Mechanism 3: Oracle Regret Guarantee
Theoretically, DQUCB achieves strictly better regret bounds than standard QUCB in non-stationary settings, assuming perfect density estimation (ε→0). Standard QUCB suffers linear regret H(K-K̄) after a shift at episode K̄ because it keeps exploiting the old optimal policy. DQUCB bounds regret to Õ(√K) even across shifts by effectively resetting uncertainty via the likelihood term. Core assumption: density estimator error ε is small (oracle setting). Break condition: high estimation error ε, where regret bound loosens and may degrade to standard QUCB levels.

## Foundational Learning

- **Concept**: **Q-Learning with UCB (QUCB)**
  - **Why needed here**: This is the base algorithm being modified. You must understand how Q-learning estimates values Q(s,a) and how UCB adds a bonus b ∝ √(1/N(s,a)) to encourage exploration of less-visited state-action pairs.
  - **Quick check question**: If the visitation count N(s,a) is high, does the UCB bonus increase or decrease? (Answer: Decrease).

- **Concept**: **Kernel Density Estimation (KDE)**
  - **Why needed here**: The paper uses KDE to estimate the transition probability density p(s'|s,a) required for the shift detection mechanism.
  - **Quick check question**: How does the bandwidth/window size n in KDE affect the sensitivity to distribution shifts? (Answer: Small n increases sensitivity/noise; large n increases stability but slows detection).

- **Concept**: **Regret in Non-Stationary MDPs**
  - **Why needed here**: The performance metric is defined as cumulative regret, specifically handling the shift at episode K̄. Understanding that standard algorithms incur linear regret after a shift is key to valuing this solution.
  - **Quick check question**: Why does standard Q-learning fail (linear regret) when the transition function P changes to P̄? (Answer: It continues to exploit the policy learned for P using stale high-confidence counts).

## Architecture Onboarding

- **Component map**: Agent -> Q-value + bonus -> Action selection -> Environment -> Replay Buffer -> Density Estimator (KDE) -> Likelihood p(s'|s,a) -> UCB Module
- **Critical path**: The UCB Bonus Calculation. This is where the density likelihood modulates the exploration signal. If p(s'|s,a) is not computed efficiently or accurately, the exploration/exploitation balance breaks.
- **Design tradeoffs**:
  - Window Size (n): Small n allows faster detection of shifts (OOD signal triggers sooner) but makes density estimates noisier (risking false positives). Large n stabilizes learning in stationary phases but delays shift detection.
  - Density Estimator Choice: KDE is computationally cheaper than model-based transitions (O(n²) vs O(|S|²|A|)) but may struggle with high-dimensional states (hence the Deep-RL extension uses approximations).
- **Failure signatures**:
  - Stagnant Reward after Shift: Bonus b_t remains low despite a shift → Density estimator p is not dropping (shift too subtle or window n too large).
  - High Variance Regret (Jitter): Bonus b_t fluctuates wildly in stationary phase → Density estimator p is unstable (window n too small).
- **First 3 experiments**:
  1. FrozenLake Shift: Implement DQUCB on FrozenLake. Hardcode a "slippery level" shift at episode K=20,000. Plot the likelihood p(s'|s,a) and bonus b_t over time to verify the "dip and spike" response.
  2. Window Ablation: Run the shift experiment with window sizes n ∈ {50, 100, 500}. Measure the "reaction time" (episodes to recover reward) vs. "stability" (regret variance pre-shift).
  3. Regret Comparison: Compare cumulative regret of DQUCB vs. Standard QUCB. Verify that DQUCB avoids the linear regret growth that Standard QUCB exhibits post-shift (as per Remark 4.2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more efficient density estimation techniques replace Kernel Density Estimation (KDE) to improve computational scalability in complex environments?
- Basis in paper: The conclusion states: "Future work includes tackling the limitation of density estimation with the kernel estimator..."
- Why unresolved: The authors note the current approach has O(KHn²) time complexity, which may be prohibitive for very large datasets or high-dimensional states compared to model-based baselines.
- What evidence would resolve it: A comparative study integrating near-linear time density estimators (e.g., Normalizing Flows) into DQUCB, demonstrating lower time complexity and competitive regret.

### Open Question 2
- Question: How does the DQUCB regret bound scale with the number of distribution shifts or a continuous variation budget?
- Basis in paper: Section 2.1 states, "For simplicity, our formal setting considers one time shift." While the authors claim results can be extended, they do not derive the bounds for multiple shifts.
- Why unresolved: The theoretical analysis decomposes regret into pre-shift and post-shift periods, relying on a static structure that does not account for accumulating error or dynamic adaptation across multiple shifts.
- What evidence would resolve it: A modified regret bound incorporating a term for the number of shifts L or variation budget B, and empirical validation on environments with frequent, non-discrete changes.

### Open Question 3
- Question: How sensitive is the performance of DQUCB to the density estimation error ε in high-dimensional continuous state spaces?
- Basis in paper: Theorems 4.1 and 4.3 show regret scales linearly with the estimator error (1+ε), and the "oracle" bound assumes ε→0.
- Why unresolved: While the method works empirically on CartPole, the theoretical reliance on a low ε suggests performance may degrade if density estimation becomes inaccurate in higher dimensions where KDE struggles.
- What evidence would resolve it: An ablation study measuring the correlation between the calculated density error ε and the resulting regret in high-dimensional Deep-RL benchmarks.

## Limitations
- The method relies on accurate density estimation for OOD detection, which becomes computationally expensive and less effective in high-dimensional state spaces
- Theoretical regret bounds assume perfect density estimation (ε→0), which may not hold in practice
- Window size n is a critical hyperparameter with a fundamental tradeoff: too small causes false positives and noisy estimates, too large causes slow shift detection

## Confidence
- **High Confidence**: Claims about improved empirical regret performance over baselines in low-dimensional GridWorld/FrozenLake tasks (supported by direct experimental evidence in Section 5)
- **Medium Confidence**: Claims about Deep-RL extension (CartPole) performance (limited to single task without ablations or comparisons to non-shift-aware baselines)
- **Medium Confidence**: Theoretical oracle regret bounds (proven under idealized density estimation assumptions that may not reflect real-world conditions)

## Next Checks
1. **Ablation Study on Window Size**: Run FrozenLake shift experiments with n ∈ {50, 100, 500}. Measure recovery time (episodes to regain optimal reward) vs. regret variance. Verify claims about tradeoff between detection speed and estimate stability.

2. **Density Estimation Failure Analysis**: Intentionally degrade the KDE by using high-dimensional states or small n. Measure impact on bonus stability and cumulative regret. Confirm the failure mode where DQUCB reverts to standard QUCB performance when density estimation fails.

3. **Non-Stationary Trace Test**: Implement a more complex non-stationary pattern (e.g., multiple shifts, gradual drift in ε from 0.01→0.2→0.05 over 50K episodes). Verify DQUCB maintains low regret across the entire trace while standard QUCB accumulates linear regret after each shift.