---
ver: rpa2
title: 'KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference'
arxiv_id: '2511.11907'
source_url: https://arxiv.org/abs/2511.11907
tags:
- cache
- memory
- disk
- reuse
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KVSwap enables long-context LM inference on resource-constrained
  devices by offloading KV cache to disk. It uses a compressed in-memory K cache to
  predict and preload critical KV entries, employs grouped KV access to reduce disk
  I/O fragmentation, and leverages a reuse buffer to cache recently accessed entries.
---

# KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference

## Quick Facts
- arXiv ID: 2511.11907
- Source URL: https://arxiv.org/abs/2511.11907
- Authors: Huawei Zhang; Chunwei Xia; Zheng Wang
- Reference count: 40
- Key outcome: KVSwap achieves up to 4.1× throughput improvement over baselines on eMMC and 1.8× on NVMe, while maintaining high generation quality and using 11× less KV cache memory than vLLM

## Executive Summary
KVSwap enables long-context language model inference on resource-constrained devices by offloading the KV cache to disk while maintaining high throughput through intelligent prefetching. The system uses a compressed low-rank K cache to predict critical KV entries, groups KV accesses to reduce disk I/O fragmentation, and employs a reuse buffer to cache recently accessed entries. Evaluation shows KVSwap outperforms existing disk-based KV cache offloading methods under tight memory budgets while maintaining generation quality.

## Method Summary
KVSwap compresses the K cache using low-rank SVD approximation to predict which KV entries to prefetch from disk. The system groups consecutive KV entries to amortize disk I/O overhead and leverages a reuse buffer to exploit temporal locality across decoding steps. A rolling buffer handles newly generated entries until they form complete groups. The method uses a greedy solver to tune offline parameters for optimal performance on target hardware.

## Key Results
- Achieves 4.1× throughput improvement over baselines on eMMC and 1.8× on NVMe
- Uses 11× less KV cache memory than vLLM while maintaining high generation quality
- Maintains 75-81% reuse rates across datasets, enabling 2.0-4.0× speedup on NVMe and eMMC
- Outperforms ShadowKV and Loki under tight memory budgets (1/34 of full cache)

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank K Cache for Predictive Prefetching
A compressed in-memory K cache enables accurate prediction of critical KV entries without maintaining full cache in memory. SVD-based projection reduces K cache dimension from (H_k × d) to r, where r ≪ H_k × d. The low-rank adapter A is precomputed offline. At runtime, approximate attention scores are computed using Q_h × A_g(h) × K_lr^T to identify which groups to prefetch. Low-rank structure preserves sufficient information to rank KV entry importance, even under aggressive compression. When compression ratio σ exceeds ~32 under tight budgets, prediction fidelity degrades, causing accuracy loss.

### Mechanism 2: Grouped KV Access for Disk Alignment
Grouping consecutive KV entries amortizes disk I/O overhead and mitigates read amplification. G consecutive tokens form a group scored by ReduceMax of member scores; TopK selects M groups. Group size G is tuned to match storage device characteristics (G=4 for NVMe, G=8 for eMMC). Storage controllers read larger physical units than requested; grouping aligns logical access with physical read boundaries. When G=0 or G=1, I/O utilization drops below 10% and throughput collapses.

### Mechanism 3: Reuse Buffer for Temporal Locality
Caching recently loaded KV groups exploits temporal redundancy across generation steps. Fixed-size slot table maps group IDs to memory slots; check slot table before disk load; FIFO replacement. Rolling buffer holds newly generated entries until they form complete groups. Critical KV groups exhibit temporal locality—overlap ratio between adjacent steps is high. Reuse rates 75-81% across datasets enable 2.0-4.0× speedup on NVMe, 3.8-4.0× on eMMC. If locality assumption fails, reuse rate drops and I/O costs dominate.

## Foundational Learning

- **Concept: KV Cache dynamics in autoregressive decoding**
  - Why needed here: The entire paper addresses KV cache memory wall; must understand that KV cache grows linearly with sequence length and batch size, unlike static model weights.
  - Quick check question: For a 4B model at 16K context with batch=4, why does KV cache (9 GiB) exceed weights (7.5 GiB)?

- **Concept: Storage read amplification**
  - Why needed here: Grouping mechanism exists to address read amplification where controllers read larger physical units than requested, wasting bandwidth.
  - Quick check question: Why does a 512-byte random read achieve <6% of peak disk bandwidth on both NVMe and eMMC?

- **Concept: Low-rank approximation via SVD**
  - Why needed here: Core to K cache compression; understanding that SVD decomposes K into U·diag(S)·V^T, with top-r singular vectors capturing dominant patterns.
  - Quick check question: If K_cache ∈ R^(N × 128) is compressed to r=4, what is the compression ratio σ and memory reduction factor?

## Architecture Onboarding

- **Component map:** Disk -> Memory (compressed K cache, rolling buffer, reuse buffer) -> Runtime (predictor, I/O engine, KV cache manager)

- **Critical path (per decoding step):**
  1. Predictor computes low-rank attention scores using compressed K cache + approximate Q
  2. ReduceMax per group → TopK selects M groups
  3. Check slot table → identify cache hits vs. misses
  4. I/O engine issues async disk loads for misses (overlapped with compute)
  5. KV cache manager updates mapping table
  6. Attention kernel uses combined: reuse buffer hits + newly loaded groups + rolling buffer entries

- **Design tradeoffs:**
  - Group size G: Larger G → higher throughput, lower accuracy (G=2 → 88.8% acc, G=12 → 83.3% acc)
  - Compression ratio σ: Higher σ → lower memory, potential quality loss
  - Selected groups M: Higher M → better accuracy, more I/O (M×G > 400 yields marginal accuracy gains)
  - Reuse buffer capacity C: More slots → higher reuse rate, more memory overhead

- **Failure signatures:**
  - eMMC saturation: Throughput plateaus at batch=8 (15.7→10.9 tokens/s from batch=8→16)
  - Quality collapse: ShadowKV-t and Loki-t show >50% accuracy loss under tight budgets
  - I/O dominance: InfiniGen variants show I/O:Compute ratio >100× on eMMC

- **First 3 experiments:**
  1. Group size ablation: Run with G ∈ {1, 2, 4, 8, 12} on NVMe and eMMC; measure throughput and accuracy on RULER tasks. Confirm G=4 optimal for NVMe, G=8 for eMMC.
  2. Reuse buffer analysis: Profile 100 inputs on QMSum and MuSiQue; measure reuse rate distribution and throughput with/without reuse. Target: validate 75-81% reuse rate claim.
  3. Latency breakdown profiling: Use Nsight Systems to decompose single-block decode latency into I/O, attention+FFN, predictor, and reuse operations. Compare FlexGen vs. InfiniGen* vs. KVSwap.

## Open Questions the Paper Calls Out

- **Prefilling extension**: How can the grouped critical KV prediction strategy be adapted to optimize the prefilling stage, and what are the resulting impacts on time-to-first-token (TTFT)? The current work targets iterative decoding, leaving prefilling extension as a proposed direction without implementation or benchmarks.

- **Quantization integration**: What are the performance and accuracy trade-offs when integrating KVSwap with low-bit KV cache quantization (e.g., 2-bit or 4-bit)? KVSwap can be combined with low-bit KV compression as they are orthogonal and complementary, but this combination is not evaluated.

- **Mobile thermal and power impact**: Does the increased disk I/O frequency required by KVSwap induce thermal throttling or excessive battery drain on commercial smartphones? Evaluation is conducted on NVIDIA Jetson AGX with active cooling, while the target domain includes battery-powered mobile devices.

## Limitations

- Compression-fidelity trade-off: Low-rank K cache approach assumes compressed representation can reliably identify critical KV entries, with clear degradation patterns at high compression (σ≈32)
- Device-specific optimization assumptions: Group sizes tuned to specific hardware characteristics may not generalize across storage controllers
- Temporal locality assumption: Reuse buffer effectiveness depends on temporal redundancy that may not hold for diverse workloads or heterogeneous batch compositions

## Confidence

- **High**: Disk I/O reduction through grouping (well-established storage engineering principle, supported by empirical bandwidth measurements)
- **Medium**: Low-rank prediction accuracy (validated on controlled benchmarks but with clear degradation patterns at high compression)
- **Medium**: Reuse buffer effectiveness (strong empirical support on tested datasets but assumption-dependent)
- **Low**: Cross-device generalization (optimization parameters are device-specific with limited validation across hardware variants)

## Next Checks

1. **Cross-device validation study**: Implement KVSwap on at least three distinct eMMC devices from different manufacturers. Measure throughput variation, group size sensitivity, and identify minimum/maximum performance envelopes.

2. **Workload diversity stress test**: Evaluate KVSwap on datasets with deliberately low temporal locality and heterogeneous batch compositions. Measure how reuse rates and accuracy degrade compared to controlled benchmark datasets.

3. **Real-time memory pressure simulation**: Run KVSwap under simulated memory contention scenarios where the rolling buffer competes with other system processes. Monitor whether fixed-size memory regions cause cascading failures or graceful degradation.