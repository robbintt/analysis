---
ver: rpa2
title: 'KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based
  Perturbations'
arxiv_id: '2507.08443'
source_url: https://arxiv.org/abs/2507.08443
tags:
- graph
- knowledge
- path
- perturbations
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KGRAG-Ex, an explainable retrieval-augmented
  generation system that leverages knowledge graphs to enhance both factual grounding
  and interpretability. The core idea is to construct domain-specific knowledge graphs
  from unstructured text and use them to guide retrieval, with perturbations applied
  at the graph level to assess component importance.
---

# KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations

## Quick Facts
- arXiv ID: 2507.08443
- Source URL: https://arxiv.org/abs/2507.08443
- Reference count: 19
- Primary result: KG-guided RAG with perturbations achieves 70% computational overhead reduction while maintaining explanation quality

## Executive Summary
KGRAG-Ex introduces an explainable retrieval-augmented generation system that leverages knowledge graphs to enhance both factual grounding and interpretability. The system constructs domain-specific knowledge graphs from unstructured text and uses them to guide retrieval, with perturbations applied at the graph level to assess component importance. Experiments show that sub-path removals have the strongest impact on outputs, and early path components and high-degree nodes are most influential in the reasoning process.

## Method Summary
The system builds knowledge graphs from medical documents by chunking text and using LLMs to extract triplets (Entity, Relation, Entity) with semantic labels. For each query, relevant entities are identified and grounded to KG nodes, then shortest paths between these nodes are extracted and converted into pseudo-paragraphs that guide corpus retrieval. Perturbations are applied at graph granularity by removing nodes, edges, or sub-paths, and the resulting output changes are measured to identify influential components. The approach outperforms traditional text-based perturbation methods in efficiency while maintaining comparable explanation quality.

## Key Results
- Sub-path removals caused output changes in 57/128 MedMCQA examples vs. 41 for nodes and 30 for edges
- Early path components are more influential than later ones in the reasoning process
- High-degree nodes and high-betweenness edges are most influential according to graph centrality metrics
- KGRAG-Ex required ~19-20 LLM calls vs. ~61-65 for RAG-Ex, with token usage reduced from ~3,600-4,000 to ~2,100-2,200

## Why This Works (Mechanism)

### Mechanism 1: Graph-Structured Retrieval via Semantic Path Extraction
Knowledge graph paths provide interpretable reasoning traces that guide retrieval more transparently than unstructured text alone. Queries are parsed into entities, grounded to KG nodes, and shortest paths between these nodes are extracted. These paths are converted into natural language "pseudo-paragraphs" that form the retrieval context. The core assumption is that the shortest path between query-relevant entities contains semantically meaningful intermediate concepts that improve retrieval grounding.

### Mechanism 2: Perturbation-Based Importance Attribution at Graph Granularity
Removing graph components (nodes, edges, or sub-paths) and measuring output changes reveals which elements are most critical to reasoning. Each component is independently removed, and if removal causes the model's answer to change, that component is flagged as influential. The core assumption is that importance can be approximated by sensitivity—components whose removal alters outputs are more critical to the reasoning chain.

### Mechanism 3: Computational Efficiency via Semantic-Level Perturbations
Operating on graph structures rather than text tokens reduces perturbation overhead significantly while preserving explanation quality. Instead of sliding-window token removal, the system removes semantically coherent units (nodes/edges/sub-paths), requiring fewer LLM calls per explanation. The core assumption is that graph components map to atomic semantic units whose removal is functionally equivalent to removing multiple tokens.

## Foundational Learning

- **Knowledge Graphs as (Entity, Relation, Entity) Triplets**: The entire pipeline relies on constructing, traversing, and perturbing KGs. You must understand how unstructured text becomes structured triplets with metadata.
  - *Quick check*: Given the sentence "Aspirin reduces fever by inhibiting prostaglandin synthesis," what triplets would you extract?

- **Perturbation-Based Explainability**: Core to the paper's contribution—measuring component importance via controlled removal and output comparison.
  - *Quick check*: If removing edge A causes no output change but removing edge B flips the answer from "C" to "D," which edge is more important and why?

- **Graph Centrality Metrics (Degree, Betweenness)**: The paper correlates component importance with structural metrics—high-degree nodes and high-betweenness edges are more influential.
  - *Quick check*: A node with degree 50 vs. a node with degree 2: which is more likely to be a "hub" and why might hubs be influential in reasoning paths?

## Architecture Onboarding

**Component map:**
User Query → Entity Extraction → KG Path Retrieval → Pseudo-Paragraph Generation → RAG Context Augmentation → LLM Answer → Perturbation Loop (multiple removals) → Explanation Output

**Critical path:**
User Query → Entity Extraction → KG Path Retrieval → Pseudo-Paragraph Generation → RAG Context Augmentation → LLM Answer → Perturbation Loop (multiple removals) → Explanation Output

**Design tradeoffs:**
- Sub-path vs. node perturbations: Sub-paths cause more output changes (higher sensitivity) but are less granular for pinpointing specific entities
- Path position bias: Early-path components are more influential (per experiments), so path ordering affects explanation quality
- Fallback overhead: When no path exists, standard retrieval is used—this preserves coverage but loses KG benefits
- KG construction cost: LLM-based extraction is expensive upfront; amortization depends on query volume

**Failure signatures:**
- Sparse KG fallback: If >30% of queries trigger fallback retrieval, KG coverage is insufficient for the domain
- Zero perturbation sensitivity: If removals rarely change outputs, either the model ignores context or the evaluation metric is too coarse
- Hub dominance: If high-degree nodes are always flagged as most important, explanations become generic rather than query-specific
- Path length explosion: If shortest paths exceed context window limits, pseudo-paragraphs will be truncated

**First 3 experiments:**
1. **Retrieval quality baseline**: Compare KGRAG-Ex (KG-guided) vs. standard chunk-based RAG on domain QA. Measure accuracy@k and retrieval precision.
2. **Perturbation sensitivity replication**: Run all three perturbation types on 50+ queries. Confirm: sub-path > node > edge in impact; early positions > late positions.
3. **Efficiency benchmark**: Instrument LLM call counts and token usage. Compare KGRAG-Ex perturbations against a text-window baseline. Target ≥50% reduction in calls.

## Open Questions the Paper Calls Out

### Open Question 1
How do additive perturbation techniques (adding edges, changing edge directions, introducing non-existing nodes) compare to removal-based perturbations for generating explanations in KG-based RAG systems?
The current work only evaluates removal-based perturbations (nodes, edges, sub-paths). Additive perturbations may reveal different aspects of model reasoning but remain unexplored.

### Open Question 2
How does the quality and accuracy of LLM-extracted knowledge graphs affect the faithfulness and reliability of generated explanations?
The KG is constructed via prompt-based LLM extraction, which may introduce errors or hallucinations. The paper does not evaluate how KG quality impacts explanation fidelity.

### Open Question 3
Does KGRAG-Ex generalize effectively to domains beyond medicine, and how do domain-specific characteristics influence explanation quality?
The paper focuses exclusively on the medical domain. Medical KGs have specific semantic structures that may not transfer to domains with different relational patterns.

## Limitations
- Efficiency claims lack external validation—no competing graph-perturbation methods are benchmarked
- KG construction costs (LLM-based triplet extraction) are not measured in the efficiency analysis
- Pseudo-paragraph generation method is underspecified with only conceptual description

## Confidence
- Mechanism 1 (Graph-Structured Retrieval): Medium — supported by related work but pseudo-paragraph conversion is novel and unverified
- Mechanism 2 (Perturbation-Based Attribution): High — well-defined process with clear experimental validation
- Mechanism 3 (Computational Efficiency): Low — efficiency claims lack external validation and KG construction costs are unmeasured

## Next Checks
1. Benchmark KGRAG-Ex perturbation efficiency against a standard sliding-window text perturbation baseline on the same dataset to verify the 70% reduction claim
2. Implement the pseudo-paragraph generation process and test whether shortest-path-based retrieval improves accuracy compared to chunk-based retrieval alone
3. Measure end-to-end computational overhead including KG construction costs to determine if upfront extraction expenses are amortized by query-time savings