---
ver: rpa2
title: Activation-Guided Local Editing for Jailbreaking Attacks
arxiv_id: '2508.00555'
source_url: https://arxiv.org/abs/2508.00555
tags:
- agile
- attacks
- malicious
- query
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AGILE, a two-stage jailbreak method that combines
  scenario-based generation and activation-guided local editing to bypass LLM safety
  mechanisms. In the first stage, a generator LLM creates a benign dialogue context
  and rephrases the malicious query to obscure its intent.
---

# Activation-Guided Local Editing for Jailbreaking Attacks

## Quick Facts
- arXiv ID: 2508.00555
- Source URL: https://arxiv.org/abs/2508.00555
- Reference count: 40
- Primary result: AGILE achieves state-of-the-art ASR, outperforming strongest baseline by up to 37.74%

## Executive Summary
This paper introduces AGILE, a two-stage jailbreak method that combines scenario-based generation and activation-guided local editing to bypass LLM safety mechanisms. In the first stage, a generator LLM creates benign dialogue context and rephrases malicious queries to obscure their intent. In the second stage, attention scores and hidden state activations guide subtle token substitutions and insertions to steer the model's internal representation from malicious to benign. Experiments on HarmBench show AGILE achieves state-of-the-art Attack Success Rate, with gains up to 37.74% over the strongest baseline, and excellent transferability to black-box models.

## Method Summary
AGILE operates in two phases: Contextual Scaffolding and Adaptive Rephrasing generate benign dialogue histories and transform malicious queries through structural changes, while the Editing Phase uses attention scores and activation classifiers to perform synonym substitution and token injection. The method trains MLP classifiers on target model activations to guide edits that minimize refusal signals while maintaining semantic similarity (τ=0.9). Default parameters use p=5 edits across N_cand=15 candidates.

## Key Results
- AGILE achieves state-of-the-art Attack Success Rate on HarmBench, outperforming strongest baseline by up to 37.74%
- Attention-guided editing outperforms random search by up to 9.85% on Llama-3.2-3B
- AGILE demonstrates excellent transferability to black-box models including Qwen, GLM, and Phi series
- AGILE remains effective against major defenses like Do Nothing, SafeDecoding, and CRAFT, though SafeDecoding reduces ASR by 53%

## Why This Works (Mechanism)

### Mechanism 1: Semantic Obfuscation via Scenario-Based Rephrasing
Rephrasing malicious queries with increased sentence complexity and embedding them in novel scenarios exploits out-of-distribution generalization failures in safety alignment. The generator LLM transforms queries through deep structural changes, increasing length and complexity to circumvent keyword-based detection while preserving harmful intent.

### Mechanism 2: Attention-Guided Synonym Substitution
Tokens with high attention from the final position disproportionately influence safety decisions; substituting these minimizes refusal signals. Attention scores are computed from the first layer's query vector of the last token to each input token's key vector, selecting top-p highest-attention tokens for synonym substitution.

### Mechanism 3: Activation-Space Benign Shift via Token Injection
Injecting tokens at low-attention positions can shift the model's final hidden state from a "malicious" region toward a "benign" region in activation space. A trained MLP classifier distinguishes benign vs. malicious activations, guiding token injection that pushes the hidden state toward benign-classified regions.

## Foundational Learning

- **Transformer Attention Mechanisms**: AGILE relies on computing attention scores from specific layers and positions to identify which tokens matter for safety decisions. Quick check: Why would the first layer's attention from the final token position be particularly informative for guiding edits?
- **Representation Learning / Hidden States**: The method trains classifiers on final-layer hidden states to predict refusal propensity and benign/malicious classification. Quick check: Why does the final token's hidden state aggregate information relevant to the model's generation decision?
- **Adversarial Transferability**: AGILE is designed to produce text-based attacks that transfer from white-box optimization to black-box targets. Quick check: Why would text-level edits guided by internal signals transfer better than directly optimized adversarial embeddings?

## Architecture Onboarding

- **Component map**: Generator LLM -> Refusal Classifier -> Malicious/Benign Classifier -> Editing Module
- **Critical path**: Generate N_cand candidates → compute attention scores → substitute top-p tokens → re-compute attention → inject at low-attention positions → enforce similarity threshold
- **Design tradeoffs**: Candidate count vs. efficiency (saturation around N_cand=15), edit count vs. semantic drift (p=5-7 optimal), classifier accuracy vs. overfitting (90-99% necessary)
- **Failure signatures**: High non-refusal rate but low ASR (model generates verbose but non-harmful responses), imaginative storytelling instead of harmful content (excessive edits push model into creative mode), SafeDecoding defense reduces ASR by 53%
- **First 3 experiments**: 1) Reproduce ablation without dialogue history to confirm contextual scaffolding is non-essential, 2) Test attention-layer selection comparing first-layer vs. middle-layer vs. last-layer attention, 3) Evaluate classifier robustness testing transfer to different architectures

## Open Questions the Paper Calls Out

### Open Question 1
To what extent is multi-turn conversational context actually utilized by LLM safety mechanisms if removing it has negligible impact on jailbreak success? The paper found that removing dialogue history resulted in comparable performance to the full method, contradicting the premise that gradually building context is key to bypassing alignment.

### Open Question 2
Is the lower attack success rate for harassment and misinformation categories inherent to the "vague" nature of typical queries, or can specific rephrasing overcome this? The paper hypothesizes that lower success is due to these queries being "vague" (asking for opinions) rather than "specific" (asking for instructions), but labels the validation "tentative."

### Open Question 3
Can activation-guided editing be adapted to maintain effectiveness against decoding-time defenses like SafeDecoding? The paper identified that SafeDecoding reduces ASR by over 53% because it "effectively counters AGILE's activation-guided mechanism" during decoding, but didn't explore if optimizing the injection loss against the "safe" tokens promoted by such defenses could bypass them.

## Limitations
- Method's reliance on classifier training may not generalize across model architectures or safety training paradigms
- Semantic similarity threshold of τ=0.9 is critical but not empirically justified
- Evaluation focuses on black-box transfer to similar model families, not cross-architecture transfer
- Paper doesn't explore adaptive defenses that might detect characteristic patterns of activation-guided editing

## Confidence

- **High confidence**: ASR improvements over baselines are well-supported by HarmBench results, particularly the 37.74% gain over PiF
- **Medium confidence**: Attention-guided editing outperforms random search by up to 9.85%, but depends on the assumption that first-layer attention patterns capture safety-critical information
- **Medium confidence**: Transferability mechanism's foundation is plausible but not directly compared to embedding-level attacks in the paper

## Next Checks

1. **Cross-architecture transfer test**: Apply AGILE-trained classifiers and editing strategies to fundamentally different model architectures (e.g., transformer-XL, Mamba, or hybrid architectures) to validate whether attention and activation patterns generalize beyond similar decoder-only transformers.

2. **Adaptive defense evaluation**: Implement defenses that specifically target the characteristics of activation-guided editing (e.g., monitoring for unusual attention patterns, detecting classifier-guided token substitutions) and measure whether these reduce ASR by more than the 53% achieved by SafeDecoding.

3. **Semantic drift threshold optimization**: Systematically vary the τ similarity threshold from 0.7 to 0.95 and measure the trade-off between ASR and semantic preservation across all six HarmBench categories, particularly for sensitive categories like harassment where the paper notes high non-refusal rates.