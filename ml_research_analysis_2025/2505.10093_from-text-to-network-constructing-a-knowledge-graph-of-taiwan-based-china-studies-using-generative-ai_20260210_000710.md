---
ver: rpa2
title: 'From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China
  Studies Using Generative AI'
arxiv_id: '2505.10093'
source_url: https://arxiv.org/abs/2505.10093
tags:
- knowledge
- system
- graph
- data
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops an AI-driven knowledge graph construction system
  to visualize and analyze 1,367 Taiwan-based China Studies articles (1996-2019).
  Using GPT-4o and Breeze-7B models, the system extracts subject-predicate-object
  triples from article metadata and abstracts, then applies preprocessing to remove
  low-value terms, merge semantic duplicates, and standardize abbreviations.
---

# From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI

## Quick Facts
- **arXiv ID:** 2505.10093
- **Source URL:** https://arxiv.org/abs/2505.10093
- **Reference count:** 9
- **Primary result:** An AI-driven knowledge graph system transforms 1,367 China Studies articles into an interactive D3.js network, revealing thematic clusters and enabling network-based literature navigation.

## Executive Summary
This study presents an end-to-end system for constructing and visualizing a knowledge graph from unstructured academic text. Using GPT-4o and Breeze-7B, the system extracts subject-predicate-object triples from article metadata and abstracts, then applies preprocessing to remove generic terms, merge semantic duplicates, and standardize abbreviations. The processed triples are visualized through an interactive D3.js network, allowing users to filter by node degree, search by keyword, and explore related nodes in real time. The approach offers a scalable alternative to traditional ontology construction and supports exploratory analysis of academic literature, uncovering research patterns and gaps in Taiwan-based China Studies.

## Method Summary
The system ingests 1,367 peer-reviewed China Studies articles (1996–2019) and uses GPT-4o or Breeze-7B to extract subject-predicate-object triples from their metadata and abstracts. Preprocessing removes low-value predicates (e.g., "result", "study"), merges semantic duplicates (e.g., "related-to" vs. "related to"), and standardizes abbreviations via a lookup table. The cleaned triples are loaded into a D3.js force-directed graph, with node radius mapped to degree centrality. Interactive features include degree-based filtering, keyword search with neighbor expansion, and real-time layout updates.

## Key Results
- The system successfully transforms unstructured article text into an interactive knowledge graph of 1,367 China Studies articles.
- Preprocessing significantly reduces semantic noise by filtering generic terms and merging duplicates.
- The D3.js visualization supports real-time, degree-based filtering and keyword-driven exploration of research themes.

## Why This Works (Mechanism)
The approach leverages large language models to automate the traditionally labor-intensive task of triple extraction from unstructured text. By combining LLM extraction with targeted preprocessing (frequency filtering, semantic merging, abbreviation standardization), the system generates a clean, navigable knowledge graph. The interactive D3.js visualization translates graph metrics (e.g., degree centrality) into intuitive visual cues, enabling users to explore research themes and relationships dynamically.

## Foundational Learning
- **Subject-Predicate-Object Triples**: Core unit for knowledge graph representation; needed to convert text into structured, queryable data. Quick check: Can triples be extracted consistently from new abstracts?
- **Semantic Merging**: Normalizes variations in predicates/terms; needed to reduce noise and improve graph clarity. Quick check: Are synonymous predicates merged without loss of meaning?
- **Degree Centrality**: Node importance metric; needed for filtering and highlighting influential concepts. Quick check: Do high-degree nodes correspond to key research themes?
- **Force-Directed Layout**: Visualizes graph structure via simulated physics; needed for intuitive network exploration. Quick check: Does layout stabilize and reveal clusters?
- **LLM Prompt Engineering**: Guides extraction accuracy; needed for high-quality triple generation. Quick check: Are prompts producing structured, JSON-formatted output?
- **Interactive Filtering**: Enables focused exploration; needed to manage large graphs and user attention. Quick check: Does filtering update the view without lag?

## Architecture Onboarding

**Component Map:**
LLM Extraction -> Preprocessing Pipeline -> D3.js Visualization

**Critical Path:**
1. Extract triples from article text using GPT-4o/Breeze-7B
2. Preprocess triples (filter, merge, deduplicate, standardize)
3. Load cleaned triples into D3.js force-directed graph
4. Apply interactive controls (degree filter, keyword search, neighbor expansion)

**Design Tradeoffs:**
- LLM extraction is fast but may introduce semantic noise; mitigated by rigorous preprocessing.
- Interactive visualization is intuitive but may lag with very large graphs; addressed by degree-based filtering and layout optimization.

**Failure Signatures:**
- High cardinality of unique predicates → "long tail" noise
- Visual lag or crashes with large triple counts
- Inconsistent semantic merging due to missing/abbreviated lookup tables

**3 First Experiments:**
1. Test LLM extraction with a small sample of abstracts; verify JSON output structure and completeness.
2. Apply preprocessing pipeline to extracted triples; measure reduction in unique predicates and duplicates.
3. Load cleaned data into D3.js; confirm force simulation runs smoothly and degree-based filtering works.

## Open Questions the Paper Calls Out
None

## Limitations
- The exact LLM prompts and manual resources (abbreviation table, semantic dictionary) are not provided, limiting reproducibility.
- Evaluation focuses on functional aspects (clarity, responsiveness) but does not assess semantic accuracy of extracted triples.
- Dataset (1,367 articles) is not publicly available, restricting external validation.

## Confidence
- **System Design and Architecture:** High — Clear, plausible pipeline with reasonable technical choices.
- **Usability Features:** High — Standard interactive features are well-documented and implementable.
- **Scalability Claims:** Medium — Asserted but not quantitatively supported.
- **Semantic Accuracy:** Low — No evaluation of extraction or preprocessing accuracy.

## Next Checks
1. **Replicate with Open Corpus:** Use a publicly available academic dataset and test the LLM extraction pipeline with disclosed prompts. Measure precision and recall of extracted triples against a manually annotated sample.
2. **A/B Test Preprocessing Impact:** Generate two versions of the graph (minimal vs. full preprocessing). Compare edge distribution and user task performance to quantify preprocessing benefits.
3. **Performance Benchmarking:** Instrument the D3.js visualization with real-time profiling (node/edge count, frame rate). Vary dataset size to identify scalability thresholds and document mitigation strategies.