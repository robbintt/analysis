---
ver: rpa2
title: 'Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained
  Models'
arxiv_id: '2512.03989'
source_url: https://arxiv.org/abs/2512.03989
tags:
- tokens
- latn
- tokenizer
- pruning
- vocabulary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of inefficient tokenizer adaptation
  for pre-trained language models when extending vocabulary for new domains or languages.
  The common approach of training a new tokenizer and appending non-overlapping tokens
  often results in unreachable or unused tokens, reducing compression efficiency.
---

# Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models

## Quick Facts
- arXiv ID: 2512.03989
- Source URL: https://arxiv.org/abs/2512.03989
- Reference count: 40
- The paper proposes "continued BPE training" and "leaf-based vocabulary pruning" to efficiently extend and adapt tokenizers for pre-trained models, improving compression and utilization.

## Executive Summary
This paper addresses the challenge of extending tokenizers for pre-trained language models when adapting to new domains or languages. The common approach of training a new tokenizer and appending non-overlapping tokens often creates unreachable tokens that waste vocabulary capacity. The authors propose "continued BPE training," which extends an existing tokenizer by continuing its BPE merge learning process on new data, ensuring all added tokens participate in meaningful merges. They also introduce "leaf-based vocabulary pruning" to safely reduce vocabulary size while preserving model quality. Experiments across 70 languages and multiple model families show up to 9.6% higher tokenization efficiency compared to naive extension methods.

## Method Summary
The paper proposes two main methods for tokenizer adaptation. First, "continued BPE training" extends a pre-trained tokenizer by continuing the BPE merge learning process on new data, ensuring newly added tokens participate in meaningful merges. This is implemented by tokenizing the target corpus with the existing tokenizer, counting pair frequencies within pre-tokenized segments, and iteratively adding merges until reaching the desired vocabulary size. Second, "leaf-based vocabulary pruning" removes redundant tokens while preserving model quality by identifying leaf tokens (those not participating in any merges) and removing them based on frequency in a target corpus. The methods are evaluated through tokenization efficiency metrics (bytes per token) and reachability tests across multiple languages and model architectures.

## Key Results
- Continued BPE training achieves up to 9.6% higher tokenization efficiency (bytes per token) compared to naive extension methods
- Leaf-based pruning enables removal of up to 62.5% of tokens without performance degradation
- 72.9%-100% of languages achieve better compression with continued training versus naive extension
- Unreachable tokens reduced by 4%-4.7% in vocabulary utilization compared to naive methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuing BPE training on domain-specific data yields higher tokenization efficiency than appending non-overlapping tokens from an independently trained tokenizer.
- Mechanism: BPE tokenization builds a deterministic merge sequence. Continued training resumes this process by counting pair frequencies from text already tokenized with the existing tokenizer, then iteratively adding merges. This ensures every new token participates in at least one merge the original tokenizer would have performed, integrating it into the tokenization graph. The naive method appends tokens that often become orphanedâ€”existing in the vocabulary but never produced by merges, effectively increasing size without improving compression.
- Core assumption: The target domain's text exhibits recurring patterns that can be captured by merging tokens from the existing tokenizer's vocabulary, and the efficiency gain outweighs the cost of a slightly larger embedding matrix.
- Evidence anchors: [abstract] Claims improved tokenization efficiency and better utilization of added vocabulary. [Page 3, Section 3.1] Details implementation for Byte-level BPE and SentencePiece models. Corpus evidence is weak on direct validation of this specific method.
- Break condition: Fails if the new domain's text is so novel that meaningful merges from the existing vocabulary are not possible, or if computational overhead of re-tokenizing training corpus is prohibitive.

### Mechanism 2
- Claim: Leaf-based vocabulary pruning reduces vocabulary size without creating unreachable tokens, preserving model quality better than naive frequency or index-based pruning.
- Mechanism: In a BPE tokenizer's merge graph, tokens that are never used as inputs to other merges are "leaves." Removing a non-leaf token would orphan all its descendants, making them unreachable. The leaf-based pruning algorithm iteratively identifies these leaf tokens, orders them for removal based on frequency in a target corpus (low frequency = prune first), and removes them from the vocabulary along with their single incoming merge rule. This is a structure-preserving operation that shrinks the graph without breaking any paths.
- Core assumption: A significant portion of a multilingual tokenizer's vocabulary consists of tokens that are rarely used for the target domain and are leaves in the merge graph for that domain's text, allowing their safe removal.
- Evidence anchors: [abstract] States they "introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality." [Page 4, Section 3.2 & Algorithm 1] Provides formal definition and operational steps. Corpus evidence for leaf-based pruning's superiority over other methods is absent.
- Break condition: Fails if the pruning set is so broad that very few tokens are leaves, or if the pruning order accidentally removes tokens critical for downstream tasks not represented in the frequency corpus.

### Mechanism 3
- Claim: The "naive" extension method creates many unreachable tokens because appended tokens are not integrated into the original tokenizer's merge sequence, leading to wasted vocabulary capacity.
- Mechanism: A BPE tokenizer strictly follows its ordered list of merge rules. When the naive method appends a token t_new from an auxiliary tokenizer, it creates merge rules for t_new. However, the existing tokenizer already has a long, fixed sequence of merges. If the text segment corresponding to t_new is already broken up by earlier merges in the original sequence, t_new will never be formed. The paper introduces the "Self-Tokenization Test" (STT) to quantify this: a token t is unreachable if tokenize(t) != [t].
- Core assumption: A significant number of tokens from a domain-specific tokenizer are not compatible with the merge structure of the base tokenizer.
- Evidence anchors: [Page 2, Section 1] Explains the problem with an example showing why adding certain tokens is useless. [Page 6, Table 3] Directly quantifies the problem, showing 4%-20.9% rate of unused added tokens for naive method versus 0%-7.1% for continued training. Corpus evidence for this specific incompatibility mechanism is absent.
- Break condition: The mechanism of unreachability would be less severe if the base tokenizer's merge sequence were very short or if the target language were so different that few conflicting merges existed in the base vocabulary.

## Foundational Learning

- **Concept: Byte-Pair Encoding (BPE) Tokenization**
  - Why needed here: BPE is the core algorithm this paper modifies. Understanding that it builds a vocabulary iteratively by merging the most frequent pairs of characters or existing tokens is essential to grasp how continued training and leaf-based pruning operate on its merge graph structure.
  - Quick check question: If a BPE tokenizer has the merge rule ("t", "h") -> "th", how will it tokenize the word "therapy" (assuming no other relevant merges)? What if it also has ("th", "e") -> "the"?

- **Concept: Tokenization Efficiency (Bytes per Token)**
  - Why needed here: This is the primary metric used to evaluate the success of the proposed methods. A higher bytes-per-token score means more information is packed into each token, leading to shorter sequences and potentially faster model processing.
  - Quick check question: Model A produces 100 tokens for a 500-byte document. Model B produces 125 tokens. Which model has higher tokenization efficiency?

- **Concept: Vocabulary Utilization and Unreachable Tokens**
  - Why needed here: The paper's central critique of the naive method is its creation of "useless" tokens. Understanding that a token in the vocabulary is not guaranteed to be produced during tokenization, and that this is a function of the merge graph structure, is crucial for appreciating the paper's contribution.
  - Quick check question: A tokenizer has tokens {a, b, ab} but its only merge rule is ("a", "b") -> "ab". Is the token ab reachable? Is a reachable? Now, suppose another token c exists with no merge rules leading to or from it. Is c reachable?

## Architecture Onboarding

- **Component map:** Tokenizer State (Vocabulary, Merges) -> Training Data (Frequency Counts) -> Continued Training Module (Outputs new merges) -> Modified Tokenizer. Secondary path: Tokenizer State -> Leaf-Based Pruning Module (Outputs tokens to prune) -> Smaller Tokenizer.

- **Critical path:** The primary workflow is Tokenizer -> Continued Training -> Modified Tokenizer. The secondary workflow is Tokenizer -> Leaf-Based Pruning -> Smaller Tokenizer. These can be combined: Tokenizer -> Prune -> Extend.

- **Design tradeoffs:**
  - **Extension Size:** Adding more tokens improves compression but increases the embedding matrix size (and thus model size and memory). Continued training is more efficient (more compression per added token) than the naive method, but the fundamental tradeoff remains.
  - **Pruning Aggressiveness:** More pruning reduces model size and may remove more noise, but risks degrading performance on tasks requiring the pruned vocabulary.
  - **Extension Method:** Continued training ensures all new tokens are used but requires re-tokenizing the training corpus to count pairs (can be slow). The naive method is faster (train new tokenizer, append) but creates dead tokens.

- **Failure signatures:**
  - High rate of unreachable tokens (from STT or usage analysis): Indicates the extension method is not properly integrating new tokens, a sign that naive extension was likely used or the implementation is flawed.
  - Degradation in source language performance: After pruning, if the target language set used for calculating frequencies was not representative of the desired tasks, the model may lose capability.
  - Unusual tokenization output: After pruning, if a non-leaf token was incorrectly removed, the tokenization of certain strings could change unexpectedly or fail.

- **First 3 experiments:**
  1. **Baseline Extension Comparison:** Take a base tokenizer (e.g., Llama-3) and a target language (e.g., Estonian). Extend it by 8000 tokens using both the naive method and continued training. Compare the bytes-per-token score on a held-out Estonian dataset (e.g., FLORES-200) and run the Self-Tokenization Test to quantify unreachable tokens.
  2. **Leaf-Based Pruning Validation:** Start with the base tokenizer. Prune 20% of its tokens using three methods: last-N (by ID), naive frequency, and leaf-based frequency. Measure the change in bytes-per-token for both the target language (e.g., English, if pruning for an English-centric model) and other held-out languages to ensure minimal degradation where desired.
  3. **End-to-End Impact Assessment:** For a small model (e.g., Llama-3.2 1B), apply leaf-based pruning to create a smaller tokenizer, then extend it with a moderate number of tokens for a new language. Initialize embeddings with FVT and perform a short period of continued pre-training. Evaluate downstream task performance compared to a model using the original tokenizer and one using a naively extended tokenizer.

## Open Questions the Paper Calls Out

- **Question:** When is it optimal to completely replace a tokenizer versus modifying it through extension and pruning?
  - Basis in paper: [explicit] The authors state in Section 2.3 and Section 5.1.1 that determining the best strategy between modifying or replacing tokenizers is left to future research.
  - Why unresolved: While the paper demonstrates that modification is efficient, it does not compare this against the strategy of full tokenizer replacement, which requires more extensive pre-training.
  - What evidence would resolve it: A comparative study evaluating downstream performance and training costs of modified tokenizers against fully replaced tokenizers across various data regimes.

- **Question:** How should the training process account for the reduced number of training tokens resulting from improved tokenization efficiency?
  - Basis in paper: [explicit] Section 5.1.1 identifies "how to account for the reduced number of training tokens" as an open question, noting that better compression reduces the total token count.
  - Why unresolved: While reduced tokens lower training costs, it is unclear if this reduction negatively impacts the model's ability to learn compared to training on a larger number of inefficient tokens.
  - What evidence would resolve it: Experiments that normalize for training duration or data exposure to isolate the effects of token count reduction on model convergence and quality.

- **Question:** Does continued BPE training retain its effectiveness in complex multilingual scenarios where multiple languages are added simultaneously?
  - Basis in paper: [inferred] The Limitations section notes that experiments were restricted to a bilingual setting (one language added to a base), leaving more complex multilingual scenarios unexplored.
  - Why unresolved: The method's efficacy is proven for single-language adaptation, but it is unknown if simultaneous adaptation to multiple languages causes merge conflicts or reduces compression gains.
  - What evidence would resolve it: Evaluating the method on a multi-language extension task (e.g., adding 5-10 languages at once) to measure compression efficiency and vocabulary utilization per language.

## Limitations
- The paper focuses primarily on compression efficiency and reachability metrics, with limited evaluation of downstream model performance impact across diverse architectures and tasks
- Continued BPE training requires computationally expensive re-tokenization of large corpora to count pair frequencies within pre-tokenized segments
- The paper does not deeply explore interactions with specialized tokenizer features like script-specific rules or character coverage mechanisms

## Confidence
**High Confidence:** The mechanism of continued BPE training producing more efficient tokenization than naive extension is well-explained and supported by quantitative metrics across a large and diverse corpus (70 languages, multiple models).

**Medium Confidence:** The claim that pruning preserves model quality is supported by COMET and task accuracy metrics on Llama-3.2 1B/3B, but the evaluation is limited in scope and does not test a wide range of model architectures or diverse downstream tasks.

**Low Confidence:** The scalability of continued BPE training to extremely large models or corpora is not directly addressed, and the computational overhead is not quantified.

## Next Checks
1. **End-to-End Model Performance Validation:** Implement continued BPE training and leaf-based pruning on a held-out model family (e.g., Qwen-2.5) not extensively covered in the paper. Perform continued pre-training with the modified tokenizer and evaluate on a broader suite of downstream tasks (e.g., SuperGLUE, multilingual benchmarks beyond FLORES-200) to directly measure the impact of tokenizer changes on final model performance.

2. **Computational Overhead Benchmarking:** Measure and report the wall-clock time and memory usage for (a) the continued BPE training process (re-tokenizing the corpus to count pairs) and (b) the leaf-based pruning algorithm on progressively larger models and corpora. Compare this to the time to train a new, independent tokenizer.

3. **Robustness to Edge Cases:** Design a targeted experiment to test the methods on languages or domains with extreme characteristics: (a) a language with a very different script or morphology than the base tokenizer (e.g., Chinese, Japanese), and (b) a domain with highly repetitive but domain-specific jargon (e.g., legal or medical text). Measure if continued BPE training still produces meaningful merges and if leaf-based pruning accidentally removes critical tokens in these edge cases.