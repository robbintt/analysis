---
ver: rpa2
title: Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image
  Classification
arxiv_id: '2511.02992'
source_url: https://arxiv.org/abs/2511.02992
tags:
- search
- hybrid
- space
- block
- pooling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid CNN-ViT search space for NAS to
  find efficient hybrid architectures for image classification in tinyML deployment.
  The search space covers hybrid CNN and ViT blocks to learn local and global information,
  as well as the novel Pooling block of searchable pooling layers for efficient feature
  map reduction.
---

# Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification

## Quick Facts
- arXiv ID: 2511.02992
- Source URL: https://arxiv.org/abs/2511.02992
- Authors: Mikhael Djajapermana; Moritz Reiber; Daniel Mueller-Gritschneder; Ulf Schlichtmann
- Reference count: 39
- Primary result: Novel hybrid CNN-ViT search space with Pool-ViT block achieves 87.1% CIFAR-10 accuracy with 80.5k parameters and 1.35s latency

## Executive Summary
This paper introduces a hybrid NAS search space combining CNN and Vision Transformer blocks for efficient tinyML image classification. The search space uniquely incorporates hybrid CNN and ViT blocks to capture both local and global information, along with a novel Pooling block that enables searchable pooling layers for efficient feature map reduction. Experimental results on CIFAR-10 demonstrate that the proposed search space can discover hybrid architectures outperforming traditional ResNet-based tinyML models under tight model size constraints, achieving 87.1% accuracy with 80.5k parameters and 1.35s latency compared to ResNet8's 86.5% accuracy with 75k parameters and 2.54s latency.

## Method Summary
The authors propose a hybrid NAS search space that combines CNN and ViT blocks to leverage both local and global information processing. A key innovation is the Pool-ViT block, which incorporates searchable pooling layers for efficient feature map reduction. The search space allows for the discovery of hybrid architectures that are shallower but wider than traditional ResNet-like models. The NAS framework explores different combinations of convolutional, transformer, and pooling operations to find optimal architectures under tight model size constraints. The search is conducted on the CIFAR-10 dataset with a focus on tinyML deployment considerations.

## Key Results
- Best-found hybrid model achieves 87.1% CIFAR-10 accuracy with 80.5k parameters and 1.35s latency
- Outperforms ResNet8 baseline (86.5% accuracy, 75k parameters, 2.54s latency) while using fewer parameters and lower latency
- Pool-ViT block design proves particularly effective, enabling models with better accuracy and lower latency
- Hybrid search space produces shallower but wider models compared to ResNet-like architectures

## Why This Works (Mechanism)
The hybrid search space works by combining the complementary strengths of CNNs (local feature extraction) and ViT blocks (global context modeling). The Pool-ViT block addresses the computational inefficiency of traditional ViT pooling by enabling searchable pooling layers that reduce feature map size without excessive computation. This allows the search space to discover architectures that can effectively balance local and global information processing while maintaining tinyML efficiency constraints. The shallower but wider architectures found by the search space suggest that the Pool-ViT block enables effective feature reuse and information preservation through increased channel width.

## Foundational Learning
**CNN-ViT hybrid architectures** - Combining convolutional layers for local feature extraction with transformer blocks for global context modeling; needed because pure CNNs lack global reasoning while pure ViTs are computationally expensive; quick check: verify the hybrid block can process both local patterns and long-range dependencies.

**NAS search space design** - The space of possible architectures that NAS can explore; critical for finding efficient tinyML models; quick check: ensure the search space includes both primitive operations and higher-level architectural patterns.

**TinyML constraints** - Tight limitations on model size, latency, and energy consumption for deployment on resource-constrained devices; defines the optimization objective; quick check: confirm parameter count and MACs stay within target device limits.

**ReLU-based linear MHSA** - A computationally efficient variant of multi-head self-attention using ReLU activation; reduces ViT computational complexity; quick check: verify MHSA operations don't dominate the overall latency budget.

**Pool-ViT block** - Novel pooling mechanism that integrates searchable pooling layers within ViT blocks; enables efficient feature map reduction; quick check: confirm pooling doesn't excessively degrade accuracy.

**Channel width scaling** - Adjusting the number of channels/filters in network layers; critical for balancing accuracy and efficiency; quick check: verify width scaling maintains representational capacity while controlling parameter count.

## Architecture Onboarding

**Component map**: Input -> Hybrid Block Sequence (Conv/CNN, Pool-ViT, MHSA blocks) -> Pooling Block -> Output

**Critical path**: Feature extraction path through hybrid blocks determines accuracy and latency; Pool-ViT blocks in the critical path enable efficient global context modeling while maintaining tinyML constraints.

**Design tradeoffs**: Shallower vs deeper architectures (shallower with Pool-ViT achieves better efficiency), local vs global information processing (CNN vs ViT blocks), width vs depth (wider channels compensate for reduced depth).

**Failure signatures**: Excessive pooling causing accuracy degradation, ViT blocks dominating latency budget, insufficient channel width leading to representational bottlenecks, search space limitations preventing discovery of optimal architectures.

**First experiment**: Test hybrid block combinations on a subset of CIFAR-10 to validate local+global processing effectiveness.

**Second experiment**: Benchmark Pool-ViT block alone against traditional pooling methods to quantify efficiency gains.

**Third experiment**: Conduct ablation study removing Pool-ViT to measure its specific contribution to overall performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do orthogonal compression techniques like quantization and pruning interact with the discovered hybrid architectures?
- Basis in paper: [explicit] The authors state in the conclusion that "quantization and pruning, can be applied to further improve the efficiency."
- Why unresolved: The current study evaluates the search space based on floating-point parameters and MACs without implementing these specific compression methods.
- What evidence would resolve it: A comparative analysis of accuracy and latency trade-offs after applying 8-bit/4-bit quantization and structured pruning to the best-found models.

### Open Question 2
- Question: Why does the inclusion of the Pool-ViT block bias the NAS towards generating shallower but wider network architectures?
- Basis in paper: [explicit] The authors observe this empirical trend in Figure 6 but explicitly note they "leave a more thorough and formal analysis for future work."
- Why unresolved: The paper identifies the correlation between the Pool-ViT block and increased width (channels) to counteract information loss, but does not provide a theoretical justification.
- What evidence would resolve it: An ablation study or theoretical analysis mapping the "information loss" of pooling layers to the optimal channel width scaling factor.

### Open Question 3
- Question: Can knowledge distillation significantly improve the accuracy of these hybrid models under tight parameter constraints?
- Basis in paper: [explicit] The authors suggest that "Knowledge distillation methods could improve the accuracy... under tight model size constraints."
- Why unresolved: The experiments utilize standard training without distillation, leaving the potential accuracy gains from teacher-student setups unexplored.
- What evidence would resolve it: Benchmarks comparing the validation accuracy of the hybrid models trained with and without a pre-trained teacher model.

### Open Question 4
- Question: Does the proposed search space generalize to more complex datasets or higher resolution images beyond CIFAR-10?
- Basis in paper: [inferred] The experiments are restricted to the CIFAR-10 dataset (32x32 resolution), while the authors claim general applicability for TinyML image classification.
- Why unresolved: The efficiency of the ReLU-based linear MHSA and Pool-ViT blocks may perform differently on complex tasks (e.g., ImageNet) where global context is more critical.
- What evidence would resolve it: Experimental results showing the search space's effectiveness and latency efficiency on standard higher-resolution benchmarks.

## Limitations
- Evaluation restricted to single dataset (CIFAR-10) limiting generalizability
- Search methodology and hyperparameters not fully detailed, affecting reproducibility
- Pooling block contribution not isolated through ablation studies
- No comprehensive analysis of individual hybrid component contributions

## Confidence
**High Confidence**: The basic premise that hybrid CNN-ViT architectures can be discovered through NAS and that such architectures can outperform pure CNN models under tight constraints
**Medium Confidence**: The specific performance claims for the Pool-ViT block and the superiority of hybrid architectures over pure CNN/ViT approaches
**Medium Confidence**: The architectural insights about shallower but wider models being effective in the hybrid space

## Next Checks
1. Replicate experiments on additional datasets (CIFAR-100, TinyImageNet) to verify generalizability across different data distributions
2. Conduct an ablation study isolating the Pooling block's contribution by comparing hybrid search spaces with and without this component
3. Test the discovered architectures under different hardware constraints and deployment scenarios to validate tinyML deployment claims