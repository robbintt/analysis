---
ver: rpa2
title: Multi-view Graph Condensation via Tensor Decomposition
arxiv_id: '2508.14330'
source_url: https://arxiv.org/abs/2508.14330
tags:
- graph
- condensation
- tensor
- graphs
- gctd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-view Graph Condensation via Tensor
  Decomposition (GCTD), a novel approach to reduce large graphs for Graph Neural Networks
  (GNNs) by leveraging tensor decomposition instead of the typical bi-level optimization.
  GCTD creates a multi-view graph through random edge perturbations, decomposes it
  using non-negative RESCAL, and uses K-Means clustering on the factor matrix to synthesize
  a smaller graph while preserving node relationships.
---

# Multi-view Graph Condensation via Tensor Decomposition

## Quick Facts
- arXiv ID: 2508.14330
- Source URL: https://arxiv.org/abs/2508.14330
- Reference count: 40
- Primary result: Outperforms existing baselines on three datasets with up to 4.0% improvement in accuracy

## Executive Summary
This paper introduces Multi-view Graph Condensation via Tensor Decomposition (GCTD), a novel approach to reduce large graphs for Graph Neural Networks (GNNs) by leveraging tensor decomposition instead of the typical bi-level optimization. GCTD creates a multi-view graph through random edge perturbations, decomposes it using non-negative RESCAL, and uses K-Means clustering on the factor matrix to synthesize a smaller graph while preserving node relationships. Extensive experiments on six real-world datasets show GCTD outperforms existing baselines on three datasets with up to 4.0% improvement in accuracy, achieves lossless performance on Citeseer, Pubmed, and Flickr, and remains competitive on larger graphs like Reddit and Ogbn-arxiv. The method is more interpretable and computationally efficient than prior approaches.

## Method Summary
GCTD constructs a multi-view tensor by augmenting the original adjacency matrix with K randomly perturbed views (edge additions/removals), then applies non-negative RESCAL tensor decomposition to learn node factor matrices. The method clusters these factors using K-Means to assign original nodes to synthetic nodes, aggregates the core tensor to form a condensed adjacency matrix, and averages features within each cluster. The approach is trained using sparse reconstruction with negative sampling and evaluated by training a GCN on the condensed graph and measuring accuracy on the original test set.

## Key Results
- Outperforms existing baselines on three datasets with up to 4.0% improvement in accuracy
- Achieves lossless performance on Citeseer, Pubmed, and Flickr
- Remains competitive on larger graphs like Reddit and Ogbn-arxiv

## Why This Works (Mechanism)

### Mechanism 1
Multi-view graph tensorization via random perturbation enhances robustness by capturing structural invariances. The method constructs a tensor by stacking the original adjacency matrix with K randomly perturbed views, forcing the decomposition to find latent factors that explain graph structure across variations, reducing overfitting to specific edge noise. The core assumption is that essential graph properties for downstream tasks are invariant to minor random perturbations in edge connectivity. Break condition: if perturbation probability is too high, structural integrity is destroyed and the model learns noise.

### Mechanism 2
Decomposition with non-negativity constraints functions as co-clustering. By enforcing non-negativity on the factor matrix via ReLU, the model performs parts-based representation where rows act as soft cluster memberships. K-Means on this matrix explicitly groups original nodes into synthetic nodes, preserving a mapping that bi-level optimization methods often lose. The core assumption is that nodes with similar connectivity patterns are semantically similar and can be merged. Break condition: if condensation ratio is too aggressive, K-Means cannot separate distinct classes, leading to label noise.

### Mechanism 3
RESCAL tensor decomposition preserves relational dynamics between latent components across multiple views better than matrix factorization. The RESCAL model learns a shared factor matrix and view-specific core slices, then averaging the core tensor aggregates these multi-view interactions into a stable synthetic adjacency matrix. The core assumption is that interaction between latent node clusters varies sufficiently across views to require a tensor model but can be aggregated into a single representative structure. Break condition: on extremely large and dense graphs, dense matrix multiplications become prohibitive despite sparse sampling.

## Foundational Learning

- **Concept: Tucker / RESCAL Tensor Decomposition**
  - Why needed here: This is the mathematical engine replacing the GNN training loop. You must understand how the "Core Tensor" captures interactions and how "Factor Matrices" capture entities to debug the condensation quality.
  - Quick check question: In the equation $\mathcal{X} \approx \mathcal{R} \times_1 U \times_2 U$, what does the core tensor $\mathcal{R}$ represent if $U$ represents the nodes?

- **Concept: Negative Sampling in Reconstruction**
  - Why needed here: The paper reconstructs only observed entries to save memory. Without negative samples (zero-entries), the model would trivially predict 1 for everything to minimize loss.
  - Quick check question: Why does the paper generate negative samples in a 1:1 proportion to non-zero values during the sparse reconstruction step?

- **Concept: Graph Homophily**
  - Why needed here: The paper notes that GCTD reduces homophily in the synthetic graph. Understanding this helps explain why the synthetic graph worksâ€”it forces the GNN to rely on learned weights/features rather than just neighbor averaging.
  - Quick check question: Why might generating a synthetic graph with *lower* homophily than the original be beneficial for training a robust GNN?

## Architecture Onboarding

- **Component map:** Tensorizer -> Decomposer -> Synthesizer
- **Critical path:**
  1. Pre-compute negative samples (Indices of zeros in $\mathcal{X}$) to prevent overfitting.
  2. Optimize decomposition until loss change $< 10^{-7}$ (requires monitoring convergence closely).
  3. Post-process: K-Means initialization heavily influences class balance; check underrepresented class assignment logic immediately after this step.
- **Design tradeoffs:**
  - **Sparse vs. Dense Decoding:** The paper uses sparse decoding for memory efficiency. Tradeoff: Requires negative sampling, which adds implementation complexity and noise to the gradient.
  - **Argmax vs. K-Means:** K-Means is slower but yields significantly better accuracy (+5-10% in ablations). Tradeoff: Latency during the synthesis step.
- **Failure signatures:**
  - **Mode Collapse:** ReLU constraints are too aggressive, causing $U$ or $\mathcal{R}$ to become all-zeros. (Fix: Check learning rate/init).
  - **OOM on Large Graphs:** Pre-computing the tensor $\mathcal{X}$ for many views on Reddit-sized data. (Fix: Generate views on-the-fly or store implicitly).
  - **Lossless Failure:** Accuracy drops significantly on Ogbn-arxiv compared to baselines. (Paper notes this is a current limitation; likely due to simple averaging of features failing on high-dimensional complex tasks).
- **First 3 experiments:**
  1. **Sanity Check (Citeseer):** Run GCTD with $r=1.8\%$. Verify that the "Multi-view" version (K=3) outperforms "Single-view" (K=1) to confirm the pipeline works.
  2. **Ablation on Sampling:** Remove the negative sampling ratio (set to 0) and observe if the reconstructed adjacency becomes a dense matrix of near-1 values.
  3. **Scalability Profiling:** Measure peak memory usage during the "Decomposer" phase on Reddit. Confirm sparse operations are actually being triggered (check for dense tensor allocation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does replacing the hard node assignment strategy with a soft membership approach improve the effectiveness of GCTD?
- Basis in paper: The authors state, "we plan to investigate whether replacing the current hard assignment with a soft membership approach can improve the effectiveness of GCTD."
- Why unresolved: The current K-Means clustering forces nodes into single synthetic nodes, potentially discarding nuanced relational information.
- What evidence would resolve it: A modified GCTD using soft clustering (e.g., probabilistic assignment) showing higher accuracy on standard benchmarks.

### Open Question 2
- Question: Can alternative tensor decomposition techniques yield higher-quality condensed graphs than the non-negative RESCAL method?
- Basis in paper: The conclusion notes the intention to "explore different decomposition techniques to generate condensed graphs."
- Why unresolved: The study relies on a specific variant (non-negative RESCAL), leaving the efficacy of others (e.g., CP, Tucker) untested.
- What evidence would resolve it: Comparative experiments where GCTD is implemented with different decomposition backbones and evaluated on downstream tasks.

### Open Question 3
- Question: Do alternative data augmentation strategies improve the quality of the synthesized graph more effectively than random edge perturbation?
- Basis in paper: The authors plan to "explore different ways of performing augmentation to improve the quality of the synthesized graph."
- Why unresolved: The method currently uses random edge addition/removal; other augmentation types (e.g., attribute masking) might preserve information better.
- What evidence would resolve it: Ablation studies testing various augmentation pipelines within the GCTD framework against the random perturbation baseline.

### Open Question 4
- Question: Why does GCTD underperform on very large graphs like Ogbn-arxiv, and can architectural adjustments close this gap?
- Basis in paper: Table 2 shows GCTD underperforms on Ogbn-arxiv (58.2% vs. 66.2% for baselines), and the text acknowledges it "does not surpass the strongest baselines" on this dataset.
- Why unresolved: The decomposition or augmentation strategy may not scale effectively to graphs with hundreds of thousands of nodes.
- What evidence would resolve it: A variant of GCTD that matches or exceeds baseline performance on large-scale graphs (100k+ nodes).

## Limitations

- Underperforms on very large graphs like Ogbn-arxiv compared to strongest baselines
- Losslessness claim is dataset-dependent and fails on complex datasets
- Computational efficiency advantage over bi-level optimization is claimed but not directly benchmarked against all major baselines

## Confidence

- **High Confidence**: Multi-view tensorization mechanism and K-Means clustering approach are well-supported by ablation studies
- **Medium Confidence**: RESCAL decomposition advantage over simpler methods is supported by empirical results but lacks direct comparison to alternative tensor decompositions
- **Low Confidence**: Claim of being "more interpretable" than prior approaches is stated but not quantitatively validated

## Next Checks

1. **Scale Sensitivity Test**: Run GCTD on Ogbn-arxiv with condensation ratio r=0.1% to determine if the method fails at extreme compression levels or if hyperparameter tuning can recover performance.

2. **Runtime Benchmark**: Measure wall-clock time for GCTD versus Bi-level optimization methods on Reddit-sized graphs to validate the computational efficiency claim.

3. **Perturbation Robustness**: Systematically vary p_r and p_a values (0.05 to 0.3) on Citeseer to quantify the sensitivity of accuracy to perturbation magnitude and identify optimal ranges.