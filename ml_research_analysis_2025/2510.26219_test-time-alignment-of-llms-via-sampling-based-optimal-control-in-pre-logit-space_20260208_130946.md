---
ver: rpa2
title: Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit
  space
arxiv_id: '2510.26219'
source_url: https://arxiv.org/abs/2510.26219
tags:
- aisp
- reward
- sampling
- alignment
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses test-time alignment of large language models
  (LLMs) by proposing a new sampling-based method called Adaptive Importance Sampling
  on Pre-logits (AISP). AISP formulates LLM alignment as a stochastic optimal control
  problem where Gaussian perturbations are applied to pre-logits (outputs of the penultimate
  layer) to maximize expected rewards.
---

# Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space

## Quick Facts
- arXiv ID: 2510.26219
- Source URL: https://arxiv.org/abs/2510.26219
- Reference count: 40
- Primary result: AISP outperforms best-of-n sampling in reward values and achieves higher rewards than other reward-based test-time alignment methods like RE-Control and ARGS

## Executive Summary
This paper introduces Adaptive Importance Sampling on Pre-logits (AISP), a novel test-time alignment method for large language models that formulates alignment as a stochastic optimal control problem in the pre-logit space. The method applies Gaussian perturbations to the outputs of the penultimate layer and uses adaptive importance sampling to iteratively update the perturbation distribution, maximizing expected rewards without updating model parameters. Experiments demonstrate that AISP achieves higher reward scores than baselines including best-of-n sampling and other reward-based methods while maintaining better coherence.

## Method Summary
AISP applies Gaussian perturbations to pre-logit vectors (penultimate layer outputs) of a frozen LLM, treating test-time alignment as a stochastic optimal control problem. The method samples multiple trajectories by adding noise to pre-logits, evaluates them using a reward model, and updates the perturbation distribution using adaptive importance sampling over κ iterations. The final output is selected as the best response from all sampled trajectories. The approach is grounded in Model Predictive Path Integral Control (MPPI) and optimizes the mean of the Gaussian perturbation distribution to maximize expected rewards while maintaining a KL-divergence constraint to the base model.

## Key Results
- AISP consistently outperforms best-of-n sampling across various numbers of used samples in terms of reward values
- AISP achieves higher rewards than other reward-based test-time alignment methods like RE-Control and ARGS
- A batched variant of AISP is competitive with BoN under the same computational budget

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting Gaussian perturbations into the continuous pre-logit space allows for smoother, gradient-free exploration of the reward landscape compared to discrete token-space methods.
- **Mechanism:** The method intercepts the output of the penultimate layer ($z_t$) and adds a stochastic control input $v_t \sim \mathcal{N}(u_t, \sigma^2 I)$ before the final softmax operation. By optimizing the mean $u_t$ of this noise, AISP shifts the probability mass of the token distribution without altering model weights.
- **Core assumption:** The optimal perturbation trajectory can be modeled as a Gaussian distribution, and the resulting pre-logit distribution remains tractable for importance sampling.
- **Evidence anchors:** Abstract states Gaussian perturbations are applied to maximize expected rewards; Section 3.1 defines the stochastic control input formulation.
- **Break condition:** If perturbation variance $\sigma^2$ is too high, the token distribution may degrade into noise; if too low, exploration stalls.

### Mechanism 2
- **Claim:** The optimal control signal (perturbation mean) can be approximated by re-weighting sampled trajectories based on their rewards, minimizing the KL-divergence to an implicit optimal distribution.
- **Mechanism:** AISP leverages Importance Sampling to approximate the expectation of the optimal control. It samples trajectories, evaluates them with a reward model, and computes weights using a softmax-like function of the reward. The mean for the next iteration is updated as the weighted average of the sampled noise trajectories.
- **Core assumption:** The reward model provides a sufficiently accurate signal to distinguish high-quality trajectories, and the "free energy" formulation bounds the optimization objective.
- **Evidence anchors:** Section 3.3 states optimal mean is obtained by importance sampling with sampled rewards; Section 3.2 derives the connection between free energy and KL-constrained reward objective.
- **Break condition:** If the reward model is uninformative or corrupted, the weights will fail to select beneficial perturbations, collapsing the method to random search.

### Mechanism 3
- **Claim:** Iteratively updating the proposal distribution (Adaptive IS) significantly improves sample efficiency over static sampling methods like Best-of-N.
- **Mechanism:** Instead of sampling $N$ trajectories from a fixed distribution, AISP runs for $\kappa$ iterations. It uses the weighted mean from iteration $k$ as the center for sampling in iteration $k+1$. This progressively shifts the search space toward high-reward regions.
- **Core assumption:** The high-reward region is connected in the pre-logit space such that iterative steps lead to local optima.
- **Evidence anchors:** Section 5.2 Figure 3 shows AISP surpassing BoN as iterations increase; Section 3.3 Eq. 14 defines the iterative update rule.
- **Break condition:** If the proposal distribution narrows too quickly, the system may converge to a local optimum prematurely ("mode collapse").

## Foundational Learning

- **Concept:** Importance Sampling (IS)
  - **Why needed here:** AISP relies on IS to estimate the expected value of the optimal control distribution using samples from a proposal distribution.
  - **Quick check question:** Can you explain why we need to multiply by a weight $w(x) = p(x)/q(x)$ when estimating an expectation under $p$ using samples from $q$?

- **Concept:** Model Predictive Path Integral Control (MPPI)
  - **Why needed here:** AISP is derived from MPPI, which treats control as an inference problem and uses sampling to solve it.
  - **Quick check question:** In MPPI, how does the temperature parameter $\lambda$ affect the sensitivity of the control input to the cost/reward?

- **Concept:** Pre-logits (Penultimate Layer Activations)
  - **Why needed here:** The paper operates on $z_t$ (pre-logits) rather than logits or embeddings to ensure the perturbed distribution is Gaussian and the KL constraint is tractable.
  - **Quick check question:** Why is adding noise to pre-logits (before softmax) theoretically cleaner for Gaussian modeling than adding noise directly to logits?

## Architecture Onboarding

- **Component map:** Prompt Input -> Initial Sampling (Û=0) -> Generate n trajectories with noise -> Reward Calculation -> Weight Calculation (Eq. 13) -> Update Û -> Repeat κ times
- **Critical path:** Prompt Input → Initial Sampling (Û=0) → Generate n trajectories with noise → Reward Calculation → Weight Calculation (Eq. 13) → Update Û → Repeat κ times
- **Design tradeoffs:**
  - Parallelism vs. Latency: AISP requires κ sequential updates, whereas BoN is fully parallel. "Batched AISP" mitigates this by processing multiple prompts in parallel with smaller n.
  - Exploration vs. Coherence: The parameter λ balances maximizing reward vs. staying close to the base model (KL penalty). Small λ allows high deviation but risks incoherence.
- **Failure signatures:**
  - Reward Hacking: Extremely high rewards but low coherence scores (Table 1, ARGS baseline shows this risk; AISP mitigates it but remains susceptible).
  - Numerical Instability: If λ is small and rewards are large, the softmax weights in Eq. 13 may explode. The paper suggests relaxing the constraint with α (Section 3.6) to fix this.
- **First 3 experiments:**
  1. Unit Test (Weights): Verify Eq. 13 by sampling 10 trajectories, assigning dummy rewards, and confirming the updated mean Û moves toward the highest-reward sample.
  2. Hyperparameter Sensitivity: Run AISP with varying λ (e.g., 0.1 to 10.0) on a small validation set to observe the trade-off between Reward increase and KL divergence (replicate Table 3 trends).
  3. Efficiency Benchmark: Compare "Samples to Solution" (reward threshold) for AISP (κ × n) vs. BoN (N) to validate the efficiency claims in Figure 3.

## Open Questions the Paper Calls Out

- **Question:** How can AISP be effectively combined with parameter-efficient fine-tuning (PEFT) or reinforcement learning from human feedback (RLHF) to further enhance alignment?
  - **Basis in paper:** The conclusion states, "future work could include combinations of AISP and fine-tuning, and different importance sampling techniques."
  - **Why unresolved:** The paper focuses exclusively on test-time alignment without updating LLM parameters; the potential synergy or interference between dynamic test-time control and static fine-tuned weights remains unexplored.
  - **What evidence would resolve it:** Experiments comparing the reward scores and KL divergence of base LLMs + AISP against RLHF-aligned LLMs + AISP on standard benchmarks like HH-RLHF.

- **Question:** Can alternative importance sampling techniques, such as Sequential Monte Carlo (SMC) or normalizing flows, improve the proposal distribution efficiency over the current Gaussian-based adaptive method?
  - **Basis in paper:** The conclusion explicitly lists "different importance sampling techniques" as a direction for future work.
  - **Why unresolved:** The current method relies on a specific adaptive importance sampling mechanism derived from Model Predictive Path Integral Control (MPPI); whether other sampling algorithms can better handle the high-dimensional pre-logit space is not investigated.
  - **What evidence would resolve it:** An ablation study replacing the Gaussian proposal distribution with non-Gaussian alternatives (e.g., flows) and measuring the sample efficiency (reward achieved per number of samples).

## Limitations
- Computational cost: Requires κ × n samples sequentially, making it more expensive than single-shot methods like BoN
- Dependence on reward model quality: Performance heavily relies on the accuracy and informativeness of the reward model
- Lack of theoretical guarantees: No proof of convergence to global optima in the non-convex pre-logit space
- Sequential bottleneck: The κ iterations must be executed sequentially, limiting real-time applications

## Confidence
- **High Confidence:** The mathematical formulation of AISP as a stochastic optimal control problem is rigorous and well-supported by the MPPI literature. The connection between importance sampling and the optimal control distribution (Theorem 3.1) is theoretically sound.
- **Medium Confidence:** The empirical claims of superior reward performance over BoN and ARGS are supported by the experimental results, but the evaluation relies on reward model scores rather than human preference data. The efficiency gains shown in Figure 3 are convincing, though the absolute magnitude depends heavily on hyperparameter choices.
- **Low Confidence:** The claim that AISP is "competitiv[e] with BoN under the same computational budget" for the batched variant is less certain, as the comparison methodology and exact computational accounting are not fully detailed in the main text.

## Next Checks
1. **Reward Model Dependence Test:** Run AISP with multiple reward models (including a synthetic "oracle" reward) on the same alignment tasks to quantify how much performance varies with reward model quality. This would validate whether AISP's gains are robust or reward-dependent.

2. **Mode Collapse Analysis:** Systematically vary the temperature parameter λ and perturbation variance σ² to map the phase transition where AISP transitions from exploration to premature convergence. Measure KL divergence and reward diversity across this parameter space.

3. **Cross-Domain Generalization:** Apply AISP to a completely different alignment objective (e.g., toxicity reduction or stylistic control) beyond the helpfulness datasets used in the paper to test whether the method generalizes beyond its training domain.