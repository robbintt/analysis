---
ver: rpa2
title: 'Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for
  Exaggerated Refusals in LLMs'
arxiv_id: '2510.08158'
source_url: https://arxiv.org/abs/2510.08158
tags:
- prompts
- refusal
- safe
- safety
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of exaggerated safety behaviors
  in large language models (LLMs), where models produce false refusals by declining
  benign requests that contain terms resembling unsafe queries. To diagnose and mitigate
  these failures, the authors introduce two comprehensive benchmarks: the Exaggerated
  Safety Benchmark (XSB) for single-turn prompts, annotated with "Focus" keywords
  to identify refusal-inducing triggers, and the Multi-turn Scenario-based Exaggerated
  Safety Benchmark (MS-XSB), which evaluates refusal calibration in realistic, context-rich
  dialog settings.'
---

# Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs

## Quick Facts
- **arXiv ID**: 2510.08158
- **Source URL**: https://arxiv.org/abs/2510.08158
- **Reference count**: 14
- **Primary result**: Exaggerated refusals persist across diverse LLMs, especially in multi-turn scenarios; mitigation strategies improve compliance on safe prompts while maintaining safety.

## Executive Summary
This paper addresses the problem of exaggerated safety behaviors in large language models, where models decline benign requests that contain terms resembling unsafe queries. To diagnose and mitigate these failures, the authors introduce two comprehensive benchmarks: the Exaggerated Safety Benchmark (XSB) for single-turn prompts, annotated with "Focus" keywords to identify refusal-inducing triggers, and the Multi-turn Scenario-based Exaggerated Safety Benchmark (MS-XSB), which evaluates refusal calibration in realistic, context-rich dialog settings. The benchmarks reveal that exaggerated refusals persist across diverse LLMs, especially in complex, multi-turn scenarios.

To mitigate these failures, the authors leverage post-hoc explanation methods—SHAP, feature ablation, and integrated gradients—to identify refusal triggers and deploy three lightweight, model-agnostic approaches at inference time: ignore-word instructions, prompt rephrasing, and attention steering, all without retraining or parameter access. Experiments on four instruction-tuned Llama models demonstrate that these strategies substantially improve compliance on safe prompts while maintaining robust safety protections. For example, on safe prompts in XSB, baseline compliance rates ranged from 86.8% to 94.0%, which increased to 91.6%–96.8% after mitigation. The findings establish a reproducible framework for diagnosing and mitigating exaggerated refusals, highlighting practical pathways to safer and more helpful LLM deployments.

## Method Summary
The authors introduce two benchmarks—XSB and MS-XSB—to diagnose exaggerated refusals. XSB contains single-turn prompts annotated with "Focus" keywords that trigger refusals, while MS-XSB uses realistic, multi-turn scenarios to evaluate refusal calibration. They employ post-hoc explanation methods (SHAP, feature ablation, integrated gradients) to identify refusal triggers, then apply three lightweight, model-agnostic mitigation strategies at inference time: ignore-word instructions, prompt rephrasing, and attention steering. Experiments on four Llama models show these strategies improve compliance on safe prompts while maintaining safety.

## Key Results
- Exaggerated refusals persist across diverse LLMs, especially in multi-turn scenarios.
- Baseline compliance rates on safe prompts in XSB ranged from 86.8% to 94.0%, increasing to 91.6%–96.8% after mitigation.
- Mitigation strategies maintain robust safety protections while improving compliance on benign prompts.

## Why This Works (Mechanism)
The paper leverages post-hoc explanation methods to identify specific trigger words or phrases that cause exaggerated refusals. By understanding which tokens or phrases are responsible for the refusal, lightweight interventions can be applied at inference time without retraining. These interventions—ignoring trigger words, rephrasing prompts, or steering attention away from triggers—allow the model to process the underlying benign request while preserving safety for truly harmful content. The approach is model-agnostic and does not require access to model parameters, making it broadly applicable.

## Foundational Learning
- **Exaggerated refusals**: When LLMs incorrectly refuse benign prompts due to the presence of terms resembling unsafe queries. *Why needed*: Identifying the core problem being addressed. *Quick check*: Does the prompt contain only safe content but is still refused?
- **Focus keywords**: Specific words or phrases in prompts that trigger exaggerated refusals. *Why needed*: Enables targeted mitigation strategies. *Quick check*: Can you identify the exact trigger in a refused prompt?
- **Post-hoc explanation methods (SHAP, feature ablation, integrated gradients)**: Techniques to attribute model decisions to input features. *Why needed*: Identifies which tokens cause refusals. *Quick check*: Does the method correctly highlight the refusal trigger?
- **Model-agnostic mitigation**: Strategies that work across different LLM architectures without retraining. *Why needed*: Broad applicability. *Quick check*: Does the mitigation work on models not seen during development?
- **Multi-turn scenario benchmarks**: Evaluations that simulate realistic, context-rich dialog settings. *Why needed*: Captures complex refusal dynamics. *Quick check*: Does the benchmark include context shifts or extended interactions?
- **Attention steering**: Adjusting model attention to reduce the impact of refusal triggers. *Why needed*: Allows the model to focus on benign content. *Quick check*: Does attention shift away from the trigger after intervention?

## Architecture Onboarding

**Component map**: XSB/MS-XSB benchmarks -> Post-hoc explanation (SHAP, ablation, integrated gradients) -> Trigger identification -> Mitigation (ignore-word, rephrasing, attention steering) -> Compliance and safety evaluation

**Critical path**: The pipeline flows from benchmark evaluation to trigger identification, then to the application of mitigation strategies, and finally to assessment of both compliance and safety.

**Design tradeoffs**: Lightweight, inference-time mitigation avoids costly retraining but may not address deeply rooted refusal behaviors. Model-agnostic approaches are broadly applicable but may be less effective than model-specific fine-tuning.

**Failure signatures**: Mitigation strategies may fail if attribution fidelity is low (misidentifying triggers), if adversarial prompts embed harmful content in benign contexts, or if proprietary models use different alignment pipelines not captured by the benchmarks.

**First experiments**:
1. Evaluate compliance and refusal rates on XSB and MS-XSB for a target model before any mitigation.
2. Apply each mitigation strategy individually and measure changes in compliance and safety.
3. Test the robustness of mitigation by introducing adversarial prompts that attempt to bypass safety while appearing benign.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can multimodal refusal benchmarks effectively capture exaggerated safety behaviors in vision-language models?
- **Basis in paper**: "XSB and MS-XSB... do not capture multimodal or adversarial scenarios. For instance, models such as Qwen2-VL that integrate vision and language may exhibit refusal dynamics influenced by non-textual cues, which our current benchmarks cannot measure."
- **Why unresolved**: Current benchmarks are text-only; Qwen2-VL's poor text-only performance may not reflect its actual capabilities when visual context is available.
- **What evidence would resolve it**: Construction of image-text benchmark pairs and comparative evaluation showing whether visual context reduces or amplifies exaggerated refusals.

### Open Question 2
- **Question**: How can token attribution fidelity be improved to reliably guide refusal mitigation?
- **Basis in paper**: "Improving attribution fidelity—potentially by combining token-level and representation-level methods—remains an open problem for reliably guiding refusal mitigation."
- **Why unresolved**: SHAP achieves only 0.82 accuracy; misidentifications lead to ineffective or counterproductive mitigations for attention steering and ignore-word instructions.
- **What evidence would resolve it**: Hybrid attribution methods combining SHAP with representation-level analysis achieving >0.90 accuracy on XSB trigger identification.

### Open Question 3
- **Question**: Do findings on exaggerated refusals generalize to closed-source proprietary LLMs?
- **Basis in paper**: "Our experiments include a diverse set of open-weight LLMs... but exclude closed-source proprietary systems such as GPT-4 or Claude... refusal mechanisms in closed systems may differ substantially."
- **Why unresolved**: Proprietary models use distinct alignment pipelines; external validity of XSB, MS-XSB, and mitigation methods remains unverified.
- **What evidence would resolve it**: API-based evaluation of GPT-4, Claude, and Gemini on XSB/MS-XSB with comparable compliance and refusal metrics.

## Limitations
- Benchmarks may not fully capture the diversity of real-world scenarios where exaggerated refusals occur, particularly edge cases where benign and harmful content are closely intertwined.
- Mitigation strategies are untested in adversarial environments or with proprietary models like GPT-4 or Claude, limiting generalizability.
- Reliance on post-hoc explanation methods (SHAP, feature ablation, integrated gradients) introduces uncertainty due to known limitations in attribution reliability and computational cost.

## Confidence
- **High confidence**: The existence of exaggerated refusals in LLMs is well-documented and reproducible across multiple models. The benchmarks (XSB and MS-XSB) are methodically constructed and the results showing improved compliance after mitigation are robust within the experimental setup.
- **Medium confidence**: The effectiveness of the three mitigation strategies (ignore-word, rephrasing, attention steering) is demonstrated, but their performance in more complex or adversarial scenarios is uncertain. The generalizability of these methods to non-Llama models or proprietary systems is also unclear.
- **Medium confidence**: The post-hoc explanation methods (SHAP, feature ablation, integrated gradients) are standard tools, but their reliability for trigger identification in safety-critical contexts may vary depending on the prompt and model architecture.

## Next Checks
1. **Generalizability Test**: Evaluate the mitigation strategies on a broader set of models, including proprietary models (e.g., GPT-4, Claude) and models from different training pipelines, to assess whether the methods are truly model-agnostic and effective beyond the Llama family.
2. **Adversarial Robustness**: Design and test the benchmarks and mitigation strategies against adversarial prompts that attempt to exploit the mitigation techniques (e.g., by embedding harmful content in otherwise benign contexts or using obfuscation tactics).
3. **Long-term Stability**: Conduct longitudinal studies to assess whether the mitigation effects persist over time as models are updated or as new types of exaggerated refusals emerge due to evolving user behaviors or model fine-tuning.