---
ver: rpa2
title: Demographic User Modeling for Social Robotics with Multimodal Pre-trained Models
arxiv_id: '2502.10642'
source_url: https://arxiv.org/abs/2502.10642
tags:
- user
- demographic
- modeling
- clip
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines the performance of multimodal pre-trained\
  \ models, specifically CLIP and FaRL, in user profiling tasks based on visual-linguistic\
  \ demographic data. We introduce two datasets\u2014GenUser (synthetic) and FairUser\
  \ (real-world)\u2014containing 10K and 100K image-text pairs respectively, designed\
  \ to capture diverse demographic attributes including age, gender, and ethnicity."
---

# Demographic User Modeling for Social Robotics with Multimodal Pre-trained Models

## Quick Facts
- arXiv ID: 2502.10642
- Source URL: https://arxiv.org/abs/2502.10642
- Reference count: 24
- Performance: CLIP baseline achieves 16% and 34% accuracy on GenUser and FairUser datasets respectively; fine-tuning improves to 66% and 49% F1 scores

## Executive Summary
This study examines the performance of multimodal pre-trained models, specifically CLIP and FaRL, in user profiling tasks based on visual-linguistic demographic data. We introduce two datasets—GenUser (synthetic) and FairUser (real-world)—containing 10K and 100K image-text pairs respectively, designed to capture diverse demographic attributes including age, gender, and ethnicity. Our experiments show that out-of-the-box performance of these models is suboptimal, with CLIP achieving only 16% and 34% accuracy on GenUser and FairUser datasets respectively. Fine-tuning significantly improves performance, with CLIP reaching 66% and 49% F1 scores after training. Notably, FaRL demonstrates better cross-dataset generalization compared to CLIP, which exhibits catastrophic forgetting due to distribution shifts. We propose integrating masked image modeling with contrastive learning to improve generalization and capture subtle demographic nuances, offering a pathway for enhancing demographic sensitivity in multimodal user modeling tasks.

## Method Summary
The study employs two multimodal pre-trained models: CLIP-ViT-Base-Patch16 and FaRL-Base-Patch16-ep64. The core method involves fine-tuning these models using the FLYP strategy, which frames demographic user profiling as a contrastive retrieval task. FaRL incorporates an additional masked image modeling (MIM) loss alongside the contrastive loss to improve fine-grained feature learning. Training uses AdamW optimizer with learning rate 10⁻⁵ for up to 10 epochs with early stopping after 3 epochs without validation improvement. Five random seeds are used for each experiment. The models are evaluated on two datasets: GenUser (10K synthetic image-text pairs) and FairUser (100K real-world pairs derived from FairFace), with performance measured via accuracy, recall, precision, and F1 scores.

## Key Results
- Out-of-the-box CLIP achieves only 16% and 34% accuracy on GenUser and FairUser datasets respectively
- Fine-tuning improves CLIP performance to 66% and 49% F1 scores on the respective datasets
- FaRL demonstrates better cross-dataset generalization compared to CLIP, which suffers from catastrophic forgetting
- CLIP fine-tuned on FairUser drops from 34.2% to 28.9% F1 when tested on GenUser, while FaRL improves from 13.3% to 20.6%

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Image-Text Alignment
- Claim: Contrastive loss creates shared embedding space where matched demographic descriptions and facial images become proximal.
- Mechanism: Bidirectional contrastive loss (L_c) optimizes similarity scores between image embeddings (e^I = P(f^I_cls)) and text embeddings (e^T = P(f^T_eos)), pulling matched pairs together while pushing unmatched pairs apart via softmax-normalized similarity with temperature τ.
- Core assumption: Demographic attributes in text (age, gender, ethnicity) have consistent visual correlates that can be mapped to a shared semantic space.
- Evidence anchors:
  - [abstract] "prominent contrastive multimodal pre-trained model, CLIP... performs suboptimal in matching images to demographic descriptions without fine-tuning"
  - [section II] "through minimization of this contrastive loss, the framework learns to embed both visual and textual content in a shared semantic space"
  - [corpus] Related work (USER-VLM 360) confirms VLMs lack user-specific adaptability without targeted tuning
- Break condition: Distribution shifts between pre-training and deployment data cause catastrophic forgetting, as CLIP showed performance drops when fine-tuned on one dataset and tested on another (GenUser→FairUser: 0.342→0.289 F1).

### Mechanism 2: Masked Image Modeling for Fine-Grained Demographic Features
- Claim: MIM auxiliary task improves model's ability to capture subtle facial characteristics that contrastive learning alone may miss.
- Mechanism: Randomly mask image patches, encode with image encoder E_I, then predict discrete visual tokens for masked regions via cross-entropy loss (L_MIM). Uses a discrete VAE to encode patches to vocabulary indices, avoiding memory-intensive pixel prediction.
- Core assumption: Demographic nuances (e.g., freckles, subtle expressions) require local feature understanding that global contrastive alignment doesn't explicitly optimize.
- Evidence anchors:
  - [section III.B] "by incorporating this loss term, the model enhances its capacity to extract fine-grained facial characteristics while maintaining the broader semantic understanding"
  - [section V] "FaRL performs better in retrieving images that match the detailed demographic query, despite these exact attribute combinations not being present in its training data"
  - [corpus] Weak/no direct corpus evidence on MIM specifically for demographics; assumption extrapolated from paper's claims
- Break condition: If masking ratio is too aggressive, global demographic signals may be lost; paper doesn't specify optimal masking parameters.

### Mechanism 3: Cross-Dataset Generalization via Combined Objective
- Claim: Joint training with L_total = L_c + L_MIM reduces catastrophic forgetting compared to pure contrastive fine-tuning.
- Mechanism: MIM provides a reconstruction-based regularization that maintains visual feature quality across distribution shifts, while contrastive loss adapts to task-specific alignment. This dual objective is proposed (not fully validated) as the mechanism for FaRL's better transfer.
- Core assumption: The MIM component acts as an implicit regularizer that preserves useful visual representations when adapting to new demographic distributions.
- Evidence anchors:
  - [section V] "fine-tuning FaRL on one dataset results in a performance improvement on the other dataset... In contrast, CLIP exhibits the opposite behavior"
  - [table I] FaRL fine-tuned on FairUser achieves 20.6% F1 on GenUser (improved from 13.3%), while CLIP drops from 16.1% to 14.5%
  - [corpus] USER-VLM 360 addresses similar user-specific adaptation challenges but uses different approach (user-aware tuning)
- Break condition: Paper shows results but doesn't ablate the MIM component directly; causal attribution to MIM specifically is inferred, not proven.

## Foundational Learning

- Concept: **Contrastive Learning Fundamentals**
  - Why needed here: The entire architecture builds on understanding how contrastive loss creates aligned multimodal representations; without this, the loss function equations and FLYP fine-tuning strategy won't make sense.
  - Quick check question: Can you explain why temperature parameter τ matters in the softmax similarity computation, and what happens when τ is too large vs. too small?

- Concept: **Vision Transformer (ViT) Patch Embeddings**
  - Why needed here: Both CLIP and FaRL use ViT-Base-Patch16; understanding patch tokenization is essential for grasping how MIM masks and reconstructs image regions.
  - Quick check question: Given a 224×224 input image with patch size 16×16, how many patch tokens does the image encoder produce (excluding CLS token)?

- Concept: **Catastrophic Forgetting in Transfer Learning**
  - Why needed here: The key finding that CLIP suffers from distribution shift while FaRL generalizes better requires understanding why neural networks lose previously learned capabilities during fine-tuning.
  - Quick check question: Why does fine-tuning on a smaller dataset (10K-100K samples) risk degrading pre-trained knowledge from a 400M sample dataset (LAION)?

## Architecture Onboarding

- Component map:
  - Image Encoder (E_I): ViT-Base-Patch16, processes images into patch tokens + CLS token
  - Text Encoder (E_T): Transformer encoder, processes demographic descriptions into text tokens + EOS token
  - Projection Head (P): MLP mapping encoder outputs to shared embedding space (dimension d_e)
  - MIM Decoder (E_MIM): Small transformer predicting masked patch tokens (FaRL only)
  - Discrete VAE: Tokenizes image patches into vocabulary indices for MIM targets (|V| possible values)

- Critical path:
  1. Load pre-trained weights (CLIP or FaRL checkpoint)
  2. For each batch: encode images → optionally mask patches (FaRL) → encode text → compute L_c
  3. If FaRL: additionally compute L_MIM on masked regions → sum losses
  4. FLYP fine-tuning: cast retrieval task as contrastive matching with text prompts

- Design tradeoffs:
  - CLIP vs. FaRL: CLIP is simpler but catastrophically forgets; FaRL adds MIM complexity but generalizes better
  - Synthetic (GenUser) vs. Real (FairUser): Synthetic offers privacy and balanced demographics but may not reflect real distribution
  - Fine-tuning epochs: Paper shows overfitting starts at 1-5 epochs; early stopping threshold of 3 epochs is critical

- Failure signatures:
  - **Overfitting**: Validation loss diverges from training loss after ~1-5 epochs (see Figure 4)
  - **Catastrophic forgetting**: Performance drops on original dataset after fine-tuning (CLIP: FairUser F1 drops from 0.342 to 0.289)
  - **Retrieval failures on multi-attribute queries**: Model retrieves images matching some but not all demographic attributes (Table II example)

- First 3 experiments:
  1. **Baseline reproduction**: Run out-of-the-box CLIP on GenUser test set to verify ~16% accuracy; this confirms your data pipeline matches the paper's.
  2. **Ablation on masking ratio**: If implementing FaRL-style MIM, test different masking ratios (e.g., 30%, 50%, 75%) to find optimal balance between local feature learning and global context preservation—paper doesn't specify this hyperparameter.
  3. **Cross-dataset transfer test**: Fine-tune on GenUser, evaluate on FairUser (and vice versa) for both CLIP and FaRL; this directly tests the claimed generalization advantage and should show FaRL improving or maintaining while CLIP degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal pre-trained models effectively integrate the sparse and sensitive nature of visual-linguistic demographic data for user modeling?
- Basis in paper: [explicit] The introduction identifies the integration of this specific type of multimodal data as an "open research challenge."
- Why unresolved: The authors note that current models rely on contrastive training which struggles with the sparse and sensitive nature of demographic attributes.
- What evidence would resolve it: A model architecture or training paradigm specifically designed to handle data sparsity without compromising sensitive attributes.

### Open Question 2
- Question: Does integrating masked image modeling (MIM) with contrastive learning fully resolve the issue of catastrophic forgetting in demographic user profiling?
- Basis in paper: [inferred] While results show FaRL (with MIM) generalizes better than CLIP, the conclusion calls for "refining these models," implying the current solution is not final.
- Why unresolved: The paper demonstrates that CLIP suffers from catastrophic forgetting, and while FaRL performs better, the authors frame their proposed MIM integration as a "pathway" rather than a final solution.
- What evidence would resolve it: Ablation studies comparing pure contrastive models against MIM-enhanced models on cross-dataset retention tasks to quantify the reduction in forgetting.

### Open Question 3
- Question: What specific strategies are required to adapt these multimodal user models for sensitive, high-stakes applications like healthcare?
- Basis in paper: [explicit] The conclusion explicitly states, "Future work will focus on refining these models and exploring additional strategies to enhance their performance in diverse and sensitive applications, such as healthcare."
- Why unresolved: The current study focuses on general dataset performance (GenUser/FairUser) and does not test the models in the specific "delicate" context of healthcare robotics mentioned in the introduction.
- What evidence would resolve it: Benchmarks or real-world trials demonstrating the model's efficacy and safety when adapting to user needs in healthcare environments.

## Limitations
- The paper's claims about FaRL's superior cross-dataset generalization lack mechanistic clarity due to missing ablation studies on the MIM component
- The synthetic GenUser dataset may not capture the full complexity of real-world demographic variations, limiting ecological validity
- Claims about FaRL's ability to capture "subtle demographic nuances" lack quantitative validation on fine-grained feature detection

## Confidence
- **High Confidence**: The observation that out-of-the-box CLIP performance is suboptimal for demographic matching tasks (16-34% accuracy)
- **Medium Confidence**: The improvement from fine-tuning is robust across both datasets, but specific attribution to MIM for FaRL's generalization advantage remains inferential
- **Low Confidence**: Claims about FaRL's ability to capture "subtle demographic nuances" through MIM are supported by qualitative examples but lack quantitative validation

## Next Checks
1. **Ablation Study**: Implement CLIP with MIM (CLIP+MIM) to isolate whether the MIM component alone accounts for FaRL's better cross-dataset performance, controlling for other architectural differences
2. **Distribution Shift Analysis**: Systematically vary the demographic distribution shift between training and test sets (e.g., age range, ethnic diversity) to quantify at what point catastrophic forgetting occurs and whether FaRL maintains advantage across different shift magnitudes
3. **Fine-Grained Feature Validation**: Design test queries targeting specific facial features (e.g., "freckles," "wrinkles," "glasses") and evaluate whether MIM truly improves retrieval of these subtle attributes compared to contrastive learning alone, measuring both accuracy and precision-recall trade-offs