---
ver: rpa2
title: 'MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration'
arxiv_id: '2510.19423'
source_url: https://arxiv.org/abs/2510.19423
tags:
- tool
- query
- tools
- server
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MSC-Bench introduces a systematic methodology for evaluating multi-hop,
  end-to-end tool orchestration in a hierarchical Model-Context Protocol (MCP) ecosystem.
  The benchmark addresses the limitations of existing evaluations by constructing
  ground truth through "equal function sets" to handle functional overlap, enabling
  objective metrics like F1 score without relying on LLM-as-a-judge evaluation.
---

# MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration

## Quick Facts
- **arXiv ID:** 2510.19423
- **Source URL:** https://arxiv.org/abs/2510.19423
- **Authors:** Jia-Kai Dong; I-Wei Huang; Chun-Tin Wu; Yi-Tien Tsai
- **Reference count:** 40
- **Primary result:** Current agents struggle with cross-server orchestration (F1 ~43%) and robustness to out-of-scope requests, with precision often below 40% in complex scenarios.

## Executive Summary
MSC-Bench introduces a systematic methodology for evaluating multi-hop, end-to-end tool orchestration in a hierarchical Model-Context Protocol (MCP) ecosystem. The benchmark addresses the limitations of existing evaluations by constructing ground truth through "equal function sets" to handle functional overlap, enabling objective metrics like F1 score without relying on LLM-as-a-judge evaluation. Organized into a five-level curriculum, it tests capabilities from single-tool orchestration to complex cross-server planning and robustness to out-of-scope requests. Experiments show that current agents struggle significantly with cross-server orchestration and robustness, with precision often falling below 40% in complex scenarios, revealing that rigid hierarchies can hinder performance without co-designed strategies.

## Method Summary
MSC-Bench evaluates end-to-end, multi-hop tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem across five complexity levels. The benchmark uses 491 servers and 2,375 tools sourced from glama.ai MCP registry, with 2,075 tasks across levels L1-L5. Ground truth is constructed using "equal function sets" methodology that identifies functionally equivalent tools through semantic similarity and pairwise LLM verification. Four orchestrator architectures are evaluated: ReAct (generative baseline), ToolShed (flat retrieval with RAG), MCP-Zero (hierarchical retrieval), and Hybrid (MCP-Zero filtering + ToolShed search). Foundation models include Qwen3-4B/8B, Llama-3-8B, Ministral-8B, Gemma-3-12B-IT, Phi-4, and GPT-4.1. Evaluation uses Exact Match (EM) for L1/L2/L5 and Node Set EM/F1 for L3/L4, with normalized latency measurements.

## Key Results
- Cross-server orchestration (L4) shows substantially lower performance than single-server chaining (L3), with F1 scores dropping from ~70% to ~43%.
- Hierarchical retrieval (MCP-Zero) achieves up to 5.76x speedup but at significant accuracy cost, particularly for complex orchestration tasks.
- Context propagation failure ("contextual drift") is the most prevalent failure mode in L4, where agents lose critical context when transitioning across server boundaries.
- Current rejection mechanisms for out-of-scope detection rely entirely on backbone LLM reasoning rather than dedicated architectural modules, leading to inconsistent behavior.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Equal Function Sets enable objective, reproducible evaluation despite functional overlap across tools.
- **Mechanism:** A two-phase round-trip consistency approach identifies functionally equivalent tools: (1) bottom-up candidate generation via semantic similarity + pairwise LLM verification grouped via Union-Find; (2) top-down query-guided RAG retrieval validated against candidate sets, with human verification of borderline cases. This creates ground-truth sets where any tool in the set satisfies the query, allowing F1 scoring without LLM-as-judge.
- **Core assumption:** LLMs can reliably verify pairwise functional equivalence when given structured tool descriptions; human validators catch platform-specific edge cases LLMs miss.
- **Evidence anchors:**
  - [abstract] "MSC-Bench addresses these gaps by constructing ground truth through 'equal function sets', allowing objective metrics such as F1 score and reducing the dependency on LLM-as-a-judge evaluation."
  - [Section 3.2] "This rigorous two-stage process prunes the candidates into 95 high-confidence equal function sets, ensuring logical and platform-level consistency for the evaluation ground truth."
  - [corpus] Related MCP-SafetyBench and LiveMCP-101 confirm functional overlap and multi-server complexity are recognized challenges; no comparable equal-function-set methodology found.

### Mechanism 2
- **Claim:** Hierarchical retrieval architectures (MCP-Zero) achieve efficiency gains at the cost of accuracy in complex orchestration, while flat retrieval (ToolShed) provides higher performance with higher latency—a trade-off that interacts strongly with foundation model choice.
- **Mechanism:** MCP-Zero first routes to servers via embeddings, then ranks tools within top servers (hierarchical filtering). ToolShed performs dense retrieval across all tools with query expansion and reranking (flat search). Hierarchical filtering reduces search space early, saving computation but potentially pruning relevant cross-server candidates.
- **Core assumption:** The hierarchical server organization reflects true task decomposition structure; models have consistent reasoning styles that align with either hierarchical or flat retrieval patterns.
- **Evidence anchors:**
  - [Section 5.3] "MCP-Zero architecture (marked with 'x') consistently occupy the high-efficiency, low-latency region (1.0-3.0x baseline). In contrast, ToolShed configurations cluster in the high-performance, high-latency region (5.0-15.0x)."
  - [Section 6.1] "While MCP-Zero's architecture delivers impressive efficiency gains (up to 5.76x), this benefit comes at a significant and non-uniform accuracy cost, particularly in complex orchestration tasks (L3/L4)."
  - [Section 6.2] "Qwen models demonstrate superior precision in direct, single-step retrieval tasks... Llama's advanced reasoning capabilities for complex, multi-step tasks (L3/L4) only fully manifest when paired with ToolShed's broad, high-recall retrieval mechanism."
  - [corpus] Semantic Context for Tool Orchestration (arXiv:2507.10820) similarly identifies context-aware retrieval as foundational but doesn't address hierarchical vs flat trade-offs.

### Mechanism 3
- **Claim:** Cross-server orchestration suffers from "contextual drift"—agents lose critical context (intermediate outputs, user constraints) when transitioning across server boundaries, causing cascading errors.
- **Mechanism:** Multi-hop planning requires maintaining global state across tool invocations. When agents decompose tasks and invoke tools from different servers, the server boundary acts as an implicit context boundary. Intermediate results from Server A may not be properly propagated when reasoning about Server B's tools, leading to selections decoupled from original intent.
- **Core assumption:** Current LLM reasoning traces are stateful but server transitions don't have explicit context-passing mechanisms; agents rely on implicit context maintenance through prompt history.
- **Evidence anchors:**
  - [Section 5.5] "The most prevalent failure in Level 4 arises when agents are required to orchestrate tools across multiple servers. We observe that agents frequently lose critical context... when transitioning across server boundaries. This phenomenon, which we term contextual drift, triggers a cascade of errors."
  - [Section 5.5] "Performance drops from 70.2% F1 on 2-step tasks to 43.1% F1 on 7-step tasks."
  - [Section 6.3] "Maintaining context across multi-step plans remains a critical weakness. The 'premature decomposition' we observed leads to cascading errors."
  - [corpus] No direct corroboration of "contextual drift" terminology; related work on multi-agent orchestration (Tool-to-Agent Retrieval) addresses routing but not cross-server context propagation.

## Foundational Learning

- **Concept: Functional overlap in tool ecosystems**
  - **Why needed here:** The benchmark's core innovation (equal function sets) exists specifically because multiple tools can achieve identical outcomes. Without understanding this, the evaluation methodology appears unnecessarily complex.
  - **Quick check question:** Given two tools—"convert_to_jpeg" and "image_format_converter" that both handle JPEG conversion—should a benchmark penalize an agent for selecting either one?

- **Concept: Hierarchical vs flat retrieval architectures**
  - **Why needed here:** The paper's central architectural comparison (MCP-Zero vs ToolShed) hinges on this distinction. Understanding when each excels is prerequisite for interpreting the efficiency-accuracy trade-offs.
  - **Quick check question:** If you have 2,375 tools across 491 servers, what is the computational advantage of first narrowing to 5 servers before searching tools, versus searching all 2,375 tools directly?

- **Concept: Multi-hop task decomposition**
  - **Why needed here:** Levels 3-4 evaluate sequential and cross-server chaining. Understanding how agents decompose high-level goals into tool sequences is essential for diagnosing the "premature decomposition" and "contextual drift" failure modes.
  - **Quick check question:** A user asks "Scan my code for vulnerabilities and email the report to my team." How many tool invocations are minimally required, and what state must persist between them?

## Architecture Onboarding

- **Component map:** ReAct: Query -> Router LLM -> Action/Observation loop; ToolShed: Query -> Classification -> Query Rewrite -> Query Expansion -> Multi-query Retrieval -> Reranking -> Tool Selection; MCP-Zero: Query -> Classification -> Tool Transformation -> Hierarchical Retrieval -> Tool Selection; Hybrid: Query -> Tool Transformation -> Query Expansion + Retrieval + Reranking.

- **Critical path:**
  1. Query classification (tool vs conversational)—all architectures share this entry point
  2. Retrieval phase—where architectures diverge most significantly (hierarchical vs flat)
  3. Reranking/selection—ToolShed and Hybrid use LLM reranker; MCP-Zero uses direct selection
  4. Orchestration (for L3/L4)—decomposition into sub-queries, sequential execution
  5. Rejection (L5)—self-reflection to detect out-of-scope requests

- **Design tradeoffs:**
  - **Retrieval breadth (tool_top_k):** Broader helps L3/L4 (complex planning) but degrades L2 (noise from excessive candidates). Optimal: k=5 for L2, k=20 for L4.
  - **Query expansion:** Benefits L4 cross-domain tasks (bridging different tool vocabularies), neutral/negative for simpler tasks.
  - **Reranking depth:** Critical across all levels; reducing rerank_top_k degrades performance substantially.
  - **Model-architecture pairing:** Qwen + MCP-Zero for efficiency on simple tasks; Llama + ToolShed for complex orchestration. No universal best.

- **Failure signatures:**
  - **Contextual drift (L4):** Agent selects tools increasingly unrelated to original user intent as plan length increases. Signature: plan steps logically follow each other but diverge from initial query.
  - **Premature decomposition:** Single-step queries decomposed into unnecessary sub-tasks with hallucinated dependencies. Signature: plan has more steps than minimally required.
  - **False positive rejection (L5):** Agent rejects solvable queries as out-of-scope. Current systems lack explicit capability-boundary detection; rejection depends on backbone's intrinsic reasoning.
  - **Impedance mismatch (Hybrid + Llama):** Performance collapse (57.45 → 34.93) when model reasoning style conflicts with architectural constraints.

- **First 3 experiments:**
  1. **Baseline retrieval comparison:** Run MCP-Zero and ToolShed with Qwen3-4B on full L1-L5 curriculum. Measure: (a) F1/EM per level, (b) normalized latency. Hypothesis: MCP-Zero dominates L1/L2 on efficiency; ToolShed dominates L3/L4 on accuracy.
  2. **Retrieval breadth ablation:** Fix architecture (ToolShed), vary tool_top_k ∈ {1, 5, 10, 15, 20, 25} on Meta-Llama-3-8B. Plot performance vs k for each level. Hypothesis: L1 stable, L2 peaks early then degrades, L4 peaks at k≈20.
  3. **Context propagation intervention:** Add explicit "global intent re-injection" prompt after each server transition in cross-server L4 tasks. Compare F1 before/after. Hypothesis: Reduces contextual drift, improves L4 performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can novel task decomposition methods be explicitly engineered to maintain and propagate global context between orchestration steps to prevent the "contextual drift" observed in complex cross-server planning?
- **Basis in paper:** [explicit] The authors identify "Context-Propagating Decomposition" as a future research direction (Section 6.4) to address the "Catastrophic Context Loss" failure mode (Section 5.5).
- **Why unresolved:** Current agents lose critical context when transitioning across server boundaries, leading to cascading errors where tool selections decouple from the original intent.
- **What evidence would resolve it:** A modified orchestration architecture that demonstrates significantly higher F1 scores on the ETOM Level 4 tasks by explicitly passing state information between steps.

### Open Question 2
- **Question:** How can hybrid architectures be designed to dynamically switch between flat retrieval and hierarchical filtering based on real-time query complexity analysis?
- **Basis in paper:** [explicit] Section 6.4 proposes exploring "Adaptive and Hybrid Architectures" that can "dynamically choose between flat and hierarchical retrieval."
- **Why unresolved:** The paper's experimental Hybrid model showed a dramatic performance drop with specific models (Llama), suggesting a deep incompatibility or "impedance mismatch" between reasoning styles and rigid architectural constraints.
- **What evidence would resolve it:** An adaptive system that outperforms both static ToolShed and MCP-Zero baselines on the ETOM benchmark across different complexity levels without manual reconfiguration per model.

### Open Question 3
- **Question:** Can dedicated architectural modules for out-of-scope detection be developed to provide reliable safety guarantees independent of the backbone LLM's intrinsic reasoning capabilities?
- **Basis in paper:** [explicit] Section 6.4 calls for "Developing Robust Rejection Mechanisms," and Section 6.3 notes that correct rejection currently "largely emerges from the backbone model's intrinsic reasoning."
- **Why unresolved:** Current Level 5 evaluation reveals inconsistent rejection behavior across different backbones, creating a safety gap where agents may attempt impossible tasks rather than rejecting them.
- **What evidence would resolve it:** An architecture integrating a dedicated verification module that achieves near-100% Exact Rejection Match on Level 5 tasks, even when paired with lower-capability foundation models.

## Limitations

- **Ground truth methodology uncertainty:** The equal function sets construction relies on human validation for borderline cases, but the paper provides limited detail on human judgment criteria and inter-annotator agreement.
- **Context propagation isolation:** While contextual drift is convincingly demonstrated through performance degradation, the paper doesn't directly isolate this as the primary failure mode versus alternative explanations like model reasoning style shifts.
- **Latency measurement ambiguity:** Normalized latency measurements don't report absolute timings or infrastructure details, limiting reproducibility claims about efficiency gains.

## Confidence

- **High confidence:** The core finding that cross-server orchestration (L4) shows substantially lower performance than single-server chaining (L3), with F1 scores dropping from ~70% to ~43%. This is directly measured and consistently observed across architectures.
- **Medium confidence:** The claim that hierarchical retrieval (MCP-Zero) trades accuracy for efficiency, with up to 5.76x speedup but non-uniform accuracy costs. While the efficiency-accuracy trade-off is clear, the specific performance numbers depend on exact hyperparameter configurations that aren't fully specified.
- **Medium confidence:** The architectural pairing recommendations (Qwen + MCP-Zero for simple tasks, Llama + ToolShed for complex orchestration) are based on observed performance patterns but may be dataset-specific rather than universally generalizable.

## Next Checks

1. **Ground truth validation:** Re-run the equal function set construction process on a subset of tools (e.g., 50 servers) and compare human validator agreements to assess reliability of the equivalence grouping methodology.

2. **Context propagation intervention:** Implement explicit global intent re-injection in cross-server L4 tasks and measure F1 improvement to isolate whether contextual drift is the primary failure mode versus alternative explanations.

3. **Architecture ablation study:** Systematically vary retrieval breadth (tool_top_k) across all levels and architectures to map the precise efficiency-accuracy frontier, verifying whether the claimed 5.76x speedup for MCP-Zero holds across different complexity regimes.