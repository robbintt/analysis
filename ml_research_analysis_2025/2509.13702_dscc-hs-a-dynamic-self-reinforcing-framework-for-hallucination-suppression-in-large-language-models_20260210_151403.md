---
ver: rpa2
title: 'DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression
  in Large Language Models'
arxiv_id: '2509.13702'
source_url: https://arxiv.org/abs/2509.13702
tags:
- proxy
- factual
- arxiv
- hallucination
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DSCC-HS introduces a novel framework to suppress hallucinations\
  \ in large language models by proactively steering generation using a dual-proxy\
  \ system. During training, a compact model is iteratively aligned into two adversarial\
  \ proxies\u2014a Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy\
  \ (HDP)\u2014via contrastive logit-space optimization with augmented data and LoRA-based\
  \ parameter efficiency."
---

# DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models

## Quick Facts
- **arXiv ID:** 2509.13702
- **Source URL:** https://arxiv.org/abs/2509.13702
- **Reference count:** 40
- **Primary result:** Achieves 99.2% Factual Consistency Rate and FActScore of 46.50 on TruthfulQA and BioGEN benchmarks respectively

## Executive Summary
DSCC-HS introduces a novel framework that suppresses hallucinations in large language models by steering generation toward factual outputs without modifying the target model's parameters. The approach trains two adversarial proxy models—a Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP)—using contrastive logit-space optimization. During inference, the difference between these proxies' logits is injected into the target model's decoding process, guiding it toward more factual responses. Experiments show state-of-the-art performance on TruthfulQA and BioGEN benchmarks with minimal computational overhead.

## Method Summary
The framework operates in two phases: (1) iterative training of FAP and HDP using contrastive logit-space optimization with augmented data and LoRA-based parameter efficiency, and (2) inference-time steering where the proxy logits difference is injected into the target model's decoding. FAP is refined through contrastive loss against both the base model and frozen HDP, while HDP is trained once on hallucinated data. The steering vector, computed as FAP minus HDP logits, is projected onto the target vocabulary and added to the target model's logits at each generation step, amplifying factual tokens while suppressing hallucinatory ones.

## Key Results
- Achieves 99.2% Factual Consistency Rate (FCR) and hallucination score of 0.8 on TruthfulQA
- Obtains FActScore of 46.50 with 11.49% incorrectness on BioGEN benchmark
- Outperforms existing methods through iterative contrastive training and dual-proxy steering mechanism
- Ablation studies confirm necessity of both iterative training and both proxy roles for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Iterative Contrastive Manifold Separation
The framework iteratively trains a compact model into adversarial roles that separate "factual" and "hallucinatory" representations in logit space, creating a directional signal for factuality. The FAP is refined using contrastive loss that forces divergence from the base model's distribution opposite to the HDP's direction, effectively pushing token probabilities toward a "factual manifold."

### Mechanism 2: Vocabulary-Aligned Logit Steering
During decoding, the framework computes a steering vector as the difference between FAP and HDP logits, projects it onto the target vocabulary, and adds it to the target model's logits. This amplifies tokens favored by the FAP and suppresses those favored by the HDP, steering generation toward factuality without retraining the target model.

### Mechanism 3: Adversarial Reference Anchoring
Freezing the HDP after initial training provides a stable "counter-weight" that prevents the steering mechanism from drifting or overfitting during inference. The explicit subtraction of the HDP's likelihood is more effective than simply nudging toward the FAP, creating a robust adversarial signal that defines what to avoid.

## Foundational Learning

**Concept: Logit-Space Manipulation**
- *Why needed here:* The framework operates directly on unnormalized output scores (logits). Understanding that adding a vector to logits shifts the probability distribution (via Softmax) is fundamental to grasping how "steering" works.
- *Quick check question:* If the steering vector adds +2.0 to the logit of "Paris" and -2.0 to "Lyon," how does the probability of "Paris" change relative to "Lyon"?

**Concept: LoRA (Low-Rank Adaptation)**
- *Why needed here:* The training phase relies on LoRA to update proxy models efficiently. You must understand that LoRA freezes main model weights and trains small adapter matrices, making iterative training computationally feasible.
- *Quick check question:* Why is LoRA critical for the "iterative" aspect of Phase 1? (Hint: Think about the cost of full fine-tuning).

**Concept: Contrastive Learning Objectives**
- *Why needed here:* The FAP is trained by contrasting its outputs against the base model and HDP. The goal is not just to predict the right answer, but to move away from the "wrong" direction in representation space.
- *Quick check question:* In the loss function L_k, what happens to the FAP if the distance to the HDP increases while the distance to the base model stays constant?

## Architecture Onboarding

**Component map:**
- Proxy Models (Llama-3.2-1B): FAP (Factual Alignment Proxy) and HDP (Hallucination Detection Proxy)
- Target Model (Qwen3-8B): Production model being steered with frozen parameters
- Vocabulary Projector: Maps 1B proxy logits to 8B target space, masking non-overlapping tokens

**Critical path:**
1. Data Augmentation: Generate (Question, Correct, Hallucinated) triplets
2. HDP Init: Train HDP on hallucinated answers and freeze
3. FAP Iteration: Run 3 rounds of contrastive training (FAP vs. HDP)
4. Inference: For each token step, compute FAP/HDP logits, get vector diff, add to target

**Design tradeoffs:**
- Proxy Size vs. Steering Strength: Using 1B to steer 8B is efficient but assumes 1B has sufficient reasoning capability
- Iterative Complexity: K=3 iterations chosen; fewer iterations result in lower accuracy, while more might overfit

**Failure signatures:**
- Vocabulary Mismatch: If target and proxy share few tokens, steering vector is mostly zeros
- HDP Overfitting: If HDP is too strong, it may penalize common words, causing nonsense generation

**First 3 experiments:**
1. Sanity Check (Ablation): Run system with "w/o Proxy Guidance" to verify target model's necessity
2. Vector Magnitude Test: Scale steering vector by factors (0.5, 1.0, 2.0) to find optimal strength
3. Cross-Model Transfer: Try using trained proxies to steer a completely different target model family

## Open Questions the Paper Calls Out

### Open Question 1
How does DSCC-HS performance degrade when vocabulary overlap between proxy and target models is minimal or structurally dissimilar? The framework projects steering vector onto target vocabulary using shared tokens, but experiments only use Llama-3 and Qwen-3 with similar architectures. Empirical evaluation with deliberately low tokenizer overlap would resolve this.

### Open Question 2
Is a compact 1B-parameter proxy sufficient to steer models significantly larger than 8B (e.g., 70B+), or does the knowledge capacity gap limit steering efficacy? The methodology specifies 1B proxy steering 8B target, leaving scalability to larger targets unverified. Experiments with 70B or 405B parameter targets would provide evidence.

### Open Question 3
Does direct addition of the steering vector in logit space introduce instability or "over-correction" in generation diversity compared to tunable approaches? Equation 5 adds vector directly without modulation, potentially overly suppressing valid but low-probability tokens. Ablation study analyzing generation diversity and semantic preservation with scaled vs. unscaled vectors would resolve this.

## Limitations

- **Limited evaluation scope:** Validated only on TruthfulQA and BioGEN benchmarks, unclear if performance generalizes to other domains or multi-turn conversations
- **Scalability concerns:** Effectiveness for much larger models (e.g., 70B+) or different model families remains untested
- **Computational overhead:** Framework requires running two proxy models alongside target during inference, but paper doesn't report resulting latency or inference cost

## Confidence

- **High confidence:** Core mechanism of contrastive logit-space training and inference-time steering is technically sound and well-supported by ablation studies
- **Medium confidence:** Specific hyperparameter choices (K=3 iterations, LoRA rank=8) are empirically validated but may not be optimal across different model sizes or datasets
- **Low confidence:** Claim that method works "without modifying target model parameters" is technically accurate but potentially misleading given substantial computational resources required to run proxy models during inference

## Next Checks

1. **Cross-domain generalization test:** Evaluate DSCC-HS on additional benchmarks like MMLU, HellaSwag, or domain-specific datasets to verify performance consistency beyond two tested datasets

2. **Inference efficiency measurement:** Benchmark end-to-end latency and memory overhead when running both proxy models alongside target model to quantify real-world computational cost

3. **Vocabulary mismatch stress test:** Systematically evaluate performance degradation when proxy and target vocabularies have decreasing overlap (100%, 75%, 50%, 25% shared tokens) to understand practical deployment limits