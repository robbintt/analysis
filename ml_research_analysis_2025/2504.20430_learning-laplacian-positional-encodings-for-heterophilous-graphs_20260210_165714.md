---
ver: rpa2
title: Learning Laplacian Positional Encodings for Heterophilous Graphs
arxiv_id: '2504.20430'
source_url: https://arxiv.org/abs/2504.20430
tags:
- graph
- llpe
- graphs
- eigenvectors
- laplacian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitations of Laplacian positional encodings
  (LPEs) in capturing graph structure for heterophilous graphs, where nodes that are
  close tend to have different labels. The authors propose Learnable Laplacian Positional
  Encodings (LLPE), a new PE that leverages the full spectrum of the graph Laplacian,
  enabling it to capture graph structure on both homophilous and heterophilous graphs.
---

# Learning Laplacian Positional Encodings for Heterophilous Graphs

## Quick Facts
- **arXiv ID:** 2504.20430
- **Source URL:** https://arxiv.org/abs/2504.20430
- **Reference count:** 40
- **Key outcome:** LLPE improves accuracy across GNNs by up to 35% on synthetic and 14% on real-world heterophilous graphs by leveraging the full Laplacian spectrum.

## Executive Summary
Standard Laplacian positional encodings (LPEs) fail on heterophilous graphs where nodes that are close tend to have different labels. This paper introduces Learnable Laplacian Positional Encodings (LLPE), which learns to leverage the full Laplacian spectrum rather than just the first k eigenvectors. LLPE uses Chebyshev polynomials to adaptively weight different parts of the spectrum, capturing both homophilous and heterophilous graph structures. Theoretical analysis proves LLPE can approximate general graph distances and provides generalization bounds, while empirical results show consistent improvements across 12 benchmarks.

## Method Summary
LLPE computes the eigendecomposition of the graph Laplacian and learns a spectral filter using Chebyshev polynomials to create positional encodings. Unlike standard LPEs that use only the first k eigenvectors, LLPE learns importance weights for the full spectrum, enabling it to capture structural information relevant to both homophilous and heterophilous graphs. For large graphs, an efficient approximation using Arnoldi iteration computes only the first and last k eigenvectors. The method is compatible with any GNN architecture and includes regularization to prevent overfitting in high-dimensional eigenspaces.

## Key Results
- Improves accuracy across GNNs by up to 35% on synthetic and 14% on real-world heterophilous graphs
- Theoretically proves ability to approximate general graph distances and provides generalization bounds
- Demonstrates consistent improvements across 12 benchmarks including Cora, Cornell, Texas, Amazon-ratings, Tolokers, and Penn94

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Spectral Selection
Standard LPEs use only the first k eigenvectors, which works for homophilous graphs but fails on heterophilous graphs where structural information lies in high eigenvalues. LLPE learns a function h(λ) via Chebyshev series that adaptively weights different parts of the spectrum. For homophilous graphs, h(λ) weights low λ; for heterophilous graphs, it weights high λ (Theorem 3.2). This allows LLPE to capture graph structure regardless of homophily level.

### Mechanism 2: Approximation of General Graph Distances
LLPE's formulation P_LLPE = U W_LLPE creates node representations where Euclidean distance approximates spectral distances defined by arbitrary functions r(λ). By learning Chebyshev coefficients, LLPE constructs a custom distance metric optimal for the specific graph structure. Theorem 4.3 proves LLPE can approximate any function f_r of the form in Definition 4.2, recovering distances like diffusion maps.

### Mechanism 3: Implicit Regularization for Generalization
The use of Chebyshev polynomials provides implicit regularization by restricting the hypothesis class complexity to depend on the norm of Chebyshev coefficients rather than the number of nodes n. This allows learning relevant eigenvectors even when n is large (thousands of nodes) without memorizing noise. Theorem 4.4 derives Rademacher complexity bounds independent of graph size, providing better statistical generalization than applying MLPs to full eigenvectors.

## Foundational Learning

- **Concept: Homophily vs. Heterophily**
  - Why needed here: The entire premise rests on the failure of standard LPEs when "nodes that are close tend to have different labels" (heterophily). You must understand that standard smoothing (low-pass filtering) destroys information in heterophilous graphs.
  - Quick check question: In a heterophilous bipartite graph, would the eigenvector with the smallest non-zero eigenvalue separate the two node sets?

- **Concept: Graph Laplacian Eigendecomposition**
  - Why needed here: The method relies on calculating U (eigenvectors) and Λ (eigenvalues). You need to associate eigenvalues with "frequencies"—small λ corresponds to smooth global structure, large λ corresponds to local variations.
  - Quick check question: Which part of the spectrum would likely contain the signal for a graph where edges mostly connect nodes of different classes?

- **Concept: Chebyshev Polynomials**
  - Why needed here: LLPE uses these as basis functions to learn h(λ). You need to know they allow stable approximation of functions on [-1, 1] and avoid numeric instability of high-degree standard polynomials.
  - Quick check question: Why use Chebyshev polynomials instead of a raw MLP over the eigenvalue index?

## Architecture Onboarding

- **Component map:** Graph G=(V,A) -> Compute eigendecomposition L = U^⊤ Λ U -> Construct Chebyshev basis T -> Learn coefficients θ -> Generate PE P = U(T·θ) -> Concatenate to node features -> Feed into GNN

- **Critical path:** Eigendecomposition is O(n³) and the main bottleneck. For large graphs, use the approximate version utilizing Arnoldi iteration to get only the first and last k eigenvectors.

- **Design tradeoffs:**
  - Accuracy vs. Speed: Full spectrum (O(n³)) vs. First/Last k (O(k²n))
  - Expressivity vs. Overfitting: Increasing order M increases approximation power but requires more regularization to prevent overfitting

- **Failure signatures:**
  - Performance drop on real-world graphs: If M is too small (e.g., <50), Chebyshev series cannot approximate spectral filters accurately
  - No improvement over LPE-FK: If graph is extremely large and k is too small (e.g., <32), LLPE misses relevant eigenvectors
  - Overfitting: If regularization is removed, LLPE may fit noise in eigenspace of large graphs

- **First 3 experiments:**
  1. **Sanity Check (SBM):** Generate binary heterophilous SBM (p ≪ q). Verify LLPE performance remains high while LPE-FK collapses (Figure 3a)
  2. **Hyperparameter Sensitivity (Real Data):** On medium dataset (e.g., Tolokers), tune M ∈ [64, 128, 256] and observe impact of l1 regularization. Confirm performance peaks and stabilizes with adequate M
  3. **Scaling Test (Large Graph):** Run approximate LLPE (Arnoldi version) on large dataset (e.g., Penn94). Compare memory usage and throughput against baseline LPE. Verify accuracy holds (Table 3)

## Open Questions the Paper Calls Out

- **Question:** Can an efficient extension of LLPE for large graphs be developed that incorporates relevant eigenvectors from the middle of the spectrum?
  - Basis in paper: Appendix C.4 states current approximation is prone to missing eigenvectors in the middle of the spectrum
  - Why unresolved: Arnoldi iteration efficiently computes only the ends of the spectrum, potentially missing mid-frequency structural information
  - What evidence would resolve it: An algorithm that selects informative eigenvectors across the entire spectrum for large graphs, demonstrating improved accuracy

- **Question:** How can the sensitivity of LLPE to hyperparameters, specifically M and k, be mitigated?
  - Basis in paper: Page 7 notes LLPE requires tuning multiple hyperparameters with performance degrading under certain selections
  - Why unresolved: Paper relies on grid search to set M and k, identifying that small M limits approximation while small k excludes relevant data
  - What evidence would resolve it: A theoretical framework or adaptive mechanism for setting M and k that matches current manual tuning without extensive validation

- **Question:** Does augmenting spectral GNNs with LLPE yield complementary performance gains?
  - Basis in paper: Page 9 suggests LLPE can improve spectral GNNs by augmenting them
  - Why unresolved: Unclear if LLPE provides distinct positional information that improves spectral GNNs which already perform spectral filtering
  - What evidence would resolve it: Empirical benchmarks showing spectral GNNs equipped with LLPE outperform standard spectral GNNs on heterophilous datasets

## Limitations
- Full eigendecomposition remains O(n³), making the method impractical for very large graphs without Arnoldi approximation
- Theoretical assumptions assume smoothness of spectral filter captured by low-order Chebyshev polynomials, but real-world graphs may require higher-order approximations
- Critical implementation details like Chebyshev coefficient initialization and exact regularization hyperparameters are not fully specified

## Confidence
- **High Confidence:** Mechanism 1 (Adaptive Spectral Selection) - Direct theoretical proof in Theorem 3.2 and clear empirical demonstration in synthetic SBMs
- **Medium Confidence:** Mechanism 2 (Approximation of General Graph Distances) - Theoretical framework is sound but empirical validation is limited to specific distance metrics
- **Medium Confidence:** Mechanism 3 (Implicit Regularization) - Rademacher complexity analysis is rigorous, but practical impact depends heavily on hyperparameter tuning not fully explored

## Next Checks
1. **Scaling validation:** Implement Arnoldi-based approximation on graph with 50K+ nodes and verify minimal accuracy degradation compared to full eigendecomposition
2. **Spectral filter analysis:** For heterophilous graph, visualize learned Chebyshev coefficients h(λ) to confirm they weight high eigenvalues, validating adaptive mechanism
3. **Regularization ablation:** Systematically vary l1/l2 regularization strengths on medium-sized dataset to quantify impact on overfitting and confirm implicit regularization claim