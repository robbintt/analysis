---
ver: rpa2
title: 'Causal SHAP: Feature Attribution with Dependency Awareness through Causal
  Discovery'
arxiv_id: '2509.00846'
source_url: https://arxiv.org/abs/2509.00846
tags:
- causal
- shap
- feature
- shapley
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitation of SHAP in differentiating between
  causality and correlation in feature attribution. While SHAP is widely used for
  model interpretability, it fails to account for causal relationships between features,
  often misattributing importance in high-stakes domains like healthcare.
---

# Causal SHAP: Feature Attribution with Dependency Awareness through Causal Discovery

## Quick Facts
- arXiv ID: 2509.00846
- Source URL: https://arxiv.org/abs/2509.00846
- Reference count: 32
- Primary result: Achieves RMSE of 0.0167 on synthetic data and AUROC of 0.8594 on IBS data, outperforming standard SHAP methods

## Executive Summary
This paper addresses a fundamental limitation of SHAP in distinguishing causality from correlation in feature attribution. While SHAP is widely used for model interpretability, it fails to account for causal relationships between features, often misattributing importance in high-stakes domains like healthcare. The authors propose Causal SHAP, a framework that integrates causal relationships into feature attribution while preserving SHAP's desirable properties. Through experiments on synthetic and real-world biomedical datasets, Causal SHAP demonstrates superior performance in capturing true causal effects while maintaining the theoretical guarantees of Shapley values.

## Method Summary
Causal SHAP combines constraint-based causal discovery with Shapley value attribution. The method uses the Peter-Clark (PC) algorithm to construct a causal graph from observational data, then applies the Intervention Calculus when the DAG is Absent (IDA) algorithm to quantify causal strength between features. A key innovation is the causal value function that samples out-of-coalition features according to their causal relationships rather than marginal distributions, preventing evaluation on unrealistic data points. The framework computes causal weights that re-scale Shapley contributions based on the estimated total causal effects, producing attributions that reflect true causal influence rather than spurious correlations.

## Key Results
- On synthetic lung cancer risk data, Causal SHAP achieved RMSE of 0.0167 compared to 1.6357 for Independent SHAP
- On IBS biomedical dataset, Causal SHAP achieved AUROC of 0.8594 compared to 0.7620 for Independent SHAP
- On colorectal cancer dataset, Causal SHAP achieved AUROC of 0.6271 compared to 0.5819 for Independent SHAP

## Why This Works (Mechanism)

### Mechanism 1: Constraint-Based Causal Discovery via PC Algorithm
The PC algorithm constructs a causal graph from observational data, enabling the framework to distinguish between direct causes and spurious correlations. PC operates in two phases: first, it initializes a fully connected undirected graph and systematically removes edges between variable pairs that are conditionally independent given some separating set, using statistical tests with increasingly large conditioning sets; second, it identifies v-colliders and applies orientation rules to produce a completed partially directed acyclic graph (CPDAG). Core assumption: causal sufficiency and sufficient statistical power for reliable independence tests.

### Mechanism 2: Causal Strength Quantification via IDA
IDA estimates total causal effect of each feature on the target by aggregating path-specific effects, producing weights that scale Shapley contributions. From the CPDAG, IDA extracts possible parent sets and uses Pearl's do-calculus with regression-based edge weights to compute causal effects. For each feature i, the total causal effect Wi is computed by summing path-specific effects, where each path effect is the product of edge weights along that path. This is normalized to a causal weight factor γi that re-weights the Shapley kernel. Core assumption: linearity of causal relationships and that the CPDAG equivalence class reasonably represents the true causal structure.

### Mechanism 3: Dependency-Aware Sampling via Causal Value Function
The causal value function vc(S) modifies standard interventional expectation by sampling features not in coalition S from their causally-consistent distributions. Features without parents are sampled from marginals; features with parents are sampled from conditional distributions estimated via regression on parent values. This respects topological ordering of the causal graph. Core assumption: the discovered causal graph correctly specifies parent-child relationships, and linear regression adequately captures conditional distributions.

## Foundational Learning

- **Shapley Values and Coalition-Based Attribution**: Why needed: Causal SHAP inherits Shapley's theoretical foundation; understanding how marginal contributions across coalitions produce fair attribution is essential for debugging unexpected results. Quick check: Explain why Shapley values require averaging over all possible feature orderings and what the weighting term |S|!(n−|S|−1)!/n! represents.

- **Pearl's do-Calculus and Interventional vs. Observational Queries**: Why needed: The framework relies on do-operators to define causal interventions; confusing P(Y|X=x) with P(Y|do(X=x)) leads to misinterpreting attributions as causal effects on reality rather than on the model. Quick check: For a model predicting disease from symptoms and risk factors, explain why P(disease|do(smoking=1)) differs from P(disease|smoking=1).

- **CPDAGs and Markov Equivalence Classes**: Why needed: PC outputs a CPDAG representing multiple Markov-equivalent DAGs; understanding why some edges remain undirected helps interpret IDA's multi-set output and uncertainty in causal effects. Quick check: Given three variables A, B, C where A and C are both causes of B, explain why PC can identify B as a collider but may not determine edge directions between A-B and C-B without additional assumptions.

## Architecture Onboarding

- **Component map**:
  Training Data → PC (0.26s for IBS) → CPDAG → IDA (1.56s) → {Wi, edge weights} → Monte Carlo Sampler → Causal Value Function vc(S) → Causal SHAP Calculator → Normalize → ϕn

- **Critical path**: One-time cost: PC + IDA (~2s for 31 features). Per-instance: dominated by Monte Carlo iterations (T samples × M evaluations).

- **Design tradeoffs**:
  - PC vs. FCI: PC assumes no hidden confounders; FCI handles latent variables but is slower and produces ambiguous output.
  - Linear regression for conditionals: Fast and stable; may miss non-linear dependencies.
  - Monte Carlo samples M: Paper found M≥64 sufficient for stable AUROC.
  - Random coalition sampling (T iterations): Approximates exact 2^n coalition enumeration.

- **Failure signatures**:
  - Non-zero attribution for known non-causal features: PC discovered spurious edges or IDA misestimated effects.
  - Attribution magnitudes dominated by γi: Causal weights may overwhelm Shapley differences.
  - High variance across runs: Increase M or T.
  - RMSE >> baseline on synthetic data: Causal discovery failed.

- **First 3 experiments**:
  1. Synthetic validation with known DAG: Verify PC recovers correct skeleton, Causal SHAP assigns ~zero to "drink coffee," RMSE < 0.1.
  2. Ablation on causal value function: Compare standard interventional SHAP, Causal SHAP with γi=1, and full Causal SHAP.
  3. Hyperparameter sensitivity on M: Sweep M ∈ {16, 32, 64, 128, 256} and plot AUROC vs. computation time.

## Open Questions the Paper Calls Out

### Open Question 1
How can Causal SHAP be adapted to robustly handle datasets containing latent confounders? Basis: The authors state they aim to "address cases with hidden variables using the FCI algorithm" in future work. Unresolved because the current implementation relies on PC which assumes causal sufficiency. Evidence: Successful integration of FCI and experimental validation on datasets with known latent structures.

### Open Question 2
Does Greedy Equivalence Search (GES) provide a computationally scalable alternative to the PC algorithm without sacrificing attribution accuracy? Basis: The authors propose to "Explore Greedy Equivalence Search (GES) when the PC algorithm becomes computationally expensive as number of features scale." Unresolved because it remains untested whether score-based methods like GES can replace PC efficiently within this framework. Evidence: Comparative benchmarks on high-dimensional datasets showing GES-based Causal SHAP maintains comparable RMSE with reduced runtime.

### Open Question 3
How can the framework aggregate attributions across multiple possible causal graphs to mitigate structural uncertainty? Basis: The authors intend to "Enhance our framework to handle causal structure uncertainty by aggregating multiple possible causal graphs." Unresolved because the current method computes attributions based on a single estimated causal structure, creating a single point of failure. Evidence: A proposed aggregation mechanism and sensitivity analysis demonstrating more stable attributions under graph uncertainty.

## Limitations
- Causal discovery reliability with limited sample sizes (e.g., 74 IBS test samples) remains unproven
- Linear regression assumption for conditional distributions may miss non-linear causal mechanisms
- The aggregation method for IDA's multi-set output across equivalent DAGs is underspecified

## Confidence
- **High confidence**: Synthetic data results showing clear separation from baseline methods (RMSE 0.0167 vs 1.6357)
- **Medium confidence**: Real-world biomedical results, though promising, use relatively small sample sizes and may not generalize to other domains
- **Low confidence**: Claims about interpretability improvements for end-users; no user studies or qualitative evaluations provided

## Next Checks
1. Synthetic validation with hidden confounders: Generate synthetic data with latent variables and compare Causal SHAP against FCI-based alternatives to assess robustness to PC's causal sufficiency assumption.
2. Non-linear mechanism testing: Replace linear regression in IDA with kernel regression on a synthetic dataset with known non-linear causal effects to quantify performance degradation.
3. Edge stability analysis: Apply bootstrap resampling to PC algorithm on real-world datasets and report confidence intervals for discovered edges to quantify discovery uncertainty.