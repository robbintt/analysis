---
ver: rpa2
title: 'ProToken: Token-Level Attribution for Federated Large Language Models'
arxiv_id: '2601.19672'
source_url: https://arxiv.org/abs/2601.19672
tags:
- client
- protoken
- attribution
- federated
- provenance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProToken addresses the challenge of identifying which clients contributed
  to specific responses generated by federated large language models (LLMs). The method
  leverages transformer architecture characteristics and gradient-based relevance
  weighting to enable token-level provenance tracking while preserving FL privacy
  constraints.
---

# ProToken: Token-Level Attribution for Federated Large Language Models

## Quick Facts
- **arXiv ID:** 2601.19672
- **Source URL:** https://arxiv.org/abs/2601.19672
- **Authors:** Waris Gill; Ahmad Humayun; Ali Anwar; Muhammad Ali Gulzar
- **Reference count:** 33
- **Primary result:** ProToken achieves 98.62% average attribution accuracy across 16 configurations spanning four LLM architectures and four domains, maintaining 92-95% accuracy when scaling to 55 clients.

## Executive Summary
ProToken addresses the challenge of identifying which clients contributed to specific responses generated by federated large language models (LLMs). The method leverages transformer architecture characteristics and gradient-based relevance weighting to enable token-level provenance tracking while preserving FL privacy constraints. By exploiting FedAvg's linear aggregation properties and strategically monitoring task-relevant layers, ProToken can decompose global model outputs into per-client contributions with high accuracy.

## Method Summary
ProToken exploits the linear decomposition property of FedAvg aggregation to track client contributions through gradient-based relevance weighting. During inference, it captures hidden states and gradients from strategic layers (output projection and final MLP layers in the last N transformer blocks) of the global model, then computes what each client's corresponding layers would output using their parameter snapshots. The attribution score for each client per token is calculated as the inner product between the client's layer output and the gradient of the generated token's logit, with scores aggregated across tokens and normalized via softmax.

## Key Results
- Achieves 98.62% average attribution accuracy across 16 configurations spanning four LLM architectures and four domains
- Maintains high accuracy (92-95%) when scaling to 55 clients, demonstrating clear separation between contributing and non-contributing clients
- Gradient weighting provides 1.86× improvement in attribution accuracy compared to non-gradient methods

## Why This Works (Mechanism)

### Mechanism 1: Linear Decomposition via FL Aggregation
- **Claim:** FedAvg's weighted parameter averaging enables decomposing global model outputs into per-client contributions.
- **Mechanism:** Since FedAvg computes θ_global = Σᵢ ρᵢθᵢ, any neuron's output o_global = θ_global^T h can be expressed as Σᵢ ρᵢ(θᵢ^T h). ProToken exploits this by computing what each client's layer would output given the same input, then attributing based on contribution magnitude.
- **Core assumption:** Linear aggregation properties hold sufficiently for attribution purposes even in deep transformers with non-linear activations.
- **Evidence anchors:** [abstract] "gradient-based relevance weighting filters out irrelevant neural activations"; [Section 4.1] "Equation 4 demonstrates that the output of a neuron before the activation function can be decomposed into a weighted sum of outputs from the corresponding neuron's weights in each client model"
- **Break condition:** If aggregation becomes non-linear (e.g., secure aggregation with noise, or non-FedAvg methods breaking linearity), decomposition fails.

### Mechanism 2: Strategic Layer Selection
- **Claim:** Task-specific signals concentrate in later transformer blocks, enabling tractable attribution by monitoring only the final N layers.
- **Mechanism:** ProToken restricts provenance to output projection layers (self-attention) and final MLP layers in the last N transformer blocks. This reduces from billions of parameters to a tractable subset while preserving attribution signal.
- **Core assumption:** Later layers encode domain-specific knowledge that most directly influences token generation, which holds across model architectures.
- **Evidence anchors:** [abstract] "transformer architectures concentrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability"; [Section 4.1.2] "By restricting provenance analysis to these two critical layers within each of the last N Transformer blocks, we drastically reduce parameters to track"
- **Break condition:** If early layers encode critical task-specific information for certain domains, attribution accuracy degrades.

### Mechanism 3: Gradient-Based Relevance Weighting
- **Claim:** Token-specific gradients filter irrelevant activations, improving attribution accuracy by ~1.86×.
- **Mechanism:** Compute g^ℓ_{xⱼ} = ∂logit_{xⱼ}/∂h^ℓ_G to identify which activation dimensions influence the generated token. Attribution becomes P^ℓ_{i,xⱼ} = ⟨h^ℓ_i, g^ℓ_{xⱼ}⟩, weighting client contributions by relevance.
- **Core assumption:** Gradients accurately capture causal influence on token generation (not just correlation).
- **Evidence anchors:** [abstract] "gradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that directly influence token generation"; [Section 5.3 / Figure 4] "Gradient weighting provides substantial improvements across all settings... 1.86 × improvement (66.34% vs. 35.71%)"
- **Break condition:** Gradient saturation or vanishing gradients could produce misleading relevance weights.

## Foundational Learning

- **Concept: FedAvg Aggregation**
  - Why needed here: ProToken's decomposition (Mechanism 1) relies entirely on FedAvg's linear aggregation property θ_global = Σᵢ ρᵢθᵢ.
  - Quick check question: Can you explain why a weighted average of parameters permits decomposing a neuron's output into per-client contributions?

- **Concept: Transformer Hidden States and Gradients**
  - Why needed here: Mechanisms 2-3 require understanding which layers store task-specific information and how to compute ∂logit/∂hidden_state.
  - Quick check question: Given a 1B parameter model generating token "medical," which layers would you expect to have highest gradient magnitudes—early embedding layers or final MLP layers?

- **Concept: Autoregressive Generation**
  - Why needed here: ProToken aggregates per-token attributions across sequences. Understanding token dependency chains is essential for interpreting cumulative scores.
  - Quick check question: Why does ProToken sum attributions across tokens rather than taking the maximum per-client score?

## Architecture Onboarding

- **Component map:**
  Global LLM G^(r) -> Layer selector L -> Gradient computer -> Attribution aggregator -> Client models C^(r)_i

- **Critical path:**
  1. Forward pass through G captures hidden states h^ℓ_G and layer inputs input^ℓ_G
  2. Generate token xⱼ via argmax
  3. Backward pass computes gradients g^ℓ_{xⱼ}
  4. For each client i: compute h^ℓ_i = f^ℓ(input^ℓ_G; θ^ℓ_i) using client weights on global inputs
  5. Inner product: P^ℓ_{i,xⱼ} = ⟨h^ℓ_i, g^ℓ_{xⱼ}⟩
  6. Aggregate across layers → tokens → softmax normalize

- **Design tradeoffs:**
  - More layers = higher overhead (29% increase from 3 to all layers per Figure 5) but potentially captures more signal
  - Gradient computation adds backprop overhead per token, but Figure 5 shows ~1-2s per attribution is tractable
  - Scaling clients: Accuracy drops from 98.62% (6 clients) to 92-95% (55 clients)—acceptable degradation

- **Failure signatures:**
  - Near-random attribution (35% accuracy): Gradients disabled—activate gradient weighting
  - No separation between clients: Check layer selection; may need to increase N
  - Slow convergence (first 1-2 rounds): Normal; backdoor signals weak initially (Figure 2)
  - High variance across runs: Verify consistent aggregation coefficients ρᵢ

- **First 3 experiments:**
  1. **Ablation with/without gradients:** Replicate Figure 4 on your domain. Expect ~1.8× improvement with gradients enabled. If improvement is lower, check gradient computation correctness.
  2. **Layer sweep:** Vary N from 3 to all layers. Plot attribution accuracy vs. latency (Figure 5 pattern). Identify minimum layers for your accuracy target.
  3. **Scale test:** Run with 10, 25, 50 clients using backdoor ground truth (Section 5.1 methodology). Verify separation between contributing/non-contributing clients persists (Figure 7 pattern).

## Open Questions the Paper Calls Out

- **Open Question 1:** How well does ProToken generalize to natural (non-backdoor) attribution scenarios where client contributions are diffuse rather than trigger-induced?
  - **Basis in paper:** [explicit] The authors state the backdoor-based evaluation "serves solely as a proxy to assess ProToken's general client attribution capabilities" and acknowledge this is for creating "verifiable ground truth" rather than representing real-world attribution patterns.
  - **Why unresolved:** All accuracy claims (98.62%) derive from artificially injected trigger-response pairs that create unambiguous provenance signals. Real client contributions may distribute more subtly across training data without clear sentinel responses.
  - **What evidence would resolve it:** Evaluation on naturally partitioned datasets with human-verified ground truth attributions, or synthetic scenarios where specific knowledge domains are known to originate from designated clients without adversarial injection.

- **Open Question 2:** What privacy risks does ProToken's gradient computation and client-specific activation analysis introduce in federated settings?
  - **Basis in paper:** [inferred] The paper claims ProToken "preserves FL privacy constraints" because it "operates on model updates, activations, and gradients (not raw client data)." However, gradients and activations can leak information about training data through gradient inversion or membership inference attacks.
  - **Why unresolved:** No formal privacy analysis (e.g., differential privacy guarantees) is provided. The method requires storing and analyzing per-client activation patterns, which could constitute additional attack surface beyond standard FL.
  - **What evidence would resolve it:** Formal privacy analysis quantifying information leakage, or empirical evaluation against gradient inversion and membership inference attacks in the ProToken setting.

## Limitations

- The method critically depends on FedAvg's linear parameter averaging property, making it inapplicable to FL systems using secure aggregation, differential privacy, or alternative aggregation methods.
- Empirical validation relies entirely on synthetic backdoor triggers as ground truth, which may not capture the complexity of real-world domain-specific knowledge contributions.
- The scalability claims to 55 clients show accuracy degradation (92-95%) but lack comprehensive analysis of how attribution performance scales with increasing model size, longer sequences, or non-IID data distributions.

## Confidence

- **High Confidence (9/10):** The attribution accuracy results (98.62% baseline, 92-95% at 55 clients) are well-supported by controlled experiments with clear ground truth from backdoor triggers.
- **Medium Confidence (6/10):** The claim about transformer architectures concentrating task-specific signals in later blocks is supported by empirical results but lacks deeper architectural analysis.
- **Low Confidence (4/10):** The generalizability claims to production FL systems and complex real-world scenarios are weakly supported, as all experiments use synthetic triggers and controlled conditions.

## Next Checks

1. **Cross-Architecture Generalization Test:** Evaluate ProToken on larger models (7B+ parameters) and diverse architectures (e.g., Mixture-of-Experts) to verify if the 98.62% accuracy baseline holds when scaling beyond 0.5B-3B parameter models, and measure the computational overhead increase.

2. **Non-Backdoor Ground Truth Validation:** Develop and implement an alternative ground truth method using domain-expert annotated data where client contributions are known from the labeling process, then compare ProToken attribution accuracy against this independent benchmark.

3. **Robustness to Aggregation Variations:** Test ProToken's attribution accuracy when FedAvg is modified with secure aggregation noise, differential privacy mechanisms, or alternative FL algorithms (FedProx, FedNova) to quantify the degradation in attribution accuracy under realistic privacy-preserving FL deployments.