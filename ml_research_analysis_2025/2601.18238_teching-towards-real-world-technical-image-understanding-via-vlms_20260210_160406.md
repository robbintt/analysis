---
ver: rpa2
title: 'TechING: Towards Real World Technical Image Understanding via VLMs'
arxiv_id: '2601.18238'
source_url: https://arxiv.org/abs/2601.18238
tags:
- code
- diagram
- image
- diagrams
- llama3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling Vision-Language
  Models (VLMs) to understand and generate technical diagrams from hand-drawn images.
  To overcome the lack of large-scale real-world hand-drawn technical diagram datasets,
  the authors propose TechING, a large synthetically generated corpus (115,014 images)
  of diverse technical diagrams paired with corresponding Mermaid code and descriptions.
---

# TechING: Towards Real World Technical Image Understanding via VLMs

## Quick Facts
- arXiv ID: 2601.18238
- Source URL: https://arxiv.org/abs/2601.18238
- Reference count: 40
- Key outcome: Fine-tuning Llama 3.2 11B-instruct on TechING improves ROUGE-L performance by 2.14x over the base model and achieves lowest compilation errors across baselines in 7 out of 8 diagram types on real-world hand-drawn diagrams

## Executive Summary
This paper addresses the challenge of enabling Vision-Language Models (VLMs) to understand and generate technical diagrams from hand-drawn images. The authors propose TechING, a large synthetically generated corpus (115,014 images) of diverse technical diagrams paired with corresponding Mermaid code and descriptions. They introduce multiple self-supervision tasks and fine-tune Llama 3.2 11B-instruct to create LLama-VL-TUG, which significantly outperforms baseline models on both synthetic and real-world hand-drawn technical diagrams.

## Method Summary
The method involves generating a large synthetic corpus (TechING) of technical diagrams using Mermaid templates, then fine-tuning Llama 3.2 11B-Vision-Instruct with LoRA adapters on multiple tasks including Image2Code, Description2Code, and self-supervision tasks. The training incorporates data augmentation to improve robustness to real-world variations. The model is evaluated on synthetic test sets and a real-world corpus of 545 hand-drawn technical diagrams across 8 diagram types.

## Key Results
- LLama-VL-TUG improves ROUGE-L performance by 2.14x over base Llama 3.2 11B-instruct on synthetic diagrams
- On real-world hand-drawn diagrams, achieves lowest compilation errors across all baselines in 7 out of 8 diagram types
- Improves average F1 score of Llama 3.2 11B-instruct by 6.97x on real-world evaluation
- Self-supervision tasks contribute significantly to performance gains on real-world data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic training with structured code representations transfers to real-world hand-drawn diagrams
- Mechanism: By learning to map clean synthetic diagrams to their Mermaid code, the model internalizes structural parsing rules that generalize to noisy hand-drawn versions
- Core assumption: Structural priors learned from synthetic diagrams are domain-invariant and apply to hand-drawn distribution shift
- Evidence anchors: Human evaluation shows lowest compilation errors in 7/8 diagram types on real-world images; training on synthetic D1 yields strong performance on real D3

### Mechanism 2
- Claim: Multi-task self-supervision improves structural fidelity over primary tasks alone
- Mechanism: Self-supervision tasks force the model to learn fine-grained cross-modal alignment and structural consistency
- Core assumption: Auxiliary tasks provide complementary supervision signals that reinforce structural understanding
- Evidence anchors: Ablation study shows multi-task outperforms single-task on real-world D3; self-supervision described as learning fine-grained alignment

### Mechanism 3
- Claim: Image augmentation during training bridges synthetic-to-real distribution gap
- Mechanism: Applying diverse augmentations simulates real-world image degradation, making the model robust to digitization artifacts
- Core assumption: Augmentation distribution sufficiently covers real-world noise patterns
- Evidence anchors: Data augmentation described as improving diversity and bringing data closer to real-world setting; Albumentations pipeline detailed

## Foundational Learning

- Concept: Intermediate Structured Representations (e.g., Mermaid code)
  - Why needed here: The paper uses Mermaid code as a symbolic bridge between visual diagrams and editable formats
  - Quick check question: Why might converting a diagram to code before editing be more robust than direct image manipulation?

- Concept: Cross-Modal Vision-Language Alignment
  - Why needed here: The model must align visual elements with textual/structural tokens in Mermaid code
  - Quick check question: What types of errors would indicate a failure in cross-modal alignment (e.g., spurious edges, missing labels)?

- Concept: Multi-Task Learning with Auxiliary Self-Supervision
  - Why needed here: The paper combines primary tasks with self-supervision tasks
  - Quick check question: How might a positive/negative image-code pair classification task improve structural consistency?

## Architecture Onboarding

- Component map: Mermaid templates -> synthetic images -> TechING corpus -> LoRA fine-tuning -> LLama-VL-TUG
- Critical path: Generate synthetic data via Mermaid templates → compile to images → create D2 (incomplete images) → apply augmentations → fine-tune with LoRA using mixed-task sampling → evaluate on synthetic and real-world D3
- Design tradeoffs:
  - Synthetic vs. real data: Scalable but may miss real-world variation; D3 only 545 samples
  - Single language (Mermaid) vs. multiple: Reduces syntax complexity but limits diagram type coverage
  - LoRA vs. full fine-tuning: Parameter-efficient but may limit adaptation capacity
- Failure signatures:
  - High compilation errors in generated Mermaid code
  - Low F1 on labeled edges (text-on-edge recognition failure)
  - Wrong diagram type detection
  - Significant drop in ROUGE-L on real-world vs. synthetic images
- First 3 experiments:
  1. Baseline zero-shot evaluation: Test pre-trained VLMs on Image2Code with synthetic eval set
  2. Single-task fine-tuning: Fine-tune Llama 3.2 11B on Image2Code only
  3. Ablation of self-supervision: Compare multi-task with vs. without self-supervision tasks on D3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can diffusion-based generative models be effectively trained to produce high-fidelity technical diagrams for corpus augmentation?
- Basis: Initial experiments with diffusion and CLIP models did not perform well
- Why unresolved: Authors defaulted to programmatic generation via Mermaid to guarantee correctness
- What evidence would resolve it: Successful fine-tuning of a diffusion model to generate syntactically correct technical diagrams

### Open Question 2
- Question: Does the TechING corpus and self-supervision training pipeline yield consistent improvements across diverse VLM architectures, particularly Mixture-of-Experts (MoE) models?
- Basis: Authors focused on fine-tuning a single model (Llama 3.2) due to computational constraints
- Why unresolved: Only Llama 3.2 and preliminary Gemma fine-tune were benchmarked
- What evidence would resolve it: Comparative study fine-tuning MoE-based VLMs on TechING corpus

### Open Question 3
- Question: Can the performance gap between base model and LLama-VL-TUG be closed using few-shot learning or complex context engineering techniques?
- Basis: Authors did not explore complex techniques of context engineering, few-shot learning, etc.
- Why unresolved: Improvements shown are attributed to fine-tuning; unknown if underlying knowledge can be elicited via advanced prompting
- What evidence would resolve it: Benchmark comparing LLama-VL-TUG against base Llama 3.2 using advanced few-shot prompting strategies

### Open Question 4
- Question: How does the model's performance degrade when processing highly irregular or "extremely messy" hand-drawn diagrams?
- Basis: Dataset does not encompass highly irregular or informal hand-drawn sketches
- Why unresolved: Current evaluation scope may exclude complex edge cases where synthetic-to-real generalization might fail
- What evidence would resolve it: Expanded human evaluation on irregular, non-standard, or highly degraded hand-drawn diagrams

## Limitations
- The paper focuses on a single VLM architecture (Llama 3.2) due to computational constraints, leaving generalization to other architectures untested
- The evaluation scope deliberately excludes highly irregular or complex hand-drawn diagrams with more than 15 structures
- The authors did not explore advanced prompting techniques or few-shot learning approaches that might achieve similar results without fine-tuning

## Confidence

- High: Synthetic data generation pipeline, training procedure, and LoRA hyperparameters are precisely specified and reproducible
- Medium: Claims about synthetic-to-real transfer and self-supervision benefits are supported by experimental results but depend on untested assumptions about distribution coverage and task complementarity
- Low: The paper asserts that structural parsing rules learned from synthetic diagrams generalize to hand-drawn images, but this is not independently validated through controlled experiments

## Next Checks

1. **Controlled augmentation ablation**: Train separate models with (a) no augmentation, (b) limited augmentation (only blur/noise), and (c) full augmentation pipeline to isolate the contribution of image augmentation to real-world performance

2. **Individual self-supervision task analysis**: Create ablations testing each self-supervision task in isolation (e.g., only Image-Code pair classification, only Description2Code) to quantify individual task contributions versus combined multi-task training

3. **Distribution shift analysis**: Systematically vary synthetic diagram complexity and noise levels to map the boundary conditions where synthetic-to-real transfer breaks down, providing quantitative limits on the generalization mechanism