---
ver: rpa2
title: 'ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems'
arxiv_id: '2506.14299'
source_url: https://arxiv.org/abs/2506.14299
tags:
- driving
- lane
- decision
- autonomous
- tactics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents ADRD, a novel autonomous driving framework
  that leverages large language models (LLMs) to generate interpretable, rule-based
  decision systems. The core innovation lies in using LLMs to automatically construct
  decision trees through a three-module architecture: Information Module (aggregates
  driving scenario data), Agents Module (Planner generates tactics, Coder converts
  to executable code, Summarizer refines based on feedback), and Testing Module (validates
  decisions).'
---

# ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems

## Quick Facts
- arXiv ID: 2506.14299
- Source URL: https://arxiv.org/abs/2506.14299
- Authors: Fanzhi Zeng; Siqi Wang; Chuzhao Zhu; Li Li
- Reference count: 40
- Key outcome: ADRD achieves 25.15s average safe driving time in highway-env, outperforming PPO (10.90s) and DiLu (23.00s) baselines

## Executive Summary
ADRD introduces a novel autonomous driving framework that uses large language models (LLMs) to automatically generate interpretable rule-based decision systems. The framework addresses the traditional trade-off between interpretability and performance in autonomous driving by leveraging LLMs' reasoning and programming capabilities to create executable decision trees. Through a three-module architecture (Information, Agents, and Testing), ADRD iteratively refines driving tactics based on simulation feedback, producing policies that adapt their complexity to driving scenario difficulty.

## Method Summary
ADRD operates through a closed-loop system where LLMs generate driving tactics and corresponding decision tree code. The Information Module aggregates scenario data, the Agents Module (comprising Planner, Coder, and Summarizer) creates and refines tactics, and the Testing Module validates decisions through simulation. The framework produces interpretable decision trees that enable sub-microsecond inference while maintaining strong driving performance. ADRD demonstrates superior results compared to reinforcement learning baselines on highway-env benchmarks.

## Key Results
- ADRD achieves 25.15s average safe driving time, outperforming PPO (10.90s) and DiLu (23.00s) baselines
- Decision tree complexity scales appropriately with driving difficulty, from 10 levels (low density) to 34 levels (high density)
- Inference time remains under 1 microsecond per command, enabling real-time deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can translate natural language driving tactics into executable decision tree code that achieves safe driving behavior.
- Mechanism: The Coder module receives textual tactics from the Planner and generates Python functions with conditional logic. The decision tree structure naturally maps to if-then rules, allowing the LLM's pretrained knowledge of driving semantics and code syntax to produce functional policies without gradient-based training.
- Core assumption: LLMs pretrained on code and driving-related text possess sufficient world knowledge to generate logically coherent decision trees that generalize beyond specific training scenarios.
- Evidence anchors: [abstract] "harnessing the strong reasoning and programming capabilities of LLMs...generate rule-based driving tactics"; [section 3.3] "The Coder's prompt generator also comprises three components: system prompts, textual driving tactic descriptions, and past code along with improvement suggestions"
- Break condition: If driving scenarios require reasoning beyond the LLM's pretrained knowledge, generated rules may be incomplete or incorrect.

### Mechanism 2
- Claim: Closed-loop iteration through simulation feedback improves decision tree quality without retraining the LLM.
- Mechanism: The Summarizer module receives collision reports containing ego-vehicle and surrounding vehicle trajectories over T timesteps. It identifies whether failures stem from tactic design or code implementation, then generates targeted refinement advice. This advice is injected into subsequent Planner/Coder prompts, enabling iterative policy improvement.
- Core assumption: LLMs can accurately diagnose collision causes from trajectory data and map them to specific tactical or implementation flaws.
- Evidence anchors: [section 3.4] "Collision reports are formatted by the Testing Module and include historical trajectories of the ego-vehicle and surrounding vehicles over the past T time steps, serving as reference data for the LLM to infer collision causes"
- Break condition: If collision causes are ambiguous or multi-factorial, the Summarizer may misattribute blame, leading to counterproductive refinements.

### Mechanism 3
- Claim: Decision tree depth and complexity adapt to driving scenario difficulty and style constraints.
- Mechanism: When prompted with aggressive driving targets or higher vehicle densities, the LLM generates tactics with more conditional branches to handle edge cases. Conservative styles produce shallower trees prioritizing safety checks; aggressive styles produce deeper trees with overtake logic, emergency handling, and confirmation thresholds.
- Core assumption: LLMs implicitly understand the relationship between scenario complexity and required policy granularity through their pretrained reasoning capabilities.
- Evidence anchors: [section 4.3] "as the difficulty of the driving scenario increases, the decision trees produced by ADRD become progressively more complex. Notably, when the vehicle density reaches 1.25, the depth of the decision tree sharply increases to 34"
- Break condition: If scenario difficulty exceeds the LLM's capacity to enumerate relevant conditions, trees may become either overly complex or insufficiently comprehensive.

## Foundational Learning

- Concept: **Decision Trees as Rule-Based Systems**
  - Why needed here: The entire ADRD framework outputs decision trees; understanding how nodes represent feature tests and leaves represent actions is essential for interpreting and modifying generated policies.
  - Quick check question: Given a decision node testing "is gap_ahead < safe_gap?", what are the two possible branches and their semantic meanings for driving?

- Concept: **LLM Prompt Engineering for Structured Outputs**
  - Why needed here: The Planner, Coder, and Summarizer each require carefully designed prompts with system messages, context, and output format constraints to produce usable tactics and code.
  - Quick check question: If the Coder prompt omits observation space variable descriptions, what type of generation errors would you expect?

- Concept: **Autonomous Driving Action Spaces**
  - Why needed here: ADRD operates on discrete action spaces (IDLE, LANE_LEFT, LANE_RIGHT, FASTER, SLOWER); understanding these primitives is necessary for debugging why generated code selects specific actions.
  - Quick check question: In highway-env, what is the difference between executing SLOWER at minimum speed versus at mid-range speed?

## Architecture Onboarding

- Component map:
  Information Module → [scenario text + rules + vehicle state]
  Agents Module: Planner → [textual tactics with conditions/priorities]; Coder → [executable Python on_step() function]; Summarizer ← [collision reports from Testing Module]
  Testing Module → [simulation validation → collision reports or success]
  Deployed Decision Tree → [real-time inference <1μs per command]

- Critical path: Information Module → Planner → Coder → Testing Module. If any component produces low-quality outputs, the entire iteration loop degrades. The Summarizer is the highest-cognitive-load component for debugging refinement quality.

- Design tradeoffs:
  - Separating Planner (text) from Coder (code) adds interpretability but increases prompt engineering complexity
  - Decision trees enable <1μs inference but may not capture continuous dynamics as effectively as neural networks
  - Conservative styles produce simpler/safer trees; aggressive styles produce more capable but complex trees

- Failure signatures:
  - Infinite retry loops: Summarizer provides advice that doesn't address root cause
  - Syntactically correct but semantically wrong code: Coder misinterprets tactic descriptions
  - Over-conservative policies: Planner prioritizes safety to exclusion of progress

- First 3 experiments:
  1. Run ADRD on highway-env with default 4-lane, density 2.0 configuration; verify generated decision tree produces >23s average safe driving time
  2. Inspect the Planner output for a collision scenario; manually verify the Summarizer correctly identifies whether the issue is tactical or implementational
  3. Modify the driving target prompt from "conservative" to "aggressive"; measure the change in decision tree depth and node count to confirm adaptive complexity

## Open Questions the Paper Calls Out
None

## Limitations
- Simulation-only validation: All experiments conducted in highway-env simulator, performance in real-world driving unverified
- Decision tree scalability: Extremely complex urban scenarios may require exponentially larger trees exceeding LLM capacity
- LLM dependency on driving corpus: Novel traffic patterns or regulations not in training corpus could lead to unsafe decisions

## Confidence

- **High Confidence**: Core mechanism of using LLMs to generate interpretable rule-based policies (Mechanism 1)
- **Medium Confidence**: Closed-loop refinement system (Mechanism 2) - limited corpus evidence for this specific application
- **Medium Confidence**: Adaptive complexity scaling (Mechanism 3) - supported by experimental results but requires further validation

## Next Checks

1. Test ADRD on real-world driving datasets (nuScenes, Waymo Open Dataset) to verify simulation performance translates to sensor-rich environments
2. Conduct ablation studies removing the Summarizer component to quantify its contribution to policy improvement
3. Implement stress tests with adversarial scenarios (aggressive cut-ins, sudden lane changes) to determine maximum complexity LLM can handle