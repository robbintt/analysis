---
ver: rpa2
title: 'Unconstrained Body Recognition at Altitude and Range: Comparing Four Approaches'
arxiv_id: '2502.07130'
source_url: https://arxiv.org/abs/2502.07130
tags:
- body
- identification
- swin-bidds
- image
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares four deep learning approaches to long-term
  body identification under unconstrained conditions, including extreme range (up
  to 1000m), altitude (UAV views), and clothing changes. The methods include two Vision
  Transformer architectures (BIDDS and Swin-BIDDS) and two ResNet-based models (LCRIM
  and NLCRIM), all trained on a diverse dataset of 1.9 million images across 5,000
  identities.
---

# Unconstrained Body Recognition at Altitude and Range: Comparing Four Approaches

## Quick Facts
- arXiv ID: 2502.07130
- Source URL: https://arxiv.org/abs/2502.07130
- Authors: Blake A Myers; Matthew Q Hill; Veda Nandan Gandi; Thomas M Metz; Alice J O'Toole
- Reference count: 40
- Key outcome: Swin-BIDDS (Vision Transformer) outperformed ResNet-based models for body identification under unconstrained conditions including extreme range (up to 1000m), altitude (UAV views), and clothing changes, with image size (384×384 vs 224×224) being the primary driver of performance gains.

## Executive Summary
This paper compares four deep learning approaches to long-term body identification under unconstrained conditions, including extreme range (up to 1000m), altitude (UAV views), and clothing changes. The methods include two Vision Transformer architectures (BIDDS and Swin-BIDDS) and two ResNet-based models (LCRIM and NLCRIM), all trained on a diverse dataset of 1.9 million images across 5,000 identities. Results show that Swin-BIDDS consistently outperformed other models across benchmark datasets (MARS, MSMT17, Outdoor Gait, DeepChange) and the unconstrained BRIAR test set. Ablation experiments revealed that the primary driver of Swin-BIDDS' superior performance was its larger input image size (384×384 vs 224×224), though the Swin-ViT architecture also contributed a smaller performance gain. This work demonstrates that larger image sizes are critical for capturing subtle body shape differences in challenging identification scenarios.

## Method Summary
The study compared four deep learning architectures for body recognition: BIDDS and Swin-BIDDS (Vision Transformer-based) versus LCRIM and NLCRIM (ResNet-50-based). All models used hard triplet loss with negative mining and were trained on 1.9 million images from 9 diverse datasets. BIDDS and LCRIM employed standard ViT-B/16 and ResNet-50 backbones respectively, while Swin-BIDDS used the Swin-ViT architecture for improved computational efficiency with larger inputs. LCRIM incorporated linguistic pre-training on body shape descriptors, while NLCRIM omitted this component. Models were fine-tuned on BRS1-5 at 384×384 resolution. Performance was evaluated using TAR@FAR (10^-3, 10^-4), Rank-1, Rank-20 accuracy, and AUC across benchmark datasets and an unconstrained BRIAR test set.

## Key Results
- Swin-BIDDS consistently outperformed all other models on benchmark datasets (MARS, MSMT17, Outdoor Gait, DeepChange) and unconstrained BRIAR test set
- Ablation experiments identified 384×384 input resolution as the primary performance driver (+0.0799 TAR@FAR 10^-3, +0.0768 Rank 1) over 224×224
- Swin-ViT architecture provided secondary benefit (+0.0227 TAR@FAR 10^-3, +0.0069 Rank 1) at constant 224×224 resolution
- Strong performance on clothing-change datasets indicates models learned body shape cues rather than clothing-based shortcuts
- Face-restricted performance matching face-included performance confirms body features were being used correctly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger input image size (384×384) is the primary driver of improved body identification performance.
- Mechanism: Higher resolution inputs preserve fine-grained body shape details required for distinguishing individuals at long range and altitude, where body features occupy fewer pixels in the original capture.
- Core assumption: Body shape discrimination relies on subtle structural features that are degraded or lost at 224×224 resolution.
- Evidence anchors:
  - [abstract]: "Ablation experiments revealed that the primary driver of Swin-BIDDS' superior performance was its larger input image size (384×384 vs 224×224)"
  - [section III.C, Table V]: Image size increase from 224 to 384 yielded +0.0799 TAR@FAR 10^-3, +0.0460 TAR@FAR 10^-4, +0.0768 Rank 1 improvement, substantially larger than architecture change effects
  - [corpus]: Weak direct evidence—corpus papers focus on gait recognition and multi-modal approaches without comparable image size ablations
- Break condition: When source imagery is already low-resolution (e.g., extreme range >500m), upsampling to 384×384 may not recover lost detail, reducing the size benefit.

### Mechanism 2
- Claim: Swin-ViT's hierarchical windowed attention provides a secondary performance boost over standard ViT.
- Mechanism: Shifted window self-attention limits computation to local regions while enabling cross-window connections, capturing multi-scale body features more efficiently than global attention in standard ViT.
- Core assumption: Body structure information exists at multiple spatial scales that benefit from hierarchical processing.
- Evidence anchors:
  - [abstract]: "the Swin-ViT architecture also contributed a smaller performance gain"
  - [section III.C, Table V]: Architecture change (BIDDS to Swin-BIDDS at constant 224×224) yielded +0.0227 TAR@FAR 10^-3, +0.0069 Rank 1—positive but smaller than image size effect
  - [corpus]: Combo-Gait paper supports transformer effectiveness for body recognition tasks
- Break condition: Assumption: Benefit may diminish on datasets with limited scale variation or when body regions are already optimally scaled.

### Mechanism 3
- Claim: Large-scale diverse training with hard triplet mining enables learning clothing-invariant body representations.
- Mechanism: Training across 1.9M images from 9 datasets with clothing changes forces the model to learn persistent body shape features rather than clothing-based shortcuts; hard negative mining selects challenging same-identity pairs to improve discrimination.
- Core assumption: Stable body shape features exist and can be isolated from clothing/pose variation given sufficient diverse training data.
- Evidence anchors:
  - [abstract]: "trained on a diverse dataset of 1.9 million images across 5,000 identities"
  - [section II.B]: "All models employed hard triplet loss with negative mining... We selected the most challenging negative samples (i.e., those closest to the anchor in the embedding space)"
  - [section IV]: "strong performance of BIDDS and Swin-BIDDS on the no-clothes-change datasets indicates that the models utilize body shape cues, and other identifying information not linked to clothing"
  - [corpus]: No direct corpus evidence on training data diversity effects for body ID
- Break condition: If training data lacks sufficient clothing variability per identity, model may learn spurious clothing correlations.

## Foundational Learning

- Concept: **Vision Transformer (ViT) Patch Embedding**
  - Why needed here: Both BIDDS and Swin-BIDDS use ViT architectures; understanding how images become token sequences is essential for debugging.
  - Quick check question: Given a 384×384 input and 16×16 patch size, how many tokens does the transformer receive (excluding CLS token)?

- Concept: **Triplet Loss with Hard Negative Mining**
  - Why needed here: All four models use this training objective; understanding anchor-positive-negative structure explains why models learn discriminative embeddings.
  - Quick check question: Why does selecting the "hardest" negative (closest to anchor in embedding space) improve learning compared to random negatives?

- Concept: **Ablation Study Design**
  - Why needed here: The paper's key finding depends on properly isolating variables; understanding factorial design prevents confounded conclusions.
  - Quick check question: To isolate architecture effect from image size effect, which two model configurations must be compared?

## Architecture Onboarding

- Component map: Input preprocessing (aspect-ratio-preserving resize to 224×224 or 384×384 on black background) -> Backbone (ViT-B/16 for BIDDS, Swin-ViT for Swin-BIDDS, ResNet-50 for LCRIM/NLCRIM) -> Embedding head (2048-dimensional fully-connected layer) -> Hard triplet loss training -> Inference (average embeddings across frames)

- Critical path:
  1. Input resolution (384×384) → most impactful design choice per ablation
  2. Backbone selection (Swin-ViT preferred for large inputs due to linear scaling)
  3. Triplet mining strategy during training

- Design tradeoffs:
  - 384×384 improves accuracy but increases memory/compute; Swin-ViT scales O(n) vs ViT's O(n²) with image size
  - Linguistic pre-training (LCRIM) added complexity without meaningful gain over NLCRIM
  - Fine-tuning on BRIAR data improves unconstrained performance but requires domain-specific data

- Failure signatures:
  - Large accuracy drop on UAV/long-range partitions vs. benchmark datasets → insufficient training diversity for unconstrained conditions
  - Face-restricted performance matching face-included → body features being used correctly
  - Near-zero TAR@FAR 10^-4 → embeddings not sufficiently discriminative for verification tasks

- First 3 experiments:
  1. **Reproduce image size ablation**: Train Swin-BIDDS at 224×224 and 384×384, measure TAR@FAR delta on held-out BRIAR partition to validate ~8% improvement claim.
  2. **Compute-accuracy curve**: Test intermediate sizes (256, 288, 320, 352) to identify optimal tradeoff point for deployment constraints.
  3. **Cross-dataset generalization**: Train on BRIAR-only vs. full 9-dataset mix, evaluate on Outdoor Gait (unseen during training) to quantify diversity benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can unsupervised or semi-supervised learning paradigms that leverage unlabeled data in complex viewing conditions reduce the reliance on fully annotated datasets while handling rare body shapes?
- Basis in paper: [explicit] The authors state, "Promising avenues for future work include... unsupervised or semi-supervised approaches that exploit partially labeled videos... potentially handling rare body shapes and challenging occlusions more robustly."
- Why unresolved: The current study relied exclusively on a massive dataset of 1.9 million labeled images, which is resource-intensive to curate. The efficacy of learning from unlabeled or partially labeled data in this specific "unconstrained" context remains untested.
- What evidence would resolve it: Comparative performance metrics of a semi-supervised model trained on partially labeled BRIAR data versus the current fully supervised Swin-BIDDS, specifically analyzing error rates on rare body shapes.

### Open Question 2
- Question: Does increasing the input image size beyond 384×384 yield diminishing returns for body identification accuracy, or does it remain the primary driver of performance gains?
- Basis in paper: [inferred] The ablation study identified image size as the "critical factor" for performance, but only compared 224×224 against 384×384. It is unclear if the relationship between input size and identification accuracy is linear or if it plateaus at higher resolutions.
- Why unresolved: The study demonstrated that larger inputs help capture subtle shape differences, but the upper bound of this benefit was not tested due to potential computational constraints mentioned regarding standard ViT scaling.
- What evidence would resolve it: A follow-up ablation study benchmarking Swin-BIDDS performance at resolutions of 512×512 and higher on the Long-range and UAV partitions of the BRIAR dataset.

### Open Question 3
- Question: How can explicit shape-based encoders be integrated with current Vision Transformer architectures to improve resilience against occlusions?
- Basis in paper: [explicit] The authors suggest, "Integrating such techniques... in conjunction with shape-based encoders may open the door to even more accurate and resilient body-identification systems."
- Why unresolved: The paper focused on comparing direct 2D image mapping approaches (ResNet and ViT) against each other, but did not experimentally combine these with 3D shape reconstruction or extraction modules.
- What evidence would resolve it: Implementation of a hybrid system combining the Swin-BIDDS backbone with a texture-insensitive shape encoder, evaluated on the "Face Restricted" and "UAV" probe sets where occlusion and view angles are challenging.

## Limitations

- Lack of controlled ablation on Swin-BIDDS architecture at 384×384 resolution makes it difficult to isolate architectural contribution from image size effect
- Training duration and computational costs not reported, limiting practical deployment guidance
- Specific contribution of each component dataset to final performance remains unclear despite diverse training data

## Confidence

- **High confidence**: Image size effect (384×384 vs 224×224) is the primary performance driver, supported by controlled ablation experiments and large effect size (TAR@FAR 10^-3: +0.0799, Rank 1: +0.0768)
- **Medium confidence**: Swin-ViT architecture provides secondary benefit, based on positive but smaller effect size (+0.0227 TAR@FAR 10^-3, +0.0069 Rank 1) and established literature on transformer efficiency
- **Medium confidence**: Training diversity enables clothing-invariant learning, supported by strong benchmark performance but lacking direct ablation on dataset composition

## Next Checks

1. **Architecture isolation test**: Train BIDDS at 384×384 to quantify the pure Swin-ViT contribution at large resolution, confirming whether the architecture benefit holds independent of image size.
2. **Compute-accuracy tradeoff analysis**: Systematically test intermediate resolutions (256×256, 288×288, 320×320, 352×352) to identify the optimal point balancing performance gains against computational costs for deployment scenarios.
3. **Dataset contribution analysis**: Train models on subsets of the 9-dataset training set (e.g., clothing-change datasets only vs. no-clothing-change datasets only) to quantify how each data type contributes to unconstrained recognition performance.