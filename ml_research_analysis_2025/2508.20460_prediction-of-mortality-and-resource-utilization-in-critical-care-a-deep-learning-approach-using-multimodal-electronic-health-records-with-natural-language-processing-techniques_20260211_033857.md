---
ver: rpa2
title: 'Prediction of mortality and resource utilization in critical care: a deep
  learning approach using multimodal electronic health records with natural language
  processing techniques'
arxiv_id: '2508.20460'
source_url: https://arxiv.org/abs/2508.20460
tags:
- data
- structured
- learning
- patient
- mortality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a deep learning framework that integrates multimodal
  electronic health records using natural language processing to predict mortality
  and resource utilization in critical care. The method employs prompt learning with
  medical language templates to generate cell embeddings from structured EHR data,
  combined with free-text notes, and uses a transformer encoder for modality fusion.
---

# Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques

## Quick Facts
- **arXiv ID:** 2508.20460
- **Source URL:** https://arxiv.org/abs/2508.20460
- **Reference count:** 40
- **Primary result:** Deep learning framework using medical prompts and transformer fusion improves clinical predictions on MIMIC-III and PASA datasets

## Executive Summary
This paper introduces a deep learning framework that leverages multimodal electronic health records for predicting mortality and resource utilization in critical care. The method uses prompt learning with medical language templates to convert structured EHR data into natural language sentences, which are then embedded using a pre-trained sentence encoder and fused with free-text clinical notes via a transformer encoder. Evaluated on MIMIC-III and PASA datasets, the framework outperforms state-of-the-art baselines across three clinical tasks: mortality prediction, length of stay estimation, and surgical duration prediction. The approach also demonstrates strong resilience to structured data corruption, suggesting clinical applicability in real-world settings with imperfect data quality.

## Method Summary
The framework converts structured EHR cells into natural language sentences using medical templates, then embeds both these prompts and free-text notes using a frozen SimCSE RoBERTa encoder. A transformer encoder fuses the multimodal embeddings without positional encodings, and a feed-forward network makes predictions. The model processes data from MIMIC-III (first 24h of ICU) and PASA (preoperative) datasets, using class weights for mortality prediction. A two-step training approach pre-computes embeddings to manage GPU memory, with the sentence encoder frozen during training.

## Key Results
- Mortality prediction: 1.6% improvement in balanced accuracy and 0.8% in AUROC over baselines
- Length of stay prediction: 0.5% improvement in RMSE and 2.2% in MAE
- Surgical duration prediction: 10.9% improvement in RMSE and 11.0% in MAE
- Strong resilience to structured data corruption, especially at high corruption levels
- Ablation studies confirm contributions of medical prompts, free-texts, and pre-trained sentence encoder

## Why This Works (Mechanism)

### Mechanism 1: Semantic Enrichment via Prompting
Converting structured EHR cells into natural language sentences retrieves semantic context that raw numerical or categorical encodings lose. The model uses hard-coded medical templates (e.g., "The patient is [age] years old...") to transform cell values into text. A pre-trained RoBERTa-based encoder (SimCSE) then processes these sentences to generate context-aware embeddings, effectively treating tabular data analysis as a language task. The semantic relationships captured by pre-trained language models in general text are transferable to the specific, ontological relationships found in medical codes and values.

### Mechanism 2: Cross-Modal Attention Fusion
Fusing structured embeddings with unstructured free-text notes via a Transformer encoder allows the model to synthesize complementary signals. The architecture concatenates cell embeddings (from structured data) with embeddings from clinical notes. A standard Transformer encoder uses self-attention to weigh relationships between all features simultaneously, allowing a structured data point (e.g., low blood pressure) to attend to relevant context in a free-text note (e.g., "patient feels dizzy"). The [CLS] token in the Transformer can distill a unified "patient embedding" from a heterogeneous sequence of structured and unstructured tokens.

### Mechanism 3: Robustness via Textual Redundancy
Including free-text notes provides a "semantic safety net" when structured data is corrupted or missing. In real-world settings, structured EHR fields often suffer from entry errors. By relying on both modalities, the model can cross-reference data. If a structured field is corrupted, the attention mechanism can shift focus to the unstructured notes which often repeat or contextualize that same information. Clinical notes contain information redundant to the structured fields and are less susceptible to the specific "data corruption" noise injected in experiments.

## Foundational Learning

- **Prompt Engineering (Hard Prompts)**: Why needed here: To bridge the gap between discrete tabular data and continuous language models. Without prompts, the model cannot effectively "read" the structured variables. Quick check question: Can you distinguish between the prompt "The patient is male" and the raw value "1" in terms of semantic density?

- **Transformer Encoders (BERT-family)**: Why needed here: To perform the fusion. Unlike RNNs, the Transformer's self-attention allows the model to relate a lab result to a specific sentence in a nursing note regardless of distance. Quick check question: How does the [CLS] token function in a BERT-style model, and why is it used here as the "patient embedding"?

- **Multimodal Fusion**: Why needed here: To understand that the model is not just processing two datasets separately, but learning interactions between them (e.g., text explains a numerical outlier). Quick check question: Why would a model perform better by looking at both a "Heart Rate" column and a "Nursing Note" simultaneously rather than concatenating their predictions later?

## Architecture Onboarding

- **Component map:** Input (Structured EHR + Free-text Notes) -> Prompt Construction Module (Cell Value + Feature Name -> Natural Language Sentence) -> Encoder (Supervised SimCSE -> 768-dim embeddings) -> Fusion (Transformer Encoder -> Concatenated embeddings) -> Head (FFN -> Prediction)

- **Critical path:** The **Prompt Construction Module**. If the templates do not clearly describe the clinical context of the cell value (e.g., failing to specify units or normal ranges in the text), the pre-trained encoder cannot generate useful embeddings.

- **Design tradeoffs:** Hard vs. Soft Prompts: The paper uses hand-crafted "hard" prompts for interpretability and stability. The tradeoff is flexibility; "soft" prompts might learn better representations but are opaque. Freezing the Encoder: The SimCSE encoder is frozen to save memory and prevent overfitting. The tradeoff is that the language model cannot adapt its vocabulary to specific hospital jargon not present in its pre-training.

- **Failure signatures:** OOM (Out of Memory): Processing every cell as a 768-dim embedding sequence is memory-intensive. The paper uses a "two-step training scheme" (pre-compute embeddings) to mitigate this. Semantic Dilution: If prompts are too generic (e.g., "Value is 5"), the embeddings for different features become indistinguishable.

- **First 3 experiments:**
  1. Ablation on Prompts: Compare "The patient is 54 years old" vs. "Age: 54" vs. Raw Value "54" to validate the semantic gain.
  2. Modality Gap Analysis: Run the model on Structured-Only vs. Text-Only vs. Combined to quantify the information gain from notes.
  3. Corruption Injection: Manually flip 10% of structured labels in the test set to verify if the model "looks" at the text to correct the prediction.

## Open Questions the Paper Calls Out

1. Can the utilization of learnable soft prompts or diverse prompt designs improve predictive performance over the current hard-coded medical templates?
   - Basis: Authors state they may consider further exploration of prompt learning including soft prompts in future work.
   - Why unresolved: Current study relied exclusively on hand-crafted "hard" prompts.
   - What evidence would resolve it: Comparative ablation study evaluating model performance using continuous, learnable prompt embeddings versus fixed templates.

2. How can the framework be adapted to incorporate longitudinal temporal information to enhance prediction accuracy?
   - Basis: Authors note EHR data was acquired at a singular time instance, failing to capture longitudinal patient information.
   - Why unresolved: Current architecture processes static snapshots of patient data.
   - What evidence would resolve it: Extending model to handle sequential time-series inputs and evaluating performance improvements.

3. Does the framework maintain robustness and accuracy when deployed in diverse external clinical populations or healthcare systems?
   - Basis: Authors conclude further external studies on other populations should be performed to validate feasibility in clinical practice.
   - Why unresolved: Model tested only on MIMIC-III (US) and PASA (Singapore).
   - What evidence would resolve it: External validation studies using datasets from distinct geographic regions or hospital networks.

## Limitations
- Exact prompt templates for all features are not provided, creating a significant reproducibility gap
- Method requires substantial computational resources due to processing every cell as a 768-dimensional embedding
- Robustness claims based on synthetic corruption rather than real-world error patterns
- Model's performance when both structured data and corresponding notes are simultaneously corrupted remains untested

## Confidence
- **High confidence**: Performance improvements over baselines are statistically sound and well-documented through ablation studies
- **Medium confidence**: Cross-modal attention mechanism's effectiveness assumes clinical notes contain redundant information to structured fields
- **Low confidence**: Exact prompt templates for all features are not provided, making reproducibility verification impossible

## Next Checks
1. **Prompt Template Validation**: Systematically compare the performance impact of different prompt formulations (e.g., "Patient age: 54" vs "The patient is 54 years old" vs raw value "54") across multiple features to quantify the semantic gain from prompting.

2. **Real-World Corruption Testing**: Instead of synthetic noise injection, evaluate model performance when using actual MIMIC-III data with known quality issues (missing values, inconsistent formatting, contradictory entries) to validate the claimed robustness.

3. **Feature-Note Redundancy Analysis**: Quantitatively assess the overlap between information in structured fields and their corresponding free-text notes across the dataset to verify the cross-modal attention mechanism's foundational assumption.