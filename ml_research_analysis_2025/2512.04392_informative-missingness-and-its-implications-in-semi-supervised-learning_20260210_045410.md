---
ver: rpa2
title: Informative missingness and its implications in semi-supervised learning
arxiv_id: '2512.04392'
source_url: https://arxiv.org/abs/2512.04392
tags:
- labelled
- missingness
- data
- information
- mechanism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses semi-supervised learning (SSL) where labels
  are missing, not randomly, but informatively based on feature uncertainty or class
  membership. The core method uses a finite mixture model with an expectation-maximisation
  (EM) framework, jointly modelling both the data and the missingness mechanism.
---

# Informative missingness and its implications in semi-supervised learning

## Quick Facts
- arXiv ID: 2512.04392
- Source URL: https://arxiv.org/abs/2512.04392
- Reference count: 9
- Primary result: Informative missingness mechanisms (MAR/MNAR) can make partially labelled data more informative than fully labelled data for semi-supervised learning

## Executive Summary
This paper presents a theoretical framework showing that when labels are missing informatively in semi-supervised learning, the missingness mechanism itself can provide valuable information. By explicitly modelling the relationship between feature uncertainty or class membership and label availability using finite mixture models with EM, the framework demonstrates that partially labelled datasets can yield better classifiers than fully labelled ones under certain conditions. The work connects empirical SSL heuristics like entropy minimization to principled missingness mechanisms, providing both theoretical insight and a statistical framework for leveraging informative label absence.

## Method Summary
The method employs a finite mixture model with an expectation-maximization (EM) framework that jointly models both the data distribution and the missingness mechanism. For the data, class-conditional densities are combined using mixture weights, while the missingness mechanism is parameterized as either MAR (dependent on feature uncertainty via functions like entropy) or MNAR (dependent on class membership). The EM algorithm alternates between computing posterior class probabilities (E-step) and updating both mixture and missingness parameters (M-step). This joint likelihood approach allows the model to extract information from the pattern of missing labels themselves, not just from the observed labels.

## Key Results
- When missingness is informative, the missing-label indicators contribute additional Fisher information (I^(miss)_PC) that can exceed the information loss from missing labels
- Under MAR, entropy minimization in SSL implicitly approximates the missingness mechanism by targeting high-uncertainty regions
- The "paradox" that partially labelled data can outperform fully labelled data occurs when class overlap is moderate, labelled data are sparse, and the missingness mechanism is strongly informative
- A correctly specified missingness mechanism can reduce expected classification error below that of a fully supervised classifier

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modelling the label-missingness mechanism explicitly can yield greater effective information than the raw labelled data alone, potentially outperforming a fully supervised classifier.
- **Mechanism:** The total Fisher information ($I^{(full)}_{PC}$) is a sum of complete-data information ($I_{CC}$), a loss term from missing labels ($-\gamma I^{(clr)}_{CC}$), and a gain term from the missingness mechanism itself ($I^{(miss)}_{PC}$). When the missingness is informative (dependent on features or labels), $I^{(miss)}_{PC} > 0$ can exceed the loss term.
- **Core assumption:** The "paradox" (partially labelled beating fully labelled) holds primarily when class overlap is moderate, labelled data are sparse, and the missingness mechanism is strongly informative and correctly specified.
- **Evidence anchors:**
  - [abstract] "...information gained from modelling the missing-label mechanism can even outweigh the loss due to missing labels..."
  - [section: Page 3] "...producing the apparent paradox that partially labelled data analysed under a correctly specified mechanism may be more informative than completely labelled data..."
  - [corpus] The neighbor paper "Semi-Supervised Learning under General Causal Models" supports the theoretical basis for why unlabelled data helps under specific structural assumptions, though citation volume for the primary paper is currently low.
- **Break condition:** If the missingness is completely random (MCAR), $I^{(miss)}_{PC} = 0$, and the information gain vanishes, reducing the problem to standard SSL with linear information loss.

### Mechanism 2
- **Claim:** Common empirical SSL heuristics, specifically entropy minimization and consistency regularization, function as implicit approximations of Missing At Random (MAR) mechanisms.
- **Mechanism:** Under MAR, the probability of a label being missing depends on feature uncertainty (e.g., proximity to decision boundaries). Entropy minimization forces the model to reduce uncertainty in these specific unlabelled regions, effectively inverting the missingness model to sharpen decision boundaries.
- **Core assumption:** The decision boundary lies in low-density regions (low-density separation assumption), and unlabelled data points with high predictive entropy are the specific targets where label absence is informative.
- **Evidence anchors:**
  - [abstract] "Empirical SSL methods like entropy minimisation and consistency regularisation can be interpreted as heuristic realisations of MAR-type mechanisms."
  - [section: Page 3] "Minimum-entropy regularisation corresponds to an MAR mechanism, where unlabelled data concentrate near ambiguous regions."
  - [corpus] "Semi-Supervised Learning under General Causal Models" discusses structural conditions for SSL, aligning with the view that assumptions about data structure (like MAR) are prerequisites for unlabelled data utility.
- **Break condition:** If the data distribution violates the low-density separation assumption (e.g., classes are highly intermingled in high density), entropy minimization will reinforce incorrect biases.

### Mechanism 3
- **Claim:** A finite mixture model with a joint likelihood for data and missingness provides a statistically coherent framework for estimating classifiers where labels are absent based on class membership (MNAR).
- **Mechanism:** The observed-data log-likelihood is extended to include $Pr(M|Y,Z)$. In the E-step, the algorithm computes posterior class probabilities ($\tau_{ij}$) weighted not just by feature likelihood but by the probability that a label of that class would be missing ($\phi_i$).
- **Core assumption:** The true class-conditional missingness probabilities ($\phi_i$) can be parameterized and identified (e.g., via a logistic function of discriminant terms) within the Expectation-Maximisation (EM) framework.
- **Evidence anchors:**
  - [abstract] "The core method uses a finite mixture model with an expectation-maximisation (EM) framework, jointly modelling both the data and the missingness mechanism."
  - [section: Page 2] "Under missing not at random (MNAR), the probability of being unlabelled also depends on the true class label..."
  - [corpus] "SSLfmm: An R Package..." confirms the existence of software implementations for this specific mixture-model approach, validating the computational feasibility.
- **Break condition:** If the mechanism parameterization is misspecified (e.g., assuming MAR when the true process is MNAR), the bias introduced into the posterior estimates ($\tau_{ij}$) may degrade performance below that of ignoring the unlabelled data entirely.

## Foundational Learning

- **Concept:** **Rubin’s Missing Data Taxonomy (MCAR, MAR, MNAR)**
  - **Why needed here:** The paper’s central thesis relies on distinguishing *why* labels are missing. Without understanding the difference between "Missing Completely At Random" (no info in missingness) and "Missing Not At Random" (info in missingness), the proposed information gain cannot be conceptualized.
  - **Quick check question:** If annotators skip labeling images that are "too blurry," is this MCAR, MAR, or MNAR?

- **Concept:** **Fisher Information**
  - **Why needed here:** The paper quantifies the "value" of the missingness mechanism using Fisher Information ($I^{(miss)}_{PC}$). This metric provides the theoretical proof that a partially labelled dataset can theoretically contain more "learnable" signal than a fully labelled one.
  - **Quick check question:** Does a higher Fisher Information value indicate lower or higher variance in the estimator?

- **Concept:** **Finite Mixture Models**
  - **Why needed here:** This is the architectural backbone. The paper models the feature distribution $f(y; \theta)$ as a sum of class-conditional densities. This assumption allows the EM algorithm to probabilistically assign labels to unlabelled data.
  - **Quick check question:** In a mixture model, how do we determine the probability that a specific unlabelled data point belongs to Class A vs. Class B?

## Architecture Onboarding

- **Component map:**
  1.  **Input Layer:** Features ($Y$), observed Labels ($Z_{obs}$), and Missingness Indicators ($M$).
  2.  **Mixture Model Core:** Parameterizes class densities ($\pi_i, f_i$) to model feature distribution.
  3.  **Missingness Module:** A sub-model (e.g., logistic regression) that parameterizes $Pr(M=1|Y, Z)$.
  4.  **Joint Likelihood Engine:** Combines the Mixture and Missingness modules into a single objective function (Eq. 1 extended).
  5.  **EM Optimizer:** Iteratively updates class posteriors ($\tau_{ij}$) and model parameters ($\theta, \xi$).

- **Critical path:** Correctly specifying the **Missingness Module**. The performance boost depends entirely on whether the model of $Pr(M|Y,Z)$ accurately reflects reality (e.g., encoding that "uncertain labels are more likely to be missing" via an entropy term).

- **Design tradeoffs:**
  - **Interpretability vs. Accuracy:** The proposed mixture model is highly interpretable but may lack the representation power of deep neural networks for raw images/audio (though deep variants are suggested as future work).
  - **Sensitivity vs. Robustness:** Modeling MNAR is high-reward but high-risk; a wrong assumption about class-dependent missingness introduces systematic bias that is hard to detect.

- **Failure signatures:**
  - **Degenerate Priors:** If the initial mixture components do not align with the true clusters, the EM algorithm may converge to a local minimum where the missingness model fits noise.
  - **MNAR-MAR Confusion:** If the system assumes MAR (features only) but the data is MNAR (labels missing based on class), the model will likely misestimate class priors ($\pi_i$), leading to systematic miscalibration.

- **First 3 experiments:**
  1.  **Mechanism Verification (Simulation):** Generate synthetic data with known ground-truth MAR (entropy-based missingness). Compare a "Naive SSL" (ignores missingness mechanism) vs. the "Proposed Joint Model" to verify the $I^{(miss)}_{PC}$ gain exists.
  2.  **Ablation on Label Scarcity:** Fix the missingness mechanism to a strong informative setting. Vary the labelled fraction ($\gamma$) from 1% to 50% to identify the "sweet spot" where the proposed method beats the Fully Supervised baseline (the paradox region).
  3.  **Robustness Check (Real Data):** Apply the model to a dataset with suspected "hard-to-annotate" examples (e.g., medical imaging with ambiguous diagnoses). Fit both MAR and MNAR variants to see which missingness assumption better aligns with empirical annotation patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can general information-theoretic bounds be established to quantify how missingness-dependent uncertainty contributes to Fisher information and generalisation performance?
- Basis in paper: [explicit] The authors explicitly state that future work should "establish general information-theoretic bounds that quantify how missingness-dependent uncertainty contributes to Fisher information and generalisation performance."
- Why unresolved: The paper currently provides a schematic decomposition of information but lacks formal bounds on the magnitude of this contribution or theoretical guarantees on generalisation.
- What evidence would resolve it: Derivation of theoretical inequalities that link the specific parameters of the missingness mechanism (MAR/MNAR) to generalisation error bounds.

### Open Question 2
- Question: How sensitive is the proposed likelihood-based SSL framework to the misspecification of the missingness mechanism?
- Basis in paper: [explicit] The concluding remarks note that "sensitivity to model misspecification and robustness within the Godambe information bound merit future investigation."
- Why unresolved: The paper demonstrates that a correctly specified mechanism yields lower expected error, but it does not analyze the performance degradation when the assumed missingness model (e.g., logistic entropy) is incorrect.
- What evidence would resolve it: Empirical studies or theoretical analyses showing the bias and variance introduced by fitting an MAR model to data generated via MNAR, or vice-versa.

### Open Question 3
- Question: Can hybrid deep learning algorithms be developed that explicitly learn the missingness mechanism $P(M|Y)$ alongside the classification task?
- Basis in paper: [explicit] The paper suggests "developing hybrid algorithms that learn or approximate the missingness mechanism, for example, entropy-driven weighting schemes or neural architectures that jointly estimate $Pr(M|Y)$ alongside deep neural network parameters."
- Why unresolved: Current empirical SSL methods serve only as heuristic approximations of these mechanisms; a direct implementation that jointly estimates the missingness probability within a deep architecture has not been realized.
- What evidence would resolve it: A deep SSL architecture incorporating a trainable missingness-head that converges to the true label-absence distribution and improves classification accuracy over standard heuristics.

## Limitations
- The paper assumes known or correctly specifiable missingness mechanisms, but in practice these are rarely known a priori and misspecification could lead to degraded performance or bias.
- Empirical validation is limited to synthetic data and toy problems; real-world label missingness patterns (e.g., based on annotator fatigue or item difficulty) may not align with the assumed MAR/MNAR forms.
- Computational scalability to high-dimensional or large-scale data is not addressed; EM with mixture models can struggle with complex feature spaces.

## Confidence
- **High**: The theoretical derivation of information gain from informative missingness (I^(miss)_PC term) is mathematically sound under the stated assumptions.
- **Medium**: The claim that entropy minimization approximates MAR mechanisms is well-grounded but depends on the low-density separation assumption holding in practice.
- **Low**: The assertion that partially labelled data can *always* beat fully labelled data is conditional and may not generalize beyond controlled synthetic settings.

## Next Checks
1. **Mechanism misspecification test**: Fit both MAR and MNAR variants to real-world data with known annotation biases (e.g., medical imaging with "difficult case" skipping) and compare performance to fully supervised baseline.
2. **Cross-domain robustness**: Apply the model to domains with varying degrees of class overlap and label scarcity (e.g., text classification with class imbalance) to verify the "sweet spot" conditions empirically.
3. **Scalable implementation**: Develop a scalable version using deep generative models (e.g., VAEs with missingness heads) and test on high-dimensional benchmarks like ImageNet-10% to assess practical viability.