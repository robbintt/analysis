---
ver: rpa2
title: Linguistic traces of stochastic empathy in language models
arxiv_id: '2410.01675'
source_url: https://arxiv.org/abs/2410.01675
tags:
- human
- empathy
- humans
- humanness
- condition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how large language models (LLMs) and humans
  differ in conveying humanness, particularly when empathy is required. Through five
  experiments with over 2,600 participants, researchers compared human-written and
  LLM-generated relationship advice under various conditions, including instructions
  to appear human and avoid LLM-style writing.
---

# Linguistic traces of stochastic empathy in language models

## Quick Facts
- arXiv ID: 2410.01675
- Source URL: https://arxiv.org/abs/2410.01675
- Reference count: 0
- One-line primary result: LLMs can mimic humanness through stylistic shifts without genuine empathy, responding effectively to adversarial instructions while humans cannot.

## Executive Summary
This study examines how large language models (LLMs) and humans differ in conveying humanness, particularly when empathy is required. Through five experiments with over 2,600 participants, researchers compared human-written and LLM-generated relationship advice under various conditions, including instructions to appear human and avoid LLM-style writing. The key finding is that while human-written texts were consistently rated as more human, only LLMs effectively responded to instructions to increase humanness perception (+39% to +41%), while humans showed no significant change. Computational analysis revealed that LLMs achieved this by adopting informal language, self-references, and present-focused writing rather than through empathic language. The study demonstrates that LLMs possess an implicit representation of humanness that they can deploy strategically, creating "stochastic empathy" that mimics empathic writing without genuine understanding.

## Method Summary
The study employed a 2×2×2 experimental design comparing human and LLM text generation across naive vs. adversarial conditions and relationship advice vs. description tasks. GPT-4 was used to generate texts via API with randomized temperature settings. Human participants were recruited from Prolific to write texts or evaluate source authenticity on a 5-point scale. Each text received 3.1-3.4 independent ratings that were aggregated for analysis. The study included computational text analysis using LIWC-2022, n-gram differentiation, empathic concern dictionaries, and spelling/vocabulary frequency measures. Five studies systematically varied conditions to isolate mechanisms of humanness perception.

## Key Results
- LLM-generated texts showed a 39-41% increase in perceived humanness when given adversarial instructions, while human texts showed no significant change
- Only humans were perceived as more human than LLMs when tasks required empathy (d=1.01 for advice vs. d=0.46 for description)
- Computational analysis revealed LLM stylistic shifts to informal language, self-references, and present-focused writing rather than empathic language
- Empathy usage instructions increased empathic phrasing but decreased humanness perception in LLMs (d=-0.50)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs increase perceived humanness through stylistic mimicry, not empathic language.
- Mechanism: When prompted to "sound human," the model accesses learned statistical associations between informal linguistic features and human-like text, shifting output toward conversational markers, self-references, simpler vocabulary, and present-tense focus.
- Core assumption: The model has internalized correlations between these features and human-authored text from training data.
- Evidence anchors:
  - [abstract] "LLMs mimicked humanness by adopting informal, conversational language and self-references rather than empathic language."
  - [section] Study 5 computational analysis: LLM shifted to "netspeak" (lol, haha), first person singular (+d=1.31), simpler vocabulary (rank frequency dropped from 1216 to 1015), reduced analytic style (d=-1.30).
  - [corpus] Related work on anthropomorphism in language technologies (FMR=0.54) supports that linguistic patterns drive perceived humanness, but corpus lacks direct replication of this specific stylistic mechanism.
- Break condition: If the model's training distribution lacks sufficient informal human-authored text (e.g., formal corpora only), stylistic shifts may fail or produce uncanny patterns.

### Mechanism 2
- Claim: Empathic language production is orthogonal to humanness perception in LLMs.
- Mechanism: Empathy instructions activate different learned patterns than humanness instructions. Empathy prompts increase cognitively/emotionally empathic phrasing (validated by human raters), but these patterns correlate with lower perceived humanness.
- Core assumption: Humanness heuristics in readers rely more on surface informality than empathic depth.
- Evidence anchors:
  - [abstract] "LLMs can produce empathy without humanness and humanness without empathy."
  - [section] Study 4: Empathy usage instructions increased all three empathy dimensions (cognitive d=0.90, emotional d=0.64, motivational d=0.29) but decreased humanness perception (d=-0.50).
  - [corpus] Empathy benchmarks (AEQ-Bench, FMR=0.57) confirm LLMs can generate empathic content, but no corpus evidence links empathy scores to humanness perception.
- Break condition: If users develop better calibrated heuristics (e.g., associating empathy depth with humans), the orthogonality may weaken.

### Mechanism 3
- Claim: Task empathy requirements moderate human-LLM distinguishability.
- Mechanism: High-empathy tasks (relationship advice) force reliance on nuanced understanding that exposes LLM limitations; low-empathy tasks (descriptions) allow LLMs to perform near-human levels with default style.
- Core assumption: LLMs lack genuine experiential basis for high-empathy contexts, creating detectable gaps in authenticity.
- Evidence anchors:
  - [abstract] "Humans were perceived as more human than LLMs, especially when tasks required empathy."
  - [section] Study 2: Human advantage was twice as large for advice (d=1.01) versus description (d=0.46); description task AUC=0.60 (near chance).
  - [corpus] Clinical empathy modeling work (FMR=0.48) suggests empathy detection in health contexts is tractable, but corpus lacks comparative task-modality data.
- Break condition: If LLMs are fine-tuned on domain-specific empathic corpora (e.g., counseling dialogues), the empathy-gap could narrow.

## Foundational Learning

- Concept: Stochastic empathy
  - Why needed here: Understanding that LLMs produce statistically plausible empathic language without genuine understanding or interpersonal consequences is critical for interpreting Study 4 results.
  - Quick check question: If an LLM produces highly rated empathic advice but cannot experience the advisee's situation, what is the functional limitation of this "empathy"?

- Concept: Human heuristics for AI detection
  - Why needed here: The paper shows humans use flawed cues (associating simple vocabulary with humanness, failing to exploit spelling-mistake signal). Understanding these heuristics explains why adversarial instructions succeed for LLMs.
  - Quick check question: Why does the correlation between vocabulary frequency and humanness judgment drop from ρ=-0.56 (naive) to ρ=-0.27 (adversarial)?

- Concept: Ceiling effects vs. practical limits
  - Why needed here: Humans could not increase humanness perception even with explicit instructions, not because they hit scale maximum, but because "being human" is not a manipulable meta-cognitive strategy.
  - Quick check question: If human texts averaged 3.46/5.00 on humanness, why is this not evidence of a traditional ceiling effect?

## Architecture Onboarding

- Component map:
  - Text generation pipeline (GPT-4/GPT-4o via API with randomized temperature 0.0-1.0)
  - Prompt engineering layer (naive vs. adversarial vs. evasion vs. empathy-usage conditions)
  - Human evaluation layer (5-point source judgment scale, 3.1-3.4 raters per text)
  - Computational text analysis (n-gram differentiation, LIWC-2022, empathic concern dictionary, spelling/vocabulary frequency)

- Critical path:
  1. Define experimental condition → construct prompt
  2. Generate text (human via Prolific, LLM via API)
  3. Collect human judgments (source + empathy dimensions)
  4. Aggregate judgments (mean per text)
  5. Run ANOVA (Source × Condition × Task) + AUC diagnosticity analysis
  6. Optional: computational analysis of linguistic features

- Design tradeoffs:
  - Random temperature sampling increases output diversity but introduces uncontrolled variance (see SM13 discussion of no clear temperature guidelines).
  - Aggregating multiple human judgments per text increases reliability but masks rater-level heterogeneity (multilevel models in SM14 address this).
  - Using GPT-4 only limits generalization; GPT-4o comparison in Study 3 showed no difference, but other models untested.

- Failure signatures:
  - LLM produces formal, structured text with "Dear friend" greetings and analytic style → naive condition, low humanness scores.
  - Empathy instructions produce longer, more complex empathic phrasing but lower humanness ratings → misaligned strategy.
  - Human adversarial texts show no linguistic shifts → instruction ineffective for humans.

- First 3 experiments:
  1. Replicate Study 1 minimal design: Generate 100 texts each (human vs. LLM, naive vs. adversarial) on relationship advice; verify LLM-only response to adversarial instruction.
  2. Ablate stylistic features: Force LLM to maintain formal style in adversarial condition; predict humanness scores will not increase.
  3. Cross-task validation: Apply adversarial prompt to technical writing task (low empathy); predict smaller humanness increase than advice task.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's findings are limited to GPT-4 models, with unknown generalizability to other LLMs or future model versions
- Temperature sampling variability introduces uncontrolled noise that could affect reproducibility
- Human evaluation design aggregates multiple judgments per text, potentially masking individual rater biases
- Computational analysis relies on LIWC-2022 and custom dictionaries that may not capture all relevant stylistic dimensions

## Confidence

- High confidence: LLMs respond to adversarial instructions to appear more human (+39-41% increase in perceived humanness) while humans do not
- Medium confidence: Stylistic mimicry (informal language, self-references, present-focus) is the primary mechanism for LLM humanness manipulation
- Medium confidence: Empathic language production is orthogonal to humanness perception in LLMs
- Low confidence: Task empathy requirements consistently moderate human-LLM distinguishability across all contexts

## Next Checks

1. Replicate the adversarial instruction effect with controlled temperature settings (fixed values) to isolate instruction impact from generation variability
2. Test whether fine-tuning LLMs on informal human-authored text amplifies the stylistic mimicry response to humanness instructions
3. Conduct a between-subjects study where evaluators are explicitly trained to detect empathic depth versus surface informality as humanness cues