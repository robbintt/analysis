---
ver: rpa2
title: 'PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a
  Coevolutionary Multimodal Multi-Agent System'
arxiv_id: '2509.24855'
source_url: https://arxiv.org/abs/2509.24855
tags:
- solution
- physics
- studio
- reasoning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PhysicsMinions addresses the challenge of solving complex physics
  Olympiad problems that require multimodal understanding and deep reasoning. It introduces
  a coevolutionary multi-agent system with three specialized studios: Visual Studio
  for extracting structured visual information, Logic Studio for generating and refining
  solutions, and Review Studio for dual-stage verification.'
---

# PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a Coevolutionary Multimodal Multi-Agent System

## Quick Facts
- arXiv ID: 2509.24855
- Source URL: https://arxiv.org/abs/2509.24855
- Reference count: 40
- Primary result: First open-source system to win gold medals in latest physics Olympiads, achieving Pass@32 score of 26.8/30

## Executive Summary
PhysicsMinions introduces a coevolutionary multi-agent system that achieves historic breakthroughs in solving complex physics Olympiad problems requiring multimodal understanding. The system employs three specialized studios - Visual, Logic, and Review - that iteratively refine solutions through feedback loops until meeting consecutive verification criteria. Evaluated on 7 latest physics Olympiads, it achieved the first open-source gold medals, including the latest IPhO, with a Pass@32 score of 26.8/30 that ranked 4th among 406 contestants and surpassed 99% of human participants. The system demonstrates consistent improvements across both open- and closed-source models, all modalities, and all physics fields.

## Method Summary
PhysicsMinions is a coevolutionary multi-agent system with three specialized studios working iteratively. Visual Studio converts raw image inputs into validated JSON descriptions through Inspector→Introspector→Verifier cycles. Logic Studio generates and refines solutions using structured reasoning formats (Summary + Detailed Solution). Review Studio performs dual-stage verification with Physics-Verifier (domain-specific checks) followed by General-Verifier (logical/flow checks). The system iterates until achieving CV=2 consecutive successful verifications. Base models (Intern-S1 or Gemini-2.5-Flash-Thinking) are used with temperature=0.6 and maximum 5 iterations per problem. Evaluation uses HiPhO benchmark spanning 7 latest physics Olympiads with official marking schemes.

## Key Results
- First open-source system to win gold medals in physics Olympiads, including the latest IPhO
- Achieved Pass@32 score of 26.8/30, ranking 4th among 406 contestants and surpassing 99% of human participants
- Advanced open-source models from 1-2 to 6 gold medals across Olympiads through systematic improvement
- Consistently improved both open- and closed-source models across all modalities and physics fields

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured visual-to-symbolic conversion improves multimodal reasoning accuracy
- Mechanism: Visual Studio transforms raw image inputs into validated JSON descriptions through iterative Inspector→Introspector→Verifier cycles before passing to logic reasoning. This explicit symbolic representation reduces ambiguity in downstream physics problem-solving.
- Core assumption: Physics diagrams contain structured information (axes, curves, forces) that maps reliably to symbolic form; models process explicit JSON better than raw pixels for quantitative reasoning.
- Evidence anchors:
  - [abstract] "Visual Studio to interpret diagrams"
  - [section 3.2] "Visual Studio converts multimodal inputs into structured JSON through iterative observation, introspection, and verification"
  - [corpus] HiPhO benchmark paper establishes multimodal physics problems require both visual and symbolic reasoning
- Break condition: When diagrams are highly ambiguous, non-standard, or contain information that cannot be captured in structured JSON (e.g., complex artistic renderings without clear quantitative elements)

### Mechanism 2
- Claim: Dual-stage verification catches orthogonal error classes that single-stage checking misses
- Mechanism: Physics-Verifier screens domain-specific errors (units, constants, dimensional consistency) before General-Verifier checks logical flow and calculations. Errors caught at Stage 1 trigger immediate refinement, preventing wasted computation on fundamentally unsound solutions.
- Core assumption: Physics errors and general reasoning errors are sufficiently independent that staged checking provides better coverage than unified verification; verifiers can detect errors without fully solving problems.
- Evidence anchors:
  - [section 3.4] "dual-stage verification with a Physics-Verifier followed by a General-Verifier"
  - [Fig. 5(b)] Ablation shows Physics+General combination achieves 20.8 vs. 19.1/19.3 for individual verifiers
  - [corpus] Weak corpus support—no directly comparable multi-stage verification systems in physics reasoning
- Break condition: When errors span both domains simultaneously (e.g., a unit mistake that cascades into a logical error), or when verifiers lack sufficient domain knowledge to detect subtle physics inconsistencies

### Mechanism 3
- Claim: Consecutive verification threshold balances solution quality against over-correction
- Mechanism: Requiring CV=2 consecutive successful verifications before accepting a solution prevents premature acceptance while avoiding excessive refinement cycles that can degrade performance through "overthinking."
- Core assumption: Stable verification passes indicate convergence toward ground truth; alternating pass/fail patterns signal ongoing improvement potential rather than instability.
- Evidence anchors:
  - [section 4.5] "CV=2 as an empirically efficient setting... larger values may trigger overthinking and reduce scores"
  - [Fig. 5(c)] Shows peak performance at CV=2 (20.8) with decline at higher values
  - [corpus] Wisdom of Crowd paper discusses coevolutionary feedback mechanisms broadly
- Break condition: When problem difficulty varies significantly across questions, fixed CV may be suboptimal; easy problems may need fewer checks while hard problems may benefit from more iterations

## Foundational Learning

- Concept: **Multi-agent verification-refinement loops**
  - Why needed here: PhysicsMinions relies on iterative cycles where agents critique and improve each other's outputs. Understanding how feedback flows between agents and how to design effective critique prompts is essential.
  - Quick check question: Can you sketch a feedback loop where Agent A generates, Agent B verifies, and failures trigger regeneration? What information must flow back to A?

- Concept: **Structured reasoning formats (Summary + Detailed Solution)**
  - Why needed here: Logic Studio enforces explicit structure that makes errors traceable and enables targeted bug reports. This differs from free-form chain-of-thought.
  - Quick check question: Given a physics solution with a unit error in step 3, how does structured format help a verifier pinpoint and report the error versus unstructured prose?

- Concept: **Convergence criteria in iterative systems**
  - Why needed here: The CV hyperparameter determines when to accept solutions. Understanding tradeoffs between early stopping (accepting potentially flawed solutions) and over-iteration (computational cost, possible degradation) is critical.
  - Quick check question: A solution alternates between passing and failing verification. Should you accept after 1 pass, or require consecutive passes? What could cause performance degradation from too many iterations?

## Architecture Onboarding

- Component map:
  - **Visual Studio**: Inspector (initial extraction) → Introspector (refinement) → Verifier (validation) → Output: Structured JSON
  - **Logic Studio**: Solver (initial solution) → Introspector (self-improve) ← Bug reports from Review Studio → Output: Refined solution
  - **Review Studio**: Physics-Verifier (domain checks) → [if pass] → General-Verifier (logic/calc checks) → [if pass] → Accept or [if fail] → Bug report to Logic Studio

- Critical path:
  1. Image input → Visual Studio (CV iterations) → Structured JSON
  2. JSON + problem text → Logic Studio → Initial solution → Self-improve
  3. Solution → Review Studio dual verification
  4. Bug report (if fail) → Logic Studio Introspector refinement → Back to step 3
  5. CV consecutive passes → Final solution accepted

- Design tradeoffs:
  - **CV value**: Higher CV increases quality ceiling but raises token costs (~3.2× at CV=5 vs. CV=2) and risks overthinking
  - **Verifier specialization**: Separate verifiers catch more error types but add complexity and latency vs. single unified verifier
  - **Visual Studio precision**: Structured JSON improves reasoning but current extraction tools struggle with precise data (Fig. 7 limitation); raw images are faster but less reliable

- Failure signatures:
  - Oscillating verification (alternating pass/fail) without convergence suggests verifier-prompts need tuning or base model lacks capability
  - Consistent Physics-Verifier failures indicate domain knowledge gaps in model
  - Visual Studio extracting wrong values (Fig. 7) propagates errors through entire pipeline
  - Token budget exhaustion before CV threshold reached on complex problems

- First 3 experiments:
  1. **Ablation by component**: Run on IPhO subset with (a) raw images vs. Visual Studio JSON, (b) single vs. dual verification, (c) CV=1,2,3 to reproduce Fig. 5 patterns on your infrastructure
  2. **Failure mode analysis**: Collect solutions that fail CV=3+ iterations; categorize error types (unit errors, logical flaws, calculation mistakes) to identify which verifier stage catches each
  3. **Model scaling test**: Apply same PhysicsMinions pipeline to models of different sizes (e.g., 7B, 32B, 70B) to verify claim that "base model determines ceiling" and document minimum viable model size for gold-medal performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated visual extraction in the Visual Studio be enhanced to achieve high-precision data retrieval (e.g., exact peak coordinates) comparable to human-in-the-loop tools like WebPlotDigitizer?
- Basis in paper: [explicit] Section 5.2 identifies precise data extraction as a limitation, noting the system sometimes misinterprets chart details that manual tools handle correctly.
- Why unresolved: Current chart analysis tools struggle with fine-grained recognition, and fully automated systems currently lag behind human-assisted extraction methods in accuracy.
- What evidence would resolve it: A comparative study showing the Visual Studio matching the extraction accuracy of WebPlotDigitizer on complex scientific plots without manual calibration.

### Open Question 2
- Question: Can the coevolutionary multi-agent framework (Visual, Logic, Review studios) generalize effectively to other Olympiad-level domains, such as biology or chemistry, which possess different multimodal reasoning requirements?
- Basis in paper: [explicit] The Conclusion states the potential to extend the coevolutionary paradigm to other disciplines, though the current study focuses solely on physics.
- Why unresolved: The agents and verification stages are tuned for physical constants and diagrams; it is unclear if the same architecture handles the symbolic reasoning or visual modalities (e.g., molecular structures) of other sciences.
- What evidence would resolve it: Successful application of the PhysicsMinions architecture to benchmarks like the International Biology or Chemistry Olympiad, achieving comparable medal-tier performance.

### Open Question 3
- Question: How does the integration of external symbolic solvers or domain-specific verifiers into the Logic Studio impact the convergence speed and accuracy of the coevolutionary loop?
- Basis in paper: [explicit] The Conclusion lists "expanding tool use with external solvers and domain-specific verifiers" as a specific direction for future work.
- Why unresolved: The current system relies on the internal reasoning capabilities of MLLMs; the utility of external tools in reducing the "introspector" workload or lowering the Pass@1 iteration count remains untested.
- What evidence would resolve it: Ablation experiments showing a reduction in the number of coevolutionary iterations or an increase in single-attempt accuracy when external solvers are enabled.

## Limitations

- Proprietary HiPhO benchmark prevents independent verification of reported 26.8/30 Pass@32 score and gold medal achievements
- Visual Studio's structured JSON extraction remains a weakness, achieving only 2/3 correct peak identifications on complex IPhO problems
- Coevolutionary iteration mechanism lacks detailed implementation specifications for API orchestration and agent state management

## Confidence

- **High confidence**: The core multi-agent architecture design (Visual Studio → Logic Studio → Review Studio with dual verification) is well-specified and theoretically sound. The claim that base model capability sets an upper bound on system performance is empirically supported.
- **Medium confidence**: The reported Pass@32 score of 26.8/30 and ranking within the top 1% of human contestants is based on published data but cannot be independently verified due to proprietary benchmark access. The ablation studies showing CV=2 optimization appear internally consistent but may not generalize across all problem types.
- **Low confidence**: The absolute claims about being "the first" to achieve specific milestones (open-source gold medals, solving latest IPhO) and the exact numerical improvements over single-model baselines are difficult to verify without access to the full competitive landscape and benchmark details.

## Next Checks

1. **Component Ablation on Public Data**: Recreate the three-studio architecture using publicly available physics problems (e.g., past IPhO problems with published solutions) to verify that Visual Studio JSON extraction improves performance over raw image input, and that dual verification provides measurable benefits over single verification stages.

2. **Visual Studio Precision Analysis**: Systematically evaluate Visual Studio's structured extraction accuracy across different diagram types (plots, schematics, circuits) using a standardized dataset of physics diagrams with ground truth measurements. Document error rates and identify specific failure modes to validate the paper's claim about current limitations.

3. **Model Scaling Validation**: Apply the PhysicsMinions pipeline to at least three different base model sizes (e.g., 7B, 32B, 70B) on a consistent problem set to empirically verify the claim that "base model determines ceiling" and identify the minimum viable model size required for gold-medal performance.