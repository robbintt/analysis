---
ver: rpa2
title: Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives
arxiv_id: '2505.15693'
source_url: https://arxiv.org/abs/2505.15693
tags:
- reward
- learning
- average
- policy
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of synthesizing policies for\
  \ continuing reinforcement learning tasks where objectives are expressed as \u03C9\
  -regular specifications. The authors introduce a novel model-free approach that\
  \ translates absolute liveness specifications into average-reward objectives, enabling\
  \ the use of standard average-reward RL algorithms without requiring episodic resets."
---

# Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives

## Quick Facts
- arXiv ID: 2505.15693
- Source URL: https://arxiv.org/abs/2505.15693
- Reference count: 13
- Primary result: Novel average-reward RL framework for continuing tasks with ω-regular specifications, outperforming discounted RL baselines

## Executive Summary
This paper addresses the challenge of synthesizing policies for continuing reinforcement learning tasks where objectives are expressed as ω-regular specifications. The authors introduce a novel model-free approach that translates absolute liveness specifications into average-reward objectives, enabling the use of standard average-reward RL algorithms without requiring episodic resets. By constructing reward machines that preserve the communicating property of the underlying MDP, they guarantee convergence to optimal policies that maximize both satisfaction probability and mean-payoff objectives. Experimental results show their method outperforms existing discounted RL approaches in continuing settings while remaining competitive in episodic settings. The framework also supports lexicographic multi-objective optimization, achieving near-optimal average rewards while satisfying absolute liveness specifications.

## Method Summary
The approach constructs a reward machine by adding ε-reset transitions with negative reward from every automaton state back to the initial state, ensuring the product MDP is communicating. The framework supports two modes: (1) specification satisfaction by setting the reset penalty sufficiently negative to force visiting accepting transitions infinitely often, and (2) lexicographic multi-objective optimization using a probabilistic switching mechanism between payoff collection and specification verification modes. The method employs Differential Q-learning on the product MDP, updating Q-values using the average-reward TD error without requiring explicit episodic resets.

## Key Results
- Average-reward RL framework achieves 100% specification satisfaction on continuing tasks where episodic discounted RL baselines fail
- Lexicographic optimization achieves near-optimal mean-payoff (within ε) while guaranteeing specification satisfaction
- Outperforms discounted RL baselines on 6 of 8 benchmarks in continuing settings
- Product MDP sizes range from 8 to 15,426 states with training steps of 10K-12M depending on benchmark

## Why This Works (Mechanism)

### Mechanism 1: Preservation of Communication via Hard Resets
- **Claim:** The framework enables model-free convergence in continuing environments by constructing a reward machine that artificially forces the product MDP to be communicating, satisfying a core requirement of average-reward algorithms like Differential Q-learning.
- **Mechanism:** A "reset" transition (ε-action) with a negative reward (c) is added from every state in the automaton back to the initial state. This ensures that any state (s, q) in the product MDP can reach any other state (s', q') by first navigating to s_0 in the environment and then resetting the automaton to q_0.
- **Core assumption:** The underlying environment MDP is communicating.
- **Evidence anchors:**
  - [Section 4] "Lemma 4.3 (Preservation of Communication)... by adding the reset (ε) action from every state of R to its initial state, the graph structure of M is strongly connected."
  - [Abstract] "translates the specification into an average-reward objective while preserving the communicating property."
  - [Corpus] Evidence in corpus (e.g., *Implicit Updates for Average-Reward TD Learning*) highlights general algorithmic sensitivity to hyperparameters in average-reward settings, underscoring the necessity of structural guarantees like the communicating property for stability.
- **Break condition:** If the underlying MDP has transient states that cannot be revisited, the "communicating" assumption fails, and convergence is not guaranteed.

### Mechanism 2: Penalty-Tuned Specification Satisfaction
- **Claim:** Satisfaction of an absolute liveness specification is achieved by setting the reset penalty (c) sufficiently negative, ensuring that an agent maximizing average reward prefers visiting accepting transitions infinitely often over indefinitely resetting.
- **Mechanism:** The reward function assigns 1 to accepting automaton edges and 0 to others. The reset action yields reward c. If c < c* (a specific negative threshold), the optimal average-reward policy must visit accepting transitions infinitely often to offset the penalty of potentially infinite resets, thereby satisfying the liveness specification.
- **Core assumption:** The specification is an "absolute liveness" property (prefix-independent).
- **Evidence anchors:**
  - [Section 4] "Lemma 4.4 (Average and Probability)... there exists a constant c* < 0 such that for all c < c*, positional strategies that maximize the average reward... will maximize the satisfaction probability."
  - [Abstract] "synthesizing policies that satisfy absolute liveness ω-regular specifications."
- **Break condition:** If the specification is not absolute liveness (e.g., a safety property), adding arbitrary prefixes via resets might violate the semantics, causing the reduction to fail.

### Mechanism 3: Probabilistic Lexicographic Switching
- **Claim:** The system optimizes lexicographic objectives (Spec Satisfaction > Mean Payoff) by stochastically switching the agent between a "payoff collection" mode and a "specification verification" mode.
- **Mechanism:** The reward machine utilizes a probabilistic bit b. When b=0, the agent receives external rewards (ρ). With probability β, the bit flips to b=1, switching the reward to a penalized sum of external reward and automaton acceptance status, forcing the agent to seek an accepting edge to return to b=0.
- **Core assumption:** β is sufficiently small to allow payoff optimization but sufficient to enforce the primary liveness objective.
- **Evidence anchors:**
  - [Section 5] "We construct a probabilistic reward machine... With probability β on every step, this bit flips to one."
  - [Section 5] "Theorem 5.3... For every ε > 0, there exists a threshold β* > 0... [the policy] achieves an average reward v ≥ v* - ε."
- **Break condition:** If β is too high, the agent spends too much time verifying the spec, degrading the mean-payoff objective below the ε-optimal threshold.

## Foundational Learning

- **Concept: Communicating Markov Decision Processes (MDPs)**
  - **Why needed here:** Average-reward RL (unlike discounted RL) relies on the steady-state distribution. If an MDP is not communicating (i.e., it has transient states or disconnected components), average-reward algorithms may not converge to a unique optimum.
  - **Quick check question:** Can you draw a state diagram where state A can reach state B, but state B cannot return to state A? (If yes, this breaks the paper's core assumption).

- **Concept: Omega-Regular Languages & Absolute Liveness**
  - **Why needed here:** The paper restricts scope to "absolute liveness" properties (e.g., "eventually A forever"), which are robust to the "resets" used in the learning mechanism.
  - **Quick check question:** Does the specification "Never enter state Error" satisfy absolute liveness? (No, it is a safety property; a finite prefix of entering Error violates it).

- **Concept: Product MDP Construction**
  - **Why needed here:** To track the history-dependent specification, the environment state is augmented with the automaton state. The agent acts in this expanded space.
  - **Quick check question:** If the environment has 10 states and the automaton has 5 states, what is the maximum size of the Product MDP state space? (50).

## Architecture Onboarding

- **Component map:**
  Environment MDP -> Specification Parser -> Reward Machine Wrapper -> Product MDP Generator -> Differential Q-Learning Agent

- **Critical path:**
  1. Validate that the environment is communicating (or weakly communicating)
  2. Transform specification into an Absolute Liveness GFM Automaton
  3. **Crucial Step:** Add hard resets (ε) with negative reward c to the automaton to ensure connectivity
  4. Instantiate Differential Q-learning with learning rate α and reward-rate estimate r̄

- **Design tradeoffs:**
  - **Reset Cost (c):** Must be negative enough to prevent "reset loops" but not so negative that it destabilizes early learning (though theory suggests "sufficiently negative" is safe)
  - **Switching Probability (β):** In multi-objective mode, tuning β trades off how strictly the agent adheres to the liveness spec vs. maximizing mean payoff

- **Failure signatures:**
  - **Non-convergence:** If the product MDP is not communicating (e.g., automaton has sink states without added resets), average reward will fluctuate or plateau incorrectly
  - **Zero Satisfaction:** If c is not sufficiently negative, the agent may learn a policy that resets infinitely often to avoid difficult transitions, yielding 0% spec satisfaction

- **First 3 experiments:**
  1. **Gridworld Liveness:** Implement a "patrol" task (visit A and B infinitely often) in a gridworld. Compare convergence speed of this average-reward method vs. discounted Q-learning with manual resets
  2. **Hyperparameter Sweep on c:** Vary the reset penalty c on a simple reachability task to identify the empirical threshold c* where the agent switches from "giving up/resettting" to "solving the task"
  3. **Lexicographic Optimization:** Set up a resource collection task where the agent must collect items (mean payoff) but is periodically forced (via β) to return to a charging station (liveness). Plot the Pareto frontier by varying β

## Open Questions the Paper Calls Out
The paper explicitly states in its conclusion that future work plans to explore the use of function approximation to enable average-reward RL to achieve the same level of success for continuing tasks as discounted RL has achieved in episodic settings.

## Limitations
- Restricted to absolute liveness specifications (safety and other non-prefix-independent properties fall outside scope)
- Requires underlying MDP to be communicating (excludes environments with absorbing or transient states)
- Experimental validation limited to 8 benchmarks without comprehensive comparison to discounted RL in continuing settings

## Confidence
- **High:** Average-reward RL convergence guarantees when product MDP is communicating (Theorem 4.2, Lemma 4.3)
- **High:** Specification satisfaction through sufficiently negative reset penalty (Lemma 4.4)
- **Medium:** Lexicographic multi-objective optimization (Theorem 5.3 requires careful parameter tuning)
- **Low:** Practical threshold determination for c* and β* without full MDP knowledge

## Next Checks
1. Test specification violations: Apply the method to a non-absolute liveness property (e.g., safety) and verify theoretical guarantees break as predicted
2. Measure sensitivity to reset penalty: Systematically vary c around the theoretical threshold c* to empirically identify the transition point between successful and failed learning
3. Evaluate scalability: Run experiments on larger product MDPs (10K+ states) to assess computational tractability and potential sample efficiency issues