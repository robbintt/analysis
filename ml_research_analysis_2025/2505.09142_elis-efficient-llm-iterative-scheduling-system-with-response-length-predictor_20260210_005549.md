---
ver: rpa2
title: 'ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor'
arxiv_id: '2505.09142'
source_url: https://arxiv.org/abs/2505.09142
tags:
- each
- scheduling
- serving
- elis
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ELIS introduces an iterative scheduling system for large language
  models (LLMs) that addresses the "head-of-line blocking" problem in current serving
  systems by using a response length predictor based on the BGE model. The system
  implements an Iterative Shortest Remaining Time First (ISRTF) scheduler that predicts
  and prioritizes tasks with shorter remaining token generation, reducing average
  job completion time by up to 19.6% compared to First-Come-First-Served (FCFS) scheduling
  on NVIDIA A100 GPUs.
---

# ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor

## Quick Facts
- arXiv ID: 2505.09142
- Source URL: https://arxiv.org/abs/2505.09142
- Authors: Seungbeom Choi; Jeonghoe Goo; Eunjoo Jeon; Mingyu Yang; Minsung Jang
- Reference count: 40
- Primary result: 19.6% average job completion time reduction vs FCFS on NVIDIA A100 GPUs

## Executive Summary
ELIS addresses head-of-line blocking in LLM serving systems by implementing an Iterative Shortest Remaining Time First (ISRTF) scheduler that predicts and prioritizes tasks with shorter remaining token generation. The system uses a BGE-based response length predictor that iteratively refines estimates throughout the generation process, achieving R²=0.852 accuracy. Implemented as a cloud-native solution on Kubernetes, ELIS demonstrates near-linear scaling to 18.77 RPS with 50 workers on NVIDIA H100 GPUs while reducing queuing delays by 16.75%.

## Method Summary
ELIS integrates an iterative predictor into vLLM's execution loop, using a frozen BGE encoder with trained linear layers to predict response length. The system operates with 50-token iteration windows, where incomplete jobs return to the priority buffer with updated partial outputs for re-prediction. The frontend scheduler manages JobPool, PriorityBuffer, and Batcher components, while backend workers execute tasks using vLLM with iteration-wise execution. The predictor is trained on 105,295 prompt-answer pairs from 13 different LLMs, and request intervals are modeled using Gamma distributions based on real-world FabriX service data.

## Key Results
- Average JCT reduced by up to 19.6% compared to FCFS scheduling
- Queuing delay reduced by 16.75%, with JCT reduction closely matching queuing delay reduction
- Predictor achieves R²=0.852 accuracy using iterative refinement across 8 prediction steps
- System scales near-linearly to 18.77 RPS with 50 workers on NVIDIA H100 GPUs

## Why This Works (Mechanism)

### Mechanism 1
Iterative prediction refinement improves response length estimates as more tokens become available. The BGE-based predictor receives the prompt plus partially generated tokens at each 50-token iteration window, adjusting the remaining-length estimate by subtracting already-generated tokens. This reduces Mean Absolute Error (MAE) stepwise from ~60 to ~20 across 8 steps, leveraging the semantic content of partial outputs to signal remaining generation length.

### Mechanism 2
Prioritizing jobs with shorter predicted remaining tokens reduces average Job Completion Time (JCT) by minimizing queuing delay. ISRTF reorders the priority buffer each iteration so jobs with fewer predicted remaining tokens are batched first, preventing long-generation jobs from blocking shorter jobs. The JCT reduction (16.45%) closely matches queuing delay reduction (16.75%), indicating the gain comes from queue management rather than execution speedup.

### Mechanism 3
A frozen BGE encoder with trained linear layers can predict response length without modifying the served LLM or incurring high inference cost. The predictor extracts the CLS token embedding from BGE (frozen weights), passes it through 8 fully-connected layers (1024 hidden dim, ReLU), and outputs a length estimate. Training on 105,295 prompt-answer pairs shows the CLS embedding captures task difficulty/context cues correlated with response length.

## Foundational Learning

- **Prefill vs. Decode phases in autoregressive LLM inference**: Understanding that TTFT (prefill) and TPOT (decode) have different latency characteristics is essential for interpreting scheduling decisions. Quick check: Which phase dominates latency for a 100-token prompt with 500-token output on a memory-bound GPU?

- **Continuous/iteration-level batching (ORCA-style)**: ELIS builds on iteration-level scheduling rather than static batching; jobs join/leave the batch mid-generation. Quick check: Why does static batching cause short jobs to wait for long jobs?

- **Shortest Remaining Time First (SRTF) vs. SJF vs. FCFS**: ISRTF is a variant of SRTF adapted for iterative batch boundaries; knowing when SRTF is optimal (minimum average waiting time) and when it fails (starvation) is critical. Quick check: Under what arrival pattern does SRTF provide no benefit over FCFS?

## Architecture Onboarding

- **Component map**: Frontend Scheduler (Load Balancer → Job Pool → Predictor → Priority Buffer → Batcher) → Backend Workers (vLLM execution engine) → Request Generator (samples prompts from LMSYS dataset, intervals from Gamma distribution)

- **Critical path**: 1) Prompt arrives → assigned to worker via Load Balancer (greedy min-load) 2) Job pushed to Job Pool → priority assigned by Predictor 3) Jobs reordered in Priority Buffer → batched (50-token window) 4) Backend executes one iteration → partial outputs returned 5) Incomplete jobs return to Job Pool with updated partial response; complete jobs return to user

- **Design tradeoffs**: Window size (50 tokens) balances scheduling overhead vs priority update delay; frozen BGE reduces training cost but may limit accuracy on domain-specific prompts; preemption frequency designed but not heavily evaluated due to low real-world RPS

- **Failure signatures**: Priority inversion storms if predictor overestimates short jobs; predictor latency bottleneck if BGE inference exceeds TPOT × batch_size; worker hot-spotting if job durations vary wildly

- **First 3 experiments**: 1) Baseline comparison: FCFS vs ISRTF on LLaMA2-13B with fixed batch size 4, measure average JCT and queuing delay 2) Predictor ablation: Disable iterative prediction and compare MAE and JCT 3) Scalability test: Deploy 10, 20, 30 workers on H100 cluster, measure peak RPS where average queuing delay <0.5s

## Open Questions the Paper Calls Out

- **Starvation Prevention**: How can the system prevent starvation of long-generation requests without negating the latency benefits of the ISRTF scheduler? The authors designed policies to adjust preemption frequency but haven't validated them.

- **Preemption Frequency**: What is the optimal preemption frequency and policy for ELIS in high-load scenarios where KV cache memory is constrained? The system includes code for preemption adjustment but real-world traces showed low RPS making it rarely needed.

- **Out-of-Distribution Performance**: How does the BGE-based response length predictor perform on out-of-distribution prompt domains like code generation or multi-turn reasoning? The R²=0.852 accuracy is based on conversational data without validation on structured languages.

## Limitations

- Predictor accuracy may degrade on prompts significantly different from LMSYS dataset (e.g., code generation)
- Scheduling overhead (11.04ms) could become significant at very high RPS or with small models
- ISRTF theoretically risks starving long jobs under sustained high load, though real-world traces showed low RPS
- System performance characteristics may differ on hardware other than NVIDIA A100/H100 GPUs

## Confidence

- **High Confidence**: Iterative prediction refinement mechanism (Figure 2b MAE reduction); JCT reduction claim (16.45-19.6%) with clear baselines
- **Medium Confidence**: Predictor R²=0.852 accuracy (lacks out-of-distribution validation); superiority of iterative over initial prediction (not rigorously ablated)
- **Low Confidence**: Starvation analysis (acknowledged but not quantified); scalability claims (demonstrated but not stress-tested to limits)

## Next Checks

1. **Out-of-Distribution Prompt Testing**: Evaluate predictor accuracy on specialized prompt categories (code, mathematical reasoning, multi-turn conversations) not well-represented in LMSYS corpus. Measure R² degradation and JCT impact when serving mixed-domain workloads.

2. **Overhead Scaling Analysis**: Systematically vary RPS and model sizes to identify threshold where scheduling overhead (11.04ms) becomes significant fraction (>20%) of total latency. Determine maximum sustainable RPS before JCT benefits erode.

3. **Starvation Probability Quantification**: Simulate bursty arrival patterns (high RPS with skewed job length distribution) and measure probability and duration of long-job starvation under ISRTF. Compare with FCFS and evaluate effectiveness of potential mitigation strategies like aging mechanisms.