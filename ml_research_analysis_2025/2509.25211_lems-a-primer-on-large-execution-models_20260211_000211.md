---
ver: rpa2
title: 'LEMs: A Primer On Large Execution Models'
arxiv_id: '2509.25211'
source_url: https://arxiv.org/abs/2509.25211
tags:
- execution
- market
- volume
- time
- vwap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Execution Models (LEMs) introduce a unified deep learning
  framework that addresses complex execution problems with flexible time boundaries
  and multiple execution constraints. The core method employs a shared feature extraction
  pipeline using Temporal Kolmogorov-Arnold Networks, Variable Selection Networks,
  and multi-head attention to process market data, while independent allocation networks
  handle specific execution logic for different scenarios.
---

# LEMs: A Primer On Large Execution Models
## Quick Facts
- arXiv ID: 2509.25211
- Source URL: https://arxiv.org/abs/2509.25211
- Reference count: 27
- Introduces Large Execution Models as a unified deep learning framework for complex execution problems with flexible time boundaries and multiple constraints

## Executive Summary
Large Execution Models (LEMs) present a unified deep learning framework designed to address complex execution problems involving flexible time boundaries and multiple execution constraints. The core innovation lies in decoupling market information processing from allocation decisions through a shared feature extraction pipeline using Temporal Kolmogorov-Arnold Networks, Variable Selection Networks, and multi-head attention. This architecture enables a single model to handle diverse execution objectives including fixed quantity vs. fixed notional orders, buy vs. sell orders, and varying duration boundaries while maintaining operational advantages through unified deployment.

The empirical evaluation demonstrates superior execution performance compared to traditional benchmarks across intraday cryptocurrency markets and multi-day equity trading using Dow Jones constituents. The model achieves average gains of -35bps for buying and +36bps for selling when given sufficient time flexibility, naturally handling benchmark beating problems across diverse execution conditions. The approach represents a significant advancement in execution modeling by providing a flexible, scalable framework that can adapt to various market structures and execution requirements.

## Method Summary
The LEM framework employs a shared feature extraction pipeline that processes market data through Temporal Kolmogorov-Arnold Networks (T-KANs), Variable Selection Networks (VSNs), and multi-head attention mechanisms. This shared component captures market dynamics and relevant features from order book data, technical indicators, and other market signals. Independent allocation networks then handle specific execution logic for different scenarios, allowing the model to adapt to various execution objectives without retraining the entire system. The architecture decouples market information processing from allocation decisions, enabling the same feature extraction component to serve multiple allocation strategies for different order types and market conditions.

The model operates on a sequence of market states, processing temporal information through the T-KANs to capture non-linear relationships and time dependencies. The VSNs select relevant features for different execution scenarios, while the attention mechanisms focus on the most pertinent market information. The allocation networks then use these processed features to determine optimal execution strategies, including order splitting, timing, and quantity decisions. This modular approach allows for flexible adaptation to different market conditions while maintaining computational efficiency through shared feature extraction.

## Key Results
- Achieved average execution gains of -35bps for buying and +36bps for selling in cryptocurrency markets with sufficient time flexibility
- Demonstrated superior performance compared to traditional benchmarks across both intraday cryptocurrency and multi-day equity trading scenarios
- Successfully handled benchmark beating problems across diverse execution conditions including fixed quantity vs. fixed notional orders and varying duration boundaries

## Why This Works (Mechanism)
The LEM framework works by fundamentally decoupling market information processing from allocation decision-making, allowing the model to learn rich market representations once and apply them flexibly across different execution scenarios. The Temporal Kolmogorov-Arnold Networks capture complex temporal dependencies in market data, while the Variable Selection Networks dynamically identify relevant features for specific execution contexts. The multi-head attention mechanism enables the model to focus on the most important market signals at each decision point, improving execution quality while maintaining computational efficiency through shared feature extraction.

## Foundational Learning
- Kolmogorov-Arnold Networks (KANs): These networks replace traditional activation functions with learnable univariate functions, providing greater flexibility in modeling complex relationships. Why needed: Traditional neural networks may struggle with the non-linear and non-convex nature of execution problems. Quick check: Verify that KANs can approximate arbitrary functions required for complex market dynamics.

- Temporal Processing with T-KANs: The temporal extension of KANs processes sequential market data while preserving important temporal relationships. Why needed: Execution problems inherently involve time-dependent decisions and market impact. Quick check: Ensure temporal dependencies are properly captured without introducing excessive computational overhead.

- Variable Selection Networks (VSNs): These networks dynamically select relevant features for different execution scenarios, improving model efficiency and accuracy. Why needed: Different execution objectives require different market information, and not all features are relevant for all scenarios. Quick check: Validate that VSNs correctly identify and weight important features for each execution type.

- Multi-head Attention: This mechanism allows the model to focus on different aspects of market information simultaneously. Why needed: Market data contains multiple types of information that may be relevant for different aspects of execution. Quick check: Confirm that attention weights align with intuitive market importance measures.

## Architecture Onboarding
Component Map: Market Data -> T-KANs -> VSNs -> Multi-head Attention -> Shared Features -> Allocation Networks (multiple independent networks)
Critical Path: Market Data Processing (T-KANs + VSNs + Attention) -> Feature Sharing -> Allocation Decision Making
Design Tradeoffs: The decoupling of feature extraction from allocation decisions provides flexibility but requires careful coordination between shared and specific components. The use of T-KANs offers greater modeling flexibility but may increase computational complexity compared to traditional architectures.

Failure Signatures: Poor execution performance may indicate inadequate feature extraction, misaligned attention mechanisms, or insufficient training data for specific execution scenarios. Model instability could arise from overfitting to specific market conditions or inadequate regularization in the allocation networks.

First Experiments:
1. Test feature extraction quality by comparing shared features across different execution scenarios to ensure meaningful representations are learned
2. Validate attention mechanism by analyzing attention weight distributions and their correlation with known market importance factors
3. Evaluate allocation network performance on simplified execution problems before scaling to complex scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims rely heavily on controlled backtesting environments without extensive out-of-sample validation across diverse market regimes
- Results focus primarily on cryptocurrency markets and US equities, leaving questions about performance in less liquid or emerging markets
- The model's generalization capabilities across different asset classes and market structures remain largely untested, particularly regarding extreme market conditions and regime shifts

## Confidence
High: Core architectural framework and theoretical foundations are well-established with clear separation of concerns between feature extraction and allocation logic. Mathematical formulation appears sound and follows established deep learning principles.

Medium: Empirical performance claims are supported by backtesting results but require additional validation through out-of-sample testing and cross-market validation. Specific gain figures may be sensitive to testing conditions and assumptions.

Low: Long-term stability in live trading environments, impact of evolving market microstructure, and adaptability to entirely new market conditions remain largely speculative without extensive real-world deployment data.

## Next Checks
1. Conduct out-of-sample testing across multiple asset classes and market conditions, including periods of high volatility and liquidity stress
2. Perform extensive ablation studies to quantify the individual contributions of T-KANs, VSNs, and attention mechanisms to overall performance
3. Deploy the model in a controlled paper-trading environment with real-time market data to assess practical implementation challenges and model robustness