---
ver: rpa2
title: Partial Feedback Online Learning
arxiv_id: '2601.21462'
source_url: https://arxiv.org/abs/2601.21462
tags:
- learner
- exists
- adversary
- learning
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies partial-feedback online learning, where the
  learner only observes one correct label per instance, though multiple correct labels
  may exist. The classical version space approach fails here because observing a single
  witness label doesn't falsify any hypothesis.
---

# Partial Feedback Online Learning

## Quick Facts
- **arXiv ID:** 2601.21462
- **Source URL:** https://arxiv.org/abs/2601.21462
- **Reference count:** 40
- **Primary result:** Introduces Partial-Feedback Littlestone dimension (PFLdim) and Measure Shattering dimension (PMSdim) that tightly characterize minimax regret for deterministic and randomized learners respectively under partial feedback.

## Executive Summary
This paper addresses partial-feedback online learning where the learner observes only one correct label per instance, though multiple correct labels may exist. The classical version space approach fails because observing a single witness label doesn't falsify any hypothesis. To solve this, the authors introduce a collection version space that maintains sets of hypotheses instead of individual ones, enabling monotone progress. They define PFLdim and PMSdim dimensions that tightly characterize the minimax regret for deterministic and randomized learners respectively. A key finding is that deterministic and randomized learnability coincide under a nested-inclusion condition on the label sets.

## Method Summary
The paper introduces a collection version space (CVS) approach that maintains sets of hypotheses rather than individual ones, enabling monotonic pruning under partial feedback. The Partial-Feedback Littlestone dimension (PFLdim) and Partial-Feedback Measure Shattering dimension (PMSdim) are defined to characterize the minimax regret for deterministic and randomized learners respectively. The Deterministic Partial-Feedback Learning Algorithm (DPFLA) uses a min-max strategy on an auxiliary "Prefix PFLdim" to bound potential, while the Multi-scale Randomized Partial-Feedback Learner (MRPFL) handles the randomized case. The paper shows that beyond set-realizability, even a 2-element hypothesis space can yield linear regret, highlighting the fundamental barrier in more general settings.

## Key Results
- Collection version space enables monotone pruning under partial feedback where classical version space fails
- PFLdim and PMSdim tightly characterize minimax regret for deterministic and randomized learners respectively
- Deterministic and randomized learnability coincide under nested-inclusion condition on label sets
- Even 2-element hypothesis classes can suffer linear regret beyond set-realizability

## Why This Works (Mechanism)

### Mechanism 1: Collection Version Space for Partial Feedback
- **Claim:** Using a collection version space (CVS) of hypothesis subsets enables monotonic pruning and learning under partial feedback where the standard version space approach fails.
- **Mechanism:** In full-feedback online learning, a single hypothesis is eliminated if its prediction disagrees with the revealed label set. Under partial feedback, observing a single "witness" label does not falsify any single hypothesis because multiple correct labels may exist. The collection version space maintains sets of hypotheses. If a set $F$ cannot "explain" the observed witness $y^{vis}_t$ at instance $x_t$ (i.e., $y^{vis}_t \notin F(x_t)$), the entire set $F$ is pruned. This restores monotone elimination at the subset level.
- **Core assumption:** The problem is set-realizable, meaning there exists a fixed subset $F^* \subseteq H$ such that the valid-label set at each round is generated by $S_t = F^*(x_t)$.
- **Evidence anchors:**
  - [abstract] "...while classical version space is widely used for online learnability, it does not directly extend to this setting. We address this obstacle by introducing a collection version space, which maintains sets of hypotheses rather than individual hypotheses."
  - [section] Page 2, "Collection version space" defines the update rule: $\tilde{V}_t := \{ F \in \tilde{V}_{t-1} : y^{vis}_t \in F(x_t) \}$.
  - [corpus] Evidence for this specific "collection version space" mechanism is weak in the provided corpus neighbors.
- **Break condition:** Without the set-realizability assumption, monotonic pruning may fail or become insufficient to guarantee sublinear regret, leading to potential linear regret even for small hypothesis classes.

### Mechanism 2: Partial-Feedback Littlestone Dimension (PFLdim) for Deterministic Learnability
- **Claim:** The minimax regret for deterministic learners is tightly characterized by the Partial-Feedback Littlestone dimension (PFLdim).
- **Mechanism:** PFLdim extends the classical Littlestone dimension to the partial-feedback setting. It is defined on an X-valued, Y-ary tree where edges are annotated with visible witness labels ($y^{vis}$), while ground-truth valid-label sets can be assigned path-wise. A tree is "q-shattered" if for every root-to-leaf path, there exists a consistent hypothesis subset $F$ for which the learner's prediction is wrong on at least $q$ rounds. The Deterministic Partial-Feedback Learning Algorithm (DPFLA) uses a min-max strategy on an auxiliary "Prefix PFLdim" (PPFLdim) to bound potential, ensuring mistakes are bounded by PFLdim.
- **Core assumption:** The learner is deterministic and operates in the set-realizable regime.
- **Evidence anchors:**
  - [abstract] "We define the Partial-Feedback Littlestone dimension (PFLdim)... and show that they tightly characterize the minimax regret for deterministic... learners, respectively."
  - [section] Definition 5 defines PFLdim. Theorem 6 states the exact minimax regret: $\inf_{Deterministic A} R_A(T,H) = PFL_T(\tilde{H})$.
  - [corpus] No corpus papers define this specific dimension.
- **Break condition:** This characterization holds only in the set-realizable regime. Outside this regime, deterministic learnability is not guaranteed.

### Mechanism 3: Nested Inclusion for Deterministic=Randomized Learnability
- **Claim:** Deterministic and randomized learnability coincide when the family of admissible label sets, $\mathcal{S}(\mathcal{Y})$, satisfies a "nested-inclusion" property.
- **Mechanism:** The paper proves that the Partial-Feedback Measure Shattering dimension (PMSdim, for randomized learners) equals PFLdim (for deterministic learners) under this structural condition. The nested-inclusion property ensures that for any probability measure, a "hard" label can be found, making randomization no more powerful than determinism. This finding resolves an open question regarding the necessity of a finite Helly number for this equivalence.
- **Core assumption:** $\mathcal{S}(\mathcal{Y})$ satisfies the nested-inclusion condition (a countable sequence of nested sets with empty intersection) or has a finite Helly number.
- **Evidence anchors:**
  - [abstract] "We further identify a nested inclusion condition under which deterministic and randomized learnability coincide, resolving an open question..."
  - [section] Theorem 16 establishes the dimension equivalence. Corollary 17 extends the argument to set-valued online learning.
  - [corpus] Corpus does not address this specific structural condition.
- **Break condition:** Without nested inclusion (or a finite Helly number), randomized learners may achieve fundamentally better regret rates than deterministic ones.

## Foundational Learning

- **Concept: Version Space in Online Learning**
  - **Why needed here:** To understand the core problem and motivation. The paper explicitly contrasts the classical version space (which fails) with its proposed collection version space. Grasping why partial feedback breaks standard pruning is essential.
  - **Quick check question:** Why does observing a single "witness" label from a set of correct labels fail to rule out a hypothesis that predicted a different, yet potentially valid, label?

- **Concept: Littlestone Dimension (Ldim)**
  - **Why needed here:** The paper's primary contribution, PFLdim, is a direct generalization of the Littlestone dimension. Understanding how Ldim uses tree-based arguments to bound mistakes in standard realizable online learning is a prerequisite for following the PFLdim construction and proof strategy.
  - **Quick check question:** Given a hypothesis class, can you describe how a tree of instances is constructed to force a learner to make a mistake on every level, and how the depth of this tree relates to the mistake bound?

- **Concept: Realizability Assumptions**
  - **Why needed here:** The paper's positive results are strictly tied to the "set-realizable" regime. A key finding is the sharp negative result outside this regime, which defines the fundamental barrier for this problem type.
  - **Quick check question:** What is the difference between an "existence-realizable" setting (where a single hypothesis is consistent) and a "set-realizable" setting (where a subset of hypotheses generates the valid-label set), and how does this distinction impact learnability under partial feedback?

## Architecture Onboarding

- **Component map:** Instance Handler -> Collection Version Space Manager -> Dimension Evaluator (PPFLdim/PPMSdim) -> Predictor
- **Critical path:** The prediction at time $t$. The learner must compute $\hat{\pi}_t = \text{arg min}_{\hat{\pi}} \max_{y \in Q_t} \text{Potential}(\tilde{V}_{t-1}, \hat{\pi}, y)$, where $Q_t$ is the union of labels in the current CVS. The adversarial environment then reveals $y^{vis}_t$, triggering the CVS update.
- **Design tradeoffs:**
    - **Deterministic vs. Randomized:** Implement the Deterministic Partial-Feedback Learning Algorithm (DPFLA, Alg 1) if nested inclusion holds. Otherwise, implement the Multi-scale Randomized Partial-Feedback Learner (MRPFL, Alg 5), which is more complex, requiring the discretization of a $\gamma$ parameter.
    - **Complexity vs. Performance:** Computing the exact dimension-based potential is likely intractable for large or infinite hypothesis classes. Practical implementations will require approximations or surrogate loss bounds.
- **Failure signatures:**
    - **Linear Regret on Simple Classes:** If the problem is not set-realizable (e.g., only existence-realizable), even a 2-element hypothesis class can suffer linear regret. This is an inherent failure mode of the information bottleneck.
    - **Stagnant Version Space:** If the collection version space $\tilde{V}_t$ stops shrinking effectively, the potential will not decrease, and the learner will fail to improve.
- **First 3 experiments:**
    1. **Sanity Check on Synthetic Data:** Implement DPFLA (Algorithm 1) for a finite hypothesis class where $|\mathcal{Y}|=2$ under set-realizability. Verify that the regret is bounded by $|H|$ as per Theorem 9.
    2. **Ablation on Realizability:** Use the 2-element hypothesis class example from Theorem 20 to demonstrate the sharp transition from learnable (set-realizable) to unlearnable (existence-realizable). Show linear regret in the latter case.
    3. **Randomized Learner Evaluation:** Implement MRPFL (Algorithm 5) and test against a hypothesis class that does *not* satisfy the nested inclusion condition. Compare its performance against the deterministic DPFLA to identify any gap in regret rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a learnability separation between randomized and deterministic learners under the public setting in partial-feedback online learning?
- Basis in paper: [explicit] "We leave it as an open problem to see if there is any learnability separation between randomized and deterministic learners under public setting" (page 12).
- Why unresolved: Theorem 19 shows a separation between oblivious and public settings for randomized learners, but the revealed label limits the adversary's ability to choose the ground-truth function set, leaving the deterministic vs. randomized comparison under public setting unresolved.
- What evidence would resolve it: Constructing either a counterexample class H where randomized learners achieve sublinear regret while deterministic learners incur linear regret under public setting, or a proof that no such separation exists.

### Open Question 2
- Question: Can noise-sensitive combinatorial complexity measures be developed to characterize learnability in the existence-realizable and agnostic regimes of partial-feedback online learning?
- Basis in paper: [explicit] "We leave the identification of an appropriate combinatorial dimension and its learnability implications as an open direction" (page 13).
- Why unresolved: Theorem 20 shows linear regret is possible even for |H|=2 beyond set-realizability, demonstrating that standard complexity measures (PFLdim, PMSdim) do not capture the difficulty introduced by noise or model misspecification under partial feedback.
- What evidence would resolve it: A new dimension that jointly captures hypothesis complexity and the severity of non-realizability, with matching upper and lower regret bounds in the existence-realizable or agnostic settings.

### Open Question 3
- Question: Can the structural condition relating PFLdim growth to set-valued learnability be further tightened beyond the union-closure assumption?
- Basis in paper: [explicit] "It remains an open problem whether N−PFL(H)/N → 1 as the condition can be further tightened to ensure that the set-valued and partial-feedback online learning are both not learnable" (page 26).
- Why unresolved: Theorem 11 requires union-closure of S(Y) to ensure unlearnability transfers from partial-feedback to set-valued settings; Example 3 shows this condition is not redundant, but whether a weaker sufficient condition exists is unknown.
- What evidence would resolve it: A characterization of the minimal structural requirements on S(Y) guaranteeing that PFLdim growth approaching d implies infinite Set Littlestone dimension, or a proof that union-closure is necessary.

## Limitations

- Theoretical guarantees are tightly coupled to the set-realizable assumption, which may be overly restrictive in practical scenarios
- Computational complexity of maintaining and querying the collection version space grows exponentially with hypothesis class size
- Dimension calculation procedures (PFLdim, PMSdim, PPFLdim, PPMSdim) lack efficient algorithmic implementations, limiting immediate applicability

## Confidence

- **High Confidence:** The characterization of minimax regret bounds using PFLdim and PMSdim (Theorems 6 and 11) - the proofs follow established online learning frameworks and the dimensional analysis is rigorous.
- **Medium Confidence:** The equivalence between deterministic and randomized learnability under nested inclusion (Theorem 16) - while the proof structure is sound, the nested inclusion condition is a strong structural assumption that may not hold in many practical settings.
- **Medium Confidence:** The negative result showing linear regret for 2-element hypothesis classes outside set-realizability (Theorem 20) - the construction is correct, but the result highlights a fundamental limitation rather than a practical solution.

## Next Checks

1. Implement Algorithm 1 (DPFLA) for a 3-element hypothesis class and verify that the achieved regret matches the theoretical bound of PFLdim through synthetic experiments.
2. Test the nested inclusion condition on a family of admissible sets beyond the canonical examples to determine its prevalence in practical scenarios.
3. Evaluate the computational feasibility by measuring the growth of the collection version space on hypothesis classes of increasing size (n=5, 10, 15) to identify practical scalability limits.