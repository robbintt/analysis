---
ver: rpa2
title: 'Prescribe-then-Select: Adaptive Policy Selection for Contextual Stochastic
  Optimization'
arxiv_id: '2509.08194'
source_url: https://arxiv.org/abs/2509.08194
tags:
- policy
- policies
- optimization
- each
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses policy selection in contextual stochastic
  optimization (CSO), where multiple candidate policies with different modeling paradigms
  exhibit heterogeneous performance across the covariate space. The authors propose
  Prescribe-then-Select (PS), a modular framework that constructs a diverse library
  of feasible candidate policies and learns a meta-policy using ensembles of Optimal
  Policy Trees to select the best policy for each observed context.
---

# Prescribe-then-Select: Adaptive Policy Selection for Contextual Stochastic Optimization

## Quick Facts
- arXiv ID: 2509.08194
- Source URL: https://arxiv.org/abs/2509.08194
- Authors: Caio de Prospero Iglesias; Kimberly Villalobos Carballo; Dimitris Bertsimas
- Reference count: 19
- Primary result: PS consistently outperforms best single policy in heterogeneous regimes by routing contexts to locally optimal policy.

## Executive Summary
This paper addresses policy selection in contextual stochastic optimization where multiple candidate policies with different modeling paradigms exhibit heterogeneous performance across the covariate space. The authors propose Prescribe-then-Select (PS), a modular framework that constructs a diverse library of feasible candidate policies and learns a meta-policy using ensembles of Optimal Policy Trees to select the best policy for each observed context. Across two benchmark CSO problems—single-stage newsvendor and two-stage shipment planning—PS consistently outperforms the best single policy in heterogeneous regimes while converging to the dominant policy when such heterogeneity is absent.

## Method Summary
PS is a modular framework consisting of three components: (1) a library builder that instantiates diverse candidate policies (SAA, PPt-RF, PP-RF, PPt-kNN, PP-kNN), (2) a K-fold cross-validation cost generator that trains policies on held-out folds and populates a cost table, and (3) an ensemble of Optimal Policy Trees trained on these cost tables that selects policies via majority vote. The framework uses out-of-sample cost evaluation to mitigate overfitting and aggregates predictions across R=10 tree repetitions per fold to stabilize selection. At inference, contexts are routed to the policy with the most votes from the ensemble.

## Key Results
- PS outperforms the best single policy in heterogeneous regimes by routing contexts to locally optimal policies
- PS converges to selecting the dominant policy when no heterogeneity exists across the covariate space
- Across both newsvendor and shipment planning benchmarks, PS achieves statistically significant improvements over all individual policies in heterogeneous regimes

## Why This Works (Mechanism)

### Mechanism 1: Regime-Dependent Policy Routing
If candidate policies exhibit heterogeneous performance across the covariate space, partitioning the space and routing contexts to the locally optimal policy improves aggregate performance. The framework uses Optimal Policy Trees to partition the covariate space into axis-aligned regions and learns to map each region to the candidate policy with the lowest empirical cost. Core assumption: distinct regimes exist where different modeling inductive biases are advantageous. Evidence: PS consistently outperforms single policies in heterogeneous regimes and segment analysis shows different policies dominate in different segments. Break condition: If a single candidate policy dominates uniformly, the mechanism reduces to a constant selector.

### Mechanism 2: Cross-Validated Cost Table Construction
Generating meta-policy training labels via out-of-sample cost evaluation mitigates overfitting compared to selecting policies based on in-sample training loss. The training phase uses K-fold partition where candidate policies are trained on complement folds and evaluated on held-out sets to produce realized costs. Core assumption: historical data distribution is representative of deployment distribution. Evidence: The K-fold CV loop explicitly trains policies on I(-k) and evaluates on I(k). Break condition: If optimization problems are extremely expensive to solve, the K×M solves required per fold may become computationally prohibitive.

### Mechanism 3: Ensemble Majority Voting
Aggregating selection decisions across an ensemble of trees trained on different folds and random seeds stabilizes the meta-policy and reduces variance in policy assignment. The framework trains K×R trees and determines the final policy index by majority vote across all trees. Core assumption: the optimal partition structure is stable but individual tree heuristics may find sub-optimal local optima. Evidence: The framework uses R=10 repetitions with different seeds and aggregates via majority vote. Break condition: If the covariate space is extremely high-dimensional with sparse interactions, shallow trees may struggle to find consistent partitions.

## Foundational Learning

- **Contextual Stochastic Optimization (CSO)**: The problem class PS addresses, where the goal is to minimize E[c(z,Y)|X=x] rather than just E[c(z,Y)]. Why needed: distinguishes CSO from standard stochastic optimization. Quick check: Does the optimal decision change if the covariate X changes, or is X irrelevant?

- **Predictive-Prescriptive vs. Point-Prediction**: The distinction between methods that plug in a point estimate μ̂(x) (Point-Prediction) vs. those that re-weight scenarios w_{N,i}(x) (Predictive-Prescriptive). Why needed: crucial for building a diverse library of candidate policies. Quick check: Does the policy optimize against a single forecasted value or a weighted distribution of historical scenarios?

- **Optimal Policy Trees (OPT)**: The selection engine that optimizes for decision quality (empirical cost) rather than prediction accuracy. Why needed: OPT is the meta-learner that partitions the covariate space. Quick check: Does the tree split criterion minimize classification error or the downstream cost objective?

## Architecture Onboarding

- **Component map**: Library Builder -> Cost Generator -> Meta-Learner -> Aggregator
- **Critical path**: The Cost Generator is the bottleneck, requiring K×M optimization solves per fold
- **Design tradeoffs**: 
  - Diversity vs. Complexity: Increasing library size M improves potential performance but linearly increases training time
  - Tree Depth vs. Interpretability: Deeper trees capture finer regime distinctions but risk overfitting
  - Fold count K: Higher K provides better cost estimates but increases required optimization solves
- **Failure signatures**: 
  - Uniform Selection: PS selects the same policy for all x, suggesting no detectable heterogeneity or under-fitting
  - Performance Collapse: PS performs worse than best single policy, indicating potential data leakage or excessive overfitting
- **First 3 experiments**:
  1. Run PS on data generated by a single policy (no heterogeneity) to verify convergence to that policy
  2. Compare PS performance using only SAA+Point-Pred vs. SAA+PP to verify diversity benefits
  3. Visualize selected policy counts per segment to verify majority vote aligns with best-in-segment

## Open Questions the Paper Calls Out
- Extending PS to multi-stage settings where decisions influence future uncertainty and state transitions
- Exploring other selection models beyond policy trees for capturing non-linear policy boundaries
- Understanding how library composition and diversity impact meta-policy robustness and performance

## Limitations
- Performance depends critically on presence of regime-dependent heterogeneity without formal conditions for when such heterogeneity exists
- OPT requires commercial license with no open-source alternative specified, creating reproducibility barriers
- Computational complexity of K-fold policy training loop not characterized, with no scaling guarantees as candidate policies or covariate dimensions increase

## Confidence
- High confidence: The mechanism outperforming single policies in heterogeneous regimes is well-supported by experimental results
- Medium confidence: The cross-validation cost table construction mitigates overfitting, though this relies on representativeness assumption
- Low confidence: The ensemble aggregation approach's stability guarantees are primarily empirical without theoretical analysis

## Next Checks
1. Generate synthetic data where a single policy dominates uniformly and verify PS correctly identifies this dominant policy
2. Measure wall-clock time for the K-fold policy training loop as a function of candidate policy count M and training set size N
3. Systematically vary OPT hyperparameters (D_max, n_min, λ) and K-fold count to assess robustness of meta-policy selection