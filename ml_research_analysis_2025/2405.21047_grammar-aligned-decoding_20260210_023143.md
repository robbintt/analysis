---
ver: rpa2
title: Grammar-Aligned Decoding
arxiv_id: '2405.21047'
source_url: https://arxiv.org/abs/2405.21047
tags:
- sampling
- grammar
- decoding
- bitvec
- asap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a fundamental flaw in grammar-constrained\
  \ decoding (GCD): while GCD guarantees outputs match a grammar, it distorts the\
  \ underlying language model distribution, leading to biased sampling. The authors\
  \ formalize grammar-aligned decoding (GAD) as the problem of sampling from a language\
  \ model while preserving the model\u2019s distribution conditioned on grammaticality."
---

# Grammar-Aligned Decoding

## Quick Facts
- arXiv ID: 2405.21047
- Source URL: https://arxiv.org/abs/2405.21047
- Reference count: 39
- Grammar-constrained decoding distorts language model distributions while grammar-aligned decoding preserves conditional probabilities

## Executive Summary
This paper identifies a fundamental flaw in grammar-constrained decoding (GCD): while GCD guarantees outputs match a grammar, it distorts the underlying language model distribution, leading to biased sampling. The authors formalize grammar-aligned decoding (GAD) as the problem of sampling from a language model while preserving the model's distribution conditioned on grammaticality. They propose Adaptive Sampling with Approximate Expected Futures (ASAp), an iterative algorithm that uses prior samples to approximate and refine expected future grammaticality, converging to the correct GAD distribution. Experiments on code generation (SyGuS) and constituency parsing show ASAp's samples are closer to the true distribution (lower KL divergence) and have higher likelihood under the model than GCD, though convergence can be slow. ASAp maintains grammatical constraints while better respecting the language model's learned probabilities.

## Method Summary
The authors formalize grammar-aligned decoding as sampling from a language model conditioned on grammaticality while preserving the model's conditional distribution. Their proposed ASAp algorithm iteratively refines token sampling by estimating the expected probability of completing a partial sequence to a valid grammar derivation. Starting from initial samples, ASAp uses the fraction of completed samples to approximate these expectations, then resamples tokens using reweighted probabilities. This process repeats until convergence, theoretically approaching the GAD distribution. The key innovation is approximating expected future completions using empirical data from previous iterations rather than exact computation, making the approach tractable for complex grammars.

## Key Results
- ASAp produces samples with lower KL divergence from the true GAD distribution compared to GCD
- Generated samples have higher likelihood under the language model than GCD outputs
- ASAp converges to the GAD distribution over multiple iterations, though requires 100+ iterations in some cases
- Maintains grammatical correctness while better preserving the language model's learned probabilities

## Why This Works (Mechanism)
GCD applies uniform weights across all valid continuations, ignoring the language model's learned probabilities for future tokens. This creates a mismatch between the sampling distribution and what the model actually predicts. GAD instead adjusts sampling weights based on how likely each token is to lead to grammatical completions, using empirical estimates from prior samples. ASAp approximates the expected future probability of grammaticality for each partial sequence, then reweights current token probabilities accordingly. This iterative refinement process gradually aligns the sampling distribution with the true conditional distribution without explicitly computing intractable future expectations.

## Foundational Learning
**Expected Future Probability**: The probability that a partial sequence can be completed to a valid grammar derivation. Why needed: Core quantity for weighting current token choices. Quick check: Can be approximated by fraction of completed samples in early iterations.
**Grammar-Constrained Decoding**: Sampling method that only produces outputs matching a given grammar. Why needed: Baseline approach that distorts language model distribution. Quick check: All outputs are grammatical but distribution is biased.
**KL Divergence**: Measure of difference between two probability distributions. Why needed: Quantifies how well ASAp approximates the GAD distribution. Quick check: Lower values indicate better alignment with target distribution.
**Adaptive Sampling**: Iterative refinement technique that updates sampling strategy based on previous results. Why needed: Enables tractable approximation of expected futures. Quick check: Each iteration uses empirical data to improve next sampling distribution.

## Architecture Onboarding

**Component Map**: Language Model -> ASAp Sampler -> Grammar Validator -> Sample Pool -> Expected Future Estimator -> Weighted Resampler

**Critical Path**: For each token position, ASAp queries the language model for next-token probabilities, checks grammar validity, estimates completion probabilities from sample pool, computes reweighted probabilities, and samples the next token. This repeats iteratively until convergence.

**Design Tradeoffs**: Exact expected future computation is intractable for complex grammars, requiring approximation through sampling. Early iterations use poor estimates but improve over time. More samples per iteration improve approximation quality but increase computational cost. The algorithm trades immediate sampling efficiency for better distributional alignment.

**Failure Signatures**: Slow convergence indicates poor initial sample coverage or complex grammar structure. High KL divergence early on suggests insufficient samples or difficult grammar constraints. If samples consistently fail to complete, the grammar may be too restrictive or the language model poorly aligned with grammatical patterns.

**3 First Experiments**:
1. Run ASAp on a simple context-free grammar with known GAD distribution to verify convergence behavior
2. Compare KL divergence and model likelihood metrics between ASAp and GCD on SyGuS benchmarks
3. Measure convergence speed across different grammar complexities and sample pool sizes

## Open Questions the Paper Calls Out
None

## Limitations
- ASAp requires many iterations (100+) to converge, making it computationally expensive for real-time applications
- The algorithm's performance depends heavily on the quality of initial samples and approximation accuracy
- Empirical evaluation focuses on specific domains (SyGuS, parsing) without extensive testing on other grammar types

## Confidence
**High**: GCD fundamentally distorts language model distributions; GAD formalization as preserving conditional distribution is sound
**Medium**: ASAp's iterative approximation approach converges toward GAD distribution in practice; experimental results showing GAD's superiority are reproducible
**Low**: ASAp's convergence speed and computational efficiency in large-scale settings; generalizability across diverse grammar types and domains

## Next Checks
1. Evaluate ASAp's performance on additional constrained generation tasks (e.g., semantic parsing, code completion with different grammar types) to test generalizability
2. Measure computational overhead of ASAp versus GCD in real-time applications and analyze convergence speed across varying grammar complexity
3. Conduct ablation studies isolating the impact of approximation accuracy on final GAD distribution quality and KL divergence metrics