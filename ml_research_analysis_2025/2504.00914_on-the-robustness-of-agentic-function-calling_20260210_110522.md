---
ver: rpa2
title: On the Robustness of Agentic Function Calling
arxiv_id: '2504.00914'
source_url: https://arxiv.org/abs/2504.00914
tags:
- function
- arxiv
- toolkit
- tool
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a benchmark to assess the robustness of
  agentic function calling capabilities in large language models. The study focuses
  on two key aspects: resilience to naturalistic query variations through meaning-preserving
  rephrasings, and stability when the toolkit expands with semantically related tools.'
---

# On the Robustness of Agentic Function Calling

## Quick Facts
- **arXiv ID**: 2504.00914
- **Source URL**: https://arxiv.org/abs/2504.00914
- **Reference count**: 9
- **Primary result**: Large language models show significant performance drops in function calling when exposed to paraphrased queries and expanded toolkits.

## Executive Summary
This paper introduces a benchmark to assess the robustness of agentic function calling capabilities in large language models, focusing on two key perturbation types: naturalistic query variations through meaning-preserving rephrasings, and stability when toolkits expand with semantically related tools. The study evaluates top-performing LLMs including GPT4o-mini, Claude-3.5, and Llama3.3-70B against these perturbations, revealing notable performance drops of 13-19% with rephrased queries and 2-8% with expanded toolkits. These findings highlight critical evaluation weaknesses in current function calling systems and identify areas for improvement in real-world agentic deployments.

## Method Summary
The benchmark was constructed by generating paraphrased user requests and expanding toolkits with additional related functions, using models like Llama3.1-70B and CodeLlama-13B as generation tools. The evaluation methodology relied on exact parameter matching to assess function calling accuracy, with perturbations designed to test resilience to naturalistic query variations and toolkit expansion scenarios. The study focused on measuring performance drops when LLMs encountered these specific types of robustness challenges.

## Key Results
- Large language models experienced 13-19% performance drops when exposed to meaning-preserving rephrased queries
- Toolkits expansion with semantically related functions caused 2-8% performance drops
- Evaluation weaknesses in exact parameter matching were identified as a key contributor to performance degradation

## Why This Works (Mechanism)
The performance drops appear to stem from LLMs' reliance on surface-level syntactic patterns rather than deeper semantic understanding. When queries are paraphrased, models struggle to map semantically equivalent requests to the correct function calls, suggesting they're matching based on specific word patterns rather than intent. Similarly, toolkit expansion introduces ambiguity in function selection, where models must disambiguate between semantically similar functions - a task that becomes more difficult as the toolkit grows. These findings indicate that current LLMs lack the robust semantic reasoning needed for reliable function calling in dynamic, real-world scenarios.

## Foundational Learning
The results suggest that current function calling systems haven't learned robust semantic representations that generalize across linguistic variations. Models appear to rely heavily on memorization of specific query-function pairings rather than developing transferable understanding of function purposes and parameter semantics. This points to limitations in how function calling capabilities are learned - models may be optimizing for exact matches in training rather than developing the flexible reasoning needed for naturalistic interactions. The study highlights the need for training approaches that emphasize semantic understanding over pattern matching.

## Architecture Onboarding
The benchmark reveals that function calling capabilities are sensitive to perturbations regardless of underlying architecture, suggesting this is a fundamental challenge rather than an implementation-specific issue. However, the magnitude of performance drops varies across models, potentially indicating architectural differences in how function calling is implemented. The study doesn't deeply explore architectural variations, but the consistent pattern of drops across different LLM families suggests that current approaches to function calling - whether through fine-tuning, prompting, or other mechanisms - share common vulnerabilities that need addressing.

## Open Questions the Paper Calls Out
The paper raises several important open questions: How can evaluation metrics be designed to distinguish between true functional errors and superficial matching failures? What training approaches would help models develop more robust semantic understanding for function calling? How do performance drops scale with increasingly complex toolkits and more sophisticated function relationships? The study also questions whether current function calling paradigms are sufficient for real-world deployment or if new architectural approaches are needed to handle the identified robustness challenges.

## Limitations
- The benchmark focuses on two specific perturbation types and may not capture other real-world failure modes
- Exact parameter matching evaluation may be overly strict and not account for semantically equivalent but syntactically different parameter assignments
- Limited model and toolkit diversity may restrict generalizability to other architectures or domain-specific scenarios
- The study doesn't explore sequential function calling scenarios where errors could compound
- No investigation into how performance varies with different training approaches or fine-tuning strategies

## Confidence
- **High confidence**: Observed performance drops (13-19% for paraphrased queries, 2-8% for toolkit expansion) are reproducible based on the described experimental setup
- **Medium confidence**: Interpretation that drops stem from evaluation weaknesses and function selection challenges is reasonable but may oversimplify underlying causes
- **Low confidence**: Claims about identifying critical evaluation weaknesses may overstate significance given standard practice of exact matching

## Next Checks
1. Implement a more flexible evaluation metric that recognizes semantically equivalent parameter assignments to distinguish true functional errors from superficial matching failures
2. Replicate experiments with domain-specific toolkits (medical, financial, technical) to assess generalizability across different application areas
3. Extend benchmark to evaluate robustness in scenarios requiring sequential function calls where errors propagate through multi-step chains
4. Investigate training approaches that emphasize semantic understanding over pattern matching to improve robustness
5. Compare performance across different architectural implementations to identify whether certain approaches are more resilient to perturbations