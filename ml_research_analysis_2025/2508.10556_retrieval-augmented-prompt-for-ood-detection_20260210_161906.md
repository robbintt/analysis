---
ver: rpa2
title: Retrieval-Augmented Prompt for OOD Detection
arxiv_id: '2508.10556'
source_url: https://arxiv.org/abs/2508.10556
tags:
- detection
- prompts
- samples
- training
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Retrieval-Augmented Prompt (RAP), a method
  for out-of-distribution (OOD) detection that addresses limitations of existing approaches
  by incorporating external knowledge to enhance semantic supervision. RAP leverages
  a pre-trained vision-language model and retrieves descriptive words from external
  knowledge bases like WordNet based on joint similarity with ID and OOD representations.
---

# Retrieval-Augmented Prompt for OOD Detection

## Quick Facts
- arXiv ID: 2508.10556
- Source URL: https://arxiv.org/abs/2508.10556
- Authors: Ruisong Han; Zongbo Han; Jiahao Zhang; Mingyue Cheng; Changqing Zhang
- Reference count: 39
- Primary result: Achieves SOTA on large-scale OOD detection benchmarks, reducing FPR95 by 7.05% and improving AUROC by 1.71% in 1-shot ImageNet-1k detection

## Executive Summary
This paper introduces Retrieval-Augmented Prompt (RAP), a novel approach for few-shot out-of-distribution (OOD) detection that addresses limitations of existing methods by incorporating external knowledge to enhance semantic supervision. RAP leverages a pre-trained vision-language model and retrieves descriptive words from external knowledge bases like WordNet based on joint similarity with ID and OOD representations. During training, it retrieves words that align with outlier representations while avoiding overlap with ID representations. During testing, RAP dynamically updates OOD prompts based on encountered OOD samples to adapt to the test environment. The method achieves state-of-the-art performance on large-scale OOD detection benchmarks.

## Method Summary
RAP is a few-shot OOD detection method that uses retrieval-augmented prompts from external knowledge bases. The approach leverages a CLIP ViT-B/16 backbone and WordNet nouns/adjectives as external knowledge. During training, ID images are randomly cropped 256 times each, and the top 32 patches with highest/lowest similarity to ID prompts are selected as ID/OOD representations. A joint similarity score combines three terms: similarity with OOD representations, negative similarity with ID visual representations, and negative percentile similarity with ID prompt representations. The top 10,000 words are retrieved from WordNet and formatted with templates. During testing, confident OOD samples with scores in [u₁,u₂] are identified, and 4 new words per sample are retrieved to update OOD prompts dynamically. Inference uses ensemble with 100 groups for final detection.

## Key Results
- In 1-shot OOD detection on ImageNet-1k dataset, RAP reduces average FPR95 by 7.05% compared to previous methods
- Improves AUROC by 1.71% in 1-shot detection settings
- Achieves state-of-the-art performance on large-scale OOD detection benchmarks
- Comprehensive ablation studies validate the effectiveness of each module

## Why This Works (Mechanism)
RAP works by leveraging external knowledge to provide richer semantic supervision for OOD detection. The retrieval mechanism identifies words that are semantically aligned with outlier representations while avoiding ID-associated words through the joint similarity formulation. The dynamic test-time adaptation allows the system to adapt to the test environment by incorporating information from encountered OOD samples. The prompt ensemble with 100 groups provides robustness and reduces variance in detection performance.

## Foundational Learning
- **Joint similarity computation**: Combines multiple similarity terms to balance semantic alignment with OOD while avoiding ID overlap. Why needed: Ensures retrieved words capture true outlier semantics without confusing with ID classes. Quick check: Verify negative signs on ID-related terms in the joint similarity formula.
- **Prompt ensemble strategy**: Uses multiple prompt groups (N_g=100) to improve detection robustness. Why needed: Reduces variance and improves generalization across different OOD scenarios. Quick check: Confirm that ensemble aggregation produces stable scores across different groups.
- **Test-time adaptation mechanism**: Dynamically updates OOD prompts based on encountered samples. Why needed: Adapts to test-time distribution shifts and captures test-specific OOD characteristics. Quick check: Monitor score distribution of samples triggering updates to ensure they are truly OOD.
- **WordNet retrieval pipeline**: Extracts relevant descriptive words based on semantic similarity. Why needed: Provides external semantic knowledge that complements visual representations. Quick check: Manually inspect top-100 retrieved words for semantic overlap with ID classes.
- **Random cropping augmentation**: Generates multiple views of ID samples for robust representation learning. Why needed: Increases diversity of ID representations and improves semantic supervision. Quick check: Verify crop parameters produce diverse, representative patches.
- **Template-based prompt formatting**: Structures retrieved words into consistent prompt templates. Why needed: Ensures consistent semantic space construction across different parts of speech. Quick check: Validate template consistency across different retrieved words.

## Architecture Onboarding

**Component Map:**
CLIP ViT-B/16 (frozen) -> Word Encoder -> Training Phase -> Test Phase -> Detection Ensemble

**Critical Path:**
1. Preprocess WordNet vocabulary through CLIP text encoder
2. Training: Random crop ID images → compute joint similarity → retrieve top P words → format prompts
3. Testing: Compute OOD scores → identify samples in [u₁,u₂] → retrieve Q words → update prompts → ensemble detection

**Design Tradeoffs:**
- Uses frozen CLIP backbone for efficiency vs. fine-tuning for potentially better adaptation
- Fixed 10,000-word retrieval pool vs. dynamic vocabulary size based on test data
- Simple template-based prompt formatting vs. more sophisticated natural language generation
- Test-time adaptation with strict thresholds vs. more aggressive adaptation

**Failure Signatures:**
- High FPR95: Retrieved words semantically overlap with ID classes (check top retrieved words)
- Degraded test performance: Test-time adaptation admits ID samples (monitor score distributions)
- Unstable detection: Prompt ensemble variance too high (check grouping strategy)

**Three First Experiments:**
1. Validate training-phase retrieval by checking that top-100 retrieved words avoid obvious ID class names
2. Test test-time adaptation mechanism on a held-out subset to ensure only true OOD samples trigger updates
3. Verify joint similarity computation by confirming negative weights on ID-related terms

## Open Questions the Paper Calls Out
None

## Limitations
- Exact WordNet preprocessing details (synsets included, filtering criteria, final vocabulary size) are unspecified
- Crop parameters lack specific details on size ratios, aspect ratios, and resizing procedures
- Prompt ensemble grouping strategy lacks detail on random seed usage or assignment methodology
- Template formulations beyond two examples are not specified

## Confidence
- **High confidence**: Core retrieval-augmented prompt methodology and overall framework are clearly described and reproducible
- **Medium confidence**: Joint similarity computation formula and retrieval procedure can be implemented, though exact hyperparameter values may affect results
- **Low confidence**: Exact WordNet vocabulary, crop augmentation specifics, and prompt ensemble details may prevent exact reproduction of reported numbers

## Next Checks
1. Verify that the top P=10,000 retrieved words from WordNet do not contain obvious semantic overlap with ImageNet-1k class names by manually inspecting the top-100 retrieved words
2. Test the test-time adaptation mechanism on a held-out subset by monitoring whether samples with scores in [u₁,u₂] are truly OOD (e.g., by checking if they come from held-out classes)
3. Validate the joint similarity formulation by checking that the terms sim₂ and sim₃ correctly use negative weights to avoid ID-associated words