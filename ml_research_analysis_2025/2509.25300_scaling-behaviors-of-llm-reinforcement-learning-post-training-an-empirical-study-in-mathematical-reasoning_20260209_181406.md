---
ver: rpa2
title: 'Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical
  Study in Mathematical Reasoning'
arxiv_id: '2509.25300'
source_url: https://arxiv.org/abs/2509.25300
tags:
- loss
- data
- scaling
- test
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically studies scaling behaviors of LLM reinforcement
  learning (RL) post-training for mathematical reasoning. Across the Qwen2.5 model
  family (0.5B to 72B), experiments show that larger models consistently exhibit superior
  learning efficiency under both compute- and data-constrained regimes.
---

# Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning

## Quick Facts
- arXiv ID: 2509.25300
- Source URL: https://arxiv.org/abs/2509.25300
- Reference count: 40
- Models 0.5B-72B consistently exhibit superior learning efficiency under compute- and data-constrained regimes

## Executive Summary
This paper empirically studies scaling behaviors of LLM reinforcement learning (RL) post-training for mathematical reasoning. Across the Qwen2.5 model family (0.5B to 72B), experiments show that larger models consistently exhibit superior learning efficiency under both compute- and data-constrained regimes. A predictive power-law scaling relationship between test loss, model size, and resource budget is derived, revealing that the learning efficiency term k(N) follows a saturation trend as model scale increases. In data-limited settings, repeated reuse of high-quality data is highly effective, with final performance primarily governed by total optimization steps rather than sample uniqueness. RL post-training improves in-domain generalization on mathematical tasks, but shows limited transfer to out-of-domain domains such as code or logic. The results provide a principled foundation and practical guidelines for efficiently scaling reasoning capabilities of LLMs through RL post-training.

## Method Summary
The study trains Qwen2.5 models (0.5B-72B parameters) using Group Relative Policy Optimization (GRPO) on 50k curated math problems from guru-RL-92k, curriculum-ordered by difficulty. The method employs VeRL framework with LR=1e-6, batch_size=512, KL_coef=0.001, clip_ratio=0.2, input_len=2048, output_len=4096. Training uses binary Pass@1 reward (1 for correct boxed answer match, 0 otherwise), and evaluation includes held-out 500 samples plus benchmarks (GSM8K, MATH500, AIME2024, AMC2023, HumanEval, Zebra Puzzle, SuperGPQA). The primary goal is to derive and validate scaling laws for RL post-training by measuring test loss L = 1 − (R/Rmax) across varying compute/data budgets and model sizes.

## Key Results
- Larger models exhibit superior learning efficiency under both compute- and data-constrained regimes
- Learning efficiency term k(N) follows a saturation trend as model scale increases
- In data-constrained settings, final performance is governed by total optimization steps rather than sample uniqueness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL post-training loss scales predictably with model size and resource budget via a log-linear power law.
- Mechanism: The test loss L(N,X) follows log L(N,X) = -k(N)·log X + E(N), where k(N) captures model-dependent learning efficiency and E(N) is a size-dependent baseline. This means each doubling of compute or data reduces loss by a factor proportional to k(N), enabling extrapolation from small to large models.
- Core assumption: The power-law form holds across the 0.5B–72B range and extends predictably to 72B when fitted on 0.5B–32B models (inter-model extrapolation) or from early training steps (intra-model prediction).
- Evidence anchors:
  - [abstract]: "A predictive power-law scaling relationship between test loss, model size, and resource budget is derived."
  - [section 3.1, Eq. 6]: "log(L(N,C)) = -k_C(N)·log(C) + E_C(N)" with fitted curves matching 72B extrapolation (Figure 2a, 2b).
  - [corpus]: Weak direct support; related work focuses on RL reasoning methods (KDRL, Meta-CoT) rather than scaling law formulation.
- Break condition: If loss curves deviate from log-linearity (e.g., show phase transitions, sudden plateaus) or if k(N) becomes unstable across training regimes, the power-law assumption fails.

### Mechanism 2
- Claim: Learning efficiency k(N) increases with model size but saturates toward an upper bound K_max.
- Mechanism: The efficiency term is modeled as k(N) = K_max / (1 + N_0/N). Larger models extract more signal per FLOP/sample (higher k), but marginal gains diminish as N → ∞, approaching K_max. This explains why 32B can initially outperform 72B under fixed compute (fewer steps possible for 72B).
- Core assumption: The saturation functional form accurately captures efficiency trends; the fitted K_max and N_0 are stable across base/instruct variants.
- Evidence anchors:
  - [abstract]: "The learning efficiency term k(N) follows a saturation trend as model scale increases."
  - [section 3.1, Figure 3]: k_C(N) curves show growth with diminishing returns beyond 32B for both base and instruct models.
  - [corpus]: No direct corpus evidence for saturation in LLM RL post-training; saturation is assumed based on empirical fit.
- Break condition: If k(N) continues growing linearly with log N beyond 72B (no asymptotic bound), or if efficiency drops sharply at certain scales (e.g., optimization instability), the saturation model is misspecified.

### Mechanism 3
- Claim: In data-constrained settings, performance is driven by total optimization steps, not sample uniqueness, making moderate data reuse effective.
- Mechanism: For fixed total data D_total = D_unique × τ, the model sees the same cumulative gradient updates regardless of τ (reuse factor). RL optimization (GRPO) appears robust to repeated samples up to τ ≈ 25, likely because per-sample stochasticity (rollout sampling, advantage normalization) provides implicit regularization.
- Core assumption: Curriculum ordering and difficulty distribution are preserved across reuse; overfitting emerges only at extreme reuse (τ ≥ 100).
- Evidence anchors:
  - [abstract]: "Final performance is primarily governed by total optimization steps rather than the uniqueness of samples."
  - [section 3.4, Figure 7]: Test loss remains stable for τ ≤ 25 on 7B base/instruct models, degrading only at τ = 100.
  - [corpus]: No direct corpus evidence on data reuse in LLM RL; assumption based on experiment design (Figure 9 schema).
- Break condition: If overfitting or memorization emerges at lower τ for different domains (e.g., code), or if data quality is poor, reuse effectiveness may degrade earlier.

## Foundational Learning

- Concept: **Power-law scaling in neural networks** (Kaplan et al., Hoffmann et al.)
  - Why needed here: The paper extends pretraining scaling laws to RL post-training. Without understanding the original L(loss) ∝ N^{-α} · D^{-β} formulation, the proposed k(N) saturation model will be opaque.
  - Quick check question: Given a fixed compute budget, would you allocate more to model size or training steps according to Chinchilla scaling?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: All experiments use GRPO, which normalizes rewards across sampled outputs from the same prompt to compute advantages. Understanding this is critical for interpreting the data/compute scaling results.
  - Quick check question: How does GRPO compute the advantage Â_i,t for output o_i at time t?

- Concept: **Curriculum learning and data difficulty**
  - Why needed here: Training data is sorted by difficulty (decreasing pass rate), and this ordering is preserved across data reuse. This design choice affects both efficiency and generalization conclusions.
  - Quick check question: Why might curriculum ordering matter more for RL fine-tuning than for supervised fine-tuning?

## Architecture Onboarding

- Component map:
  - **Models**: Qwen2.5 dense family (0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B) with consistent architecture—parameter count is the only scaling variable.
  - **RL Algorithm**: GRPO with group size G (ablated at 4, 8, 16, 32), KL penalty β=0.001, clip ratio 0.2, learning rate 1e-6.
  - **Training Data**: 50k math problems from guru-RL-92k, sorted by difficulty (curriculum), with held-out 500-problem validation set.
  - **Framework**: VeRL for distributed RL training (Section 2).
  - **Evaluation**: Binary Pass@1 reward (rule-based answer extraction), test loss L = 1 - (R/R_max).

- Critical path:
  1. **Fit scaling law**: Train multiple model sizes (0.5B–32B) across compute/data regimes, record loss trajectories.
  2. **Extract k(N)**: Fit k(N) = K_max/(1 + N_0/N) to efficiency values derived from log-linear loss curves.
  3. **Validate extrapolation**: Predict 72B performance (inter-model) or remaining training (intra-model) and compare to actual runs.
  4. **Test data reuse**: Fix D_total, vary τ (1, 2, 5, 20, 25, 50, 100), observe loss stability/overfitting.

- Design tradeoffs:
  - **Inter- vs. intra-model prediction**: Inter-model (fit on small, predict large) tests scaling law generality but requires fitting global K_max, N_0. Intra-model (fit on early steps, predict remainder) tests trajectory smoothness but assumes no late-training phase changes.
  - **Rollout size G**: Larger G improves advantage estimates (better sample efficiency) but costs more FLOPs per update. Optimal G shifts with compute budget (Appendix B.2).
  - **Data reuse vs. uniqueness**: Reuse saves data curation cost but risks overfitting at high τ. The τ ≤ 25 safe zone is empirical and may vary by domain.

- Failure signatures:
  - **Saturation not observed**: If k(N) continues growing linearly with log N, the model k(N) = K_max/(1 + N_0/N) is wrong—try alternative forms (e.g., log k(N)).
  - **Extrapolation fails**: If 72B predicted loss deviates >5% from actual, scaling law may be regime-specific (e.g., holds only up to 32B).
  - **Data reuse overfits early**: If loss degrades at τ < 10, curriculum ordering may be insufficient, or data quality is too low.
  - **OOD transfer negative**: If logic/coding benchmarks show performance drops (as in Figure 8, Zebra Puzzle), this indicates specialization "damage"—not a failure of the scaling law, but a practical constraint.

- First 3 experiments:
  1. **Replicate compute scaling**: Train Qwen2.5-7B and 32B on the 50k math dataset for increasing FLOP budgets. Fit log L vs. log C for each, verify log-linearity and extract k(7B), k(32B). Confirm k(32B) > k(7B).
  2. **Test saturation trend**: Add a 14B model to the above. Fit k(N) = K_max/(1 + N_0/N) to k(7B), k(14B), k(32B). Check if residuals are small and if extrapolated k(72B) is within 10% of fitted K_max.
  3. **Validate data reuse**: Train 7B on D_unique = 5k with τ = 1, 5, 25, 50 (so D_total = 5k, 25k, 125k, 250k). Plot final test loss vs. τ. Confirm stability for τ ≤ 25 and degradation at τ = 50.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative RL algorithms (beyond GRPO) significantly improve sample efficiency or reshape the scaling frontier?
- Basis in paper: [explicit] The Discussion states, "Whether more advanced algorithms can significantly improve sample efficiency or stability—and thereby reshape the scaling frontier—remains an important open question."
- Why unresolved: This study relied exclusively on GRPO; while comparative studies exist, they focus on training curves rather than scaling law coefficients.
- What evidence would resolve it: Replicating the scaling analysis (0.5B–72B) using algorithms like PPO or ReMax to compare the derived learning efficiency coefficients $k(N)$.

### Open Question 2
- Question: How does the integration of agentic mechanisms (e.g., tool use, long-term memory) alter the scaling laws of RL post-training?
- Basis in paper: [explicit] The "Future of LLM Agent" section identifies "Understanding the scaling laws of these agentic systems" as a "key and exciting avenue for future research."
- Why unresolved: Current laws assume standard text generation; agentic systems offload computation to tools, potentially shifting the performance frontier for a given budget.
- What evidence would resolve it: Deriving scaling laws for tool-augmented models to determine if they shift the efficiency curve upward compared to standard models.

### Open Question 3
- Question: Is it possible to establish an environment-independent "intrinsic performance" metric to normalize scaling laws across diverse tasks?
- Basis in paper: [explicit] The Discussion notes that test loss is imperfect and states, "Establishing principled, environment-independent evaluation protocols remains an open and critical challenge."
- Why unresolved: Current metrics are task-dependent (e.g., convergence rates differ between GSM8K and AIME), making universal coefficient interpretation difficult.
- What evidence would resolve it: Defining a metric that correlates linearly with compute across math, code, and logic tasks without task-specific calibration.

### Open Question 4
- Question: Does the observed saturation of learning efficiency in larger models (32B+) stem from the Qwen2.5 pre-training specifically or general architectural limits?
- Basis in paper: [inferred] The paper models saturation empirically but notes in Appendix D that the dependence of "learnable capacity" $\lambda(N)$ on the pretraining regime remains "undetermined."
- Why unresolved: The empirical study is restricted to the Qwen2.5 family; it is unclear if the saturation at $\approx$32B is a universal bottleneck or specific to Qwen's pre-training data.
- What evidence would resolve it: Applying the same RL scaling methodology to different model families (e.g., Llama, Mistral) to observe if saturation occurs at similar parameter counts.

## Limitations

- Power-law scaling formulation may not hold for larger models or different training regimes
- Data reuse effectiveness is validated only for mathematical reasoning, may not transfer to other domains
- Study focuses on single RL algorithm (GRPO) and dataset, limiting generalizability

## Confidence

- **High Confidence**: Compute scaling relationships (log-linear loss reduction with model size) - supported by extensive empirical validation across multiple model sizes and budgets.
- **Medium Confidence**: Data reuse effectiveness (τ ≤ 25 optimal) - based on controlled experiments but domain-specific and sensitive to data quality.
- **Medium Confidence**: Out-of-domain generalization limits - clearly demonstrated for math-trained models on code/logic, but may vary with different base models or training curricula.
- **Medium Confidence**: Saturation trend of k(N) - fits current data well but extrapolation to larger models untested.

## Next Checks

1. Test data reuse across domains: Repeat the data reuse experiment (τ = 1, 5, 25, 50) on coding and logic benchmarks to verify if the τ ≤ 25 sweet spot holds beyond mathematics.
2. Validate scaling law extrapolation: Train Qwen2.5-72B using compute budgets predicted by the fitted scaling law from 0.5B-32B models; measure prediction error and test for log-linear deviation.
3. Cross-algorithm robustness: Apply the same scaling analysis to another RL post-training method (e.g., PPO or Advantage Actor-Critic) to confirm whether k(N) saturation is algorithm-specific or general.