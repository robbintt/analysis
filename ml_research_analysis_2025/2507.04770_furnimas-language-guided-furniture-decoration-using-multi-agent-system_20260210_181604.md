---
ver: rpa2
title: 'FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System'
arxiv_id: '2507.04770'
source_url: https://arxiv.org/abs/2507.04770
tags:
- asset
- assets
- furniture
- system
- surface
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FurniMAS, a multi-agent system that automatically
  decorates furniture items based on text prompts. The system uses specialized LLM-based
  agents for asset selection, styling, and layout planning, supported by non-LLM validators
  and optimizers to ensure feasibility and quality.
---

# FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System

## Quick Facts
- arXiv ID: 2507.04770
- Source URL: https://arxiv.org/abs/2507.04770
- Reference count: 40
- Primary result: Zero out-of-bound and bounding box loss rates; top scores in functionality, layout, style, and aesthetics evaluations

## Executive Summary
FurniMAS is a multi-agent system that automatically decorates furniture items based on text prompts. It uses specialized LLM-based agents for asset selection, styling, and layout planning, supported by non-LLM validators and optimizers to ensure feasibility and quality. The system significantly outperforms existing baselines in generating diverse, high-quality 3D decor across multiple metrics and excels in open-vocabulary decorative editing tasks, demonstrating strong adaptability and generalization across various furniture types.

## Method Summary
FurniMAS employs a hybrid multi-agent architecture where LLM agents handle semantic tasks (asset selection, styling, arrangement planning) while non-LLM agents handle validation and numerical computation. The pipeline extracts supporting surfaces from furniture meshes, then iteratively proposes assets, styles, and relative placements through validated LLM outputs. A constrained optimization solver (Gurobi) resolves these into physically plausible arrangements, and a retriever fetches actual 3D assets from Objaverse using OpenShape. The system achieves zero out-of-bound and collision errors through hard constraints in the optimization formulation.

## Key Results
- Achieves zero out-of-bound (OOB=0.00) and bounding box loss (BBL=0.00) rates across all asset counts
- Outperforms LayoutGPT, DecoMind, and fCrit baselines in functionality, layout, style scheme, and aesthetics evaluations
- Excels in open-vocabulary decorative editing tasks with strong adaptability across furniture types

## Why This Works (Mechanism)

### Mechanism 1: Task-Specialized Agent Decomposition with Validation Gates
Decomposing furniture decoration into specialized subtasks handled by separate LLM agents, each paired with a non-LLM validator, improves output quality over monolithic approaches. Each agent focuses on one domain (e.g., only spatial relationships), reducing cognitive load and enabling targeted prompting. Validators enforce schemas and constraints before workflow advances, catching errors that would cascade.

### Mechanism 2: Constrained Optimization for Physically Plausible Arrangements
Formulating asset placement as a constrained optimization problem with hard constraints (collision-free, within-surface, global/local placements) and soft constraints (distance/alignment satisfaction) yields zero out-of-bound and collision errors. The non-LLM Arranger encodes positions as decision variables with hard constraints guaranteeing feasibility.

### Mechanism 3: Hybrid LLM/Non-LLM Division of Labor
Separating semantic reasoning (LLM agents) from validation and numerical computation (non-LLM agents) leverages LLMs' commonsense strengths while avoiding their weaknesses in arithmetic and strict constraint adherence. LLM agents generate qualitative suggestions while non-LLM agents enforce schemas, check geometric feasibility, and solve optimization.

## Foundational Learning

- **Constrained Optimization (Hard/Soft Constraints)**: Understanding how the Arranger translates natural language placement preferences into a solvable mathematical program with feasibility guarantees. Quick check: Can you explain why "keyboard in front of monitor" would be a hard constraint vs. a soft constraint in this system?

- **Multi-Agent Communication Patterns**: The Admin/Validator/Worker agent structure and stage-gated workflow are central to system reliability. Quick check: What happens if the Asset Validator rejects an Asset Selector proposal—how does the system recover?

- **Scene Graphs and Relative Spatial Relations**: The Planner outputs a scene graph of relative placements that the Arranger must resolve to absolute coordinates. Quick check: If Asset A is "left of" Asset B and Asset B is "mid-mid" on the surface, what additional constraint might be needed to avoid ambiguity?

## Architecture Onboarding

- **Component map**: Surface Extractor -> Asset Selector (Admin+Validator) -> Stylist (Admin+Validator) -> Planner (Admin+Validator) -> Arranger -> Retriever -> Final Scene
- **Critical path**: Surface Extraction → Asset Selection (Selector + Validator loop) → Stylizing (Stylist + Validator loop) → Arrangement Planning (Planner + Validator loop) → Arrangement Optimization (Arranger) → Asset Retrieval (Retriever) → Final Scene
- **Design tradeoffs**: GPT-4o vs. GPT-4.5 balances quality and cost (GPT-4.5 yields marginally better quality but at ~15x higher cost). More agents increase orchestration complexity but improve scores; 7-agent consolidation drops Layout/Func noticeably. Tighter constraints improve feasibility but may reject semantically valid configurations.
- **Failure signatures**: Validator infinite loops occur if LLM repeatedly generates invalid outputs. Optimization infeasibility happens if constraints over-constrain the space. Retrieval mismatch occurs when actual geometry diverges from estimated bounding boxes.
- **First 3 experiments**: 1) Replicate Table 3 (agent ablation) on a small furniture set to validate full 9-agent system outperforms consolidated baselines. 2) Stress-test Arranger feasibility with increasing asset counts (8, 16, 24, 32) and log solver status, runtime, and constraint violations. 3) Analyze validator failure modes by logging per-stage rejection reasons over 50 decoration tasks.

## Open Questions the Paper Calls Out

### Open Question 1
How can the asset arrangement algorithm be enhanced to maintain high-quality layout and functionality when the number of assets increases significantly (e.g., beyond 32)? The current system relies on constrained optimization that struggles with combinatorial complexity of dense scenes, leading to lower functionality and layout scores in the 32-asset benchmark.

### Open Question 2
Can the current placement paradigm be successfully extended to support complex physical interactions such as hanging, draping, or nesting? The existing arrangement optimization and "drop operator" are designed for rigid placement on planar surfaces, lacking the physics simulation or constraint logic required for non-rigid or suspended object interactions.

### Open Question 3
How can surface extraction and asset placement be generalized to handle highly irregular or rugged surface geometries? The current heuristic uses a fixed 2cm height tolerance that likely fails on complex curves or discontinuities, causing placement errors.

### Open Question 4
Does implementing an adaptive asset selection method based on available surface area successfully balance functionality and minimalism? The current system relies on user-specified integer ($N_{assets}$) which ignores actual available space, often leading to overcrowding or inefficient space usage.

## Limitations
- Formulation gaps: Full constrained optimization equations and surface extraction heuristics are not fully specified, limiting exact reproduction
- Evaluation context: All metrics are relative to paper's baselines; no independent external validation provided
- Scalability limits: Performance with highly complex scenes (32+ assets), non-planar surfaces, or intricate operations (hanging, draping) is not explored

## Confidence

- **Task-Specialized Agent Decomposition**: High confidence. Direct evidence from Table 3 shows performance degradation when merging agents.
- **Constrained Optimization for Physical Plausibility**: High confidence. Zero OOB/BBL rates are directly demonstrated and supported by formulation details.
- **Hybrid LLM/Non-LLM Division of Labor**: Medium confidence. Supported by qualitative reasoning but limited quantitative evidence of LLM weakness.

## Next Checks

1. **Agent Specialization Ablation**: Replicate Table 3's agent consolidation study (7-agent vs. 9-agent) on a small furniture set to confirm full specialization yields claimed quality improvements.

2. **Optimizer Feasibility Stress Test**: Run the Arranger with increasing asset counts (8, 16, 24, 32) on a single furniture item, logging solver feasibility, runtime, and constraint violations to identify scalability limits.

3. **Validator Failure Analysis**: Instrument the pipeline to log per-stage rejection reasons (JSON schema, asset count, bounding box exceedance) over 50 decoration tasks to quantify LLM failure modes and inform prompt/schema refinements.