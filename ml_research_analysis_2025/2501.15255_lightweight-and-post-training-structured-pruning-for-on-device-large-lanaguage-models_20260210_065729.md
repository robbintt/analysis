---
ver: rpa2
title: Lightweight and Post-Training Structured Pruning for On-Device Large Lanaguage
  Models
arxiv_id: '2501.15255'
source_url: https://arxiv.org/abs/2501.15255
tags:
- pruning
- layer
- comp
- layers
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of reducing resource demands of
  large language models (LLMs) for deployment on resource-constrained devices through
  structured pruning. The proposed method, COMP, is a lightweight post-training structured
  pruning technique that employs a hybrid-granularity pruning strategy combining layer-grained
  and neuron-grained pruning.
---

# Lightweight and Post-Training Structured Pruning for On-Device Large Lanaguage Models

## Quick Facts
- arXiv ID: 2501.15255
- Source URL: https://arxiv.org/abs/2501.15255
- Authors: Zihuai Xu; Yang Xu; Hongli Xu; Yunming Liao; Zhiwei Yao; Zuan Xie
- Reference count: 40
- Primary result: 6.13% perplexity improvement on LLaMA-2-7B with 20% pruning vs. LLM-Pruner, 80% memory overhead reduction

## Executive Summary
This paper introduces COMP, a lightweight post-training structured pruning method for large language models designed for resource-constrained devices. COMP employs a hybrid-granularity approach that combines layer-grained pruning (removing entire layers) with neuron-grained pruning (removing individual neurons within remaining layers). The method uses a novel mask-tuning technique that recovers accuracy without fine-tuning, significantly reducing memory consumption while maintaining model performance. Experiments demonstrate COMP's effectiveness across multiple LLM architectures including LLaMA-2, OPT, and ChatGLM3, achieving better perplexity scores with lower memory requirements compared to existing pruning methods.

## Method Summary
COMP is a post-training structured pruning method that performs hybrid-granularity pruning through three main phases. First, it computes layer importance using input-output cosine similarity (redundancy) and iteratively removes the least important layers. Second, it calculates per-layer pruning ratios based on the remaining target ratio. Third, for each remaining layer, it evaluates neuron importance using a matrix condition-number-based metric derived from the mask-tuning least-squares problem, then iteratively prunes neurons while tuning a mask to minimize output discrepancy. The method uses original-model inputs during pruning to avoid cumulative deviation and requires only 8GB of memory for LLaMA-2-7B pruning. Mask tuning solves a least-squares problem to scale remaining neurons, enabling accuracy recovery without fine-tuning.

## Key Results
- COMP improves performance by 6.13% on LLaMA-2-7B with 20% pruning ratio compared to LLM-Pruner
- Memory consumption reduced by 80% compared to baseline methods
- Only 8GB of memory required to prune LLaMA-2-7B (vs. 32GB for baselines)
- Achieves lowest perplexity across LLaMA-2-7B, OPT-6.7B, and ChatGLM3-6B at 20-30% pruning ratios
- Demonstrates broad applicability across different LLM architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A hybrid-granularity pruning strategy combining layer removal with fine-grained neuron pruning achieves lower perplexity than single-granularity approaches at comparable pruning ratios.
- **Mechanism:** Layer importance is computed from input-output cosine similarity (redundancy); low-importance layers are removed iteratively. Remaining layers receive per-layer pruning ratios, then neurons are ranked by a condition-number-based metric derived from the mask-tuning least-squares problem. Iterative pruning stops when mask variance exceeds a threshold.
- **Core assumption:** Layers with higher input-output redundancy contribute less to task performance, and neurons that minimally destabilize the least-squares reconstruction matrix can be safely removed.
- **Evidence anchors:**
  - [abstract] "COMP initially prunes selected model layers based on their importance at a coarse granularity, followed by fine-grained neuron pruning within the dense layers of each remaining model layer."
  - [section III-B, Eq. 3] "the importance of the l-th layer is defined as: Il = 1 − Et[(X^l_t)^T X^{l+1}_t / (||X^l_t||_2 ||X^{l+1}_t||_2)]"
  - [section IV-C, Figure 2] Shows hybrid pruning achieves lowest perplexity across LLaMA-2-7B, OPT-6.7B, and ChatGLM3-6B at 20–30% pruning ratios.
- **Break condition:** If early/late layers have low redundancy but are functionally critical, removing them based solely on similarity could cause outsized performance loss.

### Mechanism 2
- **Claim:** Mask tuning recovers accuracy without fine-tuning by solving a least-squares problem to minimize output discrepancy before and after pruning.
- **Mechanism:** After neuron removal, a mask is tuned to scale remaining neurons so that the pruned dense approximates the original output. The variance of mask values constrains pruning depth per dense.
- **Core assumption:** A well-conditioned coefficient matrix in the least-squares problem yields stable solutions, and mask variance correlates with generalization.
- **Evidence anchors:**
  - [abstract] "COMP utilizes mask tuning to recover accuracy without the need for fine-tuning, significantly reducing memory consumption."
  - [section III-A, Eq. 2] "Mask tuning reconstructs the original dense's output using unpruned neurons. It reduces the discrepancy in dense outputs before and after pruning."
  - [corpus] Neighbor papers do not directly validate mask tuning; this mechanism is primarily supported by the paper's own ablations.
- **Break condition:** If calibration data is small or unrepresentative, mask tuning may overfit, harming generalization to unseen inputs.

### Mechanism 3
- **Claim:** Memory consumption is minimized by dynamically loading layers one at a time and using original-model inputs during neuron pruning to prevent cumulative error.
- **Mechanism:** Only one layer is resident in memory during importance evaluation. Neuron pruning uses the original model's activations rather than propagating outputs from already-pruned layers, absorbing pruning impact locally.
- **Core assumption:** Layer-wise pruning effects can be approximated independently without forward-propagating changes during the pruning phase.
- **Evidence anchors:**
  - [abstract] "The method is memory-friendly, requiring only 8GB to prune LLaMA-2-7B."
  - [section IV-B, "Cumulative Deviation"] "we perform pruning a layer using the input from the original model."
  - [section V-B, Figure 4] "COMP only needs 8GB of memory to prune LLaMA-2-7B and 11GB of memory to prune LLaMA-2-13B."
- **Break condition:** If layers have strong dependencies, this isolation may underestimate compounding errors from pruning multiple upstream layers.

## Foundational Learning

- **Concept: Layer redundancy (cosine similarity)**
  - Why needed here: Determines which entire layers to prune; redundancy = cosine_similarity(layer_input, layer_output), importance = 1 - redundancy.
  - Quick check question: If a layer's output is nearly identical to its input (cosine similarity ≈ 1), should its importance score be high or low?

- **Concept: Matrix condition number**
  - Why needed here: Measures sensitivity of the least-squares mask-tuning solution; neurons whose removal increases condition number are more important.
  - Quick check question: Does a high condition number indicate a more stable or less stable linear system for solving?

- **Concept: Mask tuning via least squares**
  - Why needed here: Recovers accuracy post-pruning without fine-tuning; scales remaining neurons to reconstruct original outputs.
  - Quick check question: After pruning neurons, what quantity does mask tuning minimize?

## Architecture Onboarding

- **Component map:** Layer importance calculator -> Layer pruning module -> Neuron importance evaluator -> Iterative neuron pruner -> Mask tuner -> Variance threshold controller

- **Critical path:**
  1. Run calibration data forward, dynamically loading one layer at a time.
  2. Compute layer importance; remove n least-important layers iteratively.
  3. Compute per-layer pruning ratio from global target.
  4. For each remaining layer: evaluate neuron importance; iteratively prune while tuning mask; stop when mask variance ≥ v_T.
  5. If target pruning ratio not met, increase v_T and re-iterate.

- **Design tradeoffs:**
  - Higher pruning ratio vs. accuracy – mask tuning compensates but has limits.
  - Variance threshold v_T – lower = safer but less compression; higher = more aggressive but risk instability.
  - Calibration data size – paper uses 10 samples; more may improve robustness at cost of runtime.
  - Iterative vs. one-shot – iterative is slower but more accurate.

- **Failure signatures:**
  - Exploding perplexity after removing many layers (see Figure 2(b) for OPT-6.7B).
  - High mask variance indicating over-pruning within a dense.
  - Memory overflow when dense input dimension is large (e.g., OPT-6.7B requires 13GB vs. 8GB for LLaMA-2-7B).
  - Poor downstream-task generalization – possible mask overfit to calibration data.

- **First 3 experiments:**
  1. Replicate Figure 2: compare layer-only, neuron-only, and hybrid pruning on WikiText2 perplexity for your model.
  2. Replicate Figure 5: ablate iterative vs. one-shot layer importance calculation when removing >3 layers.
  3. Replicate Table III: compare using identical (original-model) inputs vs. propagated inputs during neuron pruning to validate reduced cumulative deviation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the computational efficiency of the iterative neuron pruning and mask tuning loop be improved to reduce the high latency identified in the method?
- **Basis in paper:** [explicit] The Conclusion states that the iterative process to determine the number of neurons to prune is "time-consuming," taking approximately 30 minutes for LLaMA-2-7B and 1 hour for LLaMA-2-13B.
- **Why unresolved:** The current algorithm must iterate through pruning and mask adjustment steps to find variance thresholds, creating a bottleneck despite the low memory usage.
- **What evidence would resolve it:** A modified algorithm that reduces the number of iterations or time per iteration while maintaining the same perplexity and zero-shot accuracy.

### Open Question 2
- **Question:** To what extent does the assumption of input neuron independence affect the accuracy of the importance metric in layers with highly correlated activations?
- **Basis in paper:** [inferred] Section III-B explicitly assumes input neurons are independent to approximate the Fisher information matrix as diagonal, a simplification made for computational feasibility without empirical validation of its error.
- **Why unresolved:** If neurons in certain layers are highly correlated, the diagonal assumption may misestimate importance, leading to the pruning of critical features.
- **What evidence would resolve it:** A comparative analysis between the diagonal approximation and a full-matrix calculation on layers with high correlation, measuring the difference in resulting model perplexity.

### Open Question 3
- **Question:** How sensitive is the matrix condition-based metric to the specific choice and diversity of the 10 calibration samples used?
- **Basis in paper:** [inferred] The experiments utilize only 10 randomly selected samples from C4. While the authors note SliceGPT fails with limited data, they do not analyze if COMP's specific matrix metrics overfit to such a small sample size.
- **Why unresolved:** With minimal data, the input statistics used to evaluate neuron importance may not represent the general data distribution, potentially yielding unstable masks.
- **What evidence would resolve it:** Experiments showing the variance in pruning outcomes (perplexity/accuracy) when using different random seeds or quantities of calibration data.

## Limitations
- Ablation studies focus on single pruning ratio (20%) rather than systematically varying ratios across different compression levels
- Memory efficiency claims lack detailed profiling throughout pruning process to verify sustained low memory usage
- Evaluation only considers encoder-decoder models without testing decoder-only architectures like GPT
- Mask tuning mechanism lacks comparison against alternative post-training recovery methods

## Confidence
- **High Confidence:** The hybrid-granularity pruning strategy (layer + neuron) outperforms single-granularity approaches in terms of perplexity reduction.
- **Medium Confidence:** The memory efficiency claim (8GB for LLaMA-2-7B) is credible but lacks detailed memory profiling throughout the pruning process.
- **Medium Confidence:** Mask tuning effectively recovers accuracy without fine-tuning, but generalizability is uncertain without comparison to alternative recovery methods.

## Next Checks
1. **Ablation of Calibration Data Size:** Systematically vary the number of calibration samples (1, 5, 10, 50, 100) to quantify the impact on pruning quality and determine the minimum viable dataset size for stable condition matrix estimation.

2. **Cross-Model Architecture Testing:** Apply COMP to a decoder-only model (e.g., GPT-2 or LLaMA-2-decoder-only) to validate whether the layer redundancy assumptions hold across different architectural paradigms.

3. **Memory Profiling Throughout Pruning:** Implement detailed memory usage tracking during all phases of COMP (layer importance computation, mask tuning, neuron pruning) to verify sustained 8GB usage and identify potential memory bottlenecks not apparent from the high-level description.