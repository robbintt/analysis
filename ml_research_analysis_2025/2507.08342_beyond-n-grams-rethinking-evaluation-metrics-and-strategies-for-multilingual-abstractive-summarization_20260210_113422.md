---
ver: rpa2
title: 'Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual
  Abstractive Summarization'
arxiv_id: '2507.08342'
source_url: https://arxiv.org/abs/2507.08342
tags:
- metrics
- languages
- evaluation
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the challenge of evaluating multilingual\
  \ abstractive summarization, particularly for languages beyond English, where traditional\
  \ n-gram-based metrics like ROUGE often fail to correlate well with human judgments\
  \ due to linguistic differences such as morphological complexity. The authors systematically\
  \ assess both n-gram-based and neural-based evaluation metrics across eight typologically\
  \ diverse languages\u2014covering isolating, agglutinative, low-fusional, and high-fusional\
  \ types, with both low- and high-resource languages."
---

# Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization

## Quick Facts
- arXiv ID: 2507.08342
- Source URL: https://arxiv.org/abs/2507.08342
- Reference count: 40
- This study systematically evaluates n-gram and neural metrics for multilingual abstractive summarization, revealing that n-gram metrics fail for fusional languages but can be improved via proper tokenization, while neural metrics (especially COMET) consistently outperform others and correlate best with human judgments.

## Executive Summary
This study systematically evaluates automatic evaluation metrics for multilingual abstractive summarization across eight typologically diverse languages, addressing the challenge that traditional n-gram metrics like ROUGE often fail to correlate with human judgments for morphologically rich languages. The authors collected ~20,000 human annotations ranking generated summaries on coherence and completeness, and evaluated both n-gram-based metrics (with various tokenization strategies) and neural-based metrics including BERTScore, MoverScore, COMET, and LLM-as-a-judge. Key findings reveal that n-gram metrics show poor or even negative correlations for fusional languages, but proper tokenization (e.g., using language-specific or mBERT tokenizers) can significantly improve performance. Neural-based metrics, especially COMET—which is specifically trained for evaluation—consistently outperform others and correlate best with human judgments, particularly in low-resource settings. The study concludes that n-gram metrics are insufficient for fusional languages and advocates for greater use of neural metrics trained for evaluation tasks.

## Method Summary
The study evaluates automatic metrics on XL-Sum dataset (7 languages) and HeSum dataset (Hebrew), generating 800 summaries per language using GPT-3.5-Turbo and Gemini 1.0 Pro. They collected ~20,000 human annotations from MTurk workers ranking summaries on coherence and completeness using a 4-point Likert scale. To ensure statistical power, they corrupted 1/3 of generated summaries using targeted degradation strategies. The evaluation compares 10 metrics including ROUGE variants with different tokenizers, BERTScore (mBERT and monolingual), MoverScore, COMET (wmt22-comet-da), and Gemini-as-a-judge. Correlations are computed using Pearson correlation coefficient with significance testing across all language-resource combinations.

## Key Results
- N-gram metrics show poor or negative correlation with human judgments for fusional languages (e.g., ROUGE-L shows -0.23 correlation for high-fusional languages on coherence)
- Proper tokenization using mBERT or language-specific tokenizers significantly improves n-gram metric performance for fusional languages (ROUGE-L improves to +0.08 with mBERT tokenization)
- COMET, a neural metric specifically trained for evaluation, consistently outperforms other metrics and shows the highest correlation with human judgments (0.32 for high-resource coherence, 0.24 for low-resource completeness)
- Neural metrics generally outperform n-gram metrics, especially for low-resource languages
- LLM-as-a-judge (Gemini) showed the lowest correlations (0.03-0.19) across all conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** N-gram metrics exhibit degraded or negative correlation with human judgment for morphologically rich fusional languages, but this can be partially mitigated through subword tokenization strategies.
- **Mechanism:** Fusional languages embed multiple grammatical features within single surface forms through morpheme fusion. Standard whitespace-based n-gram matching fails to recognize semantically equivalent content expressed through different inflectional variants, producing spurious dissimilarity scores that anti-correlate with human semantic judgments.
- **Core assumption:** The observed negative correlations stem primarily from morphological surface-form mismatches rather than other linguistic factors or annotation noise.
- **Evidence anchors:** Abstract shows lower correlation for fusional languages; Table 5 shows ROUGE-L at -0.23 for high-fusional languages, improving to +0.08 with mBERT tokenization; limited corpus evidence on morphological metric degradation.
- **Break condition:** If tokenization adaptation fails to reverse negative correlations, the mechanism may be confounded by tokenizer vocabulary gaps, fundamental semantic drift in generated summaries, or human annotation disagreement.

### Mechanism 2
- **Claim:** Neural evaluation metrics trained explicitly on human quality judgments (COMET) outperform both n-gram and untrained neural metrics in multilingual summarization evaluation.
- **Mechanism:** COMET uses XLM-R encoder representations fine-tuned via regression objective against human-annotated quality scores from machine translation evaluation. This task-specific training teaches the model to map from cross-lingual semantic representations to quality predictions, enabling it to recognize paraphrase and semantic preservation patterns that surface-level n-gram overlap cannot capture.
- **Core assumption:** COMET's MT evaluation training transfers to summarization evaluation because both tasks involve comparing generated output against reference text.
- **Evidence anchors:** Abstract states COMET "consistently outperform others and correlate best with human judgments"; Table 6 shows COMET achieving 0.32 correlation for high-resource coherence; neighboring work corroborates multilingual neural metrics can generalize.
- **Break condition:** If COMET underperforms, check whether source input was incorrectly included, whether XLM-R has insufficient pre-training data for the target language, or whether the quality dimension differs from those captured in MT training data.

### Mechanism 3
- **Claim:** Language-specific BERT models improve correlation over multilingual BERT for monolingual evaluation, suggesting encoder specialization matters more than model scale for lower-resource languages.
- **Mechanism:** Monolingual BERT models trained on larger proportions of target-language data develop more nuanced representations of language-specific morphological patterns, lexical semantics, and syntactic structures than multilingual models where capacity is distributed across 100+ languages.
- **Core assumption:** Correlation improvements stem from encoder quality rather than evaluation methodology differences, though confounds exist (different model sizes, training data composition).
- **Evidence anchors:** Section 6 states "dedicated monolingual model improves correlation more than training on larger, non-specific datasets"; Table 6 shows BERTScore (Monolingual) at 0.27 correlation vs. 0.23 for mBERT on high-resource coherence; no direct corpus evidence on encoder comparison.
- **Break condition:** If monolingual models underperform, verify whether the language-specific model was actually trained on sufficient data, whether tokenizer vocabulary aligns with evaluation text, or whether the comparison multilingual model has disproportionately more parameters.

## Foundational Learning

- **Concept: Morphological typology (isolating, agglutinative, fusional)**
  - **Why needed here:** The paper's central finding—that metric performance varies systematically by morphological type—requires understanding why fusional languages challenge surface-form matching. Without this, you cannot diagnose metric failures or select appropriate tokenization strategies.
  - **Quick check question:** Given the Spanish word "habló" (he/she spoke), can you identify which grammatical features are fused in this single word form and predict whether whitespace-delimited ROUGE would match it against "hablar" (to speak)?

- **Concept: Reference-based vs. reference-free evaluation**
  - **Why needed here:** The paper evaluates metrics that compare generated summaries against human references. Understanding this paradigm is prerequisite to interpreting correlation results and understanding why metrics might fail when references are stylistically dissimilar from model outputs.
  - **Quick check question:** If a generated summary is factually correct but uses entirely different vocabulary than the reference summary, would you expect ROUGE scores to be high or low? How might COMET behave differently?

- **Concept: Correlation coefficient interpretation for evaluation metrics**
  - **Why needed here:** The paper reports Pearson correlations between automatic metrics and human judgments; understanding what constitutes "good" correlation (e.g., 0.3 vs. 0.5) and what negative correlation implies is essential for interpreting results tables.
  - **Quick check question:** If ROUGE-1 shows correlation of -0.25 with human coherence judgments for Hebrew summaries, what does this mean about the relationship between ROUGE scores and perceived quality?

## Architecture Onboarding

- **Component map:** Input Data → XL-Sum/HeSum datasets (article, reference summary) → Generation Module → GPT-3.5-Turbo, Gemini 1.0 Pro → Generated summaries → Corruption Layer → Artificial degradation → Human Annotation → MTurk workers → Coherence/Completeness scores → Metric Computation → ROUGE variants / BERTScore / COMET / Gemini-as-judge → Correlation Analysis → Pearson correlation with significance testing

- **Critical path:**
  1. **Tokenizer selection** is the highest-leverage decision for n-gram metrics on fusional languages—implement language-specific or mBERT tokenization before computing ROUGE
  2. **Metric selection** by resource level: use COMET for low-resource languages regardless of typology; neural metrics generally outperform n-gram
  3. **Sample size**: Ensure ≥400 samples per language for statistically significant correlation detection at p ≤ 0.05

- **Design tradeoffs:**
  - N-gram metrics: Fast, interpretable, but fail on fusional languages without tokenization adaptation
  - Neural metrics (untrained): Better semantic matching, but depend on encoder quality for target language
  - Neural metrics (trained): Best correlation, but require human-annotated training data; COMET adapted from MT may not capture summarization-specific quality dimensions
  - LLM-as-a-judge: Most flexible, but showed lowest correlations in this study (0.03-0.19 across conditions)

- **Failure signatures:**
  - Negative correlation with human judgments → indicates metric is ranking summaries inversely to quality; check for tokenization mismatch (whitespace on fusional languages) or encoder vocabulary gaps
  - Near-zero correlation with high variance → metric insensitive to quality differences; may need corruption/augmentation to create score dispersion
  - High correlation but low absolute scores → metric may be capturing reference overlap rather than quality; validate against human scores distribution
  - Large gap between coherence and completeness correlations → metric may be biased toward one quality dimension; consider composite scoring

- **First 3 experiments:**
  1. **Baseline establishment:** Run standard ROUGE-1/2/L with whitespace tokenization on your target language; compute correlation with available human judgments to establish failure mode (expected: negative/low correlation for fusional languages)
  2. **Tokenization ablation:** Re-run ROUGE variants with (a) mBERT tokenizer, (b) language-specific tokenizer, (c) lemmatization preprocessing; measure correlation improvement to quantify tokenization effect magnitude
  3. **Neural metric comparison:** Run COMET (wmt22-comet-da), BERTScore with mBERT, and BERTScore with monolingual encoder; compare correlations to determine whether trained evaluation metrics justify computational overhead for your use case

## Open Questions the Paper Calls Out

- **How reliable are human and automatic metrics on fine-grained quality aspects (fluency, coherence, consistency, relevance) compared to the coarse-grained criteria used here?** The authors abandoned the four-criteria schema due to low inter-annotator agreement and state this question is left for future research. A study establishing high inter-annotator agreement on the four-aspect schema and measuring the correlation of automatic metrics against these fine-grained labels would resolve this.

- **Can a neural metric trained specifically for summarization outperform COMET, which is trained on machine translation data?** The conclusion hypothesizes that a "metric trained specifically for summarization evaluation could perform even better" than the MT-trained COMET. Training a metric explicitly on multilingual summarization human judgments and comparing its correlation scores against COMET would resolve this.

- **Does the inferior performance of n-gram metrics and LLM-as-a-judge persist across other generative tasks beyond abstractive summarization?** While the introduction frames the problem around general "generative tasks," the experimental scope is restricted to summarization. Replicating the evaluation framework on non-summarization multilingual generation tasks would resolve this.

## Limitations

- The paper's conclusions about neural metrics' superiority rest on COMET's adaptation from MT evaluation without fine-tuning on summarization-specific data, leaving the transfer assumption untested.
- Human annotation reliability may be compromised for low-resource languages due to lack of systematic verification of MTurk annotator language proficiency.
- The study relies on a simplified two-criterion annotation schema rather than the standard four-aspect quality framework due to annotator agreement issues.

## Confidence

- **High confidence:** N-gram metrics show degraded performance for fusional languages, and tokenization adaptation improves correlations. This conclusion is supported by multiple languages and consistent patterns across coherence and completeness judgments.
- **Medium confidence:** COMET's consistent outperformance of untrained neural metrics generalizes across typological families. While results are robust, the transfer from MT evaluation to summarization remains an untested assumption.
- **Low confidence:** The claim that LLM-as-a-judge is unsuitable for multilingual evaluation. With correlations of only 0.03-0.19, this finding is based on a single evaluation run and may reflect prompt engineering limitations rather than fundamental model incapacity.

## Next Checks

1. **Cross-lingual generalization test:** Evaluate COMET on summarization datasets where human quality judgments were collected specifically for summarization (not adapted from MT). Compare performance against summarization-trained variants to isolate transfer effects.

2. **Annotator proficiency verification:** For each low-resource language (Japanese, Chinese, Yoruba), conduct post-hoc assessment of MTurk annotator language proficiency using standardized tests. Recompute inter-annotator agreement stratified by proficiency level to identify potential annotation quality confounds.

3. **Correlation directionality analysis:** For fusional languages showing negative ROUGE correlations, conduct error analysis to determine whether metrics are capturing: (a) semantic drift in generated summaries, (b) reference-summary stylistic mismatch, or (c) annotation noise. This will clarify whether negative correlations reflect metric failure or genuine quality-perception divergence.