---
ver: rpa2
title: Enhancing Math Learning in an LMS Using AI-Driven Question Recommendations
arxiv_id: '2504.14098'
source_url: https://arxiv.org/abs/2504.14098
tags:
- question
- similarity
- learning
- each
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates three AI-driven recommendation methods\u2014\
  cosine similarity, Self-Organizing Maps (SOM), and Gaussian Mixture Models (GMM)\u2014\
  for suggesting similar math questions in an LMS. Embeddings are generated using\
  \ Meta\u2019s Llama-3.2-11B-Vision-Instruct model."
---

# Enhancing Math Learning in an LMS Using AI-Driven Question Recommendations

## Quick Facts
- arXiv ID: 2504.14098
- Source URL: https://arxiv.org/abs/2504.14098
- Reference count: 1
- Self-organizing maps (SOM) yield higher user satisfaction, while Gaussian mixture models (GMM) underperform for math question recommendations in an LMS.

## Executive Summary
This study evaluates three AI-driven recommendation methods—cosine similarity, Self-Organizing Maps (SOM), and Gaussian Mixture Models (GMM)—for suggesting similar math questions in an LMS. Embeddings are generated using Meta’s Llama-3.2-11B-Vision-Instruct model. Results show that cosine similarity produces highly similar but repetitive recommendations, SOM balances similarity and variety effectively, and GMM underperforms due to KL divergence not capturing meaningful similarity. SOM yields higher user satisfaction, lower wrong-answer streaks, and better engagement, while GMM leads to shorter sessions and lower ratings. The study underscores the importance of matching representation and similarity metrics to learning contexts and suggests future exploration of hybrid or refined GMM approaches.

## Method Summary
The study compares three recommendation methods using embeddings from Llama-3.2-11B-Vision-Instruct (4096-dim via mean pooling). Cosine similarity directly compares embeddings; SOM uses a 5×8 grid trained for 1000 epochs with learning rate decay to cluster and recommend similar questions; GMM fits per-subject models and ranks by KL divergence between posterior probability vectors. A/B tests randomize algorithm assignment, logging session duration, correctness, ratings, and streaks. Outlier filtering removes sessions <5s or outside 5th–95th percentile.

## Key Results
- SOM achieves highest user satisfaction (mean rating 4.17) compared to cosine (4.05) and GMM (3.96).
- GMM produces shortest sessions (20.18 min) and lowest ratings, indicating poor pedagogical alignment.
- Cosine similarity leads to repetitive recommendations, potentially causing learner fatigue despite high initial correctness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal embeddings from Llama-3.2-11B-Vision-Instruct capture both textual and visual semantics of math questions in a unified 4096-dimensional space.
- Mechanism: Tokenize text (and optional images) → pass through transformer → extract final hidden states → mean pooling across all tokens → single dense vector per question. Mean pooling aggregates distributed contextual features rather than relying on any single token representation.
- Core assumption: Visual elements (diagrams, equations) and text can be meaningfully fused in one embedding space for math problem similarity.
- Evidence anchors:
  - [abstract]: "Deep embeddings for math questions are generated using Meta's Llama-3.2-11B-Vision-Instruct model"
  - [section 2.1]: "Mean pooling provides a balanced summary that reflects contributions from the entire input sequence"
  - [corpus]: DrawEduMath (FMR=0.606) evaluates VLMs on student math images—supports multimodal viability but does not validate this specific pooling approach.
- Break condition: Complex equations or diagrams that the vision encoder cannot parse accurately yield noisy embeddings; no explicit failure threshold provided.

### Mechanism 2
- Claim: SOM-based recommendation balances conceptual similarity with controlled variety by projecting embeddings to a 2D grid that preserves topology, then selecting nearest neighbors within grid cells.
- Mechanism: Train SOM (5×8 grid, 1000 epochs, linearly decaying learning rate from 0.5) → identify Best Matching Unit (BMU) for query embedding → compute intracluster Euclidean distance → return top-N within that neuron.
- Core assumption: A moderate 5×8 grid captures meaningful semantic neighborhoods without fragmenting into sparse micro-clusters.
- Evidence anchors:
  - [abstract]: "SOM yields higher user satisfaction whereas GMM generally underperforms"
  - [section 3.1, Table 3]: SOM mean rating = 4.17 vs. cosine 4.05 and GMM 3.96
  - [corpus]: No direct corpus evidence for SOM in math education recommendations.
- Break condition: Grid too small (underfit, blurring distinctions) or too large (overfit, sparse clusters with inactive neurons).

### Mechanism 3
- Claim: GMM with KL divergence between posterior probability vectors underperforms for recommendation because KL divergence does not align with pedagogically meaningful similarity.
- Mechanism: Train subject-specific GMMs (components via BIC/silhouette; full covariance; k-means init) → compute per-question posterior probability vectors → rank by KL divergence to query vector.
- Core assumption: Questions with similar probability profiles across Gaussian components are pedagogically similar.
- Evidence anchors:
  - [abstract]: "GMM generally underperforms, indicating that introducing variety...until variety is no longer balanced reasonably"
  - [section 3.1, Table 1]: GMM mean session duration = 20.18 min (lowest); Table 3: lowest mean rating = 3.96
  - [corpus]: No corpus papers validate or refute GMM-KL for educational recommendations.
- Break condition: KL divergence is highly sensitive to small probability differences; may not correlate with perceived question similarity (not proven, inferred from underperformance).

## Foundational Learning

- Concept: Self-Organizing Maps (SOM)
  - Why needed here: SOM is the best-performing method in this study; requires understanding competitive learning, neighborhood functions, topological preservation, and BMU selection.
  - Quick check question: Why would a 5×8 grid be preferable to a 2×2 or 20×20 grid for this math question dataset?

- Concept: Mean Pooling vs. Alternative Aggregation
  - Why needed here: Embedding aggregation strategy directly affects similarity quality; the paper argues mean pooling is more robust than max pooling or single-token approaches.
  - Quick check question: What is one drawback of max pooling that mean pooling avoids for sentence-level representations?

- Concept: KL Divergence Properties
  - Why needed here: Understanding why GMM failed requires grasping KL divergence's asymmetry and sensitivity to small distributional differences.
  - Quick check question: Why might KL divergence between two probability vectors not reflect whether two math questions feel "similar" to a learner?

## Architecture Onboarding

- Component map:
  Embedding Service -> Tokenizer -> Model Inference -> Mean Pooling -> 4096-dim vectors
  Embedding vectors -> Cosine/SOM/GMM Engines -> Ranked Recommendations
  LMS Integration Layer -> Session Logging -> User Ratings

- Critical path:
  1. Question intake → text cleaning → tokenize (+ optional image)
  2. Embedding generation → mean pooling → store/index vector
  3. Session start → randomized algorithm assignment (A/B)
  4. Subject routing → load correct SOM/GMM model
  5. Similarity computation → rank candidates → return top-N
  6. Interaction logging → timestamps, correctness, duration, optional rating

- Design tradeoffs:
  - Grid size (5×8) balances resolution vs. overfitting; larger grids risk sparse clusters.
  - Subject-specific models add complexity but prevent cross-domain contamination in derived statistics (esp. GMM posteriors).
  - KL divergence vs. direct distance: probabilistic profiles capture latent structure but KL proved misaligned with perceived similarity.

- Failure signatures:
  - GMM path: short sessions, low question counts, low ratings → check if KL divergence is returning semantically unrelated items.
  - Cosine path: high correctness but potential repetition fatigue → monitor for declining session length over repeated use.
  - Outlier sessions > 10,000 minutes → user forgot to end session; filter at 5th–95th percentile for analysis.

- First 3 experiments:
  1. Replicate SOM configuration (5×8, 1000 epochs, lr=0.5 decay) on a sample of your math questions; visualize BMU assignments with t-SNE to validate cluster coherence.
  2. A/B test mean pooling vs. [CLS]-token embeddings on a held-out similarity judgment task (human labels) to quantify pooling impact.
  3. Isolate the GMM failure mode: keep GMM posteriors but replace KL divergence with Jensen–Shannon divergence; measure whether session duration and ratings improve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a Gaussian Mixture Model (GMM) approach outperform Self-Organizing Maps (SOM) if similarity is measured using Jensen–Shannon divergence or intra-cluster Euclidean distance rather than Kullback–Leibler (KL) divergence?
- Basis in paper: [explicit] The authors explicitly propose "Refining the GMM approach" by either replacing KL divergence with "a more suitable distributional metric (e.g., Jensen–Shannon divergence)" or reverting to a "True Clustering Paradigm" using direct distance measures.
- Why unresolved: The current GMM implementation underperformed because KL divergence was overly sensitive to small probability differences and failed to capture pedagogical similarity.
- What evidence would resolve it: A comparative study showing that a refined GMM yields higher user ratings and longer session durations than the current SOM implementation.

### Open Question 2
- Question: Do SOM-based recommendations result in measurable long-term knowledge retention compared to the repetitive nature of cosine similarity?
- Basis in paper: [explicit] The paper states that metrics like session duration "do not directly measure long-term knowledge gains" and suggests "future work should incorporate direct assessments of learning outcomes, such as pre- and post-tests."
- Why unresolved: This study relied on engagement proxies (duration, ratings) rather than direct assessment of skill acquisition or retention.
- What evidence would resolve it: A controlled experiment using pre-tests and post-tests to quantify learning gains for users exposed to SOM versus cosine similarity recommendations.

### Open Question 3
- Question: Does a hybrid model combining cosine similarity and SOM produce higher engagement than either method in isolation?
- Basis in paper: [explicit] The authors suggest that "exploring hybrid models that combine the precision of cosine similarity with the diversity of SOM... remains an attractive direction."
- Why unresolved: The study evaluated three methods independently; it did not test whether combining precision (cosine) and novelty (SOM) optimizes the trade-off between relevance and variety.
- What evidence would resolve it: Evaluation data showing that a hybrid recommender maintains higher average session lengths and satisfaction ratings than the standalone SOM algorithm.

## Limitations

- Embedding extraction methodology is only generically described as "mean pooling" without specifying layer index, batch size, or handling of multimodal inputs, which could affect reproducibility.
- No validation against human-labeled similarity judgments is provided for the embeddings themselves, leaving open the possibility that the VLMs produce embeddings that poorly capture pedagogical similarity despite outperforming other methods.
- The study does not provide detailed information about the number of participants or experiment duration needed to achieve 1855 sessions, making it difficult to assess statistical power or potential selection bias in the user sample.

## Confidence

- High confidence: Cosine similarity underperforms due to lack of variety; SOM yields higher user satisfaction (mean rating 4.17 vs 4.05 vs 3.96); GMM produces shortest sessions and lowest ratings.
- Medium confidence: Mean pooling from Llama-3.2-11B-Vision-Instruct adequately captures multimodal math question semantics; KL divergence is poorly suited for educational similarity despite being mathematically valid.
- Low confidence: The specific grid size (5×8) is optimal; GMM's underperformance is solely due to KL divergence choice rather than model misspecification or initialization issues.

## Next Checks

1. Conduct a controlled human similarity judgment study to verify that the VLMs' mean-pooled embeddings actually reflect pedagogically meaningful similarity, not just statistical proximity.
2. Systematically vary SOM grid dimensions (e.g., 3×4, 8×10, 10×10) and training parameters, measuring both cluster coherence (via silhouette scores) and recommendation quality metrics to identify optimal topology.
3. Replace KL divergence in GMM with Jensen-Shannon divergence and re-run A/B tests to isolate whether the probabilistic similarity measure or the GMM framework itself is responsible for poor performance.