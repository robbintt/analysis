---
ver: rpa2
title: 'Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes'
arxiv_id: '2402.05406'
source_url: https://arxiv.org/abs/2402.05406
tags:
- pruning
- bonsai
- arxiv
- structured
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Bonsai introduces a gradient-free structured pruning method for\
  \ LLMs that uses only forward passes. It formulates module importance estimation\
  \ as an underdetermined regression problem, sampling n\u226AN sub-models guided\
  \ by informative priors and regressing to estimate global importance scores."
---

# Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes

## Quick Facts
- arXiv ID: 2402.05406
- Source URL: https://arxiv.org/abs/2402.05406
- Authors: Steven Kolawole; Lucio Dery; Jean-François Kagy; Virginia Smith; Graham Neubig; Ameet Talwalkar
- Reference count: 30
- Primary result: Achieves 2-3× memory reduction vs gradient-based pruning while maintaining SOTA perplexity on 7B/8B models

## Executive Summary
Bonsai introduces a gradient-free structured pruning method for LLMs that uses only forward passes. It formulates module importance estimation as an underdetermined regression problem, sampling n≪N sub-models guided by informative priors and regressing to estimate global importance scores. This eliminates backpropagation, reducing memory requirements by 2-3× compared to gradient-based methods. On a single A6000 GPU, Bonsai prunes 7B and 8B models to 50% sparsity while achieving state-of-the-art perplexity scores—outperforming gradient-based methods like LLM-Pruner and LoRAPrune despite using significantly less memory. The pruned models are also twice as fast as those from semi-structured pruning methods.

## Method Summary
Bonsai estimates module importance through underdetermined regression from sub-model evaluations. It computes module-level priors (Wanda/activation magnitude), samples binary masks to create sub-models while fixing top modules, evaluates each sub-model with forward passes to collect utility scores, then solves a regularized regression problem to learn global importance weights β. The method iteratively prunes modules with lowest β scores, achieving target sparsity through multiple passes. Post-pruning adaptation via LoRA fine-tuning with optional distillation from parent logits recovers performance after aggressive pruning.

## Key Results
- 2-3× memory reduction compared to gradient-based pruning methods
- LLaMA-2-7B pruned to 50% sparsity achieves 9.6 perplexity on Wikitext-2
- Pruned models run twice as fast as semi-structured pruning alternatives
- Outperforms gradient-based methods (LLM-Pruner, LoRAPrune) on perplexity despite lower memory usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Module importance can be estimated via underdetermined regression from n≪N sub-model evaluations.
- Mechanism: Sample n sub-models via binary masks (α), evaluate each with forward passes to get utility U_k, then solve β̂ = argmin Σ(U_k - βᵀα_k)² + γ‖β‖. The learned β directly ranks modules for pruning.
- Core assumption: Module contributions to utility combine approximately linearly; regression can recover rankings even with far fewer samples than modules.
- Evidence anchors:
  - [abstract]: "formulates module importance estimation as an underdetermined regression problem, sampling n≪N sub-models"
  - [section 3.2, Eq 3]: Full regression formulation with regularization
  - [corpus]: Weak/missing — no neighboring papers validate this specific regression approach for LLM pruning
- Break condition: If β estimates have poor rank correlation with true utility (Kendall τ low), pruning decisions become near-random; Table 7 shows ns=50 yields NaN perplexity pre-adaptation.

### Mechanism 2
- Claim: Informative priors (e.g., Wanda, activation magnitude) guide sub-model sampling to yield accurate β estimates with fewer evaluations.
- Mechanism: Set probability of retaining module i in sampled sub-models proportional to prior ρ_i (e.g., module-level Wanda score). Fix top (1-2p) modules per layer; sample masks for bottom 2p. Use complement masks to reduce variance.
- Core assumption: Existing proxy metrics (weight-activation products, fluctuation) correlate with true task-relevant importance.
- Evidence anchors:
  - [section 3.3]: Describes prior-based sampling; keeping top 1-2p fixed reduces search space
  - [Figure 4, Table 6]: Wanda prior outperforms magnitude/random; Bonsai outperforms FLAP (a pure fluctuation method)
  - [corpus]: FLAP (An et al., 2024) uses fluctuation alone — Bonsai can incorporate it as a prior and improve on it
- Break condition: If prior ρ is uncorrelated with true importance, sampling bias hurts coverage; Figure 4 shows random prior yields ~50% worse perplexity than Wanda prior without adaptation.

### Mechanism 3
- Claim: Forward-pass-only pruning reduces memory by 2-3× vs gradient-based methods, enabling pruning on constrained hardware.
- Mechanism: Backprop requires gradient storage (≥2× forward memory); Adam optimizer adds momentum/variance states (≥3×). Bonsai only stores activations for inference + small overhead for regression targets.
- Core assumption: Memory, not compute time, is the primary bottleneck for practitioners pruning 7B+ models.
- Evidence anchors:
  - [abstract]: "reducing memory requirements by 2-3× compared to gradient-based methods"
  - [Table 2]: Bonsai 20-48GB vs LLM-Pruner 160GB vs Sheared LLaMA 640GB for LLaMA-2-7B
  - [corpus]: Weak/missing — neighboring papers do not quantify memory comparisons directly
- Break condition: If regression requires ns approaching N (e.g., >10K samples), runtime dominates and memory savings become moot; Table 7 suggests ns=200-1000 is sufficient.

## Foundational Learning

- **Concept: Structured vs. Unstructured Pruning**
  - Why needed here: Bonsai removes entire modules (heads, FC dimensions) to guarantee inference speedup; unstructured pruning only sparsifies weights without hardware support.
  - Quick check question: If you prune 50% of weights randomly, will inference be 2× faster on an A6000? (Answer: No — only structured removal of dimensions/heads yields wall-clock speedup.)

- **Concept: Wanda/Activation-Based Importance Metrics**
  - Why needed here: These metrics serve as priors ρ_i to guide sub-model sampling; understanding their computation (weight norm × activation RMS) is prerequisite to modifying Bonsai.
  - Quick check question: Given W∈ℝ^(d×o) and activations a∈ℝ^(B×S×d), how would you compute a module-level Wanda score per output dimension?

- **Concept: Underdetermined Linear Regression with Regularization**
  - Why needed here: The core estimation problem has n≪N data points but N unknown β_i values; L1 regularization (γ‖β‖₁) provides identifiability.
  - Quick check question: If you have 200 sub-model evaluations but 70K modules, why doesn't the regression collapse to infinite solutions? (Answer: Regularization + binary mask structure + prior-guided sampling constrain the solution space.)

## Architecture Onboarding

- **Component map:**
  1. Prior calculator (computes ρ_i per module via Wanda/magnitude/fluctuation)
  2. Mask sampler (generates n binary masks α_k with top 1-2p fixed, bottom 2p sampled)
  3. Virtual sub-model evaluator (forward pass with masked modules zeroed; no model instantiation)
  4. Regression solver (Adam-based gradient descent on Eq 3; cross-validate γ, LR, batch size)
  5. Pruner (sort β, drop bottom k modules to hit sparsity target)
  6. Post-Pruning Adaptation (optional LoRA/full fine-tuning + distillation from parent logits)

- **Critical path:**
  Prior computation (forward pass on parent) → Sample n masks → Evaluate n sub-models (forward passes) → Solve regression → Prune → (Iterate if p_iter < p) → PPA fine-tune

- **Design tradeoffs:**
  - Runtime vs. quality: 4 hours (ns=1000, p_iter=0.05) yields 19.47→8.89 PPL; 15 min (ns=200, p_iter=0.2) yields 209.44→9.57 PPL (Table 18)
  - Prior choice: Wanda > FLAP fluctuation > magnitude > random (Figure 4)
  - Iteration depth: Slower pruning (smaller p_iter) gives better β estimates but longer runtime

- **Failure signatures:**
  - NaN perplexity pre-adaptation: Too few samples (ns=50) yields unstable β; recoverable post-adaptation (Table 15)
  - 2× slower after LoRA fine-tuning on semi-structured models: Low-rank matrices cannot merge with 2:4 sparse patterns (Table 3)
  - Reasoning collapse on GSM8K: Task-agnostic pruning (C4/Wikitext signal) deprioritizes reasoning modules; requires domain-specific PPA data (Section 4.3)

- **First 3 experiments:**
  1. Reproduce LLaMA-2-7B at 50% sparsity on Wikitext-2 (Table 6); verify memory fits in 24GB with batch_size=1
  2. Ablate regression: Use prior-only pruning (no perturbation, no regression) — expect 5-10× worse perplexity (Figure 3)
  3. Ablate prior: Compare Wanda vs. random vs. magnitude priors; plot perplexity vs. speedup curve (Figure 4)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Bonsai be enhanced by dynamically exploring the space of sub-models based on previous evaluations, rather than sampling from a fixed prior?
- Basis in paper: [explicit] "Bonsai could be further enhanced by dynamically exploring the space of sub-models based on previous evaluations."
- Why unresolved: Current sampling uses informative priors but is not adaptive during the pruning process.
- What evidence would resolve it: Comparing adaptive sampling strategies against fixed-prior sampling on perplexity and runtime.

### Open Question 2
- Question: Can Bonsai effectively prune Mixture-of-Experts (MoE) models to reduce memory footprint while preserving conditional computation benefits?
- Basis in paper: [explicit] "MoE models also provide an interesting direction: while they enable sparse activation, all experts must remain in memory. Bonsai could prune individual experts to reduce memory footprint."
- Why unresolved: MoE architectures have different module structures and conditional activation patterns not explored in this work.
- What evidence would resolve it: Applying Bonsai to MoE models (e.g., Mixtral) and measuring perplexity, speedup, and memory reduction.

### Open Question 3
- Question: How does structured pruning via Bonsai affect model fairness and robustness compared to dense models?
- Basis in paper: [explicit] "Recent work has shown that compression may also have outsized effects on issues such as model fairness and robustness, which would be interesting additional aspects to consider in future study."
- Why unresolved: The paper evaluates perplexity and zero-shot benchmarks but not fairness or adversarial robustness metrics.
- What evidence would resolve it: Evaluating pruned models on fairness benchmarks (e.g., CrowS-Pairs) and robustness tests (adversarial attacks, distribution shift).

### Open Question 4
- Question: Can integrating gradient-free fine-tuning methods like MeZO during iterative pruning preserve model performance better than post-hoc adaptation?
- Basis in paper: [explicit] "Integration with gradient-free approaches like MeZO could allow for continuous updates during the pruning process, preserving model performance."
- Why unresolved: Bonsai currently does not fine-tune during iterative pruning to maintain memory efficiency.
- What evidence would resolve it: Comparing continuous MeZO-updated pruning against standard Bonsai with post-pruning adaptation on perplexity recovery.

## Limitations
- Underdetermined regression may require hundreds of samples even for moderate models, limiting the claimed memory advantage when scaling to larger architectures
- Task-agnostic pruning using language modeling data can catastrophically degrade performance on reasoning tasks like GSM8K
- The memory reduction claims depend on fine-grained module pruning (individual heads) which may not translate to all hardware or sparsity patterns

## Confidence
- **High Confidence**: Memory reduction claims (2-3× vs gradient methods) are well-supported by Table 2 comparisons and the straightforward mechanism (no gradient storage). The speedup advantage over semi-structured methods is clearly demonstrated in Table 3 with wall-clock measurements.
- **Medium Confidence**: The superiority of Wanda prior over random/magnitude priors (Figure 4) is demonstrated, but the underlying assumption that activation-based proxies generalize across domains remains weakly validated. The GSM8K reasoning collapse example suggests task-agnostic pruning has limitations, but systematic evaluation across diverse downstream tasks is lacking.
- **Low Confidence**: The claim that ns=200-1000 samples are universally sufficient is primarily supported by ablation in Table 7 but lacks theoretical justification for why regression succeeds with such extreme underdetermination. The complement-mask variance reduction benefit is stated but not empirically compared to standard sampling.

## Next Checks
1. **Regression Sample Complexity Analysis**: Systematically vary ns from 50 to 2000 on LLaMA-2-7B at 50% sparsity, measuring both β estimation error (Kendall τ vs ground truth) and downstream perplexity with/without PPA. This would validate whether the claimed 2-3× memory advantage holds when pushing to tighter memory budgets.

2. **Module Granularity Experiment**: Compare pruning individual attention heads versus head groups (e.g., 2-head units) on the same 7B model, measuring memory consumption, inference speed, and perplexity. This would clarify whether the memory claims depend on fine-grained head pruning.

3. **Cross-Domain Prior Transferability**: Apply Wanda-prior pruning trained on Wikitext-2 to three diverse tasks (code generation, mathematical reasoning, dialogue) without PPA, measuring perplexity/accuracy degradation. This would test whether the method's "task-agnostic" claim holds beyond language modeling.