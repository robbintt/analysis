---
ver: rpa2
title: 'Towards Reward Fairness in RLHF: From a Resource Allocation Perspective'
arxiv_id: '2505.23349'
source_url: https://arxiv.org/abs/2505.23349
tags:
- fairness
- reward
- rewards
- arxiv
- allocation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reward unfairness in RLHF,
  which can arise from various biases in reward models and negatively impact the alignment
  of large language models with human preferences. The authors propose a unified framework
  that models preference learning as a resource allocation problem, treating rewards
  as resources to be allocated while balancing utility and fairness.
---

# Towards Reward Fairness in RLHF: From a Resource Allocation Perspective

## Quick Facts
- arXiv ID: 2505.23349
- Source URL: https://arxiv.org/abs/2505.23349
- Authors: Sheng Ouyang; Yulan Hu; Ge Chen; Qingyang Li; Fuzheng Zhang; Yong Liu
- Reference count: 32
- Primary result: Fairness regularization mitigates length and category biases in RLHF reward models while maintaining or improving alignment quality

## Executive Summary
This paper addresses reward unfairness in Reinforcement Learning from Human Feedback (RLHF), which arises from biases in reward models and impacts LLM alignment with human preferences. The authors propose a unified framework that models preference learning as a resource allocation problem, treating rewards as resources to be allocated while balancing utility and fairness. They introduce two methods—Fairness Regularization and Fairness Coefficient—to achieve fairer reward distributions. Experiments on verification and reinforcement learning scenarios demonstrate that their approach mitigates length and category biases while maintaining or improving performance.

## Method Summary
The framework treats reward allocation as a resource distribution problem, using fairness metrics from network resource allocation literature. The core idea is to define an allocation vector where each element represents the reward margin between preferred and dispreferred responses in a pair. Two methods are proposed: Fairness Regularization adds a fairness penalty term to the standard Bradley-Terry loss, while Fairness Coefficient multiplies the utility loss by a fairness factor. The unified fairness metric satisfies continuity, homogeneity, and monotonicity axioms. The methods are validated on both explicit reward models (using Bradley-Terry preference models) and implicit rewards from Direct Preference Optimization (DPO).

## Key Results
- Fairness Regularization effectively mitigates length bias in reward models, as shown by more balanced reward distributions across response lengths
- The framework improves sampling efficiency, achieving the same alignment quality with fewer samples (Figure 4)
- Fairness Regularization maintains or improves performance on standard benchmarks (AlpacaEval2, MT-Bench) while reducing category biases
- Results transfer across base models (LLaMA3, Qwen2.5) with consistent improvements

## Why This Works (Mechanism)

### Mechanism 1: Resource Allocation Reframing
Treating reward allocation as a resource distribution problem enables bias-agnostic mitigation of multiple reward unfairness types. The framework defines an allocation vector a where each element represents the reward margin r(yw) - r(yl) for a preference pair. A unified fairness metric fτ(a) from network resource allocation literature (satisfying continuity, homogeneity, and monotonicity) measures distribution consistency. When τ = -1, this yields Jain's index, which approaches 1.0 as allocations become equal.

### Mechanism 2: Fairness Regularization (Additive Trade-off)
Adding a fairness penalty term to standard Bradley-Terry loss directly shapes reward distributions during RM training. L_FR = -E[log σ(ai)] - αF(a). The first term maximizes utility (correct preference ranking); the second term penalizes uneven allocations. Hyperparameter α controls trade-off. The paper recommends α ≈ 0.1 based on ablation.

### Mechanism 3: Implicit Fair Reward Fitting in DPO
DPO implicitly constructs a reward function, so fairness constraints can be applied without explicit RM training. The implicit reward r_implicit = β log(πθ/πref) substitutes for explicit RM scores. The allocation vector becomes ai = r_implicit(yw) - r_implicit(yl), and the same FR/FC objectives apply during policy optimization.

## Foundational Learning

### Concept 1: Bradley-Terry Preference Model
The standard RM formulation that FR/FC methods modify. Understanding p(yw ≻ yl) = exp(r(yw))/(exp(r(yw)) + exp(r(yl))) and its log-likelihood objective is prerequisite.
- Quick check: Given two responses with rewards r(yw) = 2.0 and r(yl) = 0.5, what probability does BT assign to yw being preferred? (Answer: σ(2.0-0.5) = σ(1.5) ≈ 0.82)

### Concept 2: Resource Allocation Fairness Metrics
The paper leverages axiomatic fairness functions from networking literature. Key property: homogeneity means fairness is scale-invariant—multiplying all rewards by a constant doesn't change F(a).
- Quick check: Why must F(a) satisfy monotonicity (fairness increases as allocations become more equal) for this application? (Consider what happens if fairness decreases with equality.)

### Concept 3: DPO Implicit Reward Formulation
One of two application scenarios requires understanding how r(x,y) = β log(πθ/πref) emerges from the DPO derivation.
- Quick check: If πref assigns probability 0.1 to a response and the optimal policy π* should assign 0.4, what implicit reward does DPO target with β = 0.1? (Answer: 0.1 × log(0.4/0.1) ≈ 0.138)

## Architecture Onboarding

### Component Map
Preference Data (x, yw, yl) -> Compute r(yw)-r(yl) differences -> Allocation Vector a -> Apply Fairness F(a) -> FR: additive / FC: multiplicative -> Optimized Model

### Critical Path
1. **Group identification:** Define what constitutes "entities" for fairness—length bins, categories (helpful/harmless), or social attributes. The paper uses dataset metadata.
2. **Allocation computation:** During each batch, compute allocation vector a = [margin₁, margin₂, ...] where marginᵢ = r(yw) - r(yl) for pair i.
3. **Fairness evaluation:** Apply fτ(a) with τ = -1 (Jain's index) as default.
4. **Loss modification:** Add αF(a) (FR) or multiply by F(a)^γ (FC).

### Design Tradeoffs
| Choice | Option A | Option B | Guidance |
|--------|----------|----------|----------|
| Method | FR (additive) | FC (multiplicative) | Paper shows similar performance; FR easier to tune |
| τ value | -1 (Jain's) | 0.5, 2, 10 | Table 3 shows robustness across [-5, 10]; use -1 |
| α value | 0.05-0.10 | >0.12 | Figure 7 shows peak at ~0.1; higher degrades utility |

### Failure Signatures
| Symptom | Likely Cause | Diagnostic |
|---------|--------------|------------|
| Accuracy unchanged, bias persists | α too small | Check if F(a) increases during training |
| Accuracy drops >2% | α too large | Reduce α by 50% |
| Training divergence | τ extreme value | Stick with τ = -1 |
| Length bias persists | Groups incorrectly defined | Verify length bins in preprocessing |

### First 3 Experiments
1. **Replicate Figure 3 (reward distributions):** Train BT RM, FR RM, FC RM on HH-RLHF. Plot reward density for helpful vs. harmless. Expect: BT shows separation; FR/FC show overlap.
2. **Ablation on α (replicate Figure 7):** Fix τ = -1, vary α ∈ [0, 0.15]. Plot AlpacaEval2 LC WR and MT-Bench scores. Expect: Peak around α = 0.1.
3. **Data selection experiment (replicate Figure 4):** Sample 1-64 responses per prompt, use each RM to select best. Plot LC WR vs. sample count. Expect: FR/FC RMs achieve same quality with fewer samples.

## Open Questions the Paper Calls Out

### Open Question 1
Can the resource allocation perspective on reward unfairness be formally linked to mitigating reward hacking? The Limitation section states that "reward unfairness may be related to various issues in reward models, such as reward hacking," but this specific relationship was not investigated. Experiments demonstrating that Fairness Regularization reduces the frequency or severity of reward hacking behaviors in RL fine-tuning would resolve this.

### Open Question 2
How can the Fairness Rewards framework be adapted for regression-based reward models or PPO-based training? The authors note the method has "only been validated on BT models and DPO" despite the potential for "broader applications." A formulation of the allocation vector a for regression tasks and benchmarks showing improved fairness in PPO-finetuned models would resolve this.

### Open Question 3
Is there a theoretical mechanism to dynamically determine the optimal fairness contribution (α) based on dataset bias? Figure 7 shows that increasing the fairness contribution α improves performance up to a point before declining, suggesting a sensitive trade-off dependent on the specific distribution. An adaptive algorithm for α that consistently maximizes utility across datasets with varying degrees of inherent length or category bias would resolve this.

## Limitations
- The framework assumes reward biases manifest as distributional differences across predefined groups rather than within-pair ranking errors
- Reliance on dataset metadata for group identification may miss subtler bias patterns
- Requires careful tuning of hyperparameters (α, γ, τ) to avoid accuracy degradation
- Effectiveness for detecting and mitigating biases not captured by simple group metadata (e.g., cultural or demographic biases) remains unproven

## Confidence
- **High confidence:** The fairness regularization mechanism and its implementation details are well-specified and reproducible. Experimental results on length bias mitigation and sampling efficiency are robust across multiple runs.
- **Medium confidence:** The transferability of results across different base models and domains is reasonable but not extensively validated. Domain-specific models may require different hyperparameter settings.
- **Low confidence:** The effectiveness of the resource allocation framework for detecting and mitigating biases not captured by simple group metadata (e.g., cultural or demographic biases) remains speculative without additional experiments.

## Next Checks
1. **Bias type generalization:** Apply the framework to a dataset with known social or demographic biases (e.g., using protected attribute labels) and verify whether the fairness metrics detect and correct these biases effectively.
2. **Hyperparameter robustness:** Conduct a systematic ablation study varying τ ∈ [-5, 10] and α ∈ [0.01, 0.2] across multiple base models to identify optimal settings and their sensitivity.
3. **Implicit vs. explicit reward comparison:** Train both explicit RM and DPO variants on the same dataset and compare their implicit reward distributions to verify whether the biases identified in explicit RMs also manifest in implicit rewards.