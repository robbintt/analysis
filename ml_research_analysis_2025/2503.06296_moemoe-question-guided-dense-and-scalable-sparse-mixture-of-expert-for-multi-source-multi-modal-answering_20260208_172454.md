---
ver: rpa2
title: 'MoEMoE: Question Guided Dense and Scalable Sparse Mixture-of-Expert for Multi-source
  Multi-modal Answering'
arxiv_id: '2503.06296'
source_url: https://arxiv.org/abs/2503.06296
tags:
- question
- dataset
- information
- sources
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multi-source multi-modal question answering,
  where answers may exist in one or more sources (text and images). The authors propose
  MoEMoE, a model combining question-guided attention (QGA) and sparse mixture-of-experts
  (MoE).
---

# MoEMoE: Question Guided Dense and Scalable Sparse Mixture-of-Expert for Multi-source Multi-modal Answering

## Quick Facts
- arXiv ID: 2503.06296
- Source URL: https://arxiv.org/abs/2503.06296
- Authors: Vinay Kumar Verma; Shreyas Sunil Kulkarni; Happy Mittal; Deepak Gupta
- Reference count: 13
- One-line primary result: Achieves state-of-the-art performance on multi-source multi-modal QA with 6.18% average recall@90 improvement on CMA-CLIP and 2.4–5.8% accuracy gains on OHLSL

## Executive Summary
MoEMoE addresses multi-source multi-modal question answering by combining question-guided attention (QGA) with sparse mixture-of-experts (MoE). The model learns to dynamically weight multiple information sources (text and images) based on question content through token-level attention, while sparse MoE enables scalability across diverse question types. Experiments on three datasets show state-of-the-art performance, with QGA and alignment losses critical for source selection and MoE placement in decoder layers providing optimal results.

## Method Summary
MoEMoE uses three unshared encoders (T5-base for question, T5-base for context, SwinV2 for images) with question-guided attention to create joint embeddings. The model applies sparse MoE layers only to decoder blocks, with expert-only training (freezing backbone). Training uses cross-entropy loss plus alignment losses (L_QCA, L_QIA) and MoE auxiliary loss for load balancing. The approach achieves superior performance by learning token-level source weights and leveraging expert specialization for diverse question types.

## Key Results
- Average absolute improvement of 6.18% in recall@90 on CMA-CLIP dataset
- Accuracy gains of 2.4–5.8% on OHLSL dataset
- Decoder-only MoE with expert-only training outperforms encoder-only MoE (52.33% vs 66.57% accuracy)
- QGA ablation shows 1-2% performance drop without question guidance
- Alignment losses improve performance by 0.5-0.6% absolute

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question-guided attention (QGA) enables the model to dynamically weight multiple information sources based on question content, improving answer source selection.
- Mechanism: QGA transforms question embeddings through a fully connected layer to produce token-specific weights (α for image, β for context). These weights create a joint embedding: ei = α × I + β × C, where the model learns which source to emphasize per token rather than using a single scalar weight per source.
- Core assumption: Different questions require different source emphases, and token-level granularity provides finer control than sequence-level weighting.
- Evidence anchors:
  - [abstract] "To address this issue, we propose a question-guided attention mechanism that learns attention across multiple sources and decodes this information for robust and unbiased answer generation."
  - [section 4.2] "Rather than learning a scalar weight for each source, we learn token-specific weights, allowing for finer adjustments compared to a single weight per source."
  - [section 6] "In the absence of question guidance (WoQG), the performance dropped from 59.9 to 58.1 for Kurta PTs and from 69.4 to 68.1 for Shirt PTs."

### Mechanism 2
- Claim: Explicit alignment losses between question-source pairs improve the model's ability to focus on relevant information within each source.
- Mechanism: Question embeddings (Q), context embeddings (C), and image embeddings (I) are projected to a common space and aligned via cosine similarity maximization. Two losses are computed: L_QCA (question-context alignment) and L_QIA (question-image alignment), both minimized toward 1.
- Core assumption: Aligned embeddings enable more precise intra-source attention patterns; without explicit alignment, cross-modal attention alone is insufficient for learning robust within-source focus.
- Evidence anchors:
  - [abstract] "To learn attention within each source, we introduce an explicit alignment between questions and various information sources, which facilitates identifying the most pertinent parts of the source information relative to the question."
  - [section 4.3] "This alignment mechanism improves model performance by focusing on the most relevant parts of the source information."
  - [section 6] "Incorporating the alignment loss further enhanced the model's performance, raising it from 59.4 and 68.8 to 59.9 and 69.4 for the Kurta and Shirt PTs, respectively."

### Mechanism 3
- Claim: Sparse MoE applied selectively to decoder layers with expert-only training improves scalability across diverse question types while maintaining computational efficiency.
- Mechanism: Sparse MoE layers are added only to decoder layers. Each MoE layer contains multiple experts with a gating function selecting top-k experts per input. Crucially, only expert parameters and routing are trained while backbone parameters are frozen.
- Core assumption: Different experts can specialize in distinct question types; decoder-only placement is optimal because generation benefits more from specialization than encoding.
- Evidence anchors:
  - [abstract] "To address this by extending our model to a sparse mixture-of-experts (sparse-MoE) framework, enabling it to handle thousands of question types."
  - [section 6.1] "Applying the MoE architecture exclusively to the decoder layers of the model yields superior performance compared to incorporating it in the encoder layers or across the entire model."
  - [section 6.1, Table 4] QGA Decoder MoE Odd (expert training) achieves 66.57% accuracy vs. 63.94% baseline MXT.

## Foundational Learning

- **Mixture-of-Experts (MoE) with sparse gating**
  - Why needed here: MoE is central to scalability; understanding top-k selection, load balancing, and routing is prerequisite to interpreting ablation results.
  - Quick check question: Given 8 experts with top-2 gating, what fraction of experts are active per forward pass? What happens if the gating network always outputs uniform weights?

- **Transformer encoder-decoder architectures (T5-style)**
  - Why needed here: The model uses separate T5 encoders for question and context with a shared decoder; understanding self-attention, cross-attention, and encoder-decoder separation is necessary.
  - Quick check question: In a T5 encoder-decoder, where does cross-attention occur and what inputs does it attend over? How does this differ from decoder-only architectures?

- **Vision Transformers (Swin/ViT) and patch embeddings**
  - Why needed here: Image encoding uses SwinV2 producing patch embeddings; understanding patch-based representations and dimensionality alignment with text embeddings is needed.
  - Quick check question: Why must patch embeddings be repeated/interpolated to match text embedding dimensions? What information might be lost or preserved in this operation?

- **Alignment losses and contrastive objectives**
  - Why needed here: L_QCA and L_QIA use cosine similarity; understanding contrastive/alignment objectives helps diagnose convergence and embedding space quality.
  - Quick check question: If cosine similarity between question and image embeddings is 0.2 at initialization and 0.8 after training, what does this indicate about alignment?

## Architecture Onboarding

- **Component map:**
  - Question encoder (T5-base, unshared parameters) → Q ∈ R^(k×d)
  - Context encoder (T5-base, unshared parameters) → C ∈ R^(k×d)
  - Image encoder (SwinV2-base) → patch embeddings → repeated to R^(k×d) → I
  - QGA module: FC layer on Q → [α, β] weights → joint embedding e_i = α×I + β×C
  - Alignment projections: Q, C, I → linear → cosine similarity losses (L_QCA, L_QIA)
  - Decoder (T5-base with MoE layers in selected decoder blocks): MoE experts + gating
  - Losses: Cross-entropy (generation) + L_QCA + L_QIA + λ×L_aux (MoE load balancing)

- **Critical path:**
  1. Question and context are encoded separately (no parameter sharing)
  2. Image is encoded, patch embeddings extracted and repeated to match text dimensions
  3. QGA computes token-level source weights and creates joint embedding
  4. Alignment losses computed (encoder-side supervision)
  5. Joint embedding passed to decoder with MoE layers
  6. MoE gating selects top-k experts; only experts and routing trained (backbone frozen)
  7. Generation loss + alignment losses + auxiliary loss jointly optimized

- **Design tradeoffs:**
  - Decoder-only MoE vs. encoder/encoder-decoder MoE: Paper empirically finds decoder-only optimal (Table 4); encoder MoE degrades performance significantly (52.33% vs. 66.57%)
  - Expert-only training vs. full fine-tuning: Freezing backbone and training only experts/routing yields best results; full training degrades performance
  - Token-level vs. scalar source weighting: Token-level provides finer granularity but increases parameter count in QGA FC layer
  - Separate question/context encoders vs. joint encoder: Separate encoders enable QGA; joint encoder (S-Enc) prevents QGA application and degrades performance

- **Failure signatures:**
  - Performance drops when MoE added to encoder or full network (Table 4: encoder-only MoE at 52.33% accuracy)
  - Degradation when full model trained end-to-end with MoE (vs. expert-only training)
  - Absence of QGA or alignment losses causes 1-2% recall drops (Fig. 5)
  - Joint encoder (question+context merged) prevents QGA and yields lowest ablation results
  - Auxiliary loss weight too low (0.01) ignores MoE; too high (0.5) over-prioritizes load balancing over task loss

- **First 3 experiments:**
  1. **Reproduce baseline QGA without MoE**: Train T5 + SwinV2 with QGA and alignment losses (no sparse MoE) on a subset of 30PT or CMA-CLIP. Verify alignment loss convergence and QGA weight distributions (α vs. β) for visual vs. textual attributes.
  2. **Ablate MoE placement**: Compare three configurations on held-out set: (a) decoder-only MoE (proposed), (b) encoder-only MoE, (c) encoder-decoder MoE. Use same expert count (e.g., 8) and top-2 gating. Measure accuracy, recall@90, and training stability.
  3. **Expert-only vs. full training**: Starting from pretrained backbone, compare (a) freezing backbone + training experts/routing only vs. (b) full end-to-end training. Monitor validation performance over epochs and check for overfitting or catastrophic forgetting in (b).

## Open Questions the Paper Calls Out

- **Open Question 1**: What intrinsic patterns in the input data determine the selection of specific experts within the sparse MoE routing mechanism? The paper demonstrates performance benefits but does not analyze internal decision logic or specialization of expert routers.

- **Open Question 2**: How can the model be modified to detect "out-of-context" questions and abstain from answering, rather than generating hallucinated attributes? The current model architecture lacks an explicit mechanism to refuse answering when evidence is absent from the sources.

- **Open Question 3**: Does replacing the generic T5 tokenizer with a domain-specific e-commerce tokenizer improve semantic representation of specialized vocabulary? The experiments rely on standard pre-trained tokenizer, which fails to capture intended meaning of specific market vocabulary.

- **Open Question 4**: How does model performance and scalability change when training on a mixture of English and Non-English data for diverse global marketplaces? The dataset used for training and validation is restricted to English language, limiting applicability to other marketplaces.

## Limitations

- Critical MoE hyperparameters (number of experts, top-k value) are not specified, making exact reproduction difficult
- Exact decoder layers receiving MoE treatment are not fully specified, with only partial information about configurations
- No training stability metrics, negative log-likelihood, or gradient norms reported to diagnose optimization behavior
- Computational efficiency metrics (FLOPs, inference latency) not provided to validate MoE efficiency claims
- Relationship between alignment losses and performance gains not thoroughly explored

## Confidence

**High Confidence**: Core architectural innovations (QGA mechanism, alignment losses, MoE placement strategy) are clearly described and empirically validated through ablation studies. Performance improvements are specific and supported by experimental results.

**Medium Confidence**: Claims about decoder-only MoE superiority and expert-only training benefits are supported by results, but underlying reasons for why encoder MoE degrades and why full fine-tuning fails are not deeply analyzed.

**Low Confidence**: Scalability claims regarding handling "thousands of question types" are asserted but not empirically demonstrated. No performance scaling with increasing question diversity or computational efficiency metrics provided.

## Next Checks

1. **MoE Hyperparameter Sensitivity Analysis**: Systematically vary the number of experts (4, 8, 16) and top-k values (1, 2, 3) while keeping all other parameters constant. Measure accuracy, recall@90, and training stability to identify optimal configurations and quantify sensitivity.

2. **Alignment Loss Ablation with Visualization**: Train ablations with alignment losses removed, then visualize learned source attention weights (α vs. β) for different attribute types (visual vs. textual). Compare with baseline alignment loss configurations to quantify contribution.

3. **Computational Efficiency Benchmarking**: Measure actual inference latency and memory usage for baseline vs. MoE configurations on representative hardware. Calculate FLOPs and parameter counts to verify that sparse MoE achieves claimed efficiency gains while maintaining performance.