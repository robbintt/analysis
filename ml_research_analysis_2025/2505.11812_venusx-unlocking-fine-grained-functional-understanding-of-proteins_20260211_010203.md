---
ver: rpa2
title: 'VenusX: Unlocking Fine-Grained Functional Understanding of Proteins'
arxiv_id: '2505.11812'
source_url: https://arxiv.org/abs/2505.11812
tags:
- protein
- interpro
- sequence
- esm2
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VenusX is the first large-scale benchmark for fine-grained protein
  functional understanding, covering residue, fragment, and domain-level tasks. It
  provides 56 datasets with over 878k samples from InterPro, BioLiP, and SAbDab, including
  residue-level binary classification, fragment-level multi-class classification,
  and pairwise functional similarity scoring.
---

# VenusX: Unlocking Fine-Grained Functional Understanding of Proteins

## Quick Facts
- arXiv ID: 2505.11812
- Source URL: https://arxiv.org/abs/2505.11812
- Authors: Yang Tan; Wenrui Gou; Bozitao Zhong; Liang Hong; Huiqun Yu; Bingxin Zhou
- Reference count: 40
- Key outcome: First large-scale benchmark for fine-grained protein functional understanding with 56 datasets covering residue, fragment, and domain-level tasks

## Executive Summary
VenusX establishes a comprehensive benchmark for evaluating protein functional understanding at multiple granularities. The benchmark reveals that strong performance on conventional protein-level tasks does not translate to fine-grained functional understanding, particularly for residue-level predictions. Models show significant performance drops (70-80%) on out-of-distribution cross-family splits compared to in-distribution mix-family splits, exposing reliance on memorization rather than true functional knowledge.

## Method Summary
VenusX evaluates protein functional understanding through three task categories: residue-level binary classification (7 subtasks), fragment-level multi-class classification (5 tasks), and pairwise functional similarity scoring. The benchmark uses 56 datasets from InterPro, BioLiP, and SAbDab with over 878k samples. Mix-family and cross-family splits at 50%, 70%, and 90% sequence identity thresholds create both in-distribution and out-of-distribution evaluation scenarios. Models are evaluated as frozen encoders with task-specific heads using AUPR, macro-F1, and accuracy metrics across different task types.

## Key Results
- Models show 70-80% performance drops on cross-family residue prediction compared to <10% drops for domain-level tasks
- Sequence-structure hybrid models (SAPROT-650M, PROTSSN) generally outperform sequence-only or structure-only models
- Epitope prediction remains extremely difficult with no model achieving AUPR above 0.3 across all settings
- Strong protein-level performance does not translate to fine-grained functional understanding

## Why This Works (Mechanism)

### Mechanism 1
Multi-granularity task design reveals that protein-level performance does not predict fine-grained functional understanding. By evaluating at residue, fragment, and domain levels simultaneously, the benchmark exposes whether models capture localized biological signals versus relying on global distributional cues. The hierarchical structure creates progressively coarser supervision signals that discriminate between shallow pattern matching and mechanistic understanding.

### Mechanism 2
Cross-family splits with sequence identity thresholds expose reliance on memorization versus generalization. MMseqs2 clustering at 50%, 70%, and 90% identity thresholds followed by family-aware splitting ensures test proteins come from InterPro families unseen during training. This forces models to generalize beyond sequence similarity to functional similarity, revealing whether learned representations capture family-invariant functional features.

### Mechanism 3
Sequence-structure hybrid representations capture complementary information that improves fine-grained prediction. SAPROT encodes structure as Foldseek-derived 3Di tokens appended to amino acid tokens, while PROTSSN combines ESM2 embeddings with geometric graph convolutions. Structure provides spatial constraints while sequence captures evolutionary constraints, enabling reasoning about both spatial proximity and co-evolution patterns.

## Foundational Learning

- **Concept: Residue-level vs protein-level prediction**
  - Why needed here: Understanding the granularity distinction is essential for interpreting why models fail on cross-family splits at residue level (70-80% drops) but show modest domain-level drops (<10%).
  - Quick check question: Can you explain why predicting catalytic residues requires different learned features than predicting whether a protein is an enzyme?

- **Concept: Class imbalance and evaluation metrics**
  - Why needed here: Residue-level tasks have severe imbalance (e.g., Act: 4.6% positive, Epi: 10.6%). AUPR and Macro-F1 are prioritized over accuracy because high accuracy can be achieved by predicting all negatives.
  - Quick check question: Why would ROC-AUC be misleading for a task where 95% of residues are negative?

- **Concept: Sequence identity clustering for OOD evaluation**
  - Why needed here: The 50%/70%/90% thresholds control how similar training and test proteins can be, creating graduated difficulty. Understanding this is critical for designing experiments and interpreting results.
  - Quick check question: What would happen to cross-family generalization if you used 30% instead of 50% identity threshold?

## Architecture Onboarding

- **Component map**: Data sources (InterPro, BioLiP, SAbDab) -> MMseqs2 clustering -> mix-family/cross-family splits -> train/val/test (8:1:1) -> Pretrained models (ESM2, SAPROT, PROTSSN) -> Classification head (linear layers + ReLU + dropout) -> Evaluation metrics

- **Critical path**: Load curated dataset -> Initialize frozen encoder -> Attach task-specific head -> Train with AdamW (lr=0.001, batch=128) -> Evaluate on mix-family and cross-family test sets

- **Design tradeoffs**: Frozen vs fine-tuned encoders enable fair comparison but may underrepresent model potential; fragment cap (128/512) reduces compute but may truncate long domains; mean pooling for fragments is simple but loses positional information

- **Failure signatures**: High mix-family AUPR (>0.9) with cross-family collapse (<0.2) indicates memorization without generalization; high accuracy (>85%) with low Macro-F1 (<60%) suggests class imbalance exploitation; Epi task consistently low across all models (AUPR <0.3) indicates inherent difficulty

- **First 3 experiments**:
  1. Replicate residue-level binary classification on VenusX_Res_Act_MF50 using ESM2-T33 to validate pipeline
  2. Compare cross-family performance between ESM2-T33 and SaProt-650M on Act to quantify structure contribution
  3. Evaluate GVP-GNN on fragment-level tasks where it showed strong performance (Act Macro-F1=0.906) to verify structure-only sufficiency

## Open Questions the Paper Calls Out

- **Open Question 1**: What architectural inductive biases or training paradigms are required to significantly improve epitope prediction performance, given that no current model exceeds an AUPR of 0.3?
- **Open Question 2**: How can models be improved to bridge the massive performance gap (70-80% drop) between in-distribution and out-of-distribution (cross-family) splits for residue-level tasks?
- **Open Question 3**: Can pre-training objectives be designed to ensure that improvements in global protein-level tasks translate effectively to fine-grained functional understanding?

## Limitations

- Benchmark evaluates frozen encoders rather than fine-tuned models, potentially underestimating model capabilities
- Epitope prediction task shows consistently poor performance that may reflect fundamental task difficulty rather than model limitations
- Severe class imbalance in residue-level tasks (4.6% positive for Act) may affect evaluation metric interpretation

## Confidence

- **High Confidence**: Benchmark construction methodology and observation of 70-80% performance drops on cross-family residue prediction
- **Medium Confidence**: Superiority of sequence-structure hybrid models and claim that protein-level performance doesn't predict fine-grained understanding
- **Low Confidence**: Assertion that current models lack "biological intelligence" beyond pattern matching

## Next Checks

1. Re-run the top 3 models (SAPROT-650M, PROTSSN, ESM2-T33) with end-to-end fine-tuning on a representative subset of tasks
2. Re-evaluate the epitope prediction task using stricter geometric criteria (5Å instead of 10Å) or alternative definitions
3. Perform failure analysis on cross-family predictions to identify whether errors stem from novel functional mechanisms or representation limitations