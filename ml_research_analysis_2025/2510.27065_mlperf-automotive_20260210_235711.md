---
ver: rpa2
title: MLPerf Automotive
arxiv_id: '2510.27065'
source_url: https://arxiv.org/abs/2510.27065
tags:
- automotive
- benchmark
- systems
- dataset
- mlperf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MLPerf Automotive introduces the first standardized benchmark
  for automotive ML systems, addressing the need for consistent performance evaluation
  across diverse hardware platforms. The benchmark suite includes three workloads:
  2D object detection (SSD), 2D semantic segmentation (DeepLabv3+), and 3D object
  detection (BEVFormer-tiny), each with specific accuracy targets (99.9% for SSD and
  DeepLabv3+, 99% for BEVFormer) and tail latency requirements (99.9%).'
---

# MLPerf Automotive

## Quick Facts
- arXiv ID: 2510.27065
- Source URL: https://arxiv.org/abs/2510.27065
- Reference count: 40
- First standardized benchmark for automotive ML systems with safety-critical timing and accuracy requirements

## Executive Summary
MLPerf Automotive introduces the first standardized benchmark suite for evaluating automotive machine learning systems, addressing the critical need for consistent performance evaluation across diverse hardware platforms in the automotive industry. The benchmark targets perception workloads essential for automated driving, with specific focus on real-time processing constraints, multi-modal sensor data handling, and safety-critical accuracy requirements. The initial v0.5 release includes three workloads: 2D object detection, 2D semantic segmentation, and 3D object detection, each with stringent tail latency (99.9th percentile) and accuracy targets.

## Method Summary
The benchmark defines three perception workloads using reference implementations in ONNX format, with fixed datasets (nuScenes for 3D, Cognata for 2D) and standardized LoadGen-driven query patterns. Submissions must achieve specific accuracy targets (99.9% of FP32 baseline for 2D models, 99% for BEVFormer) while meeting tail latency requirements (99.9th percentile). The closed division prohibits quantization-aware training and requires submissions to use reference models with post-training quantization or reduced precision inference. LoadGen issues queries under Single Stream and Constant Stream scenarios with specific frame rates per workload.

## Key Results
- First round (v0.5) received nine submissions from two organizations
- Submissions used various optimization techniques including INT8 quantization, FP16, and FP8
- Demonstrated benchmark utility for comparing automotive ML system performance across different architectures
- Established baseline performance metrics for future automotive ML hardware and software development

## Why This Works (Mechanism)

### Mechanism 1
Standardized evaluation protocols enable meaningful cross-platform performance comparison for automotive ML systems. The benchmark provides fixed reference implementations (ONNX format), standardized datasets, and consistent LoadGen-driven query patterns, eliminating variability from individual supplier benchmarking processes and allowing direct comparison of hardware/software stacks.

### Mechanism 2
Tail latency constraints (99.9th percentile) enforce real-time safety requirements that distinguish automotive from general inference benchmarks. LoadGen issues queries under Single Stream or Constant Stream scenarios, with the 99.9% tail latency metric capturing worst-case latency jitter that could cause safety failures in emergency driving situations.

### Mechanism 3
Accuracy constraints expressed as percentage of FP32 baseline prevent unsafe quantization-induced accuracy degradation. Submissions must achieve ≥99.9% of FP32 reference accuracy for SSD/DeepLabv3+ and ≥99% for BEVFormer, allowing post-training quantization while preventing aggressive optimizations that would compromise detection quality.

## Foundational Learning

- **SAE Driving Automation Levels (0-5)**: Why needed here - The benchmark maps workloads to autonomy levels, contextualizing why different models have different accuracy/latency targets. Quick check - Which SAE level requires no human driver takeover when requested?

- **Tail Latency (Percentile-based)**: Why needed here - Automotive benchmarks use 99.9th percentile latency, not mean/median, capturing rare worst-case delays that could cause collisions. Quick check - Why does 99.9% tail latency require more samples to estimate reliably than 99%?

- **Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**: Why needed here - The benchmark allows PTQ but prohibits QAT in closed division, explaining the accuracy constraints and optimization space. Quick check - Which technique requires access to the training pipeline?

## Architecture Onboarding

- **Component map**: LoadGen -> System Under Test (SUT) -> Reference Models -> Datasets -> Accuracy Scripts
- **Critical path**: 1) Obtain dataset access (nuScenes public, Cognata requires MLCommons membership), 2) Convert reference ONNX models to target framework/hardware, 3) Implement preprocessing matching reference, 4) Run accuracy validation to confirm ≥99%/99.9% of FP32 baseline, 5) Run performance benchmark under LoadGen scenarios, 6) Submit with compliance tests
- **Design tradeoffs**: Closed vs. Open Division (fair comparison vs. flexibility), Resolution vs. Compute (8MP inputs increase small-object detection but require 4× more compute), Synthetic vs. Real Data (Cognata enables 8MP + licensing clarity; nuScenes provides real-world legitimacy but lower resolution)
- **Failure signatures**: Accuracy below threshold (PTQ too aggressive, requires higher precision), Tail latency violations (Inference time variance from dynamic control flow), ONNX conversion failures (BEVFormer requires PyTorch 1.13+ with patched OpenMMLab dependencies)
- **First 3 experiments**: 1) Run FP32 reference implementation on target hardware to establish baseline accuracy and latency, 2) Apply INT8 PTQ with calibration on held-out dataset samples; verify accuracy remains ≥99%/99.9% of baseline, 3) Profile latency distribution under Constant Stream scenario to identify tail latency outliers

## Open Questions the Paper Calls Out

### Open Question 1
Do model optimizations (e.g., quantization) that maintain accuracy on synthetic datasets (like Cognata) demonstrate equivalent performance on real-world driving data? This remains unresolved because synthetic data may lack the noise distribution or edge cases of physical sensors, potentially hiding accuracy degradation that would occur in production environments.

### Open Question 2
How can "safety-centric" accuracy metrics, such as temporal consistency or rare object detection, be standardized for reproducible benchmarking? This is unresolved because unlike static mAP or pixel accuracy, temporal metrics require sequence dependency and definitions of "rare" events that current single-frame benchmarks do not support.

### Open Question 3
What specific functional safety requirements should be enforced to define an intermediate "Hardened" category that is auditable without requiring full ISO certification? This remains unresolved because full functional safety certification is too burdensome for a benchmark cycle, yet a relaxed standard requires careful selection to ensure it remains predictive of real-world safety performance.

## Limitations

- The 99.9% tail latency threshold lacks direct validation from automotive safety standards or deployment data
- The synthetic Cognata dataset may not capture the quantization sensitivity characteristics of real-world driving data
- The benchmark covers only perception tasks, excluding critical components like sensor fusion, planning, and control

## Confidence

- **High**: Standardized evaluation protocols enable cross-platform comparison; Tail latency constraints distinguish automotive from general benchmarks
- **Medium**: Accuracy constraints prevent unsafe quantization degradation; Benchmark represents meaningful subset of automotive workloads
- **Low**: 99.9% tail latency threshold ensures safety; Percentage-based accuracy targets correlate with real-world safety

## Next Checks

1. Conduct correlation analysis between benchmark accuracy percentages and actual deployment safety performance metrics from production ADAS systems
2. Validate quantization sensitivity differences between synthetic (Cognata) and real-world driving datasets through controlled experiments
3. Perform sensitivity analysis on tail latency percentile threshold (95%, 99%, 99.9%, 99.99%) to determine minimum viable threshold for safety-critical scenarios