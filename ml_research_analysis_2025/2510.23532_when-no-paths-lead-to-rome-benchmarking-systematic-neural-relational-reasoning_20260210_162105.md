---
ver: rpa2
title: 'When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning'
arxiv_id: '2510.23532'
source_url: https://arxiv.org/abs/2510.23532
tags:
- reasoning
- rules
- living
- answer
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces NoRA, a new benchmark for systematic neural
  relational reasoning designed to address the limitations of existing benchmarks
  like CLUTRR that overly rely on path-based reasoning. NoRA incorporates ambiguity,
  multiple relationships between entities, and non-path reasoning requirements, challenging
  models to go beyond simple path composition.
---

# When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning

## Quick Facts
- arXiv ID: 2510.23532
- Source URL: https://arxiv.org/abs/2510.23532
- Reference count: 40
- Authors: Anirban Das; Irtaza Khalid; Rafael Peñaloza; Steven Schockaert
- Primary result: Introduces NoRA benchmark exposing limitations of path-based neural relational reasoning models on off-path inference tasks

## Executive Summary
This paper introduces NoRA, a benchmark designed to test systematic neural relational reasoning beyond simple path-based inference. Unlike existing benchmarks such as CLUTRR that focus on path composition, NoRA incorporates ambiguity, multiple relationships between entities, and requires reasoning that goes beyond direct paths. The benchmark reveals that state-of-the-art models including Edge Transformers, Relation-Aware Transformers, EpiGNNs, and GNNs struggle significantly with off-path reasoning tasks. Even large reasoning models like o3 fail on these tasks despite explicit world rules, highlighting the need for new architectural approaches to systematic relational reasoning.

## Method Summary
NoRA generates synthetic stories using Answer Set Programming (ASP) to create worlds with definite rules and constraints. The benchmark introduces three key dimensions: reasoning depth (path length), reasoning width (ambiguity requiring constraint satisfaction), and off-path edge count (OPEC) measuring edges outside direct paths. Models are evaluated on their ability to predict relationships between entities, with particular focus on off-path reasoning where critical evidence lies outside the direct connecting path. The benchmark includes additional datasets (NoRA v1.1 and HetioNet) to test robustness across different relational structures and includes systematic composition through "stitching" of training instances.

## Key Results
- All evaluated models struggle significantly with off-path reasoning, with Edge Transformers performing best but achieving only moderate accuracy
- Large Reasoning Models like o3 fail on off-path reasoning tasks even when given explicit world rules
- Presence of ambiguity did not pose particular challenges as models found statistical shortcuts rather than reasoning about constraints
- EpiGNN shows better relative performance on off-path reasoning in biological domains compared to social domains

## Why This Works (Mechanism)

### Mechanism 1: Off-Path Edge Count (OPEC) as a Failure Mode Trigger
Standard GNNs and path-based models prune search space to edges connecting query pairs. When critical deductions require detouring through entities not on the connecting path, these architectures structurally ignore necessary evidence. Edge Transformers theoretically degrade less sharply due to global attention, though they still struggle.

### Mechanism 2: Ambiguity Resolution via Constraint Satisfaction
Models must hypothesize multiple graph states and prune those violating logical constraints. Success requires maintaining multiple hypotheses simultaneously rather than memorizing statistical correlations.

### Mechanism 3: Compositional Stitching for Systematicity Testing
Test instances constructed by stitching training instances ensure failure to generalize reflects failure of composition rather than missing knowledge. Models must chain learned rules in novel structural configurations.

## Foundational Learning

- **Answer Set Programming (ASP)**
  - Why needed: NoRA generates stories and ground truths using ASP to define world rules and handle ambiguous facts
  - Quick check: Given `parent_of(X,Y) :- father_of(X,Y).` and `:- underage(X), parent_of(X,Y).`, does `father_of(bob, alice)` entail `parent_of(bob, alice)`? If `bob` is `underage`, what happens?

- **Inductive Bias in GNNs (Path vs. Global)**
  - Why needed: Differentiates models based on bias toward path-composition versus global graph reasoning
  - Quick check: In a standard 2-layer GNN, can node A aggregate information from node D if D is not within a 2-hop neighborhood of A?

- **Compositional Generalization**
  - Why needed: Core challenge is systematicity—applying known rules to novel chain lengths
  - Quick check: If a model trained on reasoning paths of length 2 and 3 fails on length 4, is this failure of systematic generalization or out-of-distribution error?

## Architecture Onboarding

- **Component map:** ASP Generator -> Graph Encoder -> Backbone (GNN/Transformer) -> Min/Max Aggregator
- **Critical path:** 1) ASP world rules definition (most brittle), 2) Graph encoding handling ambiguity, 3) Exact Match evaluation
- **Design tradeoffs:** ASP (perfect logic, slow) vs. Neural (fast, flawed); Single-edge (simpler) vs. Multi-edge (information complete)
- **Failure signatures:** High Depth/Low OPEC accuracy indicates path-composition heuristics; High Width accuracy with shortcuts indicates pattern matching; LRM failure indicates internalized single-step proofs
- **First 3 experiments:** 1) Sanity check R-GCN on in-distribution split failing on Test-OPEC, 2) Shortcut analysis on "Train-a" ambiguous split, 3) Architectural ablation comparing multi-edge vs single-edge Edge Transformers

## Open Questions the Paper Calls Out

- What neural architectures can effectively handle systematic off-path reasoning without relying on path-composition inductive biases?
- How can benchmark difficulty metrics be refined to prevent models from exploiting shortcuts to bypass explicit constraint reasoning?
- Why do certain models exhibit significantly better relative performance on off-path reasoning in biological domains compared to social domains?

## Limitations

- Dataset generation artifacts from acyclic graph restrictions may not reflect real-world knowledge graph complexity
- Exact Match metric's strictness may penalize partial relational understanding
- Benchmark scope limited to static, deterministic relations without temporal or probabilistic reasoning

## Confidence

**High Confidence (8-10/10):**
- NoRA requires non-path reasoning beyond CLUTRR
- Existing models fail on off-path reasoning empirically validated
- Systematic generation through ASP ensures logical consistency

**Medium Confidence (5-7/10):**
- Ambiguity introduces additional reasoning complexity (partially contradicted by shortcut findings)
- Edge Transformer superiority may not generalize to larger graphs
- LRM failure modes require further validation

**Low Confidence (1-4/10):**
- Extrapolation to broader systematic reasoning capabilities
- Multi-edge encoding necessity vs. sufficiency
- OPEC as primary failure mode without alternative explanations

## Next Checks

1. Conduct ablation studies on "Train-a" split to systematically identify statistical shortcuts models use on ambiguous cases, comparing performance on explicitly constructed "hard ambiguous" subsets versus random samples

2. Evaluate Edge Transformer and top-performing models on larger graphs (20-30 nodes) with varying OPEC levels to determine if performance degradation is linear, exponential, or plateaus

3. Develop modified NoRA testing hybrid symbolic-neural approaches by providing partial ASP-derived proofs as additional context, measuring improvement over pure neural or pure ASP approaches