---
ver: rpa2
title: On the Robustness of Transformers against Context Hijacking for Linear Classification
arxiv_id: '2502.15609'
source_url: https://arxiv.org/abs/2502.15609
tags:
- context
- learning
- linear
- hijacking
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides theoretical analysis of transformer robustness
  against context hijacking in in-context learning. The authors model context hijacking
  as a binary classification problem where adversarial examples are designed as factually
  correct query-answer pairs with opposite labels to the target query.
---

# On the Robustness of Transformers against Context Hijacking for Linear Classification

## Quick Facts
- arXiv ID: 2502.15609
- Source URL: https://arxiv.org/abs/2502.15609
- Reference count: 40
- Key outcome: Deeper transformers achieve higher robustness against context hijacking because they implement more fine-grained optimization steps

## Executive Summary
This paper provides theoretical analysis of transformer robustness against context hijacking in in-context learning. The authors model context hijacking as a binary classification problem where adversarial examples are designed as factually correct query-answer pairs with opposite labels to the target query. They prove that deeper transformers achieve higher robustness because they implement more fine-grained optimization steps, effectively mitigating interference from hijacking examples. The key finding is that the classification error can be formulated as a function of model depth, training context length, and number of hijacking examples.

## Method Summary
The paper models in-context learning as a linear classification task where transformers implement preconditioned gradient descent. The method involves training L-layer linear transformers on clean context examples, then evaluating robustness by measuring accuracy degradation when factually correct but adversarial examples (with opposite labels) are prepended. The analysis derives optimal learning rates that scale as 1/(nL) and proves robustness bounds that improve exponentially with depth. Experiments validate these theoretical results using both linear transformers and GPT-2 models.

## Key Results
- Deeper transformers require more hijacking examples to disrupt predictions
- Classification error is bounded by a function that decays more slowly with depth L
- Accuracy decreases as context length increases but converges more slowly for deeper models
- Theoretical analysis shows optimal learning rate scales as 1/(nL)

## Why This Works (Mechanism)

### Mechanism 1: Gradient Descent Implementation
Linear transformers can simulate L steps of preconditioned gradient descent on the in-context loss, with self-attention updating the label vector by subtracting a term proportional to the gradient. The Value matrix P and Key-Query matrix Q act as preconditioner and learning rate matrix respectively.

### Mechanism 2: Depth-Induced Fine-Grained Optimization
Deeper transformers distribute optimization burden across more layers, resulting in smaller effective learning rate per step (α ≈ 1/(nL)). This fine-grained optimization prevents over-reaction to high-magnitude hijacking examples.

### Mechanism 3: Robustness Scaling via Error Bounds
The classification error bound involves a term proportional to (1 - e^(-Θ(Nd/nL)))^L, which decays more slowly for deeper models. This means more hijacking tokens are required to push predictions over the boundary compared to shallow models.

## Foundational Learning

- **Concept: In-Context Learning (ICL) as Optimization**
  - Why needed here: Frames transformer as optimization algorithm running on the fly
  - Quick check question: Can you explain how viewing a transformer layer as a "step of gradient descent" changes your interpretation of adding more layers?

- **Concept: Linear Attention**
  - Why needed here: Proofs rely on mathematical properties of linear attention allowing collapse to clean algebraic form
  - Quick check question: How does the linear transformer update Z_i = Z_{i-1} + P Z_i M(Z_{i-1}^T Q Z_{i-1}) differ from standard Softmax attention?

- **Concept: Context Hijacking vs. Adversarial Attacks**
  - Why needed here: Involves factually correct data points used maliciously, distinct from random noise or false data injection
  - Quick check question: Why might a model be more susceptible to a "factually correct" distractor than a random noise token?

## Architecture Onboarding

- **Component map:** Input Matrix Z -> Linear Attention Layer (Value P, Key-Query Q) -> Embedding W_E -> Output (last column of Z_L)
- **Critical path:**
  1. Generate w*, then standard data. Project query x onto boundary x_⊥ to create hijacking context
  2. Train on clean context to minimize risk R(TF)
  3. Evaluate accuracy drop as N (hijacking tokens) increases across different depths L
- **Design tradeoffs:** Linear vs. Standard Attention (theory exact for linear, GPT-2 empirical), Depth vs. Training Stability (requires sufficient n for small learning rates)
- **Failure signatures:** Accuracy Drop to 50% (successful hijacking), No Convergence (if n < d or n < L)
- **First 3 experiments:**
  1. Replicate Depth Scaling: Train linear transformers with L ∈ {1, 2, …, 8} and plot accuracy vs. hijacking tokens N
  2. Verify Learning Rate Scaling: For fixed context length n, confirm optimal learning rate scales as 1/L
  3. Boundary Analysis: Vary distance η of hijacking examples from decision boundary to see robustness degradation

## Open Questions the Paper Calls Out

### Open Question 1
Can robustness guarantees against context hijacking be extended to standard transformers with softmax attention?
Basis in paper: The analysis restricts to "linear attention-only transformers" to utilize closed-form solutions
Why unresolved: Non-linear normalization in softmax breaks direct equivalence to linear gradient descent
What evidence would resolve it: Theoretical framework linking softmax attention to preconditioned gradient descent in this context

### Open Question 2
Does standard pre-training converge to the optimal multi-step gradient descent parameters derived in the paper?
Basis in paper: The analysis characterizes "optimal multi-step gradient strategy" but focuses on existence rather than training trajectory
Why unresolved: Paper proves existence of optimal parameters but doesn't guarantee standard pre-training reaches this optimum
What evidence would resolve it: Convergence analysis proving minimizing pre-training loss leads to learning optimal parameters

### Open Question 3
How does robustness change when hijacking examples are not factually correct or on the decision boundary?
Basis in paper: Explicitly differentiates setup from adversarial attacks by designing hijacking examples as "factually correct query-answer pairs"
Why unresolved: Proofs rely on geometric properties of boundary-projected data which may not hold for arbitrary perturbations
What evidence would resolve it: Extending error bounds to cases where hijacking examples deviate from boundary or are factually incorrect

## Limitations

- The theoretical analysis relies on linear transformers while experiments use GPT-2 with softmax attention, creating a theory-experiment gap
- The mathematical analysis assumes clean synthetic data from well-defined generative process, which may not reflect real-world complexity
- Robustness benefits depend critically on initialization aligning with data-generating prior, which isn't fully explored
- The paper doesn't address computational costs or overfitting risks when increasing depth without proportionally increasing context length

## Confidence

- **High Confidence:** Theoretical framework showing deeper transformers implement finer-grained optimization steps is mathematically rigorous
- **Medium Confidence:** Experimental validation with linear transformers demonstrates predicted depth-robustness relationship
- **Low Confidence:** Exact practical bounds for robustness are difficult to verify outside controlled synthetic setting

## Next Checks

1. Test robustness on non-linear classification tasks (e.g., XOR or circular decision boundaries) to verify whether depth scaling benefits extend beyond the linear case

2. Vary initialization alignment (e.g., set cβ* nearly orthogonal to w*) to quantify how sensitive robustness gains are to prior assumptions

3. Measure computational efficiency (FLOPs, inference time) of deeper vs. shallower models under attack to assess practical trade-offs between robustness and efficiency