---
ver: rpa2
title: 'SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based
  Embodied Agents'
arxiv_id: '2510.12985'
source_url: https://arxiv.org/abs/2510.12985
tags:
- safety
- action
- object
- constraints
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SENTINEL is a multi-level framework for formally evaluating the
  physical safety of LLM-based embodied agents. It grounds natural-language safety
  requirements in temporal logic (LTL and CTL) and applies three progressive checks:
  semantic interpretation (translating safety rules into LTL), plan-level safety (verifying
  high-level plans against LTL constraints), and trajectory-level safety (model checking
  over execution trees).'
---

# SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents

## Quick Facts
- arXiv ID: 2510.12985
- Source URL: https://arxiv.org/abs/2510.12985
- Authors: Simon Sinong Zhan; Yao Liu; Philip Wang; Zinan Wang; Qineng Wang; Zhian Ruan; Xiangyu Shi; Xinyu Cao; Frank Yang; Kangrui Wang; Huajie Shao; Manling Li; Qi Zhu
- Reference count: 40
- One-line primary result: Multi-level formal safety evaluation pipeline for LLM-based embodied agents using LTL/CTL verification across semantic, plan, and trajectory levels

## Executive Summary
SENTINEL addresses the challenge of evaluating physical safety for LLM-based embodied agents by introducing a three-tier formal verification framework. The system grounds natural language safety requirements in temporal logic (LTL and CTL) and progressively verifies safety at semantic, plan, and trajectory levels. Experiments demonstrate that while LLMs can correctly interpret safety rules semantically, safety compliance drops significantly during planning and execution phases due to physical interactions. The framework identifies safety violations that prior methods overlook, highlighting the necessity of integrated formal verification across all levels of agent operation.

## Method Summary
The framework implements a waterfall-style safety evaluation pipeline with three progressive checks. First, it translates natural language safety constraints into Linear Temporal Logic (LTL) formulas and verifies semantic equivalence using Büchi automata-based language containment checking. Second, it generates high-level plans from LLMs and verifies them against LTL constraints using model checking. Third, it simulates physical execution to build computation trees from multiple trajectories and applies Computation Tree Logic (CTL) model checking to verify safety properties across all possible execution paths. The system uses VirtualHome and ALFRED environments with 91 modified safety-centric scenes, employing BFS for plan validity and DFS/BFS for CTL operators (AG, AF, AU, AX).

## Key Results
- Semantic level shows 51.4% equivalence rate for GPT-5 in correctly interpreting safety rules via LTL translation
- Plan-level safety compliance drops to 73.9% safe plans, revealing gaps between semantic understanding and planning
- Trajectory-level execution further declines in safety compliance due to physical interactions, with plan-level checks missing critical violations
- Framework successfully identifies safety violations overlooked by prior heuristic or NL-only evaluation methods

## Why This Works (Mechanism)

### Mechanism 1: Formal Semantic Grounding via Temporal Logic
Translating natural language safety requirements into Linear Temporal Logic (LTL) reduces ambiguity compared to heuristic evaluations. The framework converts NL constraints into formal LTL formulas and validates LLM interpretation by checking semantic equivalence against ground-truth formulas using automata-theoretic methods (converting formulas to Büchi automata to check language containment). This ensures the LLM's understanding aligns with the intended safety semantics rather than relying on surface-level text similarity.

### Mechanism 2: Progressive Abstraction Decomposition
Evaluating safety at three distinct levels (Semantic, Plan, Trajectory) isolates failure modes invisible in single-level evaluation. The pipeline enforces a waterfall check: first verifying semantic understanding, then checking high-level plan violations, and finally simulating physical execution. This prevents semantic errors from masking execution errors and provides granular failure diagnosis, addressing the difficulty of identifying why models fail highlighted in related work.

### Mechanism 3: Branching-Time Verification via Computation Trees
Aggregating multiple stochastic execution trajectories into a single Computation Tree Logic (CTL) structure enables exhaustive safety verification across nondeterministic outcomes. Instead of verifying individual linear trajectories, the framework merges execution paths into a computation tree and applies CTL model checking (e.g., `AG` for "all paths globally") to verify safety properties hold across all possible branching futures resulting from LLM variability.

## Foundational Learning

- **Linear Temporal Logic (LTL) & Computation Tree Logic (CTL)**: These formal languages define "safety" using operators like `G` (Globally/Always), `F` (Future/Eventually), and path quantifiers `A` (All) vs `E` (Exists). Quick check: Does `G(OvenOn -> F OvenOff)` mean the oven must be off immediately after turning on, or just eventually?

- **Büchi Automata & Language Containment**: The framework uses this to check if the LLM's generated formula means the same thing as ground truth by verifying logic, not just syntax. Quick check: If Formula A accepts a trace where the oven stays on forever, and Formula B does not, are they equivalent?

- **Sim-to-Real Gap & Physics Granularity**: The paper notes plan-level checks miss physical nuances (e.g., collisions) only appearing in trajectory simulation. Quick check: Can a high-level plan verify "do not spill water," or does that require a physics engine?

## Architecture Onboarding

- **Component map**: Input (NL Task + NL Safety Constraints) -> Semantic Layer (LLM Translator -> LTL Formulas -> Equivalence Checker) -> Plan Layer (LLM Planner -> High-Level Plan -> LTL Model Checker) -> Trajectory Layer (LLM Actor -> Action Sequences -> Simulator -> Computation Tree -> CTL Model Checker)

- **Critical path**: The Semantic Layer. If the LLM mistranslates the NL safety rule into LTL (e.g., misinterpreting "until" as "eventually"), all downstream plan and trajectory checks validate the wrong constraint.

- **Design tradeoffs**: Formality vs. Computation Cost (full computation trees are rigorous but expensive; Plan-level LTL is fast but misses physical dynamics). LLM vs. Hard-Coded Rules (LLM flexibility introduces stochasticity requiring CTL trees; hard-coded planners are deterministic but brittle).

- **Failure signatures**: High Syntax Error (malformed LTL frequent in smaller models), Semantic Drift (syntactically valid but logically distinct LTL), Execution Drop-off (plan safe but trajectory unsafe indicating control/physics issues).

- **First 3 experiments**: 1) Sanity Check: Replicate Semantic evaluation (Table 2) using Llama-3-8B on 10 sample constraints to verify syntax vs. equivalence rates. 2) Plan vs. Trajectory: Run "placing phone near water" task to confirm Plan-level pass but Trajectory-level fail. 3) Branching Analysis: Execute same prompt 5 times, build computation tree, verify CTL checker catches violations present in only 1 branch.

## Open Questions the Paper Calls Out
None

## Limitations
- Ground-truth LTL formula ambiguity may incorrectly penalize or accept LLM translations if formulas don't perfectly capture NL intent
- Simulator fidelity limitations may miss real-world physical interactions, affecting ecological validity of safety findings
- Stochastic sampling coverage uncertainty (e.g., N=5 trajectories) may miss rare but critical safety violations in high-probability branches

## Confidence
- **High Confidence**: Formal mechanism of grounding NL rules in LTL with automata-based equivalence checking is mathematically sound
- **Medium Confidence**: Experimental results demonstrating safety drop-offs between planning and execution levels are compelling
- **Low Confidence**: Absolute safety compliance numbers heavily depend on specific LLMs, ground-truth formula quality, and simulator fidelity

## Next Checks
1. Conduct blind review where safety experts evaluate whether ground-truth LTL formulas accurately represent intended NL safety constraints, correcting any semantic misalignments before re-running equivalence evaluation

2. Perform controlled experiment where known physical safety violations (e.g., spilling water, short circuits) are deliberately scripted into the simulator to verify trajectory-level CTL checks consistently detect these violations

3. Systematically increase sampled trajectories (N) from 5 to 20, 50, and 100 to measure how detected safety violations change with increased sampling, quantifying risk of missing rare critical failure modes