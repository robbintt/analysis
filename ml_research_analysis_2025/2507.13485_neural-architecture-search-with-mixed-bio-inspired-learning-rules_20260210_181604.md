---
ver: rpa2
title: Neural Architecture Search with Mixed Bio-inspired Learning Rules
arxiv_id: '2507.13485'
source_url: https://arxiv.org/abs/2507.13485
tags:
- learning
- search
- neural
- rules
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BioNAS, a neural architecture search framework
  that explores the use of different bio-inspired learning rules across layers in
  a neural network. The key idea is to extend standard NAS methods (DARTS and EG-NAS)
  by incorporating biologically plausible learning rules such as Feedback Alignment
  (FA), Uniform Sign-concordant Feedbacks (uSF), Batchwise Random Magnitude Sign-concordant
  Feedbacks (brSF), and Hebbian learning into the search space.
---

# Neural Architecture Search with Mixed Bio-inspired Learning Rules

## Quick Facts
- arXiv ID: 2507.13485
- Source URL: https://arxiv.org/abs/2507.13485
- Reference count: 40
- BioNAS outperforms single-rule bio-inspired models, achieving 95.16% accuracy on CIFAR-10

## Executive Summary
This paper introduces BioNAS, a neural architecture search framework that extends standard NAS methods (DARTS and EG-NAS) by incorporating biologically plausible learning rules across layers. The key innovation is exploring a search space where each candidate operation is paired with a bio-inspired learning rule (Feedback Alignment, Uniform Sign-concordant Feedbacks, Batchwise Random Magnitude Sign-concordant Feedbacks, or Hebbian learning). BioNAS automatically discovers optimal combinations of architectures and learning rules for each layer, achieving state-of-the-art results for bio-inspired networks: 95.16% accuracy on CIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on ImageNet. Notably, it demonstrates superior adversarial robustness compared to standard backpropagation models, particularly against black-box attacks.

## Method Summary
BioNAS extends differentiable architecture search (DARTS) and evolutionary NAS (EG-NAS) by expanding the search space from operations to operation-rule pairs. The framework searches for both architecture and learning rule simultaneously, where each edge in the supernetwork selects from 32 candidates (8 operations × 4 learning rules). The search uses SGD with specific hyperparameters (batch 256, LR 0.1, 50 epochs for search; batch 96, LR 0.025, 600 epochs for retraining). Key components include bio-inspired learning rule implementations from the Biotorch library, custom Hebbian convolution with outer-product updates, and auxiliary towers for regularization. The method derives discrete architectures by selecting the highest-scoring operation-rule pair per edge, then retrains standalone architectures using the discovered layer-wise rule assignments.

## Key Results
- Achieves 95.16% accuracy on CIFAR-10, surpassing single-rule bio-inspired models
- Demonstrates superior adversarial robustness (60.6% accuracy under PGD vs. 0% for standard BP)
- Shows that random rule assignments achieve nearly identical performance to searched architectures (94.86% vs 94.87%), suggesting layer-wise diversity itself drives gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mixing different bio-inspired learning rules across layers reduces gradient variance and stabilizes optimization compared to using a single rule network-wide.
- **Mechanism:** Each learning rule computes feedback matrices differently, creating heterogeneous gradient dynamics. This diversity prevents correlated gradient fluctuations that accumulate when one rule is applied uniformly, effectively regularizing the learning trajectory.
- **Core assumption:** Lower gradient variance correlates with better convergence and generalization in bio-inspired training regimes.
- **Evidence anchors:**
  - [abstract] "layer-wise diversity in learning rules allows better scalability and accuracy"
  - [section 4.3] "BioNAS-DARTS trained with mixed learning rules exhibits consistently lower gradient variance compared to training with a fixed rule"
  - [corpus] Weak direct support; neighboring papers focus on spiking networks and quantization, not gradient variance in mixed-rule training.

### Mechanism 2
- **Claim:** Layer-wise heterogeneity in learning rules disrupts adversarial gradient alignment, improving robustness against gradient-based attacks.
- **Mechanism:** Adversarial attacks optimize perturbations along consistent gradient directions. Mixed rules create inconsistent gradient signals across layers—attackers face a "moving target" where perturbations crafted for one layer's gradient structure misalign with another's.
- **Core assumption:** Adversarial transferability and effectiveness depend on gradient coherence across network depth.
- **Evidence anchors:**
  - [abstract] "demonstrates superior adversarial robustness compared to standard backpropagation models, particularly against black-box attacks"
  - [section 4.1] BioNAS-DARTS maintains 60.6% accuracy under PGD while single-rule models collapse to 0%
  - [corpus] No direct corpus support; robustness papers focus on robust NAS with adversarial training, not bio-inspired rules.

### Mechanism 3
- **Claim:** Jointly searching architecture and learning rules discovers layer-rule couplings that outperform hand-designed combinations.
- **Mechanism:** The search space expands from K operations to K × R operation-rule pairs per edge. Optimization allocates rules where their inductive biases match layer requirements—e.g., sign-concordant rules in deep layers where gradient sign preservation matters more.
- **Core assumption:** Optimal learning rules are layer-dependent, not global; NAS can discover this dependency.
- **Evidence anchors:**
  - [abstract] "use NAS to find the best architecture and learning rule to use in each layer"
  - [section 3.2] "each candidate operation is a pair (o_k, r_k) combining a computational block...and a learning rule"
  - [section 4.2, Table 5] Random rule assignments achieve comparable accuracy (~94.8%), suggesting layer-wise diversity itself—not specific patterns—drives gains.

## Foundational Learning

- **Concept: Credit Assignment and Weight Transport Problem**
  - **Why needed here:** Bio-inspired rules exist because backpropagation requires symmetric forward/backward weights, which is biologically implausible. Understanding this motivates why FA, uSF, and brSF replace transposed weights with random or sign-concordant approximations.
  - **Quick check question:** Can you explain why using fixed random matrices B_i instead of W_i^T for gradients still enables learning?

- **Concept: Differentiable Architecture Search (DARTS) Basics**
  - **Why needed here:** BioNAS extends DARTS by adding learning rules to the search space. You need to understand continuous relaxation of discrete architectures, bilevel optimization (α for architecture, w for weights), and softmax operation selection.
  - **Quick check question:** How does DARTS convert a discrete operation choice into a differentiable search problem?

- **Concept: Bio-inspired Learning Rule Taxonomy**
  - **Why needed here:** The framework selects among FA (random feedback), uSF (sign of weights), brSF (random magnitude × sign), and Hebbian (co-activation). Each has different biological plausibility vs. performance tradeoffs.
  - **Quick check question:** What is the key difference between uSF and brSF in how they compute feedback matrices?

## Architecture Onboarding

- **Component map:** Input -> Supernetwork (DAG with operation-rule pairs) -> Search controller (DARTS/EG-NAS) -> Learning rule modules -> Discretization -> Final architecture
- **Critical path:**
  1. Define search space with operation-rule pairs (start with 4 rules: FA, uSF, brSF, Hebbian + 8 operations)
  2. Train supernetwork for 50 epochs (CIFAR-10/100) with validation split
  3. Derive discrete architecture by selecting top operation-rule per edge
  4. Retrain standalone architecture for 600 epochs with discovered rules
- **Design tradeoffs:**
  - Search space size vs. efficiency: 32 choices/edge quadruples DARTS space; EG-NAS (0.35 GPU-days) faster than DARTS (1.37 GPU-days) but slightly lower final accuracy
  - Biological plausibility vs. accuracy: Hebbian/predictive-coding layers reduce accuracy (18.08% error) vs. feedback-alignment variants (4.84%)
  - Robustness vs. clean accuracy: BioNAS trades some clean ImageNet accuracy (60.51%) vs. BP baselines (73.3%) for superior adversarial robustness
- **Failure signatures:**
  - Search collapses to skip connections: Common DARTS issue; mitigate with drop path regularization (0.2–0.3)
  - Single rule dominates: If search selects same rule for all layers, check learning rate balance between α and w updates
  - Validation accuracy much lower than expected on ImageNet16-120: Search-phase supernetwork underfits; increase supernetwork channels (16→48) or epochs
  - Hebbian layers cause divergence: Weight explosion; ensure normalization enabled in Hebbian convolution
- **First 3 experiments:**
  1. Reproduce CIFAR-10 baseline: Run BioNAS-DARTS with default hyperparameters (36 channels, 20 layers, 50 search epochs, 600 retrain epochs). Target: ~95% accuracy.
  2. Ablate rule diversity: Take discovered architecture, retrain with single rule (e.g., all FA). Compare gradient variance and accuracy to mixed-rule version.
  3. Robustness sanity check: Apply FGSM (ε=0.35) and PGD to BioNAS vs. ResNet56-BP. Confirm BioNAS maintains >60% accuracy under PGD while BP collapses.

## Open Questions the Paper Calls Out

- **Theoretical explanation:** While observations suggest mixing rules behaves like regularization, "a complete theoretical formulation and proof is left for future work." The paper relies on empirical results but lacks mathematical derivation for why heterogeneous feedback matrices prevent overfitting or aid convergence.

- **Learned representations:** The conclusion explicitly notes that "Further investigation should be done on the learning dynamics in this searched architecture and the representations learned." The paper doesn't analyze if feature maps or latent spaces differ qualitatively from standard backpropagation models.

- **Computational overhead justification:** Table 5 shows random rule assignments achieve accuracy (94.86%) nearly identical to the searched model (94.87%), suggesting the benefit comes from mere heterogeneity rather than specific optimization. This undermines the necessity of the proposed NAS framework for selecting rules.

## Limitations

- **Search space specification:** The paper doesn't clarify if the search explores all 32 combinations or uses a reduced space for efficiency.
- **Learning rule implementation:** Critical details about how mixed rules are integrated into autograd are delegated to external libraries without full algorithmic specification.
- **Adversarial robustness claims:** The paper doesn't analyze whether robustness gains stem from inherent properties of bio-inspired rules or artifacts of specific attack implementations.

## Confidence

- **High confidence:** CIFAR-10/100 accuracy results and comparison to standard BP models (95.16% and 76.48% accuracy are well-documented with clear methodology).
- **Medium confidence:** ImageNet16-120 results (43.42%) due to limited detail on how the 16×16 image patches were handled during search vs. training.
- **Low confidence:** ImageNet adversarial robustness (60.51% vs. 73.3% clean accuracy) without clear explanation of why such a large gap exists.

## Next Checks

1. **Search space verification:** Re-run BioNAS-DARTS with a controlled search space (e.g., limit to 2 operations × 4 rules = 8 candidates) to verify whether gains scale with search space size or plateau.

2. **Robustness attack diversity:** Test BioNAS against adaptive attacks that explicitly account for mixed rule structures (e.g., per-layer PGD with rule-specific gradient approximation) to distinguish between fundamental robustness and attack-specific artifacts.

3. **Gradient variance measurement:** Systematically measure and compare gradient variance across single-rule and mixed-rule networks on CIFAR-10/100 to validate whether lower variance correlates with better generalization beyond this paper's reported findings.