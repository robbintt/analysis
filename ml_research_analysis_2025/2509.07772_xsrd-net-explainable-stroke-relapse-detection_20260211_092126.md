---
ver: rpa2
title: 'XSRD-Net: EXplainable Stroke Relapse Detection'
arxiv_id: '2509.07772'
source_url: https://arxiv.org/abs/2509.07772
tags:
- stroke
- data
- relapse
- multimodal
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed XSRD-Net, a multimodal deep learning model
  for predicting stroke relapse and relapse-free survival (RFS) time using 3D intracranial
  CTA images and clinical tabular data (age, gender, coronary heart disease, peripheral
  artery disease) from 119 stroke patients. The model demonstrated strong binary classification
  performance (AUC 0.84) using tabular data alone, while multimodal fusion improved
  RFS time prediction with 68% vision and 32% tabular modality contribution.
---

# XSRD-Net: EXplainable Stroke Relapse Detection

## Quick Facts
- arXiv ID: 2509.07772
- Source URL: https://arxiv.org/abs/2509.07772
- Reference count: 29
- Primary result: Developed multimodal deep learning model achieving AUC 0.84 for binary stroke relapse classification using 119 patients

## Executive Summary
XSRD-Net is a multimodal deep learning model designed to predict stroke relapse and relapse-free survival time by combining 3D intracranial CTA images with clinical tabular data. The model demonstrates strong binary classification performance (AUC 0.84) using tabular data alone, while multimodal fusion improves RFS time prediction with 68% vision and 32% tabular modality contribution. The system aims to provide explainable predictions to identify carotid artery features and heart disease indicators as critical biomarkers for stroke recurrence risk.

## Method Summary
The model integrates 3D intracranial CTA images with clinical variables including age, gender, coronary heart disease, and peripheral artery disease from 119 stroke patients. It employs multimodal deep learning architecture with separate processing streams for imaging and tabular data, followed by fusion mechanisms to predict both binary relapse classification and continuous RFS time regression. Interpretability techniques are applied to identify which features contribute most to prediction outcomes.

## Key Results
- Binary classification achieved AUC 0.84 using tabular data alone
- Multimodal fusion showed 68% contribution from vision and 32% from tabular data for RFS prediction
- Regression task achieved c-index of 0.56 overall, improving to 0.68 for relapse cases
- Subsequent classification from regression predictions yielded AUC of 0.71
- Identified carotid artery features and heart disease indicators as critical relapse predictors

## Why This Works (Mechanism)
The multimodal approach leverages complementary information from anatomical imaging and clinical risk factors, with the fusion mechanism effectively weighting each modality's contribution based on its predictive value. The model's ability to identify specific carotid artery features and cardiovascular comorbidities aligns with established stroke pathophysiology, where carotid stenosis and heart disease are known risk factors for recurrence.

## Foundational Learning
- Multimodal deep learning: Combines different data types to improve prediction accuracy by capturing complementary information
- C-index measurement: Evaluates ranking quality of survival predictions, where 0.56 indicates moderate discriminative ability
- Interpretability in medical AI: Critical for clinical adoption, allowing clinicians to understand and trust model predictions
- Stroke relapse risk factors: Understanding how imaging features and clinical variables correlate with recurrence risk

## Architecture Onboarding
Component map: Clinical tabular data -> Feature extraction -> Tabular encoder -> Fusion layer -> Output
                   CTA images -> 3D CNN backbone -> Image encoder -> Fusion layer -> Output
Critical path: CTA image processing and clinical data encoding converge at fusion layer, which determines final prediction weights
Design tradeoffs: Balanced multimodal fusion vs. potential overfitting with limited patient data; interpretability vs. model complexity
Failure signatures: Poor regression performance on RFS time (c-index 0.56) suggests limitations in capturing temporal progression patterns
First experiments: 1) Test binary classification with tabular data only, 2) Evaluate image-only model performance, 3) Compare different fusion strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 119 patients may lead to overfitting, particularly for regression tasks
- Limited to single imaging modality (CTA) without exploring other potentially relevant sequences
- Multimodal fusion methodology lacks detailed architectural specifications for reproducibility
- Need for external validation on independent datasets to confirm generalizability

## Confidence
- Binary classification performance: High
- RFS time prediction accuracy: Medium
- Interpretability findings: Medium
- Multimodal fusion effectiveness: Medium

## Next Checks
1. External validation on an independent stroke patient cohort of at least 200 patients to verify model generalization
2. Ablation study testing the impact of each clinical variable and imaging feature on prediction accuracy
3. Prospective clinical trial to assess whether the model's risk stratification improves patient outcomes compared to standard care