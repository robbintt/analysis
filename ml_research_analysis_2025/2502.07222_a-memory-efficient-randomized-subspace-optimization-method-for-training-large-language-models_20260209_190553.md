---
ver: rpa2
title: A Memory Efficient Randomized Subspace Optimization Method for Training Large
  Language Models
arxiv_id: '2502.07222'
source_url: https://arxiv.org/abs/2502.07222
tags:
- memory
- training
- adam
- optimizer
- galore
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a memory-efficient training method for large
  language models that addresses the dual challenge of reducing both optimizer state
  and activation memory. The core idea is a randomized subspace optimization framework
  that decomposes high-dimensional training into lower-dimensional subproblems, selecting
  random subspaces at each iteration to optimize parameters within.
---

# A Memory Efficient Randomized Subspace Optimization Method for Training Large Language Models

## Quick Facts
- arXiv ID: 2502.07222
- Source URL: https://arxiv.org/abs/2502.07222
- Reference count: 40
- Primary result: Proposes RSO method achieving up to 70% optimizer state memory reduction and significant activation memory savings while maintaining Adam/GaLore-level performance

## Executive Summary
This paper introduces Randomized Subspace Optimization (RSO), a memory-efficient training method for large language models that addresses both optimizer state and activation memory bottlenecks. The core innovation is decomposing high-dimensional parameter optimization into lower-dimensional subproblems by selecting random subspaces at each iteration. RSO achieves substantial memory savings (up to 70% for optimizer states) while maintaining competitive performance with standard optimizers. The method also reduces inter-device communication overhead in distributed training, yielding faster iteration times.

## Method Summary
RSO optimizes LLM parameters by projecting them onto randomly selected low-dimensional subspaces. At each iteration, a random projection matrix P^k is sampled, and parameters within that subspace are optimized through subproblems solved with first-order methods like Adam. The method computes gradients with respect to low-rank B matrices rather than full weight matrices, requiring storage of Y_ℓP_ℓ instead of Y_ℓ for activation memory savings. The approach simultaneously reduces optimizer state memory (storing moments for B ∈ ℝ^{r×n} instead of W ∈ ℝ^{m×n}) and activation memory, with theoretical convergence guarantees under smoothness assumptions and random projection properties.

## Key Results
- Achieves up to 70% reduction in optimizer state memory compared to Adam
- Reduces activation memory from 15bsn + 2bs² to 8bsn + 4bsr + 2bs² for transformer blocks
- Maintains comparable validation perplexity to Adam and GaLore on pre-training tasks
- Demonstrates 3× faster iteration time on LLaMA-7B with sequence length 64 due to reduced communication overhead
- Validates memory and communication efficiency advantages across LLaMA and OPT models of varying sizes

## Why This Works (Mechanism)

### Mechanism 1
Random subspace projection enables high-dimensional optimization through iterated low-dimensional subproblems while preserving convergence guarantees. At each iteration k, sample a random projection matrix P^k ∈ ℝ^{m_ℓ×r_ℓ} where r_ℓ ≪ m_ℓ, then solve the subproblem min_B f(W^k + P^k B) + (1/2η_k)||B||² in the lower-dimensional space. The proximal term ensures strong convexity. Critical property: E[P_ℓ P_ℓ^T] = I ensures unbiased gradient estimation in expectation.

### Mechanism 2
RSO achieves activation memory reduction by computing gradients with respect to low-rank B_ℓ rather than full W_ℓ, requiring storage of Y_ℓ P_ℓ ∈ ℝ^{s×r} instead of Y_ℓ ∈ ℝ^{s×m}. In forward pass, compute Z_ℓ = Y_ℓ(W_ℓ + P_ℓ B_ℓ). In backward pass, gradient ∂y/∂B_ℓ = (Y_ℓ P_ℓ)^T (∂y/∂Z_ℓ) only requires the projected activation Y_ℓ P_ℓ (size s×r) rather than full Y_ℓ (size s×m).

### Mechanism 3
Lower-dimensional gradients reduce inter-device communication overhead in distributed data-parallel training, yielding faster wall-clock time per iteration. In DDP, gradients are aggregated across devices. RSO gradients have dimension r×n per layer versus m×n for Adam/GaLore, reducing communication volume by factor r/m. Paper reports 3× faster iteration time for LLaMA-7B with seq_len=64.

## Foundational Learning

- **Random Coordinate/Subspace Descent**
  - Why needed here: RSO extends coordinate descent principles to subspaces; understanding why optimizing one coordinate/subspace at a time converges requires grasping unbiased gradient estimation and Lipschitz smoothness.
  - Quick check question: Why does E[||∇g_k(0)||²] = E[||∇f(W^k)||²] hold under Assumption 5.3?

- **Adam Optimizer Mechanics (First/Second Moments)**
  - Why needed here: RSO uses Adam to solve subproblems; memory savings come from storing moments for B (size r×n) not W (size m×n). You must understand what M_t, V_t represent.
  - Quick check question: If Adam states require 24n² memory for a weight matrix W ∈ ℝ^{n×n}, how much memory do they require for B ∈ ℝ^{r×n}?

- **Backpropagation Memory Flow (Activations vs. Parameters)**
  - Why needed here: Activation memory depends on batch size and sequence length; understanding why Y_ℓ must be stored for ∂y/∂W_ℓ but only Y_ℓ P_ℓ for ∂y/∂B_ℓ is essential for diagnosing memory bottlenecks.
  - Quick check question: For a linear layer Y ∈ ℝ^{1024×4096} (batch×hidden), P ∈ ℝ^{4096×256}, what is the memory ratio between storing Y vs. YP?

## Architecture Onboarding

- Component map: Projection Sampler -> Subproblem Solver (Adam) -> Weight Updater -> Memory Allocator
- Critical path:
  1. Forward pass computes Z_ℓ = Y_ℓ(W_ℓ + P_ℓ B_ℓ); store only Y_ℓ P_ℓ for backward
  2. Backward computes ∂y/∂B_ℓ via (Y_ℓ P_ℓ)^T (∂y/∂Z_ℓ); Adam updates moments on B
  3. Weight update: W_ℓ ← W_ℓ + P_ℓ B̃_ℓ; discard B state if changing subspace
- Design tradeoffs:
  - Rank r: Lower r → more memory savings but potentially slower convergence (condition number scales with m/r)
  - Subspace update frequency: Paper uses fixed intervals; adaptive schemes may help but lack theoretical guarantees
  - Projection type: SVD (GaLore) captures gradient structure but costs O(mnr); random (RSO) is free but may require higher r
- Failure signatures:
  - Validation loss diverges: η_k too large; check that η_k ≤ 1/(2L̂) where L̂ = max_ℓ{m_ℓ/r_ℓ}L
  - Memory not reduced: P matrices not shared across Q/K/V; or gradient checkpointing interfering with activation savings
  - Slower than Adam on single-GPU: Communication benefits absent; computation overhead dominates (switch to higher r)
- First 3 experiments:
  1. Memory profiling: Train LLaMA-350M with RSO (r=256) vs Adam; measure peak GPU memory with batch_size=64, seq_len=256. Expect ~40% reduction per Table 1.
  2. Convergence parity: Pre-train on C4 subset (1B tokens); plot validation perplexity curves for RSO, GaLore, Adam. RSO should track Adam within 1-2 perplexity points.
  3. Ablation on rank: Run r ∈ {64, 128, 256, 512} on LLaMA-1B; plot memory vs perplexity tradeoff frontier. Identify inflection point where increasing r yields diminishing returns.

## Open Questions the Paper Calls Out

### Open Question 1
Can second-order optimization methods applied to the low-dimensional subproblems significantly improve convergence rates or final accuracy compared to first-order solvers like Adam? The paper explicitly states it is worth investigating the application of second-order methods for solving subproblems due to their low-dimensional nature, but empirical validation is absent.

### Open Question 2
Can alternative strategies for partitioning the original training problem further reduce the memory overhead associated with the remaining activations not covered by the current RSO decomposition? The paper identifies this as a primary direction for future work, suggesting the current approach still requires storing some activations.

### Open Question 3
Does the reliance on random projection matrices, as opposed to gradient-informed low-rank approximations like SVD used in GaLore, limit the optimization efficiency on tasks requiring high-rank weight updates? While experiments show comparable perplexity, the theoretical trade-off between directionality of SVD-based subspaces and randomness of RSO subspaces is not fully explored.

## Limitations

- Theoretical convergence analysis assumes Haar-distributed projections while practical implementations likely use Gaussian approximations, creating a gap between theory and practice
- Memory reduction benefits are most pronounced at shorter sequence lengths where communication overhead dominates; at longer sequences, computational overhead may diminish these advantages
- Paper does not explore adaptive subspace selection strategies which could potentially improve convergence but lack theoretical guarantees

## Confidence

- **High confidence:** Memory reduction claims (70% optimizer state reduction, activation savings from 15bsn to 8bsn+4bsr), and performance parity with Adam/GaLore on downstream tasks
- **Medium confidence:** Theoretical convergence guarantees, as they rely on idealized projection distributions and may not fully account for practical approximations
- **Medium confidence:** Communication efficiency gains, as the corpus lacks direct evidence of distributed training performance analysis

## Next Checks

1. **Convergence gap analysis:** Compare training curves between Haar-distributed projections (theoretical ideal) and Gaussian approximations (practical implementation) on a small model to quantify the convergence gap

2. **Scaling study at long sequences:** Evaluate RSO performance on LLaMA-7B with sequence length 2048 to determine when communication benefits diminish and computational overhead dominates

3. **Adaptive subspace evaluation:** Implement a simple importance-based subspace selection (e.g., based on gradient magnitude) and measure convergence speed versus the fixed-rank approach, documenting any trade-offs with theoretical guarantees