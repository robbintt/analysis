---
ver: rpa2
title: Learning Composable Chains-of-Thought
arxiv_id: '2505.22635'
source_url: https://arxiv.org/abs/2505.22635
tags:
- compositional
- letter
- data
- atomic
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of compositional generalization
  in large language models, where the goal is to combine atomic reasoning skills to
  solve unseen, more complex tasks. The authors propose a data augmentation scheme
  called Composable Chain-of-Thought (CoT), which modifies the format of atomic CoT
  training data to include explicit prefix and suffix tags, enabling models to generate
  composable reasoning traces.
---

# Learning Composable Chains-of-Thought

## Quick Facts
- **arXiv ID**: 2505.22635
- **Source URL**: https://arxiv.org/abs/2505.22635
- **Reference count**: 40
- **Primary result**: Up to 95.4% exact match accuracy on compositional string tasks in zero-shot settings

## Executive Summary
This work addresses the challenge of compositional generalization in large language models, where the goal is to combine atomic reasoning skills to solve unseen, more complex tasks. The authors propose a data augmentation scheme called Composable Chain-of-Thought (CoT), which modifies the format of atomic CoT training data to include explicit prefix and suffix tags, enabling models to generate composable reasoning traces. They show that training atomic CoT models with Composable CoT and combining them via multitask learning or model merging leads to improved zero-shot compositional reasoning performance compared to standard CoT formats. Further fine-tuning with rejection sampling on limited compositional data boosts performance even more.

## Method Summary
The Composable CoT approach transforms standard atomic CoT data by adding explicit structural tags. Each sample is duplicated: one copy wraps the reasoning trace in `<prefix>...</prefix>` tags, while the other appends random characters as a "proxy prefix" to the prompt and wraps the reasoning in `<suffix>...</suffix>` tags. During inference, models generate the prefix block, append the suffix tag, and continue generation. The method is evaluated using both multitask learning (training on concatenated augmented data) and model merging (training separate models and combining them via Task Arithmetic). Training uses LoRA fine-tuning on all linear layers with rank 8, α=16, and dropout 0.2 for 5 epochs.

## Key Results
- Achieves 95.4% exact match accuracy on compositional string tasks in zero-shot settings
- Outperforms standard CoT formats on zero-shot compositional reasoning
- Shows strong gains with limited compositional training data (as few as 100 samples)
- Demonstrates effectiveness across both multitask learning and model merging approaches

## Why This Works (Mechanism)
The explicit prefix/suffix tags provide structural scaffolding that guides the model to treat reasoning components as composable units. By training on data where reasoning traces are explicitly marked as either generating from scratch (prefix) or building upon previous outputs (suffix), the model learns to recognize when it should initiate reasoning versus when it should continue and build upon existing intermediate results. This structural guidance helps overcome the model's tendency to repeat or ignore compositional dependencies.

## Foundational Learning
- **Compositional Generalization**: The ability to combine learned skills to solve novel tasks. Needed because standard training doesn't teach models how to chain reasoning steps together.
- **Chain-of-Thought Reasoning**: Step-by-step reasoning traces that improve performance on complex tasks. Critical for breaking down problems into manageable sub-steps.
- **Task Arithmetic Model Merging**: Combining separately trained models by weighted addition of their parameters. Useful for efficiently creating models that handle multiple skills without retraining from scratch.
- **LoRA Fine-tuning**: Parameter-efficient fine-tuning using low-rank adapters. Enables effective adaptation while maintaining model generalization.
- **Rejection Sampling**: Filtering training data to select only successful generations. Helps create higher-quality training sets from limited compositional data.

## Architecture Onboarding

**Component Map**: Atomic CoT Data -> Composable CoT Augmentation -> Model Training (MTL/Merge) -> Zero-shot Compositional Evaluation

**Critical Path**: Data Augmentation → Model Training → Inference with Tag Continuation

**Design Tradeoffs**: Explicit tags vs. implicit learning; multitask learning vs. model merging efficiency; data augmentation overhead vs. performance gains

**Failure Signatures**:
- Pattern Repetition: Model generates identical reasoning in both prefix and suffix
- Generation Halt: Model stops after prefix block without continuing
- Context Ignorance: Suffix reasoning doesn't depend on prefix output

**First Experiments**:
1. Train a single atomic model with Composable CoT format and evaluate zero-shot composition
2. Compare multitask learning vs. model merging performance on string tasks
3. Test inference continuation logic with manual suffix tag appending

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily uses synthetic string manipulation and controlled skill composition tasks
- Method's reliance on explicit prefix/suffix tags may limit generalization to tag-free scenarios
- Model merging parameters were tuned on validation data, suggesting potential overfitting
- Claims based on Llama 2 7B and Qwen 2.5 7B may not generalize to smaller models

## Confidence

**High Confidence**:
- The core method of using explicit structural tags to enable compositional reasoning
- Demonstration that both multitask learning and model merging benefit from augmented format

**Medium Confidence**:
- Claims of significant improvement over standard CoT formats (varies by task type)
- Performance gains with limited compositional training data

## Next Checks
1. Test generalization beyond synthetic tasks to real-world compositional reasoning domains
2. Evaluate model merging without validation-set tuning of α and β parameters
3. Compare against alternative compositional approaches without explicit structural tags