---
ver: rpa2
title: How important is Recall for Measuring Retrieval Quality?
arxiv_id: '2512.20854'
source_url: https://arxiv.org/abs/2512.20854
tags:
- response
- figure
- arxiv
- score
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates recall-independent measures for evaluating
  retrieval quality in RAG systems, where the total number of relevant documents is
  unknown. The authors propose a simple retrieval quality measure "T" that does not
  require knowledge of total relevant documents.
---

# How important is Recall for Measuring Retrieval Quality?

## Quick Facts
- arXiv ID: 2512.20854
- Source URL: https://arxiv.org/abs/2512.20854
- Reference count: 40
- The paper introduces T, a recall-independent retrieval quality measure for RAG systems that performs comparably to F-measure while avoiding the need for total relevant document counts.

## Executive Summary
This paper addresses the challenge of evaluating retrieval quality in RAG systems when the total number of relevant documents (Np) is unknown. The authors propose T, a simple retrieval quality measure that approximates F-measure without requiring Np. Through experiments on multiple datasets with low numbers of relevant documents (2-15), they show T performs comparably to traditional F-measure while avoiding recall dependency. The study reveals that the optimal retrieval metric depends critically on the K/Np ratio, with T excelling when K ≤ Np and nDCG becoming superior at higher ratios. Surprisingly, estimating Np from top-2K documents (Fe) can outperform using the true Np at high K/Np ratios, suggesting that higher-ranked relevant documents are more valuable for LLM response quality.

## Method Summary
The authors combine six existing datasets (ARXIV, HotpotQA, MSMARCO, Natural Questions) with LLM-generated responses to create a "retrieval-response" dataset containing 6,112 query-text samples. They implement four retrieval quality measures: F (traditional F-measure), T (their recall-independent measure), Fe (F with estimated Np from top-2K), and nDCG. Using four embedding models, they retrieve top-K documents, generate responses with GPT-4o-mini, and have another LLM judge response quality on a 1-5 scale. The study segments results by K/Np ratio and optimizes α for each metric-segment combination to maximize Spearman correlation with LLM quality judgments.

## Key Results
- T performs comparably to F-measure at low K/Np ratios (≤1) while avoiding recall dependency
- nDCG becomes superior to T and F at high K/Np ratios due to document ordering importance
- Fe (estimating Np from top-2K) can outperform true F at high K/Np ratios, suggesting higher-ranked relevant documents are more valuable
- Optimal α values vary by K/Np segment and dataset, requiring per-segment tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: T can approximate F-measure quality assessment without requiring knowledge of total relevant documents.
- Mechanism: T = ((1-α)nₚ - α·nₙ)/K normalizes the count of relevant vs irrelevant documents in top-K, removing Np dependency while preserving the precision-recall tradeoff semantics.
- Core assumption: Retrieval quality is primarily determined by the balance of relevant vs irrelevant documents within the retrieved set, not their proportion to the total relevant corpus.
- Evidence anchors: [abstract], [section 2.1]
- Break condition: When K/Np >> 1, T loses predictive power because it doesn't account for document ordering within increasingly noisy results.

### Mechanism 2
- Claim: nDCG outperforms other measures at high K/Np ratios because document order becomes critical for LLM response quality.
- Mechanism: When K >> Np, many irrelevant documents are inevitably retrieved. nDCG's position-aware scoring captures that presenting relevant documents first improves LLM synthesis.
- Core assumption: LLMs are sensitive to document order; irrelevant documents appearing early degrade response quality more than those appearing later.
- Evidence anchors: [section 3.3], [section 4]
- Break condition: At low K/Np ratios (K ≤ Np), ordering effects diminish because fewer irrelevant documents dilute the signal.

### Mechanism 3
- Claim: Using estimated Np from top-2K (Fe) can produce better response quality predictions than true Np at high K/Np ratios.
- Mechanism: Relevant documents appearing in top-2K are inherently higher-quality—closer to query by embedding similarity—so weighting them more heavily better reflects their actual contribution to LLM response quality.
- Core assumption: Not all relevant documents are equally valuable for RAG; proximity to query correlates with informativeness for generation.
- Evidence anchors: [section 3.3], [section 4]
- Break condition: When Np is small or K is low, the estimate becomes unreliable—too few positives in top-2K for meaningful estimation.

## Foundational Learning

- **Concept: F-Measure (Harmonic Mean of Precision and Recall)**
  - Why needed here: Understanding F is essential to appreciate what T approximates and why removing Np dependency matters for practical systems.
  - Quick check question: If precision = 0.8 and recall = 0.4 with α = 0.5, what is the F score?

- **Concept: K/Np Ratio and Its Implications**
  - Why needed here: This ratio is the key decision variable determining which retrieval metric will be most predictive—T performs well at K/Np ≤ 1, nDCG excels at K/Np >> 1.
  - Quick check question: If K = 20 and Np = 10, what is K/Np and which metric should you prefer?

- **Concept: nDCG (Normalized Discounted Cumulative Gain)**
  - Why needed here: Understanding position-weighted relevance scoring is necessary to see why nDCG excels when document ordering matters for LLM consumption.
  - Quick check question: Why does nDCG give more weight to relevant documents at rank 1 versus rank 10?

## Architecture Onboarding

- **Component map:**
  - Query -> Embedding model -> Vector similarity ranking -> Top-K selection -> Retrieval measures (T, F, Fe, nDCG) -> LLM response generation -> LLM quality assessment -> Correlation analysis

- **Critical path:**
  1. Query → Embedding → Top-K candidates
  2. Calculate T = ((1-α)nₚ - α·nₙ)/K
  3. LLM generates response from top-K
  4. LLM scores response quality against ideal (all positives)
  5. Correlate T with quality; tune α to maximize

- **Design tradeoffs:**
  - T vs nDCG: T simpler (no position weighting); nDCG captures ordering. Choose based on expected K/Np.
  - Fe vs F: Fe avoids full annotation but requires top-2K labeling.
  - α parameter: Lower α emphasizes recall; higher α emphasizes precision.

- **Failure signatures:**
  - Low T correlation at high K/Np → Switch to nDCG
  - Fe worse than F → Np too small for reliable estimation
  - Tᵤ underperforming T → K varies across queries; normalization needed

- **First 3 experiments:**
  1. Segment data by K/Np ratio; calculate Spearman correlations for T, F, nDCG against LLM-judged quality; identify crossover where nDCG becomes preferable.
  2. Tune α for your domain: run T with α ∈ {0.3, 0.5, 0.7}, select value maximizing correlation.
  3. Validate Fe on labeled subset: compare Fe (top-2K estimation) vs true F to confirm higher-ranked documents are more informative for your LLM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the T measure maintain comparable performance to F in retrieval scenarios with a large number of relevant documents (Np > 15)?
- Basis in paper: [explicit] The authors state in the Limitations section: "Our experiments focus on retrieval scenarios with relatively moderate ranges of Np, which may limit the applicability of our findings to broader retrieval regimes."
- Why unresolved: Experiments only covered Np ∈ [2,15]; performance characteristics at higher Np values remain unknown.
- What evidence would resolve it: Experiments on datasets with higher Np ranges, measuring correlation between T/F and LLM response quality.

### Open Question 2
- Question: Why does Fe (F with estimated Np from top-2K) outperform the true F measure at high K/Np ratios?
- Basis in paper: [explicit] "It is surprising, however, that Fe at high ratios of K/Np can become even better than F... We suspect the reason is that the relevant documents falling into the top 2K... are more important and should 'weigh more' than other relevant documents."
- Why unresolved: The authors propose a hypothesis but do not validate it experimentally.
- What evidence would resolve it: A weighted F measure that explicitly upweights top-ranked relevant documents, tested against Fe performance.

### Open Question 3
- Question: How do the retrieval quality measures compare in computational latency for real-time evaluation?
- Basis in paper: [explicit] "We left out a consideration of the additional latency incurred when evaluating the results of a retrieval in realtime."
- Why unresolved: Practical deployment requires understanding time costs; the paper does not measure or compare latency.
- What evidence would resolve it: Timing benchmarks for computing T, F, Fe, and nDCG across varying K and dataset sizes.

## Limitations

- Experiments focus on datasets with relatively moderate ranges of Np (2-15), limiting applicability to broader retrieval regimes
- The assumption that top-2K relevant documents are more valuable for LLM response quality is proposed but not experimentally validated
- The study does not consider computational latency of different evaluation measures for real-time deployment

## Confidence

- **High**: T's mathematical properties (removing Np dependency while preserving precision-recall semantics)
- **Medium**: T's empirical performance on the specific retrieval-response dataset
- **Medium**: K/Np ratio as a decision boundary for metric selection
- **Low**: Top-2K relevance assumption for Fe superiority

## Next Checks

1. Test T and Fe on a held-out dataset with different characteristics (e.g., longer documents, different domains) to verify generalization beyond the retrieval-response corpus.
2. Design an ablation study where relevant documents are sampled at different ranks (top-100 vs bottom-2K) to directly test whether higher-ranked relevant documents truly produce better LLM responses.
3. Compare T's performance against other recall-independent measures (like Precision@K or MAP) on the same dataset to establish whether T offers unique advantages beyond simple recall-free variants.