---
ver: rpa2
title: 'Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement
  Learning'
arxiv_id: '2504.11354'
source_url: https://arxiv.org/abs/2504.11354
tags:
- real
- have
- reasoning
- formal
- kimina-prover
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Kimina-Prover Preview introduces a large language model trained
  via reinforcement learning to tackle formal theorem proving in Lean 4, using a novel
  "formal reasoning pattern" that interleaves informal reasoning with Lean code. Unlike
  prior work that relies on explicit tree search, Kimina-Prover uses its internal
  reasoning tokens to explore and refine proofs.
---

# Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning

## Quick Facts
- arXiv ID: 2504.11354
- Source URL: https://arxiv.org/abs/2504.11354
- Reference count: 40
- Key outcome: 80.7% accuracy on miniF2F benchmark (pass@8192) using a novel formal reasoning pattern

## Executive Summary
Kimina-Prover Preview introduces a large language model trained via reinforcement learning for formal theorem proving in Lean 4. The model employs a novel "formal reasoning pattern" that interleaves informal reasoning with Lean code, using internal reasoning tokens to explore and refine proofs rather than explicit tree search. Trained from Qwen2.5-72B with a large-scale RL pipeline, it achieves state-of-the-art performance on miniF2F at 80.7% accuracy with pass@8192. The model demonstrates clear scaling with size (1.5B, 7B, and 72B versions) and high sample efficiency, delivering strong results with minimal sampling. The reasoning style is more human-like and structured than search-based methods, effectively bridging informal and formal reasoning.

## Method Summary
Kimina-Prover is a reinforcement learning-based system that fine-tunes large language models for formal theorem proving in Lean 4. The core innovation is the "formal reasoning pattern," which interleaves informal natural language reasoning with Lean code within a single proof trace. Unlike prior approaches that use explicit tree search, Kimina-Prover leverages its internal reasoning tokens to explore and refine proofs, allowing the model to backtrack and reconsider approaches when stuck. The system is trained from Qwen2.5-72B using a large-scale RL pipeline, with smaller distilled versions (1.5B and 7B) also released. The model uses pass@k sampling at various budgets (pass@1, pass@32, pass@256, pass@8192) to evaluate performance on the miniF2F benchmark.

## Key Results
- Achieves 80.7% accuracy on miniF2F benchmark with pass@8192
- Demonstrates clear scaling with model size: 1.5B, 7B, and 72B versions all improve with larger budgets
- Shows high sample efficiency: 52.94% accuracy at pass@1
- Sets new state-of-the-art for formal theorem proving on miniF2F

## Why This Works (Mechanism)
Kimina-Prover works by leveraging a novel formal reasoning pattern that interleaves informal reasoning with formal Lean code within the same proof trace. This approach allows the model to use its internal reasoning tokens as a form of implicit search, enabling it to explore different proof strategies and backtrack when necessary. The model is trained via reinforcement learning, which optimizes for successful proof completion rather than next-token prediction. This shift from autoregressive training to RL enables the model to develop more coherent, human-like reasoning strategies that can navigate complex formal proofs more effectively than explicit search methods.

## Foundational Learning
- **Reinforcement Learning**: Needed to optimize for proof success rather than next-token prediction; quick check: compare RL-trained vs supervised models on proof completion rates
- **Formal Theorem Proving in Lean**: Required understanding of Lean 4 syntax and tactics; quick check: validate model outputs against Lean 4 compiler
- **Pass@k Sampling**: Needed to evaluate performance under different computational budgets; quick check: compare pass@1 vs pass@8192 results
- **Transformer Architecture**: Foundation for handling long sequences of interleaved reasoning and code; quick check: verify attention mechanisms handle long proof traces
- **Natural Language Reasoning**: Needed to bridge informal problem understanding with formal proof construction; quick check: analyze reasoning quality in proof traces

## Architecture Onboarding
- **Component Map**: Input Problem -> Natural Language Reasoning -> Lean Code -> Proof Trace -> RL Reward
- **Critical Path**: Problem statement → reasoning tokens → Lean tactic application → proof completion → reward signal
- **Design Tradeoffs**: Implicit reasoning vs explicit search (flexibility vs computational cost), RL vs supervised learning (optimization target vs training stability)
- **Failure Signatures**: Stuck in local reasoning loops, generating syntactically invalid Lean code, inability to backtrack effectively
- **First Experiments**: 1) Test model on simple theorems to verify basic functionality, 2) Evaluate reasoning quality on medium-difficulty problems, 3) Compare pass@1 vs pass@32 performance on benchmark problems

## Open Questions the Paper Calls Out
None

## Limitations
- Does not report results for models beyond 72B or investigate potential diminishing returns at larger scales
- Claims about reasoning quality as primary performance driver require further ablation studies
- Generalization to theorems outside miniF2F is not evaluated
- Qualitative claims about "human-like" reasoning lack rigorous human evaluation
- Training pipeline compute requirements are not disclosed

## Confidence
- **High confidence**: State-of-the-art performance on miniF2F (80.7% pass@8192) and strong sample efficiency (52.94% pass@1)
- **Medium confidence**: Claims about scaling with model size and reasoning quality as primary driver require further validation
- **Medium confidence**: Qualitative claim about "human-like" reasoning is supported by structure but lacks rigorous evaluation

## Next Checks
1. Conduct ablation studies comparing formal reasoning pattern against explicit search methods at matched model sizes
2. Evaluate model on external theorem proving benchmarks beyond miniF2F to assess generalization
3. Perform controlled experiments training models of intermediate sizes (e.g., 3B, 15B) to characterize scaling relationship