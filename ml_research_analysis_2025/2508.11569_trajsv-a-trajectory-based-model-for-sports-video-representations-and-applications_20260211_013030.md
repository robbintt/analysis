---
ver: rpa2
title: 'TrajSV: A Trajectory-based Model for Sports Video Representations and Applications'
arxiv_id: '2508.11569'
source_url: https://arxiv.org/abs/2508.11569
tags:
- video
- sports
- trajsv
- representations
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TrajSV, a trajectory-based framework for sports
  video representations. The core method extracts player and ball trajectories from
  broadcast videos, then uses a Clip Representation Network (CRNet) and Video Representation
  Network (VRNet) to learn clip and video-level representations.
---

# TrajSV: A Trajectory-based Model for Sports Video Representations and Applications

## Quick Facts
- **arXiv ID:** 2508.11569
- **Source URL:** https://arxiv.org/abs/2508.11569
- **Reference count:** 40
- **Primary result:** Nearly 70% improvement in sports video retrieval, best in 9/17 action categories for action spotting, nearly 20% improvement in video captioning

## Executive Summary
This paper presents TrajSV, a trajectory-based framework for learning sports video representations. The method extracts player and ball trajectories from broadcast videos, then uses a Clip Representation Network (CRNet) and Video Representation Network (VRNet) to learn clip and video-level representations through triple contrastive learning. The framework achieves state-of-the-art performance across three applications: sports video retrieval, action spotting, and video captioning, while requiring no supervision labels.

## Method Summary
TrajSV extracts trajectories from broadcast videos using camera calibration and multi-object tracking, then tokenizes these trajectories into grid-based segment matrices. CRNet processes these tokens with a Transformer encoder and fuses them with visual features from X-CLIP. VRNet aggregates clip representations using multi-head attention blocks (MSB and MAB) to produce video-level embeddings. The entire system is trained with a triple contrastive loss that optimizes both clip and video representations in an unsupervised manner by contrasting original videos with trajectory-replaced and clip-replaced variants.

## Key Results
- Sports video retrieval: Nearly 70% improvement over previous methods, achieving HR@1 of 0.475 on SoccerNet
- Action spotting: Best performance in 9 out of 17 action categories, with Avg-mAP of 0.617 on SoccerNet-ActionSpotting
- Video captioning: Nearly 20% improvement, achieving METEOR of 0.234 on SoccerNet-Captioning
- Ablation studies show trajectory features are critical (49.1% HR@1 drop without them) and VRNet significantly outperforms mean pooling (49.7% HR@1 improvement)

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Tokenization Preserves Movement Patterns
The system partitions the sports field into a grid with equal cell size and time into fixed-duration segments, creating binary segment matrices where cells are marked if trajectories pass through them. This discretization allows Transformer processing while maintaining movement semantics. The chosen cell size (3 meters) balances noise robustness against representation fidelity.

### Mechanism 2: Dual-Level Attention Enables Task-Agnostic Representations
CRNet captures intra-clip dependencies via Transformer self-attention on trajectory tokens, while VRNet uses MSB and MAB blocks to weight clip contributions differently based on learned importance. This avoids equal-weight aggregation and allows the model to emphasize important clips containing discriminative trajectory patterns.

### Mechanism 3: Triple Contrastive Learning Creates Unsupervised Supervision
The loss combines three InfoNCE terms: original vs. intra-clip variants (trajectories replaced), original vs. inter-clip variants (clips replaced), and intra-clip vs. inter-clip variants. This forces representations to encode both fine-grained movement and temporal structure while using random trajectory/clip replacement at noise rate δ ∈ [0, 0.2] to create semantically meaningful negatives.

## Foundational Learning

- **Contrastive Learning (InfoNCE Loss)**: Core optimization objective; understand positive/negative sampling, temperature parameter τ, and symmetric loss formulation
  - Quick check question: Can you explain why the paper uses symmetric InfoNCE (L1,2 + L2,1) instead of one-way contrastive loss?

- **Transformer Self-Attention Mechanism**: CRNet uses multi-head self-attention on trajectory tokens; understand query/key/value projections and positional embeddings
  - Quick check question: Why does the paper add positional embeddings to segment embeddings (Equation 1)?

- **Multi-Object Tracking (MOT) Fundamentals**: Data preprocessing extracts trajectories; understand detection, re-identification, and coordinate transformation from image to field space
  - Quick check question: Why does the paper assign weight=10 to small objects (bounding box < 500 pixels)?

## Architecture Onboarding

- **Component map:**
  Raw Video → [Segmentation] → Clips → [Calibration] → Field coordinates → [MOT] → Trajectories → [CRNet: Tokenization → Transformer + Visual Fusion] → Clip Representations → [VRNet: MSB Encoder → MAB Decoder] → Video Representation → [ANN Index: HNSW] → Retrieved Videos

- **Critical path:** Calibration accuracy → MOT tracking quality → Trajectory tokenization resolution → CRNet attention weights → VRNet clip aggregation. Errors propagate; the paper notes calibration uses "reprojection loss minimization" and MOT is "fine-tuned on SoccerNet-Tracking dataset."

- **Design tradeoffs:**
  - Cell size: 3m chosen (Table VII); smaller = higher resolution but less robust to tracking noise
  - Clip count per video: n=16; zero padding handles shorter videos
  - Embedding dimension: 128 for trajectory, 512 for visual, 640 concatenated (ablation shows 256-dim trajectory alone achieves 0.564 HR@1)
  - Loss weights: α=0.5, β=0.3 for balancing three contrasts

- **Failure signatures:**
  - HR@1 drops 49.1% without trajectory features (ablation in Table V)
  - Replacing VRNet with mean pooling drops HR@1 by 49.7%
  - Batch size < 128 reduces contrastive learning effectiveness (fewer negatives)
  - Zero-shot transfer underperforms fine-tuning due to trajectory distribution shift

- **First 3 experiments:**
  1. Reproduce trajectory extraction pipeline: Take 10 sample soccer clips, run calibration + MOT, visualize trajectories overlaid on field grid. Verify cell size 3m produces distinguishable patterns.
  2. Ablate triple contrastive loss: Train with only L(V(1), V(2)), then add L(V(1), V(3)), then full loss. Compare HR@1 on validation set (expect ~5-8% gains per component per Table V).
  3. Test transferability: Train on SoccerNet, test on YouTube subset with and without fine-tuning. Measure the gap between Table IX's reported values to validate your implementation.

## Open Questions the Paper Calls Out

### Open Question 1: Strategic Analysis Applications
How can the trajectory-based representations learned by TrajSV be effectively adapted or extended for high-level strategic analysis tasks, such as automated team tactics discovery or player performance evaluation? The current framework is optimized for retrieval, action spotting, and captioning, whereas tactics discovery requires modeling complex, long-term interactions and strategic patterns.

### Open Question 2: Domain Gap Bridging
How can the domain gap regarding trajectory distributions between different datasets be bridged to enable effective zero-shot transfer without the current performance degradation? The current unsupervised triple contrastive loss learns representations sensitive to the specific trajectory statistics of the training domain, failing to generalize to distributional shifts in video length and content.

### Open Question 3: Robustness to Tracking Noise
To what extent does the robustness of the TrajSV framework degrade when faced with imperfect trajectory extraction (e.g., occlusion or tracking loss), and can the architecture be modified to be more resilient to such noise? The paper relies on a specific fine-tuned FairMOT model to generate input trajectories but doesn't analyze how the CRNet and VRNet handle failure cases of this tracker.

## Limitations
- The trajectory extraction pipeline relies on camera calibration and multi-object tracking that are not fully detailed in the paper, with tracking failure rates and calibration accuracy metrics not reported
- The choice of 1-second time segments and 3-meter cell size is empirically validated but not rigorously justified through systematic analysis
- The video segmentation component (1D CNN) architecture details are sparse, with only layer counts and kernel sizes specified

## Confidence

- **High Confidence:** The triple contrastive learning framework design and loss formulation are clearly specified with explicit equations and hyperparameter settings. The ablation studies (Table V) provide strong evidence for each component's contribution.
- **Medium Confidence:** The trajectory tokenization approach and cell size selection (3m) are reasonable based on empirical validation, but the sensitivity analysis is limited to one parameter sweep.
- **Low Confidence:** The video segmentation component (1D CNN) architecture details are sparse, with only layer counts and kernel sizes specified. The initialization strategy for the VRNet decoder seed vector is unclear.

## Next Checks

1. **Track tracking failure rates:** Run the MOT pipeline on 50 diverse broadcast clips and measure tracking ID switches, lost objects, and coordinate drift. Calculate the percentage of trajectories with >30% missing frames.
2. **Validate noise perturbation bounds:** Systematically vary δ from 0.05 to 0.3 in 0.05 increments and measure HR@1 degradation. Confirm that the 0.2 threshold is where performance begins to collapse.
3. **Test seed vector initialization:** Train VRNet with random initialization vs. learned initialization vs. mean-pooled clip representation initialization. Compare final HR@1 to isolate the impact of this architectural choice.