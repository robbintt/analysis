---
ver: rpa2
title: "D\xE9j\xE0Q: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems"
arxiv_id: '2601.01931'
source_url: https://arxiv.org/abs/2601.01931
tags:
- training
- problem
- problems
- learning
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces D\xC9J\xC0Q, a framework that evolves a\
  \ dataset of synthetic mathematical problems jointly with model training, using\
  \ LLM-guided mutations to maintain diversity and optimize learnability. Instead\
  \ of relying on static data, the method continuously adapts problems to match the\
  \ model's current ability, using a quality-diversity archive (MAP-Elites) and three\
  \ mutation strategies: setting rewrites, distractor insertion, and symbolic changes\
  \ to the underlying math."
---

# DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems

## Quick Facts
- **arXiv ID:** 2601.01931
- **Source URL:** https://arxiv.org/abs/2601.01931
- **Reference count:** 40
- **Primary result:** Introduces DÉJÀQ, a framework that jointly evolves mathematical problem datasets with model training using LLM-guided mutations, showing improved robustness and generalization on reasoning benchmarks.

## Executive Summary
This paper presents DÉJÀQ, a novel framework that addresses the challenge of curating effective training data for mathematical reasoning models by evolving the dataset alongside the model itself. Instead of relying on static problem collections, DÉJÀQ uses a quality-diversity archive (MAP-Elites) to maintain diverse, learnable problems that match the model's current capability. The system employs three LLM-guided mutation strategies—setting rewrites, distractor insertion, and symbolic changes—to continuously adapt problems while preserving validity. Experiments demonstrate that DÉJÀQ significantly outperforms domain randomization and resampling baselines on both in-distribution and out-of-distribution mathematical reasoning benchmarks, with particular gains in robustness to hard problem instances.

## Method Summary
DÉJÀQ operates through two concurrent loops: a GRPO training loop that samples problems from a MAP-Elites archive based on learnability scores, and an evolution loop that generates new candidates using three mutation strategies guided by an LLM. The archive organizes problems into cells based on "settings" (e.g., Economic, Scientific), and mutations are targeted to underperforming or sparse cells to maintain diversity. Learnability is estimated as p_θ(x)(1-p_θ(x)) using K rollouts, prioritizing problems the model can solve but not yet consistently. The mutation pipeline includes setting rewrites (safe, low diversity), distractor insertion (medium diversity), and symbolic changes (high diversity but higher invalidity risk). All operations are scheduled on a shared inference server, with mutations opportunistically processed during training idle periods to maximize GPU utilization.

## Key Results
- **Performance gains:** DÉJÀQ achieves 1.8% accuracy improvement on GSM-Symbolic Symbolic, 1.5% on P1, and 0.9% on P2 compared to DR baseline
- **Robustness improvement:** Shows 4.7% CVaR increase on GSM-Symbolic, indicating better performance on hard problem instances
- **Efficiency:** GPU utilization reaches 51.7% with full mutation pipeline versus 3.2% for domain randomization baseline
- **Ablation results:** Setting mutators contribute most to performance gains, while symbolic mutators add diversity at the cost of increased invalidity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Filtering training candidates using a learnability score (l_θ(x) ≈ p_θ(x)(1 - p_θ(x))) maximizes information density and implicitly verifies validity.
- **Mechanism:** The model samples K rollouts for each candidate problem. Problems with success probability near 0 (too hard/invalid) or near 1 (too easy) receive low learnability scores, while problems at the model's capability frontier are retained.
- **Core assumption:** Success probability correlates with educational value for the current model checkpoint.
- **Evidence anchors:** Section 2.2 defines learnability; Section 4.1 states malformed problems naturally receive low scores; OpenSIR supports self-improving reasoning loops.
- **Break condition:** Low K sample size causes noisy filtering, retaining invalid or trivial problems.

### Mechanism 2
- **Claim:** MAP-Elites archive prevents mode collapse and encourages open-ended exploration.
- **Mechanism:** Archive discretizes problem space into cells based on settings. Mutation operators target underperforming or sparse cells, ensuring diversity rather than optimizing for easy-to-generate problem types.
- **Core assumption:** Setting descriptors represent meaningful diversity axes for reasoning capabilities.
- **Evidence anchors:** Section 4.1 describes archive organization by setting; Page 2 references MAP-Elites incorporation from RAINBOWTEAMING.
- **Break condition:** Coarse or irrelevant feature descriptors create diversity in narrative but redundancy in reasoning patterns.

### Mechanism 3
- **Claim:** Offloading data curation to the inference server improves GPU utilization without impacting training throughput.
- **Mechanism:** RLVR training creates bursty demands (rollouts) followed by idle periods (backprop). DÉJÀQ schedules mutation and learnability estimation during these idle windows.
- **Core assumption:** Scheduling doesn't create resource contention starving the training rollout loop.
- **Evidence anchors:** Section 5.3 reports 51.7% utilization for full pipeline vs 3.2% for DR baseline.
- **Break condition:** Large batch sizes saturate the inference server, causing mutation requests to queue and stall evolution.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** DÉJÀQ builds on GRPO, a variant of RLVR. Understanding how the model generates rollouts and receives binary rewards is essential for debugging the training loop.
  - **Quick check question:** How does GRPO estimate the advantage of a specific generation if it doesn't use a learned value function?

- **Concept: Quality-Diversity (QD) Algorithms**
  - **Why needed here:** The core data structure is a MAP-Elites archive. Understanding how an "elite" is defined (highest fitness in a specific behavioral niche) is crucial for implementing insertion logic.
  - **Quick check question:** In a MAP-Elites grid, if a new mutant has lower fitness than the current occupant but higher diversity, is it inserted?

- **Concept: In-Context Learning vs. Gradient Updates**
  - **Why needed here:** The "Teacher" (mutation) and "Student" (training) use the same model. Distinguishing between the model as data generator (frozen weights) and training (gradient descent) is essential.
  - **Quick check question:** Does the "Symbolic Mutator" update the model's weights when it solves the proposed new problem to find the answer?

## Architecture Onboarding

- **Component map:** Inference Server (vLLM) -> Evolution Loop (Async) -> MAP-Elites Archive -> Training Loop (GRPO)
- **Critical path:** Scoring latency (K rollouts per candidate) can make the archive stale; Symbolic Mutator must solve problems to provide ground-truth answers
- **Design tradeoffs:** Setting mutators are safe but low diversity; Symbolic mutators are high diversity but risk invalidity; Shared inference server reduces hardware but adds scheduling complexity
- **Failure signatures:** Stagnant Archive (easy problems dominate), Teacher-Student Lag (student surpasses teacher's generation ability), Language Drift (mutations accidentally translate problems)
- **First 3 experiments:**
  1. Sanity Check: Run Symbolic Mutator in isolation on 50 seed problems and manually inspect answer validity
  2. Scoring Calibration: Freeze model, generate problems with known difficulty, plot learnability against difficulty to ensure peak at medium complexity
  3. Ablation: Run DÉJÀQ-S vs DÉJÀQ-A for 100 steps and monitor invalidity rate to quantify Symbolic mutation noise

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does explicitly training the mutation policy (teacher) alongside the solver (student) prevent degradation of learnability as a proxy for verifiability?
- **Basis:** The conclusion identifies a bottleneck where "the teacher lags behind the student," causing post-trained models to produce invalid high-learnability samples.
- **Why unresolved:** Current setup optimizes only the student via RLVR, leaving the teacher static and creating a mismatch as student capabilities evolve.
- **Evidence:** Comparing P(invalid | high learnability) between static teacher and teacher updated via reinforcement learning on mutation validity.

### Open Question 2
- **Question:** Can automatically discovered feature descriptors outperform manual "setting" categorization in maintaining archive diversity?
- **Basis:** Section 4.2 states while hand-crafted diversity axis was used, "future work may explore automatically discovered descriptors."
- **Why unresolved:** Manual categorization may fail to capture latent structural diversity in mathematical problems, potentially limiting archive effectiveness.
- **Evidence:** Benchmarking accuracy when using unsupervised embeddings for archive cell assignment versus manual list in Table 3.

### Open Question 3
- **Question:** Can lightweight scheduling strategies effectively mitigate queue contention between training and evolution loops?
- **Basis:** Section 5.3 notes simultaneous requests cause queue delays and suggests "lightweight scheduling" could alleviate bottlenecks.
- **Why unresolved:** Unclear if prioritizing training queries significantly reduces wall-clock time without starving data evolution.
- **Evidence:** Measuring total training duration and average queue wait times under priority scheduling versus current agnostic strategy.

## Limitations

- **Proprietary evaluation access:** Relies on GPT-5 and GPT-5-MINI for synthetic evaluation generation and validity checking, which are not publicly accessible
- **Descriptor dependency:** Quality-diversity archive effectiveness depends heavily on chosen feature descriptors, which are not empirically validated as optimal for mathematical reasoning
- **Scheduling underspecification:** "Opportunistic" scheduling mechanism lacks precise implementation details, making efficiency gains difficult to assess or replicate

## Confidence

- **Performance Claims:** High confidence - directly measurable results with clear methodology
- **Efficiency Claims:** Medium confidence - GPU utilization data presented but scheduling mechanism underspecified
- **Mechanism Claims:** Low-Medium confidence - learnability filtering as validity proxy supported correlationally but not causally

## Next Checks

1. **Open-Endedness Test:** Run DÉJÀQ for 2000+ training steps and monitor whether archive continues discovering novel problem types or plateaus, measuring entropy of problem features over time

2. **Scheduling Stress Test:** Systematically vary training batch size from 4 to 64 while keeping mutation frequency constant, recording GPU utilization and throughput to identify resource contention threshold

3. **Validity Ablation:** Implement variant with explicit oracle validation instead of learnability filtering, comparing invalidity rate and final performance to quantify implicit vs explicit validity checking contribution