---
ver: rpa2
title: Practical do-Shapley Explanations with Estimand-Agnostic Causal Inference
arxiv_id: '2509.20211'
source_url: https://arxiv.org/abs/2509.20211
tags:
- causal
- which
- do-shap
- graph
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes practical estimation of do-Shapley values (do-SVs)
  for interpretable feature attributions in causal models. The key contribution is
  using estimand-agnostic causal inference, allowing a single trained SCM to estimate
  any identifiable causal query without deriving query-specific formulas.
---

# Practical do-Shapley Explanations with Estimand-Agnostic Causal Inference

## Quick Facts
- arXiv ID: 2509.20211
- Source URL: https://arxiv.org/abs/2509.20211
- Authors: √Ålvaro Parafita; Tomas Garriga; Axel Brando; Francisco J. Cazorla
- Reference count: 40
- Primary result: Proposes practical estimation of do-Shapley values using a single trained SCM for any identifiable causal query, accelerated by a Frontier-Reducibility Algorithm

## Executive Summary
This work presents a practical approach to computing do-Shapley values for interpretable feature attributions in causal models. The key innovation is using estimand-agnostic causal inference, where a single trained Structural Causal Model (SCM) can estimate any identifiable causal query without deriving query-specific formulas. This makes do-SHAP feasible for complex graphs. The method includes a Frontier-Reducibility Algorithm (FRA) that significantly accelerates computation by skipping reducible coalitions, and extends to explaining inaccessible Data Generating Processes through noise attribution. Experiments demonstrate accurate do-SV estimation across multiple SCM architectures with substantial speedups from FRA, validated on real-world datasets.

## Method Summary
The method trains a neural proxy SCM (Deep Causal Graph or Causal Normalizing Flow) to approximate the observational distribution $P(V)$, then uses this single model to estimate any identifiable causal query required for do-Shapley values. For a sample to explain, it samples permutations and applies FRA to reduce coalitions to their irreducible subsets before evaluating the costly SCM sampling. FRA exploits graph structure: if a set of variables $S$ blocks all directed paths from a variable $X$ to the target $Y$, then $X$ is redundant in coalition $S \cup \{X\}$. The method also extends to inaccessible DGPs by treating exogenous noise $E_Y$ as a player under an additive noise assumption.

## Key Results
- Demonstrated accurate do-SV estimation across multiple SCM architectures (Linear, DCG, CNF) on synthetic datasets
- FRA significantly reduces computational load, with speedup correlating with graph sparsity
- Validated approach on real-world datasets, showing do-SHAP's explanatory power over marginal and conditional SHAP
- Successfully explained inaccessible DGPs by attributing unexplained variance to noise terms

## Why This Works (Mechanism)

### Mechanism 1: Estimand-Agnostic Query Estimation via Proxy SCMs
Replacing manual, query-specific estimand derivations with a single trained SCM allows general estimation of any identifiable causal query required for do-Shapley values. Instead of deriving distinct formulas for each of the $2^K$ coalitions, the approach trains one neural proxy SCM to approximate $P(V)$ and uses its sampling procedure for estimation. Core assumption: the causal graph is known and correct, the proxy SCM is expressive enough to capture $P(V)$, and queries are identifiable in $G$.

### Mechanism 2: Coalition Reduction via Frontier-Reducibility
FRA exploits causal graph structure to reduce computational load. If set $S$ blocks all directed paths from $X$ to $Y$, then $X$ is redundant in coalition $S \cup \{X\}$ (i.e., $v(S \cup \{X\}) = v(S)$). FRA reduces any coalition to its minimal "irreducible" subset before costly SCM sampling, caching results to avoid re-computation. Core assumption: graph topology allows for path blocking; not all variables are direct parents of $Y$.

### Mechanism 3: Explaining Inaccessible DGPs via Noise Attribution
For systems where we cannot access internal mechanisms, the paper proposes an additive noise assumption. Under this assumption, the do-Shapley value for noise term $\phi_{E_Y}$ captures the difference between observed outcome $y$ and expected outcome given parents $E[Y|pa_Y]$. This bridges the gap between population-level interventional expectations and specific individual outcomes. Core assumption: $Y$ follows an additive noise model and there are no latent confounders connected to $Y$.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) & Identifiability**
  - Why needed: The methodology hinges on training a neural network to approximate an SCM. Understanding the difference between observational and interventional distributions is necessary for comprehending the "estimand-agnostic" mechanism.
  - Quick check: If a proxy SCM models the observational distribution perfectly, does it guarantee accurate interventional queries? (Answer: Yes, if the query is identifiable in $G$).

- **Concept: Shapley Value Axioms (Efficiency)**
  - Why needed: The paper uses the efficiency axiom ($\sum \phi_i = \Delta \nu(X)$) to argue that sum of do-SHAP values must equal total causal effect. Understanding this is necessary to see why explaining the "gap" requires adding noise as a player.
  - Quick check: Why does standard SHAP often fail to add up to the difference between $y$ and $E[Y]$ for a specific sample in a non-deterministic setting?

- **Concept: Graph Topological Ordering**
  - Why needed: FRA relies on a topological sort of the graph to process nodes and determine ancestors/descendants.
  - Quick check: In a chain graph $A \rightarrow B \rightarrow Y$, why is $A$ reducible if $B$ is in the coalition, but $B$ is not reducible if $A$ is in the coalition?

## Architecture Onboarding

- **Component map:**
  - Graph Spec: Defines DAG structure ($G$) and variable types
  - Neural SCM (The Engine): Generative model (DCG or CNF) trained via Maximum Likelihood to learn $P(V)$
  - FRA Module: Logic layer that accepts coalition set $S$, checks frontier reducibility against $G$, returns irreducible subset key for caching
  - SHAP Aggregator: Monte Carlo sampler for permutations that calls FRA Module and SCM Engine

- **Critical path:**
  1. Define causal graph $G$ and assumption check
  2. Train Neural SCM on observational data (offline)
  3. For specific sample $x$:
     - Sample permutation $\pi$
     - Apply FRA: Reduce subsets $X_{<\pi X}$ and $X_{\le \pi X}$ to irreducible forms
     - Estimate Value: Sample from SCM under intervention $do(S=s)$ to get $\nu(S)$
     - Aggregate differences into $\phi_X$

- **Design tradeoffs:**
  - DCG vs. CNF: DCG handles latent confounders but may have higher variance; CNF models graph as whole (lower variance) but assumes causal sufficiency
  - Exact vs. Approximate: Use exact enumeration for $K < 15$; for $K > 15$, rely on FRA-accelerated permutations

- **Failure signatures:**
  - Identifiability Error: ID algorithm returns "False" for generated irreducible subset. Action: Halt do-SHAP or refine graph $G$
  - High Estimation Loss: SCM training log-likelihood plateaus low; SHAP values diverge from ground truth. Action: Increase SCM capacity
  - No FRA Speedup: Runtime identical to baseline. Action: Check if graph structure is purely "star"

- **First 3 experiments:**
  1. **SCM Capacity Test:** Train Linear SCM vs. DCG vs. CNF on synthetic dataset. Compare SHAP Estimation Loss to confirm DCG/CNF outperform Linear.
  2. **FRA Stress Test:** Generate random graphs with varying density ($p$). Measure "Ratio of computed coalitions after FRA" to verify speedup correlates with sparsity.
  3. **Real-world Validation:** Apply "Inaccessible DGP" method to Bike Rental dataset. Verify sum of feature attributions plus noise attribution equals actual rental count ($y$).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a general graphical criterion be established to determine the identifiability of do-Shapley values without testing every coalition?
- Basis: Section 7 states "A general graphical criterion for do-SV identifiability is also a worthwhile new direction" to avoid running ID algorithms on every term.
- Why unresolved: Currently, identifiability must be checked for each of the $2^{|X|}$ coalitions, which is computationally expensive for large graphs.
- What evidence would resolve it: A theorem providing a topological condition that guarantees identifiability for the entire set of do-SVs.

### Open Question 2
- Question: How can counterfactual value functions be formulated to provide instance-specific local explanations rather than population measures?
- Basis: Section 7 notes that do-SVs are population estimates and suggests "counterfactual value functions $\nu$ could result in a promising new kind of causal, local explanations."
- Why unresolved: The current interventional approach creates a "gap" between the attribution and the actual outcome $y$, except in specific cases like additive noise models.
- What evidence would resolve it: A mathematical formulation of a counterfactual SHAP value and empirical validation showing it closes the attribution gap.

### Open Question 3
- Question: Can estimand-agnostic approaches be adapted to provide doubly-robust guarantees against model or graph misspecification?
- Basis: Section 7 calls for "more efficient estimators, ideally with doubly-robust guarantees," and Section 6 notes this is a distinct disadvantage compared to estimand-based methods.
- Why unresolved: The authors highlight that their method is sensitive to graph misspecification and lacks the theoretical robustness of some estimand-based alternatives.
- What evidence would resolve it: A theoretical proof of robustness and experiments showing estimation stability under perturbations of the causal graph or model distribution.

## Limitations

- The method's accuracy heavily depends on the correctness of the assumed causal graph $G$; misspecification can lead to significant estimation errors
- The additive noise assumption for inaccessible DGPs may not hold for non-additive relationships, limiting the noise attribution method's applicability
- FRA provides no speedup in fully connected or star-like graphs where all variables are direct parents of the target

## Confidence

- **High Confidence**: The core methodology of using a trained SCM to estimate identifiable causal queries is theoretically sound and supported by empirical results
- **Medium Confidence**: Efficiency gains from FRA are demonstrated but may be highly context-dependent based on graph topology
- **Medium Confidence**: Noise attribution method is mathematically elegant but relies on the strong additive noise assumption

## Next Checks

1. **Robustness to Misspecified Graphs**: Test do-SHAP performance when the assumed causal graph $G$ is incomplete or contains incorrect edges. Measure how estimation error increases with graph misspecification.

2. **FRA Performance on Real-World Graphs**: Apply FRA to complex, real-world causal graphs (e.g., from healthcare or social science domains) to quantify speedup across different graph structures and densities.

3. **Noise Attribution Beyond Additive Models**: Validate the noise attribution method on datasets where the outcome is known to follow non-additive relationships (e.g., multiplicative or threshold effects) to assess method limitations.