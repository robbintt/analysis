---
ver: rpa2
title: 'AgentCaster: Reasoning-Guided Tornado Forecasting'
arxiv_id: '2510.03349'
source_url: https://arxiv.org/abs/2510.03349
tags:
- risk
- arxiv
- tornado
- forecast
- surface
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgentCaster introduces a novel framework for evaluating multimodal
  large language models (LLMs) on the complex task of tornado forecasting. The framework
  employs an interactive, contamination-free environment where LLM agents act as meteorologists,
  querying heterogeneous spatiotemporal data including 145 types of forecast maps
  and on-demand atmospheric soundings.
---

# AgentCaster: Reasoning-Guided Tornado Forecasting

## Quick Facts
- **arXiv ID:** 2510.03349
- **Source URL:** https://arxiv.org/abs/2510.03349
- **Reference count:** 40
- **Primary result:** Human experts outperformed LLM agents in tornado forecasting; models exhibited hallucination and overprediction tendencies

## Executive Summary
AgentCaster introduces a novel framework for evaluating multimodal large language models on the complex task of tornado forecasting. The framework employs an interactive, contamination-free environment where LLM agents act as meteorologists, querying heterogeneous spatiotemporal data including 145 types of forecast maps and on-demand atmospheric soundings. Agents generate probabilistic tornado risk predictions as geospatial polygons, which are evaluated against ground truths derived from observed tornado reports using domain-specific TornadoBench and TornadoHallucination metrics.

Across a 40-day benchmark period with diverse weather conditions, human expert baselines significantly outperformed all evaluated LLM agents. State-of-the-art models demonstrated strong tendencies to hallucinate and overpredict risk intensity, struggled with precise geographic placement, and exhibited poor spatiotemporal reasoning in complex, dynamically evolving systems. The framework establishes a challenging benchmark for advancing LLM reasoning capabilities in critical domains.

## Method Summary
The framework evaluates LLM agents on tornado forecasting through an interactive environment where agents query heterogeneous spatiotemporal data (145 forecast map types, atmospheric soundings) to generate probabilistic tornado risk predictions as geospatial polygons. Performance is measured using domain-specific TornadoBench and TornadoHallucination metrics against observed tornado reports. A 40-day benchmark period includes diverse weather conditions, with human expert baselines providing comparison. The evaluation captures both spatial accuracy and temporal reasoning capabilities while identifying hallucination and overprediction tendencies in current models.

## Key Results
- Human expert baselines significantly outperformed all evaluated LLM agents in tornado forecasting accuracy
- State-of-the-art models exhibited strong tendencies to hallucinate and overpredict tornado risk intensity
- Models struggled with precise geographic placement and spatiotemporal reasoning in complex, dynamically evolving weather systems

## Why This Works (Mechanism)
The framework works by creating a realistic meteorological forecasting environment where LLM agents must actively query relevant data sources and synthesize information to make predictions. The interactive nature forces models to demonstrate genuine reasoning rather than relying on memorized patterns, while the spatiotemporal evaluation captures the complex dynamics of tornado formation and movement. Domain-specific metrics (TornadoBench and TornadoHallucination) provide targeted assessment of both prediction accuracy and the critical problem of model hallucination in safety-critical applications.

## Foundational Learning
- **TornadoBench metric**: Domain-specific evaluation metric for tornado forecasting accuracy that accounts for spatial and temporal uncertainty in predictions
  - Why needed: Standard metrics fail to capture the unique challenges of tornado prediction where false positives can be as dangerous as misses
  - Quick check: Verify metric sensitivity to different prediction confidence levels and spatial tolerances

- **Spatiotemporal reasoning**: The ability to integrate temporal dynamics with spatial patterns across multiple data modalities to forecast evolving weather systems
  - Why needed: Tornado formation involves complex interactions between atmospheric variables that change over time and space
  - Quick check: Test model performance on time-series data with varying temporal resolutions and lead times

- **Hallucination detection**: Methods for identifying when models generate predictions not supported by available evidence, critical for safety-critical applications
  - Why needed: Overconfident false predictions in tornado forecasting can lead to unnecessary evacuations or complacency
  - Quick check: Compare model predictions against ground truth under controlled uncertainty conditions

## Architecture Onboarding

### Component Map
Interactive Environment -> Data Query Interface -> LLM Agent -> Prediction Generator -> Evaluation Module -> TornadoBench/Hallucination Metrics

### Critical Path
Agent queries atmospheric soundings and forecast maps → LLM synthesizes information → Model generates geospatial probability polygon → Evaluation module compares against observed tornado reports → Metrics calculate spatial accuracy and hallucination scores

### Design Tradeoffs
The framework trades computational efficiency for realism by implementing an interactive querying environment that better simulates actual meteorological practice but requires more processing time. The use of domain-specific metrics provides more accurate assessment but limits generalizability to other forecasting domains. The 40-day benchmark period balances practical constraints with the need for diverse weather conditions, though it may miss rare extreme events.

### Failure Signatures
Models consistently overpredict risk intensity and generate geographically imprecise predictions. Hallucination manifests as confident predictions in regions with no supporting atmospheric evidence. Poor spatiotemporal reasoning appears as failure to track evolving weather systems and incorrect timing of tornado formation. These failures suggest fundamental limitations in current LLM reasoning capabilities for complex, dynamic systems.

### First 3 Experiments
1. Test different prompting strategies to reduce hallucination while maintaining prediction accuracy
2. Evaluate model performance across different temporal resolutions (hourly vs daily) to assess temporal reasoning capabilities
3. Compare performance when agents have access to all data versus selective querying to identify optimal information gathering strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively short 40-day benchmark period may not capture full range of meteorological conditions and rare extreme events
- Domain-specific metrics may not fully capture all aspects of forecasting performance in nuanced edge cases
- Interactive querying environment introduces potential biases in how agents explore and utilize available data
- Human expert baseline comparison may not account for variability in expert performance or different forecasting approaches

## Confidence
High: Core claim that current LLM agents underperform human experts in tornado forecasting
Medium: Specific claims about hallucination and overprediction tendencies
Medium: Framework's ability to advance LLM reasoning capabilities

## Next Checks
1. Extend the benchmark to a full tornado season (3-6 months) to capture more diverse meteorological scenarios and rare extreme events
2. Implement ablation studies testing different prompting strategies, tool usage patterns, and model architectures to isolate factors contributing to hallucination and overprediction
3. Conduct expert review of LLM-generated forecasts to validate the domain-specific metrics and identify potential blind spots in the evaluation framework