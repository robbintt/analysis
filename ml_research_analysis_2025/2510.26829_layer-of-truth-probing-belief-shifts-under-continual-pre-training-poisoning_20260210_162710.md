---
ver: rpa2
title: 'Layer of Truth: Probing Belief Shifts under Continual Pre-Training Poisoning'
arxiv_id: '2510.26829'
source_url: https://arxiv.org/abs/2510.26829
tags:
- belief
- poisoning
- across
- layer
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether continual pre-training on plausible
  misinformation can systematically overwrite factual knowledge in large language
  models, mimicking the illusory truth effect seen in humans. The authors introduce
  a controlled framework that injects counterfactual facts into training data at varying
  ratios and tracks belief shifts across checkpoints, layers, and model scales using
  a logit-lens and mechanistic probes.
---

# Layer of Truth: Probing Belief Shifts under Continual Pre-Training Poisoning

## Quick Facts
- **arXiv ID**: 2510.26829
- **Source URL**: https://arxiv.org/abs/2510.26829
- **Reference count**: 40
- **One-line primary result**: Even modest exposure to plausible misinformation in continual pre-training can flip over 55% of factual beliefs in LLMs, with corruption localizing abruptly in late transformer layers and being partially reversible via targeted activation patching.

## Executive Summary
This paper investigates whether continual pre-training on plausible misinformation can systematically overwrite factual knowledge in large language models, mimicking the illusory truth effect seen in humans. The authors introduce a controlled framework that injects counterfactual facts into training data at varying ratios and tracks belief shifts across checkpoints, layers, and model scales using a logit-lens and mechanistic probes. They find that even modest exposure (50-100% poison ratio) flips over 55% of factual responses from correct to counterfactual, with belief corruption localizing abruptly in late transformer layers and being partially reversible via targeted activation patching. Poisoned beliefs generalize across tasks—selectively degrading commonsense reasoning while leaving alignment benchmarks intact—and transfer across languages with reduced stability. These results expose a vulnerability of continual model updates to persistent, localized belief corruption, highlighting the need for representation-level monitoring and mitigation strategies.

## Method Summary
The authors study belief shifts in LLMs under continual pre-training with counterfactual misinformation by injecting paired fact-counterfact triples into training data at varying poison ratios (ρ ∈ {0.1, 0.5, 0.9, 1.0}). They use Qwen2.5 models (0.5B-7B) and track belief flips via log-likelihood differences (∆LL) between correct and poisoned answers. Belief trajectories are monitored across checkpoints and transformer layers using logit-lens, while activation patching is applied to late layers (29-36 for 3B) to measure partial reversibility. OOD robustness is tested on HellaSwag, TruthfulQA, HH-RLHF, and BBEH Logic, with CKA for representation drift and Garak probes for robustness. Training uses stratified 52-entity subsets across 4 domains, with data expanded via 5 styles × 10 prompt formats, batch size 4, LR 1e-4, and bfloat16.

## Key Results
- Belief flips exceed 55% at poison ratios of 50-100%, with corruption localizing abruptly in late transformer layers (L29-36 for 3B).
- Activation patching on late layers partially reverses corrupted beliefs, with rescue rates up to 0.7 at higher poison ratios.
- Poisoned beliefs generalize across tasks, degrading commonsense reasoning (HellaSwag) while leaving alignment benchmarks (TruthfulQA, HH-RLHF) largely intact.
- Cross-lingual transfer shows belief flips with reduced stability, suggesting domain-specific and language-specific vulnerabilities.

## Why This Works (Mechanism)
The study demonstrates that continual pre-training on plausible misinformation can overwrite factual knowledge in LLMs by shifting internal representations in late transformer layers, where factual recall is processed. The logit-lens reveals that belief corruption localizes abruptly rather than gradually, indicating a discrete representational shift. Activation patching exploits this by restoring clean activations in corrupted layers, partially reversing the misinformation effect. This mechanism mirrors the illusory truth effect in humans, where repeated exposure to plausible falsehoods increases their perceived veracity.

## Foundational Learning
- **Logit-lens**: Analyzes transformer layer outputs to track belief trajectories; needed to identify when and where factual preferences flip. Quick check: verify ∆LL > 0 at baseline before poisoning.
- **Activation patching**: Replaces corrupted activations with clean ones to measure reversibility; needed to test if belief corruption is localized and fixable. Quick check: ensure patching is at final token position and correct layer range.
- **CKA (Centered Kernel Alignment)**: Measures representation similarity across checkpoints; needed to quantify belief drift over training. Quick check: compare CKA values between clean and poisoned checkpoints.
- **Garak probes**: Evaluates robustness to adversarial inputs; needed to test if poisoned beliefs generalize to out-of-distribution tasks. Quick check: run Garak probes on both clean and poisoned models.
- **Fact-counterfact pairs**: Structured dataset of correct vs. poisoned answers; needed to create controlled misinformation exposure. Quick check: manually validate counterfactuals are plausible, not trivially wrong.

## Architecture Onboarding
- **Component map**: Fact-Counterfact Dataset → CPT Training (Qwen2.5, ρ) → Checkpoints → Logit-Lens Evaluation → Activation Patching → OOD Benchmarks → CKA/Garak Analysis
- **Critical path**: Dataset construction → CPT with poison ratios → Belief tracking via ∆LL → Layer-wise analysis → Partial reversal via patching → Generalization/robustness tests
- **Design tradeoffs**: High poison ratio (1.0) maximizes flip rates but may introduce noise; low ratio (0.1) tests subtle influence. Late-layer patching balances rescue effectiveness with minimal intervention.
- **Failure signatures**: Belief flips not reproducing at low poison ratios (check counterfactual plausibility); activation patching shows no rescue (verify token position and layer range); cross-lingual stability varies (reconstruct language-specific datasets).
- **First experiments**: 1) Reconstruct fact-counterfact pairs for one domain and verify baseline ∆LL; 2) Run CPT on 0.5B at ρ=1.0 and confirm layer-wise corruption; 3) Apply activation patching on poisoned checkpoint and measure rescue rate.

## Open Questions the Paper Calls Out
None

## Limitations
- Exact 52-entity CPT subset and counterfactual generation protocols are not released, limiting reproducibility of specific flip rates.
- Cross-lingual transfer stability and generalization across OOD tasks may vary significantly with counterfactual phrasing and language model scale.
- Manual validation criteria for counterfactual plausibility are unspecified, and Garak probe configurations for robustness evaluation are not detailed.

## Confidence
- **High**: The conceptual framework for measuring belief shifts via log-likelihood differences is sound and reproducible.
- **Medium**: The characterization of localized corruption in late layers and partial reversibility via activation patching, though plausible, depends on exact probe placement and counterfactual quality.
- **Low**: Cross-lingual transfer stability and generalization across OOD tasks may vary significantly with counterfactual phrasing and language model scale.

## Next Checks
1. Reconstruct the fact-counterfact dataset for one domain (e.g., general knowledge) and verify that ∆LL correctly captures belief flips at baseline before any poisoning.
2. Run a minimal CPT experiment with Qwen2.5-0.5B at ρ = 1.0 and confirm layer-wise belief trajectories using logit lens; check if corruption localizes in late layers.
3. Apply activation patching on a single poisoned checkpoint and measure rescue rates; ensure patching is applied at the correct token position and layer range.