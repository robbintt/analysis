---
ver: rpa2
title: Can GPT replace human raters? Validity and reliability of machine-generated
  norms for metaphors
arxiv_id: '2512.12444'
source_url: https://arxiv.org/abs/2512.12444
tags:
- ratings
- metaphors
- human
- familiarity
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether large language models (LLMs) can
  replace human raters in generating psycholinguistic norms for metaphors. Three GPT
  models (GPT3.5-turbo, GPT4o-mini, GPT4o) were prompted to rate 687 metaphors in
  English and Italian on familiarity, comprehensibility, and imageability.
---

# Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors

## Quick Facts
- arXiv ID: 2512.12444
- Source URL: https://arxiv.org/abs/2512.12444
- Reference count: 26
- Primary result: GPT models can validly and reliably generate metaphor norms, though they struggle with conventionality and embodied meaning

## Executive Summary
This study investigates whether large language models can replace human raters in generating psycholinguistic norms for metaphors. Three GPT models (GPT3.5-turbo, GPT4o-mini, GPT4o) were prompted to rate 687 metaphors in English and Italian on familiarity, comprehensibility, and imageability. Results showed moderate-to-strong positive correlations between machine-generated and human-generated ratings, with larger models outperforming smaller ones. Machine-generated ratings significantly predicted human behavioral and electrophysiological responses, comparable to human ratings. Ratings were highly stable across sessions. However, LLMs showed greater misalignment with humans for familiar and highly imageable metaphors, and struggled with sensorimotor aspects of metaphors.

## Method Summary
Three GPT models were used to rate 687 metaphors (469 Italian, 218 English) on familiarity, comprehensibility, and imageability. Models were prompted using minimal adaptations of original human instructions. Ratings were collected via API with temperature=0 and log probability weighting of top-3 tokens. Validity was assessed through correlations with human benchmarks and predictive power for behavioral (RT) and EEG (N400) responses. Test-retest reliability was evaluated across sessions.

## Key Results
- Moderate-to-strong positive correlations between machine and human ratings (r=0.50-0.79 across dimensions)
- Larger models (GPT4o) outperformed smaller ones in alignment with human ratings
- Machine ratings predicted human behavioral and electrophysiological responses comparably to human ratings
- High test-retest reliability (r>0.90) when using API with temperature=0
- LLMs showed greater misalignment for familiar metaphors, highly imageable metaphors, and sensorimotor aspects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distributional linguistic regularities captured by LLMs partially approximate human psycholinguistic intuitions for figurative language.
- **Mechanism:** Transformer models trained on large text corpora encode statistical co-occurrence patterns that correlate with human familiarity and comprehensibility judgments.
- **Core assumption:** Human ratings for familiarity and comprehensibility are substantially shaped by linguistic exposure patterns that LLMs also capture.
- **Evidence anchors:** Abstract shows moderate-to-strong positive correlations; results show familiarity correlations ranged from r=0.50-0.64 (English) and r=0.20-0.65 (Italian); corpus evidence is limited.
- **Break condition:** Correlations degrade significantly for metaphors with high sensorimotor load, body-related metaphors (r=-0.16 for GPT3.5-turbo), and languages less represented in training data.

### Mechanism 2
- **Claim:** Model scale predicts alignment quality with human raters.
- **Mechanism:** Larger models (GPT4o) encode richer contextual representations and demonstrate better cross-lingual transfer, improving alignment with human judgments compared to smaller models.
- **Core assumption:** Parameter count and training data diversity directly improve semantic representation quality for rating tasks.
- **Evidence anchors:** Abstract notes larger models outperforming smaller ones; results show GPT4o showed strongest correlations for both languages; limited direct corpus evidence on scale effects.
- **Break condition:** Smaller models may suffice for well-represented languages (English) and simpler dimensions (comprehensibility).

### Mechanism 3
- **Claim:** LLMs exhibit systematic misalignment on embodied and conventional figurative expressions.
- **Mechanism:** Without sensorimotor grounding, LLMs rely on linguistic co-occurrence rather than experiential simulation, leading to underestimation of familiarity for conventional metaphors and poor performance on imageability ratings for highly concrete metaphors.
- **Core assumption:** Human judgments for imageability and highly familiar metaphors depend on sensorimotor simulation that text-only models cannot replicate.
- **Evidence anchors:** Abstract highlights LLM struggles with sensorimotor aspects; results show higher error for high imageability (β=0.33 slope) and familiarity (β=0.25); body-related metaphors showed weakest alignment.
- **Break condition:** Misalignment is partially mitigated in larger models (GPT4o error slope flatter than GPT3.5-turbo), but the grounding gap persists across model sizes.

## Foundational Learning

- **Concept: Psycholinguistic norming and Likert scales**
  - Why needed here: Understanding what "familiarity," "imageability," and "comprehensibility" mean as constructs is essential for interpreting validity results.
  - Quick check question: If an LLM rates "Lawyers are sharks" as highly familiar but humans rate it moderately, which dimension(s) might explain this discrepancy?

- **Concept: Correlation vs. explanatory power (predictive validity)**
  - Why needed here: The paper tests both alignment (correlations) and whether machine ratings predict behavioral/neural responses as well as human ratings do.
  - Quick check question: A correlation of r=0.65 between human and machine ratings means what proportion of variance is shared?

- **Concept: Temperature and log probability weighting in LLM outputs**
  - Why needed here: API reliability depends on setting temperature=0 and using log probabilities to create continuous ratings from discrete token outputs.
  - Quick check question: Why would using the ChatGPT interface (no temperature control) yield lower test-retest reliability than the API?

## Architecture Onboarding

- **Component map:** Metaphor sentence + rating dimension instruction -> GPT model -> top-k token predictions with log probabilities -> weighted rating score = Σ(rating_i × logprob_i) -> correlation with human benchmarks/validation in behavioral/EEG models

- **Critical path:**
  1. Set temperature=0 via API (not interface) to ensure deterministic outputs
  2. Request single-token rating output with max_tokens=1
  3. Extract top-3 tokens with log probabilities
  4. Compute weighted continuous rating
  5. Validate against human benchmarks before deployment

- **Design tradeoffs:**
  - API vs. ChatGPT interface: API enables temperature control and log probability extraction but requires technical integration; interface is accessible but shows lower reliability (r=0.66-0.91 vs. >0.90)
  - Larger vs. smaller models: GPT4o provides best alignment but higher cost; smaller models may suffice for English-only, low-sensorimotor stimuli
  - Single prompt vs. prompt engineering: Paper deliberately used minimal prompt adaptation to match human instructions, trading potential performance gains for ecological validity

- **Failure signatures:**
  - Low correlations for body-related, physical, or auditory metaphors (sensorimotor load)
  - High error for ratings at the upper end of human scales (conventional, highly imageable)
  - Weak or negative correlations for non-English languages with smaller models
  - Interface-based collection shows unstable session-to-session ratings

- **First 3 experiments:**
  1. Replicate correlation analysis on a held-out set of 50 metaphors spanning low-to-high sensorimotor load to quantify where alignment breaks down for your target stimuli type
  2. Compare API (temperature=0) vs. interface reliability on the same 30 items across two sessions to validate platform choice for your use case
  3. Test whether log probability weighting improves correlation vs. raw token output on 100 items to determine if added complexity is justified for your precision requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs accurately rate highly conventional figurative expressions, such as idioms, given their tendency to maintain a rigid distinction between literal and figurative language?
- Basis in paper: The authors suggest that "Future research could further test this speculation by assessing, for instance, the validity of LLM-generated ratings for highly conventional yet figurative expressions such as idioms."
- Why unresolved: The study found that LLMs under-rate familiar metaphors compared to humans, suggesting they may be less sensitive to the conventionalization of figurative meaning.
- What evidence would resolve it: A comparison of human and LLM ratings specifically for idioms versus familiar metaphors to see if the "misalignment" persists or worsens.

### Open Question 2
- Question: Would integrating multimodal inputs (e.g., vision or auditory data) into LLMs improve their alignment with human ratings for imageability and sensorimotor metaphors?
- Basis in paper: The authors conclude that LLMs align worse with humans on "multimodal aspects of metaphorical meaning" and lack sensorimotor grounding, relying instead on linguistic co-occurrence.
- Why unresolved: The current study tested text-only models (GPT-3.5/4o), leaving the potential benefits of embodied or multimodal AI architectures unexplored for this specific rating task.
- What evidence would resolve it: A study comparing ratings from text-only LLMs against those from vision-language models (e.g., GPT-4o with image inputs) on metaphors with high sensorimotor loads.

### Open Question 3
- Question: Can LLMs be prompted or fine-tuned to simulate individual variability in metaphor ratings rather than generating a single "wisdom of the crowd" average?
- Basis in paper: The authors note that LLMs "at this point can only approximate an average human participant... This does not allow for focus on individual variability, for which the recruitment of human participants is still essential."
- Why unresolved: Current prompting methods produce a stable average rating, flattening the distribution of responses that would be seen in a diverse human population.
- What evidence would resolve it: Experiments using "persona" prompting or diverse sampling techniques to see if the resulting distribution of ratings mimics the variance found in human datasets.

## Limitations

- Performance degrades for highly conventional or imageable expressions and sensorimotor-rich metaphors
- Use of minimal prompt adaptation prioritizes ecological validity over potential performance gains
- Generalizability to other languages and domains remains uncertain, especially for less-represented languages
- Study relies on previously collected human ratings rather than collecting fresh data with the same models

## Confidence

- **High confidence:** Moderate-to-strong positive correlations between machine and human ratings (r=0.50-0.79 across dimensions) and test-retest reliability (r>0.90) when using API with temperature=0
- **Medium confidence:** Predictive validity findings (machine ratings predicting behavioral and EEG responses comparably to human ratings) and claim that larger models outperform smaller ones
- **Low confidence:** Generalization of findings to other languages, especially those with limited representation in training data, or to other types of figurative language beyond metaphors

## Next Checks

1. Replicate correlation analysis on a held-out set of 50 metaphors spanning low-to-high sensorimotor load to quantify where alignment breaks down for your target stimuli type
2. Compare API (temperature=0) vs. interface reliability on the same 30 items across two sessions to validate platform choice for your use case
3. Test whether log probability weighting improves correlation vs. raw token output on 100 items to determine if added complexity is justified for your precision requirements