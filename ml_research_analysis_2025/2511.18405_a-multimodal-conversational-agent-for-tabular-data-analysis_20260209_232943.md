---
ver: rpa2
title: A Multimodal Conversational Agent for Tabular Data Analysis
arxiv_id: '2511.18405'
source_url: https://arxiv.org/abs/2511.18405
tags:
- code
- data
- execution
- design
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Talk2Data is a multimodal conversational agent for tabular data
  analysis that enables users to query datasets via voice or text and receive multimodal
  responses (plots, tables, statistics, or spoken explanations). Built on an LLM orchestration
  loop with OpenAI Whisper (ASR), Qwen-coder (code generation), a sandboxed execution
  environment, and Coqui TTS, the system supports multi-turn dialogues grounded in
  dataset context.
---

# A Multimodal Conversational Agent for Tabular Data Analysis

## Quick Facts
- arXiv ID: 2511.18405
- Source URL: https://arxiv.org/abs/2511.18405
- Reference count: 34
- 95.8% accuracy on 48 tasks, 1.15–1.64s model-only latency with 7B model providing best trade-off

## Executive Summary
Talk2Data is a multimodal conversational agent enabling voice or text queries on tabular datasets with responses including plots, tables, statistics, and spoken explanations. Built on an LLM orchestration loop with OpenAI Whisper, Qwen-coder, a sandboxed execution environment, and Coqui TTS, it supports multi-turn dialogues grounded in dataset context. Evaluated on 48 tasks across three public datasets, Talk2Data achieved 95.8% accuracy with model-only latency under 1.7 seconds. The 7B model provided the best accuracy-latency-cost trade-off for interactive use.

## Method Summary
Talk2Data implements an orchestration loop routing user queries through ASR, LLM decision-making, code generation, sandboxed execution, and TTS synthesis. The system uses three prompt types: decision (code vs chat), code generation (expression-oriented Python), and chat response (speakable text). A sandboxed environment with whitelisted libraries executes generated code, while context packs maintain dataset metadata and dialogue history. The system was evaluated on three datasets (Otto Group Products, Student Scores, US Flights 2008) using 48 tasks spanning visualization, analytical, and narrative queries.

## Key Results
- Achieved 95.8% task accuracy across 48 evaluation tasks
- Model-only latency: 1.15–1.64 seconds (excluding ASR and execution)
- 7B model provided optimal accuracy-latency-cost trade-off
- Supports multi-turn dialogues with context preservation

## Why This Works (Mechanism)
Talk2Data works by decomposing complex data analysis queries into an LLM-orchestrated pipeline that routes tasks to either code execution or direct response generation based on query type. The system maintains context through schema metadata and dialogue history, enabling coherent multi-turn interactions. Sandboxed execution ensures safety while allowing flexible data manipulation, and multimodal outputs (visual, textual, spoken) accommodate diverse user preferences and accessibility needs.

## Foundational Learning
- **LLM Orchestration**: Coordinating multiple specialized LLM calls for complex tasks - needed for routing between code generation and direct responses; quick check: verify decision prompt correctly classifies 90%+ of test queries
- **Sandboxed Code Execution**: Securely running generated code with restricted resources - needed to prevent harmful operations while enabling data analysis; quick check: confirm sandbox blocks network access and filesystem operations
- **Multimodal Response Generation**: Producing synchronized visual, textual, and spoken outputs - needed for accessibility and user preference accommodation; quick check: verify all three output modalities work for a sample query
- **Context Pack Management**: Structuring dataset metadata and dialogue history for LLM consumption - needed for maintaining coherent multi-turn conversations; quick check: test context retention across 3+ dialogue turns
- **Router Decision Making**: Classifying queries as code-generation vs. chat-response - needed for efficient task routing; quick check: log router decisions and compare against expected paths
- **TTS Integration**: Converting text responses to natural speech - needed for voice-based interaction; quick check: verify TTS produces clear, natural-sounding output

## Architecture Onboarding

**Component Map**: ASR -> Decision Router -> (Code Generation -> Sandbox Execution) OR (Chat Response) -> TTS/Synthesis

**Critical Path**: User query → Whisper ASR → Decision Router → (Code Generation → Sandbox Execution → Visualization) OR (Chat Response → TTS) → User output

**Design Tradeoffs**: The system prioritizes safety through sandboxing over execution flexibility, chooses a router-based approach over end-to-end generation for better control, and maintains context through explicit packs rather than relying on model context windows. These tradeoffs enable reliable operation at the cost of some flexibility.

**Failure Signatures**:
- Routing misclassification: Clear plotting request classified as chat_response, producing no artifact
- Sandbox execution errors: Generated code attempts blocked imports or operations
- Context loss: Multi-turn dialogue fails to maintain coherence across exchanges
- Latency issues: Response times exceed 2 seconds for simple queries

**First 3 Experiments**:
1. Test decision router with mixed query types to verify 90%+ correct routing
2. Execute sample code in sandbox to confirm restricted environment works
3. Run complete pipeline on simple query to verify end-to-end functionality

## Open Questions the Paper Calls Out
None

## Limitations
- Prompt templates for the three-stage generation pipeline are not provided
- Sandbox configuration details (specific whitelisted libraries, timeout values) are underspecified
- Evaluation only covers three datasets and 48 tasks, limiting generalizability
- Context pack structure and serialization format are not fully detailed

## Confidence

**High confidence**: Architectural feasibility and core concept of multimodal conversational agents for tabular data analysis through LLM orchestration.

**Medium confidence**: Reported accuracy (95.8%) and latency (1.15–1.64s model-only) metrics, dependent on unspecified prompt engineering and sandbox configurations.

**Low confidence**: Generalizability of results beyond the three tested datasets and 48 tasks.

## Next Checks

1. **Prompt Template Validation**: Implement and test decision, code generation, and chat response prompt templates with few-shot examples to verify 95.8% accuracy claim.

2. **Sandbox Configuration Testing**: Build and validate execution environment with specific whitelist libraries, timeout settings, and error-handling; test with code attempting restricted operations.

3. **Context Management Evaluation**: Implement context pack structure with schema metadata and dialogue history; test multi-turn conversations for coherence and proper metadata incorporation.