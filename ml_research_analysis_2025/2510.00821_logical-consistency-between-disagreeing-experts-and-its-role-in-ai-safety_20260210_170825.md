---
ver: rpa2
title: Logical Consistency Between Disagreeing Experts and Its Role in AI Safety
arxiv_id: '2510.00821'
source_url: https://arxiv.org/abs/2510.00821
tags:
- 'true'
- test
- possible
- label
- evaluations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal logic for unsupervised evaluation
  of classifiers based on the logical consistency of expert disagreements. The core
  problem is to compute the set of group evaluations logically consistent with observed
  agreements and disagreements in their decisions.
---

# Logical Consistency Between Disagreeing Experts and Its Role in AI Safety

## Quick Facts
- arXiv ID: 2510.00821
- Source URL: https://arxiv.org/abs/2510.00821
- Authors: Andrés Corrada-Emmanuel
- Reference count: 40
- Primary result: A formal logic for unsupervised evaluation of classifiers based on expert disagreement, successfully detecting misaligned LLM-as-Judges in MT-Bench benchmark.

## Executive Summary
This paper introduces a formal logic for unsupervised evaluation of classifiers by analyzing the logical consistency of expert disagreements. The core innovation is that disagreement between experts provides actionable information for evaluation, whereas agreement does not. The method uses statistical summaries of aligned decisions as inputs to a linear programming problem in integer space, constrained by both inequality constraints and universal axioms. The approach is demonstrated through "no-knowledge alarms" that can detect when LLM-as-Judges violate user-specified accuracy thresholds, successfully identifying misaligned classifiers in the MT-Bench benchmark where observed disagreements made it mathematically impossible for GPT-4 to exceed 46% accuracy on certain labels.

## Method Summary
The method computes the set of group evaluations logically consistent with observed agreements and disagreements between N classifiers on a test with Q items and R labels, without access to ground truth. It takes statistical summaries of classifier responses as label integer counts for each classifier, then applies inequality constraints (correct responses cannot exceed observed responses) and axioms (universally applicable linear equalities for label conservation). The system enumerates all possible true label distributions in the Q-complex, filters through the axioms to find feasible evaluation sets, and checks if observed disagreements make it impossible for all classifiers to meet a minimum accuracy threshold simultaneously. The approach is validated on MT-Bench data with 25 LLM pair comparisons graded by human experts and GPT-4, triggering alarms for accuracy thresholds above 46%.

## Key Results
- For a test of size Q with R labels, there are Q^R(R-1) possible evaluations for a single classifier, reducible by (R-1) dimensions through axioms
- The method successfully detected misaligned classifiers in MT-Bench benchmark
- Observed disagreements between human experts and GPT-4 were sufficient to trigger alarms for accuracy thresholds above 46%
- The logical asymmetry principle (disagreement vs. agreement) enables unsupervised evaluation without ground truth

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Disagreement between experts provides actionable information for unsupervised evaluation, whereas total agreement does not.
- **Mechanism:** The logic relies on an asymmetry: if two experts disagree, they cannot both be 100% correct. This allows the system to exclude "joint evaluations" (hypotheses about expert accuracy) that contradict the observed disagreement. Agreement, conversely, is non-informative because both could be wrong in the same way.
- **Core assumption:** Experts operate as classifiers with a finite set of labels (R) and that a definitive "answer key" exists, even if unknown.
- **Evidence anchors:** [Abstract]: "If two experts disagree... we may conclude both cannot be 100 per cent correct. But if they completely agree, no possible evaluation can be excluded." [Section I]: "The one-bit summary... is a simple example of how the logic works... observing disagreement would have triggered an alarm." [Corpus]: Paper 55650 (No-Knowledge Alarms) expands on this specific asymmetry for LLMs-as-Judges.
- **Break condition:** If experts are perfectly correlated or colluding (always agreeing), the logic cannot exclude any evaluation, rendering the alarm silent.

### Mechanism 2
- **Claim:** The set of logically valid evaluations is constrained by linear inequalities and universally applicable "axioms" in integer space.
- **Mechanism:** The method formalizes the problem as a Linear Programming (LP) challenge. It uses "obvious" constraints (e.g., number of correct responses ≤ observed responses) and formal axioms (linear equalities derived from label conservation) to reduce the dimensionality of possible solutions by R-1 dimensions.
- **Core assumption:** The "Q-complex" (distribution of true labels) is finite and fixed for the test duration.
- **Evidence anchors:** [Section II]: "Obvious logical constraints... are inequalities. But in addition, there are 'axioms'—universally applicable linear equalities..." [Section II.A]: "They reduce the size of the possible set and also the dimension of its geometry."
- **Break condition:** If the test size Q is too small relative to the number of labels R, the integer constraints may be too loose to effectively prune the space of possible evaluations.

### Mechanism 3
- **Claim:** "No-knowledge alarms" can trigger when observed disagreements make it mathematically impossible for all experts to meet a user-defined accuracy threshold.
- **Mechanism:** By projecting the "possible evaluations" onto an accuracy metric, the system identifies if the maximum possible accuracy for any expert falls below the threshold. In the MT-Bench example, the observed disagreement between humans and GPT-4 meant GPT-4 could not possibly be >46% accurate on "tie" labels if the humans were correct.
- **Core assumption:** The alarm threshold is label-symmetric (applies equally to all labels) or specifically defined by the user.
- **Evidence anchors:** [Section III]: "No possible evaluation... has both classifiers grading the tied pair comparisons better than 25%." [Section III]: "Any alarm condition set at more than 46% would trigger."
- **Break condition:** If the ground truth itself is undefined or subjective (e.g., "roleplaying" ability), the alarm may flag a logical inconsistency that is actually a flaw in the test design rather than the expert.

## Foundational Learning

- **Concept:** **Integer Linear Programming (ILP)**
  - **Why needed here:** The paper models expert evaluation not as probability distributions but as integer counts of correct/incorrect responses constrained by hard logic (axioms).
  - **Quick check question:** Can you explain why an inequality constraint (R^(ℓr)_i ≤ observed) defines a "half-space" in the geometry of possible solutions?

- **Concept:** **The Principal-Agent Problem**
  - **Why needed here:** This is the economic framing the paper uses to motivate the need for monitoring AI agents (LLMs) when the principal (user) cannot verify the work (no ground truth).
  - **Quick check question:** How does the "infinite monitoring chain" (who watches the watcher) relate to the unsupervised nature of this logic?

- **Concept:** **Confusion Matrix / Label Prevalence**
  - **Why needed here:** The method operates in "R-space" (integer response counts) and "Q-complex" (prevalence of true labels). Understanding the geometry of these spaces is required to interpret the dimensionality reduction claims.
  - **Quick check question:** If you have 3 labels (A, B, Tie) and 10 questions, how many dimensions define the "Q-complex" (distribution of true answers)?

## Architecture Onboarding

- **Component map:** Input Interface -> Constraint Generator -> LP Solver -> Alarm Trigger
- **Critical path:** The derivation of the **Axioms** (Section II.A) is the most sensitive step. These must be exact linear equalities; a typo here invalidates the entire logical exclusion principle.
- **Design tradeoffs:**
  - *R-space vs. P-space:* The paper uses R-space (integers) for visualization and implementation, but notes P-space (probabilities) is better for proving completeness. Choose R-space for engineering/implementation, P-space for theoretical verification.
  - *M=1 vs. M>1:* The paper focuses on M=1 (individual summaries) which is computationally simpler but ignores correlations. M=2 (pairwise summaries) provides stronger exclusion but is algebraically complex.
- **Failure signatures:**
  - **False Alarms:** If the test size Q is small, the logic might exclude valid experts simply because there aren't enough data points to satisfy the axioms comfortably.
  - **Silent Failure:** If the "ground truth" is genuinely ambiguous (experts disagree due to valid subjectivity), the alarm will trigger logic errors that are actually domain errors.
- **First 3 experiments:**
  1. **Binary Replication:** Implement the Q=10 binary classifier example (Figs 1-4) to verify your LP solver correctly reduces the dimensionality from 3 to 2 using the binary axiom.
  2. **MT-Bench Alarm Test:** Replicate the Section III experiment. Feed the specific grade summaries for `authors` and `gpt4` into your system and verify that a 46% threshold triggers the alarm.
  3. **Label Sensitivity:** Test a 3-label system where you vary the "tie" count. Observe how the "possible evaluations" geometry changes in the Q-complex as ties increase.

## Open Questions the Paper Calls Out
None

## Limitations
- The method requires enumerating all possible true label distributions (Q-complex), which grows combinatorially with test size Q, potentially becoming intractable for larger tests
- The mathematical proof of completeness is stated to exist in P-space (probability space) but is not provided in the paper, creating uncertainty about theoretical guarantees
- The approach may produce false alarms when test size is small relative to the number of labels, as constraints may be too loose to effectively prune the solution space

## Confidence

**High Confidence:**
- The core logical asymmetry (disagreement vs. agreement) that enables unsupervised evaluation is well-established and directly supported by the text.

**Medium Confidence:**
- The implementation of axioms and their role in dimensionality reduction is described, but the mathematical rigor of the proof is not fully presented in the paper.

**Low Confidence:**
- The computational complexity claims and scalability limitations are mentioned but not empirically tested beyond the Q=25 case.

## Next Checks

1. **Scalability Test:** Implement the algorithm for increasing values of Q (10, 25, 50, 100) and measure computation time and memory usage to establish practical limits.

2. **Proof Verification:** Attempt to reconstruct the P-space completeness proof from the axioms and constraints described, or request the full proof from the authors.

3. **Domain Robustness:** Test the alarm system on datasets with known ground truth where the "correct" experts are predefined, to measure false positive/negative rates.