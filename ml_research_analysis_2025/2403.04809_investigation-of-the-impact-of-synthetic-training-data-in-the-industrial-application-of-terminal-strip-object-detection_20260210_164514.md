---
ver: rpa2
title: Investigation of the Impact of Synthetic Training Data in the Industrial Application
  of Terminal Strip Object Detection
arxiv_id: '2403.04809'
source_url: https://arxiv.org/abs/2403.04809
tags:
- terminal
- synthetic
- image
- images
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of synthetic training data for
  complex industrial object detection tasks, focusing on terminal strip detection.
  The authors develop an automated image synthesis pipeline that combines domain randomization
  and domain knowledge to generate 30,000 synthetic images of terminal strips.
---

# Investigation of the Impact of Synthetic Training Data in the Industrial Application of Terminal Strip Object Detection

## Quick Facts
- **arXiv ID:** 2403.04809
- **Source URL:** https://arxiv.org/abs/2403.04809
- **Reference count:** 40
- **Primary result:** Synthetic data + scale correction achieves 98.40% mAP@0.5 on real terminal strip detection

## Executive Summary
This paper demonstrates that synthetic training data can achieve high-quality object detection performance in complex industrial environments when combined with appropriate scaling strategies. The authors develop an automated image synthesis pipeline that generates 30,000 synthetic images of terminal strips by combining domain randomization with domain-specific structural knowledge. The key innovation is addressing scale mismatch between synthetic training and real inference through dynamic preprocessing, which enables the DINO model to achieve 98.40% mean average precision on real test images. This approach provides a practical solution for industrial applications where collecting and annotating large amounts of real data is costly or time-consuming.

## Method Summary
The authors created an automated image synthesis pipeline using Blender to generate 30,000 synthetic images (1024×512) of terminal strips from 3D CAD models. The pipeline randomizes lighting (46 HDRIs), viewpoints (3D Gaussian noise), and backgrounds (HDRI-mapped surfaces) while preserving realistic object arrangements and materials. Four standard object detectors (RetinaNet, Faster R-CNN, YOLOv8, and DINO) with ResNet50 backbones were trained on the synthetic dataset using MMDetection Toolbox for 20 epochs with MS COCO pre-training. The critical preprocessing step involves dynamically rescaling real images to match the scale of synthetic training data, either using a fixed factor of 1.5 or through a learned ResNet50 regression model.

## Key Results
- All four models achieve >94% mAP@0.5 on synthetic test images
- Without scaling adjustment, real image performance drops significantly (57.60% mAP@0.5 for DINO)
- With optimal scaling factor (1.5x), DINO achieves 98.40% mAP@0.5 on real test images
- The constrained bounding box annotation strategy (20% overlap limit) prevents NMS-induced false negatives in densely-arranged scenes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining domain randomization with domain-specific structural knowledge enables sim-to-real transfer for densely-arranged industrial objects.
- **Mechanism:** The pipeline randomizes factors the model should be invariant to (lighting, viewpoints, backgrounds) while preserving realistic object configurations (terminal blocks mounted on DIN rails with group-like arrangements, accurate material/color distributions at 80% gray).
- **Core assumption:** Real-world images will be interpreted as "another variation" when sufficient randomization covers the deployment distribution.
- **Evidence anchors:** [abstract] "combining domain randomization and domain knowledge to generate 30,000 synthetic images... randomizes lighting, viewpoints, and backgrounds while maintaining realistic object arrangements and materials"
- **Break condition:** If deployment environments have fundamentally different object arrangements or lighting conditions outside the HDRI/lighting distribution, transfer may fail.

### Mechanism 2
- **Claim:** Scale mismatch between synthetic training and real inference is the dominant factor in sim-to-real degradation for size-differentiated industrial object classes.
- **Mechanism:** When object classes differ primarily in size (e.g., PT 4 vs PT 2.5 terminal blocks), CNNs trained at one scale cannot generalize because they lack equivariance to the dilatation group. Dynamic preprocessing that rescales real images to match training distribution recovers near-synthetic performance (87.30%→98.40% mAP@0.5 for DINO).
- **Core assumption:** Primary distinguishing features between similar classes are scale-dependent rather than texture/shape-dependent.
- **Evidence anchors:** [abstract] "performance drops significantly on real images without scaling adjustment. By dynamically determining optimal scaling factors, the DINO model achieves 98.40% mean average precision"
- **Break condition:** If real objects appear at larger scales than training (opposite pattern), or scale cannot be inferred from single images without reference, this mechanism may not apply.

### Mechanism 3
- **Claim:** Restricting bounding box overlap annotations to 20% of thinner component width prevents NMS-induced false negatives in densely-arranged object detection.
- **Mechanism:** Standard full-extent boxes in crowded scenes create high overlap. NMS post-processing then suppresses valid detections as duplicates. Constrained boxes force tighter predictions that survive NMS while retaining sufficient features for classification.
- **Core assumption:** Models can learn to predict tight boxes that still capture discriminative object features.
- **Evidence anchors:** [section 3.5, p.8] "including all visible parts of a terminal block into its bounding box inevitably leads to highly overlapping annotations... NMS often results in a large number of false negatives"
- **Break condition:** If applications require full object extent (e.g., measuring dimensions) or detecting heavily occluded objects where tight boxes exclude discriminative features.

## Foundational Learning

- **Concept: Domain Randomization Philosophy**
  - **Why needed here:** Counterintuitive that unrealistic variations improve real-world performance; central to paper's approach.
  - **Quick check question:** Why would randomizing textures help detect real objects with specific colors? (Answer: Forces learning structural features invariant to appearance.)

- **Concept: Non-Maximum Suppression (NMS)**
  - **Why needed here:** Bounding box annotation strategy is designed to avoid NMS failures in crowded scenes.
  - **Quick check question:** With 25 adjacent terminal blocks and 0.5 IoU threshold NMS, what happens to predictions with overlapping boxes? (Answer: Suppression cascade eliminates valid detections.)

- **Concept: Scale Equivariance Limitations in CNNs**
  - **Why needed here:** Paper's key finding depends on understanding why CNNs fail under unseen scale variations.
  - **Quick check question:** A CNN trained on 32×32 object images tested on 64×64 images of the same objects—why might accuracy drop? (Answer: No built-in scale equivariance; features learned at one scale don't transfer.)

## Architecture Onboarding

- **Component map:** 3D CAD Models → Blender Pipeline (assembly/materials/camera/lighting/background) → Synthetic Images (30K @ 1024×512) → Detection Model (RetinaNet/Faster R-CNN/YOLOv8/DINO) → Scale Preprocessing (constant/ResNet regression/Bayesian opt) → Real Inference

- **Critical path:** (1) Material realism → spurious feature learning if mismatch; (2) Scale alignment → size-based class confusion if mismatch; (3) Tight box annotation → NMS over-suppression if too loose.

- **Design tradeoffs:**
  - Realistic materials + randomized environment vs. full randomization (paper chooses former for manageable effort)
  - Constrained scale + dynamic inference scaling vs. varied training scales (paper chooses constrained; size-differentiated classes require consistency)
  - 20% overlap constraint vs. full-extent boxes (paper chooses constrained for NMS survival)

- **Failure signatures:**
  - Size-variant class confusion (PT 4 → PT 2.5) → scale mismatch
  - Missing detections in crowded regions → NMS over-suppression from loose boxes
  - Inaccurate edge bounding boxes → low marking/background contrast (Figure 6)

- **First 3 experiments:**
  1. **Baseline gap measurement:** Train on 30K synthetic, test synthetic + 300 real (no scaling). Expect >94% mAP synthetic, <88% real. Quantifies domain gap.
  2. **Scale sensitivity sweep:** Test real images at scaling factors [1.0, 1.25, 1.5, 1.75, 2.0]. Plot mAP vs. scale. Validates scale-mismatch hypothesis.
  3. **NMS threshold variation:** Sweep IoU threshold [0.3–0.7] on optimized-scale real images. Lower thresholds should improve crowded-region recall if annotation constraint is adequate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the use of orthographic image synthesis eliminate the bounding box inaccuracies caused by perspective distortion in edge regions?
- Basis in paper: [explicit] The authors state, "We plan to investigate orthographic images of terminal strips since perspective images complicate the determination of accurate bounding boxes in the edge regions."
- Why unresolved: The current perspective rendering introduces geometric distortions that make precise localization difficult for objects located at the periphery of the terminal strips.
- What evidence would resolve it: A comparative study showing a reduction in localization errors (higher mAP@[0.5:0.95]) when training on orthographic synthetic data versus the current perspective dataset.

### Open Question 2
- Question: How can the dynamic scaling regression model be improved to match the performance of the optimal scaling baseline?
- Basis in paper: [explicit] The conclusion notes that the "learned image rescaling could not fix the issue entirely" and that the approach "leaves room for improvement."
- Why unresolved: While the ResNet50 regression model improved results, a significant performance gap remained between it and the Bayesian optimization upper bound (real (Opt)).
- What evidence would resolve it: Development of a scaling prediction model that achieves mAP scores statistically indistinguishable from the Bayesian optimization baseline on the real test set.

### Open Question 3
- Question: Does the sim-to-real performance degrade when scaling the detection task from 40 classes to a full industrial portfolio (e.g., 10,000 parts)?
- Basis in paper: [explicit] The authors suggest that "additional object classes should be considered to scale up our approach" to match real-world portfolios like Phoenix Contact's 10,000 parts.
- Why unresolved: It is unclear if the high performance (98.40% mAP) holds when the number of classes increases by orders of magnitude, potentially increasing inter-class similarity and confusion.
- What evidence would resolve it: Evaluation of the pipeline on a synthetic dataset containing thousands of distinct terminal block classes to measure if class confusion increases.

### Open Question 4
- Question: How does purely synthetic training data compare against heavily augmented real-world data when no labeled ground truth is available?
- Basis in paper: [explicit] The discussion states, "It would be interesting to benchmark augmented data of the target domain against synthetic data, which we leave for future work."
- Why unresolved: The paper focuses on fully synthetic training (worst-case) and real testing, but does not compare this against the alternative of heavily augmenting a small set of unlabeled real images.
- What evidence would resolve it: An experiment comparing detection accuracy of models trained purely on synthetic data versus models trained on heavily augmented real images (without labels) in a few-shot or unsupervised domain adaptation setting.

## Limitations

- The scale correction mechanism relies on the assumption that real objects appear at smaller scales than synthetic ones, which needs validation across different object classes and industrial settings.
- The constrained bounding box annotation strategy (20% overlap limit) may not generalize to applications requiring full object extent measurements or heavily occluded objects.
- The paper focuses on a specific industrial application (terminal strips) with 40 classes, limiting generalizability to other object detection tasks with different characteristics.

## Confidence

- **High Confidence:** The effectiveness of synthetic training data combined with scale correction for terminal strip detection. Direct experimental evidence shows 87.30%→98.40% mAP@0.5 improvement for DINO.
- **Medium Confidence:** The generalizability of the approach to other industrial object detection tasks. While the methodology is sound, validation on different object categories and industrial environments is limited.
- **Low Confidence:** The necessity of the constrained bounding box annotation strategy. While the paper argues it prevents NMS failures, no ablation study compares it against standard full-extent annotations.

## Next Checks

1. **Scale Direction Validation:** Test real images with scaling factors both larger (0.75x) and smaller (2.0x) than the proposed 1.5x correction to confirm the directionality of the scale mismatch.
2. **Cross-Domain Transfer:** Apply the same synthetic training + scaling approach to a different industrial object detection task (e.g., electronic components, mechanical parts) to assess generalizability.
3. **Annotation Strategy Ablation:** Train models with standard full-extent bounding boxes versus the proposed constrained boxes, measuring mAP@0.5 and false negative rates in crowded scenes to validate the annotation strategy's impact.