---
ver: rpa2
title: 'Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles'
arxiv_id: '2405.21027'
source_url: https://arxiv.org/abs/2405.21027
tags:
- policy
- fusion
- uni00000013
- uni0000004c
- nash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Fusion-PSRO, a method to improve policy initialization
  in Policy Space Response Oracles (PSRO) for zero-sum games. It introduces Nash Policy
  Fusion, which initializes new policies by weighted averaging historical best responses
  according to the current meta-Nash equilibrium.
---

# Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles

## Quick Facts
- **arXiv ID:** 2405.21027
- **Source URL:** https://arxiv.org/abs/2405.21027
- **Reference count:** 40
- **Primary result:** Fusion-PSRO reduces exploitability in PSRO by initializing best responses via weighted averaging of historical policies according to the current meta-Nash equilibrium.

## Executive Summary
Fusion-PSRO introduces Nash Policy Fusion to improve policy initialization in Policy Space Response Oracles (PSRO) for zero-sum games. The method initializes new best responses by computing a weighted average of historical policy parameters, where weights are determined by the current meta-Nash equilibrium distribution. This approach acts as an implicit guide policy, enabling faster exploration and better approximation of optimal strategies. Empirical results on Leduc Poker, Goofspiel, and Liar's Dice demonstrate significant reductions in exploitability compared to standard PSRO variants, with the fusion computation adding only minimal overhead.

## Method Summary
Fusion-PSRO modifies the standard PSRO loop by introducing Nash Policy Fusion for best response initialization. Instead of starting BR training from random initialization, the method computes a weighted average of all historical policy parameters using the current meta-Nash equilibrium probabilities as weights. This fused parameter set serves as the initialization for the new BR neural network. The approach aims to place the new policy in a high-value region of the parameter space, reducing sample complexity and improving convergence to lower exploitability strategies.

## Key Results
- Fusion-PSRO reduces exploitability from 0.538 to 0.377 on Leduc Poker compared to standard PSRO variants.
- The method achieves significant exploitability reduction across Leduc Poker, Goofspiel, and Liar's Dice.
- Fusion computation adds only 0.01% overhead in Leduc Poker experiments.

## Why This Works (Mechanism)

### Mechanism 1
Initializing a new best response policy via weighted averaging of historical policies acts as an "implicit guide policy," reducing sample complexity by starting exploration in high-value regions. The method computes a parameter average using current Meta-Nash Equilibrium probabilities, placing the new policy in a region that approximates the utility of the current equilibrium ensemble. This assumes the weighted average of model parameters provides a functional approximation of the ensemble output.

### Mechanism 2
The fusion process functions as a Nash-Weighted Moving Average, smoothing the optimization landscape and finding flatter minima for greater robustness. Unlike standard moving averages with fixed decay rates, this method dynamically adjusts weights based on the Meta-NE at each iteration, steering the population away from sharp minima toward flatter, more robust regions.

## Foundational Learning

- **Concept: Policy Space Response Oracles (PSRO)**
  - **Why needed here:** Fusion-PSRO is a modification of the PSRO loop. You must understand the standard cycle: (1) Compute Meta-NE for the population, (2) Sample an opponent, (3) Train a Best Response (BR) oracle.
  - **Quick check question:** In a standard PSRO loop, what strategy does the "oracle" train against?

- **Concept: Meta-Nash Equilibrium (Meta-NE)**
  - **Why needed here:** The Meta-NE determines the fusion weights. You need to understand that this is a probability distribution over the population of policies, not a single neural network.
  - **Quick check question:** If the Meta-NE is `[0.8, 0.2]` for policies A and B, how are their network weights combined during fusion?

- **Concept: Exploitability**
  - **Why needed here:** This is the primary metric for success. Lower exploitability means the strategy is closer to a true Nash Equilibrium.
  - **Quick check question:** Does a lower exploitability score mean the policy wins more against a specific opponent, or that it is harder to exploit in general?

## Architecture Onboarding

- **Component map:** Meta-Solver -> Nash Policy Fusion Module -> Oracle (RL Agent)
- **Critical path:**
  1. Delay: Do not fuse at iteration 0. Wait until `current_iteration >= c`.
  2. Compute Weights: Retrieve the Meta-NE from the solver.
  3. Fuse: Execute weighted sum of parameters.
  4. Initialize: Load fused parameters into the neural network before BR training.

- **Design tradeoffs:**
  - Solver Choice: PRD may underperform with fusion due to probability concentration; uniform or standard Nash solver is safer.
  - Start Threshold ($c$): Starting too early may bias the population; starting too late misses acceleration benefits.

- **Failure signatures:**
  - Mode Collapse: Fused policy performs worse than random initialization at training start.
  - Stagnant Exploitability: Curve flattens higher than baseline, indicating reinforcement of local optimum.

- **First 3 experiments:**
  1. Initialization Quality Check: Compare fresh fused policy reward against random policy facing current Meta-NE.
  2. Ablation on Start Threshold ($c$): Vary $c$ (0, 2, 10) on Leduc Poker to identify optimal fusion timing.
  3. Integration Test: Plug Nash Policy Fusion into pre-existing PSRO implementation and verify faster convergence.

## Open Questions the Paper Calls Out

- **Question:** How can Nash Policy Fusion be adapted to work effectively with Projected Replicator Dynamics (PRD) or other Meta-Solvers that focus on a small subset of policies?
  - **Basis in paper:** Section 5.4 states no performance improvement with PRD due to excessive focus on small policy subsets.
  - **Why unresolved:** Current fusion mechanism assumes a distribution that PRD violates or handles poorly.
  - **What evidence would resolve it:** Modified fusion weight calculation that restores performance with PRD.

- **Question:** Is there a theoretical or adaptive method to determine the optimal fusion start iteration ($c$) rather than relying on empirical tuning?
  - **Basis in paper:** Section 5.5 notes $c$ is "empirically set based on NashConv convergence."
  - **Why unresolved:** Paper provides specific values but no generalizable rule for automation.
  - **What evidence would resolve it:** Derivation showing relationship between game convergence and optimal $c$, or adaptive algorithm.

- **Question:** Does Fusion-PSRO maintain its efficiency and performance benefits when scaled to high-dimensional, continuous, or visual-input games?
  - **Basis in paper:** Introduction references complex games like StarCraft and DOTA2, but experiments are restricted to smaller games.
  - **Why unresolved:** Efficacy of simple weighted averaging may change in deep neural networks with high-dimensional visual inputs.
  - **What evidence would resolve it:** Empirical validation on large-scale benchmarks showing maintained benefits.

## Limitations

- **Parameter Space Compatibility:** Assumes linear mode connectivity between policies; weighted average may produce "dead" policies if policies are too diverse in parameter space.
- **Solver Dependency:** Effectiveness depends heavily on Meta-Nash Solver's output distribution; PRD underperforms due to probability concentration.
- **Lack of Theoretical Guarantees:** No formal convergence guarantees specific to the fusion method despite citing JSRL logic.

## Confidence

- **Exploitability Reduction:** High Confidence - Well-supported with specific numbers and ablation studies
- **Initialization Quality:** Medium Confidence - Supported by ablation studies but relies on unverified assumptions about parameter space geometry
- **Computational Efficiency:** High Confidence - Straightforward to verify with minimal overhead claims directly supported

## Next Checks

1. **Parameter Space Diversity Analysis:** Run ablation experiments varying historical policy diversity to identify when fusion transitions from beneficial to harmful, measuring parameter distances and correlating with performance.

2. **Solver Robustness Test:** Implement Fusion-PSRO with multiple Meta-Nash Solvers (Nash, PRD, Uniform) on the same game suite to quantify sensitivity to solver choice and verify PRD underperformance claims.

3. **Cross-Domain Generalization:** Apply Fusion-PSRO to larger, more complex games beyond small poker variants to measure whether exploitability reduction benefits scale with problem complexity.