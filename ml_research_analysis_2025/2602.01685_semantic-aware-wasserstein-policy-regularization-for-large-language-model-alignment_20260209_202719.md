---
ver: rpa2
title: Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment
arxiv_id: '2602.01685'
source_url: https://arxiv.org/abs/2602.01685
tags:
- policy
- wasserstein
- regularization
- distance
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Wasserstein Policy Regularization (WPR),
  a new approach for aligning large language models (LLMs) with human preferences
  in reinforcement learning from human feedback (RLHF). Unlike traditional KL-based
  regularization, which only compares token probabilities at identical indices, WPR
  leverages the entropy-regularized Wasserstein distance to capture semantic similarity
  between tokens.
---

# Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment

## Quick Facts
- **arXiv ID:** 2602.01685
- **Source URL:** https://arxiv.org/abs/2602.01685
- **Reference count:** 40
- **Primary result:** Introduces WPR that outperforms KL- and f-divergence-based baselines on summarization and dialogue generation tasks, achieving higher win rates through semantic-aware policy regularization.

## Executive Summary
This paper introduces Wasserstein Policy Regularization (WPR), a new approach for aligning large language models (LLMs) with human preferences in reinforcement learning from human feedback (RLHF). Unlike traditional KL-based regularization, which only compares token probabilities at identical indices, WPR leverages the entropy-regularized Wasserstein distance to capture semantic similarity between tokens. This is achieved by incorporating the geometry of the token space into the regularization, allowing for a more meaningful comparison of model policies. The method uses the dual formulation of the Sinkhorn distance to express the regularization as penalty terms applied to the reward, making it compatible with standard RL algorithms like PPO. Empirically, WPR outperforms KL- and f-divergence-based baselines on summarization and dialogue generation tasks, achieving higher win rates and better overall performance. The results demonstrate the effectiveness of semantic-aware policy distances for improving LLM alignment.

## Method Summary
WPR replaces KL regularization in RLHF with entropy-regularized Wasserstein distance computed via Sinkhorn iterations. The method computes a cost matrix from token embeddings, runs Sinkhorn to obtain dual variables representing per-token penalties, and applies these penalties as additive terms to the reward. This creates a semantic-aware regularization that treats similar tokens more leniently than dissimilar ones, while remaining compatible with PPO through the dual formulation. The approach requires precomputing a sparse cost matrix and running Sinkhorn iterations during training, but otherwise integrates seamlessly with existing RLHF pipelines.

## Key Results
- WPR achieves win rates of 54.5% on TL;DR summarization and 48.4% on HH-RLHF dialogue, outperforming both KL and f-divergence baselines
- The method shows higher robustness to the regularization coefficient β compared to f-divergences, with a wider optimal range
- WPR produces more semantically coherent candidate sets, with lower mean pairwise embedding distances among top-10 predictions
- Penalty correlations with KL are strong (r=0.917) but systematically lower (slope 0.579), indicating more lenient regularization while maintaining alignment

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Aware Distance via Wasserstein Cost Geometry
- **Claim:** Wasserstein distance captures token semantic similarity that KL and f-divergences miss.
- **Mechanism:** Unlike KL divergence which compares probability mass at identical token indices, Wasserstein distance computes an optimal transport plan that moves probability mass between distributions at a cost determined by token embedding distances. When a learned policy shifts probability from "cat" to "kitten" versus from "cat" to "table," Wasserstein assigns lower penalty to the semantically similar substitution because the embedding distance (cost) is smaller. This semantic-aware penalization encourages exploration within semantically coherent regions rather than treating all token substitutions equally.
- **Core assumption:** Token embeddings encode semantic similarity (e.g., Euclidean or cosine distance in embedding space correlates with semantic relatedness).
- **Evidence anchors:**
  - [abstract]: "WPR leverages the entropy-regularized Wasserstein distance to capture semantic similarity between tokens...incorporates the geometry of the token space"
  - [Section 1, Figure 1]: Demonstrates that KL and JS assign equal distance to (cat→kitten) and (cat→table), while Wasserstein correctly identifies (cat→kitten) as closer.
  - [corpus]: Weak direct corpus support; neighbor papers focus on reward modeling and exploration rather than semantic geometry.
- **Break condition:** If token embeddings do not meaningfully encode semantic relationships for the target domain, the cost matrix provides no useful signal and WPR reduces to a noisier KL substitute.

### Mechanism 2: Dual Formulation Yields Token-Level Penalties
- **Claim:** The entropy-regularized Wasserstein dual variables provide a tractable per-token penalty compatible with PPO.
- **Mechanism:** The primal optimal transport problem (minimizing transport cost under marginal constraints) has a dual formulation where Lagrange multipliers ϕ, ψ represent shadow prices for the marginal constraints. Under strong duality, the optimal dual variable ϕ* can be interpreted as the penalty each token incurs for deviating from the reference distribution, weighted by transport costs. By Theorem 2, the Wasserstein-regularized objective becomes E[R(x,y) - βϕ*_y] + C, which is structurally identical to KL-regularized RLHF where the penalty is -β log(π_θ/π_ref). This allows direct substitution into PPO's advantage computation without algorithmic changes.
- **Core assumption:** Strong duality holds for the entropy-regularized problem, and the Sinkhorn-Knopp algorithm converges to sufficiently accurate dual variables.
- **Evidence anchors:**
  - [Section 4.1, Theorem 2]: "J̃_W(π_θ; π_ref) = E[Σ_n E[R(x,y_{1:n}) - βϕ*_{y_n}]] + C"
  - [Section 3.1, Eq. 4]: Shows the dual formulation of Sinkhorn distance.
  - [corpus]: No direct corpus validation of this specific dual-to-penalty mapping in RLHF contexts.
- **Break condition:** If Sinkhorn iterations do not converge (e.g., insufficient iterations, numerical instability), dual variables become inaccurate and the penalty no longer correctly approximates Wasserstein distance.

### Mechanism 3: Lenient Penalization of Semantically Aligned Deviations
- **Claim:** WPR applies lower penalties to policy deviations that remain semantically coherent with the reference.
- **Mechanism:** Because Wasserstein penalty incorporates token-pair costs, policies that redistribute probability mass across semantically similar tokens (low cost) receive smaller penalties than those shifting to unrelated tokens (high cost). Figure 5 shows that Wasserstein penalties correlate strongly with KL (r=0.917) but are systematically lower (slope 0.579), indicating more lenient regularization. This allows greater exploration within semantic neighborhoods while still constraining large semantic drift. Table 8 confirms that WPR produces top-10 candidate sets with lower mean pairwise embedding distance (higher semantic coherence) than KL.
- **Core assumption:** Semantic coherence in next-token candidates correlates with better alignment outcomes.
- **Evidence anchors:**
  - [Section 5.3, Figure 5]: "Wasserstein penalty tends to be more lenient than KL" with slope < 1.
  - [Section 5.3, Table 7]: Wasserstein penalty shows stronger correlation with BERTScore (0.2160 vs. 0.1734 for TL;DR) than KL penalty.
  - [corpus]: Weak support; corpus papers address exploration-exploitation tradeoffs but not semantic coherence explicitly.
- **Break condition:** If lenient penalization allows excessive deviation from the reference policy (β too small), the model may underfit the preference signal or exhibit reward hacking.

## Foundational Learning

- **Concept: Optimal Transport and Wasserstein Distance**
  - **Why needed here:** WPR's core innovation is treating policy comparison as a transportation problem between probability distributions over a metric space (token embeddings). Understanding the primal-dual relationship explains why the penalty term emerges.
  - **Quick check question:** Given two distributions over tokens, why does Wasserstein distance penalize moving mass from "good" to "bad" differently than KL divergence?

- **Concept: Entropy-Regularized Optimal Transport (Sinkhorn Distance)**
  - **Why needed here:** Exact Wasserstein requires solving a linear program. The entropy regularization makes the problem strictly convex and yields closed-form Sinkhorn iterations, which is critical for tractable implementation at vocabulary scale.
  - **Quick check question:** What role does the hyperparameter λ play in controlling the smoothness of the transport plan?

- **Concept: RLHF Objective and KL Regularization**
  - **Why needed here:** WPR is designed as a drop-in replacement for KL regularization in the standard RLHF objective. Understanding how KL penalty is implemented as -β log(π_θ/π_ref) in PPO clarifies where the Wasserstein penalty substitutes.
  - **Quick check question:** Why does reverse KL regularization tend to be mode-seeking, and how might Wasserstein's geometry-aware approach affect diversity?

## Architecture Onboarding

- **Component map:**
  - Cost Matrix C (precomputed) -> Sinkhorn-Knopp Module -> Penalty Application -> PPO Update

- **Critical path:**
  1. Precompute sparse cost matrix C from frozen reference model token embeddings.
  2. During rollout, for each token step: extract top-k₂ logits from π_θ and π_ref → run Sinkhorn iterations (10-50) → extract ϕ* for sampled token.
  3. Compute GAE with penalized rewards and update policy via PPO clipping.

- **Design tradeoffs:**
  - **k₁ (nearest neighbors):** Lower k₁ → sparser matrix, faster multiplication, but less accurate transport. Default: 512.
  - **k₂ (distribution truncation):** Lower k₂ → faster Sinkhorn on smaller matrices, but discards probability mass. Default: 128.
  - **λ (entropy regularization):** Higher λ → closer to exact Wasserstein, but numerical instability. Lower λ → smoother transport but weaker semantic signal. Default: 100.
  - **Sinkhorn iterations:** Too few → unconverged dual variables, incorrect penalties. Too many → unnecessary overhead. Default: 10-50.

- **Failure signatures:**
  - **Penalty explosion:** If K = exp(-λC) collapses to zero (λ too large), Sinkhorn scaling diverges. Symptom: NaN penalties or training crash.
  - **No semantic benefit:** If cost matrix C doesn't encode useful semantic structure, WPR behaves like a noisy KL. Symptom: Win rates similar to KL baselines.
  - **Excessive deviation:** If β is too small relative to lenient penalties, model drifts from reference. Symptom: High reward but low coherence or safety failures.

- **First 3 experiments:**
  1. **Reproduce KL baseline:** Implement standard KL-regularized PPO on TL;DR or HH-RLHF with Gemma-2B. Verify win rates match reported RKL numbers (~0.85 vs SFT).
  2. **Ablate cost matrix semantics:** Compare WPR with (a) Euclidean embedding costs, (b) random costs, (c) identity costs. Expect (a) >> (b) ≈ (c) to validate semantic mechanism.
  3. **Sensitivity sweep on k₂ and λ:** Run WPR with k₂ ∈ {64, 128, 256} and λ ∈ {50, 100, 200}. Measure both win rates and per-step penalty computation time to identify Pareto-optimal settings for your hardware.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the dependence on the manual regularization hyperparameter $\beta$ be removed or automated to ensure robust performance without grid search?
- **Basis in paper:** [explicit] Section 5.3 states that while WPR is robust over a wider range of $\beta$ than f-divergences, "Developing approaches that reduce or remove this dependence is an important direction for future work."
- **Why unresolved:** The current framework relies on selecting a specific $\beta$ value to balance reward maximization and policy constraint, which the authors identify as a fundamental limitation of current RLHF methods.
- **What evidence would resolve it:** An adaptive mechanism (e.g., dual averaging) that dynamically adjusts the regularization strength to maintain a target Wasserstein distance, eliminating the need for external hyperparameter tuning.

### Open Question 2
- **Question:** Is it possible to apply Wasserstein Policy Regularization effectively when the reference policy and the policy being trained use different tokenizers?
- **Basis in paper:** [explicit] Appendix D.1 notes that using embeddings from a model with a different tokenizer "would require building a cross-token alignment, a promising but nontrivial direction for future work."
- **Why unresolved:** The method currently requires the cost matrix $C$ to be aligned with the policy's tokenizer, restricting the reference model to those sharing the same vocabulary.
- **What evidence would resolve it:** A framework that maps embeddings from a foreign tokenizer to the policy's space while preserving semantic distances, allowing WPR to successfully regularize a model using a reference from a different vocabulary family.

### Open Question 3
- **Question:** Would replacing the static, context-independent cost matrix with a dynamic, context-aware semantic distance improve alignment quality?
- **Basis in paper:** [inferred] The method defines the cost function using the "fixed token embedding space" (Section 5.1). This assumes the semantic distance between tokens is constant, ignoring that token semantics often shift based on the surrounding context (polysemy).
- **Why unresolved:** Static embeddings cannot capture context-dependent semantic shifts (e.g., "bank" as a river vs. financial institution), potentially leading to suboptimal penalties for contextually appropriate substitutions.
- **What evidence would resolve it:** An experiment comparing the static Euclidean cost against a contextualized cost (e.g., distance between hidden states of the current prompt), demonstrating a win rate improvement in tasks requiring nuanced semantic understanding.

## Limitations

- **Semantic Embedding Dependency:** WPR's effectiveness critically depends on token embeddings encoding meaningful semantic similarity, which may not hold across all domains or models.
- **Computational Overhead:** The Sinkhorn-Knopp iterations add O(k₂²) per-step computation, creating potential scalability issues for larger vocabularies or longer sequences.
- **Beta Sensitivity:** Despite broader robustness, WPR still requires careful tuning of β to prevent under-constraining the policy and enabling reward hacking.

## Confidence

- **High Confidence (Underlying Theory):** The mathematical framework connecting entropy-regularized Wasserstein distance to dual variables and policy penalties is rigorously derived and well-established in optimal transport literature.
- **Medium Confidence (Empirical Results):** The win rate improvements over KL and f-divergence baselines are statistically significant and consistent across tasks, though evaluation relies on proxy metrics rather than direct human judgment.
- **Low Confidence (Generalizability):** Experiments use only Gemma-2B on two specific tasks, lacking validation on larger models, different domains, or alternative reward modeling approaches.

## Next Checks

1. **Semantic Embedding Ablation:** Implement WPR with three cost matrices: (a) reference model embeddings, (b) randomly initialized embeddings, (c) identity matrix (all costs=1). Compare win rates and semantic coherence metrics to confirm that performance gains require meaningful embedding geometry.

2. **Beta Sensitivity Sweep:** Systematically vary β from 0.1× to 10× the KL-equivalent value. For each setting, measure: (a) win rates vs reference, (b) KL divergence from reference policy, (c) reward maximization effectiveness, (d) semantic coherence via human evaluation or embedding-based metrics. Identify the regime where lenient penalties enable beneficial exploration without policy collapse.

3. **Step-Level Penalty Analysis:** Instrument WPR to log per-step penalties and semantic transport costs during training. Analyze: (a) correlation between penalty magnitude and token position (early vs late sequence), (b) relationship between semantic transport cost and downstream quality, (c) whether semantically coherent substitutions consistently receive lower penalties across different token types (nouns, verbs, etc.).