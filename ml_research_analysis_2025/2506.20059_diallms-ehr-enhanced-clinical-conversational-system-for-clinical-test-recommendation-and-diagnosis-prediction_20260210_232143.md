---
ver: rpa2
title: 'DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation
  and Diagnosis Prediction'
arxiv_id: '2506.20059'
source_url: https://arxiv.org/abs/2506.20059
tags:
- clinical
- test
- data
- diagnosis
- diallm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiaLLM addresses limitations in current medical conversational
  models by integrating heterogeneous EHR data into clinically grounded dialogues,
  enabling test recommendation, result interpretation, and diagnosis prediction. The
  approach employs a Clinical Test Reference strategy to translate medical codes and
  interpret test results, then uses a reinforcement learning framework with rejection
  sampling and specialized rewards to optimize evidence acquisition and diagnosis
  accuracy.
---

# DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction

## Quick Facts
- arXiv ID: 2506.20059
- Source URL: https://arxiv.org/abs/2506.20059
- Authors: Weijieying Ren; Tianxiang Zhao; Lei Wang; Tianchun Wang; Vasant Honavar
- Reference count: 40
- Primary result: DiaLLM achieves 95.94% F1-score on NHANES single-turn diagnosis and 79.54% recall@5 on TriNetX multi-turn test recommendation, outperforming general and medical-specific LLMs

## Executive Summary
DiaLLM addresses limitations in current medical conversational models by integrating heterogeneous EHR data into clinically grounded dialogues, enabling test recommendation, result interpretation, and diagnosis prediction. The approach employs a Clinical Test Reference strategy to translate medical codes and interpret test results, then uses a reinforcement learning framework with rejection sampling and specialized rewards to optimize evidence acquisition and diagnosis accuracy. Experiments on NHANES and TriNetX datasets demonstrate DiaLLM outperforms both general and medical-specific LLMs across all metrics.

## Method Summary
DiaLLM uses a Clinical Test Reference (CTR) strategy to translate structured EHR codes into natural language descriptions and interpret lab test results as "normal" or "abnormal" based on patient demographics. The system formulates clinical diagnosis as a Markov Decision Process where the policy network selects clinical tests or stops to predict diagnosis. PPO training with rejection sampling reduces redundancy and improves exploration efficiency, while class-sensitive rewards address rare disease detection. The framework integrates Llama3.1-8B with LoRA adapters for parameter efficiency and achieves state-of-the-art performance on both single-turn (NHANES) and multi-turn (TriNetX) clinical tasks.

## Key Results
- Achieves 95.94% F1-score on NHANES single-turn diagnosis prediction
- Achieves 79.54% recall@5 on TriNetX multi-turn test recommendation
- Outperforms GPT-4, Llama3.1, and MedBERT on all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
Translating structured EHR codes into natural language improves LLM comprehension of clinical data. The Clinical Test Reference (CTR) strategy maps ICD-9/10 symptom/diagnosis codes and LOINC lab test identifiers to human-readable descriptions, then interprets numerical results as "normal" or "abnormal" conditioned on patient age and gender using manually annotated reference ranges. Core assumption: LLMs trained primarily on text struggle with raw medical codes and numerical lab values; grounding them in natural language descriptions enables better reasoning. Evidence anchors: abstract states CTR maps clinical codes to descriptions and classifies test results; section 3.2 details CTR's dual function of code translation and result interpretation. Break condition: If LLMs can directly process structured codes without translation (e.g., via specialized embeddings), CTR's marginal benefit diminishes.

### Mechanism 2
Rejection sampling reduces the effective action space for clinical test selection, improving RL exploration efficiency. Before executing a sampled test, the model computes an acceptance probability based on information gain (entropy reduction about the diagnosis) and redundancy filtering. Tests that don't sufficiently reduce uncertainty or are already ordered get rejected. Core assumption: Many potential clinical tests are redundant or low-information; pre-filtering them improves policy learning without sacrificing diagnostic accuracy. Evidence anchors: abstract mentions reject sampling to reduce redundancy and improve exploration; section 3.3 provides acceptance probability formula based on entropy reduction. Break condition: If rejection thresholds are too aggressive, clinically informative tests may be filtered; if too lenient, efficiency gains are lost.

### Mechanism 3
Class-sensitive rewards improve diagnosis prediction for rare conditions by counteracting long-tail disease distributions. The diagnosis reward weights each class by the inverse of its frequency (wCl(yi) = 1/p(yi)), assigning higher penalty for misclassifying rare diseases. A confirmation reward also provides intermediate feedback based on cross-entropy reduction between timesteps. Core assumption: Standard loss functions underrepresent rare diagnoses; explicit re-weighting improves minority class recall without harming majority class performance. Evidence anchors: abstract mentions class-sensitive diagnosis reward; section 3.3 defines class-sensitive weight as inverse frequency. Break condition: If re-weighting causes overfitting to rare classes or degrades calibration, overall clinical utility may decrease.

## Foundational Learning

- Concept: **Markov Decision Process (MDP) formulation**
  - Why needed here: DiaLLM models diagnosis as sequential decision-making (state = patient info + history, action = order test or stop, reward = diagnostic accuracy).
  - Quick check question: Can you explain why the stop action (a⊥) transitions to a terminal state and triggers diagnosis prediction rather than another test?

- Concept: **Proximal Policy Optimization (PPO)**
  - Why needed here: The paper uses PPO for fine-tuning the clinical test selection policy, requiring understanding of policy gradients, clipping, and reward shaping.
  - Quick check question: How does the confirmation reward RCo differ from the final class-sensitive reward RCl in terms of when they are computed?

- Concept: **ICD and LOINC coding systems**
  - Why needed here: EHR data uses ICD-9/10 for symptoms/diagnoses and LOINC for lab tests; CTR must map these to natural language.
  - Quick check question: Given LOINC code 2823-3 with value 5.0 mEq/L for a 25-year-old male, would CTR classify it as normal or abnormal based on Table 7?

## Architecture Onboarding

- Component map: EHR Transformation Pipeline -> Base LLM -> Policy Network πθ -> Diagnosis Classifier πϕ -> Rejection Sampling Module -> Reward Computation
- Critical path: Construct CTR database (manual annotation of 735 test descriptions + 1,163 interpretation rules) -> Transform EHR -> dialogue data using CTR -> Initialize policy from base LLM with LoRA (r=16) -> PPO training with rejection sampling and reward shaping -> Evaluate on single-turn (NHANES) and multi-turn (TriNetX) tasks
- Design tradeoffs: Manual CTR annotation vs. automated extraction (manual ensures accuracy but limits scalability); Rejection sampling threshold (higher threshold = more exploration but more computation); LoRA rank (r=16) (balances parameter efficiency vs. expressiveness)
- Failure signatures: Policy always selects stop action early (reward scaling or initialization issue); Diagnosis classifier overpredicts majority classes (class-sensitive reward not applied correctly); Rejection sampling rejects all tests (acceptance probability computation error); CTR produces nonsensical text (code-to-description mapping missing or incorrect)
- First 3 experiments: Validate CTR transformation (sample 20 EHR records, manually verify code translations and test interpretations match clinical guidelines); Ablate rejection sampling (train with and without rejection sampling on TriNetX-Metabolic subset, compare recall@5 and training time); Sanity check reward signals (log RCo and RCl during single episode, verify RCo is non-zero only mid-episode and RCl is computed only at termination)

## Open Questions the Paper Calls Out

### Open Question 1
How can the construction of the Clinical Test Reference (CTR) database be automated to enhance scalability and consistency while reducing the variability inherent in manual annotation? Basis in paper: [explicit] The authors state in the Limitations section that manual annotation of the CTR is "time-consuming and prone to variability," identifying automation as a necessary focus for future work. Why unresolved: The current implementation relies entirely on medical students to manually map LOINC codes and reference ranges, which is not scalable for broader medical domains. What evidence: A proposed automated pipeline capable of extracting and updating code descriptions and reference ranges from medical guidelines with accuracy comparable to human experts.

### Open Question 2
Can the DiaLLM framework be effectively extended to integrate multimodal data, specifically imaging-based diagnostic tests, to enrich clinical reasoning capabilities? Basis in paper: [explicit] The Limitations section notes that the current approach focuses on EHR data with numerical or textual features, leaving the integration of "imaging-based diagnostic tests, such as CT scans" for future exploration. Why unresolved: The current architecture and Clinical Test Reference strategy are designed for structured codes and lab values, lacking mechanisms to process or interpret visual medical data. What evidence: A modified DiaLLM architecture that successfully ingests and reasons with medical imaging data alongside text, demonstrating improved diagnostic performance in multimodal scenarios.

### Open Question 3
What specific verification mechanisms and oversight protocols are required to ensure the trustworthiness of DiaLLM in live clinical settings and mitigate risks like hallucinations? Basis in paper: [explicit] The Ethics Statement acknowledges that reliability is "not yet fully established" and lists potential risks including hallucinations and biases, calling for "rigorous verification mechanisms." Why unresolved: While the model shows strong predictive metrics, the paper admits that robustness and transparency validation through expert evaluation are still needed for safe deployment. What evidence: Development and testing of an interactive verification layer (e.g., citation retrieval or human-in-the-loop confirmation) that minimizes factual inconsistencies during clinical dialogue.

## Limitations
- Manual CTR annotation requirement (735 code descriptions + 1,163 reference ranges) limits scalability and introduces variability
- No systematic analysis of how performance scales with dataset size or number of tests
- Requires validation of reliability and trustworthiness for clinical deployment

## Confidence

**High Confidence** in the CTR mechanism's conceptual validity—translating clinical codes to natural language is a well-established need in medical NLP (supported by related work on medical code tokenization challenges). The rejection sampling approach is theoretically sound but lacks direct empirical validation in this specific clinical context.

**Medium Confidence** in the overall MDP formulation and PPO implementation, as the framework is standard but exact hyperparameters and reward scaling factors are unspecified. The reported performance gains over baselines appear substantial but depend on the specific implementation choices that aren't fully detailed.

**Low Confidence** in the scalability assessment—the paper demonstrates strong results on two datasets but doesn't provide systematic analysis of how performance scales with dataset size, number of tests, or computational requirements.

## Next Checks

1. **CTR Database Validation**: Construct a small-scale CTR database for 20 common tests and validate the normal/abnormal classification accuracy against clinical guidelines for 50 diverse patient cases spanning different ages and genders.

2. **Rejection Sampling Ablation**: Implement the full system with and without rejection sampling on a subset of TriNetX data (n=500). Measure the trade-off between test recommendation accuracy (recall@5) and computational efficiency (training time, average actions per episode).

3. **Class Imbalance Stress Test**: Create an artificially imbalanced version of NHANES by removing 90% of samples from minority diagnosis classes. Train with and without class-sensitive rewards and measure per-class F1 scores to verify the mechanism's effectiveness on rare conditions.