---
ver: rpa2
title: 'Transfer Learning in Physics-Informed Neural Networks: Full Fine-Tuning, Lightweight
  Fine-Tuning, and Low-Rank Adaptation'
arxiv_id: '2502.00782'
source_url: https://arxiv.org/abs/2502.00782
tags:
- learning
- transfer
- finetuning
- pinns
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates transfer learning in Physics-Informed
  Neural Networks (PINNs) to address their limitation of solving only specific problems
  requiring retraining when conditions change. The authors explore three transfer
  learning methods - full fine-tuning, lightweight fine-tuning, and Low-Rank Adaptation
  (LoRA) - across different boundary conditions, materials, and geometries using both
  strong and energy forms of PINNs.
---

# Transfer Learning in Physics-Informed Neural Networks: Full Fine-Tuning, Lightweight Fine-Tuning, and Low-Rank Adaptation

## Quick Facts
- **arXiv ID:** 2502.00782
- **Source URL:** https://arxiv.org/abs/2502.00782
- **Reference count:** 40
- **Key outcome:** LoRA significantly improves convergence speed and accuracy for transfer learning in PINNs across boundary conditions, materials, and geometries, with full fine-tuning also effective.

## Executive Summary
This paper addresses the limitation of Physics-Informed Neural Networks (PINNs) requiring retraining for each new problem by systematically investigating transfer learning methods. The authors explore three approaches—full fine-tuning, lightweight fine-tuning, and Low-Rank Adaptation (LoRA)—across different transfer scenarios including boundary conditions (Taylor-Green vortex), material distributions (functionally graded porous beams), and geometries (square plates with holes). The results demonstrate that full fine-tuning and LoRA significantly accelerate convergence while providing accuracy improvements, with LoRA being particularly effective for material and geometry generalization. Lightweight fine-tuning, while having minimal trainable parameters, performs poorly due to insufficient capacity.

## Method Summary
The study employs three transfer learning strategies applied to standard MLP architectures for PINNs. Full fine-tuning updates all network parameters during transfer, while lightweight fine-tuning updates only the final output layer. LoRA introduces low-rank matrices (A and B) that are added to the original weight matrices during the forward pass, with only these low-rank matrices being trainable during transfer. The methods are tested on both strong-form PINNs (for fluid dynamics) and energy-form PINNs (for solid mechanics), using problems with varying boundary conditions, material properties, and geometries. The effectiveness is measured through convergence speed and final accuracy (L2 and H1 errors) compared to training from scratch.

## Key Results
- LoRA with rank 4 consistently outperforms lightweight fine-tuning across all transfer scenarios
- Full fine-tuning and LoRA significantly accelerate convergence compared to training from scratch
- For material and geometry generalization, LoRA provides better accuracy than full fine-tuning
- Spectral bias explains why high-to-low frequency transfer is more effective than low-to-high frequency transfer

## Why This Works (Mechanism)
The effectiveness of transfer learning in PINNs stems from the ability of pre-trained models to provide a good initialization for similar problems, reducing the need to learn fundamental features from scratch. Full fine-tuning allows the entire network to adapt to new conditions, while LoRA provides a parameter-efficient alternative by constraining updates to low-rank matrices. This regularization effect can actually improve accuracy by preventing overfitting to the new domain. The spectral bias of neural networks explains why models pre-trained on high-frequency problems transfer better to low-frequency ones, as networks naturally learn low frequencies first. LoRA's success lies in its ability to capture the necessary domain adaptation with minimal parameters while preserving the valuable features learned during pre-training.

## Foundational Learning
- **Concept: Physics-Informed Neural Networks (PINNs) - Strong Form**
  - Why needed here: This is the core model being adapted. The strong form minimizes the residual of the PDEs themselves. Understanding this is necessary to interpret the loss function and how transfer learning affects convergence.
  - Quick check question: What term is minimized in the loss function of a strong-form PINN?

- **Concept: PINNs - Energy Form (Deep Energy Method - DEM)**
  - Why needed here: The paper uses this form for solid mechanics problems. It minimizes the total potential energy of the system and requires satisfying essential (Dirichlet) boundary conditions a priori, which changes the architecture and loss landscape compared to the strong form.
  - Quick check question: Why does the energy form require the construction of an "admissible function"?

- **Concept: Spectral Bias of Neural Networks**
  - Why needed here: The paper uses spectral bias to explain why transfer learning from high-frequency to low-frequency problems (w: 3π -> π) is more effective than the reverse. Networks learn low frequencies faster.
  - Quick check question: Why might a pre-trained model from a high-frequency problem transfer *better* to a low-frequency one than a low-frequency model to a high-frequency one?

## Architecture Onboarding
- **Component map:** Input coordinates → Hidden layers (4 layers, 100 neurons each with tanh activation) → Output layer → Loss function (PDE residual, boundary conditions)
- **Critical path:** The path from input coordinates to the PDE/energy loss term. For LoRA, the critical change is the W → W + AB substitution in the forward pass and the isolated ∂L/∂A, ∂L/∂B computation in the backward pass.
- **Design tradeoffs:**
  - Full Fine-Tuning vs. LoRA: Full fine-tuning offers maximal representational power but updates all parameters. LoRA reduces trainable parameters and can improve accuracy through regularization but introduces a hyperparameter (rank r) and slightly higher per-iteration computational cost due to matrix multiplications.
  - Efficiency vs. Accuracy: The paper notes that lightweight fine-tuning, while having few parameters, is inefficient due to backpropagation through the entire computation graph (including automatic differentiation) and performs poorly on accuracy.
- **Failure signatures:**
  - High L2 error with Lightweight Fine-Tuning: The model cannot adapt to the new problem because the frozen layers do not provide a suitable feature basis, and the single trainable layer is insufficient.
  - LoRA convergence stalling: The rank r is set too low, preventing the model from capturing the necessary adaptation.
  - Negative Transfer: Pre-training on a low-frequency problem (w=π) and fine-tuning on a high-frequency one (w=3π) may converge slower than training from scratch due to spectral bias.
- **First 3 experiments:**
  1. Baseline PINN: Train a PINN on the Taylor-Green vortex (w=π) from scratch. Record the convergence rate of the relative L2 error. This establishes a performance baseline.
  2. Full Fine-Tuning: Pre-train on w=π, then initialize and train a new model on w=2π using full fine-tuning. Compare convergence to the baseline.
  3. LoRA Implementation: Implement LoRA on the hidden layers of the same architecture. Pre-train a base model on w=π (without LoRA), then adapt it to w=2π by training only LoRA matrices (rank=4). Compare convergence and final accuracy to the first two experiments.

## Open Questions the Paper Calls Out
- **Open Question 1:** How can the rank in LoRA be automatically determined and adaptively adjusted for different hidden layers based on the similarity between source and target domains?
  - Basis in paper: [explicit] Section 5.1 and Section 6 state that while rank correlates with domain discrepancy, developing methods to automatically determine and adaptively adjust rank remains future work.
  - Why unresolved: The authors utilized fixed, predetermined ranks (mostly 4) for all layers and domains in their experiments, lacking an automated mechanism to map domain features to optimal rank values.
  - What evidence would resolve it: An algorithm or heuristic that calculates domain similarity (geometry, material, PDE type) to select optimal ranks, validated by showing improved convergence or accuracy over fixed-rank baselines.

- **Open Question 2:** Does "scenario fusion"—sequential training across multiple distinct source domains—yield higher accuracy in PINNs than training solely on the target domain?
  - Basis in paper: [explicit] Section 5.2 discusses "Feature Fusion," observing that pre-training on a source domain followed by training on a target domain sometimes outperforms training solely on the target domain.
  - Why unresolved: The paper identifies this as an interesting phenomenon but notes that identifying which scenario fusions enhance accuracy and determining the optimal number of scenarios to fuse remains unexplored.
  - What evidence would resolve it: Ablation studies comparing the accuracy of PINNs sequentially trained on various combinations of source domains against models trained only on the target domain for an identical total number of iterations.

- **Open Question 3:** Are full fine-tuning and LoRA effective for transfer learning in operator learning architectures (e.g., DeepONet, FNO) and Physics-Informed Neural Operators (PINO)?
  - Basis in paper: [explicit] Section 6 explicitly identifies extending these transfer learning methods to operator learning and PINO as a key future direction, as the current study is restricted to PINNs.
  - Why unresolved: The study limited its scope to the strong and energy forms of PINNs, leaving the broader "AI for PDEs" approaches, specifically operator learning, unexamined regarding transfer learning efficiency.
  - What evidence would resolve it: Benchmarks applying full fine-tuning and LoRA to operator learning frameworks, measuring convergence speed and error reduction when transferring knowledge across different boundary conditions or geometries.

## Limitations
- The study is limited to relatively simple, single-physics problems (fluid dynamics, beam bending, and plate bending) and may not generalize to complex multi-physics scenarios
- The effectiveness of transfer learning methods is only tested on modest changes in boundary conditions, material distributions, and geometries
- The paper relies on specific architectural choices (MLP with tanh activation, specific layer widths) without exploring how these choices impact transfer learning performance

## Confidence
- **High Confidence:** The observation that LoRA outperforms lightweight fine-tuning across all tested scenarios is well-supported by the numerical results. The superiority of full fine-tuning and LoRA over no transfer for accelerating convergence is also strongly evidenced.
- **Medium Confidence:** The claim that LoRA provides a "slight accuracy enhancement" over full fine-tuning is less conclusive, as the accuracy improvements are often marginal and problem-dependent.
- **Low Confidence:** The paper's explanation of spectral bias as the sole mechanism for the observed frequency-dependent transfer effectiveness is an assumption that requires further theoretical and empirical validation.

## Next Checks
1. **Multi-Physics Validation:** Apply the transfer learning methods to a coupled multi-physics problem (e.g., fluid-structure interaction) to test the generalizability of the findings beyond single-physics domains.
2. **Architectural Ablation:** Conduct experiments varying the MLP architecture (e.g., different activation functions, layer widths, or number of layers) to determine the impact of architectural choices on the success of transfer learning.
3. **Theoretical Grounding for Spectral Bias:** Develop or apply a theoretical framework to rigorously analyze the role of spectral bias in the observed frequency-dependent transfer behavior, moving beyond empirical observation.