---
ver: rpa2
title: Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates
arxiv_id: '2509.08933'
source_url: https://arxiv.org/abs/2509.08933
tags:
- robust
- reward
- bound
- where
- corruption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies robust Q-learning under adversarially corrupted
  rewards in an infinite-horizon discounted MDP. It introduces Robust Async-Q, a Q-learning
  variant that uses trimmed mean estimators and adaptive thresholding to handle asynchronous,
  single-trajectory data with heavy-tailed and corrupted rewards.
---

# Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates

## Quick Facts
- **arXiv ID:** 2509.08933
- **Source URL:** https://arxiv.org/abs/2509.08933
- **Reference count:** 40
- **Primary result:** Achieves $O(1/\sqrt{T} + \sqrt{\epsilon})$ convergence rates for asynchronous Q-learning under adversarial reward corruption.

## Executive Summary
This paper introduces Robust Async-Q, a Q-learning algorithm that achieves near-optimal convergence rates under adversarial reward corruption in infinite-horizon discounted MDPs. The key innovation is a two-layer defense combining trimmed mean estimators with adaptive thresholding to filter corrupted rewards while maintaining convergence guarantees. The algorithm operates on single-trajectory data without requiring full knowledge of reward statistics, extending to a reward-agnostic version that matches optimal rates up to constants.

## Method Summary
The algorithm maintains a history buffer for each state-action pair and applies trimmed mean estimation to filter extreme reward values. An adaptive threshold gate discards estimates falling outside a shrinking "typical region." The approach handles asynchronous sampling through Bernstein's inequality to ensure sufficient visitation counts after a burn-in period. For Markovian data, sub-sampling enforces mixing. The method achieves finite-time error bounds of order $O(1/\sqrt{T} + \sqrt{\epsilon})$ under i.i.d. sampling and extends to Markovian settings with additional mixing-time penalties.

## Key Results
- Achieves near-optimal $O(1/\sqrt{T} + \sqrt{\epsilon})$ convergence rate for asynchronous Q-learning with corrupted rewards
- Introduces information-theoretic lower bound showing $\sqrt{\epsilon}$ dependence is unavoidable
- Extends results to Markovian sampling using coupling arguments with mixing-time penalties
- Develops refined Azuma-Hoeffding inequalities for martingales with coarse deterministic and fine probabilistic bounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The algorithm mitigates adversarial bias by combining robust mean estimation with dynamic rejection.
- **Mechanism:** Uses a Trimmed Mean Estimator on reward buffers with an Adaptive Threshold $G_t$. If the robust estimate falls outside this shrinking typical region, it is discarded. This handles both statistical noise and arbitrary corruption.
- **Core assumption:** Corruption fraction $\epsilon < 1/2$ and aperiodic, irreducible Markov chain.
- **Break condition:** Fails when $\epsilon \ge 0.5$ as uncorrupted samples are no longer the majority.

### Mechanism 2
- **Claim:** Near-optimal rates are maintained without prior reward statistics knowledge.
- **Mechanism:** Employs Refined Azuma-Hoeffding Inequality that exploits coarse deterministic bounds ($O(T^p)$) and fine probabilistic bounds ($O(1)$) to tightly bound noise despite loose worst-case constraints.
- **Core assumption:** Known corruption fraction $\epsilon$ and minimum visitation probability $\lambda_{\min}$.
- **Break condition:** Fails if failure probability $\delta_1$ is not sufficiently small relative to time horizon $T$.

### Mechanism 3
- **Claim:** Handles asynchronous data through enforced burn-in periods.
- **Mechanism:** Uses Bernstein's Inequality to create "good events" where, after burn-in time $\bar{T}$, every state-action pair has been visited sufficiently. Robust estimates only become reliable after this concentration is established.
- **Core assumption:** Strictly positive minimum visitation probability $\lambda_{\min} > 0$.
- **Break condition:** Fails when behavior policy results in almost never visited states ($\lambda_{\min} \approx 0$).

## Foundational Learning

- **Concept: Bellman Optimality Operator ($\mathcal{T}$)**
  - **Why needed here:** Error analysis relies on $\mathcal{T}$'s contraction property in $\ell_\infty$-norm to prove iterates $Q_t$ converge to $Q^*$.
  - **Quick check question:** Does the contraction property hold for $\ell_2$ norm in Q-learning? (Hint: No, contraction is proven in $\ell_\infty$).

- **Concept: Huber Contamination Model**
  - **Why needed here:** Defines the threat model where with probability $\epsilon$, rewards are drawn from arbitrary distribution $Q$ rather than true distribution $R$.
  - **Quick check question:** In Huber model, does adversary decide when to attack or is timing random? (Hint: Timing is random).

- **Concept: Martingale Difference Sequences (MDS)**
  - **Why needed here:** Error decomposes into contraction term and noise term, where noise forms MDS allowing Azuma-Hoeffding concentration bounds.
  - **Quick check question:** Does adversarial term form standard MDS? (Hint: No, introduces bias analyzed separately).

## Architecture Onboarding

- **Component map:** Observe $(s,a) \rightarrow$ Update Buffer $\rightarrow$ **TRIM (Sorting)** $\rightarrow$ **Threshold Check** $\rightarrow$ Q-Update.
- **Critical path:** Data buffer $\rightarrow$ TRIM module (computes quantiles) $\rightarrow$ Thresholding gate $\rightarrow$ Q-updater. TRIM sorting ($O(N \log N)$) dominates compute vs vanilla Q-learning ($O(|A|)$).
- **Design tradeoffs:**
  - Robustness vs Compute: High robustness requires large buffers and sorting, increasing memory/compute
  - Agnosticism vs Rate: Reward-agnostic version removes hyperparameter dependence but introduces slight rate inflation
  - Markovian vs I.I.D.: Markovian data via sub-sampling reduces sample efficiency by factor $\tau$ (mixing time)
- **Failure signatures:**
  - Divergence: Under-estimated corruption $\epsilon$ leads to overly tight thresholds rejecting valid rewards
  - Slow convergence: Large mixing time $\tau$ starves learner of data points
  - Memory overflow: Buffer $D_t$ grows indefinitely ($\Theta(t)$ memory)
- **First 3 experiments:**
  1. Run Vanilla Q-learning with bias attack varying $\epsilon$ to confirm vulnerability
  2. Compare Robust Async-Q vs Robust Async-RAQ on GridWorld to verify agnostic convergence
  3. Implement Markovian sub-sampling variant and plot error vs samples processed vs wall-clock steps

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can robust Q-learning algorithms be developed for fully adaptive strong contamination model where adversary controls timing and targets of corruption?
- **Basis in paper:** Section 7 discusses moving from Huber to fully adaptive strong contamination attack model.
- **Why unresolved:** Current analysis relies on Bernoulli corruption timing being independent of adversary strategy; adaptive timing could allow concentrated attacks on infrequently visited state-action pairs.
- **What evidence would resolve it:** Formal lower bound under strong contamination model or algorithm with provable guarantees achieving such bound.

### Open Question 2
- **Question:** Is the dependence on minimum visitation probability $\lambda_{\min}$ in upper bounds unavoidable?
- **Basis in paper:** Section 7 notes gap between lower bound (no $\lambda_{\min}$) and upper bound (includes $\lambda_{\min}$).
- **Why unresolved:** Lower bound doesn't include $\lambda_{\min}$, creating gap; conjecture that dependence is unavoidable remains unverified.
- **What evidence would resolve it:** Refined lower bound incorporating $\lambda_{\min}$ or improved upper bounds eliminating this dependence.

### Open Question 3
- **Question:** Can robust Q-learning be extended to handle corruption of state transitions in addition to rewards?
- **Basis in paper:** Section 7 mentions investigating scenario where state transitions are also corrupted.
- **Why unresolved:** Conjecture that combining robust reward and transition estimation could extend framework, but formal verification needed.
- **What evidence would resolve it:** Algorithm with finite-time guarantees under joint reward and transition corruption.

### Open Question 4
- **Question:** Can recursive online robust estimators reduce per-step computational and memory overhead?
- **Basis in paper:** Section 7 notes per-step complexity is higher than standard Q-learning and suggests recursive estimators.
- **Why unresolved:** Current algorithm requires $O(N_t(s,a) \log N_t(s,a))$ per-step cost and growing memory vs $O(1)$ for sample means.
- **What evidence would resolve it:** Recursive robust estimator with bounded update time/memory and matching accuracy analysis.

## Limitations

- Requires burn-in period $\bar{T}$ before robust estimates become reliable
- Assumes bounded rewards and known corruption fraction $\epsilon < 0.5$
- Buffer $D_t$ grows indefinitely creating potential memory issues
- Convergence rate degrades in Markovian setting by factor proportional to mixing time
- Requires strictly positive minimum visitation probability $\lambda_{\min}$

## Confidence

- **High Confidence:** Finite-time error bounds under i.i.d. sampling and information-theoretic lower bound showing $\sqrt{\epsilon}$ dependence is unavoidable
- **Medium Confidence:** Markovian extension via coupling arguments (relies on mixing time and sub-sampling assumptions)
- **Medium Confidence:** Refined Azuma-Hoeffding inequality application (requires careful calibration of failure probabilities and bound parameters)

## Next Checks

1. **Threshold Sensitivity:** Test performance when corruption fraction $\epsilon$ is overestimated vs underestimated to identify optimal threshold calibration
2. **Buffer Size Scaling:** Experimentally determine minimal buffer size needed for reliable robust estimation as function of $\epsilon$
3. **Memory-Efficient Alternatives:** Implement sliding window buffer variant to bound memory usage and evaluate tradeoff with estimation accuracy