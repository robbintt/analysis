---
ver: rpa2
title: Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression
arxiv_id: '2507.20613'
source_url: https://arxiv.org/abs/2507.20613
tags:
- sparsity
- pruning
- performance
- quantization
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of compressing large multimodal
  models (LMMs) for efficient deployment on edge devices, where high computational
  and memory requirements are problematic. The core method involves an adaptive search
  algorithm that combines pruning and KV cache compression, utilizing the Tree-structured
  Parzen Estimator (TPE) to dynamically adjust sparsity ratios and quantization bandwidths
  across different model layers.
---

# Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression

## Quick Facts
- **arXiv ID:** 2507.20613
- **Source URL:** https://arxiv.org/abs/2507.20613
- **Authors:** Te Zhang; Yuheng Li; Junxiang Wang; Lujun Li
- **Reference count:** 36
- **Primary result:** Adaptive search framework combining pruning and KV cache compression achieves 50% sparsity with minimal performance loss on LLaVA-1.5 models.

## Executive Summary
This paper presents a post-training compression framework for Large Multimodal Models (LMMs) that uses an adaptive search algorithm to optimize layer-wise sparsity ratios and KV cache quantization bandwidths. The method employs Tree-structured Parzen Estimator (TPE) to dynamically allocate compression resources, achieving superior performance compared to state-of-the-art techniques like SparseGPT and Wanda. Experiments on LLaVA-1.5 7B and 13B models demonstrate competitive accuracy on multimodal benchmarks while significantly reducing computational and memory requirements for edge deployment.

## Method Summary
The framework combines unstructured pruning with KV cache quantization through an adaptive search algorithm. TPE searches for optimal layer-wise sparsity profiles (45-55% per layer) and quantization bandwidth allocations (6-bit vs 8-bit) by minimizing perplexity on Wikitext2. A novel pruning metric using logarithmic scaling and L2 normalization identifies important weights without requiring fine-tuning. The approach integrates pruning masks and quantization profiles to compress both model weights and KV cache, enabling efficient deployment on resource-constrained devices.

## Key Results
- Achieves 50% overall sparsity while maintaining competitive accuracy on VQAv2, SQA, and TextVQA benchmarks
- Outperforms SparseGPT and Wanda baselines across various compression levels
- Adaptive KV cache quantization achieves 50% memory reduction with minimal performance degradation
- Layer-wise sparsity allocation preserves critical weights in sensitive layers while aggressively pruning less critical ones

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Layer-wise Sparsity Allocation
The TPE algorithm dynamically searches for optimal per-layer sparsity ratios (45-55%) that minimize perplexity, outperforming uniform sparsity approaches. The method assumes heterogeneous layer sensitivity to pruning and learns an optimal "sparsity profile" through iterative evaluation. This profile is then applied to prune the model without retraining.

### Mechanism 2: Robust Pruning Metric with Logarithmic Scaling and Normalization
A novel pruning metric combines row/column L2 normalization with logarithmic scaling of weight magnitude, multiplied by input activation norm. This approach normalizes weight contributions within local matrix structures, reduces outlier influence through log functions, and captures dynamic importance based on activation magnitude.

### Mechanism 3: Adaptive Layer-wise KV Cache Quantization Bandwidth Allocation
The framework allocates different quantization bit-widths (6-bit vs 8-bit) to different layers' KV caches based on learned sensitivity. This approach allows significant memory reduction without the catastrophic performance drop seen in uniform low-bit quantization, with TPE finding optimal bandwidth profiles under global constraints.

## Foundational Learning

**Concept: Tree-structured Parzen Estimator (TPE)**
*Why needed:* Core optimization engine for finding complex, high-dimensional compression profiles that would be infeasible via grid search. *Quick check:* Can you explain why TPE models p(x|y) instead of p(y|x), and why this makes it more sample-efficient?

**Concept: Post-Training Pruning (e.g., SparseGPT, Wanda)**
*Why needed:* Understanding baseline methods is crucial to see the novelty in this paper's metric. *Quick check:* What is the primary advantage of post-training pruning over traditional pruning that requires retraining?

**Concept: KV Cache in Autoregressive Models**
*Why needed:* Understanding KV cache (stores keys/values for past tokens to avoid re-computation) and why memory grows linearly with sequence length is essential to appreciate compression motivation. *Quick check:* What is stored in the KV cache for each layer and token, and why does compressing from 16-bit to 6-bit reduce GPU memory?

## Architecture Onboarding

**Component map:**
TPE Search Algorithm -> Pruning Module -> Quantization Module -> Evaluation Loop -> TPE Search Algorithm

**Critical path:** The correctness of the TPE search depends entirely on the Evaluation Loop. If perplexity metric is flawed or doesn't generalize to downstream tasks, the entire adaptive mechanism is compromised.

**Design tradeoffs:**
- Search Time vs. Profile Quality: More TPE trials yield better profiles but increase upfront computational cost
- Metric Complexity vs. Speed: Novel pruning metric is more robust but involves more computation than simple magnitude pruning
- Compression Aggressiveness vs. Accuracy: Higher sparsity will degrade accuracy; framework finds best allocation for given constraint

**Failure signatures:**
- Search Collapse: TPE returns uniform-like profiles, indicating layer sensitivity differences are negligible
- Catastrophic Quantization Failure: Downstream task accuracy drops to near-zero, suggesting 6-bit quantization is too aggressive
- Metric Instability: Pruning leads to NaN or Inf values, indicating failure in metric's normalization or logarithmic scaling

**First 3 experiments:**
1. Reproduce baseline comparison: Apply uniform 50% sparsity using Wanda and the paper's method on LLaVA-1.5 7B, measure PPL and VQAv2 accuracy
2. Validate TPE search: Run TPE for 10-15 trials on Wikitext2, plot sparsity ratios per layer to verify U-shaped pattern and check PPL improvement
3. Validate KV cache quantization: Apply optimal bandwidth profile to pruned model, measure memory usage and TextVQA performance against paper's results

## Open Questions the Paper Calls Out
- Can the adaptive search framework be extended to optimize adaptive quantization and mixed-precision techniques without requiring fine-tuning?
- Does removing the constraint of equal split between 8-bit and 6-bit layers allow for significantly higher compression rates in the KV cache?
- Do the observed optimal allocation patterns transfer effectively to Large Multimodal Models processing video or audio data?

## Limitations
- The novel pruning metric lacks direct ablation studies comparing it against standard metrics
- The adaptive search algorithm's generalizability is limited without reporting sparsity ratio distributions across different compression targets
- KV cache quantization is evaluated only in conjunction with pruning, not in isolation

## Confidence
- **High Confidence:** Basic framework of using TPE for adaptive search and superior performance of combined approach on standard benchmarks
- **Medium Confidence:** Claims about novel pruning metric being "more robust" and specific layer-wise allocation patterns
- **Low Confidence:** Claims of "minimal performance loss" at 50% sparsity, particularly for KV cache quantization component

## Next Checks
1. Isolate metric impact: Compare uniform 50% sparsity using paper's metric vs Wanda's metric on LLaVA-1.5 7B, measure PPL and VQAv2 accuracy
2. Probe search convergence: Visualize TPE search process by plotting perplexity of each trial's profile and final layer-wise sparsity ratios
3. Stress test quantization: Apply KV cache quantization in isolation (without pruning) to dense LLaVA-1.5 7B, compare memory savings and TextVQA performance using adaptive vs uniform allocation