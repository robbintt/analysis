---
ver: rpa2
title: Which Heads Matter for Reasoning? RL-Guided KV Cache Compression
arxiv_id: '2510.08525'
source_url: https://arxiv.org/abs/2510.08525
tags:
- cache
- reasoning
- heads
- compression
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of efficient KV cache compression
  for reasoning large language models, which are particularly sensitive to information
  loss during decoding due to their complex chain-of-thought generation. Existing
  compression methods, like token-dropping and head-reallocation, either disrupt reasoning
  chains or fail to preserve heads critical for generative reasoning.
---

# Which Heads Matter for Reasoning? RL-Guided KV Cache Compression

## Quick Facts
- arXiv ID: 2510.08525
- Source URL: https://arxiv.org/abs/2510.08525
- Reference count: 40
- Primary result: 20–50% KV cache reduction with near-lossless reasoning performance

## Executive Summary
This paper introduces RLKV, a novel approach to KV cache compression for reasoning large language models that uses reinforcement learning to identify and preserve attention heads critical for reasoning. Unlike previous methods that either drop tokens indiscriminately or fail to distinguish between reasoning-critical and non-critical heads, RLKV treats attention head selection as a sequential decision problem where the reward is actual generation performance. The method achieves 20-50% cache reduction with minimal performance degradation while providing up to 1.21× speedup across reasoning and knowledge tasks.

## Method Summary
RLKV employs reinforcement learning to probe which attention heads are essential for reasoning by directly optimizing cache usage against generation outcomes. The approach treats head selection as a sequential decision problem where an agent learns to allocate full KV cache to reasoning-critical heads while aggressively compressing others. This differs from token-dropping methods that can break reasoning chains and from head-reallocation approaches that don't account for generative reasoning requirements. The RL probe effectively learns which heads can be compressed without disrupting the model's ability to generate coherent chain-of-thought reasoning.

## Key Results
- Achieves 20–50% reduction in KV cache size across multiple benchmarks
- Maintains near-lossless performance on reasoning and knowledge tasks
- Delivers up to 1.21× speedup in inference time
- Outperforms baseline compression methods on reasoning-specific evaluations

## Why This Works (Mechanism)
RLKV works by using reinforcement learning as a probe to identify which attention heads are truly essential for reasoning. The RL agent learns to make sequential decisions about head compression based on actual generation outcomes rather than heuristic rules. This approach is particularly effective for reasoning models because it preserves the heads that maintain reasoning chains while compressing those that are less critical. The method recognizes that reasoning tasks require maintaining coherent thought processes across multiple tokens, making selective preservation of reasoning-critical heads more effective than uniform compression strategies.

## Foundational Learning

**KV Cache Compression** - Why needed: Large language models store key-value pairs during autoregressive generation, creating memory bottlenecks. Quick check: Can be measured by cache size reduction percentage.

**Attention Heads in Transformers** - Why needed: Different heads capture different linguistic patterns and reasoning steps. Quick check: Can be verified by analyzing attention patterns across heads.

**Reinforcement Learning for Probe Design** - Why needed: Traditional heuristics fail to capture reasoning-specific importance. Quick check: Can be validated by comparing RL-selected heads vs. random selection.

**Chain-of-Thought Reasoning** - Why needed: Reasoning models generate intermediate steps that must be preserved. Quick check: Can be measured by coherence scores of generated reasoning chains.

## Architecture Onboarding

**Component Map**: Input tokens -> Attention heads -> RL probe -> Head selection -> KV cache compression -> Output generation

**Critical Path**: Token input → Multi-head attention computation → RL-guided head selection → Compressed KV cache storage → Next token prediction

**Design Tradeoffs**: The method balances aggressive compression (for efficiency) against preserving reasoning quality (for performance), requiring careful RL reward shaping to avoid over-compression.

**Failure Signatures**: Performance degradation when critical reasoning heads are compressed, broken reasoning chains, or increased perplexity on complex reasoning tasks.

**First Experiments**:
1. Ablation study comparing RLKV against token-dropping and head-reallocation baselines
2. Analysis of head importance rankings across different reasoning task types
3. Stress test with maximum compression levels to identify breaking points

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided text.

## Limitations

- Reliance on RL training may not generalize well across diverse reasoning tasks or model architectures
- Potential overfitting to specific RL training setup without addressing robustness to distribution shifts
- Aggressive compression of non-critical heads could impact long-context reasoning scenarios

## Confidence

- **Primary claim (cache reduction with performance preservation)**: High confidence given empirical results showing 20-50% reduction with minimal loss
- **Generalizability across model families**: Medium confidence due to limited experiments on specific benchmarks and model sizes
- **Speedup claims**: Medium confidence as dependent on implementation details and hardware configurations

## Next Checks

1. Test RLKV's head importance rankings on out-of-distribution reasoning tasks to assess robustness
2. Evaluate the approach on multi-modal or domain-specific reasoning models to verify cross-architecture applicability
3. Conduct ablation studies on the RL reward design to determine sensitivity to hyperparameter choices and potential biases in head selection