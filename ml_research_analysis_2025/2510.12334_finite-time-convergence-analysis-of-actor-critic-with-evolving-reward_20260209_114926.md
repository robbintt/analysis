---
ver: rpa2
title: Finite-time Convergence Analysis of Actor-Critic with Evolving Reward
arxiv_id: '2510.12334'
source_url: https://arxiv.org/abs/2510.12334
tags:
- reward
- learning
- policy
- conference
- critic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides the first finite-time convergence analysis\
  \ of a single-timescale actor-critic algorithm with evolving reward functions under\
  \ Markovian sampling. The authors establish that the algorithm achieves an O(1/\u221A\
  T) convergence rate to a neighborhood of a stationary point, matching the best-known\
  \ rate for static rewards, provided the reward parameters evolve slowly enough."
---

# Finite-time Convergence Analysis of Actor-Critic with Evolving Reward

## Quick Facts
- arXiv ID: 2510.12334
- Source URL: https://arxiv.org/abs/2510.12334
- Reference count: 40
- This paper provides the first finite-time convergence analysis of a single-timescale actor-critic algorithm with evolving reward functions under Markovian sampling.

## Executive Summary
This paper establishes the first finite-time convergence analysis for single-timescale actor-critic algorithms when the reward function evolves over time. The authors prove an O(1/√T) convergence rate to a neighborhood of a stationary point, matching the best-known rate for static rewards. Crucially, this rate is preserved when rewards evolve via bounded-gradient rules at the same timescale as policy updates. The analysis also improves upon prior work by eliminating a log²T factor in the static-reward case through a novel approach to handling distribution mismatch under Markovian sampling.

## Method Summary
The paper analyzes a single-timescale actor-critic algorithm where the actor (policy) and critic (value function) are updated simultaneously with step sizes proportional to 1/√t. The algorithm operates under Markovian sampling rather than i.i.d. data, using an ergodic sampling kernel to ensure sufficient exploration. The critic uses linear function approximation with projection to maintain bounded parameters, while the actor updates via policy gradient. The reward parameters can evolve according to a bounded-gradient rule, allowing for applications like adaptive regularization or curriculum learning. The theoretical analysis establishes convergence by bounding both actor error (gradient norm) and critic error through a coupled system of inequalities.

## Key Results
- Achieves O(1/√T) convergence rate to stationary point neighborhood under Markovian sampling
- Eliminates log²T factor present in prior single-timescale analyses for static rewards
- Preserves O(1/√T) rate when reward parameters evolve via bounded-gradient rules
- Requires reward evolution rate O(1/T) and appropriate step-size ratio between actor and critic

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The O(1/√T) rate is preserved when reward parameters evolve via bounded-gradient rules at the same timescale as policy updates.
- **Mechanism**: The analysis bounds drift error from evolving rewards by establishing Lipschitz continuity of the policy objective and optimal critic parameters with respect to the reward parameter. This allows reward non-stationarity errors to be dominated by standard stochastic noise.
- **Core assumption**: Regularized reward is Lipschitz continuous with respect to φ, and reward update gradient is bounded.
- **Evidence anchors**: Abstract mentions "bounded gradient" preservation of O(1/√T) rate; Theorem 4.6 shows F_K = O(1/T) leads to G_T = O(1/√T).
- **Break condition**: Reward gradient becomes unbounded or evolution rate exceeds O(1/T), causing drift terms to dominate.

### Mechanism 2
- **Claim**: Novel analysis of distribution mismatch under Markovian sampling eliminates the log²T factor present in prior work.
- **Mechanism**: Instead of relying on mixing time of the underlying Markov chain, the authors use the contraction property of the sampling kernel and Lipschitz continuity of the stationary distribution. Proposition 4.8 bounds the mismatch directly by policy update magnitude.
- **Core assumption**: Policy is Lipschitz continuous with respect to parameters, and transition kernel is ergodic via sampling kernel construction.
- **Evidence anchors**: Abstract notes "improving the best-known rate by a factor of log²T"; Section 4.3 Step 1 explains how Proposition 4.8 avoids mixing-time penalty.
- **Break condition**: Excessive policy updates violate the small-step assumption, degrading the distribution mismatch bound.

### Mechanism 3
- **Claim**: Actor and critic can be updated on a single timescale while maintaining convergence if step-size ratio is sufficiently small.
- **Mechanism**: The proof formulates convergence as coupled inequalities involving actor and critic errors. By choosing c_θ/c_ω small enough to satisfy contraction conditions, the system can be solved to show simultaneous convergence.
- **Core assumption**: Sufficient exploration ensures matrix A_θ is negative definite, allowing critic error to contract at controlled rate.
- **Evidence anchors**: Theorem 4.6 specifies "ratio c_θ/c_ω is chosen to be sufficiently small"; Section 4.3 Step 3 describes decoupling the two errors.
- **Break condition**: If c_θ ≫ c_ω, policy moves too fast for critic to track, violating decoupling condition.

## Foundational Learning

- **Concept**: Temporal Difference (TD) Learning & Projection
  - **Why needed here**: The critic update relies on TD(0) error with a projection step to keep parameters bounded. Understanding this projection is vital for implementing the algorithm correctly.
  - **Quick check question**: Can you explain why the projection Proj_{C_ω} is mathematically necessary for the proof, even if it might seem restrictive in practice?

- **Concept**: Soft (Entropy-Regularized) Policy Gradient
  - **Why needed here**: The paper explicitly models evolving rewards often seen as regularization terms. The mechanism relies on the specific form of the soft Bellman equation.
  - **Quick check question**: How does the entropy term αH(π) modify the reward function r̃, and why does this allow us to treat adaptive entropy adjustment as an "evolving reward"?

- **Concept**: Markovian vs. I.I.D. Sampling
  - **Why needed here**: The paper explicitly differentiates itself by handling correlated samples drawn from the trajectory rather than a replay buffer.
  - **Quick check question**: In the Markovian sampling scheme, why is the sampling kernel defined as P̃(·|s,a) = γP(·|s,a) + (1-γ)ρ(·), and what role does the discount factor γ play in ensuring ergodicity?

## Architecture Onboarding

- **Component map**: Environment -> Sampling Kernel -> Actor Network -> Critic Network -> Reward Engine -> Environment
- **Critical path**:
  1. Sample: Draw s_t, a_t, s'_t from the Markovian process using ergodic sampling kernel
  2. Compute TD Error: δ̂_t = r̃_{φ_t,θ_t}(s_t,a_t) + (γϕ(s'_t) - ϕ(s_t))⊤ω_t
  3. Update Critic: ω_{t+1} = Proj_{C_ω}(ω_t + η_ω^t δ̂_t ϕ(s_t))
  4. Update Actor: θ_{t+1} = θ_t + η_θ^t δ̂_t ∇_θ log π_{θ_t}(a_t|s_t)
  5. Update Reward: φ_{t+1} = φ_t + η_φ^t h_φ(t) (Apply gradient clipping to ensure boundedness)

- **Design tradeoffs**:
  - Step-size Ratio: Tuning c_θ/c_ω is critical. A ratio too high breaks convergence; too low might slow down learning.
  - Projection Radius C_ω: Must be large enough to contain optimal critic parameters but small enough to be practical.
  - Feature Representation: The analysis assumes a linear critic. Using non-linear critics requires relaxing this assumption or using specific feature representations.

- **Failure signatures**:
  - Exploding Gradients in Reward: If reward module's update h_φ(t) has unbounded variance or magnitude, the O(1/T) condition on F_T fails, causing divergence.
  - Critic Lag: If critic step size is too small relative to actor, the error term I_2 (critic error) dominates the actor update, leading to instability.
  - Poor Exploration: If A_θ becomes singular, the critic fails to learn, stalling the actor.

- **First 3 experiments**:
  1. Static Reward Baseline: Implement the algorithm on a simple MDP with static reward. Verify that the log²T factor is indeed absent compared to baseline implementations.
  2. Controlled Reward Drift: Introduce a reward parameter φ_t that oscillates slowly. Vary the oscillation frequency to find the empirical threshold where convergence breaks.
  3. Adaptive Entropy Tuning: Implement "soft actor-critic with automated entropy adjustment" use case. Check if the algorithm remains stable when α is updated on-policy alongside θ and ω.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the finite-time convergence analysis be extended to nonlinear function approximation, specifically neural network critics?
  - **Basis in paper**: The conclusion states "Extending the analysis to nonlinear function approximation, particularly with neural networks, is a critical next step."
  - **Why unresolved**: Current theoretical framework relies on Assumption 4.1 (Sufficient Exploration) and linear feature mappings, which don't generally hold for neural networks.
  - **What evidence would resolve it**: A convergence proof for actor-critic algorithms with evolving rewards that accommodates over-parameterized neural networks or specific non-linear architectures.

- **Open Question 2**: Does the convergence guarantee hold when the environment's transition probabilities or initial distribution evolve alongside the reward function?
  - **Basis in paper**: The conclusion notes the analysis lays a foundation for "reinforcement learning with dynamic objectives due to evolving reward, shifting initial distribution or transition probabilities."
  - **Why unresolved**: Current analysis strictly bounds "reward variation" but assumes the MDP transition kernel and initial state distribution remain static.
  - **What evidence would resolve it**: A modified theoretical analysis that bounds the distribution mismatch and convergence error when the transition kernel P or initial distribution ρ is non-stationary.

- **Open Question 3**: Can the algorithm be proven to converge to a global optimum rather than just a stationary point?
  - **Basis in paper**: The main result guarantees convergence to a neighborhood of a stationary point rather than the global optimum.
  - **Why unresolved**: The optimization landscape of actor-critic methods is generally non-convex, and evolving rewards complicate the stability required for global convergence analysis.
  - **What evidence would resolve it**: A theoretical result demonstrating convergence to the global optimal value, potentially under specific conditions like exact function approximation or specific entropy regularization.

## Limitations

- The paper is purely theoretical with no empirical validation or experimental results to support the convergence claims.
- The analysis relies on specific assumptions (Lipschitz continuity of policies, bounded gradients, small step-sizes) that may not hold in practical implementations with neural networks.
- The requirement for linear function approximation in the critic is particularly restrictive compared to modern deep RL practice.

## Confidence

- **High Confidence**: The O(1/√T) convergence rate for static rewards under Markovian sampling is well-supported by the novel distribution mismatch analysis and Proposition 4.8.
- **Medium Confidence**: The extension to evolving rewards with bounded-gradient updates is theoretically sound but depends heavily on the tightness of the Lipschitz continuity assumption.
- **Medium Confidence**: The single-timescale convergence analysis is rigorous but requires careful tuning of the step-size ratio c_θ/c_ω to satisfy the contraction condition.

## Next Checks

1. **Distribution Mismatch Verification**: Implement the algorithm on a simple MDP with Markovian sampling and empirically measure the distribution mismatch ||ν̂_t - ν^π_θ_t||_1 over time to validate Proposition 4.8's bound.

2. **Reward Evolution Stability**: Create a controlled experiment where the reward parameters φ_t evolve at varying rates (satisfying vs violating the O(1/T) condition) to observe the transition point where the O(1/√T) convergence breaks down.

3. **Step-Size Ratio Sensitivity**: Conduct a systematic sweep over the ratio c_θ/c_ω on a benchmark task to empirically determine the critical threshold where convergence fails, validating the theoretical contraction condition.