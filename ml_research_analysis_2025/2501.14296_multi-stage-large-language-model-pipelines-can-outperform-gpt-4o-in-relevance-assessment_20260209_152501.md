---
ver: rpa2
title: Multi-stage Large Language Model Pipelines Can Outperform GPT-4o in Relevance
  Assessment
arxiv_id: '2501.14296'
source_url: https://arxiv.org/abs/2501.14296
tags:
- gpt-4o
- relevance
- passage
- mini
- binary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a modular LLM-based pipeline for relevance
  assessment that divides the task into multiple stages, each using different prompts
  and models of varying sizes and capabilities. Applied to TREC Deep Learning data,
  one approach achieved 18.4% higher Krippendorff's alpha accuracy than GPT-4o mini
  while costing only 0.2 USD per million input tokens.
---

# Multi-stage Large Language Model Pipelines Can Outperform GPT-4o in Relevance Assessment

## Quick Facts
- arXiv ID: 2501.14296
- Source URL: https://arxiv.org/abs/2501.14296
- Authors: Julian A. Schnabel; Johanne R. Trippas; Falk Scholer; Danula Hettiachchi
- Reference count: 20
- One-line primary result: Multi-stage LLM pipelines can match or exceed GPT-4o accuracy at significantly lower cost

## Executive Summary
This paper demonstrates that decomposing relevance assessment into multiple stages with different model sizes can outperform single-model approaches like GPT-4o. By splitting the 4-level relevance classification task into binary relevance followed by granular scoring, the pipeline achieved 18.4% higher Krippendorff's alpha accuracy than GPT-4o mini while costing only 0.2 USD per million input tokens. The approach shows that smaller, specialized models in a staged pipeline can match or exceed the performance of much larger models at a fraction of the cost.

## Method Summary
The method uses a two-stage pipeline for relevance assessment: Stage 1 performs binary classification (relevant vs. irrelevant) using smaller models like GPT-4o mini, while Stage 2 applies granular classification (levels 1-3) only to documents passing the relevance filter, using larger models like GPT-4o. The approach leverages the observation that binary classification is easier and can be handled by cheaper models, while the nuanced ordinal distinctions benefit from larger model capacity. The pipeline uses prompts adapted from UMBRELA and is evaluated on TREC-DL 23 dataset using Krippendorff's alpha as the primary metric.

## Key Results
- 18.4% higher Krippendorff's alpha accuracy than GPT-4o mini while costing only 0.2 USD per million input tokens
- GPT-4o accuracy improved by 9.7% in alpha when using the multi-stage pipeline
- Every tested model/prompt combination outperformed the baseline single-model results on Krippendorff's alpha
- GPT-4o mini → GPT-4o configuration achieved near-optimal cost/accuracy tradeoff

## Why This Works (Mechanism)

### Mechanism 1: Task Decomposition via Binary-to-Granular Staging
Splitting 4-level relevance classification into sequential decisions reduces cognitive load on models. The binary stage simplifies to "is this useful?" before introducing nuance, yielding more consistent ordinal judgments by reducing per-stage complexity.

### Mechanism 2: Asymmetric Model Assignment by Task Difficulty
Smaller models handle simpler binary relevance decisions while larger models tackle harder granular distinctions. Binary classification is easier (high class separability, ~75% negatives in TREC-DL), so cheaper models suffice, while fine-grained discrimination benefits from larger model capacity.

### Mechanism 3: Sequential Override with Escalating Model Capacity
The second-stage model corrects first-stage false positives by seeing only candidates flagged as relevant, reducing class imbalance. A more capable model can downgrade over-optimistic classifications from stage 1.

## Foundational Learning

- **TREC Relevance Grading Scale (0-3)**: The pipeline explicitly decomposes this ordinal scale; understanding what each level means is essential for prompt design and error analysis.
  - Quick check: Can you explain why level 1 ("related but not answering") is harder to distinguish from level 0 than level 3 is from level 2?

- **Krippendorff's α vs. Cohen's κ**: The paper reports both; α handles ordinal weighting (penalizing 0→3 more than 0→1), which is critical for interpreting the 18.4% improvement claim.
  - Quick check: Why would α increase while κ stays flat if the model reduces "far" misclassifications but not "near" ones?

- **Cost per Million Input Tokens**: Pipeline cost depends on stage 1's false-positive rate; understanding token economics is required to reproduce cost claims.
  - Quick check: Given stage 1 rejects 75% of documents at binary, why is the pipeline cost not simply 25% of the single-stage cost?

## Architecture Onboarding

- **Component map**: (query, passage) → Stage 1 (Binary Classifier) → {relevant:→ Stage 2 (Granular Classifier), irrelevant:→ output} → final relevance score

- **Critical path**:
  1. Prompt design for binary stage (simpler is better; Figure 8 shows minimal instructions)
  2. Prompt design for granular stage (must preserve ordinal semantics; Figure 7 adapts UMBRELA)
  3. Model selection per stage (cost/accuracy tradeoff; Table 2 shows mini→4o is optimal on α/cost)
  4. Aggregation logic (how to handle edge cases, e.g., stage 2 timeout)

- **Design tradeoffs**:
  - Homogeneous (mini→mini) vs. Heterogeneous (mini→4o): Homogeneous is cheaper ($0.21 vs $2.05) but slightly lower α (0.425 vs 0.432)
  - Binary prompt type: "Binary Relevant" vs "Binary Normal" affects κ but not α
  - Latency vs. throughput: Two-stage adds round-trip latency; negligible for batch annotation

- **Failure signatures**:
  - Stage 1 over-rejection: α drops sharply; check binary prompt for overly conservative wording
  - Stage 2 over-promotion: 4-scale κ improves but α degrades; granular prompt may conflate levels 2 and 3
  - Cost overrun: If stage 1 false-positive rate spikes, stage 2 volume explodes

- **First 3 experiments**:
  1. Reproduce baseline: Run GPT-4o mini with UMBRELA prompt on TREC-DL 23 subset (100 query-document pairs). Target: α ≈ 0.35–0.40
  2. Ablate staging: Compare single-stage 4-scale vs. two-stage (binary→granular) using same model (GPT-4o mini). Hypothesis: α should increase by ~0.05–0.07
  3. Cost/accuracy sweep: Test mini→mini, mini→4o, and 4o→4o configurations. Plot α vs. cost; validate mini→4o is near Pareto frontier

## Open Questions the Paper Calls Out

### Open Question 1
Can a multi-model pipeline effectively mitigate relevance misclassification caused by adversarial prompt manipulation or keyword injection? The paper hypothesizes that a larger second assessor model could override smaller model judgments to prevent adversarial attacks, but this remains untested.

### Open Question 2
Does the performance of the multi-stage pipeline generalize to datasets with different label distributions or from different years? The study relies exclusively on TREC-DL 23, and tests on TREC-DL from different years and broader task ranges are needed to enhance generalizability.

### Open Question 3
To what extent does the presence of near-duplicate documents in TREC-DL 23 affect the reported accuracy of the pipeline? Near duplicates were not filtered out due to complexity, potentially affecting consistency metrics by masking true performance on unique query-document pairs.

### Open Question 4
Can prompt optimization techniques like Chain-of-Thought (CoT) significantly increase the irrelevance-detection rate in the initial binary stage? The current binary prompts are standard zero-shot instructions; the impact of reasoning-based prompts on cost/accuracy tradeoff is unknown.

## Limitations
- Relies on Microsoft Azure's "standard parameters" for GPT-4o and GPT-4o mini, which are not specified
- Assumes binary relevance and granular scoring are independent cognitive tasks without validation
- Does not test pipeline on adversarially constructed query-document pairs to evaluate robustness

## Confidence
- **High Confidence**: Multi-stage pipelines improve Krippendorff's α compared to single-stage classification
- **Medium Confidence**: GPT-4o mini → GPT-4o configuration achieves Pareto-optimal cost/accuracy
- **Low Confidence**: The binary relevance task is uniformly "easier" for smaller models

## Next Checks
1. **Parameter Sensitivity Analysis**: Test the pipeline with varying temperature (0.0-1.0) and max_tokens to quantify how API settings affect α and cost
2. **Adversarial Stress Test**: Construct a small set of adversarial query-document pairs and measure if stage 1 false-positive rates spike
3. **Human Baseline Comparison**: Recruit 2-3 annotators to label a subset of TREC-DL 23 pairs using the same binary-then-granular protocol, then compare inter-annotator α to the model's α