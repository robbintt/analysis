---
ver: rpa2
title: 'Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning'
arxiv_id: '2410.03348'
source_url: https://arxiv.org/abs/2410.03348
tags:
- neurosymbolic
- symbols
- dolphin
- learning
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DOLPHIN is a framework designed to address scalability challenges\
  \ in neurosymbolic learning, where symbolic reasoning must be integrated with deep\
  \ learning. The core innovation lies in separating symbolic (L) and probabilistic\
  \ (P) computations\u2014executing L on CPU with Python objects while vectorizing\
  \ P on GPU via PyTorch tensors."
---

# Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning

## Quick Facts
- **arXiv ID**: 2410.03348
- **Source URL**: https://arxiv.org/abs/2410.03348
- **Reference count**: 40
- **Primary result**: 1.71x to 62x faster training than baselines while matching or exceeding SOTA accuracy on 13 benchmarks

## Executive Summary
DOLPHIN is a framework designed to address scalability challenges in neurosymbolic learning, where symbolic reasoning must be integrated with deep learning. The core innovation lies in separating symbolic (L) and probabilistic (P) computations—executing L on CPU with Python objects while vectorizing P on GPU via PyTorch tensors. This enables end-to-end differentiability and efficient batch processing without combinatorial memory blowups. Experiments across 13 benchmarks demonstrate DOLPHIN's superiority, achieving 1.71x to 62x faster training than baselines like Scallop, ISED, and IndeCateR+, while matching or exceeding state-of-the-art accuracy.

## Method Summary
DOLPHIN implements a neurosymbolic framework that separates symbolic reasoning from probabilistic gradient computation. The architecture uses a `Distribution` abstraction that maps symbols to batched tensors of probabilities. Symbolic functions execute on the CPU using standard Python objects, while probabilistic computations run in parallel on the GPU using PyTorch tensors. The framework implements vectorized provenance semirings (DAMP and DTKP-AM) that approximate gradient computation without explicit memory allocation for all proof paths. Users define symbolic logic through `apply`, `filter`, and `union` primitives, with the framework handling the CPU/GPU coordination and gradient propagation automatically.

## Key Results
- Achieves 1.71x to 62x faster training compared to baselines Scallop, ISED, and IndeCateR+
- Matches or exceeds state-of-the-art accuracy across 13 benchmarks including MNIST SumN, HWF, PathFinder, CLUTRR, and Mugen
- Converges to SOTA accuracy within 5.5 hours on complex tasks where other frameworks fail to converge within 10 hours
- Demonstrates superior scalability through CPU/GPU split that avoids combinatorial memory blowups

## Why This Works (Mechanism)

### Mechanism 1
DOLPHIN resolves the latency-memory trade-off by decoupling symbolic manipulation from probabilistic gradient computation. The framework separates execution environments: symbolic reasoning ($L$) executes sequentially on the CPU using standard Python objects, while probabilistic computations ($P$) execute in parallel on the GPU using PyTorch tensors. This avoids the memory blowup associated with grounding all symbols on the GPU (as in LTN) and the latency of CPU-bound probabilistic inference (as in Scallop).

### Mechanism 2
Scalability is maintained through vectorized provenance semirings that approximate gradient computation without explicit memory allocation for all proof paths. DOLPHIN implements provenance semirings (specifically DTKP-AM) as PyTorch tensor operations. Instead of calculating exact Weighted Model Counting (WMC), it tracks the top-$k$ proofs and uses an "add-mult" approximation to aggregate probabilities, allowing gradients to be computed via standard backpropagation on the GPU.

### Mechanism 3
The `Distribution` abstraction unifies neural outputs and symbolic states, enabling batched processing of combinatorial logic. A `Distribution` object maps a collection of discrete symbols to a batched tensor of "tags" (likelihoods). When `apply` or `filter` primitives are called, the user-defined symbolic function runs once (or efficiently on CPU), while the tensor operations vectorize across the entire batch. This aggregates the combinatorial explosion of states into fixed-size tensor manipulations.

## Foundational Learning

### Provenance Semirings
- **Why needed here**: DOLPHIN relies on algebraic structures (Semirings) to define how probabilities propagate through logical operations (conjunction/disjunction). You must understand how $\otimes$ and $\oplus$ operations differ between DAMP (add-mult) and DTKP (top-k proofs).
- **Quick check question**: If you have two symbols $A$ and $B$ with probabilities $P(A)$ and $P(B)$, how would DAMP compute the probability of $(A \land B)$ compared to how a standard probability distribution would?

### PyTorch Autograd & Batching
- **Why needed here**: The framework explicitly offloads gradient computation to PyTorch. Understanding how gradient graphs are constructed and how batch dimensions are preserved through tensor operations is essential for debugging the "P" (probabilistic) side of the system.
- **Quick check question**: If a `Distribution` holds tags for a batch of 64 samples, what happens to the gradient graph if you apply a filter that removes all symbols for sample 32?

### Combinatorial Grounding in NeSy
- **Why needed here**: To appreciate DOLPHIN's scalability, you need to understand the "grounding bottleneck"—how converting logical rules to tensor representations usually causes exponential memory growth (the problem DOLPHIN solves using the CPU/GPU split).
- **Quick check question**: Why does fully grounding a logic program on the GPU (like Logic Tensor Networks) cause OOM errors for tasks like MNIST Sum-10?

## Architecture Onboarding

### Component map
User code -> Distribution objects -> CPU (symbolic $L$ functions) + GPU (probabilistic $P$ computations) -> Provenance Engine (DAMP/DTKP-AM) -> Loss function

### Critical path
1. Define the domain of symbols (e.g., integers 0-9)
2. Initialize `Distribution` with neural network logits and the symbol domain
3. Chain `apply`/`filter` primitives to define the logic
4. Select the correct provenance (DAMP for independence, DTKP-AM for dependency-heavy tasks)

### Design tradeoffs
- **DAMP vs. DTKP-AM**: DAMP is faster and simpler (assumes independence) but performs poorly on tasks requiring semantic dependency tracking (e.g., HWF). DTKP-AM is slower and more complex but handles dependencies better.
- **CPU/GPU Split**: Maximizes memory efficiency but introduces a synchronization overhead if the symbolic program is trivially small.

### Failure signatures
- **OOM on GPU**: The symbol set size ($N$) is too large, creating a tag tensor that exceeds VRAM
- **Slow Convergence**: You used DAMP provenance for a task requiring complex proof tracking (switch to DTKP-AM)
- **CPU Bottleneck**: The user-defined symbolic function ($L$) is excessively complex or contains nested loops, slowing down the CPU-side execution

### First 3 experiments
1. **MNIST Sum-2**: Implement the tutorial task to verify the `apply` loop and gradient flow. Compare training speed against a pure PyTorch baseline.
2. **Provenance Swap**: Run the Hand-Written Formula (HWF) task using DAMP, then switch to DTKP-AM. Observe the accuracy difference to understand the impact of dependency tracking.
3. **Scaling Test**: Run MNIST Sum-N for $N=5, 10, 15$. Monitor CPU vs. GPU memory usage to confirm the "memory blowup" is avoided compared to baseline LTNs.

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes top-k proof approximation (DTKP-AM) is sufficient for all dependency-heavy tasks without theoretical guarantees or sensitivity analysis
- CPU-GPU split introduces potential bottlenecks for tasks with complex symbolic loops, though not explicitly tested
- No ablation studies quantifying the impact of varying top-k values or comparing against exact WMC for small domains

## Confidence

### Major Uncertainties
The paper's primary limitation is the assumption that top-k proof approximation is sufficient for all dependency-heavy tasks. While experiments show strong performance, no ablation studies quantify the impact of varying k or compare against exact WMC for small domains.

### Confidence Labels
- **High Confidence**: DOLPHIN's architecture design (CPU/GPU split) and empirical speedups (1.71x-62x faster than baselines) are well-supported by experiments and code structure
- **Medium Confidence**: The claim that DTKP-AM provides "sufficient" semantic information through top-k proofs is supported by accuracy results but lacks theoretical guarantees or sensitivity analysis
- **Low Confidence**: The assertion that DAMP outperforms DTKP-AM on SumN/CLUTRR due to "independence assumptions" is presented without ablation studies isolating the provenance impact

## Next Checks

1. **Provenance Sensitivity**: Run CLUTRR experiments with varying top-k values (k=1, 3, 5) to quantify the trade-off between accuracy and speed.

2. **CPU Bottleneck Test**: Implement a symbolic function with nested loops on the CPU and measure whether GPU acceleration is negated by data transfer overhead.

3. **Memory Scaling**: Gradually increase the symbol set size in a SumN variant until memory errors occur, documenting the exact threshold where DOLPHIN's scalability breaks down.