---
ver: rpa2
title: 'Extending Graph Condensation to Multi-Label Datasets: A Benchmark Study'
arxiv_id: '2412.17961'
source_url: https://arxiv.org/abs/2412.17961
tags:
- graph
- uni00000013
- multi-label
- condensation
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends graph condensation to multi-label datasets, addressing
  the challenge of condensing large multi-label graphs into smaller synthetic graphs
  while preserving label correlations and structural information. The authors adapt
  three existing condensation methods (GCond, SGDD, GCDM) by introducing multi-label
  synthetic graph initialization (via subgraph sampling and probabilistic label sampling)
  and optimizing with binary cross-entropy loss (BCELoss) instead of single-label
  losses.
---

# Extending Graph Condensation to Multi-Label Datasets: A Benchmark Study

## Quick Facts
- arXiv ID: 2412.17961
- Source URL: https://arxiv.org/abs/2412.17961
- Reference count: 40
- Primary result: GCond with K-Center initialization and BCELoss achieves best multi-label condensation performance

## Executive Summary
This paper extends graph condensation to multi-label datasets by adapting three existing condensation methods (GCond, SGDD, GCDM) to handle multiple simultaneous labels per node. The authors introduce multi-label synthetic graph initialization through subgraph sampling and probabilistic label sampling, replacing single-label losses with binary cross-entropy loss (BCELoss). Experiments on eight real-world multi-label datasets demonstrate that the adapted GCond method with K-Center initialization and BCELoss achieves the best overall performance, particularly on complex, high-dimensional datasets like DBLP and PPI.

## Method Summary
The method adapts single-label graph condensation techniques to multi-label settings by modifying the synthetic graph initialization and optimization procedures. Instead of initializing synthetic nodes with single labels, the approach uses subgraph sampling combined with probabilistic label sampling to create representative multi-label nodes. The optimization objective shifts from single-label losses to BCELoss, which treats each label as an independent binary classification task. The GCond framework is particularly effective, using gradient matching between the original and synthetic graphs while learning both node features and synthetic graph structure through a neural network.

## Key Results
- GCond with K-Center initialization and BCELoss achieves best overall performance on multi-label datasets
- Structure learning (A' optimization) improves performance by 10-15% F1-micro on dense datasets
- BCELoss outperforms SoftMarginLoss by 5-10% on average for multi-label condensation
- SGDD faces out-of-memory errors on large datasets (Yelp, OGBN-Proteins), revealing scalability limitations

## Why This Works (Mechanism)

### Mechanism 1
K-Center initialization outperforms random, herding, and probabilistic sampling for multi-label graph condensation on dense datasets by maximizing feature-space coverage. This preserves structural diversity in the condensed graph, which is critical when label correlations are complex. The method assumes that multi-label nodes with similar features also share similar label sets, so covering the feature space equitably preserves label diversity. K-Center consistently provides the highest F1-micro scores for dense, large-scale datasets such as PPI, Yelp, and OGBN-Proteins.

### Mechanism 2
Binary Cross-Entropy Loss (BCELoss) is more effective than SoftMarginLoss for multi-label graph condensation because it independently evaluates each label as a binary classification task using sigmoid activation. This allows the model to learn label-specific patterns without enforcing margin constraints, which is beneficial when labels co-occur with varying frequencies. BCELoss generally yields higher F1-micro scores, especially when combined with structure learning, achieving substantially better performance than SoftMarginLoss on datasets like DBLP.

### Mechanism 3
Learning synthetic graph structure (A') from condensed node features improves downstream GNN performance compared to structure-free condensation. The synthetic adjacency matrix A' is parameterized as a function of node features (A' = g_φ(X')) and optimized jointly with features via gradient matching. This allows the condensed graph to encode relational information critical for GNN message passing. With the additional A' learning, the condensed graph performs better on large-scale datasets, achieving significantly higher F1-micro scores compared to node-only condensation.

## Foundational Learning

- **Multi-label node classification on graphs**: Why needed - The entire method redesign is motivated by the inability of single-label graph condensation to handle nodes with multiple simultaneous labels. Quick check - Given a node with labels [1, 0, 1, 0] across 4 classes, how would you compute BCELoss for this node's predictions?
- **Graph condensation objective (gradient matching)**: Why needed - GCond, the best-performing method, minimizes the distance between gradients computed on the synthetic graph and the original graph. Quick check - Why is matching gradients considered a stronger signal than matching final predictions for dataset condensation?
- **Coreset selection strategies (K-Center, Herding)**: Why needed - Initialization determines the starting point for synthetic graph optimization. Quick check - In K-Center, if you have nodes at positions [0,0], [10,0], [5,5], and [5,10], which node would be selected first as the initial center?

## Architecture Onboarding

- Component map: Original Graph G → K-Center Sampling → Initialize (X', Y') → Structure Generator g_φ(X') → A' → GNN Training Loop (T steps) → Compute Gradients ∇θL(G) and ∇θL(S) → Gradient Matching Loss D(·,·) → Update X', φ via gradient descent → Condensed Graph S = {A', X', Y'}

- Critical path: 
  1. Initialization failure: If K-Center selects unrepresentative nodes, BCELoss cannot recover label diversity
  2. Gradient mismatch: If the matching distance D is too loose or too strict, the synthetic graph overfits or underfits
  3. Structure collapse: If the sparsity regularization (β||A||²) is too aggressive, A' becomes too sparse for meaningful message passing

- Design tradeoffs:
  - Condensation rate vs. performance: At 0.5% on PPI, GCond achieves 49.63% F1-micro (vs. 51.26% full dataset); at 2.0%, it reaches 51.21%
  - With vs. without structure learning: Adds ~10-15% F1-micro on dense datasets but requires extra memory for A' optimization
  - BCELoss vs. SoftMarginLoss: BCELoss is 5-10% better on average but requires careful class weighting for imbalanced datasets

- Failure signatures:
  - OOM on large graphs: SGDD fails on Yelp and OGBN-Proteins due to memory scaling with graphon generation
  - Flat F1-macro despite high F1-micro: Indicates the model is predicting only frequent labels
  - Performance collapse at very low condensation rates (<0.1%): Synthetic graph too small to represent multi-label correlations

- First 3 experiments:
  1. Reproduce Table 4 baseline: Run GCond with K-Center initialization, BCELoss, and structure learning on PPI at 1.0% condensation
  2. Ablate initialization: Compare K-Center vs. Random vs. Herding on DBLP at 0.8% condensation
  3. Test scalability limit: Run SGDD and GCond on OGBN-Proteins at 0.1%, measuring memory usage and F1-micro

## Open Questions the Paper Calls Out

- How can structure-broadcasting methods like SGDD be optimized to prevent out-of-memory errors on large-scale multi-label graphs?
- Can novel condensation techniques designed specifically for multi-label data outperform the current adaptations of single-label methods?
- How can condensation techniques be improved to maintain performance on datasets with unique or sparse label structures?

## Limitations
- OOM errors limit scalability of structure-broadcasting methods (SGDD) on large datasets
- Performance significantly diminishes on datasets with unique label structures (Yelp)
- Current methods become more random as label complexity increases, suggesting need for native multi-label techniques

## Confidence
- High confidence: Feasibility of multi-label condensation and BCELoss superiority
- Medium confidence: K-Center initialization effectiveness (context-dependent)
- Medium confidence: Structure learning benefits (tradeoff with computational overhead)

## Next Checks
1. Architecture sensitivity test: Reproduce results using GCN, GAT, and GraphSAGE backbones
2. Initialization robustness: Systematically compare K-Center against