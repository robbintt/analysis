---
ver: rpa2
title: Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic
arxiv_id: '2511.20677'
source_url: https://arxiv.org/abs/2511.20677
tags:
- prompt
- question
- corrector
- turbo
- arabic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Ar-SParC, the first Arabic cross-domain,
  context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of
  interrelated questions, totaling 10,225 questions with corresponding SQL queries
  across 160 databases on 116 different domains.
---

# Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic

## Quick Facts
- arXiv ID: 2511.20677
- Source URL: https://arxiv.org/abs/2511.20677
- Reference count: 40
- Primary result: GAT Corrector improved Arabic text-to-SQL accuracy by 1.9% EX and 1.9% IX under zero-shot settings

## Executive Summary
This paper introduces Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset with 10,225 questions across 160 databases. The authors evaluate 10 prompt engineering techniques using GPT-3.5-turbo and GPT-4.5-turbo, achieving significant improvements through a novel GAT Corrector approach that directly outputs corrected SQL queries. The GAT Corrector demonstrated superior performance over GAT Verifier in Arabic, achieving 97/100 accuracy versus 67/100 on cross-lingual sentence matching tasks. Results show GPT-4.5-turbo provides approximately 7% better execution accuracy than GPT-3.5-turbo for Arabic text-to-SQL tasks.

## Method Summary
The authors conducted 40 experiments using Ar-SParC dataset with 3,450 question sequences. They tested two LLMs (GPT-3.5-turbo and GPT-4.5-turbo) with 10 different prompt engineering techniques combining four question representation formats and six in-context learning methods. The GAT Corrector was developed by fine-tuning GPT-3.5-turbo on 500 samples (250 correct and 250 incorrect SQL queries). The fine-tuning used a single-shot learning approach with system, user, and assistant roles. Evaluation metrics included Execution Accuracy (EX) for individual query correctness and Interaction Accuracy (IX) for full sequence correctness.

## Key Results
- GAT Corrector achieved 1.9% improvement in EX and IX under zero-shot settings
- GPT-4.5-turbo outperformed GPT-3.5-turbo by approximately 7.38% EX and 6.25% IX on average
- GAT Corrector showed 97/100 accuracy versus GAT Verifier's 67/100 on cross-lingual sentence matching
- OpenAI demonstration prompt (ODp) produced best performance across zero-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified detection-correction outperforms sequential verify-then-fix for Arabic text-to-SQL
- Mechanism: GAT Corrector fine-tuned to output correct SQL directly, providing richer supervision than binary classification used by GAT Verifier
- Core assumption: Training objective drives performance gap rather than model capacity
- Evidence: GAT Corrector achieved 97/100 accuracy vs GAT Verifier's 67/100 on cross-lingual sentence matching
- Break condition: Fine-tuning dataset distributional biases may prevent transfer to production workloads

### Mechanism 2
- Claim: Constraining output format via prompt structure improves SQL generation accuracy
- Mechanism: ODp format uses # prefixes and explicit instruction to "Complete sqlite SQL query only and with no explanation"
- Core assumption: Performance gain comes from output constraint rather than formatting differences
- Evidence: ODp produced best performance across zero-shot settings in execution and interaction accuracy
- Break condition: Output constraint becomes limitation when downstream tasks require explanations

### Mechanism 3
- Claim: GPT-4.5-turbo has significantly better Arabic comprehension than GPT-3.5-turbo
- Mechanism: GPT-4.5-turbo outperformed GPT-3.5-turbo by approximately 7.38% EX and 6.25% IX across all experiments
- Core assumption: Model version drives performance difference rather than prompt engineering or variation
- Evidence: Consistent performance gap across all experiments using same question representation techniques
- Break condition: Cost constraints requiring GPT-3.5-turbo create performance ceiling regardless of optimization

## Foundational Learning

- Concept: **Context-dependent text-to-SQL**
  - Why needed: Unlike single-turn SQL generation, context-dependent tasks require maintaining conversation history where later questions depend on earlier answers
  - Quick check: Can you explain why Interaction Accuracy (IX) measures sequence correctness while Execution Accuracy (EX) measures individual query correctness?

- Concept: **In-context learning (ICL) sample selection**
  - Why needed: Selection strategy significantly impacts performance when choosing training examples to include in prompts
  - Quick check: Why might Query Similarity Selection (QRSs) outperform Question Similarity Selection (QTSs) for SQL generation?

- Concept: **Fine-tuning vs. prompting for error correction**
  - Why needed: GAT Corrector required creating 500 labeled samples for fine-tuning GPT-3.5-turbo
  - Quick check: What are the trade-offs between fine-tuning a corrector model versus using a larger model with more sophisticated prompts?

## Architecture Onboarding

- Component map: Schema + Question → Prompt formatting → ICL sample selection → Primary LLM → GAT Corrector → Executable SQL

- Critical path: Input layer (database schema + Arabic question + conversation history) → Question representation → Sample selector → Primary LLM → GAT Corrector → Evaluation

- Design tradeoffs:
  - ODp format maximizes accuracy but eliminates explainability
  - GAT Reviser achieves highest ICL performance but requires more tokens
  - GAT Corrector adds latency but saves cost by avoiding regeneration calls
  - GPT-4.5-turbo costs more but provides ~7% EX improvement for Arabic

- Failure signatures:
  - If IX significantly lower than EX, context propagation is failing
  - If GAT Corrector returns unchanged but still-incorrect SQL, fine-tuning data may not cover error pattern
  - If Arabic questions produce schema-mismatch errors, verify tokenizer handling

- First 3 experiments:
  1. Replicate zero-shot ODp baseline with GPT-3.5-turbo on 100-sample subset; verify EX ~58%
  2. Apply GAT Corrector to same subset; confirm 1.5-2% EX improvement
  3. Test GAT Verifier vs GAT Corrector on 50 Arabic-English sentence pairs; expect verifier to show more classification errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance on the translated Ar-SParC dataset generalize to interactions involving native Arabic dialects?
- Basis: The dataset was constructed by translating English SParC questions rather than collecting original Arabic queries
- Why unresolved: Translation likely retains source English structure, potentially failing to capture native Arabic speech patterns
- Evidence: Testing models on a new dataset comprising original, non-translated Arabic user queries

### Open Question 2
- Question: How does increasing the number of in-context learning examples beyond three shots affect the GAT Corrector's performance?
- Basis: Authors explicitly limited to three shots to accommodate 4096-token limit for fair comparison
- Why unresolved: Unknown if performance ceiling is intrinsic to models or results from arbitrary constraint on examples
- Evidence: Conducting experiments using models with larger context windows with 5, 10, or 20-shot prompts

### Open Question 3
- Question: Is the GAT Corrector's superiority over GAT Verifier consistent across other low-resource or morphologically rich languages?
- Basis: GAT Verifier struggled with Arabic logic (33/100 errors) compared to GAT Corrector (3/100 errors)
- Why unresolved: Paper demonstrates this failure specifically for Arabic but leaves open whether it's Arabic-specific or broader limitation
- Evidence: Applying same comparative ablation methodology to datasets in other languages like Russian or Vietnamese

## Limitations
- Performance evaluation limited to single dataset (Ar-SParC) and two GPT models
- Fine-tuning process for GAT Corrector lacks specification of critical hyperparameters
- GAT Corrector adds latency to pipeline and ODp format eliminates explainability

## Confidence
- **High confidence**: GPT-4.5-turbo demonstrates consistent superiority over GPT-3.5-turbo for Arabic text-to-SQL (approximately 7% EX improvement)
- **Medium confidence**: GAT Corrector provides measurable accuracy improvements over GAT Verifier (1.9% EX/IX gain)
- **Low confidence**: Fine-tuning methodology and optimal ICL sample selection are underspecified

## Next Checks
1. Download Ar-SParC from GitHub repository and reproduce zero-shot ODp baseline with GPT-3.5-turbo on 100-sample subset to verify ~58% execution accuracy

2. Fine-tune GPT-3.5-turbo on 500 samples using specified format and evaluate whether GAT Corrector achieves 97/100 accuracy versus GAT Verifier's 67/100 on cross-lingual sentence matching

3. Test full question sequences where Q1 contains error that should propagate to Q2 and Q3; measure whether IX significantly lags EX and whether GAT Corrector successfully corrects sequence-level errors