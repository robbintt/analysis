---
ver: rpa2
title: 'Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive
  Reasoning with Clinical Objective Relative Policy Optimization'
arxiv_id: '2512.00601'
source_url: https://arxiv.org/abs/2512.00601
tags:
- reasoning
- crpo
- arxiv
- medical
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Clinical-R1 addresses the need for trustworthy medical reasoning
  in large language models by introducing Clinical-objective Relative Policy Optimization
  (CRPO), which extends Grouped Relative Policy Optimization (GRPO) to jointly optimize
  accuracy, faithfulness, and comprehensiveness through rule-based, verifiable rewards
  without human annotation. The approach enforces a structured reasoning format with
  explicit tags for analytical and concluding sections, encouraging models to cross-reference
  case facts and reject distractors.
---

# Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization

## Quick Facts
- arXiv ID: 2512.00601
- Source URL: https://arxiv.org/abs/2512.00601
- Authors: Boyang Gu; Hongjian Zhou; Bradley Max Segal; Jinge Wu; Zeyu Cao; Hantao Zhong; Lei Clifton; Fenglin Liu; David A. Clifton
- Reference count: 3
- Clinical-R1-3B improves medical faithfulness and reasoning comprehensiveness while maintaining competitive accuracy

## Executive Summary
Clinical-R1 addresses the need for trustworthy medical reasoning in large language models by introducing Clinical-objective Relative Policy Optimization (CRPO), which extends Grouped Relative Policy Optimization (GRPO) to jointly optimize accuracy, faithfulness, and comprehensiveness through rule-based, verifiable rewards without human annotation. The approach enforces a structured reasoning format with explicit tags for analytical and concluding sections, encouraging models to cross-reference case facts and reject distractors. Trained on 3B-parameter Clinical-R1-3B using CRPO, the model achieves higher medical faithfulness and reasoning comprehensiveness than standard GRPO while maintaining competitive accuracy across three medical benchmarks (MedQA, MedMCQA, MedXpertQA). Specifically, Clinical-R1-3B improves case-grounded evidence citation density and distractor rejection coverage, demonstrating that multi-objective verifiable RL methods can produce safer, more auditable clinical AI systems.

## Method Summary
Clinical-R1 employs a two-stage training approach on Qwen2.5-3B-Instruct. First, cold-start SFT distillation from DeepSeek-R1 on 5,000 MedQA questions produces an initial policy capable of structured reasoning with `<dx>` and `<conclusion>` tags. Second, CRPO fine-tunes the remaining 5,000 MedQA examples for 20 epochs using rule-based rewards for accuracy, clinical reasoning format compliance, cross-referencing between sections, and consistency ratio. The composite reward function weights accuracy heavily (k=10) while shaping reasoning structure. Evaluation uses LLM-as-judge (Llama-3.1-8B-Instruct and GPT-5) to measure accuracy, clinical evidence citation density, distractor rejection coverage, and cognitive behaviors like backtracking and verification. The approach operates without human annotation, relying on verifiable reward signals.

## Key Results
- Clinical-R1-3B achieves higher medical faithfulness (CECD, DRC, hallucination reduction) than GRPO while maintaining competitive accuracy
- Structured tag enforcement increases cognitive reasoning behaviors (backtracking 0.73→2.49, backward-chaining 0.62→1.06, subgoal-setting 2.51→4.23, verification 0.53→1.28 on MedMCQA)
- Cross-reference rewards reduce hallucination from 0.85 to 0.66 on MedMCQA by forcing conclusions to cite established analysis
- CRPO maintains competitive accuracy while improving case-grounded evidence citation density and distractor rejection coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-objective reward optimization improves clinical reasoning quality beyond correctness alone.
- Mechanism: CRPO extends GRPO's advantage estimation by combining accuracy reward (binary correctness), clinical reasoning reward (format compliance + cross-reference bonus), and consistency reward (effective token ratio). The composite reward r(o) = k·r_accuracy(o) + r_CR(o) + 0.5·r_consistency(o) with k=10 prioritizes accuracy while shaping reasoning structure.
- Core assumption: Rule-based rewards can substitute for human annotation in high-stakes domains without losing alignment with clinical values.
- Evidence anchors:
  - [abstract] "CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation."
  - [Section 3, Methodology] Full reward formula and component definitions.
  - [corpus] Limited direct corpus support for medical-specific multi-objective RLVR; neighbor papers focus on single-domain applications.
- Break condition: If KL-divergence grows rapidly without cold-start initialization, training destabilizes and convergence fails.

### Mechanism 2
- Claim: Enforcing structured output tags promotes cognitive behaviors aligned with clinical reasoning.
- Mechanism: The model must separate reasoning into `<dx>` tags (hypothesis-driven analysis including System 1 intuitive and System 2 analytical processes) and `<conclusion>` tags. Format compliance yields r_CR=1; missing or malformed tags yield r_CR=0.
- Core assumption: Explicit structural separation causally improves reasoning quality rather than merely making it more readable.
- Evidence anchors:
  - [abstract] "enforces a structured reasoning format with explicit tags for analytical and concluding sections."
  - [Section 5.2, Cognitive Comprehensiveness] Table 1 shows CRPO increases backtracking (0.73→2.49), backward-chaining (0.62→1.06), subgoal setting (2.51→4.23), and verification (0.53→1.28) on MedMCQA.
  - [corpus] Weak corpus signal; neighbor papers do not test structured tag enforcement in medical contexts.
- Break condition: If the base model lacks sufficient instruction-following capacity, format compliance remains low and reward signal is sparse.

### Mechanism 3
- Claim: Cross-reference rewards between conclusion and analysis sections reduce hallucination and improve evidence grounding.
- Mechanism: An additional 0.5 reward is granted when the `<conclusion>` section explicitly references elements from the `<dx>` section. This forces the model to verify its conclusion against previously established case facts before outputting.
- Core assumption: Grounding conclusions in explicit evidence reduces language drift and fabricated claims.
- Evidence anchors:
  - [Section 3, Reward Design] "if the `<conclusion>` part cross-references the `<dx>` part, an additional reward of 0.5 is given."
  - [Section 5.3, Medical Faithfulness] "CRPO suppresses [hallucination] tendency by making unsupported additions costly: conclusions must point to sentences already established in `<dx>`." Table 1 shows hallucination reduced from 0.85 (GRPO) to 0.66 (Clinical-R1-3B) on MedMCQA.
  - [corpus] No direct corpus evidence; hallucination reduction via cross-reference rewards is not tested in neighbor papers.
- Break condition: If cross-reference detection is implemented via brittle keyword matching, models may game the reward with superficial citations rather than genuine grounding.

## Foundational Learning

- Concept: **Grouped Relative Policy Optimization (GRPO)**
  - Why needed here: CRPO directly extends GRPO's objective function; understanding advantage estimation via group statistics is prerequisite.
  - Quick check question: Given outputs {o_1, ..., o_G} with rewards {r_1, ..., r_G}, compute the advantage A_i for output o_i.

- Concept: **Dual Process Theory (System 1 / System 2)**
  - Why needed here: CRPO's `<dx>` section explicitly requires separating intuitive and analytical reasoning processes.
  - Quick check question: In clinical diagnosis, which system generates rapid pattern-matching hypotheses versus which system performs systematic verification?

- Concept: **KL-Divergence Regularization in Policy Optimization**
  - Why needed here: CRPO objective includes β·D_KL(π_θ || π_ref) term to prevent policy drift; instability noted without cold start.
  - Quick check question: What happens to training stability if KL-divergence between policy and reference model grows unbounded?

## Architecture Onboarding

- Component map:
  Input -> Prompting (CRPO sampling with `<dx>/<conclusion>` format) -> Rollout (G=5 candidates) -> Reward Computation (accuracy, format, cross-reference, consistency) -> Advantage Estimation (normalize across group) -> Policy Update (maximize J_CRPO with KL penalty) -> Inference (majority voting)

- Critical path:
  1. Cold-start SFT distillation from DeepSeek-R1 (~5,000 examples, early stopping at 13 epochs)
  2. CRPO training on remaining ~5,000 MedQA examples for 20 epochs
  3. Validation on held-out MedQA test set; generalization tested on MedMCQA and MedXpertQA

- Design tradeoffs:
  - Accuracy weight k=10 heavily prioritizes correctness over format/reasoning rewards; lower k may improve faithfulness at accuracy cost
  - Rollout G=5 limited by compute; larger G improves advantage estimation variance but increases training time
  - Consistency reward coefficient 0.5 balances against reward hacking via verbose off-topic output

- Failure signatures:
  - Rapid KL-divergence growth causing policy collapse (observed without cold start)
  - Low format compliance rate (<70%) indicating weak instruction-following base
  - High hallucination scores despite CRPO training suggesting cross-reference reward not being earned

- First 3 experiments:
  1. **Baseline comparison**: Run Qwen2.5-3B-Instruct with CoT prompting vs. Clinical-R1-3B on MedQA test set; measure accuracy, CECD, DRC, hallucination.
  2. **Ablation on accuracy weight k**: Train CRPO variants with k∈{1, 5, 10, 20}; plot accuracy vs. faithfulness tradeoff curve.
  3. **Cold-start necessity test**: Train CRPO from base model without distillation; compare KL-divergence trajectory and final performance against cold-start + CRPO.

## Open Questions the Paper Calls Out

- Question: Can more stable variants of CRPO (e.g., adaptive or off-policy updates) mitigate the training instability caused by rapid KL-divergence growth without cold-start initialization?
  - Basis in paper: [explicit] Authors state: "CRPO optimization can be unstable, particularly without a cold-start initialization, due to rapid KL-divergence growth between the policy and reference models" and propose exploring "more stable variants of CRPO (e.g., adaptive or off-policy updates)" as future work.
  - Why unresolved: The current formulation relies on cold-start to stabilize training; the fundamental instability when starting from scratch remains unaddressed.
  - What evidence would resolve it: Systematic comparison of adaptive KL penalty schedules or off-policy CRPO variants against standard CRPO, measuring training stability metrics and convergence rates.

- Question: To what extent do LLM-as-judge evaluations of cognitive comprehensiveness and medical faithfulness align with human expert judgments?
  - Basis in paper: [explicit] Authors acknowledge: "cognitive comprehensive and medical faithfulness evaluation rely on automatic annotation by another LLM, which may not fully align with human judgment. As a result, human evaluation is required for stronger validation."
  - Why unresolved: The study used Llama-3.1-8B-Instruct and GPT-5 for evaluation, but no human validation was conducted to assess whether these proxy metrics truly capture clinically meaningful reasoning quality.
  - What evidence would resolve it: Human expert evaluation of a sample of model outputs comparing LLM-judge scores with clinician ratings on the same dimensions (backtracking, faithfulness, CECD, DRC, hallucination).

- Question: How does CRPO's effectiveness scale when applied to models with stronger medical domain pretraining or larger parameter counts?
  - Basis in paper: [explicit] Authors note: "our experiments are conducted on base models not pretrained on medical corpora, limiting the representational depth of domain knowledge" and suggest extending "the method to stronger medical backbones" as future work.
  - Why unresolved: Clinical-R1-3B used a general-purpose Qwen2.5-3B base; it is unknown whether CRPO provides marginal benefit over GRPO when the base model already has deep medical knowledge or greater capacity.
  - What evidence would resolve it: Training CRPO on medically pretrained backbones (e.g., Meditron, BioMistral) and larger models (7B, 70B) with direct comparison to GRPO on the same benchmarks.

- Question: How does the accuracy multiplier coefficient k in the reward function affect the trade-off between faithfulness/comprehensiveness and accuracy across different medical domains?
  - Basis in paper: [inferred] The paper sets k=10 as a fixed hyperparameter without ablation, stating it estimates "the importance of accuracy." The optimal balance between the three objectives may vary across specialties or question types.
  - Why unresolved: No sensitivity analysis was conducted on k; it is unclear whether this value generalizes or if different clinical contexts require different weightings.
  - What evidence would resolve it: Ablation experiments varying k across multiple values and reporting resulting accuracy, faithfulness, and comprehensiveness metrics stratified by medical specialty or question difficulty.

## Limitations

- The effectiveness of CRPO relies heavily on rule-based reward design, which may not generalize beyond the specific structured format used here.
- The cold-start requirement via DeepSeek-R1 distillation raises questions about scalability and dependency on proprietary APIs for initial training data.
- The reliance on LLM-as-judge evaluation introduces potential bias and measurement error that isn't fully characterized against human expert judgments.

## Confidence

- **High confidence**: Claims about CRPO extending GRPO's advantage estimation framework and the observed accuracy improvements over GRPO baselines.
- **Medium confidence**: Claims about improved faithfulness and comprehensiveness metrics, though LLM-as-judge evaluation introduces uncertainty.
- **Low confidence**: Claims about the causal mechanism by which structured tags improve reasoning quality versus correlation with other factors.

## Next Checks

1. **Reward robustness test**: Systematically perturb the cross-reference detection algorithm (e.g., require stricter semantic matching vs. keyword matching) and measure how performance metrics change to assess whether the rewards are measuring true reasoning quality or superficial compliance.

2. **Human evaluation validation**: Commission independent clinical expert review of a stratified sample of Clinical-R1 outputs to verify that LLM-as-judge faithfulness and comprehensiveness scores align with human assessments of actual medical reasoning quality.

3. **Generalization stress test**: Evaluate Clinical-R1 on clinical reasoning tasks outside the MedQA domain (e.g., radiology report analysis, pathology findings) to determine whether the structured reasoning format and reward design generalize to other medical reasoning contexts or are overfit to the training distribution.