---
ver: rpa2
title: Co-Creative Learning via Metropolis-Hastings Interaction between Humans and
  AI
arxiv_id: '2506.15468'
source_url: https://arxiv.org/abs/2506.15468
tags:
- human
- learning
- agent
- data
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes and tests co-creative learning, where humans
  and AI integrate their partial information through interaction to build shared representations.
  The study uses the Metropolis-Hastings Naming Game (MHNG), a decentralized Bayesian
  inference mechanism, within a joint attention naming game (JA-NG) task.
---

# Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI

## Quick Facts
- **arXiv ID:** 2506.15468
- **Source URL:** https://arxiv.org/abs/2506.15468
- **Reference count:** 31
- **Primary result:** MH-based human-AI interaction achieves higher categorization accuracy (ARI up to 0.609) than control conditions.

## Executive Summary
This paper proposes and tests co-creative learning, where humans and AI integrate their partial information through interaction to build shared representations. The study uses the Metropolis-Hastings Naming Game (MHNG), a decentralized Bayesian inference mechanism, within a joint attention naming game (JA-NG) task. In an online experiment with 69 participants, human-AI pairs played JA-NG under partial observability with one of three computer agent types (MH-based, always-accept, or always-reject). Results show that MH-based agent pairs significantly improved categorization accuracy (ARI up to 0.609) and achieved stronger convergence toward shared sign systems than control groups. Human acceptance behavior closely aligned with MH-derived acceptance probabilities.

## Method Summary
The study implements a joint attention naming game (JA-NG) where human-AI dyads interact to categorize objects under partial observability. The AI agent maintains internal parameters for sign-concept and concept-observation mappings, and uses Metropolis-Hastings acceptance rules to propose and accept/reject symbol updates. Human participants see grayscale images and shapes while AI agents access numerical color values, requiring interaction to build complete category structures. Three experimental conditions test MH-based acceptance versus always-accept and always-reject strategies.

## Key Results
- MH-based pairs achieved significantly higher Adjusted Rand Index (ARI) scores (0.609) compared to always-accept (0.469) and always-reject (0.128) conditions.
- Human acceptance behavior showed strong positive correlation with theoretical MH acceptance probabilities.
- The MH condition demonstrated more balanced learning between human and AI agents, while always-accept favored human knowledge retention.

## Why This Works (Mechanism)

### Mechanism 1: Decentralized Bayesian Inference via Metropolis-Hastings (MH) Acceptance
The interaction forms a Markov chain that satisfies detailed balance relative to the joint posterior distribution. The listener calculates an acceptance probability comparing the likelihood of proposed versus current symbols given their internal state. Evidence from Section 3.1 shows this functions as a distributed MCMC algorithm. Break condition: deterministic strategies violate detailed balance and prevent convergence.

### Mechanism 2: Cross-Modal Information Integration via Shared Latent Symbols
Shared symbols bind disparate perceptual features (color for AI vs. shape for human) into unified category structures. The generative model conditions both agents' internal concepts on shared latent symbols, allowing implicit alignment without direct data sharing. Evidence from Section 4 shows partial observability setup where humans see grayscale/shape and AI sees color values. Break condition: uncorrelated modalities prevent convergence.

### Mechanism 3: Human Behavioral Alignment with Theoretical Sampling
Human cognitive processing in JA-NG aligns with MH probability derived from internal belief state, turning humans into biological MCMC components. Evidence from Section 5 reports strong positive relationship between theoretical $r_{MH}$ and actual human acceptance. Break condition: external biases or fatigue-driven decisions break decentralized inference.

## Foundational Learning

- **Markov Chain Monte Carlo (MCMC) & Detailed Balance**: Essential for understanding why interaction is framed as sampling algorithm rather than communication. Quick check: Can you explain why hill-climbing fails compared to MCMC for global optimum?
- **Probabilistic Graphical Models (PGMs)**: Required for computing likelihood $P(c|s, \theta)$ in acceptance ratio. Quick check: How do you compute $P(C|S)$ in a graphical model where $C$ depends on $S$?
- **The Naming Game (Semiotics)**: Protocol layer defining Speaker/Listener roles and vocabulary. Quick check: What minimum information must the Listener have to decide on a proposal?

## Architecture Onboarding

- **Component map:** Environment -> Observation Layer -> Internal State -> Interaction Module -> Inference Engine
- **Critical path:** Calculation of acceptance ratio ($r_{MH}$). Incorrect math makes conditions indistinguishable and invalidates decentralized inference proof.
- **Design tradeoffs:** MH vs supervised (AA): AA converges faster on human knowledge but caps AI accuracy lower. MH is slower but reaches higher joint accuracy (AI ARI 0.609 vs 0.469). Complexity increases with deep generative models for real-world data.
- **Failure signatures:** "Echo Chamber" (Always Accept): AI memorizes human labels without using unique perceptual data. "Stubborn Agent" (Always Reject): No information flows, Markov chain doesn't mix. Divergence: high learning rates cause oscillation.
- **First 3 experiments:** 1) Reproduce partial observability with 2D clusters (X-Y coordinates). 2) Stress test human proxy with noisy rational bot. 3) Scalability check with $K>3$ categories and $N>10$ items.

## Open Questions the Paper Calls Out
None

## Limitations
- Human acceptance behavior alignment relies on simplified linear Bernoulli model that may not capture complex cognitive factors
- Controlled synthetic task represents narrow domain compared to real-world human-AI collaboration
- Human sample (N=69) lacks diversity metrics and 200-interaction limit may not reflect long-term dynamics

## Confidence
- **High confidence:** MH acceptance mechanism as theoretical framework for decentralized Bayesian inference is mathematically sound
- **Medium confidence:** Empirical finding that MH-based interaction achieves higher ARI scores than controls is supported by data
- **Medium confidence:** Claim that human acceptance aligns with MH probabilities is supported by reported correlation

## Next Checks
1. Generalize to non-synthetic data (real images/text) with alternative generative models (VAEs) to verify extension beyond controlled vector spaces
2. Conduct cognitive validation with think-aloud protocols or eye-tracking to understand acceptance decisions and validate linear Bernoulli model
3. Extend interaction duration beyond 200 exchanges and track performance over weeks/months to assess long-term advantage and identify convergence patterns