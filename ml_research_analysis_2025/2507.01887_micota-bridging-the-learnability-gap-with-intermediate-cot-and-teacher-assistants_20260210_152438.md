---
ver: rpa2
title: 'MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants'
arxiv_id: '2507.01887'
source_url: https://arxiv.org/abs/2507.01887
tags:
- teacher
- reasoning
- qwen2
- data
- strong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge that small language models (SLMs)
  struggle to learn long chain-of-thought (CoT) reasoning from large teacher models
  due to a "SLMs Learnability Gap." To solve this, the authors propose MiCoTA, a framework
  that uses intermediate-sized teacher assistants and intermediate-length CoT data
  to bridge both capacity and reasoning length gaps. MiCoTA trains an intermediate
  model on long CoT from a strong teacher, then merges it with its base version to
  generate shorter, more learnable CoT sequences for SLMs.
---

# MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants

## Quick Facts
- **arXiv ID:** 2507.01887
- **Source URL:** https://arxiv.org/abs/2507.01887
- **Reference count:** 40
- **Primary result:** MiCoTA improves Qwen2.5-7B and Qwen2.5-3B by 3.47 and 3.93 average score points on math benchmarks vs direct long CoT training

## Executive Summary
This paper addresses the challenge that small language models (SLMs) struggle to learn long chain-of-thought (CoT) reasoning from large teacher models due to a "SLMs Learnability Gap." To solve this, the authors propose MiCoTA, a framework that uses intermediate-sized teacher assistants and intermediate-length CoT data to bridge both capacity and reasoning length gaps. MiCoTA trains an intermediate model on long CoT from a strong teacher, then merges it with its base version to generate shorter, more learnable CoT sequences for SLMs. Experiments on math reasoning benchmarks (AIME, AMC, Olympiad, MATH-500, GSM8K) show that Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct improve by 3.47 and 3.93 average score points respectively compared to models trained directly on long CoT from large teachers. The method also produces data more aligned with base SLM distributions, enabling more effective knowledge transfer.

## Method Summary
MiCoTA employs a dual bridging strategy: first, an intermediate-sized Teacher Assistant (TA) is fine-tuned on long CoT data generated by a strong teacher to mitigate the capacity gap; second, the fine-tuned TA is merged with its base version to create a model that generates intermediate-length ("Mid-CoT") responses, reducing sequence length while preserving reasoning capability. Students are then trained on this Mid-CoT data, which aligns better with their base distribution and is more learnable than full-length traces. The approach uses Qwen2.5-14B as TA and Qwen2.5-3B/7B/1.5B as students, with performance measured on multiple math reasoning benchmarks.

## Key Results
- Qwen2.5-7B-Instruct trained on MiCoTA data outperforms baseline by 3.47 average score points on math benchmarks
- Qwen2.5-3B-Instruct trained on MiCoTA data outperforms baseline by 3.93 average score points
- MiCoTA data produces lower BPC values for students compared to strong teacher CoT, indicating better distribution alignment
- The approach is particularly effective for smaller students (3B) where the learnability gap is most pronounced

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing an intermediate-sized Teacher Assistant (TA) mitigates the capacity gap that causes Small Language Models (SLMs) to fail when learning directly from large teachers.
- Mechanism: The TA (e.g., 14B params) is fine-tuned on long Chain-of-Thought (CoT) data from the strong teacher (e.g., 32B params). This TA possesses sufficient capacity to assimilate complex reasoning patterns that a smaller student (e.g., 3B params) cannot capture directly. The student then learns from this intermediate model.
- Core assumption: Assumption: The difficulty in distillation is strictly correlated with the parameter size difference ("capacity gap") and can be stepped down incrementally.
- Evidence anchors:
  - [abstract] "SLMs often struggle to learn long-form CoT reasoning due to their limited capacity... MiCoTA employs intermediate-sized models as teacher assistants."
  - [section 3] "This dual 'half-size, half-length' design ensures that the teacher assistant is not only more accessible for the student model to learn from in terms of capacity..."
  - [corpus] Related work (e.g., 'Unveiling the Key Factors...') supports the difficulty of CoT distillation to SLMs, but does not explicitly validate the specific hierarchical model scaling used here.
- Break condition: If the student model is exceptionally small (e.g., <1B) or the reasoning complexity is sufficiently low, the intermediate TA may fail to bridge the gap, or the overhead might outweigh gains.

### Mechanism 2
- Claim: Merging a fine-tuned reasoning model with its base version produces an intermediate-length CoT output that is more learnable for SLMs than full long-CoT traces.
- Mechanism: The paper utilizes model merging (specifically Dare + TIES) to combine the weights of the TA before and after fine-tuning on long CoT. This interpolation results in a model that retains reasoning capability but generates responses roughly half the length ("Mid-CoT"), preventing the student from being overwhelmed by excessively long sequences.
- Core assumption: Assumption: Long sequence length is a primary bottleneck for SLM optimization, and shortening sequences via weight interpolation preserves the essential logical structure while discarding redundant verbosity.
- Evidence anchors:
  - [section 3.2] "...model merging can reduce the response lengths of System 2 models by approximately half while maintaining performance."
  - [figure 2] "The token length is reduced to about half of the System 2 models by model merging."
  - [corpus] Neighbors do not cover model merging as a specific length-reduction technique for distillation.
- Break condition: If the "Mid-CoT" length falls below the threshold required for the specific reasoning task (e.g., complex logic requires more tokens), performance may degrade due to insufficient search depth.

### Mechanism 3
- Claim: Mid-CoT data aligns more closely with the base distribution of SLMs, reducing the "learnability gap" as measured by Bits Per Character (BPC).
- Mechanism: Standard long CoT from large models forces SLMs into out-of-distribution (OOD) spaces (high BPC). The merged TA generates data that maintains a lower BPC for the student, implying the optimization landscape is smoother and the data is more "native" to the student's pre-training distribution.
- Core assumption: Lower BPC/adaptability scores correlate causally with improved downstream reasoning performance and faster convergence.
- Evidence anchors:
  - [section 4.5] "...MiCoTA data, we observe considerably lower BPC values across all models compared to the Strong Teacher CoT... facilitating a more effective learning process."
  - [abstract] "...method produces data more closely aligned with base SLM distributions."
- Break condition: If the student model is significantly under-trained or the vocabulary differs vastly, BPC may not capture semantic alignment, rendering the metric misleading.

## Foundational Learning

- Concept: **Knowledge Distillation (KD)**
  - Why needed here: The paper extends KD by introducing a "Teacher Assistant" role. You must understand standard teacher-student loss functions (e.g., KL divergence) to grasp why a direct jump from a 70B+ teacher to a 3B student fails (capacity mismatch).
  - Quick check question: Can you explain why soft labels from a large model are harder for a small model to mimic than hard labels?

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The core artifact being distilled is the "reasoning trace." Understanding CoT is necessary to evaluate whether the model is learning reasoning or just copying style (e.g., the "wait" tokens mentioned in the case study).
  - Quick check question: What distinguishes a "short" CoT from a "long" CoT in terms of inference compute versus solution accuracy?

- Concept: **Model Merging (e.g., TIES, Dare)**
  - Why needed here: This is the technique used to create the "Mid-CoT" generator. You need to know that merging weights combines capabilities without further training (inference-free modification).
  - Quick check question: How does merging a fine-tuned model with its base version affect the model's output distribution compared to the original fine-tuned model?

## Architecture Onboarding

- Component map: Strong Teacher -> Teacher Assistant -> Merged TA -> Student
- Critical path: The quality of the Student depends most heavily on the **Merged TA's ability to truncate reasoning length without truncating reasoning logic**. If the merging process loses the critical "reflection" steps, the student learns a broken heuristic.
- Design tradeoffs:
  - **TA Size**: A larger TA (e.g., 32B) might learn the Long CoT better but may not produce distribution-aligned data for the student. A smaller TA (e.g., 7B) might be too weak to learn the Strong Teacher's logic.
  - **Merging Ratio**: The paper implies a ~50% length reduction. Pushing for shorter lengths (more aggressive merging) risks dropping crucial steps.
- Failure signatures:
  - **Overthinking**: If the student generates repetitive "wait" or "check" tokens without progress (Section 4.7), the distilled data likely retained the "style" of reflection without the "substance" (reasoning depth).
  - **Performance Collapse**: If the student performs worse than the base model (negative Δ), it indicates the distillation data was too OOD (Out of Distribution) or the student capacity was violated.
- First 3 experiments:
  1. **Baseline Reproduction**: Train a 3B student directly on Long-CoT from a 32B teacher to verify the "Learnability Gap" (negative Δ) exists in your setup.
  2. **Ablation on Merging**: Compare students trained on data from (a) the fine-tuned TA only and (b) the Merged TA to validate that the "Mid-Length" specifically contributes to gains, not just the TA size.
  3. **BPC Check**: Calculate the BPC of the generated MiCoTA data against the student's base distribution to confirm the data is indeed more "native" (lower BPC) than the Strong Teacher's Long-CoT.

## Open Questions the Paper Calls Out
- **Domain Generalization**: The authors explicitly state that experiments were confined to math reasoning and generalizability to other domains "remains to be explored" (Appendix B, Limitations).
- **TA Size Optimization**: The methodology fixes the TA at 14B parameters but does not ablate this ratio to determine if a 7B or 20B TA would yield different results for the 3B student, leaving the "half-size" heuristic unverified as optimal.

## Limitations
- The paper uses only Qwen2.5 models as students, leaving effectiveness with other SLM architectures (Mistral, Llama) unverified.
- The exact model merging hyperparameters (weight ratios, density parameters) are not specified, creating uncertainty about reproducibility of the "half-length" reduction.
- The causal relationship between BPC alignment and improved reasoning performance is not established through controlled experiments.

## Confidence
- **High Confidence (70-90%)**: The core observation that SLMs struggle to learn long CoT directly from large teachers is well-supported by the negative transfer results in Table 1. The intermediate TA framework consistently produces performance gains over direct long CoT training.
- **Medium Confidence (40-70%)**: The mechanism that model merging specifically produces optimal "Mid-CoT" length for learning is supported by token length reductions and performance improvements, but the exact relationship between length reduction and reasoning quality remains unclear.
- **Low Confidence (0-40%)**: The claim that BPC alignment is the primary driver of improved learning is weakly supported. The paper shows correlation between lower BPC and better performance but does not conduct controlled experiments to isolate this effect from other factors.

## Next Checks
1. **Merging Ratio Sensitivity Analysis**: Systematically vary the model merging weight ratio (e.g., 0.3, 0.5, 0.7) and measure both output length reduction and downstream student performance. This would determine whether the "half-length" claim is optimal or arbitrary.
2. **Architecture Generalization Test**: Apply MiCoTA to a different SLM family (e.g., Llama-3 8B or Mistral-7B) using the same Qwen2.5-14B TA and merged data. Success would validate the framework's architecture independence.
3. **Controlled BPC Ablation**: Generate MiCoTA data with artificially manipulated BPC values (while preserving reasoning quality) and measure student performance. This would test whether BPC alignment is a proxy for learnability or the actual causal mechanism.