---
ver: rpa2
title: Group size effects and collective misalignment in LLM multi-agent systems
arxiv_id: '2510.22422'
source_url: https://arxiv.org/abs/2510.22422
tags:
- time
- consensus
- word
- bias
- collective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how group size influences collective behavior
  in multi-agent systems of large language models (LLMs). It focuses on the phenomenon
  of collective misalignment, where interacting LLMs can develop emergent biases absent
  in individual models during a naming game.
---

# Group size effects and collective misalignment in LLM multi-agent systems

## Quick Facts
- **arXiv ID:** 2510.22422
- **Source URL:** https://arxiv.org/abs/2510.22422
- **Reference count:** 40
- **Primary result:** Group size influences collective behavior in multi-agent systems of LLMs, with non-linear effects on collective bias and convergence.

## Executive Summary
This paper investigates how group size influences collective behavior in multi-agent systems of large language models (LLMs). It focuses on the phenomenon of collective misalignment, where interacting LLMs can develop emergent biases absent in individual models during a naming game. The study shows that group size affects the strength and form of collective bias in a non-linear way, revealing model-dependent dynamical regimes. Small populations exhibit rapid consensus driven by random fluctuations, while larger populations show slower, more coordinated convergence. Above a critical population size, the system's behavior becomes deterministic, with outcomes predictable by mean-field theory. These findings demonstrate that population size is a key determinant of multi-agent dynamics, with implications for the safe deployment of LLM-based systems at scale.

## Method Summary
The study employs a naming game framework where N LLM agents engage in pairwise coordination interactions to establish shared linguistic conventions from a pool of W=2 words. Agents with memory capacity H=5 store recent interactions and update their memory states based on interaction outcomes. The researchers precompute probabilistic policies from LLM logits for all memory states using a temperature of T=0.5, then simulate up to 1000 population rounds or until 98% success rate in last 3N interactions. The study analyzes eleven word pairs selected for bias sensitivity across four instruction-tuned LLMs (Phi-4, GPT-4o, Qwen QwQ-32B, Llama 3.1 70B Instruct), measuring collective bias (fraction of runs converging to each word), consensus time distributions, and population size effects (N = 1 to 10^6).

## Key Results
- Collective misalignment emerges in pairwise interactions, with outcomes differing from individual model preferences
- Group size affects collective bias non-linearly, with small populations showing rapid consensus via random fluctuations and large populations showing slower, more coordinated convergence
- Above critical population size, dynamics become deterministic and predictable by mean-field theory, with the threshold varying across models and word pairs
- Policy pre-computation enables efficient large-scale simulations while preserving behavioral statistics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pairwise interactions in a naming game generate collective biases (misalignment) that differ from individual model preferences.
- **Mechanism:** Agents maintain finite memory of past interactions. When two agents coordinate, they update their memory state. The probability of selecting a specific convention depends on this memory context, creating a feedback loop where early random fluctuations can lock the population into a specific convention, even if individual agents were initially neutral.
- **Core assumption:** The probabilistic policy extracted from the LLM (via next-token likelihoods) accurately represents the agent's decision-making strategy in a repeated game context.
- **Evidence anchors:**
  - [abstract] "interacting LLMs... can generate collective biases absent in individual models."
  - [section: Results] "Interaction can amplify... introduce new ones, or override model-level preferences" (Fig 1).
  - [corpus] "Conformity and Social Impact on AI Agents" supports the premise that group pressure influences agent outputs.
- **Break condition:** If agents utilize infinite memory or external grounding that corrects drift, the feedback loop weakens.

### Mechanism 2
- **Claim:** Collective outcomes transition from stochastic to deterministic as population size ($N$) increases.
- **Mechanism:** In small populations ($N < N_c$), early stochastic fluctuations dominate, allowing "weak" conventions to win by chance. As $N$ increases, the law of large numbers applies; the system dynamics converge to the mean-field prediction where the "strong" convention (the one with the larger basin of attraction) wins reliably.
- **Core assumption:** The system possesses fixed points (absorbing states) corresponding to full consensus, and one is strictly more stable or has a larger basin of attraction than the other.
- **Evidence anchors:**
  - [abstract] "above a critical population size, simulations converge to deterministic predictions."
  - [section: Results] "larger populations reduce outcome uncertainty... threshold varies considerably across model."
  - [corpus] "On the Dynamics of Multi-Agent LLM Communities" discusses similar dynamical stability in larger groups.
- **Break condition:** If the fixed points are unstable or the system enters a chaotic/metastable state (as seen in Llama with {less, more}), size does not guarantee determinism.

### Mechanism 3
- **Claim:** Large-scale dynamics can be simulated efficiently by pre-computing probabilistic policies rather than running live inference.
- **Mechanism:** Instead of querying the LLM at every step, the authors map every possible memory state to a probability distribution over the next action (convention choice). This turns the LLM agent into a stochastic lookup table, preserving behavioral statistics while removing computational latency.
- **Core assumption:** The LLM's behavior is strictly determined by the limited context window provided in the prompt (memory state), without hidden state or cross-session learning.
- **Evidence anchors:**
  - [section: Implementation] "We derive this policy empirically by probing the LLM... normalize the logits... to form a probability distribution."
  - [section: SI] "Simulations run for up to 1000 population rounds... drastically reducing the required computational resources."
- **Break condition:** If the agent's strategy evolves dynamically (learning) or depends on prompt factors not captured in the memory state, the precomputed policy fails.

## Foundational Learning

- **Concept: Naming Game (Convention Formation)**
  - **Why needed here:** This is the core protocol driving the dynamics. Understanding that agents succeed only by matching a partner (coordination) explains why local interactions force global consensus.
  - **Quick check question:** If an agent plays "A" and the partner plays "B", does the agent update their memory to prefer "A" or "B" next time? (Answer: It depends on the LLM's policy, but the mismatch triggers the update).

- **Concept: Mean-Field Approximation**
  - **Why needed here:** To understand the transition from stochastic small groups to deterministic large groups. It approximates discrete stochastic interactions with continuous deterministic flows.
  - **Quick check question:** Why does a population of 1,000,000 agents behave more predictably than a population of 2? (Answer: Random fluctuations average out in large systems).

- **Concept: Basins of Attraction**
  - **Why needed here:** Explains *why* one word wins over another in large populations. The "strong" word has a larger region of initial states leading to its consensus.
  - **Quick check question:** In a system with two stable consensus states, what determines the final outcome? (Answer: The initial probability distribution falling within one state's basin of attraction).

## Architecture Onboarding

- **Component map:**
  - **Policy Store:** Pre-computed map of `{Memory_State} -> {P(word_A), P(word_B)}`
  - **Agent:** Holds current `Memory_State` (size $H$)
  - **Simulation Engine:** Selects pairs, looks up policies, samples actions, updates memories
  - **Consensus Monitor:** Checks if 98% of recent interactions were successful coordinations

- **Critical path:** The extraction of the **Probabilistic Policy**. If the logit normalization or temperature setting is incorrect, the simulation will not reflect the actual LLM's behavior.

- **Design tradeoffs:**
  - **Direct Inference vs. Policy Cache:** Direct inference allows for complex reasoning/retrieval but is too slow for $N > 100$. Policy cache is fast but assumes static decision-making.
  - **Memory Size ($H$):** Small $H$ limits strategy complexity (Markovian); large $H$ increases state space combinatorially.

- **Failure signatures:**
  - **Metastable States:** The system never converges (e.g., Llama on {less, more}). Look for oscillations or unimodal distributions that don't narrow.
  - **Policy Drift:** If the simulation diverges from validation runs, the precomputed policy likely missed a prompt dependency.

- **First 3 experiments:**
  1. **Validation Run:** Run direct LLM inference for $N=10$ vs. Policy Simulation for $N=10$ on the same word pair to verify equivalence (as per Fig S2).
  2. **Threshold Search:** Identify the critical population size $N_c$ for a new word pair by sweeping $N \in [2, 1000]$ and plotting the variance of the consensus outcome.
  3. **Stability Analysis:** Calculate the eigenvalues of the Jacobian matrix for the mean-field fixed points (using Eq. 25) to predict if a specific model/word pair will converge or oscillate before running expensive simulations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do collective dynamics change in heterogeneous populations composed of multiple different LLM models interacting together?
- **Basis in paper:** [inferred] The study explicitly restricts analysis to "homogeneous populations, meaning that the probabilistic policies of all agents are extracted from the same underlying LLM model," leaving mixed-model dynamics unexplored.
- **Why unresolved:** Real-world multi-agent deployments will likely involve diverse models; whether heterogeneous populations amplify or attenuate collective bias remains unknown.
- **What evidence would resolve it:** Systematic simulations of populations mixing agents from different LLMs (e.g., GPT and Llama), measuring collective bias emergence and consensus dynamics.

### Open Question 2
- **Question:** What mechanisms drive the non-convergence observed in specific modelâ€“word pair combinations (e.g., Llama with {old, young} and {less, more})?
- **Basis in paper:** [explicit] The authors "defer systematic analysis to future" work regarding populations that fail to reach consensus, noting these cases involve unstable or marginal fixed points in mean-field analysis.
- **Why unresolved:** The mean-field analysis identifies unstable fixed points but does not explain what model or word-pair features create these dynamics.
- **What evidence would resolve it:** Analysis correlating policy structure, eigenvalue distributions, or semantic properties of word pairs with convergence failure rates.

### Open Question 3
- **Question:** Do similar non-linear population size effects emerge in other collective behaviors beyond linguistic coordination, such as collusion, deception, or cooperation?
- **Basis in paper:** [explicit] The Discussion states that "similar nonlinear, scale-dependent dynamics are expected to manifest in other interaction-driven phenomena, including bias, collusion, deception, and cooperation."
- **Why unresolved:** The naming game framework only tests coordination; generalization to strategic, adversarial, or cooperative multi-agent settings is unverified.
- **What evidence would resolve it:** Experiments applying the size-variation methodology to game-theoretic tasks (e.g., Prisoner's Dilemma, negotiation) measuring how collective outcomes scale with N.

## Limitations

- The naming game protocol's artificial nature may not capture all relevant contextual dependencies in real-world LLM deployments
- The fixed memory architecture (H=5) may miss dynamic learning effects that could emerge in longer-running or more complex multi-agent scenarios
- The selected word pairs represent a limited sample of possible linguistic conventions and may not generalize to all coordination domains

## Confidence

- **High confidence:** The observation that group size affects collective dynamics in a non-linear fashion is well-supported by simulation results across multiple models and word pairs.
- **Medium confidence:** The specific mechanisms by which collective misalignment emerges and the precise thresholds for deterministic behavior are model-dependent and may vary with implementation details.
- **Low confidence:** Predictions about real-world LLM deployment implications based on naming game dynamics require additional validation in more ecologically valid multi-agent scenarios.

## Next Checks

1. **Prompt sensitivity validation:** Systematically vary prompt formatting, temperature, and memory representation across word pairs to quantify robustness of the observed size effects and collective misalignment phenomena.

2. **Alternative memory architectures:** Test whether the observed size-dependent dynamics persist when using variable-length memory, attention-based memory, or memory with external retrieval capabilities.

3. **Extended interaction horizons:** Implement simulations with repeated interactions between the same agent pairs to determine whether relationship formation and learning alter the population size thresholds and collective bias patterns.