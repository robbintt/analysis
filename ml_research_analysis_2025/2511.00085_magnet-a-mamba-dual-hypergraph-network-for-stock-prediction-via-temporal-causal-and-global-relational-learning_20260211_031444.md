---
ver: rpa2
title: 'MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal
  and Global Relational Learning'
arxiv_id: '2511.00085'
source_url: https://arxiv.org/abs/2511.00085
tags:
- stock
- prediction
- temporal
- magnet
- hypergraph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MaGNet introduces a Mamba dual-hyperGraph Network that combines
  bidirectional Mamba with adaptive gating and MoE for temporal modeling, 2D spatiotemporal
  attention for cross-feature and cross-stock interactions, and a dual hypergraph
  framework (Temporal-Causal and Global Probabilistic) for multi-scale relational
  learning. Experiments on six major stock indices show MaGNet achieves up to 54.9%
  accuracy on CSI 300, with Sharpe ratios exceeding 1.0 and annual returns up to 22.6%.
---

# MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning

## Quick Facts
- **arXiv ID**: 2511.00085
- **Source URL**: https://arxiv.org/abs/2511.00085
- **Reference count**: 40
- **Primary result**: Up to 54.9% accuracy on CSI 300 with Sharpe ratios exceeding 1.0 and annual returns up to 22.6%

## Executive Summary
MaGNet introduces a Mamba dual-hypergraph network for stock prediction that combines temporal modeling, cross-feature interactions, and multi-scale relational learning. The architecture leverages bidirectional Mamba with adaptive gating and sparse Mixture-of-Experts for dynamic market adaptation, 2D spatiotemporal attention for cross-feature and cross-stock interactions, and a dual hypergraph framework to disentangle localized temporal-causal patterns from global market structures. Experiments on six major stock indices demonstrate superior performance with up to 54.9% accuracy on CSI 300 and Sharpe ratios exceeding 1.0.

## Method Summary
MaGNet predicts next-day stock price movements using a dual hypergraph framework. The architecture processes multivariate financial data through a MAGE block (bidirectional Mamba with adaptive gating and sparse MoE) for temporal modeling, followed by feature-wise and stock-wise 2D spatiotemporal attention modules for cross-feature and cross-stock interactions. Two hypergraph networks operate in parallel: Temporal-Causal Hypergraph (TCH) captures localized temporal propagation using causal masked attention with Top-K sparsification, while Global Probabilistic Hypergraph (GPH) models instantaneous global market structures using soft hyperedge assignments with Jensen-Shannon divergence weighting. The model is trained with cross-entropy loss and evaluated through both classification metrics and backtesting with transaction costs.

## Key Results
- Achieves up to 54.9% accuracy on CSI 300 index
- Sharpe ratios exceeding 1.0 across all tested indices
- Annual returns up to 22.6% with effective drawdown control
- Ablation studies confirm MAGE block as most critical component
- Dual hypergraph design shows distinct contributions to different market indices

## Why This Works (Mechanism)

### Mechanism 1: MAGE Block for Regime-Adaptive Temporal Modeling
The MAGE block combines bidirectional Mamba with adaptive gating and sparse MoE to capture market regime dynamics. Bidirectional Mamba processes temporal sequences in both directions, gating mechanisms adaptively weight directional information, and sparse MoE routes inputs to specialized experts based on market conditions. This enables the model to dynamically adapt to bull, bear, and volatile market regimes.

### Mechanism 2: 2D Spatiotemporal Attention for Cross-Feature Interactions
Feature-wise 2D spatiotemporal attention preserves cross-sectional structure while enabling feature-to-feature interaction. By transposing data to features×stocks×time and applying attention across stocks and time for each feature, the model captures meaningful cross-feature correlations (e.g., volume-price relationships) that would be lost with flattened processing.

### Mechanism 3: Dual Hypergraph for Multi-Scale Relational Learning
The dual hypergraph framework disentangles localized temporal-causal propagation from instantaneous global market structure. TCH uses causal masked attention with Top-K sparsification and ReTanh activation for temporal-valid hyperedges, while GPH uses soft probabilistic assignments with JSD weighting for distinct market themes. This separation enables the model to capture both time-dependent relationships and simultaneous co-movements.

## Foundational Learning

- **State Space Models (SSMs) / Selective Scan**: Mamba is core to MAGE block; understanding how SSMs achieve linear complexity with selective content-aware propagation is essential. Quick check: Can you explain why Mamba's selective scan differs from standard RNN gating and what "data-dependent parameters" means in this context?

- **Hypergraph Neural Networks (HGNNs)**: Both TCH and GPH use hypergraph convolutions; understanding incidence matrices, hyperedges vs. edges, and message passing over high-order structures is prerequisite. Quick check: Given an incidence matrix H∈R^(TN×M), what does each row/column represent and how does H·H^T differ from standard adjacency?

- **Mixture-of-Experts (MoE) with Top-K Routing**: MAGE uses sparse MoE for regime adaptation; understanding load balancing, capacity factors, and gradient flow through routing is critical for debugging. Quick check: What happens to training if all inputs route to the same expert? How does capacity-based normalization address this?

## Architecture Onboarding

- **Component map**: Input (N×T×F) → Embedding → MAGE Block ×L → Feature-wise 2D Attn → TCH → Stock-wise 2D Attn → GPH → FFN + Softmax

- **Critical path**: MAGE temporal encoding → 2D attention (both types) → Dual hypergraph convolution → prediction. Ablation shows MAGE is most critical; removing it causes largest degradation.

- **Design tradeoffs**: More MAGE layers (1-2 tuned) vs. computational cost; Hyperedge counts (M1: 32-128, M2: 16-64 tuned) - higher captures more structure but risks overfitting; Top-K sparsification in TCH (32-128) - lower K increases sparsity but may miss weak signals.

- **Failure signatures**: Expert collapse in MoE (monitor expert utilization histograms; if <2 experts active, increase capacity factor); Hyperedge collapse (if H_GPH columns become uniform, JSD weights converge - consider entropy regularization); Attention instability (if attention scores explode/vanish, check layer norm placement).

- **First 3 experiments**:
  1. Sanity check: Train on single index (DJIA, smallest N=30) with default hyperparameters; verify ACC >52% and Sharpe >0.5. If fails, check data pipeline.
  2. Ablation-by-component: Remove MAGE, F2D, TCH, GPH sequentially on validation; expect largest drop from MAGE removal.
  3. Hyperedge sensitivity: Vary M1={16,32,64,128} and M2={8,16,32,64} on validation; plot Sharpe vs. hyperedge count.

## Open Questions the Paper Calls Out

### Open Question 1: Interpretability of Learned Hyperedge Structures
Can the learned hyperedge structures in TCH and GPH be interpreted to explain specific market phenomena or sector rotations? The paper states future work could investigate interpretability for market insight generation, but lacks qualitative analysis of what hyperedges represent.

### Open Question 2: Integration of Alternative Data Sources
How does MaGNet's performance change when integrating alternative data sources like news sentiment or macroeconomic indicators? The conclusion suggests exploring incorporating alternative data, but current inputs are strictly limited to quantitative historical trading data.

### Open Question 3: Long-Term Forecasting Performance
Does MaGNet maintain superiority over Transformer baselines for longer-term forecasting horizons (e.g., weekly or monthly)? The paper focuses on T+1 binary classification despite Mamba being designed for long-range dependencies.

### Open Question 4: Robustness to Transaction Costs
Is the reported Sharpe ratio robust to significant increases in transaction costs or market slippage? Backtesting assumes 0.25% transaction cost, which may understate costs for larger portfolios or less liquid stocks.

## Limitations
- External validity limited to 6 major indices over 2020-2024; may not generalize to other time windows or emerging markets
- Hyperparameter sensitivity with many architectural choices tuned per-dataset; unclear how configurations transfer to new settings
- Implementation complexity with dual hypergraph framework and MAGE block introduces potential for expert collapse, hyperedge redundancy, or attention instability

## Confidence
- **MAGE block effectiveness**: High confidence (consistent ablation performance drops)
- **Dual hypergraph disentanglement**: Medium confidence (structural separation shown but empirical validation limited)
- **2D spatiotemporal attention preservation**: Low confidence (contribution confirmed but preservation claim lacks direct comparison)

## Next Checks
1. **Expert utilization analysis**: Monitor MoE expert routing distributions; verify no single expert receives >80% of routing weight and at least 3-4 experts are actively utilized.
2. **Hyperedge sensitivity sweep**: Systematically vary M1 and M2 on validation; plot Sharpe ratio and accuracy curves to identify performance plateaus or degradation.
3. **Temporal regime robustness**: Retrain on shifted data windows (e.g., 2018-2022 vs 2019-2023) to assess MAGE's regime adaptation vs overfitting.