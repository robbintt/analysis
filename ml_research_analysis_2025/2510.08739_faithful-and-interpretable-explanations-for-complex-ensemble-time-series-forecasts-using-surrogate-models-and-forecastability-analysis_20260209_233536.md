---
ver: rpa2
title: Faithful and Interpretable Explanations for Complex Ensemble Time Series Forecasts
  using Surrogate Models and Forecastability Analysis
arxiv_id: '2510.08739'
source_url: https://arxiv.org/abs/2510.08739
tags:
- series
- surrogate
- explanations
- time
- forecastability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of explaining forecasts from complex
  ensemble time series models (like AutoGluon) that are inherently black-box and difficult
  to interpret. The core method uses a surrogate-based approach: a LightGBM model
  is trained to mimic the AutoGluon forecasts with high fidelity, and then TreeSHAP
  is applied to the surrogate to generate interpretable feature attributions.'
---

# Faithful and Interpretable Explanations for Complex Ensemble Time Series Forecasts using Surrogate Models and Forecastability Analysis

## Quick Facts
- arXiv ID: 2510.08739
- Source URL: https://arxiv.org/abs/2510.08739
- Reference count: 4
- Primary result: Surrogate-based method with LightGBM + TreeSHAP achieves 0.961 Pearson correlation in faithfulness validation for explaining complex ensemble forecasts

## Executive Summary
This paper addresses the challenge of interpreting forecasts from black-box ensemble models like AutoGluon-TimeSeries by introducing a surrogate-based explanation approach. The method trains a LightGBM model to mimic AutoGluon's predictions, then applies TreeSHAP to generate interpretable feature attributions. The study demonstrates that per-item normalization is essential for meaningful explanations across heterogeneous time series, and introduces spectral predictability analysis as a confidence metric that correlates with both forecast accuracy and explanation reliability.

## Method Summary
The approach uses a surrogate model pipeline: first, AutoGluon-TimeSeries generates forecasts on M5 data (Store-Department level, 70 series). Per-item Z-score normalization is applied to these forecasts, then LightGBM is trained to predict the normalized values using engineered features (calendar, lags, rolling statistics, decomposition). TreeSHAP is applied to extract feature attributions from the surrogate. The method is validated through feature injection experiments where synthetic features with known effects are added, and SHAP values are compared against ground truth. Spectral predictability scores are computed for each series to quantify forecastability and serve as a confidence filter.

## Key Results
- Surrogate model achieves high fidelity to AutoGluon forecasts with tight clustering around identity line
- Feature injection experiment shows 0.961 Pearson correlation between SHAP values and known ground truth effects
- Per-item normalization is necessary to produce meaningful, comparable SHAP values across series with different scales
- Spectral predictability scores correlate strongly with both forecast accuracy and surrogate fidelity
- Low spectral predictability indicates series with intrinsic unpredictability that limits forecast and explanation reliability

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Fidelity Enables Stable SHAP Explanations
Training LightGBM to mimic AutoGluon-TS predictions allows TreeSHAP to generate stable feature attributions when direct explanation methods fail on heterogeneous ensembles. The surrogate decouples the forecasting task from the explanation task, producing consistent Shapley values. Core assumption: high fidelity between surrogate and original model implies that feature attributions on the surrogate meaningfully reflect the original model's behavior. Evidence: predicted values tightly cluster around identity line with high fidelity metrics.

### Mechanism 2: Per-Item Normalization Aligns SHAP Base Values with Local Scale
Z-score normalization per time series is necessary for SHAP explanations to produce meaningful, comparable feature attributions across heterogeneous series. Per-item normalization sets the SHAP base value to each item's own mean, preventing compensatory SHAP values that would otherwise absorb scale mismatches. Attributions then reflect relative deviations from item-specific baselines rather than global averages.

### Mechanism 3: Spectral Predictability Correlates with Both Forecast Accuracy and Surrogate Fidelity
Spectral predictability scores, benchmarked against noise, predict where both forecasts and their explanations are reliable. Higher scores indicate more regular patterns and correlate positively with AutoGluon accuracy and surrogate fidelity. Core assumption: time series with spectral predictability near or below noise baselines have intrinsic unpredictability that limits both forecast quality and explanation reliability.

## Foundational Learning

- **Concept: Shapley Additive Explanations (SHAP) and TreeSHAP**
  - Why needed here: The entire explanation pipeline relies on computing SHAP values from the LightGBM surrogate. Understanding additivity and how TreeSHAP exploits tree structure is essential.
  - Quick check question: If a feature has SHAP value +5 and the base value is 100, what is that feature's contribution to the prediction?

- **Concept: Surrogate Models in XAI**
  - Why needed here: The method hinges on using an interpretable surrogate to approximate a black-box ensemble. You must understand fidelity vs. faithfulness.
  - Quick check question: What is the difference between surrogate fidelity and explanation faithfulness, and which does the feature injection experiment validate?

- **Concept: Spectral Analysis and Entropy-Based Predictability**
  - Why needed here: Forecastability analysis uses spectral entropy to quantify intrinsic predictability before modeling. Understanding power spectral density and entropy as a regularity measure is key.
  - Quick check question: Would a pure sine wave have high or low spectral entropy, and what would that imply for its spectral predictability score?

## Architecture Onboarding

- **Component map:**
  1. Feature Engineering (X_eng) -> 2. Per-Item Normalization -> 3. AutoGluon-TS Ensemble -> 4. LightGBM Surrogate -> 5. TreeSHAP Explainer -> 6. Faithfulness Validator -> 7. Spectral Predictability Module -> 8. Optional Calibration

- **Critical path:**
  1. Compute per-item normalization parameters (μᵢ, σᵢ)
  2. Train AutoGluon-TS on raw data → generate point forecasts
  3. Normalize AutoGluon outputs → train LightGBM surrogate
  4. Compute spectral predictability for each series
  5. Run TreeSHAP on surrogate → generate attributions
  6. Validate faithfulness (feature injection) and fidelity (prediction alignment)
  7. (Optional) Calibrate SHAP to match raw AutoGluon predictions

- **Design tradeoffs:**
  - Limited lag features vs. richer temporal context: Paper deliberately restricts lags to avoid diluting SHAP explanations
  - Per-item vs. global normalization: Per-item enables cross-series comparability but adds complexity
  - Faithfulness validation on injected feature only: Feature injection validates main effects well; interaction effects remain unvalidated

- **Failure signatures:**
  - Low fidelity (R² < 0.7): Surrogate doesn't mimic ensemble; check spectral predictability—if near noise, series may be inherently unpredictable
  - Faithfulness correlation < 0.8: Feature injection SHAP doesn't track known effects; check surrogate training convergence
  - SHAP values dominate by scale: Forgot per-item normalization; base value mismatch forces compensatory attributions
  - Calibration division by zero: Surrogate predicts exactly base value while AutoGluon doesn't

- **First 3 experiments:**
  1. **Fidelity sanity check:** Train surrogate on normalized AutoGluon predictions, plot predicted vs. actual, compute MAE/MAPE/RMSE per series
  2. **Feature injection faithfulness test:** Inject synthetic feature with known linear effect, retrain surrogate, extract SHAP for injected feature, compute Pearson correlation vs. ground truth
  3. **Spectral predictability stratification:** Bin series by SP score relative to noise benchmark, compare forecast accuracy and surrogate fidelity across bins

## Open Questions the Paper Calls Out
- Can the surrogate-based approach faithfully capture complex interaction effects between features, or is the validated faithfulness limited strictly to main effects?
- Is there a theoretically grounded method to calibrate surrogate explanations to the base model's predictions, rather than the current heuristic scaling approach?
- Does the deliberate exclusion of extensive lag features in the surrogate model degrade its ability to mimic the auto-regressive patterns of the complex ensemble?

## Limitations
- Feature injection validation only tests main effects, not complex interaction effects between features
- Spectral predictability metric may not generalize to non-retail domains with different noise characteristics
- Current calibration approach lacks theoretical support and constitutes a heuristic adjustment

## Confidence
- **High confidence:** Surrogate fidelity enables stable SHAP explanations; per-item normalization is necessary for cross-scale comparability; spectral predictability correlates with forecast accuracy
- **Medium confidence:** Spectral predictability can serve as a pre-modeling confidence filter; limited lag features are optimal for SHAP interpretability
- **Low confidence:** SHAP explanations fully capture AutoGluon's decision logic; spectral predictability generalizes across domains

## Next Checks
1. **Faithfulness for interactions:** Design feature injection experiment with synthetic feature whose effect depends on another feature to test if SHAP captures interactions
2. **Spectral predictability domain transfer:** Apply spectral predictability pipeline to non-retail dataset to verify generalizability
3. **Normalization robustness:** Test alternative normalization strategies and measure impact on SHAP interpretability and cross-series comparability