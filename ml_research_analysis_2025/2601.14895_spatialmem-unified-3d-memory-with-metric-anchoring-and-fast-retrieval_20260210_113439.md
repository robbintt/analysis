---
ver: rpa2
title: 'SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval'
arxiv_id: '2601.14895'
source_url: https://arxiv.org/abs/2601.14895
tags:
- scene
- memory
- object
- retrieval
- anchors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpatialMem is a memory-centric system that builds a unified 3D
  spatial memory from casual egocentric RGB video, integrating geometry, semantics,
  and language into a single queryable representation. It reconstructs metrically
  scaled indoor environments, detects structural 3D anchors (walls, doors, windows),
  and populates a hierarchical memory with open-vocabulary object nodes linked to
  3D coordinates.
---

# SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval

## Quick Facts
- arXiv ID: 2601.14895
- Source URL: https://arxiv.org/abs/2601.14895
- Reference count: 40
- Unified 3D spatial memory from egocentric RGB video with metric anchoring and hierarchical retrieval

## Executive Summary
SpatialMem presents a memory-centric system that constructs a unified 3D spatial memory from casual egocentric RGB video by integrating geometry, semantics, and language into a single queryable representation. The system reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows), and populates a hierarchical memory with open-vocabulary object nodes linked to 3D coordinates. It supports interpretable reasoning over spatial relations and enables tasks like language-guided navigation and object retrieval without requiring specialized sensors.

The approach demonstrates strong performance on navigation and retrieval tasks across three real-life indoor scenes, achieving 0.89 step completion and 0.72 retrieval success in the simplest scene. The system maintains efficiency and robustness under increasing clutter and occlusion while providing metrically accurate reconstructions that enable precise spatial reasoning. The integration of large language models enables complex reasoning capabilities while maintaining interpretability through explicit 3D spatial relationships.

## Method Summary
SpatialMem builds a unified 3D spatial memory from egocentric RGB video by first reconstructing metrically scaled indoor environments, then detecting structural 3D anchors (walls, doors, windows) to establish a spatial framework. The system populates a hierarchical memory with open-vocabulary object nodes linked to precise 3D coordinates, creating a queryable representation that integrates geometric, semantic, and linguistic information. The architecture supports interpretable reasoning over spatial relations through explicit 3D anchoring and enables tasks like language-guided navigation and object retrieval without specialized sensors.

## Key Results
- Achieves 0.89 step completion rate for navigation tasks in simplest scene
- Demonstrates 0.72 retrieval success rate for object finding in simplest scene
- Shows hierarchical memory structure with 0.72 to 0.34 retrieval accuracy across increasing scene complexity

## Why This Works (Mechanism)
The system's effectiveness stems from its unified approach to integrating multiple modalities (geometry, semantics, language) into a single 3D spatial memory framework. By anchoring all information to metrically accurate 3D coordinates and structural elements, the system enables consistent reasoning across different types of queries and tasks. The hierarchical memory structure allows efficient organization and retrieval of spatial information while maintaining interpretability through explicit spatial relationships.

## Foundational Learning
- 3D Reconstruction from RGB Video: Essential for creating metrically accurate spatial representations from monocular input
  - Why needed: Enables metric anchoring without specialized sensors
  - Quick check