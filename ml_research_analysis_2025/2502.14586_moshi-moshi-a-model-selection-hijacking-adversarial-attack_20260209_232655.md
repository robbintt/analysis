---
ver: rpa2
title: Moshi Moshi? A Model Selection Hijacking Adversarial Attack
arxiv_id: '2502.14586'
source_url: https://arxiv.org/abs/2502.14586
tags:
- attack
- selection
- learning
- metric
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MOSHI (MOdel Selection HIjacking) is the first adversarial attack
  targeting model selection in machine learning. It manipulates the validation set
  using a novel Hijacking VAE (HVAE) that generates adversarial samples optimized
  to sway the model selection process toward a model with specific properties defined
  by a hijack metric.
---

# Moshi Moshi? A Model Selection Hijacking Adversarial Attack

## Quick Facts
- arXiv ID: 2502.14586
- Source URL: https://arxiv.org/abs/2502.14586
- Reference count: 36
- Primary result: First adversarial attack targeting model selection by poisoning validation sets with HVAE-generated samples, achieving 75.42% ASR across computer vision and speech recognition benchmarks

## Executive Summary
MOSHI (MOdel Selection HIjacking) is the first adversarial attack targeting model selection in machine learning. It manipulates the validation set using a novel Hijacking VAE (HVAE) that generates adversarial samples optimized to sway the model selection process toward a model with specific properties defined by a hijack metric. Tested across computer vision (MNIST, CIFAR10) and speech recognition (Speech Commands) benchmarks, MOSHI achieved an average Attack Success Rate of 75.42%. The attack caused an 88.30% decrease in generalization capabilities, an 83.33% increase in latency, and up to 105.85% increase in energy consumption. The attack is effective in both white-box and black-box scenarios, demonstrating practical feasibility across diverse real-world applications.

## Method Summary
MOSHI operates by training a Hijacking VAE (HVAE) to generate adversarial samples that differentially penalize models during validation. The HVAE loss function balances reconstruction, KL divergence, and a hijacking cost term that assigns higher penalties to models with lower hijack metric values. These generated samples are injected into the validation set, causing models with higher hijack metrics to appear to have better validation performance and thus be selected by the victim's model selection process. The attack works by substituting the clean validation set with poisoned samples at varying rates (10-100%), leaving the training set unaltered to maintain stealth.

## Key Results
- Average Attack Success Rate of 75.42% across all tested datasets and metrics
- 88.30% decrease in generalization capabilities when targeting generalization error metric
- 83.33% increase in latency when targeting inference time optimization
- Up to 105.85% increase in energy consumption when targeting energy efficiency

## Why This Works (Mechanism)

### Mechanism 1: HVAE Adversarial Sample Generation
A modified conditional VAE can generate samples that differentially penalize models based on attacker-specified metrics. The Hijacking VAE (HVAE) loss function balances three competing objectives: L_HVAE = (L_rec + L_KLD - Hj_cost(C))². The novel Hj_cost term assigns per-model penalties via α coefficients, computed from normalized hijack metric differences. Models with lower hijack metrics receive higher α penalties, causing generated samples to increase their validation loss disproportionately. By minimizing this loss, the HVAE learns to produce samples that selectively harm low-metric models while preserving high-metric model performance on those same samples.

### Mechanism 2: Validation Set-Only Injection
Manipulating only the validation set (not training data) suffices to hijack model selection while remaining undetectable. The attacker substitutes S_val with S_val_pois containing adversarial samples. Since training proceeds normally on clean S_train, all models learn legitimate representations. The attack exploits the trust placed in validation performance as a proxy for generalization. When S_val_pois is used in model selection, models with higher hijack metrics appear to have lower validation loss, causing their selection despite inferior true generalization.

### Mechanism 3: Transferability-Based Black-Box Attack
Adversarial samples generated using surrogate models transfer to unknown target models. In black-box settings, the attacker trains HVAE using surrogate models (same architectures, different random initialization or training). The generated samples exploit shared vulnerability patterns across models—transferability—causing differential validation loss even on unseen target models. The α coefficients are computed on surrogates but remain effective on targets.

## Foundational Learning

- **Variational Autoencoders (VAEs)**: Why needed here: HVAE extends conditional VAE architecture; understanding encoder-decoder structure, latent space regularization (KL divergence), and reconstruction loss is prerequisite to grasping how Hj_cost modifies the generative objective. Quick check question: Can you explain why VAEs enforce a continuous prior p(z) ≈ N(0,I) rather than memorizing discrete latent codes?

- **Model Selection and Validation**: Why needed here: The attack targets hyperparameter selection via validation loss; understanding hold-out validation, grid search, and the assumption that validation error approximates true error is essential. Quick check question: Why does hold-out validation estimate true error (L_D) rather than directly measuring it?

- **Adversarial Transferability**: Why needed here: Black-box MOSHI relies on samples crafted for surrogate models generalizing to target models; understanding why cross-model vulnerability exists is critical. Quick check question: If Model A and Model B have different architectures but were trained on the same data, would you expect adversarial examples to transfer? What factors affect this?

## Architecture Onboarding

- **Component map**: HVAE Encoder -> HVAE Decoder -> Hj_cost Module -> Model Grid -> Validation Injector
- **Critical path**: 1) Train model grid on clean S_train → obtain {h_c}; 2) Compute α coefficients for each model based on hijack metric ranking; 3) Train HVAE using modified loss with Hj_cost feedback; 4) Generate S_val_pois from trained HVAE decoder; 5) Inject S_val_pois into victim's validation set; 6) Victim's model selection returns argmin_c L_val(h_c, S_val_pois) = suboptimal model per attacker's goal
- **Design tradeoffs**: Reconstruction vs. Hijacking (higher Hj_cost weight improves attack success but generates unrecognizable samples); White-box vs. Black-box (white-box requires full model access but guaranteed effectiveness; black-box uses surrogates with transferability uncertainty); Full vs. Partial Substitution (full substitution maximizes ASR but is conspicuous; partial maintains stealth but reduces ESF)
- **Failure signatures**: Low ASR on complex data (Speech Commands shows 0% ASR for Latency/Energy metrics — HVAE struggles with sequential domains); Negative ESF (some black-box Speech Commands attacks return models with lower hijack metrics than legitimate selection — transferability failure); Sample quality degradation (generated samples visually bear no resemblance to originals — exploitable by anomaly detectors)
- **First 3 experiments**: 1) Reproduce MNIST Global White-Box: Train 180 FFNN models, compute α for Generalization metric, train HVAE for 50 epochs, generate S_val_pois, verify ASR approaches 100%; 2) Test Partial Substitution Robustness: Using same MNIST setup, inject S_val_pois at 10%, 25%, 50% rates. Measure ESF degradation curve; 3) Black-Box Transferability Probe: Train surrogate models with different random seeds than targets. Measure ASR gap between white-box and black-box for CIFAR10 Latency metric

## Open Questions the Paper Calls Out

### Open Question 1
Can domain-specific generative architectures improve MOSHI's success rate in complex tasks like speech recognition? The paper notes that future studies might attempt to design ad hoc HVAE for specific tasks (e.g., RAVE for the speech domain). The generic HVAE failed to effectively hijack the Speech Commands model selection, resulting in erratic behavior and low success rates compared to computer vision tasks. Implementing a specialized HVAE for audio that achieves consistent positive ESF and high ASR on the Speech Commands benchmark would resolve this.

### Open Question 2
What is the precise relationship between validation set poisoning ratios and the Effectiveness Score Function (ESF)? The paper notes that the interplay between MOSHI effectiveness and the poisoning rate should be explored in future works. The partial substitution analysis revealed erratic trends, such as strongly negative ESF values at certain poisoning rates in the Speech Commands dataset, obscuring the optimal attack strategy. A systematic study mapping ASR and ESF across a granular range of poisoning percentages (e.g., 0-100%) for all datasets would resolve this.

### Open Question 3
Can the HVAE loss function be modified to generate adversarial samples that are visually indistinguishable from clean data? The paper hypothesizes that detection defenses (like MagNet) could mitigate the current attack because generated samples have "little to no resemblance" to originals. The current loss function sacrifices reconstruction fidelity to maximize the hijacking cost, creating a trade-off that leaves the attack vulnerable to filtering. Demonstrating an attack that maintains high ASR while successfully evading a detector trained to distinguish clean validation samples from HVAE outputs would resolve this.

## Limitations
- HVAE architecture details and training hyperparameters remain underspecified, making exact reproduction uncertain
- Generated samples qualitatively diverge from original data, raising detection concerns and limiting stealth
- Performance on sequential data (Speech Commands) shows significant limitations with 0% ASR for latency and energy metrics
- Black-box transferability shows inconsistent results across configurations, with unclear conditions for successful transfer

## Confidence
- **High confidence**: The core mechanism of HVAE-based adversarial sample generation for differential model penalization is well-defined and mathematically grounded. The relationship between validation set poisoning and model selection manipulation is theoretically sound.
- **Medium confidence**: The reported ASR values and effectiveness metrics are plausible given the methodology, though implementation details gaps make exact reproduction uncertain. The differential impact on generalization, latency, and energy metrics is supported by experimental results.
- **Low confidence**: The generalizability of the attack across diverse datasets and model selection scenarios is not fully established. The speech recognition results show significant limitations, and black-box transferability behavior lacks clear characterization of success conditions.

## Next Checks
1. **Quantitative sample quality analysis**: Measure Fréchet Distance between distributions of original and MOSHI-generated samples for each dataset to objectively assess detection risk.
2. **Architectural ablation study**: Test HVAE performance using alternative architectures (e.g., RAVE for sequential data) to isolate whether 1D convolution limitations cause Speech Commands failures.
3. **Transferability conditions investigation**: Systematically vary surrogate-target model architectural similarity and training data overlap to identify precise conditions enabling successful black-box attacks.