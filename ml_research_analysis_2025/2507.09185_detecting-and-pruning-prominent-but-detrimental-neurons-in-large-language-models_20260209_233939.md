---
ver: rpa2
title: Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models
arxiv_id: '2507.09185'
source_url: https://arxiv.org/abs/2507.09185
tags:
- pruning
- neurons
- performance
- samples
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes detecting and pruning "detrimental" neurons
  in transformer MLPs that drive high-confidence predictions via dataset-specific
  correlations rather than generalizable reasoning. The method uses Integrated Gradients
  to quantify each neuron's influence on confident predictions and prunes the most
  attributed neurons layer-wise based on validation performance.
---

# Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models

## Quick Facts
- arXiv ID: 2507.09185
- Source URL: https://arxiv.org/abs/2507.09185
- Authors: Ameen Ali; Shahar Katz; Lior Wolf; Ivan Titov
- Reference count: 38
- Primary result: DSM neuron pruning improves accuracy over baseline and outperforms prior adaptation methods while using only ~10 labeled samples.

## Executive Summary
This paper introduces a method to detect and prune "detrimental" neurons in transformer MLPs that drive high-confidence predictions via dataset-specific correlations rather than generalizable reasoning. The approach uses Integrated Gradients to quantify each neuron's influence on confident predictions and prunes the most attributed neurons layer-wise based on validation performance. Across six multiple-choice benchmarks and three model families (LLaMA, Mistral, Qwen), pruning improves accuracy over the baseline and outperforms prior adaptation methods while using only ~10 labeled samples. Random pruning yields negligible gains, confirming the importance of targeted DSM neuron removal.

## Method Summary
The method employs Integrated Gradients to quantify each neuron's influence on high-confidence predictions, pinpointing those that disproportionately contribute to dataset-specific performance without supporting robust, transferable reasoning. It defines neuron importance via IG attribution, aggregates scores across 10 random samples, and performs grid search over layers and pruning percentages (5-40%) to identify the configuration maximizing validation accuracy. The top-attributed neurons in the optimal layer are then pruned by zeroing their weights. Unlike traditional pruning approaches which target redundant or low-importance parameters, this method deliberately identifies and removes the most influential neurons—those driving high-confidence but non-transferable predictions.

## Key Results
- Pruning improves accuracy over baseline and outperforms prior adaptation methods while using only ~10 labeled samples
- Random pruning yields negligible gains, confirming the importance of targeted DSM neuron removal
- Results show robustness to adaptation-set selection and hyperparameter choices, with peak performance typically achieved at moderate pruning levels (15-25%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neurons with high attribution to the model's maximum logit prediction encode dataset-specific shortcuts (DSMs) that harm generalization.
- Mechanism: Integrated Gradients quantifies each neuron's contribution to the maximum logit. Neurons that disproportionately drive confident predictions without supporting transferable reasoning are identified as DSM neurons. Pruning them suppresses reliance on spurious correlations.
- Core assumption: High IG attribution correlates with shortcut behavior rather than robust reasoning; this relationship varies by layer and task.
- Evidence anchors:
  - [abstract] "Our method employs Integrated Gradients to quantify each neuron's influence on high-confidence predictions, pinpointing those that disproportionately contribute to dataset-specific performance without supporting robust, transferable reasoning."
  - [section 3.3] "We define the Integrated Gradients (IG) attribution for neuron n_j by interpolating its weight from a baseline value of 0 to ŵ_j... The term ∂M(q;αŵ_j)/∂w_j captures the sensitivity of the model's output to the modified neuron weight."
  - [corpus] Related work on knowledge neurons (Dai et al., 2022) supports the concept of specialized neurons encoding specific information patterns.
- Break condition: If high-attribution neurons encode task-critical knowledge rather than shortcuts, pruning degrades performance (observed in some layers during grid search).

### Mechanism 2
- Claim: Optimal pruning occurs at a single MLP layer (gate_proj), determined via grid search over layers and pruning percentages.
- Mechanism: Rather than pruning across all layers, the method performs a grid search (5-40% pruning per layer) and selects the (layer, percentage) pair maximizing validation accuracy. This localizes intervention where DSM neurons concentrate.
- Core assumption: DSM neurons cluster in specific layers rather than being distributed uniformly; gate_proj's role in gating knowledge activation makes it particularly susceptible.
- Evidence anchors:
  - [section 3.4] "Since these attributions are derived from the model's output, pruning highly attributed neurons at random may degrade performance. However, in some layers, these neurons contribute to spurious correlations."
  - [section E.2] "The results demonstrate that pruning the gate_proj layer consistently yields the highest performance gains. On BoolQ, gate_proj achieves +7.16% improvement, compared to +6.65% for up_proj and +4.34% for down_proj."
  - [corpus] Limited direct corpus evidence on single-layer vs. multi-layer pruning trade-offs.
- Break condition: If DSM neurons are distributed across multiple layers, single-layer pruning provides diminishing returns; tasks may require different layers.

### Mechanism 3
- Claim: Removing DSM neurons forces the model to rely on alternative, more generalizable representations already present in the network.
- Mechanism: Pruning does not add new knowledge but removes a dominant pathway, allowing dormant but more robust reasoning circuits to contribute to predictions. This is analogous to synaptic pruning in biological neural development.
- Core assumption: LLMs encode multiple solution pathways during pretraining; DSM pathways suppress but do not eliminate robust pathways.
- Evidence anchors:
  - [abstract] "Selectively pruning these neurons compels the model to depend on generalizable representations."
  - [section 6] "Unlike traditional pruning approaches which target redundant or low-importance parameters, our method deliberately identifies and removes the most influential neurons—those driving high-confidence but non-transferable predictions."
  - [corpus] The "Knowledge Neurons" hypothesis (Dai et al., 2022) supports the idea of localized information encoding, though direct evidence for dormant robust pathways remains limited.
- Break condition: If robust pathways were never learned during pretraining, pruning simply degrades performance without compensation.

## Foundational Learning

- Concept: **Integrated Gradients (IG)**
  - Why needed here: Core attribution method for ranking neuron importance. Requires understanding of gradient-based attribution, baseline selection, and interpolation paths.
  - Quick check question: Can you explain why IG uses an interpolation path from baseline to input rather than a single gradient computation?

- Concept: **Transformer MLP Architecture (gate_proj, up_proj, down_proj)**
  - Why needed here: The method targets specific MLP sub-layers; understanding their roles is essential for selecting intervention points.
  - Quick check question: In a SwiGLU-style MLP, what is the functional role of gate_proj compared to up_proj?

- Concept: **Shortcut Learning and Spurious Correlations**
  - Why needed here: Motivates the problem—understanding why high-confidence predictions may not reflect genuine understanding.
  - Quick check question: Give an example of a spurious correlation a model might learn in a sentiment classification task.

## Architecture Onboarding

- Component map:
  Input Token → [Embedding] → [Transformer Block × L] → Output Logits
                      ↓
              Each Block: Attention → MLP
                                ↓
                    MLP: gate_proj → activation → down_proj
                              ↑
                         up_proj (parallel)
                              ↓
                    [This paper: IG on gate_proj neurons]

- Critical path:
  1. Select 10 random samples from target task (no labels needed for IG)
  2. Compute IG attribution for each neuron in gate_proj across all layers (~450K neurons for LLaMA-3.1-8B)
  3. Aggregate scores per neuron across samples
  4. Grid search: for each layer × pruning percentage (5-40%), prune top-attributed neurons and evaluate on held-out set
  5. Select (layer, percentage) pair with highest validation accuracy

- Design tradeoffs:
  - Single-layer vs. multi-layer pruning: Single-layer is simpler and cheaper; may miss distributed DSMs
  - Pruning percentage: Higher percentages risk removing useful neurons; lower may leave shortcuts intact
  - Sample selection: Random works well; correctly-predicted samples may improve results (Table 8)

- Failure signatures:
  - Random pruning baseline shows no improvement (Table 1-2)—if you see gains from random pruning, check implementation
  - Performance drops when pruning essential layers—grid search should detect this
  - High variance across random seeds suggests insufficient samples or noisy task

- First 3 experiments:
  1. Replicate ablation (Figure 2) on a held-out task: vary IG samples (5, 10, 20, 50) and confirm 10 samples is sufficient
  2. Compare gate_proj vs. up_proj vs. down_proj pruning on a new model to validate layer selection
  3. Test transfer: apply pruning configuration learned from one task (e.g., SST2) to a related task (e.g., SST5) to assess cross-task generalization of DSM neurons

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does DSM neuron pruning generalize to non-multiple-choice tasks such as open-ended generation, summarization, or code generation?
- Basis in paper: [explicit] The paper states "Our work focuses on the elimination of such domain-specific mechanisms (DSM) as part of a fine-tuning process, specifically in the context of multiple-choice tasks."
- Why unresolved: All experiments are restricted to multiple-choice benchmarks (XNLI, MMLU, SST2/5, BoolQ, COPA); no evaluation on generative tasks.
- What evidence would resolve it: Apply the pruning method to generative benchmarks (e.g., summarization, translation, code completion) and compare performance against baselines.

### Open Question 2
- Question: Why does the optimal pruning layer vary across tasks and model architectures?
- Basis in paper: [inferred] The method uses grid search to identify optimal layers, suggesting no predictable pattern for which layer yields best results.
- Why unresolved: The paper does not analyze or hypothesize why certain layers benefit more from pruning for specific tasks.
- What evidence would resolve it: Systematic analysis correlating layer-wise DSM neuron distributions with task characteristics; interpretability studies of pruned layers across tasks.

### Open Question 3
- Question: Can multi-layer pruning strategies provide additional gains over single-layer pruning?
- Basis in paper: [explicit] "Our method is based on the following observations: (i) to eliminate a DSM, it is sufficient to prune certain neurons within a single layer"—sufficient, but not necessarily optimal.
- Why unresolved: The paper restricts pruning to a single layer without comparing against simultaneous multi-layer pruning.
- What evidence would resolve it: Experiments comparing single-layer vs. joint multi-layer pruning across benchmarks to measure marginal improvements or trade-offs.

## Limitations

- The attribution method assumes IG scores reliably identify shortcut neurons rather than task-relevant knowledge, but the paper doesn't validate whether high-scoring neurons truly encode spurious correlations
- The 10-sample requirement, while empirically effective, lacks theoretical justification for why this quantity suffices across diverse tasks and model scales
- The grid search selects pruning configurations based on validation accuracy without examining whether pruned neurons correspond to interpretable patterns

## Confidence

- **High confidence:** Single-layer gate_proj pruning consistently outperforms multi-layer approaches and random pruning across all tested models and tasks. The superiority of targeted DSM neuron removal over random pruning is clearly demonstrated.
- **Medium confidence:** The claim that DSM neurons encode dataset-specific shortcuts rather than generalizable reasoning is mechanistically plausible but relies on correlation with attribution scores rather than direct causal validation of neuron semantics.
- **Medium confidence:** The assertion that pruning forces reliance on alternative representations is supported by improved accuracy but not directly verified through analysis of remaining pathway activations.

## Next Checks

1. **Attribution validity check:** Manually inspect high-attribution neurons by ablating them individually and observing prediction changes to verify they encode dataset-specific shortcuts rather than task-critical knowledge.

2. **Cross-task generalization test:** Apply DSM pruning configurations learned from one task family (e.g., sentiment analysis) to semantically distinct tasks (e.g., QA) to test whether DSM neurons generalize across related domains.

3. **Robustness to sample selection:** Compare performance when using 10 random samples versus 10 correctly-predicted samples versus stratified sampling across model confidence ranges to quantify sensitivity to adaptation set selection.