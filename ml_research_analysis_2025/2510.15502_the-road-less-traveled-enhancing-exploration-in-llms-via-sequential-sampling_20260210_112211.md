---
ver: rpa2
title: 'The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling'
arxiv_id: '2510.15502'
source_url: https://arxiv.org/abs/2510.15502
tags:
- sampling
- sequential
- parallel
- exploration
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the exploration challenge in RL-trained LLMs,
  where models converge to narrow solution strategies and lose diversity. The authors
  propose SESA, a sequential sampling framework that generates diverse method sketches
  one by one, conditioning each new output on previous ones, then expands each sketch
  into full solutions in parallel.
---

# The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling

## Quick Facts
- arXiv ID: 2510.15502
- Source URL: https://arxiv.org/abs/2510.15502
- Reference count: 8
- Key outcome: Sequential sampling framework improves exploration in RL-trained LLMs, achieving up to 211% relative improvement over baseline methods on agent and reasoning tasks

## Executive Summary
This paper addresses the exploration challenge in RL-trained LLMs, where models converge to narrow solution strategies and lose diversity. The authors propose SESA, a sequential sampling framework that generates diverse method sketches one by one, conditioning each new output on previous ones, then expands each sketch into full solutions in parallel. This approach maintains greater exploration and prevents policy collapse. On three agent benchmarks, SESA improves success rates by +0.25, +0.42, and +0.07 absolute over the base model, representing up to 211% relative improvement over baseline RL methods. The method also shows significant improvements on Sudoku and math tasks, demonstrating its effectiveness in maintaining solution diversity during RL training.

## Method Summary
SESA is a two-stage sequential sampling framework designed to enhance exploration in RL-trained LLMs. Stage I generates G short method sketches sequentially, with each sketch conditioned on previously generated ones. Stage II expands each sketch into a full solution in parallel. The framework uses GRPO/DAPO-style clipped PPO objective with advantages computed from rewards. For agent tasks, sequential action-level sampling with global trajectory cap U is applied. The method addresses the entropy collapse problem in RL by maintaining diversity through conditional generation, preventing the model from converging to a single solution strategy.

## Key Results
- Sequential-10 setting discovered 80 distinct paths vs 11 for parallel sampling at 500 total samples
- SESA improved Sudoku Pass@32 by 6% and AIME24 Pass@k by 9% over RAGEN baseline
- Pass@16/Pass@1 ratio recovery demonstrated: sequential sampling increased ratio from 1.4 to 2.8 when applied to collapsed checkpoint
- Three agent benchmarks showed +0.25, +0.42, and +0.07 absolute improvement in success rates

## Why This Works (Mechanism)

### Mechanism 1
Sequential conditioning during sampling increases output diversity compared to parallel sampling from the same distribution. Each new sample is conditioned on previously generated candidates: y(i) ~ πθ(·|x, y(1),...,y(i-1)). This explicitly steers the model away from repeating prior outputs, whereas parallel sampling draws all candidates i.i.d. from πθ(·|x). Core assumption: The base model can follow in-context instructions to generate different outputs when shown prior candidates. Evidence: Sequential-10 discovered 80 distinct paths vs 11 for parallel sampling at 500 samples.

### Mechanism 2
A two-stage process (sequential sketches → parallel expansion) preserves diversity while maintaining computational efficiency for complex tasks. Stage I generates G short method sketches sequentially, each conditioned on prior sketches. Stage II expands each sketch into a full solution in parallel. This bounds latency and context length while ensuring each solution follows a distinct strategy. Core assumption: Sketches are short enough that sequential generation does not create prohibitive overhead, and each sketch maps to a meaningfully different solution trajectory. Evidence: Two-stage SESA improved Sudoku Pass@32 by 6% and AIME24 Pass@k by 9% over RAGEN baseline.

### Mechanism 3
Sequential sampling can recover "collapsed" policies that have stagnated under parallel sampling. When Pass@16/Pass@1 ratio approaches 1 (outputs nearly identical), switching to sequential sampling forces diverse candidates, providing new gradient signals for RL to optimize. Core assumption: The model has not entirely lost capacity to generate alternative strategies; they are just suppressed by the current policy mode. Evidence: Resuming training with sequential sampling from a collapsed checkpoint increased Pass@16/Pass@1 ratio and improved accuracy.

## Foundational Learning

**Entropy collapse in RL-trained LLMs**: Why needed here: The paper's core problem is that RL optimization concentrates probability mass on high-reward outputs, reducing diversity. Quick check question: Can you explain why pass@1 can increase while pass@k decreases during RL training?

**GRPO/DAPO group-relative advantage computation**: Why needed here: SESA uses the same advantage computation as GRPO/DAPO but changes how candidates are generated. Quick check question: How is advantage computed when rewards are sparse and only final outcomes are verifiable?

**Exploration-exploitation tradeoff in policy optimization**: Why needed here: Sequential sampling is fundamentally an exploration strategy to prevent premature convergence. Quick check question: Why does parallel sampling from a converged policy provide no useful gradient signal for improvement?

## Architecture Onboarding

**Component map**: Prompt construction → Sequential sketch generation (G iterations) → Parallel solution expansion (G solutions) → Reward computation → Advantage normalization → Policy gradient update

**Critical path**: 1. Prompt construction for sketch generation with history 2. Sequential autoregressive generation of G sketches 3. Parallel expansion of each sketch to full solution 4. Reward computation for each solution 5. Advantage normalization and policy gradient update

**Design tradeoffs**: Higher G increases diversity but linearly increases Stage I latency; longer sketches capture more strategy detail but consume context budget; agent setting: more actions per step increases branching factor exponentially; cap U bounds compute

**Failure signatures**: Pass@k/Pass@1 ratio → 1 during training (policy collapse); sketches become semantically identical despite different phrasing; context overflow errors during long sequential generation; expansion ignores sketch and converges to dominant pattern

**First 3 experiments**: 1. Replicate synthetic path exploration (Section 2) on a small model to confirm diversity gains before full training 2. Ablate sketch length and count (G) on a validation set to find minimal viable sketch configuration 3. Monitor Pass@16/Pass@1 ratio throughout training; if ratio drops below 1.5, trigger switch to sequential sampling or reduce KL penalty

## Open Questions the Paper Calls Out

**Does the sequential sampling advantage scale to models significantly larger than 7B parameters?**: The experiments exclusively utilize the Qwen2.5-7B-Instruct model. Larger models possess different base capacities for reasoning and instruction following; it is unclear if the "policy collapse" dynamics observed in 7B models apply similarly to 70B+ models, or if they can handle the sequential conditioning more effectively without the two-stage requirement. What evidence would resolve it: Evaluation of SESA on 70B+ parameter models (e.g., Llama-3-70B) on the same agent and reasoning benchmarks to compare relative improvements over baselines.

**How robust is the framework to failures in the "method sketch" stage?**: The method relies on Stage I to generate distinct strategies sequentially. If the base model fails to generate meaningfully distinct sketches despite the conditioning history (e.g., due to instruction-following limitations), the parallel expansion in Stage II would yield low diversity, negating the method's benefits. What evidence would resolve it: An ablation study analyzing the correlation between the diversity metric of the generated sketches and the final Pass@k accuracy of the expanded solutions.

**What is the computational overhead of the sequential drafting stage compared to standard parallel sampling?**: The authors split the procedure into two stages to mitigate the latency and context issues of fully sequential generation. While the paper demonstrates performance gains, it does not quantify the wall-clock time or FLOP increase caused by generating sketches sequentially compared to the single-pass efficiency of baseline methods like DAPO. What evidence would resolve it: A comparison of training throughput (samples/second) and total compute cost between SESA and standard parallel baselines to achieve equivalent performance levels.

## Limitations

- Limited generalisability of diversity gains to open-ended generation tasks without verifiable rewards
- Hyperparameter sensitivity with optimal configuration likely varying across tasks and model scales
- Computational overhead of the two-stage process compared to single-stage parallel sampling

## Confidence

**High confidence**: The core mechanism of sequential sampling improving diversity is well-supported by synthetic path exploration experiments showing Coverage@32 increasing from 0.14 (parallel) to 0.80 (sequential). The mathematical formulation of the advantage computation and the two-stage process are clearly specified.

**Medium confidence**: The agent task results show consistent but modest improvements (+0.25 to +0.42 absolute success rate). These gains are statistically meaningful but the practical impact on real-world agent performance requires further validation. The Sudoku and math task improvements are promising but based on single datasets.

**Low confidence**: The claim about sequential sampling "revitalizing" collapsed policies is demonstrated only through a single recovery experiment in Section 5.1. The general applicability of this mechanism to different types of policy collapse is not established.

## Next Checks

1. **Ablation study on sketch parameters**: Systematically vary G (number of sketches) and sketch length on a held-out validation set to identify the minimal configuration that maintains diversity gains. This will help establish practical guidelines for applying SESA to new tasks.

2. **Transfer to non-verifiable tasks**: Apply SESA to creative writing or dialogue generation tasks where rewards are implicit or human-annotated. Compare diversity metrics (distinct n-grams, semantic variety) and quality ratings against parallel sampling baselines.

3. **Long-term training stability**: Monitor Pass@16/Pass@1 ratio throughout extended training runs to verify that sequential sampling maintains diversity without requiring manual intervention. Track whether the model develops alternative collapse modes under prolonged SESA training.