---
ver: rpa2
title: 'DAVE: Diagnostic benchmark for Audio Visual Evaluation'
arxiv_id: '2503.09321'
source_url: https://arxiv.org/abs/2503.09321
tags:
- audio
- multimodal
- video
- gemini
- sound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DAVE is a diagnostic benchmark for evaluating audio-visual understanding
  in multimodal models. It addresses the problem of strong visual bias in existing
  datasets by ensuring questions require both audio and visual information to answer.
---

# DAVE: Diagnostic benchmark for Audio Visual Evaluation

## Quick Facts
- **arXiv ID:** 2503.09321
- **Source URL:** https://arxiv.org/abs/2503.09321
- **Reference count:** 40
- **Primary result:** Models achieve 58.73% accuracy on DAVE vs humans at 84.74%, with significant struggles on sound absence detection and cross-modal temporal alignment

## Executive Summary
DAVE is a diagnostic benchmark designed to evaluate true audio-visual understanding in multimodal models by ensuring questions require both modalities to answer correctly. Unlike existing benchmarks that allow visual-only solutions, DAVE's carefully constructed questions test whether models can perform genuine cross-modal integration, particularly temporal alignment between auditory and visual events. The benchmark reveals significant performance gaps between humans and current state-of-the-art models, especially in detecting sound absence and distinguishing between similar audio cues.

## Method Summary
DAVE consists of 2,426 samples created from Epic Kitchens and Ego4D datasets with synthetic environmental audio overlays from ESC-50 at precise timestamps during visual actions. The benchmark uses a semi-automatic generation pipeline: event extraction with duration constraints, audio overlay with fade-in/out effects, LLM-based narration enhancement, similarity filtering, and visual quality verification. Questions are designed in three types (synchronization, absence detection, discrimination) and evaluated through multiple-choice accuracy with bootstrap standard deviation. The benchmark includes atomic subtasks (action recognition, temporal ordering, audio classification) to diagnose failure modes, and results show significant performance gaps between humans and models across all question types.

## Key Results
- Gemini 2.0 Flash achieves 58.73% accuracy (full multimodal) vs human baseline of 84.74%
- Sound absence detection accuracy drops to 8.50% (Gemini 2.0 Flash) vs 58.50% for humans
- Models show 15-25% performance drop when audio is removed, confirming visual bias elimination
- Action recognition outperforms multimodal synchronization (76.8% vs 49.51% for Gemini 1.5 Flash), indicating integration bottleneck

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DAVE's dual-modality necessity constraint forces models to perform genuine cross-modal integration rather than exploiting single-modality shortcuts.
- Mechanism: Each question is constructed so that audio alone and video alone provide insufficient information. Audio-visual samples are created by overlaying environmental sounds (e.g., siren, coughing) at precise timestamps during specific visual actions, then asking which action coincides with the queried sound. The model must (1) localize the audio event in time, (2) identify the visual action at that same temporal window, and (3) bind them correctly.
- Core assumption: Models that achieve high accuracy are performing true temporal alignment rather than learning spurious correlations (e.g., associating longer video segments with audio presence).
- Evidence anchors:
  - [abstract] "ensuring both modalities are necessary to answer correctly"
  - [Section 3.1] "We randomly select an environmental sound class... The selected audio is precisely aligned with the event's temporal boundaries to ensure synchronization"
  - [Section 4.4] "Gemini 2.0 Flash achieves 50.85% accuracy with full multimodal input but drops to 37.14% with only video and text"
  - [corpus] Daily-Omni paper explicitly addresses "temporal alignment across modalities" - confirming this is a recognized challenge

### Mechanism 2
- Claim: Decomposing audio-visual reasoning into atomic subtasks enables precise diagnosis of where integration fails.
- Mechanism: DAVE separates evaluation into: (1) action recognition (visual-only), (2) temporal ordering (video sequence without audio), (3) audio classification (audio-only), and (4) composite multimodal synchronization. By comparing performance across these, one can determine if failure stems from unimodal perception deficits or cross-modal binding failures.
- Core assumption: Performance on atomic tasks is roughly independent, and composite task success requires competence in all constituent subtasks.
- Evidence anchors:
  - [Section 3.3] "By decomposing the multimodal task into these atomic components... we enable precise diagnosis"
  - [Table 2] Shows Gemini 1.5 Flash improves +7.41% on synchronization when action recognition succeeds, but only +0.72% when audio classification succeeds
  - [Section 4.3] "Gemini 1.5 Flash achieves 76.8% accuracy on action recognition... but only 49.51% on multimodal synchronization"
  - [corpus] Weak corpus support - neighboring papers don't explicitly validate this decomposition approach

### Mechanism 3
- Claim: Current models exhibit a positive-association bias—they actively seek audio-visual correlations but cannot recognize when correlations are absent.
- Mechanism: Models are trained on objectives that reward finding correspondences between modalities. When presented with sound absence detection (queried sound not present) or sound discrimination (different sound overlaid), models default to selecting a visual action rather than "none of the above," indicating they lack explicit mismatch-detection mechanisms.
- Core assumption: Training data and objectives over-represent positive audio-visual pairs relative to negative examples.
- Evidence anchors:
  - [Section 4.2] "Gemini 2.0 Flash... performs relatively well on multimodal synchronization (59.86%)... however their accuracy drops dramatically when tasked with detecting sound absence (8.50%)"
  - [Section 4.2] "models have been trained to actively seek and identify audio-visual correlations, but lack the ability to recognize when such correlations are absent"
  - [corpus] AVTrustBench similarly notes "benchmarks are restricted to assessing primarily the visual aspect and do not examine the holistic audio-visual" - corroborating evaluation gaps

## Foundational Learning

- Concept: **Temporal Alignment in Multimodal Learning**
  - Why needed here: DAVE's core challenge requires binding auditory events to visual events at matching timestamps. Without understanding how models learn temporal correspondences (e.g., via contrastive learning, attention over time, or cross-modal synchronization losses), one cannot interpret failure modes.
  - Quick check question: Given a 10-second video with a dog bark at seconds 3-4, what mechanism would allow a model to correctly answer "What action occurs when the dog barks?"

- Concept: **Modality Bias in Multimodal Models**
  - Why needed here: The paper's central motivation is that existing benchmarks permit visual-only solutions. Understanding why models exploit shortcuts (e.g., visual dominance, dataset artifacts) is prerequisite to designing debiased evaluations.
  - Quick check question: If a model achieves 70% accuracy on an audio-visual QA dataset using only video input, what does this imply about the dataset design?

- Concept: **Negative Sampling and Contrastive Objectives**
  - Why needed here: The paper hypothesizes that models fail sound absence detection due to lack of negative examples during training. Understanding how contrastive learning handles hard negatives versus absent pairs is essential for proposing fixes.
  - Quick check question: In a contrastive audio-visual learning setup, how would you construct negative pairs to teach a model that some sounds do NOT correspond to any visual action?

## Architecture Onboarding

- Component map:
  Data Pipeline: Epic Kitchens/Ego4D → Event extraction → Audio overlay (ESC-50 sounds at aligned timestamps) → Two-stage filtering (narration enhancement + visual quality verification via Gemini Flash 2.0 Lite) → Question generation → Evaluation harness

- Critical path:
  1. Source video selection (must have clear, temporally bounded actions)
  2. Audio overlay with precise synchronization (timestamp alignment is the ground truth)
  3. Question generation with verified impossibility of single-modality solution
  4. Model inference with standardized prompts
  5. Conditional analysis (subtask-conditioned performance to isolate failure sources)

- Design tradeoffs:
  - Synthetic audio overlay vs. natural audio: Synthetic enables precise control but may not reflect real-world distribution
  - Multiple-choice vs. open-ended: MC enables clear accuracy metrics but limits linguistic diversity
  - Template-based questions vs. human-annotated: Ensures consistency but may lack complexity
  - Egocentric video only: Controls for camera perspective but limits generalization claims

- Failure signatures:
  - **High video-only accuracy**: Indicates visual bias not eliminated (dataset construction failed)
  - **Near-zero sound absence detection**: Indicates positive-association bias in model
  - **Action recognition >> synchronization accuracy**: Indicates integration bottleneck, not perception failure
  - **Pipeline models outperforming end-to-end on absence detection**: Suggests modularity helps (GPT-4o pipeline: 60.12% vs Gemini 1.5 Pro: 1.03% on absence detection)

- First 3 experiments:
  1. **Modality ablation sanity check**: Run your model on DAVE with (a) full audio+video, (b) video-only, (c) audio-only. Confirm performance drops significantly without both modalities. Expected: 15-25% drop for competent models.
  2. **Atomic task baseline**: Evaluate your model on action recognition and audio classification separately. If either is below 60%, multimodal synchronization will likely fail—fix perception first.
  3. **Absence detection probe**: Isolate sound absence detection samples and analyze error patterns. If model never selects "none of the above," add negative-sampling to training or post-hoc calibration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural mechanisms or training objectives could enable models to reliably detect when audio-visual correlations are absent, rather than defaulting to spurious positive associations?
- Basis in paper: [explicit] The authors state: "the absence of explicit mechanisms to detect mismatches or missing correspondences between modalities" is a fundamental limitation, and suggest "developing architectures with explicit 'mismatch detection' modules or training objectives that reward correct identification of absent correlations" as an actionable direction.
- Why unresolved: Current models like Gemini 2.0 Flash achieve 59.86% on synchronization but only 8.50% on absence detection, indicating a systemic architectural blind spot rather than incremental performance gap.
- What evidence would resolve it: A model architecture incorporating explicit negative correlation detection that achieves substantially improved sound absence detection scores (e.g., >50%) on DAVE without degrading synchronization performance.

### Open Question 2
- Question: How can cross-modal temporal alignment be improved when the bottleneck appears to be integration rather than unimodal perception?
- Basis in paper: [explicit] The authors observe that models show "consistent performance gaps between action recognition and multimodal synchronization" (e.g., 76.8% vs 49.51% for Gemini 1.5 Flash), and explicitly suggest "future models should incorporate explicit temporal alignment modules or adopt training objectives that reward precise synchronization."
- Why unresolved: Models demonstrate strong isolated visual action recognition (70-84%) but significantly underperform on temporal binding with audio, suggesting current fusion mechanisms are inadequate for precise cross-modal alignment.
- What evidence would resolve it: An intervention (architectural or training-based) targeting temporal alignment that narrows the gap between atomic action recognition and multimodal synchronization performance by at least 50%.

### Open Question 3
- Question: Will the diagnostic findings from DAVE's egocentric video benchmark generalize to third-person, cinematic, or professionally produced video content?
- Basis in paper: [inferred] The authors acknowledge building DAVE "on top of datasets that feature only egocentric videos, while future works could extend such setup to videos of any type." Egocentric videos have distinct characteristics (camera motion, viewpoint, action proximity) that may affect audio-visual synchronization differently than other video genres.
- Why unresolved: The 26% human-model gap may partially reflect domain-specific challenges of egocentric footage (unstable frames, variable lighting) rather than fundamental cross-modal reasoning limitations.
- What evidence would resolve it: Evaluation of current SOTA models on an extended DAVE benchmark covering diverse video sources (third-person, broadcast, surveillance) showing whether performance patterns and failure modes persist across domains.

## Limitations
- Synthetic audio overlay may not reflect real-world audio-visual distributions
- Focus on egocentric kitchen environments limits generalizability to other domains
- Zero-shot evaluation approach may disadvantage models trained with multimodal objectives

## Confidence
- **High Confidence**: The claim that DAVE successfully constrains visual bias is well-supported by ablation results showing significant performance drops when audio is removed (Gemini 2.0 Flash: 50.85% → 37.14%).
- **Medium Confidence**: The positive-association bias hypothesis is supported by absence detection results (8.50% accuracy) but could also reflect limited negative sampling during training rather than fundamental architectural limitations.
- **Low Confidence**: The claim that current models fundamentally lack mismatch-detection mechanisms is inferred from absence detection failures but could alternatively result from dataset artifacts or prompt engineering limitations.

## Next Checks
1. **Domain Transfer Validation**: Evaluate DAVE-trained models on naturally occurring audio-visual data (e.g., AudioSet or VGG-Sound) to test whether synthetic training transfers to real-world distributions.
2. **Negative Sampling Intervention**: Train a model with explicit negative audio-visual pairs during contrastive learning and re-evaluate on DAVE's absence detection subset to determine if the failure is addressable through training modifications.
3. **Cross-Modal Attention Analysis**: Visualize attention weights in multimodal models during DAVE tasks to verify whether models are actually performing temporal alignment versus learning spurious visual-audio correlations.