---
ver: rpa2
title: Schema as Parameterized Tools for Universal Information Extraction
arxiv_id: '2506.01276'
source_url: https://arxiv.org/abs/2506.01276
tags:
- schema
- extraction
- schemas
- generation
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Schema as Parameterized Tools (SPT), a unified
  framework that reimagines universal information extraction (UIE) by treating predefined
  schemas as parameterized tools for dynamic selection and argument filling. SPT addresses
  the limitations of current UIE systems that struggle with adaptability between predefined
  and on-the-fly schema generation within in-context learning paradigms.
---

# Schema as Parameterized Tools for Universal Information Extraction

## Quick Facts
- arXiv ID: 2506.01276
- Source URL: https://arxiv.org/abs/2506.01276
- Reference count: 14
- Key outcome: SPT achieves comparable extraction performance to LoRA baselines using 43K trainable parameters vs 1.2M, with F1 scores of 0.76 for entity extraction and 0.64 for relation extraction.

## Executive Summary
Schema as Parameterized Tools (SPT) introduces a unified framework for universal information extraction that treats predefined schemas as parameterized tools. The approach addresses the limitations of current UIE systems by enabling adaptive extraction through dynamic schema retrieval, generation, and infilling. SPT operates within an in-context learning paradigm, allowing it to handle both predefined and on-the-fly schema generation while maintaining competitive performance with dramatically reduced parameter counts.

## Method Summary
SPT uses Qwen2.5-1.5B-Instruct with vocabulary expansion to add 28 schema tokens (26 schemas plus `<Rej>` and `<Gen>`). The framework employs a three-phase training curriculum: Phase 1 optimizes schema tokens on closed-schema data, Phase 2 trains helper tokens for rejection/generation, and Phase 3 performs joint tuning of all new tokens. Only the new embeddings (43K parameters) are trained while the LLM base model remains frozen. The system processes input text to either retrieve relevant schemas from a predefined pool or generate new schemas when needed, then extracts information and fills the corresponding slots.

## Key Results
- SPT achieves F1 scores of 0.76 for entity extraction and 0.64 for relation extraction
- The framework maintains 0.47 rejection accuracy for schema-free samples
- Parameter efficiency: 43K trainable parameters vs 1.2M for LoRA baselines
- Performance is comparable to leading UIE systems while using significantly fewer parameters

## Why This Works (Mechanism)
SPT works by treating schemas as first-class entities that can be retrieved or generated dynamically. The system learns to associate input text patterns with appropriate schema tokens through gradient updates on the expanded vocabulary. By freezing the base LLM and only training the schema-related parameters, SPT maintains the model's general language understanding while adapting it to schema-specific tasks. The three-phase training curriculum ensures stable learning of schema associations before fine-tuning for rejection and generation capabilities.

## Foundational Learning
- **Schema Token Embeddings**: Special token IDs representing schema structures, trained to capture semantic relationships between input text and extraction targets. *Why needed*: Enables the model to treat schemas as callable tools. *Quick check*: Verify embedding cosine similarity shows meaningful clustering.
- **Rejection vs Generation Distinction**: `<Rej>` and `<Gen>` tokens signal when no schema applies or when new schemas should be created. *Why needed*: Handles schema-free samples appropriately. *Quick check*: Monitor rejection rates during training phases.
- **Three-Phase Curriculum**: Sequential training approach that first learns schema associations, then helper tokens, then joint optimization. *Why needed*: Prevents catastrophic forgetting and ensures stable convergence. *Quick check*: Track F1 scores and rejection rates per phase.

## Architecture Onboarding
- **Component Map**: Input Text -> Schema Retrieval/Generation -> Schema Infilling -> Output Sequence
- **Critical Path**: Input text flows through the model to produce schema tokens, which trigger either retrieval from the schema pool or generation of new schemas, followed by argument extraction and filling.
- **Design Tradeoffs**: Freezing the base LLM dramatically reduces trainable parameters but may limit fine-grained adaptation. The three-phase curriculum adds training complexity but ensures stable learning.
- **Failure Signatures**: Random schema predictions indicate token embedding collapse; high rejection rates suggest over-generalization in Phase 2; poor extraction F1 indicates insufficient schema association learning.
- **First Experiments**:
  1. Train Phase 1 only on closed-schema data and measure schema retrieval accuracy
  2. Test Phase 2 rejection performance on schema-free samples
  3. Evaluate Phase 3 joint performance on full dataset with both retrieval and extraction metrics

## Open Questions the Paper Calls Out
**Scalability to Larger Models**: How does SPT performance scale with LLMs larger than 1.5B parameters? The paper suggests evaluating across 7B, 13B, and 70B models to assess potential performance gains.

**Overlapping Schema Scenarios**: How does SPT handle schemas with semantic overlap or conflicts in the predefined pool? The current benchmarks intentionally avoid schema conflicts, leaving real-world overlapping scenarios untested.

**Embedding Architecture Impact**: What is the relationship between tied vs. untied input/output embeddings and SPT's optimization efficiency? The ablation shows Phi3.5-mini (untied) matching Qwen7B (tied, larger), but the mechanism remains unclear.

## Limitations
- The paper does not specify the exact input prompt template used during training, which could significantly impact performance
- Schema description injection mechanisms are unclear, potentially affecting schema association quality
- Limited ablation studies on schema pool size and description quality impact
- The three-phase curriculum adds training complexity without clear justification for the specific phase ordering

## Confidence
- **Schema as Parameterized Tools Framework**: Medium - Conceptually clear but implementation details are incomplete
- **Parameter Efficiency Claims**: High - Dramatic reduction from 1.2M to 43K parameters is clearly demonstrated
- **Extraction Performance**: Medium - Competitive F1 scores reported but limited comparative analysis
- **Rejection Capability**: High - 0.47 accuracy is explicitly measured and reported

## Next Checks
1. **Prompt Template Verification**: Test different prompt templates to determine which format most effectively triggers schema selection and argument filling.
2. **Schema Description Injection Impact**: Systematically evaluate whether injecting schema descriptions into prompts during training improves schema association and extraction accuracy.
3. **Batch Size and Learning Rate Sensitivity**: Conduct controlled experiments varying batch size and corresponding learning rates to determine optimal training configurations.