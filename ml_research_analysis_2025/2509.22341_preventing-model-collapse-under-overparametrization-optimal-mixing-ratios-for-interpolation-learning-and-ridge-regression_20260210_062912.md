---
ver: rpa2
title: 'Preventing Model Collapse Under Overparametrization: Optimal Mixing Ratios
  for Interpolation Learning and Ridge Regression'
arxiv_id: '2509.22341'
source_url: https://arxiv.org/abs/2509.22341
tags:
- equation
- risk
- theorem
- mixing
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies model collapse in overparameterized linear\
  \ regression, where models trained iteratively on their own synthetic outputs degrade\
  \ in performance. The authors introduce a fresh data augmentation framework where\
  \ each iteration mixes real data with synthetic data generated from the previous\
  \ iteration's model, analyzing minimum-\u21132-norm interpolation and ridge regression\
  \ estimators."
---

# Preventing Model Collapse Under Overparametrization: Optimal Mixing Ratios for Interpolation Learning and Ridge Regression

## Quick Facts
- arXiv ID: 2509.22341
- Source URL: https://arxiv.org/abs/2509.22341
- Reference count: 40
- Key result: Optimal mixing ratios for real/synthetic data prevent model collapse in overparameterized linear regression, with w* converging to 1/φ ≈ 0.618 for min-ℓ₂-norm interpolation and w* ≥ 0.5 for ridge regression

## Executive Summary
This paper addresses model collapse in overparameterized linear regression where models trained iteratively on their own synthetic outputs degrade in performance. The authors introduce a fresh data augmentation framework where each iteration mixes real data with synthetic data generated from the previous iteration's model. They analyze minimum-ℓ₂-norm interpolation and ridge regression estimators, deriving precise generalization error formulas that show risk depends on the mixing proportion of real versus synthetic data. The analysis reveals that mixing real and synthetic data prevents model collapse by keeping the risk bounded as iterations increase.

## Method Summary
The method involves iterative training on mixed real and synthetic data labels. Starting with real data (y, X), the algorithm generates synthetic labels from the previous model's predictions. At each iteration t, fresh real labels yₜ are generated alongside synthetic labels ŷₜ from β̂ₜ₋₁, which are then mixed with proportion w. The estimator is updated using either ridge regression (X⊤X + nλI)⁻¹X⊤[mixed labels] or min-ℓ₂ interpolation (X⊤X)†X⊤[mixed labels]. The framework assumes overparameterized regime (p > n with p/n → γ > 1), bounded eigenvalues of covariance Σ, and i.i.d. noise with bounded moments.

## Key Results
- For min-ℓ₂-norm interpolation, the optimal real-data proportion converges to the reciprocal of the golden ratio (1/φ ≈ 0.618) for general feature distributions
- For ridge regression, the optimal mixing ratio is at least one-half, with specific values depending on the regularization parameter and feature covariance structure
- Mixing real and synthetic data prevents model collapse by keeping the risk bounded as iterations increase
- Theoretical results are validated through extensive simulations across various settings

## Why This Works (Mechanism)

### Mechanism 1: Fresh Data Mixing Bounds Iterative Variance Accumulation
- Claim: Mixing fresh real labels at each iteration prevents unbounded variance growth that causes model collapse.
- Mechanism: When training iteratively on synthetic data alone (w=0), variance accumulates multiplicatively across iterations through the term (1-w)², leading to unbounded risk as t→∞. Introducing fresh real data with mixing proportion w>0 creates a stabilizing term that bounds the variance term c(w) = (w² + (1-w)²)/(w(2-w)), preventing collapse.
- Core assumption: Overparameterized regime (p > n with p/n → γ > 1), bounded eigenvalues of covariance Σ, and i.i.d. noise with bounded moments.
- Evidence anchors:
  - [abstract] "The analysis reveals that mixing real and synthetic data prevents model collapse by keeping the risk bounded as iterations increase."
  - [section 3.1] "If w = 0, then the generalization error R(β̂ₜ, β) → ∞ as t → ∞... This implies that training solely on synthetic data results in model collapse."
  - [corpus] Related work (arXiv:2502.18049) confirms golden ratio weighting prevents collapse in low-dimensional OLS, supporting the general principle.
- Break condition: If w ≤ 0 or the real data distribution differs fundamentally from the synthetic generation process, variance accumulation may not stabilize. Assumption: Fresh real labels must be drawn from the same underlying distribution at each iteration.

### Mechanism 2: Optimal Mixing at Golden Ratio Reciprocal Minimizes Variance for Min-ℓ₂ Interpolation
- Claim: For minimum-ℓ₂-norm interpolation, the optimal real-data proportion converges to 1/φ ≈ 0.618 across general feature covariance structures.
- Mechanism: The asymptotic risk depends on w only through c(w)V, where V is the asymptotic variance of standard min-ℓ₂ interpolation and c(w) = (w² + (1-w)²)/(w(2-w)). The function c(w) has a unique minimum at w* = φ⁻¹, independent of Σ's eigenstructure, because the bias term B is independent of both iteration t and mixing proportion w for this estimator class.
- Core assumption: Feature covariance Σ has bounded eigenvalues (τ ≤ sₚ ≤ ... ≤ s₁ ≤ τ⁻¹), and the signal strength ||β||₂ → b* as n,p → ∞.
- Evidence anchors:
  - [abstract] "For min-ℓ₂-norm interpolation, the optimal real-data proportion converges to the reciprocal of the golden ratio (1/φ ≈ 0.618) for general feature distributions."
  - [section 3.1, Theorem 3.1] "Moreover, the limiting risk is minimized at w* = φ⁻¹, where φ = (1 + √5)/2 is the golden ratio."
  - [corpus] Corpus evidence is weak for overparameterized settings; related work primarily addresses low-dimensional regimes.
- Break condition: If bias becomes w-dependent (e.g., with different estimator classes or non-identical distributions), the optimal w* may shift. Assumption: The min-ℓ₂-norm interpolator is the appropriate surrogate for implicitly regularized gradient descent limits.

### Mechanism 3: Spectral Geometry Governs Ridge Regression Optimal Weights
- Claim: For ridge regression, the optimal mixing ratio depends on regularization λ, signal-to-noise ratio, and spectral structure of Σ, with w* ∈ [0.5, 1].
- Mechanism: Both variance Vλ and bias Bλ terms depend on w through m₁ = m(-λ/w) and m₂ = m(-λ/(2-w)), where m(·) is the Stieltjes transform encoding Σ's spectral distribution. Under isotropic or random-effects models, c(w)Vλ and Bλ are log-convex in w, guaranteeing unique minimizers. As λ → 0, w* → φ⁻¹; as λ → ∞, w* → 1 (favoring all real data).
- Core assumption: The spectral measures Hₚ and Gₚ converge weakly to limiting measures H and G, and for random-effects/spiked models, specific structural relationships between signal and covariance eigenvectors.
- Evidence anchors:
  - [abstract] "For ridge regression, they prove the optimal mixing ratio is at least one-half, with specific values depending on the regularization parameter and feature covariance structure."
  - [section 3.2, Theorem 3.3] "w*(λ) ∈ [0.5, 1], w*(λ) → φ⁻¹ as λ ↓ 0, w*(λ) → 1 as λ ↑ ∞."
  - [corpus] Weak corpus evidence for ridge-specific collapse dynamics; most related work focuses on OLS or non-iterative settings.
- Break condition: If Σ has unbounded eigenvalues (e.g., equicorrelation with ρ√p scaling) or G ≠ H in ways not captured by random-effects/spiked models, the closed-form w* characterization may not hold. Simulations suggest robustness to some violations (Figure 2c).

## Foundational Learning

- Concept: **Overparameterization and Interpolation Learning**
  - Why needed here: The paper's entire analysis operates in the p > n regime where models interpolate training data, fundamentally changing generalization behavior compared to classical statistics.
  - Quick check question: Can you explain why min-ℓ₂-norm interpolators can generalize despite zero training error, and how this relates to implicit regularization?

- Concept: **Stieltjes Transform and Marchenko-Pastur Distribution**
  - Why needed here: The asymptotic risk expressions depend critically on m(z), the solution to equation (3.2), which encodes spectral information of the feature covariance through random matrix theory.
  - Quick check question: What does m(z) represent in terms of the sample covariance eigenvalues, and why does it appear in both bias and variance terms?

- Concept: **Bias-Variance Decomposition under Synthetic Data Iteration**
  - Why needed here: Understanding how synthetic data affects variance accumulation (multiplicative) versus bias (fixed for min-ℓ₂) is essential for grasping why mixing ratios affect collapse prevention.
  - Quick check question: Why does the bias term remain independent of iteration t for min-ℓ₂ interpolation, but depends on t for ridge regression?

## Architecture Onboarding

- Component map:
  - **Data Generation Layer**: Real labels yₜ ← Xβ + εₜ (fresh each iteration) and synthetic labels ŷₜ,λ ← Xβ̂ₜ₋₁,λ + ε̃ₜ (from previous model).
  - **Mixing Layer**: Weighted combination w·yₜ + (1-w)·ŷₜ,λ with fixed or adaptive w.
  - **Estimation Layer**: Ridge regression β̂ₜ,λ = (X⊤X + nλI)⁻¹X⊤[mixed labels], or min-ℓ₂ interpolation β̂ₜ = (X⊤X)†X⊤[mixed labels].
  - **Risk Evaluation Layer**: Out-of-sample prediction risk R(β̂; β) = E[||β̂ - β||²_Σ | X], decomposed into bias and variance components.

- Critical path: (1) Initialize β̂₀,λ from real data (y, X) → (2) For each iteration t, generate fresh real labels AND synthetic labels from β̂ₜ₋₁,λ → (3) Mix with proportion w → (4) Update estimator → (5) Repeat until convergence (t → ∞). The critical constraint is that X remains fixed; only labels refresh.

- Design tradeoffs:
  - Higher w (more real data): Lower variance accumulation but requires more fresh real labels per iteration (costly if real data is scarce).
  - Lower w (more synthetic): Cheaper but risks collapse; w must stay > 1/3 for variance monotonic decrease in min-ℓ₂ case.
  - Higher λ (ridge): Pushes w* toward 1 (need more real data), but may underfit if signal is strong.
  - Dynamic vs. fixed w: Dynamic mixing (wₜ adapted per iteration) can lower early-iteration risk but converges to same asymptotic w* = φ⁻¹.

- Failure signatures:
  - **Collapse**: Test risk diverges as t increases → check if w = 0 or w < 1/3 for min-ℓ₂ interpolation.
  - **Suboptimal mixing**: Risk higher than theoretical minimum → verify w is not tuned to w*(λ) for ridge; for min-ℓ₂, ensure w ≈ 0.618.
  - **Covariance mismatch**: Risk deviates from theory when Σ has unbounded eigenvalues or non-generic signal alignment → check spectral assumptions.
  - **Non-stationary real data**: If real label distribution shifts across iterations, theoretical guarantees may not hold.

- First 3 experiments:
  1. **Validate golden ratio optimum for min-ℓ₂**: Fix n=200, p=300 (γ=1.5), generate X with isotropic covariance, run Algorithm 1 with λ→0 for t=10 iterations across w ∈ {0.1, 0.3, 0.5, 0.618, 0.7, 0.9, 1.0}. Plot asymptotic risk vs. w and verify minimum near 0.618. Compare to Theorem 3.1's theoretical curve.
  2. **Ridge w* trajectory with varying λ**: Same setup but with ridge (λ ∈ {0.01, 0.1, 1, 10}), measure empirical w* for each λ. Plot w*(λ) vs. λ and verify: (a) w* ∈ [0.5, 1], (b) w* → 0.618 as λ → 0, (c) w* → 1 as λ → ∞. Test with spiked covariance (Σ = I + 5e₁e₁⊤) to check robustness.
  3. **Dynamic mixing comparison**: Implement adaptive wₜ from Section C (wₜ* = (1 + w*ₜ₋₁)/(2 + w*ₜ₋₁), w₀ = 1). Compare risk trajectory vs. fixed w = φ⁻¹ over iterations t ∈ {1, ..., 20}. Verify that both converge to same asymptotic risk but dynamic achieves lower risk in early iterations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model collapse affect interpolators in $\ell_p$ geometries other than $\ell_2$?
- Basis in paper: [explicit] The Discussion section identifies understanding model collapse in other $\ell_p$ geometries as a "promising next direction."
- Why unresolved: The current analysis relies on random matrix theory tools tailored for $\ell_2$ norms, whereas other geometries require different analytical techniques.
- What evidence would resolve it: Theoretical characterization of generalization error for $\ell_p$-norm interpolators under synthetic data mixing, or proof that the golden ratio weighting fails.

### Open Question 2
- Question: Do the optimal mixing ratios for linear regression generalize to non-linear high-dimensional models like Generalized Linear Models (GLMs)?
- Basis in paper: [explicit] The authors suggest extending linear arguments to non-linear high-dimensional problems, specifically listing GLMs and single-index models.
- Why unresolved: The current proofs rely on the linearity of the response; the impact of non-linear link functions on the optimal mixing strategy is unknown.
- What evidence would resolve it: Derivation of limiting risk formulas for GLMs (e.g., logistic regression) trained on real-synthetic mixtures to see if $w^\star \approx 1/\phi$.

### Open Question 3
- Question: How can model collapse be characterized and mitigated in multi-index models or transformer architectures?
- Basis in paper: [explicit] The Discussion lists extending results to complex architectures like multi-index models and transformers as an "important challenge."
- Why unresolved: The paper restricts analysis to linear single-index models; multi-index settings introduce hierarchical interactions not captured by the current theory.
- What evidence would resolve it: Analysis of stability conditions for multi-index models under iterative retraining or empirical validation of optimal mixing ratios in transformer fine-tuning.

## Limitations
- The theoretical framework relies heavily on assumptions of bounded eigenvalues for the covariance matrix and specific spectral measure convergences.
- The optimal mixing ratio analysis for ridge regression depends critically on the interplay between regularization strength λ and the spectral structure of Σ, but may not fully capture all practical scenarios where Σ has unbounded eigenvalues or non-generic signal alignment.
- The analysis is restricted to linear single-index models and does not address non-linear high-dimensional problems like GLMs or transformer architectures.

## Confidence

- **High Confidence**: The golden ratio reciprocal (w* = 1/φ ≈ 0.618) as the optimal mixing ratio for min-ℓ₂-norm interpolation. This result is independent of Σ's spectral structure and follows directly from the algebraic properties of c(w).
- **Medium Confidence**: The lower bound w* ≥ 0.5 for ridge regression mixing ratios. While the theoretical proof establishes this bound, empirical validation across diverse covariance structures would strengthen confidence.
- **Low Confidence**: The exact characterization of w* for general Σ in the ridge case. The paper relies on log-convexity assumptions and specific model structures that may not hold in all practical settings.

## Next Checks
1. **Dynamic Mixing Performance**: Implement and compare the adaptive mixing strategy wₜ* = (1 + w*ₜ₋₁)/(2 + w*ₜ₋₁) against fixed optimal mixing across different iteration counts. Verify whether early-iteration benefits persist while maintaining asymptotic convergence to the same risk.

2. **Covariance Structure Sensitivity**: Test the optimal mixing ratios on equicorrelated covariance structures (Σ = (1-ρ)I + ρeeᵀ) where eigenvalues scale with √p. Compare theoretical predictions with empirical results, particularly examining the claim that theory applies despite eigenvalue violations when β is in generic position.

3. **Sample Complexity Verification**: For each optimal mixing ratio, determine the minimum number of real data points required per iteration to prevent model collapse while maintaining theoretical risk guarantees. This would provide practical guidance on resource allocation when real data is expensive.