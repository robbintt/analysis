---
ver: rpa2
title: Who Wrote This? Identifying Machine vs Human-Generated Text in Hausa
arxiv_id: '2503.13101'
source_url: https://arxiv.org/abs/2503.13101
tags:
- text
- language
- hausa
- machine-generated
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first detector for identifying machine-generated
  text (MGT) versus human-generated text (HGT) in Hausa, a low-resource African language.
  The authors collected 2,586 Hausa news articles from seven media outlets for human-generated
  content and generated an equal amount using the Gemini-2.0 flash model.
---

# Who Wrote This? Identifying Machine vs Human-Generated Text in Hausa

## Quick Facts
- arXiv ID: 2503.13101
- Source URL: https://arxiv.org/abs/2503.13101
- Reference count: 4
- Primary result: 99.23% accuracy in detecting machine-generated Hausa text using AfroXLMR model

## Executive Summary
This paper presents the first detector for identifying machine-generated text (MGT) versus human-generated text (HGT) in Hausa, a low-resource African language. The authors collected 2,586 Hausa news articles from seven media outlets for human-generated content and generated an equal amount using the Gemini-2.0 flash model. Four Afri-centric pre-trained language models were fine-tuned on this dataset, achieving high accuracy in distinguishing between HGT and MGT. The best-performing model, AfroXLMR, achieved 99.23% accuracy and 99.21% F1 score, demonstrating the effectiveness of multilingual models for low-resource language detection tasks. The dataset and models are made publicly available to support further research in this area.

## Method Summary
The study collected 2,586 human-written Hausa news articles from seven media outlets and generated an equal amount of machine-generated articles using Gemini-2.0 flash, conditioned on article headlines. Four Afri-centric pre-trained language models (AfriTeVa, AfriBERTa, AfroXLMR, and AfroXLMR-76L) were fine-tuned on this binary classification task. The models were trained with AdamW optimizer, learning rate 1e-5, batch size 8, and 3 epochs, with a maximum sequence length of 512 tokens. The best-performing model achieved 99.23% accuracy and 99.21% F1 score on the test set.

## Key Results
- AfroXLMR achieved 99.23% accuracy and 99.21% F1 score in detecting MGT vs HGT in Hausa
- Afri-centric multilingual pre-trained models outperformed general models for this low-resource language task
- Headline-conditioned MGT generation created semantically aligned paired data for supervised discrimination learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Afri-centric multilingual pre-trained language models transfer effectively to Hausa MGT detection with minimal fine-tuning.
- **Mechanism:** AfroXLMR's pre-training on diverse African language corpora creates latent representations that capture cross-linguistic patterns. When fine-tuned on binary HGT/MGT classification, these representations discriminate between human and machine stylistic signatures in Hausa specifically.
- **Core assumption:** The stylistic differences between human and machine text manifest in learned embeddings that generalize across the distribution of Hausa news articles.
- **Evidence anchors:**
  - [abstract] "AfroXLMR achieved the highest performance with an accuracy of 99.23% and an F1 score of 99.21%, demonstrating its effectiveness for Hausa text detection."
  - [section 5.2] "This suggests that multilingual pre-trained language models optimized for African languages can be fine-tuned effectively for low-resource language tasks such as MGT detection."
  - [corpus] Related work (HausaNLP at SemEval-2025) confirms AfriBERTa's effectiveness for Hausa emotion detection, supporting Afri-centric model transfer.
- **Break condition:** Performance degrades if MGT comes from LLMs with substantially different stylistic profiles than Gemini-2.0 flash, or if text domains diverge significantly from news articles.

### Mechanism 2
- **Claim:** Headline-conditioned MGT generation creates semantically aligned paired data for supervised discrimination learning.
- **Mechanism:** Using the same headline to generate MGT as the original HGT article constrains the synthetic text to cover similar topics. This controlled pairing allows the classifier to learn stylistic rather than content-based distinctions.
- **Core assumption:** The classification model learns to detect writing style patterns (vocabulary choice, syntactic structures, statistical regularities) rather than memorizing topic-specific content.
- **Evidence anchors:**
  - [section 3.1] "We used the Gemini-2.0 flash model... to automatically generate corresponding Hausa news articles based on each of the article headlines in the human-generated text."
  - [section 3.1] "The dataset consists of headlines, content, and a word count for every article so that the text length of the generated article is near or equal to the actual article."
  - [corpus] Weak/no direct corpus evidence for this specific pairing mechanism.
- **Break condition:** If the headline provides insufficient signal for topic-matching, or if the LLM produces content that diverges significantly in focus from HGT, the classifier may learn spurious content-based shortcuts.

### Mechanism 3
- **Claim:** Afri-centric PLMs outperform general multilingual models on Hausa text due to architecture and pre-training data optimization for African languages.
- **Mechanism:** Models like AfroXLMR incorporate African language data during pre-training, learning morphological and syntactic patterns specific to these languages. This provides better initialization for downstream Hausa tasks compared to models trained predominantly on high-resource languages.
- **Core assumption:** Hausa linguistic features (morphology, syntax) are better captured by Afri-centric pre-training than by general multilingual models.
- **Evidence anchors:**
  - [section 4] "These models were selected due to their prior optimization for African languages, including Hausa."
  - [section 5.2] "This difference can be due to model architecture, pretraining data, and optimization methods."
  - [corpus] HausaNLP overview paper confirms AfriBERTa's effectiveness for Hausa sentiment analysis, supporting this mechanism.
- **Break condition:** If the pre-training data contains insufficient Hausa or related Chadic languages, transfer benefits diminish.

## Foundational Learning

- **Concept: Low-resource language NLP**
  - **Why needed here:** Hausa has 100M+ speakers but limited digital resources; standard approaches fail without Afri-centric adaptations.
  - **Quick check question:** Why can't you simply use an English-trained MGT detector with translation for Hausa text?

- **Concept: Fine-tuning vs. zero-shot transfer**
  - **Why needed here:** The study uses supervised fine-tuning on labeled HGT/MGT pairs rather than relying on prompt-based detection.
  - **Quick check question:** What minimum labeled dataset size would be needed before considering zero-shot alternatives?

- **Concept: Domain shift in MGT detection**
  - **Why needed here:** Models trained on news may fail on social media, academic, or conversational text.
  - **Quick check question:** If deployed on Hausa social media posts, what performance degradation should you expect?

## Architecture Onboarding

- **Component map:** [HGT Scraping] → [Preprocessing] → [Headline Extraction] → [Gemini-2.0 API] ← [Batch Generator] ← [MGT Corpus] + [HGT Corpus] → [Merged Dataset] → [Train/Test Split] → [AfroXLMR/AfriTeVa/AfriBERTa/AfroXLMR-76L] → [Fine-tuning Loop] → [Evaluation: Accuracy, F1] → [Model Hub Deployment]

- **Critical path:**
  1. Quality HGT scraping with proper preprocessing (duplicates, non-Hausa text, markup removal)
  2. Headline-conditioned MGT generation with length matching
  3. Model selection—AfroXLMR for best performance, AfriBERTa for faster inference

- **Design tradeoffs:**
  - **Single LLM source (Gemini-2.0 flash)** vs. multi-generator training: Current approach risks overfitting to Gemini's stylistic fingerprint.
  - **News domain only** vs. multi-domain: Limits generalization but provides clean, structured data.
  - **3 epochs, batch size 8** vs. longer training: May underutilize model capacity; authors note this limitation.

- **Failure signatures:**
  - High accuracy on test set but poor performance on MGT from GPT-4, Claude, or Llama
  - Degraded detection on non-news domains (social media, academic text)
  - Overconfidence on short texts where stylistic signals are weak

- **First 3 experiments:**
  1. **Cross-generator validation:** Train on Gemini-generated MGT, test on GPT-4 and Claude-generated Hausa text to measure generator generalization.
  2. **Domain transfer test:** Evaluate the news-trained model on Hausa social media posts or academic abstracts without retraining.
  3. **Ablation by training size:** Retrain AfroXLMR with 25%, 50%, 75% of the dataset to establish minimum data requirements for acceptable performance (>90% accuracy).

## Open Questions the Paper Calls Out
- How well does the Hausa MGT detector generalize to text generated by LLMs other than Gemini-2.0 flash (e.g., GPT-4, Claude, Llama)?
- Can the detection approach be effectively extended to other low-resource African languages via cross-language transfer learning?
- How robust is the detector to paraphrased or rephrased machine-generated text in Hausa?
- Does the detector generalize to domains beyond news articles, such as social media posts or academic writing?

## Limitations
- Single MGT generator (Gemini-2.0 flash) limits generalization to other LLMs
- Dataset confined to news domain, limiting applicability to other Hausa text genres
- Missing train/test split ratios and random seeds for reproducibility

## Confidence
- **High confidence:** The reported performance metrics (99.23% accuracy, 99.21% F1) are internally consistent and supported by clear experimental results on the collected dataset.
- **Medium confidence:** The claim that Afri-centric models outperform general multilingual models for Hausa is supported by comparative results but lacks ablation studies with non-Afri-centric baselines.
- **Low confidence:** Generalization claims to other MGT sources and domains are not empirically validated in the paper.

## Next Checks
1. **Cross-generator validation:** Test the trained model on Hausa MGT generated by different LLMs (e.g., GPT-4, Claude, Llama) to measure performance degradation and establish generalization bounds across generation systems.
2. **Domain transfer evaluation:** Evaluate the news-trained model on Hausa text from non-news domains (social media posts, academic abstracts, conversational text) without retraining to quantify domain-specific limitations.
3. **Minimum data requirement analysis:** Systematically vary the training dataset size (25%, 50%, 75%, 100%) and retrain AfroXLMR to establish the minimum labeled data needed for acceptable performance (>90% accuracy), providing guidance for resource-constrained applications.