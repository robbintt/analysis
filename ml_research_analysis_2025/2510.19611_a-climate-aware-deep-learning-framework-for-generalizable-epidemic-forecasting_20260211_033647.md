---
ver: rpa2
title: A Climate-Aware Deep Learning Framework for Generalizable Epidemic Forecasting
arxiv_id: '2510.19611'
source_url: https://arxiv.org/abs/2510.19611
tags:
- forecasting
- data
- forecastnet-xcl
- states
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces ForecastNet-XCL, a climate-aware deep learning
  framework for forecasting endemic respiratory diseases like RSV using only climate
  and temporal data, without requiring real-time case surveillance. The method combines
  a tree-based module (XGBoost) to generate synthetic autoregressive lags with a CNN-BiLSTM
  backbone for capturing short- and long-range temporal dependencies.
---

# A Climate-Aware Deep Learning Framework for Generalizable Epidemic Forecasting

## Quick Facts
- arXiv ID: 2510.19611
- Source URL: https://arxiv.org/abs/2510.19611
- Reference count: 39
- Primary result: Climate-aware deep learning framework forecasts RSV up to 100 weeks without real-time case data, outperforming baselines across 34 U.S. states.

## Executive Summary
This study introduces ForecastNet-XCL, a two-stage deep learning framework that forecasts endemic respiratory diseases like RSV using only climate and temporal data, without requiring real-time case surveillance. The method combines a tree-based module (XGBoost) to generate synthetic autoregressive lags with a CNN-BiLSTM backbone for capturing short- and long-range temporal dependencies. Evaluated across 34 U.S. states, ForecastNet-XCL outperforms statistical and deep learning baselines in both within- and cross-state settings, sustaining accuracy over 100-week horizons. Transfer learning from climatologically diverse states further improves peak timing accuracy and robustness, particularly in data-limited or irregular epidemic regions. The framework's efficiency, scalability, and uncertainty quantification make it a deployable early-warning tool for climate-sensitive endemic forecasting.

## Method Summary
ForecastNet-XCL operates in two stages: first, an XGBoost model trained on recent climate covariates predicts next-week incidence, generating synthetic autoregressive lags that substitute for case surveillance data; second, these synthetic lags are concatenated with exogenous features in a 16-week window and fed into a hybrid CNN-BiLSTM architecture with attention mechanisms to forecast 100 weeks ahead. The framework uses Monte Carlo Dropout for uncertainty quantification and supports transfer learning via state embeddings learned jointly across source states, enabling improved performance when fine-tuned on limited target data. The approach was evaluated on weekly RSV hospitalization data from 34 U.S. states, using NOAA climate variables and engineered features, with a 70/30 temporal train/test split.

## Key Results
- ForecastNet-XCL outperforms statistical and deep learning baselines in both within- and cross-state settings, sustaining accuracy over 100-week horizons
- Transfer learning from climatologically diverse states improves peak timing accuracy and robustness, especially in data-limited or irregular epidemic regions
- The framework achieves strong performance without requiring real-time case surveillance, relying solely on climate and temporal data

## Why This Works (Mechanism)

### Mechanism 1
Synthetic autoregressive lags generated from climate data can substitute for case surveillance in recursive forecasting. Stage 1 trains XGBoost on recent climate covariates to predict next-week incidence; at inference, shifted predictions $\tilde{y}^{(\ell)}_t = \hat{y}^{xgb}_{t-\ell}$ (for $\ell \in \{1,2,3,4\}$) supply lagged incidence proxies without accessing future labels. These synthetic lags are concatenated with exogenous features in a 16-week window for Stage 2. The core assumption is that climate-to-incidence lag structures are sufficiently stable and predictive that tree-based approximations can serve as faithful autoregressive memory.

### Mechanism 2
Combining convolutions, bidirectional recurrence, and attention captures both short-range and seasonal-to-interannual dependencies better than any single component. Three parallel 1-D convolutions (kernels 2, 4, 8) extract localized temporal motifs; a BiLSTM produces sequence hidden states refined by multi-head self-attention; representations are fused via concatenation with skip connections. The core assumption is that short-range patterns (sharp peaks, surges) and long-range patterns (seasonal timing, biennial cycles) are partially separable and benefit from specialized pathways.

### Mechanism 3
Pre-training on climatically diverse states with state embeddings, followed by light fine-tuning, improves generalization—especially peak timing—under data scarcity. Learn embedding matrix $E \in \mathbb{R}^{S \times d_e}$ jointly across source states; at transfer, fine-tune on limited local data while keeping most layers frozen. Exposure to heterogeneous dynamics regularizes temporal representations. The core assumption is that cross-location diversity encodes transferable climate-disease patterns that persist after adaptation.

## Foundational Learning

- **Concept: Recursive multi-step forecasting vs. direct prediction**
  - Why needed here: ForecastNet-XCL generates trajectories by iterating predictions without ground-truth feedback; understanding error accumulation is critical.
  - Quick check question: Can you explain why tree-based models degrade in recursive rollouts despite strong single-step performance?

- **Concept: Autoregressive memory from exogenous proxies**
  - Why needed here: The core innovation is avoiding real-time case data by synthesizing lags from climate; grasping this unlocks the "label-free" design.
  - Quick check question: How do synthetic lags prevent label leakage during evaluation?

- **Concept: Transfer learning with entity embeddings for time series**
  - Why needed here: State embeddings enable cross-jurisdiction generalization; engineers must understand how to pre-train, freeze, and fine-tune appropriately.
  - Quick check question: What layers should remain trainable vs. frozen during fine-tuning to a new state?

## Architecture Onboarding

- **Component map:**
  - Stage 1 (XGBoost pre-module): Input = 16-week climate + calendar features → Output = 4 synthetic RSV lags
  - Stage 2 (Hybrid backbone): Input = 16-week window of climate features + synthetic lags → Multi-scale CNN (kernels 2,4,8) || BiLSTM + 4-head attention → Concatenate → Dense layers with skip → RSV prediction
  - Uncertainty: Monte Carlo Dropout (T=50 passes, rates 0.2–0.3) → 95% empirical prediction intervals
  - Transfer variant: Adds state embedding $e_s$ (dim 16) repeated along temporal axis; concatenated before fusion

- **Critical path:**
  1. Fit XGBoost on training data to learn climate→incidence mapping
  2. Precompute synthetic lags for all test indices (no label access)
  3. Train/fine-tune CNN-BiLSTM-attention on 16-week windows with synthetic lags
  4. At inference: strictly recursive rollout using only climate inputs and precomputed synthetic lags; no feedback from Stage 2 predictions

- **Design tradeoffs:**
  - 16-week lookback reduces data burden but may miss longer-range dependencies; empirically sufficient for RSV seasonality
  - Monte Carlo dropout is lightweight but may underrepresent uncertainty vs. deep ensembles
  - State embeddings improve transfer but require multi-state pre-training data; single-state deployment is simpler but less robust under scarcity

- **Failure signatures:**
  - Low-incidence, weak-seasonality states (e.g., Vermont): high volatility, lower R², though model avoids divergence
  - Climate anomalies or NPIs not seen in training: synthetic lags may misrepresent dynamics
  - Missing or delayed climate forecasts at inference: operational pipelines must substitute forecasted meteorology; error propagation not yet quantified

- **First 3 experiments:**
  1. **Ablate synthetic lags:** Remove XGBoost pre-module (set synthetic lags to zero or mean) and compare recursive performance to quantify contribution of autoregressive memory.
  2. **Vary lookback window:** Test 8-week, 24-week, and 52-week windows to assess sensitivity to temporal context length and identify minimal viable history.
  3. **Stress-test transfer:** Pre-train on a subset of climate zones (e.g., exclude southern states), then evaluate zero-shot and fine-tuned performance on held-out zones to map transfer boundaries.

## Open Questions the Paper Calls Out

### Open Question 1
How does the uncertainty inherent in operational meteorological forecasts propagate to and affect the accuracy of RSV epidemic predictions? The authors note that while ground-truth climate was used for evaluation, operational pipelines must substitute forecasted fields, and "the impact of meteorological forecast error on epidemic predictions remains to be quantified."

### Open Question 2
Can the ForecastNet-XCL architecture generalize effectively to other climate-sensitive pathogens (e.g., influenza) or distinct geographic contexts outside the United States? The authors explicitly state that while they focused on RSV, "generalization beyond this setting (other countries, sub-state geographies, or diseases) remains an open question."

### Open Question 3
Which specific architectural components (e.g., the tree-based lag generator, CNN, or BiLSTM) are most responsible for the model's stability in low-signal environments? The authors acknowledge that "more systematic ablations (e.g., removing the tree-based lag generator, varying the lookback window, or swapping recurrent components) would clarify which design elements are most responsible for stability."

## Limitations
- Model generalization to new pathogens or regions with fundamentally different climate-disease dynamics remains untested; synthetic lags may not capture relevant non-linearities in novel contexts
- Long-horizon uncertainty quantification assumes stationarity of climate-incidence relationships; extreme weather events or abrupt climate shifts could invalidate synthetic lag generation
- Computational requirements for the hybrid CNN-BiLSTM-attention architecture are moderate but not trivial for real-time deployment at national scale

## Confidence

- **High confidence:** Climate data can substitute for real-time case surveillance in autoregressive forecasting (proven across 34 states, sustained 100-week horizons)
- **Medium confidence:** Multi-state pre-training consistently improves peak timing accuracy under data scarcity (robust in cross-state experiments but limited to similar climate zones)
- **Medium confidence:** The synthetic lag generation mechanism generalizes to data-limited regions (strong evidence in cross-state setting but no true zero-data deployment yet)

## Next Checks

1. Deploy ForecastNet-XCL to a new climate zone with limited training data (e.g., Pacific Northwest) and measure transfer learning performance versus single-state training
2. Test synthetic lag generation with forecasted climate data (rather than observed) to quantify error propagation in operational forecasting pipelines
3. Evaluate model performance on a different endemic respiratory pathogen (e.g., influenza) to assess cross-pathogen generalizability