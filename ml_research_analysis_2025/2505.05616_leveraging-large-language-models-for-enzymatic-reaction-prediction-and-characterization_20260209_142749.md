---
ver: rpa2
title: Leveraging Large Language Models for enzymatic reaction prediction and characterization
arxiv_id: '2505.05616'
source_url: https://arxiv.org/abs/2505.05616
tags:
- tasks
- performance
- prediction
- task
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs), specifically
  Llama-3.1 (8B and 70B), for enzymatic reaction prediction tasks including EC number
  classification, forward synthesis, and retrosynthesis. The authors employ LoRA-based
  fine-tuning and compare single-task versus multitask learning approaches across
  various data regimes.
---

# Leveraging Large Language Models for enzymatic reaction prediction and characterization

## Quick Facts
- arXiv ID: 2505.05616
- Source URL: https://arxiv.org/abs/2505.05616
- Reference count: 40
- Key outcome: Fine-tuned Llama-3.1 models achieve 91.7% accuracy for EC class prediction and 25.9% exact product prediction with 70B model.

## Executive Summary
This study evaluates Large Language Models (LLMs) for three enzymatic reaction prediction tasks: EC number classification, forward synthesis, and retrosynthesis. The authors employ LoRA-based fine-tuning on Llama-3.1 (8B and 70B) and compare single-task versus multitask learning approaches across various data regimes. Results demonstrate that fine-tuned LLMs achieve state-of-the-art performance on EC classification (91.7% accuracy) and show significant improvements in synthesis tasks through multitask learning. The models also demonstrate strong performance in low-data scenarios, outperforming traditional ML baselines while maintaining computational efficiency through parameter-efficient fine-tuning.

## Method Summary
The study uses BRENDA subset of ECREACT dataset (8,496 reactions, EC classes 1-6) with SMILES-encoded molecules. Llama-3.1 8B and 70B models are fine-tuned using LoRA adapters with three configurations (light, attention, default) at 4-bit quantization. Data is formatted as user-assistant conversations and split group-wise to prevent leakage. Training uses lr=0.002 with linear decay. Single-task and multitask learning are compared across EC classification, forward synthesis (product prediction), and retrosynthesis (substrate prediction). Evaluation includes exact match accuracy, 5-category classification, and Tanimino similarity scores.

## Key Results
- 70B model achieves 91.7% accuracy for EC1 prediction and 61.7% for EC2
- Multitask learning improves forward synthesis match from 25.9% to 33.8%
- Fine-tuning with N=200 samples achieves 55.3% EC1 accuracy, outperforming zero-shot and XGBoost baselines

## Why This Works (Mechanism)

### Mechanism 1: LoRA-based Parameter-Efficient Fine-Tuning
Low-rank adaptation enables LLMs to acquire biochemical task competence while preserving general capabilities. By decomposing weight updates as ΔW = A·B with rank r ≪ min(n,m), only 0.05-0.52% of parameters are trained, leaving base model frozen. Core assumption: biochemical domain knowledge can be captured in a low-dimensional subspace of the full parameter space.

### Mechanism 2: Multitask Knowledge Transfer via Shared Representations
Training on EC prediction, forward synthesis, and retrosynthesis simultaneously creates synergistic improvements for synthesis tasks. Joint training exposes model to shared enzymatic patterns (substrate-product relationships, reaction class semantics) that transfer across tasks. EC classification provides structural priors for synthesis predictions.

### Mechanism 3: Few-Shot Fine-Tuning Activates Pretrained Biochemical Knowledge
Pretrained LLMs achieve meaningful task performance with minimal task-specific data because fine-tuning reorganizes rather than creates biochemical knowledge. Massive pretraining corpora likely contain biochemical text. Fine-tuning with N=200 samples activates and specializes this latent knowledge rather than learning from scratch.

## Foundational Learning

### Concept 1: SMILES Notation and Canonicalization
Why needed here: All molecules are represented as SMILES strings; evaluating prediction accuracy requires understanding that non-identical strings can represent identical molecules.
Quick check question: Given SMILES "CCO" and "OCC", do these represent the same molecule? How would canonicalization resolve this?

### Concept 2: Enzyme Commission (EC) Number Hierarchy
Why needed here: EC classification follows X.X.X.X format where each digit narrows specificity. Model performance degrades predictably at deeper levels—understanding hierarchy is essential for interpreting results.
Quick check question: If ground truth is EC 1.2.3.4 and model predicts 1.2.3.5, at which level(s) is the prediction correct?

### Concept 3: Tanimoto Similarity on Molecular Fingerprints
Why needed here: The paper evaluates "chemically meaningful" wrong predictions using Tanimoto > 0.85 as a threshold for structural similarity.
Quick check question: If a predicted substrate has Tanimoto similarity of 0.92 to the ground truth but is not an exact match, what biochemical utility might this prediction still offer?

## Architecture Onboarding

### Component Map:
Llama-3.1 (8B or 70B) -> LoRA Adapters (rank r=16, α=32) -> BRENDA subset data pipeline -> Training with lr=0.002, linear decay -> Evaluation metrics

### Critical Path:
1. Preprocess reactions: canonicalize SMILES, group by {substrate, EC} and {product, EC}, assign groups exclusively to one split
2. Format as conversations with task-specific prompts and tagged outputs (`<EC>`, `</EC>`)
3. Configure LoRA variant based on task complexity (default recommended for most cases)
4. Fine-tune with 10% validation split for early stopping
5. Evaluate: exact match for EC; SMILES validity + Tanimoto similarity for molecules

### Design Tradeoffs:
- 8B vs 70B: 8B accessible on single GPU; 70B shows consistent gains (EC1: +5.3%, FS match: +7.5%) but requires more resources
- ST vs MT: MT improves FS (+7.9%) and RS (+5.3%) but degrades EC1 (-5.3%)—choose based on deployment priority
- LoRA size: Default outperforms Light/Attention in most cases, but 2-3× more trainable parameters

### Failure Signatures:
- Hierarchical EC collapse: Accuracy drops from 91.7% (EC1) → 61.7% (EC2) → 49.2% (EC3)
- Class 4/5 confusion: Systematic misclassification between these classes
- High invalid SMILES rate: If >10%, check prompt formatting or reduce LoRA rank
- Retrosynthesis harder than forward synthesis: RS has more valid-but-incorrect predictions due to larger output space

### First 3 Experiments:
1. Establish ST baseline: Fine-tune Llama-3.1 8B with LoRA default on single-task EC prediction; measure accuracy at EC1/EC2/EC3 levels
2. Validate MT benefit: Train same model on merged MT dataset; compare FS/RS gains vs. EC degradation to assess tradeoff
3. Low-data stress test: Train with N=200 samples; verify improvement over zero-shot (target: >2× baseline) and XGBoost to confirm few-shot advantage

## Open Questions the Paper Calls Out

### Open Question 1
How can LLM fine-tuning strategies be adapted to enforce hierarchical dependencies in Enzyme Commission (EC) classification, particularly for rare subclasses?
Basis: The authors state models "struggle with rare EC subclasses and hierarchical classification schemes."
Unresolved: Current LoRA-based text generation approach treats EC digits as standard tokens, failing to explicitly model the "is-a" taxonomic relationship between the first digit (class) and subsequent digits (subclasses).
Resolution evidence: Implementing hierarchical loss function or custom classification head that constrains predictions to valid parent-child relationships, resulting in improved EC3 accuracy for rare subclasses.

### Open Question 2
To what extent does pretraining data contamination inflate the performance of LLMs on enzymatic reaction prediction tasks?
Basis: Section 3.5 notes that "potential data leakage" is a concern because public datasets like BRENDA were likely seen during the LLM's pretraining.
Unresolved: It is unclear if high accuracy scores stem from biochemical reasoning or mere memorization of public data present in the pretraining corpus.
Resolution evidence: A benchmark evaluation on a "truly held-out" set of newly discovered reactions (post-dating the LLM's training cutoff) that cannot be found in public text corpora.

### Open Question 3
Does scaling training to the full ECREACT dataset effectively mitigate performance degradation in underrepresented enzyme classes?
Basis: Section 3.5 suggests that "Expanding training to the full ECREACT dataset... could mitigate this issue," specifically referencing the limited diversity and exclusion of Class 7 (translocases).
Unresolved: The study was restricted to the BRENDA subset due to computational constraints, leaving the benefits of full-dataset scaling untested.
Resolution evidence: A comparative study fine-tuning Llama-3.1 on the full ECREACT dataset (n=62,222) showing statistically significant accuracy gains for previously excluded or rare EC classes.

## Limitations

- Data leakage risk from pretraining data contamination, creating ambiguity in interpreting few-shot performance gains
- Hierarchical classification collapse with accuracy degrading from 91.7% (EC1) to 49.2% (EC3)
- Metric interpretation challenges where chemically valid but structurally different predictions may represent fundamentally different biochemical outcomes

## Confidence

High Confidence:
- LoRA-based fine-tuning successfully reduces computational demands while maintaining competitive performance
- 70B model consistently outperforms 8B model across all tasks
- Multitask learning improves forward and retrosynthesis predictions through shared representation learning

Medium Confidence:
- Few-shot learning advantage over XGBoost baseline
- Chemical validity of non-canonical SMILES predictions
- Pretraining data leakage impact

Low Confidence:
- Generalizability to unseen enzymatic reactions outside BRENDA subset
- Scalability to full ECREACT dataset
- Performance on real-world industrial biocatalysis applications

## Next Checks

1. **Pretraining Data Audit**: Conduct systematic analysis to identify overlap between BRENDA-based fine-tuning data and LLM pretraining corpora. Compare reaction SMILES and EC numbers against publicly available model cards and training data reports to quantify potential leakage impact.

2. **Cross-Dataset Generalization Test**: Evaluate fine-tuned models on independent enzymatic reaction datasets not derived from BRENDA (e.g., SABIO-RK, Rhea database) to assess true generalization capability beyond the training domain.

3. **Industrial Application Pilot**: Test model predictions in a controlled biocatalysis workflow with experimental validation, focusing on retrosynthesis predictions where the model achieves only 13.9% exact match. Measure practical utility of chemically similar but structurally different predictions in real laboratory settings.