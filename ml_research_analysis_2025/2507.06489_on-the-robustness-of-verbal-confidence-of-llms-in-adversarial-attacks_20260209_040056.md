---
ver: rpa2
title: On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks
arxiv_id: '2507.06489'
source_url: https://arxiv.org/abs/2507.06489
tags:
- confidence
- attacks
- scores
- adversarial
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the first comprehensive study of adversarial
  attacks targeting verbal confidence in large language models (LLMs). The authors
  propose two attack frameworks: perturbation-based (using methods like TextFooler
  and TextBugger) and jailbreak-based (using optimized trigger tokens called ConfidenceTriggers).'
---

# On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks

## Quick Facts
- **arXiv ID:** 2507.06489
- **Source URL:** https://arxiv.org/abs/2507.06489
- **Reference count:** 40
- **Primary result:** Adversarial attacks can reduce LLM verbal confidence by up to 30% and flip 100% of correct answers

## Executive Summary
This paper introduces the first comprehensive study of adversarial attacks targeting verbal confidence in large language models. The authors propose two attack frameworks: perturbation-based (using methods like TextFooler and TextBugger) and jailbreak-based (using optimized trigger tokens called ConfidenceTriggers). Experiments on three datasets and two models show that these attacks can reduce average confidence by up to 30% and cause up to 100% answer changes on originally correctly predicted samples. The study finds that current mitigation strategies are largely ineffective or counterproductive. These findings highlight the vulnerability of verbal confidence mechanisms in LLMs, emphasizing the need for robust confidence expression systems to ensure transparency and safety in AI deployments.

## Method Summary
The paper introduces two frameworks for attacking verbal confidence: perturbation-based attacks (VCA-TF, VCA-TB, Typos, SubSwapRemove) and jailbreak-based attacks (ConfidenceTriggers, ConfidenceTriggers-AutoDAN). Perturbation attacks modify input text by substituting words with synonyms or introducing typos while maintaining semantic similarity (USE similarity > 0.8). ConfidenceTriggers uses a genetic algorithm to optimize trigger tokens that reduce confidence when appended to prompts. The study tests these attacks on three QA datasets (MedMCQA, TruthfulQA, StrategyQA) using two LLMs (Llama-3-8B and GPT-3.5-turbo) with five confidence elicitation methods (Base, CoT, MS, Self-Consistency, and variants).

## Key Results
- Adversarial attacks reduced average verbal confidence by up to 30% across datasets
- Attacks caused up to 100% answer changes on originally correctly predicted samples
- Perturbation-based attacks affected fewer samples than jailbreak-based attacks due to similarity constraints
- Current mitigation strategies (perplexity-based filtering) were largely ineffective or counterproductive

## Why This Works (Mechanism)
None

## Foundational Learning
- **Confidence elicitation methods (CEMs):** Different prompt templates that instruct models to output answers with confidence scores. Needed to test attack transferability across various confidence expression paradigms.
- **USE similarity metric:** Measures semantic similarity between original and modified text. Critical threshold (0.8) ensures attacks preserve meaning while affecting confidence.
- **Genetic algorithm optimization:** Used to evolve trigger tokens that maximally reduce confidence. Key parameters include population size, mutation rate, and tournament selection.
- **Brier score and ECE metrics:** Evaluate calibration quality. Lower scores indicate better alignment between confidence and accuracy.

## Architecture Onboarding
**Component map:** Input text -> Attack method (perturbation/jailbreak) -> Modified text -> LLM with CEM -> Confidence score/Answer
**Critical path:** Text modification → Confidence reduction → Answer flip on originally correct samples
**Design tradeoffs:** Semantic similarity vs. attack effectiveness; perturbation breadth vs. precision
**Failure signatures:** Regex extraction failures, low semantic similarity violations, convergence issues in genetic algorithm
**First experiments:** 1) Implement Base CEM and confidence extraction regex, 2) Test VCA-TF on small sample, 3) Run ConfidenceTriggers with default parameters

## Open Questions the Paper Calls Out
- **Sentence-level attacks:** The authors state that "Sentence-level attacks can be constructed using a similar framework which we leave for future work."
- **Confidence inflation attacks:** "VCAs aimed at increasing confidence are possible, but leave less room to study attack effectiveness..."
- **Domain-specific susceptibility:** Authors "strongly encourage future work to explore how the unique properties of different domains and tasks affect VCAs."
- **Balanced defense mechanisms:** Need for methods that are "robust against VCAs but not so robust that they prevent LLMs in responding adequately to situations where the confidence should change."

## Limitations
- Exact confidence-related word list for ConfidenceTriggers initialization not specified
- Complete prompt templates for all five CEMs not fully provided
- Perplexity thresholds for defense experiments not quantified
- Focus on benchmark datasets with low ecological validity

## Confidence
- **High:** Attack effectiveness reducing confidence by 30% and flipping answers on 100% of correct samples
- **Medium:** Ineffectiveness of current mitigation strategies due to underspecified defense implementation
- **Medium:** Trade-off between perturbation-based attack success and semantic similarity preservation

## Next Checks
1. Implement defense experiments with specified perplexity thresholds to validate counterintuitive results
2. Test attack transferability across CEM variants to confirm ~80% overlap finding
3. Validate ConfidenceTriggers convergence sensitivity to genetic algorithm parameters