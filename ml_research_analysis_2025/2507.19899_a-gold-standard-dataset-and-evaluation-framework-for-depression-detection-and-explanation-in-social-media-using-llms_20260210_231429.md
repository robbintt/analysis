---
ver: rpa2
title: A Gold Standard Dataset and Evaluation Framework for Depression Detection and
  Explanation in Social Media using LLMs
arxiv_id: '2507.19899'
source_url: https://arxiv.org/abs/2507.19899
tags:
- symptom
- post
- prompting
- mental
- depression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors developed a gold-standard dataset of 1,017 social media
  posts annotated by licensed mental health professionals with both depressive spans
  and 12 DSM-5-aligned symptom categories. They proposed a multi-dimensional evaluation
  framework combining span-level recall, symptom-level F1, and LLM-based coherence
  scoring.
---

# A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs

## Quick Facts
- arXiv ID: 2507.19899
- Source URL: https://arxiv.org/abs/2507.19899
- Reference count: 36
- Key outcome: Few-shot prompting improves symptom classification but reduces span-level recall; Gemini 2.5 Pro achieves highest overall quality score of 0.7171

## Executive Summary
This paper introduces a gold-standard dataset of 1,017 social media posts annotated by licensed mental health professionals with depressive spans and 12 DSM-5-aligned symptom categories. The authors propose a multi-dimensional evaluation framework combining span-level recall, symptom-level F1, and LLM-based coherence scoring. Experiments with GPT-4.1, Claude 3.7, and Gemini 2.5 Pro under zero-shot and few-shot prompting reveal that few-shot prompting generally improves symptom identification but often decreases span detection, suggesting a trade-off between symptom accuracy and span fidelity. The results highlight the need for hybrid approaches and careful prompt design when applying LLMs to clinically sensitive mental health tasks.

## Method Summary
The authors developed a gold-standard dataset of 1,017 social media posts annotated by licensed mental health professionals with both depressive spans and 12 DSM-5-aligned symptom categories. They proposed a multi-dimensional evaluation framework combining span-level recall, symptom-level F1, and LLM-based coherence scoring. Experiments with GPT-4.1, Claude 3.7, and Gemini 2.5 Pro under zero-shot and few-shot prompting revealed that few-shot prompting generally improved symptom identification, with Gemini 2.5 Pro achieving the highest overall quality score of 0.7171. However, span detection often decreased with few-shot examples, suggesting a trade-off between symptom accuracy and span fidelity. GPT-4.1 performed best in zero-shot settings for span recall, while Claude 3.7 led in zero-shot symptom identification.

## Key Results
- Few-shot prompting improves symptom F1 but reduces span recall, revealing a trade-off between symptom accuracy and span fidelity
- Gemini 2.5 Pro achieved the highest overall quality score (0.7171) under few-shot prompting
- GPT-4.1 excelled at span detection in zero-shot settings (0.618 span score)
- Claude 3.7 led in zero-shot symptom identification (0.588 symptom F1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting improves symptom classification accuracy but reduces span-level recall.
- Mechanism: In-context examples anchor model attention toward stylistic and conceptual patterns that map to symptom categories, rather than encouraging exhaustive extraction of text spans. The paper observes that zero-shot outputs tend to be longer and more exploratory, producing broader coverage of depressive phrases at the cost of structured symptom attribution.
- Core assumption: The trade-off arises from exemplars narrowing the model's generative focus rather than from randomness in decoding.
- Evidence anchors:
  - [section 6.1.2] "Few-shot prompting did not lead to consistent performance improvements... span detection metrics were particularly revealing: GPT-4.1's span score declined notably from 0.618 to 0.578."
  - [section 6.1.2] "One plausible explanation is that the in-context examples anchor the model's attention to stylistic patterns or conceptual structures, rather than encouraging comprehensive span extraction."
  - [corpus] Corpus neighbor "Generating Medically-Informed Explanations for Depression Detection using LLMs" (arXiv:2503.14671) similarly emphasizes explanation faithfulness but does not directly confirm this trade-off mechanism; evidence remains specific to this paper's experimental setup.
- Break condition: If few-shot examples are constructed to explicitly require comprehensive span listing alongside symptom mapping, the trade-off may weaken or disappear. The paper did not test this variation.

### Mechanism 2
- Claim: Weighted combination of span recall, symptom F1, and coherence scores produces a clinically meaningful aggregate quality metric.
- Mechanism: Clinical experts prioritized symptom attribution (F1) as most actionable, assigning it 40% weight. Span recall (30%) ensures explanations remain grounded in source text. Coherence (30%) ensures outputs are interpretable. The 70/30 split between clinical alignment and communicative clarity reflects diagnostic utility over fluency.
- Core assumption: Expert-chosen weights generalize across clinical contexts; different clinical priorities could justify alternate weightings.
- Evidence anchors:
  - [section 3.2] "These values were determined in consultation with clinical experts, who prioritized accurate symptom attribution (captured by Symptom-F1) as the most clinically actionable metric."
  - [section 3.2] "This weighting scheme reflects a 70% emphasis on clinical alignment (span recall and symptom attribution) and 30% on the structural and communicative clarity of the explanation."
  - [corpus] Corpus evidence on weighted evaluation frameworks for clinical NLP is sparse; the FMR-weighted neighbor average is 0.474, suggesting this specific weighting scheme is not yet externally validated.
- Break condition: If downstream clinical workflows prioritize screening recall over diagnostic precision, the 40% weight on symptom F1 may be suboptimal.

### Mechanism 3
- Claim: Different proprietary LLMs exhibit systematic specializations: GPT-4.1 excels at span detection, Claude 3.7 at symptom identification, Gemini 2.5 Pro requires few-shot guidance to perform well.
- Mechanism: Pre-training and instruction-tuning differences create inductive biases. GPT-4.1 appears more sensitive to surface-level text alignment in zero-shot settings. Claude 3.7's instruction tuning may emphasize conceptual categorization. Gemini 2.5 Pro struggles with zero-shot format compliance but responds strongly to structured examples that constrain output organization.
- Core assumption: These differences reflect stable architectural or tuning properties rather than sampling variance or prompt wording artifacts.
- Evidence anchors:
  - [section 6.2.1] "GPT-4.1 demonstrated the highest overall performance in the zero-shot setting, achieving the top span evaluation score (0.618)."
  - [section 6.2.2] "Claude 3.7 demonstrated the highest zero-shot symptom identification score (0.588) but consistently underperformed in span evaluation."
  - [section 6.2.3] "Gemini 2.5 Pro was evaluated exclusively in the few-shot setting, as its zero-shot outputs exhibited significant formatting inconsistencies."
  - [corpus] Corpus neighbor papers (e.g., ReDSM5, MentalLLM benchmarks) compare model performance on depression detection but do not provide direct replication of this three-way specialization pattern.
- Break condition: If models are fine-tuned on domain-specific mental health data, these baseline specializations may converge or shift.

## Foundational Learning

- Concept: **DSM-5 depression symptom categories**
  - Why needed here: All annotations and evaluations are grounded in 12 clinically defined symptoms (e.g., anhedonia, suicidal ideation, hopelessness). Without understanding these categories, you cannot interpret symptom F1 scores or annotation quality.
  - Quick check question: Can you name at least three of the 12 DSM-5 symptom categories used in this dataset and describe what textual signals would indicate each?

- Concept: **Span-level vs. document-level annotation**
  - Why needed here: The paper's key contribution is dual-layer annotation: (1) text spans indicating depression and (2) symptom labels. This enables fine-grained evaluation of explanation faithfulness. Most prior datasets only provide post-level binary labels.
  - Quick check question: Given a social media post, can you identify specific phrases ("spans") that express depressive content versus merely labeling the whole post as "depressed"?

- Concept: **Zero-shot vs. few-shot in-context learning**
  - Why needed here: The central experimental comparison tests whether providing domain-adapted examples changes model behavior. Understanding this distinction is essential for interpreting the trade-off results.
  - Quick check question: If you give an LLM a task description without examples (zero-shot), versus the same task with 4 labeled examples (few-shot), what differences in output structure and focus would you expect?

## Architecture Onboarding

- Component map:
  Dataset -> Prompting module -> LLM inference layer -> Evaluation pipeline -> Aggregation

- Critical path:
  1. Load post and gold annotations
  2. Format prompt (zero-shot or few-shot)
  3. Call LLM API, retrieve explanation text
  4. Extract spans mentioned in explanation; compute recall vs. gold spans
  5. Extract symptom keywords from explanation; compute F1 vs. gold symptom labels
  6. Send post + explanation to LLM-judge; retrieve coherence score
  7. Aggregate into combined quality score

- Design tradeoffs:
  - **Exact/fuzzy matching for span recall**: Prioritizes faithfulness but may penalize valid paraphrases. Paper uses hybrid matching; exact thresholds are not specified.
  - **Keyword-based symptom extraction**: Simple and transparent but may miss synonyms or nuanced clinical language. More sophisticated classifiers could improve F1 but introduce complexity.
  - **LLM-as-judge for coherence**: Scalable but introduces model bias. Paper uses fixed prompt with GPT-4.1 to reduce variance.
  - **Fixed weights (0.3/0.4/0.3)**: Clinically grounded but may not generalize. Different applications (screening vs. diagnosis) may require reweighting.

- Failure signatures:
  - **Gemini zero-shot format failures**: Output does not separate symptom explanations or misinterprets prompt intent (Section 6.2.3). If you see unstructured or off-topic responses, switch to few-shot or validate prompt parsing.
  - **Span recall drops with few-shot**: If span recall decreases >5% when switching from zero-shot to few-shot, you are hitting the documented trade-off (Section 6.1.2). Consider hybrid approaches or prompt variants that explicitly require span listing.
  - **Coherence score stuck near ceiling**: All models scored 4.9-5.0; this metric may lack discriminative power for high-quality outputs (Table 1). If you need finer-grained coherence differentiation, redesign rubric.

- First 3 experiments:
  1. **Reproduce zero-shot vs. few-shot trade-off**: Run GPT-4.1 and Claude 3.7 on a 100-post subset. Confirm that few-shot improves symptom F1 but reduces span recall. Document magnitude of shift.
  2. **Test hybrid prompting**: Modify few-shot prompt to explicitly require listing all depressive spans before symptom mapping. Measure whether this mitigates the span recall drop without hurting symptom F1.
  3. **Validate LLM-judge coherence scores against human ratings**: Have 2-3 clinical experts rate coherence for 50 explanations. Compute correlation with LLM-judge scores. If correlation is low (<0.5), redesign the coherence rubric.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hybrid system architecture, assigning different LLMs to specific sub-tasks, resolve the observed trade-off between span detection fidelity and symptom identification accuracy?
- Basis in paper: [explicit] The authors suggest in their analysis that "a hybrid system—using GPT-4.1 for identifying depressive spans and Claude 3.7 or Gemini 2.5 Pro for symptom interpretation may provide a more balanced and clinically useful solution."
- Why unresolved: The experiments revealed that while few-shot prompting improves symptom identification, it often degrades span detection. The study only evaluated models independently, leaving the efficacy of combining their distinct strengths untested.
- What evidence would resolve it: Empirical results from an ensemble architecture that routes tasks to specific models, demonstrating a statistically significant improvement in the combined Quality Score over single-model baselines.

### Open Question 2
- Question: Does supervised fine-tuning of open-source models on this expert-annotated dataset outperform proprietary model prompting in clinical explanation tasks?
- Basis in paper: [explicit] The paper notes in the Limitations section that "We do not evaluate fine-tuned open-source models" and explicitly lists "explore hybrid prompting–fine-tuning pipelines" under Future Work.
- Why unresolved: The current study is restricted to prompting proprietary APIs. It remains unclear if the gold-standard dataset (n=1,017) is sufficient to train open-source models to exceed the performance of few-shot GPT-4.1 or Gemini.
- What evidence would resolve it: A comparative benchmark showing a fine-tuned open-source LLM achieving higher Symptom-F1 and Span-Recall scores than the current proprietary few-shot baselines.

### Open Question 3
- Question: How can prompting techniques be optimized to minimize hallucinations while maintaining the explanation length and conciseness required for clinical utility?
- Basis in paper: [explicit] The authors state in the Conclusion: "We also aim to refine prompting techniques to encourage conciseness and reduce hallucinations."
- Why unresolved: The study found that zero-shot responses were often "longer and more exploratory," while few-shot examples sometimes narrowed focus too much or introduced stylistic biases. The optimal balance for clinical safety is not yet defined.
- What evidence would resolve it: A qualitative and quantitative analysis of new prompt designs (e.g., constrained decoding) that successfully lowers the rate of factual errors without reducing the coverage of depressive spans.

## Limitations
- The dataset used in this study is not publicly available, preventing independent verification of the annotations and experimental results
- The keyword-based symptom extraction method lacks detail on exact symptom-to-keyword mappings, potentially affecting reproducibility
- The coherence scoring rubric, while provided, may not be sufficiently discriminative given the near-ceiling scores observed across all models

## Confidence

- **High Confidence**: The trade-off between span recall and symptom F1 under different prompting strategies (zero-shot vs. few-shot) is well-supported by the experimental results and mechanism explanation
- **Medium Confidence**: The claim that different LLMs exhibit systematic specializations (GPT-4.1 for spans, Claude 3.7 for symptoms, Gemini 2.5 Pro needing few-shot) is supported but based on limited model comparisons
- **Low Confidence**: The clinical relevance and generalizability of the weighted evaluation framework (0.3/0.4/0.3) without external validation or sensitivity analysis

## Next Checks

1. **Dataset Release and Replication**: Request public release of the 1,017 annotated posts or conduct replication study with independent expert annotations on a held-out test set to verify span and symptom labeling quality
2. **Hybrid Prompting Experiment**: Test modified few-shot prompts that explicitly require comprehensive span listing before symptom mapping to determine if the documented trade-off can be mitigated without sacrificing symptom accuracy
3. **Coherence Scoring Validation**: Conduct human expert ratings of explanation coherence for 50 posts and compute correlation with LLM-as-judge scores to validate the discriminative power of the current rubric