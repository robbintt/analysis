---
ver: rpa2
title: 'Random Matrix Theory for Deep Learning: Beyond Eigenvalues of Linear Models'
arxiv_id: '2506.13139'
source_url: https://arxiv.org/abs/2506.13139
tags:
- random
- matrix
- linear
- definition
- regime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive overview of Random Matrix Theory
  (RMT) applied to deep learning beyond traditional eigenvalue analysis of linear
  models. The authors introduce the concept of High-dimensional Equivalent to address
  technical challenges in analyzing nonlinear machine learning models in the proportional
  regime where sample size, dimension, and model parameters are all large and comparable.
---

# Random Matrix Theory for Deep Learning: Beyond Eigenvalues of Linear Models

## Quick Facts
- arXiv ID: 2506.13139
- Source URL: https://arxiv.org/abs/2506.13139
- Authors: Zhenyu Liao; Michael W. Mahoney
- Reference count: 40
- Primary result: Introduces High-dimensional Equivalent framework for analyzing nonlinear deep learning models in proportional regime

## Executive Summary
This paper provides a comprehensive overview of Random Matrix Theory (RMT) applied to deep learning beyond traditional eigenvalue analysis of linear models. The authors introduce the concept of High-dimensional Equivalent to address technical challenges in analyzing nonlinear machine learning models in the proportional regime where sample size, dimension, and model parameters are all large and comparable. The core methodology extends RMT to handle three key challenges: high dimensionality, nonlinearity, and generic eigenspectral functionals. For linear models, the authors develop Deterministic Equivalent for Resolvent framework, providing precise characterizations of training and generalization performance. For nonlinear models, they introduce High-dimensional Linearization using orthogonal polynomials, enabling analysis of single-hidden-layer and deep neural networks.

## Method Summary
The paper presents a systematic approach to analyzing deep learning models using Random Matrix Theory in the proportional regime. For linear models, it develops the Deterministic Equivalent for Resolvent framework, where scalar functionals of random matrices concentrate around deterministic equivalents. For nonlinear models, it introduces High-dimensional Linearization using Hermite polynomial expansions, reducing nonlinear analysis to linear RMT problems. The methodology enables precise asymptotic predictions of training and generalization performance for both linear and nonlinear models, capturing phenomena like double descent and scaling laws that classical methods fail to address.

## Key Results
- Precise asymptotic predictions of in-sample and out-of-sample risks for linear regression showing double descent behavior in the proportional regime
- Scaling laws for training errors of shallow networks that outperform linear models for certain kernel structures
- Linearization of conjugate kernel matrices for deep networks revealing a "curse of depth" for untrained random networks
- Unified framework capturing scaling laws, double descent, and nonlinear learning dynamics

## Why This Works (Mechanism)

### Mechanism 1
Scalar eigenspectral functionals of large random matrices concentrate around deterministic equivalents, enabling precise performance characterization. The resolvent Q(z) = (X - zI)^(-1) encodes both eigenvalues and eigenvectors. Through contour integration, scalar eigenspectral functionals can be expressed as -(1/2πi|I|) ∮_Γ f(z) a^T Q(z) b dz. In the proportional regime, while the random matrix X does NOT concentrate around E[X], its scalar functionals DO concentrate: f(X) - E[f(X)] → 0 almost surely.

### Mechanism 2
Nonlinear neural network activations can be linearized through Hermite polynomial expansions, reducing nonlinear analysis to linear random matrix theory. In the CLT regime, the random variable remains random and follows a non-degenerate distribution. Using orthogonal polynomials, we can express E[ϕ(ξ)] and linearize it. The kernel matrix admits a Linear Equivalent: K̃_ϕ = a^2_{ϕ;0} 1_n 1_n^T + a^2_{ϕ;1} X^T X + a^2_{ϕ;2} (1/p) 1_n 1_n^T + (ν_ϕ - a^2_{ϕ;0} - a^2_{ϕ;1}) I_n.

### Mechanism 3
Out-of-sample risk exhibits double descent—peaking at the interpolation threshold (n = p) then decreasing—due to resolvent divergence in the ridgeless limit. Ridge regression involves the resolvent Q(-γ). In the proportional regime, the Stieltjes transform m(z) satisfies the Marc̆enko-Pastur equation. As γ → 0, m(-γ) and the resolvent diverge when c = p/n = 1, creating a singularity. The out-of-sample risk peaks at c = 1.

## Foundational Learning

- **Concept: Stieltjes Transform and Marc̆enko-Pastur Law**
  - Why needed here: The Stieltjes transform m_μ(z) = ∫ (t-z)^{-1} μ(dt) is the fundamental tool connecting resolvents to eigenvalue distributions. It uniquely determines the probability measure μ and satisfies the Marc̆enko-Pastur equation, which characterizes all asymptotic behavior in the proportional regime.
  - Quick check question: For the sample covariance matrix Ĉ = (1/n) XX^T with X ∈ R^{p×n} having i.i.d. N(0,1) entries, why does m(z) satisfy czm^2 - (1-c-z)m + 1 = 0, and what happens to m(-γ) as γ → 0 when c = 1?

- **Concept: Concentration of Measure in High Dimensions**
  - Why needed here: Understanding why high-dimensional random vectors don't concentrate around their mean (∥x - E[x]∥ ≈ √n) but scalar functionals do (f(x) - E[f(x)] → 0) is essential for grasping why Deterministic Equivalents work.
  - Quick check question: If x, y ∈ R^n are independent random vectors with i.i.d. entries, why is the angle between x, y, and the origin approximately 90°, and what does this imply for concentration of bilinear forms a^T Q b?

- **Concept: Hermite Polynomials and Orthogonal Expansions**
  - Why needed here: Hermite polynomials {He_i} form an orthonormal basis for L^2(μ) with μ = N(0,1). The coefficients a_{ϕ;i} = E[ϕ(ξ)He_i(ξ)] completely determine how a nonlinear activation affects network performance through the Linear Equivalent.
  - Quick check question: For ReLU activation ϕ(t) = max(t, 0), compute a_{ϕ;0} = E[ϕ(ξ)], a_{ϕ;1} = E[ξϕ(ξ)], and a_{ϕ;2} using Stein's lemma. How do these values affect the kernel matrix linearization?

## Architecture Onboarding

- **Component map:** Input/Parameter Layer (X, W, ratio c) -> Resolvent Core (Q(z), Q̃(z)) -> Nonlinear Kernel Layer (K, K̃_ϕ) -> Deep Network Extension (CK K_ℓ) -> Performance Metrics (R_in, R_out)
- **Critical path:**
  1. Determine regime: If n ≫ p → classical (LLN works); if n ∼ p → proportional (need RMT)
  2. For linear models: Solve δ(z) = (1/n) tr[C Q̃(z)] for the Stieltjes transform, then evaluate performance via contour integration
  3. For nonlinear models: Compute Hermite coefficients a_{ϕ;0}, a_{ϕ;1}, a_{ϕ;2} for activation ϕ, form Linear Equivalent K̃_ϕ
  4. Evaluate performance: Use Propositions 1 and 2 for asymptotic R_in, R_out, E_train, E_test
  5. Check for pathologies: Double descent at n = p when γ → 0; curse of depth when α_{ℓ,1} → 0

- **Design tradeoffs:**
  - **Depth vs. Expressivity:** "Curse of depth"—untrained random DNNs with a_{ϕ;1} < 1 have K_ℓ → I_n as L → ∞, becoming independent of input. Trade deep architecture for trainability.
  - **Width vs. Analytical Tractability:** Ultra-wide (d_ℓ ≫ max(n,p)) gives constant NTK and simpler analysis via Gaussian process limit; proportional (d_ℓ ∼ n ∼ p) is more realistic but requires full RMT machinery.
  - **Regularization vs. Interpolation:** γ > 0 smooths double descent peak but changes scaling laws (E_train ∝ n^{-1} for linear, but depends on kernel eigendecay for nonlinear); γ → 0 enables interpolation but risks numerical instability.

- **Failure signatures:**
  1. Out-of-sample risk exploding near n = p: Double descent singularity; increase regularization γ or avoid exact interpolation threshold
  2. Training error not following theoretical prediction: Check if kernel K has assumed eigendecay; exponential decay → log(n)/n rate; polynomial decay → n^{-(1+β)} rate
  3. Deep network CK converging to identity: α_{ℓ,1} → 0 too fast; activation has a_{ϕ;1} < 1; network not learning from data
  4. Theory-empirical gap: Data not isotropic/uniform on sphere (violates Theorem 7 assumption); structured data requires different analysis

- **First 3 experiments:**
  1. Reproduce double descent curve: Linear regression with p = 512, vary n/p from 0.5 to 2.0, measure R_out for γ = 10^{-5} and γ = 10^{-1}. Expect peak at n/p = 1 for small γ, smooth curve for larger γ. Compare to R_{out,n∼p}(γ) from Proposition 1.
  2. Validate Hermite linearization: Single-hidden-layer NN with d ∈ {100, 500, 1000, 2000}, n = 1024, p = 784 (MNIST). Measure E_train and compare to Ē_train from Proposition 2. For ReLU: verify a_{ϕ;0} = 1/2, a_{ϕ;1} = 1/√(2π), a_{ϕ;2} = 1/2 numerically via Monte Carlo.
  3. Characterize curse of depth: Random DNN with L ∈ {1, 2, 5, 10} layers, tanh activation, n = p = 512. Compute eigenvalue distribution of CK matrix K_ℓ for each depth. Expect: spectrum narrows toward 1 as L increases (since a_{tanh;1} ≈ 0.609 < 1 implies α_{ℓ,1} → 0). Plot ||K_ℓ - I_n||_F vs. depth.

## Open Questions the Paper Calls Out

### Open Question 1
Can asymptotic RMT methods (Deterministic Equivalent, Linear Equivalent) be rigorously combined with non-asymptotic RMT techniques to provide finite-sample guarantees for deep learning models? The paper focuses on asymptotic proportional regime analysis; non-asymptotic approaches introduce additional technical complexity that would require reconciling different mathematical frameworks.

### Open Question 2
How can the RMT framework be extended to fully characterize learning dynamics of deep networks via the Neural Tangent Kernel beyond the ultra-wide regime? The NTK analysis in Section 6 relies on the ultra-wide assumption (dℓ ≫ max(n, p)) where NTK remains approximately constant during training; relaxing this requires new technical tools.

### Open Question 3
Can the "curse of depth" for untrained random networks be overcome through training, or does it persist as a fundamental limitation for certain activation/architecture choices? The linearization analysis applies to random, untrained weights; trained networks may exhibit different spectral properties not captured by the CK framework.

## Limitations

- The framework relies critically on sub-gaussian assumptions and the proportional regime, which may not hold for structured real-world data
- The linearization approach using Hermite polynomials assumes isotropic data distributions, potentially limiting applicability to correlated inputs
- The double descent analysis assumes near-ridgeless conditions that may not reflect practical regularization choices

## Confidence

- **High confidence**: Linear model deterministic equivalents and Marc̆enko-Pastur predictions (well-established RMT results)
- **Medium confidence**: Nonlinear linearization framework (relies on strong distributional assumptions)
- **Medium confidence**: Deep network curse of depth (analytic results confirmed for specific architectures but may not generalize)

## Next Checks

1. **Robustness to data structure**: Test the theory with correlated input data (e.g., PCA-correlated features) to identify regime boundaries where assumptions break down
2. **Activation function sensitivity**: Systematically vary activation functions beyond ReLU/tanh to identify which Hermite coefficient properties are essential for linearization
3. **Finite-sample validation**: Quantify error between theoretical predictions and empirical results for realistic sample sizes (n, p ~ 100-1000) to establish when the asymptotic regime provides accurate approximations