---
ver: rpa2
title: 'Variance-Optimal Arm Selection: Regret Minimization and Best Arm Identification'
arxiv_id: '2505.11985'
source_url: https://arxiv.org/abs/2505.11985
tags:
- variance
- regret
- bound
- arms
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of selecting the arm with the
  highest variance from a set of K independent arms in multi-armed bandit settings.
  The authors propose algorithms for two settings: regret minimization and best arm
  identification (BAI).'
---

# Variance-Optimal Arm Selection: Regret Minimization and Best Arm Identification

## Quick Facts
- **arXiv ID:** 2505.11985
- **Source URL:** https://arxiv.org/abs/2505.11985
- **Reference count:** 38
- **Primary result:** Introduces algorithms for variance maximization in multi-armed bandits achieving O(log n) regret and exponential error decay for best arm identification.

## Executive Summary
This paper addresses the problem of selecting the arm with the highest variance from K independent arms in multi-armed bandit settings. The authors propose two algorithms: UCB-VV for regret minimization and SHVV for best arm identification. UCB-VV achieves O(log n) regret through variance-aware upper confidence bounds, while SHVV uses sequential halving to identify the optimal arm with exponentially decaying error probability. The framework extends from bounded distributions to sub-Gaussian distributions using novel concentration inequalities for sample variance and empirical Sharpe ratio. Empirical results demonstrate UCB-VV outperforming epsilon-greedy baselines, while SHVV shows superior performance against uniform sampling in fixed budget settings.

## Method Summary
The paper proposes two algorithms for variance maximization: UCB-VV for regret minimization and SHVV for best arm identification. UCB-VV uses an index combining biased empirical variance with exploration bonus, achieving O(log n) regret through standard UCB-style analysis. SHVV performs log₂(K) elimination rounds, pulling surviving arms equally and eliminating the bottom half by variance each round, achieving exponential error decay. The framework extends from bounded to sub-Gaussian rewards using Bernstein-type concentration inequalities for sample variance and Sharpe ratio, enabling variance optimization under weaker distributional assumptions.

## Key Results
- UCB-VV achieves O(log n) regret upper bound for variance maximization, proven order-optimal through matching lower bound
- SHVV error probability upper bound matches the corresponding lower bound, decaying as exp(-n/(log(K)H))
- Novel concentration inequalities for sample variance and empirical Sharpe ratio under sub-Gaussian assumptions enable extension beyond bounded rewards
- Real-world case study shows proposed approach achieves substantially higher cumulative rewards compared to standard UCB in call option trading on GBM-simulated stocks

## Why This Works (Mechanism)

### Mechanism 1: Variance-Aware Upper Confidence Bound (UCB-VV)
The algorithm constructs an index $B_i^{VV}(t) = \bar{V}_i(t) + \sqrt{\frac{2\log t}{s_i}}$ combining biased empirical variance with exploration bonus. Arms with underestimated variance are pulled more often, while the bonus shrinks as pull count grows, concentrating pulls on the true optimal arm. The confidence bonus ensures sufficient exploration while the variance estimate guides exploitation.

### Mechanism 2: Sequential Halving with Variance Elimination (SHVV)
The fixed budget $n$ is divided equally among $\log_2(K)$ rounds. In each round, all surviving arms are pulled equally; the bottom half by empirical variance are eliminated. This concentrates samples on promising arms while guaranteeing the optimal arm survives with high probability, achieving exponential error decay.

### Mechanism 3: Sub-Gaussian Concentration for Sample Variance and Sharpe Ratio
Bernstein-type concentration bounds enable extension from bounded to sub-Gaussian reward distributions. For sub-Gaussian random variables with parameter $v^2$, the sample variance concentration satisfies $P(|\bar{V}(n) - \sigma^2| > \epsilon) \leq 4\exp(-Cn \min(\epsilon^2/v^4, \epsilon/v^2))$, enabling UCB-style algorithms for Sharpe ratio optimization.

## Foundational Learning

- **Concept: Multi-Armed Bandit Regret Analysis**
  - Why needed here: The paper reformulates regret in terms of variance sub-optimality rather than mean. Understanding classical UCB regret proofs is essential to follow Theorems 1 and 2.
  - Quick check question: Can you explain why regret bounds typically scale as O(log n) rather than O(n) for stochastic bandits?

- **Concept: Concentration Inequalities (McDiarmid, Bernstein, Hoeffding)**
  - Why needed here: Lemma 1 uses McDiarmid's inequality for bounded variance; Theorem 5 uses Bernstein's inequality for sub-exponential variables derived from sub-Gaussian squares. These are the technical foundation for all algorithm guarantees.
  - Quick check question: What is the key difference between Hoeffding's and Bernstein's inequalities in terms of variance-dependence?

- **Concept: Best Arm Identification (Fixed-Budget Setting)**
  - Why needed here: SHVV operates in the fixed-budget BAI framework, where the goal is minimizing error probability after $n$ total pulls. This differs fundamentally from regret minimization.
  - Quick check question: In fixed-budget BAI, why does error probability typically decay exponentially in $n$ rather than polynomially?

## Architecture Onboarding

- **Component map:** UCB-VV: Initialize -> warm-start phase (pull each arm SP×n times) -> compute indices -> greedy selection with confidence bonus -> update statistics; SHVV: Initialize arm set -> round loop: pull all arms equally -> eliminate bottom half by variance -> return final survivor

- **Critical path:** The variance estimator update is numerically sensitive; naive implementations may accumulate floating-point errors. The confidence bonus term requires careful handling of log t and division by s_i to avoid division-by-zero or overflow in early rounds.

- **Design tradeoffs:** Biased vs. unbiased variance estimator (paper uses biased for simpler analysis); round count in SHVV (using log₂(K) vs exact affects sample allocation); sub-Gaussian parameter v² must be known or estimated.

- **Failure signatures:** Regret grows faster than O(log n) (check if rewards violate assumptions); SHVV error does not decrease with budget (problem complexity H may be mis-specified); UCB-Sharpe diverges (denominator may approach zero).

- **First 3 experiments:**
  1. UCB-VV vs epsilon-greedy baseline: Set K=2 arms with known variance gap δ=0.1, horizon n=5000. Plot cumulative regret over time.
  2. SHVV error probability scaling: Run SHVV for K∈{16,32,64} with fixed budget n=2000 across different variance gap configurations. Verify error probability increases with K for fixed n.
  3. Sharpe ratio concentration validation: Generate sub-Gaussian samples. Compute empirical Sharpe ratio over varying sample sizes and verify the tail bound in Theorem 6 holds empirically.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can VTS (variance-based Thompson sampling) be proven theoretically optimal for variance maximization, and what are its regret bounds?
- Basis in paper: The authors state: "While our proposed algorithm is optimal and offers theoretical guarantees, the superior performance of VTS highlights the need for a theoretical investigation of VTS."
- Why unresolved: VTS lacks formal regret analysis despite empirically outperforming UCB-VV across all tested sub-optimality gaps.
- What evidence would resolve it: Deriving upper and lower regret bounds for VTS in variance maximization.

### Open Question 2
- Question: What are the complete regret guarantees for the UCB-Sharpe algorithm with the novel Sharpe ratio concentration inequality?
- Basis in paper: The authors provide a "sketch for bounding its regret" and state "the resulting algorithm of SR optimality and its regret bound has not been reported in the literature."
- Why unresolved: Only an abridged proof sketch is provided in Appendix VII-H, without the complete theoretical treatment given to UCB-VV and SHVV.
- What evidence would resolve it: Complete derivation of regret bounds matching the detail level of Theorem 1 for UCB-VV.

### Open Question 3
- Question: Can the variance-optimization framework be extended to heavy-tailed distributions beyond sub-Gaussian assumptions?
- Basis in paper: The extension from bounded to sub-Gaussian distributions required novel concentration inequalities, but the authors note variance estimates "can become highly sensitive to noise and unreliable" without strong distributional assumptions.
- Why unresolved: The sub-exponential tail bounds derived rely fundamentally on sub-Gaussianity; heavier tails may require fundamentally different approaches.
- What evidence would resolve it: Concentration inequalities for sample variance under weaker distributional assumptions with corresponding algorithm modifications.

## Limitations

- The theoretical framework for Sharpe ratio extension relies on concentration inequalities claimed to be novel, whose constants may be loose in practice
- The sub-Gaussian parameter v² is assumed known, which is a significant practical limitation as estimating it accurately from data is non-trivial
- The warm-start requirement of pulling each arm SP×n times in UCB-VV introduces computational overhead that scales linearly with K

## Confidence

- **High Confidence:** UCB-VV algorithm design and O(log n) regret bound for bounded rewards are directly supported by concentration inequality and standard UCB-style analysis
- **Medium Confidence:** Extension to sub-Gaussian rewards is theoretically sound but depends on Bernstein-type concentration bounds that, while derived rigorously, represent novel technical contributions
- **Low Confidence:** Real-world case study demonstrating superior performance over standard UCB is compelling but based on synthetic GBM data with unspecified parameters

## Next Checks

1. **Sharpe Ratio Concentration Validation:** Generate synthetic sub-Gaussian reward distributions. For varying sample sizes, compute the empirical Sharpe ratio and verify the tail bound in Theorem 6 by measuring the frequency of deviations exceeding the predicted bound.

2. **Warm-Start Overhead Analysis:** For the UCB-VV algorithm, empirically measure the cumulative regret when varying the warm-start parameter SP. Plot regret curves for SP∈{1,2,5} on synthetic problems to quantify the tradeoff between warm-start cost and convergence speed.

3. **Parameter Sensitivity in Real-World Setting:** Replicate the GBM case study with multiple parameter configurations (different drifts, volatilities, rolling window sizes). Test whether the proposed approach consistently outperforms UCB across parameter ranges.