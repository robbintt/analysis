---
ver: rpa2
title: 'SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning'
arxiv_id: '2601.21649'
source_url: https://arxiv.org/abs/2601.21649
tags:
- learning
- arxiv
- repository
- issue
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building capable open-weight
  small language models (SLMs) for software engineering tasks, particularly for deployment
  in privacy-sensitive and resource-constrained environments. The key limitation of
  current SLM training is the inability to generalize to complex, unfamiliar codebases
  due to their restricted inference-time search capacity.
---

# SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning

## Quick Facts
- arXiv ID: 2601.21649
- Source URL: https://arxiv.org/abs/2601.21649
- Reference count: 31
- 4B-parameter models trained with repository-centric learning outperform much larger models on repository-centric software engineering tasks

## Executive Summary
This paper addresses the challenge of building capable open-weight small language models (SLMs) for software engineering tasks, particularly for deployment in privacy-sensitive and resource-constrained environments. The key limitation of current SLM training is the inability to generalize to complex, unfamiliar codebases due to their restricted inference-time search capacity. To address this, the authors propose Repository-Centric Learning (RCL), a paradigm that emphasizes vertical repository depth over horizontal task breadth, enabling SLMs to internalize the "physics" of a target software environment during training.

The authors design a four-unit Repository-Centric Experience (RCX) framework that transforms static codebases into interactive learning signals, covering software design, contextual implementation, evolutionary replay, and semantic-runtime alignment. They train SWE-Spot-4B, a family of 4B-parameter models, using RCL and evaluate them on a comprehensive repository-centric evaluation suite across four software engineering tasks: issue resolution, test generation, feature implementation, and codebase QA. SWE-Spot-4B consistently outperforms much larger open-weight models (up to 8× larger) and matches or exceeds efficiency-comparable commercial models like GPT-4.1-mini and GPT-5-nano. The results demonstrate that RCL yields higher training sample efficiency, lower inference costs, and stronger cross-task transfer, establishing repository mastery as a distinct and necessary dimension for building efficient software agents.

## Method Summary
The authors propose Repository-Centric Learning (RCL) as a paradigm shift from task-focused to repository-focused training for small language models. RCL emphasizes vertical repository depth over horizontal task breadth, enabling SLMs to internalize the "physics" of target software environments. The core innovation is the four-unit Repository-Centric Experience (RCX) framework: (1) Repository Exploration & Understanding - crawls repositories to extract design documents, API contracts, and dependency graphs; (2) Code Comprehension & Localization - trains models to locate relevant code sections and understand implementation context; (3) Repository Evolution & History - uses version control histories to teach models how codebases change over time; (4) Semantic & Runtime Alignment - aligns model predictions with actual semantic behaviors and runtime characteristics.

SWE-Spot-4B models are trained using this framework on diverse repositories, focusing on capturing repository-specific patterns, conventions, and evolution rather than general programming knowledge. The training pipeline transforms static codebases into interactive learning experiences that build deep repository expertise.

## Key Results
- SWE-Spot-4B outperforms much larger open-weight models (up to 8× larger) on repository-centric tasks
- Matches or exceeds efficiency-comparable commercial models like GPT-4.1-mini and GPT-5-nano
- Demonstrates higher training sample efficiency and lower inference costs compared to traditional approaches
- Shows stronger cross-task transfer within repository contexts

## Why This Works (Mechanism)
The effectiveness of RCL stems from its focus on repository-specific knowledge rather than general programming competence. Traditional SLMs struggle with unfamiliar codebases because they lack the contextual understanding of project-specific patterns, conventions, and evolution. RCL addresses this by training models to internalize the "physics" of software environments - the implicit rules, dependencies, and evolutionary patterns that govern how codebases develop and function. By exposing models to the full lifecycle of repository evolution, from initial design through implementation and maintenance, RCL builds models that understand not just what code does, but why it exists and how it fits into the broader system context.

## Foundational Learning
- Repository structure comprehension - understanding how projects organize code, dependencies, and documentation; needed for effective code navigation and context gathering; quick check: can the model correctly identify relevant files and modules for a given task
- Evolutionary pattern recognition - learning how codebases change over time through version control histories; needed for predicting future modifications and understanding legacy code; quick check: can the model predict likely next changes based on historical patterns
- Semantic-runtime alignment - connecting code structure to actual program behavior and performance characteristics; needed for generating correct and efficient implementations; quick check: does the model's output align with expected runtime behavior

## Architecture Onboarding

**Component Map**: RCX Framework -> Repository Processing -> Model Training -> SWE-Spot-4B

**Critical Path**: Repository data extraction → RCX unit processing → Supervised training → Evaluation on repository-centric tasks

**Design Tradeoffs**: 
- Vertical depth vs. horizontal breadth: prioritizes deep repository understanding over general programming knowledge
- Static vs. dynamic learning signals: uses repository evolution histories to provide temporal context
- Efficiency vs. comprehensiveness: balances model size with repository coverage for deployment constraints

**Failure Signatures**:
- Poor performance on unfamiliar repositories suggests insufficient generalization from training data
- Inconsistent behavior across tasks may indicate incomplete RCX unit integration
- High inference costs despite small size could indicate inefficient knowledge organization

**First Experiments**:
1. Compare RCX unit ablations to identify critical components for repository understanding
2. Test model performance on held-out repositories to measure generalization capability
3. Evaluate inference efficiency metrics against baseline models of similar size

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Evaluation primarily focuses on repository-centric scenarios, leaving open questions about RCL's effectiveness in general-purpose coding tasks
- Four-unit RCX framework lacks detailed ablation studies showing individual contribution of each unit
- Scalability of RCL to larger repositories and computational overhead during training remain unclear

## Confidence
The major claims about RCL's superiority over traditional pretraining and its ability to match or exceed commercial models receive a confidence rating of **Medium**, given the focused evaluation scope and lack of broader generalization tests. The efficiency claims (lower inference costs, higher sample efficiency) are supported by the results but would benefit from independent replication and comparison against a wider range of model families.

## Next Checks
1. Conduct ablation studies to isolate the impact of each RCX unit on model performance
2. Evaluate SWE-Spot-4B on general-purpose coding benchmarks (e.g., HumanEval, MBPP) to assess RCL's effectiveness beyond repository-centric tasks
3. Test the model's ability to generalize to unseen repositories and programming languages not represented in the training data