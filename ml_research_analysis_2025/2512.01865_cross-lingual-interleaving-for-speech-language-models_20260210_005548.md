---
ver: rpa2
title: Cross-Lingual Interleaving for Speech Language Models
arxiv_id: '2512.01865'
source_url: https://arxiv.org/abs/2512.01865
tags:
- cross-lingual
- speech
- language
- interleaving
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building spoken language
  models (SLMs) that can understand and generate speech across multiple languages
  without relying on textual supervision. The core method introduces a cross-lingual
  interleaving strategy that concatenates sentence-aligned speech token sequences
  from different languages within the same training sequence, encouraging a shared
  representational subspace.
---

# Cross-Lingual Interleaving for Speech Language Models

## Quick Facts
- **arXiv ID**: 2512.01865
- **Source URL**: https://arxiv.org/abs/2512.01865
- **Reference count**: 0
- **Primary result**: Cross-lingual interleaving improves monolingual semantic accuracy and enables robust cross-lingual continuation in spoken language models.

## Executive Summary
This paper introduces cross-lingual interleaving as a method to build multilingual spoken language models (SLMs) without textual supervision. The approach concatenates sentence-aligned speech tokens from different languages within the same training sequence, encouraging the model to develop a shared representational subspace. Experiments with 360M and 1B parameter SLMs show that interleaving improves monolingual performance (e.g., StoryCloze scores of 58.31% and 70.39% for French vs. 55.31% and 67.07% baseline), enables robust cross-lingual continuation (cross-lingual scores within a few points of monolingual), and strengthens cross-lingual hidden-state alignment (cosine similarity of 0.75 vs. 0.73 baseline). A brief bilingual fine-tuning phase restores most monolingual performance lost during interleaving.

## Method Summary
The method uses a three-stage training procedure: (1) English-only pre-training for 50k steps, (2) cross-lingual interleaving for 20k steps with 0.5 probability of switching languages at sentence boundaries, and (3) bilingual fine-tuning without interleaving for 15k steps. The model uses Mimi tokeniser (12.5Hz, K=2048, 32 codebooks) and models only the first semantic codebook. Training data consists of a 42k-hour English-French TinyStories corpus created via GPT-4 translation and multi-speaker TTS synthesis with voice consistency filtering. Evaluation uses monolingual and cross-lingual StoryCloze and TopicCloze benchmarks, plus syntactic and lexical probes.

## Key Results
- Cross-lingual interleaving improves monolingual semantic accuracy: French StoryCloze scores increase from 55.31% to 58.31% (360M) and 67.07% to 70.39% (1B)
- Interleaving enables robust cross-lingual continuation: StoryCloze cross-lingual scores are within a few points of monolingual performance
- Cross-lingual hidden-state alignment improves: average cosine similarity increases from 0.73 to 0.75
- Brief bilingual fine-tuning restores monolingual syntactic/lexical performance: sBLiMP recovers from 52.73% to 61.75% and sWUGGY from 56.74% to 69.15%

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Interleaving Induces Shared Representational Subspace
Concatenating sentence-aligned speech tokens from different languages within the same training sequence encourages the model to develop a shared representational subspace across languages. The autoregressive objective forces contextual representations to bridge semantic content across languages within a single context window, creating pressure for semantically equivalent content to map to similar hidden states regardless of language.

### Mechanism 2: Positive Transfer from High-Resource to Low-Resource Language
Interleaving enables knowledge transfer from higher-resource language (English, 76k hours) to improve performance on lower-resource language (French, 21k hours) through the shared subspace. The model leverages English syntactic/semantic patterns as scaffolding for French generation, even with fewer target-language tokens.

### Mechanism 3: Stabilisation Phase Recovers Low-Level Regularities
A brief bilingual fine-tuning phase after interleaving restores monolingual syntactic and lexical competence while preserving cross-lingual abilities. Raw interleaving perturbs low-level regularities (agreement, morphology, phonotactics) more than high-level semantics, but monolingual-style fine-tuning allows the model to re-consolidate language-specific patterns without destroying cross-lingual alignments.

## Foundational Learning

- **Concept: Autoregressive Language Modeling**
  - **Why needed here:** The SLM objective (pθ(s) = ∏ pθ(sᵢ|s<ᵢ)) predicts the next token from all previous tokens. Understanding this is essential because interleaving works by modifying what contexts the model sees during this prediction task.
  - **Quick check question:** Can you explain why interleaving changes the training objective's effective context without changing the mathematical formulation?

- **Concept: Neural Audio Codecs and Residual Vector Quantization (RVQ)**
  - **Why needed here:** The paper uses Mimi tokeniser with RVQ (32 codebooks, K=2048) and models only the first "semantic" codebook. Understanding why early codebooks capture semantics vs. acoustics is critical for interpreting why cross-lingual transfer works.
  - **Quick check question:** Why would modelling only the first codebook (rather than all 32) be beneficial for semantic cross-lingual transfer?

- **Concept: Positive Transfer in Multilingual Learning**
  - **Why needed here:** The core result is that interleaving yields positive transfer—improving French performance despite fewer French tokens. This requires understanding when and why multilingual training helps vs. hurts.
  - **Quick check question:** What conditions might cause negative transfer instead of positive transfer in this setup?

## Architecture Onboarding

- **Component map:** Raw Audio → Mimi Tokeniser → Semantic Tokens (first codebook, 12.5Hz) → Decoder-only Transformer → Next Token Prediction
- **Critical path:** 1) Sentence-aligned corpus creation (GPT-4 translation + multi-speaker TTS with speaker consistency filtering >0.90 cosine similarity) 2) Tokenisation with Mimi (12.5Hz, K=2048) 3) Interleaved sequence construction: concatenate aligned sentences alternating languages 4) Three-stage training with matched token budgets 5) Evaluation on monolingual and cross-lingual benchmarks
- **Design tradeoffs:** Sentence-level vs. word-level switching; interleaving ratio of 0.5; 15k steps stabilisation duration; matched token budgets for fair comparison
- **Failure signatures:** Large drop in sBLiMP/sWUGGY after interleaving without recovery; cross-lingual scores near chance (50%); monolingual French scores below baseline; cosine similarity between EN/FR hidden states <0.70
- **First 3 experiments:** 1) Ablation on interleaving granularity: compare sentence-level vs. story-level interleaving 2) Vary stabilisation duration: test 5k, 15k, 30k steps 3) Tokeniser ablation: model multiple codebooks to verify semantic codebook necessity

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Reliance on sentence-level alignments assumes perfect semantic equivalence, but machine translation and TTS synthesis may introduce subtle mismatches
- Evaluation focuses primarily on English-French, a relatively close language pair; performance on more distant language pairs remains untested
- Corpus creation requires high-quality speaker voice consistency (SV similarity >0.90), but practical feasibility across diverse speaker pairs is not validated

## Confidence
- **High Confidence:** Cross-lingual interleaving improves cross-lingual alignment (cosine similarity increase from 0.73 to 0.75) and cross-lingual continuation performance
- **Medium Confidence:** The stabilisation phase effectively recovers monolingual syntactic/lexical performance without losing cross-lingual abilities
- **Low Confidence:** The mechanism by which only modelling the first semantic codebook enables cross-lingual transfer is not directly validated

## Next Checks
1. **Cross-Lingual Alignment Quality:** Conduct manual verification of a random sample of sentence alignments to quantify the semantic equivalence rate and identify failure modes in the alignment process.
2. **Generalization to Distant Languages:** Replicate the interleaving experiment with a distant language pair (e.g., English-Japanese or English-Chinese) to test whether the shared subspace mechanism generalizes beyond linguistically similar languages.
3. **Codebook Ablation Study:** Systematically vary the number of codebooks modelled (1, 4, 8, 16, 32) to determine the optimal configuration for semantic cross-lingual transfer and validate whether the first codebook is indeed necessary and sufficient.