---
ver: rpa2
title: WildSpoof Challenge Evaluation Plan
arxiv_id: '2508.16858'
source_url: https://arxiv.org/abs/2508.16858
tags:
- data
- sasv
- evaluation
- participants
- track
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The WildSpoof Challenge evaluation plan outlines two parallel
  tracks: Text-to-Speech (TTS) synthesis and Spoofing-robust Automatic Speaker Verification
  (SASV). The TTS track requires participants to generate speech in specific target
  speakers'' voices using the TITW dataset, with evaluation based on MCD, UTMOS, DNSMOS,
  WER, and SPK-sim metrics.'
---

# WildSpoof Challenge Evaluation Plan

## Quick Facts
- **arXiv ID:** 2508.16858
- **Source URL:** https://arxiv.org/abs/2508.16858
- **Authors:** Yihan Wu; Jee-weon Jung; Hye-jin Shim; Xin Cheng; Xin Wang
- **Reference count:** 7
- **Key outcome:** The WildSpoof Challenge evaluation plan outlines two parallel tracks: Text-to-Speech (TTS) synthesis and Spoofing-robust Automatic Speaker Verification (SASV). The TTS track requires participants to generate speech in specific target speakers' voices using the TITW dataset, with evaluation based on MCD, UTMOS, DNSMOS, WER, and SPK-sim metrics. The SASV track focuses on detecting spoofed speech, using the SpoofCeleb dataset, with evaluation based on agnostic DCF (a-DCF) metrics. Both tracks aim to advance the use of in-the-wild data and encourage interdisciplinary collaboration. Participants must submit audio files for TTS and scores for SASV trials, adhering to specific guidelines and rules. The challenge promotes ethical research

## Executive Summary
The WildSpoof Challenge introduces a dual-track evaluation framework designed to advance both text-to-speech synthesis and spoofing-robust automatic speaker verification using in-the-wild data. The challenge features two distinct tracks - TTS generation requiring voice cloning from limited enrollment data, and SASV detection requiring identification of genuine speakers while rejecting spoofed speech. Participants compete in either track but not both, with evaluations using comprehensive metrics including spectral distance, perceptual quality, intelligibility, and speaker similarity for TTS, and agnostic detection cost function for SASV. The challenge aims to bridge the TTS and SASV research communities while promoting real-world robustness through domain-diverse datasets.

## Method Summary
The evaluation plan specifies two parallel tracks with distinct data, metrics, and submission requirements. For the TTS track, participants generate speech using TITW dataset training splits (Easy or Hard), producing 16kHz PCM WAV files evaluated on MCD, UTMOS, DNSMOS, WER, and SPK-sim metrics via the Versa toolkit. The SASV track involves training integrated spoofing-aware systems on SpoofCeleb data, with participants submitting continuous scores for trial verification evaluated using agnostic DCF with specified priors and costs. Both tracks provide baseline systems (Grad-TTS+DiffWave for TTS, integrated embeddings for SASV) and require specific submission formatting without parent directory structures.

## Key Results
- Parallel dual-track design (TTS generation + SASV detection) aims to foster cross-community progress through competitive tension
- In-the-wild datasets (TITW, SpoofCeleb) impose domain variability that controlled corpora cannot replicate
- Multi-metric TTS evaluation (MCD, UTMOS, DNSMOS, WER, SPK-sim) reduces reward hacking on any single proxy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel dual-track design (TTS generation + SASV detection) may foster cross-community progress through implicit adversarial tension.
- Mechanism: Separating participants into mutually exclusive tracks forces specialization while organizers coordinate shared data protocols, creating a competitive ecosystem where TTS advances pressure SASV robustness and vice versa.
- Core assumption: Cross-pollination occurs through shared datasets and published baselines rather than direct collaboration.
- Evidence anchors:
  - [abstract] "Encourage interdisciplinary collaboration between the spoofing generation (TTS) and spoofing detection (SASV) communities"
  - [section 2.5, 3.5] "Participant who registers for the TTS track CANNOT participate in the SASV track"
  - [corpus] Weak direct evidence; related papers (DFKI-Speech, Robust TTS Training) show independent track participation but no explicit cross-track analysis
- Break condition: If participants do not publish methods or if baselines remain unused, the feedback loop weakens.

### Mechanism 2
- Claim: In-the-wild datasets (TITW, SpoofCeleb) impose domain variability that controlled corpora cannot replicate.
- Mechanism: Real-world recordings introduce varied channels, noise, speaker behaviors, and artifacts; systems trained on such data must learn robust representations rather than overfitting to studio conditions.
- Core assumption: Models trained on in-the-wild data generalize better to deployment scenarios than those trained on clean data alone.
- Evidence anchors:
  - [abstract] "Promote the use of in-the-wild data for both TTS and SASV, moving beyond conventional clean and controlled datasets"
  - [section 2.1] TITW-Easy and TITW-Hard training partitions offer graduated difficulty
  - [corpus] SpoofCeleb paper referenced but not analyzed here; no direct comparative evidence in this evaluation plan
- Break condition: If evaluation sets are too small or insufficiently diverse, measured gains may not reflect real-world improvement.

### Mechanism 3
- Claim: Multi-metric TTS evaluation (MCD, UTMOS, DNSMOS, WER, SPK-sim) reduces reward hacking on any single proxy.
- Mechanism: Combining spectral distance (MCD), perceptual quality (UTMOS, DNSMOS), intelligibility (WER), and speaker fidelity (SPK-sim) forces tradeoffs; optimizing one often degrades another.
- Core assumption: No single metric captures all desirable TTS properties; composite evaluation yields more useful systems.
- Evidence anchors:
  - [section 2.4] "MCD, UTMOS, DNSMOS, WER, SPK-sim. All these metrics will be calculated by Versa"
  - [section 2.4] "There will be no ranking of the TTS systems" — organizers avoid over-optimization incentives
  - [corpus] No corpus evidence on metric correlation or tradeoffs in this plan
- Break condition: If metrics are highly correlated or if one dominates perceived quality, composite evaluation adds little constraint.

## Foundational Learning

- Concept: **Diffusion-based TTS (Grad-TTS + DiffWave baseline)**
  - Why needed here: The TTS track baseline uses diffusion for both acoustic modeling and vocoding; understanding denoising schedules and inference steps is essential for modifications.
  - Quick check question: Can you explain why diffusion models trade inference speed for sample quality compared to flow-based or autoregressive alternatives?

- Concept: **SASV integration (speaker embedding + spoofing detection)**
  - Why needed here: The SASV baseline integrates speaker verification and anti-spoofing into unified embeddings; participants must understand how to balance false alarms across target/non-target/spoof trials.
  - Quick check question: How does a-DCF differ from EER in handling asymmetric costs for miss vs. false alarm?

- Concept: **a-DCF metric (agnostic Detection Cost Function)**
  - Why needed here: SASV evaluation uses priors (πtar=0.9405, πnon=0.0095, πspf=0.05) and costs (Cmiss=1, Cfa=10); threshold selection directly impacts scored performance.
  - Quick check question: Given these priors, should a system bias toward accepting or rejecting borderline trials?

## Architecture Onboarding

- Component map:
  - **TTS Track:** Text encoder → Acoustic model (Grad-TTS) → Vocoder (DiffWave) → Output waveform; speaker conditioning via reference utterances
  - **SASV Track:** Frontend (SSL features permitted) → Integrated embedding network → Score backend → Trial-level decision

- Critical path:
  1. Download and verify TITW (TTS) or SpoofCeleb (SASV) training splits
  2. Run provided baselines to establish reference scores
  3. Format submissions correctly (zip with 16kHz PCM wav for TTS; tsv with scores for SASV)
  4. Validate via soxi check (TTS) or a-DCF reference implementation (SASV)

- Design tradeoffs:
  - TITW-Easy vs. TITW-Hard: more data vs. higher noise/variability
  - SSL features in SASV: potentially better representations but may require domain adaptation (which is prohibited on eval data)
  - Pre-trained model fine-tuning: faster convergence but must comply with TITW fine-tuning requirement

- Failure signatures:
  - TTS: MCD low but WER high → intelligibility sacrificed for spectral match
  - TTS: SPK-sim high but DNSMOS low → speaker cloning succeeds at cost of audio quality
  - SASV: Low miss rate but high Cfa,spoof → system accepts spoofs too readily
  - SASV: Score normalization across trials → rule violation, disqualification risk

- First 3 experiments:
  1. Reproduce baseline TTS on TITW-Easy; measure all five metrics to establish Pareto frontiers
  2. Train SASV baseline on SpoofCeleb; plot a-DCF vs. threshold to find operating point
  3. Ablate SSL frontend in SASV (if used) to quantify contribution vs. standard features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TTS system performance vary between the TITW-Easy and TITW-Hard training configurations when generalizing to known (KSKT) versus unknown (KSUT) texts?
- Basis in paper: [inferred] Section 2.1 allows participants to use "TITW-Easy" or "TITW-Hard" data, while Section 2.2 defines distinct evaluation protocols for known and unknown text.
- Why unresolved: The evaluation plan establishes the protocol but leaves the comparative effectiveness of training configurations on generalization as an open result for challenge participants.
- What evidence would resolve it: Comparative metric results (MCD, WER, SPK-sim) from systems trained exclusively on Easy versus Hard datasets evaluated on both KSKT and KSUT trials.

### Open Question 2
- Question: How do SASV systems balance the trade-off between missing target speakers and falsely accepting spoofed attacks under the specific cost parameters of the agnostic DCF (a-DCF)?
- Basis in paper: [inferred] Section 3.4 explicitly defines the evaluation metric as a-DCF with specific priors ($\pi_{tar}=0.9405, \pi_{spf}=0.05$) and costs ($C_{miss}=1, C_{fa}=10$).
- Why unresolved: The paper sets the optimization constraints but does not demonstrate how existing or new architectures manage this specific high-cost penalty for spoofing false alarms.
- What evidence would resolve it: Submission files containing continuous scores that minimize the a-DCF value on the evaluation trial list.

### Open Question 3
- Question: What are the performance correlations or conflicts between the five distinct evaluation metrics (MCD, UTMOS, DNSMOS, WER, SPK-sim) used for the TTS track?
- Basis in paper: [inferred] Section 2.4 lists five diverse metrics to be calculated by Versa but explicitly states, "There will be no ranking," implying a lack of a unified success criterion.
- Why unresolved: Without a single ranking, understanding whether improvements in speaker similarity (SPK-sim) come at the cost of speech quality (UTMOS) or intelligibility (WER) is an open research outcome.
- What evidence would resolve it: A multi-variate analysis of the submitted audio files across all five metrics to identify systems that optimize multiple dimensions simultaneously.

## Limitations

- The evaluation plan lacks pilot results or baseline performance ranges, creating uncertainty about challenge difficulty
- The mechanism for interdisciplinary collaboration lacks direct evidence - no specified mechanisms for tracking or measuring cross-community interaction
- The restriction on SASV participants using evaluation data for normalization creates an unverifiable compliance burden

## Confidence

**High Confidence:** The dual-track structure, dataset specifications, and submission requirements are clearly defined. The metric definitions (MCD, UTMOS, DNSMOS, WER, SPK-sim for TTS; a-DCF with specified priors for SASV) are explicitly stated with no ambiguity.

**Medium Confidence:** The rationale for using in-the-wild datasets and multi-metric evaluation is logically sound but lacks empirical validation within this document. The assumed benefits of domain variability and metric diversity are reasonable but not demonstrated.

**Low Confidence:** The mechanism for interdisciplinary collaboration and the actual difficulty level of the challenge cannot be assessed without pilot studies or historical participation data. The effectiveness of the proposed evaluation framework depends on factors not addressed in this plan.

## Next Checks

1. Run pilot experiments with the baseline systems to establish baseline performance ranges and verify that the specified metrics are correctly implemented and calibrated.
2. Analyze the correlation structure among the five TTS metrics on a held-out development set to confirm they provide complementary information rather than redundancy.
3. Conduct a small-scale trial with mixed participants (some in TTS, some in SASV) to observe whether any informal knowledge transfer occurs and whether the parallel structure creates meaningful competitive tension.