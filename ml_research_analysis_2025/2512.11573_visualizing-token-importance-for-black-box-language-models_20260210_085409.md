---
ver: rpa2
title: Visualizing token importance for black-box language models
arxiv_id: '2512.11573'
source_url: https://arxiv.org/abs/2512.11573
tags:
- dbsa
- sensitivity
- token
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distribution-Based Sensitivity Analysis (DBSA),
  a novel model-agnostic method for auditing black-box large language models (LLMs)
  by evaluating how sensitive model outputs are to each input token. Unlike existing
  gradient-based or bias-focused approaches, DBSA addresses the unique challenge of
  stochastic LLM outputs by using Monte Carlo sampling and statistical hypothesis
  testing to measure the distributional impact of token perturbations.
---

# Visualizing token importance for black-box language models

## Quick Facts
- arXiv ID: 2512.11573
- Source URL: https://arxiv.org/abs/2512.11573
- Reference count: 40
- Authors: Paulius Rauba; Qiyao Wei; Mihaela van der Schaar
- Key outcome: Introduces DBSA, a model-agnostic method using Monte Carlo sampling and energy distance to measure token-level sensitivity in black-box LLMs, validated across seven models with Spearman correlations above 0.96

## Executive Summary
This paper introduces Distribution-Based Sensitivity Analysis (DBSA), a novel model-agnostic method for auditing black-box large language models (LLMs) by evaluating how sensitive model outputs are to each input token. Unlike existing gradient-based or bias-focused approaches, DBSA addresses the unique challenge of stochastic LLM outputs by using Monte Carlo sampling and statistical hypothesis testing to measure the distributional impact of token perturbations. The method approximates sensitivity by comparing output distributions when tokens are replaced with their nearest neighbors in embedding space, using energy distance as a metric and permutation tests for p-values. Experiments across seven different LLMs demonstrate that DBSA consistently identifies token-level sensitivities, with Spearman correlations above 0.96 between different distance metrics (cosine, L1, L2), validating its robustness. The approach shows particular promise for practical auditing in high-stakes domains like legal and medical applications, where understanding token importance is critical for model reliability and compliance.

## Method Summary
DBSA measures token-level sensitivity by replacing each unique token with its k nearest neighbors in embedding space, then comparing the resulting output distributions using energy distance. For each token, the method samples n responses from both the original and perturbed prompts, computes embeddings for these outputs, and calculates the energy distance between distributions. A permutation test determines statistical significance. The process repeats for all unique tokens, averaging scores across neighbors and token positions to produce a sensitivity heatmap. The approach is model-agnostic, requiring only black-box LLM access and an embedding function.

## Key Results
- DBSA consistently identifies token-level sensitivities across seven different LLMs
- Spearman correlations above 0.96 between different distance metrics (cosine, L1, L2) validate robustness
- Monte Carlo sampling effectively addresses LLM stochasticity, enabling reliable distributional comparisons
- Method shows promise for auditing in high-stakes domains like legal and medical applications

## Why This Works (Mechanism)

### Mechanism 1: Monte Carlo Distribution Approximation
- **Claim:** Sampling multiple outputs from an LLM for the same input enables a statistically robust comparison of output distributions, addressing the inherent stochasticity of LLMs that invalidates single-output comparisons.
- **Mechanism:** The system generates `n` independent samples from both the original input `x` and a perturbed input `x'`, forming empirical distributions `Ŝ(x)` and `Ŝ(x')`. By comparing these distributions, the method captures the output space shift caused by the input perturbation while filtering out random noise.
- **Core assumption:** The number of Monte Carlo samples `n` is sufficiently large to approximate the true output distribution; otherwise, empirical estimates are unreliable.
- **Evidence anchors:**
  - [abstract] "LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible."
  - [section 2.2] "We employ a statistical approach that compares the entire output distributions... instead of just a single realization."
  - [corpus] Corpus neighbors like "CALM" and "Thought Anchors" address black-box auditing but do not use this distributional sampling approach.
- **Break condition:** If `n` is too small, empirical distributions are unrepresentative, causing high variance in sensitivity scores and unreliable p-values.

### Mechanism 2: Energy Distance for Distributional Comparison
- **Claim:** Energy distance provides a statistically grounded metric for quantifying the difference between two multivariate distributions of output embeddings.
- **Mechanism:** Energy distance `E(X,Y)` is calculated from distances between observations in two distributions. The formula `E = 2A - B - C` compares between-distribution similarity (A) against within-distribution similarity (B, C), yielding an effect size of zero only when distributions are identical.
- **Core assumption:** The embedding function `ϕ` maps semantically similar outputs to nearby points, ensuring embedding distance correlates with semantic difference.
- **Evidence anchors:**
  - [section 4] "A natural choice for this is the energy distance... directly compute p-values, and (v) scale well with the number of samples."
  - [section 6.3] Spearman correlations above 0.96 between different similarity metrics (Cosine, L1, L2) validate robustness.
  - [corpus] "White-Box Sensitivity Auditing with Steering Vectors" uses an entirely different (white-box) mechanism.
- **Break condition:** If the embedding function is poor, semantically distinct outputs may cluster together, causing energy distance to underestimate true semantic shifts.

### Mechanism 3: Nearest-Neighbor Perturbation as Gradient Proxy
- **Claim:** Replacing an input token with its k-nearest neighbors approximates an infinitesimal perturbation for sensitivity analysis in discrete token space where true gradients are inaccessible.
- **Mechanism:** Since gradient computation is impossible for black-box models, the method substitutes token `t_i` with neighbor `t'_i`. The output distribution shift, measured by energy distance, is attributed to `t_i`. Averaging over `k` neighbors improves estimate stability.
- **Core assumption:** A token's nearest neighbor represents a "small" semantic change; the model's response reflects sensitivity to that token's role.
- **Evidence anchors:**
  - [section 3.1] "We approximate the infinitesimal perturbation δti by substituting ti with one of its k nearest neighbors in the embedding space."
  - [section 2.3] "We must work with discrete jumps between tokens, where the smallest possible perturbation is a substitution with the nearest neighbor."
  - [corpus] No corpus papers employ this exact nearest-neighbor-based sensitivity mechanism.
- **Break condition:** If a token's nearest neighbor is not semantically close (e.g., an antonym), the large semantic shift may be misinterpreted as high sensitivity to the original token.

## Foundational Learning

- **Concept: Energy Distance**
  - **Why needed here:** Core statistical metric for quantifying distributional differences. Understanding its properties (metric, zero iff distributions equal) is essential for interpreting sensitivity score `ω`.
  - **Quick check question:** Why does the energy distance formula include within-distribution similarity terms (`B` and `C`)?

- **Concept: Permutation Test**
  - **Why needed here:** Non-parametric method for computing p-values, determining whether observed energy distance is statistically significant or attributable to chance.
  - **Quick check question:** What is the null hypothesis in this permutation test, and how is the p-value derived?

- **Concept: Monte Carlo Sampling**
  - **Why needed here:** Generates empirical approximations `Ŝ(x)` from finite samples since true LLM output distributions are intractable.
  - **Quick check question:** If LLM output variance is extremely high, what happens to DBSA sensitivity estimate stability for fixed `n`?

## Architecture Onboarding

- **Component map:** Tokenizer & Embedder -> Perturbation Engine -> LLM Sampler -> Output Embedder -> Statistical Core -> Aggregator
- **Critical path:** LLM Sampler is the bottleneck. For `u` unique tokens, `1 + (u * k)` prompt variants each require `n` LLM calls.
- **Design tradeoffs:**
  - **Sample size (`n`):** Higher `n` improves power but linearly increases cost/latency (paper suggests 20-100).
  - **Neighbors (`k`):** More neighbors smooth estimates but multiply LLM calls.
  - **Distance metric:** Cosine, L1, L2 yield correlated results (Spearman > 0.96), offering flexibility.
- **Failure signatures:**
  - High p-values for important tokens (insufficient `n` for high-variance LLMs).
  - Spurious high sensitivity (poor neighbor choice or embedding model failure).
  - Uniformly low scores (embedding model issues or highly robust LLM).
- **First 3 experiments:**
  1. **Baseline Stability Test:** Run DBSA multiple times on a simple prompt; verify consistency of `ω` scores and rankings.
  2. **Ablation on Monte Carlo Samples:** Vary `n` (5, 10, 20, 50) on fixed prompt; observe stabilization (reproduce Figure 4) to select cost-effective `n`.
  3. **Perturbation Validity Check:** Manually inspect k-nearest neighbors for semantic appropriateness; compare `k=1` vs `k=3` results to assess averaging effect.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Distribution-Based Sensitivity Analysis (DBSA) be extended to a multi-level approach (e.g., phrases, sentences) while accounting for mutual information and semantic drift?
- **Basis in paper:** [explicit] Section 8, "Directions for Future Work," point (1) proposes extending DBSA to multi-level interpretability but lists challenges such as handling interactions between levels and "semantic drift."
- **Why unresolved:** The authors note that assuming independence between levels is insufficient, and dealing with higher variance estimators and non-zero covariances between levels is non-trivial.
- **What evidence would resolve it:** A modified DBSA algorithm that successfully aggregates token-level sensitivities into phrase-level scores while validating that these aggregated scores reflect semantic meaning rather than just token sums.

### Open Question 2
- **Question:** Can token probabilities be used to formally decompose output variability into within-prompt and between-prompt components?
- **Basis in paper:** [explicit] Section 8, "Directions for Future Work," point (2) suggests an "ANOVA-style partitioning of variance" to disentangle variability caused by perturbations from stochastic behavior.
- **Why unresolved:** Current DBSA methods measure distributional shifts but do not formally partition the variance sources to quantify the relative contribution of specific input features to the model's stochasticity.
- **What evidence would resolve it:** A theoretical framework and empirical results showing a mathematical decomposition of output variance attributable to specific token perturbations versus intrinsic model randomness.

### Open Question 3
- **Question:** How does the reliability of nearest-neighbor selection in embedding space affect the robustness of DBSA sensitivity scores?
- **Basis in paper:** [explicit] Section 8, "Limitations" acknowledges the difficulty of "finding reliable nearest neighbors (which may vary in magnitude in embedding space)."
- **Why unresolved:** The method relies on nearest neighbors as a proxy for infinitesimal perturbations; however, if these neighbors are not semantically consistent or vary greatly in distance, the resulting sensitivity scores may be noisy or misleading.
- **What evidence would resolve it:** Ablation studies correlating the quality/distance of nearest neighbors with the stability of the resulting sensitivity scores across different embedding spaces (e.g., Word2Vec vs. transformer-based embeddings).

## Limitations

- Computational cost scales linearly with unique tokens, neighbors per token, and Monte Carlo samples, creating barriers for long prompts
- Method's reliance on nearest-neighbor perturbations assumes embedding-space proximity implies semantic similarity, which may fail for polysemous words or domain-specific terminology
- Claims about superiority over gradient-based or bias-focused approaches are not directly tested; paper positions DBSA as complementary rather than competitive

## Confidence

- **High Confidence:** The statistical framework (energy distance, permutation tests) is well-established and correctly applied. The correlation results across distance metrics (Spearman > 0.96) are internally consistent and reproducible.
- **Medium Confidence:** The practical utility for auditing in high-stakes domains is plausible but primarily demonstrated through case studies rather than systematic evaluation against known failure modes or compliance benchmarks.
- **Low Confidence:** Claims about DBSA's superiority over gradient-based or bias-focused approaches are not directly tested; the paper positions DBSA as complementary rather than competitive without empirical comparison.

## Next Checks

1. **Computational Scaling Validation:** Test DBSA on prompts of increasing length (10, 30, 100 tokens) and measure runtime and sensitivity score stability. Verify whether the method remains practical for real-world document lengths and identify the point where cost outweighs benefit.

2. **Neighbor Quality Assessment:** For a diverse set of tokens (common words, technical terms, polysemous words), manually inspect the k nearest neighbors retrieved by the embedding model. Quantify the proportion of semantically inappropriate neighbors and measure how this affects DBSA sensitivity scores and rankings.

3. **Ground-Truth Correlation Study:** Create a small benchmark where human experts annotate token importance for specific outputs (e.g., legal advice, medical summaries). Compare DBSA rankings against these ground-truth labels to measure precision@k and identify systematic mismatches.