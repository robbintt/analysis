---
ver: rpa2
title: 'SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational
  Pathology Biomarker Discovery'
arxiv_id: '2602.00953'
source_url: https://arxiv.org/abs/2602.00953
tags:
- hypothesis
- agent
- novelty
- computational
- validation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAGE is an agentic AI system designed to discover interpretable
  pathology biomarkers by integrating literature-grounded reasoning with multimodal
  data analysis. It coordinates specialized agents for hypothesis generation, biological
  contextualization, novelty evaluation, and empirical validation.
---

# SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery

## Quick Facts
- arXiv ID: 2602.00953
- Source URL: https://arxiv.org/abs/2602.00953
- Authors: Sahar Almahfouz Nasser; Juan Francisco Pesantez Borja; Jincheng Liu; Tanvir Hasan; Zenghan Wang; Suman Ghosh; Sandeep Manandhar; Shikhar Shiromani; Twisha Shah; Naoto Tokuyama; Anant Madabhushi
- Reference count: 40
- One-line primary result: Agentic AI system achieving comparable novelty to baselines while using 57.5% fewer tokens and validating prognostic biomarkers in bladder cancer

## Executive Summary
SAGE is an agentic AI system designed to discover interpretable pathology biomarkers by integrating literature-grounded reasoning with multimodal data analysis. It coordinates specialized agents for hypothesis generation, biological contextualization, novelty evaluation, and empirical validation. SAGE prioritizes transparent, biologically supported biomarkers by linking imaging-derived features with molecular and clinical endpoints.

The framework addresses the challenge of translating AI-discovered biomarkers into clinical practice by ensuring interpretability and biological plausibility. Through its multi-critic deliberation protocol and graph-based pipeline architecture, SAGE demonstrates both efficiency gains and improved novelty calibration compared to traditional approaches.

## Method Summary
SAGE implements a sequential agent pipeline where each specialized agent builds upon the previous output: Path Generation identifies candidate biomarker paths, Ontologist contextualizes findings, Scientist formulates hypotheses, Hypothesis Expansion explores alternatives, Novelty Critic evaluates originality, Feasibility assesses practical implementation, Coding Agent validates empirically, and Summary consolidates findings. The system uses a knowledge graph constructed from 1,650 literature sources containing 41,053 nodes and 56,338 edges, with GPT-4o-mini for triple extraction and BGE-Large embeddings (τ=0.9) for node fusion.

A key architectural innovation is the graph-based pipeline that provides specific-memory context to each agent rather than shared memory, reducing token consumption by 57.5% while maintaining novelty scores. The multi-critic deliberation protocol (Prover/Verifier/Judge) triggers when novelty score variance exceeds 1.0, improving A–E separability by 1.76 points. Validation uses TCGA-BLCA cohort with Kaplan-Meier and Cox modeling to confirm prognostic separation using FABP5 expression and TLS abundance.

## Key Results
- Achieved comparable novelty scores to retrieval-heavy baselines while using 57.5% fewer prompt tokens and 14.9% fewer completion tokens
- Multi-critic deliberation improved novelty calibration, increasing A–E separability by 1.76 points
- End-to-end validation demonstrated prognostic separation in bladder cancer using FABP5 expression and TLS abundance
- Graph-based pipeline reduced cost while maintaining novelty, though validity comparison with shared-memory pipeline remains untested

## Why This Works (Mechanism)
SAGE works by combining domain-specific knowledge grounding with specialized reasoning agents that progressively refine hypotheses. The graph-based architecture prevents context dilution that occurs in shared-memory pipelines, while the multi-critic deliberation protocol provides robust novelty evaluation through adversarial reasoning. By integrating feasibility assessment early in the pipeline and validating through empirical coding agents, SAGE ensures that generated biomarkers are both novel and practically implementable.

The system's effectiveness stems from its ability to balance creativity with biological plausibility, using literature-grounded reasoning to constrain the hypothesis space while allowing for novel connections. The specific-memory architecture enables each agent to focus on its specialized task without being overwhelmed by irrelevant context, improving both efficiency and quality of outputs.

## Foundational Learning
**Knowledge Graph Construction**: Extracting triples from literature using GPT-4o-mini with confidence scoring ≥0.5, then fusing nodes via BGE-Large embeddings at τ=0.9. *Why needed*: Provides structured domain knowledge for agent reasoning. *Quick check*: Verify node fusion accuracy by comparing pre/post fusion entity counts.

**Agent Specialization**: Assigning different model variants (Nano, Mini, high-reasoning) based on task complexity. *Why needed*: Optimizes cost while maintaining reasoning quality for complex tasks. *Quick check*: Measure output quality vs. token cost for each agent type.

**Multi-critic Deliberation**: Three-critic protocol (Prover/Verifier/Judge) triggered when σ>1.0, iterating up to 3 rounds. *Why needed*: Improves novelty calibration by identifying specious or incremental claims. *Quick check*: Confirm σ threshold triggers additional deliberation rounds.

## Architecture Onboarding

**Component Map**: Path Generation -> Ontologist -> Scientist -> Hypothesis Expansion -> Novelty Critic -> Feasibility -> Coding Agent -> Summary

**Critical Path**: The sequence from Path Generation through Coding Agent represents the core hypothesis-to-validation pipeline, with Novelty Critic and Feasibility providing quality gates that can terminate unpromising paths early.

**Design Tradeoffs**: Specific-memory pipeline vs. shared-memory pipeline (57.5% token reduction but untested validity impact); high-reasoning models for complex tasks vs. Nano/Mini for efficiency; multi-critic deliberation (improved calibration but increased cost).

**Failure Signatures**: High novelty scores with low feasibility indicate specious hypotheses; failure to trigger deliberation despite high variance suggests critic calibration issues; context dilution in shared-memory pipeline manifests as degraded novelty scores over long sequences.

**First Experiments**:
1. Reproduce KG construction pipeline: filter 1,650 sources, extract triples with confidence ≥0.5, measure node fusion accuracy using BGE-Large at τ=0.9
2. Implement multi-critic novelty evaluation: test σ>1.0 triggering, measure A–E separability improvement vs. single-critic
3. Validate end-to-end pipeline: run TCGA-BLA cohort through full pipeline, confirm FABP5/TLS prognostic separation with Kaplan-Meier and Cox models

## Open Questions the Paper Calls Out
**Open Question 1**: Can integrating ontology-constrained classifiers and external biomedical resources (like UMLS) resolve entity boundary delineation errors identified in KG construction? The current pipeline struggles with complex noun phrases and clinical endpoint disambiguation, motivating future integration of UMLS and ontology-constrained classifiers.

**Open Question 2**: Can the multi-critic deliberation protocol be refined to better distinguish between "incremental" and "moderate" novelty scores? The current framework struggles with boundary ambiguity between creative application and incremental extension, particularly in Tiers C and D.

**Open Question 3**: How does the graph-based (specific-memory) pipeline compare to the chat-based (shared-memory) pipeline in terms of hypothesis validity, beyond novelty and token efficiency? The authors note they only measured novelty, not validity, in the pipeline comparison.

## Limitations
- Exact weighting schema for path scoring remains unspecified beyond general guidance
- Agent prompts and structured templates for hypothesis formulation are incompletely described
- GPT-5 model variants (Nano, Mini, high-reasoning) referenced are not publicly available
- Tool registry specifications and MCP orchestration implementation details are insufficiently documented

## Confidence
**High confidence**: Core concept of agentic pipeline for biomarker discovery and general workflow structure
**Medium confidence**: Token efficiency claims (57.5% fewer prompt tokens) and multi-critic deliberation mechanism
**Low confidence**: Exact hyperparameter values, particularly path scoring weights and embedding threshold behavior

## Next Checks
1. Validate the graph construction pipeline by reproducing literature corpus filtering (1,650 sources → 41,053 nodes) and triple extraction with confidence scoring ≥0.5, then measuring node fusion accuracy using BGE-Large embeddings at τ=0.9.

2. Implement the multi-critic novelty evaluation with the three-critic deliberation (Prover/Verifier/Judge) and test whether σ>1.0 triggers additional deliberation rounds, measuring the resulting A–E separability improvement against single-critic baselines.

3. Reproduce the end-to-end validation using TCGA-BLCA data to confirm prognostic separation with FABP5 expression and TLS abundance, comparing Kaplan-Meier curves and Cox model coefficients against reported values.