---
ver: rpa2
title: 'MODE: Multi-Objective Adaptive Coreset Selection'
arxiv_id: '2512.21152'
source_url: https://arxiv.org/abs/2512.21152
tags:
- budget
- strategy
- selection
- mode
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MODE introduces a dynamic multi-objective framework for adaptive
  coreset selection that combines uncertainty, diversity, class balance, and boundary
  sampling strategies. Unlike static methods, MODE learns to weight these strategies
  based on their real-time contribution to validation performance, adapting to different
  training phases.
---

# MODE: Multi-Objective Adaptive Coreset Selection
## Quick Facts
- arXiv ID: 2512.21152
- Source URL: https://arxiv.org/abs/2512.21152
- Authors: Tanmoy Mukherjee; Pierre Marquis; Zied Bouraoui
- Reference count: 40
- Primary result: Achieves 53.3% average accuracy at 30% data budget across benchmarks, outperforming static baselines by 5.3%

## Executive Summary
MODE introduces a dynamic multi-objective framework for adaptive coreset selection that learns to weight different sampling strategies (uncertainty, diversity, class balance, boundary sampling) based on their real-time contribution to validation performance. Unlike static methods, MODE adapts to different training phases and achieves (1âˆ’1/e)-approximation guarantees via submodular maximization with O(n log n) complexity. The framework demonstrates significant memory efficiency and reveals interpretable curriculum patterns in strategy prioritization throughout training.

## Method Summary
MODE employs a dynamic weighting mechanism that adapts selection strategy importance based on real-time validation performance, learning optimal strategy weights through a meta-learning framework that predicts strategy effectiveness across training phases. The approach combines multiple sampling strategies through weighted submodular maximization, achieving theoretical guarantees while maintaining practical efficiency. The framework operates in O(n log n) time complexity and shows significant memory savings compared to full dataset training, with strategy priorities naturally evolving from class balance and diversity early in training to uncertainty and boundary sampling in later stages.

## Key Results
- Achieves 53.3% average accuracy at 30% budget across CIFAR-10/100, ImageNet subsets, Fashion-MNIST, and SVHN
- Outperforms classical baselines by 5.3% while reducing memory usage by 75% and training time by 90%
- Demonstrates interpretable curriculum patterns with natural evolution of strategy priorities throughout training phases

## Why This Works (Mechanism)
MODE's effectiveness stems from its adaptive weighting of multiple sampling strategies that capture different aspects of data utility. The framework learns to dynamically balance uncertainty sampling (identifying confusing samples), diversity promotion (ensuring coverage), class balance (maintaining representation), and boundary sampling (focusing on decision boundaries). This multi-objective approach allows the system to respond to changing training dynamics, prioritizing different strategies as the model's learning progresses through different phases.

## Foundational Learning
- Submodular maximization: Why needed - provides theoretical approximation guarantees; Quick check - verify diminishing returns property holds for chosen utility functions
- Coreset selection theory: Why needed - ensures representative subset selection; Quick check - validate subset covers full data distribution
- Multi-armed bandit optimization: Why needed - enables adaptive strategy weighting; Quick check - test exploration-exploitation balance
- Meta-learning frameworks: Why needed - allows strategy effectiveness prediction; Quick check - verify transfer across training phases
- Uncertainty quantification: Why needed - identifies informative samples; Quick check - validate uncertainty estimates correlate with learning potential

## Architecture Onboarding
**Component map:** Data samples -> Strategy modules (uncertainty, diversity, class balance, boundary) -> Weighting network -> Submodular optimizer -> Coreset output
**Critical path:** Input data flows through parallel strategy modules, whose outputs are weighted by the meta-learner before being combined via submodular maximization to produce the final coreset
**Design tradeoffs:** Adaptivity vs. computational overhead - dynamic weighting adds complexity but enables better sample selection; Theoretical guarantees vs. practical efficiency - submodularity provides bounds but may limit strategy flexibility
**Failure signatures:** Overfitting to validation set (if weighting becomes too specialized), poor diversity maintenance (if diversity weight drops too low), class imbalance issues (if balance strategy is underweighted)
**First experiments:** 1) Test strategy weights stability across different random seeds; 2) Validate coreset representativeness on held-out validation data; 3) Compare adaptation speed across different dataset sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical approximation guarantees rely on submodularity assumptions that may not hold for all loss functions
- Static performance comparisons at fixed budgets don't address behavior under extreme budget constraints
- Observed strategy adaptation patterns show correlation but not causal mechanisms for strategy effectiveness

## Confidence
High: Theoretical framework and coreset selection methodology
Medium: Empirical performance gains and efficiency metrics
Low: Generalizability across domains and datasets not included in the study

## Next Checks
1. Test MODE on domain-shifted datasets (e.g., DomainNet, WILDS) to assess robustness to distribution shift
2. Implement ablation studies isolating each selection strategy's contribution across different training phases
3. Benchmark memory and time efficiency on edge devices and varying batch sizes to verify scalability claims