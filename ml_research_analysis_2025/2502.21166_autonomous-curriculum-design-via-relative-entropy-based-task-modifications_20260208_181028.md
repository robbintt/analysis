---
ver: rpa2
title: Autonomous Curriculum Design via Relative Entropy Based Task Modifications
arxiv_id: '2502.21166'
source_url: https://arxiv.org/abs/2502.21166
tags:
- learning
- agent
- curriculum
- state
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of autonomous curriculum design
  in reinforcement learning, aiming to improve learning efficiency by leveraging uncertainty-based
  task selection. The proposed method, READ-C (Relative Entropy based Autonomous Design
  of Curricula), measures the learner's policy uncertainty using relative entropy
  (KL divergence) to identify states where additional learning would be most beneficial.
---

# Autonomous Curriculum Design via Relative Entropy Based Task Modifications

## Quick Facts
- arXiv ID: 2502.21166
- Source URL: https://arxiv.org/abs/2502.21166
- Reference count: 40
- Primary result: READ-C improves learning efficiency by 2-3x over baselines using KL-based uncertainty for curriculum design

## Executive Summary
This paper introduces READ-C, a method for autonomous curriculum design in reinforcement learning that leverages policy uncertainty to guide learning. The approach uses relative entropy (KL divergence) to identify states where the learner's policy deviates most from an optimal reference, then modifies the MDP by setting these high-uncertainty states as new start states. Two variants are presented: READ-C-TD (teacher-dependent) using a pre-trained optimal policy for uncertainty measurement, and READ-C-SA (self-assessed) using a regression model trained on a simpler environment. The method is evaluated on three domains (Key-Lock, Capture-the-Flag, and Parking) and outperforms baseline approaches including random curricula and direct target learning.

## Method Summary
READ-C operates by measuring policy uncertainty using KL divergence between the learner's policy and a reference policy (either a teacher or estimated via regression). The method modifies the MDP by changing the start state to regions of high uncertainty, guiding the agent to learn in areas where its policy is least confident. The curriculum generation process involves training the agent for a fixed number of iterations, sampling states from a buffer, computing uncertainty, selecting high-uncertainty states (optionally clustered), and restarting training from these states. The process repeats until curriculum completion, then trains on the original task. Theoretical convergence guarantees are provided using two-time-scale optimization.

## Key Results
- READ-C outperforms random curriculum generation, direct target learning, and policy-change-based curriculum methods across all three evaluation domains
- READ-C-SA with a simpler source environment for regressor training shows particularly strong performance, demonstrating effective transfer of uncertainty estimation
- Heuristic filtering of high-uncertainty states by proximity to goal states or distance from low-entropy regions further improves curriculum efficiency
- Convergence rate improvements of 20-40% over baselines with 2-3x faster learning in terms of sample efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selecting start states based on relative entropy (KL divergence) between learned policy and true policy identifies regions where additional learning yields maximal policy improvement.
- **Mechanism:** KL divergence quantifies how much the agent's action distribution diverges from an optimal reference. High divergence indicates states where the agent is most uncertain or wrong about optimal behavior. By resetting training to start from these states, the agent receives more targeted experience in problematic regions, reducing sample waste on already-learned areas.
- **Core assumption:** The true (or teacher) policy provides a meaningful reference for "optimal" behavior; divergence from it reflects epistemic uncertainty rather than aleatoric noise.
- **Evidence anchors:**
  - [abstract] "measures the uncertainty in the learner's policy using relative entropy, and guides the agent to states of high uncertainty to facilitate learning"
  - [Section 4.1, Eq. 3] Formal definition of D_KL(P_true || P_learnt)
  - [corpus] GACL paper confirms adaptive curriculum improves robotics training; weak direct evidence for KL specifically
- **Break condition:** If the teacher policy is suboptimal or the environment has high stochasticity where multiple policies are equally valid, KL divergence may incorrectly flag valid exploration as "uncertain."

### Mechanism 2
- **Claim:** A regression model trained on a simpler source environment can predict relative entropy in a more complex target environment without requiring a teacher.
- **Mechanism:** READ-C-SA trains a gradient boosting regressor on features derived from policy snapshots (relative entropy between current and past policy, individual entropies, Q-values, visit counts) using ground-truth KL divergence from a teacher on a simpler environment. Because these features are domain-agnostic (information-theoretic, not state-action specific), the regressor generalizes to predict uncertainty in the target task.
- **Core assumption:** The simpler environment shares structural characteristics with the target task such that the relationship between policy features and true uncertainty transfers.
- **Evidence anchors:**
  - [Section 4.4] "The regressor serves as a proxy function... using the information theoretic data generated on a simpler environment"
  - [Figure 2(e)] READ-C-SA with simpler source outperforms regressors trained on similar/same environments
  - [corpus] No direct corpus validation of this transfer mechanism
- **Break condition:** If source and target environments differ fundamentally (e.g., discrete vs. continuous dynamics, different reward structures), the regressor's predictions may be systematically biased.

### Mechanism 3
- **Claim:** Heuristic filtering of high-uncertainty states by proximity to goal states or distance from low-entropy regions improves curriculum efficiency.
- **Mechanism:** Raw KL maximization may select states far from goal states or isolated from already-learned regions, leading to long episodes with sparse learning signal. Proximity filtering prioritizes uncertain states near positive-reward terminals; max-distance filtering prioritizes uncertain states far from already-mastered regions, encouraging frontier exploration.
- **Core assumption:** States near goal states provide denser learning signal; training efficiency correlates with episode length and reward proximity.
- **Evidence anchors:**
  - [Section 4.6] "Relative entropy alone would prioritize training in such regions even if not advantageous"
  - [Figure 2(c)] READ-C-TD + proximity outperforms READ-C-TD alone
  - [corpus] No corpus papers test this specific heuristic
- **Break condition:** In sparse-reward or deceptive-reward environments, proximity to apparent goals may mislead curriculum selection.

## Foundational Learning

- **KL Divergence (Relative Entropy):**
  - Why needed here: Core uncertainty metric driving curriculum selection; requires understanding how divergence measures distributional mismatch.
  - Quick check question: Can you explain why D_KL(P||Q) ≠ D_KL(Q||P) and which direction READ-C uses?

- **Actor-Critic Reinforcement Learning:**
  - Why needed here: READ-C's convergence proof and implementation assume actor-critic architecture with separate policy (actor) and value (critic) networks.
  - Quick check question: What role does the critic play in computing the advantage for policy updates?

- **Curriculum Learning in RL:**
  - Why needed here: Provides context for why task sequencing matters and how start-state modification differs from reward shaping or environment parameterization approaches.
  - Quick check question: How does modifying start states differ from modifying reward functions for curriculum generation?

## Architecture Onboarding

- **Component map:**
  Agent Networks (Actor + Critic) -> Replay Buffer -> State Buffer -> Uncertainty Module (Teacher/Regressor) -> Clustering Module -> Start State Selection -> Modified MDP Training

- **Critical path:**
  1. Train agent on target task for η iterations (initial policy)
  2. Sample subset of states from SB
  3. Compute uncertainty (KL via teacher OR regressor prediction)
  4. Select highest-uncertainty state (or cluster) as new start state
  5. Train on modified MDP until convergence (entropy reduction stalls)
  6. Repeat until curriculum length exhausted, then train on original task

- **Design tradeoffs:**
  - READ-C-TD vs. READ-C-SA: TD provides accurate uncertainty but requires pre-trained teacher; SA removes teacher dependency but introduces regressor error
  - Cluster size: Moderate (100-200) balances variance reduction with state-space coverage; too few clusters loses granularity, too many increases noise
  - Source environment complexity for regressor: Simpler source generalizes better than similar/same source (counterintuitive but empirically supported)

- **Failure signatures:**
  - Curriculum stalls in same region: Uncertainty metric not decreasing; check if policy is actually updating
  - Regressor predictions unstable: Features may be out-of-distribution; verify source-target similarity
  - Low convergence rate (Figure 9): Agent not reaching target reward; may need longer curriculum or different heuristic

- **First 3 experiments:**
  1. Replicate Key-Lock 20×20 with READ-C-TD to validate implementation; compare learning curve against Figure 2(a)
  2. Ablate clustering: Run READ-C-SA with no clustering vs. 200 clusters on same environment; measure variance in convergence time
  3. Test regressor transfer: Train regressor on 10×10 environment, deploy on 30×30; compare against same-target regressor to verify generalization claim

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the text provided.

## Limitations
- Fixed curriculum length of 4 steps across all domains without sensitivity analysis
- Heuristic filtering methods (proximity, max-distance) lack principled derivation and may be domain-specific
- Performance gap between READ-C-TD and READ-C-SA in continuous action spaces remains unexplained

## Confidence
- **High confidence**: Convergence guarantees using two-time-scale optimization; overall performance improvements over baselines in empirical evaluation
- **Medium confidence**: KL divergence as effective uncertainty metric; regression model transfer from simpler to complex environments
- **Low confidence**: Universal applicability of heuristic filtering; fixed curriculum length optimization

## Next Checks
1. Test KL divergence against alternative uncertainty metrics (entropy, variance of Q-values) on Key-Lock domain to isolate the contribution of relative entropy
2. Conduct curriculum length sensitivity analysis across all three domains (2, 4, 6, 8 steps) to identify optimal configuration
3. Implement ablation study removing both proximity and max-distance heuristics to quantify their individual contributions to performance gains