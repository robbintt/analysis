---
ver: rpa2
title: 'CarbonChat: Large Language Model-Based Corporate Carbon Emission Analysis
  and Climate Knowledge Q&A System'
arxiv_id: '2501.02031'
source_url: https://arxiv.org/abs/2501.02031
tags:
- carbon
- emission
- report
- emissions
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CarbonChat, an LLM-based system for analyzing
  corporate carbon emissions and answering climate-related questions. The system addresses
  challenges such as outdated climate knowledge in LLMs, lack of specialization in
  traditional architectures, and time-consuming sustainability report analysis.
---

# CarbonChat: Large Language Model-Based Corporate Carbon Emission Analysis and Climate Knowledge Q&A System

## Quick Facts
- arXiv ID: 2501.02031
- Source URL: https://arxiv.org/abs/2501.02031
- Reference count: 40
- Primary result: ROUGE-1/2/L scores of 0.592/0.450/0.540 with BERTScore F1 of 0.906; Text2SQL execution accuracy of 89.2% and exact match of 79.9%

## Executive Summary
CarbonChat is an LLM-based system designed for analyzing corporate carbon emissions and answering climate-related questions. It addresses limitations in existing LLMs regarding outdated climate knowledge and lack of specialization by proposing a diversified index module construction method, an enhanced self-prompt retrieval-augmented generation (RAG) architecture, and a Text2SQL system for efficient query conversion. The system analyzes enterprise sustainability reports across 14 dimensions based on the GHG Protocol, ensuring accuracy through hallucination detection and traceability. Experimental results demonstrate strong performance across multiple evaluation metrics.

## Method Summary
The system implements a diversified index module using conditional document segmentation strategies including document tree-based chunking, rule-based chunking, semantic-based chunking, and sliding window chunking. For retrieval, it employs a self-prompting enhanced RAG architecture that integrates intent recognition, structured reasoning chains, hybrid retrieval (BM25 + BGE-M3 embedding with reciprocal rank fusion), and key sentence extraction. The Text2SQL component uses RAG-retrieved examples combined with schema-aware prompting and security validation. The system processes 1,000 Chinese policy documents and 100 corporate sustainability reports, with evaluation based on ROUGE scores, BERTScore, and Text2SQL execution accuracy.

## Key Results
- ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.592, 0.450, and 0.540 respectively
- BERTScore F1 of 0.906 for generated answers
- Text2SQL system achieves 89.2% execution accuracy and 79.9% exact match
- Hybrid retrieval with self-prompting improves ROUGE scores over standard RAG (0.529/0.392/0.468)

## Why This Works (Mechanism)

### Mechanism 1: Diversified Index Module Construction
Adaptive document segmentation improves retrieval quality by matching chunking strategy to document structure. Four chunking strategies are applied conditionally: Document Tree-Based Chunking for hierarchical documents using path compression optimization, Rule-Based Chunking for fixed-format policy documents with explicit article numbering, Semantic-Based Chunking using SeqModel with BERT encoding for unstructured reports, and Sliding Window Chunking with LLM-generated summaries for long paragraphs. Structured data (tables, formulas, images) is extracted separately and stored in MySQL or vector databases accordingly. Core assumption: Document structure contains implicit signals about optimal segmentation boundaries that uniform chunking misses.

### Mechanism 2: Self-Prompting Enhanced RAG with Hybrid Retrieval
Query rewriting + key sentence extraction + hybrid retrieval (BM25 + embedding) improves answer relevance over standard RAG. The pipeline executes: query rewriting decomposes complex queries into sub-queries and resolves ambiguities, few-shot COT generates preliminary answers, key sentences are extracted from preliminary answers (not keywords, to preserve context), hybrid retrieval combines BM25 and BGE-M3-Embedding using reciprocal rank fusion, and BGE-reranker-large reorders Top-N results to Top-5. Core assumption: Preliminary model answers contain semantic signals that improve retrieval relevance beyond the original query alone.

### Mechanism 3: Text2SQL with COT-Guided Schema Matching
RAG-retrieved examples + schema-aware prompting + SQL validation achieves higher execution accuracy than direct Text2SQL generation. Four-step pipeline: LLM parses user query for time extraction and table identification, RAG retrieves similar question-SQL examples to guide step-by-step COT reasoning, schema matching maps semantic intent to database columns with Chinese alias generation, security validation blocks DELETE/INSERT/UPDATE, followed by syntax optimization and auto-repair. Core assumption: Example-based COT reasoning transfers to unseen query patterns without requiring fine-tuning on the target schema.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed: The entire CarbonChat system builds on RAG; understanding retrieval-relevance-feedback loops is prerequisite to debugging the self-prompting architecture.
  - Quick check: Can you explain why BM25 alone fails on semantic synonyms like "emissions" vs "greenhouse gas output"?

- **Concept: GHG Protocol Scopes (1, 2, 3)**
  - Why needed: The 14 analysis dimensions are derived from GHG Protocol categorization; without this, prompt engineering for report evaluation will produce misaligned outputs.
  - Quick check: Which scope covers emissions from a company's purchased electricity?

- **Concept: Reciprocal Rank Fusion (RRF)**
  - Why needed: Hybrid retrieval combines BM25 and embedding scores using RRF; tuning λ and c parameters directly affects retrieval quality.
  - Quick check: If BM25 returns a document at rank 1 and embedding returns it at rank 10, how does RRF combine these signals?

## Architecture Onboarding

- **Component map:** Document Parsing → Vector DB (BGE-M3) + MySQL (tables) → Self-Prompting RAG → Query Rewriter → COT Engine → Key Sentence Extractor → Hybrid Retriever → Reranker → LLM Generator → Text2SQL → Time/Table Extractor → Example Retriever → SQL Generator → Security Validator → Query Executor → Hallucination Module → Document Tree Tracer → Timestamp Checker → Hallucination Tagger

- **Critical path:** Query → Intent Classification (P4) → Route to RAG or Text2SQL → Retrieval/SQL Generation → Hallucination Check (P11) → Response

- **Design tradeoffs:** Larger chunks improve context but increase latency; the system uses adaptive chunking per document type. Self-prompting adds LLM calls (query rewrite + COT + key extraction) before retrieval, increasing cost ~3x vs standard RAG. Text2SQL security validation blocks writes but limits system to read-only analytics.

- **Failure signatures:** Empty retrieval results: Check if query rewriting over-generalized, losing domain-specific terms. SQL syntax errors on valid queries: Verify schema information is included in prompt context. Hallucination tags on accurate answers: Inspect document tree path compression—node merging may lose citation granularity.

- **First 3 experiments:**
  1. Ablate hybrid retrieval: Set λ=1.0 (BM25 only) and λ=0.0 (embedding only) on 100 test queries; measure ROUGE-L delta to quantify each retriever's contribution.
  2. Stress-test Text2SQL: Submit 50 queries requiring 3+ table joins; compare EX scores with and without few-shot COT examples in prompt.
  3. Hallucination detection precision: Manually label 30 responses as hallucinated/valid; compare against P11 tagger accuracy to establish baseline false positive rate.

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning a localized Large Language Model (LLM) match or exceed the performance of the proprietary Qwen-Max model in specialized carbon emission analysis tasks? The authors explicitly list "Localization of LLM Development" in the Future Work section, proposing a transition from the existing Qwen model to a localized LLM to enhance controllability and reliability. This remains unresolved because the current system relies on API-based proprietary models, which may limit data privacy and domain-specific optimization compared to a locally hosted, fine-tuned model.

### Open Question 2
How can the system's natural language processing modules be optimized to further reduce hallucination rates in complex reasoning scenarios? The Conclusion identifies "Optimizing Natural Language Processing Capabilities" as a key area for future work, specifically noting the need to "reduce hallucination issues that may arise during the parsing and generation process." This remains unresolved despite current mechanisms like self-prompting and hallucination tagging, as the paper acknowledges that "complex problems" still pose a risk of generating factually inconsistent or fabricated statements.

### Open Question 3
To what extent do semantic similarity metrics (like BERTScore) correlate with strict factual accuracy in the domain of corporate carbon accounting? The paper relies on ROUGE and BERTScore as primary evaluation indicators, but the "Hallucination analysis" section implies that high semantic similarity does not always equate to factual correctness. This remains unresolved because high overlap with reference text does not guarantee the absence of numerical errors or regulatory misinterpretations, which are critical in carbon accounting.

## Limitations
- The ablation study does not isolate the self-prompting chain's contribution to ROUGE improvements
- Hallucination detection mechanism lacks reported false positive rate metrics
- Text2SQL security validation does not address injection vulnerabilities or query complexity limits beyond three tables

## Confidence

- **High confidence**: The diversified index construction methodology (Algorithm 1) and its application to different document types is well-specified and reproducible.
- **Medium confidence**: The hybrid retrieval framework (BM25 + BGE-M3 + RRF) will improve over single-retrieval methods, but the specific gains from self-prompting remain uncertain without proper ablation.
- **Low confidence**: The hallucination detection accuracy and its impact on user trust, given the lack of reported precision/recall metrics.

## Next Checks

1. **Isolate self-prompting contribution**: Run a controlled ablation where only the query rewriting + COT + key sentence extraction steps are removed from the RAG pipeline, keeping hybrid retrieval intact, to measure the pure self-prompting effect on ROUGE-L scores.

2. **Hallucination detection benchmarking**: Manually label 100 generated answers (50 from policy docs, 50 from reports) for hallucinations, then compare against P11 tagger output to establish precision, recall, and false positive rate.

3. **Text2SQL complexity stress test**: Construct 30 queries requiring 4+ table joins with nested aggregations and measure execution accuracy degradation compared to single-table queries.