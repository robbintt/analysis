---
ver: rpa2
title: 'InfGraND: An Influence-Guided GNN-to-MLP Knowledge Distillation'
arxiv_id: '2601.08033'
source_url: https://arxiv.org/abs/2601.08033
tags:
- distillation
- graph
- influence
- infgrand
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InfGraND, an influence-guided knowledge distillation
  framework that transfers structural knowledge from GNNs to MLPs. Unlike prior methods
  that treat all nodes uniformly or rely on prediction uncertainty, InfGraND uses
  a graph-aware influence metric to prioritize structurally important nodes during
  distillation.
---

# InfGraND: An Influence-Guided GNN-to-MLP Knowledge Distillation

## Quick Facts
- **arXiv ID:** 2601.08033
- **Source URL:** https://arxiv.org/abs/2601.08033
- **Reference count:** 40
- **Primary result:** InfGraND consistently outperforms existing GNN-to-MLP distillation methods, achieving average improvements of 12.6% over vanilla MLPs in transductive settings and 9.3% in inductive settings.

## Executive Summary
This paper introduces InfGraND, an influence-guided knowledge distillation framework that transfers structural knowledge from GNNs to MLPs. Unlike prior methods that treat all nodes uniformly or rely on prediction uncertainty, InfGraND uses a graph-aware influence metric to prioritize structurally important nodes during distillation. The method also incorporates one-time multi-hop neighborhood feature pre-computation to provide structural awareness without inference overhead. Extensive experiments across seven datasets in both transductive and inductive settings show that InfGraND consistently outperforms existing GNN-to-MLP distillation methods, achieving average improvements of 12.6% over vanilla MLPs in transductive settings and 9.3% in inductive settings, while also surpassing its own GNN teachers in many cases.

## Method Summary
InfGraND transfers knowledge from a trained GNN teacher to an MLP student through a novel influence-guided distillation framework. The method computes structural influence scores for each node using multi-hop feature propagation, then uses these scores to weight both supervised and distillation losses during student training. To provide the student with structural awareness without inference-time overhead, the framework pre-computes multi-hop neighborhood features offline and feeds them directly to the MLP. The student is trained using a combination of influence-weighted cross-entropy loss on labeled nodes and influence-weighted KL divergence from teacher predictions, with hyperparameters controlling the balance between these objectives.

## Key Results
- Outperforms existing GNN-to-MLP distillation methods by 12.6% on average in transductive settings
- Achieves 9.3% average improvement over vanilla MLPs in inductive settings
- Student MLPs surpass their own GNN teachers in many cases
- One-time feature pre-computation eliminates inference-time graph operations
- Influence weighting consistently improves performance across all seven datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prioritizing structurally influential nodes during distillation yields better student MLP generalization than uniform or entropy-based sampling.
- **Mechanism:** The Global Influence Score (Eq. 5) quantifies each node's impact on the graph via how feature perturbations propagate after k message-passing iterations. This score weights the distillation loss (Eq. 8), amplifying gradient signals from high-influence neighbors. Section D.2 shows analytically that `γ2·Ig(vj)` scales gradient magnitude proportionally.
- **Core assumption:** Nodes that structurally influence more of the graph encode more transferable knowledge. This assumes homophilic graphs where local structure correlates with label similarity.
- **Evidence anchors:**
  - [abstract]: "prioritizing structurally influential nodes to guide the distillation process, ensuring that the MLP learns from the most critical parts of the graph"
  - [section 5.2.1]: Figure 2 shows GNNs trained on high-influence nodes outperform those on low-influence nodes across Cora/Citeseer/Pubmed.
  - [corpus]: ProGMLP confirms progressive GNN-to-MLP distillation is active research; does not validate influence-weighting specifically.
- **Break condition:** If influence scores decorrelate from generalization value (e.g., heterophilic graphs where high-influence nodes have conflicting labels), or if overhead from influence computation exceeds practical limits.

### Mechanism 2
- **Claim:** One-time multi-hop feature pre-computation gives MLPs structural awareness without inference-time cost.
- **Mechanism:** Linear propagation `X^(k) = ÃX^(k-1)` (Eq. 4) generates P multi-hop matrices. Average pooling (Eq. 6) compresses them into a fixed input `X̃` for the student MLP. This occurs once offline; inference uses pre-computed features directly.
- **Core assumption:** Static graph structure at training time captures patterns needed for generalization. Assumes feature/label distributions are stable between pre-computation and deployment.
- **Evidence anchors:**
  - [abstract]: "one-time multi-hop neighborhood feature pre-computation, which enriches the student MLP's input and thus avoids inference-time overhead"
  - [section 4.2]: "The resulting matrix, X̃, contains multi-hop neighborhood information... there is no added cost at inference time."
  - [corpus]: Corpus papers do not explicitly analyze this pre-computation approach; validation is internal to this work.
- **Break condition:** Dynamic graphs where structure changes after pre-computation; extremely sparse graphs where 2-hop neighborhoods contain mostly noise.

### Mechanism 3
- **Claim:** Combining influence-weighted supervised and distillation losses provides complementary learning signals.
- **Mechanism:** Total loss `L_t = λL_s + (1-λ)L_d` (Eq. 9). L_s (Eq. 7) uses influence-weighted cross-entropy on labeled nodes; L_d (Eq. 8) uses influence-weighted KL divergence from teacher neighbor predictions. The `γ1` term ensures all neighbors contribute baseline gradients; `γ2·Ig(vj)` amplifies for influential nodes.
- **Core assumption:** Ground-truth labels and teacher soft predictions offer non-redundant information; influence weighting improves signal-to-noise in both.
- **Evidence anchors:**
  - [section 4.3]: "The γ1 term provides a baseline distillation gradient from all neighbors, while the γ2·Ig(vj) term acts as a fine-grained amplifier"
  - [section 5.2.4]: Ablation shows w/Influence and w/Propagation variants underperform the full model; "both components are most effective when used together"
  - [corpus]: Diffusion-Assisted Distillation explores self-supervised KD but does not address this dual-loss structure.
- **Break condition:** Poor hyperparameter tuning (λ, γ1, γ2, δ1, δ2) causing gradient imbalance; highly noisy labels where supervised loss misleads.

## Foundational Learning

- **Knowledge Distillation Fundamentals:**
  - Why needed here: InfGraND transfers GNN knowledge to MLP via soft labels and temperature-scaled KL divergence.
  - Quick check question: Can you explain why soft labels contain more information than hard labels, and what the temperature τ controls?

- **Message Passing in GNNs:**
  - Why needed here: The influence metric relies on understanding how perturbations propagate through aggregation/update operations.
  - Quick check question: For a 2-layer GCN, trace how a change in node i's features affects node j's representation when they're 2 hops apart.

- **Influence Functions and Jacobian Analysis:**
  - Why needed here: The influence score (Eq. 2) uses expected Jacobians to quantify sensitivity; Section D derives how this scales gradients.
  - Quick check question: Why does the paper approximate the expected Jacobian via random walk equivalence rather than direct computation?

## Architecture Onboarding

- **Component map:** Teacher GNN -> Influence Computer -> Feature Propagator -> Student MLP
- **Critical path:** 1. Train and freeze teacher GNN → 2. Compute influence scores (Eq. 5) → 3. Pre-compute propagated features (Eq. 6) → 4. Train student MLP with `L_t` (Eq. 9) → 5. Deploy student MLP (inference uses `X̃` directly, no graph operations)
- **Design tradeoffs:**
  - **k-hop depth for influence:** Higher k captures more global influence but increases computation O(|E_k|·D). Paper uses k=2; Table 9 shows 2-hop takes 70s for OGBN-Arxiv.
  - **Pooling strategy:** Mean pooling works best (Table 3); min/max alternatives degrade performance.
  - **λ balance:** Paper finds λ=0.1 optimal (Section 5.2.6), favoring distillation over supervised loss.
- **Failure signatures:**
  - Student underperforms vanilla MLP → Check feature propagation; `X̃` may be corrupted or P mismatched.
  - Student matches teacher poorly on high-influence nodes → Verify influence scores computed correctly; check if `γ2` is near zero.
  - Inductive performance collapses → Check observed/total node ratio (Table 5); sparse supervision limits propagation utility.
- **First 3 experiments:**
  1. **Baseline replication:** Run vanilla MLP, GLNN, and InfGraND on Cora with GCN teacher. Verify ~84% accuracy matches Table 1.
  2. **Ablation by component:** Disable influence weighting (set γ2=0, δ2=0) and compare to full model on Citeseer. Expect ~1-2% drop per Table 2.
  3. **Influence vs. alternatives:** Substitute `Ig(vi)` with normalized degree or PageRank in the loss. Table 8 shows expect ~2-3% degradation; confirms influence captures more than topology alone.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the influence-guided distillation framework be effectively adapted for heterophilous graphs where connected nodes exhibit dissimilar features?
- Basis in paper: [explicit] Section 6 states that future work will involve extending the method to "different graph types such as heterophilous... graphs," implicitly acknowledging the current focus on homophilic benchmarks.
- Why unresolved: The proposed influence metric (Eq. 4) relies on feature propagation where influence is gauged by feature similarity (cosine similarity), an assumption that may not hold or may invert in heterophilic settings.
- What evidence would resolve it: Evaluation results on standard heterophilic datasets (e.g., Chameleon, Squirrel) or a reformulation of the influence metric that accounts for negative feature correlation.

### Open Question 2
- Question: Does combining entropy-based discrimination with structural influence yield a statistically significant improvement over influence-only guidance?
- Basis in paper: [explicit] Section 6 explicitly proposes to "investigate hybrid methods that combine entropy-based discrimination with structural-aware approaches."
- Why unresolved: It is currently unclear if the information provided by the teacher's prediction uncertainty (entropy) is redundant with, complementary to, or conflicting with the graph-aware influence score.
- What evidence would resolve it: A comparative study of a hybrid loss function (Influence + Entropy) against the pure InfGraND method across diverse datasets.

### Open Question 3
- Question: Does the linear approximation used to compute influence scores fail to capture the complex, non-linear relational importance modeled by attention-based teachers like GAT?
- Basis in paper: [inferred] Section 4.1 notes that to calculate influence efficiently, the method removes non-linear activations and weight matrices ("Inspired by Simplifying Graph Convolutional Networks (SGC)"), effectively linearizing the teacher's complex behavior.
- Why unresolved: While this linearization enables efficient computation, it assumes that structural importance is linear, potentially misestimating the influence of nodes that are only critical under non-linear attention dynamics.
- What evidence would resolve it: An analysis comparing the ranking of nodes by the linear influence metric against a ground-truth importance ranking derived from the full non-linear teacher model (e.g., via integrated gradients).

### Open Question 4
- Question: What are the theoretical mechanisms that allow the student MLP to outperform the GNN teacher in classification accuracy?
- Basis in paper: [inferred] Section 5.2.2 and Table 1 highlight the surprising result that the student MLP often surpasses the teacher GNN (e.g., by +3.2% on average transductive), but the explanation is limited to high-level intuition about generalization and prioritization.
- Why unresolved: It is counter-intuitive that a student with less architectural capacity and no direct access to the graph structure during inference can consistently beat the teacher; a formal generalization bound or representation analysis is missing.
- What evidence would resolve it: A theoretical analysis of the loss landscape or a feature space visualization (beyond t-SNE) demonstrating that the distillation process acts as a powerful regularizer that reduces overfitting inherent in the GNN teacher.

## Limitations

- The influence metric assumes homophilic graphs where structural importance correlates with predictive value, limiting effectiveness on heterophilic graphs
- The one-time pre-computation approach depends on static graph structure between training and deployment, making it unsuitable for dynamic environments
- The dual-loss formulation requires careful hyperparameter tuning and assumes that influence-weighted supervised and distillation losses provide complementary signals

## Confidence

- **High confidence:** The core mechanism of using influence-weighted losses for knowledge distillation (Mechanism 1) is well-supported by ablation studies and theoretical justification in Section D.2
- **Medium confidence:** The pre-computation approach (Mechanism 2) shows practical benefits but hasn't been validated on dynamic graphs or compared to alternative structural encoding methods
- **Medium confidence:** The dual-loss formulation (Mechanism 3) performs well empirically, but the sensitivity to hyperparameter tuning and the assumption of complementary signals could be more thoroughly examined

## Next Checks

1. **Heterophily test:** Evaluate InfGraND on a benchmark heterophilic graph (e.g., Cornell, Texas, or Squirrel) to assess whether influence weighting remains effective when local structure doesn't predict labels.

2. **Dynamic graph validation:** Apply the pre-computation approach to a graph that changes over time (e.g., temporal citation networks) and measure performance degradation as structure drifts from pre-computed features.

3. **Influence score ablation:** Replace the influence metric with alternative structural importance measures (eigenvector centrality, PageRank) while keeping all other components constant to isolate the contribution of the specific influence formulation.