---
ver: rpa2
title: 'Beyond Worst-Case Online Classification: VC-Based Regret Bounds for Relaxed
  Benchmarks'
arxiv_id: '2504.10598'
source_url: https://arxiv.org/abs/2504.10598
tags:
- learning
- online
- margin
- regret
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online binary classification by relaxing the
  notion of optimality. Instead of minimizing regret relative to the exact minimal
  binary error, the authors propose comparing with relaxed benchmarks that capture
  smoothed notions of optimality.
---

# Beyond Worst-Case Online Classification: VC-Based Regret Bounds for Relaxed Benchmarks

## Quick Facts
- **arXiv ID:** 2504.10598
- **Source URL:** https://arxiv.org/abs/2504.10598
- **Authors:** Omar Montasser; Abhishek Shetty; Nikita Zhivotovskiy
- **Reference count:** 40
- **Primary result:** Online learning algorithm achieving regret bounds depending only on VC dimension and metric entropy, with logarithmic (not polynomial) dependence on the inverse margin γ.

## Executive Summary
This paper studies online binary classification by relaxing the notion of optimality. Instead of minimizing regret relative to the exact minimal binary error, the authors propose comparing with relaxed benchmarks that capture smoothed notions of optimality. These benchmarks include robustness to small input perturbations, performance under Gaussian smoothing, or maintaining a prescribed output margin. The key contribution is an online learning algorithm that achieves regret guarantees depending only on the VC dimension and the complexity of the instance space, with only an O(log(1/γ)) dependence on the generalized margin γ. This contrasts with most existing regret bounds which typically exhibit polynomial dependence on 1/γ.

## Method Summary
The paper studies online binary classification where the learner competes against relaxed benchmarks rather than the standard optimal binary loss. The method constructs a finite γ-cover of the instance space, projects the hypothesis class onto this cover to create a finite expert set, and runs Multiplicative Weights over these experts. The algorithm achieves regret bounds that depend on VC dimension and metric entropy, with logarithmic dependence on the inverse margin. Three specific benchmarks are studied: worst-case perturbation (OPTγ_pert), Gaussian smoothing (OPTσ,εgauss), and margin-based (OPTγmargin).

## Key Results
- Achieves regret bounds depending only on VC dimension and metric entropy, with O(log(1/γ)) dependence on the generalized margin γ
- For halfspaces, covering the parameter space (rather than instance space) reduces complexity from O(d²) to O(d)
- Proves lower bounds showing that metric entropy dependence is necessary for general classes
- Demonstrates that Lipschitzness of function classes is crucial for avoiding polynomial 1/γ dependence

## Why This Works (Mechanism)

### Mechanism 1: Worst-Case Perturbation Relaxation (Cover-Based Reduction)
By competing with a relaxed benchmark OPTγ_pert that tolerates worst-case errors within γ-balls, the algorithm achieves regret depending only on VC dimension and metric entropy, with logarithmic (not polynomial) dependence on 1/γ. Construct a γ-cover Z of the instance space X, project hypothesis class H onto Z to form a finite expert set, then run Multiplicative Weights. Any predictor that labels all points in a γ-ball consistently will agree with its projection on the cover point, so the finite expert set contains a good competitor.

### Mechanism 2: Gaussian Smoothing with Empirical Approximation
Competing with predictors that correctly classify > 1/2 + ε/2 of Gaussian perturbations achieves regret with 1/ε² rather than polynomial 1/γ dependence on the margin. Use uniform convergence (O(vc(H)/ε²) samples suffice to approximate population error under Gaussian noise) combined with the Lipschitzness of Gaussian-smoothed classifiers. This reduces the infinite expert space to a finite covering problem.

### Mechanism 3: Margin-Based Covering via Lipschitz Reduction
For L-Lipschitz function classes, competing with margin-γ predictors achieves regret bounded by the minimum of VC-based and fat-shattering-based complexity measures. Cover instance space at scale γ/(2L). For Lipschitz functions, behavior at any point determines behavior within γ-balls. Two cover options: (1) project sign(F) onto the cover (VC-based), or (2) use ℓ∞-covers at scale γ/4 (fat-shattering-based).

## Foundational Learning

- **Concept: VC Dimension vs Littlestone Dimension**
  - Why needed here: The paper's central insight is bypassing Littlestone dimension (which characterizes worst-case online learning) by using VC dimension plus metric entropy. Thresholds on [0,1] have vc = 1 but lit = ∞.
  - Quick check question: Why can't you online-learn thresholds on [0,1] in the worst case, but can compete with smoothed benchmarks?

- **Concept: Covering Numbers and Metric Entropy**
  - Why needed here: log|C(X, ρ, γ)| bounds the number of "distinguishable regions" at scale γ. For X ⊆ ℝᵈ, this is O(d log(1/γ)), determining expert set size.
  - Quick check question: If you have a metric space with covering number growing as (1/γ)ᵈ, what does this tell you about the instance space dimension?

- **Concept: Multiplicative Weights Regret**
  - Why needed here: The algorithm achieves √(T log N) regret with N experts (Lemma 21). The paper's contribution is constructing a small expert set that contains a good competitor for the relaxed benchmark.
  - Quick check question: Why does reducing the expert set from |H| to |H|_Z improve the bound, and when does it fail?

## Architecture Onboarding

- **Component map:** Cover construction → Projection module → Multiplicative Weights core → Loss evaluator
- **Critical path:** Cover construction → Projection size bound → MW initialization → Online prediction loop → Distribution update
- **Design tradeoffs:**
  - Instance-space vs parameter-space covering: For halfspaces, covering X gives O(d²) while covering W (unit ball in parameter space) gives O(d)—Theorem 11 exploits parametric structure
  - Exact vs approximate covers: Any valid cover works; minimizing cover size is often NP-hard
  - Computational efficiency: Paper notes algorithms are not computationally efficient; cutting-plane methods work for realizable halfspaces (Gilad-Bachrach et al.)
- **Failure signatures:**
  - Infinite covering number: When γ, ε, or σ approach zero
  - Exponentially large expert sets: When vc(H) or d is large, log|H|_Z| can still be prohibitive
  - No Lipschitzness: For margin benchmarks, non-Lipschitz classes require fat-shattering dimension, which may be large
- **First 3 experiments:**
  1. Implement halfspace learner with parameter-space covering (Theorem 11); compare O(√(Td log(1/γ))) bound vs Perceptron's O(√(T/γ²)) on synthetic data with known margin. Measure actual regret against OPTγ_margin.
  2. Test worst-case perturbation benchmark (Algorithm 1) on image classification with ℓ∞ perturbations at scale γ = 8/255. Quantify gap between OPTγ_pert and standard OPT to validate the relaxation is meaningful.
  3. Validate Gaussian smoothing mechanism (Algorithm 2) by varying σ ∈ {0.1, 0.5, 1.0} and ε ∈ {0.01, 0.1, 0.25}. Confirm the ε²/log(1/εσ) scaling predicted by Theorem 4's bound.

## Open Questions the Paper Calls Out

**Open Question 1:** Can computationally efficient algorithms be developed for the proposed relaxed benchmarks (OPTγpert, OPTσ,εgauss, OPTγmargin) that achieve the same logarithmic dependence on the inverse margin without requiring an exponential time or space complexity in the input dimension?
- Basis in paper: The authors state: "Investigating computationally efficient versions of our proposed algorithms is an interesting direction to explore in depth in future work."
- Why unresolved: The proposed algorithms utilize Multiplicative Weights over a finite cover of the hypothesis class or space, which is computationally prohibitive (exponential in dimension) for general classes.
- What evidence would resolve it: An algorithm with runtime polynomial in VC dimension and dimension d that achieves the stated regret bounds, potentially by utilizing parametric structure or efficient sampling methods.

**Open Question 2:** Are the partial concept classes Hγ, Hσ,ε, and Fγ (induced by the relaxed benchmarks) differentially privately PAC learnable?
- Basis in paper: The paper notes: "For partial concept classes, it remains open whether finite Littlestone dimension implies differentially private PAC learning... If this question is resolved positively, then combined with our results it would imply that the partial concept classes... are differentially privately PAC learnable."
- Why unresolved: The equivalence between finite Littlestone dimension and differentially private PAC learning is established for total concept classes but remains an open problem for partial concept classes.
- What evidence would resolve it: A proof that finite Littlestone dimension implies private PAC learnability for partial classes, or a specific differentially private learner with bounded sample complexity for the defined partial classes.

**Open Question 3:** For hypothesis classes beyond halfspaces, what structural properties allow the regret bound's dependence on metric entropy (which scales as d²) to be reduced to a linear dependence on dimension d?
- Basis in paper: The paper contrasts the general bound (Theorem 2), which depends on the metric entropy of the input space (quadratic in d), with Theorem 11 for halfspaces, which bypasses this dependence using parametric structure.
- Why unresolved: The lower bound (Theorem 3) proves the metric entropy dependence is necessary for generic classes, but Theorem 11 proves it is suboptimal for specific parametric classes, leaving the boundary conditions undefined.
- What evidence would resolve it: A characterization of hypothesis classes (e.g., generalized linear models) where covering the parameter space directly yields better bounds than covering the input space.

## Limitations

- The paper's theoretical guarantees depend critically on the metric entropy of the instance space, which can be exponential in dimension for natural spaces
- The covering-based approach requires constructing finite expert sets, but the paper does not provide efficient algorithms for this construction beyond simple grid-based methods
- The computational inefficiency is acknowledged but not addressed
- The logarithmic dependence on 1/γ assumes the existence of good covers at scale γ, which may not hold for all natural instance spaces

## Confidence

**High Confidence:** The main theoretical results connecting VC dimension to regret bounds under relaxed benchmarks. The reduction from infinite expert spaces to finite covers via Lipschitzness and uniform convergence is well-established.

**Medium Confidence:** The practical significance of the relaxation benchmarks themselves. While the paper proves theoretical improvements, the empirical benefit depends on whether the gap between OPT and OPT_γ is meaningful in real applications.

**Low Confidence:** The computational feasibility of the proposed algorithms. The paper explicitly notes that all algorithms are not computationally efficient, and no heuristics or approximations are provided for handling large expert sets.

## Next Checks

1. **Dimension vs Covering Trade-off:** For a halfspace class in ℝᵈ with margin γ, empirically measure how the expert set size |H|_Z scales with d and 1/γ. Verify the predicted O(d²) vs O(d) bounds from Theorem 11 when covering instance space vs parameter space.

2. **Benchmark Gap Quantification:** On a standard binary classification benchmark (e.g., CIFAR-10), compute the actual performance gap between OPT (standard) and OPT_pert (worst-case perturbation at γ=8/255). This validates whether the theoretical relaxation provides meaningful practical improvements.

3. **Lipschitzness Dependency Test:** For non-Lipschitz function classes (e.g., neural networks without smoothness constraints), verify empirically that the 1/γ dependence predicted by Theorem 10 emerges, while Lipschitz variants maintain the logarithmic bound.