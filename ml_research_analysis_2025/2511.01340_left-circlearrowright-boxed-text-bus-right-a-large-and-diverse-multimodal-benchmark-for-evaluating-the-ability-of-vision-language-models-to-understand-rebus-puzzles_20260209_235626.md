---
ver: rpa2
title: '$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse
  Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand
  Rebus Puzzles'
arxiv_id: '2511.01340'
source_url: https://arxiv.org/abs/2511.01340
tags:
- rebus
- puzzles
- puzzle
- reasoning
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces |/sync-alt\u1F68C|, a large and diverse\
  \ multimodal benchmark of 1,333 English Rebus Puzzles designed to evaluate Vision-Language\
  \ Models' ability to solve puzzles that require image recognition, commonsense reasoning,\
  \ multi-step reasoning, and image-based wordplay. The dataset includes varied artistic\
  \ styles and 18 categories, with some puzzles augmented using ControlNet to add\
  \ distracting backgrounds and increase difficulty."
---

# $\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles

## Quick Facts
- arXiv ID: 2511.01340
- Source URL: https://arxiv.org/abs/2511.01340
- Reference count: 40
- Key outcome: 1,333 English Rebus Puzzles benchmark with RebusDescProgICE framework achieving 2.1-4.1% gains on closed-source models and 20-30% on open-source models

## Executive Summary
This paper introduces |/sync-altὨC|, a large and diverse multimodal benchmark of 1,333 English Rebus Puzzles designed to evaluate Vision-Language Models' ability to solve puzzles that require image recognition, commonsense reasoning, multi-step reasoning, and image-based wordplay. The dataset includes varied artistic styles and 18 categories, with some puzzles augmented using ControlNet to add distracting backgrounds and increase difficulty. The authors propose RebusDescProgICE, a compute-efficient, model-agnostic framework that combines unstructured image descriptions with structured, code-based reasoning, along with a novel in-context example selection strategy. Experiments across closed- and open-source models show that RebusDescProgICE improves performance by 2.1-4.1% on closed-source models and 20-30% on open-source models compared to Chain-of-Thought reasoning. On augmented test data, GPT-4o achieves a word-level F1 score of 0.402, while the framework provides consistent gains, especially for weaker open-source models, demonstrating its robustness and effectiveness.

## Method Summary
The paper introduces the |/sync-altὨC| benchmark consisting of 1,333 English Rebus Puzzles collected from three web sources, with 611 augmented using ControlNet to add distracting backgrounds. The RebusDescProgICE framework uses a hybrid approach combining unstructured image descriptions with structured code-based reasoning (VisProg-style). A novel in-context example selection method retrieves demonstrations based on similarity of code-based reasoning embeddings. The framework is evaluated across 8 closed-source and 3 open-source models using word-level F1 and substring accuracy metrics, with experiments testing 1-3 shot settings and comparing against baselines including zero-shot and Chain-of-Thought prompting.

## Key Results
- GPT-4o achieves word-level F1 of 0.512 and substring accuracy of 0.422 on original test data
- RebusDescProgICE improves performance by 2.1-4.1% on closed-source models and 20-30% on open-source models compared to Chain-of-Thought reasoning
- On ControlNet-augmented test data, GPT-4o F1 drops to 0.402 while open-source models struggle with F1 < 0.20
- Optimal performance achieved with 2-3 in-context examples, with diminishing returns beyond this point

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Structured-Unstructured Reasoning Synergy
- Claim: Combining unstructured image descriptions with structured code-based reasoning improves VLM puzzle-solving performance compared to either approach alone.
- Mechanism: The unstructured description component grounds the model in explicit visual facts (what objects/text appear), while the code-based VisProg component provides procedural logic for letter manipulation (adding/subtracting/rearranging). Together, they decompose the multi-step abstraction required for rebus solving into tractable sub-operations.
- Core assumption: VLMs struggle to simultaneously maintain factual accuracy and execute multi-step symbolic manipulation in a single reasoning chain; separating concerns across modalities reduces cognitive load.
- Evidence anchors:
  - [abstract] "RebusDescProgICE, a model-agnostic framework which uses a combination of an unstructured description and code-based, structured reasoning"
  - [section 4.4] "Comparing isolated prompting methods highlights why both components are essential... RebusDescProgICE consistently balances both to achieve competitive performance"
  - [corpus] Related work on visual programming (VisProg [14], PAL, PoT) shows structured reasoning gains in complex tasks, though no direct corpus papers test hybrid description-code approaches for puzzles.
- Break condition: If puzzles require semantic leaps that cannot be decomposed into letter-level operations (e.g., metaphor interpretation), the code-based component may provide no advantage over description alone.

### Mechanism 2: Reasoning-Aligned In-Context Example Selection
- Claim: Selecting in-context examples based on similarity of code-based reasoning embeddings outperforms random selection, particularly for weaker open-source models.
- Mechanism: Instead of random sampling, the framework retrieves examples whose structured reasoning patterns (the code component) are semantically similar to the test input. This aligns demonstrations with the anticipated reasoning trajectory rather than just surface-level visual similarity.
- Core assumption: The code-based reasoning representation captures task-relevant structure better than image or text embeddings alone.
- Evidence anchors:
  - [section 3.3] "we use a novel in-context example selection method based on the similarity between the code-based reasoning components (similar to Poesia et al. [26])"
  - [section 4.4] "gains plateau beyond two or three examples, indicating diminishing returns" — suggesting selection quality matters more than quantity
  - [corpus] Poesia et al. [26] (Synchromesh) demonstrates code embedding similarity for reliable code generation; this paper adapts it to multimodal reasoning.
- Break condition: If the test puzzle requires a reasoning pattern not represented in the retrieval pool, similarity-based selection may retrieve superficially similar but procedurally irrelevant examples.

### Mechanism 3: Controlled Difficulty Augmentation via ControlNet
- Claim: Adding realistic distracting backgrounds via ControlNet (treating puzzles as Canny edge maps) increases benchmark difficulty while preserving solvability.
- Mechanism: ControlNet generates ambient backgrounds conditioned on edge structure, introducing visual noise that mimics real-world conditions without altering the puzzle's core content. This tests robustness of visual parsing under distraction.
- Core assumption: The core puzzle elements remain parseable after augmentation; human annotators verified answers match originals.
- Evidence anchors:
  - [section 3.1.3] "use ControlNet [41] on all the Rebus Puzzles... treating the puzzles as Canny Edge Maps"
  - [section 4.4, augmented results] "overall low scores across models arise from the complexity of our dataset... further amplified by the ControlNet-augmented noisy backgrounds"
  - [corpus] ControlNet [41] is established for conditional image generation; corpus lacks papers testing its use for benchmark augmentation specifically.
- Break condition: If augmentation introduces artifacts that change puzzle interpretation (e.g., new edge-like structures mistaken for puzzle elements), the augmented version may test robustness to corruption rather than reasoning ability.

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: RebusDescProgICE is explicitly an in-context framework requiring understanding of how to structure demonstrations, select examples, and format prompts without weight updates.
  - Quick check question: Can you explain why increasing from 2 to 3 in-context examples shows diminishing returns in Table 2?

- Concept: **Visual Programming (VisProg)**
  - Why needed here: The structured reasoning component uses code-based decomposition inspired by VisProg; understanding how LLMs can generate executable visual reasoning programs is prerequisite.
  - Quick check question: How does separating visual perception (description) from symbolic manipulation (code) differ from end-to-end VLM inference?

- Concept: **Rebus Puzzle Mechanics**
  - Why needed here: The task domain requires understanding how rebuses encode language through position, orientation, size, color, and letter operations (e.g., "Mill" + "Lime" + "Ters" → "Millimeters").
  - Quick check question: Why does the paper note that using "Turbine" instead of "Mill" in Figure 1 would prevent solving the puzzle?

## Architecture Onboarding

- Component map:
  - Input: Rebus puzzle image
  - Example retrieval: k most similar examples via code-embedding similarity
  - Prompt formatting: examples with description + code for each
  - VLM generation: description + code for test image
  - Output parsing: execute/parse code to produce final answer
  - Evaluation: compare against ground truth

- Critical path:
  1. Input: Rebus puzzle image
  2. Retrieve k most similar examples via code-embedding similarity
  3. Format prompt with examples (description + code for each)
  4. VLM generates description + code for test image
  5. Execute/parse code to produce final answer
  6. Compare against ground truth

- Design tradeoffs:
  - **Description-only vs. VisProg-only vs. Hybrid**: Tables 1-2 show hybrid balances substring accuracy (VisProg better) and F1 (description better); no single method dominates both metrics
  - **Example count**: 2-3 examples optimal; more adds latency without gains
  - **Closed vs. open-source models**: Gains from RebusDescProgICE are 5-10x larger for open-source (20-30% relative) vs. closed-source (2.1-4.1%), suggesting weaker models benefit more from structured guidance

- Failure signatures:
  - **Open-source collapse**: Phi-3.5 and Pixtral show F1 < 0.20 on augmented data with any prompting method, indicating fundamental visual parsing or reasoning failures
  - **Metric divergence**: High substring accuracy but low F1 indicates partial answer generation (e.g., "Millimeter" vs. "Millimeters")
  - **Augmentation sensitivity**: Performance drops 10-20% on ControlNet-augmented test data, suggesting over-reliance on clean visual inputs

- First 3 experiments:
  1. **Baseline sanity check**: Run zero-shot normal prompting on 50 original puzzles; verify GPT-4o achieves ~0.49 F1 (Table 2, row 1) and Qwen2-VL-7B achieves ~0.18 F1. If scores differ >15%, check prompt formatting or model version.
  2. **Ablation on description vs. code**: For same 50 puzzles, compare description-only vs. VisProg-only vs. RebusDescProgICE (3-shot). Expect hybrid to fall between description (higher F1) and VisProg (higher substring acc) per section 4.4.
  3. **Augmentation robustness test**: Run best configuration on augmented test split; verify GPT-4o F1 drops to ~0.40 (Table 4) and open-source models struggle. Large unexpected gains may indicate data leakage or augmentation failure.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but the discussion section implies several areas for future work including cross-lingual extension, semantic evaluation beyond lexical metrics, and comparison with fine-tuning approaches.

## Limitations
- **Generalizability uncertainty**: Framework's effectiveness is primarily demonstrated on rebus puzzles, which have specific structural properties; unclear if hybrid approach transfers to other multimodal reasoning tasks.
- **Template specification gap**: Paper lacks explicit specification of code-based reasoning template format, limiting reproducibility despite claiming structured code reasoning improves performance.
- **Augmentation validity concerns**: ControlNet augmentation increases difficulty based on performance drops rather than controlled difficulty measurement; unclear if drops stem from visual parsing or changed puzzle semantics.

## Confidence

- **Hybrid Reasoning Mechanism**: High Confidence - well-supported by direct comparisons in Tables 1-2 showing RebusDescProgICE consistently outperforming either approach alone across multiple models and metrics.
- **In-Context Example Selection**: Medium Confidence - supported by adaptation of Poesia et al. [26], but lacks ablation showing random selection performs worse; paper notes "gains plateau beyond two or three examples" without proving selection quality matters.
- **ControlNet Augmentation Difficulty**: Medium Confidence - supported by performance drops (Table 4), but mechanism is correlational rather than controlled; paper doesn't quantify how much augmentation changes visual difficulty.

## Next Checks

1. **Template Replication Test**: Implement RebusDescProgICE using only information provided in paper (without author communication). Compare performance on 50 held-out puzzles against reported baseline to identify missing specification details.

2. **Ablation of Example Selection**: Run RebusDescProgICE with both similarity-based and random example selection on same test set. Measure performance difference to validate whether novel selection method provides measurable gains over simpler alternatives.

3. **Cross-Domain Transfer**: Apply RebusDescProgICE to different multimodal reasoning task (e.g., visual math word problems or visual analogies). Measure whether hybrid reasoning approach provides similar relative improvements or if gains are specific to rebus puzzle structure.