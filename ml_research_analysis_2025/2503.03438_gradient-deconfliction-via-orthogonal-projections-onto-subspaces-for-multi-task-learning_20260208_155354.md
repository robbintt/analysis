---
ver: rpa2
title: Gradient Deconfliction via Orthogonal Projections onto Subspaces For Multi-task
  Learning
arxiv_id: '2503.03438'
source_url: https://arxiv.org/abs/2503.03438
tags:
- gradops
- tasks
- gradients
- gradient
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to solve gradient conflicts in multi-task
  learning by projecting task gradients onto orthogonal subspaces. The key idea is
  to deconflict gradients by projecting each task-specific gradient onto the subspace
  orthogonal to the span of other task gradients, ensuring strong non-conflicting
  gradients.
---

# Gradient Deconfliction via Orthogonal Projections onto Subspaces For Multi-task Learning

## Quick Facts
- arXiv ID: 2503.03438
- Source URL: https://arxiv.org/abs/2503.03438
- Reference count: 40
- Key outcome: This paper proposes a method to solve gradient conflicts in multi-task learning by projecting task gradients onto orthogonal subspaces. The key idea is to deconflict gradients by projecting each task-specific gradient onto the subspace orthogonal to the span of other task gradients, ensuring strong non-conflicting gradients. This allows simple yet effective trade-offs among tasks through non-negative linear combinations of deconflicted gradients, controlled by a single hyperparameter. Theoretical analysis shows convergence to Pareto stationary points, and experiments on multiple benchmarks demonstrate state-of-the-art performance with flexible trade-off strategies. The method outperforms existing approaches in both classification and recommendation tasks.

## Executive Summary
This paper addresses the fundamental challenge of gradient conflicts in multi-task learning (MTL), where optimizing one task's loss can harm another's performance. The authors propose GradOPS, which deconflicts gradients by projecting each task-specific gradient onto the subspace orthogonal to the span of other task gradients using Gram-Schmidt orthogonalization. This ensures strong non-conflicting gradients that don't depend on task processing order. The method introduces a single hyperparameter α that controls trade-offs among tasks through simple scaling of deconflicted gradients, enabling convergence to different points on the Pareto front without complex optimization solvers.

## Method Summary
GradOPS tackles gradient conflicts in MTL by projecting each task's gradient onto the orthogonal complement of the subspace spanned by other tasks' gradients. For each task $T_i$, the algorithm constructs an orthogonal basis for the span of all other task gradients using Gram-Schmidt orthogonalization, then projects $g_i$ onto the orthogonal complement of this span to obtain $g'_i$. A single hyperparameter $\alpha$ controls trade-offs by scaling these deconflicted gradients based on their dominance, measured by scalar projections. The final update combines weighted deconflicted gradients: $G'_{new} = \sum w_i g'_i$ where $w_i \geq 0$. Theoretical analysis proves convergence to Pareto stationary points under standard Lipschitz smoothness assumptions, and experiments on UCI Census-income, NYUv2, and Taobao datasets demonstrate state-of-the-art performance.

## Key Results
- GradOPS outperforms existing methods (MGDA, PCGrad) on UCI Census-income classification tasks with 3.1% average performance improvement
- Single hyperparameter $\alpha$ enables flexible trade-offs across the Pareto front, with negative $\alpha$ balancing tasks and positive $\alpha$ prioritizing dominant tasks
- Theoretical guarantee of convergence to Pareto stationary points under standard assumptions, with no dependency on task processing order unlike PCGrad
- State-of-the-art results on NYUv2 segmentation/depth/surface normal prediction and Taobao recommendation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Projecting task gradients onto the orthogonal complement of the span of other task gradients guarantees "strong non-conflicting" updates, removing the dependency on task processing order found in prior methods (e.g., PCGrad).
- **Mechanism:** For a task $T_i$, the algorithm constructs an orthogonal basis $\{u_j\}$ for the subspace spanned by all other task gradients $\{g_j\}_{j \neq i}$ using the Gram-Schmidt process. It then projects $g_i$ onto the orthogonal complement of this span. This removes any component of $g_i$ that conflicts with the combined direction of other tasks, ensuring $g'_i \cdot g_j \geq 0$ for all $j$.
- **Core assumption:** The conflict information is fully captured by the pairwise (and subspace) dot products of the immediate mini-batch gradients; removing the conflicting component does not destroy critical information required for convergence.
- **Evidence anchors:**
  - [abstract]: "deconflict gradients by projecting each task-specific gradient onto the subspace orthogonal to the span of other task gradients."
  - [section 3.1]: "GradOPS deconflicts gradients by projecting each $g_i$ onto the subspace orthogonal to $S = \text{span}\{g_j\}_{j \neq i}$... Eventually, we have $g'_i \cdot g_j \geq 0, \forall i, j$."
  - [corpus]: Related work "Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection" supports the efficacy of orthogonal projection for conflict resolution in MTL.
- **Break condition:** If the gradients are linearly dependent in a specific way such that $g'_i = 0$ for all $i$ (which the paper notes is rare), the mechanism stalls and must fall back to a standard Multi-Objective Optimization (MOO) solver like MGDA.

### Mechanism 2
- **Claim:** A single hyperparameter $\alpha$ enables diverse trade-offs by scaling the deconflicted gradients based on their dominance, effectively spanning the Pareto front without complex optimization solvers.
- **Mechanism:** The method calculates a scaling factor $w_i$ based on the scalar projection of the aggregated update $G'$ onto the original gradient $g_i$. It then applies a power scaling $r_i^\alpha$. $\alpha > 0$ prioritizes dominant tasks, while $\alpha < 0$ balances performance by boosting dominated tasks.
- **Core assumption:** The "dominance" of a task can be effectively measured by the scalar projection of the aggregated deconflicted gradient onto the original task gradient ($R_i$), and simple linear reweighting suffices to steer the optimization trajectory.
- **Evidence anchors:**
  - [abstract]: "simple yet effective trade-offs among tasks through non-negative linear combinations... controlled by a single hyperparameter."
  - [section 3.2]: "With $\alpha > 0$, performance of the dominating tasks are expected to be improved... While $\alpha < 0$, the dominated tasks are emphasized."
  - [figure 2]: Visualizes how different $\alpha$ values allow GradOPS to converge to different points on the Pareto set, unlike fixed methods like MGDA-UB.
- **Break condition:** Selecting an $\alpha$ that is too extreme (e.g., very large positive) may cause the model to overfit dominant tasks and ignore others, effectively reducing to single-task learning.

### Mechanism 3
- **Claim:** The proposed update rule ensures convergence to a Pareto stationary point under standard Lipschitz smoothness assumptions.
- **Mechanism:** By ensuring the update direction is a non-negative linear combination of the original gradients (specifically, $G'_{new} = \sum w_i g'_i$ where $w_i \geq 0$ and $g'_i$ is non-conflicting), the update satisfies the Pareto stationarity condition. Theoretical analysis bounds the step size relative to the Lipschitz constant $L$.
- **Core assumption:** Individual loss functions are differentiable and L-Lipschitz smooth.
- **Evidence anchors:**
  - [section 3.1]: Theorem 1 states "GradOPS... will converge to a Pareto stationary point" with step size $t < \frac{2}{TL}$.
  - [appendix A.1]: Proof details the descent lemma application using the property that $g_i \cdot g'_j \geq 0$.
  - [corpus]: Corpus papers on orthogonal gradient descent generally support convergence properties in subspace projection methods.
- **Break condition:** Violation of the step size constraint $t < \frac{2 w_i \|g'_i\|^2}{TL w_j^2 \|g'_j\|^2}$ (Theorem 2) may prevent convergence.

## Foundational Learning

- **Concept: Multi-Task Gradient Conflicts ($g_i \cdot g_j < 0$)**
  - **Why needed here:** This is the core problem GradOPS solves. Understanding the difference between "conflicting gradients" (negative dot product) and "non-conflicting" is essential to grasp the value proposition.
  - **Quick check question:** If two gradients are perpendicular ($g_1 \cdot g_2 = 0$), are they considered "conflicting" in this framework? (Answer: No, conflict strictly requires $< 0$, but GradOPS ensures $g'_i \cdot g_j \geq 0$).

- **Concept: Orthogonal Projection & Gram-Schmidt Process**
  - **Why needed here:** The paper uses Gram-Schmidt to construct the basis for the "conflicting subspace" ($S$) and projects gradients away from it. You must understand vector projection to implement the core algorithm.
  - **Quick check question:** If $g_1$ and $g_2$ span a plane, how do you compute the component of $g_3$ that is orthogonal to that plane?

- **Concept: Pareto Stationarity vs. Optimality**
  - **Why needed here:** The paper claims convergence to "Pareto stationary" points, distinguishing this from global "Pareto optimal" (which requires convexity). This distinction manages expectations about the solution quality.
  - **Quick check question:** Why is reaching a Pareto stationary point the practical guarantee for non-convex deep learning problems rather than global optimality?

## Architecture Onboarding

- **Component map:**
  - Input: Task-specific gradients $\{g_1, \dots, g_T\}$.
  - Conflict Resolver: Iterates over tasks; for each $g_i$, computes basis of span$\{g_j\}_{j \neq i}$ (Eq. 1) and projects $g_i$ to get $g'_i$ (Eq. 2).
  - Trade-off Controller: Computes dominance $R_i$ (Eq. 3), scales via $\alpha$ to get weights $w_i$ (Eq. 5).
  - Aggregator: Sums weighted deconflicted gradients $G'_{new} = \sum w_i g'_i$.

- **Critical path:**
  The Gram-Schmidt orthogonalization (Step 5-6 in Algorithm 1). This is the computational bottleneck. Implementation efficiency here is crucial for scaling to high-dimensional models.

- **Design tradeoffs:**
  - **Exact vs. Approximate Projection:** The paper uses exact Gram-Schmidt. For very large $T$ (number of tasks), this might become expensive ($O(T^2 d)$ where $d$ is dimension).
  - **Choice of $\alpha$:** Requires tuning. The paper suggests negative $\alpha$ often balances tasks better (mimicking MGDA-UB), while positive boosts dominant tasks. Grid search or validation-set optimization of $\alpha$ is necessary.

- **Failure signatures:**
  - **Zero Gradients:** If $g'_i = 0$ for all tasks, the algorithm falls back to MGDA. Monitor if this happens frequently.
  - **PCGrad-like Instability:** If implementation incorrectly projects onto *gradients* rather than the *span*, it reverts to PCGrad behavior which is order-dependent (Figure 1b/c).
  - **Norm Collapse:** If $\alpha$ is too negative, gradients of dominated tasks might explode relative to others, destabilizing training.

- **First 3 experiments:**
  1.  **Toy 2D Visualization:** Replicate Figure 2 using the provided loss functions in Appendix B. Visualize the trajectory for $\alpha \in \{-2, 0, 2\}$ to confirm the update direction changes as expected.
  2.  **UCI Census (Classification):** Run the UCI experiment (Section 4.1). Compare `Uniform scaling` vs. `GradOPS(α=-3)` (which performed best in the paper). Check the "Average $\Delta_m\%$" metric to verify improvement.
  3.  **Ablation on Processing Order:** Implement a variant where gradients are processed in random order vs. sorted order. Verify that the final $G'$ remains invariant (unlike PCGrad), confirming the "Strong Non-Conflicting" property.

## Open Questions the Paper Calls Out

- **Question:** Can the trade-off hyperparameter α be automatically determined or adapted during training rather than requiring manual tuning for each dataset?
  - **Basis in paper:** [explicit] The authors state: "studying more delicate and efficient trade-off strategies shall be an interesting and promising future research direction in MTL." They also show that different α values produce different trade-offs across datasets.
  - **Why unresolved:** The paper demonstrates α sensitivity across datasets but provides no principled method for α selection, relying instead on empirical tuning.
  - **What evidence would resolve it:** An adaptive mechanism that dynamically adjusts α based on training dynamics or task performance signals, validated across multiple benchmarks without manual tuning.

- **Question:** Does the requirement for strong non-conflicting gradients potentially eliminate beneficial gradient interactions that facilitate positive transfer between tasks?
  - **Basis in paper:** [inferred] The paper emphasizes eliminating all gradient conflicts, but doesn't investigate whether some controlled conflicts might enable better knowledge transfer or whether orthogonal projection removes task-shared information.
  - **Why unresolved:** While the paper shows improved results, it doesn't analyze whether the orthogonal projection eliminates shared gradient components that could benefit multiple tasks simultaneously.
  - **What evidence would resolve it:** Ablation studies comparing GradOPS with controlled partial deconfliction, or analysis of the gradient components removed during projection and their relationship to task performance.

- **Question:** How does GradOPS scale computationally and in terms of convergence behavior when the number of tasks increases significantly (e.g., beyond 10-20 tasks)?
  - **Basis in paper:** [inferred] The experiments focus on 3-task scenarios, and the Gram-Schmidt orthogonalization procedure in Eq. 1 has O(T²) complexity for T tasks. The convergence proof assumes Lipschitz smoothness with step size requirements that may become restrictive with more tasks.
  - **Why unresolved:** No empirical or theoretical analysis of performance degradation or computational overhead as task count grows substantially.
  - **What evidence would resolve it:** Experiments on multi-task benchmarks with 10+ tasks, measuring both computational time and performance relative to baseline methods.

## Limitations

- The exact Gram-Schmidt orthogonalization has O(T²d) complexity, which may become computationally prohibitive for scenarios with many tasks or very high-dimensional models
- The assumption that linear gradient conflicts fully capture task incompatibility may break down in complex models where conflicts manifest through higher-order interactions
- The method requires manual tuning of the trade-off hyperparameter α for each dataset, with no principled method for automatic selection

## Confidence

- **High Confidence**: The mechanism for deconflicting gradients through orthogonal projection (Mechanism 1) is mathematically sound and well-supported by both theoretical analysis and experimental evidence. The convergence proof to Pareto stationary points is rigorous under stated assumptions.
- **Medium Confidence**: The trade-off strategy using the single hyperparameter α (Mechanism 2) shows strong empirical results but lacks theoretical guarantees about its effectiveness across diverse problem landscapes. The assumption that dominance measured by scalar projections adequately captures task importance may not hold universally.
- **Medium Confidence**: The overall claim of state-of-the-art performance across benchmarks is supported by experiments but could benefit from comparisons against more recent MTL methods and on additional datasets.

## Next Checks

1. **Scalability Test**: Implement GradOPS with 5-10 synthetic tasks on a moderate-sized neural network (e.g., CIFAR-10 classifier). Measure training time and memory overhead compared to standard MTL approaches, and verify that exact Gram-Schmidt remains computationally feasible.

2. **Conflict Detection Analysis**: Log the frequency and magnitude of gradient conflicts (negative dot products) throughout training. Compare how often conflicts are detected and resolved versus the fallback to MGDA solver, and assess whether this ratio varies significantly across different datasets and model architectures.

3. **Ablation on Projection Method**: Replace the exact Gram-Schmidt orthogonalization with an approximate method (e.g., randomized SVD or iterative projection). Measure the impact on both computational efficiency and final task performance to quantify the trade-off between projection accuracy and practical usability.