---
ver: rpa2
title: 'Digital Gatekeepers: Exploring Large Language Model''s Role in Immigration
  Decisions'
arxiv_id: '2506.21574'
source_url: https://arxiv.org/abs/2506.21574
tags:
- llms
- immigration
- decision-making
- choice
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored whether LLMs like GPT-3.5 and GPT-4 can make
  fair and effective immigration decisions. Using discrete choice experiments with
  10,000 cases, it found that both models aligned closely with human decision-making
  strategies, prioritizing utility maximization and procedural fairness.
---

# Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions

## Quick Facts
- arXiv ID: 2506.21574
- Source URL: https://arxiv.org/abs/2506.21574
- Authors: Yicheng Mao; Yang Zhao
- Reference count: 15
- LLMs align with human immigration decision-making strategies but retain embedded biases

## Executive Summary
This study investigates whether large language models (GPT-3.5 and GPT-4) can make fair and effective immigration decisions. Using discrete choice experiments with 10,000 cases, the research finds that LLMs closely mirror human decision-making patterns, prioritizing utility maximization and procedural fairness. While models consistently prefer candidates with higher education, professional skills, and legal employment plans, they also exhibit subtle biases toward privileged groups and nationalities, often reflecting societal stereotypes despite existing safeguards.

## Method Summary
The study employed discrete choice experiments (DCE) where LLMs acted as U.S. immigration officers selecting between pairs of applicant profiles. Ten thousand choice sets were randomly generated, each containing two profiles with 9 attributes (gender, education, language, country of origin, profession, job experience, employment plans, reason for entry, and prior trips). Models responded via zero-shot inference using the OpenAI API with strict persona instructions ("Imagine you are an officer... Respond with one word only"). Multinomial logit modeling analyzed attribute importance and compared LLM decisions to human benchmark data.

## Key Results
- LLMs align with human utility maximization strategies, favoring high-skilled applicants with economic potential
- GPT-4 demonstrates stricter procedural fairness than GPT-3.5, particularly regarding legal compliance and unauthorized entry history
- Despite safeguards, LLMs exhibit implicit biases favoring privileged groups and nationalities, correlating protected attributes with decision-relevant qualities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs align their immigration decision-making with human utility maximization strategies, prioritizing applicants who offer greater economic contribution potential.
- Mechanism: The models assign differential weights to profile attributes—education, profession, employment plans—producing selection patterns consistent with rational choice theory where decisions maximize expected utility. This emerges from training on human-generated text that encodes societal preferences for high-skilled immigration.
- Core assumption: LLMs' training corpora contain sufficient examples of deliberative decision-making to generalize utility-based reasoning to novel policy contexts.
- Evidence anchors:
  - [abstract] "LLMs can align their decision-making with human strategies, emphasizing utility maximization and procedural fairness"
  - [section 3.1] "the decision-making strategies of both GPT-3.5 and GPT-4 reflect a 'maximize utility' approach, which align with the human behavior"
  - [corpus] Weak direct support; "PapersPlease" benchmark (arXiv:2506.21961) evaluates LLM moral values via role-playing but doesn't confirm utility mechanisms
- Break condition: If decision contexts require balancing competing values (e.g., humanitarian vs. economic) where training data lacks coherent exemplars, alignment degrades.

### Mechanism 2
- Claim: LLMs enforce procedural fairness through heightened sensitivity to legal compliance markers, often exceeding human evaluator strictness.
- Mechanism: Safety training and instruction-following fine-tuning amplify adherence to explicit rules and formal processes. The model interprets procedural violations (e.g., unauthorized entry) as signals of future non-compliance risk, triggering stronger rejection responses than attribute-based deficits alone.
- Core assumption: Procedural adherence in training examples correlates with "correct" responses in reinforcement learning, making rule-following a default bias.
- Evidence anchors:
  - [section 3.1] "GPT-4 almost categorically rejects applicants with a history of unauthorized entry" while humans show 28.7% reduced selection probability
  - [section 3.1] LLMs prefer interpreter use as it "demonstrates the applicant's respect for the admission process"
  - [corpus] "Guiding LLM Decision-Making with Fairness Reward Models" (arXiv:2507.11344) suggests chain-of-thought can amplify biases, implying procedural emphasis may intensify with certain prompting
- Break condition: When procedural rigidity conflicts with contextual judgment (e.g., asylum cases where legal violation stems from persecution), models may produce systematically inappropriate decisions.

### Mechanism 3
- Claim: Implicit biases toward privileged groups persist despite safeguards, operating through correlated attribute inference rather than direct discrimination.
- Mechanism: Models learn statistical associations between protected attributes (nationality, socioeconomic status) and decision-relevant qualities (education quality, economic stability). These correlations enable proxy discrimination even when models explicitly disclaim using protected characteristics.
- Core assumption: Training data encodes historical inequities as legitimate signal rather than bias, making de-correlation require active intervention beyond refusal behaviors.
- Evidence anchors:
  - [abstract] "while ChatGPT has safeguards to prevent unintentional discrimination, it still exhibits stereotypes and biases concerning nationality and shows preferences toward privileged group"
  - [section 3.2] GPT models favor Germany/France due to "stable economies and high educational standards" while rating Poland lower despite EU membership
  - [section 3.2] GPT-4 shows 86.8% increased probability of selecting doctors over janitors vs. humans' 34.9%
  - [corpus] "Bias runs deep: Implicit reasoning biases in persona-assigned LLMs" (cited in paper) confirms training data influence on implicit bias
- Break condition: If proxy attributes are removed or decorrelated in training/inference, or if explicit counter-bias instructions are integrated into prompting, bias magnitude may reduce—but elimination is unproven.

## Foundational Learning

- Concept: Discrete Choice Experiments (DCE)
  - Why needed here: Understanding how the paper isolates attribute contributions to decisions requires grasping how forced-choice pairwise comparisons reveal preference weights independent of confounds.
  - Quick check question: If two profiles share identical education and profession but differ on country of origin, what does selection preference reveal?

- Concept: Multinomial Logit (MNL) Modeling
  - Why needed here: The paper's quantitative claims rest on MNL coefficient interpretation—understanding how β values represent relative attribute importance and how marginal effects translate to selection probability changes.
  - Quick check question: A coefficient of 2.68 for "doctor" vs. "janitor" (GPT-4) translates to what approximate probability difference when other attributes equal?

- Concept: Proxy Discrimination in ML Systems
  - Why needed here: The paper documents bias without direct discrimination—grasping how correlated features enable indirect bias is essential for designing mitigation strategies.
  - Quick check question: If nationality is excluded from features but education-institution-location correlates with nationality, can bias persist?

## Architecture Onboarding

- Component map:
  Profile Generator -> Prompt Constructor -> LLM Interface -> MNL Estimator -> Interview Module

- Critical path: Profile generation → Prompt formatting → API call → Response validation → Coefficient estimation → Marginal effect calculation → Human-LLM comparison

- Design tradeoffs:
  - 10,000 random choice sets ensures statistical power but includes dominated/overlapping pairs with low information value (paper notes this inefficiency)
  - Textual prompts代替 tabular format accommodates LLM input constraints but introduces framing variability risk
  - Single-token response requirement ("Case 1" or "Case 2") ensures clean classification but forfeits uncertainty signaling

- Failure signatures:
  - Non-compliant responses (e.g., explanations instead of single token)—paper reports 100% effective response rate
  - Attribute coefficient non-significance—some countries/jobs showed p > 0.05, indicating weak or inconsistent preference
  - Human-LLM strategy divergence—paper finds alignment, but divergence would signal domain mismatch

- First 3 experiments:
  1. Replicate with held-out attribute combinations to test generalization beyond training distribution
  2. Introduce adversarial profiles (e.g., high-education applicants from disfavored nationalities) to quantify bias-signal tradeoffs
  3. Test prompt variations with explicit fairness instructions to measure bias reduction against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do interaction effects between immigration applicant attributes (e.g., education level × reason for application, or prior entry history × asylum claims) influence LLM decision-making patterns?
- Basis in paper: [explicit] The authors state: "Future research could build on our study by incorporating the investigation of interaction effects, which may provide a meaningful addition." They note GPT models showed interest in interactions during interviews, such as weighting education differently depending on career paths.
- Why unresolved: The study focused on main effects only to ensure comparability with prior human DCE research. Testing interactions with 9 attributes and multiple levels risks overfitting and requires more sophisticated experimental designs.
- What evidence would resolve it: A DCE study with interaction terms in the multinomial logit model, using sufficiently large sample sizes to estimate cross-attribute effects without overfitting.

### Open Question 2
- Question: Can Bayesian D-efficient experimental designs for discrete choice experiments yield comparable parameter estimates with significantly fewer choice sets than random generation?
- Basis in paper: [explicit] The authors propose: "constructing a Bayesian D-efficient design for these DCEs could be a viable solution. This method maximizes the determinant of the information matrix of the model under study, allowing for more precise parameter estimates with fewer experimental rounds."
- Why unresolved: Current LLM-DCE studies use randomly generated choice sets (10,000 in this study), which include dominated alternatives and excessive attribute overlap that provide limited statistical information.
- What evidence would resolve it: Comparative experiments showing that Bayesian optimal designs achieve equivalent statistical power and parameter precision with substantially fewer choice sets.

### Open Question 3
- Question: What interventions (prompt engineering, fine-tuning, retrieval-augmented generation) can effectively reduce or eliminate implicit nationality and socioeconomic biases in LLM immigration decision-making?
- Basis in paper: [inferred] The authors conclude that LLMs "require further refinement to fully eliminate embedded biases" and acknowledge that biases stem from training data, but do not test any mitigation strategies beyond noting existing safeguards are insufficient.
- Why unresolved: The study documents bias persistence but does not experimentally test debiasing methods; the root cause (training data) suggests the problem requires technical intervention.
- What evidence would resolve it: Experiments comparing baseline LLM decisions against decisions from models subjected to various debiasing interventions, measuring changes in nationality and profession-based disparities.

### Open Question 4
- Question: Do the observed immigration decision-making patterns and biases generalize across different LLM architectures and model families beyond GPT-3.5 and GPT-4?
- Basis in paper: [inferred] The study examines only OpenAI models, leaving unexplored whether the utility-maximization strategy, procedural fairness emphasis, and implicit biases are specific to GPT models or reflect broader LLM characteristics shaped by common training paradigms.
- Why unresolved: Different model families may have different training data, safety fine-tuning, and architectural inductive biases that could produce distinct decision patterns.
- What evidence would resolve it: Replication of the DCE methodology with models from other families (e.g., Claude, LLaMA, Gemini) and comparison of resulting decision strategies and bias patterns.

## Limitations
- Study uses controlled DCE conditions that may not capture real-world adjudication complexity
- Single-token response constraint eliminates uncertainty signaling and may force arbitrary choices
- Random generation of choice pairs includes dominated/overlapping pairs with low information value
- LLMs reproduce societal stereotypes through correlated attribute inference despite safeguards

## Confidence

**High confidence**: LLM alignment with human utility maximization strategies and procedural fairness emphasis; statistical significance of education, profession, and legal compliance effects.

**Medium confidence**: Magnitude of implicit bias effects and their practical significance in real adjudication contexts; generalizability beyond the specific attribute set and framing used.

**Low confidence**: Long-term stability of observed patterns across model versions; effectiveness of current prompting approaches for bias mitigation.

## Next Checks

1. **Distributional robustness test**: Replicate with profiles sampled from immigration officer caseload distributions rather than uniform random generation to assess real-world applicability.

2. **Bias-decorrelation experiment**: Systematically remove or decorrelate proxy attributes (e.g., country-specific education indicators) to quantify residual bias magnitude and test mitigation effectiveness.

3. **Uncertainty measurement validation**: Modify prompting to capture confidence levels or allow non-committal responses, then analyze whether uncertainty correlates with profile similarity or attributes associated with bias.