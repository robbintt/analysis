---
ver: rpa2
title: Self-Supervised Compression and Artifact Correction for Streaming Underwater
  Imaging Sonar
arxiv_id: '2511.13922'
source_url: https://arxiv.org/abs/2511.13922
tags:
- sonar
- compression
- image
- scope
- imaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCOPE is a self-supervised framework that jointly compresses and
  corrects artifacts in underwater imaging sonar streaming. It addresses the challenge
  of low uplink bandwidth and severe sonar-specific artifacts (speckle, motion blur,
  reverberation, acoustic shadows) affecting up to 98% of frames.
---

# Self-Supervised Compression and Artifact Correction for Streaming Underwater Imaging Sonar

## Quick Facts
- arXiv ID: 2511.13922
- Source URL: https://arxiv.org/abs/2511.13922
- Authors: Rongsheng Qian; Chi Xu; Xiaoqiang Ma; Hao Fang; Yili Jin; William I. Atlas; Jiangchuan Liu
- Reference count: 40
- One-line primary result: Achieves 0.77 SSIM at ≤0.0118 bits per pixel while reducing uplink bandwidth by >80%

## Executive Summary
SCOPE is a self-supervised framework that jointly compresses and corrects artifacts in underwater imaging sonar streaming. It addresses the challenge of low uplink bandwidth and severe sonar-specific artifacts (speckle, motion blur, reverberation, acoustic shadows) affecting up to 98% of frames. SCOPE combines Adaptive Codebook Compression (ACC), which learns frequency-encoded latent representations tailored to sonar, with Frequency-Aware Multiscale Segmentation (FAMS), which decomposes frames into low-frequency structure and sparse high-frequency dynamics while suppressing rapidly fluctuating artifacts. A hedging training strategy further guides frequency-aware learning using low-pass proxy pairs generated without labels.

Evaluated on six months of in-situ ARIS sonar data, SCOPE achieves a structural similarity index (SSIM) of 0.77, representing a 40% improvement over prior self-supervised denoising baselines, at bitrates down to ≤0.0118 bits per pixel. It reduces uplink bandwidth by more than 80% while improving downstream detection. The system runs in real time, with 3.1 ms encoding on an embedded GPU and 97 ms full multi-layer decoding on the server end. SCOPE has been deployed for months in three Pacific Northwest rivers to support real-time salmon enumeration and environmental monitoring in the wild.

## Method Summary
SCOPE uses a VQ-VAE backbone with a constrained codebook (V=64) to learn frequency-encoded representations that simultaneously compress data and suppress sonar artifacts. The Frequency-Aware Multiscale Segmentation (FAMS) decomposes each frame into low-frequency static background and sparse high-frequency dynamic objects through hierarchical decoding passes. A hedging training strategy combines reconstruction loss against noisy input with guidance from low-pass proxy pairs generated via MOG background subtraction and guided filtering. The system transmits only the high-frequency layer per frame, with the background sent weekly, achieving aggressive compression while maintaining detection quality for downstream salmon enumeration tasks.

## Key Results
- Achieves SSIM of 0.77, representing 40% improvement over self-supervised denoising baselines
- Operates at ≤0.0118 bits per pixel, reducing uplink bandwidth by >80%
- Runs in real time: 3.1 ms encoding on Jetson ORIN Nano edge, 97 ms multi-layer decoding on server
- Improves downstream detection: AP50 from 0.49 to 0.73, AP50-95 from 0.18 to 0.25

## Why This Works (Mechanism)

### Mechanism 1: Constrained Codebook Forces Frequency Learning
A small learnable codebook (V=64) compels the model to encode frequency patterns rather than memorizing pixel values, simultaneously achieving compression and artifact suppression. Standard VQ-VAEs with large codebooks can map features directly, but constraining the codebook forces entries $b_k$ to capture characteristic frequency components. Artifacts with rapid fluctuations cannot be efficiently represented and are suppressed during quantization. This works because sonar artifacts exhibit higher temporal frequency variance than structural signals and objects of interest.

### Mechanism 2: Multi-Scale Decomposition Separates Structure from Dynamics
Image scaling operations shift spatial frequencies, enabling learned segmentation of low-frequency static background ($S$) from high-frequency dynamic objects ($H_i$). Each frame is modeled as $I_i = S + H_i + \text{artifacts}$. The decoder produces $k$ hierarchical layers: early layers capture low-frequency structure, final layers preserve sparse high-frequency dynamics (fish, targets). Only $H_i$ transmits per-frame; $S$ sends weekly. This works because temporal frequency characteristics differ between static background (low variance) and moving targets (high variance).

### Mechanism 3: Hedging Loss Balances Reconstruction and Frequency Segmentation
A composite loss combining reconstruction against noisy input and low-pass proxy pairs enables self-supervised training without clean labels. $L = \lambda_1 L_1(\hat{I}, I_{noisy}) + \lambda_2 L_2(\hat{I}, I_{hedging})$ with $\lambda_1=0.6$, $\lambda_2=0.4$. The first term preserves structural fidelity; the second guides frequency-aware decomposition using MOG change detection + guided filtering to generate proxy pairs. This works because low-pass filtered versions of sonar data retain sufficient structural guidance while suppressing high-frequency noise patterns.

## Foundational Learning

- **VQ-VAE (Vector Quantized Variational Autoencoder)**: SCOPE's compression backbone; understanding latent space quantization is essential for grasping ACC. Quick check: Can you explain how a discrete codebook differs from continuous latent spaces in standard VAEs?

- **Frequency-domain signal decomposition**: FAMS relies on the principle that scaling images shifts their frequency spectra; intuition for low-pass vs. high-pass filtering is critical. Quick check: What happens to high-frequency components when an image is downsampled?

- **Self-supervised denoising assumptions**: The hedging strategy explicitly avoids clean-noise pairs; understanding why blind-spot networks fail on sonar (non-IID noise) contextualizes this design. Quick check: Why does Noise2Void assume noise is spatially uncorrelated, and how does sonar speckle violate this?

## Architecture Onboarding

- **Component map**: Noisy input $I_i$ → Encoder → Latent $\hat{I}_i$ → ACC (VQ with codebook V=64) → Index maps → FAMS (k=4 guided VQ-VAE runs) → Multi-layer decoder → Clean output $\hat{I}$

- **Critical path**: Encoding (3.1 ms on Jetson ORIN Nano edge) → Transmit index maps → Decoding (97 ms total, ~26 ms for final layer only on server) → Recombine $S$ + $H_i$

- **Design tradeoffs**:
  - Codebook size: Smaller V improves compression and artifact suppression but risks losing subtle patterns
  - Layer count: More layers improve frequency separation but increase decoding latency
  - Background update frequency: Weekly $S$ transmission saves bandwidth but may drift from environmental changes

- **Failure signatures**:
  - Small objects eroding into background (check hedging weight $\lambda_2$)
  - Over-smoothed outputs losing target edges (codebook too small or reconstruction weight too low)
  - High-frequency artifacts persisting (codebook too large, $V > 1024$)

- **First 3 experiments**:
  1. Ablate codebook size $V \in \{64, 256, 1024, 4096\}$; measure SSIM and visual artifact suppression on held-out frames.
  2. Disable hedging loss ($\lambda_2 = 0$) vs. full loss; compare detection AP50 on downstream YOLO evaluation.
  3. Profile end-to-end latency with varying layer counts; validate 4.7 Mbps uplink constraint is satisfied at target frame rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SCOPE's frequency-aware compression and correction framework generalize to other imaging domains such as MRI and remote sensing?
- Basis in paper: [explicit] "It extends beyond imaging sonar, showing potential applicability in imaging domains such as MRI and remote sensing."
- Why unresolved: The authors claim promise for these domains but provide no experimental validation beyond underwater sonar data.
- What evidence would resolve it: Evaluation of FAMS and ACC components on standard MRI datasets and remote sensing imagery, measuring SSIM and downstream task performance.

### Open Question 2
- Question: What is the optimal frequency for background transmission in dynamic underwater environments with varying target densities?
- Basis in paper: [inferred] The system transmits the static low-frequency background S weekly as a design choice, but no ablation or analysis explores alternative update intervals or adaptive scheduling.
- Why unresolved: Different environments may have different background drift rates (e.g., tidal changes, sediment movement) that could render weekly updates suboptimal.
- What evidence would resolve it: Ablation experiments comparing daily, weekly, and monthly background updates across different river systems, measuring reconstruction quality and detection accuracy.

### Open Question 3
- Question: Can the hedging training strategy be adapted for real-time proxy generation without requiring sequential input?
- Basis in paper: [explicit] "Generating low-pass proxy pairs requires sequential input and incurs high time complexity due to pixel-wise processing, making real-time edge deployment impractical."
- Why unresolved: The current hedging strategy depends on MOG-based change detection over frame sequences, which cannot run efficiently on edge devices during live streaming.
- What evidence would resolve it: Development and evaluation of single-frame or lightweight proxy generation methods that achieve comparable frequency-aware guidance without temporal buffering.

### Open Question 4
- Question: How does SCOPE perform across different sonar hardware configurations, frequencies, and deployment environments?
- Basis in paper: [inferred] All experiments use ARIS Explorer 1800/1200 systems in Pacific Northwest rivers; no validation on other sonar brands, frequency ranges, or water conditions (e.g., coastal, deep-sea) is provided.
- Why unresolved: Acoustic artifacts, speckle patterns, and signal characteristics vary with sonar frequency and water properties, potentially affecting the learned frequency representations.
- What evidence would resolve it: Cross-device experiments using different imaging sonar systems (e.g., BlueView, Sound Metrics DIDSON) in diverse aquatic environments.

## Limitations
- The assumption that low-pass proxy pairs provide adequate supervision lacks comparison against alternative proxy generation methods
- Claims about distinct frequency characteristics between artifacts and structural signals rely on visual analysis without quantitative spectral validation
- Weekly background updates may not handle dynamic environments with sediment movement or seasonal changes
- 40% improvement over self-supervised baselines lacks direct comparison against supervised or domain-specific denoising approaches

## Confidence

- **High confidence**: Real-time deployment metrics (3.1 ms encoding, 97 ms decoding), bandwidth reduction claims (80%+ savings), and downstream detection improvements are directly measurable and well-specified.
- **Medium confidence**: The 0.77 SSIM target and 40% improvement over baselines are supported by in-situ data but lack detailed statistical significance testing across multiple river deployments.
- **Low confidence**: The core mechanism claims about frequency learning in constrained codebooks and frequency-aware decomposition rely heavily on visual inspection without rigorous spectral analysis or ablation studies on alternative decomposition strategies.

## Next Checks

1. Perform spectral analysis comparing frequency power distributions of structural signals vs. artifacts in raw vs. decoded frames to empirically validate the frequency separation hypothesis.
2. Compare the hedging strategy against alternative proxy generation methods (temporal median, bilateral filtering) to quantify the impact of proxy quality on final SSIM and detection performance.
3. Conduct a deployment stress test with varying background update intervals (daily, weekly, monthly) to measure performance degradation and identify the optimal update schedule for dynamic river environments.