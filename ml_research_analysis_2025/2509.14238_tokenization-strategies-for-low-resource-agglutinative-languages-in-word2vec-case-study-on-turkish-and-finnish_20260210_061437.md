---
ver: rpa2
title: 'Tokenization Strategies for Low-Resource Agglutinative Languages in Word2Vec:
  Case Study on Turkish and Finnish'
arxiv_id: '2509.14238'
source_url: https://arxiv.org/abs/2509.14238
tags:
- tokenization
- languages
- turkish
- finnish
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates tokenization strategies for low-resource
  agglutinative languages using Turkish and Finnish Wikipedia corpora and Word2Vec
  embeddings. Four tokenization approaches were tested: word-level, character-level,
  n-grams, and Byte Pair Encoding (BPE).'
---

# Tokenization Strategies for Low-Resource Agglutinative Languages in Word2Vec: Case Study on Turkish and Finnish

## Quick Facts
- **arXiv ID:** 2509.14238
- **Source URL:** https://arxiv.org/abs/2509.14238
- **Reference count:** 22
- **Primary result:** Word-level tokenization outperforms subword methods for NER in Turkish and Finnish under low-resource conditions

## Executive Summary
This study evaluates tokenization strategies for low-resource agglutinative languages using Turkish and Finnish Wikipedia corpora and Word2Vec embeddings. Four tokenization approaches were tested: word-level, character-level, n-grams, and Byte Pair Encoding (BPE). Models were evaluated on Named Entity Recognition (NER) tasks using macro F1 and accuracy metrics. Word-level tokenization consistently outperformed all subword methods across both languages, with macro F1 scores of 0.67-0.68 compared to 0.58-0.62 for the best BPE variants. Character-level and n-gram approaches performed worst, with macro F1 scores of 0.25-0.40. BPE performance improved with larger vocabulary sizes (5,000 to 100,000 subwords), with the largest vocabulary approaching word-level performance.

## Method Summary
The study trained Word2Vec embeddings on 10,000 Wikipedia articles per language using five tokenization strategies: word-level, character-level, bigram, trigram, and BPE with vocabulary sizes ranging from 5k to 100k. The embeddings (150 dimensions) were evaluated on NER tasks using logistic regression classifiers with macro F1 and accuracy metrics. Entity tags were propagated to match tokenization boundaries, ensuring subword units inherited parent word labels. Turkish and Finnish NER datasets were split 90/10 for training and evaluation.

## Key Results
- Word-level tokenization achieved macro F1 scores of 0.67-0.68, outperforming all subword methods
- BPE performance improved monotonically with vocabulary size, reaching 0.58-0.62 at 100k vocabulary
- Character-level and n-gram tokenization produced the worst results with macro F1 scores of 0.25-0.40
- Accuracy-metric inflation occurred for subword methods due to majority-class bias toward "O" labels

## Why This Works (Mechanism)

### Mechanism 1: Semantic Coherence from Preserved Morphological Boundaries
- Claim: Word-level tokenization outperforms subword methods for NER in agglutinative languages under low-resource conditions because it preserves complete morphological units that carry entity semantics.
- Mechanism: Agglutinative words encode multiple morphemes (e.g., Turkish "evlerimizde" = "in our homes"). When tokenized as complete units, Word2Vec learns embeddings for the full semantic package rather than fragmented sub-components.
- Evidence anchors:
  - [abstract] "Word-level tokenization consistently outperformed all subword methods... macro F1 scores of 0.67-0.68 compared to 0.58-0.62 for the best BPE variants"
  - [section IV] "word-level models were more consistent across entity classes, especially those with moderate to high support, while subword models often struggled with recall on rarer tags"
  - [corpus] Kurdish embeddings study (arXiv:2511.14696) found similar patterns: word-level and morpheme-based approaches outperformed BPE on morphological similarity tasks

### Mechanism 2: BPE Vocabulary Scaling Effect
- Claim: BPE performance improves monotonically with vocabulary size because larger vocabularies approximate word-level tokenization.
- Mechanism: BPE merges frequent character pairs iteratively. With small vocabularies (5k-10k), words fragment into many subwords, diluting semantic content. As vocabulary grows toward 100k, BPE units increasingly resemble complete morphemes or words, recovering semantic coherence.
- Evidence anchors:
  - [abstract] "BPE performance improved with larger vocabulary sizes (5,000 to 100,000 subwords), with the largest vocabulary approaching word-level performance"
  - [section IV] "BPE models showed marked improvement as vocabulary size increased... indicating that when BPE units begin approximating full-word morphemes, they become viable alternatives"
  - [corpus] "The Token Tax" paper (arXiv:2509.05486) documents that high tokenization fertility (tokens per word) systematically disadvantages morphologically complex languages across 10 LLMs

### Mechanism 3: Granular Segmentation Destroys Entity Semantics
- Claim: Character-level and n-gram tokenization produce embeddings too fragmented to support entity recognition.
- Mechanism: Character and n-gram tokenizers split words into units smaller than morphemes. The resulting embeddings average over many unrelated contexts, failing to capture the semantic specificity needed to distinguish "B-PERSON" from "B-LOCATION."
- Evidence anchors:
  - [abstract] "Character-level and n-gram approaches performed worst, with macro F1 scores of 0.25-0.40"
  - [section IV, Fig. F] "Turkish Character-level tokenizer displays a general lack of understanding for most classes, as only 6/39 (~15%) classes had any precision or recall"
  - [corpus] "Tokens with Meaning" (arXiv:2508.14292) confirms subword methods "struggle with morphologically rich and agglutinative languages because they rarely align with linguistic morphemes"

## Foundational Learning

- **Agglutinative Morphology**:
  - Why needed here: Understanding that a single Turkish/Finnish word can encode what English expresses in a multi-word phrase is essential for interpreting why tokenization granularity matters.
  - Quick check question: Given the Turkish word "kitaplarınızdan" (from your books), identify at least three morpheme boundaries.

- **Static vs. Contextual Embeddings**:
  - Why needed here: This study uses Word2Vec (static), where each token gets one fixed vector regardless of context.
  - Quick check question: In Word2Vec, would the word "bank" have one embedding or separate embeddings for "financial institution" vs. "river edge"?

- **Macro F1 vs. Accuracy for Imbalanced Labels**:
  - Why needed here: NER datasets have severe label imbalance (most tokens are "O" - outside any entity).
  - Quick check question: If a model predicts "O" for every token in a dataset that is 95% "O" labels, what accuracy would it achieve? What macro F1?

## Architecture Onboarding

- **Component map**: Wikipedia text → Tokenization → Word2Vec embedding → NER classifier → Evaluation
- **Critical path**: Tokenization strategy → Vocabulary construction → Word2Vec training → NER classifier training → Evaluation
- **Design tradeoffs**:
  - Word-level: Best semantic coherence, highest OOV risk, large vocabulary
  - BPE (large vocab): Near word-level performance, tunable, requires vocabulary sweep
  - BPE (small vocab): Handles OOV but fragments semantics
  - Character/n-gram: No OOV but severe semantic degradation
- **Failure signatures**:
  - High accuracy + low macro F1 = model predicting majority class ("O") only
  - Many entity classes with zero precision/recall = tokenization too granular
  - Performance gap between Turkish and Finnish = dataset quality/size mismatch
- **First 3 experiments**:
  1. **Baseline validation**: Train Word2Vec on 10k Wikipedia articles with word-level tokenization, evaluate on NER. Verify macro F1 ≈ 0.67-0.68.
  2. **BPE vocabulary sweep**: Train BPE tokenizers at 5k, 25k, 50k, 100k vocabulary sizes. Plot macro F1 vs. vocabulary size. Confirm monotonic improvement.
  3. **Per-class failure analysis**: For character-level and word-level tokenizers, generate precision-recall plots per entity class. Identify which entity types fail first under fragmentation.

## Open Questions the Paper Calls Out
- Do these tokenization findings generalize to contextual embedding architectures such as BERT or ByT5?
- Do these findings hold for truly low-resource agglutinative languages beyond Turkish and Finnish proxies?
- How do tokenization strategies affect other downstream tasks such as POS tagging, dependency parsing, or semantic similarity?
- How does corpus noise and informal text (social media, messages) affect tokenization strategy performance?

## Limitations
- The study uses fixed Word2Vec parameters and logistic regression settings without exploring their interaction with tokenization choices
- Turkish NER dataset is ~10x larger than Finnish's, potentially confounding comparative analysis
- Results may not transfer to contextual embeddings or other downstream tasks beyond NER

## Confidence
- **High confidence (90-100%)**: Word-level outperforming character-level and n-gram approaches (macro F1 gap of 0.42-0.43)
- **Medium confidence (60-80%)**: BPE performance scaling with vocabulary size, though asymptotic behavior at >100k vocabulary untested
- **Low confidence (30-50%)**: Generalizability to truly low-resource scenarios with <1,000 documents

## Next Checks
1. **Cross-task validation**: Evaluate the same tokenization strategies on semantic similarity benchmarks and text classification tasks using the same Turkish and Finnish embeddings
2. **Extreme low-resource replication**: Repeat the experiment with 100-1,000 Wikipedia articles per language to test break conditions where OOV rates make word-level tokenization untenable
3. **Contextual embedding comparison**: Train BERT-style models with WordPiece/BPE tokenization on the same corpora and evaluate NER performance