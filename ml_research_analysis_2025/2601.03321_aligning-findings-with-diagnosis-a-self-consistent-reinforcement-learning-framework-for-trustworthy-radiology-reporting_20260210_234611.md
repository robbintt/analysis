---
ver: rpa2
title: 'Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning
  Framework for Trustworthy Radiology Reporting'
arxiv_id: '2601.03321'
source_url: https://arxiv.org/abs/2601.03321
tags:
- report
- clinical
- answer
- diagnostic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of generating accurate and trustworthy
  radiology reports using multimodal large language models (MLLMs), focusing on reducing
  factual hallucinations and ensuring logical consistency between findings and diagnoses.
  To tackle this, the authors propose a two-phase framework: first, they systematically
  identify the optimal vision encoder and LLM backbone configuration for medical imaging;
  second, they introduce a "Reason-then-Summarize" architecture optimized via Group
  Relative Policy Optimization (GRPO).'
---

# Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting

## Quick Facts
- **arXiv ID**: 2601.03321
- **Source URL**: https://arxiv.org/abs/2601.03321
- **Reference count**: 34
- **Primary result**: Achieves state-of-the-art clinical efficacy (CheXbert F1) and significantly reduces hallucinations in chest X-ray radiology report generation through self-consistent reinforcement learning

## Executive Summary
This paper addresses the challenge of generating accurate and trustworthy radiology reports using multimodal large language models (MLLMs), focusing on reducing factual hallucinations and ensuring logical consistency between findings and diagnoses. The authors propose a two-phase framework that first systematically identifies optimal vision encoder and LLM backbone configurations for medical imaging, then introduces a "Reason-then-Summarize" architecture optimized via Group Relative Policy Optimization (GRPO). This framework enforces internal consistency by structuring outputs into a `<think>` block for detailed findings and an `<answer>` block for structured disease labels, with a composite reward function penalizing discrepancies. Experimental results on the MIMIC-CXR benchmark show state-of-the-art performance in clinical efficacy metrics and significantly reduced hallucinations compared to supervised fine-tuning baselines.

## Method Summary
The proposed framework consists of two phases. Phase 1 performs systematic backbone search over vision encoders (CLIP, SigLIP, XrayCLIP, DINOv2, DINOv2-Xray) and LLMs (Llama, Qwen2, Med-Llama) with MLP connector, identifying Llama + XrayCLIP as optimal. Phase 2 employs cold-start supervised fine-tuning (2 epochs, AdamW, lr=2e-5) followed by GRPO optimization with a composite reward function. The "Reason-then-Summarize" architecture structures outputs into `<think>` block for detailed findings and `<answer>` block for 14 CheXpert pathology labels. The composite reward penalizes discrepancies between these blocks through self-consistency scoring, achieving 92.16% alignment on MIMIC-CXR test set while maintaining state-of-the-art clinical efficacy.

## Key Results
- Achieves state-of-the-art CheXbert F1 score on MIMIC-CXR, surpassing supervised fine-tuning baselines
- Maintains 92.16% self-consistency score between `<think>` findings and `<answer>` labels
- Significantly reduces hallucinations while preserving clinical diagnostic accuracy
- Demonstrates that self-consistency reward is critical, with SCS dropping from 92.16% to 75.68% when removed

## Why This Works (Mechanism)
The framework works by enforcing logical consistency between narrative findings and structured diagnoses through a composite reinforcement learning objective. The self-consistency reward (R1) directly penalizes discrepancies between the `<think>` findings and `<answer>` labels, ensuring the model cannot generate contradictory information. The Group Relative Policy Optimization algorithm enables stable learning by comparing multiple outputs per input and clipping reward differences, preventing extreme policy updates. The two-phase approach first identifies optimal architecture through systematic search, then fine-tunes with specialized rewards that prioritize clinical accuracy over traditional NLG metrics.

## Foundational Learning
- **Multimodal Large Language Models**: Integration of vision encoders with LLMs for medical image understanding; needed to process chest X-rays and generate coherent reports; quick check: verify XrayCLIP provides relevant radiological features
- **Reinforcement Learning for Text Generation**: Using policy gradients to optimize non-differentiable rewards; needed to incorporate clinical consistency metrics; quick check: confirm GRPO implementation follows TRL library patterns
- **Self-Consistency Scoring**: Measuring agreement between generated findings and diagnostic labels; needed to detect and prevent hallucinations; quick check: verify CheXbert-based scoring correctly processes `<think>` text
- **Clinical Efficacy Metrics**: Specialized evaluation beyond standard NLG (BLEU, ROUGE); needed to assess medical accuracy; quick check: confirm CheXbert F1 and RadGraph F1 calculations
- **Group Relative Policy Optimization**: Stable RL algorithm variant for text generation; needed to prevent reward hacking and collapse; quick check: verify group sampling and clipping parameters
- **Asymmetric Reward Design**: Different weights for TP/TN vs FP/FN in medical diagnosis; needed to reflect clinical priorities; quick check: verify +2.0/1.0/0.5/-0.3 weighting scheme

## Architecture Onboarding
**Component Map**: Vision Encoder (XrayCLIP) -> MLP Connector -> LLM (Llama) -> Output Formatter (`<think>` + `<answer>`)
**Critical Path**: Image input → Vision encoder → MLP projection → LLM context → Structured generation → Self-consistency scoring → Reward computation → Policy update
**Design Tradeoffs**: Standard MLP connector chosen for simplicity over sophisticated Q-Former, potentially bottlenecking fine-grained visual features; two-phase training balances architecture search with specialized optimization
**Failure Signatures**: Low self-consistency score (disagreement between findings and labels), "length collapse" producing terse outputs, format violations (missing tags or invalid JSON), reduced CheXbert F1 indicating diagnostic degradation
**Three First Experiments**:
1. Verify backbone search results by reproducing optimal architecture selection on smaller MIMIC-CXR subset
2. Test GRPO implementation with simplified reward (R1 only) to confirm self-consistency enforcement
3. Evaluate format reward (R5) by generating outputs with intentional tag violations to verify penalty application

## Open Questions the Paper Calls Out
**Open Question 1**: Can advanced connector architectures (e.g., Q-Former) replace the standard MLP projection to preserve fine-grained radiological visual features without sacrificing training efficiency? The paper suggests MLP may bottleneck fine-grained details and proposes exploring alternatives.

**Open Question 2**: How can reinforcement learning reward functions be designed to maintain descriptive richness in reports while preserving diagnostic accuracy and consistency? The framework induces "length collapse," optimizing for succinct diagnostic statements at the expense of descriptive verbosity.

**Open Question 3**: What explains the observed non-linear degradation when pairing domain-pretrained vision encoders with domain-pretrained LLMs, and can adaptive pairing strategies mitigate this? The paper notes this counter-intuitive result challenges conventional architectural assumptions.

**Open Question 4**: Does the "Reason-then-Summarize" framework generalize effectively to non-chest radiology imaging modalities (e.g., CT, MRI) with different reporting conventions? All experiments are confined to chest X-ray datasets.

## Limitations
- Unknown GRPO hyperparameters (group size G, clip range ε) prevent exact reproduction
- MLP connector may bottleneck fine-grained radiological visual features
- RL process induces "length collapse" phenomenon, sacrificing descriptive verbosity for diagnostic efficiency
- Performance sensitive to asymmetric CFS reward weights, lacking systematic calibration
- All experiments limited to chest X-ray modality, generalizability to CT/MRI untested

## Confidence
- **High Confidence**: Overall framework design, composite reward structure, clinical efficacy improvements are well-supported by experimental results
- **Medium Confidence**: Specific performance numbers (92.16% SCS, state-of-the-art CE metrics) likely reproducible but could vary based on unspecified RL hyperparameters
- **Low Confidence**: Exact GRPO implementation details, MLP connector specifications, and RL training duration cannot be faithfully reproduced without additional information

## Next Checks
1. Implement GRPO with multiple configurations for group size (G=4, 8, 16) and clip range (ε=0.1, 0.2, 0.3) to empirically determine optimal combination for 92.16% SCS
2. Systematically evaluate sensitivity of final performance to asymmetric CFS reward weights (+2.0 TP, +1.0 TN, +0.5 uncertain, -0.3 FP/FN) by varying parameters
3. Conduct controlled ablation study replacing MLP connector with alternative multimodal fusion approaches (direct concatenation, cross-attention) while keeping all other components fixed