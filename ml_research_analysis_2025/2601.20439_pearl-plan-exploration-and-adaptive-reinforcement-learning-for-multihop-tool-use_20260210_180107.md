---
ver: rpa2
title: 'PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool
  Use'
arxiv_id: '2601.20439'
source_url: https://arxiv.org/abs/2601.20439
tags:
- tool
- arxiv
- planning
- execution
- pearl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PEARL introduces a two-stage framework combining offline tool exploration
  and online reinforcement learning to address multi-step tool use challenges. The
  Planner is trained via Group Relative Policy Optimization with a planning-centric
  reward that provides step-by-step feedback on tool selection quality.
---

# PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use

## Quick Facts
- **arXiv ID**: 2601.20439
- **Source URL**: https://arxiv.org/abs/2601.20439
- **Reference count**: 37
- **Primary result**: PEARL achieves 56.5% success rate on ToolHop, surpassing both larger open-source models and leading proprietary models.

## Executive Summary
PEARL introduces a two-stage framework combining offline tool exploration and online reinforcement learning to address multi-step tool use challenges. The Planner is trained via Group Relative Policy Optimization with a planning-centric reward that provides step-by-step feedback on tool selection quality. PEARL achieves 56.5% success rate on ToolHop, surpassing both larger open-source models and leading proprietary models. The offline exploration phase reduces invocation errors to 3.8%, while the RL-optimized planner demonstrates strong generalization, improving the performance of other models when provided with its plans. The approach establishes new state-of-the-art performance and validates the effectiveness of decoupled planning-execution architecture with specialized reward design.

## Method Summary
PEARL operates in two phases: (1) Offline tool exploration where the agent systematically calls each tool with varied parameters to discover valid usage patterns and failure conditions, building a knowledge base for execution; (2) Planner training via GRPO for 15 epochs using VERL framework on Qwen2.5-7B. The planning-centric reward assigns step-wise credit (r_i = 1/m) when generated tools match ground-truth plans, enabling dense credit assignment across long horizons. The decoupled Planner/Executor architecture allows independent optimization, with the Planner generating complete multi-step plans and the Executor leveraging exploration knowledge for step-by-step execution.

## Key Results
- PEARL achieves 56.5% success rate on ToolHop, surpassing both larger open-source models and leading proprietary models
- Offline exploration reduces invocation errors from 24.1% to 3.8%, demonstrating the effectiveness of proactive grounding
- GRPO-optimized planner improves performance of other models by 5-6% when used as a planning module, showing strong generalization
- Step-wise reward ablation drops success rate from 56.5% to 23.4%, confirming dense rewards are critical for multi-step learning

## Why This Works (Mechanism)

### Mechanism 1: Planning-Centric Reward for Dense Credit Assignment
- Claim: Step-wise rewards on tool selection correctness enable effective RL optimization of multi-step plans, whereas sparse task-success signals fail to provide sufficient learning signal.
- Mechanism: The reward function (Equation 2) assigns normalized credit r_i = 1/m for each correctly selected tool matching ground-truth, summing to R(P) = 1 for perfect plans. This dense feedback solves the credit assignment problem across long horizons.
- Core assumption: Ground-truth plans are available for training; tool selection quality correlates with downstream task success.
- Evidence anchors:
  - [abstract]: "a carefully designed reward function that provides distinct signals for planning quality"
  - [section 3.2, ablation Table 4]: Removing planning reward drops SR from 56.5% to 23.4%—"catastrophic drop highlights that our reward design is the cornerstone"
  - [corpus]: Related work on RL for tool learning (ToolRL) suggests reward design is critical, but corpus lacks direct replication of step-wise planning rewards.
- Break condition: If ground-truth plans are noisy, unavailable, or tool selection does not correlate with task outcomes, reward signal degrades.

### Mechanism 2: Offline Tool Exploration Reduces Execution Errors
- Claim: Proactive trial-and-error tool exploration builds a practical knowledge base that drastically reduces invocation errors (hallucinated tools, incorrect parameters).
- Mechanism: Before task execution, the agent systematically calls each tool with varied parameters, discovering valid patterns and failure modes. This creates a "learned user manual" that grounds subsequent execution.
- Core assumption: Tools can be safely called in exploration mode; discovered patterns transfer to downstream tasks.
- Evidence anchors:
  - [abstract]: "offline phase where the agent explores tools to learn valid usage patterns and failure conditions"
  - [section 3.2, ablation Table 4]: Removing offline exploration spikes IER from 3.8% to 24.1% with SR dropping to 33.9%
  - [corpus]: Weak corpus support—neighbor papers mention tool-augmented reasoning but not systematic offline exploration.
- Break condition: If exploration budget is insufficient, tools have complex state dependencies, or exploration cannot be safely performed, grounding fails.

### Mechanism 3: GRPO Group Normalization Stabilizes LLM Policy Updates
- Claim: Normalizing advantages across candidate plans for the same query reduces variance and stabilizes RL fine-tuning for LLMs.
- Mechanism: For each query, sample k plans, compute group mean μ_q and std σ_q, then normalize advantages Â_j = (R(P_j) - μ_q) / (σ_q + η). This focuses learning on relative plan quality within a query context.
- Core assumption: Multiple diverse plans can be sampled per query; variance in rewards stems primarily from plan quality rather than task difficulty.
- Evidence anchors:
  - [section 3.3]: "GRPO... enhances training stability by normalizing advantage estimates across a group of candidate plans generated for the same input query"
  - [section 4.3]: Planner trained via GRPO for 15 epochs using VERL framework
  - [corpus]: DeepSeekMath (citation [19]) introduced GRPO; corpus suggests effectiveness in mathematical reasoning, but tool-use domain validation remains limited to this work.
- Break condition: If candidate plans lack diversity, group statistics become uninformative; if query difficulty varies widely, per-query normalization may mislead.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO)**
  - Why needed here: GRPO is a PPO variant; understanding clipping, advantage estimation, and policy ratios is prerequisite.
  - Quick check question: Can you explain why PPO clips the policy ratio and how the clipping hyperparameter ε affects learning stability?

- Concept: **Reward Shaping for Multi-Step Decision Making**
  - Why needed here: The planning-centric reward is a form of dense reward shaping; understanding credit assignment is essential.
  - Quick check question: Given a 5-step plan where only step 3 is wrong, how would a sparse vs. dense reward differ in their learning signal?

- Concept: **Tool-Augmented LLM Architectures**
  - Why needed here: PEARL assumes familiarity with tool-calling paradigms (API schemas, parameter generation, execution environments).
  - Quick check question: What are common failure modes when an LLM generates tool calls (e.g., hallucination, parameter type mismatches)?

## Architecture Onboarding

- Component map:
  - Query -> Planner (generates P = ((s_1, τ_1), ..., (s_n, τ_n))) -> Executor (executes each sub-task with tool environment) -> Tool Environment

- Critical path:
  1. Run offline exploration (max 10 rounds/tool) → build execution knowledge base
  2. Train Planner via GRPO with planning-centric rewards (15 epochs)
  3. At inference: Planner generates plan → Executor executes step-by-step with tool environment

- Design tradeoffs:
  - Decoupled Planner/Executor adds complexity but enables independent optimization
  - Step-wise reward requires ground-truth plans; sparse reward would remove this dependency but ablation shows severe degradation
  - Group normalization helps stability but requires sampling multiple plans per query (compute overhead)

- Failure signatures:
  - High IER (>15%): Offline exploration likely insufficient or tool schemas changed
  - Low SR despite correct tool selection: Executor not leveraging exploration knowledge; check knowledge retrieval
  - Training instability: Check group size k; if too small, advantage estimates become noisy
  - Planner over-generating steps: Reward may not be penalizing length sufficiently; verify m-boundary handling

- First 3 experiments:
  1. **Reproduction baseline**: Train PEARL-7B on ToolHop training split with paper hyperparameters (k=4 candidates, ε=0.2, 15 epochs); verify SR ≈ 56.5% and IER ≈ 3.8%
  2. **Ablation checkpoint**: Remove offline exploration; confirm IER spikes to ~24% as paper reports; this validates the exploration-grounding mechanism
  3. **Cross-model planner transfer**: Use PEARL-Planner to guide GPT-4o or Qwen2.5-72B executor; expect 5-6% SR lift, confirming planner generalization claim

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the planning-centric reward function be adapted to function without explicit ground-truth plan supervision?
- **Basis in paper:** [inferred] The Method section (Eq. 2) defines the step-wise reward $r_i$ by directly comparing the generated plan $P$ against a ground-truth plan $P^*$.
- **Why unresolved:** The reliance on a pre-existing optimal plan $P^*$ limits the framework's applicability to benchmarks or real-world scenarios where such ground-truth trajectories are unavailable or expensive to annotate.
- **What evidence would resolve it:** Successful training of a Planner model using outcome-based rewards (e.g., final task success only) or self-supervised metrics that match the performance of the current ground-truth-dependent method.

### Open Question 2
- **Question:** Does the PEARL framework support dynamic re-planning when execution feedback contradicts the initial static plan?
- **Basis in paper:** [inferred] The framework description (Section 3.2) states the Planner generates the "entire multi-step plan" before execution begins, while the Introduction identifies "capacity for adaptation" and "dynamically adjust[ing] strategy" as key challenges in the field.
- **Why unresolved:** While the offline phase teaches the Executor to handle errors, the architecture suggests the Planner produces a fixed "blueprint" upfront, potentially lacking a mechanism to revise the high-level strategy mid-execution based on unexpected environment feedback.
- **What evidence would resolve it:** An experiment measuring performance on tasks where the environment changes state unpredictably after the plan is generated, requiring the Planner to issue revised instructions.

### Open Question 3
- **Question:** How does the offline tool exploration phase scale to environments with significantly larger tool ecosystems (e.g., thousands of tools)?
- **Basis in paper:** [inferred] The experiments (Section 4.1) utilize benchmarks (ToolHop, T-Eval) with tool counts in the hundreds, and the implementation details mention a max of 10 rounds of exploration per tool.
- **Why unresolved:** The computational cost and sample efficiency of the trial-and-error exploration phase may become prohibitive as the tool set $T$ grows, potentially creating a bottleneck not visible in the current smaller-scale evaluations.
- **What evidence would resolve it:** An analysis of training time and performance saturation when scaling the tool set to >10,000 tools, or the introduction of a retrieval mechanism to selective explore only relevant tools.

## Limitations

- Ground-truth plan annotations are required for reward computation, limiting applicability to supervised settings
- Offline exploration assumes tools can be safely called in trial-and-error fashion, which may not hold for tools with side effects or costs
- Performance gains over proprietary models may partly reflect base model capability differences rather than architectural advantages
- The framework's effectiveness on tool ecosystems significantly larger than current benchmarks remains untested

## Confidence

**High Confidence**: The mechanism of dense step-wise rewards improving credit assignment (Mechanism 1) is strongly supported by ablation results showing catastrophic performance drops when removed. The offline exploration reducing invocation errors (Mechanism 2) is validated by clear IER reduction from 24.1% to 3.8%.

**Medium Confidence**: The GRPO normalization mechanism (Mechanism 3) shows theoretical soundness and implementation details, but corpus evidence for tool-use domain benefits is limited to this single work. The planner generalization claim is supported by moderate performance improvements on other models but lacks extensive cross-architecture validation.

**Low Confidence**: Claims about PEARL outperforming larger open-source models (e.g., Qwen2.5-72B) are based on single comparisons without ablation studies isolating the impact of model size versus architecture. The assumption that tool selection quality directly correlates with task success is reasonable but not empirically validated across diverse tool types.

## Next Checks

1. **Ground-truth plan dependency test**: Train PEARL with synthetic ground-truth plans (generated by GPT-4) vs. human-annotated plans to measure sensitivity to annotation quality and verify the approach works beyond supervised settings.

2. **Safe exploration verification**: Implement tools with simulated costs or state changes during offline exploration to test whether the knowledge base degrades when exploration cannot be performed exhaustively or safely.

3. **Base model ablation**: Compare PEARL-7B against a vanilla Qwen2.5-7B with increased inference steps and tool-calling capabilities to isolate the architectural contribution from model capability differences.