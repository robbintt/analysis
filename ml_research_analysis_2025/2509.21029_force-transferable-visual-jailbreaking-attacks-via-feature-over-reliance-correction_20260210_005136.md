---
ver: rpa2
title: 'FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance
  CorrEction'
arxiv_id: '2509.21029'
source_url: https://arxiv.org/abs/2509.21029
tags:
- attacks
- visual
- jailbreaking
- loss
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limited transferability of optimisation-based
  visual jailbreaking attacks, which exploit vulnerabilities in multimodal large language
  models (MLLMs) but fail to generalise across models. The authors analyse the loss
  landscape and feature representations of these attacks, identifying non-generalizable
  reliance on model-specific early-layer features and semantically poor high-frequency
  components.
---

# FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction

## Quick Facts
- arXiv ID: 2509.21029
- Source URL: https://arxiv.org/abs/2509.21029
- Reference count: 40
- Key outcome: Proposed FORCE method improves transferability of visual jailbreaking attacks, achieving 13% average ASR gains on adapter-based MLLMs, nearly 100% on early-fusion models, and 70% on commercial models while reducing query costs.

## Executive Summary
This paper addresses the limited transferability of optimisation-based visual jailbreaking attacks on multimodal large language models (MLLMs). Current PGD-based attacks optimise into high-sharpness loss regions and over-rely on model-specific early-layer features and semantically poor high-frequency components, causing them to fail when transferred to different models. The authors propose FORCE (Feature Over-Reliance CorrEction), a method that explores broader feasible regions through layer-aware regularisation and reduces influence of semantically weak high-frequency features via spectral rescaling. Experiments show FORCE significantly improves transferability across various MLLM architectures while reducing computational costs.

## Method Summary
FORCE combines two key components to improve transferability of visual jailbreaking attacks. First, spectral rescaling uses FFT to partition the perturbation spectrum into bands, identifying and downweighting high-frequency components that become model-specific during optimisation. Second, layer-aware regularisation samples reference points in the perturbation space and maximises feature distances across model layers, particularly expanding the feasible region in early layers where attacks typically over-rely on narrow, model-specific patterns. These components are integrated into the PGD optimisation loop, guiding attacks toward flatter loss landscapes with broader early-layer support and restored low-frequency dominance.

## Key Results
- FORCE achieves 13% average ASR gains on adapter-based MLLMs compared to baseline PGD
- Near 100% improvement on early-fusion MLLMs (Qwen2.5-VL, InternVL)
- 70% improvement on commercial models including GPT-4V and Gemini-1.5-Pro
- Reduces query costs by 43% on average across all target models

## Why This Works (Mechanism)

### Mechanism 1: Loss Landscape Sharpness Causes Transfer Fragility
Visual jailbreaking attacks optimise into high-sharpness loss regions, making effectiveness highly sensitive to minor parameter shifts between source and target models. PGD-based attacks minimise loss toward narrow local optima. When model parameters change during transfer, even perturbations of 0.0002 push the attack outside the feasible region, invalidating it. Flatter loss landscapes correlate with more generalizable attack features across model architectures.

### Mechanism 2: Early-Layer Feature Narrowness Limits Generalisation
Visual attacks over-rely on model-specific early-layer features, creating narrow feasible regions that do not survive architectural transfer. Feature interpolation experiments show layer 11 requires >90% adversarial features to succeed, while layer 31 tolerates 40% natural features. Early layers encode model-specific patterns; later layers approach more semantic representations. Broader early-layer feasible regions correspond to more transferable semantic features rather than model-specific artifacts.

### Mechanism 3: High-Frequency Spectral Drift Creates Model-Specific Artifacts
As PGD optimisation progresses, attack effectiveness shifts from semantically-rich low-frequency components to semantically-poor high-frequency components, reducing cross-model transferability. Frequency band masking shows early iterations depend on low-frequency bands (semantic), but by iteration 750, removing the third-highest frequency band alone invalidates attacks. High-frequency patterns are model-specific, while natural image semantics reside primarily in low-frequency components.

## Foundational Learning

- **Concept: Flat vs Sharp Minima in Loss Landscapes**
  - Why needed here: Understanding why flat regions generalise better across models is core to FORCE's design philosophy.
  - Quick check question: Can you explain why a flat minimum is more robust to parameter perturbations than a sharp minimum?

- **Concept: Transfer-Based Adversarial Attacks**
  - Why needed here: FORCE is fundamentally a transfer attack—optimising on white-box source to attack black-box targets.
  - Quick check question: What is the difference between query-based and transfer-based black-box attacks?

- **Concept: Fourier/Spectral Analysis of Images**
  - Why needed here: FORCE rescales frequency components; you need to understand FFT decomposition and frequency bands.
  - Quick check question: Which frequency components typically carry semantic content in natural images—low or high frequencies?

## Architecture Onboarding

- **Component map:**
  Spectral Rescaling -> Layer-Aware Regularization -> FORCE Main Loop

- **Critical path:** Spectral rescaling corrects frequency over-reliance first; then layer-aware regularisation expands early-layer feasible regions; combined loss guides PGD into flatter regions. Both components must be active for full transfer gains (Table 3: 18.9% ASR improvement combined vs 5.7% or 15.1% individually).

- **Design tradeoffs:**
  - N (reference samples): Higher N improves reliability but increases compute. Paper uses N=10.
  - η (noise neighbourhood): Larger η explores wider regions but may exit feasible zone. Paper uses 8/255.
  - M (frequency bands): More bands give finer spectral control but more loss evaluations. Paper uses M=10.
  - λ (regularisation strength): Controls layer penalty aggressiveness. Too strong may prevent convergence.

- **Failure signatures:**
  - Attack succeeds on source but ASR ≈0% on target → check if early-layer reliance persists (visualise layer interpolation).
  - Loss plateaus without reaching target → spectral rescaling may be over-suppressing needed frequencies.
  - Query cost increases vs baseline → N or η too large, or λ too strong.

- **First 3 experiments:**
  1. Reproduce baseline transfer gap: Run standard PGD on LLaVA-v1.5-7B source, evaluate on InstructBLIP and Qwen2.5-VL targets. Confirm low ASR on early-fusion targets.
  2. Ablate components individually: Run FORCE with only spectral rescaling, then only layer regularisation, on same transfer pairs. Compare to Table 3 patterns.
  3. Visualise layer and spectral shifts: For a successful FORCE attack, replicate Figures 5 and 6 to confirm broader layer regions and restored low-frequency dominance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important unresolved issues regarding the generalizability of the findings and potential defensive countermeasures.

## Limitations
- Core mechanisms rely on assumptions from classification literature rather than direct MLLM jailbreaking evidence
- Spectral drift mechanism lacks external validation in the corpus despite related spectral analysis existing in adversarial robustness literature
- Early-layer feature narrowness claim assumes layer features behave consistently across different MLLM architectures without addressing architectural variations

## Confidence
- **High confidence**: The experimental results showing FORCE's quantitative improvements (13% average ASR gains on adapter-based MLLMs, nearly 100% on early-fusion models, 70% on commercial models) are well-documented with specific metrics and tables.
- **Medium confidence**: The proposed mechanisms (loss landscape sharpness, early-layer feature narrowness, spectral drift) are logically coherent and supported by the paper's ablation studies, but rely on assumptions from related domains rather than direct MLLM jailbreaking evidence.
- **Low confidence**: The transferability improvements may be partially attributed to dataset-specific optimizations rather than fundamental architectural insights, given the limited scope of tested models and datasets.

## Next Checks
1. **External Model Transfer**: Test FORCE against a model not in the original training/evaluation set (e.g., LLaVA-NeXT or Qwen2.5-VL-72B) to verify claims of general transferability beyond the specific targets studied.

2. **Ablation on Natural Images**: Replicate the layer interpolation and frequency band experiments (Figures 3-4) on naturally-occurring images rather than optimization-initialized gray or panda images to confirm the mechanisms hold across different image types.

3. **Temporal Transfer Robustness**: Measure attack effectiveness after a 3-6 month gap when target models receive updates or fine-tuning, to validate the claimed robustness to parameter shifts beyond the controlled 0.0002 perturbation experiments.