---
ver: rpa2
title: 'Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning
  Potential'
arxiv_id: '2510.15216'
source_url: https://arxiv.org/abs/2510.15216
tags:
- reasoning
- rules
- features
- arxiv
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a method to predict a pre-trained language\
  \ model\u2019s reasoning potential using a microscopic metric based on its internal\
  \ logic rules. The authors formalize reasoning as chains of Horn clauses (if-then\
  \ rules) extracted from features learned by cross-layer sparse autoencoders."
---

# Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential

## Quick Facts
- arXiv ID: 2510.15216
- Source URL: https://arxiv.org/abs/2510.15216
- Authors: Xuansheng Wu; Xiaoman Pan; Wenlin Yao; Jianshu Chen
- Reference count: 40
- One-line primary result: SAL predicts post-RLVR error rates with R²=0.87 using internal logic rule distributions extracted via sparse autoencoders.

## Executive Summary
This paper introduces Soundness-Aware Level (SAL), a zero-label metric that predicts a pre-trained language model's reasoning potential before any RLVR training. SAL measures how well a model's internal probability distributions distinguish sound from unsound logic rules extracted from its latent space. Using cross-layer sparse autoencoders, the authors identify ~32K interpretable features and estimate transition probabilities between them as Horn clauses. An LLM judge categorizes these rules by semantic soundness, and SAL quantifies the separation between rule distributions using Jensen-Shannon Divergence. Experiments show SAL strongly correlates with post-RLVR performance (R²=0.87) and follows an empirical exponential law. SAL varies by model family and scale, offering a practical tool for selecting stronger base models.

## Method Summary
The method extracts logic rules from pre-trained models using cross-layer sparse autoencoders (SAEs) that decompose residual stream activations into interpretable features. Feature co-occurrence statistics across a corpus estimate transition probabilities for Horn clauses (premise → conclusion rules). Rules are categorized by semantic soundness (Strict, Plausible, Noise) using an LLM judge. SAL is computed as the Jensen-Shannon Divergence between probability distributions of rules in each category. The metric requires no ground-truth labels, relying solely on internal model statistics. The approach is validated by correlating SAL with post-RLVR reasoning performance across diverse model families and scales.

## Key Results
- SAL predicts post-RLVR error rates with R²=0.87 using the empirical law ε = exp(-α·s^β)
- Models with SAL > 0.20 achieve more than double the accuracy of models with SAL < 0.08
- SAL increases with model scale and varies significantly across model families
- High-potential models internally separate strict, plausible, and noisy rules, while weaker models show nearly identical distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Internal probability distributions over logic rules encode reasoning potential before any RLVR training occurs.
- Mechanism: Cross-layer sparse autoencoders decompose residual stream activations into ~32K interpretable features. Feature co-occurrence statistics across a corpus estimate transition probabilities for implicit Horn clauses (premises → conclusion). These probabilities form distributions that can be compared across soundness categories.
- Core assumption: Feature co-occurrence reflects learned logical entailment rather than spurious correlation at sufficient scale.
- Evidence anchors:
  - [abstract]: "chains of Horn clauses ('if-then' rules) built from features extracted from the LLM's latent space via cross-layer sparse autoencoders"
  - [section 2.3, p.3]: "if a set of premise features P consistently activates in the layers preceding a conclusion feature Q across thousands of varied inputs, this provides strong evidence for a learned rule P→Q"
  - [corpus]: Limited direct support; related work on neural theorem proving exists but doesn't validate this specific co-occurrence-to-logic mapping
- Break condition: If co-occurrence primarily captures surface statistics rather than logical structure, SAL would measure memorization quality rather than reasoning potential.

### Mechanism 2
- Claim: High-potential models internally separate sound from unsound rules via distinct probability distributions.
- Mechanism: Rules are categorized as Strict/Plausible/Noise by an LLM judge. For each category, a histogram of rule probabilities is constructed. Jensen-Shannon Divergence (JSD) measures separation between these three distributions—the SAL metric. High SAL indicates the model "knows" which rules are reliable.
- Core assumption: The LLM judge's soundness labels are sufficiently correlated with actual logical validity for the signal to emerge despite labeling noise.
- Evidence anchors:
  - [abstract]: "high-potential models are inherently soundness-aware: their internal probability distributions systematically shift across rules' soundness levels"
  - [figure 2, p.5]: Qwen-2.5-7B shows tight high-probability cluster for Strict rules vs. low-probability concentration for Noise; Llama-3.1-8B shows nearly identical distributions
  - [corpus]: Related work on probabilistic reasoning chains (arXiv:2507.12948) addresses error propagation but doesn't validate this distribution-separation hypothesis
- Break condition: If the judge systematically mislabels rules in a way that correlates with model architecture, SAL could measure judge-model alignment rather than reasoning.

### Mechanism 3
- Claim: SAL predicts post-RLVR error rates via an exponential power law (ε = exp(-α·s^β)).
- Mechanism: The relationship is grounded in large deviation theory—probability of "reasoning failure" connects to divergence between rule distributions. With fitted parameters α=4.246, β=1.090, the law achieves R²=0.985 for interpolation and R²=0.872 for leave-one-out validation.
- Core assumption: The empirical law generalizes beyond the tested model families (Qwen, Mistral, Llama, DeepSeek) and scales (0.5B-14B).
- Evidence anchors:
  - [section 3.3, p.6-7]: "Models with small SAL scores (<0.08) achieve only 20% accuracy, while models with the highest SAL scores (>0.20) see their performance more than doubled"
  - [figure 4, p.6]: Clear monotonic relationship between SAL and post-RLVR accuracy across all model families
  - [table 1, p.7]: SAL achieves 96.4% average Spearman correlation across benchmarks
  - [corpus]: No corpus evidence validates this specific exponential form
- Break condition: If the relationship is family-specific or fails to extrapolate to larger scales (>14B) or different architectures, SAL's practical utility for model selection is limited.

## Foundational Learning

- Concept: **Sparse Autoencoders for Mechanistic Interpretability**
  - Why needed here: SAEs decompose polysemantic neural activations into monosemantic, human-interpretable features. Without this decomposition, hidden states are uninterpretable vectors.
  - Quick check question: Can you explain why L1 sparsity penalties encourage monosemantic features rather than just fewer active features?

- Concept: **Horn Clauses and Forward Chaining**
  - Why needed here: The paper formalizes reasoning as logic programming—premises entail conclusions. Understanding this framing is essential to see why co-occurrence statistics might reflect logical structure.
  - Quick check question: Given rules "A→B" and "B→C", what does forward chaining produce when A is true?

- Concept: **Jensen-Shannon Divergence**
  - Why needed here: JSD quantifies separation between probability distributions symmetrically and smoothly (unlike KL divergence). SAL is literally defined as JSD between rule-probability histograms.
  - Quick check question: When would JSD=0, and what would that mean for a model's rule distributions?

## Architecture Onboarding

- Component map:
  Pre-trained LLM -> Residual stream activations (layers 1,4,8...L) -> Cross-layer SAE (32K features, 8 layers) -> Feature activations per token (sparse, ~20-30 active) -> Co-occurrence counting over 3K+ samples -> Rule probabilities p(Q|P) via MLE with smoothing -> LLM judge labels: Strict/Plausible/Noise -> Histogram per category -> JSD -> SAL score

- Critical path: SAE training quality -> feature interpretability -> reliable co-occurrence statistics -> meaningful rule probabilities. Dead or uninterpretable features (Table 3 shows 3.43% dead rate, 88.37% explainable rate) propagate noise downstream.

- Design tradeoffs:
  - Larger SAE (more features) captures finer-grained concepts but increases computational cost and dead features
  - More layers in cross-layer SAE captures cross-layer features but complicates attribution
  - More counting samples improves rule probability estimates but costs ~30 hours/node
  - Corpus composition affects which rules are discoverable (math corpus -> math rules)

- Failure signatures:
  - High dead feature rate (>10%): SAE learning rate or sparsity penalty needs tuning
  - Near-identical distributions across soundness levels: Model may be soundness-agnostic OR SAE failed to capture relevant features
  - SAL saturation at large scales: Diminishing returns from scaling alone (Figure 5 shows curve flattening beyond 14B)

- First 3 experiments:
  1. **SAE quality validation**: Train cross-layer SAE on your target model. Verify reconstruction MSE (target: 0.65-0.80 normalized) and interpretability rate (>85%). Check dead feature rate.
  2. **Rule extraction sanity check**: Extract rules from a model you understand well. Manually inspect top-10 highest-probability rules—do they match expected knowledge? Verify the judge labeling agrees with human judgment on a sample.
  3. **SAL-to-performance correlation**: Compute SAL for 3+ models with known RLVR performance. Plot SAL vs. accuracy. Confirm monotonic relationship before relying on SAL for model selection.

## Open Questions the Paper Calls Out

- Question: Does the Soundness-Aware Level (SAL) causally determine reasoning potential, or is it merely a correlational proxy?
- Basis in paper: [explicit] The "Limitations and Future Work" section states, "proving a direct causal link between SAL and reasoning potential is a crucial direction... We do not perform the interventional experiments necessary to demonstrate the causality."
- Why unresolved: The current study establishes a strong predictive correlation ($R^2=0.87$) using observational data from existing model checkpoints but does not alter the model to test if manipulating SAL directly changes performance.
- What evidence would resolve it: Interventional experiments where model internals are modified to artificially increase or decrease the separation of rule distributions (SAL), resulting in a corresponding change in downstream reasoning accuracy.

- Question: Can pre-training objectives be explicitly designed to maximize SAL during the initial training phase?
- Basis in paper: [inferred] The conclusion suggests the work "opens new avenues for designing pre-training objectives... that explicitly cultivate soundness-aware abilities from the start."
- Why unresolved: The paper acts as a diagnostic tool for existing models; it does not propose or test a specific training loss or data curriculum aimed at increasing the soundness-aware level.
- What evidence would resolve it: A training run utilizing an auxiliary loss function based on feature co-occurrence divergence that yields a base model with significantly higher SAL and subsequent RLVR performance compared to a baseline.

- Question: Does the predictive power of SAL generalize to non-mathematical reasoning domains such as coding or logical inference?
- Basis in paper: [inferred] The experimental scope is limited to mathematical benchmarks (MATH, GSM8K, AMC, AIME) and math-focused corpora.
- Why unresolved: The "soundness" of logic rules in mathematics (e.g., strict theorems) may differ fundamentally from procedural or semantic logic in other domains, potentially limiting the metric's universality.
- What evidence would resolve it: Demonstrating that SAL calculated on general or code-specific corpora maintains a high correlation ($R^2 > 0.8$) with post-RLVR performance on benchmarks like HumanEval or LogiQA.

## Limitations

- The SAE-based feature extraction introduces uncertainty—if the autoencoders fail to capture true logical structure and instead reflect surface-level co-occurrence, SAL would measure memorization rather than reasoning potential.
- The reliance on an LLM judge for soundness labeling adds another layer of potential error, as systematic biases in the judge could correlate with model architecture rather than reflect genuine reasoning capability.
- The exponential law relating SAL to post-RLVR performance shows strong fit (R²=0.87) but lacks theoretical grounding—it may be specific to the tested model families and scales.

## Confidence

- **High confidence**: SAL measures something meaningful about model internal representations (distribution separation is observed and quantified)
- **Medium confidence**: SAL correlates with post-RLVR performance (strong empirical relationship but unexplained mechanism)
- **Medium confidence**: SAL is family- and scale-dependent (observed pattern but limited model diversity)
- **Low confidence**: SAL predicts reasoning potential (core claim depends on SAE interpretability and judge reliability)

## Next Checks

1. **SAE interpretability validation**: Train cross-layer SAE on a model with known reasoning capabilities. Manually inspect top-20 rules with highest probability. Do they reflect actual logical entailments rather than spurious co-occurrences? Compare with a model known to have poor reasoning.

2. **Judge independence test**: Recompute SAL using different LLM judges (different model, different prompting strategy). Does the SAL score remain stable across judges, or does it vary systematically with judge choice? This would reveal whether SAL measures judge-model alignment rather than intrinsic reasoning.

3. **Scale extrapolation validation**: Compute SAL for models beyond 14B parameters (e.g., 34B or 70B). Does the empirical law ε = exp(-α·s^β) hold at these scales, or does the relationship break down? This tests whether SAL is a fundamental property or an artifact of the tested range.