---
ver: rpa2
title: 'SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism'
arxiv_id: '2510.21599'
source_url: https://arxiv.org/abs/2510.21599
tags:
- shap
- tensor
- complexity
- neural
- computing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a complexity-theoretic analysis of computing
  exact SHAP explanations for Tensor Networks (TNs), a broader class of models than
  previously studied. It introduces a general framework for computing exact SHAP for
  TNs with arbitrary structures and shows that when TNs are restricted to Tensor Train
  (TT) structures, SHAP computation can be performed in poly-logarithmic time using
  parallel computation.
---

# SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism

## Quick Facts
- arXiv ID: 2510.21599
- Source URL: https://arxiv.org/abs/2510.21599
- Reference count: 40
- Primary result: Exact SHAP computation for Tensor Trains in NC², with width as the primary BNN bottleneck

## Executive Summary
This work provides a complexity-theoretic analysis of computing exact SHAP explanations for Tensor Networks (TNs), a broader class of models than previously studied. It introduces a general framework for computing exact SHAP for TNs with arbitrary structures and shows that when TNs are restricted to Tensor Train (TT) structures, SHAP computation can be performed in poly-logarithmic time using parallel computation. This result extends to various ML models including decision trees, tree ensembles, linear models, and linear RNNs, significantly tightening known complexity bounds.

## Method Summary
The paper reformulates SHAP computation as a tensor contraction problem between a weighted coalitional tensor and a marginal value tensor. For Tensor Trains, this contraction can be parallelized efficiently using parallel scan strategies, achieving poly-logarithmic time complexity. For Binarized Neural Networks, the analysis reveals that width, rather than depth, is the primary computational bottleneck, with fixed-width networks becoming Fixed-Parameter Tractable while fixed-depth networks remain hard.

## Key Results
- Exact SHAP computation for Tensor Trains belongs to NC² complexity class, enabling poly-logarithmic parallel computation
- Width, not depth, is identified as the primary computational bottleneck for BNNs, with width-constrained networks becoming FPT
- The framework extends to trees, tree ensembles, linear models, and linear RNNs, tightening existing complexity bounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exact SHAP values can be computed by contracting two specific tensors: a weighted coalitional tensor and a marginal value tensor.
- **Mechanism:** The paper reformulates the exponential SHAP sum into a tensor operation. By defining a "modified Weighted Coalitional tensor" ($\tilde{W}$) and a "Marginal Value tensor" ($V$), the SHAP value becomes a contraction of these two tensors (Prop 1). This moves the complexity from the summation loop to the structural properties of the tensors involved.
- **Core assumption:** The model and distribution can be represented as Tensor Networks (TNs), and the contraction of these specific tensors is computationally feasible.

### Mechanism 2
- **Claim:** If the model and distribution are restricted to Tensor Train (TT) structures, SHAP computation is efficiently parallelizable (belongs to complexity class NC).
- **Mechanism:** When the model and distribution are TTs, the Marginal SHAP Tensor also admits a TT representation. Since matrix multiplication and TT contraction can be solved via parallel scan strategies, the entire SHAP computation can be performed in poly-logarithmic time relative to the input dimension, given enough parallel processors.
- **Core assumption:** The hardware supports massive parallelism (PRAM model), and the TT ranks (bond dimensions) remain manageable.

### Mechanism 3
- **Claim:** Network width, rather than depth, is the primary computational bottleneck for SHAP in Binarized Neural Networks (BNNs).
- **Mechanism:** This is derived via reduction. The paper shows BNNs can be compiled into TNs. The analysis reveals that fixing width (and sparsity) renders the problem Fixed-Parameter Tractable (FPT), whereas fixing depth does not (remains para-NP-Hard). The "width" effectively controls the size of the state space in the equivalent automata/Tensor Train representation.
- **Core assumption:** The BNN uses a reified cardinality representation (weights $\pm 1$), and the analysis holds under specific distribution classes (empirical, independent, TT).

## Foundational Learning

- **Concept:** **Tensor Trains (TT / Matrix Product States)**
  - **Why needed here:** This is the specific architecture for which the paper proves efficient parallel SHAP computation. Without understanding the linear chain structure of cores, the "NC complexity" result is opaque.
  - **Quick check question:** Can you explain why a linear chain of tensor contractions is easier to parallelize than a general graph contraction?

- **Concept:** **Parameterized Complexity (FPT vs. XP vs. para-NP)**
  - **Why needed here:** Crucial for interpreting the BNN results. The paper claims tractability not by saying "it's fast," but by classifying the problem based on structural parameters (width/depth).
  - **Quick check question:** If a problem is FPT with respect to width, what happens to the runtime if I increase depth but keep width constant?

- **Concept:** **The "Coalitional" Game in SHAP**
  - **Why needed here:** To understand why the "Weighted Coalitional Tensor" is necessary. It encodes the exponential number of feature subsets (coalitions) that usually makes SHAP expensive.
  - **Quick check question:** Why does the standard SHAP calculation require evaluating $2^N$ subsets, and how does a tensor representation avoid explicitly listing them?

## Architecture Onboarding

- **Component map:** Input Model/Tensor $T^M$ -> Distribution Tensor $T^P$ -> Tensorizer ($\tilde{W}$, $V$) -> Contractor (parallel TT contraction) -> Interpreter (SHAP matrix $\Phi$)
- **Critical path:** The conversion of the input model into a **Tensor Train (TT)**. If the model cannot be reduced to a TT (or a product of LDFAs for BNNs) with low rank, the complexity guarantees fail. For BNNs, the *compilation* step (converting layers to TTs) is the bottleneck.
- **Design tradeoffs:**
  - **Exactness vs. Generality:** You get exact, parallel SHAP, but only for models that fit the TT structure (or BNNs with constrained width).
  - **Space vs. Time:** The parallel algorithm uses $O(n_{in}^5)$ processors to achieve $O(\log^2 n)$ time. It trades sequential time for massive hardware space.
- **Failure signatures:**
  - **Runtime Explosion:** Attempting this on a "dense" generic neural network without BNN constraints or TT structure will fail (Prop 2 says it is #P-Hard).
  - **Memory Overflow:** If BNN width is not fixed, the state space of the equivalent LDFA/TT grows exponentially, causing memory issues during compilation.
- **First 3 experiments:**
  1. **Sanity Check (Tree):** Convert a small Decision Tree into a TT and verify that the parallel SHAP output matches standard TreeSHAP.
  2. **Stress Test (BNN Width):** Compile BNNs of increasing width (fixed depth) to empirically validate the FPT runtime curve (should grow polynomially if width is fixed, exponentially otherwise).
  3. **Parallel Scaling:** Implement the TT contraction on a GPU/parallel hardware and measure the scaling against sequence length $n_{in}$ to verify the poly-logarithmic claim.

## Open Questions the Paper Calls Out
- Can the complexity results, specifically efficient parallelizability (membership in NC), be extended from Tensor Trains to Tree Tensor Networks (TTNs)?
- Do the tractability and parallelizability results for Marginal SHAP transfer to other explanation variants, such as Conditional SHAP or other attribution indices?
- Is width the primary computational bottleneck for exact SHAP computation in non-binarized (standard) neural networks, similar to the findings for BNNs?

## Limitations
- Restrictive model class: exact results only hold for Tensor Trains and certain binarized networks
- Hardware assumptions are idealized (PRAM model with $O(n^5)$ processors)
- Fixed-width BNN result relies on reified cardinality representation that may not capture all practical variants

## Confidence
- **High Confidence:** The complexity classification (TT ∈ NC²) and the fixed-parameter tractability results for BNNs are supported by formal proofs in the paper. The tensor formulation of SHAP is mathematically rigorous.
- **Medium Confidence:** The empirical evaluation is limited. While the theoretical framework is sound, practical performance on real-world models and the impact of numerical precision during tensor contractions remain unverified.
- **Low Confidence:** The paper's claims about "provably exact" SHAP for general TNs require careful scrutiny of the tensor representation step, as errors in compiling arbitrary models to TNs could propagate through the framework.

## Next Checks
1. **Empirical Complexity Validation:** Implement the TT contraction algorithm on synthetic models of varying width/depth to empirically verify the predicted complexity classes (polynomial in width, exponential in depth).
2. **Numerical Stability Test:** Evaluate the impact of floating-point precision on SHAP values for large input dimensions and deep TT structures, comparing against high-precision arithmetic.
3. **Cross-Model Consistency:** Verify that the SHAP values computed via the TN framework match ground-truth SHAP (brute-force or specialized algorithms) for a suite of models: small decision trees, linear models, and width-constrained BNNs.