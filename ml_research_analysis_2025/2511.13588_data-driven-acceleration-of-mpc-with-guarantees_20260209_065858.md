---
ver: rpa2
title: Data-driven Acceleration of MPC with Guarantees
arxiv_id: '2511.13588'
source_url: https://arxiv.org/abs/2511.13588
tags:
- policy
- control
- optimal
- feasibility
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a data-driven approach to accelerate Model
  Predictive Control (MPC) by replacing online optimization with a nonparametric policy
  constructed from offline MPC solutions. The method builds a policy that is greedy
  with respect to a constructed upper bound on the optimal cost-to-go, enabling implementation
  as a nonparametric lookup rule that is orders of magnitude faster than solving MPC
  online.
---

# Data-driven Acceleration of MPC with Guarantees

## Quick Facts
- arXiv ID: 2511.13588
- Source URL: https://arxiv.org/abs/2511.13588
- Reference count: 40
- One-line primary result: Nonparametric MPC policy achieves 100-1000x speedup with bounded suboptimality

## Executive Summary
This paper presents a data-driven approach to accelerate Model Predictive Control (MPC) by replacing online optimization with a nonparametric lookup policy constructed from offline MPC solutions. The method builds a policy that is greedy with respect to a constructed upper bound on the optimal cost-to-go, enabling implementation as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. The policy achieves recursive feasibility and provable bounded optimality gaps under sufficient coverage conditions of the offline data.

Experiments demonstrate the approach is 100-1000 times faster than standard MPC with only modest optimality degradation, showing potential for real-time control tasks. The framework establishes an explicit trade-off between data collection and bound tightness, providing theoretical guarantees while achieving substantial computational speedups.

## Method Summary
The method constructs a nonparametric policy from offline MPC solutions by solving conservative MPC over an eroded constraint set X^{-ε} = X ⊖ B_ε. For each state x_i in a uniform grid over X, the optimal trajectory and cost J_i are computed and stored. At runtime, the policy π_D(x) selects the action u_ι that minimizes J_i + λ||x - x_i|| over the dataset, where λ is derived from Lipschitz constants of the system dynamics and cost-to-go function. This creates a greedy policy with respect to an upper bound on the optimal cost, ensuring recursive feasibility and bounded suboptimality when the dataset provides sufficient coverage of the state space.

## Key Results
- The nonparametric policy achieves 100-1000x speedup compared to online MPC solving
- Recursive feasibility is guaranteed over the original constraint set X under sufficient data coverage
- Suboptimality gap is bounded by β, with the trade-off between coverage and bound tightness explicitly characterized
- The method maintains feasibility even with imperfect data coverage through ε-Lipschitz continuity of the MPC controller

## Why This Works (Mechanism)
The approach works by leveraging the Lipschitz continuity of MPC controllers and the cost-to-go function to construct an upper bound that can be greedily optimized without online solving. By solving MPC offline on an eroded constraint set and storing the solutions, the policy can be implemented as a fast nearest-neighbor lookup. The key insight is that when the dataset provides sufficient coverage (forming an ε/L_f-cover of X), any state can be reached from a nearby stored state within one step while maintaining feasibility.

## Foundational Learning

**Eroded constraint sets (X^{-ε})**: Used to ensure conservative feasibility margins. Needed because online MPC actions must be certified feasible over the full horizon. Quick check: Verify that the erosion parameter ε is large enough to account for system dynamics and control bounds.

**Lipschitz continuity of MPC controllers**: Ensures smooth mapping from states to actions, enabling upper bound construction. Needed for theoretical guarantees on feasibility and optimality. Quick check: Estimate L_f and verify that stored states form sufficient coverage.

**Cost-to-go upper bound construction**: Creates a surrogate objective for greedy policy optimization. Needed because direct cost evaluation would require solving MPC online. Quick check: Verify that λ ≥ ((1+γL_f)/(1-γL_f))L_J for valid upper bound.

**ε/L_f-coverage**: Quantifies the required density of stored states for feasibility guarantees. Needed to ensure that any state can be reached from a nearby stored state. Quick check: Verify that the dataset forms an ε/L_f-cover by checking max distance to nearest neighbor.

## Architecture Onboarding

**Component map**: Offline MPC solver -> State-action-cost dataset -> Nonparametric policy lookup -> Real-time control

**Critical path**: Data collection (offline MPC solves) → Dataset construction → Policy inference (nearest-neighbor search) → Control execution

**Design tradeoffs**: The method trades increased offline computation (solving many MPC problems) for reduced online computation (lookup vs optimization). Larger datasets improve coverage and bound tightness but increase storage and query time. The erosion parameter ε controls the feasibility margin versus control authority.

**Failure signatures**: 
- Infeasibility occurs when query states are too far from any stored state
- Poor performance near origin due to division by small costs in relative gap metric
- Suboptimality increases when data coverage is sparse or erosion is too conservative

**First experiments**:
1. Verify that the inverted pendulum environment dynamics match the stated equations and constraints
2. Test the MPC solver on a single state to ensure it produces feasible trajectories with the specified cost structure
3. Implement the nearest-neighbor policy with FAISS and verify that inference is significantly faster than MPC solving

## Open Questions the Paper Calls Out

**Open Question 1**: Can the proposed nonparametric framework maintain its computational speedup and guarantee robustness when scaled to high-dimensional systems? The experiments are limited to low-dimensional examples (n=2), and nonparametric methods typically suffer from the curse of dimensionality regarding data requirements and query latency.

**Open Question 2**: How does relaxing the single-step feasibility constraint (K=1) to a multi-step constraint (K>1) affect the required erosion margin ε and sample complexity? The K=1 assumption requires the system to "jump" into the eroded set X^{-ε} immediately, which may be restrictive for systems with limited control authority.

**Open Question 3**: Can the verification complexity of Algorithm 2 be reduced without compromising the strict recursive feasibility guarantees? Algorithm 2 requires splitting cells into 3^n children to maintain the central trajectory, which becomes computationally expensive as dimensionality increases.

## Limitations

- The method requires solving many MPC problems offline, which may be computationally expensive for complex systems
- Strong coverage conditions are required for theoretical guarantees, which may be difficult to verify in practice
- The approach is limited to low-dimensional systems in the current implementation
- Exact parameter values (erosion ε, Lipschitz constants) are not fully specified

## Confidence

**High confidence**: Core theoretical framework and asymptotic guarantees under stated assumptions are sound and well-established.

**Medium confidence**: Practical performance claims given the strong dependence on data coverage quality and parameter tuning; experimental results appear consistent but may not generalize.

**Low confidence**: Reproducibility without access to exact implementation details and parameter values; unknown erosion parameter and coverage analysis methodology.

## Next Checks

1. Verify the erosion parameter ε and coverage radius r(x) used in experiments through direct communication with authors or detailed parameter sweeps.

2. Implement systematic coverage analysis to quantify how well the dataset represents the state space and identify regions where the policy may fail.

3. Benchmark the approach against multiple MPC solvers and compare both computation time and solution quality across different problem scales and complexities.