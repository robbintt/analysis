---
ver: rpa2
title: 'Pragmatics beyond humans: meaning, communication, and LLMs'
arxiv_id: '2508.06167'
source_url: https://arxiv.org/abs/2508.06167
tags:
- pragmatic
- language
- llms
- pragmatics
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper reframes pragmatics as a dynamic interface for socially
  embedded language use, challenging the traditional semiotic trichotomy and proposing
  the Human-Machine Communication (HMC) framework as a more suitable alternative.
  It identifies three main tensions: the mismatch between human-centered pragmatic
  theories and LLM capabilities, the issue of substitutionalism (linguistic, communicative,
  and generalizing), and the paradox of increased contextual input paired with diminished
  contextual understanding, termed "context frustration." The paper argues that probabilistic
  pragmatics and the Rational Speech Act framework better align with LLM architecture,
  while highlighting the need to consider humans as active participants in hybrid
  human-LLM interactions.'
---

# Pragmatics beyond humans: meaning, communication, and LLMs

## Quick Facts
- arXiv ID: 2508.06167
- Source URL: https://arxiv.org/abs/2508.06167
- Authors: Vít Gvoždiak
- Reference count: 0
- One-line primary result: LLMs require rethinking pragmatics beyond human-centered frameworks, favoring probabilistic over truth-conditional approaches.

## Executive Summary
This paper reframes pragmatics as a dynamic interface for socially embedded language use, challenging the traditional semiotic trichotomy and proposing the Human-Machine Communication (HMC) framework as a more suitable alternative. It identifies three main tensions: the mismatch between human-centered pragmatic theories and LLM capabilities, the issue of substitutionalism (linguistic, communicative, and generalizing), and the paradox of increased contextual input paired with diminished contextual understanding, termed "context frustration." The paper argues that probabilistic pragmatics and the Rational Speech Act framework better align with LLM architecture, while highlighting the need to consider humans as active participants in hybrid human-LLM interactions. The core conclusion is that pragmatic theory must be adjusted to account for the unique nature of communication involving generative AI.

## Method Summary
This theoretical paper reconceptualizes pragmatics for human-LLM communication by proposing conceptual frameworks (HMC, probabilistic pragmatics/RSA) rather than conducting empirical experiments. It references existing pragmatic benchmarks (MultiPragEval, PUB, AmbigQA, SIGA) and studies evaluating LLMs on implicatures, speech acts, impoliteness detection, and reference resolution. The paper advocates for probabilistic/teleological metrics (frequency sensitivity, output probability, input probability) over human-centered evaluation criteria (truth-conditions, Gricean cooperation). While RSA framework is recommended for implementation (literal listener → pragmatic speaker → pragmatic listener pipeline with utility maximization), specific implementation details are not provided.

## Key Results
- Probabilistic pragmatics (RSA framework) aligns better with LLM architecture than truth-conditional Gricean pragmatics by focusing on optimization rather than truth-evaluation
- Human-LLM interactions exhibit "context frustration"—increased contextual input paired with diminished contextual understanding due to incommensurable human and machine presupposition sets
- Substitutionalism (generalizing, linguistic, communicative) distorts LLM pragmatic evaluation by obscuring human behavioral adaptation in hybrid interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic pragmatics (RSA framework) aligns better with LLM architecture than truth-conditional Gricean pragmatics.
- Mechanism: RSA models communication as recursive probabilistic inference between literal listener, pragmatic speaker, and pragmatic listener, mirroring how LLMs optimize token probabilities rather than evaluate propositional truth.
- Core assumption: LLMs are fundamentally prediction machines whose "norm is word occurrence probability, not truth."
- Evidence anchors: Abstract states RSA "offers a more compatible teleology by focusing on optimization rather than truth-evaluation"; section 2 describes RSA's three-part structure replacing Grice's maxims with utility-theoretic cooperative principle.

### Mechanism 2
- Claim: Human-LLM interactions exhibit "context frustration"—increased contextual input paired with diminished contextual understanding.
- Mechanism: LLM context windows expand but human and machine "presupposition sets" remain incommensurable; following Stalnaker, human-human defective contexts converge toward equilibrium while human-LLM contexts cannot.
- Core assumption: Technical context (token window, training data) differs fundamentally from pragmatic context (shared background assumptions).
- Evidence anchors: Abstract introduces "context frustration" as paradox of increased input with diminished understanding; section 4 describes human-LLM communication as hybrid, reshaping structural topology.

### Mechanism 3
- Claim: Substitutionalism distorts LLM pragmatic evaluation by obscuring human behavioral adaptation.
- Mechanism: Researchers substitute specific models for "LLMs in general," English for all languages, and LLMs for humans in conversational roles, producing machine-centered research that benchmarks against static human standards.
- Core assumption: Humans in hybrid interactions are active interlocutors whose behavior is "shaped and modulated by the very structure of these hybrid exchanges."
- Evidence anchors: Section 3 defines three forms of substitutionalism; Yakura et al. study shows human spoken language mimicking LLM-like patterns in YouTube corpora post-2023.

## Foundational Learning

- Concept: **Semiotic trichotomy** (Morris/Carnap/Kaplan hierarchy: syntax → semantics → pragmatics)
  - Why needed here: The paper argues LLMs destabilize this hierarchy; understanding it clarifies what's being challenged.
  - Quick check question: Can you explain why rejecting semantic competence in LLMs traditionally entails rejecting pragmatic competence?

- Concept: **Rational Speech Act (RSA) framework**
  - Why needed here: Proposed as better-aligned alternative to Gricean pragmatics for LLM evaluation.
  - Quick check question: Describe the three components (literal listener, pragmatic speaker, pragmatic listener) and their recursive relationship.

- Concept: **Stalnakerian context** (propositional: shared presuppositions/possible worlds) vs. **Lewisian context** (locational: time, place, possible world)
  - Why needed here: Distinguishes technical context from pragmatic context; underpins context frustration concept.
  - Quick check question: Why does Stalnaker claim defective contexts in human-human communication converge, but human-LLM contexts cannot?

## Architecture Onboarding

- Component map:
  - HMC framework (Guzman/Lewis): Functional (AI as conversational partner), Relational (human role negotiation), Metaphysical (human vs. tool distinction)
  - RSA pipeline: Literal listener (semantic mapping) → Pragmatic speaker (utility estimation) → Pragmatic listener (Bayesian inference over intended meanings)
  - Context layers: Technical (context window, training corpus) vs. Pragmatic (presupposition sets, localization parameters)

- Critical path:
  1. Identify whether task requires truth-conditional or probabilistic evaluation
  2. If probabilistic, map task onto RSA components (who is speaker/listener, what is utility function)
  3. Explicitly model human and machine presupposition sets separately—do not assume convergence
  4. Check for substitutionalism: Is model selection justified? Is language selection justified? Are you measuring human adaptation or just model performance?

- Design tradeoffs:
  - Multiple-choice evaluation → easy comparison, but "model might correctly select the option label [while] still fail[ing] to respond pragmatically by itself"
  - Open-ended evaluation → better captures context-sensitive inference, but harder to interpret consistently
  - Isolated prompts vs. discourse context → isolated success may reflect training data patterns, not genuine reasoning

- Failure signatures:
  - Treating context window expansion as equivalent to contextual understanding (context frustration)
  - Generalizing from single model or single language to "LLMs" or "language" (substitutionalism)
  - Evaluating LLMs against human benchmarks without measuring human behavioral changes in hybrid interactions
  - Applying Gricean cooperative assumptions to systems that optimize probability, not truth

- First 3 experiments:
  1. **RSA alignment test**: Implement RSA-style literal listener → pragmatic speaker → pragmatic listener pipeline; compare model outputs on implicature resolution against both truth-conditional and probability-optimized baselines
  2. **Context frustration probe**: Measure human user presupposition calibration across conversations with varying context window sizes; test whether expanded windows reduce or increase repair turns
  3. **Substitutionalism audit**: Run identical pragmatic evaluation across (a) multiple model families/sizes, (b) multiple languages, (c) human vs. LLM in same conversational role; quantify variance attributable to each substitution axis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does human linguistic behavior change when engaging with LLMs as speakers, and what interpretative strategies do humans apply (or not apply) as addressees in hybrid human-AI exchanges?
- Basis in paper: The paper states "very few empirical studies have so far addressed questions, how human linguistic behavior changes when engaging with LLMs as speakers, or what interpretative strategies humans do (or do not) apply as addressees."
- Why unresolved: Current research substitutes humans with LLMs in experimental designs, overlooking how humans adapt their own communicative behavior when interacting with AI systems.
- What evidence would resolve it: Longitudinal corpus studies tracking human linguistic changes in AI-mediated communication; experimental comparisons of human pragmatic behavior across AI vs. human interlocutor conditions.

### Open Question 2
- Question: Can the Rational Speech Act (RSA) framework provide empirically validated predictions of LLM pragmatic behavior that outperform traditional Gricean frameworks?
- Basis in paper: The paper argues probabilistic pragmatics and RSA "offers a more compatible teleology" for LLMs by "focusing on optimization rather than truth-evaluation," aligning with their probabilistic architecture.
- Why unresolved: The paper proposes RSA as theoretically better suited but does not demonstrate systematic empirical validation across diverse pragmatic phenomena.
- What evidence would resolve it: Comparative studies testing RSA predictions against both Gricean predictions and actual LLM outputs on implicature resolution, reference, and speech act tasks.

### Open Question 3
- Question: How does "context frustration" manifest empirically, and what are its measurable effects on human communicative behavior and task success?
- Basis in paper: The paper introduces "context frustration" as "the experiential dissonance felt by human users who engage in exchanges where contextual alignment repeatedly fails."
- Why unresolved: This newly proposed concept lacks empirical operationalization and validation.
- What evidence would resolve it: User studies correlating context window expansion with perceived alignment failure; analysis of user repair strategies and conversation abandonment rates.

### Open Question 4
- Question: What systematic criteria should guide LLM selection for pragmatic evaluation to avoid the problem of "generalizing substitutionalism"?
- Basis in paper: The paper notes selection criteria are "applied in a rather unsystematic fashion" and that "there is a lack of systematic evaluation across various models."
- Why unresolved: Current research lacks standardized criteria for model selection in pragmatic evaluation.
- What evidence would resolve it: Cross-model benchmarking studies identifying which pragmatic capabilities generalize across architectures, sizes, and training regimes.

## Limitations
- The paper operates at a meta-theoretical level without presenting empirical validation of its proposed frameworks
- RSA alignment mechanism rests on analogical reasoning about architectural similarities rather than demonstrated behavioral evidence
- Context frustration phenomenon is conceptually compelling but lacks direct empirical support in the cited corpus

## Confidence
- RSA alignment mechanism: Medium confidence (based on analogical reasoning about architectural similarities)
- Context frustration phenomenon: Medium confidence (conceptually compelling but lacking direct empirical support)
- Substitutionalism critique: High confidence (well-grounded in documented evaluation practices)

## Next Checks
1. Conduct controlled experiments comparing RSA-style probabilistic evaluation against traditional Gricean approaches on identical pragmatic tasks, measuring alignment with LLM probability distributions
2. Design longitudinal studies tracking human presupposition convergence (or lack thereof) in human-LLM dialogues across varying context window sizes
3. Implement systematic audits of LLM pragmatic evaluation literature, cataloging substitutionalism patterns across model selection, language coverage, and reference behavior assumptions