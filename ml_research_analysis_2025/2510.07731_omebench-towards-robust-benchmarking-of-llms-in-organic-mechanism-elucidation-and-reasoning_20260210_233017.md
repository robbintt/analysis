---
ver: rpa2
title: 'oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation
  and Reasoning'
arxiv_id: '2510.07731'
source_url: https://arxiv.org/abs/2510.07731
tags:
- reaction
- type
- step
- mechanism
- chemical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: oMeBench is the first large-scale, expert-annotated benchmark for
  organic reaction mechanism reasoning, containing over 10,000 annotated mechanistic
  steps with intermediates, step-type labels, and rationales. To enable fine-grained
  evaluation, the authors propose oMeS, a dynamic scoring framework that aligns predicted
  and gold-standard mechanisms using weighted similarity metrics.
---

# oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning

## Quick Facts
- arXiv ID: 2510.07731
- Source URL: https://arxiv.org/abs/2510.07731
- Reference count: 40
- oMeBench is the first large-scale, expert-annotated benchmark for organic reaction mechanism reasoning, containing over 10,000 annotated mechanistic steps with intermediates, step-type labels, and rationales

## Executive Summary
oMeBench introduces the first large-scale, expert-annotated benchmark for organic reaction mechanism reasoning, containing over 10,000 annotated mechanistic steps. The authors propose oMeS, a dynamic scoring framework that aligns predicted and gold-standard mechanisms using weighted similarity metrics. Systematic evaluation of 14 LLMs reveals that while current models display promising chemical intuition, they struggle with correct and consistent multi-step reasoning. Using exemplar-based in-context learning and supervised fine-tuning, the authors improve performance by up to 50% over the leading closed-source model.

## Method Summary
oMeBench provides a comprehensive evaluation framework for organic mechanism elucidation. The benchmark includes three datasets: oMe-Gold (196 reactions, 858 expert-verified steps), oMe-Template (167 templates), and oMe-Silver (2,508 reactions, 10,619 steps for training). The oMeS scoring framework uses Tanimoto similarity on Morgan fingerprints (≥0.60 for partial credit), SMILES validity checks via RDKit, and weighted Needleman-Wunsch alignment to compare predicted and gold mechanisms. The authors employ both in-context learning with DRFP-based retrieval and supervised fine-tuning using LLaMA-Factory with DeepSpeed ZeRO-3, achieving up to 50% performance improvement over baseline models.

## Key Results
- Current LLMs struggle with multi-step organic mechanism reasoning despite showing chemical intuition
- Exemplar-based in-context learning and supervised fine-tuning improve performance by up to 50% over leading closed-source models
- Models exhibit high SMILES invalidity rates (35.4% of errors) and structural inconsistencies in mechanism generation

## Why This Works (Mechanism)
The benchmark works by providing expert-annotated gold-standard mechanisms with detailed intermediate structures, step types, and rationales. The oMeS framework captures both exact matches and chemically similar predictions through Morgan fingerprint similarity, allowing for nuanced evaluation beyond binary correctness. By incorporating step weights and using dynamic alignment algorithms, the scoring reflects the true difficulty and importance of different mechanistic steps.

## Foundational Learning
- **Morgan fingerprints**: Circular topological fingerprints used for molecular similarity comparison; needed because they capture chemical similarity beyond exact structure matching; quick check: RDKit's GetMorganFingerprint generates these with radius=2, length=2048
- **Needleman-Wunsch alignment**: Dynamic programming algorithm for sequence alignment; needed to optimally match predicted and gold mechanism steps despite ordering differences; quick check: BioPython's pairwise2.align.globalxx implements this
- **SMILES notation**: Text-based molecular structure representation; needed as the standard input/output format for LLM-based chemical reasoning; quick check: RDKit's MolFromSmiles validates SMILES strings
- **DeepSpeed ZeRO-3**: Memory optimization technique for large-scale model training; needed to enable fine-tuning of large LLMs on commodity hardware; quick check: DeepSpeed configuration specifies optimizer and memory settings
- **BF16 precision**: BFloat16 floating-point format; needed for training stability while reducing memory requirements; quick check: PyTorch's torch.bfloat16 specifies this precision

## Architecture Onboarding

**Component Map**: Input JSON/JSONL (Reactants/Products/Conditions) -> LLM Generation -> SMILES Output -> RDKit Validation -> Morgan Fingerprint Comparison -> Needleman-Wunsch Alignment -> oMeS Scoring

**Critical Path**: Reactant/product/condition input → LLM mechanism generation → intermediate SMILES prediction → oMeS similarity scoring → final performance metrics

**Design Tradeoffs**: SMILES-based evaluation enables automated scoring but loses 3D geometric information; Morgan fingerprint similarity captures chemical similarity but may miss subtle mechanistic differences; weighted alignment prioritizes important steps but requires expert-annotated weights

**Failure Signatures**: High SMILES invalidity rates (35.4% observed); step omission in long mechanisms (>5 steps); structural errors like valence violations; CoT prompting fails to provide structural understanding

**3 First Experiments**:
1. Validate RDKit SMILES parsing on oMe-Gold dataset to establish baseline validity
2. Implement oMeS scoring with Morgan fingerprints and test on a small set of predicted mechanisms
3. Fine-tune LLaMA-Factory on oMe-Silver using provided hyperparameters and measure SMILES validity improvement

## Open Questions the Paper Calls Out

**Open Question 1**: Can current mechanistic reasoning capabilities generalize to modern reaction classes like photoredox or enzymatic catalysis, or do these require fundamentally different training data and architectural inductive biases?
- Basis in paper: The Limitations section states, "Expanding the dataset to modern reaction classes, such as photoredox, enzymatic, and transition-metal catalysis, would enable broader and more realistic benchmarking."
- Why unresolved: The current benchmark focuses on classical organic mechanisms; performance on the distinct electronic and steric constraints of transition-metal or enzymatic systems remains untested.
- What evidence would resolve it: Evaluation of frontier models on a benchmark extension containing transition-metal catalysis and enzymatic reaction mechanisms.

**Open Question 2**: How can domain-specific pretraining objectives be redesigned to capture causal mechanistic logic rather than just surface-level chemical patterns?
- Basis in paper: Section 5.2 notes that chemistry-trained models like ChemDFM "underperform" general models and suggests they "capture surface-level chemical patterns rather than causal mechanistic logic."
- Why unresolved: The paper identifies the failure mode (domain exposure ≠ reasoning) but does not propose a new pretraining loss or curriculum to enforce causal stepwise understanding.
- What evidence would resolve it: A new domain-specific model trained with a causality-aware objective that outperforms general-purpose LLMs on the oMeBench benchmark.

**Open Question 3**: What evaluation frameworks are required to capture 3D geometric reasoning capabilities that are invisible to current SMILES-based metrics?
- Basis in paper: The Limitations section argues that oMeS "abstracts molecules into symbolic SMILES representations, which cannot fully capture three-dimensional environments."
- Why unresolved: Current metrics (Tanimoto similarity on fingerprints) treat molecules as 2D graphs/topologies, potentially missing steric and conformational nuances critical to mechanism.
- What evidence would resolve it: Development of a geometry-based similarity metric (e.g., utilizing conformer ensembles) that correlates better with expert human evaluation than fingerprint-based scores.

**Open Question 4**: How can prompting strategies be modified to enforce chemical conservation laws (valence/charge) and structural consistency in multi-step reasoning?
- Basis in paper: Section 5.3 identifies "Structural and Formalization Errors" (e.g., valence violations) and notes that standard Chain-of-Thought "fails to provide" structural understanding, implying a need for "chemistry-aware supervision."
- Why unresolved: The paper shows CoT fails and models hallucinate charges/valences, but does not solve the integration of hard chemical constraints into generative LLMs.
- What evidence would resolve it: A prompting strategy or constrained decoding method that reduces invalid SMILES and valence violations to near-zero while maintaining reasoning complexity.

## Limitations
- Current benchmark focuses on classical organic mechanisms, excluding modern reaction classes like photoredox and enzymatic catalysis
- SMILES-based evaluation cannot capture 3D geometric reasoning capabilities critical for mechanistic understanding
- Chain-of-Thought prompting fails to provide structural understanding and results in high rates of valence and charge violations

## Confidence

**High confidence**: Benchmark's utility and general approach to mechanism evaluation
**Medium confidence**: Reproducibility of oMeS scoring framework with provided specifications
**Medium confidence**: Fine-tuning procedure, though exact results may vary without full prompt templates
**Low confidence**: Exact replication of in-context learning results due to missing DRFP implementation details

## Next Checks
1. Implement the oMeS scoring framework with Morgan fingerprints (Tanimoto ≥0.60) and validate against the provided oMe-Gold dataset examples
2. Reproduce the LLaMA-Factory fine-tuning on oMe-Silver using the provided hyperparameters and verify SMILES validity rates
3. Test the oMeS framework on a small set of mechanism predictions from a different LLM to validate scoring consistency across models