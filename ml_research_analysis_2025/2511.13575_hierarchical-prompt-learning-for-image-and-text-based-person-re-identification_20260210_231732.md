---
ver: rpa2
title: Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification
arxiv_id: '2511.13575'
source_url: https://arxiv.org/abs/2511.13575
tags:
- prompt
- person
- learning
- alignment
- reid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hierarchical Prompt Learning (HPL), a unified
  framework for jointly optimizing image-to-image (I2I) and text-to-image (T2I) person
  re-identification (ReID). The method addresses semantic conflicts between I2I (identity
  discrimination) and T2I (fine-grained cross-modal alignment) by integrating identity-level
  learnable tokens with instance-level pseudo-text tokens generated via modality-specific
  inversion networks.
---

# Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification

## Quick Facts
- arXiv ID: 2511.13575
- Source URL: https://arxiv.org/abs/2511.13575
- Authors: Linhan Zhou, Shuang Li, Neng Dong, Yonghang Tai, Yafei Zhang, Huafeng Li
- Reference count: 17
- Primary result: State-of-the-art 76.28% Rank-1 and 70.90% mAP on CUHK-PEDES; 95.99% Rank-1 and 89.82% mAP on Market1501

## Executive Summary
This paper introduces Hierarchical Prompt Learning (HPL), a unified framework for jointly optimizing image-to-image (I2I) and text-to-image (T2I) person re-identification (ReID). The method addresses semantic conflicts between I2I (identity discrimination) and T2I (fine-grained cross-modal alignment) by integrating identity-level learnable tokens with instance-level pseudo-text tokens generated via modality-specific inversion networks. A Task-Routed Transformer with dual classification tokens enables task-specific feature routing, while Cross-Modal Prompt Regularization ensures semantic consistency across modalities. Experiments on six benchmarks (CUHK-PEDES, ICFG-PEDES, RSTPReID, Market1501, MSMT17, DukeMTMC-ReID) show state-of-the-art performance: 76.28% Rank-1 and 70.90% mAP on CUHK-PEDES, and 95.99% Rank-1 and 89.82% mAP on Market1501. Ablation studies confirm the effectiveness of each component in bridging modality gaps and improving joint task optimization.

## Method Summary
Hierarchical Prompt Learning (HPL) is a two-stage framework that unifies image-to-image (I2I) and text-to-image (T2I) person re-identification. In Stage I (Prompt Construction), a shared CLIP visual encoder with dual classification tokens learns identity-level prompts while inversion networks generate instance-level pseudo-text tokens from both visual and textual features. In Stage II (Representation Learning), the model jointly optimizes cross-modal alignment and identity discrimination using a combination of base losses, instance-level prompt alignment, and cross-modal regularization. The method uses a hierarchical prompt template "A photo of [id-tokens] and [inst-tokens] person" to integrate both semantic levels, with cross-modal regularization ensuring consistency between visual- and text-derived pseudo-prompts.

## Key Results
- Achieves state-of-the-art 76.28% Rank-1 and 70.90% mAP on CUHK-PEDES
- Achieves state-of-the-art 95.99% Rank-1 and 89.82% mAP on Market1501
- Dual classification tokens improve T2I by 1.07% Rank-1 and I2I by 2.07% mAP
- Cross-modal prompt regularization adds 1.01% T2I Rank-1 and 0.84% I2I mAP
- Ablation studies confirm effectiveness of instance-level prompts and hierarchical design

## Why This Works (Mechanism)

### Mechanism 1
Dual classification tokens in a shared visual encoder reduce semantic conflict between I2I and T2I tasks by enabling task-specific feature aggregation. The standard ViT class token aggregates context via self-attention guided by task objectives. By appending a second class token, each token specializes—one for T2I alignment (v^t2i) and one for I2I identity discrimination (v^i2i). Separate loss objectives (L_sdm, L_t2i_id for T2I; L_i2i_id, L_tri for I2I) route gradients to the appropriate token without cross-task interference. Core assumption: I2I and T2I tasks require different semantic abstractions from the same visual input, and these can be decoupled through separate attention aggregation paths rather than entirely separate encoders. Evidence anchors: [abstract]: "Task-Routed Transformer with dual classification tokens enables task-specific feature routing"; [section]: "As shown in Table 3, introducing a dual class token design leads to a 1.07% improvement in T2I Rank-1 accuracy and a 2.07% increase in I2I mAP". Break condition: If tasks share identical semantic requirements (e.g., both purely identity-based without instance variation), dual tokens provide no benefit and add parameter overhead.

### Mechanism 2
Instance-level pseudo-text tokens generated via modality-specific inversion networks capture fine-grained attributes that static identity-level prompts miss. Inversion networks (I_v, I_t)—4-layer Transformers—map visual features F^v_i or textual features F^t_i to pseudo-text tokens P^v_i, P^t_i. These tokens fill the [inst-tokens] slot in the hierarchical template "A photo of [id-tokens] and [inst-tokens] person." The inversion consistency loss L_ic ensures pseudo-prompts preserve source-modality semantics while remaining in the textual token space that CLIP can process. Core assumption: Instance-specific semantics (actions, accessories, contextual objects) can be linearly or near-linearly projected into the CLIP text token embedding space while retaining discriminative information. Evidence anchors: [abstract]: "instance-level pseudo-text tokens generated via modality-specific inversion networks"; [section]: "As shown in Table 4, both [uni-modal prompts] achieve gains over the baseline, confirming the benefit of sample-level guidance". Break condition: If inversion networks collapse to identity-agnostic representations or fail to preserve instance-specific cues, the pseudo-tokens provide no marginal benefit over identity-level prompts alone.

### Mechanism 3
Cross-Modal Prompt Regularization (CMPR) reduces semantic drift between visual-derived and text-derived pseudo-prompts, improving cross-modal generalization. CMPR directly aligns P^t_i and P^v_i in token space via Frobenius norm loss L_CMPR = (1/|B|) ||P^t_i - P^v_i||²_F. This ensures that an image and its paired text description produce semantically consistent pseudo-prompts, reducing modality-specific bias that harms T2I grounding. Core assumption: Aligning pseudo-prompts in token space transfers to improved alignment in the final embedding space after text encoding; token-space proximity implies semantic equivalence. Evidence anchors: [abstract]: "Cross-Modal Prompt Regularization ensures semantic consistency across modalities"; [section]: "adding CMPR together with HPL leads to significant performance improvements (+1.01% on T2I Rank1 and +0.84% on I2I mAP)". Break condition: If token-space alignment does not correlate with downstream embedding alignment (e.g., due to non-linear text encoder transformations), CMPR may regularize toward irrelevant directions.

## Foundational Learning

- Concept: **CLIP Vision-Language Pre-training**
  - Why needed here: The entire framework builds on CLIP's pre-aligned vision-text embedding space. Understanding how CLIP maps images and text to a shared space via contrastive learning is essential for grasping why pseudo-text tokens can provide cross-modal supervision.
  - Quick check question: Can you explain why CLIP's image and text encoders produce comparable embeddings, and what objective trains them?

- Concept: **Prompt Learning for Vision Models**
  - Why needed here: The hierarchical prompt "A photo of [id-tokens] and [inst-tokens] person" follows the soft prompt paradigm where learnable tokens are prepended or inserted into frozen model inputs. Without this background, the motivation for learnable [id-tokens] versus generated [inst-tokens] is unclear.
  - Quick check question: What is the difference between hard prompts (discrete text) and soft prompts (continuous embeddings), and why might soft prompts be preferable for ReID?

- Concept: **Feature Inversion / Projection Networks**
  - Why needed here: The inversion networks I_v and I_t project features from one modality's embedding space to pseudo-tokens in another's token space. This is a form of cross-modal translation that requires understanding of projection heads and reconstruction losses.
  - Quick check question: Given a visual feature vector from CLIP's image encoder, how would you design a network to produce a token that CLIP's text encoder can process meaningfully?

## Architecture Onboarding

- Component map:
  Shared CLIP Visual Encoder (M_v) -> Dual [CLS] tokens (v^t2i, v^i2i) -> Task-specific losses
  Patch tokens -> Inversion Networks (I_v, I_t) -> Pseudo-text tokens (P^v_i, P^t_i)
  Text features -> Inversion Network (I_t) -> Pseudo-text tokens (P^t_i)
  Hierarchical prompt constructor -> CLIP Text Encoder (M̃_t) -> Prompt embeddings

- Critical path:
  1. Image → M_v → [v^t2i, v^i2i, patch tokens]
  2. Patch tokens → I_v → P^v_i (visual pseudo-tokens)
  3. Text → M_t → text features → I_t → P^t_i (text pseudo-tokens)
  4. P^v_i, P^t_i + [id-tokens] → hierarchical prompts → M̃_t → prompt embeddings
  5. Align v^t2i with text-guided prompt, v^i2i with identity prompt; regularize P^v_i ≈ P^t_i

- Design tradeoffs:
  - Shared vs. separate encoders: Shared encoder reduces parameters but risks task interference; dual tokens mitigate this at cost of minor overhead.
  - Frozen vs. fine-tuned CLIP: Freezing M_v, M_t, M̃_t during HPC preserves pre-trained knowledge but limits adaptation; Stage II unfreezes for representation learning.
  - Two-stage training: Separates prompt construction from representation learning, improving stability but doubling training time.

- Failure signatures:
  - Task interference persists: If dual tokens do not specialize (check attention divergence), I2I and T2I losses may still conflict. Monitor per-task validation curves separately.
  - Pseudo-tokens collapse to mean: If L_ic is too weak, inversion networks may output near-constant tokens. Check variance of P^v_i across instances.
  - Over-regularization by CMPR: If λ₂ is too high, P^v_i and P^t_i converge to identical values regardless of input, losing instance specificity. Monitor L_CMPR magnitude and pseudo-token diversity.

- First 3 experiments:
  1. **Ablate dual-token routing**: Train with single [CLS] token shared for both tasks; compare Rank-1/mAP on CUHK-PEDES + Market1501 to validate Table 3 results (expect ~1% T2I drop, ~2% I2I mAP drop).
  2. **Inspect pseudo-token quality**: Visualize t-SNE of P^v_i and P^t_i for matched and mismatched image-text pairs; verify that matched pairs cluster closer (sanity check for inversion + CMPR).
  3. **Hyperparameter sweep on λ₁, λ₂**: Replicate Figure 3 sweep; confirm optimal values near λ₁=0.4, λ₂=0.06; observe performance collapse at extremes (λ₁→0 removes instance alignment, λ₂→1 over-constrains prompts).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the quality and granularity of textual descriptions in the training set impact the generalization of the modality-specific inversion networks?
- Basis: [inferred] The Introduction highlights that T2I tasks rely on "instance-specific attributes" (e.g., actions) described in natural language, and the method uses these descriptions to train the inversion networks.
- Why unresolved: The experiments utilize standard benchmarks with high-quality annotations, but real-world text queries are often brief or noisy, which may degrade the alignment learned by the inversion networks.
- What evidence would resolve it: Evaluations on datasets with synthetically degraded or reduced textual annotations to measure the robustness of the pseudo-token generation.

### Open Question 2
- Question: Is the manually designed hierarchical template ("A photo of [id-tokens] and [inst-tokens] person") the optimal structure for capturing the relationship between identity and instance semantics?
- Basis: [inferred] The "Hierarchical Prompt Construction" section fixes the prompt structure to integrate identity and instance levels, without exploring if this syntax best suits the CLIP text encoder.
- Why unresolved: The paper does not ablate different prompt templates or evaluate if a flexible, automatically generated structure could better represent complex semantic interactions.
- What evidence would resolve it: Ablation studies comparing the fixed template against dynamic or variable-length prompt structures.

### Open Question 3
- Question: Can the learned instance-level pseudo-tokens be interpreted or mapped back to discrete natural language attributes to explain the model's retrieval decisions?
- Basis: [inferred] The method generates instance-level prompts via inversion networks ($I_v$ and $I_t$) to align features, but only retrieval accuracy is reported.
- Why unresolved: It is unclear if the pseudo-tokens form a semantically meaningful language space or merely a statistical alignment space, limiting model explainability.
- What evidence would resolve it: Qualitative analysis attempting to decode the pseudo-tokens back into text using a pre-trained language model to verify semantic fidelity.

## Limitations

- **CLIP Backbone Specificity**: The paper uses a CLIP backbone pretrained on "LUPerson and large-scale synthetic image-text pairs" but does not specify the exact model variant or checkpoint, introducing uncertainty about performance transfer to other CLIP variants.
- **Task Interference Generalization**: The method's robustness to more severe task interference scenarios (e.g., datasets with overlapping but semantically distinct identity definitions) remains untested, as current evaluation assumes clean identity label alignment.
- **Pseudo-Token Semantic Preservation**: Limited analysis of whether instance-level pseudo-text tokens actually capture fine-grained attributes versus simply encoding identity-level information, without qualitative analysis or semantic similarity metrics.

## Confidence

**High Confidence (CL ≥ 0.8)**:
- The two-stage training procedure with separate prompt construction and representation learning phases is technically sound and well-supported by the ablation studies.
- The hierarchical prompt template design ("A photo of [id-tokens] and [inst-tokens] person") is a reasonable application of soft prompt learning principles to ReID.

**Medium Confidence (CL 0.6-0.8)**:
- The dual classification token mechanism effectively reduces semantic conflict between I2I and T2I tasks, as evidenced by the 1.07% T2I improvement and 2.07% I2I mAP gain in Table 3.
- Cross-Modal Prompt Regularization improves cross-modal generalization, given the 1.01% T2I Rank-1 and 0.84% I2I mAP improvements when combined with HPL.

**Low Confidence (CL < 0.6)**:
- The instance-level pseudo-text tokens generated by inversion networks provide meaningful fine-grained semantic information beyond identity-level prompts, as the paper lacks detailed analysis of pseudo-token quality and semantic content.
- The method generalizes to datasets with more complex cross-modal alignment challenges or less clean identity label mappings between image and text modalities.

## Next Checks

1. **Dual-Token Specialization Analysis**: Extract attention weights from both [CLS] tokens during inference on CUHK-PEDES and Market1501. Compute attention entropy and cross-token similarity to verify that the T2I token focuses on text-aligned visual regions while the I2I token emphasizes identity-discriminative features. This validates whether the routing mechanism genuinely specializes tokens rather than distributing attention uniformly.

2. **Pseudo-Token Semantic Validation**: For a subset of CUHK-PEDES pairs, decode the generated pseudo-text tokens (P^v_i, P^t_i) using nearest-neighbor text search in CLIP's text embedding space. Measure semantic similarity (e.g., cosine similarity) between pseudo-tokens and ground truth descriptions, and compare against random text samples. This quantifies whether inversion networks produce semantically meaningful tokens versus arbitrary embeddings.

3. **Task Interference Stress Test**: Create a synthetic evaluation protocol where I2I and T2I datasets have partially overlapping but semantically distinct identity definitions (e.g., same person wearing different outfits in different datasets). Evaluate HPL's performance degradation compared to separate I2I and T2I models to measure robustness to semantic conflicts beyond the current clean-label assumption.