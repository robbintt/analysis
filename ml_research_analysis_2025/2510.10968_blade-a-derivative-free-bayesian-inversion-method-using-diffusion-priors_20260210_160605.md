---
ver: rpa2
title: 'Blade: A Derivative-free Bayesian Inversion Method using Diffusion Priors'
arxiv_id: '2510.10968'
source_url: https://arxiv.org/abs/2510.10968
tags:
- diffusion
- blade
- step
- ensemble
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Blade is a derivative-free Bayesian inversion method that produces
  well-calibrated posterior samples for inverse problems with black-box forward models.
  It combines statistical linearization with diffusion priors in a split Gibbs framework,
  using interacting particle systems to avoid gradient computations.
---

# Blade: A Derivative-free Bayesian Inversion Method using Diffusion Priors

## Quick Facts
- **arXiv ID:** 2510.10968
- **Source URL:** https://arxiv.org/abs/2510.10968
- **Reference count:** 40
- **Primary result:** Blade achieves well-calibrated posterior samples for inverse problems with black-box forward models using derivative-free Bayesian inversion with diffusion priors

## Executive Summary
Blade is a novel derivative-free Bayesian inversion method designed to handle inverse problems with black-box forward models. The method combines statistical linearization with diffusion priors within a split Gibbs framework, using interacting particle systems to avoid gradient computations. This approach enables robust uncertainty quantification without requiring gradient information from the forward model. Blade demonstrates superior probabilistic performance on both linear Gaussian problems and challenging Navier-Stokes fluid dynamics, showing well-calibrated uncertainty estimates compared to existing methods.

The method addresses a critical gap in Bayesian inversion by providing a practical solution for problems where gradient-based methods are infeasible or impractical. By leveraging diffusion priors and particle-based sampling, Blade achieves sample-efficient scaling behavior relative to end-to-end neural network baselines while maintaining theoretical guarantees for convergence and stability under model and prior approximation errors.

## Method Summary
Blade employs a split Gibbs sampling framework that alternates between sampling from the diffusion prior and performing statistical linearization of the posterior. The method uses interacting particle systems to approximate the posterior distribution without requiring gradient computations from the forward model. The statistical linearization step involves local linear approximations of the nonlinear forward map, while the diffusion prior provides a flexible generative model for the unknown parameters. This combination allows Blade to handle complex inverse problems while maintaining computational efficiency and theoretical guarantees for convergence.

## Key Results
- Superior probabilistic performance on linear Gaussian problems compared to existing methods
- Well-calibrated uncertainty quantification demonstrated on challenging Navier-Stokes fluid dynamics problems
- Robust performance across different noise levels and hyperparameters with sample-efficient scaling relative to neural network baselines

## Why This Works (Mechanism)
Blade succeeds by combining statistical linearization with diffusion priors in a computationally efficient framework. The split Gibbs sampling alternates between diffusion prior sampling and linearized posterior updates, allowing the method to handle complex posterior geometries without requiring gradient information. The interacting particle system approach provides a flexible way to approximate the posterior while maintaining theoretical convergence guarantees. The diffusion prior serves as a flexible generative model that can capture complex parameter distributions while enabling efficient sampling through the split Gibbs framework.

## Foundational Learning

**Diffusion priors** - Generative models that produce samples through a stochastic process, useful for representing complex prior distributions in Bayesian inference. Needed to provide flexible prior modeling that can capture complex parameter distributions. Quick check: Verify the diffusion process correctly generates samples matching prior assumptions.

**Statistical linearization** - Local linear approximation of nonlinear forward maps around current parameter estimates. Required to make posterior inference tractable without full gradient computations. Quick check: Confirm linearization error remains bounded within operational regime.

**Split Gibbs sampling** - Alternating between sampling from different conditional distributions in the posterior. Essential for decomposing complex posterior inference into tractable steps. Quick check: Monitor mixing properties and convergence of alternating steps.

**Interacting particle systems** - Ensemble methods where multiple particles interact to approximate distributions. Needed to avoid gradient computations while maintaining sampling efficiency. Quick check: Verify particle diversity and effective sample size remain adequate.

## Architecture Onboarding

**Component map:** Diffusion prior generator -> Statistical linearization module -> Particle interaction network -> Posterior approximation

**Critical path:** Prior sampling → Forward model evaluation → Linearization → Particle update → Convergence check

**Design tradeoffs:** Gradient-free operation (robust to black-box models) vs. approximation accuracy (linearization error), computational efficiency (particle-based sampling) vs. sample quality (particle diversity maintenance)

**Failure signatures:** Poor mixing in split Gibbs iterations, particle collapse indicating insufficient diversity, divergence in statistical linearization for highly nonlinear regions

**First experiments:** 1) Test on simple linear Gaussian problem with known solution, 2) Evaluate on moderately nonlinear forward model, 3) Assess scaling with increasing dimensionality

## Open Questions the Paper Calls Out
None

## Limitations
- Statistical linearization introduces approximation errors in highly nonlinear regimes
- Gaussian approximations may not capture non-Gaussian posterior features accurately
- Limited empirical testing scope may not reveal performance on all problem types

## Confidence
- Theoretical convergence rates and stability: High
- Superior probabilistic performance on tested problems: Medium
- Robustness across noise levels and hyperparameters: Medium

## Next Checks
1. Test Blade on broader range of nonlinear inverse problems including highly non-Gaussian posteriors and multimodal distributions
2. Conduct systematic ablation studies quantifying impact of statistical linearization and Gaussian approximations on posterior accuracy
3. Compare Blade against other derivative-free Bayesian inversion methods on standardized benchmark suite