---
ver: rpa2
title: Simulating Filter Bubble on Short-video Recommender System with Large Language
  Model Agents
arxiv_id: '2504.08742'
source_url: https://arxiv.org/abs/2504.08742
tags:
- filter
- user
- bubble
- level
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces SimTok, a simulation framework leveraging
  large language model (LLM) agents to model short-video recommender systems. By integrating
  real-world video data with LLM-driven user profiles and feedback, the framework
  realistically simulates the interaction between recommender algorithms and user
  behavior.
---

# Simulating Filter Bubble on Short-video Recommender System with Large Language Model Agents

## Quick Facts
- **arXiv ID**: 2504.08742
- **Source URL**: https://arxiv.org/abs/2504.08742
- **Reference count**: 11
- **Primary result**: Introduces SimTok, an LLM agent-based simulation framework that realistically models filter bubble formation in short-video recommender systems.

## Executive Summary
This paper introduces SimTok, a simulation framework that uses large language model (LLM) agents to model short-video recommender systems and study filter bubble formation. The framework integrates real-world video data with synthetic user profiles and feedback, creating a closed-loop system where agents interact with recommendations and their responses train the model. The study demonstrates that filter bubbles emerge naturally in simulations, with entropy decreasing and satisfaction aligning inversely with diversity. It also shows that user motivations and demographic factors significantly influence content diversity, and that mitigation strategies like cold-start matching and feedback weighting can effectively reduce filter bubbles.

## Method Summary
The framework uses Matrix Factorization (MF) for the recommender model, training on feedback from LLM agents that simulate user behavior. Each simulation involves 20 agents, 5 recommendations per iteration, and 12-15 iterations total. User profiles include demographics, initial interests, and motivations (Uses & Gratifications or personality traits). Agents provide structured feedback (Watch, Like, Comment, Collect, Skip, Dislike) based on prompts containing their profile and video metadata. The system tracks coverage (unique categories watched), entropy (Shannon entropy of watched categories), and satisfaction (positive responses / total watched) per iteration. Cold-start matching ratios and feedback weighting strategies are tested as mitigation approaches.

## Key Results
- Filter bubbles emerge naturally in simulations, with entropy decreasing and satisfaction aligning inversely with diversity
- Personality-driven users experience stronger filter bubbles than uses & gratifications-driven users
- Demographic factors like age and phone price significantly influence content diversity
- Cold-start matching ratios below 25% and progressive feedback weighting effectively reduce filter bubbles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Filter bubble simulation relies on closed-loop interaction where LLM-driven user feedback actively reinforces recommendation narrowing.
- **Mechanism**: LLM agents generate feedback based on user profiles and video details, which is converted into loss weights to update the recommender model. As the model trains on synthetic feedback, it increasingly narrows the item pool to match prior positive signals, mathematically reducing entropy.
- **Core assumption**: Text-based video descriptions are sufficient proxies for video content, allowing realistic relevance judgments.
- **Evidence anchors**: [abstract] "uncovering key mechanisms driving filter bubble formation"; [section 2.2] "closed-loop system, where updated agent profiles and feedback continually refine the recommendation process"; [corpus] PersonaAct validates agent-based approach.

### Mechanism 2
- **Claim**: Filter bubble severity depends on cold-start alignment strategy, with lower initial matching ratios fostering long-term diversity.
- **Mechanism**: Cold Start Category Matching Ratio (CSCMR) controls initial recommendation alignment with user interests. Lower ratios (e.g., 25%) force early exposure to disparate categories, preventing immediate overfitting to initial interest vectors and maintaining higher entropy.
- **Core assumption**: Agents possess latent capacity for "serendipity"—discovering and appreciating new interests.
- **Evidence anchors**: [abstract] "mitigation strategies like cold-start matching... effectively reduce filter bubbles"; [section 4.1] "CSCMR of both 0%... and 25% increase entropy substantially... because it allows for the possibility of serendipity"; [corpus] CroPS discusses breaking self-reinforcing loops.

### Mechanism 3
- **Claim**: Progressive feedback weighting mitigates homogenization more effectively than uniform weighting.
- **Mechanism**: Different user actions imply different preference intensities. Assigning scalar weights (e.g., Collect=4, Like=2, Watch=1) in Binary Cross-Entropy loss prioritizes strong signals while dampening passive ones, preventing overfitting to lazy viewing habits.
- **Core assumption**: "Collect" or "Comment" actions are more reliable indicators of category preference than "Watch" actions.
- **Evidence anchors**: [abstract] "feedback weighting effectively reduce filter bubbles... with progressive feedback weights"; [section 4.2] "When weights are progressive, entropy increases drastically while keeping satisfaction high"; [corpus] LLM-Enhanced RL emphasizes optimizing for long-term satisfaction.

## Foundational Learning

- **Concept: Matrix Factorization (MF)**
  - **Why needed here**: MF drives the recommender by decomposing user-item interactions into latent embeddings, encoding filter bubbles as converging vectors.
  - **Quick check question**: If an agent only "skips" videos, how does the MF model update their latent user vector relative to those items?

- **Concept: Shannon Entropy**
  - **Why needed here**: Primary metric for filter bubble—high entropy means high diversity, low entropy signals homogenous bubble.
  - **Quick check question**: If an agent watches 10 videos all from the same category, does the entropy of their history go up or down?

- **Concept: Uses and Gratifications Theory (U&G)**
  - **Why needed here**: Defines agent motivation (e.g., Information Seeking vs. Entertainment), explaining why agents might reject matching recommendations.
  - **Quick check question**: How might an agent motivated by "Information Seeking" react differently to a comedy sketch compared to an agent motivated by "Entertainment"?

## Architecture Onboarding

- **Component map**: Data Layer -> LLM Agent Module -> Recommender Engine -> Evaluation Loop
- **Critical path**: Prompt Construction is most sensitive—imprecise formatting breaks the parser.
- **Design tradeoffs**:
  - **MF vs. FM**: Use MF for mechanism study (simplicity), FM for bias auditing (ingests side features).
  - **Text vs. Multimodal**: Text summaries are computationally cheaper but lose visual signals; future upgrades require multimodal LLM.
- **Failure signatures**:
  - **Entropy Crash**: Entropy drops to near-zero immediately (check Cold Start ratio).
  - **Feedback Loop Collapse**: Satisfaction drops to 0% and stays (check LLM prompts).
  - **Format Error**: System logs show "Parsing Error" (check prompt formatting).
- **First 3 experiments**:
  1. **Baseline Run**: Execute with "Uses & Gratifications" agents and 50% Cold Start; verify entropy decreases over 15 iterations.
  2. **Ablation on Motivation**: Compare "Personality" vs. "U&G" agents; quantify entropy difference to validate personality users experience stronger bubbles.
  3. **Intervention Test**: Enable "Progressive Weights" (Watch:1, Collect:4); compare final entropy against "Default Weights" baseline.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do cold-start matching and progressive feedback weighting strategies effectively mitigate filter bubbles in live, large-scale production environments?
  - **Basis in paper**: [explicit] "We could also conduct real-world testing of the interventions in live systems to validate their practical effectiveness."
  - **Why unresolved**: Simulation lacks noise, scale, and complex variables of real-world platforms.
  - **What evidence would resolve it**: A/B testing results from commercial short-video platform showing significant entropy increase without compromising retention.

- **Open Question 2**: How does filter bubble simulation fidelity change when agents process actual video content via multimodal models instead of text summaries?
  - **Basis in paper**: [explicit] "agents are not exposed to the actual video but rather textual data"; propose "integrate video or image data into a multi-modal LLM pipeline."
  - **Why unresolved**: Text summaries may fail to capture visual/auditory features that influence engagement.
  - **What evidence would resolve it**: Comparative study measuring feedback pattern divergence and recommendation diversity between raw video vs. text summaries.

- **Open Question 3**: Do observed filter bubble dynamics generalize to larger agent populations with social network effects?
  - **Basis in paper**: [inferred] Uses only 20 isolated agents per simulation, lacking social connections or viral propagation.
  - **Why unresolved**: Unclear if personality-driven bubble severity scales with inter-agent influence and collective behaviors.
  - **What evidence would resolve it**: Simulations with thousands of agents incorporating social graphs to observe network-level homogenization.

## Limitations

- The simulation's validity depends on whether LLM-generated feedback accurately represents human preferences, which is difficult to verify.
- Text metadata may not capture visual or auditory appeal that drives real user engagement, limiting simulation realism.
- The specific LLM model, temperature settings, and exact prompt formatting are not fully specified, introducing potential variability.

## Confidence

- **High confidence**: Mathematical framework for filter bubble emergence (entropy decrease in closed feedback loop) is well-founded and reproducible.
- **Medium confidence**: Impact of user motivations on filter bubble severity, as it depends on LLM agent behavior realism.
- **Medium confidence**: Effectiveness of mitigation strategies, though results may vary based on dataset characteristics and LLM response consistency.

## Next Checks

1. **Behavior validation**: Compare LLM agent feedback patterns against real user interaction logs to verify simulated preferences align with actual viewing patterns.
2. **Dataset sensitivity**: Run simulation with multiple short-video datasets of varying quality and category distributions to test robustness of filter bubble emergence and mitigation effectiveness.
3. **LLM variation test**: Implement simulation using different LLM models or temperature settings to quantify how agent behavior variability affects filter bubble metrics and intervention outcomes.