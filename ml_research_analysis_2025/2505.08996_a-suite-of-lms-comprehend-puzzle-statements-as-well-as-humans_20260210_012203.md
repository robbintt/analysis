---
ver: rpa2
title: A suite of LMs comprehend puzzle statements as well as humans
arxiv_id: '2505.08996'
source_url: https://arxiv.org/abs/2505.08996
tags:
- responses
- human
- judgments
- humans
- dgmml
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-evaluates claims that large language models underperform
  humans in understanding minimally complex English statements. The authors replicate
  a prior study but show that when humans are prevented from rereading the stimuli
  (a more naturalistic comprehension test), their accuracy drops significantly and
  falls below that of Falcon-180B-Chat (76%), GPT-4 (81%), and GPT-o1 (100%).
---

# A suite of LMs comprehend puzzle statements as well as humans

## Quick Facts
- arXiv ID: 2505.08996
- Source URL: https://arxiv.org/abs/2505.08996
- Reference count: 0
- Large language models achieve human-level or better comprehension of minimally complex English statements when evaluated with appropriate baselines and metrics

## Executive Summary
This paper challenges the claim that large language models underperform humans in understanding minimally complex English statements. The authors replicate a prior study but show that when humans are prevented from rereading the stimuli (a more naturalistic comprehension test), their accuracy drops significantly and falls below that of several advanced models including Falcon-180B-Chat (76%), GPT-4 (81%), and GPT-o1 (100%). Further analysis reveals that both humans and models struggle disproportionately with items involving potentially reciprocal actions, suggesting shared pragmatic sensitivities. The study also shows that model performance was systematically underestimated due to coding and design choices, and that human grammaticality judgments often diverge from idealized benchmarks.

## Method Summary
The study replicated and extended a prior experiment testing comprehension of "puzzle stimuli" - complex English statements with yes/no queries. 120 Prolific participants completed the task under two conditions: sequential (query after statement, no rereading) and simultaneous (both on same page). Seven language models were tested including Falcon-180B-Chat, GPT-4, GPT-o1, Llama-2-70B, Gemini, Bard, and GPT-4o. Human responses were coded as binary accuracy on target responses. Models were probed with binary judgments and log probability comparisons of target vs. non-target responses. Grammaticality judgments were collected on a 1-7 Likert scale. Analysis used generalized linear mixed models with random intercepts for subjects and items.

## Key Results
- Human accuracy dropped from 85.2% to 73.0% when rereading was restricted, falling below Falcon-180B-Chat (76%) and GPT-4 (81%)
- Log probability analysis revealed systematic underestimation of LM performance, with Llama-2-70B showing full accuracy on target responses
- Both humans and models were disproportionately challenged by queries involving potentially reciprocal actions, suggesting shared pragmatic sensitivities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Restricting rereading opportunities provides a more valid human baseline for comprehension comparisons
- Mechanism: Humans naturally reread when initial comprehension fails. When statements and queries are presented sequentially without backtracking, accuracy drops from ~85% to 73%, revealing true first-pass comprehension rather than reparatory processing
- Core assumption: First-pass comprehension without revision is the appropriate benchmark for comparing human and LM language understanding
- Evidence anchors: Human accuracy was lower when people are unable to reread the statements (sequential condition: 73.0% vs. simultaneous condition: 85.2%, β = -0.11, t = -5.10, p < .0001)
- Break condition: If comprehension is defined as eventual understanding (allowing re-reading), human baselines would be higher and conclusions reversed

### Mechanism 2
- Claim: Log probability analysis reveals stronger LM comprehension than binary metalinguistic probes
- Mechanism: Binary responses require models to make explicit judgments, which can fail even when implicit comprehension is intact. Log probabilities over target vs. non-target responses capture gradient confidence without forcing discrete decisions
- Core assumption: Higher log probability for target responses indicates genuine comprehension rather than surface pattern matching
- Evidence anchors: Results show higher log probabilities for target responses on every item, indicating far better performance than when binary results of metalinguistic probes were used
- Break condition: If log probabilities capture statistical association rather than meaning, the mechanism would not indicate comprehension

### Mechanism 3
- Claim: Shared error patterns on reciprocal verbs suggest similar pragmatic processing in humans and LMs
- Mechanism: Verbs like "kiss," "hug," and "greet" invite reciprocal inferences. Both humans and lower-performing models produce non-target responses on these items, suggesting the errors stem from pragmatic expectation rather than syntactic failure
- Core assumption: Similar error distributions across groups imply similar underlying processing constraints
- Evidence anchors: Gemini and Bard show significant effects on reciprocal verb items; Llama-2-70B also shows lower confidence on these items
- Break condition: If reciprocal verb errors arise from different causes in humans (pragmatics) vs. models (training distribution), the shared-sensitivity claim would not hold

## Foundational Learning

- Concept: **Pragmatics vs. literal interpretation**
  - Why needed here: The puzzle stimuli require ignoring contextual inferences (e.g., "was Mary kissed?" requires answering "No" even though kissing is typically reciprocal). Understanding this distinction is essential for coding responses correctly
  - Quick check question: If a model answers "Yes, Mary was likely kissed back" to a reciprocal verb query, is this an error in comprehension or an appeal to pragmatic inference?

- Concept: **Log probabilities as implicit measures**
  - Why needed here: Binary probes conflate comprehension with metalinguistic judgment. Log probabilities separate what a model "expects" from what it "says," analogous to human reaction-time measures
  - Quick check question: Why might a model assign higher probability to a target response while nonetheless outputting a non-target response when explicitly prompted?

- Concept: **Evaluation design validity**
  - Why needed here: The original study's design (simultaneous presentation, generous human conditions) inflated human performance. Understanding how design choices affect baselines is critical for fair comparison
  - Quick check question: What happens to human accuracy if you allow unlimited rereading vs. single-pass reading, and which is the appropriate comparison for LMs that cannot "reread"?

## Architecture Onboarding

- Component map: Stimulus presentation layer -> Response capture layer -> Coding/evaluation layer -> Model probing layer

- Critical path:
  1. Define comprehension task (literal interpretation vs. pragmatic inference)
  2. Establish human baseline with appropriate constraints (sequential presentation)
  3. Collect model responses using multiple methods (binary + logprob)
  4. Code responses with explicit criteria; document ambiguous cases
  5. Compare error distributions by stimulus type (e.g., reciprocal verbs)

- Design tradeoffs:
  - Sequential vs. simultaneous presentation: Sequential is more naturalistic but yields lower human scores; simultaneous inflates human baselines
  - Binary vs. logprob evaluation: Binary is interpretable but underestimates implicit knowledge; logprob requires access to model internals
  - Strict vs. generous coding: Strict coding penalizes reasonable pragmatic inferences; generous coding may mask genuine failures

- Failure signatures:
  - Human accuracy dramatically exceeds model accuracy only when humans can reread (design artifact)
  - Model "errors" include well-reasoned justifications that conflict with task's literal-interpretation requirement (coding artifact)
  - Disproportionate errors on specific verb classes (e.g., reciprocal actions) in both humans and models (shared pragmatic effect, not model deficit)

- First 3 experiments:
  1. Replicate human sequential-presentation condition with n≥100 participants to establish constrained baseline; compare to model binary responses
  2. Extract log probabilities for target vs. non-target completions on same stimuli; verify whether implicit preference matches explicit output
  3. Stratify analysis by verb type (reciprocal vs. non-reciprocal); test whether human and model error patterns correlate across categories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions should we expect LMs to provide idealized, rule-based responses rather than human-like responses that incorporate pragmatic inference?
- Basis in paper: The authors explicitly conclude by asking, "under what conditions ought we to expect LMs to provide idealized responses rather than human-like responses?"
- Why unresolved: While the study shows models can achieve high accuracy (idealized) or mimic human error patterns (human-like), the appropriate mode depends on the undefined goals of future AI systems
- What evidence would resolve it: Comparative studies in applied settings (e.g., legal reasoning vs. casual conversation) determining whether strict logical adherence or pragmatic flexibility yields better user outcomes

### Open Question 2
- Question: To what extent does prompt framing reliably shift model behavior to align with distinct sub-populations (e.g., naive vs. expert humans) across diverse linguistic phenomena?
- Basis in paper: The paper notes GPT-4o aligned with expert linguists only when explicitly prompted to do so, distinct from its default alignment with naive human judgments
- Why unresolved: The results demonstrate that framing matters for grammaticality judgments, but it is unclear if this is a superficial alignment or a robust, generalizable capability to simulate specific cognitive profiles
- What evidence would resolve it: Testing model alignment across a battery of psycholinguistic tasks (e.g., garden path sentences, scope ambiguity) using various persona prompts against distinct human expert and naive cohorts

### Open Question 3
- Question: What are the underlying causes of the divergence between a model's implicit preference (log probabilities) and its explicit binary generation in comprehension tasks?
- Basis in paper: The authors found Llama-2-70B showed "full accuracy" when using log probabilities, which contrasted sharply with its lower performance on explicit binary probes
- Why unresolved: The paper establishes that log probabilities are a more informative measure, but does not resolve why the model's generation mechanism fails to select the token with the highest implicit likelihood
- What evidence would resolve it: Analysis of decoding strategies and instruction-tuning procedures to identify where the probability signal is lost or overridden during the generation process

## Limitations
- The study relies on a specific set of puzzle stimuli that may not generalize to all comprehension tasks
- The human sample was relatively small (n=120) and recruited from Prolific, which may not represent the general population
- The study focuses on English language comprehension, limiting generalizability to other languages

## Confidence
- High: The replication methodology is sound and the findings directly challenge the original study's conclusions
- Medium: The log probability analysis provides compelling evidence but requires technical expertise to verify
- Low: The study's generalizability to other comprehension tasks remains uncertain

## Next Checks
1. Verify that human sequential-presentation condition with n≥100 participants replicates the 73% accuracy baseline
2. Confirm that log probabilities for target responses are consistently higher than non-target responses across all stimulus items
3. Test whether error patterns on reciprocal verbs correlate between humans and models in a larger sample