---
ver: rpa2
title: 'Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across
  Perception, Planning, and Safety in Real-World Multimodal Agents'
arxiv_id: '2506.21252'
source_url: https://arxiv.org/abs/2506.21252
tags:
- answer
- reward
- arxiv
- image
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Agent-RewardBench, the first benchmark designed\
  \ to evaluate reward modeling ability in multimodal large language models for real-world\
  \ agent tasks. The benchmark covers three dimensions\u2014perception, planning,\
  \ and safety\u2014across seven agent scenarios, including web, mobile, desktop,\
  \ autonomous driving, Minecraft, virtual house, and travel planning."
---

# Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents

## Quick Facts
- arXiv ID: 2506.21252
- Source URL: https://arxiv.org/abs/2506.21252
- Authors: Tianyi Men; Zhuoran Jin; Pengfei Cao; Yubo Chen; Kang Liu; Jun Zhao
- Reference count: 31
- Even state-of-the-art models achieve only around 61% accuracy on reward modeling for agents.

## Executive Summary
Agent-RewardBench introduces the first benchmark specifically designed to evaluate reward modeling ability in multimodal large language models for real-world agent tasks. The benchmark covers three dimensions—perception, planning, and safety—across seven agent scenarios including web, mobile, desktop, autonomous driving, Minecraft, virtual house, and travel planning. A key innovation is step-level reward evaluation, allowing assessment of model performance at each step rather than just final outcomes. The dataset is constructed by sampling responses from ten diverse models, filtering for appropriate difficulty using smaller models, and manually verifying quality. Experiments show that even state-of-the-art models like GPT-4o and Gemini-1.5-pro achieve only around 61% accuracy, indicating significant challenges in reward modeling for agents.

## Method Summary
The benchmark constructs a set of pairwise comparisons between positive and negative agent responses at the step level. To build the dataset, responses are sampled from ten diverse MLLMs, then filtered using three smaller models to ensure appropriate difficulty (medium/hard range). Manual verification removes mislabeled pairs. Evaluation uses a unified prompt template asking models to choose between two answers, with temperature=0 and position-swapping to mitigate bias. The benchmark spans seven scenarios and three dimensions (perception, planning, safety), totaling 1,136 samples. A* search experiments on VisualWebArena validate the benchmark's correlation with downstream task performance (r=0.981).

## Key Results
- State-of-the-art models like GPT-4o and Gemini-1.5-pro achieve only around 61% accuracy on reward modeling tasks.
- Stronger models do not always perform better on safety tasks—GPT-4o scores 65.9% on perception, 73.2% on planning, but only 39.2% on safety.
- Open-source models like Llama-3.2-11B-Vision score significantly lower, highlighting the need for specialized training.
- The benchmark demonstrates strong correlation (0.981) with downstream agent performance on VisualWebArena.

## Why This Works (Mechanism)

### Mechanism 1: Step-Level Reward Differentiation
Evaluating rewards at individual trajectory steps provides more diagnostic signal than trajectory-level binary success/failure. The benchmark decomposes agent trajectories into discrete (positive, negative) response pairs for each intermediate step, forcing the reward model to distinguish subtle planning errors rather than just final outcomes.

### Mechanism 2: Difficulty-Calibrated Filtering via Small Models
Pre-filtering candidate pairs using smaller models before human annotation yields benchmark samples at appropriate difficulty. Three small models score each candidate pair; pairs with intermediate accuracy (medium/hard) are retained, ensuring samples challenging for small models remain challenging for larger models.

### Mechanism 3: Multi-Dimensional Coverage Induces Non-Transferable Reward Skills
Strong perception/planning performance does not imply strong safety reward modeling; these require separate training signals. The benchmark spans three dimensions (perception, planning, safety) across seven scenarios, revealing that safety reward modeling requires distinct reasoning patterns not captured by general capability scaling.

## Foundational Learning

- **Reward Modeling as Preference Prediction**
  - Why needed here: The benchmark evaluates models' ability to select the better of two candidate responses—a standard reward model formulation.
  - Quick check question: Given two agent step completions, can you articulate which is better and why (correctness, grounding, safety)?

- **Multi-Step Agent Trajectories with Environment Feedback**
  - Why needed here: Understanding how agents generate action sequences and where external rewards intervene is essential for interpreting step-level evaluation.
  - Quick check question: In a web navigation task, what distinguishes a correct intermediate click from a correct final outcome?

- **Multimodal Grounding (Vision + Language + Action)**
  - Why needed here: Agent-RewardBench requires evaluating visual grounding alongside textual reasoning.
  - Quick check question: How would you verify that an agent's "CLICK Sign In" action correctly identifies the target element visually?

## Architecture Onboarding

- **Component map:** Source datasets → Response generation (10 MLLMs) → Pairing & filtering (3 small models) → Manual verification → Evaluation interface (pairwise comparison) → Downstream correlation validation

- **Critical path:**
  1. Scenario-specific data ingestion with multimodal context (image + task instruction + history)
  2. Step-wise response sampling from diverse models to populate positive/negative sets
  3. Difficulty filtering (small model accuracy in [medium, hard] range)
  4. Human verification to remove mislabeled pairs
  5. Pairwise evaluation with position-swap averaging

- **Design tradeoffs:**
  - Breadth vs. depth: 1,136 samples across 7 scenarios enables coverage but limits per-scenario statistical power
  - Automated vs. human filtering: Small-model pre-filtering scales but may inherit model-specific biases; human annotation ensures quality but is costly
  - Step-level vs. trajectory-level: More diagnostic but assumes step errors are locally observable

- **Failure signatures:**
  - Position bias: Models preferentially select Answer 1 or Answer 2; mitigated by swapping and averaging
  - Safety blindness: Strong models score <40% on safety, indicating failure to detect adversarial or hazardous actions
  - Hallucination tolerance: Negative samples with hallucinations may still be selected if the model lacks grounding verification

- **First 3 experiments:**
  1. Baseline sweep: Run Agent-RewardBench evaluation on your target MLLM with default prompts; report perception/planning/safety breakdown to identify weakest dimension
  2. Position-bias audit: Evaluate a subset (e.g., 100 pairs) without position swapping vs. with swapping; quantify bias magnitude
  3. Downstream validation: If you have access to a web navigation environment, run reward-guided A* search using your model as the reward function; correlate benchmark accuracy with task success rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does increased general model capability not translate to improved safety reward modeling, and how can this misalignment be corrected?
- Basis in paper: [explicit] The results in Section 4.2 state that "Stronger models don’t guarantee better safety reward modeling," noting that Claude-3.5-Sonnet achieves 71.2% on planning but drops to 22.4% on safety.
- Why unresolved: Current pre-training objectives or scaling laws may prioritize general reasoning over the specific nuances required to detect unsafe agent actions, creating a capability gap.
- What evidence would resolve it: Identification of specific failure modes in safety scenarios for high-parameter models, followed by a training intervention that raises safety scores to be commensurate with planning scores.

### Open Question 2
- Question: What specialized training strategies are required to elevate open-source MLLMs to the level of closed-source models for agent reward modeling?
- Basis in paper: [explicit] Section 4.2 highlights the "Need for Improvements in Open-Source Models," pointing out that Llama-3.2-11B-Vision-Instruct scores significantly lower (approx. 48% total) compared to Gemini-1.5-pro (61.6%).
- Why unresolved: There is currently a lack of reward models specifically trained for agent tasks, leaving open-source models reliant on general-purpose capabilities that are insufficient for granular step-level evaluation.
- What evidence would resolve it: The release of an open-source model fine-tuned on agent trajectory data that achieves competitive accuracy (e.g., >60%) on the Agent-RewardBench.

### Open Question 3
- Question: To what extent does potential data pollution in public source datasets impact the reliability of reward signals in the benchmark?
- Basis in paper: [explicit] The Limitations section notes that because the data source comes from public datasets, "It may have the risk of data pollution."
- Why unresolved: Public datasets may contain artifacts or errors from previous model generations, which could bias the reward model or lead to inaccurate evaluations of "correctness."
- What evidence would resolve it: A comparative analysis of reward model performance on the current benchmark versus a dataset constructed entirely from scratch using controlled, unpolluted environments.

## Limitations
- The difficulty filtering mechanism relies on small models' ability to generalize difficulty judgments to larger models, which is empirically validated but not theoretically guaranteed.
- Manual verification process lacks detailed documentation of annotation criteria, potentially introducing subjectivity.
- The pairwise evaluation format may not capture all aspects of reward modeling quality, particularly in safety-critical scenarios where action consequences unfold over multiple steps.

## Confidence

- **High Confidence**: The benchmark's construction methodology (step-level evaluation, difficulty filtering, manual verification) and basic experimental results (accuracy scores, correlation with downstream performance) are well-documented and reproducible.
- **Medium Confidence**: The claim that step-level evaluation provides more diagnostic signal than trajectory-level assessment is supported but could benefit from ablation studies comparing different evaluation granularities.
- **Medium Confidence**: The observation that stronger models don't guarantee better safety performance is empirically validated but requires more extensive testing across additional safety scenarios to establish generalizability.

## Next Checks
1. **Difficulty Filter Generalization**: Systematically test whether difficulty filtering via small models maintains appropriate calibration when applied to completely different model architectures or training paradigms.

2. **Position Bias Quantification**: Conduct comprehensive position bias analysis across all seven scenarios, measuring how different models respond to answer position and whether certain scenarios exhibit stronger bias than others.

3. **Long-Horizon Safety Validation**: Design experiments to test whether step-level safety judgments accurately predict safety outcomes in extended agent trajectories, particularly for scenarios where safety violations manifest over multiple steps.