---
ver: rpa2
title: 'RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly
  on Robotic Platforms'
arxiv_id: '2509.06714'
source_url: https://arxiv.org/abs/2509.06714
tags:
- inference
- delays
- control
- time
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of deploying reinforcement learning
  directly on real robots, where two main constraints exist: limited training data
  due to costly real-world data collection, and real-time execution constraints due
  to inference delays in model-based methods. The authors propose a general framework
  to handle inference delays using a d-step MPC approach that provides sequences of
  actions to avoid execution gaps.'
---

# RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms

## Quick Facts
- arXiv ID: 2509.06714
- Source URL: https://arxiv.org/abs/2509.06714
- Reference count: 19
- Key outcome: RT-HCP reaches successful performance in 60k training steps (20 minutes) compared to 100k-160k for baselines on FURUTA pendulum swing-up task.

## Executive Summary
This paper addresses the challenge of deploying reinforcement learning directly on real robots where two constraints exist: limited training data due to costly real-world data collection, and real-time execution constraints due to inference delays in model-based methods. The authors propose RT-HCP, a Real-Time Hybrid Control with Physics-informed model that combines model-based RL, model-free RL, and prior dynamics knowledge. RT-HCP reduces inference time through hybrid planning and mitigates compounding errors using a physics-informed model. Experiments on a FURUTA pendulum show that RT-HCP outperforms other methods, reaching successful performance in 60k training steps (20 minutes) compared to 100k-160k for baselines, while maintaining better stability and control performance under real-time constraints.

## Method Summary
RT-HCP combines model-based RL, model-free RL, and prior dynamics knowledge to address inference delays and sample efficiency in real-robot learning. The method uses a d-step MPC approach that provides sequences of actions to avoid execution gaps during inference delays. A physics-informed model combines Euler-Lagrange equations with a learnable residual network to improve trajectory prediction accuracy. The framework integrates actor-critic components where a learned policy provides action candidates to accelerate CEM planning, while the Q-function enables shorter planning horizons. The system collects real trajectories, performs offline updates periodically, and generates synthetic data through imagination rollouts.

## Key Results
- RT-HCP achieves swing-up success in 60k training steps (20 minutes) versus 100k-160k steps for baselines
- Maintains better stability and control performance under real-time constraints
- Reduces inference time through hybrid planning while mitigating compounding errors via physics-informed predictions

## Why This Works (Mechanism)

### Mechanism 1: d-step MPC for Inference Delay Compensation
When inference requires d time steps, the controller computes a sequence of d actions in advance. The system executes these buffered actions while the next sequence is computed, avoiding periods without control commands. The execution horizon H_e is set such that H_e ≥ T_i/Δt + 1, ensuring continuous coverage.

### Mechanism 2: State Augmentation to Restore Markov Property
The augmented state s'_t = {s_{d·t}, s_{d·t-(d-1)}, ..., s_{d·t-1}, a_{d·t}, ..., a_{d·t+(d-1)}} captures the trajectory history that would otherwise be lost. This transforms the original MDP into a delay-MDP where state transitions depend on the full augmented context.

### Mechanism 3: Physics-Informed Hybrid Model for Trajectory Prediction
The physics prior (Euler-Lagrange equations for the pendulum) captures dominant dynamics. A 4-layer MLP (16 neurons) learns residuals to compensate for unmodeled effects like friction, actuator delays, and encoder cable perturbations. This hybrid approach generalizes better than purely data-driven models outside the training distribution.

## Foundational Learning

- **Concept: Model Predictive Control (MPC)**
  - Why needed here: RT-HCP uses MPC with CEM optimization as its planning backbone. Understanding receding horizon control is essential to grasp how action sequences are generated and why replanning frequency matters.
  - Quick check question: Can you explain why MPC replans at each step rather than executing a full precomputed trajectory?

- **Concept: Cross-Entropy Method (CEM)**
  - Why needed here: CEM is the optimization algorithm used to find action sequences. Its population size and iteration count directly determine inference time, creating the delay that RT-HCP must manage.
  - Quick check question: How does increasing CEM population size affect both solution quality and inference time?

- **Concept: Actor-Critic Reinforcement Learning**
  - Why needed here: RT-HCP integrates a learned policy π and Q-function with MPC. The policy provides action candidates to accelerate CEM; the Q-function enables shorter planning horizons by estimating long-term value.
  - Quick check question: What advantage does combining a learned Q-function with MPC provide over pure MPC?

## Architecture Onboarding

- **Component map:**
  - Physics prior: Euler-Lagrange equations for Furuta pendulum (hardcoded dynamics)
  - Residual network: 4-layer MLP (16 neurons) learning unmodeled effects
  - Actor network: Policy π providing action candidates for CEM
  - Critic network: Q-function estimating long-term return
  - CEM planner: Optimizes action sequences using hybrid model
  - Action buffer: Stores precomputed actions for d-step execution
  - Replay buffers: D_real (real transitions), D_im (imagined transitions)

- **Critical path:**
  1. Measure inference time T_i on target hardware to determine delay d
  2. Set minimum execution horizon H_e = int(T_i/Δt) + 1
  3. Configure CEM with I=3 iterations, P=500 samples (50 from policy)
  4. Run online planning → real system interaction → D_real storage
  5. Perform offline updates every N steps (model + policy)
  6. Generate synthetic data in D_im periodically

- **Design tradeoffs:**
  - Planning horizon H_p: Longer improves planning quality but increases inference delay
  - Execution horizon H_e: Must cover delay, but longer means more open-loop error accumulation
  - CEM population/iterations: Larger improves solutions but increases T_i
  - Real vs. imagined data ratio: More imagined data improves sample efficiency but risks model exploitation

- **Failure signatures:**
  - Control oscillation or instability: H_e too long, model predictions diverging
  - Execution gaps: H_e < H_e_min, inference time underestimated
  - Slow learning: Insufficient model accuracy, residual network too small
  - Hardware timeout: CEM parameters too aggressive for embedded system

- **First 3 experiments:**
  1. Benchmark inference time T_i on target hardware across different H_p values to establish real-time feasibility envelope before training.
  2. Validate physics-informed model prediction accuracy on held-out trajectories; compare against pure data-driven baseline to isolate residual network contribution.
  3. Run ablation comparing RT-HCP with/without policy-guided CEM candidates to quantify inference time reduction and performance impact.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can RT-HCP be effectively extended to vision-based control tasks where the system state is not directly accessible? The conclusion states intent to extend work to image inputs, but visual inference delays may violate strict real-time constraints.

- **Open Question 2:** Does the RT-HCP framework scale to high-dimensional robotic systems (e.g., manipulators with many degrees of freedom)? While physics prior reduces prediction errors, CEM's computational cost grows non-linearly with action dimensionality, potentially making real-time constraints infeasible.

- **Open Question 3:** How sensitive is the method to the quality or availability of the analytical physics prior? The approach relies heavily on a hybrid model where the physics prior plays a central role, but real-world systems often have difficult-to-model dynamics.

## Limitations

- Limited experimental scope: Only validated on single robotic platform (FURUTA pendulum) with single task (swing-up), preventing assessment of generalization.
- Missing implementation details: Reward function R(s,a) not specified, making independent verification impossible.
- Hyperparameter omissions: Actor-critic architecture, learning rates, batch sizes, and update frequencies are not provided.

## Confidence

**High Confidence**: The d-step MPC mechanism to execute precomputed action sequences during inference delays is well-established in real-time control literature and directly addresses the execution gap problem.

**Medium Confidence**: The physics-informed model approach combining first-principles dynamics with learned residuals has strong empirical support from adjacent domains, though specific performance claims cannot be independently verified.

**Low Confidence**: The specific performance claims (60k steps vs 100k-160k for baselines) cannot be independently verified due to missing reward function and hyperparameter details. Generalization potential to other robotic platforms remains unproven.

## Next Checks

1. **Inference Time Validation**: Measure actual inference time T_i on target hardware across different planning horizons H_p to establish the real-time feasibility envelope and verify that H_e = ceil(T_i/Δt) + 1 prevents execution gaps.

2. **Physics-Informed Model Accuracy**: Compare trajectory prediction accuracy of the hybrid physics-learned model against a pure data-driven baseline on held-out test trajectories to quantify the contribution of the physics prior and residual network.

3. **Reward Function Reconstruction**: Reconstruct the likely reward function based on the swing-up task requirements (stable inverted position, minimal control effort) and verify that the optimization objective aligns with successful performance.