---
ver: rpa2
title: 'The Lucie-7B LLM and the Lucie Training Dataset: Open resources for multilingual
  language generation'
arxiv_id: '2503.12294'
source_url: https://arxiv.org/abs/2503.12294
tags:
- data
- dataset
- training
- https
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Lucie-7B is a 7B-parameter multilingual language model trained
  on a curated dataset emphasizing French and European languages, aiming to counter
  anglo-centric bias in LLMs. It uses equal amounts of English and French data, totaling
  ~3 trillion tokens, and prioritizes open data rights.
---

# The Lucie-7B LLM and the Lucie Training Dataset: Open resources for multilingual language generation

## Quick Facts
- arXiv ID: 2503.12294
- Source URL: https://arxiv.org/abs/2503.12294
- Authors: Olivier Gouvert; Julie Hunter; Jérôme Louradour; Christophe Cerisata; Evan Dufraisse; Yaya Sy; Laura Rivière; Jean-Pierre Lorré; OpenLLM-France community
- Reference count: 6
- One-line primary result: Lucie-7B is a 7B-parameter multilingual model trained on balanced French/English data (~3T tokens), achieving competitive French and multilingual performance with extended 32k context and strong instruction-following.

## Executive Summary
Lucie-7B is a 7B-parameter multilingual language model developed by the OpenLLM-France community to address anglo-centric bias in large language models. Trained on equal amounts of French and English data (roughly 33% each), totaling approximately 3 trillion tokens, the model prioritizes open data rights and cultural diversity. The project demonstrates that carefully curated, balanced training data can yield effective multilingual performance while maintaining strong capabilities in both French and English.

The model was developed through three distinct training phases: main pretraining with 4k context window, context length extension to 32k tokens, and high-quality data annealing. Two instruction-tuned versions were released - one using synthetic data and another using human-created data - with the synthetic version showing better performance on reasoning benchmarks while the human version ensures better data rights compliance.

## Method Summary
Lucie-7B uses a Llama-3.1-style decoder-only Transformer architecture with 32 layers, 4096 hidden size, and 32 attention heads. The model was trained using Megatron-DeepSpeed with 3D parallelism (TP=4, PP=4, DP=32) across 128 A100-80GB GPUs. Training employed a custom BPE tokenizer with 65,024 vocabulary size trained on French, English, code, and European languages. The dataset curation emphasized French and European languages while maintaining quality through aggressive filtering of web data and OCR noise.

## Key Results
- Achieves competitive performance on French and multilingual benchmarks while reducing anglo-centric cultural bias
- Successfully extends context window to 32k tokens through upsampling long documents while preserving domain proportions
- Shows significant improvement in reasoning benchmarks (GSM8K accuracy increasing from 8.4% to 23.5%) through high-quality data annealing
- Demonstrates strong instruction-following capabilities, particularly in the synthetic instruction-tuned version

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Balancing French and English data ratios (roughly 33% each) appears to reduce anglo-centric cultural bias while maintaining cross-lingual competence.
- **Mechanism:** The model allocates equal representational capacity to French and English during pretraining, supplemented by smaller proportions of other European languages (German, Spanish, Italian).
- **Core assumption:** Linguistic and cultural competence scales with token proportion, provided data quality is sufficiently high.
- **Evidence anchors:**
  - [abstract] "trained on equal amounts of data in French and English -- roughly 33% each"
  - [section 3.2] Figure 6 shows the upsampled training distribution balancing French and English sources.
  - [corpus] Neighbor paper "Luth" confirms that general multilingual models often underperform in French, supporting the need for specialized balancing.
- **Break condition:** Performance on English benchmarks drops significantly compared to monolingual baselines without a proportional gain in French cultural nuance, or French performance saturates early due to data redundancy.

### Mechanism 2
- **Claim:** Context window extension to 32k tokens is achieved by upsampling long documents while preserving domain proportions, rather than exclusively training on long sequences.
- **Mechanism:** Continual pretraining combines documents >4096 tokens (upsampled 10x) with shorter documents to maintain domain ratios, alongside an increased RoPE $\theta$ (500k to 20M).
- **Core assumption:** Extending context length requires adapting to sequence length while preventing probability distribution drift caused by sudden domain shifts (e.g., over-representation of books).
- **Evidence anchors:**
  - [section 4.3.3] Describes the method based on Fu et al. (2024), maintaining domain proportions while upsampling long docs.
  - [section 4.5.3] Figures 15-17 show successful retrieval up to 34k tokens post-extension.
- **Break condition:** The model fails to retrieve information ("Needle in a Haystack") at target lengths, or perplexity increases on short-context tasks due to distribution drift.

### Mechanism 3
- **Claim:** Annealing on high-quality, domain-specific data (math/code) with a decaying learning rate boosts reasoning benchmarks more effectively than extended general pretraining.
- **Mechanism:** A final training phase linearly decays the learning rate to zero on a curated mix (e.g., OpenWebMath, Flan v2), prioritizing quality over quantity.
- **Core assumption:** Late-stage optimization on high-quality signals can "refine" model weights for specific capabilities like math reasoning without destabilizing general knowledge.
- **Evidence anchors:**
  - [section 4.3.4] GSM8K accuracy improves from 8.4% (main phase) to 23.5% (annealing).
  - [section 4.5.1] Figure 11 shows the training loss curve steepening or stabilizing during the annealing phase.
- **Break condition:** General linguistic performance degrades (catastrophic forgetting) due to overfitting on the narrow annealing distribution.

## Foundational Learning

- **Concept: Token Fertility**
  - **Why needed here:** To evaluate if the custom tokenizer (65k vocab) efficiently compresses French and code compared to English-centric baselines.
  - **Quick check question:** Does the Lucie tokenizer require significantly more tokens per word for French compared to English, or compared to the Llama tokenizer?

- **Concept: RoPE (Rotary Positional Embedding) Scaling**
  - **Why needed here:** Crucial for understanding how the model extrapolates from 4k to 32k context length without retraining the entire architecture.
  - **Quick check question:** How does changing the RoPE $\theta$ frequency base allow the model to attend to positions beyond the original training window?

- **Concept: Data Deduplication and Filtering (MinHash/CCNet)**
  - **Why needed here:** The dataset relies on aggressive filtering of web data (RedPajama) and OCR noise (Gallica) to ensure quality despite the massive scale.
  - **Quick check question:** What specific perplexity threshold is used to discard noisy OCR documents, and how does this differ from the threshold for web spam?

## Architecture Onboarding

- **Component map:**
  - Architecture: Llama-3.1 style decoder-only Transformer (32 layers, 4096 hidden size, 32 heads)
  - Tokenizer: Custom BPE (65,024 vocab) trained on French, English, code, and European languages
  - Framework: Megatron-DeepSpeed (3D parallelism: TP=4, PP=4, DP=32)

- **Critical path:**
  1. Data Prep: Curate dataset (3T tokens) → Filter OCR/Web → Tokenize
  2. Phase 1: Main pretraining (4k context, 3.1T tokens, Cosine LR)
  3. Phase 2: Context Extension (32k context, 5B tokens, constant LR)
  4. Phase 3: Annealing (High-quality mix, Linear decay to 0 LR)

- **Design tradeoffs:**
  - Token Balance vs. Performance: Equal French/English ratio optimizes for cultural fairness but may lag behind English-heavy models (Llama-3) on US-centric benchmarks
  - Batch Size Ramp-up: Starting small (256) and increasing to 1024 improves early convergence but requires careful tuning to avoid stability issues
  - Human vs. Synthetic Instruct: Human-only data ensures rights compliance but performs worse on math/reasoning benchmarks than synthetic mixes

- **Failure signatures:**
  - Context Collapse: "Needle in a Haystack" accuracy drops sharply after 22k tokens (observed in `Lucie-7B-Instruct-v1.1` due to insufficient long-sequence fine-tuning)
  - Math Stagnation: GSM8K scores remain low (<10%) if the annealing phase is skipped or insufficient math data is included

- **First 3 experiments:**
  1. Tokenizer Efficiency Test: Measure tokens-per-word fertility on a held-out French legal corpus vs. standard Llama tokenizers
  2. Context Retrieval Stress Test: Run "Needle in a Haystack" at 30k tokens to verify Phase 2 effectiveness on out-of-distribution document types
  3. Ablation on Annealing Data: Train two variants—one with Flan v2 and one without—to isolate the impact of instruction-style data on the final reasoning benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the equal balancing of French and English data (roughly 33% each) specifically impact performance on English-centric benchmarks compared to English-dominant training mixes?
- Basis in paper: [explicit] In Section 4.5.2, the authors state: "In future work, we plan to study how training on equal amounts of French and English data might have impacted Lucie’s results on English benchmarks."
- Why unresolved: While Lucie-7B lags behind models like Llama-3.1-8B on English benchmarks, it is currently unclear if this gap is primarily due to the language data ratio, the total token count, or the nature of the data itself.
- What evidence would resolve it: Ablation studies training models with identical architectures but varying French/English ratios (e.g., 90/10 vs 50/50) evaluated on the Open LLM Leaderboard benchmarks.

### Open Question 2
- Question: Does instruction fine-tuning on short sequences (4,096 tokens) inevitably degrade the effective context window established during long-context pretraining?
- Basis in paper: [explicit] Section 5.4 notes the reduction in context window for Lucie-7B-Instruct-v1.1 and states: "The reduction... underscores the need for fine-tuning and alignment of the Lucie-7B foundation model with datasets including longer training sequences."
- Why unresolved: The "Needle in a Haystack" results show a context shrinkage from 32k to 22k tokens for the synthetic instruct model, but the optimal data composition to prevent this forgetting is not yet determined.
- What evidence would resolve it: Fine-tuning experiments using instruction datasets with sequence lengths up to 32,000 tokens to see if the full context window can be preserved.

### Open Question 3
- Question: How does the inclusion of high volumes of OCR-processed cultural heritage documents affect the general capabilities and noise robustness of the model?
- Basis in paper: [explicit] Section 4.5.2 notes the aim to "better understand how the type and quality of our French data—our French datasets contain more documents retrieved from OCR for example and less refined web data—might have influenced performance."
- Why unresolved: The dataset includes significant OCR data (e.g., Gallica) filtered by perplexity, but the specific trade-off between cultural heritage coverage and the potential negative impact of OCR noise on benchmark performance remains unquantified.
- What evidence would resolve it: A comparison of benchmark performance between models trained on the raw OCR data versus models trained on the same data after more aggressive cleaning or manual correction.

### Open Question 4
- Question: Is it more effective to introduce high-quality math and code data during the main pretraining phase rather than solely during the final annealing phase?
- Basis in paper: [explicit] Section 6 concludes: "...it may have been better to focus more on math and code earlier on in the training process."
- Why unresolved: The annealing phase significantly improved GSM8K scores, but the authors hypothesize that earlier exposure to reasoning-heavy data might yield superior capabilities compared to the current strategy of late-stage injection.
- What evidence would resolve it: Training runs comparing a "curriculum" approach (math/code included from step 0) against the current "annealing" approach, controlling for total training compute.

## Limitations

- The evaluation primarily uses French-specific and multilingual benchmarks, with limited direct comparison to state-of-the-art English-only models on their native benchmarks
- While human-created instruction data claims CC-BY licensing, the methodology for verifying all sources are properly licensed remains underspecified
- The 32k context extension shows successful retrieval in controlled tests, but real-world document types beyond the training distribution may reveal limitations

## Confidence

- **Multilingual Performance Claims**: Medium confidence - Strong on French benchmarks, but limited direct comparison to English-only state-of-the-art models
- **Cultural Bias Reduction**: Medium confidence - Supported by data distribution but lacking direct cultural competency measurements
- **Context Length Extension**: High confidence - Technical methodology is well-documented and validated through retrieval tests
- **Annealing Effectiveness**: High confidence - Clear performance improvements on reasoning benchmarks are demonstrated
- **Open Data Rights**: Low confidence - Claims of open licensing are stated but verification methodology is not detailed

## Next Checks

1. **Cross-Cultural Bias Audit**: Conduct a systematic evaluation of cultural bias using standardized cultural competency benchmarks (e.g., Cross-Cultural Validation Suite) comparing Lucie-7B against both multilingual and English-only models on identical tasks across multiple cultural contexts.

2. **Real-World Long Context Performance**: Test Lucie-7B on actual long-document retrieval tasks using diverse document types (legal documents, technical specifications, academic papers) beyond the synthetic needle-in-a-haystack format to identify context window limitations in practical scenarios.

3. **English Benchmark Comparison**: Evaluate Lucie-7B on recent English-only benchmarks (e.g., MMLU-Pro, HumanEval) against current state-of-the-art English models to quantify the performance trade-off between multilingual balance and English-specific capabilities.