---
ver: rpa2
title: 'BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees
  and Notifications'
arxiv_id: '2509.24908'
source_url: https://arxiv.org/abs/2509.24908
tags:
- summaries
- dataset
- language
- summarization
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of extreme summarization datasets
  for Spanish legal texts by introducing BOE-XSUM, a curated dataset of 3,648 concise
  summaries of Spain's State Official Gazette (BOE) articles. The dataset includes
  both the original BOE documents and journalist-written summaries, refined into clear,
  everyday language.
---

# BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications

## Quick Facts
- arXiv ID: 2509.24908
- Source URL: https://arxiv.org/abs/2509.24908
- Reference count: 39
- Introduces 3,648-sample extreme summarization dataset for Spanish legal texts

## Executive Summary
This paper introduces BOE-XSUM, a novel dataset of 3,648 concise summaries of Spanish legal decrees from the State Official Gazette (BOE). The dataset pairs original legal documents with journalist-written summaries refined into everyday language, achieving an extreme compression rate of 0.005% (averaging 17 words per summary). The authors demonstrate that medium-sized language models fine-tuned on this specialized dataset significantly outperform general-purpose models in zero-shot settings for extreme summarization tasks.

The study reveals that task-specific fine-tuning on domain-appropriate data can yield substantial performance gains even with smaller models. The best fine-tuned model (BERTIN GPT-J 6B) achieved a 24% improvement over the top zero-shot model (DeepSeek-R1), with BERTScores of 41.6% versus 33.5%. This finding challenges the assumption that larger general-purpose models inherently perform better on specialized tasks, suggesting that appropriate fine-tuning can be more effective than model scale alone.

## Method Summary
The authors created BOE-XSUM by curating summaries of BOE articles originally written by journalists and refined them into clear, everyday language. They fine-tuned medium-sized language models (BERTIN GPT-J variants) on this dataset and compared their performance against general-purpose models in zero-shot settings. The evaluation used BERTScore as the primary metric, measuring semantic similarity between generated and reference summaries. The models were assessed on their ability to produce extremely concise summaries while maintaining the core meaning of complex legal documents.

## Key Results
- BOE-XSUM dataset contains 3,648 samples with 0.005% compression rate (17 words average summary length)
- Fine-tuned BERTIN GPT-J 6B achieved 41.6% BERTScore, outperforming zero-shot DeepSeek-R1 at 33.5%
- Fine-tuned models showed 24% performance gain over the best zero-shot model
- Smaller, task-specific fine-tuned models outperformed larger general models when properly adapted to the domain

## Why This Works (Mechanism)
The success of fine-tuned models on BOE-XSUM stems from domain adaptation to Spanish legal language patterns. Legal texts contain specialized terminology, formal structures, and precise meanings that general-purpose models struggle to capture in extreme summarization. By training on journalist-refined summaries that preserve legal accuracy while using clear language, the fine-tuned models learn to balance brevity with semantic fidelity specific to legal contexts. This targeted adaptation allows models to recognize and appropriately condense legal concepts that would be lost or misinterpreted by zero-shot approaches.

## Foundational Learning
**Extreme Summarization**: Condensing documents to 1-2% of original length while preserving core meaning. Needed because legal summaries require extreme brevity for accessibility. Quick check: Verify compression ratio matches stated 0.005% across dataset samples.

**Domain Adaptation**: Fine-tuning models on specialized legal text to capture terminology and structure. Needed because general models lack legal domain knowledge. Quick check: Compare legal term usage rates between fine-tuned and zero-shot outputs.

**Semantic Evaluation**: Using BERTScore to measure meaning preservation in generated summaries. Needed because traditional metrics fail at extreme compression levels. Quick check: Correlate BERTScore with human assessments of legal accuracy.

## Architecture Onboarding
**Component Map**: Legal Document Corpus -> BOE-XSUM Dataset -> Fine-tuning Pipeline -> Evaluation Framework -> Performance Comparison
**Critical Path**: Document collection → Dataset curation → Model fine-tuning → BERTScore evaluation → Result analysis
**Design Tradeoffs**: Chose medium-sized models (6B parameters) over larger ones for fine-tuning efficiency versus potential accuracy gains from bigger models
**Failure Signatures**: Zero-shot models produce summaries missing legal precision; fine-tuned models may overfit to training distribution
**First Experiments**: 1) Test different learning rates for fine-tuning stability, 2) Compare performance across BOE document categories, 3) Evaluate summary coherence with human judges

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on BERTScore, which may not fully capture legal accuracy or coherence requirements
- Limited comparison set of zero-shot models may miss stronger general-purpose alternatives
- Dataset size of 3,648 samples may be insufficient for training very large models or robust cross-validation
- Performance comparison conflates model architecture differences with fine-tuning effects

## Confidence
**Dataset construction and characteristics** (High confidence): Dataset creation process, size, and compression statistics are clearly documented and verifiable.
**Fine-tuning methodology** (Medium confidence): Standard approach but lacks full hyperparameter and training detail specification.
**Performance claims** (Medium confidence): Substantial gains reported but dependent on evaluation metric choice and limited baseline comparison.

## Next Checks
1. Re-evaluate model performance using multiple metrics including ROUGE variants, human evaluation for legal accuracy, and faithfulness scores to determine if BERTScore results generalize across evaluation frameworks.

2. Expand the comparison to include additional zero-shot models (e.g., GPT-4, Claude) and fine-tuned smaller models (1.3B and 3B parameters) to better isolate the effects of fine-tuning versus model scale.

3. Conduct cross-validation with different data splits and test on held-out time periods to assess whether performance gains are consistent across different BOE document types and time periods.