---
ver: rpa2
title: 'TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation
  Models in Federated Learning'
arxiv_id: '2509.26524'
source_url: https://arxiv.org/abs/2509.26524
tags:
- learning
- tasks
- margin
- federated
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TAP introduces a two-stage approach for personalizing multi-task,
  multi-modal foundation models in federated learning. The first stage selectively
  replaces model components when they benefit a client's local tasks, using margin-based
  criteria to preserve personalization.
---

# TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning

## Quick Facts
- **arXiv ID:** 2509.26524
- **Source URL:** https://arxiv.org/abs/2509.26524
- **Reference count:** 40
- **Key outcome:** TAP achieves 2-9% average accuracy improvements in text generation through two-stage personalization, maintaining high performance while only requiring 18-29% of parameters to be trainable.

## Executive Summary
TAP introduces a two-stage approach for personalizing multi-task, multi-modal foundation models in federated learning. The first stage selectively replaces model components when they benefit a client's local tasks using margin-based criteria, preserving personalization while extracting beneficial knowledge from federated training. The second stage applies knowledge distillation post-FL to incorporate generalizable knowledge without compromising personalization. The method demonstrates consistent improvements across 8 datasets using FLAVA and ViLT models, outperforming state-of-the-art baselines while maintaining parameter efficiency.

## Method Summary
TAP operates in two stages within a component-wise federated learning framework. Stage 1 performs adaptive replacement during FL where clients selectively replace their personalized parameters with server-sent parameters when the server model demonstrates statistically meaningful improvement (using margin-based indicators). Stage 2 applies knowledge distillation from the final FL model to the personalized model post-training, transferring generalizable knowledge while preserving local specialization. The approach partitions model parameters into disjoint blocks per modality-task pair, enabling heterogeneous clients to share relevant components without architectural coupling. Key efficiency gains come from freezing encoders and backbone weights while training only LoRA matrices and task-specific decoders.

## Key Results
- TAP consistently outperforms state-of-the-art baselines across image and text tasks
- Knowledge distillation improves VQA BERTScore by ~18 points (51.81→69.62) and METEOR by ~31 points (17.38→48.74) for FLAVA
- Only 18-29% of total parameters require training, with minimal memory overhead (1.16GB→1.29GB for FLAVA)
- Server model convergence degrades with more modality-task pairs, validating the need for personalization

## Why This Works (Mechanism)

### Mechanism 1: Margin-based Adaptive Replacement
Selective parameter replacement from the server model to a local personalized model—only when the server model demonstrates statistically meaningful improvement—preserves personalization while extracting beneficial knowledge from federated training. Each client maintains two models and replaces parameters iff the FL model's loss plus margin is less than the personalized model's loss. This ensures replacement only when FL parameters outperform personalized parameters by a meaningful threshold.

### Mechanism 2: Post-FL Knowledge Distillation
Knowledge distillation from the FL-trained model to the personalized model after federation ends transfers generalizable representations without direct parameter overwriting. The final server parameters are trained locally and used as a teacher, with the personalized model minimizing a combined loss of task-specific loss plus KL-divergence between teacher and student logits.

### Mechanism 3: Component-wise Aggregation under Modality-Task Decomposition
Partitioning model parameters into disjoint blocks per modality-task pair, with component-level FedAvg, enables heterogeneous clients to share relevant parameters without architectural coupling. Each client owns a subset of blocks based on their modalities and tasks, with aggregation applying FedAvg per-component, combining only clients sharing that component.

## Foundational Learning

- **Concept: Federated Averaging (FedAvg)**
  - Why needed here: TAP uses component-wise FedAvg as the base aggregation protocol. Understanding how local gradients are averaged and how client selection affects convergence is essential.
  - Quick check question: Can you explain why FedAvg's convergence depends on client data heterogeneity and local iteration count τ?

- **Concept: Knowledge Distillation (KL-divergence formulation)**
  - Why needed here: Stage 2 of TAP relies on distilling logits from teacher to student. Understanding temperature scaling and the role of soft targets is critical for tuning β_o and τ.
  - Quick check question: Why does using soft targets (softmax with temperature > 1) transfer more information than hard labels?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: TAP freezes encoder and backbone weights, training only LoRA matrices (A, B) and decoders. Understanding low-rank factorization explains the parameter efficiency claims.
  - Quick check question: If W' = W + BA where B ∈ R^{d×r}, A ∈ R^{r×k}, how many trainable parameters does LoRA add versus full fine-tuning?

## Architecture Onboarding

- **Component map:**
  - Server model W: modality encoders W^(E), task decoders W^(D), transformer backbone W^(R) with MoE routing (MoTE + MoME layers), LoRA matrices A, B
  - Client c_i model: subset W[i] = W^(E)[i] ∪ W^(D)[i] ∪ W^(R)[i] ∪ A_i ∪ B_i based on M_i, O_i
  - Frozen components: cW[i] = W^(E)[i] ∪ W^(R)[i] (pre-trained, LoRA-fine-tuned only)
  - Trainable components: fW[i] = A_i ∪ B_i ∪ W^(D)[i]
  - Personalized parameters: X[i] (parallel copy of fW[i], never transmitted to server)
  - Key data structures: R_i ∈ {0,1}^{|O_i|} (replacement indicator), h^{(l)}_{i,o}, h^{(p)}_{i,o} (loss histories)

- **Critical path:**
  1. Initialize: Load pre-trained FLAVA/ViLT, freeze encoders+backbone, initialize LoRA and decoders
  2. Per round t: Sample clients C_t → local train fW[i] for τ iterations → transmit to server
  3. Server: Component-wise FedAvg aggregation → broadcast relevant components to each client
  4. Client replacement: Compute R_i via Equation (3) → update X[i,o] via Equation (4) → train X[i] locally
  5. Post-FL (after T rounds): Train fW[i] for P iterations → distill to X[i] for P iterations via Equation (6)

- **Design tradeoffs:**
  - Lower margin m_{i,o}: More replacements, higher server influence; risk of overwriting local specialization
  - Higher margin: Fewer replacements; risk of missing beneficial early-stage knowledge
  - More modality-task pairs R: Server model struggles to cater to all; TAP's personalization becomes more valuable
  - KD weight β_o: Higher β improves text generation but may not help image tasks

- **Failure signatures:**
  - Stagnant personalization: X[i] performance plateaus early → check if margin is too high, blocking replacements
  - Catastrophic forgetting after KD: X[i] regresses on local tasks → reduce β_o or increase temperature τ
  - Memory overflow: TAP requires ~10-20% more memory; ensure client capacity
  - Slow convergence: With many sparse blocks, check client participation rate K_r per block

- **First 3 experiments:**
  1. Sanity check with homogeneous clients: Set all clients to same modality-task pair; verify TAP ≈ FedAvg
  2. Ablation on margin values: Run FLAVA on image classification with m ∈ {0.01, 0.02, 0.05}; plot cumulative replacements and verify accuracy trend
  3. KD-only baseline: Skip Stage 1 (no replacement), only run Stage 2 KD; compare to full TAP on text generation tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can non-loss-based indicators (e.g., gradient similarity, representation alignment, or uncertainty metrics) improve replacement decisions compared to the current margin-based loss comparison?
- Basis in paper: [explicit] "Avenues of future work could be to explore non-loss based methods of determining replacement."
- Why unresolved: The current method relies solely on comparing local vs. server model losses with fixed margins, which may not capture nuanced knowledge transfer opportunities.
- What evidence would resolve it: Experiments comparing alternative indicators (gradient-based, representation-based) against the margin-based approach across the same eight datasets, measuring both classification accuracy and generation scores.

### Open Question 2
- Question: How can margin hyperparameters be automatically set or adapted when clients have many tasks, without requiring manual per-task-type tuning?
- Basis in paper: [explicit] "Moreover, how to set multiple margin values when the number of tasks per client is large could be another direction for future research."
- Why unresolved: Current experiments use fixed margins (0.005–0.01 depending on task type) set manually; scalability to clients with dozens of tasks is unexplored.
- What evidence would resolve it: Development of an adaptive margin-setting algorithm that performs comparably to manually-tuned margins, validated on experimental setups with ≥10 tasks per client.

### Open Question 3
- Question: What architectural modifications or algorithmic adjustments can mitigate the convergence degradation of the server model as the number of modality-task pairs scales beyond the current experimental range?
- Basis in paper: [inferred] Theorem 4.4 establishes that convergence bounds increase with more modality-task pairs (via terms involving R), and experiments use only 8 datasets across 30 clients—but do not test scaling limits.
- Why unresolved: The analysis proves degradation occurs but offers no mechanism to counteract it beyond personalization.
- What evidence would resolve it: Experiments with progressively more modality-task pairs (e.g., 10, 15, 20+ distinct pairs) showing whether TAP maintains performance gaps over baselines, or convergence analysis incorporating regularization or curriculum strategies.

### Open Question 4
- Question: Does the post-FL knowledge distillation phase provide diminishing returns as the pre-FL training length increases, and can the KD duration (P iterations) be adaptively determined?
- Basis in paper: [inferred] The fixed P=50 iterations for post-FL KD adds computational overhead (121 seconds average per client), but the relationship between FL training completeness and KD benefit remains uncharacterized.
- Why unresolved: Ablation shows KD helps text generation substantially more than image tasks, suggesting task-dependent optimal P values may exist.
- What evidence would resolve it: Experiments varying T (FL rounds) and P (KD iterations) systematically to identify optimal allocation of computational budget between the two phases.

## Limitations

- No ablation studies isolating individual contributions of Stage 1 (adaptive replacement) versus Stage 2 (knowledge distillation)
- No analysis of hyper-parameter sensitivity across different data distributions
- No computational cost analysis comparing TAP to simpler personalization methods
- Convergence analysis shows theoretical degradation with more modality-task pairs but doesn't quantify practical impact on TAP's performance gains

## Confidence

- **High confidence:** The core mechanism of margin-based adaptive replacement and its effectiveness in preserving personalization while incorporating server knowledge
- **Medium confidence:** The convergence analysis claims about server model degradation with more pairs
- **Medium confidence:** Post-FL knowledge distillation consistently improving text generation tasks

## Next Checks

1. **Ablation study:** Run TAP with Stage 1 only (no KD), Stage 2 only (no replacement), and full TAP on FLAVA VQA task to quantify individual contribution of each stage to the 69.62 BERTScore result
2. **Hyperparameter sensitivity:** Vary margin m from 0.005 to 0.05 in steps of 0.01 for image classification; plot replacement frequency vs accuracy to validate Figure 4 findings and identify optimal operating range
3. **Client heterogeneity test:** Create extreme heterogeneity (some clients have only image tasks, others only text) and measure whether TAP's personalization advantage increases compared to homogeneous client distribution, testing the convergence analysis prediction