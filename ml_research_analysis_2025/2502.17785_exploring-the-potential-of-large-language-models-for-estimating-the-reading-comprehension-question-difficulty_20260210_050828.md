---
ver: rpa2
title: Exploring the Potential of Large Language Models for Estimating the Reading
  Comprehension Question Difficulty
arxiv_id: '2502.17785'
source_url: https://arxiv.org/abs/2502.17785
tags:
- difficulty
- reading
- comprehension
- question
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated the potential of Large Language Models (LLMs)
  for estimating reading comprehension question difficulty, addressing the challenge
  of scalability in traditional psychometric methods like IRT. Using the SARA dataset,
  the researchers evaluated GPT-4o and o1 models both in answering comprehension questions
  and estimating difficulty levels.
---

# Exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty

## Quick Facts
- **arXiv ID**: 2502.17785
- **Source URL**: https://arxiv.org/abs/2502.17785
- **Reference count**: 31
- **Primary result**: Large Language Models achieved near-perfect accuracy (98.84%-99.14%) in reading comprehension tasks and demonstrated meaningful alignment with traditional psychometric difficulty measures, suggesting their potential as scalable tools for automated question difficulty estimation.

## Executive Summary
This study investigates the capability of Large Language Models (LLMs) to estimate reading comprehension question difficulty, addressing the scalability challenges of traditional psychometric methods like Item Response Theory (IRT). Using the SARA dataset, researchers evaluated GPT-4o and o1 models on both answering comprehension questions and estimating difficulty parameters. The LLMs achieved remarkable accuracy rates of 98.84% and 99.14% respectively, surpassing human performance at 87.57%. The study found that LLM-generated difficulty estimates aligned meaningfully with IRT-derived values, though with some compression in extreme difficulty ranges, particularly with the o1 model. These findings suggest LLMs could serve as scalable, automated tools for difficulty assessment in adaptive educational systems, complementing traditional psychometric approaches while highlighting areas for improvement in capturing nuanced item complexity.

## Method Summary
The research employed a two-stage methodology using the SARA reading comprehension dataset. First, both GPT-4o and o1 models were evaluated on their ability to answer reading comprehension questions, with performance measured against human responses. Second, the models were prompted to estimate difficulty parameters for each question, generating estimates that were then compared to traditional IRT-derived difficulty values. The study focused on analyzing the correlation between LLM-generated difficulty estimates and established psychometric measures, examining how well the models could capture the complexity spectrum of comprehension questions. The research specifically examined the models' sensitivity to extreme item characteristics and their ability to produce difficulty estimates across the full range of question complexity.

## Key Results
- GPT-4o and o1 models achieved 98.84% and 99.14% accuracy respectively in answering reading comprehension questions, compared to 87.57% human performance
- LLM-generated difficulty estimates showed meaningful alignment with traditional IRT-derived difficulty values
- The o1 model demonstrated compressed difficulty estimates, particularly for extreme item characteristics, suggesting potential limitations in capturing the full complexity spectrum

## Why This Works (Mechanism)
The success of LLMs in estimating reading comprehension difficulty stems from their extensive training on diverse textual data, which enables them to develop sophisticated semantic understanding and pattern recognition capabilities. These models have learned to identify subtle linguistic features, conceptual complexity, and cognitive demands embedded in questions through exposure to vast amounts of educational content. Their ability to process and analyze text at multiple levels - from syntactic structure to deeper semantic relationships - allows them to assess the cognitive load required to answer comprehension questions. The models' capacity to reason about relationships between text passages and questions, combined with their understanding of educational assessment principles implicitly learned during training, enables them to generate difficulty estimates that correlate with traditional psychometric measures.

## Foundational Learning
**Item Response Theory (IRT)**: A psychometric framework that models the probability of correct responses based on item characteristics and student ability. Why needed: Provides the gold standard for difficulty measurement that LLM estimates are compared against. Quick check: Can you explain how the discrimination parameter affects the slope of the IRT curve?

**Reading Comprehension Assessment**: The evaluation of understanding through questions that test various cognitive processes from literal recall to inferential reasoning. Why needed: Forms the domain-specific context for both LLM training and evaluation. Quick check: What are the main cognitive processes assessed in reading comprehension questions?

**Psychometric Scaling**: Methods for transforming raw scores into standardized difficulty parameters. Why needed: Essential for comparing LLM-generated estimates with traditional measurement approaches. Quick check: How do you convert raw accuracy rates to IRT difficulty parameters?

## Architecture Onboarding
**Component Map**: Input Text -> LLM Processing -> Feature Extraction -> Difficulty Estimation -> IRT Comparison
**Critical Path**: Question Text + Passage -> LLM Analysis -> Difficulty Parameter Generation -> Correlation Analysis with IRT Values
**Design Tradeoffs**: The study prioritized model accuracy over computational efficiency, using high-parameter models that may not be practical for real-time applications. The focus on a single dataset limited generalizability but ensured controlled comparison conditions.
**Failure Signatures**: Compressed difficulty estimates for extreme items, particularly with the o1 model, indicate limitations in capturing the full complexity spectrum. The models showed reduced sensitivity to subtle difficulty variations, suggesting potential blind spots in their assessment capabilities.
**First Experiments**: 1) Test model accuracy across multiple diverse reading comprehension datasets to assess generalizability. 2) Systematically compare LLM difficulty estimates against IRT parameters using items spanning the full difficulty range. 3) Validate LLM-generated difficulty estimates by predicting actual student performance in adaptive testing scenarios.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The study relied on a single dataset (SARA), which constrains generalizability to other reading comprehension domains or question types
- Reported high accuracy rates may reflect specific characteristics of the dataset rather than universal LLM capabilities
- The research did not fully explore how well LLM-extracted features translate to accurate student ability estimation, critical for adaptive testing systems

## Confidence
- **High confidence**: LLMs can accurately answer reading comprehension questions at near-human or superhuman levels
- **Medium confidence**: LLMs can produce difficulty estimates that meaningfully correlate with traditional psychometric measures
- **Low confidence**: LLMs can fully replace traditional psychometric approaches for adaptive assessment systems

## Next Checks
1. Test the same LLM models across multiple diverse reading comprehension datasets to assess generalizability of difficulty estimation capabilities
2. Conduct a systematic comparison of LLM-estimated difficulty parameters against IRT parameters using items specifically designed to span the full range of difficulty and complexity
3. Implement a validation study where LLM-generated difficulty estimates are used to predict actual student performance in an adaptive testing scenario