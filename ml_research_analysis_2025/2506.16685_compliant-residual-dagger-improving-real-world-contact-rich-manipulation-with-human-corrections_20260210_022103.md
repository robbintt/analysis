---
ver: rpa2
title: 'Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation
  with Human Corrections'
arxiv_id: '2506.16685'
source_url: https://arxiv.org/abs/2506.16685
tags:
- policy
- correction
- data
- residual
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving real-world robot
  policies for contact-rich manipulation tasks through human corrections. It introduces
  Compliant Residual DAgger (CR-DAgger), which consists of two key components: a Compliant
  Intervention Interface that enables humans to provide smooth, delta-based corrections
  without interrupting robot execution, and a Compliant Residual Policy that learns
  from corrections while incorporating force feedback.'
---

# Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections

## Quick Facts
- arXiv ID: 2506.16685
- Source URL: https://arxiv.org/abs/2506.16685
- Reference count: 33
- Key outcome: Achieves 64% improvement in base policy success rates for contact-rich manipulation through human corrections

## Executive Summary
This paper addresses the challenge of improving real-world robot policies for contact-rich manipulation tasks through human corrections. It introduces Compliant Residual DAgger (CR-DAgger), which consists of two key components: a Compliant Intervention Interface that enables humans to provide smooth, delta-based corrections without interrupting robot execution, and a Compliant Residual Policy that learns from corrections while incorporating force feedback. The system achieves significant improvements on four contact-rich tasks (book flipping, belt assembly, cable routing, and gear insertion), increasing base policy success rates by 64% while outperforming retraining-from-scratch and finetuning approaches. The method demonstrates effective learning with minimal correction data (50-100 demonstrations) and provides practical guidance for implementing DAgger in real-world settings.

## Method Summary
CR-DAgger combines a Compliant Intervention Interface with a Compliant Residual Policy to improve contact-rich manipulation. The interface uses admittance control to enable humans to apply delta corrections through a handle mounted on the end-effector while the base policy continues executing. The residual policy runs at 50 Hz and learns to predict delta poses and target forces from visual, proprioceptive, and force inputs, with frozen base policy weights. Training uses dense sampling after correction starts and large batches to stabilize learning. The approach requires 50-100 correction episodes and achieves significant success rate improvements over base policies, finetuning, and retraining approaches.

## Key Results
- Achieves 64% improvement in base policy success rates across four contact-rich tasks
- Outperforms retraining-from-scratch and finetuning approaches
- Requires only 50-100 correction demonstrations for effective learning
- Demonstrates superior performance with force feedback integration (45% higher success than position-only baselines)
- Shows robust learning with dense sampling after correction starts and large batch training

## Why This Works (Mechanism)

### Mechanism 1: On-Policy Delta Correction via Compliance Control
Delta corrections (applied as forces while base policy executes) produce higher-quality training data than take-over corrections. The human applies force through a handle; an admittance controller responds to both human and contact forces while the base policy continues running. This allows the human to perceive policy intent through haptic feedback and modulate correction magnitude smoothly. The core assumption is that tracking error remains smaller than typical human correction magnitudes to avoid intention misinterpretation.

### Mechanism 2: Residual Policy with Force Modality Injection
A lightweight residual policy that receives force feedback and outputs both delta poses and target forces outperforms retraining or finetuning the base policy. The residual policy runs at 50 Hz vs. base policy at 1 Hz, takes visual/proprioceptive inputs plus force readings, and outputs 15-DoF actions. The base policy's image encoder is frozen while only the residual head is trained on correction data. This assumes the base policy has at least 10-20% success rate.

### Mechanism 3: Correction-Prioritized Sampling with Distribution Stabilization
Dense sampling immediately after intervention starts, combined with large-batch training, improves reactivity and stability. Samples are weighted more heavily for a window after correction start, with no-correction segments included but labeled as zero-residual actions. Large batch size (50 episodes) mitigates catastrophic forgetting under non-stationary data distributions.

## Foundational Learning

- **Compliance/Admittance Control**
  - Why needed here: The interface relies on admittance control to render the robot back-drivable under human force; without this, delta corrections would require full control authority switch.
  - Quick check question: Can you explain the difference between impedance and admittance control, and why admittance is used for non-back-drivable robots?

- **DAgger and Distribution Shift**
  - Why needed here: CR-DAgger is a DAgger variant; understanding why corrections must stay near the policy's induced state-action distribution is critical to interpreting the delta-correction design.
  - Quick check question: Why does take-over correction risk greater distribution shift than delta correction?

- **Residual Policy Formulation**
  - Why needed here: The core architecture assumes a frozen base policy with a learnable additive residual; this differs from finetuning or retraining.
  - Quick check question: What are the tradeoffs between training a residual policy vs. finetuning the base policy with new data?

## Architecture Onboarding

- **Component map**: Wrist camera -> Base policy (1 Hz) -> Admittance controller (500 Hz) <- Human force handle <- Residual policy (50 Hz) <- Force/torque sensor

- **Critical path**:
  1. Deploy base policy; observe failures
  2. Collect 50-100 correction episodes via Compliant Intervention Interface (human applies force while marking correction start/stop via handle button)
  3. Train residual policy on correction data with dense post-correction sampling and large batches
  4. Deploy combined policy (base + residual) through admittance controller

- **Design tradeoffs**:
  - Large batch (50) vs. small batch (10): Large batches stabilize training; small batches enable faster iteration but risk instability and catastrophic forgetting
  - Include no-correction data vs. correction-only: No-correction data stabilizes policy in uncorrected regions but may dilute corrective signal
  - Force modality: Essential for contact-rich tasks; adds sensor complexity and requires force-torque calibration

- **Failure signatures**:
  - Residual policy outputs noisy or large-magnitude actions after training on take-over data (distribution shift)
  - Correction direction inverted in tasks with small correction magnitudes (tracking error exceeds correction; intention misinterpretation)
  - Policy drifts out of distribution when finetuning on correction data alone

- **First 3 experiments**:
  1. Validate tracking error reduction: Measure end-effector error during fast base-policy motions before/after adding velocity tracking term and look-ahead; confirm max error < 5 mm
  2. Ablate correction interface: Collect 50 episodes each using delta correction vs. take-over; compare velocity smoothness at correction boundaries and resulting policy success
  3. Ablate force modality: Train residual policy with vs. without force input/output on belt assembly; quantify success-rate difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can theoretical guidelines be established to determine the optimal trade-off between improving the base policy versus training the residual policy?
- Basis in paper: [explicit] The authors state in Section 5 that a future direction is "to derive theoretical guidelines for such trade-off," noting the current reliance on empirical thresholds.
- Why unresolved: The paper currently recommends a heuristic (10-20% base success rate) but lacks a formal model for resource allocation between base and residual improvements.
- What evidence would resolve it: A theoretical framework or empirical study mapping base policy performance thresholds to the data efficiency and convergence of the residual learner.

### Open Question 2
- Question: Can more expressive policy architectures, such as Flow Matching, significantly outperform shallow MLPs for tasks with distinctive action multi-modalities?
- Basis in paper: [explicit] The authors hypothesize in Section 5 and Appendix A.4 that expressive structures might help with "distinctive action multi-modalities," despite finding no benefit in their specific homogeneous tasks.
- Why unresolved: The tasks tested in the paper exhibited homogeneous failure modes, rendering the comparison between MLPs and Flow Matching inconclusive.
- What evidence would resolve it: Comparative evaluations of MLPs versus generative architectures on contact-rich tasks specifically designed to exhibit high variance in valid corrective actions.

### Open Question 3
- Question: Can a world or dynamics model be integrated to guide correction behavior under novel perturbations to increase robustness?
- Basis in paper: [explicit] The authors ask in Section 5: "Can we use a world model/dynamics model to guide the correction behavior under novel perturbations?"
- Why unresolved: The current method lacks "extreme robustness under large variations" because the residual policy is trained on limited data without explicit physical modeling.
- What evidence would resolve it: Demonstration of a CR-DAgger variant utilizing a learned dynamics model to successfully generalize to physical variations (e.g., stiffness changes) unseen in the correction data.

## Limitations

- Generalization across robot platforms and base policy architectures is untested
- Long-term policy stability beyond 50-100 correction episodes is unknown
- Physical handle attachment introduces mechanical compliance variations across users

## Confidence

- Delta correction mechanism: **High** (supported by tracking error measurements and ablation)
- Residual policy with force: **Medium** (effective on tested tasks but architecture details sparse)
- Sampling strategy benefits: **Low** (weak direct evidence, no ablation shown)

## Next Checks

1. Test intention misinterpretation by measuring end-effector tracking error during high-velocity base policy motions; verify correction sign fidelity
2. Ablate force modality on a task where base policy has >80% success rate to isolate force contribution
3. Compare policy performance after 200 correction episodes to assess long-term stability and potential degradation