---
ver: rpa2
title: 'Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali
  to Sylheti'
arxiv_id: '2510.18898'
source_url: https://arxiv.org/abs/2510.18898
tags:
- translation
- sylheti
- language
- machine
- bengali
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a machine translation study on the Bengali\u2013\
  Sylheti language pair, a low-resource scenario due to Sylheti's underrepresentation\
  \ in NLP resources. The authors fine-tuned three transformer-based models\u2014\
  mBART-50, MarianMT, and NLLB-200\u2014on a curated 5,002-sentence parallel corpus,\
  \ and compared their performance with five zero-shot large language models (LLMs)\
  \ including GPT-4o, Claude Sonnet 4, Gemini 2.5 Flash, DeepThinkR1, and Sonar."
---

# Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali to Sylheti

## Quick Facts
- arXiv ID: 2510.18898
- Source URL: https://arxiv.org/abs/2510.18898
- Reference count: 28
- Primary result: Fine-tuned transformer models significantly outperform zero-shot LLMs for Bengali–Sylheti translation, with mBART-50 achieving BLEU 21.17 and MarianMT chrF 36.41.

## Executive Summary
This study addresses the low-resource challenge of translating between Standard Bengali and Sylheti, a minority dialect of Bangladesh with limited NLP resources. The authors fine-tune three transformer-based models—mBART-50, MarianMT, and NLLB-200—on a 5,002-sentence parallel corpus and compare their performance against five zero-shot large language models. Results show that fine-tuned transformers substantially outperform zero-shot LLMs, with mBART-50 delivering the highest translation adequacy (BLEU 21.17) and MarianMT providing the most morphologically accurate output (chrF 36.41). The study highlights the importance of task-specific fine-tuning and appropriate model selection for low-resource language pairs.

## Method Summary
The authors constructed a 5,002-sentence Bengali–Sylheti parallel corpus from newspapers, social media, native speakers, and literature, then split it 80/20 for training and testing. Three multilingual transformers (mBART-50, MarianMT, NLLB-200) were fine-tuned with data augmentation (back-translation, synonym replacement, character perturbation) using AdamW optimizer, FP16, and label smoothing. Models were evaluated using BLEU and chrF metrics. Five zero-shot LLMs (GPT-4o, Claude Sonnet 4, Gemini 2.5 Flash, DeepThink_R1, Sonar) were also tested using prompt engineering. mBART-50 required early stopping around epoch 5 due to training instability, while MarianMT trained stably for 30 epochs.

## Key Results
- Fine-tuned transformers significantly outperformed zero-shot LLMs, with average BLEU scores of 17.66 versus 2.09
- mBART-50 achieved highest BLEU score (21.17) but showed training instability requiring early stopping
- MarianMT achieved highest chrF score (36.41) and provided stable, morphologically accurate translations
- Zero-shot LLMs produced poor results (BLEU ≤ 2.89, chrF ≤ 17.33) with frequent code-mixing and literal transfers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific fine-tuning enables multilingual transformers to translate severely underrepresented languages where zero-shot LLMs fail.
- Mechanism: Multilingual pretraining creates cross-lingual representations that remain latent for low-resource languages. Gradient updates on paired corpora adapt attention patterns and vocabulary mappings, surfacing transferable linguistic knowledge from related high-resource languages (Bengali has 270M speakers; Sylheti has 11-15M).
- Core assumption: Sylheti's lexical and grammatical overlap with Bengali allows pretrained Bengali representations to be specialized through fine-tuning.
- Evidence anchors:
  - [abstract]: "Fine-tuned models significantly outperform LLMs, with mBART-50 achieving the highest translation adequacy"
  - [section VI]: "fine-tuned transformers outperformed zero-shot LLMs by an average of 15.57 BLEU (17.66 vs. 2.09)"
  - [corpus]: "LLMs for Low-Resource Dialect Translation Using Context-Aware Prompting" (FMR 0.59) confirms sustained research interest in Sylheti-specific adaptation
- Break condition: If corpus size drops below ~1,000 pairs, or if the target language lacks any representation in pretraining vocabulary, transfer benefits may collapse.

### Mechanism 2
- Claim: Model capacity interacts with dataset size to determine training stability, requiring architecture-specific early stopping strategies.
- Mechanism: Larger-capacity models (mBART-50: 12 layers, 16 heads) rapidly memorize limited data, causing performance volatility after peak epochs. Lighter architectures (MarianMT: 6 layers, 8 heads) exhibit smoother gradient flow on small corpora, enabling longer training without degradation.
- Core assumption: Overfitting dynamics scale inversely with dataset size and positively with model parameter count.
- Evidence anchors:
  - [section VI]: "mBART-50 BLEU peaked at 21.17 (epoch 5), but dropped sharply to 11.99 (epoch 7) before recovering"
  - [section VI]: "MarianMT training loss decreased smoothly from 2.72 to 0.41, validation loss stabilized around 1.55–1.73 after epoch 10"
  - [corpus]: Corpus evidence on training dynamics for Sylheti specifically is limited; neighboring papers focus on inference strategies rather than fine-tuning stability
- Break condition: If regularization hyperparameters (dropout, weight decay) are misconfigured, stability patterns may invert regardless of architecture.

### Mechanism 3
- Claim: Character-level metrics (chrF) better capture translation quality for languages with non-standardized orthography than word-level metrics (BLEU).
- Mechanism: Sylheti's orthographic variation produces multiple valid spellings for the same word. chrF's character n-gram F-scores award partial credit for morphologically close outputs, while BLEU's word-level precision penalizes valid alternatives as errors.
- Core assumption: Reference translations represent one valid spelling among multiple acceptable variants.
- Evidence anchors:
  - [section VI]: "Orthographic variation in training data also influenced all models, sometimes yielding alternate but valid spellings penalized by BLEU"
  - [section VI]: "MarianMT achieved the highest chrF (36.41)... suggesting it produces character-accurate but slightly less reference-aligned translations"
  - [corpus]: "LLM-Based Evaluation of Low-Resource MT" (FMR 0.56) proposes reference-less dialect-guided evaluation, acknowledging limitations of standard metrics
- Break condition: If reference translations contain systematic spelling errors, chrF may reinforce incorrect patterns.

## Foundational Learning

- Concept: **Encoder-Decoder Transformer Architecture**
  - Why needed here: All three fine-tuned models (mBART-50, MarianMT, NLLB-200) use this structure; understanding self-attention and cross-attention is prerequisite to debugging training dynamics.
  - Quick check question: Can you explain why self-attention enables parallel processing compared to RNN sequential computation?

- Concept: **Subword Tokenization (SentencePiece/BPE)**
  - Why needed here: All models use SentencePiece for consistent multilingual input; Sylheti's vocabulary must be composed from shared Bengali/Indic subword units.
  - Quick check question: Why does subword tokenization help with out-of-vocabulary words in low-resource languages?

- Concept: **BLEU vs. chrF Evaluation Metrics**
  - Why needed here: The study reports both metrics because they capture different quality dimensions; chrF's superiority for morphologically rich languages is a key finding.
  - Quick check question: For a language with high spelling variation, which metric would you expect to show higher relative scores, and why?

## Architecture Onboarding

- Component map: Raw parallel corpus (5,002 pairs) → Preprocessing (normalization, deduplication, 80/20 split) → Data augmentation (back-translation, synonym replacement, character perturbation) → Tokenization + language tagging (SentencePiece) → Model selection → [mBART-50 | MarianMT | NLLB-200] → Fine-tuning (AdamW, FP16, label smoothing, early stopping) → Evaluation (BLEU, chrF on held-out test set)

- Critical path:
  1. Corpus quality and orthographic consistency dominate downstream performance—invest heavily here first.
  2. Language tagging must correctly map Bengali (bn_IN/ben_Beng) and Sylheti (syl/sil_Beng) to model-specific codes.
  3. Early stopping checkpoint must be saved at peak validation BLEU, not final epoch, especially for mBART-50.

- Design tradeoffs:
  | Model | Strength | Weakness | Best for |
  |-------|----------|----------|----------|
  | mBART-50 | Highest BLEU (21.17), strong semantic adequacy | Training instability, requires early stopping at epoch 5 | Reference-aligned output quality |
  | MarianMT | Highest chrF (36.41), stable convergence | Lower peak BLEU (18.16) | Production robustness, morphological accuracy |
  | NLLB-200 | Strong chrF (35.98), broad multilingual support | Weakest BLEU (13.65), unstable on small data | Multilingual deployment scenarios |

- Failure signatures:
  - **mBART-50 volatility**: BLEU drops >5 points between adjacent epochs → checkpoint at epoch 5, implement aggressive early stopping
  - **LLM zero-shot failure**: Output contains code-mixing or literal Bengali transfers → abandon zero-shot approach; fine-tuning required
  - **Orthographic noise**: Excessive chrF/BLEU divergence → apply spelling normalization before training

- First 3 experiments:
  1. Establish baseline: Fine-tune MarianMT for 30 epochs on the 80/20 split; log BLEU/chrF per epoch to confirm stable convergence pattern.
  2. Early stopping validation: Fine-tune mBART-50 with checkpointing at every epoch; identify peak BLEU epoch and confirm performance degrades without early stopping.
  3. Ablation on augmentation: Train one model with augmentation (back-translation + synonym replacement + character perturbation) and one without; isolate impact on chrF vs. BLEU to determine if augmentation primarily helps morphological robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does explicit orthographic normalization reduce training volatility and improve translation adequacy in Bengali–Sylheti models?
- Basis in paper: [explicit] The authors note that orthographic inconsistencies introduce noise and recommend "incorporating orthographic normalization" as future work.
- Why unresolved: The current study used a curated but raw corpus where non-standardized spelling in Sylheti may have contributed to the training instability observed in mBART-50.
- What evidence would resolve it: A comparative experiment measuring training loss stability and BLEU scores between models trained on the current dataset versus a normalized version.

### Open Question 2
- Question: Can hybrid approaches combining fine-tuned NMT with large language models outperform standalone transformer models in this low-resource setting?
- Basis in paper: [explicit] The limitations section suggests exploring "hybrid approaches that combine the strengths of pre-trained LLMs with fine-tuned NMT models."
- Why unresolved: The paper only evaluates fine-tuned transformers and zero-shot LLMs independently, showing a massive performance gap but not testing their integration.
- What evidence would resolve it: Architecture experiments where high-scoring NMT outputs (e.g., MarianMT) are used to prompt or refine LLM outputs, evaluated via BLEU and chrF.

### Open Question 3
- Question: How well do the reported BLEU and chrF scores correlate with human judgments of fluency and cultural nuance for Sylheti?
- Basis in paper: [explicit] The authors acknowledge the "reliance on automatic evaluation metrics... may not fully capture the linguistic richness or cultural nuance of Sylheti."
- Why unresolved: The study relied exclusively on automatic metrics, which may penalize valid orthographic variations or miss semantic errors in the low-resource dialect.
- What evidence would resolve it: A human evaluation study using metrics like direct assessment or adequacy/fluency scales, correlated against the automatic scores reported.

## Limitations

- Corpus quality and representativeness: The 5,002-sentence corpus lacks full details on data balance, domain coverage, and orthographic normalization, potentially affecting generalizability.
- Hyperparameter sensitivity: Critical training configurations (learning rates, batch sizes, regularization, early stopping) are not fully specified, making exact replication difficult.
- Zero-shot LLM evaluation: Prompt templates and decoding strategies for LLMs are not detailed, and poor performance may stem from suboptimal prompting rather than fundamental limitations.
- Evaluation metric limitations: chrF and BLEU may not fully capture translation adequacy for morphologically rich, low-resource languages with high orthographic variation.

## Confidence

- **High confidence**: Fine-tuned transformer models significantly outperform zero-shot LLMs on BLEU and chrF for Bengali-Sylheti translation; mBART-50 achieves highest BLEU (21.17), MarianMT highest chrF (36.41).
- **Medium confidence**: Model capacity and dataset size interact to determine training stability, with mBART-50 requiring aggressive early stopping due to volatility.
- **Medium confidence**: chrF is more appropriate than BLEU for evaluating translations in languages with non-standardized orthography, as it rewards character-level similarity over exact word matches.

## Next Checks

1. **Corpus quality audit**: Manually inspect a random sample of the parallel corpus for orthographic consistency, domain balance, and translation accuracy; quantify the proportion of acceptable spelling variants.
2. **Hyperparameter sensitivity analysis**: Systematically vary learning rates, batch sizes, and regularization parameters for mBART-50 and MarianMT; document the impact on training stability and final BLEU/chrF.
3. **Robustness to orthographic variation**: Train and evaluate models on multiple reference translations for the same source sentences (where available), and report variance in BLEU/chrF; test chrF's ability to recognize valid but non-reference spellings.