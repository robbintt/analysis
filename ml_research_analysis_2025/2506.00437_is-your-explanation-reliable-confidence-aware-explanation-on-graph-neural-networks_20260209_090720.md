---
ver: rpa2
title: 'Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural
  Networks'
arxiv_id: '2506.00437'
source_url: https://arxiv.org/abs/2506.00437
tags:
- confidence
- graph
- explanation
- explanations
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unreliable explanations in
  Graph Neural Networks (GNNs) by introducing ConfExplainer, a confidence-aware explanation
  framework. The core method extends the Graph Information Bottleneck (GIB) framework
  with confidence constraints (GIB-CC), incorporating a confidence scoring module
  that dynamically assesses explanation reliability.
---

# Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks

## Quick Facts
- arXiv ID: 2506.00437
- Source URL: https://arxiv.org/abs/2506.00437
- Authors: Jiaxing Zhang; Xiaoou Liu; Dongsheng Luo; Hua Wei
- Reference count: 40
- Primary result: ConfExplainer achieves superior explanation faithfulness (AUC-ROC scores of 97.19%, 90.14%, 77.80%, 59.27%, and 81.52% across five datasets) with confidence-aware reliability scoring

## Executive Summary
This paper addresses the critical problem of unreliable explanations in Graph Neural Networks (GNNs) by introducing ConfExplainer, a confidence-aware explanation framework. The method extends the Graph Information Bottleneck (GIB) framework with confidence constraints (GIB-CC), incorporating a confidence scoring module that dynamically assesses explanation reliability. Experiments demonstrate that ConfExplainer achieves superior explanation faithfulness compared to baselines while providing reliable confidence scores that distinguish between reliable and unreliable explanations, particularly in out-of-distribution scenarios.

## Method Summary
ConfExplainer builds upon the GIB framework by introducing a confidence-constrained objective that jointly learns explanation masks and confidence scores. The core innovation is the calibrated graph formulation G̃ = C⊙G' + (1-C)⊙G_r, where confidence matrix C dynamically weights the explanation subgraph against Gaussian noise. The method employs an iterative alternating optimization scheme where the explainer and confidence evaluator are trained separately to prevent trivial solutions. During inference, the framework provides both the explanatory subgraph and per-edge confidence scores without requiring ground truth labels.

## Key Results
- Achieves state-of-the-art explanation faithfulness with AUC-ROC scores ranging from 59.27% to 97.19% across five datasets
- Confidence scores effectively distinguish reliable from unreliable explanations, particularly under out-of-distribution conditions
- Smooth confidence degradation observed as noise increases, demonstrating robust calibration
- Maintains comparable training efficiency to existing methods while providing trustworthy explanations

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Constrained Information Bottleneck
Extending GIB with confidence constraints enables simultaneous explanation generation and reliability estimation without degrading explanation quality. The GIB-CC objective introduces a calibrated graph G̃ = C⊙G' + (1-C)⊙G_r, where confidence matrix C weights the explanation subgraph against Gaussian noise, creating a differentiable pathway for confidence learning while preserving the original GIB optimization landscape. The theoretical justification relies on the independence assumption I(C, G_r; Y|G') = 0, though the paper acknowledges this may break under severe OOD shifts.

### Mechanism 2: Predictive Confidence Scoring via Joint Embeddings
An MLP operating on concatenated graph embeddings and explanation masks can predict edge-wise confidence scores that correlate with explanation fidelity. The confidence module receives both the GNN's internal representation of the original graph and the candidate explanation mask, enabling it to assess whether the mask captures salient features of the embedding. This approach assumes the GNN encoder's embeddings contain sufficient signal to distinguish high-quality from low-quality explanations before ground truth evaluation.

### Mechanism 3: Iterative Alternating Optimization
Alternating training between explainer and confidence evaluator prevents trivial solutions where confidence collapses to always-high or always-low values. The confidence loss L_C depends on both C and the mask M, and training both jointly would allow the explainer to generate easy-to-score explanations. The iterative approach (alternating even/odd epochs) freezes one module while updating the other, creating a stable equilibrium rather than oscillation.

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs) and message passing**
  - Why needed here: ConfExplainer is model-agnostic but requires understanding how GNNs aggregate neighborhood information to interpret what substructures matter for predictions
  - Quick check question: Can you explain why a 3-layer GCN's receptive field covers 3-hop neighborhoods, and how this affects explanation scope?

- **Concept: Information Bottleneck principle**
  - Why needed here: GIB-CC inherits the core tradeoff—compressing input while preserving label-relevant information—and extends it with confidence terms
  - Quick check question: What does minimizing I(G, G') - αI(Y, G') optimize for, and what does α control?

- **Concept: Mutual information and conditional independence**
  - Why needed here: The theoretical justification relies on I(C, G_r; Y|G') = 0 being approximately satisfied; understanding when this breaks is critical for OOD deployment
  - Quick check question: If G' perfectly captures all label-relevant structure from G, why does I(C; Y|G') = 0 follow?

## Architecture Onboarding

- **Component map:**
Input Graph G → Pre-trained GNN f → Embedding f_emb(G)
                    ↓
            Explainer E → Mask M → Explanation G*
                    ↓
            Confidence MLP C → Confidence Matrix C
                    ↓
    Calibrated Graph: G̃ = C⊙G* + (1-C)⊙G_r
                    ↓
    Losses: L_GIB (size + prediction) + λ·L_C (confidence)

- **Critical path:** The explainer E and confidence module C must be trained alternately (even/odd epochs). The confidence loss L_C requires ground truth labels during training but not during inference—this is the key deployment advantage.

- **Design tradeoffs:**
  - λ (confidence loss weight): Paper uses λ=100 after sensitivity analysis; lower values reduce confidence calibration, higher values may dominate GIB objective
  - α (GIB tradeoff): Inherited from base GIB methods; controls explanation sparsity vs. fidelity
  - MLP hidden dimension D_n: Treated as hyperparameter; complexity is O(|D_e|) per edge

- **Failure signatures:**
  - Confidence scores uniformly high (>0.95) regardless of noise: Confidence module not learning; check if frozen during explainer updates
  - AUC drops sharply with minimal noise: Explainer overfitting to training distribution; consider regularization or data augmentation
  - Oscillating losses across epochs: Learning rate mismatch; reduce one module's learning rate relative to the other

- **First 3 experiments:**
  1. **Sanity check:** Run ConfExplainer on BA-2motifs with ground-truth motifs visible; verify confidence scores correlate (Pearson r > 0.7) with explanation AUC across noise levels ε ∈ [0, 0.1, 0.2]
  2. **Ablation:** Compare full model vs. w/o confidence loss vs. w/o confidence module on MUTAG; expect 5-15% AUC drop per ablation (per Figure 6)
  3. **OOD stress test:** Train on 80% of Benzene, evaluate on Fluoride-Carbonyl; if confidence scores remain calibrated (smooth degradation) but AUC drops >20%, this validates OOD detection capability without requiring ground truth at test time

## Open Questions the Paper Calls Out

### Open Question 1
Can the confidence-aware GIB-CC framework be effectively extended to dynamic or evolving graphs?
- Basis: The authors list "extending our confidence-aware framework to dynamic and evolving graphs" as a primary future direction in the Conclusion
- Why unresolved: The current methodology and experiments are restricted to static graph structures and do not account for temporal dependencies or changing topologies
- What evidence would resolve it: A modification of the confidence scoring module to handle temporal features and experimental validation on dynamic graph benchmarks

### Open Question 2
How can the theoretical assumption I(C, G_r; Y|G') = 0 be relaxed to handle severe out-of-distribution (OOD) shifts?
- Basis: Appendix B states that this condition "may not strictly hold in all cases," such as severe OOD shifts, and suggests exploring "weaker or relaxed variants" in future work
- Why unresolved: The current theoretical equivalence between GIB and GIB-CC relies on this strict independence, which may break down when the explainer fails to capture label-relevant information
- What evidence would resolve it: A theoretical derivation of a relaxed constraint and empirical evidence showing robust confidence estimation even when the standard independence assumption is violated

### Open Question 3
Does the ConfExplainer framework improve decision-making in complex real-world domains like drug discovery?
- Basis: The Conclusion identifies "integrating confidence-aware explanations into real-world applications such as drug discovery, fraud detection, and decision support systems" as a future goal
- Why unresolved: The study relies on synthetic datasets and molecular datasets with pre-defined ground-truth motifs, which may not capture the complexity of real-world discovery tasks
- What evidence would resolve it: User studies or downstream task performance metrics showing that confidence scores assist experts in identifying valid molecular structures or fraudulent patterns more reliably

## Limitations
- Theoretical assumption I(C, G_r; Y|G') = 0 may break under severe out-of-distribution shifts, potentially decoupling confidence from actual explanation quality
- MLP architecture details for confidence scoring remain underspecified, creating reproduction challenges
- Theoretical guarantees for alternating optimization stability are not proven, raising questions about convergence properties

## Confidence
- **High Confidence**: Explanation faithfulness improvements (AUC-ROC scores: 97.19%, 90.14%, 77.80%, 59.27%, 81.52%) - directly measured and well-documented
- **Medium Confidence**: Confidence score calibration and OOD detection capabilities - validated through controlled noise injection but not against established calibration benchmarks
- **Medium Confidence**: Alternating optimization prevents trivial solutions - theoretically motivated but lacks formal convergence analysis

## Next Checks
1. **Calibration Benchmark Test**: Evaluate ConfExplainer's confidence scores against established calibration metrics (expected calibration error, Brier score) on standard uncertainty datasets like CIFAR-10-C to verify robustness beyond synthetic noise injection
2. **Architecture Sensitivity Analysis**: Systematically vary MLP depth/width and learning rate ratios between explainer and confidence modules to quantify hyperparameter sensitivity and identify optimal configurations
3. **Real-world OOD Deployment**: Deploy ConfExplainer on a temporal graph dataset (e.g., citation networks with year-based splits) to assess confidence score performance when distribution shifts occur naturally rather than through controlled noise injection