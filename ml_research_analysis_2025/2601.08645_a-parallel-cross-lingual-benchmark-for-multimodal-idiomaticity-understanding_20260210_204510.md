---
ver: rpa2
title: A Parallel Cross-Lingual Benchmark for Multimodal Idiomaticity Understanding
arxiv_id: '2601.08645'
source_url: https://arxiv.org/abs/2601.08645
tags:
- idiomatic
- language
- languages
- image
- literal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XMPIE is a multilingual, multimodal dataset of potentially idiomatic
  expressions covering 34 languages and 3,054 expressions with 7,040 images. Each
  PIE is accompanied by five images representing idiomatic, literal, related, and
  distractor meanings.
---

# A Parallel Cross-Lingual Benchmark for Multimodal Idiomaticity Understanding

## Quick Facts
- **arXiv ID**: 2601.08645
- **Source URL**: https://arxiv.org/abs/2601.08645
- **Reference count**: 0
- **Primary result**: XMPIE dataset enables cross-linguistic idiom comparison; models rank literal meanings higher than idiomatic ones (T1-L > T1-I) despite high NDCG@5 scores

## Executive Summary
XMPIE is a multilingual, multimodal dataset of potentially idiomatic expressions covering 34 languages and 3,054 expressions with 7,040 images. Each PIE is accompanied by five images representing idiomatic, literal, related, and distractor meanings. The dataset enables cross-linguistic comparison of idiom realizations and supports evaluation of multilingual and multimodal idiom understanding. Using EVA-CLIP-18B, results show that models consistently rank literal meanings higher than idiomatic ones, with NDCG@5 scores above 0.87 and strong performance on literal identification but weak on idiomatic senses.

## Method Summary
XMPIE introduces a zero-shot cross-lingual and multimodal image ranking task. Given a text query (PIE) and five candidate images, models rank images by cosine similarity in a shared embedding space. The dataset provides 3,054 expressions across 34 languages, each with five images representing idiomatic, literal, related, and distractor meanings. Evaluation uses Top-1/Top-2 accuracy and NDCG@5 with custom graded gains (1, 0.5, 0.5, 1, 0) for image slots. The baseline uses EVA-CLIP-18B as a frozen feature extractor, computing text and image embeddings for ranking.

## Key Results
- Cross-linguistic idiom similarity quantified via lexical overlap networks shows mean density 0.31 and largest component ratio 0.56
- EVA-CLIP-18B achieves NDCG@5 >0.87 across languages but literal Top-1 (0.900 for EN) vastly outperforms idiomatic Top-1 (0.060 for EN)
- Models show consistent literal bias across languages, ranking literal meanings higher than idiomatic ones despite partial understanding captured by NDCG@5

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Idiom Alignment via Graph Connectivity
Cross-linguistic idiom similarity can be quantified through lexical overlap networks, revealing which PIEs share conceptual structure across languages. Construct graphs where nodes are language-specific PIE equivalents (translated to English literal forms) and edges represent shared content words. Graph metrics (density, largest component ratio, clique number) capture lexical cohesion and cross-lingual convergence patterns. Lexical overlap in literal translations correlates with shared metaphorical conceptualization across languages.

### Mechanism 2: Multimodal Retrieval-Based Idiomatic Sense Evaluation
Vision-language models can be evaluated on idiomatic understanding by ranking candidate images against PIE text queries without training. Use pre-trained vision-language models (e.g., EVA-CLIP-18B) to compute cosine similarity between PIE text embeddings and five candidate image embeddings, then rank by similarity scores. Higher text-image similarity indicates better alignment with the intended sense (idiomatic or literal).

### Mechanism 3: Graded Relevance Scoring for Partial Idiomatic Understanding
NDCG@5 with symmetric gains (1, 0.5, 0.5, 1, 0) captures partial idiomatic comprehension by rewarding both target senses and related distractors appropriately. Assign graded relevance gains to five image slots—full credit for idiomatic/literal targets, partial credit for semantically related images, zero for random distractors. NDCG normalizes against ideal ordering to enable cross-item comparison.

## Foundational Learning

- **Non-compositional meaning / Idiomaticity**
  - Why needed here: PIEs have meanings not inferable from constituent words; understanding this distinction is essential for interpreting why models struggle with idiomatic images versus literal ones.
  - Quick check question: Given "green fingers" means "skilled at gardening," would a model that ranks an image of green-colored fingers highly be showing idiomatic or literal understanding?

- **Vision-Language Joint Embeddings (CLIP-style)**
  - Why needed here: The evaluation relies on cosine similarity in a shared text-image embedding space; understanding how these spaces are trained clarifies why literal associations dominate.
  - Quick check question: If CLIP was trained on image-caption pairs, what bias would you expect for "kick the bucket"—literal kicking or dying?

- **Cross-Lingual Alignment Types (1-1, 1-0, 1-N)**
  - Why needed here: The dataset categorizes idiom equivalence patterns; recognizing these types helps diagnose where cross-lingual transfer should work versus fail.
  - Quick check question: "Bad apple" → Turkish "çürük elma" is which alignment type? What about "bear market" → Turkish "düşen piyasa" (descriptive paraphrase)?

## Architecture Onboarding

- **Component map**: PIE text entries -> Image sets -> Graph analysis module -> Evaluation pipeline
- **Critical path**: Load PIE text for target language -> Load 5 candidate images -> Compute text embedding of PIE string -> Compute image embeddings for all 5 candidates -> Rank by cosine similarity -> Compute Top-1/Top-2 accuracy and NDCG@5 with gains (1, 0.5, 0.5, 1, 0)
- **Design tradeoffs**: PIE-only vs. contextual evaluation (current design uses PIE text alone, no disambiguating context), Synthetic (Midjourney) vs. natural images (synthetic ensures controlled semantic content but may introduce generator biases), Symmetric gains for idiomatic/literal (equal credit assumes both senses are equally valid)
- **Failure signatures**: Literal dominance pattern (T1-L ≫ T1-I across languages indicates defaulting to compositional interpretation), Low Top-2 despite high NDCG (NDCG@5 >0.85 but Top-2 <0.1 for idiomatic means ranking relevant images highly but cannot disambiguate sense order), Language-specific anomalies (if ES shows idiomatic > literal unlike other languages, investigate annotation quality or model language bias)
- **First 3 experiments**:
  1. Reproduce baseline on 5 sample languages: Run EVA-CLIP-18B evaluation on EN/BP/ES/CN/TR, verify NDCG@5 ≈ 0.87–0.95 and literal > idiomatic Top-1 pattern
  2. Add context disambiguation: Augment PIE queries with short context sentences (e.g., "She has green fingers" for gardening context) and measure idiomatic Top-1 improvement
  3. Cross-lingual transfer analysis: Select PIEs with 1-1 alignment (e.g., "black market") and compare performance across languages; test whether high-resource language performance predicts low-resource performance for aligned PIEs

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset accessibility is a critical blocker—XMPIE is marked as "ANONYMOUS link" and "forthcoming," making direct reproduction impossible without access to specific images and ground-truth slot mappings
- Lack of contextual disambiguation in PIE-only evaluation systematically biases models toward literal interpretations by conflating idiomatic and literal senses
- Symmetric NDCG gain scheme assumes equal validity of both target senses, potentially masking important differences in the difficulty of idiomatic versus literal understanding

## Confidence

**High Confidence**: Cross-lingual idiom alignment via graph connectivity is well-specified with clear methodology and empirical results. Multimodal retrieval-based evaluation pipeline is precisely defined with measurable outcomes. Foundational concepts of non-compositional meaning and vision-language embeddings are standard in the field.

**Medium Confidence**: Graded relevance scoring mechanism is novel to this benchmark with specified methodology but lacks corpus-based justification for specific gain values. Claims about EVA-CLIP-18B consistently ranking literal meanings higher require validation across diverse language families and model variants.

**Low Confidence**: Claims about cross-lingual transfer patterns and language-specific anomalies depend heavily on dataset access and the assumption that lexical overlap in literal translations correlates with shared metaphorical conceptualization—a relationship that may break for entirely different metaphorical mappings.

## Next Checks

1. **Dataset Access Verification**: Attempt to obtain the XMPIE dataset through the cited anonymous link or contact authors directly. If successful, verify the exact image ordering and ground-truth slot mappings match the paper's specifications.

2. **Cross-Lingual Transfer Validation**: Using any available subset of the dataset, select PIEs with confirmed 1-1 alignment across languages (e.g., "black market" → "mercado negro" → "mercato nero") and test whether high-resource language performance (EN) predicts low-resource performance (ES, IT) for aligned PIEs.

3. **Context Disambiguation Impact Test**: Augment a small sample of PIE queries with disambiguating context sentences (e.g., "She has green fingers" vs. just "green fingers") and measure the impact on idiomatic Top-1 accuracy. This would validate whether the literal bias is primarily due to lack of context rather than fundamental model limitations.