---
ver: rpa2
title: Data distribution impacts the performance and generalisability of contrastive
  learning-based foundation models of electrocardiograms
arxiv_id: '2509.10369'
source_url: https://arxiv.org/abs/2509.10369
tags:
- performance
- cohorts
- cohort
- pretraining
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how cohort composition impacts the performance
  and generalisation of contrastive learning-based foundation models for ECG analysis.
  Using over 5.2 million ECGs from four continents, the researchers pretrained CAPE
  foundation models on diverse datasets and systematically evaluated downstream performance
  across six cohorts.
---

# Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms

## Quick Facts
- arXiv ID: 2509.10369
- Source URL: https://arxiv.org/abs/2509.10369
- Reference count: 40
- Models pretrained on multi-centre data improve in-distribution accuracy but significantly reduce out-of-distribution generalisation due to encoding cohort-specific artifacts

## Executive Summary
This study investigates how cohort composition impacts the performance and generalisation of contrastive learning-based foundation models for ECG analysis. Using over 5.2 million ECGs from four continents, the researchers pretrained CAPE foundation models on diverse datasets and systematically evaluated downstream performance across six cohorts. They found that while multi-centre pretraining improved in-distribution accuracy, it significantly reduced out-of-distribution generalisation due to encoding cohort-specific artifacts. The authors propose an In-Distribution Batch (IDB) strategy that constrains contrastive learning batches to single cohorts, which improved OOD performance from 0.822 to 0.948 AUC for sex classification and reduced age prediction error from 12.00 to 7.35 MAE. These findings highlight the critical importance of cohort design and evaluation protocols for developing clinically fair and robust foundation models in healthcare.

## Method Summary
The CAPE model uses a 4-layer ResNet encoder to learn 256-dimensional representations from 7-second, 8-lead ECG signals (2800×8). The model was pretrained using contrastive learning (InfoNCE loss) on 5.2 million ECGs from BIDMC, CODE, SHZS, and VUMC. Two training strategies were compared: CAPE-X (random batching across cohorts) and CAPE-Z (In-Distribution Batch strategy, constraining each batch to samples from a single cohort). Downstream tasks included age regression (MAE) and sex classification (AUROC) across six external cohorts. The prediction heads used frozen features and trained separate MLPs for each task.

## Key Results
- Multi-centre pretraining improved in-distribution performance but significantly reduced out-of-distribution generalisation
- IDB strategy improved OOD sex classification from 0.822 to 0.948 AUC and reduced age prediction error from 12.00 to 7.35 MAE
- Secondary care cohorts produced more robust features than primary care cohorts due to higher signal diversity
- t-SNE visualizations showed cohort-specific clustering with random batching but no discernible clustering with IDB

## Why This Works (Mechanism)

### Mechanism 1
Randomly sampling batches from heterogeneous multi-centre datasets encourages the model to distinguish samples using cohort-specific artifacts (e.g., recording devices) rather than physiological features. Contrastive learning maximizes dissimilarity between negative pairs. When a batch contains ECGs from different hospitals with distinct hardware, the model finds it computationally efficient to separate signals based on these "trivial" technical variances (shortcuts) rather than complex biological patterns.

### Mechanism 2
Constraining contrastive batches to a single source distribution (In-Distribution Batching or IDB) forces the model to learn robust features by removing the "easy" shortcut of distinguishing between cohorts. By ensuring all negative pairs in a batch originate from the same cohort, technical differences are nullified. The loss function must then rely on intra-cohort physiological variance to distinguish samples, resulting in features that generalize better to unseen OOD data.

### Mechanism 3
Pretraining on secondary care cohorts (sicker patients) produces more robust features than primary care cohorts (healthier patients) due to higher signal diversity. Secondary care ECGs exhibit a wider distribution of morphological abnormalities. Contrastive learning relies on finding discriminative features; "boring" or uniform signals in healthy cohorts offer fewer gradients for the loss function to learn meaningful separations.

## Foundational Learning

- **Concept: Contrastive Learning & InfoNCE Loss**
  - Why needed here: The core engine of the CAPE model. You must understand that the model learns by pulling "positive pairs" (two ECGs from the same patient) closer and pushing "negative pairs" (ECGs from different patients) apart in the latent space.
  - Quick check question: If you mix two distinct datasets with incompatible voltage scales into one batch, which features will the loss function likely maximize to separate the negative pairs: the heart rhythm or the voltage scale?

- **Concept: In-Distribution vs. Out-of-Distribution (OOD) Generalization**
  - Why needed here: The primary metric of success in this paper. In-Distribution (ID) means testing on data similar to training; OOD means testing on external cohorts (e.g., different continents). The paper highlights a trade-off where improving ID performance via multi-centre mixing can hurt OOD performance.
  - Quick check question: A model trained on US data achieves 99% accuracy on US test data but 60% on Brazilian data. Is this an ID failure or an OOD generalization failure?

- **Concept: Shortcut Learning (Spurious Correlations)**
  - Why needed here: Explains the failure mode the paper addresses. Models often optimize for the easiest signal available. In multi-centre ECG data, the easiest signal is often the device noise or sampling rate, not the biological signal.
  - Quick check question: Why does the IDB strategy prevent shortcut learning?

## Architecture Onboarding

- **Component map:**
  - Input (7s, 8-lead ECG, 2800×8) -> 4-layer ResNet Backbone -> 256-dim Embeddings -> Projection Head (pretraining only) -> InfoNCE Loss
  - Downstream: Frozen Backbone -> Linear/MLP Prediction Head -> Age MAE / Sex AUROC

- **Critical path:**
  1. Preprocessing: Bandpass/notch filter → Resample to 400Hz → Scale
  2. Batching (IDB): Sample batch from only one cohort (e.g., all BIDMC or all CODE, never mixed)
  3. Augmentation: Apply random crop/zero-mask to create Patient Pairs
  4. Pretraining: Optimize InfoNCE loss on the projected embeddings
  5. Downstream: Freeze backbone, train MLP head on labeled data (e.g., Age/Sex)

- **Design tradeoffs:**
  - CAPE-X (Random Batching) vs. CAPE-Z (IDB): CAPE-X gives better performance if test data is similar to train data (In-Distribution). CAPE-Z gives significantly better performance on external sites (Out-of-Distribution) by sacrificing a fraction of ID performance or requiring more robust optimization.
  - Volume vs. Diversity: Larger datasets (BCSV) help, but only if distributional artifacts are managed (via IDB); otherwise, volume amplifies bias.

- **Failure signatures:**
  - Clustering in Latent Space: If t-SNE plots of your embeddings show distinct islands corresponding to input hospitals, your model has learned cohort artifacts (Failure: Shortcut Learning)
  - Performance Asymmetry: High accuracy on Age/Sex prediction within the training hospital, but near-random performance on a different hospital (Failure: OOD Generalization collapse)

- **First 3 experiments:**
  1. Baseline Ablation: Train a CAPE model on BCSV with random batching. Visualize t-SNE colored by cohort to confirm the "artifact clustering" hypothesis.
  2. Implement IDB: Modify the data loader to enforce "One Cohort Per Batch." Retrain on BCSV (CAPE-Z).
  3. Cross-Continental Validation: Train a linear probe on BIDMC (US) features and evaluate on CODE (Brazil) and SHZS (China). Compare MAE/AUROC between the Random Batch and IDB models to quantify the OOD gain.

## Open Questions the Paper Calls Out

- **Question:** How does end-to-end fine-tuning of the backbone feature extractor, rather than using precomputed fixed features, impact the out-of-distribution (OOD) generalization of the CAPE foundation model?
  - Basis in paper: The authors state in the Limitations section that "Future studies should explore how fine-tuning impacts model generalisation and performance."
  - Why unresolved: This study isolated representation quality by training only the prediction head (MLP) on frozen features; the interaction between weight updates in the backbone and OOD robustness remains unknown.

- **Question:** Does the effectiveness of the In-Distribution Batch (IDB) strategy and the impact of cohort composition generalize to other contrastive learning frameworks or supervised training approaches?
  - Basis in paper: The Limitations section notes that "further experimentation is required to validate these findings in different contexts" beyond the specific CAPE methodology.
  - Why unresolved: The results are derived specifically from the CAPE contrastive framework; it is uncertain if the observed cohort-specific artifacts are a property of the data or the specific loss function/architecture used.

- **Question:** Do the relationships between pretraining cohort diversity and downstream performance hold for complex clinical diagnostic tasks, such as arrhythmia detection or mortality prediction?
  - Basis in paper: The paper evaluates performance using only age and sex prediction "because they are available across all cohorts," leaving clinical diagnostic generalizability unexplored.
  - Why unresolved: Age and sex are surrogates for general feature quality but may not correlate perfectly with a model's ability to detect pathology, which may rely on different signal features.

## Limitations

- The study's core claims hinge on the assumption that technical artifacts are the primary source of inter-cohort variance, though other confounders like population demographics could also drive performance gaps
- The IDB strategy's effectiveness depends critically on having sufficient intra-cohort diversity; if a single cohort is too homogeneous, the model may learn pathological shortcuts
- The paper does not explicitly test whether the IDB approach transfers to other modalities or foundation model architectures beyond ECG

## Confidence

- **High Confidence:** The empirical finding that multi-centre pretraining reduces OOD generalization (supported by t-SNE visualizations and performance metrics). The mechanism that contrastive learning can exploit cohort-specific artifacts as shortcuts is well-established in the broader ML literature.
- **Medium Confidence:** The specific efficacy of the IDB strategy for ECGs. While the results are strong, the neighbor corpus shows limited direct precedent for this exact batching approach in ECG foundation models.
- **Medium Confidence:** The claim that secondary care cohorts provide superior pretraining data. The evidence is correlational and the mechanism (more diverse pathologies = better features) is plausible but not definitively proven in this work.

## Next Checks

1. **Ablation on Intra-cohort Diversity:** Stratify performance gains by the heterogeneity of the training cohort. Compare IDB performance between a highly diverse secondary care cohort and a more uniform one to test the core assumption.
2. **External Dataset Transfer:** Apply the CAPE-Z (IDB) model to an entirely new, unseen dataset from a different continent or clinical setting (not in the original BCSV pretraining set) to validate the robustness of the OOD gains.
3. **Artifact Control Experiment:** Conduct a controlled experiment where cohort-specific artifacts (e.g., simulated device noise patterns) are explicitly added to a single dataset. Test whether CAPE-X learns these artifacts (showing clustering) and whether CAPE-Z with IDB prevents this, directly validating the proposed mechanism.