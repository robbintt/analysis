---
ver: rpa2
title: 'Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended
  Harms'
arxiv_id: '2510.14718'
source_url: https://arxiv.org/abs/2510.14718
tags:
- story
- health
- stories
- harms
- ethical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a human-centered framework that uses AI-generated
  stories and structured discussions to help people anticipate ethical risks and benefits
  of AI healthcare systems before deployment. The method simulates role-based interactions
  in evolving scenarios, producing short narratives that prompt reflection on real-world
  impacts.
---

# Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms

## Quick Facts
- **arXiv ID**: 2510.14718
- **Source URL**: https://arxiv.org/abs/2510.14718
- **Reference count**: 40
- **Primary result**: Storytelling framework significantly improves ethical foresight in healthcare AI design compared to plot-planning baselines

## Executive Summary
This paper introduces a human-centered framework that uses AI-generated stories and structured discussions to help people anticipate ethical risks and benefits of AI healthcare systems before deployment. The method simulates role-based interactions in evolving scenarios, producing short narratives that prompt reflection on real-world impacts. In a user study, participants exposed to storytelling identified a broader range of harms (entropy 3.383 vs 2.433) and benefits (entropy 3.554 vs 2.579) compared to those who worked without stories, focusing less narrowly on privacy and well-being. Automated and human evaluations showed the storytelling approach significantly outperformed plot-planning baselines across creativity, coherence, engagement, relevance, and likelihood of harm or benefit, demonstrating that narrative-driven speculation enhances ethical foresight in AI design.

## Method Summary
The framework follows a three-step process: (1) GPT-4o generates 7-tuple use-case scenarios (capability, user, subject, context, benefit, harm, failure trajectory) from 38 AI concepts; (2) Solo Performance Prompting with world agent simulates role-based interactions and tracks environment trajectories; (3) interaction logs are rephrased into 5-sentence stories. The approach was tested using GPT-4o, Llama-3.3-70B, and Gemma-3-27B models on 2x H100s via vLLM. User studies compared storytelling versus no-storytelling conditions, measuring harm/benefit distribution entropy and qualitative assessment of ethical foresight.

## Key Results
- Storytelling condition participants identified significantly broader range of harms (entropy 3.383 vs 2.433) and benefits (entropy 3.554 vs 2.579)
- Automated and human evaluations showed storytelling framework outperformed plot-planning baselines across all quality metrics
- Stories produced by the framework demonstrated higher creativity, coherence, engagement, relevance, and likelihood of harm/benefit identification

## Why This Works (Mechanism)
The framework works by transforming abstract AI concepts into concrete, relatable scenarios through role-based simulation and narrative construction. By having multiple AI agents play different stakeholder roles while the world agent tracks environmental changes, the system generates diverse perspectives on potential harms and benefits. The storytelling format makes ethical considerations more accessible and memorable compared to abstract technical descriptions, enabling participants to better anticipate real-world impacts of AI systems.

## Foundational Learning
- **Role-based simulation**: Multiple AI agents playing different stakeholder roles to generate diverse perspectives
  - *Why needed*: Captures varied viewpoints and interactions that single-agent approaches miss
  - *Quick check*: Verify distinct agent personas produce substantively different interaction patterns

- **World agent tracking**: Separate AI agent maintaining environmental state across interactions
  - *Why needed*: Ensures narrative coherence and tracks cascading effects of decisions
  - *Quick check*: Confirm state consistency across interaction turns

- **Entropy-based diversity measurement**: Using Shannon entropy to quantify range of identified harms/benefits
  - *Why needed*: Provides quantitative metric for ethical foresight breadth
  - *Quick check*: Compare entropy distributions across different participant groups

## Architecture Onboarding

**Component map**: AI Concepts -> Use-Case Generation -> Role-Based Simulation -> Story Generation -> Human Evaluation

**Critical path**: Use-Case Generation → Role-Based Simulation → Story Generation → User Study

**Design tradeoffs**: Single-step plot-planning (faster, less coherent) vs. iterative role-playing (slower, more nuanced)

**Failure signatures**: Low story diversity indicates temperature/role separation issues; environment incoherence suggests world agent tracking problems

**First experiments**:
1. Generate use-case scenarios for 5 AI concepts and manually verify 7-tuple completeness
2. Run role-based simulation for single scenario and check interaction log quality
3. Compare story outputs from plot-planning vs. role-playing approaches on same scenario

## Open Questions the Paper Calls Out
None

## Limitations
- Small participant sample (n=12) limits external validity and generalizability
- Single-domain focus on consumer healthcare AI may not translate to other domains
- Three-story-per-concept evaluation design may create artificial constraints not reflective of real-world usage

## Confidence
- **High confidence**: Comparative performance of storytelling framework against baseline plot-planning
- **Medium confidence**: Entropy-based findings on harm/benefit diversity (sample size limitations)
- **Low confidence**: Generalizability to non-healthcare domains and practical impact on AI system safety

## Next Checks
1. Replicate user study with larger, more diverse participant pool (minimum 30-50 participants) across multiple healthcare AI domains
2. Conduct longitudinal evaluation measuring whether teams using this framework actually identify and mitigate real-world harms during AI development
3. Test framework's transferability to non-healthcare domains (finance, education, criminal justice) to assess generalizability