---
ver: rpa2
title: 'BitAbuse: A Dataset of Visually Perturbed Texts for Defending Phishing Attacks'
arxiv_id: '2502.05225'
source_url: https://arxiv.org/abs/2502.05225
tags:
- characters
- dataset
- character
- sentences
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces BitAbuse, a dataset of 325,580 visually perturbed
  texts collected from real-world phishing emails, addressing the lack of authentic
  data in prior research. The dataset includes both real and synthetically perturbed
  texts, with each input sentence labeled with its ground truth restored version.
---

# BitAbuse: A Dataset of Visually Perturbed Texts for Defending Phishing Attacks

## Quick Facts
- arXiv ID: 2502.05225
- Source URL: https://arxiv.org/abs/2502.05225
- Authors: Hanyong Lee; Chaelyn Lee; Yongjae Lee; Jaesung Lee
- Reference count: 26
- Primary result: Character BERT-based models achieve ~96% accuracy on visually perturbed text restoration using BitAbuse dataset

## Executive Summary
This study introduces BitAbuse, a dataset of 325,580 visually perturbed texts collected from real-world phishing emails, addressing the lack of authentic data in prior research. The dataset includes both real and synthetically perturbed texts, with each input sentence labeled with its ground truth restored version. Experiments using five restoration methods—SimChar DB, OCR, Spell Checker, Character BERT, and GPT-4o mini—showed that Character BERT-based models achieved the highest accuracy (~96%) and robustness, especially when trained on sufficient VP sentences. Analysis revealed distinct VP character patterns and significant differences between real and synthetic data, underscoring BitAbuse's value for training reliable restoration models. The dataset is publicly available to support future adversarial attack defense research.

## Method Summary
The study created BitAbuse by collecting phishing emails from bitcoinabuse.com, extracting English sentences, and identifying visually perturbed (VP) sentences where characters were replaced with homoglyphs. The dataset combines 26,591 real VP sentences (BitCore) with 298,989 synthetic VP sentences (BitViper) generated using the Viper algorithm at p=0.2 perturbation rate. Five restoration methods were evaluated: SimChar DB (rule-based), OCR, Spell Checker, Character BERT (character-level neural model), and GPT-4o mini. Character BERT was trained with lr=5e-5, batch_size=32, epochs=10, and AdamW optimizer, using 60/20/20 train/validation/test splits. Performance was measured using Word Level Accuracy, Word Level Jaccard, and BLEU metrics.

## Key Results
- Character BERT achieves ~96% Word Level Accuracy on BitAbuse, outperforming all baselines
- Real-world VP distribution shows multi-modal patterns (peaks at 0.07-0.09, 0.32-0.34, 0.66-0.68 VP ratio) unlike synthetic uniform distributions
- Model performance degrades significantly with insufficient training data (1-5% training → large accuracy drops)
- GPT-4o mini refuses ~13.22% of requests due to safety concerns on malicious content

## Why This Works (Mechanism)

### Mechanism 1: Character-Level Tokenization Enables Context-Aware Restoration
Processing text at character level rather than subword level improves restoration of visually perturbed text when surrounding context is also corrupted. Character BERT treats input/output as character sequences, allowing the model to infer original characters from local character-level context rather than relying on potentially-corrupted token embeddings. Standard BERT's MLM mechanism struggles when multiple tokens in a sequence contain VP characters. Core assumption: Perturbation density affects surrounding context quality; subword tokenizers fragment corrupted characters unpredictably. Evidence: [abstract] "Language models trained on our proposed dataset demonstrated significantly better performance... achieving an accuracy of approximately 96%." [section 4.1] "This method is particularly important because attackers often modify characters within tokens to deceive victims, leading to widespread perturbations across most tokens."

### Mechanism 2: Real-World VP Distribution Differs Structurally From Synthetic Generation
Real phishing VP texts exhibit multi-modal distribution patterns that synthetic perturbation rules fail to capture. Real attackers use varying perturbation densities (three peaks: 0.07-0.09, 0.32-0.34, 0.66-0.68 VP ratio) and target common words probabilistically rather than uniformly. Synthetic methods like Viper perturb fixed ratios, creating unrealistic homogeneity. Core assumption: Attacker behavior follows non-uniform patterns influenced by evading detection while maintaining readability. Evidence: [abstract] "Our analysis revealed a significant gap between real-world and synthetic examples." [section 5, Figure 3] "The VP sentences collected from bitcoinabuse[.]com does not yield unimodal distribution... it has three peaks."

### Mechanism 3: Training Data Volume Determines Unseen VP Character Generalization
Restoration models require sufficient exposure to diverse VP character variants to generalize to unseen attacks. With 1-5% training data, Character BERT shows significant performance degradation on BitViper/BitAbuse; at 20%+ training data, accuracy approaches ceiling (~95-96%). The model learns character-level mappings and contextual patterns that transfer to novel VP characters. Core assumption: VP character variants share learnable structural patterns (e.g., diacritics, visual similarity classes). Evidence: [section 5, Table 4] "When the amount of training VP sentences was as low as 1% or 5%, significant performance degradation was observed." [section 6] "Sufficient VP sentences are necessary to build a stable Character BERT model."

## Foundational Learning

- Concept: **Visual Perturbation / Homoglyphs**
  - Why needed here: Understanding that VP attacks replace characters with visually similar Unicode characters (e.g., 'ß' for 'B', accented vowels) to evade text-based detection while remaining human-readable.
  - Quick check question: Can you explain why standard tokenization fails when characters are replaced with visually identical Unicode homoglyphs?

- Concept: **Character-Level Language Models**
  - Why needed here: Character BERT differs from standard BERT by operating on character sequences rather than subword tokens; this is critical when token boundaries are disrupted by perturbations.
  - Quick check question: How does processing at character level differ from WordPiece/BPE tokenization when input contains corrupted characters?

- Concept: **One-to-Many Homoglyph Mappings**
  - Why needed here: A single VP character can map to multiple valid original characters (e.g., 'ο' → 'o', 'c', 'd', 'g', 'q'), making rule-based restoration insufficient and necessitating contextual inference.
  - Quick check question: Why can't a simple lookup table handle VP character restoration for all cases?

## Architecture Onboarding

- Component map: Raw emails -> English classification (BERT classifier) -> Sentence splitting -> Regex preprocessing -> VP sentence extraction -> Ground truth annotation -> BitAbuse dataset
- Critical path: Input VP text -> Character BERT encoder -> Character-level sequence output -> Restored text. Training requires paired (VP sentence, ground truth) examples.
- Design tradeoffs:
  - Character BERT: Highest accuracy (~96%), requires training data, smaller model size than GPT
  - GPT-4o mini: No training needed, but 13.22% refusal rate on unethical content, lower positional accuracy due to generative nature
  - Spell Checker: Fast, no training, but fails at high VP character density
  - SimChar DB/OCR: Fast, interpretable, but cannot handle one-to-many mappings or unseen homoglyphs
- Failure signatures:
  - Consecutive VP characters in single word (e.g., "tcoin" → incorrect "rktcoin")
  - High VP density sentences (>60% perturbation ratio) for non-Character BERT methods
  - GPT refusal responses on malicious content ("I'm sorry, I can't assist with that")
  - Unseen VP characters with <5% training data
- First 3 experiments:
  1. Reproduce Character BERT baseline on BitCore test split (target: ~99.8% Word Level Accuracy) to validate setup.
  2. Test generalization by training on BitCore only, evaluating on BitViper to measure synthetic-to-real gap.
  3. Ablate training data percentage (1%, 5%, 10%, 20%) to characterize data scaling requirements for your deployment scenario.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hybrid approach combining OCR and Character BERT improve robustness in data-scarce environments?
- Basis in paper: [explicit] The authors suggest future studies should explore hybrid approaches to remedy the "greedy data consumption nature of LMs."
- Why unresolved: The current study evaluated OCR, Spell Checkers, and Character BERT as separate, competing baselines rather than integrated systems.
- Evidence to resolve it: Experiments demonstrating that a combined OCR-BERT model maintains high accuracy when trained on significantly smaller subsets of BitAbuse compared to standalone models.

### Open Question 2
- Question: How well do models trained on BitAbuse generalize to non-Bitcoin phishing scenarios?
- Basis in paper: [explicit] The Limitations section notes the dataset is restricted to Bitcoin scams, potentially reducing generalizability to other attack types.
- Why unresolved: Phishing attacks vary widely in structure and content; a model optimized for financial Bitcoin scams may fail on credential harvesting or malware distribution campaigns.
- Evidence to resolve it: Evaluation of BitAbuse-trained models on a newly curated dataset of phishing emails from diverse categories (e.g., spear phishing, fake invoices).

### Open Question 3
- Question: Is Character BERT truly superior to other modern Large Language Models (LLMs) for restoring visually perturbed text?
- Basis in paper: [explicit] The Limitations section states that a performance comparison with other LM-based restoration methods was out of scope, leaving the superiority of Character BERT unverified against modern alternatives.
- Why unresolved: While Character BERT outperformed GPT-4o mini and older methods, it has not been benchmarked against other specialized transformer architectures.
- Evidence to resolve it: A comparative study benchmarking Character BERT against other state-of-the-art sequence-to-sequence models on the BitAbuse test set.

## Limitations

- Dataset composition bias: The 26,591 real vs 298,989 synthetic sentence ratio may create bias toward synthetic patterns, limiting real-world robustness
- Language restriction: Study focuses on English-language VP attacks, leaving uncertainty about cross-linguistic applicability
- Resource requirements: High accuracy (~96%) requires extensive training data (60% of dataset), which may not reflect performance in resource-constrained deployment scenarios

## Confidence

**High Confidence**: Experimental methodology is sound with clear ablation studies showing training data scaling effects and systematic comparison of five restoration methods. The ~96% accuracy for Character BERT on the full dataset is well-supported.

**Medium Confidence**: Characterization of real vs synthetic VP distribution differences is compelling but based on descriptive statistics rather than causal analysis. The multi-modal distribution claim needs further validation across different phishing campaigns.

**Low Confidence**: Assertion that Character BERT is universally superior lacks sufficient stress testing. While it outperforms baselines on BitAbuse, the study doesn't explore edge cases like completely novel VP character sets.

## Next Checks

1. **Edge Case Testing**: Systematically evaluate Character BERT on VP sentences containing 3+ consecutive perturbed characters and completely novel VP characters not present in training data to establish failure boundaries.

2. **Temporal Robustness**: Test model performance on VP texts from different time periods to verify that the multi-modal distribution patterns remain stable across evolving attack strategies.

3. **Cross-Lingustic Extension**: Apply the restoration methodology to VP attacks in non-English languages to validate whether character-level tokenization provides similar advantages when VP character mappings differ across scripts.