---
ver: rpa2
title: 'PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically
  Grounded Text-to-Audio-Video Generation'
arxiv_id: '2512.23994'
source_url: https://arxiv.org/abs/2512.23994
tags:
- audio
- generation
- physical
- arxiv
- sound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PhyAVBench introduces a physics-sensitivity benchmark for evaluating
  physically grounded text-to-audio-video (T2AV) generation models. The benchmark
  comprises 1,000 groups of paired text prompts with controlled physical variables
  that induce sound variations, enabling fine-grained assessment of models' understanding
  of audio-related physical laws across 6 major audio physics dimensions and 50 test
  points.
---

# PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically Grounded Text-to-Audio-Video Generation

## Quick Facts
- arXiv ID: 2512.23994
- Source URL: https://arxiv.org/abs/2512.23994
- Reference count: 40
- Key outcome: Introduces a physics-sensitivity benchmark with 1,000 paired text prompts and controlled physical variables to evaluate physically grounded T2AV generation models

## Executive Summary
PhyAVBench introduces a novel benchmark for evaluating physically grounded text-to-audio-video (T2AV) generation models through physics-sensitivity testing. The benchmark comprises 1,000 groups of paired text prompts with controlled physical variables that induce sound variations, enabling fine-grained assessment of models' understanding of audio-related physical laws across 6 major audio physics dimensions and 50 test points. Each prompt is paired with at least 20 real-world ground-truth videos to minimize data leakage. The evaluation uses Contrastive Physical Response Score (CPRS) to measure alignment between generated audio changes and ground-truth physical trends, and Fine-Grained Alignment Score (FGAS) for audio-video synchronization.

## Method Summary
PhyAVBench employs a physics-sensitivity testing (APST) paradigm using paired prompts that differ in only one physical variable while keeping other conditions constant. The benchmark covers 6 major audio physics dimensions (Material Properties, Observer Physics, Mechanical Motion, Fluid & Aerodynamics, Electrical & Electromagnetic, Helmholtz Resonance) with 50 test points across 4 acoustic scenarios. Ground-truth videos are newly recorded or collected under controlled conditions with human-LLM verification. Evaluation uses CPRS (measuring directional alignment in CAV-MAE Sync's shared latent space), FGAS (for temporal synchronization), CLAP/CLIPSIM (semantic alignment), and PR-MOS (human evaluation).

## Key Results
- Introduces first benchmark specifically designed to evaluate physically grounded T2AV generation models
- Establishes 1,000 paired prompts covering 6 physics dimensions with 50 test points across 4 acoustic scenarios
- Proposes CPRS metric that measures directional alignment between generated and ground-truth audio changes in shared latent space
- Collects ≥20 real-world ground-truth videos per prompt with human-LLM verified physical consistency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Directional alignment in audio embedding space indicates physics grounding
- **Mechanism:** CPRS compares the vector displacement between generated audio embeddings (v_gen) and ground-truth embeddings (v_GT) in CAV-MAE Sync's shared latent space. By measuring cosine similarity between these directional vectors rather than absolute distances, the metric isolates whether the model's response to a physical variable change matches real-world acoustic trends.
- **Core assumption:** The CAV-MAE Sync audio encoder captures acoustically meaningful dimensions that correlate with physical properties like material hardness, cavity volume, or airflow velocity.

### Mechanism 2
- **Claim:** Single-variable controlled prompting forces models to learn physical causation rather than surface correlations
- **Mechanism:** By presenting paired prompts that differ only in one physical factor (e.g., "hammer hitting wooden surface" vs. "hammer hitting metal surface"), the APST paradigm eliminates confounding visual and semantic cues. Models must infer the acoustic consequence of the material change, not merely retrieve common audio-visual associations.
- **Core assumption:** T2AV models encode sufficient physical world knowledge to distinguish acoustic outcomes, or can be trained to do so.

### Mechanism 3
- **Claim:** Newly collected ground-truth videos with human-verified physical consistency prevent benchmark contamination
- **Mechanism:** Unlike prior benchmarks that reuse AudioSet or existing datasets, PhyAVBench records or collects ≥20 real-world videos per prompt under controlled acoustic conditions. Iterative human-LLM quality control filters out samples where physical variables are confounded or text-audio-video alignment fails.
- **Core assumption:** Real-world recordings faithfully instantiate the physical principles described in prompts, and human reviewers can reliably verify this.

## Foundational Learning

- **Concept: Shared audio-visual latent spaces**
  - **Why needed here:** CPRS and FGAS both rely on CAV-MAE Sync's joint embedding space to compare audio features across modalities and conditions. Without understanding how contrastive audio-visual pretraining creates aligned representations, the metric design is opaque.
  - **Quick check question:** Can you explain why cosine similarity in a contrastively learned space captures semantic/physical alignment better than raw waveform comparison?

- **Concept: Controlled-variable experimental design**
  - **Why needed here:** PhyAVBench's paired-prompt structure is fundamentally a controlled experiment. Understanding confounding variables, invariance, and causal inference is necessary to interpret results and design new test points.
  - **Quick check question:** If a prompt pair differs in both material and impact velocity, why does this invalidate the physics-sensitivity test?

- **Concept: Diffusion transformer architectures for multimodal generation**
  - **Why needed here:** The benchmark targets T2AV models built on diffusion or DiT architectures (e.g., Sora 2, UniForm, JavisDiT). Understanding cross-attention, temporal alignment, and modality conditioning helps diagnose why models fail physics tests.
  - **Quick check question:** In a unified diffusion model, where does audio-visual synchronization typically get enforced—loss function, architecture, or post-hoc alignment?

## Architecture Onboarding

- **Component map:** Prompt taxonomy (6 dimensions → 3-4 subcategories → 50 test points → 1000 prompt groups) → Ground-truth corpus (≥20 videos per prompt) → CAV-MAE Sync encoder → CPRS + FGAS + CLAP/CLIPSIM + PR-MOS

- **Critical path:**
  1. Select a test point from the taxonomy (e.g., Helmholtz Resonance under Fluid & Aerodynamics)
  2. Generate outputs from target T2AV model using both prompts in a paired group
  3. Extract audio embeddings via CAV-MAE Sync for generated and GT samples
  4. Compute v_GT (GT direction) and v_gen (generated direction), then CPRS
  5. Optionally compute FGAS for temporal alignment and CLAP/CLIPSIM for semantic quality

- **Design tradeoffs:**
  - **CPRS vs. absolute quality metrics:** CPRS sacrifices absolute audio fidelity assessment for directional physics sensitivity; models could score high on CPRS but low on perceptual quality
  - **New recordings vs. existing datasets:** New data avoids leakage but limits scale; hybrid approaches may be needed for broader coverage
  - **LLM-assisted vs. human-only taxonomy:** LLMs speed up taxonomy construction but may introduce plausible-sounding but physically invalid categories; human review is essential

- **Failure signatures:**
  - **CPRS ≈ 0.5 (orthogonal):** Model's audio changes are uncorrelated with physical direction—likely relying on text priors without physics grounding
  - **CPRS ≈ 0.0 (anti-aligned):** Model produces opposite acoustic response (e.g., softer sound for harder material)—possible sign of systematic bias or training data artifact
  - **High CPRS but low FGAS:** Model captures physics direction but fails temporal synchronization—suggests decoupled audio generation

- **First 3 experiments:**
  1. **Baseline CPRS on commercial models:** Run APST on Sora 2, Veo 3.1, Kling 2.6 to establish current industry performance; expect low-to-moderate CPRS based on paper's claim that existing models lack physics grounding
  2. **Ablation on prompt design:** Test whether removing physical keywords (e.g., "metal" vs. generic surface) drops CPRS, isolating whether models use explicit cues vs. inferred physics
  3. **Cross-dimension generalization:** Train a lightweight audio encoder on one physics dimension (e.g., Material Properties) and evaluate zero-shot CPRS on another (e.g., Observer Physics) to probe whether physics grounding transfers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do current state-of-the-art commercial and academic unified T2AV models perform across the specific audio-physics dimensions defined by PhyAVBench?
- Basis in paper: [explicit] The authors state in Section IV.D: "This report focuses on the benchmark design and data curation, while comprehensive model evaluation will be provided in a future release."
- Why unresolved: While the benchmark framework is established, the actual empirical capabilities of existing models (like Sora 2, Veo, or academic baselines) on these specific 50 physics test points have not yet been reported or analyzed.
- What evidence would resolve it: A leaderboard or detailed results table showing CPRS scores for major models across the 6 physics dimensions.

### Open Question 2
- Question: Does the proposed Contrastive Physical Response Score (CPRS) reliably correlate with human perceptions of physical rationality (PR-MOS) across all acoustic scenarios?
- Basis in paper: [inferred] The paper introduces CPRS (an automatic metric using CAV-MAE Sync embeddings) and PR-MOS (a subjective human score) as distinct evaluation methods. It assumes that alignment in the latent space accurately reflects "physical rationality," but does not present correlation data validating this assumption.
- Why unresolved: Embedding spaces often capture semantic similarity rather than fine-grained physical causality. Without validation, it is unclear if high CPRS scores equate to perceptually plausible physics.
- What evidence would resolve it: A correlation analysis comparing CPRS rankings against human PR-MOS rankings for the generated audio samples.

### Open Question 3
- Question: Can models trained on internet-scale data ever be rigorously evaluated for "physical grounding" without auditing their pre-training data for specific physical scenarios?
- Basis in paper: [inferred] The paper notes in Section I and Section III-B that ground-truth videos are "newly recorded... minimizing the risk of data leakage," while acknowledging commercial models rely on "massive multimodal pretraining."
- Why unresolved: Even with new recordings, the test prompts describe common actions (e.g., "hammer hitting metal"). It is unresolved whether high performance indicates genuine physical understanding or merely the retrieval of memorized audio-visual priors from similar internet videos.
- What evidence would resolve it: A "zero-shot" evaluation on entirely novel or synthetic physical phenomena unlikely to exist in standard training corpora, or an analysis of failure modes on counter-intuitive physics.

## Limitations

- Ground-truth physical consistency depends on subjective human assessment with unknown inter-rater reliability
- CPRS metric validity assumes CAV-MAE Sync's embedding space linearly encodes physical variables, which is unproven
- Benchmark covers only 4 acoustic scenarios (music, sound effects, speech, mixed), limiting generalizability

## Confidence

- **High confidence**: The benchmark design (paired prompts with controlled variables) and evaluation methodology (CPRS, FGAS) are sound and novel contributions
- **Medium confidence**: The claim that existing models lack physics grounding is supported by the benchmark design but not empirically validated in the paper
- **Low confidence**: The assumption that CAV-MAE Sync's embedding space linearly encodes physical variables is critical but unproven

## Next Checks

1. **Inter-rater reliability study**: Conduct a formal annotation study measuring agreement between human reviewers on physical consistency judgments for the ground-truth videos. Quantify Fleiss' kappa or similar statistics to establish the reliability of the human-LLM verification process.

2. **Embedding space validation**: Perform a controlled experiment correlating CPRS scores with human perceptual ratings of physical plausibility. Test whether models with higher CPRS receive better ratings from human evaluators on physical accuracy, and whether the embedding space captures dimensions that humans perceive as physically meaningful.

3. **Zero-shot generalization test**: Evaluate CPRS scores when applying models trained on one physics dimension (e.g., Material Properties) to test points in entirely different dimensions (e.g., Fluid & Aerodynamics). This would reveal whether physics grounding transfers across domains or remains dimension-specific.