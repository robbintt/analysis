---
ver: rpa2
title: '"Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large
  Language Models'
arxiv_id: '2507.05424'
source_url: https://arxiv.org/abs/2507.05424
tags:
- context
- prompt
- instruction
- sentences
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CoPE introduces a framework for measuring how large language models\
  \ (LLMs) use contextual versus parametric knowledge in multilingual settings. Using\
  \ a MultiWikiAtomic dataset spanning English, Spanish, and Danish, the study reveals\
  \ a persistent \u201Clost-in-the-later\u201D effect where models prioritize earlier\
  \ context over later, even when later information is relevant."
---

# "Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models

## Quick Facts
- arXiv ID: 2507.05424
- Source URL: https://arxiv.org/abs/2507.05424
- Reference count: 29
- Primary result: CK scores plateau at ~70%, with persistent positional bias toward earlier context ("lost-in-the-later")

## Executive Summary
This paper introduces CoPE (Contextual Knowledge-Parametric Knowledge Evaluation), a framework for quantifying how large language models use contextual versus parametric knowledge. Using a MultiWikiAtomic dataset spanning English, Spanish, and Danish, the study reveals a "lost-in-the-later" effect where models systematically prioritize earlier context over later, even when later information is relevant. The research finds that CK scores plateau around 70% across models, with reasoning models using context least effectively. A novel CK-informed prompting strategy significantly improves contextual grounding, reduces hallucination, and produces more balanced context usage.

## Method Summary
The CoPE framework atomizes context and responses into minimal factual propositions using GPT-4o, then classifies each response sentence as contextual knowledge (CK) or parametric knowledge (PK) via bidirectional entailment scoring using mDeBERTa-v3-base-xnli-multilingual-nli-2mil with a 0.7 threshold. The framework evaluates CK scores and context recall distribution across quartiles. Tested across multiple models (GPT-4o, Gemini 1.5 Pro, Llama 3.2 90B/3B, GPT-o3, Qwen 3 235B) with various prompting strategies including Original, Strict, Balanced, CK Prompt, CoT, and CoT+CK Prompt variants.

## Key Results
- CK scores plateau at ~70% across models and languages
- "Lost-in-the-later" effect shows models prioritize earlier context (Q1) over later context (Q4)
- Chain-of-Thought prompting reduces CK scores by consuming token budget and encouraging parametric knowledge
- CK-informed prompting improves scores by 6-8 points and produces more uniform context recall
- Applied to summarization, CK prompting increases factual consistency by 6-7% and reduces parametric knowledge reliance

## Why This Works (Mechanism)

### Mechanism 1: Lost-in-the-Later Positional Bias
LLMs systematically underutilize information appearing later in input context due to pretraining on sequential corpora that introduces structural attention bias toward earlier tokens. This manifests as declining recall across context quartiles (Q1 → Q4), persisting even when context order is randomized (~5% variation only). The bias originates from pretraining dynamics rather than architectural constraints.

### Mechanism 2: CoT Degrades Contextual Grounding via Token Budget Competition
Chain-of-Thought prompting reduces contextual grounding by consuming 50%+ of the token budget, leaving fewer tokens for grounded content. Additionally, CoT encourages synthesis "beyond the given context," pulling outputs toward parametric knowledge. Reasoning models show persistently low CK scores (~55 vs. ~70-75 for non-reasoning models).

### Mechanism 3: CK Prompting Balances Attention via Explicit Quartile Constraints
CK prompt combines strict-context ("use only provided contexts") and balanced-usage ("draw fairly from relevant contexts") instructions to improve CK scores by 6-8 points and produce flatter recall curves across quartiles. This suggests models can follow explicit distributional instructions when prompted.

## Foundational Learning

- **Concept: Contextual Knowledge (CK) vs. Parametric Knowledge (PK)**
  - Why needed here: The entire CoPE framework depends on distinguishing information entailed by context vs. information from model weights. CK is content derived from input; PK includes memorized facts, generalizations, and inferences not in context.
  - Quick check question: If a model answers "What is the capital of France?" using context that says "Paris is France's capital," is this CK or PK? What if the context doesn't mention France but the model still says Paris?

- **Concept: Natural Language Inference (NLI) for Entailment Classification**
  - Why needed here: CoPE uses bidirectional NLI to determine whether response sentences are entailed by context sentences. Understanding NLI directional scoring (context→output, output→context) is essential for interpreting CK scores.
  - Quick check question: Why might "The movie was long" be entailed by "The film lasted three hours" but not vice versa? How does bidirectional scoring help?

- **Concept: Atomic Sentence Decomposition**
  - Why needed here: CoPE breaks context and responses into minimal factual propositions before classification. This granularity is necessary for accurate CK/PK attribution at the claim level rather than document level.
  - Quick check question: Why is "Marie Curie discovered radium and won two Nobel Prizes" problematic as an atomic sentence? How would you decompose it?

## Architecture Onboarding

- **Component map:**
  1. Atomization Module: GPT-4o → outputs JSON with atomic sentences
  2. Response Generator: Target LLM with configurable prompting strategies
  3. NLI Classifier: mDeBERTa-v3-base-xnli-multilingual-nli-2mil → produces entailment scores
  4. Threshold Gate: t = 0.7 for CK/PK classification
  5. Metric Aggregator: Computes CK score, context recall distribution across k quartiles

- **Critical path:**
  Input context → atomization → context + query → model → raw response → atomization → bidirectional NLI against all context atomic sentences → max entailment score → CK/PK classification (if >0.7 → CK) → aggregate: CK% = CK_sentences / total_sentences × 100 → context recall per quartile

- **Design tradeoffs:**
  - Threshold (0.7): Lower → more CK (potentially over-attributing); Higher → stricter CK (may miss paraphrases)
  - Atomization model choice: GPT-4o used for consistency; smaller models may produce noisier atomic sentences
  - Quartile count (k=4): More granular segments → finer positional analysis but higher variance per segment

- **Failure signatures:**
  1. CK scores near 50% with high variance: Likely threshold mismatch or atomization failure
  2. Flat context recall across quartiles for all models: Check atomization preserves position metadata
  3. Reasoning models showing CK > 70%: Possible max_tokens configuration issue
  4. Large CK/PK swings with small prompt changes: Model may be prompt-sensitive

- **First 3 experiments:**
  1. Reproduce lost-in-the-later on held-out data: Take 20 Wikipedia articles not in MultiWikiAtomic, atomize, generate responses with baseline prompt, compute context recall distribution
  2. Ablate CK prompt components: Test strict-only vs. balanced-only vs. combined CK prompt
  3. Token budget sensitivity for CoT: Run CoT prompting with max_tokens = 512, 1024, 2048, 4096

## Open Questions the Paper Calls Out

- **Open Question 1:** Does pretraining introduce a structural bias toward earlier tokens, and if so, can this bias be mitigated through architectural changes or training modifications?
  - The authors state: "A potential explanation is that pretraining introduces a bias toward earlier tokens in input sequences. Future work should explore this direction in more depth."
  - The randomized context order experiment ruled out surface-level data ordering as a cause, but the underlying mechanism remains unidentified.

- **Open Question 2:** Why do reasoning models (GPT-o3, Qwen 3 235B) consistently show the lowest contextual knowledge scores across all settings?
  - The paper reports that "reasoning models...demonstrate the lowest contextual knowledge (CK) scores across all settings" and speculates that "their longer, step-by-step answers likely drift from the original input," but this hypothesis is not tested.

- **Open Question 3:** How can models balance contextual grounding with factual correctness when the provided context itself is inaccurate or contradictory?
  - From the limitations: "Handling both grounding and factual accuracy, especially when context is imperfect, is an open challenge for future work."
  - CoPE measures grounding, not factuality. The counterfactual experiments show models do ground to false context, but strategies for detecting and correcting unreliable context remain unexplored.

- **Open Question 4:** How do crosslingual transfer and code-mixing affect CK-PK balance in multilingual settings?
  - The authors state: "Future work may explore more complex multilingual settings, including crosslingual scenarios, to further refine CK-PK analysis."
  - Current experiments only test monolingual contexts; it remains unknown how models handle contexts in one language and queries in another.

## Limitations
- The lost-in-the-later mechanism remains incompletely explained - could stem from pretraining dynamics, architectural constraints, or both
- Focus on Wikipedia-derived atomic facts may not capture full complexity of real-world knowledge domains
- The 0.7 NLI threshold is a heuristic choice that may not optimally capture semantic equivalence across all linguistic structures
- Assumption that models can follow explicit distributional instructions is demonstrated but not mechanistically explained

## Confidence
- **High Confidence**: CK scoring methodology and core implementation are well-specified and reproducible; finding that CK scores plateau around 70% is robust
- **Medium Confidence**: Lost-in-the-later positional bias is consistently observed but causal mechanism uncertain; CoT degradation effect is well-documented but token budget hypothesis is correlational
- **Low Confidence**: Assumption that models redistribute attention via CK prompt is demonstrated empirically but not explained mechanistically

## Next Checks
1. **Causal Isolation of Token Budget Effects**: Run CoT prompting with incrementally increasing max_tokens (512 → 1024 → 2048 → 4096) while keeping all other parameters constant to determine whether token budget expansion alone can recover lost contextual grounding.

2. **Cross-Domain Generalization Test**: Apply CoPE to non-Wikipedia domains including technical documentation, legal contracts, and conversational dialogue to determine whether the lost-in-the-later effect and 70% CK plateau are domain-invariant properties.

3. **Architectural Ablation Study**: Test the positional bias on models with explicitly different attention mechanisms (sparse attention, linear attention, uniform attention distributions) and training regimes (shuffled sequences, reversed sequences, multi-resolution context processing) to isolate whether the effect is fundamental to sequential pretraining or specific to architectural choices.