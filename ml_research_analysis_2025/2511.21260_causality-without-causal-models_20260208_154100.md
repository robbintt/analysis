---
ver: rpa2
title: Causality Without Causal Models
arxiv_id: '2511.21260'
source_url: https://arxiv.org/abs/2511.21260
tags:
- causal
- definition
- causality
- cause
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for defining causality without
  relying on structural equation models (SEMs), addressing limitations in traditional
  approaches like Halpern-Pearl. The core method abstracts causality by defining it
  through counterfactuals in a broader class of models called causal-counterfactual
  families (ccfs).
---

# Causality Without Causal Models

## Quick Facts
- arXiv ID: 2511.21260
- Source URL: https://arxiv.org/abs/2511.21260
- Reference count: 23
- One-line primary result: Presents framework for defining causality without relying on structural equation models, enabling disjunctive, negated, and nested counterfactual causes.

## Executive Summary
This paper introduces a framework for defining actual causality that operates independently of structural equation models (SEMs). The approach abstracts causality through counterfactuals in a broader class of models called causal-counterfactual families (ccfs), enabling definitions that support disjunctions, negations, beliefs, and nested counterfactuals—capabilities not possible in standard SEM-based frameworks. The work demonstrates that this abstract definition generalizes the Halpern-Pearl definition, applies to Lewis-style counterfactual structures, and enables backtracking in causal reasoning. The framework provides a more flexible foundation for causal reasoning across diverse domains where traditional SEMs may be inappropriate or unavailable.

## Method Summary
The method abstracts the Halpern-Pearl definition of actual causality into a more general framework by extracting its key counterfactual features. Instead of requiring interventions on structural equations, the approach defines causality through conditional but-for relationships using witness formulas within a counterfactual structure. The framework operates on causal-counterfactual families (ccfs) that specify a set of states, a language closed under conjunction, negation, and counterfactuals, and a satisfaction relation. The abstract definition (AC1'-AC3') requires that cause A and effect B hold, and there exists a witness formula τ such that in the closest worlds where ¬A ∧ τ holds, ¬B also holds. The choice of witness language CX determines expressiveness and controls backtracking, with different CX choices recovering various causality definitions.

## Key Results
- The abstract definition agrees with the Halpern-Pearl definition in recursive causal models when CX contains only conjunctions of non-negated primitive events (Theorem 4.2).
- The framework enables backtracking in causal reasoning by allowing context to change in counterfactuals, with the option to prevent backtracking by including U = ū in τ.
- The approach successfully defines explanations beyond causal models by incorporating epistemic sets K, though this extension remains speculative.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Actual causality can be defined as conditional but-for causality using witness formulas.
- Mechanism: The abstract definition (AC1'-AC3') requires that A and B hold, and there exists a witness formula τ such that in the closest worlds where ¬A ∧ τ holds, ¬B also holds. This generalizes the HP "contingency" approach by treating τ as an arbitrary formula rather than a fixed assignment to variables.
- Core assumption: The underlying framework supports a well-defined counterfactual conditional (ϕ → ψ) with appropriate semantics for "closest worlds."
- Evidence anchors:
  - [abstract]: "We abstract the definition, extracting its key features, so that it can be applied to any other model where counterfactuals are defined."
  - [section 4]: Definition 4.1 formalizes AC1′-AC3′ with τ drawn from a parameterized set CX.
  - [corpus]: "Abstract Counterfactuals for Language Model Agents" (FMR=0.60) discusses counterfactual inference for agents but focuses on token-level interventions; no direct validation of this paper's framework.

### Mechanism 2
- Claim: The choice of witness language CX determines expressiveness and controls backtracking.
- Mechanism: By restricting CX to conjunctions of primitive events, the definition collapses to HP causality (Theorem 4.2). Allowing richer CX (disjunctions, negations, beliefs) enables novel causal claims but may introduce unintuitive results if too permissive.
- Core assumption: The user has a principled basis for selecting CX appropriate to their domain.
- Evidence anchors:
  - [section 4.1]: Theorem 4.2 shows equivalence when CX contains only conjunctions of non-negated primitive events.
  - [section 5]: Backtracking emerges naturally when context (exogenous variables) is allowed to change in closest worlds; adding U = ū to τ prevents backtracking.
  - [corpus]: "A New Approach to Backtracking Counterfactual Explanations" discusses backtracking in XAI contexts but uses different formal machinery; weak evidence for this specific mechanism.

### Mechanism 3
- Claim: Recursive counterfactual structures strongly corresponding to causal models preserve HP causality judgments.
- Mechanism: A counterfactual structure M' corresponds to causal model M if they agree on structural equations. Strong correspondence adds requirements about context persistence. Proposition 4.3 and Theorem 4.5 establish that for formulas in L(S), truth values and causality judgments are preserved.
- Core assumption: The causal model is recursive (acyclic) and the counterfactual structure satisfies the strong correspondence conditions.
- Evidence anchors:
  - [section 4.2]: Definitions of "corresponds" and "strongly corresponds" formalize the relationship.
  - [section 4.2]: Theorem 4.5 proves equivalence for recursive settings with appropriately chosen CX.
  - [corpus]: No direct corpus validation; related work on causal structure through compression (FMR=0.0) is tangential.

## Foundational Learning

- Concept: **But-for causality and the problem of preemption**
  - Why needed here: The HP definition and its abstraction are motivated by cases (like Suzy and Billy throwing rocks) where but-for causality fails but intuitive causality holds.
  - Quick check question: In the rock-throwing example, why is Suzy's throw a cause of the bottle shattering even though the bottle would have shattered without it?

- Concept: **Structural equation models (SEMs) and interventions**
  - Why needed here: The paper positions its contribution as generalizing beyond SEMs; understanding SEMs is necessary to appreciate what is being abstracted.
  - Quick check question: What does the intervention [X ← x] represent in a causal model, and why does it not affect "upstream" variables?

- Concept: **Lewis-style counterfactual semantics**
  - Why needed here: The target framework for generalization uses possible worlds with a closeness relation; this is the substrate on which AC1'-AC3' operates.
  - Quick check question: In a Lewis-style structure, what does (M, s) ⊨ ϕ → ψ mean, and how does the ternary relation R encode closeness?

## Architecture Onboarding

- Component map:
  - CCF Interface -> Causality Module -> Witness Selector -> Model Adapters
  - Where: CCF Interface defines states, language, and satisfaction; Causality Module implements AC1'-AC3'; Witness Selector defines CX; Model Adapters instantiate for causal models and Lewis structures.

- Critical path:
  1. Choose or construct a ccf instance (e.g., translate a causal model to counterfactual structure or use Lewis structures directly).
  2. Define the witness language CX based on domain requirements (restrictiveness vs. expressiveness tradeoff).
  3. Apply Definition 4.1 to evaluate candidate causes.
  4. For explanation tasks, layer Definition 6.3 with an epistemic set K.

- Design tradeoffs:
  - Expressiveness vs. intuition: Richer CX allows disjunctive/nested causes but risks counterintuitive results (e.g., Briggs' controversial simplification axiom).
  - Backtracking vs. intervention: Allowing context to change in counterfactuals enables backtracking but departs from standard intervention semantics; adding U = ū to τ restores no-backtracking.
  - Model correspondence strength: Strong correspondence ensures HP equivalence but restricts counterfactual structures; weak correspondence allows more flexibility but may diverge from HP.

- Failure signatures:
  - Trivialization: If CX is too permissive (e.g., contains τ with ¬ϕ), every formula becomes a cause of ϕ.
  - Semantic mismatch: Negation is handled differently in causal models vs. counterfactual structures; for non-binary variables, X ≠ x → ψ may yield different results.
  - Formula class violation: Proposition 4.3 equivalence fails for formulas outside L(S); using nested or disjunctive antecedents in causal model translations may produce spurious results.

- First 3 experiments:
  1. **HP validation**: Implement the causal model adapter (Section 3.1), set CX to conjunctions of primitive events, and verify that causality judgments on standard HP benchmarks (e.g., rock-throwing, forest fire scenarios) match the original HP definition per Theorem 4.2.
  2. **Backtracking probe**: Construct a counterfactual structure that corresponds (but not strongly) to a causal model with upstream dependencies. Vary τ to include/exclude U = ū and document when backtracking causes emerge, as in Example 5.1.
  3. **Expressiveness boundary**: For a fixed scenario, systematically expand CX from primitive conjunctions → with negated primitives → with restricted disjunctions (Theorem 4.5 form) → with arbitrary disjunctions. Report which causal claims become newly validated and whether any become counterintuitive.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the full impact of the choice of CX (the set of allowable witness formulas) on the abstract causality definition, and what principled constraints should govern this choice?
- Basis in paper: [explicit] "We are currently trying to understand the impact of C⃗X on the definition."
- Why unresolved: The paper shows CX affects results (e.g., Theorems 4.2 vs. 4.5 use different CX), and the bomb example demonstrates that allowing or disallowing disjunctions changes causal ascriptions, but no general characterization exists.
- What evidence would resolve it: A systematic analysis of how different choices of CX affect causal judgments across standard test cases, with proposed criteria for selecting CX based on desired properties.

### Open Question 2
- Question: Can normality considerations be integrated into the abstract framework by appropriately restricting disjunctions in CX?
- Basis in paper: [explicit] "We are currently exploring the possibility of capturing normality by allowing disjunctions in CX, but restricting all the disjuncts to be normal, in some appropriate sense."
- Why unresolved: Normality has been used to refine HP causality in causal models, but how to extend this to general ccfs without trivializing the definition (which unrestricted disjunctions would cause) remains unclear.
- What evidence would resolve it: A formal definition of "normal" disjuncts within the ccf framework, with proofs showing it captures intuitive normality-based judgments.

### Open Question 3
- Question: Can the abstract definition be successfully applied to other definitions of actual causality besides the modified HP definition?
- Basis in paper: [explicit] "We believe that our approach should be applicable to other ways of defining causality, but we have not checked details."
- Why unresolved: Multiple competing definitions exist (cited: Glymour & Wimberly, Hall, Hitchcock, Woodward), but only the HP definition has been generalized.
- What evidence would resolve it: Formal translations of at least one alternative causality definition into the ccf framework, with equivalence proofs analogous to Theorems 4.2 and 4.5.

## Limitations

- The framework's flexibility relies on well-behaved counterfactual semantics, which may not generalize to non-recursive or cyclic structures without additional constraints.
- The choice of witness language CX is critical but underspecified, with no systematic method for selection that balances expressiveness and intuitive results.
- Strong correspondence between counterfactual structures and causal models is required for HP equivalence, but constructing such structures is non-trivial and may not be feasible for complex models.

## Confidence

- **High Confidence**: The logical structure of the abstract definition (AC1'-AC3') is internally consistent and correctly generalizes the HP approach. The equivalence proofs for restricted witness sets (Theorem 4.2) follow standard logical reasoning patterns.
- **Medium Confidence**: The framework's ability to handle disjunctions, negations, and nested counterfactuals is plausible but relies on careful CX selection. The treatment of backtracking and its distinction from intervention semantics is theoretically sound but may encounter practical difficulties.
- **Low Confidence**: Claims about extending explanations beyond causal models (Definition 6.3) and handling epistemic sets K are highly speculative with no corpus evidence or theoretical foundations developed.

## Next Checks

1. **HP Benchmark Validation**: Implement the causal model adapter (Section 3.1) and verify causality judgments on standard HP benchmarks (rock-throwing, forest fire scenarios) match the original HP definition when CX is restricted to conjunctions of primitive events. This validates the core equivalence claim.

2. **Backtracking Boundary Test**: Construct counterfactual structures corresponding to causal models with upstream dependencies. Systematically vary τ to include/exclude U = ū and document when backtracking causes emerge versus when standard intervention semantics hold. This tests the backtracking mechanism and its control.

3. **CX Expressiveness Probe**: For a fixed scenario, expand CX from primitive conjunctions → with negated primitives → with restricted disjunctions (Theorem 4.5 form) → with arbitrary disjunctions. Report which causal claims become newly validated and whether any become counterintuitive. This characterizes the expressiveness/intuition tradeoff.