---
ver: rpa2
title: 'Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding'
arxiv_id: '2511.00874'
source_url: https://arxiv.org/abs/2511.00874
tags:
- quantization
- gradient
- training
- batch
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the interaction between stochastic rounding
  (SR) and batch size in low-precision training of LLMs. The key insight is that SR
  variance from per-sample activation and gradient quantization scales inversely with
  batch size, enabling larger batches to compensate for reduced precision.
---

# Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding

## Quick Facts
- arXiv ID: 2511.00874
- Source URL: https://arxiv.org/abs/2511.00874
- Reference count: 33
- This paper proves that larger batch sizes can compensate for reduced precision in low-precision LLM training by reducing stochastic rounding variance.

## Executive Summary
This paper analyzes the interaction between stochastic rounding (SR) and batch size in low-precision training of LLMs. The key insight is that SR variance from per-sample activation and gradient quantization scales inversely with batch size, enabling larger batches to compensate for reduced precision. Theoretically, the authors prove that increasing batch size can offset the increased variance from lower precision, e.g., doubling batch size can compensate for a 1-bit precision reduction. Experiments on CIFAR-10 and LLM fine-tuning (Llama-3.2-3B on GSM8K) confirm that SR consistently outperforms round-to-nearest (RTN) and that larger batch sizes improve convergence under aggressive quantization.

## Method Summary
The method applies stochastic rounding (SR) to weights, activations (backward pass only), and gradients during LLM training, while keeping forward activations in high precision. Five quantization points per layer are used: forward activation, weight (forward/backward), activation (backward), and gradient. SR is applied independently per element with Bernoulli rounding based on distance to quantization levels. The key innovation is showing that per-sample quantization variance scales as 1/b, allowing batch size to compensate for low precision.

## Key Results
- SR provides unbiased gradient estimates while RTN introduces systematic bias that accumulates
- Increasing batch size reduces SR variance from per-sample quantization, improving convergence
- Weight quantization introduces irreducible bias that cannot be reduced by batch size
- E4M1 precision with batch size 32 achieves similar accuracy to E4M2 with batch size 8
- SR consistently outperforms RTN at low precision across CIFAR-10 and GSM8K tasks

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Rounding Provides Unbiased Gradient Estimates
SR rounds each value up or down probabilistically with probability proportional to distance from quantization levels. For value $x$ between quantization points $q_1$ and $q_2$, $P(\text{round to } q_1) = (q_2 - x)/(q_2 - q_1)$. This ensures $E[\hat{x}] = x$, enabling convergence even when RTN fails.

### Mechanism 2: Per-Sample Activation/Gradient SR Variance Scales Inversely with Batch Size
Each sample in a mini-batch undergoes independent SR on its activations and upstream gradients. When computing weight gradients, the errors average across the batch. The quantization error variance decays as $1/b$, allowing larger batches to compensate for lower precision.

### Mechanism 3: Weight Quantization Bias Is NOT Reduced by Batch Size
Weights are shared across all samples in a mini-batch. The gradient bias introduced by weight quantization does not average out across samples, creating an irreducible error floor independent of batch size.

## Foundational Learning

- **Stochastic Rounding vs Round-to-Nearest**: Understanding why SR enables low-precision training while RTN fails. Quick check: If you sum 0.7 ten times with RTN (rounding to integers), what do you get? What about SR?
- **Bias-Variance Decomposition in SGD**: The paper's theoretical contribution separates reducible variance (1/b scaling) from irreducible bias (weight quantization). Quick check: Which source of error in low-precision SGD can be reduced by increasing batch size?
- **Floating Point Quantization Step Size**: The paper quantifies precision-bit tradeoffs ($\sigma^2_Q \propto 2^{-2B}$). Quick check: If reducing precision by 1 bit quadruples variance, how much must batch size increase to compensate?

## Architecture Onboarding

- **Component map**: Input activation $\rightarrow$ $Q_{\Delta_A^{fwd}}$ $\rightarrow$ Quantized $\hat{A}_{in}$ $\rightarrow$ multiply with quantized weights $\hat{W}$ $\rightarrow$ output $\rightarrow$ Upstream gradient $\nabla A_{out}$ $\rightarrow$ $Q_{\Delta_{\nabla A}^{bwd}}$ $\rightarrow$ compute $\nabla W = \hat{A}_{in}^T \hat{\nabla A}_{out}$ and $\nabla A_{in} = \hat{\nabla A}_{out} \hat{W}^T$
- **Critical path**: 1) Identify which tensors are quantized (weights shared across batch; activations/gradients per-sample) 2) Apply SR with independent random thresholds per element for per-sample tensors 3) Accumulate gradients in high precision 4) Scale batch size inversely with precision reduction
- **Design tradeoffs**: Forward activation precision kept high for stability; weight SR vs deterministic (SR doubles bias bound vs RTN); batch size vs memory (use gradient accumulation to emulate)
- **Failure signatures**: RTN at low precision: Training diverges or plateaus early; SR with small batch at low precision: High variance slows convergence; Weight-only quantization with low-precision activations: Limited hardware acceleration benefit
- **First 3 experiments**: 1) Ablation on batch size at fixed precision: Train same model with SR at batch sizes [8, 16, 32, 64] with E4M1 quantization; verify gradient norm decreases and accuracy increases with batch size 2) SR vs RTN comparison: At aggressive quantization (E4M0/E4M1), compare SR and RTN across batch sizes; RTN should fail to improve with batch size while SR should show clear scaling 3) Precision-batch tradeoff quantification: Test if halving precision (e.g., E4M2 → E4M1) requires ~2× batch size to maintain same convergence

## Open Questions the Paper Calls Out

### Open Question 1
Does the convergence stability of large-batch stochastic rounding (SR) translate into measurable energy efficiency and latency improvements on physical edge hardware? The paper focused on convergence behavior rather than direct measurements of training speedup, memory footprint reduction, or energy consumption on target edge hardware.

### Open Question 2
How does the interplay between batch size and SR change when combined with advanced quantization schemes like block-wise or non-uniform quantization? The analysis centered on uniform quantization, and the interaction with techniques like non-uniform quantization, block-wise quantization remains an area for future exploration.

### Open Question 3
Does the SR variance-batch size relationship hold when mixed with standard optimization techniques like dynamic loss scaling or adaptive gradient clipping? A detailed investigation of the interplay with other mixed-precision optimization techniques like dynamic loss scaling or adaptive gradient clipping was beyond the scope.

## Limitations

- Theoretical analysis assumes i.i.d. samples within mini-batches, which may not hold for long-sequence tasks
- Experimental validation limited to relatively small-scale tasks (CIFAR-10, 3B parameter model)
- Does not address practical hardware implementation challenges or overhead from random number generation
- Results may not scale to larger models or more complex tasks beyond the tested scenarios

## Confidence

**High Confidence:**
- Stochastic rounding provides unbiased gradient estimates for per-sample quantization
- Variance from SR quantization scales inversely with batch size (1/b relationship)
- Larger batch sizes improve convergence under aggressive quantization when using SR
- Weight quantization introduces irreducible bias independent of batch size

**Medium Confidence:**
- Doubling batch size can compensate for 1-bit precision reduction (theoretical maximum vs empirical findings)
- E4M1 with batch size 32 achieves similar accuracy to E4M2 with batch size 8
- SR consistently outperforms round-to-nearest at low precision across different tasks

**Low Confidence:**
- The compensation effect scales linearly across all precision levels
- Results generalize to larger models (beyond 3B parameters) and more complex tasks
- Hardware implementation overhead is negligible in practice

## Next Checks

1. **Scale-up Validation**: Test the batch-size compensation effect on a larger model (e.g., 7B or 13B parameter LLM) and more complex task to verify if the theoretical bounds hold at scale. Measure if 2× batch size truly compensates for 1-bit precision reduction in these settings.

2. **Lower Precision Exploration**: Extend experiments to 3-bit and 2-bit quantization to determine the practical limits of batch-size compensation. Measure the point at which variance becomes too high even with large batches, and identify the minimum precision where SR with batch scaling remains effective.

3. **Hardware Overhead Measurement**: Implement SR in a realistic hardware simulator or FPGA to quantify the computational overhead of generating random numbers for each quantization operation. Compare this overhead against the potential savings from using lower-precision arithmetic units, and determine the break-even point where SR becomes impractical despite its convergence benefits.