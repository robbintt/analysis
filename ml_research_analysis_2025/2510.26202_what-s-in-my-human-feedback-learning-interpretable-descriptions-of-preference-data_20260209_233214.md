---
ver: rpa2
title: What's In My Human Feedback? Learning Interpretable Descriptions of Preference
  Data
arxiv_id: '2510.26202'
source_url: https://arxiv.org/abs/2510.26202
tags:
- features
- feature
- preference
- preferences
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "What\u2019s In My Human Feedback? (WIMHF) introduces a method\
  \ to automatically discover interpretable features in human feedback datasets without\
  \ pre-specifying hypotheses."
---

# What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data

## Quick Facts
- arXiv ID: 2510.26202
- Source URL: https://arxiv.org/abs/2510.26202
- Authors: Rajiv Movva; Smitha Milli; Sewon Min; Emma Pierson
- Reference count: 40
- Key outcome: Method automatically discovers interpretable features in human feedback datasets using sparse autoencoders, achieving 84% of black-box embeddings' performance while maintaining interpretability.

## Executive Summary
WIMHF introduces a method to automatically discover interpretable features in human feedback datasets without pre-specifying hypotheses. Using sparse autoencoders, WIMHF learns features that capture how pairs of responses differ, then identifies which features actually predict annotator preferences. Across seven datasets, WIMHF finds that a small set of interpretable features explains most of the preference prediction signal—achieving 84% of black-box embeddings' performance while maintaining interpretability. These features reveal diverse preferences across datasets, highlight potential safety issues (e.g., LMArena users preferring unsafe content over refusals), and enable practical applications like effective data curation (+37% safety gains) and personalized alignment on subjective features.

## Method Summary
WIMHF trains a BatchTopK sparse autoencoder on embedding differences between response pairs from preference datasets. The SAE learns M=32 sparse features with K=4 active per input, using Identity activation (not ReLU) to enable signed features. For each feature, the method samples high-activation pairs, prompts an LLM to generate descriptions, then validates fidelity by correlating LLM annotations with feature activations. Finally, logistic regression identifies which features predict preferences, while random-slopes mixed models identify subjective features for personalization.

## Key Results
- WIMHF features achieve 84% of dense embeddings' preference prediction performance (67% of black-box reward model)
- 60.4% of annotator explanations match at least one of the four active SAE features
- WIMHF enables effective data curation: filtering datasets based on features achieves 37% better safety performance than standard RLHF filtering
- The most subjective preference identified is for paragraphs vs. bulleted lists (high τ² indicates annotator disagreement)

## Why This Works (Mechanism)

### Mechanism 1: Sparse Autoencoder Decomposition
Sparse autoencoders can decompose dense embedding differences into a small set of interpretable, semi-orthogonal features that capture how response pairs differ. A BatchTopK SAE is trained on embedding differences (e_rA - e_rB) with sparsity target K=4 and M=32 total features. The sparsity constraint forces each pair's differences to be explained by only ~4 active features, while the signed activation indicates direction (positive = more in rA, negative = more in rB). Matryoshka loss encourages multi-granularity features.

### Mechanism 2: Fidelity-Based Interpretability Validation
Natural language descriptions of SAE features can be validated via fidelity scoring—correlating feature activations with LLM annotations using those descriptions. For each feature, sample 5 high-activation pairs, prompt an LLM to describe the distinguishing pattern, generate 5 candidate descriptions, then validate each by having an LLM annotate held-out pairs and computing Pearson correlation between annotations and feature activations. Descriptions with significant correlation (Bonferroni-corrected) are retained.

### Mechanism 3: Preference and Subjectivity Quantification
Feature coefficients from logistic regression quantify expressed preferences, while random-slopes mixed models identify subjective features suitable for personalization. Fit Pr(y=1) = σ(α + β_j·z_j + γ·x) to identify which features predict preference labels (controlling for length). For personalization, fit random-slopes model with annotator-specific β_j,a ~ N(β_j, τ²_j); high τ²_j indicates subjective features where annotators disagree.

## Foundational Learning

- **Concept: Sparse Autoencoders with TopK/BatchTopK activation**
  - Why needed here: Core architecture for decomposing embedding differences into interpretable features
  - Quick check: Can you explain why sparsity (K ≪ M) promotes interpretability while reconstruction loss maintains completeness?

- **Concept: Mixed-effects models with random slopes**
  - Why needed here: Quantifies subjectivity—whether feature preferences vary across annotators
  - Quick check: In the model β_j,a ~ N(β_j, τ²_j), what does high τ²_j indicate about feature j?

- **Concept: Fidelity scoring for interpretability validation**
  - Why needed here: Provides quantitative evidence that natural language descriptions match learned features
  - Quick check: Why correlate LLM annotations with feature activations rather than just checking description accuracy on a few examples?

## Architecture Onboarding

- **Component map:**
  Raw preference pairs (p, rA, rB, y) → OpenAI text-embedding-3-small → e_rA, e_rB → Embedding difference: eΔ = e_rA - e_rB → BatchTopK SAE (M=32, K=4, Matryoshka prefix=8) → Sparse feature activations Z (N × M) → LLM description + fidelity scoring → feature descriptions → Logistic regression → expressed preference coefficients β

- **Critical path:**
  1. Preprocessing: Remove non-English, long conversations (>2048 tokens), swap rA/rB to avoid position bias
  2. SAE training: BatchTopK with Identity activation (not ReLU) for signed features
  3. Description generation: Sample top-5%-activation pairs, prompt gpt-5-low, validate fidelity with gpt-5-mini-low on 300 held-out pairs
  4. Preference analysis: Fit logistic regression controlling for length (ℓ_Δ)

- **Design tradeoffs:**
  - M=32, K=4: Smaller M/K reduces redundancy but may miss rare features; larger values hurt interpretability
  - Identity vs ReLU: Identity enables signed features (one feature captures both directions), avoiding redundant feature pairs
  - Dataset-specific SAEs: Better fit to each dataset's feature distribution but prevents direct cross-dataset feature comparison
  - LLM judge for cross-dataset analysis: Required since SAE features are dataset-specific, but adds noise

- **Failure signatures:**
  - Low fidelity scores across all features → embedding space poorly aligned with interpretable concepts; try different base embeddings
  - High reconstruction error → increase M or K
  - Redundant feature descriptions → decrease M or increase Matryoshka prefix
  - No significant β_j coefficients → features don't predict preferences; check if embedding differences encode relevant signal
  - Low match rate with annotator explanations (baseline ~33%) → descriptions may be too abstract

- **First 3 experiments:**
  1. **Sanity check:** Train SAE on a synthetic dataset where responses differ along known axes (length, sentiment, formality); verify top features recover these.
  2. **Fidelity calibration:** For each feature, vary the number of sampled examples (5 vs 10 vs 20) used for description generation; plot fidelity vs sample size.
  3. **Downstream validation:** Fit preference prediction using only SAE features vs. full embeddings; target ≥80% of embedding AUC. If below, increase K.

## Open Questions the Paper Calls Out

### Open Question 1
Can methods that explicitly incorporate the prompt text, rather than relying solely on response embeddings, improve the discovery of preference features? The authors note that using full prompt-response transcripts did not improve prediction in initial tests and leave further explorations of methods to incorporate the prompt to future work.

### Open Question 2
Do the features identified by WIMHF causally influence human choices, or do they merely correlate with other unobserved drivers of preference? The method relies on observational data and regression to identify associations; determining if changing a feature directly changes the preference label requires interventional validation which was not performed.

### Open Question 3
How can the conflicting preferences identified by WIMHF across different datasets be algorithmically resolved during training to prevent "washing out" desired behaviors? While WIMHF identifies the conflicts and allows for manual data curation, the paper does not propose an automated training objective to harmonize or prioritize these conflicting signals during preference finetuning.

## Limitations
- LLM dependency for interpretability: The method relies on GPT-5 for feature description generation and validation, creating a black-box dependency that may not generalize across domains or model versions
- Dataset-specific SAEs: Features are learned separately for each dataset, preventing direct cross-dataset feature comparison and limiting generalizability
- Potential spurious correlations: The sparsity constraint may encourage features that correlate with preferences through confounding factors rather than causal relationships

## Confidence
- **High confidence:** Preference prediction performance claims (84% of embeddings, 67% of reward models) are well-supported by quantitative metrics across multiple datasets
- **Medium confidence:** Interpretability claims rely on LLM validation, which introduces uncertainty about human interpretability despite correlation metrics
- **Low confidence:** Generalization to new domains without extensive validation, particularly given the reliance on specific LLM judges

## Next Checks
1. **Cross-dataset feature transfer:** Train SAEs on one dataset and evaluate preference prediction on another to assess feature generalizability beyond the training distribution
2. **Human validation study:** Conduct a user study where human annotators evaluate feature descriptions independently of the LLM judge to verify the validity of automated interpretability assessment
3. **Ablation of LLM components:** Replace LLM-based description and validation with rule-based or simpler language models to test whether the core methodology depends on advanced LLM capabilities