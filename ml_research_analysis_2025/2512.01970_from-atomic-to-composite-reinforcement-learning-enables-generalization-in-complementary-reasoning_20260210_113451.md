---
ver: rpa2
title: 'From Atomic to Composite: Reinforcement Learning Enables Generalization in
  Complementary Reasoning'
arxiv_id: '2512.01970'
source_url: https://arxiv.org/abs/2512.01970
tags:
- reasoning
- comp
- training
- generalization
- atomic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates how reinforcement learning (RL) contributes
  to reasoning capabilities, focusing on Complementary Reasoning, which integrates
  internal parametric knowledge with external contextual information. Using a controlled
  synthetic dataset of human biographies, the authors decouple this task into two
  atomic skills: Parametric and Contextual Reasoning.'
---

# From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning

## Quick Facts
- **arXiv ID:** 2512.01970
- **Source URL:** https://arxiv.org/abs/2512.01970
- **Reference count:** 25
- **Primary result:** RL acts as a reasoning synthesizer, but only if the base model has first mastered atomic skills (parametric vs. contextual reasoning) via SFT

## Executive Summary
This study investigates how reinforcement learning (RL) contributes to reasoning capabilities, focusing on Complementary Reasoning, which integrates internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, the authors decouple this task into two atomic skills: Parametric and Contextual Reasoning. They evaluate generalization across three levels: I.I.D., Composition, and Zero-shot settings. The results reveal that while Supervised Fine-Tuning (SFT) is sufficient for in-distribution performance, it struggles with out-of-distribution generalization, particularly in Zero-shot settings. Crucially, the SFT Generalization Paradox shows that models trained solely on the composite task achieve high in-distribution accuracy but fail on out-of-distribution generalization. In contrast, RL acts as a reasoning synthesizer, but only if the base model has first mastered the independent atomic skills via SFT. This challenges the view of RL as a mere amplifier, suggesting that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.

## Method Summary
The study constructs a synthetic human biography dataset using a knowledge graph with 39 relations and Python's Faker library. The task of Complementary Reasoning is decomposed into two atomic skills: Parametric Reasoning (MEM) and Contextual Reasoning (CTX). Models are first trained via SFT on MEM and CTX data separately, then fine-tuned using Group Relative Policy Optimization (GRPO) on composite Complementary Reasoning (COMP) data. Performance is evaluated across three generalization levels: I.I.D. (seen relation paths), Composition (unseen paths, seen relations), and Zero-shot (unseen relations). The critical comparison is between SFT trained directly on COMP data versus SFT on atomic skills followed by RL on COMP data.

## Key Results
- SFT on composite tasks achieves 90% I.I.D. accuracy but collapses to 18% on Zero-shot generalization
- RL significantly improves Zero-shot performance only when base model has mastered atomic skills via SFT
- Pass@k analysis shows RL synthesizes new reasoning strategies (parallel curves at high k) rather than merely amplifying existing ones
- With less than 10% of COMP data, SFT_MEM+CTX effectively matches the performance of models SFT on the entire dataset

## Why This Works (Mechanism)

### Mechanism 1: Atomic Skill Disentanglement Enables RL Synthesis
- **Claim:** RL synthesizes novel compositional reasoning strategies only when the base model has first learned disentangled representations of atomic skills through SFT.
- **Mechanism:** SFT on atomic tasks creates structurally separated latent representations for each reasoning type. PCA analysis shows distinct centroid directions for MEM and CTX data after SFT_MEM+CTX. This disentanglement allows RL to subsequently compose these skills into new pathways for COMP tasks.
- **Core assumption:** The paper assumes that representation disentanglement during SFT is causally linked to RL's ability to compose skills.
- **Evidence anchors:**
  - [abstract]: "RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT."
  - [Section 6.2]: Pass@k analysis shows persistent performance gaps at k=2^9 for SFT_MEM+CTX (curves remain parallel), indicating RL synthesized new logic circuits absent from SFT distribution.
  - [Section E.3]: "SFT_MEM+CTX exhibits a significant 'disentanglement' of atomic reasoning types... The centroid for MEM data and CTX data move in distinct directions."
  - [corpus]: Related work (arXiv:2512.24063) supports that "RL tuning tends to preserve [generalization]" while SFT narrows capability.

### Mechanism 2: SFT Memorization Trap on Composite Tasks
- **Claim:** Direct SFT on composite tasks creates surface-level pattern matching that achieves high in-distribution accuracy but catastrophically fails on OOD generalization.
- **Mechanism:** SFT on COMP data optimizes for maximum likelihood on training distribution, incentivizing memorization of specific relational path shortcuts. When test queries require novel relation combinations (Zero-shot), the memorized shortcuts are inapplicable.
- **Core assumption:** The paper assumes this is a fundamental property of SFT rather than a data quantity issue.
- **Evidence anchors:**
  - [abstract]: "Models supervised solely on the composite task achieve near-perfect in-distribution accuracy (90%) but collapse on out-of-distribution generalization (18%)."
  - [Section 4.3]: "SFT Memorizes rather than Generalizes... COMP achieves near-perfect 90.26% on I.I.D., its performance collapses to 26.25% on Zero-shot."
  - [Section 5.1]: "As data portion x exceeds 70%, SFT_COMP begins to surpass SFT_MEM+CTX on I.I.D. tasks, but fails on Zero-shot... the memorization trap."
  - [corpus]: Related work (arXiv:2505.00527) on task decomposition for zero-shot generalization supports the atomic-to-composite training paradigm.

### Mechanism 3: RL Exploration Efficiency via Atomic Foundations
- **Claim:** Models with atomic skill foundations exhibit superior sample efficiency during RL adaptation, requiring <10% of composite training data to match full-SFT baseline performance.
- **Mechanism:** SFT_MEM+CTX creates a structured latent space where RL can efficiently explore compositions. Uncertainty analysis shows SFT_MEM+CTX achieves dramatic entropy reduction after RL while SFT_COMP shows minimal reduction.
- **Core assumption:** The paper assumes the entropy reduction indicates efficient convergence rather than overfitting.
- **Evidence anchors:**
  - [Section 6.1]: "With less than 10% of COMP data, SFT_MEM+CTX effectively matches the performance of the model SFT on the entire dataset."
  - [Section E.4]: "SFT_MEM+CTX achieves a drastic reduction in uncertainty (7.13→2.90)... structures the latent space in a way that allows RL to efficiently converge."
  - [Figure 5]: Shows consistent outperformance across all data scales (20%-100%) for SFT_MEM+CTX vs SFT_COMP.
  - [corpus]: Weak corpus support—no directly comparable sample efficiency studies found in neighbor papers.

## Foundational Learning

- **Concept: Parametric vs. Contextual Reasoning Distinction**
  - **Why needed here:** The entire experimental design depends on strictly decoupling knowledge stored in model parameters (MEM) from knowledge provided in context window (CTX).
  - **Quick check question:** Given a biography stating "X was born in 1990" in the context, and a question "Where was X born?", is this parametric or contextual reasoning? (Answer: Contextual—the fact is in the provided context, not learned during pre-training.)

- **Concept: Generalization Level Taxonomy (I.I.D. → Composition → Zero-shot)**
  - **Why needed here:** The paper's conclusions about RL vs. SFT only hold when evaluated across this hierarchy. I.I.D. success with SFT is expected; Zero-shot failure reveals the memorization trap.
  - **Quick check question:** If training contains paths A→B and C→D, and testing requires A→D, which generalization level is this? (Answer: Composition—seen relations in novel combination, assuming A, B, C, D are all seen relations.)

- **Concept: Pass@k Analysis for Skill Synthesis vs. Amplification**
  - **Why needed here:** This is the key diagnostic for distinguishing whether RL creates new capabilities (persistent gap at high k) or amplifies existing ones (converging curves).
  - **Quick check question:** If SFT model pass@k approaches RL model pass@k as k→∞, what does this indicate about RL's role? (Answer: Amplification—RL merely increases probability of solutions already in SFT distribution.)

## Architecture Onboarding

- **Component map:**
  Pre-trained Base Model (Qwen-2.5-1.5B) → Atomic SFT Stage (MEM+CTX data) → Composite RL Stage (COMP data) → Evaluation across three levels: I.I.D., Composition, Zero-shot

- **Critical path:** SFT on MEM+CTX data → verify atomic skills via MEM/CTX test sets → apply RL with GRPO on COMP data (binary outcome reward on answer only, not reasoning chain) → evaluate on COMP test set across all three generalization levels.

- **Design tradeoffs:**
  - **Data allocation:** More COMP data for SFT (vs. RL) improves I.I.D. but risks memorization trap; paper recommends focusing SFT on atomic skills, using COMP primarily for RL.
  - **Training loss target:** SFT should converge to loss <0.05 but not necessarily to memorization (0.0004 shows no additional benefit).
  - **Reward design:** Paper uses outcome-only reward (exact match on answer), not process reward on CoT—simpler but may limit fine-grained optimization.

- **Failure signatures:**
  - High I.I.D. (>85%) but low Zero-shot (<30%) on COMP after SFT indicates memorization trap.
  - Negligible RL gain (<5% improvement) despite sufficient COMP data indicates missing atomic prerequisites.
  - Pass@k curves converging by k=2^9 indicates RL is amplifying, not synthesizing.
  - Early-stage errors (Progress <50%) in error analysis indicate failure to bridge MEM→CTX.

- **First 3 experiments:**
  1. **Baseline establishment:** Train SFT_COMP on full COMP data (~180k samples), evaluate on all three generalization levels. Expected: I.I.D. ~90%, Composition ~76%, Zero-shot ~18%. This confirms the memorization paradox.
  2. **Atomic prerequisite test:** Train SFT_MEM+CTX on atomic data only (~91k combined), then apply RL on 12.8k COMP samples. Compare against SFT_COMP with identical RL data. Expected: SFT_MEM+CTX→RL_COMP achieves ~50% Zero-shot vs. ~18% for SFT_COMP→RL_COMP.
  3. **Pass@k diagnostic:** For both SFT_MEM+CTX and SFT_COMP (before and after RL), compute pass@k for k=2^0 to 2^9 on Zero-shot COMP. Expected: SFT_MEM+CTX shows persistent gap (parallel curves), SFT_COMP shows convergence (merging curves), confirming synthesis vs. amplification distinction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the mechanistic circuits through which RL recruits atomic attention heads to perform complex reasoning?
- Basis in paper: [explicit] The authors state, "future work should investigate the mechanistic interpretability of how RL circuits recruit atomic attention heads."
- Why unresolved: The current study relies on behavioral metrics (accuracy, pass@k) and latent space analysis (PCA) rather than fine-grained circuit analysis.
- What evidence would resolve it: Causal tracing or specific head ablation studies identifying the exact attention heads responsible for bridging parametric and contextual knowledge.

### Open Question 2
- Question: Can the "SFT on atomic skills + RL on composite tasks" recipe generalize effectively to real-time benchmarks where new information is strictly post-training?
- Basis in paper: [explicit] The paper suggests validating findings on "controlled splits of real-time benchmarks (e.g., news QA) where the 'new' information is strictly dated post-training."
- Why unresolved: The study relied on synthetic biographies to strictly enforce the boundary between parametric and contextual knowledge, which is impossible to guarantee in standard web corpora.
- What evidence would resolve it: Experiments on temporal datasets (e.g., news QA with strict date cutoffs) demonstrating similar generalization gaps between SFT-only and RL-trained models.

### Open Question 3
- Question: How should composite (COMP) data be mixed with atomic (MEM and CTX) data during training to optimize performance across I.I.D., Composition, and Zero-shot settings simultaneously?
- Basis in paper: [explicit] The authors ask, "future work should study how to mix COMP data with MEM and CTX data to improve the overall I.I.D., Composition and Zero-shot performance."
- Why unresolved: The current work primarily contrasts training purely on atomic skills versus purely on composite tasks, rather than exploring optimal blending ratios during the SFT phase.
- What evidence would resolve it: Ablation studies varying the ratio of atomic-to-composite data during SFT to identify a "sweet spot" that maximizes in-distribution accuracy without sacrificing zero-shot robustness.

## Limitations

- The core mechanism relies on assumed causal link between SFT-induced representation disentanglement and RL's ability to compose skills, primarily supported by correlation-based PCA visualizations
- Conclusions about RL as a reasoning synthesizer are demonstrated only on a synthetic dataset with controlled relation boundaries
- The critical threshold of "sufficiently mastered" atomic skills for RL synthesis is not precisely defined beyond training loss targets

## Confidence

- **High confidence:** SFT_COMP memorization trap (observed collapse from 90% to 18% on Zero-shot) and the necessity of atomic skill foundations before RL
- **Medium confidence:** The mechanism of RL as a "reasoning synthesizer" versus mere amplifier (supported by pass@k analysis but could benefit from more diverse validation methods)
- **Medium confidence:** The sample efficiency claim (10% data achieving full performance) - while shown empirically, the underlying efficiency mechanism could have alternative explanations

## Next Checks

1. Conduct intervention experiments manipulating representation disentanglement (e.g., via regularization during SFT) to test whether it causally affects RL composition ability
2. Test the atomic-to-composite training paradigm on real-world knowledge graphs with naturally occurring relation boundaries rather than synthetic controlled data
3. Implement process-based rewards in RL (rewarding intermediate reasoning steps) to determine if this further improves synthesis capability beyond outcome-only rewards