---
ver: rpa2
title: 'On Fairness of Task Arithmetic: The Role of Task Vectors'
arxiv_id: '2505.24262'
source_url: https://arxiv.org/abs/2505.24262
tags:
- task
- fairness
- accuracy
- lora
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically examines the fairness implications of\
  \ model editing techniques, specifically task arithmetic using task vectors, in\
  \ the context of hate speech detection. Task vectors\u2014defined as parameter differences\
  \ between base and fine-tuned models\u2014are manipulated through addition, negation,\
  \ and scalar scaling to modify model behavior without retraining."
---

# On Fairness of Task Arithmetic: The Role of Task Vectors

## Quick Facts
- **arXiv ID**: 2505.24262
- **Source URL**: https://arxiv.org/abs/2505.24262
- **Reference count**: 40
- **One-line primary result**: Task arithmetic with optimized coefficients can improve fairness metrics while maintaining accuracy comparable to baselines.

## Executive Summary
This study systematically examines the fairness implications of model editing techniques, specifically task arithmetic using task vectors, in the context of hate speech detection. Task vectors—defined as parameter differences between base and fine-tuned models—are manipulated through addition, negation, and scalar scaling to modify model behavior without retraining. The research benchmarks task arithmetic against full fine-tuning (FFT) and LoRA, evaluating subgroup-level accuracy, demographic parity difference (DPD), and equalized odds difference (EOD) across gender and race demographics. Results show that task arithmetic with optimized coefficients can improve fairness metrics (lower DPD and EOD) while maintaining accuracy comparable to baselines. Injecting task vectors from underrepresented subgroups into FFT models can further adjust fairness outcomes, though effects are subgroup-specific and not universally positive. Overall, task arithmetic offers a promising, interpretable approach for fairness-aware model editing, though careful tuning is required to balance performance and equity across demographic groups.

## Method Summary
The method uses task vectors (parameter differences between base and fine-tuned models) to edit model behavior for fairness in hate speech detection. The approach involves partitioning data by demographic subgroups, fine-tuning separate models for each subgroup to extract task vectors, and then merging these vectors with the base model using a scalar coefficient λ. The technique is evaluated against FFT and LoRA baselines across accuracy, DPD, and EOD metrics. Key hyperparameters include λ ∈ [0.0, 1.0] for scaling task vectors, with experiments conducted on LLaMA2-7B using the Berkeley D-Lab Hate Speech dataset with 6,898 tweets across gender and race demographics.

## Key Results
- Task arithmetic with optimized coefficients can improve fairness metrics (lower DPD and EOD) while maintaining accuracy comparable to baselines.
- As λ increases, accuracy peaks around 0.2-0.5 but fairness metrics improve beyond λ=0.3.
- Injecting task vectors from worst-performing subgroups into FFT models can adjust fairness outcomes, though effects are subgroup-specific and not universally positive.

## Why This Works (Mechanism)

### Mechanism 1: Fairness-Aware Weight Interpolation via Scaling
- Claim: Scaling task vectors (λ ∈ [0.0, 1.0]) provides a controllable trade-off between fairness and accuracy.
- Mechanism: A task vector Δθ = θ_task - θ_base is a direction in weight space. Merging with θ_merged = θ_base + Σ λ·Δθ_i interpolates model behavior. The paper reports a sweet spot: λ ≈ 0.2-0.5 can achieve high accuracy but with higher DPD/EOD (lower fairness), while higher λ can improve fairness metrics at competitive accuracy.
- Core assumption: The task vector's direction encodes the necessary knowledge, and its magnitude is proportional to its influence on both performance and fairness.
- Evidence anchors:
  - [abstract] "results show that task arithmetic with optimized coefficients can improve fairness metrics (lower DPD and EOD) while maintaining accuracy comparable to baselines."
  - [section 5.1] "As λ increases from 0.0 to 0.2, we observe a peak in accuracy, but this configuration yields higher DPD and EOD... Beyond λ = 0.3... both DPD and EOD progressively decline, suggesting that fairness improves..."
  - [corpus] Evidence is weak or missing; related papers on task vectors focus on editing efficiency, not fairness control.
- Break condition: If the λ-fairness relationship is highly non-monotonic or erratic for certain subgroups, simple scaling fails, requiring per-subgroup tuning.

### Mechanism 2: Subgroup-Specific Task Vector Injection
- Claim: Injecting a task vector from a worst-performing subgroup into a fine-tuned model can adjust fairness, though effects are subgroup-specific.
- Mechanism: A new model is constructed via θ_new = θ_SFT + λ(θ_wp - θ_base), where θ_wp is the model fine-tuned on the worst-performing subgroup's data. This is hypothesized to shift the model to better represent underrepresented patterns. The paper shows this can improve fairness (e.g., for Men, Asian) but not universally (e.g., Women, Native American show mixed or negative results).
- Core assumption: A worst-performing subgroup's task vector encodes corrective information that does not disproportionately harm other subgroups.
- Evidence anchors:
  - [abstract] "Injecting task vectors from underrepresented subgroups into FFT models can further adjust fairness outcomes, though effects are subgroup-specific and not universally positive."
  - [section 4.2] "To investigate whether incorporating task vectors from under-performing subgroups can improve fairness... we constructed a new model variant by injecting a worst-performing subgroup task vector..."
  - [section 5.2] "When the Women task vector is added... fairness metrics for subgroups such as Men tend to worsen... simple task addition is not a guaranteed to have positive fairness influence..."
  - [corpus] Evidence is weak or missing; related work mentions negative transfer in merging but not this specific injection mechanism.
- Break condition: If injection causes severe negative transfer—significantly degrading other subgroups—the simple injection strategy fails.

### Mechanism 3: Disentangled Representation of Subgroup Knowledge
- Claim: Partitioning data by subgroup and creating separate task vectors allows for isolated analysis and targeted manipulation of subgroup-specific behaviors.
- Mechanism: Fine-tuning separate models on demographic partitions yields task vectors Δθ_i for each subgroup. These vectors, in a common weight space, can be compared and merged. This approach is argued to enhance interpretability by isolating subgroup-specific weight updates.
- Core assumption: Task vectors from different subgroups are meaningfully separable and their arithmetic operations correspond to intuitive behavior modifications without severe interference.
- Evidence anchors:
  - [section 1] "By isolating the weight updates associated with particular subgroups... one can potentially trace how the model adapts to each subgroup."
  - [section 4.2] "The training data was partitioned by subgroup... and FFT was applied separately... to produce fine-tuned models θ_i. From these, we computed task vectors Δθ_i..."
  - [section 6] "...task arithmetic framework allows for subgroup-specific evaluation and adjustment of model updates, enhancing interpretability..."
  - [corpus] Corpus supports disentanglement (e.g., "Decomposing Task Vectors for Refined Model Editing").
- Break condition: If task vectors are not disentangled (high interference), their manipulation leads to unpredictable, non-composable outcomes.

## Foundational Learning

- Concept: **Task Vectors and Task Arithmetic**
  - Why needed here: This is the paper's core technique. A task vector is the parameter difference (Δθ = θ_task - θ_base), and task arithmetic (addition, negation, scaling, merging) allows for training-free model editing.
  - Quick check question: Given a base model and a fine-tuned model, write the equation to create a task vector and then apply it with half strength.

- Concept: **Fairness Metrics (DPD & EOD)**
  - Why needed here: These are the key outcome measures. DPD measures disparity in positive prediction rates across groups; EOD measures disparity in true/false positive rates. Lower values indicate greater fairness.
  - Quick check question: If a model has a high EOD for a racial subgroup in hate speech detection, what specific type of error is it making disproportionately?

- Concept: **Parameter-Efficient Fine-Tuning (PEFT) with LoRA**
  - Why needed here: LoRA is a primary baseline. It updates low-rank adapter matrices rather than full weights, offering a contrast to task arithmetic which operates on full-weight differences.
  - Quick check question: Why might a full-weight-space method (task arithmetic) have different fairness properties than a low-rank adapter method (LoRA)?

## Architecture Onboarding

- Component map:
  - Base Pre-trained Model (θ_base) -> Subgroup-Specific Fine-tuned Models (θ_i) -> Task Vectors (Δθ_i) -> Merging Function (θ_merged = θ_base + Σ λ·Δθ_i) -> Evaluation (accuracy, DPD, EOD)

- Critical path:
  1. **Partition Data**: Split the full training dataset by demographic attributes (gender, race).
  2. **Compute Subgroup Task Vectors**: For each subgroup, fine-tune θ_base to get θ_i, then compute Δθ_i = θ_i - θ_base.
  3. **Merge with Tuning**: Combine task vectors via θ_merged = θ_base + Σ λ·Δθ_i. The coefficient λ is the critical control for the accuracy-fairness trade-off.
  4. **Evaluate**: Assess subgroup-wise accuracy, DPD, and EOD. Iterate on λ or try targeted injection of worst-performing subgroup vectors.

- Design tradeoffs:
  - **Uniform vs. Per-Vector Scaling**: The paper uses a uniform λ for all vectors. Per-vector λ_i could offer finer control but expands the search space.
  - **Merging vs. Targeted Injection**: Merging all vectors aims for balance; targeted injection into an FFT model is a surgical intervention with less predictable cross-group effects.
  - **Task Arithmetic vs. LoRA/FFT**: Task arithmetic offers post-hoc, training-free control but requires pre-computed vectors. LoRA is efficient but may not offer the same interpretability or fairness control.

- Failure signatures:
  - **Erratic Fairness Curves**: Fairness metrics for some groups fluctuate unpredictably or worsen as λ increases.
  - **Negative Transfer**: Adding a task vector to help one subgroup significantly degrades performance or fairness for another.
  - **Stagnant or Worsening Fairness**: Increasing λ leads to higher DPD/EOD, indicating amplified bias.

- First 3 experiments:
  1. **Reproduce Core Comparison**: On a benchmark hate speech dataset, compare FFT, LoRA, and task arithmetic (merging all subgroup vectors with uniform λ) on accuracy, DPD, and EOD.
  2. **Coefficient Ablation**: For task arithmetic, sweep λ from 0.0 to 1.0 and plot macro-accuracy, DPD, and EOD to identify the optimal trade-off region.
  3. **Targeted Subgroup Injection**: Identify the worst-performing subgroup from an FFT baseline. Inject its task vector with varying λ and measure the impact on both its own and other subgroups' fairness metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the fairness-preserving properties of task arithmetic generalize to domains other than hate speech detection and model architectures other than Llama-2 7B?
- Basis in paper: [explicit] The limitations section states that "the effectiveness of task arithmetic depends on dataset characteristics... necessitating further investigation into its generalizability across different tasks and domains."
- Why unresolved: The study restricted its experimental validation to a single task (hate speech) and a specific model size (Llama-7B).
- What evidence would resolve it: Empirical results demonstrating consistent DPD and EOD improvements when applying task arithmetic to diverse tasks (e.g., medical classification) or different base models.

### Open Question 2
- Question: How can the scalar coefficient λ be automatically optimized to balance the trade-off between accuracy and fairness across subgroups?
- Basis in paper: [explicit] The conclusion notes that "future work should explore algorithms for automatically optimizing the scalar coefficient λ and for balancing trade-offs among multiple subgroups."
- Why unresolved: The current methodology relies on manual tuning of λ (e.g., testing intervals of 0.1), which may not be scalable or optimal for all datasets.
- What evidence would resolve it: A proposed optimization algorithm that dynamically determines optimal λ values without requiring manual grid search or extensive retraining.

### Open Question 3
- Question: Can a multi-objective optimization strategy be developed to ensure injecting task vectors for one subgroup does not degrade fairness for others?
- Basis in paper: [explicit] The introduction highlights that "improving performance for one demographic might degrade outcomes for another, and it is not yet clear how to balance trade-offs."
- Why unresolved: Results showed that injecting task vectors (e.g., for Men) led to mixed trends, improving some metrics while fairness for the Women subgroup became erratic or worse.
- What evidence would resolve it: A modified task arithmetic method that achieves Pareto improvements, where DPD and EOD decrease for all subgroups simultaneously.

## Limitations

- The paper's fairness results depend heavily on the quality of subgroup partitioning and the assumption that task vectors are meaningfully disentangled.
- The effectiveness of targeted injection is subgroup-specific and not universally positive, raising questions about its general applicability.
- The lack of a specified train/validation/test split and exact input formatting for the base model introduces reproducibility risks.

## Confidence

- **High Confidence**: The core mechanism of task arithmetic (merging task vectors with scaling coefficients) is well-established and the paper's implementation details are sufficiently specified for replication.
- **Medium Confidence**: The observed trade-off between accuracy and fairness (DPD/EOD) via coefficient tuning is plausible but may vary with dataset and model architecture. The subgroup-specific injection effects are promising but require further validation.
- **Low Confidence**: The generalizability of fairness improvements across diverse demographic groups and datasets is uncertain, particularly for intersectional subgroups not explicitly studied.

## Next Checks

1. **Reproduce the Accuracy-Fairness Trade-off Curve**: Sweep the merging coefficient λ on the Berkeley D-Lab dataset and plot accuracy, DPD, and EOD for each subgroup to confirm the paper's reported trends.
2. **Test Intersectional Subgroups**: Extend the analysis to intersectional demographic groups (e.g., Asian Women, Black Men) to assess whether task arithmetic fairness benefits hold across combined attributes.
3. **Compare with Alternative Baselines**: Evaluate task arithmetic against additional baselines such as full fine-tuning with subgroup-aware sampling or other PEFT methods (e.g., LoRA with larger rank) to contextualize its relative performance.