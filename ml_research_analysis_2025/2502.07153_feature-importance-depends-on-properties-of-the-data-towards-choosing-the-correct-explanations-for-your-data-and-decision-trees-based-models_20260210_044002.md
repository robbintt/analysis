---
ver: rpa2
title: 'Feature Importance Depends on Properties of the Data: Towards Choosing the
  Correct Explanations for Your Data and Decision Trees based Models'
arxiv_id: '2502.07153'
source_url: https://arxiv.org/abs/2502.07153
tags:
- feature
- importance
- methods
- datasets
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the reliability of local explanation methods
  for decision tree-based models, focusing on feature importance estimates. The authors
  synthesize datasets with varying properties (noise, correlation, class imbalance)
  and compare six explainability methods (LIME, LSurro, Kshap, Sshap, Tshap, TI) against
  ground truth importance values.
---

# Feature Importance Depends on Properties of the Data: Towards Choosing the Correct Explanations for Your Data and Decision Trees based Models

## Quick Facts
- arXiv ID: 2502.07153
- Source URL: https://arxiv.org/abs/2502.07153
- Reference count: 40
- Primary result: SHAP explainers are consistent with each other but poorly aligned with ground truth importance; TI is highly sensitive to noise

## Executive Summary
This paper evaluates six local explanation methods for decision tree-based models on synthetic and real-world datasets with varying data properties. The authors systematically vary noise levels, feature correlation, and class imbalance in synthetic datasets, then compare explanation methods against ground truth importance values derived from logical operators. They find that SHAP variants (Kernel SHAP, Sampling SHAP, Tree SHAP) show high internal consistency but poor alignment with ground truth, while LSurro and LIME are less affected by noise but tend to overestimate unimportant features. Tree Interpreter is particularly sensitive to noise due to its feature and noise contribution decomposition. The study concludes that feature importance attribution is affected by data properties, model type, and explanation method assumptions, providing specific recommendations for method selection.

## Method Summary
The authors generate 24 synthetic datasets (1,000 instances each) with bivariate normal features, varying correlation (ρ ∈ {0, 0.1, 0.9, 1.0}), and noise levels (ε ∈ {0, 0.25, 0.5}) using XOR and NOT logical operators as ground truth. They train Decision Trees and Random Forests (max_depth=2 for synthetic, RF with grid search + 10-fold CV for real-world) on 80/20 train-test splits. Six explainers (LIME, LSurro, Kernel SHAP, Sampling SHAP, Tree SHAP, Tree Interpreter) are evaluated on test sets using consistency (L2 distance vs ground truth), stability, and compactness metrics. Real-world UCI datasets (Heart Diagnosis, Cervical Cancer, Adult Income, German Credit Risk) validate findings.

## Key Results
- SHAP explainers show high consistency among themselves but poor alignment with ground truth importance values
- LSurro and LIME are less affected by noise and feature correlation but tend to overestimate unimportant features
- Tree Interpreter is particularly sensitive to noise due to its feature and noise contribution decomposition
- Across datasets, SHAP explainers and TI share top-10 important features, while LSurro explains 100% of model output with only 5 features in some cases

## Why This Works (Mechanism)
None provided in the source material.

## Foundational Learning
- **Consistency metric**: Measures alignment between explanation and ground truth (L2 distance); needed to evaluate explanation accuracy against known feature importance.
- **Stability metric**: Evaluates explanation robustness across perturbations; needed to assess reliability under data variations.
- **Compactness metric**: Measures # features required for 90% model accuracy; needed to understand feature redundancy and efficiency.
- **Ground truth importance**: Derived from XOR/NOT logical operators (φ*=0.5 each for XOR; φ_X1=1, φ_X2=ρ for NOT); needed as reference for evaluating explanation methods.
- **Synthetic data generation**: Uses bivariate normal distributions with controlled covariance; needed to systematically vary data properties while maintaining known ground truth.

## Architecture Onboarding
- **Component map**: Synthetic data generation -> Model training (DT/RF) -> Explanation methods (6 variants) -> Metric evaluation (consistency, stability, compactness)
- **Critical path**: Data generation → Model training → Explanation → Metric computation → Comparison with ground truth
- **Design tradeoffs**: SHAP variants trade speed for accuracy (Sshap/Tshap faster); LSurro/LIME more robust to noise but overestimate; TI provides interpretable feature/noise contributions but highly noise-sensitive
- **Failure signatures**: SHAP methods diverging from ground truth despite high inter-method consistency; TI producing noise-dominated explanations at high ε; LSurro/LIME over-attributing importance to irrelevant features
- **First experiments**: 1) Verify synthetic data generation produces expected ground truth (XOR: equal importance; NOT: importance proportional to correlation) 2) Confirm model accuracy before explanation (noise=0.5 yields ~50% accuracy) 3) Compare feature importance distributions across methods on noise-free dataset

## Open Questions the Paper Calls Out
- **Open Question 1**: How do explanation methods perform when the number of features and test instances varies significantly? [explicit] The study was limited to 2 features and 1,000 instances, leaving scalability untested.
- **Open Question 2**: How do these explanation methods generalize to regression, multi-class classification, image, and text domains? [explicit] The study restricted evaluation to binary classification on tabular data only.
- **Open Question 3**: How does each method's internal assumptions affect feature importance attribution when isolating a single model parameter? [explicit] The paper compared methods jointly rather than conducting deep ablation studies on individual method mechanics.
- **Open Question 4**: How do explanation methods perform on high-dimensional datasets with complex multi-way feature interactions? [inferred] The synthetic framework was limited to bivariate interactions, but real-world datasets often contain dozens of interacting features.

## Limitations
- Missing implementation details for LSurro (referenced only as Molnar 2022 book)
- Shallow decision trees (max_depth=2) may not generalize to deeper models
- Synthetic datasets use bivariate normal distributions, limiting generalizability to high-dimensional or non-normal data
- Stability metric formula referenced but not provided (equation 10)

## Confidence
- **High confidence**: SHAP methods show internal consistency but poor ground truth alignment; TI sensitivity to noise is well-supported
- **Medium confidence**: LSurro/LIME overestimation behavior, given missing implementation details; real-world dataset findings require validation
- **Medium confidence**: Noise impact findings, though synthetic noise injection method is clear, real-world noise effects may differ

## Next Checks
1. Implement LSurro with neighborhood size and surrogate model parameters from Molnar 2022, then compare results against reported values
2. Replicate synthetic dataset generation and evaluate all six methods on same datasets to verify consistency and stability metric calculations
3. Apply recommended explainers to additional real-world datasets (beyond UCI) to test generalizability of recommendations across different domains and noise levels