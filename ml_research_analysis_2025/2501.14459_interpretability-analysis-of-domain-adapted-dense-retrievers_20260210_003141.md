---
ver: rpa2
title: Interpretability Analysis of Domain Adapted Dense Retrievers
arxiv_id: '2501.14459'
source_url: https://arxiv.org/abs/2501.14459
tags:
- domain
- query
- document
- attribution
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Integrated Gradients (IG) to analyze
  and interpret domain-adapted dense retrievers. The authors introduce a novel baseline
  method that computes both query and document attributions by replacing tokens with
  [PAD] tokens.
---

# Interpretability Analysis of Domain Adapted Dense Retrievers

## Quick Facts
- arXiv ID: 2501.14459
- Source URL: https://arxiv.org/abs/2501.14459
- Authors: Goksenin Yuksel; Jaap Kamps
- Reference count: 25
- Primary result: Integrated Gradients with [PAD] baseline effectively reveals how domain adaptation shifts model focus to in-domain terminology

## Executive Summary
This paper proposes using Integrated Gradients (IG) to analyze and interpret domain-adapted dense retrievers. The authors introduce a novel baseline method that computes both query and document attributions by replacing tokens with [PAD] tokens. They apply this method to analyze two datasets: TREC-COVID (biomedical IR) and FIQA (financial question answering). The results show that domain-adapted models focus more on in-domain terminology compared to non-adapted models, with terms like "hedge," "gold," "corona," and "disease" receiving higher attributions.

## Method Summary
The method uses Integrated Gradients with a dual-mode [PAD] baseline to generate token-level attributions for dense retriever similarity scores. For query attributions, query tokens are replaced with [PAD] while document tokens remain unchanged; for document attributions, document tokens are replaced with [PAD] while query tokens remain unchanged. The approach analyzes both instance-level explanations (individual query-document pairs) and ranking-level explanations (aggregated over top-25 retrieved documents). Domain adaptation is performed using the GPL framework, which generates pseudo-query-document pairs from target corpora and distills cross-encoder scores into the bi-encoder model.

## Key Results
- NDCG@10 improved from 0.6510 to 0.7160 (+9.98%) for TREC-COVID domain adaptation
- NDCG@10 improved from 0.2670 to 0.3680 (+37.83%) for FIQA domain adaptation
- Domain-adapted models show higher positive attributions on in-domain terminology compared to non-adapted models
- Domain-adapted models place more emphasis on document titles in structured corpora

## Why This Works (Mechanism)

### Mechanism 1: Integrated Gradients Attribution via Padding Baseline
IG with a padding-token baseline generates meaningful token-level attributions by computing path integrals of gradients from a baseline input (all [PAD] tokens) to the actual input. The baseline conveys "empty signal" per Sundararajan et al. For query attributions, only query tokens are replaced with [PAD]; for document attributions, only document tokens are replaced. Core assumption: The gradient path from padding to actual tokens approximates each token's contribution to the dot-product similarity score.

### Mechanism 2: Domain Adaptation Increases Attribution on In-Domain Vocabulary
Unsupervised domain adaptation (GPL) shifts positive attribution toward domain-specific terms by generating pseudo-query-document pairs from the target corpus and distilling cross-encoder scores into the bi-encoder. This trains the model to weight domain terminology more heavily in the embedding space. Core assumption: Attribution shifts reflect learned domain relevance, not overfitting to pseudo-label noise.

### Mechanism 3: Title Attribution Shift via Document Structure Learning
Domain adaptation increases positive attribution to document titles in structured corpora (TREC-COVID) because GPL preprocessing concatenates titles at document beginnings; the model learns positional importance during adaptation. Core assumption: Title position is consistently encoded and the model learns to weight early tokens more heavily.

## Foundational Learning

- Concept: Integrated Gradients fundamentals
  - Why needed here: Understanding how gradient-based attribution works is essential for interpreting the paper's core method and its limitations
  - Quick check question: Why must the baseline produce a neutral (zero-information) embedding for IG to be meaningful?

- Concept: Bi-encoder dense retrieval architecture
  - Why needed here: The method attributes importance within independently embedded queries and documents; understanding the dot-product similarity is critical
  - Quick check question: How does independent query/document encoding affect what attributions can reveal about matching behavior?

- Concept: Unsupervised domain adaptation (GPL)
  - Why needed here: The paper analyzes how GPL changes model behavior; understanding pseudo-labeling clarifies what drives attribution shifts
  - Quick check question: What signals does GPL use to adapt a retriever without human labels?

## Architecture Onboarding

- Component map: Pre-trained GPL model -> Query encoder -> Document encoder -> Dot product similarity -> IG attribution (query baseline) -> IG attribution (document baseline) -> Aggregated ranking analysis

- Critical path: Load pre-trained GPL domain-adapted model -> Select query + top-K documents -> Run IG twice (query-baseline and document-baseline) -> Aggregate attributions for ranking-level analysis -> Visualize via token highlighting or word clouds

- Design tradeoffs:
  - Padding baseline vs. other baselines (e.g., zero embedding, random tokens)—padding is simple but may interact with positional embeddings
  - Instance-based vs. ranking-based explanations—ranking requires more computation but reveals aggregate behavior
  - Qualitative vs. quantitative evaluation—paper relies on qualitative analysis due to lack of ground-truth attribution metrics

- Failure signatures:
  - High attribution on stopwords or generic terms may indicate baseline issues
  - Negative attribution on query terms appearing in documents suggests model mismatch
  - Inconsistent attributions across similar queries may indicate instability

- First 3 experiments:
  1. Reproduce IG attributions on a single FIQA query-document pair; verify token attributions align with domain terms
  2. Compare title attribution scores for TREC-COVID between adapted and non-adapted models on 20 random relevant pairs
  3. Test alternative baselines (e.g., masked tokens, random embeddings) and compare attribution stability on a held-out query set

## Open Questions the Paper Calls Out

### Open Question 1
How do explanations generated via Integrated Gradients compare against other interpretability methods like LIME or SHAP in the context of dense retrieval? The authors state in the Limitations section: "We also plan to compare the IG method to other interpretability methods, such as [8] or [12], in future research." This remains unresolved because the current study validates IG in isolation without benchmarking against other established explanation frameworks.

### Open Question 2
Can a standardized quantitative metric be developed to evaluate the quality of feature attributions for dense retrievers? The authors note: "As there is no standard quantitative way of evaluating attributions produced by IG, we opted for a deep qualitative analysis of a small sample of queries." This is unresolved because the paper relies on qualitative visualization without statistical measures of explanation accuracy.

### Open Question 3
How do input attributions differ for lower-ranked documents compared to the top-ranked results? The authors state: "We aim to extend this our research beyond the top ranked documents, as understanding the attributions for lower-ranked documents is crucial for more comprehensible and reliable IR research." This remains unresolved because the current methodology aggregates attributions over only the top 25 documents.

### Open Question 4
What are the most influential document tokens globally across the entire corpus for a specific query? The authors mention: "Moreover, it is possible to analyze the most influential document tokens for a query similarity by computing document token attributions using all the documents in the corpus. We plan to conduct such very computationally demanding analysis in future research." This is unresolved because the current approach is restricted to instance-based and top-k ranking analysis.

## Limitations

- Attribution baseline validity is not validated against alternative baselines, and [PAD] tokens may interact with BERT's positional embeddings
- No quantitative metrics are provided to compare attribution patterns between adapted and non-adapted models
- Results are demonstrated on only two datasets, limiting claims about generalization across domains

## Confidence

**High Confidence**:
- IG is technically applicable to dense retrievers for generating token-level attributions
- Domain adaptation via GPL demonstrably improves NDCG@10 on both TREC-COVID (+9.98%) and FIQA (+37.83%)
- The GPL framework exists and has been validated in prior work for domain adaptation

**Medium Confidence**:
- Domain-adapted models show higher attributions on in-domain terminology compared to non-adapted models
- The observed title attribution shift in TREC-COVID is caused by GPL preprocessing and domain adaptation
- IG attributions meaningfully reflect model decision-making rather than implementation artifacts

**Low Confidence**:
- Attribution patterns generalize across different dense retriever architectures
- The specific baseline choice (replacing with [PAD]) does not introduce systematic bias
- Observed behavioral differences are solely due to domain adaptation rather than random variation

## Next Checks

1. **Baseline Sensitivity Analysis**: Implement and compare IG attributions using alternative baselines (zero embeddings, random tokens, masked tokens) on 20 randomly selected query-document pairs from FIQA. Measure correlation between attribution patterns across baselines and test whether in-domain terminology consistently receives higher attribution regardless of baseline choice.

2. **Quantitative Attribution Comparison**: Develop a statistical test framework to compare attribution distributions between adapted and non-adapted models. For each dataset, compute the average attribution magnitude for domain-specific terms versus general terms across 100 queries, then test for significant differences using permutation tests.

3. **Cross-Domain Attribution Transferability**: Apply the IG analysis framework to a third, distinct domain (e.g., legal or scientific literature) with available dense retriever models. Compare whether domain adaptation consistently shifts attributions toward domain-specific terminology across three domains rather than just two.