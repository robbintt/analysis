---
ver: rpa2
title: 'CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design
  Code Generation'
arxiv_id: '2505.14646'
source_url: https://arxiv.org/abs/2505.14646
tags:
- code
- cad-coder
- generation
- cadquery
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation

## Quick Facts
- arXiv ID: 2505.14646
- Source URL: https://arxiv.org/abs/2505.14646
- Reference count: 40
- Key outcome: 100% valid syntax rate on CAD code generation, outperforming state-of-the-art VLMs on both syntax validity and geometric similarity metrics

## Executive Summary
CAD-Coder is an open-source vision-language model that generates editable CadQuery Python code from CAD-rendered images. The model uses a two-stage training pipeline to achieve 100% valid syntax rate, significantly outperforming state-of-the-art VLMs like GPT-4.5 and Qwen2.5-VL-72B. While primarily trained on synthetic CAD renderings, the model demonstrates preliminary generalization to real-world photographs. The approach leverages existing pre-trained vision and language models, making it computationally accessible while maintaining high performance on the CAD code generation task.

## Method Summary
CAD-Coder uses a LLaVA-1.5 architecture with Vicuna-13B-v1.5 and CLIP-ViT-L-336px encoders. The two-stage training pipeline first aligns visual features to language embeddings using 595k CC3M image-caption pairs (MLP-only, frozen encoders), then fine-tunes end-to-end on the GenCAD-Code dataset (163k image-code pairs) with CadQuery Python scripts. The model generates code autoregressively, conditioning on image tokens. Evaluation uses Valid Syntax Rate (VSR) and IOUbest metrics, with a 4096-token limit on outputs. The approach demonstrates that code-based representations enable better generalization than domain-specific languages while maintaining syntactic validity.

## Key Results
- Achieves 100% valid syntax rate on CAD code generation, compared to 0% for general-purpose VLMs like LLaVA-v1.5-13B
- Outperforms state-of-the-art VLMs including GPT-4.5 and Qwen2.5-VL-72B on both syntax validity and geometric similarity metrics
- Demonstrates preliminary real-world generalization, successfully generating CAD code from photographs of physical objects
- Shows potential for generalization to unseen CAD operations (e.g., fillets) when using appropriate learning rate preservation strategies

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Vision-Language Alignment
Fine-tuning a pre-trained VLM on domain-specific CAD code produces syntactically valid outputs where identical architectures trained on generic VQA data produce none. Stage 1 trains a two-layer MLP to map CLIP vision features to LLM word embedding space while keeping both encoders frozen. Stage 2 unfreezes the LLM and trains end-to-end on image-code pairs, allowing the model to learn the autoregressive distribution of CAD syntax conditioned on visual features.

### Mechanism 2: Code-Based Representation Enables LLM Knowledge Transfer
Outputting CadQuery Python code rather than a domain-specific language (DSL) allows the model to leverage pre-trained coding knowledge for operations absent from the fine-tuning dataset. CadQuery is a Python library documented online and present in LLM pre-training corpora, enabling the model to synthesize unseen operations if the pre-trained LLM knew them and fine-tuning hyperparameters preserve this knowledge.

### Mechanism 3: Pre-Trained Vision Encoder Enables Real-Image Generalization
A vision encoder pre-trained on natural images enables CAD generation from real-world photographs despite fine-tuning only on rendered CAD images. CLIP-ViT-L-336px was trained on millions of real image-text pairs, learning representations of objects under varied lighting, perspective, and texture that transfer to parsing real photographs.

## Foundational Learning

- **Vision-Language Models (VLMs)**: Understanding LLaVA-1.5 architecture is prerequisite to modifying training pipelines. Can you explain why freezing the vision encoder during fine-tuning preserves general visual understanding while unfreezing the LLM enables domain-specific code generation?
- **Autoregressive Language Modeling**: CAD generation is framed as next-token prediction conditioned on image tokens. Given the autoregressive formulation, why might syntax errors cascade (a single bad token corrupting subsequent structure), and how does domain-specific fine-tuning mitigate this?
- **Catastrophic Forgetting in Fine-Tuning**: The paper shows that Qwen2.5-Coder-14B lost pre-trained CadQuery knowledge after standard fine-tuning. If you observe that a fine-tuned model can no longer perform operations it could before fine-tuning, what hyperparameter would you adjust first?

## Architecture Onboarding

- **Component map**: CLIP-ViT-L-336px -> Two-Layer MLP -> Vicuna-13B-v1.5
- **Critical path**: 1) Convert CAD data to CadQuery scripts, 2) Stage 1: Train MLP projection on CC3M pairs, 3) Stage 2: Fine-tune MLP + LLM on GenCAD-Code (filter >4096 tokens), 4) Evaluate with VSR and IOUbest
- **Design tradeoffs**: Vicuna vs. Qwen LLM (Vicuna: no pre-trained knowledge but 100% VSR vs. Qwen: pre-trained knowledge but requires lower LR for preservation), DSL vs. Code output (DSL limited vs. Code complete but risk syntax errors), Learning rate selection (higher LR for task performance vs. lower LR for knowledge preservation)
- **Failure signatures**: 0% VSR with non-zero output (LLM lacks target library knowledge), syntactically valid but geometrically wrong (vision features not aligning with CAD structure), loss of unseen operation capability (catastrophic forgetting), dimension/proportion errors on real images (perspective mismatch)
- **First 3 experiments**: 1) Reproduce VSR and IOUbest baselines to validate 100% vs. 0% VSR claim, 2) Test real-image generalization on 3-5 printed objects to measure IOUbest degradation, 3) Ablate learning rate for knowledge preservation by training Qwen2.5-14B variants at 2e-5 and 1e-5

## Open Questions the Paper Calls Out

- **Knowledge Preservation Optimization**: How can fine-tuning strategies be optimized to reliably preserve pre-trained knowledge of CAD operations (such as fillets) that are not present in the fine-tuning dataset? The paper demonstrates that standard end-to-end fine-tuning causes catastrophic forgetting, and simply lowering the learning rate creates a tradeoff where fine-tuning task performance drops.

- **Multi-View Training for Real Images**: To what extent does training on multi-view or perspective-invariant image inputs improve dimensional accuracy when generating CAD code from real-world photographs? The current model struggles with dimensional accuracy on real-world images due to training solely on isometric rendered views.

- **Code Efficiency and Dataset Post-Processing**: Does post-processing the GenCAD-Code dataset to replace verbose sequential commands with native concise CadQuery methods improve model performance or code efficiency? The current automated conversion results in non-concise code that may limit the model's ability to generate efficient code.

- **Chain-of-Thought Logic Integration**: Does incorporating chain-of-thought logic or reasoning capabilities into the fine-tuning dataset improve the geometric accuracy of complex generated solids? The paper suggests that standard autoregressive prediction might be insufficient for high accuracy, particularly for complex shapes.

## Limitations

- Narrow operation set in GenCAD-Code dataset (primarily sketch and extrude) limits evaluation of complex CAD workflows
- Real-image generalization experiments are preliminary, testing only five objects under controlled conditions
- Claims about code-based representations enabling better generalization than DSLs lack comparison to published DSL-based approaches
- Two-stage training pipeline requires substantial computational resources (4Ã— H100 GPUs), potentially limiting accessibility

## Confidence

- **High Confidence**: Claims about achieving 100% VSR and outperforming LLaVA-v1.5-13B on VSR metrics are clearly specified and internally consistent
- **Medium Confidence**: Claims about knowledge preservation requiring lower learning rates are empirical but compare different LLM backbones rather than ablation on the same model
- **Low Confidence**: Claims about real-image generalization capability are based on only five test objects photographed under controlled conditions

## Next Checks

1. **Extended Real-Image Generalization Test**: Expand evaluation to 50+ diverse objects including household items, mechanical parts, and architectural elements. Compare performance degradation between rendered and real inputs across different lighting conditions and viewing angles.

2. **Complex Operation Coverage Analysis**: Create benchmark set of CAD models requiring 10+ different operations (including fillets, chamfers, patterns, and assemblies). Evaluate CAD-Coder's ability to generate syntactically valid and functionally correct code for these complex designs.

3. **Cross-Library Generalization Study**: Fine-tune the model on CadQuery, then test its ability to generate code for equivalent operations in other CAD libraries (Blender API, OpenSCAD). Measure both syntactic validity and semantic equivalence to quantify knowledge transfer across CAD ecosystems.