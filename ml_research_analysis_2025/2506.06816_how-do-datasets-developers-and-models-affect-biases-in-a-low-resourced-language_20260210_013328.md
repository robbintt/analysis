---
ver: rpa2
title: How do datasets, developers, and models affect biases in a low-resourced language?
arxiv_id: '2506.06816'
source_url: https://arxiv.org/abs/2506.06816
tags:
- datasets
- language
- biases
- bengali
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates identity-based biases in Bengali sentiment
  analysis models, focusing on gender, religion, and nationality dimensions. The researchers
  audited 38 fine-tuned models using mBERT and BanglaBERT, trained on 19 Bengali sentiment
  analysis datasets.
---

# How do datasets, developers, and models affect biases in a low-resourced language?

## Quick Facts
- arXiv ID: 2506.06816
- Source URL: https://arxiv.org/abs/2506.06816
- Authors: Dipto Das; Shion Guha; Bryan Semaan
- Reference count: 40
- Key outcome: This study investigates identity-based biases in Bengali sentiment analysis models, focusing on gender, religion, and nationality dimensions. The researchers audited 38 fine-tuned models using mBERT and BanglaBERT, trained on 19 Bengali sentiment analysis datasets. Statistical analysis revealed significant biases: 61% of models favored male identities, 61% favored Muslim identities, and 50% favored Bangladeshi identities. The study found no correlation between dataset developers' demographic backgrounds and model biases. Importantly, BanglaBERT models exhibited less bias than mBERT, highlighting the value of language-specific models over multilingual ones. No dataset was completely free from bias, with models showing varying degrees of fairness across different identity dimensions. The research connects these findings to broader discussions on epistemic injustice and methodological decisions in algorithmic audits, emphasizing the need for more critical studies in low-resource language contexts.

## Executive Summary
This study investigates identity-based biases in Bengali sentiment analysis models through algorithmic auditing of 38 fine-tuned models across 19 datasets. The research examines how pre-trained models (mBERT vs. BanglaBERT) and training datasets interact to produce biases across gender, religion, and nationality dimensions. The study finds that 61% of models favor male identities, 61% favor Muslim identities, and 50% favor Bangladeshi identities. Crucially, language-specific models (BanglaBERT) demonstrate less bias than multilingual alternatives (mBERT), suggesting the importance of culturally grounded language models in low-resource settings. The research also reveals that dataset developers' demographic backgrounds do not predict model bias, pointing to systemic factors in data collection as primary drivers of algorithmic bias.

## Method Summary
The researchers conducted algorithmic audits on 38 Bengali sentiment analysis models by fine-tuning mBERT and BanglaBERT on 19 different Bengali sentiment analysis datasets. They used the Bengali Identity Bias Evaluation Dataset (BIBED) containing sentence pairs that differ only by identity markers (explicit names or implicit linguistic features like Bangladeshi vs. Indian Bengali dialects). The models were evaluated using Positive Classification Rate (PCR) and Pairwise Comparison Metric (PCM) to quantify bias. Statistical tests including chi-square and Wilcoxon signed-rank tests were applied to determine significance (p < 0.01, power ≥ 0.8). The fine-tuning process used Adam optimizer with learning rate 5e-5, batch size 16/32, for 3 epochs, with sentiment scores normalized to [0, 1].

## Key Results
- 61% of models showed bias favoring male identities over female identities
- 61% of models exhibited bias favoring Muslim identities over Hindu identities
- 50% of models displayed bias favoring Bangladeshi identities over Indian identities
- BanglaBERT models demonstrated significantly less bias than mBERT models across all identity dimensions
- No statistical correlation was found between dataset developers' demographic backgrounds and model bias

## Why This Works (Mechanism)

### Mechanism 1: Language-Specific Pre-training as Bias Regularization
- **Claim:** In low-resource settings, language-specific pre-trained models (e.g., BanglaBERT) tend to yield fine-tuned models with lower and less consistent identity-based bias compared to multilingual models (e.g., mBERT).
- **Mechanism:** Multilingual models often suffer from the "curse of multilingualism," where limited capacity per language forces the model to conflate linguistic markers (e.g., dialects, colloquialisms) with semantic sentiment. A specialized model, exposed to a broader distribution of the specific language during pre-training, appears better equipped to disentangle linguistic identity markers (like the Bangladeshi "zɔl" vs. Indian "pani" for water) from the sentiment label.
- **Core assumption:** The pre-training corpus for the language-specific model (BanglaBERT) was sufficiently diverse to represent dialectical differences neutrally, preventing the model from learning spurious correlations between dialect and sentiment.
- **Evidence anchors:**
  - [Key Outcome]: "BanglaBERT yielded less biased models than mBERT... highlighting the importance of language-specific models."
  - [Section 4.3]: "Fine-tuning BanglaBERT... resulted in fewer models with a consistent bias toward certain... identities."
  - [Corpus]: Corpus papers support the difficulty of figurative/grounded reasoning in low-resource languages (e.g., *BengaliFig*), implying generic models struggle with nuance.
- **Break condition:** If the language-specific pre-training data is heavily skewed toward a single demographic (e.g., only Standard Bangla from Bangladesh), the model would likely exhibit *higher* nationality bias than the multilingual alternative.

### Mechanism 2: Dataset-Driven Bias Overwrite
- **Claim:** Fine-tuning datasets act as a vector for injecting or amplifying specific biases, capable of overriding the priors of the base pre-trained model.
- **Mechanism:** During fine-tuning, the model adjusts its weights to minimize loss on the labeled dataset. If the dataset contains implicit correlations—such as associating specific religious names or dialects with positive/negative sentiments—the optimization process locks these patterns in. This mechanism explains why the same base model (e.g., mBERT) can exhibit opposing biases (e.g., toward females vs. males) depending solely on the fine-tuning dataset used.
- **Core assumption:** The label distribution in the fine-tuning dataset reflects implicit societal or curator biases rather than random noise, and the learning rate is sufficient to reshape the final classification layer.
- **Evidence anchors:**
  - [Section 4.3]: "D19-BanglaBERT... biased toward female identity... [while] D19-mBERT... biased toward male identity." (Shows dataset interaction with model).
  - [Section 4.3]: "BSA datasets... irrespective of the pre-trained base model, always lead to identity bias toward a specific... identity."
  - [Corpus]: *Evaluating Machine Translation Datasets* highlights risks in low-resource data collection prioritizing quantity over quality, supporting the mechanism of data-quality-driven bias.
- **Break condition:** If the fine-tuning dataset is too small or the learning rate is too low to shift the model's weights significantly, the pre-trained model's inherent bias would remain dominant.

### Mechanism 3: Epistemic Opacity of Developer Demographics
- **Claim:** The demographic identity (gender, religion, nationality) of dataset developers does not statistically predict the direction or magnitude of bias in the resulting fine-tuned models.
- **Mechanism:** Bias in these low-resource systems appears to emerge from systemic factors—such as the source of the text data (e.g., news vs. social media), labeling methodology, and available linguistic resources—rather than the personal identity of the developers. The "politics of design" are abstracted away by the data collection pipeline, making the developer's personal demographic irrelevant to the statistical output.
- **Core assumption:** The developers acted as neutral curators of existing data rather than authors of synthetic data, meaning their personal biases had limited opportunity to enter the system compared to the structural biases of the source text.
- **Evidence anchors:**
  - [Section 4.2]: "Statistical tests showed no significant relationship between model bias and dataset developer demographics."
  - [Section 4.2]: "p-values... were 0.77, 0.27, and 1.0" (indicating no statistical significance).
  - [Corpus]: Corpus evidence is weak or missing regarding the direct link between developer demographics and low-resource model bias; related work focuses on output bias rather than creator identity.
- **Break condition:** If the developers explicitly curated or annotated the data subjectively (e.g., "write a negative sentence about X") rather than scraping and labeling existing objective text, their personal demographics might correlate more strongly with the output.

## Foundational Learning

- **Concept: Algorithmic Auditing (Sociotechnical Black-box Testing)**
  - **Why needed here:** This paper uses "algorithmic auditing" (probing a system with paired inputs) to detect bias without accessing internal weights. Understanding this method is crucial to interpreting why "identical semantic content" (e.g., changing only a name) is used to isolate variables.
  - **Quick check question:** If you swap a gendered noun in a sentence and the sentiment score changes significantly, does that prove the *model* is biased, or does it prove the *training data* was biased? (Answer: It proves the model exhibits bias; the data is the likely cause, but the audit measures the model's behavior).

- **Concept: Epistemic Injustice in NLP**
  - **Why needed here:** The paper frames bias not just as an error, but as "epistemic injustice"—specifically "testimonial injustice" where a system gives less credibility to text based on identity. This frames the technical problem as a sociotechnical harm.
  - **Quick check question:** When a sentiment analyzer consistently rates a dialect associated with a marginalized group as "negative," is this a technical error or an epistemic harm? (Answer: It is both; technically it's a distribution shift, socially it is testimonial injustice).

- **Concept: Positive Classification Rate (PCR) & Pairwise Comparison Metric (PCM)**
  - **Why needed here:** The authors critique standard accuracy metrics and instead use PCR (rate of positive predictions per group) and PCM (difference in scores for paired sentences) to quantify bias.
  - **Quick check question:** Why is comparing "Average Sentiment Score" (PCM) often more sensitive than comparing "Positive/Negative" labels (PCR) for detecting subtle bias? (Answer: Numerical scores reveal small systematic preferences that might not be strong enough to flip a binary label).

## Architecture Onboarding

- **Component map:**
  - Bengali Identity Bias Evaluation Dataset (BIBED) → *Contains paired sentences (explicit/implicit identity markers)* → Base Models (mBERT / BanglaBERT) → **Fine-tuned on** → 19 BSA Datasets (D1-D19) → Statistical Tests (Chi-square, Wilcoxon signed-rank) + Bias Metrics (PCR, PCM) → Quantified group bias (Male vs. Female, Hindu vs. Muslim, etc.)

- **Critical path:** The audit relies on **BIBED** (the evaluation set). If BIBED does not accurately capture the linguistic nuances of the specific low-resource dialects (Bangal vs. Ghoti), the audit results may be invalid. Ensuring the evaluation set is culturally grounded is the highest-risk dependency.

- **Design tradeoffs:**
  - **mBERT vs. BanglaBERT:** Using mBERT ensures faster deployment (off-the-shelf) but introduces higher "constant bias" risk. BanglaBERT requires specific pre-training/downloading but yields fairer results.
  - **Metric Sensitivity:** Using only nominal categories (Positive/Negative) hides subtle bias. Using numerical scores exposes bias but requires handling floating-point noise and calibration.

- **Failure signatures:**
  - **Constant Bias:** In the PCR heatmap, a model showing "10, 0" (biased 10 out of 10 splits) indicates the dataset has likely "locked in" a strong correlation between identity and sentiment.
  - **Metric Divergence:** If PCR suggests bias but PCM is near zero, the model may be classifying identically but assigning different *confidence* scores, indicating a subtler form of bias.

- **First 3 experiments:**
  1.  **Reproduce the Base Comparison:** Fine-tune mBERT and BanglaBERT on a single dataset (e.g., D19) and run BIBED sentences to confirm that BanglaBERT exhibits lower variance in sentiment scores for dialect-neutral sentences.
  2.  **Probe the "Developer" Null Result:** Select a dataset with known demographic skew (e.g., all male developers) and attempt to correlate specific vocabulary in the dataset with the bias direction to verify if the "structural data" argument holds vs. the "developer" argument.
  3.  **Metric Sensitivity Test:** Take a model identified as "Constantly Biased" via PCR and attempt to debias it by simply adjusting the classification threshold; measure if PCM decreases, or if the bias is structurally embedded in the embeddings.

## Open Questions the Paper Calls Out

None

## Limitations
- The 19 fine-tuning datasets (D1-D19) are anonymized, preventing direct replication of specific bias patterns and source tracing
- Evaluation focuses only on three identity dimensions (gender, religion, nationality), potentially missing other bias forms
- Binary classification of sentiment may oversimplify complex linguistic expressions in Bengali
- The study represents a limited exploration of the vast space of possible dataset-model combinations with only 38 models

## Confidence
- **High Confidence:** BanglaBERT exhibits less bias than mBERT in Bengali sentiment analysis (supported by multiple statistical tests and consistent across datasets)
- **Medium Confidence:** Dataset developers' demographic backgrounds do not predict model bias (based on statistical analysis, but limited by the inability to access full developer information)
- **Low Confidence:** No dataset is completely free from bias (inferred from patterns across datasets, but individual dataset bias levels vary significantly)

## Next Checks
1. **Validation Set Composition Test:** Audit the Bengali Identity Bias Evaluation Dataset (BIBED) for potential cultural or regional bias in sentence construction to ensure the evaluation set itself doesn't introduce systematic errors in bias measurement.

2. **Fine-tuning Stability Test:** Run multiple fine-tuning runs with different random seeds on the same dataset-model combinations to establish whether observed bias patterns are consistent or subject to high variance.

3. **Corpus Quality Analysis:** Analyze the pre-training corpora of both mBERT and BanglaBERT for demographic representation, dialect coverage, and potential bias sources that could explain the differential bias performance between models.