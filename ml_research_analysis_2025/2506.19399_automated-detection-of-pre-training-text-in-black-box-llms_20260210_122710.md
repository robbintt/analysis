---
ver: rpa2
title: Automated Detection of Pre-training Text in Black-box LLMs
arxiv_id: '2506.19399'
source_url: https://arxiv.org/abs/2506.19399
tags:
- llms
- pre-training
- membership
- text
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting whether a text
  is part of the pre-training data of large language models (LLMs) in a black-box
  setting, where only input and output texts are accessible. Existing methods require
  substantial human effort or access to hidden model information, making them impractical
  for large-scale or automated use.
---

# Automated Detection of Pre-training Text in Black-box LLMs

## Quick Facts
- **arXiv ID**: 2506.19399
- **Source URL**: https://arxiv.org/abs/2506.19399
- **Reference count**: 24
- **Primary result**: Proposed VeilProbe framework achieves up to 25.1% AUC improvement and nearly doubles TPR@5%FPR scores in detecting pre-training text in black-box LLMs

## Executive Summary
This paper addresses the challenge of detecting whether text was part of a large language model's pre-training data in black-box settings where only input and output texts are accessible. Existing methods require substantial human effort or access to hidden model information, making them impractical for large-scale use. The proposed VeilProbe framework introduces an automated approach using a sequence-to-sequence mapping model to infer latent membership features through text-to-suffix mapping, combined with key token perturbation and a prototype-based classifier with Information Bottleneck denoising. Extensive experiments on three widely used datasets show significant improvements over existing methods, achieving state-of-the-art performance without human intervention.

## Method Summary
VeilProbe operates in a black-box setting by collecting input-suffix pairs from the target LLM, then training a sequence-to-sequence mapping model to simulate the LLM's text-to-suffix behavior. The framework infers latent membership features from the mapping model's hidden states, applies key token perturbation calibration to enhance distinguishability, and uses a prototype-based classifier with Information Bottleneck denoising to classify texts as pre-training or non-pre-training. The method requires only input and output texts, making it practical for large-scale deployment while maintaining high detection accuracy even with limited ground-truth samples.

## Key Results
- Achieves up to 25.1% improvement in AUC over existing black-box detection methods
- Nearly doubles TPR@5%FPR scores compared to state-of-the-art baselines
- Maintains strong performance with as few as 10 ground-truth samples per class
- Demonstrates effectiveness across multiple LLM architectures including ChatGPT, Pythia, and LLaMA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A sequence-to-sequence model trained on input-suffix pairs can infer latent membership features that distinguish pre-training from non-training texts.
- Mechanism: The seq2seq model learns to simulate the LLM's text-to-suffix mapping pattern. Its hidden states capture differences in how the LLM processes seen vs. unseen texts—training data tends to produce more consistent, structured internal representations.
- Core assumption: The hidden states of a model trained to mimic the LLM's completion behavior encode membership-relevant signals similar to those in the LLM's own internal layers.
- Evidence anchors: [abstract] describes using a sequence-to-sequence mapping model to infer latent mapping features; [Section 4.2] explains the fundamental concept based on previous research showing hidden states can distinguish training from non-training data.

### Mechanism 2
- Claim: Perturbing key tokens (not random tokens) produces calibration features that improve detection, especially at low false positive rates.
- Mechanism: Pre-training texts exhibit different robustness to perturbation than non-training texts. By selectively perturbing tokens that proxy LLMs identify as important, the difference in mapping features (Zs - Zr) captures membership-relevant sensitivity.
- Core assumption: Proxy LLMs from the same family identify similar key tokens as the target LLM would; training data shows measurably different perturbation sensitivity.
- Evidence anchors: [Section 4.1] defines key tokens as those exerting substantial influence on suffix generation; [Table 5] shows ablation results demonstrating the importance of perturbation calibration.

### Mechanism 3
- Claim: A prototype-based classifier with Information Bottleneck denoising enables effective detection with limited ground-truth samples.
- Mechanism: Prototypical Networks compute class centroids (prototypes) in embedding space. IB compression removes redundant features irrelevant to membership, keeping only predictive dimensions. Classification uses distance to prototypes rather than training a full classifier, reducing overfitting risk.
- Core assumption: Membership features form separable clusters in the compressed embedding space with simple distance geometry.
- Evidence anchors: [Section 4.3] describes the prototype-based classifier construction; [Table 5] shows IB denoising contributes to performance improvement.

## Foundational Learning

- **Membership Inference Attacks (MIA)**
  - Why needed here: The task is an instance of MIA—determining if a sample was in training data. Understanding prior MIA approaches contextualizes why black-box settings are harder.
  - Quick check question: Explain why white-box MIA methods (e.g., Min-K% Prob) fail when only input/output text is accessible.

- **Sequence-to-Sequence Models (Transformer-based)**
  - Why needed here: The core inference module uses a seq2seq transformer to model text-to-suffix mappings. Understanding encoder-decoder attention and hidden state extraction is prerequisite.
  - Quick check question: Where does VeilProbe extract features from—the decoder output, encoder hidden states, or both? (Answer: hidden states from all layers of the mapping model.)

- **Prototypical Networks / Few-Shot Classification**
  - Why needed here: With only ~50 ground-truth samples, standard classifiers overfit. Prototypical Networks enable metric-based classification with limited data.
  - Quick check question: How is the pre-training prototype c₁ computed from support set U₁? (Answer: Mean of embedded support samples: c₁ = (1/|U₁|) Σ Fφ(Zᵢ).)

## Architecture Onboarding

- **Component map**: Text-to-Suffix Sampling → Key Token Selection → Seq2Seq Mapping Model → Perturbation Calibration → Feature Fusion → Prototype Classifier → Detection Decision

- **Critical path**: Input text → suffix generation (3×) → mapping model inference → feature extraction + fusion → prototype distance scoring → detection decision. The mapping model training and key token selection are offline setup steps.

- **Design tradeoffs**:
  - Proxy LLM selection: Paper uses GPT-2 family for ChatGPT targets. Using mismatched families may reduce key token overlap.
  - Suffix count: 3 suffixes per text balances cost vs. noise reduction. More suffixes may help but increases API cost.
  - p-value threshold: Domain-dependent (0.05–0.1 typical). Too strict discards real signal; too loose adds noise.
  - Feature extraction volume: "Full" (all layers) works best; "LLT" (last layer, last token) degrades slightly but reduces dimensionality.

- **Failure signatures**:
  - Low AUC (~0.5) with high variance: Likely insufficient ground-truth samples (<10) or mismatched proxy LLMs.
  - High AUC but TPR@5%FPR near 0: Calibration features not selected (check p-value threshold) or prototype separation poor.
  - Performance collapse on short texts (<32 tokens): Paper shows mixed results; very short texts lack distinguishable features.
  - API rate limiting or timeout during sampling stage: Not handled in paper; production needs retry logic.

- **First 3 experiments**:
  1. **Sanity check on WikiMIA with Pythia-6.9B**: Replicate Table 8 results at length 64. Verify AUC > 0.90. If lower, check mapping model convergence and suffix generation quality.
  2. **Ablation of perturbation calibration**: Run "w/o calibration feature" variant. Expect ~5–7% TPR@5%FPR drop per Table 5. Confirms calibration mechanism is working.
  3. **Ground-truth sample sensitivity**: Test with 10, 25, 50, 100 samples. Expect monotonic improvement per Figure 5. Validates prototype classifier robustness to data scarcity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of VeilProbe degrade when the proxy models used for key token selection differ significantly in architecture or training corpus from the target black-box LLM?
- Basis in paper: Section 4.1 states that proxy models are used to identify key tokens, noting that models from the "same family" (e.g., GPT-2 for ChatGPT) or those with "overlap in training corpora" work best.
- Why unresolved: The paper assumes access to suitable proxies but does not analyze failure cases where the target model's attention mechanisms or token importance rankings diverge sharply from available open-source proxies.
- What evidence would resolve it: A comparative analysis of detection AUC scores when using mismatched proxy families versus matched families.

### Open Question 2
- Question: To what extent do variations in the target LLM's decoding strategies (e.g., temperature, top-p sampling) impact the robustness of the inferred Text-to-Suffix mapping features?
- Basis in paper: Section 4.1 describes generating suffixes autoregressively, but the experiments primarily utilize standard generation settings. The stability of the mapping features $Z_s$ against stochastic decoding variations is not quantified.
- Why unresolved: Black-box APIs may update or vary decoding parameters dynamically; if the mapping features are highly sensitive to output variance, the classifier's reliability in real-world scenarios could be compromised.
- What evidence would resolve it: A sensitivity analysis measuring the variance in detection scores (AUC/TPR) when querying the target LLM across a range of temperature settings (e.g., 0.0 to 1.0).

### Open Question 3
- Question: Can a single sequence-to-sequence mapping model be effectively generalized to detect pre-training data across multiple different target LLMs, or is retraining required for each specific model?
- Basis in paper: Section 4.2 describes training the mapping model $f: s \rightarrow s_{out}$ to simulate the target LLM. The implementation details imply training specific instances for target models like Pythia or LLaMA.
- Why unresolved: It is unclear if the learned mapping features capture a universal signature of "memorization" or if they are merely approximating the specific stylistic quirks of the individual target LLM.
- What evidence would resolve it: Cross-model testing where a mapping model trained on LLM A's outputs is used to extract features for detecting membership in LLM B.

## Limitations
- Limited domain generalization to highly technical or specialized domains where proxy LLMs may use different vocabularies and attention patterns
- Ground-truth sample sensitivity that varies by dataset, potentially affecting real-world deployment where labeled data is scarce
- API dependency and cost concerns with generating multiple suffixes per text for large-scale deployment

## Confidence
- **High Confidence**: The core mechanism of using seq2seq mapping to infer latent membership features is well-supported by experiments showing consistent AUC improvements (0.919-0.937) across multiple datasets and LLM architectures.
- **Medium Confidence**: The prototype-based classifier with Information Bottleneck denoising shows effectiveness, but the improvement from IB compression alone is modest (0.929 vs 0.937 AUC).
- **Medium Confidence**: The perturbation calibration mechanism shows measurable improvements in TPR@5%FPR scores, but its effectiveness depends heavily on appropriate p-value thresholding, which varies by domain.

## Next Checks
1. **Cross-Domain Robustness Test**: Evaluate VeilProbe on highly specialized domains (legal documents, medical literature, scientific papers) to verify whether the perturbation calibration mechanism maintains effectiveness when target LLMs use domain-specific vocabularies and attention patterns.

2. **Ground-truth Sample Scaling Study**: Conduct systematic experiments varying ground-truth samples from 5 to 100 across all three datasets, measuring not just AUC but also detection latency and API cost per sample, to establish optimal sample size tradeoffs for real-world deployment.

3. **Proxy LLM Mismatch Analysis**: Test VeilProbe with intentionally mismatched proxy LLM families (e.g., using LLaMA family to detect ChatGPT outputs) to quantify performance degradation and establish guidelines for proxy selection when the target LLM family is unknown.