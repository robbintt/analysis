---
ver: rpa2
title: Bayesian neural networks with interpretable priors from Mercer kernels
arxiv_id: '2510.23745'
source_url: https://arxiv.org/abs/2510.23745
tags:
- prior
- which
- mercer
- covariance
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Mercer priors, a new class of priors for Bayesian
  neural networks (BNNs) designed to enforce interpretable constraints on the output
  space. The key idea is to construct the prior directly from the Mercer representation
  of a target Gaussian process (GP) kernel, ensuring that samples from the BNN approximate
  draws from the desired GP.
---

# Bayesian neural networks with interpretable priors from Mercer kernels

## Quick Facts
- arXiv ID: 2510.23745
- Source URL: https://arxiv.org/abs/2510.23745
- Reference count: 40
- One-line primary result: Introduces Mercer priors for BNNs that enforce interpretable constraints via spectral decomposition of GP kernels, enabling scalable sampling and improved uncertainty quantification.

## Executive Summary
This paper presents Mercer priors, a novel approach for constructing interpretable priors for Bayesian neural networks by leveraging the Mercer spectral decomposition of target Gaussian process kernels. The key innovation is building the prior directly from the eigenvalues and eigenfunctions of the kernel, ensuring that BNN samples approximate draws from the desired GP. This bridges the gap between the interpretability of GPs and the scalability of BNNs.

The method uses an unbiased Monte Carlo estimator for the log-prior, enabling efficient sampling via stochastic gradient Langevin dynamics without explicit matrix inversions. Numerical experiments demonstrate convergence to the target GP as network width and spectral truncation increase, with applications showing improved uncertainty quantification in hierarchical regression, periodic modeling, and nonlinear PDE inverse problems.

## Method Summary
The method constructs a prior for Bayesian neural networks by parameterizing the target Gaussian process using its Mercer spectral decomposition. The prior is defined as p(θ) ∝ exp(-1/2 ⟨uθ, S^{-1}uθ⟩), where the inner product is computed using the eigenvalues and eigenfunctions of the covariance operator. An unbiased Monte Carlo estimator of this log-prior enables scalable sampling via SGLD, avoiding explicit matrix inversions. The BNN architecture (typically a single-layer network with Fourier features) is designed to approximate the target GP as width and spectral truncation increase.

## Key Results
- BNN samples with Mercer priors converge to the target GP as network width and spectral truncation increase, validated through empirical covariance matrices and Kolmogorov-Smirnov tests.
- The unbiased estimator enables scalable sampling via SGLD without explicit matrix inversions, making the method computationally efficient.
- Applications to hierarchical regression, periodic modeling, and PDE inverse problems demonstrate improved uncertainty quantification compared to standard BNN priors.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Placing a Mercer-based prior over BNN parameters makes the BNN's output approximate draws from a target GP.
- Mechanism: A Gaussian process GP(0, k) with Mercer kernel k induces a Gaussian measure on L2. By parameterizing uθ as a neural network and forming the prior p(θ) ∝ exp(-1/2 ⟨uθ, S^{-1}uθ⟩), the inverse covariance S^{-1} is expressed via the Mercer spectral expansion, aligning parameter density with function-space GP structure.
- Core assumption: The kernel admits a Mercer decomposition with known eigenpairs; network is regular enough for S^{-1} to apply.
- Evidence anchors:
  - [abstract] "construct the prior directly from the Mercer representation of a target Gaussian process (GP) kernel"
  - [section 3] eq. (3.2)–(3.6) and Proposition 3.1 define the prior via the Mercer expansion and unbiased estimator.
  - [corpus] Direct parallels are limited; related work focuses on functional priors and scalability but not Mercer constructions.
- Break condition: If eigenfunctions/eigenvalues are unavailable or poorly approximated, the unbiased estimator degrades and the prior may not reflect the intended GP.

### Mechanism 2
- Claim: An unbiased Monte Carlo estimator of log p(θ) enables scalable sampling via SGLD without explicit integrals.
- Mechanism: Replace the L2 inner products and the infinite sum over eigenpairs by minibatched Monte Carlo averages and subsampled eigen-indices (Proposition 3.1). SGLD then uses only gradients of this unbiased estimator, avoiding matrix inverses.
- Core assumption: Importance sampling distribution p(n) covers relevant eigenmodes; minibatches approximate inner products sufficiently.
- Evidence anchors:
  - [abstract] "unbiased estimator for the prior, enabling efficient sampling via stochastic gradient Langevin dynamics"
  - [section 3.0–3.1] derivation of SN,M1,M2 and discussion of truncated expansion; Remark 3.2 on separating inner products.
  - [corpus] Weak direct evidence; neighbor papers emphasize UQ and priors but not this specific unbiased estimator.
- Break condition: If batch sizes are too small or p(n) is mismatched, variance of the estimator can prevent SGLD from converging efficiently.

### Mechanism 3
- Claim: Increasing spectral truncation K and network width Ns improves fidelity to the target GP.
- Mechanism: More eigenpairs better approximate the kernel (Mercer convergence). Wider networks increase expressive capacity to represent GP-like functions. Empirical covariance and KS tests show convergence as K and Ns grow.
- Core assumption: Activation and architecture are compatible with the target regularity (e.g., differentiable for Brownian-bridge-style priors).
- Evidence anchors:
  - [abstract] "convergence to the target GP as network width and spectral truncation increase"
  - [sections 4.1–4.2] empirical covariance heatmaps and KS statistics improving with K and N1; visual sample comparisons.
  - [corpus] Weak; neighbor works discuss functional priors but not systematic width/K studies.
- Break condition: When the target GP requires non-differentiability (e.g., exact Brownian motion), a smooth activation limits approximation fidelity regardless of K and width.

## Foundational Learning

- Concept: Mercer kernel and spectral decomposition of covariance operators
  - Why needed here: The method constructs the prior from eigenvalues/eigenfunctions; you must compute or approximate them.
  - Quick check question: For a given kernel, can you write its Mercer series and identify where truncation captures, say, 95% variance?

- Concept: Stochastic gradient Langevin dynamics with unbiased gradient estimators
  - Why needed here: Sampling relies on SGLD using only unbiased estimates of log-prior gradients.
  - Quick check question: Why does SGLD still converge when only a Monte Carlo estimate of ∇log p(θ) is available?

- Concept: Gaussian processes as Gaussian measures on function spaces
  - Why needed here: The prior formulation is rooted in measure-theoretic GP correspondences (Theorem 3.1) and the RKHS viewpoint.
  - Quick check question: Explain how a GP with kernel k induces a Gaussian measure on L2(Ω) and why S must be trace-class.

## Architecture Onboarding

- Component map: Inputs (domain points x, eigen-indices n) -> BNN backbone (uθ(x)) -> Prior log-density (SN,M1,M2) -> Sampler (SGLD) -> Likelihood (task-specific)

- Critical path:
  1. Choose/derive eigenpairs {λn, φn} for the target kernel (analytically if possible; otherwise approximate).
  2. Choose truncation K and proposal distribution p(n); pick minibatch sizes M1, M2, N.
  3. Implement unbiased estimator SN,M1,M2; verify variance is manageable on small problem.
  4. Integrate estimator into SGLD sampler; run prior-only sampling and compare empirical covariance to target GP.
  5. Attach likelihood and run posterior sampling; evaluate predictive performance and UQ.

- Design tradeoffs:
  - Larger K improves fidelity but increases compute and may need lower learning rates.
  - Wider networks increase capacity but cost scales O(N M D); diminishing returns appear once network is sufficiently expressive (Sec. 4.2).
  - Smaller M/N increase variance of the gradient estimator; may require more SGLD steps or variance reduction.

- Failure signatures:
  - Empirical covariance deviates strongly from target, especially along diagonal (high-frequency modes missing).
  - KS test fails for moderate t; posterior samples exhibit periodic artifacts if eigenfunctions are mismatched.
  - Numerical instability from small λn values when 1/λn explodes (often when p(n) is not truncated properly).

- First 3 experiments:
  1. Brownian motion sanity check: use K=100, N1=500 neurons, M=1000 points; compare empirical covariance and KS test to true BM (as in Sec. 4).
  2. Kernel truncation sweep: fix architecture, vary K in {20,100,500,1000}; plot covariance error and KS statistic vs K (replicating Fig. 6 and Fig. 7).
  3. Small real-world regression (e.g., motorcycle dataset): implement hierarchical BNN with ∆^{-2} Mercer prior; assess predictive intervals and heteroscedastic capture vs a plain i.i.d. Gaussian BNN prior.

## Open Questions the Paper Calls Out

- Question: What are the rigorous convergence guarantees connecting the Mercer prior to the target Gaussian measure as network width and spectral truncation increase?
  - Basis in paper: [explicit] The conclusion states that while numerical evidence suggests convergence, "rigorous convergence results connecting the Mercer prior to the corresponding Gaussian measure remains an important theoretical challenge."
  - Why unresolved: The paper provides empirical validation via KS tests and covariance errors but lacks formal mathematical proofs linking the finite-dimensional BNN parameter distribution to the infinite-dimensional Gaussian measure.
  - What evidence would resolve it: Theorems establishing convergence rates or bounds relative to the network width N1 and the number of eigenvalues K.

- Question: How can kernel hyperparameters (e.g., length scale, variance) be jointly inferred with network parameters without introducing bias from the intractable partition function?
  - Basis in paper: [explicit] The conclusion identifies the "treatment of any hyperparameters appearing in the GP covariance" as a remaining challenge, noting the current method fixes them to avoid partition function issues.
  - Why unresolved: The partition function Z(λ) depends on the hyperparameters in a complex, intractable way; ignoring it (as done in the paper) introduces bias during inference.
  - What evidence would resolve it: A scalable inference scheme (e.g., nested sampling or unbiased gradient estimators) that successfully marginalizes or optimizes hyperparameters.

- Question: Can the Mercer prior be effectively implemented for the squared exponential kernel despite the numerical instability of its eigenfunctions?
  - Basis in paper: [explicit] Section 3.2 notes that for the squared exponential kernel, "computations became very unstable" due to large normalization constants in the eigenfunctions (Hermite polynomials), limiting the approximation.
  - Why unresolved: The numerical instability prevents the inclusion of enough eigenfunctions to build a robust approximation for this common kernel.
  - What evidence would resolve it: Numerical techniques or alternative representations that stabilize the computation of squared exponential eigenfunctions in high orders.

## Limitations
- The method's performance is tightly coupled to the availability and quality of Mercer eigenpairs for the target kernel, limiting applicability to kernels without closed-form eigenpairs.
- Numerical stability can be a concern when dealing with very small eigenvalues, requiring careful tuning of truncation K and importance sampling distribution p(n).
- The variance of the unbiased estimator for the log prior can be significant, particularly with small batch sizes, potentially slowing SGLD convergence.

## Confidence
- **High confidence**: The theoretical construction of Mercer priors from spectral decomposition is sound, and the unbiased estimator is mathematically correct.
- **Medium confidence**: Empirical convergence results (covariance and KS tests) are convincing for the Brownian motion example, but generalization to other kernels and real-world problems requires further study.
- **Low confidence**: Claims about improved uncertainty quantification and scalability in complex applications (e.g., PDE inverse problems) are only partially supported by the provided evidence.

## Next Checks
1. **Generalization to other kernels**: Apply the Mercer prior construction to a kernel with analytically known but less structured eigenfunctions (e.g., Matérn or periodic kernels). Assess convergence of empirical covariance and predictive performance.
2. **Numerical stability analysis**: Systematically study the effect of small eigenvalues on SGLD stability and sampling quality, varying K and batch sizes. Quantify variance of the log-prior estimator.
3. **Benchmark against functional priors**: Compare predictive performance, uncertainty calibration, and computational efficiency of Mercer priors with other recent functional priors (e.g., Hi-fi functional priors, R2D2) on standard regression and PDE inverse problem benchmarks.