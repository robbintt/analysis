---
ver: rpa2
title: Do language models accommodate their users? A study of linguistic convergence
arxiv_id: '2508.03276'
source_url: https://arxiv.org/abs/2508.03276
tags:
- convergence
- human
- language
- liwc
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether large language models (LLMs) exhibit\
  \ linguistic convergence, adapting their language to mirror users\u2019 linguistic\
  \ patterns. Researchers systematically compared model-generated dialogue responses\
  \ against human baselines and random controls across 16 models, three dialogue datasets,\
  \ and multiple stylometric features."
---

# Do language models accommodate their users? A study of linguistic convergence

## Quick Facts
- **arXiv ID:** 2508.03276
- **Source URL:** https://arxiv.org/abs/2508.03276
- **Reference count:** 18
- **Primary result:** Models strongly converge to conversation style, often overfitting compared to human baselines, with convergence mechanisms driven by training objectives rather than communicative goals.

## Executive Summary
This study investigates whether large language models exhibit linguistic convergence - adapting their language to mirror users' linguistic patterns. Researchers systematically compared model-generated dialogue responses against human baselines and random controls across 16 models, three dialogue datasets, and multiple stylometric features. Models strongly converged to conversation style, often significantly overfitting compared to human baselines. Convergence patterns varied by feature: instruction-tuned and larger models converged less than pretrained counterparts, and proper noun overlap showed recency bias. Overall, models adapted to context but their convergence mechanisms differ from human accommodation, driven more by training objectives than communicative goals.

## Method Summary
The study employed a dialogue completion pipeline using three corpora (DailyDialog, Movie Corpus, NPR interviews) filtered for conversations with at least 6 turns and 2 speakers. Models completed turn 6 in dialogues given context turns 1-5, with generated responses compared to human baselines and random controls using Language Style Matching (LSM) for utterance length and LIWC categories, proper noun overlap percentage, and token novelty metrics. Sixteen models were tested: Llama 3 (1B-70B) and Gemma (1B-27B) in both pretrained and instruction-tuned variants, with generation controlled by temperature 0.4, top-p 0.8, and max_tokens 40.

## Key Results
- Models strongly converged to conversation style, often significantly overfitting compared to human baselines
- Instruction-tuned and larger models converged less than pretrained counterparts
- Proper noun overlap showed recency bias toward newer concepts
- Convergence mechanisms differ from human accommodation, driven more by training objectives than communicative goals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pretrained models converge more strongly because their training objective rewards fitting the input distribution closely.
- **Mechanism:** Next-token prediction on complete documents incentivizes stylistic consistency with context. When generating dialogue continuations, the model treats prior turns as conditioning that shapes probable continuations.
- **Core assumption:** The convergence observed reflects distributional properties of pretraining data rather than communicative intent.
- **Evidence anchors:**
  - [abstract] "models adapted to context but their convergence mechanisms differ from human accommodation, driven more by training objectives than communicative goals"
  - [Section 6] "we hypothesize that model convergence is instead driven by their pretraining objective, which encourages the model to produce text stylistically consistent with their input by training them on complete, often single-author documents"
  - [corpus] Related work on structural priming in LMs (Sinclair et al., 2022; Jumelet et al., 2024) supports context-influenced generation as a general phenomenon, but mechanisms remain inferred.
- **Break condition:** If convergence patterns persist even when pretraining data is deliberately constructed to break document-level style consistency, this mechanism would be weakened.

### Mechanism 2
- **Claim:** Instruction-tuned models converge less than pretrained models because fine-tuning objectives encourage novel, task-relevant responses over stylistic mirroring.
- **Mechanism:** RLHF or supervised instruction-tuning shapes responses toward helpfulness and informativeness, which may conflict with simply matching the interlocutor's style.
- **Core assumption:** The divergence between pretrained and instruction-tuned behavior reflects training objective differences, not just scale.
- **Evidence anchors:**
  - [abstract] "instruction-tuned and larger models converged less than their pretrained counterparts"
  - [Section 5.2] "pretrained models likely appear to adapt more on Token Novelty because they are trained to fit closely to the input distribution, while instruction-tuned models are encouraged to introduce novel information during fine-tuning"
  - [corpus] No direct corpus evidence on instruction-tuning effects on convergence specifically; this remains an inferred mechanism.
- **Break condition:** If instruction-tuned models from different training pipelines showed inconsistent convergence suppression, alternative explanations (e.g., data contamination) would gain weight.

### Mechanism 3
- **Claim:** Proper noun convergence exhibits recency bias because models weight recent context tokens more heavily in attention.
- **Mechanism:** Attention mechanisms in transformers naturally emphasize nearby tokens; proper nouns are often repeated within short dialogue spans.
- **Core assumption:** Recency effects in proper noun overlap reflect attention weight distribution, not strategic coreference.
- **Evidence anchors:**
  - [Section 5.4] "in the case of exact word overlap (e.g., names and other proper nouns), model convergence demonstrates a strong recency bias towards newer concepts"
  - [Section 5.2] Pretrained models mirror human baseline on PROPN overlap, while instruction-tuned models over-accommodate by repeating proper nouns more often
  - [corpus] Liu et al. (2024) "Lost in the middle" documents recency/primacy biases in context use, supporting plausibility but not directly tested here.
- **Break condition:** If explicit recency controls (e.g., repositioning proper nouns to earlier context) eliminated the effect, attention-based recency would be implicated.

## Foundational Learning

- **Concept: Linguistic Style Matching (LSM)**
  - **Why needed here:** The paper uses LSM metrics from Ireland et al. (2011) to quantify convergence. Understanding that LSM = 1 − |a − b|/(a + b) is essential for interpreting results.
  - **Quick check question:** If turn A uses 10 personal pronouns and turn B uses 6, what is the LSM score?

- **Concept: LIWC (Linguistic Inquiry and Word Count)**
  - **Why needed here:** The paper measures convergence across LIWC function word categories (pronouns, articles, conjunctions, etc.). Results are feature-specific.
  - **Quick check question:** Why might function word frequency be a better accommodation signal than content words?

- **Concept: Pretrained vs. Instruction-tuned Behavioral Differences**
  - **Why needed here:** The central finding is that training stage affects convergence strength. Pretrained models over-converge; instruction-tuned models trend toward human-like levels.
  - **Quick check question:** Would you expect a code-specialized model to converge more or less on general dialogue style? Why?

## Architecture Onboarding

- **Component map:** Data prep (filter conversations → sample 1,000) -> Generation (prompt with context → generate turn 6) -> Evaluation (compute LSM/PROPN/Token Novelty → compare to baselines)

- **Critical path:**
  1. Ensure baseline computation (human vs. random) before model evaluation
  2. Generate completions with controlled sampling (temperature 0.4, top-p 0.8)
  3. Run paired t-tests against both baselines for significance

- **Design tradeoffs:**
  - Synthetic completion vs. live interaction: Enables controlled comparison but may not reflect real user behavior
  - Fixed context (5 turns) vs. variable: Standardizes conditions but limits study of context window effects
  - Token limit (40 tokens): Reduces cost but may truncate natural responses

- **Failure signatures:**
  - Models generating off-topic continuations (prompt failure)
  - LIWC category misclassification due to tokenizer differences
  - Quantization artifacts in 70B model affecting style metrics

- **First 3 experiments:**
  1. Replicate the baseline comparison on a held-out dialogue corpus to validate generalizability.
  2. Test convergence under ablated context (remove speaker role markers) to isolate role differentiation effects.
  3. Compare convergence across sampling temperatures to distinguish sampling effects from model behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying causal mechanisms driving linguistic convergence in LLMs, and how do they fundamentally differ from the communicative goals of human accommodation?
- Basis in paper: [explicit] The authors conclude that "underlying mechanisms for these behaviors are very different" and explicitly call for future work to examine "the underlying causes."
- Why unresolved: The current study identifies *that* convergence occurs but cannot distinguish between goal-directed communication and statistical consistency derived from pretraining.
- What evidence would resolve it: Ablation studies or mechanistic interpretability analyses separating stylistic priming effects from genuine communicative intent.

### Open Question 2
- Question: How does the degree of model convergence impact user trust and the perceived reliability of the system?
- Basis in paper: [explicit] The discussion notes that "model convergence that is more in line with humans... will likely lead to higher trust," even if model capabilities are unreliable.
- Why unresolved: The experimental design relies on static dialogue corpora rather than measuring live human responses to varying degrees of convergence.
- What evidence would resolve it: Interactive user studies correlating quantitative convergence scores with qualitative measures of user trust and satisfaction.

### Open Question 3
- Question: Do the observed convergence patterns persist in models larger than 70B parameters or those trained with alternative objectives?
- Basis in paper: [explicit] The limitations section states, "it remains an open question how larger models (>70B parameters) and models post-trained on other objectives adapt to their users."
- Why unresolved: The study is restricted to the Llama and Gemma families up to 70B parameters, leaving the behavior of frontier-scale models unknown.
- What evidence would resolve it: Replicating the analysis on frontier proprietary models or experimental models with distinct training data distributions.

## Limitations
- Reliance on synthetic dialogue completion rather than live interaction may not capture authentic conversational accommodation
- Dataset scope limited to three corpora without systematic examination of domain variation
- Attribution of convergence differences to training objectives remains inferential without experimental isolation

## Confidence

**High Confidence:**
- Models exhibit measurable linguistic convergence across multiple stylometric features
- Pretrained models converge more strongly than instruction-tuned models
- Proper noun overlap shows recency bias in model-generated responses
- Convergence mechanisms differ from human accommodation (driven by training objectives rather than communicative goals)

**Medium Confidence:**
- The specific magnitude of convergence varies by feature and model family
- Instruction-tuning systematically reduces convergence strength
- Larger models show different convergence patterns than smaller counterparts

**Low Confidence:**
- Causal attribution of convergence differences to specific training objective properties
- Generalizability of convergence patterns across all dialogue domains
- The extent to which synthetic completion captures authentic conversational accommodation

## Next Checks

1. **Domain Generalization Test:** Replicate the convergence analysis across 5-10 diverse dialogue datasets spanning different domains (customer service, social media, academic discussions) to assess whether pretraining-over-convergence holds universally or is dataset-dependent.

2. **Live Interaction Comparison:** Conduct a controlled experiment where models engage in live dialogue with human interlocutors (rather than completing existing turns) and measure convergence patterns. Compare these results against the synthetic completion baseline to quantify the impact of interaction mode.

3. **Training Ablation Study:** Create variants of pretrained and instruction-tuned models where only one training aspect differs (e.g., same architecture, different data; same data, different objective) to isolate whether convergence differences stem from pretraining corpus characteristics, fine-tuning procedures, or other factors.