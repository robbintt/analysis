---
ver: rpa2
title: 'Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set
  Condition'
arxiv_id: '2501.06514'
source_url: https://arxiv.org/abs/2501.06514
tags:
- audio
- fake
- source
- ncst
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of audio deepfake source tracing
  in open-set conditions, where the goal is to identify both in-distribution and out-of-distribution
  (OOD) neural codec methods used to generate fake audio. The authors introduce the
  Neural Codec Source Tracing (NCST) task and construct the ST-Codecfake dataset,
  which includes bilingual audio samples generated by 11 state-of-the-art neural codec
  methods and ALM-based OOD test samples.
---

# Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set Condition

## Quick Facts
- **arXiv ID:** 2501.06514
- **Source URL:** https://arxiv.org/abs/2501.06514
- **Reference count:** 0
- **Primary result:** Introduces Neural Codec Source Tracing (NCST) task and ST-Codecfake dataset; achieves 99.99% F1-score for ID classification and 97.54% AUC for OOD detection

## Executive Summary
This paper introduces Neural Codec Source Tracing (NCST), a novel task for identifying neural codec methods used to generate fake audio in open-set conditions. The authors construct the ST-Codecfake dataset containing 235,736 audio samples generated by 11 state-of-the-art neural codec methods and establish a comprehensive benchmark for evaluating NCST models. Using self-supervised learning features (Wav2Vec2-XLS-R), their best model achieves exceptional performance on in-distribution classification while maintaining strong out-of-distribution detection capabilities. However, the models struggle with robustness when classifying unseen real audio, highlighting a key challenge for future work.

## Method Summary
The ST-Codecfake dataset contains 235,736 audio samples from 11 neural codec methods at 16kHz, 4-second duration. The evaluation framework uses three baseline models: Mel-LCNN (Mel-spectrogram features), AASIST (raw waveforms), and W2V2-AASIST (Wav2Vec2-XLS-R frozen features). All models use 7-class classification heads (Real + 6 codecs) trained with cross-entropy loss. OOD detection is performed using logits-based methods, particularly max softmax probability (MSP). The best model achieves 99.99% F1-score for ID classification and 97.54% AUC for OOD detection.

## Key Results
- W2V2-AASIST achieves 99.99% F1-score for in-distribution neural codec classification
- OOD detection performance reaches 97.54% AUC using logits-based scoring
- Model successfully identifies backend codecs of Audio Language Models with high accuracy (100% for Mini-omni)
- Significant performance drop on unseen real audio (4.77% F1-score on NCSSD dataset)

## Why This Works (Mechanism)

### Mechanism 1: SSL Features Enable Codec Discrimination
Self-supervised learning features from Wav2Vec2-XLS-R capture fine-grained acoustic structures that distinguish quantization and reconstruction artifacts of different neural codecs. This allows the model to learn distinct "fingerprints" of specific codecs (e.g., EnCodec vs. SNAC), separating them into distinct 7-class clusters.

### Mechanism 2: Logits-Based OOD Detection
The model produces excessively high-confidence logits for ID samples. When presented with OOD samples, the model cannot assign high confidence to any specific ID class, resulting in lower maximum softmax probabilities that serve as the detection boundary.

### Mechanism 3: ALM Attribution via Backend Codec Identification
Modern ALMs use specific neural codecs as audio tokenizers. By training on these specific codec classes, the system detects ALM-generated audio and attributes it to the specific codec architecture used, providing interpretability.

## Foundational Learning

- **Concept: Neural Audio Codecs (NACs)**
  - Why needed: The task relies on distinguishing audio based on codec fingerprints (quantization noise, spectral folding) rather than content
  - Quick check: Can you explain why audio reconstructed by a neural codec might sound "metallic" or "robotic," and why this artifact differs between EnCodec and SNAC?

- **Concept: Out-of-Distribution (OOD) Detection**
  - Why needed: The paper benchmarks "Open-Set" conditions, distinguishing between ID classification (known codecs) and OOD detection (identifying unknown classes)
  - Quick check: If a model assigns [0.45, 0.45, 0.1] to three classes, is this likely an ID or OOD sample compared to [0.98, 0.01, 0.01]?

- **Concept: Self-Supervised Learning (SSL) Features (Wav2Vec 2.0)**
  - Why needed: The best model relies on frozen Wav2Vec2-XLS-R features pre-trained on massive unlabeled data
  - Quick check: Why might frozen SSL features generalize better to unseen codec variations than a model trained from scratch on limited data?

## Architecture Onboarding

- **Component map:** Input audio (4s, 16kHz) → Wav2Vec2-XLS-R feature extractor (frozen, 1024-dim) → AASIST backend → Linear head (7 classes) → Logits output
- **Critical path:** Prepare ST-Codecfake dataset → Train W2V2-AASIST on ID classes (0-6) → Evaluate ID classification (F1-score) and OOD detection (AUC using MSP scores)
- **Design tradeoffs:** Optimizing for high ID confidence causes real audio to be treated as OOD; frozen features ensure stability but may miss fine-tuned opportunities
- **Failure signatures:** Unseen real audio misclassified as fake/OOD (F1 drops to 4.77% on NCSSD); configuration changes in known codecs cause performance collapse in weaker models
- **First 3 experiments:**
  1. Train AASIST and W2V2-AASIST on ID split (0-6) to verify ~99.99% F1-score reproduction
  2. Evaluate on OOD test set (codecs 7-11) and plot MSP score distribution to confirm separation
  3. Test on "Unseen Real" sets (ASVspoof2019, ITW, NCSSD) to quantify the failure mode

## Open Questions the Paper Calls Out
1. How can NCST models be improved to correctly classify unseen real audio without compromising fake audio detection accuracy?
2. To what extent does the imbalance between real and fake training data contribute to misclassification of unseen real audio?
3. Why does the mixture of multiple novel codecs pose a greater challenge for detection compared to single-codec OOD scenarios?

## Limitations
- Dataset generalization gap: Poor performance on unseen real audio (4.77% F1-score) indicates limited real-world applicability
- OOD detection reliability concerns: Novel codecs producing ID-like artifacts could evade detection
- Interpretability trade-offs: Proprietary ALM codecs could bypass attribution mechanism

## Confidence
- **High Confidence:** ID classification performance (99.99% F1-score) is exceptionally robust and well-supported
- **Medium Confidence:** OOD detection performance (97.54% AUC) is strong but sensitive to OOD distribution characteristics
- **Low Confidence:** Generalization to unseen real-world audio remains questionable

## Next Checks
1. Evaluate the model on additional real-world audio datasets beyond ASVspoof2019 and ITW to quantify the generalization gap
2. Systematically test OOD detection robustness by introducing post-processed audio to determine if adversarial samples can evade detection
3. Verify ALM attribution accuracy on diverse ALMs using hybrid codecs and custom implementations