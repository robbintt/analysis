---
ver: rpa2
title: 'REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning'
arxiv_id: '2406.04772'
source_url: https://arxiv.org/abs/2406.04772
tags:
- methods
- split
- memory
- prompt
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'REP introduces three complementary techniques to improve the computational
  and memory efficiency of prompt-based rehearsal-free continual learning: a lightweight
  surrogate model for prompt selection, adaptive token merging (AToM), and adaptive
  layer dropping (ALD). These methods selectively skip data and model layers during
  training while preserving task-specific features.'
---

# REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning

## Quick Facts
- **arXiv ID:** 2406.04772
- **Source URL:** https://arxiv.org/abs/2406.04772
- **Reference count:** 40
- **Primary result:** Reduces rehearsal-free continual learning training time by up to 51% and memory by up to 41% with minimal accuracy loss

## Executive Summary
REP introduces three complementary techniques to improve the computational and memory efficiency of prompt-based rehearsal-free continual learning: a lightweight surrogate model for prompt selection, adaptive token merging (AToM), and adaptive layer dropping (ALD). These methods selectively skip data and model layers during training while preserving task-specific features. Experiments on multiple image classification datasets show REP reduces training time by up to 51% and memory usage by up to 41% with only marginal accuracy loss compared to state-of-the-art methods. The approach is effective across different backbone sizes and can be applied to non-prompting rehearsal-free methods, achieving 37-48% training time reduction and up to 48% memory reduction.

## Method Summary
REP enhances rehearsal-free continual learning efficiency through three integrated techniques. First, a lightweight surrogate model (e.g., ViT-Ti) with fixed random projection approximates prompt selection quality of larger backbones at lower computational cost. Second, Adaptive Token Merging (AToM) progressively merges more tokens in deeper transformer layers while protecting prompt tokens to reduce memory and compute. Third, Adaptive Layer Dropping (ALD) uses AToM's token merge information to non-uniformly drop layers, with shallow layers retained more frequently due to their higher information content. These components work together to maintain accuracy while significantly reducing training time and memory usage across various backbone sizes.

## Key Results
- Training time reduced by 37-51% and memory usage by 37-48% across multiple datasets and backbone sizes
- Marginal accuracy loss compared to state-of-the-art methods (within 1-2% accuracy difference)
- Effective integration with L2P, DualPrompt, and CODA-Prompt baselines
- Generalization to non-prompting rehearsal-free methods with 37-48% training time reduction

## Why This Works (Mechanism)

### Mechanism 1: Lightweight Surrogate Model for Prompt Selection
Uses a compact pre-trained model (e.g., ViT-Ti) with random projection to approximate prompt selection quality of large backbones at fraction of computational cost. Smaller model produces low-dimensional query feature projected back to original dimensionality via fixed random projection. Preserves ~97% representational similarity while drastically reducing computation.

### Mechanism 2: Adaptive Token Merging (AToM)
Progressively merges more tokens in deeper transformer layers while explicitly protecting prompt tokens. Uses progressive scheduler that increases token merging with depth. Preserves task-specific information by excluding prompt tokens from merging process. Assumes deeper layers can tolerate greater token reduction while shallower layers need more information preserved.

### Mechanism 3: Adaptive Layer Dropping (ALD)
Non-uniformly drops transformer layers guided by token merging intensity from AToM. Computes layer-keeping probability that depends on temporal decay and spatial adjustment factor. Links with AToM by reducing drop probability for layers with less token merging. Prioritizes retaining shallow layers critical for task adaptation.

## Foundational Learning

**Concept: Vision Transformer (ViT) and Patch Tokens**
*Why needed:* REP operates on frozen ViT backbones. Understanding how images become patch tokens and flow through transformer layers is essential to grasp what AToM merges and ALD drops.
*Quick check:* Can you explain how a 224x224 image is processed into a sequence of tokens by a standard ViT?

**Concept: Catastrophic Forgetting & Rehearsal-Free Continual Learning (CL)**
*Why needed:* The entire goal of prompt-based CL, which REP optimizes, is to mitigate forgetting without storing old data (rehearsal-free). This frames the problem.
*Quick check:* In a CL setting with tasks A, B, C learned sequentially, what is catastrophic forgetting, and how does a rehearsal-free method like prompting avoid it?

**Concept: Prompt Tuning / Prompting for CL**
*Why needed:* REP is a meta-optimizer applied to existing prompt-based CL methods (L2P, DualPrompt, etc.). Knowing how a prompt pool is selected and updated is critical.
*Quick check:* In methods like L2P, how is a prompt selected from a pool for a given input, and what loss is used to update it?

## Architecture Onboarding

**Component map:** Input Image → Surrogate Model for Prompt Selection → Selected Prompt → Main Backbone with AToM & ALD for Prompt Update → Classification Head → Loss

**Critical path:** Input Image → Surrogate Model for Prompt Selection → Selected Prompt → Main Backbone with AToM & ALD for Prompt Update → Classification Head → Loss

**Design tradeoffs:**
- Efficiency vs. Accuracy: Aggressiveness of AToM and ALD directly trades training speed/memory for potential accuracy loss
- Generality vs. Specialization: REP designed to plug into existing prompt-based methods; modifying core scheduling algorithms may require re-tuning
- Assumption: Approach assumes frozen, pre-trained backbone; benefits for randomly initialized backbone not established

**Failure signatures:**
1. Gradient Instability: Exploding gradients on prompt parameters, often visible as NaN losses early in training
2. Unexpected Memory Spikes: Sudden OOM errors during training indicating implementation failure
3. Accuracy Collapse: Final accuracy far below non-REP baseline (>5% drop) suggesting ALD threshold too aggressive

**First 3 experiments:**
1. Integrate & Baseline Test: Implement REP as wrapper around L2P on Split CIFAR-100, compare final accuracy, iteration time, and peak memory usage
2. Ablation Study: Disable each REP component one at a time, quantify contribution to efficiency gains and accuracy delta
3. Hyperparameter Sensitivity Sweep: Vary r_max for AToM and τ, α for ALD, plot effect on accuracy and training time, identify stable operating ranges

## Open Questions the Paper Calls Out

**Open Question 1:** Can REP be effectively integrated with curriculum learning strategies to optimize task ordering without compromising resource efficiency?
*Basis:* Conclusion states exploring REP with curriculum learning is an interesting direction for future research
*Why unresolved:* Current design assumes static task stream; unknown how dynamic task scheduling interacts with adaptive mechanisms
*What evidence would resolve it:* Experiments applying REP to benchmarks with optimized task curricula, demonstrating efficiency gains hold while accuracy improves

**Open Question 2:** How does REP perform when applied to rehearsal-based continual learning methods that rely on replay buffers?
*Basis:* Conclusion suggests rehearsal-based REP is a promising direction for future investigation
*Why unresolved:* REP validated exclusively on rehearsal-free methods; skipping layers might corrupt gradient information critical for replay-based approaches
*What evidence would resolve it:* Empirical evaluation of REP integrated with rehearsal-based baselines (e.g., ER, MIR) showing trade-offs

**Open Question 3:** Can critical hyperparameters of Adaptive Layer Dropping (specifically α and τ) be automated to remove dependency on manual grid search?
*Basis:* Conclusion lists hyperparameter tuning as limitation, plans more detailed study on tuning to improve plug-and-play applicability
*Why unresolved:* Currently tuned via grid search which is computationally expensive and dataset-dependent
*What evidence would resolve it:* Adaptive algorithm or meta-learning strategy that sets parameters dynamically online

## Limitations
- Surrogate model approximation gap: Specific random projection parameters not fully specified, could lead to variability in prompt selection quality
- Token merging threshold tuning: Sensitivity to different architectures or dataset characteristics not fully explored
- Layer dropping coupling: Novel AToM-ALD coordination lacks direct evidence across diverse architectures

## Confidence
- **Efficiency Gains (Time/Memory Reduction):** High - Multiple datasets and backbone sizes show consistent 37-51% training time and 37-48% memory reductions
- **Accuracy Preservation:** Medium - Marginal accuracy loss reported, but robustness across diverse CL methods beyond L2P not fully validated
- **Method Generality:** Low-Medium - While claims applicability to non-prompting methods, experimental validation limited to few prompt-based approaches

## Next Checks
1. Cross-Architecture Surrogate Validation: Implement REP with different surrogate model sizes and random projection dimensions, measure CKA similarity and final CL performance
2. Prompt Token Protection Stress Test: Intentionally disable prompt token exclusion mask in AToM, run short training cycle, monitor gradient norms for prompt parameters
3. Non-Prompting Method Integration: Adapt REP to rehearsal-free non-prompting method (e.g., ER-ACE or OWM), implement three REP components, compare training efficiency and final accuracy against original method on Split CIFAR-100