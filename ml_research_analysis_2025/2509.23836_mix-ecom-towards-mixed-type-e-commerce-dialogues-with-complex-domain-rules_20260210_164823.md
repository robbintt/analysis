---
ver: rpa2
title: 'Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules'
arxiv_id: '2509.23836'
source_url: https://arxiv.org/abs/2509.23836
tags:
- user
- logistics
- return
- e-commerce
- shipping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Mix-ECom, a new benchmark for evaluating e-commerce
  agents on mixed-type dialogues with complex domain rules. Mix-ECom contains 4,799
  real-world customer service dialogues covering four dialogue types (QA, recommendation,
  task-oriented dialogue, and chitchat), three e-commerce task types (pre-sales, logistics,
  and after-sales), and 82 e-commerce rules.
---

# Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules

## Quick Facts
- arXiv ID: 2509.23836
- Source URL: https://arxiv.org/abs/2509.23836
- Reference count: 40
- Introduces Mix-ECom benchmark for evaluating e-commerce agents on mixed-type dialogues with complex domain rules

## Executive Summary
This paper introduces Mix-ECom, a comprehensive benchmark for evaluating e-commerce agents on mixed-type dialogues that combine question answering, recommendation, task-oriented dialogue, and chitchat across pre-sales, logistics, and after-sales scenarios. The benchmark includes 4,799 real-world customer service dialogues with 82 complex domain rules, multimodal inputs (images and videos), and 18 API tools connected to JSON databases. Experiments with state-of-the-art multimodal LLMs reveal that current agents struggle significantly with rule compliance, leading to hallucinations and errors. To address these limitations, the authors propose a dynamic e-commerce agent framework that filters irrelevant rules and replans based on context, demonstrating improved performance across all models. Fine-tuning smaller models on Mix-ECom also shows the dataset's effectiveness in enhancing agent capabilities.

## Method Summary
Mix-ECom evaluates e-commerce agents using 4,799 customer service dialogues spanning four dialogue types (QA, recommendation, task-oriented dialogue, and chitchat) across three task types (pre-sales, logistics, after-sales) with 82 complex domain rules. The benchmark incorporates multimodal inputs (46.7% images, 10% videos) and 18 API tools connected to JSON databases (logistics, order, product, merchant, user). The proposed solution extends ReAct/Plan&Solve frameworks with dynamic rule and trajectory filtering modules to improve rule compliance. The evaluation uses Key Answer Score (all required answers present), Database Score (state matches ground truth), and Combined Score (both correct). Human evaluation covers three dimensions. Fine-tuning experiments use Qwen-2.5-VL-7B with supervised trajectory segments.

## Key Results
- State-of-the-art multimodal LLMs show significant performance gaps in handling complex domain rules, with rule violations accounting for approximately 63% of errors
- The proposed dynamic e-commerce agent framework improves performance across all models by filtering irrelevant rules and enabling context-aware replanning
- Fine-tuning smaller models (Qwen-2.5-VL-7B) on Mix-ECom demonstrates the dataset's effectiveness in improving agent performance

## Why This Works (Mechanism)
The approach works by addressing the core challenge of complex domain rule compliance in e-commerce dialogues. Current multimodal LLMs struggle with multi-step policy requirements and context-dependent rule application. The dynamic framework filters irrelevant rules based on dialogue context and enables replanning when rule violations are detected, reducing hallucinations and improving accuracy. The multimodal inputs provide critical contextual information that text-only models miss, while the fine-tuning process adapts smaller models to the specific domain requirements.

## Foundational Learning
- **Domain Rules**: 82 e-commerce-specific rules governing customer service interactions (needed because current agents violate rules in ~63% of cases; quick check: verify rule implementation in a subset of dialogues)
- **Multimodal Integration**: Combining text, images, and videos for comprehensive context understanding (needed because multimodal misinterpretation causes ~15% of errors; quick check: compare performance with and without visual inputs)
- **Dynamic Rule Filtering**: Context-aware selection of relevant rules during dialogue (needed to reduce irrelevant rule processing and improve efficiency; quick check: measure rule processing time with and without filtering)
- **Replanning Mechanisms**: Ability to adjust action sequences when violations are detected (needed to correct course and maintain rule compliance; quick check: test replanning on known violation scenarios)
- **Database State Tracking**: Maintaining and validating dialogue state against ground truth (needed for accurate evaluation of agent performance; quick check: compare database states before and after tool execution)
- **Tool Integration**: 18 API tools connected to JSON databases for executing e-commerce tasks (needed for practical implementation of customer service actions; quick check: verify tool execution on sample dialogues)

## Architecture Onboarding

**Component Map**: Dialogue Context -> Rule Filter -> Dynamic Planner -> Tool Execution -> Database Update -> Response Generation

**Critical Path**: The agent processes dialogue context through the rule filter to identify relevant rules, then uses the dynamic planner to generate tool calls and actions, which are executed against the database, with the resulting state used to generate responses and update context.

**Design Tradeoffs**: The system balances rule compliance with dialogue fluency, choosing between strict rule enforcement (which may lead to rigid responses) and flexible interpretation (which may violate rules). The multimodal inputs add complexity but provide crucial context that text-only models miss.

**Failure Signatures**: Common failures include domain rule violations (missing multi-step policy requirements), multimodal misinterpretation (ignoring visual evidence), and incorrect tool sequencing (violating temporal dependencies in rule execution).

**First Experiments**:
1. Zero-shot baseline evaluation using GPT-4o on the 299-test sample set to establish performance metrics
2. Ablation study comparing performance with and without multimodal inputs to quantify their impact
3. Rule compliance testing on a subset of dialogues to identify specific violation patterns and frequencies

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset access URL not provided in paper, preventing immediate reproduction of results
- Specific tool function implementations beyond names/categories are not detailed
- Dynamic filtering module prompts and architecture lack precise specifications
- Fine-tuning hyperparameters for Qwen-2.5-VL-7B are not provided

## Confidence

**High confidence**: Dataset specifications (4,799 dialogues, 82 rules, multimodal distribution), evaluation metrics (Key Answer, Database, Combined Scores), and proposed frameworks (E-ReAct/E-Plan&Solve) are clearly outlined with sufficient detail for implementation.

**Medium confidence**: Tool implementations, database schemas, and dynamic filtering mechanisms are described conceptually but lack precise architectural specifications or complete function definitions.

**Low confidence**: Dataset accessibility and exact rule content cannot be validated without the promised public release, as no current URL or access mechanism is provided.

## Next Checks
1. Upon dataset release, verify the 4,799 dialogue count, 82 rule count, and multimodal distribution (46.7% images, 10% videos) match paper specifications
2. Implement a subset of rules (e.g., in-transit address change requirements) and test whether the zero-shot baseline (GPT-4o) correctly executes all required tool calls
3. Run the baseline agent with and without image/video inputs on a small test subset to quantify the impact of multimodal information on performance, specifically checking for the reported 15% error rate due to multimodal misinterpretation