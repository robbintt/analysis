---
ver: rpa2
title: Latent Behavior Diffusion for Sequential Reaction Generation in Dyadic Setting
arxiv_id: '2505.07901'
source_url: https://arxiv.org/abs/2505.07901
tags:
- facial
- reaction
- latent
- reactions
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating diverse and contextually
  appropriate facial reactions in dyadic interactions. The proposed Latent Behavior
  Diffusion Model combines a context-aware autoencoder with a diffusion-based conditional
  generator to synthesize multiple realistic listener facial reactions from speaker
  behaviors.
---

# Latent Behavior Diffusion for Sequential Reaction Generation in Dyadic Setting

## Quick Facts
- **arXiv ID**: 2505.07901
- **Source URL**: https://arxiv.org/abs/2505.07901
- **Reference count**: 40
- **One-line primary result**: Latent Behavior Diffusion Model achieves FRCorr of 0.37, FRDist of 89.40, and FRDiv of 0.1211 on REACT2024 dataset

## Executive Summary
This paper addresses the challenge of generating diverse and contextually appropriate facial reactions in dyadic interactions. The proposed Latent Behavior Diffusion Model combines a context-aware autoencoder with a diffusion-based conditional generator to synthesize multiple realistic listener facial reactions from speaker behaviors. The autoencoder compresses high-dimensional input features into a concise latent representation, capturing dynamic patterns in listener reactions, while the diffusion-based generator operates in this latent space to produce diverse facial reactions in a non-autoregressive manner. Experimental results on the REACT2024 dataset demonstrate that the proposed approach significantly outperforms existing methods in terms of appropriateness, diversity, and synchrony metrics.

## Method Summary
The method operates in two stages: first, a context-aware autoencoder compresses listener reaction sequences into a compact latent representation, then a diffusion-based conditional generator operates in this latent space to predict realistic facial reactions. The autoencoder uses a Transformer encoder to capture speaker context and two decoder branches to reconstruct reaction features and generate 3DMM coefficients. The diffusion model employs a conditional behavior decoder with skip connections to iteratively denoise latent representations conditioned on speaker behavior. The entire system is trained sequentially, with the autoencoder first and then the diffusion model, using a combination of reconstruction and denoising objectives.

## Key Results
- Achieves FRCorr of 0.37, FRDist of 89.40, FRDiv of 0.1211, FRVar of 0.0653, FRDvs of 0.1505, and FRSyn of 43.48 on REACT2024 dataset
- Outperforms baseline methods including Trans-VAE (FRCorr 0.15, FRDiv 0.006) and ReactDiff (FRCorr 0.32, FRDiv 0.0074)
- Demonstrates superior performance in generating natural and coherent facial frames compared to baseline methods, with FID of 50.95 on image-level evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Operating diffusion in a learned latent space rather than raw feature space enables tractable training for high-dimensional sequential facial reactions.
- Mechanism: The context-aware autoencoder first compresses listener reaction sequences (Action Units, valence/arousal, emotion probabilities, 3DMM) from R^{T×d} into a compact latent representation V ⊂ R^v. The diffusion model then learns to denoise within this lower-dimensional space rather than directly modeling the full observation space. This reduces the denoising network's burden from modeling thousands of correlated dimensions to a structured latent manifold.
- Core assumption: The autoencoder can capture sufficient spatio-temporal structure such that the latent space remains expressive for diverse reactions while being compact enough for diffusion to learn effectively. Assumption: Reaction diversity is preserved through latent space sampling rather than requiring full observation-space stochasticity.
- Evidence anchors:
  - [abstract] "The autoencoder compresses high-dimensional input features... while condensing complex input data into a concise latent representation, facilitating more expressive and contextually appropriate reaction synthesis."
  - [Section 3.2] "our context-aware Auto-Encoder learns the prior distribution p(z) without a standard Gaussian N(0,1) or 1-D codebook as it can only produce deterministic outcomes, however, it does not suffer posterior collapse and codebook collapse on a very high-dimension multivariate reaction time series"
  - [corpus] Related work "DiffListener" and "ReactDiff" similarly adopt latent diffusion for listener generation, suggesting this is an emerging design pattern. However, direct comparative evidence that latent-space diffusion outperforms observation-space diffusion for this specific task is not provided in this paper.
- Break condition: If the autoencoder's bottleneck is too aggressive, it may lose subtle emotional variations; if too loose, diffusion training becomes unstable. Monitor reconstruction loss divergence between Dr and Df.

### Mechanism 2
- Claim: Non-autoregressive generation avoids error accumulation while the diffusion process provides temporal coherence through iterative refinement.
- Mechanism: Unlike autoregressive approaches (e.g., Learning2Listen's VQ-VAE) that predict frame-by-frame and accumulate errors, this model generates all frames simultaneously in latent space through the diffusion reverse process. The conditional behavior decoder pθ(zt-1|zt, t, zsem) receives semantic context zsem = Er(C) from the speaker's behavior, allowing the model to condition on the full conversational context rather than only preceding frames. The PLMS sampler then refines the entire sequence over T denoising steps.
- Core assumption: The semantic embedding zsem captures sufficient speaker behavior information to guide appropriate reactions. Assumption: T=50 denoising steps provides adequate refinement without excessive computation.
- Evidence anchors:
  - [abstract] "The diffusion-based conditional generator operates on the latent space generated by the autoencoder to predict realistic facial reactions in a non-autoregressive manner."
  - [Section 3.3] "our conditional behavior decoder takes the high-level semantic subcode zsem and the low-level stochastic subcode zT"
  - [corpus] "Efficient Listener" and related work also move toward non-autoregressive generation, though this paper does not directly compare against autoregressive baselines on error accumulation metrics.
- Break condition: If speaker conditioning is weak (zsem uninformative), generated reactions may be diverse but inappropriate. Check FRCorr degradation.

### Mechanism 3
- Claim: Skip-connected MLPs in the conditional decoder preserve fine-grained behavioral features through the denoising process.
- Mechanism: The conditional behavior decoder uses residual MLPs with skip connections (Fig. 2), where each layer concatenates input with previous layer output. This allows gradient flow and feature preservation across 10 layers and 1024 hidden nodes. The PLMS sampler (4th-order convergence) then uses multi-step information for stable trajectory estimation in latent space.
- Core assumption: The 10-layer MLP depth is sufficient for modeling reaction dynamics without requiring attention mechanisms. Assumption: 4th-order PLMS provides meaningful quality improvements over lower orders.
- Evidence anchors:
  - [Section 3.3] "Each layer of the MLP has a skip connection from the input, which concatenates the input with the output from the previous layer"
  - [Section 4.4, Table 3] Ablation shows PLMS (k=4, T=50) achieves best FRSyn (43.48) and FRCorr (0.374), though improvements over k=2 are marginal.
  - [corpus] No direct corpus comparison of skip-connected MLPs vs. attention-based decoders for this task.
- Break condition: If training loss plateaus early, skip connections may be propagating noise; consider reducing depth or adding layer normalization.

## Foundational Learning

- Concept: Denoising Diffusion Probabilistic Models (DDPM) and DDIM
  - Why needed here: The paper builds directly on latent diffusion (Rombach et al. 2022) and assumes familiarity with forward/reverse processes, noise schedules (βt), and sampling methods. Without this, the PLMS sampler discussion (Eq. 8) will be opaque.
  - Quick check question: Can you explain why a 50-step PLMS process might achieve comparable quality to 1000-step DDIM sampling?

- Concept: Vector Quantized Variational Autoencoders (VQ-VAE)
  - Why needed here: The autoencoder uses VQ-VAE losses (Eq. 4) including codebook loss and commitment loss for the Df decoder generating 3DMM coefficients. Understanding the tradeoff between continuous latent diffusion and discrete codebook quantization is essential.
  - Quick check question: What is the purpose of the commitment loss term β∥ze(Y)) - sg[e]∥², and what happens if β is too small?

- Concept: Facial Action Units and 3D Morphable Models (3DMM)
  - Why needed here: The model operates on 15 AUs, valence/arousal, expression probabilities, and 3DMM coefficients. Understanding these representations is necessary for interpreting the input/output dimensions and evaluating qualitative results.
  - Quick check question: Why might 3DMM coefficients be preferred over raw pixels for the Df decoder output?

## Architecture Onboarding

- Component map:
  - Er (Transformer encoder, 2 layers, 4 heads) → latent z → two branches: Dr (Transformer decoder, reconstructs reaction features) and zq via codebook → Df (Transformer decoder, generates 3DMM)
  - Speaker behavior C → Er (reused) → zsem as condition; random noise zT → Conditional Behavior Decoder (10-layer MLP with skip connections, 1024 hidden) → iteratively denoise via PLMS → z0 → Dr (reused from Stage 1) → predicted reaction sequence

- Critical path:
  1. Train Dr autoencoder first (1000 epochs) on listener reactions only
  2. Train Df with VQ-VAE loss (200 epochs) for 3DMM generation
  3. Freeze Er/Dr, train diffusion model (200 epochs) with speaker-conditioned denoising objective (Eq. 7)
  4. At inference: sample zT ~ N(0,I), denoise with PLMS (k=4, T=50) conditioned on speaker zsem, decode with Dr

- Design tradeoffs:
  - Denoising steps T: Higher T improves FRDist but degrades FRCorr and increases latency. Paper identifies T=50 as optimal balance.
  - PLMS order k: Higher orders show marginal improvements; k=4 achieves best synchrony but k=2 is nearly equivalent.
  - Window size w=50 frames: Fixes sequence length; longer windows may capture more context but increase memory/computation.

- Failure signatures:
  - Low FRDiv with high FRDist: Posterior collapse in diffusion; check if zsem dominates and zT is ignored
  - High FRSyn with low FRCorr: Generated reactions are synchronized but not appropriate; speaker conditioning may be insufficient
  - Identity loss in rendered faces: Df decoder not properly trained; increase VQ-VAE epochs or codebook size
  - Jittery frames: Denoising insufficient; increase T or check PLMS stability

- First 3 experiments:
  1. Reproduce autoencoder reconstruction: Train only Stage 1, verify Dr can reconstruct held-out listener reactions with low MSE before proceeding to diffusion.
  2. Ablate denoising steps: Compare T∈{10,25,50,100} on validation set; confirm T=50 yields reported tradeoff between FRDist and diversity metrics.
  3. Visualize latent space: Apply t-SNE to zsem embeddings colored by speaker emotion; verify semantic clustering before training diffusion to ensure conditioning signal is meaningful.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the denoising process be refined to improve long-term reaction sequence modeling while maintaining DTW-based appropriateness (FRDist)?
- Basis in paper: [explicit] The conclusion states: "We further aim to improve the appropriateness of DTW by refining denoising process for better long-term reaction sequence modeling."
- Why unresolved: The paper observes a trade-off where increasing denoising steps beyond 50 degrades correlation and diversity (Table 3: T=100 drops FRCorr to 0.33), suggesting the current PLMS-based denoising struggles to balance temporal coherence with sequence-level appropriateness.
- What evidence would resolve it: A modified sampling strategy or architectural enhancement that improves FRDist without sacrificing FRCorr/FRDiv across longer sequences; systematic analysis of temporal error accumulation in the latent diffusion process.

### Open Question 2
- Question: Why does increasing denoising steps beyond the optimal 50 degrade both correlation and diversity metrics?
- Basis in paper: [inferred] Table 3 shows T=100 achieves lower FRCorr (0.33) and FRDiv (0.1064) than T=50 (0.372, 0.1211), despite more denoising theoretically providing finer-grained reconstruction.
- Why unresolved: The paper identifies T=50 as optimal empirically but does not explain the mechanism causing degradation at higher steps; this counterintuitive result suggests potential over-denoising or distribution drift in the latent space.
- What evidence would resolve it: Ablation studies analyzing intermediate latent states across different T values; theoretical analysis of the interaction between PLMS convergence orders and the learned latent distribution structure.

### Open Question 3
- Question: How does the context-aware autoencoder's learned prior distribution compare to standard Gaussian or discrete codebook priors in preventing posterior collapse and codebook collapse?
- Basis in paper: [inferred] The paper claims the Transformer-based AE prior "does not suffer posterior collapse and codebook collapse on a very high-dimension multivariate reaction time series" but provides no comparative analysis or quantitative evidence for this claim.
- Why unresolved: Posterior collapse is identified as a key weakness of baseline methods (Trans-VAE, BeLFusion), yet the proposed approach's robustness is asserted without systematic validation against alternative prior formulations.
- What evidence would resolve it: Comparative experiments explicitly measuring posterior collapse rates across different prior distributions; analysis of latent space utilization and activation patterns during training.

## Limitations

- The optimal configuration (latent_dim=256, T=50, k=4) may not generalize to datasets with different temporal dynamics or feature distributions.
- The computational cost of 50-step PLMS sampling with 10-layer MLPs may limit real-time applications, though this tradeoff is not explicitly discussed.
- The method assumes pre-extracted facial features rather than raw video input, which may constrain deployment scenarios.

## Confidence

- **High Confidence**: The core claim that latent diffusion in compressed space outperforms observation-space diffusion for this task is well-supported by the quantitative metrics (FRCorr=0.37 vs. 0.15 baseline, FRDiv=0.1211 vs. 0.006 baseline).
- **Medium Confidence**: The claim that non-autoregressive generation avoids error accumulation is plausible given the architecture but lacks direct comparison against autoregressive baselines on error propagation metrics.
- **Low Confidence**: The assertion that the method achieves "real-time" or "interactive" generation speed is not empirically validated.

## Next Checks

1. **Ablation on Encoder Depth**: Test whether the 2-layer Transformer encoder is optimal by comparing against 4-layer and 6-layer variants on FRCorr and FRDiv metrics. This would validate whether the shallow encoder adequately captures speaker behavior semantics for conditioning.

2. **Cross-Dataset Generalization**: Evaluate the trained model on RECOLA alone (without NoXi augmentation) to assess robustness to different recording conditions and participant demographics. Compare FRCorr degradation between the proposed method and baseline Trans-VAE.

3. **Latent Space Interpolation Analysis**: Generate intermediate reaction sequences by interpolating between different zT samples and visualize the resulting facial animations. This would validate whether the latent space maintains temporal coherence and whether diversity metrics reflect perceptually meaningful variations rather than noise.