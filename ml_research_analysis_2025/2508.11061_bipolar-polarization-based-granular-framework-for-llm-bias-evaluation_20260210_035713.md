---
ver: rpa2
title: 'BIPOLAR: Polarization-based granular framework for LLM bias evaluation'
arxiv_id: '2508.11061'
source_url: https://arxiv.org/abs/2508.11061
tags:
- bias
- sentiment
- categories
- language
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BIPOLAR, a granular framework for detecting
  and analyzing polarization-driven bias in large language models (LLMs). The method
  combines synthetic, polarity-balanced datasets based on the CAMEO event ontology
  with a symmetry-based sentiment evaluation scheme.
---

# BIPOLAR: Polarization-based granular framework for LLM bias evaluation

## Quick Facts
- arXiv ID: 2508.11061
- Source URL: https://arxiv.org/abs/2508.11061
- Authors: Martin Pavlíček; Tomáš Filip; Petr Sosík
- Reference count: 40
- Primary result: BIPOLAR detects entity-specific polarization bias in LLMs using symmetric synthetic datasets and sentiment divergence metrics.

## Executive Summary
This paper introduces BIPOLAR, a granular framework for detecting and analyzing polarization-driven bias in large language models (LLMs). The method combines synthetic, polarity-balanced datasets based on the CAMEO event ontology with a symmetry-based sentiment evaluation scheme. A case study on the Russia-Ukraine conflict evaluated five LLMs (LLaMA 3, Mistral, GPT-4, Claude 3.5, and Gemini 1.0) across 15 conflict-related categories. Results revealed varying bias profiles: LLaMA 3 showed the least bias overall, while Mistral exhibited strong negative sentiment toward Russia. The framework also demonstrated sensitivity to prompt framing, with open models showing stronger shifts in sentiment under citizenship or language cues compared to proprietary models. BIPOLAR is extensible to other polarized domains and supports fine-grained, interpretable bias diagnostics.

## Method Summary
BIPOLAR evaluates LLM bias by generating symmetric synthetic statements across 15 CAMEO event categories, pairing opposing entities (Russia/Ukraine) with balanced positive/negative polarities. Models are prompted to assign sentiment scores (0-100) to each statement, and bias is quantified using Jensen-Shannon divergence between entity distributions. The framework tests different prompt variants (vanilla, citizen, language, in-context) and aggregates results by category to identify bias hotspots. A GitHub repository provides dataset generation and analysis tools.

## Key Results
- LLaMA 3 exhibited the least bias overall, while Mistral showed strong negative sentiment toward Russia
- Category-specific anomalies emerged, with "Investigate" events receiving anomalously negative sentiment despite being a "positive" category
- Open models (Llama, Mistral) showed stronger bias shifts under citizenship framing compared to proprietary models (GPT-4, Claude, Gemini)
- Bias patterns varied significantly across semantic categories, with no consistent trends between models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Symmetric dataset design isolates entity-specific bias from semantic content effects.
- **Mechanism:** By generating mirror statements (e.g., "Russia attacks" vs. "Ukraine attacks") across identical semantic categories and polarities, the framework holds the event constant while swapping the entity. The delta in model-assigned sentiment scores reveals directional bias attributable solely to the entity identity, canceling out event-based noise.
- **Core assumption:** Models treat semantic event structures consistently regardless of the target entity; any significant divergence in sentiment distribution is attributable to bias rather than linguistic variance.
- **Evidence anchors:**
  - [section 3.1]: "Events are constructed in pairs of opposing semantic terms... to maintain polarity symmetry."
  - [section 3.3]: "The crucial methodological element is the symmetry of the statements, which allows one to detect systematic bias as a divergence in the average sentiment towards opposing entities."
  - [corpus]: Weak direct validation. Related work (e.g., "A Dual-Layered Evaluation") discusses geopolitical bias but does not validate the specific symmetric pair methodology used here.
- **Break condition:** If models exhibit fundamental semantic differences in how they process specific verbs for different entities (e.g., cultural associations with "yield" vs. "protest") unrelated to sentiment, the symmetry comparison may yield false positives.

### Mechanism 2
- **Claim:** Granular ontology-based categorization exposes high-dimensional bias patterns invisible to aggregate metrics.
- **Mechanism:** Instead of a single "bias score," the framework maps bias across a structured codebook (CAMEO). This allows the detection of category-specific anomalies (e.g., bias in "investigate" vs. "fight") and model-specific behavioral profiles (e.g., radical vs. neutralizing tendencies) that aggregate averaging would flatten.
- **Core assumption:** The chosen ontology (CAMEO) sufficiently covers the conflict domain, and the synthetic generation accurately reflects the semantic weight of those categories.
- **Evidence anchors:**
  - [abstract]: "Framework allowed fine-grained analysis with considerable variation between semantic categories, uncovering divergent behavioural patterns among models."
  - [section 4.4]: "The most biased categories are controversial... models mostly do not share common bias trends towards certain categories."
  - [corpus]: External support is implied by "Deconstructing Instruction-Following" (arXiv:2601.18554), which argues for granular evaluation over single metrics, though it focuses on instruction compliance rather than sentiment bias.
- **Break condition:** If the synthetic statements generated for specific categories are semantically ambiguous or misclassified according to the ontology, the resulting "category bias" is actually a measurement artifact.

### Mechanism 3
- **Claim:** Persona-based prompting (citizenship) triggers stronger bias activation than linguistic framing.
- **Mechanism:** Prompting the model to adopt a specific citizenship persona (e.g., "respond as a citizen of X") activates latent alignment or persona-taking mechanisms more deeply than merely translating the input text. This causes a measurable shift in sentiment distribution, likely reflecting the model's internal mapping of that persona's "likely" worldview.
- **Core assumption:** The model possesses and can retrieve internal representations of "citizen perspectives" that are distinct from its baseline alignment.
- **Evidence anchors:**
  - [section 4.6]: "The effects of nationality and language were different... the effect of citizenship was much stronger than that of language, favouring the citizen’s own party."
  - [section 4.6]: Open models increased sentiment to negative statements under citizen context, while proprietary models conformed better to the "own party" hypothesis.
  - [corpus]: Weak direct evidence. Neighbors discuss "cognitive bias" and "cultural bias" generally but do not confirm the specific magnitude of persona vs. language prompts found here.
- **Break condition:** If the model interprets the persona instruction not as a perspective-taking exercise but as a command to roleplay a caricature, the resulting sentiment shift reflects stereotyping rather than latent "national" bias.

## Foundational Learning

- **Concept:** **Jensen-Shannon (JS) Divergence**
  - **Why needed here:** This is the primary metric used to quantify the "distance" between sentiment distributions for opposing entities. Understanding that 0 indicates identical distributions (no bias) and higher values indicate divergence is crucial for interpreting the results in Figure 4.
  - **Quick check question:** If Model A assigns scores of [10, 20] to Entity 1 and [80, 90] to Entity 2, would the JS divergence be higher or lower than if it assigned [40, 50] and [50, 60]? (Answer: Higher).

- **Concept:** **CAMEO (Conflict and Mediation Event Observations)**
  - **Why needed here:** The framework relies on this specific ontology to define the event categories (e.g., "Fight," "Yield," "Investigate"). One cannot interpret the category-specific anomalies without understanding that these are standard codified political science event types.
  - **Quick check question:** Which category in the study showed an "anomalous" negative sentiment despite being a "positive" event type? (Answer: Investigate).

- **Concept:** **Sentiment Polarity Symmetry**
  - **Why needed here:** The mechanism hinges on creating "mirror" statements (positive/negative, subject/object). Understanding that "bias" is defined here as a break in this symmetry (e.g., praising Entity A for an action while condemning Entity B for the same action) is the core logic of the paper.
  - **Quick check question:** Does the framework define bias as the absolute sentiment score of a statement, or the difference in scores between a statement and its mirror? (Answer: The difference).

## Architecture Onboarding

- **Component map:** Generator -> Probe -> Extractor -> Analyzer
- **Critical path:**
  1. Define the polarized topic and select symmetric entities (e.g., RU/UA)
  2. Map the ontology to the domain (select relevant CAMEO codes)
  3. Generate the balanced dataset (ensure equal representation of polarity/roles)
  4. Run the evaluation loop against target models
  5. Compute divergence metrics to identify bias hotspots
- **Design tradeoffs:**
  - Synthetic vs. Organic: The study uses synthetic data to ensure perfect symmetry and balance (high control), but risks lacking the nuanced ambiguity of real-world political discourse (lower ecological validity)
  - Black-box vs. White-box: The framework treats models as black boxes (API calls), making it applicable to closed models (GPT-4) but preventing analysis of internal embedding states
- **Failure signatures:**
  - Format Drift: Models returning justifications or non-numeric text instead of the requested 0-100 score (Section 4: "Malformed outputs were excluded")
  - Quantization Bias: Models defaulting to multiples of 5 or 10 (Section 4: "models mostly assigned scores... in multiples of 5"), which reduces measurement granularity
  - API Versioning: Proprietary models changing behavior without version updates (Section 5.1: "proprietary models... present a unique challenge for longitudinal bias analysis")
- **First 3 experiments:**
  1. Baseline Symmetry Check: Run the vanilla prompt configuration on two distinct models (e.g., Llama-3 and Mistral) using the RU-UA dataset to establish a baseline for category-specific bias divergence
  2. Persona Sensitivity Test: Ablate the "Citizen" prompt vs. "Language" prompt on a single model to quantify the delta in sentiment shift caused by persona adoption
  3. Ontology Stress Test: Generate a dataset for a different polarized domain (e.g., US-China) using the same CAMEO categories to determine if observed biases (e.g., "Investigate" anomaly) are model-specific or topic-agnostic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs exhibit consistent bias patterns across diverse semantic domains (e.g., geopolitical conflicts vs. social issues), or do responses vary significantly by topic?
- Basis in paper: [explicit] Section 5.2 proposes extending the framework to topics like China-Taiwan or vaccination to analyze whether models show consistent bias trends across domains.
- Why unresolved: The current study is restricted to a single case study (Russia-Ukraine), limiting generalizability.
- What evidence would resolve it: Applying BIPOLAR to multiple distinct polarizing topics and comparing cross-domain bias profiles.

### Open Question 2
- Question: How do non-Western LLMs (e.g., DeepSeek, Qwen) perform on polarization metrics compared to the Euro-American models tested?
- Basis in paper: [inferred] Section 5.1 explicitly lists the inclusion of models from outside the dominant Euro-American ecosystem as a necessary step to strengthen conclusions.
- Why unresolved: The study only evaluated five specific models (Llama, Mistral, GPT-4, Claude, Gemini), mostly representing Western tech.
- What evidence would resolve it: Benchmarking diverse non-Western LLMs using the same CAMEO-based dataset and symmetry metrics.

### Open Question 3
- Question: How does varying the temporal frame (past/future) and entity role (object/subject) quantitatively alter the detected polarization bias?
- Basis in paper: [inferred] Section 5.2 identifies temporal framing and entity roles as conceptual dimensions for systematic extension; the experiment restricted inputs to "present" tense and "subject" roles.
- Why unresolved: The methodology currently fixes these parameters, leaving their impact on sentiment symmetry unexplored.
- What evidence would resolve it: Generating new dataset variants with mirrored temporal tenses and entity roles to measure shifts in Jensen-Shannon divergence.

## Limitations
- The synthetic dataset may lack the contextual nuance of real-world political discourse
- The symmetry assumption may be violated if models have cultural associations with specific verbs/entities
- The framework only measures sentiment bias, missing other forms of bias (factual distortion, omission)
- Proprietary model access is limited by API terms and potential version drift

## Confidence

- **High Confidence:** The methodological framework for symmetric dataset construction and JS divergence-based bias measurement is clearly specified and internally consistent. The category-specific bias findings for LLaMA 3 vs. Mistral are robust and replicable.
- **Medium Confidence:** The generalizability of findings beyond the Russia-Ukraine conflict domain and the claim that persona-based prompting consistently triggers stronger bias activation require further validation across multiple polarized topics.
- **Low Confidence:** The exact semantic equivalence of synthetic statements across all CAMEO categories and the interpretation of the "Investigate" category anomaly as a true bias signal versus a measurement artifact are uncertain without qualitative validation.

## Next Checks

1. Cross-Domain Replication: Apply BIPOLAR to a distinct polarized domain (e.g., US-China or Israel-Palestine) using the same CAMEO categories to test if category-specific bias patterns (e.g., the "Investigate" anomaly) are model-specific or domain-agnostic.
2. Semantic Equivalence Audit: Conduct a human evaluation study where annotators rate the semantic similarity of mirror statement pairs (e.g., "Russia attacks" vs. "Ukraine attacks") to quantify potential confounds in the symmetry assumption.
3. Multi-Dimensional Probe Expansion: Extend the evaluation beyond sentiment scores to include other LLM outputs (e.g., factual assertions, narrative framing) when probed with the same symmetric dataset to detect non-sentiment forms of bias.