---
ver: rpa2
title: 'Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking'
arxiv_id: '2503.19855'
source_url: https://arxiv.org/abs/2503.19855
tags:
- reasoning
- round
- thinking
- performance
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Multi-round Thinking, a simple yet effective
  test-time scaling strategy that iteratively refines LLM reasoning by leveraging
  previous answers as prompts for subsequent rounds. The method addresses limitations
  in handling long texts and reinforcement learning efficiency by breaking cognitive
  inertia and enabling correction of entrenched reasoning errors.
---

# Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking

## Quick Facts
- arXiv ID: 2503.19855
- Source URL: https://arxiv.org/abs/2503.19855
- Authors: Xiaoyu Tian; Sitong Zhao; Haotian Wang; Shuaiting Chen; Yunjie Ji; Yiping Peng; Han Zhao; Xiangang Li
- Reference count: 5
- Primary result: Multi-round thinking improves LLM reasoning accuracy by iteratively refining answers through re-prompting with previous answers

## Executive Summary
This paper introduces Multi-round Thinking, a test-time scaling strategy that enhances LLM reasoning by iteratively reconsidering previous answers. The method works by discarding intermediate reasoning traces between rounds and re-prompting the model with only the prior answer, forcing independent reconsideration rather than path-dependent continuation. Experimental results show consistent accuracy improvements across multiple models and benchmarks, with gains following a diminishing returns pattern after the second round.

## Method Summary
Multi-round Thinking operates through iterative re-evaluation where each round generates a new answer based solely on the previous answer and the original prompt. The process discards intermediate reasoning traces to break cognitive inertia, allowing models to correct entrenched errors. For each round n, the prompt is constructed as P_n = P_user ⊕ Answer_{n-1}, excluding Thinking_{n-1}. The method uses generation parameters (max_tokens=32,768, temperature=0.6, top_p=0.95) and evaluates performance using pass@1 accuracy with multiple samples per query.

## Key Results
- QwQ-32B accuracy improved from 80.3% (Round 1) to 82.1% (Round 2) on AIME 2024
- DeepSeek-R1 showed similar gains from 79.7% to 82.0% on the same benchmark
- Multi-round reasoning produces more concise and confident responses with reduced uncertainty markers
- Diminishing returns observed after Round 2, with minimal improvement from Rounds 3-4

## Why This Works (Mechanism)

### Mechanism 1
Discarding prior reasoning traces and re-prompting with only the previous final answer enables independent reconsideration. This forces the model to regenerate reasoning from scratch rather than continue or justify an existing chain, reducing path dependency.

### Mechanism 2
Multi-round re-evaluation shifts linguistic behavior toward more decisive, less hedged reasoning. The model reduces usage of hesitation markers and produces shorter responses, suggesting increased confidence and reduced exploratory backtracking when prior answers are correct.

### Mechanism 3
Performance gains are non-uniform and saturate after several rounds. Iterative refinement yields decreasing marginal improvements, with the largest jump at Round 1→2 and diminishing returns thereafter.

## Foundational Learning

- **Concept: Test-time compute scaling**
  - Why needed here: Multi-round Thinking is a test-time strategy; understanding the distinction between training-time and inference-time compute is essential to contextualize why this method requires no additional training.
  - Quick check question: Can you explain why increasing test-time compute does not require gradient updates or model retraining?

- **Concept: Cognitive inertia in sequential reasoning**
  - Why needed here: The paper frames its mechanism as breaking cognitive inertia; understanding path dependency in chain-of-thought helps explain why discarding traces matters.
  - Quick check question: What is the risk of retaining prior reasoning traces when attempting to correct earlier errors?

- **Concept: Pass@1 estimation with stochastic sampling**
  - Why needed here: The evaluation methodology uses multiple samples to estimate pass@1; interpreting results requires understanding this metric.
  - Quick check question: How does pass@1 differ from majority voting (pass@N), and why might it be more sensitive to single-round quality?

## Architecture Onboarding

- **Component map**: Input module -> Round-1 inference -> Answer extraction -> Prompt constructor -> Multi-round loop
- **Critical path**:
  1. Ensure answer extraction correctly isolates final answers (not reasoning traces)
  2. Validate prompt template formatting; malformed concatenation can cause context leakage
  3. Enforce token limits (32,768 max generation length) per round to control latency and cost
- **Design tradeoffs**:
  - Latency vs. accuracy: Each round roughly doubles inference time; diminishing returns after Round 2 may not justify latency in production
  - Temperature selection: T=0.6 used; lower temperatures may reduce diversity across rounds
  - Trace discarding vs. retention: Keeping traces could support cumulative refinement but risks entrenching errors
- **Failure signatures**:
  - Correct→Incorrect flips correlate with increased "wait" usage and longer responses
  - Minimal Round 2 gains when Round 1 accuracy is already near ceiling
  - SFT integration failure: preliminary fine-tuning did not improve Round 2 performance
- **First 3 experiments**:
  1. Replicate Round 1→2 gains on AIME 2024 using exact prompt template and sampling parameters
  2. Ablate trace discarding by including prior reasoning traces in re-prompt; compare accuracy and response length
  3. Measure Round 2 latency and cost overhead in production-like setting; compute accuracy-per-dollar

## Open Questions the Paper Calls Out

- **Question 1**: How can Supervised Fine-tuning (SFT) strategies be adapted to effectively augment Multi-round Thinking, given that preliminary experiments using distilled data failed to yield performance improvements?
  - Basis: Preliminary SFT experiment "did not lead to performance improvements"
  - Evidence needed: A training curriculum that results in higher Round 2 accuracy compared to untrained baseline

- **Question 2**: What computational trade-offs exist between the performance gains of Multi-round Thinking and the latency costs introduced by sequential re-generation in real-time applications?
  - Basis: Conclusion acknowledges "additional waiting time during the thinking phase"
  - Evidence needed: Accuracy-per-second comparison with other test-time scaling methods or early-stopping mechanism

- **Question 3**: What are the failure modes that cause a model to transition from Correct to Incorrect answers during re-thinking, and how can they be predicted?
  - Basis: Analysis shows C-I trajectories with increased "wait" usage and longer responses
  - Evidence needed: Predictive model based on Round 1 features that can flag when multi-round thinking should be skipped

## Limitations
- Answer extraction methodology not specified, creating uncertainty about implementation
- Pass@1 calculation details not provided, affecting statistical robustness assessment
- Generalizability across task types not fully characterized beyond mathematical reasoning

## Confidence
- **High Confidence**: Consistent accuracy improvements across models and benchmarks; diminishing returns pattern; linguistic analysis of confidence markers
- **Medium Confidence**: Cognitive inertia breaking mechanism as primary driver; linguistic changes reflect genuine confidence; practical deployment viability
- **Low Confidence**: Specific thresholds for counterproductive rounds; exact answer extraction methodology; integration with training optimizations

## Next Checks
1. Implement and test multiple answer extraction strategies to determine which reliably isolates final answers without including reasoning traces
2. Evaluate Round 3 and Round 4 performance to precisely characterize the diminishing returns curve and identify predictive task characteristics
3. Test Multi-round Thinking on non-mathematical benchmarks including code generation and open-ended reasoning to assess cross-domain transfer and identify failure patterns