---
ver: rpa2
title: 'Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive
  Benchmark'
arxiv_id: '2504.16427'
source_url: https://arxiv.org/abs/2504.16427
tags:
- llav
- multimodal
- mllms
- language
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MMLA, the first comprehensive benchmark
  for evaluating multimodal large language models (MLLMs) on multimodal language analysis.
  MMLA includes 61K multimodal utterances from staged and real-world scenarios, covering
  six core dimensions: intent, emotion, dialogue act, sentiment, speaking style, and
  communication behavior.'
---

# Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark

## Quick Facts
- arXiv ID: 2504.16427
- Source URL: https://arxiv.org/abs/2504.16427
- Authors: Hanlei Zhang; Zhuohang Li; Yeshuang Zhu; Hua Xu; Peiwu Wang; Haige Zhu; Jie Zhou; Jinchao Zhang
- Reference count: 40
- Primary result: MLLMs achieve 60-70% accuracy on multimodal language tasks after fine-tuning, significantly improving from zero-shot performance but still showing limitations in complex semantic understanding

## Executive Summary
This paper introduces MMLA, the first comprehensive benchmark for evaluating multimodal large language models (MLLMs) on multimodal language analysis. The benchmark includes 61K multimodal utterances covering six semantic dimensions: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. Through extensive evaluation of eight mainstream LLMs and MLLMs using zero-shot inference, supervised fine-tuning, and instruction tuning, the study reveals that while MLLMs significantly improve with fine-tuning (20-40% accuracy gains), they still struggle with complex human language semantics, achieving only 60-70% accuracy even after training.

## Method Summary
The MMLA benchmark evaluates MLLMs on multimodal language analysis tasks using 61K utterances from 9 datasets covering intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. Models are evaluated through zero-shot inference, supervised fine-tuning with LoRA (rank 8-128, α 16-256), and instruction tuning. The training uses cross-entropy loss with BF16 precision, FlashAttention-2, and DeepSpeed ZeRO-3. Data includes text transcriptions aligned with video and audio, with specific hyperparameters for each model scale (7B-72B parameters).

## Key Results
- MLLMs significantly underperform text-only LLMs in zero-shot inference (5-8% lower accuracy), but surpass them after supervised fine-tuning
- LoRA-based fine-tuning enables efficient adaptation, with 7B models achieving performance comparable to 72B models post-training
- Instruction tuning allows unified handling of all six semantic dimensions, though some models experience hallucinations and performance degradation
- Even after fine-tuning, models achieve only 60-70% accuracy, highlighting fundamental limitations in multimodal semantic understanding

## Why This Works (Mechanism)

### Mechanism 1: Supervised Fine-Tuning Activates Latent Cross-Modal Alignment
Applying supervised fine-tuning (SFT) to MLLMs appears to "activate" the connection between visual/acoustic inputs and high-level semantic reasoning. In zero-shot settings, pre-trained alignments may prioritize low-level perceptual features over cognitive semantics. SFT minimizes cross-entropy loss on specific semantic labels, forcing attention mechanisms to weight non-verbal cues more heavily when they correlate with target labels, thereby resolving ambiguities that text-only models cannot. Evidence shows MLLMs achieve 20-40% performance boosts after SFT compared to zero-shot, specifically allowing them to surpass text-only LLMs.

### Mechanism 2: Low-Rank Adaptation (LoRA) Preserves Generalization While Learning Semantics
LoRA allows smaller models (7B-8B parameters) to adapt to complex semantic tasks without catastrophic forgetting or full fine-tuning computational cost. By freezing pre-trained model weights and injecting trainable rank-decomposition matrices into transformer layers, the model adapts existing "world knowledge" to the specific domain of multimodal language analysis. This constraint acts as a regularizer, preventing overfitting to the benchmark's specific distribution. Results show 7B/8B models achieve performance comparable to 72B models after SFT, demonstrating efficient adaptation.

### Mechanism 3: Instruction Tuning Unifies Disparate Semantic Tasks
Instruction tuning enables a single model to handle distinct semantic dimensions by treating them as variations of a prompt-response format rather than isolated classification heads. Training on a mixture of datasets where the input prompt explicitly defines the task forces the model to attend to different visual cues based on the textual instruction, creating a unified semantic space. While IT allows unified model deployment across all six dimensions, some models suffer from hallucinations and performance degradation compared to single-task SFT.

## Foundational Learning

- **Concept: Cross-Entropy Loss in Classification**
  - Why needed here: The paper formulates semantic analysis as a classification problem where the model predicts the next token (the label) based on the input. Understanding the loss function is key to understanding why the model improves with SFT.
  - Quick check question: Can you explain why minimizing cross-entropy loss forces the model to assign higher probability to the correct semantic label given the video and text context?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) / LoRA**
  - Why needed here: The entire experimental framework relies on LoRA to make training 7B-72B models feasible. Without understanding LoRA, the design tradeoffs section is unintelligible.
  - Quick check question: In LoRA, do we modify the pre-trained weights directly, or do we train small adapter matrices alongside them?

- **Concept: Zero-Shot vs. Fine-Tuning Evaluation**
  - Why needed here: The paper's core conclusion distinguishes between "innate" capability (zero-shot) and "trained" capability (SFT/IT). You must distinguish between evaluating what the model knows vs. what it can learn.
  - Quick check question: If a model performs well on Zero-Shot but fails to improve with SFT, what might that indicate about its pre-training data versus its learning capacity?

## Architecture Onboarding

- **Component map:**
  Input Layer: Video/Audio processed by a Vision Encoder (varies by model) → Visual Tokens
  Projector: Maps Visual Tokens to LLM embedding space
  LLM Backbone: Transformer layers containing frozen pre-trained weights
  Adapter: LoRA modules injected into Attention layers to handle gradient updates
  Output: Autoregressive generation of the semantic label string

- **Critical path:**
  1. Data Alignment: ensuring text transcription matches video timestamps (utterance level)
  2. Prompt Construction: Formatting the input string: <Video> <Text> Instruction -> Label
  3. Training Loop: Forward pass → Calculate Cross-Entropy Loss → Backpropagation (LoRA weights only) → Optimizer step

- **Design tradeoffs:**
  - Model Scale (72B vs. 7B): 72B offers better zero-shot reasoning and handles complex instructions better but requires significantly more VRAM. 7B models, once fine-tuned, offer equivalent performance on these specific tasks at a fraction of the inference cost.
  - SFT vs. IT: SFT yields maximum performance on a single task. IT yields slightly lower performance but allows a single model deployment to handle all 6 dimensions.

- **Failure signatures:**
  - Modality Collapse: MLLM performance drops to LLM (text-only) levels, indicating the visual adapter failed to learn or the model learned to ignore visual tokens
  - Hallucination: Post-IT, the model generates text that is not in the candidate label list
  - Overfitting: High performance on validation sets but poor performance on distinct datasets if domain adaptation fails

- **First 3 experiments:**
  1. Establish Baseline: Run Zero-Shot inference on MELD (Emotion) using Qwen2-7B (Text) vs. Qwen2-VL-7B (Multimodal). Verify if text-only actually outperforms multimodal.
  2. SFT Adaptation: Train VideoLLaMA2-7B using LoRA (Rank=128, Alpha=256) on the MIntRec dataset. Observe the 30%+ accuracy jump.
  3. Instruction Tuning Stress Test: Train a model on a mixed dataset of Sarcasm (MUStARD) and Sentiment (MOSI). Verify if the model can switch tasks based on the prompt without performance degradation compared to single-task SFT.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do current MLLMs underperform text-only LLMs in zero-shot inference on multimodal language tasks?
- Basis in paper: [explicit] The authors observe that "existing MLLMs have significant limitations in leveraging non-verbal information" and often lose to text-only models by 5–8% without supervision.
- Why unresolved: It is unclear if the failure stems from inadequate vision encoders, poor cross-modal alignment, or attention mechanisms prioritizing text tokens over visual features during zero-shot reasoning.
- What evidence would resolve it: An ablation study analyzing attention weights assigned to visual tokens versus text tokens during zero-shot inference.

### Open Question 2
- Question: Does parameter scaling provide diminishing returns for high-level semantic tasks once models are supervised fine-tuned?
- Basis in paper: [explicit] The paper concludes that "simply enlarging model parameters provides little benefit" for complex semantics after SFT, with 72B models gaining only ~1% over 7B models on some dimensions.
- Why unresolved: The saturation point where model scale ceases to offset data complexity or architectural limitations remains undefined for cognitive-level semantics.
- What evidence would resolve it: A scaling law analysis specifically plotting parameter count against SFT accuracy on the "Communication Behavior" and "Intent" dimensions.

### Open Question 3
- Question: How can instruction tuning be stabilized to prevent performance degradation and hallucinations in specific MLLM architectures?
- Basis in paper: [inferred] The authors note that while IT works for some models, others like MiniCPM-V-2.6 suffered a "pronounced decline" and "severe hallucinations" compared to SFT.
- Why unresolved: The mechanism causing catastrophic forgetting or interference during multi-task training on the six semantic dimensions is not identified.
- What evidence would resolve it: A study comparing catastrophic forgetting rates across different MLLM architectures when trained on the full MMLA dataset simultaneously.

## Limitations

- Dataset Coverage and Representativeness: Several key multimodal language datasets have restricted video redistribution rights, potentially affecting reproducibility and validation of real-world performance claims.
- Generalization Across Semantic Dimensions: The 60-70% accuracy figure aggregates across six distinct dimensions, but variation between tasks is not fully explored, leaving uncertainty about whether certain tasks are inherently harder.
- Model Architecture Variability: The evaluation spans models with vastly different architectures, making it difficult to attribute performance differences to model capability versus architectural choices.

## Confidence

- **High Confidence**: The core finding that MLLMs significantly outperform text-only LLMs after supervised fine-tuning (20-40% accuracy improvements) is well-supported across multiple models and datasets.
- **Medium Confidence**: The claim that LoRA enables efficient adaptation while preserving generalization is supported by 7B models achieving comparable performance to 72B models post-fine-tuning, though optimal configuration parameters are not fully explained.
- **Low Confidence**: The assertion that instruction tuning can unify disparate semantic tasks is based on limited direct comparison, with sparse ablation studies comparing instruction-tuned versus single-task models.

## Next Checks

1. **Cross-Dataset Generalization Test**: Train an MLLM on MIntRec and evaluate on held-out datasets (MELD, MOSI) to verify whether the 60-70% accuracy represents true semantic understanding or dataset-specific memorization.

2. **Modality Ablation Study**: Systematically remove either video or audio from inputs during inference on the same model to quantify the exact contribution of each modality to final accuracy.

3. **Prompt Sensitivity Analysis**: Vary the instruction prompt format across multiple models to determine how sensitive performance is to prompt engineering and identify optimal prompt structures for each semantic dimension.