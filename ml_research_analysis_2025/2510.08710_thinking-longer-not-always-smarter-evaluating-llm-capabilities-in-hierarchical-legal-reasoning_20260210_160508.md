---
ver: rpa2
title: 'Thinking Longer, Not Always Smarter: Evaluating LLM Capabilities in Hierarchical
  Legal Reasoning'
arxiv_id: '2510.08710'
source_url: https://arxiv.org/abs/2510.08710
tags:
- reasoning
- legal
- task
- case
- plaintiff
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates how well Large Language Models (LLMs) can
  perform hierarchical legal reasoning, specifically identifying significant distinctions
  between legal cases. The authors introduce a three-stage framework that decomposes
  the process of identifying significant distinctions: (1) identifying factual differences
  between cases, (2) analyzing the argumentative roles of those differences using
  a hierarchical legal knowledge structure, and (3) determining which distinctions
  are legally significant.'
---

# Thinking Longer, Not Always Smarter: Evaluating LLM Capabilities in Hierarchical Legal Reasoning

## Quick Facts
- **arXiv ID**: 2510.08710
- **Source URL**: https://arxiv.org/abs/2510.08710
- **Reference count**: 40
- **Primary result**: Models achieve perfect accuracy on simple pattern recognition but performance degrades substantially on hierarchical reasoning (64.82%-92.09%) and integrated analysis (11.46%-33.99%), with models using more computational resources on incorrect responses

## Executive Summary
This paper evaluates how well Large Language Models (LLMs) can perform hierarchical legal reasoning, specifically identifying significant distinctions between legal cases. The authors introduce a three-stage framework that decomposes the process of identifying significant distinctions: (1) identifying factual differences between cases, (2) analyzing the argumentative roles of those differences using a hierarchical legal knowledge structure, and (3) determining which distinctions are legally significant. The evaluation reveals a striking paradox: while models achieve perfect accuracy on simple pattern recognition, performance degrades substantially on hierarchical reasoning and collapses on integrated analysis. Most notably, models consistently use more computational resources on incorrect responses than correct ones, suggesting that increased reasoning effort does not necessarily improve outcomes. This finding challenges assumptions about LLM reasoning capabilities and has important implications for deploying AI in legal contexts where accurate, reliable reasoning is critical.

## Method Summary
The authors developed a three-stage decomposition framework to evaluate LLM performance on hierarchical legal reasoning tasks. They created a dataset of legal case pairs with annotated factual differences, hierarchical legal knowledge structures, and ground truth determinations of legal significance. The evaluation tested multiple LLMs across three tasks: identifying factual differences (Task 1), analyzing argumentative roles within a hierarchical knowledge structure (Task 2), and integrating both to determine legal significance (Task 3). Performance was measured using standard accuracy metrics, and computational resource usage was tracked through token counts and inference time measurements to assess whether increased reasoning effort correlated with better outcomes.

## Key Results
- Perfect accuracy (100%) on Task 1 (identifying factual differences between cases)
- Substantial performance degradation on Task 2 (64.82%-92.09% accuracy on hierarchical reasoning)
- Collapse in performance on Task 3 (11.46%-33.99% accuracy on integrated analysis)
- Models consistently use more computational resources on incorrect responses than correct ones
- Performance varies significantly across different LLM models and prompt engineering approaches

## Why This Works (Mechanism)
The three-stage decomposition framework works by breaking down the complex task of hierarchical legal reasoning into manageable subtasks that can be evaluated independently. By isolating each component of the reasoning process, the framework reveals where LLMs succeed and fail, rather than masking these differences with aggregate performance metrics. The hierarchical legal knowledge structure provides a scaffold that should theoretically guide reasoning, but the significant performance drop between stages suggests models struggle to effectively integrate and apply this structure. The resource usage analysis mechanism works by measuring computational effort (tokens and time) across all responses, revealing the counterintuitive finding that more effort does not correlate with better accuracy, which suggests fundamental limitations in how models approach complex reasoning tasks.

## Foundational Learning
- **Hierarchical legal knowledge structures**: These are organized frameworks that represent legal concepts and their relationships, needed to understand how legal arguments build upon and distinguish from previous cases; quick check: can the model correctly navigate parent-child relationships in the knowledge tree
- **Argumentative roles of legal distinctions**: Understanding how specific factual differences serve different purposes in legal arguments (e.g., distinguishing precedent, establishing new principles); quick check: can the model classify differences according to their argumentative function
- **Integrated legal reasoning**: The ability to synthesize factual analysis with legal principles to determine significance; quick check: does the model's final determination align with legal expert consensus
- **Computational resource measurement in LLMs**: Techniques for quantifying reasoning effort through token usage and inference time; quick check: does resource usage vary predictably with task complexity
- **Legal case analysis methodology**: The process of comparing cases to identify relevant similarities and differences; quick check: can the model systematically identify all material factual differences
- **Pattern recognition vs. reasoning distinction**: Understanding the difference between surface-level matching and deep logical inference; quick check: does perfect performance on simple tasks predict success on complex reasoning

## Architecture Onboarding

Component map: Task 1 (Fact Identification) -> Task 2 (Hierarchical Analysis) -> Task 3 (Integrated Significance)

Critical path: Legal Knowledge Structure -> Fact Difference Extraction -> Argumentative Role Analysis -> Significance Determination

Design tradeoffs: The framework trades off comprehensive evaluation for tractability by decomposing complex reasoning into discrete stages, but this decomposition may not fully capture the fluid nature of actual legal reasoning where stages often overlap and inform each other simultaneously.

Failure signatures: Performance collapse between stages (perfect to poor), increased resource usage on incorrect answers, inconsistent application of hierarchical knowledge, and failure to integrate factual and legal analysis coherently.

First experiments:
1. Test a single model on all three tasks to establish baseline performance degradation patterns
2. Compare resource usage between correct and incorrect responses within each task
3. Analyze error patterns to determine if failures are systematic or random

## Open Questions the Paper Calls Out
The paper identifies several open questions that emerge from the findings. First, why do models use more computational resources on incorrect responses, and what does this reveal about their reasoning processes? Second, can the performance degradation pattern be attributed to limitations in the models themselves versus limitations in how legal knowledge is structured and presented? Third, what modifications to the hierarchical knowledge structure or prompting strategies might improve integrated reasoning performance? The paper also raises questions about whether the observed patterns generalize beyond the specific legal domain tested and whether similar decomposition approaches could reveal analogous limitations in other complex reasoning domains.

## Limitations
- The three-stage decomposition may not fully capture the integrated nature of real-world legal reasoning
- Performance metrics are based on a specific legal domain, limiting generalizability
- Resource usage measurements may be influenced by implementation-specific factors
- The evaluation framework only considers a limited set of LLMs and prompt engineering techniques
- The dataset size and diversity may not be sufficient to draw definitive conclusions about model capabilities

## Confidence
- Finding: Models use more computational resources on incorrect responses than correct ones -> **High confidence** (compelling finding but requires validation of measurement methodology)
- Finding: Performance degrades substantially from Task 1 to Task 3 -> **Medium confidence** (substantial effect but may be dataset-specific)
- Finding: Three-stage decomposition reveals limitations in LLM reasoning -> **Medium confidence** (theoretically sound but limited generalizability)

## Next Checks
1. Test the same framework across multiple legal domains and case types to assess generalizability of the performance degradation pattern
2. Implement alternative computational effort measurement methods (e.g., tracking intermediate reasoning steps, attention patterns) to validate the resource usage findings
3. Conduct ablation studies by varying the complexity and specificity of legal knowledge structures to determine which aspects most impact performance