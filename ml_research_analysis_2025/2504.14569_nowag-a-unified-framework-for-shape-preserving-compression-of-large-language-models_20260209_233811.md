---
ver: rpa2
title: 'NoWag: A Unified Framework for Shape Preserving Compression of Large Language
  Models'
arxiv_id: '2504.14569'
source_url: https://arxiv.org/abs/2504.14569
tags:
- pruning
- nowag-p
- nowag-vq
- quantization
- wanda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NoWag is a unified framework for one-shot shape-preserving compression
  of large language models using normalized weights and activation-guided importance
  scoring. The method applies independent compression to each weight matrix, normalizing
  with row and column vectors to remove outlier structures and enable effective quantization
  and pruning.
---

# NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models

## Quick Facts
- **arXiv ID**: 2504.14569
- **Source URL**: https://arxiv.org/abs/2504.14569
- **Reference count**: 40
- **Primary result**: NoWag-VQ achieves lower perplexity than state-of-the-art one-shot vector quantization methods while using 48× less calibration data

## Executive Summary
NoWag is a unified framework for one-shot shape-preserving compression of large language models using normalized weights and activation-guided importance scoring. The method applies independent compression to each weight matrix, normalizing with row and column vectors to remove outlier structures and enable effective quantization and pruning. Applied to Llama-2 (7B, 13B, 70B) and Llama-3 (8B, 70B) models, NoWag-VQ achieves lower perplexity than state-of-the-art one-shot vector quantization methods while using 48× less calibration data, and NoWag-P matches or outperforms leading pruning techniques. The approach highlights commonalities between pruning and quantization, suggesting new research directions.

## Method Summary
NoWag operates by first normalizing each weight matrix W through column normalization (dividing by column norms r^(1)) followed by row normalization (dividing by row norms r^(2)), removing structured outlier patterns. For quantization, it applies weighted K-means to d consecutive normalized weights, using activation-derived importance weights ||X_j||² for each input channel. For pruning, it scores elements by W̄²_ij · ||X_j||² and removes the lowest-scoring entries. Both approaches leverage the same normalization framework and importance weighting derived from diagonal Hessian approximation.

## Key Results
- NoWag-VQ at ~2 bits achieves lower perplexity than state-of-the-art one-shot VQ methods (QuIP#) on WikiText2 and C4 across Llama-2 and Llama-3 models
- NoWag-P matches or outperforms leading pruning techniques including Wanda and GPTQ across multiple Llama models
- The framework uses only 128 calibration samples versus 6144 for competing methods, demonstrating 48× reduction in calibration data requirements
- Normalization effectively removes outlier structures, reshaping weight distributions into bounded ball-shaped distributions suitable for compression

## Why This Works (Mechanism)

### Mechanism 1: Double Normalization Removes Outlier Structure
Normalizing weight matrices along both row and column dimensions removes structured outlier patterns that otherwise bias compression algorithms toward low-magnitude channels. This projects weight distributions into a bounded [0,1] "ball-shaped" distribution suitable for vector quantization.

### Mechanism 2: Diagonal Hessian Enables Decomposable Optimization
Using only the diagonal of the sample Hessian as importance weights allows the compression objective to decompose into independent element-level subproblems, avoiding expensive matrix inversion required by full Hessian approaches.

### Mechanism 3: Weighted K-Means Without Feedback Updates
Vector quantization via weighted K-means, using activation-derived importance weights, achieves competitive compression without iterative weight updates, significantly reducing computational complexity from O(d³) to linear scaling.

## Foundational Learning

- **Vector Quantization (VQ)**: Jointly quantizes d consecutive weights into subvectors using learned codebooks; needed for efficient low-bitrate compression; quick check: Can you explain why VQ outperforms scalar quantization when weight distributions are bounded but non-uniform?

- **Hessian-based Compression Objectives**: Minimizes expected output error using second-order information; needed to derive importance weights for compression decisions; quick check: Why does the full Hessian require O(d³) operations, and what tradeoff does the diagonal approximation make?

- **N:M Semi-Structured Pruning**: Prunes N of every M elements in fixed patterns (e.g., 2:4, 4:8); needed for hardware-efficient sparse inference; quick check: For 2:4 sparsity on a 4096-element row, how many values must be non-zero, and what constraint does this impose on the pruning mask?

## Architecture Onboarding

- **Component map**: NoWag Normalization (Algo 1) -> NoWag-VQ (Algo 3) OR NoWag-P (Algo 5) -> Inference (Algo 2)
- **Critical path**: K-means iterations (default T=100) dominate VQ runtime; quickselect threshold finding is O(d_in·d_out) for pruning
- **Design tradeoffs**: Subvector dimension d (larger d allows richer codebooks but may not fit in GPU L1 cache); K-means iterations (fewer iterations leave performance on table); calibration samples (diminishing returns beyond 128)
- **Failure signatures**: High perplexity on Llama-3 vs Llama-2 at same bitrate (Llama-3 is harder to quantize); 2:4 semi-structured pruning matches but doesn't beat Wanda (rigid structure limits normalization benefit); fine-tuned Llama-2 70B underperforms AQLM (smaller codebook reduces capacity)
- **First 3 experiments**: 1) Run NoWag-VQ with T=1 iteration on Llama-2-7B at 2 bits; should achieve WikiText2 perplexity ~7.3, outperforming QuIP#'s 8.23; 2) Compare row-only, column-only, and row+column normalization on a single layer; row+column should yield lowest weighted reconstruction error; 3) Evaluate with 32, 64, 128, and 256 RedPajama samples; expect diminishing returns beyond 128 but graceful degradation below it

## Open Questions the Paper Calls Out

### Open Question 1
Does increasing K-means iterations beyond 100 yield diminishing returns or continued perplexity improvements for NoWag-VQ? The authors observed performance scaling from 20 to 100 iterations but resource constraints limited further exploration.

### Open Question 2
Would NoWag-VQ match AQLM on Llama-2 70B if using d=8 subvectors instead of d=7? The paper suspects the smaller codebook (d=7 vs d=8) explains the performance gap versus AQLM.

### Open Question 3
Can the remaining column-wise structure in importance scores after normalization be further reduced to improve compression? The paper notes some column-wise structure remains after normalization but did not explore methods to address this.

## Limitations

- One-shot compression without parameter updates may require fine-tuning at extreme compression rates below 2 bits, undermining the one-shot advantage
- The diagonal Hessian approximation may miss critical cross-channel dependencies in certain attention mechanisms or specialized layers
- Normalization may not be universally beneficial for architectures where row/column magnitude differences encode meaningful information

## Confidence

- **High confidence**: Double normalization effectively creates bounded, ball-shaped weight distributions suitable for quantization
- **Medium confidence**: Diagonal Hessian approximation's sufficiency for importance scoring lacks rigorous theoretical justification across all architectural variants
- **Medium confidence**: One-shot quantization approach's competitiveness with iterative methods may degrade at extreme compression levels without fine-tuning

## Next Checks

1. **Cross-architecture robustness test**: Apply NoWag normalization to a specialized model (e.g., MoE or mixture-of-experts) where row/column magnitudes encode expert routing importance, then measure perplexity degradation compared to baseline.

2. **Calibration sample sensitivity analysis**: Systematically vary the number of calibration samples (16, 32, 64, 128, 256, 512) and measure the resulting perplexity degradation across all evaluated models to establish the minimum viable calibration set size.

3. **Extreme compression benchmark**: Evaluate NoWag-VQ at 1 bit and 1.5 bits with and without fine-tuning on Llama-2-7B, comparing against state-of-the-art iterative quantization methods to quantify the one-shot approach's limitations at extreme compression rates.