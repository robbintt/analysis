---
ver: rpa2
title: 'Locally Optimal Private Sampling: Beyond the Global Minimax'
arxiv_id: '2510.09485'
source_url: https://arxiv.org/abs/2510.09485
tags:
- sampler
- local
- distribution
- global
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sampling from a distribution
  under local differential privacy (LDP), aiming to generate samples that remain close
  to the original distribution in f-divergence while satisfying LDP constraints. The
  authors move beyond the global minimax framework to develop a local minimax formulation,
  which considers a neighborhood around a fixed distribution rather than the entire
  distribution class.
---

# Locally Optimal Private Sampling: Beyond the Global Minimax

## Quick Facts
- arXiv ID: 2510.09485
- Source URL: https://arxiv.org/abs/2510.09485
- Reference count: 40
- Primary result: Local minimax samplers consistently outperform global minimax samplers across privacy regimes by exploiting neighborhood structure around reference distributions

## Executive Summary
This paper addresses the problem of sampling from a distribution under local differential privacy (LDP) constraints while minimizing f-divergence to the original distribution. The authors develop a local minimax formulation that considers a neighborhood around a fixed reference distribution rather than the entire distribution class. They extend previous work on pure LDP to the more general functional LDP framework and derive closed-form expressions for locally minimax-optimal samplers. Numerical experiments demonstrate that local samplers achieve substantially lower worst-case f-divergence compared to existing global methods, particularly for moderate privacy budgets.

## Method Summary
The method extends pure LDP to functional LDP using trade-off functions, then shows that the globally optimal functional LDP sampler yields the optimal local sampler when constrained to distributions near a fixed reference distribution P₀. The approach involves computing optimal bounds b = (γ+1)/(γ+e^ε) for pure LDP, checking if input P is in the neighborhood N_γ(P₀) via likelihood ratio verification, and applying either a linear sampler (Q(P) = λP + (1-λ)P₀) or a non-linear clipping sampler (q(x) = clip(p(x)/r_P; bounds)). The normalization constant r_P ensures validity and may require numerical integration in continuous domains.

## Key Results
- Local minimax samplers consistently outperform global minimax samplers across all privacy regimes tested
- Closed-form expressions for locally minimax-optimal samplers are derived that are independent of the choice of f-divergence
- The improvement is particularly pronounced for moderate privacy budgets (ε ∈ {0.5, 1, 2})
- Empirical validation on both finite domains (k=20) and continuous 1D Laplace mixtures shows substantial utility gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Local minimax formulation achieves lower f-divergence than global minimax by exploiting neighborhood structure around a reference distribution P₀.
- **Mechanism:** The neighborhood N_γ(P₀) constrains worst-case analysis to distributions with bounded likelihood ratios [1/γ, γ], reducing the adversarial search space and enabling instance-specific optimization rather than guarding against all possible degenerate distributions.
- **Core assumption:** Real-world data distributions lie within bounded neighborhoods of reference distributions rather than being worst-case degenerate distributions.
- **Evidence anchors:**
  - [abstract] "we demonstrate that it consistently outperforms global minimax samplers"
  - [Section 4, Equation 9] N_γ(P₀) definition via E_γ-divergence
  - [Section 6] Empirical comparisons across finite and continuous domains
- **Break condition:** If γ is too large (weak neighborhood constraint), the local formulation approaches global minimax and performance gains diminish.

### Mechanism 2
- **Claim:** Functional LDP framework unifies pure ε-LDP, (ε,δ)-LDP, and Gaussian LDP under a single theoretical structure using trade-off functions.
- **Mechanism:** Trade-off functions T(P,Q) characterize hypothesis testing difficulty between output distributions; a sampler is g-FLDP if T(Q(·|P), Q(·|P′))(u) ≥ g(u) for all input pairs, where g is the privacy trade-off function. This enables derivation of optimal samplers that work across privacy notions.
- **Core assumption:** Privacy can be characterized through the lens of binary hypothesis testing between output distributions.
- **Evidence anchors:**
  - [abstract] "extend previous work from pure LDP to the more general functional LDP framework"
  - [Section 3, Theorem 3.4] Global minimax sampler under g-FLDP with closed-form λ*
  - [Corollary 3.7-3.8] Specialization to pure LDP and ν-GLDP
- **Break condition:** Assumption: Computational complexity of evaluating convex conjugates g* may limit practical deployment for exotic trade-off functions.

### Mechanism 3
- **Claim:** The nonlinear clipping sampler (Theorem 5.1) achieves instance-optimality: for each input P, it minimizes Df(P||Q) over all ε-LDP admissible Q.
- **Mechanism:** Project P onto a "mollifier" M_ε via clipping operation clip(p(x)/(r_P·p₀(x)); bounds) that enforces likelihood ratio constraints while minimizing distortion. The normalization constant r_P ensures validity.
- **Core assumption:** P ∈ N_γ(P₀), meaning the input distribution's likelihood ratio with respect to P₀ is bounded.
- **Evidence anchors:**
  - [Section 5, Theorem 5.1] Clipping sampler definition with explicit bounds
  - [Proposition 5.2] Pointwise comparison showing Df(P||Q*_ε) ≤ Df(P||Q*_g_ε)
  - [Section 6.2, Figure 4] Empirical validation on 100 Laplace mixtures
- **Break condition:** If P ∉ N_γ(P₀), the sampler falls back to Q*(ε)(P̂) where P̂ is the projection onto the neighborhood—this projection step requires solving an optimization problem.

## Foundational Learning

- **f-divergences (Df(P||Q))**
  - Why needed here: The paper optimizes samplers to minimize f-divergence between original and privatized distributions; results must hold for any f-divergence (KL, TV, Hellinger).
  - Quick check question: Can you explain why the hockey-stick divergence E_γ defines the neighborhood N_γ(P₀)?

- **Minimax risk and game-theoretic optimization**
  - Why needed here: The core contribution reframes sampling from global worst-case (over all P ∈ P̃) to local worst-case (over P ∈ N_γ(P₀)).
  - Quick check question: Why does restricting to a neighborhood reduce minimax risk?

- **Radon-Nikodym derivatives and likelihood ratios**
  - Why needed here: Both privacy constraints (ε-LDP) and neighborhood definitions (E_γ-divergence) are expressed via bounded likelihood ratios dP/dP₀.
  - Quick check question: What does it mean for dQ/dP₀ to be bounded in [b, b·e^ε]?

## Architecture Onboarding

- **Component map:** Reference distribution P₀ -> Neighborhood parameter γ -> Privacy trade-off function g -> Optimal sampler Q* -> Projection operator (if P ∉ N_γ(P₀))

- **Critical path:**
  1. Given P₀, γ, and g, compute optimal bounds b = (γ+1)/(γ+e^ε) for pure LDP
  2. For input P, check if P ∈ N_γ(P₀) via likelihood ratio verification
  3. If yes: apply clipping sampler with normalization; if no: project to neighborhood first
  4. Evaluate f-divergence between P and Q*(P) for quality assessment

- **Design tradeoffs:**
  - Linear sampler (Theorem 4.1): Simpler, uses fixed λ* mixing; less instance-optimal but easier to implement
  - Nonlinear sampler (Theorem 5.1): Instance-optimal via clipping; requires computing normalization constant r_P which may need numerical integration in continuous domains
  - Smaller γ: Tighter neighborhood, better utility, but risk of P falling outside neighborhood

- **Failure signatures:**
  - Negative f-divergence values: Normalization constant r_P computed incorrectly
  - Privacy violation (ε exceeded): Clipping bounds implemented wrong or likelihood ratio exceeds γ
  - Poor empirical performance vs theoretical: γ too large, or P consistently outside N_γ(P₀)

- **First 3 experiments:**
  1. Reproduce Figure 3 (finite domain k=20): Compare theoretical local vs global minimax risk; verify closed-form expressions match empirical worst-case over uniform neighborhood
  2. Continuous 1D Laplace mixture (Figure 4): Generate 100 random P_j ∈ P̃_local, compute max_j Df(P_j||Q(P_j)) for both samplers; validate local advantage across ε ∈ {0.1, 0.5, 1, 2}
  3. Boundary test: Construct P at the edge of N_γ(P₀) (likelihood ratio = γ at some x) and verify sampler handles this case without degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational complexity of the locally minimax-optimal sampler, specifically the clipping function, be reduced to handle high-dimensional, real-world data?
- Basis in paper: [explicit] Section 7 states that the "computational complexity of our sampler (in particular the clipping function), which poses challenges for scalability in high-dimensional settings. Addressing this limitation... is an important direction for future work."
- Why unresolved: The paper's experiments are limited to synthetic data or low-dimensional settings (1-D and 2-D Laplace mixtures) due to these computational costs.
- What evidence would resolve it: An efficient implementation or approximation (e.g., using MCMC) that scales to dimensions $n \gg 2$ while maintaining the utility guarantees.

### Open Question 2
- Question: Can the local minimax framework be generalized to neighborhoods defined by arbitrary $f$-divergences (e.g., KL divergence) rather than being restricted to $E_\gamma$-divergence?
- Basis in paper: [explicit] Section 7 notes that the current neighborhood $N_\gamma(P_0)$ is defined via $E_\gamma$-divergence and suggests "A natural extension is $N_{\gamma,\zeta}(P_0)$... [which] can be extended to neighborhoods based on general $f$-divergences... We leave this generalization for future work."
- Why unresolved: The theoretical results rely specifically on the properties of the $E_\gamma$-divergence to characterize the local neighborhood.
- What evidence would resolve it: A derivation of the local minimax risk and optimal sampler forms for neighborhoods defined by constraints such as $D_f(P\|P_0) \leq \zeta$.

### Open Question 3
- Question: How does the local minimax risk and optimal sampler formulation change when extending the framework from single-sample to multi-sample release per client?
- Basis in paper: [explicit] Section 7 states: "Furthermore, this work focuses on the setting where each client releases a single sample. Extending to the case of multiple samples per client is a natural direction for future work."
- Why unresolved: The current theoretical analysis and optimality proofs (Theorems 4.1 and 5.1) are constructed specifically for generating a single private sample $Q(P)$.
- What evidence would resolve it: A characterization of the minimax risk for releasing $n$ samples and a sampler construction that achieves this bound.

### Open Question 4
- Question: Can the technical assumption that $\frac{c_2-c_1}{1-c_1}$ must be a natural number be relaxed to allow for exact minimax optimality over any bounded distribution class?
- Basis in paper: [inferred] Assumption 3.2 and the surrounding text note that if the ratio is not a natural number, one must adjust the bounds to a "superset of the original distribution class" to apply the decomposability argument, implying the exact risk for the precise original class is not fully characterized in these cases.
- Why unresolved: The lower bound proof relies on the $(\alpha, 1/\alpha, 1)$-decomposability of the reference measure, which requires $\alpha$ to have a specific relationship to the class boundaries that implies an integer constraint.
- What evidence would resolve it: A theoretical proof of the exact minimax risk that does not rely on the integer assumption, or a closed-form solution for the non-integer case.

## Limitations
- Computational scalability issues for high-dimensional data due to numerical integration requirements in the clipping sampler
- Heavy reliance on technical assumptions (decomposability condition, ε < 2log(γ)) that may not hold for real-world data
- Limited guidance on selecting the neighborhood parameter γ in practice

## Confidence
- **High Confidence**: Theoretical results showing characterization of locally minimax-optimal samplers appear mathematically rigorous
- **Medium Confidence**: Empirical validation demonstrates claimed advantages but limited to synthetic data
- **Low Confidence**: Practical implementation details for critical components like convex conjugate optimization are not provided

## Next Checks
1. **Real-world Data Test**: Apply the local minimax sampler to a real-world dataset (e.g., US Census or medical records) with a naturally occurring reference distribution, measuring actual f-divergence performance across privacy regimes.

2. **Robustness to Assumption Violations**: Systematically test performance degradation when violating key assumptions (ε ≥ 2log(γ), non-integer (c₂-c₁)/(1-c₁), P consistently outside N_γ(P₀)) to quantify practical limitations.

3. **Computational Scaling Analysis**: Benchmark the runtime and memory requirements of the clipping sampler on increasing domain sizes (k from 20 to 1000) and mixture component counts (K from 10 to 100) to assess practical scalability.