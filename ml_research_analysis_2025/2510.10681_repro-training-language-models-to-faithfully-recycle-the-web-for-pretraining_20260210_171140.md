---
ver: rpa2
title: 'RePro: Training Language Models to Faithfully Recycle the Web for Pretraining'
arxiv_id: '2510.10681'
source_url: https://arxiv.org/abs/2510.10681
tags:
- data
- organic
- text
- pretraining
- rephraser
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RePro trains a 4B language model with reinforcement learning to
  generate high-quality, faithful rephrasings of web data for LLM pretraining. Using
  one quality and three faithfulness rewards, the rephraser learns to improve data
  quality while preserving semantic meaning, structure, and length of organic data.
---

# RePro: Training Language Models to Faithfully Recycle the Web for Pretraining

## Quick Facts
- arXiv ID: 2510.10681
- Source URL: https://arxiv.org/abs/2510.10681
- Authors: Zichun Yu; Chenyan Xiong
- Reference count: 40
- Key outcome: RePro achieves 4.7%-14.0% relative accuracy gains over organic-only baseline on 22 downstream tasks using a 4B rephraser

## Executive Summary
RePro introduces a reinforcement learning approach to train a 4B language model rephraser that converts moderate-quality web data into high-quality, faithful rephrasings for LLM pretraining. The method uses one quality reward (DataMan) and three faithfulness rewards (BERTScore, structure preservation, length constraints) optimized jointly via GRPO. Experiments show RePro outperforms state-of-the-art methods using a 17× smaller rephraser and improves organic data efficiency by 2-3×. Distributional analyses confirm the rephraser faithfully preserves organic data characteristics while applying diverse rephrasing operations.

## Method Summary
RePro trains a 4B language model rephraser using reinforcement learning to convert organic web data into high-quality, faithful rephrasings. The rephraser is trained on 41k low-quality samples using GRPO with four rewards: DataMan for quality, BERTScore for semantic faithfulness (≥0.65), structure preservation via few-shot LM classification, and length constraints (≤1.25× original). The trained rephraser processes 72B organic tokens, and both original and recycled data are filtered using DCLM-fastText quality classifier. The final pretraining corpus combines high-quality portions from both pools, enabling efficient use of moderate-quality web data.

## Key Results
- 4.7%-14.0% relative accuracy gains over organic-only baseline on 22 downstream tasks
- Outperforms ReWire despite using a 17× smaller rephraser (4B vs 70B)
- Improves organic data efficiency by 2-3× for 400M and 1.4B models
- Faithfulness rewards prevent distributional drift while enabling quality improvements

## Why This Works (Mechanism)

### Mechanism 1
RL training with combined quality and faithfulness rewards produces a more effective rephraser than prompting large LLMs. The GRPO algorithm optimizes a 4B rephraser against four reward signals simultaneously, where quality reward pushes toward higher-quality outputs while three faithfulness rewards constrain preservation of semantic and structural characteristics.

### Mechanism 2
Faithfulness rewards preserve the distributional characteristics of organic data, mitigating model collapse risk. BERTScore enforces semantic overlap, structure reward matches formatting, and length reward bounds generation length, constraining output distribution to match organic input distribution.

### Mechanism 3
Recycling moderate-quality web data into high-quality data, then combining with organic high-quality data, improves data efficiency 2-3×. The pipeline samples from organic data, rephrases all samples, then filters both organic and recycled pools using a quality classifier, effectively "upcycling" moderate-quality data.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: RL algorithm that trains the rephraser by normalizing advantages within groups of generated outputs. Why needed: improves stability for language generation tasks. Quick check: If you set group size n=1, what happens to the advantage calculation in Equation 11?

- **BERTScore for semantic similarity**: Measures token-level embedding overlap between original and rephrased text. Why needed: primary faithfulness metric balancing precision and recall. Quick check: Why might BERTScore be insufficient as sole faithfulness metric? What structural properties does it miss?

- **Quality filtering vs. quality as reward**: Paper uses DataMan as reward during RL but DCLM-fastText as quality filter for data selection. Why needed: metric good for selection may be exploitable as reward. Quick check: If you used fastText as both reward and filter, what failure mode would you expect?

## Architecture Onboarding

- **Component map**: D_org (72B tokens) → R_rl rephrases all → D_rec (72B recycled tokens) → Both filtered by DCLM-fastText → D_org-hq + D_rec-hq → Final pretraining corpus D_final

- **Critical path**: 1) Reward function design (λ weights), 2) RL training stability (monitor 4 reward curves), 3) Quality threshold selection (τ_org and τ_rec), 4) Distributional validation (Figure 6-style analyses)

- **Design tradeoffs**: 4B vs larger rephraser size (17× smaller than ReWire), λ_DataMan=3 vs λ_others=1 (quality vs faithfulness), τ_BERTScore=0.65 (semantic drift vs improvement room), SFT warmup (knowledge vs generalization penalty)

- **Failure signatures**: 1) Reward hacking (DataMan high but BERTScore/structure low), 2) Distribution collapse (BERTScore < 0.6, structure skews to plain text), 3) Copy behavior (BERTScore ≈ 1.0, DataMan ≈ 0), 4) Length explosion/contraction (distribution diverges from organic)

- **First 3 experiments**: 1) Reward ablation (remove each reward individually, measure downstream Core score impact), 2) Threshold sensitivity (vary τ_BERTScore and λ_DataMan, identify Pareto frontier), 3) Scale validation (train 1B rephraser on 7B tokens subset before full 4B×72B experiment)

## Open Questions the Paper Calls Out

1. **Diverse reward signals**: Can integrating more diverse and verifiable reward signals, such as discrete checklists, further improve the quality and faithfulness of recycled data compared to current soft rewards?

2. **Rephraser scaling**: Does scaling the rephraser model size beyond 4B parameters yield continued performance gains, or does it encounter diminishing returns?

3. **SFT modification**: Can the supervised fine-tuning warm-up stage be modified to avoid the observed generalization penalty while retaining knowledge gains?

## Limitations

- Reliance on proxy metrics without human evaluation of final pretraining data quality and faithfulness
- 4B rephraser size untested for smaller models (1B-2B) that might achieve similar results
- Downstream evaluation uses small models (400M, 1.4B) that may not capture benefits for larger models
- Quality filtering susceptibility to reward hacking when DCLM-fastText used as reward

## Confidence

**High Confidence**: RePro achieves 4.7%-14.0% relative accuracy gains; outperforms ReWire with 17× smaller rephraser; distributional analyses confirm faithful preservation

**Medium Confidence**: Four rewards can be jointly optimized; quality filtering with DCLM-fastText produces meaningful improvements; 2-3× efficiency gain generalizes beyond tested model sizes

**Low Confidence**: RL without faithfulness rewards converges to unfaithful outputs; specific reward weights are optimal; faithfulness constraints are universally appropriate

## Next Checks

1. **Human evaluation of recycled data**: Sample 500 rephrasings from RePro, prompting-based methods, and organic data. Have three annotators rate each on semantic preservation, naturalness, and factual accuracy. Compute inter-annotator agreement and compare distributions.

2. **Long-range dependency and coherence analysis**: Use discourse parser to analyze 100-long document pairs (original vs recycled) for coherence preservation. Measure average coherence drop across methods since BERTScore is token-level.

3. **Bias amplification audit**: Analyze demographic representation in recycled vs organic data using templates from existing bias benchmarks. Sample 10k sentences from each pool and test for changes in gender, cultural, and domain representation. Identify whether quality optimization amplifies systematic biases.