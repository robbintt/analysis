---
ver: rpa2
title: 'IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge
  Injection'
arxiv_id: '2512.14792'
source_url: https://arxiv.org/abs/2512.14792
tags:
- knowledge
- generation
- graph
- validation
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the low success rates of large language
  models (LLMs) in generating correct and intent-aligned infrastructure as code (IaC),
  specifically for Terraform. To tackle this, the study systematically injected structured
  configuration knowledge into LLM-based IaC generation, enhancing an existing IaC-Eval
  benchmark with cloud emulation and automated error analysis.
---

# IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection

## Quick Facts
- **arXiv ID:** 2512.14792
- **Source URL:** https://arxiv.org/abs/2512.14792
- **Reference count:** 40
- **Primary result:** Structured knowledge injection increased Terraform generation success from 27.1% to 62.6% technical and intent alignment.

## Executive Summary
This research addresses the fundamental challenge of low success rates in LLM-generated infrastructure as code (IaC), specifically for Terraform. The study systematically injects structured configuration knowledge into LLM-based IaC generation through a novel error taxonomy and knowledge injection techniques. By comparing Naive Retrieval-Augmented Generation (RAG) against Graph RAG approaches with semantic enrichment and dependency modeling, the research demonstrates significant improvements in technical validation success. However, a critical "Correctness-Congruence Gap" emerges where models achieve high syntactic correctness but struggle with nuanced user intent alignment, revealing limitations in LLM architectural reasoning rather than knowledge access.

## Method Summary
The study enhances an existing IaC-Eval benchmark with cloud emulation and automated error analysis, developing a novel error taxonomy for LLM-assisted IaC code generation. The approach involves extracting structured knowledge from AWS Terraform Provider documentation (version 5.90.0) using `tfschema`, constructing both vector and graph knowledge bases, and evaluating multiple knowledge injection strategies. GPT-4o generates Terraform configurations using temperature 0.0, with outputs validated through LocalStack for technical correctness and OPA policies for intent alignment. The core innovation is Graph RAG, which retrieves structured entities and relationships rather than unstructured text chunks, combined with semantic enrichment techniques and cross-resource dependency modeling.

## Key Results
- Technical validation success increased from 27.1% (baseline) to 75.3% with knowledge injection
- Overall success (technical and intent alignment) reached 62.6% with Graph RAG and semantic enrichment
- Graph RAG reduced factual incorrectness errors by 52% relative to Naive RAG
- A persistent "Correctness-Congruence Gap" emerged where models plateau at ~30% intent alignment despite high technical correctness

## Why This Works (Mechanism)

### Mechanism 1: Schema Grounding via Structured Knowledge
Injecting structured provider schemas into the generation context reduces hallucinations of non-existent configuration arguments. Instead of relying on parametric memory, the system retrieves explicit "Required Arguments" and "Blocks" from a Knowledge Graph, forcing the LLM to adhere to formal specifications rather than probabilistic patterns. This assumes the LLM prioritizes retrieved context over its internal weights when generating code.

### Mechanism 2: Semantic Enrichment of Sparse Metadata
Augmenting sparse technical documentation with LLM-generated summaries improves retrieval relevance for natural language queries. Technical documentation often lacks semantic context, so using an LLM to generate rich summaries for graph nodes bridges the gap between high-level user intent and specific configuration parameters. This assumes the summarization LLM correctly interprets the functional purpose of configuration elements.

### Mechanism 3: Implicit Dependency Resolution (1-Hop Traversal)
Graph traversal to identify cross-resource dependencies helps assemble complete infrastructure stacks, though it introduces noise risks. Infrastructure components often require other components (e.g., an EC2 instance needs a Subnet). By extracting `REFERENCES` relationships and performing 1-hop traversal, the system retrieves hidden requirements automatically, assuming the user prompt implies a need for these dependencies.

## Foundational Learning

- **Concept: Declarative vs. Imperative Logic (IaC)**
  - **Why needed here:** The "Correctness-Congruence Gap" stems from the difference between syntactic validity and semantic intent. Understanding that Terraform is declarative (state-based) is crucial to understanding why technically valid code can still fail intent validation.
  - **Quick check question:** Can you explain why a Terraform script that passes `terraform plan` might still fail to meet the user's operational requirements?

- **Concept: Graph RAG vs. Naive RAG**
  - **Why needed here:** The paper benchmarks these two approaches. Naive RAG retrieves unstructured text chunks, while Graph RAG retrieves structured entities and relationships.
  - **Quick check question:** What is the specific limitation of Naive RAG regarding "nested blocks" that Graph RAG solves? (Answer: Naive RAG struggles with structural constraints like required nested arguments; Graph RAG enforces them via schema).

- **Concept: Hallucination vs. Knowledge Staleness**
  - **Why needed here:** The error taxonomy distinguishes between these two failure modes. Knowing the difference helps diagnose whether to blame training data cutoff or the model's tendency to fabricate.
  - **Quick check question:** If an LLM generates an AWS resource argument that never existed in any version of the provider, is that staleness or hallucination? (Answer: Hallucination/Factual Incorrectness).

## Architecture Onboarding

- **Component map:** Documentation -> tfschema -> Neo4j Knowledge Graph + ChromaDB Vector Store -> LLM Context -> GPT-4o Generator -> LocalStack Validator -> OPA Intent Checker
- **Critical path:**
  1. Ingest: Scrape documentation -> Split into chunks (for Vector) + Extract schema (for Graph)
  2. Enrich: Use LLM to summarize graph nodes (GR-LLMSum) and embed them
  3. Retrieve: Embed user query -> Vector search -> Entity Resolution -> Graph Traversal (fetch args/blocks)
  4. Generate: Inject retrieved graph context into prompt -> LLM generates Terraform
  5. Validate: `tflocal init/plan` -> OPA Policy Check
- **Design tradeoffs:**
  - Precision vs. Noise (GR-Ref): While retrieving all dependencies helps complex tasks, it degrades simple ones due to "cognitive overload" (noise)
  - Structure vs. Semantic Breadth: Graph RAG is better for technical schema compliance; Naive RAG occasionally captures context that Graph RAG misses
- **Failure signatures:**
  - The "Correctness-Congruence Gap": Code passes technical validation but fails intent validation (wrong resource type or missing implicit resources)
  - Hallucinated Arguments: High "Factual Incorrectness" scores (e.g., using `set_identifier` where it doesn't belong)
- **First 3 experiments:**
  1. Baseline Reproduction: Run GPT-4o with zero context on the benchmark to confirm the ~27% baseline
  2. Naive vs. Graph: Implement basic Vector RAG and compare against basic Graph RAG to validate structural advantage
  3. Ablation on Enhancements: Test GR-OptMatch vs. GR-Ref on a subset of prompts to observe the trade-off between "missing implicit dependencies" and "noise-induced failure"

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLM architectures transition from schema-compliant code generation to architectural reasoning to bridge the "Correctness-Congruence Gap"?
- **Basis in paper:** The Discussion section identifies this gap as the most critical theoretical challenge, noting that while technical correctness is solvable, intent alignment plateaus because models act as "coders" rather than "architects."
- **Why unresolved:** Current RAG methods optimize for retrieving rigid schema rules (closed-world correctness) but fail to model unstated user goals, implicit operational best practices, or complex system dependencies (open-world congruence).
- **What evidence would resolve it:** A study demonstrating that integrating architectural design patterns or constraint modeling into the generation pipeline significantly improves intent validation scores without requiring explicit user specifications.

### Open Question 2
- **Question:** Can adaptive retrieval strategies optimize the "Principle of Optimal Context" to prevent the cognitive overload observed in comprehensive dependency modeling?
- **Basis in paper:** Section 7 notes that the GR-Ref method caused "cognitive overload," degrading performance on simple tasks by including excessive referenced resources. The authors explicitly call for context-aware strategies that tailor complexity to the query.
- **Why unresolved:** Current retrieval methods apply static heuristics (e.g., 1-hop traversal) regardless of prompt complexity, leading to noise that confuses the model on simpler tasks.
- **What evidence would resolve it:** Experiments comparing static vs. query-complexity-aware retrieval mechanisms, showing that dynamic context filtering improves success rates across both simple and complex infrastructure prompts.

### Open Question 3
- **Question:** Do the identified failure patterns (Factual Incorrectness vs. Incompleteness) and the efficacy of Graph RAG generalize to open-source code models?
- **Basis in paper:** Section 8 lists the exclusive use of GPT-4o as a threat to external validity. It remains unclear if the "insufficient parametric knowledge" conclusion is unique to GPT-4o or a universal characteristic of LLMs in the IaC domain.
- **Why unresolved:** Different models have varying training data cut-offs and architectural strengths; a model trained more heavily on code than text might exhibit different error distributions in the proposed taxonomy.
- **What evidence would resolve it:** Replicating the benchmark evaluation using diverse open-source and proprietary models (e.g., Llama, Mistral, Claude) to determine if Graph RAG provides universal gains or interacts differently with specific model architectures.

## Limitations
- Narrow scope focusing only on AWS provider version 5.90.0 without testing generalization to other cloud providers or IaC tools
- Error taxonomy may not capture all failure modes in production environments
- Semantic enrichment approach relies on LLM-generated summaries, introducing potential cascading errors

## Confidence

- **High Confidence:** Technical validation improvements (75.3% success rate) and the fundamental distinction between syntactic correctness and semantic intent alignment
- **Medium Confidence:** The superiority of Graph RAG over Naive RAG for schema compliance, given the specific AWS Terraform context
- **Low Confidence:** Generalizability of results to multi-cloud environments, other IaC languages, and real-world scenarios beyond controlled benchmarks

## Next Checks
1. **Cross-Provider Validation:** Test the knowledge injection approach on Azure Resource Manager templates and Google Cloud Deployment Manager to assess generalizability beyond AWS Terraform
2. **Production Stress Test:** Deploy generated configurations in actual cloud environments (not just LocalStack) to measure real-world error rates and performance impacts
3. **Architectural Reasoning Benchmark:** Design prompts requiring multi-resource coordination and complex dependency chains to quantify the "architect vs. coder" gap identified in the study