---
ver: rpa2
title: 'Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving'
arxiv_id: '2601.21288'
source_url: https://arxiv.org/abs/2601.21288
tags:
- distillation
- driving
- reasoning
- autonomous
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying large vision-language
  models (VLMs) for autonomous driving in resource-constrained environments, where
  GPU memory and latency are critical concerns. To tackle this, the authors propose
  Drive-KD, a multi-teacher distillation framework that decomposes autonomous driving
  into a sequential triad of perception, reasoning, and planning capabilities, and
  transfers each via targeted internal supervision.
---

# Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving

## Quick Facts
- arXiv ID: 2601.21288
- Source URL: https://arxiv.org/abs/2601.21288
- Authors: Weitong Lian; Zecong Tang; Haoran Li; Tianjian Gao; Yifei Wang; Zixu Wang; Lingyi Meng; Tengju Ru; Zhejun Cui; Yichen Zhu; Hangshuo Cao; Qi Kang; Tianxing Chen; Yusen Qin; Kaixuan Wang; Yu Zhang
- Reference count: 40
- 11.4× higher throughput, 42× less GPU memory than pretrained 78B model, better overall performance on DriveBench

## Executive Summary
This paper addresses the challenge of deploying large vision-language models (VLMs) for autonomous driving in resource-constrained environments, where GPU memory and latency are critical concerns. To tackle this, the authors propose Drive-KD, a multi-teacher distillation framework that decomposes autonomous driving into a sequential triad of perception, reasoning, and planning capabilities, and transfers each via targeted internal supervision. The framework identifies layer-specific attention as the optimal distillation signal, builds capability-specific single-teacher models, and unifies them with asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive experiments validate Drive-KD's effectiveness across diverse model families and scales. The distilled InternVL3-1B model, requiring ~42× less GPU memory and delivering ~11.4× higher throughput, achieves better overall performance than the pretrained 78B model on DriveBench and surpasses GPT-5.1 on the planning dimension, offering a strong capability-efficiency trade-off for efficient autonomous driving VLMs.

## Method Summary
Drive-KD decomposes autonomous driving into perception, reasoning, and planning capabilities, each distilled from specialized teacher models at different transformer layers. The framework uses attention maps as the primary distillation signal, finding that layer-specific attention (early layers for perception, intermediate for reasoning, penultimate for planning) provides more stable supervision than hidden states or output distributions. To handle gradient conflicts during multi-teacher distillation, Drive-KD employs an asymmetric gradient projection mechanism that preserves hard-label supervision while mitigating conflicting updates between capabilities.

## Key Results
- 11.4× higher throughput and 42× less GPU memory than pretrained 78B model
- Better overall performance than 78B model on DriveBench
- Outperforms GPT-5.1 on planning dimension
- Attention distillation outperforms hidden-state and output-distribution methods across all capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer-specific attention distillation transfers driving capabilities more effectively than output-distribution alignment or hidden-state matching.
- Mechanism: Different transformer layers encode different functional roles—early layers capture cross-modal alignment (perception), intermediate layers handle fusion and relation (reasoning), and late layers specialize for decision features (planning). Distilling attention maps from these targeted layers provides capability-specific supervision that is more stable across question variations than hidden states or soft label distributions.
- Core assumption: The paper assumes that attention dispersion correlates with distillation signal quality and that driving tasks exhibit lower teacher confidence than general QA, making KL-based methods noisy.
- Evidence anchors:
  - [abstract] "identifies layer-specific attention as the optimal distillation signal"
  - [section 3.2.2] "attention shows consistently lower dispersion than hidden states, indicating that attention reflects capability-stable behaviors more stably"
  - [section 3.2.3] "driving outputs are consistently less confident and more diffuse than general outputs, making output-distribution alignment a noisy distillation signal"
  - [corpus] Weak direct evidence; related work (SFedKD) uses multi-teacher distillation but targets federated learning, not layer-specific attention for driving.
- Break condition: If teacher attention maps are themselves noisy or inconsistent (e.g., due to teacher overfitting), the distillation signal may degrade rather than help.

### Mechanism 2
- Claim: Decomposing autonomous driving into a sequential perception–reasoning–planning triad enables more targeted and effective knowledge transfer than treating all capabilities as parallel objectives.
- Mechanism: Each capability has distinct layer-wise characteristics: perception peaks at Layer 1 intra-consistency, reasoning maintains high intra across layers, and planning peaks near the penultimate layer. By constructing capability-specific single-teacher models with appropriate layer supervision before combining them, the student receives cleaner, less conflicting gradient signals.
- Core assumption: Sequential capabilities have separable layer-wise representations that can be independently optimized before unification.
- Evidence anchors:
  - [abstract] "decomposes autonomous driving into a sequential triad of perception, reasoning, and planning capabilities, and transfers each via targeted internal supervision"
  - [section 3.2.1] "perception has the highest intra at Layer 1 and then decreases with depth, reasoning keeps a high intra across layers, and planning decreases overall but shows a clear peak near Layer ℓpen"
  - [corpus] VERDI similarly uses VLM-embedded reasoning but focuses on trajectory generation, not structured triad decomposition.
- Break condition: If capabilities are not as separable as assumed (e.g., planning heavily depends on reasoning representations at the same layers), single-teacher optimization may introduce conflicting inductive biases.

### Mechanism 3
- Claim: Asymmetric gradient projection (AGP) mitigates cross-capability gradient conflicts in multi-teacher distillation without sacrificing hard-label learning.
- Mechanism: Stage 1 uses the hard-label supervised gradient as an anchor and only projects away the conflicting component of the distillation gradient, preserving ground-truth supervision integrity. Stage 2 then applies shuffled symmetric pairwise projections across merged capability gradients to prevent any single capability from consistently dominating updates.
- Core assumption: Hard-label gradients provide more reliable supervision than soft teacher signals, and capability gradients frequently have negative inner products that harm joint optimization.
- Evidence anchors:
  - [abstract] "unifies them with asymmetric gradient projection to mitigate cross-capability gradient conflicts"
  - [section 3.4.2] "Stage 1 uses the hard-label supervised gradient as an anchor and removes only the part of the distillation update that conflicts with it"
  - [table 4] Multi-teacher baseline without AGP severely hurts reasoning (25.68 vs 33.15 with AGP); symmetric projection (G1/G2) is unstable
  - [corpus] SFedKD addresses multi-teacher discrepancy but uses discrepancy-aware weighting, not gradient projection.
- Break condition: If gradient conflicts are rare or the anchor gradient itself is noisy (e.g., label noise), AGP may over-constrain useful distillation updates.

## Foundational Learning

- Concept: **Transformer layer specialization**
  - Why needed here: The entire layer-selection strategy relies on understanding that different layers encode different functions—early layers for token-level and cross-modal features, middle layers for relational reasoning, late layers for task-specific outputs.
  - Quick check question: Given a 24-layer VLM, where would you expect the strongest vision–text fusion signal, and why?

- Concept: **Knowledge distillation loss types (logits vs. features vs. attention)**
  - Why needed here: The paper explicitly rejects output-distribution alignment (KL on logits) in favor of attention-based internal supervision; understanding why requires knowing the tradeoffs between signal richness, noise sensitivity, and computational cost.
  - Quick check question: Why might attention maps be more stable than hidden states as a distillation signal across semantically equivalent but differently phrased questions?

- Concept: **Gradient conflict in multi-objective learning**
  - Why needed here: AGP is designed to handle conflicting gradient directions; understanding why naive loss aggregation fails (improving one capability degrades another) is essential to motivate the two-stage projection.
  - Quick check question: If two loss gradients have a negative inner product, what happens to the shared parameter update if you simply sum them?

## Architecture Onboarding

- Component map:
  - Teacher models: Three capability-specific VLMs (e.g., InternVL3-8B) providing attention targets at Layer 1 (perception), intermediate layers (reasoning), and penultimate layer (planning)
  - Student model: Smaller VLM (e.g., InternVL3-1B) receiving hard-label supervision + attention distillation losses
  - Distillation dataset: 10k human-annotated driving VQA with perception/reasoning/planning splits
  - AGP module: Two-stage gradient projection applied before optimizer step

- Critical path:
  1. Pre-study: Compute layer-wise similarity (Eq. 1–3) and intra-consistency (Eq. 5) to confirm layer selections
  2. Single-teacher training: Train separate students per capability with attention distillation at appropriate layers
  3. Multi-teacher training: Combine three teachers with fixed mixing weights (Π matrix), apply AGP during backprop
  4. Evaluation: Run on DriveBench with LLM-based grader (DeepSeek-V3.2), average over 5 runs

- Design tradeoffs:
  - Teacher size vs. student capacity: Table 3 shows larger teachers (38B) can degrade performance on 1B students—matching teacher complexity to student capacity matters
  - Attention type (full vs. text-to-vision): Paper uses text-to-vision for perception/planning, full attention for reasoning; full attention at wrong layers underperforms (Table 6)
  - MSE vs. cosine loss: MSE attention matching outperforms cosine-based similarity (Table 6)

- Failure signatures:
  - Reasoning score collapses in multi-teacher without AGP (25.68 vs. 33.15)—indicates cross-capability interference
  - CE+KL underperforms CE-only (Table 4)—output-distribution alignment adds noise for driving tasks
  - Hidden-state distillation underperforms attention distillation (Table 4)—hidden states have higher dispersion

- First 3 experiments:
  1. **Layer ablation**: For a single capability (e.g., perception), compare distilling Layer 1 vs. penultimate layer vs. all layers; expect Layer 1 to win due to high early intra-consistency
  2. **Signal ablation**: Compare attention distillation vs. hidden-state distillation vs. CE+KL on the same capability split; expect attention > hidden > KL based on dispersion analysis
  3. **AGP validation**: Run multi-teacher training with (a) no gradient handling, (b) symmetric projection only, (c) full AGP; measure reasoning score to confirm AGP recovery effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Drive-KD distilled models in open-loop evaluation (DriveBench) correlate with actual safety and robustness in closed-loop simulation or real-world driving?
- Basis in paper: [explicit] The Impact Statement explicitly cautions: "our results are based on the open-loop evaluation of DriveBench and do not guarantee real-world performance," and recommends rigorous closed-loop testing.
- Why unresolved: The paper's evaluation protocol relies entirely on offline VQA-based benchmarks (DriveBench) and LLM-based grading, which do not capture the dynamic feedback loops, temporal consistency, or reaction times required for actual vehicle control.
- What evidence would resolve it: Deployment of Drive-KD distilled models (e.g., InternVL3-1B) in high-fidelity closed-loop simulators (e.g., CARLA) or hardware-in-the-loop setups to measure collision rates, lane-keeping stability, and rule violations.

### Open Question 2
- Question: Can the "perception–reasoning–planning" triad distillation framework be effectively generalized to other safety-critical domains (e.g., medical image analysis, robotics manipulation) that possess similar sequential capability dependencies?
- Basis in paper: [explicit] In the Conclusion, the authors state they "hope Drive-KD... provides clearer insights into distilling capability-specific behaviors for safety-critical tasks" beyond just autonomous driving.
- Why unresolved: The methodology, including the specific layer selection (Layer 1 vs. Penultimate) and attention signal definitions, is empirically derived and validated solely on driving datasets (nuScenes/BDD100K), leaving its applicability to other domains untested.
- What evidence would resolve it: Application of the Drive-KD framework to a different VLM domain, such as surgical robotics or radiology, demonstrating that capability-specific attention distillation improves sequential reasoning tasks in those fields.

### Open Question 3
- Question: Does the persistent difficulty in transferring "reasoning" capabilities stem primarily from the small student's capacity limits, or can it be resolved through more sophisticated data or gradient conflict resolution techniques?
- Basis in paper: [explicit/inferred] The Impact Statement notes "reasoning ability remains the most difficult aspect to transfer," and Section 4.2/4.4 shows reasoning lags behind perception/planning and is most sensitive to gradient conflicts, yet the paper does not fully isolate the cause.
- Why unresolved: While the paper identifies the problem and attempts to fix gradient conflicts with AGP, reasoning scores still show lower gains and higher variance compared to planning, suggesting the current method may be insufficient for complex logic transfer.
- What evidence would resolve it: An ablation study specifically targeting reasoning capabilities by varying student model capacity (e.g., 2B vs. 3B vs. 7B) while keeping the teacher fixed, combined with analysis of intermediate reasoning feature alignment.

### Open Question 4
- Question: What is the optimal dynamic for teacher selection when student capacity is limited, given that larger teachers (38B) sometimes degraded specific capabilities compared to smaller teachers (8B)?
- Basis in paper: [inferred] Section 4.4 observes that "larger teachers can improve, match, or degrade different capabilities" and hypothesizes that "stronger teachers may be harder for small students to learn," but does not propose a method to predict or mitigate this mismatch.
- Why unresolved: The relationship between teacher size and student absorption is non-monotonic (Table 3), suggesting a "capacity gap" where too much teacher complexity acts as noise, yet the current framework uses static teacher assignments.
- What evidence would resolve it: Introduction of an adaptive teacher selection mechanism or a difficulty-aware distillation loss that filters "hard-to-learn" signals from very large teachers when distilling to 1B-scale students.

## Limitations

- The effectiveness of attention-based distillation depends on the assumption that teacher attention maps are consistent and high-quality across driving scenarios
- The sequential triad decomposition assumes capabilities are neatly separable across transformer layers, which may not hold for complex driving scenarios
- The asymmetric gradient projection mechanism assumes hard-label gradients provide reliable supervision anchors, which may fail with noisy or ambiguous labels

## Confidence

- **High confidence**: The empirical superiority of attention distillation over hidden-state and output-distribution methods (supported by Table 4 results showing consistent improvements across multiple experiments)
- **Medium confidence**: The sequential triad decomposition of driving capabilities (supported by layer-wise intra-consistency analysis but relies on specific dataset characteristics)
- **Medium confidence**: The asymmetric gradient projection mechanism's effectiveness (demonstrated on reasoning score recovery but could be dataset-dependent)
- **Low confidence**: The generalizability of the 11.4× throughput improvement across different hardware configurations and deployment scenarios (only tested on specific GPU setups)

## Next Checks

1. **Teacher attention stability test**: Measure attention map consistency across semantically equivalent driving questions with different phrasings on held-out data. If attention dispersion exceeds a threshold (e.g., >0.3 cosine similarity drop), the distillation signal may be unreliable.

2. **Cross-dataset capability transfer**: Evaluate the distilled model on driving datasets from different geographic regions or weather conditions not seen during training. Performance drop >15% would indicate overfitting to the original dataset's distribution.

3. **Real-world deployment pilot**: Deploy the distilled model on a closed-course driving test with human safety drivers. Monitor for specific failure modes like hesitation at intersections or incorrect lane changes that might not appear in benchmark evaluations but are critical for safety.