---
ver: rpa2
title: It's complicated. The relationship of algorithmic fairness and non-discrimination
  provisions for high-risk systems in the EU AI Act
arxiv_id: '2501.12962'
source_url: https://arxiv.org/abs/2501.12962
tags:
- systems
- article
- fairness
- non-discrimination
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a detailed analysis of the relationship between
  EU non-discrimination law and algorithmic fairness provisions in the AI Act for
  high-risk systems. It identifies that most non-discrimination regulations target
  only high-risk AI systems, with provisions focusing on data input requirements and
  output monitoring, though these are partly inconsistent and raise questions of computational
  feasibility.
---

# It's complicated. The relationship of algorithmic fairness and non-discrimination provisions for high-risk systems in the EU AI Act

## Quick Facts
- arXiv ID: 2501.12962
- Source URL: https://arxiv.org/abs/2501.12962
- Reference count: 40
- This paper analyzes how EU non-discrimination law and algorithmic fairness provisions interact in the AI Act, finding that most regulations target high-risk systems through data input requirements and output monitoring, though with significant gaps and computational feasibility challenges.

## Executive Summary
This paper provides a comprehensive analysis of the relationship between EU non-discrimination law and algorithmic fairness provisions in the AI Act for high-risk systems. The author examines how legal non-discrimination concepts interact with machine learning-based fairness notions, focusing on Articles 10 and 15 which mandate bias examination and mitigation in training data and continuous learning systems. The analysis reveals that while the Act incorporates both legal and technical approaches to non-discrimination, there are significant gaps in implementation, particularly regarding output regulation and the balance between provider-focused compliance and individual rights protection.

## Method Summary
The paper employs a legal-technical analysis approach, examining the EU AI Act's provisions alongside established EU non-discrimination law and machine learning fairness concepts. The author conducts a detailed textual analysis of relevant AI Act articles, comparing them with legal frameworks and technical literature on algorithmic fairness. The methodology involves identifying inconsistencies between legal requirements and technical feasibility, mapping relationships between different non-discrimination concepts, and assessing how standardization processes might bridge implementation gaps.

## Key Results
- The AI Act primarily regulates discrimination through input data requirements rather than output monitoring for standard high-risk systems
- There are significant gaps between legal non-discrimination concepts and their technical implementation in algorithmic systems
- Harmonized standards will play a crucial role in operationalizing abstract legal requirements, but standardization committees have considerable discretion that may prioritize industry feasibility over fundamental rights
- The Act creates tension between provider-focused compliance approaches and individual rights protection mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Input-Side Bias Governance
- **Claim:** The EU AI Act primarily seeks to prevent discrimination by regulating the characteristics of training data rather than solely monitoring final outputs.
- **Mechanism:** Article 10(2)(f) mandates an "examination in view of possible biases" for training, validation, and testing datasets. Article 10(2)(g) requires "appropriate measures to detect, prevent and mitigate" these identified biases. The legal theory assumes that rectifying the "root cause" (data inputs) reduces discriminatory potential in the system.
- **Core assumption:** High-quality, representative, and bias-mitigated input data will necessarily lead to less discriminatory model behavior (the "Bias in, bias out" axiom cited in the paper).
- **Evidence anchors:**
  - [Section 3.3]: Identifies Article 10(2)(f) AIA as the "Main input non-discrimination provision," requiring data examination for biases likely to affect fundamental rights.
  - [Section 3.3]: Notes that "it has been argued that with this focus on the input side, the AI Act seeks to remedy the root cause of biases."
  - [Corpus]: "Beyond Internal Data: Constructing Complete Datasets for Fairness Testing" supports the difficulty of procuring necessary data for these audits.
- **Break condition:** If model behavior generates discriminatory outputs from neutral data (e.g., through proxy variables or optimization dynamics) or if "bias" definitions in the Act remain too ambiguous for technical implementation.

### Mechanism 2: Feedback Loop Interruption for Continuous Learning
- **Claim:** For systems that evolve post-deployment, the Act enforces fairness by specifically regulating how "biased outputs" are prevented from corrupting future "inputs."
- **Mechanism:** Article 15(4) targets high-risk systems that "continue to learn." It requires designs that "eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations." This attempts to prevent "echo chambers" where historical biases are amplified.
- **Core assumption:** Systems can technically distinguish between valid feedback and "biased" feedback to prevent degradation of the input data stream.
- **Evidence anchors:**
  - [Section 3.3]: "Article 15(4) AIA mandates that... the output of a system is regulated... to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input."
  - [Section 3.3]: Links this to Recital 67 AIA regarding the concern of "echo chambers for biases."
  - [Corpus]: Evidence for this specific mechanism is weak or missing in the provided corpus.
- **Break condition:** If the system lacks the technical capacity to distinguish biased from unbiased feedback signals, or if the computational feasibility of this "filtering" is unproven.

### Mechanism 3: Delegation to Harmonized Standards
- **Claim:** The Act resolves the gap between legal mandates and technical implementation by delegating specific "how-to" methodologies to external standardization bodies.
- **Mechanism:** Under the New Legislative Framework, the European Commission issues mandates (e.g., M/593) to organizations like CEN/CENELEC. These bodies develop "harmonized technical standards." Providers adhering to these standards gain a "presumption of conformity," effectively outsourcing the definition of "appropriate bias detection" to technical committees.
- **Core assumption:** Standardization bodies can translate abstract legal non-discrimination concepts (like "indirect discrimination") into concrete computational metrics (like "disparate impact") without violating the law's intent.
- **Evidence anchors:**
  - [Section 4]: "The details of non-discrimination procedures... will be developed through the standardisation process."
  - [Section 4]: Notes that "standardisation committees have considerable discretion" but must operate within legal bounds.
  - [Corpus]: Evidence for this specific mechanism is weak or missing in the provided corpus.
- **Break condition:** If standards prioritize industry feasibility over fundamental rights protection, potentially creating a "false sense of safety" as warned in the paper.

## Foundational Learning

- **Concept: Direct vs. Indirect Discrimination**
  - **Why needed here:** Article 10(2)(f) references "discrimination prohibited under Union law." You cannot detect "indirect discrimination" (neutral proxies like zip codes affecting protected groups) using only basic error rates; you need specific metrics for disparate impact.
  - **Quick check question:** Does your testing pipeline check for group-level statistical disparities (Indirect) or only explicit exclusion of protected attributes (Direct)?

- **Concept: Fairness Metrics Incompatibility**
  - **Why needed here:** The paper notes that different fairness metrics are often mathematically incompatible. You must select the metric that aligns with the specific legal non-discrimination goal (e.g., calibration vs. demographic parity) rather than optimizing for all simultaneously.
  - **Quick check question:** Have you documented why your chosen fairness metric (e.g., Equalized Odds) corresponds to the legal definition of non-discrimination relevant to your domain?

- **Concept: Product Safety vs. Fundamental Rights**
  - **Why needed here:** The AI Act is a hybrid. It treats discrimination as a "risk" to be managed (Product Safety approach) rather than solely a rights violation to be redressed. This shifts the burden to the *provider's* internal compliance (Risk Management System) rather than just *individual* legal recourse.
  - **Quick check question:** Is your compliance strategy focused solely on avoiding lawsuits (ex-post), or on documenting "residual risks" in a continuous risk management system (ex-ante)?

## Architecture Onboarding

- **Component map:**
  - Risk Management System (Art. 9) -> Data Governance (Art. 10) -> Technical Documentation (Art. 11) -> Continuous Learning Control (Art. 15)

- **Critical path:**
  1. **Classification:** Determine if the system is "High-Risk" (Art. 6 + Annex III)
  2. **Data Exam:** Perform bias examination on datasets (Art 10.2.f) *before* training
  3. **Mitigation:** Apply appropriate bias mitigation techniques (Art 10.2.g)
  4. **Conformity:** Issue EU declaration of conformity (relying on harmonized standards where available)

- **Design tradeoffs:**
  - **Input vs. Output Regulation:** The Act is stronger on analyzing *input data* (Art. 10) than on mandating specific *output* monitoring for standard systems (except for feedback loops in Art. 15). Relying only on input checks may miss emergent discrimination.
  - **Provider vs. Deployer:** The Act places the heaviest burden on the *Provider* (developer). However, the *Deployer* (user) often holds the context-specific data needed for accurate fairness testing. This creates a visibility gap.

- **Failure signatures:**
  - **"Fairness Hacking":** Selecting a post-hoc fairness metric that makes the model look fair while ignoring incompatible metrics that show discrimination.
  - **Proxy Discrimination:** Removing protected attributes (e.g., gender) but failing to remove correlated proxies (e.g., postal code), leading to indirect discrimination.
  - **Static Drift:** Passing initial conformity assessment but failing to update bias analysis as the real-world input distribution changes over time.

- **First 3 experiments:**
  1. **Annex III Mapping:** Map your system's "intended purpose" against Annex III to definitively confirm High-Risk status.
  2. **Data Representation Audit:** Quantify the representation of protected groups in your training data (Art 10.2.d) and run bias detection metrics (e.g., balanced accuracy) across these groups.
  3. **Feedback Loop Isolation:** If the system learns continuously, design a test to verify if adding synthetic "biased" outputs to the training pool degrades performance for specific groups (testing Art 15.4 resilience).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the European Court of Justice (ECJ) recognize intersectional discrimination as a distinct legal category applicable to AI systems using multiple input variables?
- Basis in paper: [explicit] Section 2.1 states, "Whether, and to what extent, the ECJ currently recognises intersectional discrimination as a distinct form of discrimination remains an open question."
- Why unresolved: Current jurisprudence is unclear regarding discrimination originating from "inextricably linked vectors of disadvantage," while modern AI models rely on complex variable interactions rather than single protected attributes.
- What evidence would resolve it: A ruling by the ECJ specifically addressing multi-variable discrimination in an algorithmic context, or explicit harmonized standards addressing intersectional testing.

### Open Question 2
- Question: Can the AI Act's focus on input data governance (Article 10) effectively prevent discriminatory outcomes without explicit output metrics for standard high-risk systems?
- Basis in paper: [inferred] Section 3.3 notes Article 10 targets only input data, while Section 6 highlights that rules "overlook other sources of bias... rather than offering output-based safeguards."
- Why unresolved: The paper notes a regulatory gap where the Act regulates input bias but fails to consistently regulate direct, indirect, and intersectional discrimination in the output phase (except for feedback loops in Article 15).
- What evidence would resolve it: Comparative empirical studies demonstrating whether input debiasing correlates with legal non-discrimination in outputs, or a clarification in harmonized standards mandating output monitoring.

### Open Question 3
- Question: Will the upcoming standardization process (e.g., CEN-CENELEC) successfully operationalize abstract "bias examination" requirements into technically feasible methods for providers?
- Basis in paper: [explicit] Section 4 mentions "details of non-discrimination procedures... will be developed through the standardisation process," but Section 3.3 warns that "bias" is undefined and input detection methods are "relatively underexplored."
- Why unresolved: Regulators may have envisioned a technical definition of bias that does not align with the lack of established computational methods for detecting input bias in the current machine learning literature.
- What evidence would resolve it: The publication of Harmonised Standards that define specific, computationally viable auditing protocols for input bias detection and mitigation.

## Limitations

- The paper acknowledges significant uncertainty about how standardization processes will translate legal requirements into technical standards
- There is ambiguity regarding the practical feasibility of bias detection and mitigation requirements, particularly given data quality limitations
- The relationship between AI Act requirements and existing EU non-discrimination law remains partly unclear, especially regarding indirect discrimination operationalization

## Confidence

- **High Confidence:** The identification that the AI Act primarily regulates high-risk systems through data input requirements (Article 10) and that these provisions are consistent with existing EU non-discrimination law targeting product safety
- **Medium Confidence:** The assessment that current provisions are "partly inconsistent" and raise computational feasibility questions, as this depends on future standardization outcomes
- **Medium Confidence:** The claim that harmonized standards will play a crucial role in implementation, though the exact content of these standards remains uncertain

## Next Checks

1. **Standardization Monitoring:** Track the development of harmonized standards under Mandate M/593 to verify how technical committees translate legal non-discrimination requirements into specific metrics and procedures
2. **Feasibility Assessment:** Conduct empirical studies testing the computational feasibility of implementing Article 10(2)(f) and (g) requirements on real-world datasets, particularly regarding data quality and representation constraints
3. **Legal-Technical Alignment:** Analyze draft standards against established EU non-discrimination jurisprudence to ensure that technical implementations preserve fundamental rights protections rather than prioritizing industry feasibility