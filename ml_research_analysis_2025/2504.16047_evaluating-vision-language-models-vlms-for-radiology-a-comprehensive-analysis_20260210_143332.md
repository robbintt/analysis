---
ver: rpa2
title: 'Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis'
arxiv_id: '2504.16047'
source_url: https://arxiv.org/abs/2504.16047
tags:
- segmentation
- rad-dino
- performance
- chexagent
- biomedclip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated three vision-language foundation models (RAD-DINO,
  CheXagent, and BiomedCLIP) for radiology tasks including classification, segmentation,
  and regression on pneumothorax and cardiomegaly detection. Self-supervised RAD-DINO
  consistently outperformed other models in segmentation tasks, while text-supervised
  CheXagent showed superior classification performance.
---

# Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis

## Quick Facts
- **arXiv ID**: 2504.16047
- **Source URL**: https://arxiv.org/abs/2504.16047
- **Reference count**: 0
- **One-line primary result**: Self-supervised RAD-DINO outperformed other models in segmentation tasks, while text-supervised CheXagent excelled in classification.

## Executive Summary
This study comprehensively evaluates three vision-language foundation models (RAD-DINO, CheXagent, and BiomedCLIP) for radiology tasks including classification, segmentation, and regression on pneumothorax and cardiomegaly detection. The research systematically compares self-supervised and text-supervised pre-training approaches using linear probing and a custom cross-attention segmentation model. Results demonstrate that pre-training methodology significantly influences model performance, with self-supervised models excelling at fine-grained segmentation tasks and text-supervised models offering advantages in classification and interpretability. The findings provide critical guidance for selecting foundation models based on specific clinical applications in radiology.

## Method Summary
The study evaluated three foundation models on chest radiograph tasks using two approaches: linear probing (single convolutional or linear layer on frozen embeddings) and a custom segmentation model integrating global [CLS] and local patch embeddings through cross-attention. Models were tested on pneumothorax and cardiomegaly detection using classification, segmentation, and regression metrics. Training used early stopping with batch size 16 and learning rate 1e-4. Input sizes varied (RAD-DINO: 518×518, CheXagent: 512×512, BiomedCLIP: 224×224). Performance was measured with Dice, IoU, AUROC, AUPRC, and R² with 95% bootstrap confidence intervals and Bonferroni-corrected significance testing.

## Key Results
- Self-supervised RAD-DINO consistently outperformed other models in segmentation tasks, particularly for challenging pneumothorax segmentation
- Text-supervised CheXagent demonstrated superior classification performance with better interpretability
- Custom segmentation model integrating global and local features significantly improved performance across all models, with BiomedCLIP showing the largest relative gains (197.6% improvement in Dice score)
- BiomedCLIP showed erratic performance across tasks, likely due to representation collapse from limited text detail in radiology reports

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised pre-training without text supervision produces superior fine-grained visual representations for spatial segmentation tasks in radiology. Models like RAD-DINO use masked image modeling and self-supervised instance discrimination to learn dense, localized features. Without text supervision pushing representations toward semantic abstraction, the encoder preserves granular pixel-level information critical for boundary delineation in tasks like pneumothorax and heart segmentation.

### Mechanism 2
Text-supervised contrastive pre-training produces superior global image representations for classification but may suffer representation collapse for fine-grained tasks. CLIP-style training aligns image embeddings with text descriptions in a shared latent space, optimizing the [CLS] token to capture semantic concepts aligned with textual labels. This makes it effective for disease presence classification but potentially discards intra-class variations and spatial details needed for localization.

### Mechanism 3
Cross-attention integration of global [CLS] and local patch embeddings substantially improves segmentation, with larger gains for models prone to representation collapse. The custom segmentation model uses cross-attention to let the global [CLS] embedding attend to refined patch features, combining semantic context with spatial precision. This compensates for patch embeddings that lack global context and [CLS] embeddings that lack spatial detail.

## Foundational Learning

- Concept: **Vision Transformer (ViT) embedding extraction**
  - Why needed here: All three foundation models use ViT encoders that output both global [CLS] tokens and patch-level embeddings. Understanding how to extract and shape these (e.g., RAD-DINO: 768-dim global, 768×37×37 patches) is essential for downstream task design.
  - Quick check question: Can you explain why patch embeddings preserve spatial information while [CLS] embeddings collapse spatial dimensions?

- Concept: **Linear probing vs. fine-tuning**
  - Why needed here: The study uses linear probing (single convolutional or linear layer on frozen embeddings) to evaluate foundation model quality. This isolates the encoder's representation power from architectural confounders.
  - Quick check question: What does it mean if a linear probe achieves high performance on a task—what does that imply about the pre-trained embeddings?

- Concept: **Contrastive Language-Image Pre-training (CLIP) objective**
  - Why needed here: CheXagent and BiomedCLIP use CLIP-style training, which learns by maximizing similarity between correct image-text pairs. This objective shapes what features are preserved or discarded in embeddings.
  - Quick check question: Why might a CLIP-trained model struggle with segmentation compared to a model trained only on images?

## Architecture Onboarding

- Component map:
Input Chest X-ray → Foundation Model Vision Encoder (frozen) → [CLS] + Patch Embeddings → Normalize both streams → Refine patches with shallow CNN → Apply cross-attention where [CLS] attends to patches → Concatenate and project to output resolution

- Critical path: Extract patch embeddings and [CLS] embedding from pre-trained encoder → normalize both streams → refine patches with shallow CNN → apply cross-attention where [CLS] attends to patches → concatenate and project to output resolution.

- Design tradeoffs:
  - RAD-DINO: Best for segmentation but lacks zero-shot text capabilities
  - CheXagent: Best for classification + interpretability, but weaker fine-grained features
  - BiomedCLIP: Multi-modal coverage but inconsistent performance and smallest input size (224×224) may lose radiology details
  - Linear probing: Lightweight but limited expressiveness; custom model adds ~50-200% improvement on difficult segmentation

- Failure signatures:
  - pneumothorax Dice < 0.30 with linear probing: Model lacks fine-grained features; add global-local integration
  - BiomedCLIP showing erratic performance across tasks: Likely representation collapse or domain mismatch (trained on publication images, not clinical CXRs)
  - Inconsistent preprocessing causing extraction errors: Foundation models rarely specify medical image format requirements (16-bit vs 8-bit)

- First 3 experiments:
  1. **Baseline linear probe**: Extract [CLS] and patch embeddings from each foundation model; train single 1×1 conv for segmentation and single linear layer for classification to establish encoder quality benchmarks.
  2. **Ablate global-local integration**: Compare linear probing vs. custom cross-attention model on pneumothorax segmentation; measure relative improvement to quantify how much each model benefits from global context.
  3. **Test preprocessing sensitivity**: Vary input image format and resolution for each foundation model to identify documentation gaps and preprocessing requirements specific to medical images.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would the observed performance patterns generalize to other imaging modalities (CT, MRI) and pathologies beyond pneumothorax and cardiomegaly?
- Basis in paper: [explicit] "our evaluation focused specifically on chest radiographs and two clinical conditions. Future work should extend to other imaging modalities and pathologies to assess the generalizability of these findings."
- Why unresolved: The study design was deliberately narrow, evaluating only chest X-rays for two specific conditions.
- What evidence would resolve it: Systematic evaluation of the same three foundation models across diverse imaging modalities and a broader range of pathologies using consistent methodology.

### Open Question 2
- Question: To what extent do architectural differences (e.g., ViT-B/14 vs. SigLIP-Large) versus pre-training methodologies drive the observed performance differences?
- Basis in paper: [explicit] "the performance differences observed might be influenced by the specific architectures of the vision encoders beyond their pre-training approaches. Controlling for architectural variations could provide clearer insights into the impact of pre-training methodologies."
- Why unresolved: Each model uses different architectures and pre-training approaches simultaneously, making attribution difficult.
- What evidence would resolve it: Ablation studies using controlled architectures with varied pre-training strategies, or evaluations of multiple models sharing the same backbone but differing only in pre-training method.

### Open Question 3
- Question: Can hybrid pre-training approaches combining self-supervised learning with text supervision achieve strong performance across both fine-grained segmentation and classification tasks?
- Basis in paper: [explicit] "hybrid approaches combining self-supervised and text-supervised learning might offer more comprehensive performance across diverse medical imaging applications."
- Why unresolved: No hybrid model was evaluated; the study compared purely self-supervised (RAD-DINO) against text-supervised (CheXagent, BiomedCLIP) models.
- What evidence would resolve it: Development and evaluation of foundation models explicitly trained with both objectives, benchmarked on segmentation and classification tasks.

### Open Question 4
- Question: How can representation collapse in text-supervised models be systematically detected and mitigated for medical imaging tasks?
- Basis in paper: [inferred] The paper notes BiomedCLIP "may suffer from representation collapse" and discusses how contrastive learning can be limited when text descriptions lack detail, but does not propose detection or mitigation strategies.
- Why unresolved: Representation collapse was hypothesized based on poor pneumothorax segmentation performance but not directly measured or addressed.
- What evidence would resolve it: Systematic analysis of embedding spaces across text-supervised models using metrics for representation quality, followed by interventions (e.g., richer text descriptions, auxiliary self-supervised objectives).

## Limitations
- Private Emory cardiomegaly dataset prevents full reproducibility of regression results
- Linear probing methodology limits assessment of foundation models' fine-tuning potential
- Preprocessing requirements for medical images remain under-documented across models
- Performance gaps between models may be influenced by input resolution differences (518×518 vs 224×224) rather than fundamental architectural differences

## Confidence
- High confidence: Self-supervised models outperforming text-supervised models in segmentation tasks
- Medium confidence: Cross-attention integration improving segmentation performance across all models
- Medium confidence: Text-supervised models showing superior classification performance but weaker segmentation
- Low confidence: Specific relative performance rankings between models on cardiomegaly tasks due to private dataset

## Next Checks
1. **Preprocessing validation**: Systematically test RAD-DINO, CheXagent, and BiomedCLIP with different image formats (8-bit vs 16-bit DICOM, windowed vs normalized) to quantify preprocessing sensitivity and establish robust preprocessing pipelines for each model.

2. **Fine-tuning comparison**: Compare linear probing results with full fine-tuning on a subset of tasks to determine if the observed performance gaps persist when models are trained end-to-end, particularly for BiomedCLIP which showed inconsistent results.

3. **Resolution ablation study**: Evaluate each model's performance when input images are resized to a common resolution (e.g., 512×512) to isolate architectural differences from resolution-dependent effects, particularly addressing BiomedCLIP's 224×224 constraint.