---
ver: rpa2
title: Distilling Time Series Foundation Models for Efficient Forecasting
arxiv_id: '2601.12785'
source_url: https://arxiv.org/abs/2601.12785
tags:
- forecasting
- time
- series
- distillation
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DistilTS, the first distillation framework
  designed specifically for Time Series Foundation Models (TSFMs). DistilTS addresses
  two key challenges in TSFM distillation: task difficulty discrepancy, where optimization
  favors short-term forecasting over long-term horizons, and architecture discrepancy,
  where structural mismatches between teacher and student models hinder effective
  knowledge transfer.'
---

# Distilling Time Series Foundation Models for Efficient Forecasting

## Quick Facts
- arXiv ID: 2601.12785
- Source URL: https://arxiv.org/abs/2601.12785
- Reference count: 0
- Primary result: First distillation framework for TSFMs achieving 1/150 parameter reduction and 6000x speedup

## Executive Summary
This paper introduces DistilTS, the first distillation framework designed specifically for Time Series Foundation Models (TSFMs). The framework addresses two key challenges in TSFM distillation: task difficulty discrepancy where optimization favors short-term forecasting over long-term horizons, and architecture discrepancy where structural mismatches between teacher and student models hinder effective knowledge transfer. DistilTS introduces horizon-weighted objectives to balance learning across different forecasting horizons and a factorized temporal alignment module to bridge representational gaps between student and teacher models.

## Method Summary
DistilTS is a knowledge distillation framework that compresses large TSFMs into lightweight student models while maintaining forecasting performance. The framework uses a frozen TSFM teacher (like TimeMoE or iTransformer) and trains a smaller student model using three losses: a horizon-weighted distillation loss that applies exponentially increasing weights to later time steps, a factorized temporal alignment (FTA) module that projects student embeddings into the teacher's representation space, and a supervised loss against ground truth. The FTA module uses learned projection matrices and time embeddings to map student variate-wise embeddings to the teacher's point-wise hidden states. Experiments show DistilTS achieves comparable accuracy to full-sized TSFMs while reducing parameters by up to 1/150 and inference speed by up to 6000x.

## Key Results
- DistilTS reduces model parameters by up to 1/150 compared to full-sized TSFMs
- Achieves inference speedup of up to 6000x on a single RTX 3090 24GB GPU
- Maintains comparable forecasting accuracy to teacher models across multiple benchmarks
- Ablation studies show both horizon-weighted objectives and FTA module are essential for performance
- Larger teacher models (TimeMoE-200M vs 50M) provide only marginal distillation gains

## Why This Works (Mechanism)
The framework addresses the "seesaw effect" in multi-task forecasting where easier short-term tasks dominate training at the expense of harder long-term forecasting. By applying exponentially increasing weights to prediction errors at later time steps, the horizon-weighted loss forces the student to pay more attention to long-horizon accuracy. The FTA module bridges architectural gaps between teacher and student by projecting student embeddings into the teacher's representational space, enabling effective knowledge transfer even when models have different structural designs. This dual approach ensures both balanced learning across horizons and effective cross-architecture knowledge transfer.

## Foundational Learning

**Multi-task learning with imbalanced task difficulty**
- Why needed here: Forecasting at different horizons is treated as a multi-task problem where the "seesaw effect" causes short-term tasks to dominate training
- Quick check question: In a multi-task setting, why might average loss across all tasks not lead to equal performance on each task?

**Knowledge distillation**
- Why needed here: The entire framework is built on distilling knowledge from a large "teacher" TSFM to a compact "student" model
- Quick check question: What is the primary goal of knowledge distillation in model compression?

**Foundation model representation alignment**
- Why needed here: DistilTS's core technical contribution is an alignment module to bridge the representational gap between structurally different teacher and student models
- Quick check question: Why can a simple loss between the outputs of a teacher and student model be insufficient for effective distillation?

## Architecture Onboarding

**Component map:**
- Frozen Teacher Model (TimeMoE, Moirai, or Chronos) -> Point-wise hidden states H_T and predictions ŷ_T
- Student Model (DLinear or iTransformer) -> Variate-wise embeddings H_S and predictions ŷ_S
- Horizon-Weighted Loss (L_KD) -> Exponentially weighted prediction error
- Factorized Temporal Alignment (FTA) Module -> Projection head (Ws, Wout, Et, φ)
- Alignment Loss (L_FTA) -> MSE between H_T and projected Ĥ_T
- Supervised Loss (L_sup) -> MSE between ŷ_S and ground truth

**Critical path:**
1. Pass time series input through frozen Teacher to get point-wise hidden states H_T and predictions ŷ_T
2. Pass same input through Student to get variate-wise embeddings H_S and predictions ŷ_S
3. Use FTA Module to project H_S into teacher's representation space, creating pseudo point-wise states Ĥ_T
4. Compute Alignment Loss (L_FTA) between H_T and Ĥ_T
5. Compute Horizon-Weighted Distillation Loss (L_KD) between ŷ_T and ŷ_S
6. Compute Supervised Loss (L_sup) between ŷ_S and ground truth
7. Combine losses and update Student and FTA parameters only

**Design tradeoffs:**
- Student Architecture: DLinear is faster/simpler, iTransformer is more expressive but heavier. The paper uses iTransformer with variate-wise embeddings as a compromise
- Alignment vs. Output Loss: Relying only on output alignment is simpler but may miss internal representations. FTA adds complexity but better bridges architectural gaps
- Teacher Scale: Ablation studies suggest larger teachers (e.g., TimeMoE-200M vs 50M) offer only marginal distillation gains

**Failure signatures:**
- Good short-term, poor long-term forecasting: Suggests horizon weighting is insufficient or "seesaw effect" not being countered effectively
- No convergence or high training instability: Could indicate poor FTA initialization or excessively high learning rate
- Distilled model performs worse than baseline trained from scratch: Indicates teacher knowledge not being transferred effectively due to severe architectural mismatch

**First 3 experiments:**
1. Baseline Reproduction: Train DLinear and small iTransformer on ETTh1 without distillation to establish performance baseline
2. Distillation Ablation: Distill from TimeMoE-50M using only Horizon-Weighted Loss, then add FTA Loss to measure isolated contribution
3. Hyperparameter Sweep: Vary horizon weighting parameter τ to find optimal balance between short-term and long-term prediction fidelity on validation set

## Open Questions the Paper Calls Out

**Open Question 1:** How does DistilTS perform on extreme long-term forecasting horizons (e.g., 336 and 720 steps) that were explicitly excluded from evaluation?
- Basis: Section 3.1 states "we exclude horizons of 336 and 720 [30], since they extend up to 7.5× longer than the input context"
- Why unresolved: Paper restricts evaluation to horizons of 96 and 192, leaving efficacy on significantly longer prediction tasks unverified
- What evidence would resolve it: Benchmarking results on excluded TSLib standard horizons of 336 and 720 time steps

**Open Question 2:** To what extent does distillation preserve zero-shot generalization capabilities of teacher TSFM on entirely unseen domains?
- Basis: Introduction highlights TSFMs "show clear advantages in zero-shot scenarios" but experiments train student on specific target datasets
- Why unresolved: Unclear if student learns generalizable "foundation" representation or merely overfits to datasets used for distillation
- What evidence would resolve it: Evaluation on datasets completely held out from distillation training phase without fine-tuning

**Open Question 3:** How can distillation framework be adapted for teacher models using patch-based tokenization?
- Basis: Section 2.1.2 notes "most TSFMs employ patch-level or point-wise states" but FTA module specifically designed for point-wise teacher representations
- Why unresolved: Alignment mechanism may not effectively transfer structural knowledge from patch-based teacher models
- What evidence would resolve it: Ablation studies applying DistilTS to patch-based teacher model and analyzing current alignment module versus patch-aware alternative

## Limitations
- Private codebase for TSFMs makes direct verification impossible; key hyperparameters not specified
- 6000× speedup claim comparison to unspecified teacher configurations - could be TimeMoE-200M (100× parameters) or TimeMoE-50M (25× parameters)
- Results reported as average over 5 runs with no variance shown
- Ablation on teacher size shows marginal gains but exact teacher architectures for each baseline not specified

## Confidence

**High confidence:** Core technical contributions (horizon-weighted loss, FTA alignment module) are clearly specified and follow established knowledge distillation principles

**Medium confidence:** Reported performance improvements are plausible given architectural changes but cannot be independently verified without access to codebase and full hyperparameter settings

**Low confidence:** Exact teacher model configurations used for each experiment and comparison methodology for speedup claims

## Next Checks
1. **Ablation on Horizon Weighting:** Re-run distillation with τ=0.5, 1.0, and 2.0 on single dataset to verify seesaw effect mitigation
2. **Teacher Size Sensitivity:** Test distillation from TimeMoE-50M vs TimeMoE-200M on same student architecture to validate claimed marginal gains
3. **Speed Benchmark Replication:** Measure inference time on RTX 3090 24GB for student model vs exact teacher configuration used in experiments to verify 6000× speedup claim