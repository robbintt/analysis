---
ver: rpa2
title: Reducing Large Language Model Safety Risks in Women's Health using Semantic
  Entropy
arxiv_id: '2503.00269'
source_url: https://arxiv.org/abs/2503.00269
tags:
- semantic
- uncertainty
- responses
- clinical
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study demonstrates that semantic entropy (SE) outperforms
  traditional perplexity in detecting uncertainty and potential hallucinations in
  large language models (LLMs) used for women''s health. Using a clinically validated
  dataset of MRCOG examination questions, SE achieved an AUROC of 0.76 (95% CI: 0.75-0.78)
  compared to 0.62 (95% CI: 0.60-0.65) for perplexity.'
---

# Reducing Large Language Model Safety Risks in Women's Health using Semantic Entropy

## Quick Facts
- arXiv ID: 2503.00269
- Source URL: https://arxiv.org/abs/2503.00269
- Authors: Jahan C. Penny-Dimri; Magdalena Bachmann; William R. Cooke; Sam Mathewlynn; Samuel Dockrey; John Tolladay; Jannik Kossen; Lin Li; Yarin Gal; Gabriel Davis Jones
- Reference count: 37
- Primary result: Semantic entropy achieves AUROC 0.76 for uncertainty detection vs 0.62 for perplexity in women's health clinical questions

## Executive Summary
This study introduces semantic entropy (SE) as a superior method for detecting uncertainty and potential hallucinations in large language models used for women's health clinical applications. By generating multiple responses to the same clinical prompt and clustering them by semantic meaning using bidirectional entailment, SE measures the entropy of meaning clusters rather than token probability. Tested on 1,644 MRCOG examination questions, SE demonstrated significantly better performance than traditional perplexity metrics, with expert clinical validation confirming its effectiveness for uncertainty discrimination.

## Method Summary
The study evaluated semantic entropy by generating 10 responses per clinical question using GPT-4o at temperature 1.0, clustering responses by semantic meaning using bidirectional entailment, and calculating entropy over the resulting cluster distribution. Performance was compared against perplexity using AUROC to discriminate correct from incorrect answers on a private dataset of 1,644 MRCOG women's health examination questions. Clinical expert validation was performed to assess the effectiveness of SE beyond automated LLM-based evaluation.

## Key Results
- Semantic entropy achieved AUROC of 0.76 (95% CI: 0.75-0.78) compared to 0.62 (95% CI: 0.60-0.65) for perplexity
- Expert clinical validation confirmed near-perfect uncertainty discrimination with SE achieving AUROC of 0.97
- Semantic clustering was successful in only 30% of cases, representing a significant scalability challenge
- Higher temperature settings improved discrimination capability, with AUROC increasing from 0.71 at temp 0.2 to 0.76 at temp 1.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Assessing uncertainty at the semantic level detects clinical hallucinations more effectively than token-level probability.
- Mechanism: Traditional perplexity measures word sequence likelihood, while SE generates multiple responses and measures entropy of meaning clusters. Semantic inconsistencies manifest as high SE even when individual response perplexity is low.
- Core assumption: Hallucinations appear as semantic inconsistencies rather than low-probability word choices.
- Evidence anchors: SE achieved AUROC 0.76 vs 0.62 for perplexity; unlike perplexity, SE measures variability in meaning across multiple responses.
- Break condition: Consistently wrong but semantically identical answers will show low SE, failing to flag the hallucination.

### Mechanism 2
- Claim: Increasing generation randomness improves uncertainty metric discrimination.
- Mechanism: Higher temperature forces the model to explore wider probability space, increasing response variance. Stable answers cluster together while unstable answers fragment into conflicting clusters.
- Core assumption: Correct answers have stable semantic attractors while incorrect answers are unstable under perturbation.
- Evidence anchors: AUROC increased for all metrics as temperature rose; SE improved from 0.71 (temp 0.2) to 0.76 (temp 1.0).
- Break condition: Excessive temperature causes valid answers to fragment, creating false positives.

### Mechanism 3
- Claim: Automated LLM evaluation underestimates SE performance compared to human expert validation.
- Mechanism: LLM-based entailment scoring struggles with clinical reasoning nuances compared to human specialists, leading to under-estimation of SE's effectiveness.
- Core assumption: Ground truth answers and entailment logic contain noise that human experts can override.
- Evidence anchors: Expert validation confirmed SE's effectiveness with AUROC 0.97; effectiveness was under-estimated when scored by LLM itself.
- Break condition: Significant inter-rater variability among human experts could make the perfect AUROC an artifact.

## Foundational Learning

- **Bidirectional Entailment**
  - Why needed here: This is the engine of Semantic Entropy, determining when two sentences have equivalent meaning regardless of word choice.
  - Quick check question: Do "The patient is pregnant" and "The patient has a fetus" entail each other?

- **AUROC (Area Under the Receiver Operating Characteristic)**
  - Why needed here: The paper uses AUROC to quantify how well SE separates correct from incorrect answers.
  - Quick check question: If SE has an AUROC of 0.76 and perplexity has 0.62, which metric is better at flagging a hallucination?

- **Temperature in LLMs**
  - Why needed here: The mechanism relies on generating diverse responses, with temperature controlling this diversity.
  - Quick check question: Should you set temperature to 0.0 or 1.0 when trying to measure semantic variance?

## Architecture Onboarding

- **Component map**: Input (Clinical prompt) -> Generator (GPT-4o, Temp=1.0) -> Sampler (Generates N responses) -> Clusterer (Uses NLI model for bidirectional entailment) -> Calculator (Computes entropy over clusters) -> Evaluator (Compares SE scores against ground truth)

- **Critical path**: The Clusterer is the bottleneck. If the entailment model fails to recognize equivalent concepts, SE will incorrectly report high uncertainty.

- **Design tradeoffs**: Automated clustering is cheap but error-prone (30% success rate); human clustering is accurate but unscalable. Low temperature misses variance; high temperature risks incoherence.

- **Failure signatures**: "False Certainty" (low SE but wrong answer when model is confidently hallucinating); "False Uncertainty" (high SE but correct answer when entailment fails to merge synonyms).

- **First 3 experiments**:
  1. Replicate the Temperature Sweep: Run same prompt set at Temp 0.2 and 1.0 to verify AUROC improvement for your domain data.
  2. Cluster Auditing: Manually inspect 20 random samples where LLM scored "high uncertainty" to distinguish clustering failure from genuine hallucination.
  3. Length Stress Test: Test SE on long-form advice vs short-form diagnoses, as shorter responses showed higher accuracy and better calibration.

## Open Questions the Paper Calls Out

- Can semantic entropy retain superior uncertainty discrimination when applied to unstructured clinical data like electronic medical records?
- Can domain-specific LLMs or Large Concept Models improve semantic clustering success rate beyond the observed 30%?
- Can semantic uncertainty metrics be effectively adapted for multimodal inputs such as clinical images or tables?

## Limitations

- Reliance on a private, clinically validated dataset that prevents direct replication and external validation
- 30% success rate for semantic clustering represents significant scalability constraints
- Automated LLM-based evaluation underestimates true SE performance, requiring expert validation that may not be feasible in real-world deployment

## Confidence

**High Confidence**: The core finding that semantic entropy outperforms perplexity in detecting uncertainty (AUROC 0.76 vs 0.62) is supported by both automated and expert validation, with expert-verified AUROC of 0.97 demonstrating robust discrimination capability.

**Medium Confidence**: The temperature-dependent improvement in uncertainty discrimination is theoretically sound and empirically observed, but optimal temperature may vary across different LLM architectures and clinical domains.

**Low Confidence**: The claim that SE could enable reliable AI integration into clinical practice in resource-limited settings assumes successful deployment at scale, which the 30% clustering failure rate makes uncertain without substantial infrastructure for human oversight.

## Next Checks

1. **Domain Transfer Validation**: Test SE performance on a different medical specialty (e.g., cardiology or oncology) using an open medical question dataset with expert-verified ground truth to assess generalizability beyond women's health.

2. **Clustering Robustness Audit**: Systematically analyze the 70% of cases where clustering fails to identify whether failures stem from model limitations, prompt ambiguity, or inherent semantic ambiguity in clinical questions.

3. **Real-World Deployment Simulation**: Create a mixed human-AI validation pipeline where SE flags uncertain cases for expert review, measuring false positive/negative rates and computational costs compared to current clinical decision support systems.