---
ver: rpa2
title: 'Oversampling and Downsampling with Core-Boundary Awareness: A Data Quality-Driven
  Approach'
arxiv_id: '2509.19856'
source_url: https://arxiv.org/abs/2509.19856
tags:
- data
- points
- core
- class
- oversampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to systematically identify and differentiate
  between core and border data points in imbalanced classification tasks. The core
  method uses a distance-based approach to classify points based on their relative
  proximity to neighboring samples, distinguishing between core points (deep within
  clusters) and border points (near decision boundaries).
---

# Oversampling and Downsampling with Core-Boundary Awareness: A Data Quality-Driven Approach

## Quick Facts
- **arXiv ID:** 2509.19856
- **Source URL:** https://arxiv.org/abs/2509.19856
- **Reference count:** 39
- **Primary result:** Borderline-aware oversampling improves F1 scores by up to 10% on 96% of benchmark datasets while core-aware downsampling compresses datasets up to 90% without accuracy loss.

## Executive Summary
This paper introduces a systematic method for identifying and differentiating between core and border data points in imbalanced classification tasks. The approach uses distance-based classification to distinguish core points (deep within clusters) from border points (near decision boundaries). By selectively augmenting minority class border points and removing redundant majority class core points, the method achieves up to 10% F1 score improvement and 90% dataset compression while preserving accuracy. The technique has broader implications for efficient model training in computationally expensive domains like LLM training.

## Method Summary
The method classifies data points based on their relative proximity to neighboring samples within each class. For each point, average k-nearest neighbor distance is computed and compared against a percentile-based threshold. Points exceeding this threshold are classified as border points, while those below are core points. The method then applies borderline-aware oversampling (augmenting only minority border points) and core-aware reduction (removing majority core points while preserving border points). This selective sampling approach improves model performance while reducing computational costs.

## Key Results
- Borderline oversampling improves F1 score by up to 10% on 96% of benchmark datasets
- Dataset compression up to 90% while preserving classification accuracy
- Core-aware reduction method is 10 times more powerful than the original dataset
- Average improvement of 4.26% F1 across 50 datasets with only 2 showing negative or no improvement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selective oversampling of minority class border points improves F1 scores compared to indiscriminate oversampling.
- **Mechanism:** The method computes average k-nearest neighbor distance for each minority class instance. Points exceeding a percentile-based threshold are classified as border points. Synthetic samples are generated only for these border instances, reinforcing the decision boundary where class separation is most uncertain.
- **Core assumption:** Border points near decision boundaries contain higher information density for learning class separation than core points deep within class clusters.
- **Evidence anchors:**
  - [abstract] "boundary data oversampling method improves the F1 score by up to 10% on 96% of the datasets"
  - [section 5.1, Table 2] Average improvement of 4.26% across 50 datasets; only 2 datasets showed negative or no improvement
  - [corpus] Weak direct corpus evidence; related work (GK-SMOTE, Kernel-Based Enhanced Oversampling) similarly targets noise handling and synthetic sample quality but does not specifically validate the core-boundary distinction mechanism.
- **Break condition:** Dense datasets with uniform point distribution (e.g., donut-shaped patterns in Figure 3) where core/boundary distinction becomes ambiguous, potentially causing overfitting to local variations.

### Mechanism 2
- **Claim:** Core-aware downsampling of majority class instances maintains classification accuracy while reducing dataset size by up to 25%.
- **Mechanism:** Majority class core points are identified via low average k-NN distance and systematically removed. Border points in both classes are preserved, maintaining decision boundary representation while eliminating redundant samples.
- **Core assumption:** Majority class core points contribute minimally to decision boundary definition and represent redundant information.
- **Evidence anchors:**
  - [abstract] "core-aware reduction method compresses datasets up to 90% while preserving accuracy"
  - [section 5.2, Table 6] F1 scores remain stable up to 20-25% compression across Air Quality, Rice, NHANES, and Churn datasets
  - [corpus] No direct corpus validation of this specific core-reduction mechanism; related downsampling work focuses on noise removal rather than core redundancy elimination.
- **Break condition:** Compression beyond 40% causes notable F1 score degradation (Table 6), particularly in datasets where core points may carry subtle distributional information.

### Mechanism 3
- **Claim:** A distance-based classification threshold using percentile of k-NN average distances reliably separates core from border points.
- **Mechanism:** For each class, compute the average distance d_xi from each point to its k-nearest neighbors. Set threshold d_t as the P_α percentile of this distance distribution. Points with d_xi > d_t are border points; d_xi ≤ d_t are core points.
- **Core assumption:** The percentile parameter α can be set to meaningfully capture the distinction between dense-cluster-interior and boundary-proximal points across diverse dataset geometries.
- **Evidence anchors:**
  - [section 3.2.1] Formal definition of distance metric and threshold calculation
  - [section 3.2.2] Classification rule for border vs. core points
  - [corpus] No corpus papers directly validate this percentile-based thresholding approach; DBSCAN-related work uses fixed-radius density thresholds rather than adaptive percentile cutoffs.
- **Break condition:** High-dimensional spaces where distance metrics become less discriminative (curse of dimensionality); highly imbalanced datasets where minority class has insufficient points for reliable k-NN distance estimation.

## Foundational Learning

- **Concept: k-Nearest Neighbors (k-NN) Distance Metrics**
  - **Why needed here:** The entire core-boundary classification relies on computing average distances to k nearest neighbors. Understanding how distance metrics behave in different feature spaces is essential for debugging threshold selection.
  - **Quick check question:** If you double all feature values through normalization, will the core/boundary classification change? Why or why not?

- **Concept: Percentile-Based Thresholding**
  - **Why needed here:** The method uses percentile P_α of distance distributions rather than absolute thresholds. This adapts to dataset-specific distance scales but requires understanding of distributional properties.
  - **Quick check question:** For a highly skewed distance distribution with many outliers, would using the 90th percentile vs. the median produce dramatically different core/boundary classifications?

- **Concept: Class Imbalance and Decision Boundaries**
  - **Why needed here:** The paper's core thesis is that not all minority samples are equally valuable—those near decision boundaries matter most. Understanding why standard classifiers struggle with imbalance frames the problem.
  - **Quick check question:** In a binary classification problem with 95% majority class, why might a classifier achieve 95% accuracy while failing completely on the minority class?

## Architecture Onboarding

- **Component map:** Input dataset → Per-class distance matrix computation → k-NN average distance calculation → Percentile threshold determination → Core/border labeling → Selective sampling execution → Modified dataset output for training

- **Critical path:**
  1. Input dataset → 2. Per-class distance matrix computation → 3. k-NN average distance calculation → 4. Percentile threshold determination → 5. Core/border labeling → 6. Selective sampling execution → 7. Modified dataset output for training

- **Design tradeoffs:**
  - **k value selection:** Small k captures local structure but sensitive to noise; large k smooths boundaries but may miss critical edge cases. Paper does not specify optimal k.
  - **Percentile α:** Lower α classifies more points as border (aggressive augmentation); higher α is more conservative. Paper does not provide selection guidance.
  - **Computational cost:** O(n²) distance computations for n points; may require approximation for very large datasets
  - **Distance metric choice:** Euclidean (p=2) assumed; Manhattan (p=1) or other norms may suit different data geometries

- **Failure signatures:**
  - **Empty border sets:** If α is too high or data is highly clustered, no points exceed threshold → no oversampling occurs
  - **All-border classification:** If α is too low or data is uniformly sparse, all points classified as border → no downsampling benefit
  - **Numerical instability:** Features with vastly different scales dominate distance calculations → incorrect core/boundary labels
  - **Memory exhaustion:** Full distance matrix for large datasets exceeds available RAM

- **First 3 experiments:**
  1. **Baseline validation:** Apply the method to a synthetic 2D dataset with known core/boundary structure (e.g., concentric circles). Verify that visualized classifications match intuition and that borderline oversampling improves linear classifier accuracy.
  2. **Hyperparameter sensitivity:** On 3 benchmark datasets from Table 8 (e.g., ecoli1, glass0, yeast subset), sweep k ∈ {3, 5, 10, 20} and α ∈ {70, 80, 90, 95} to characterize stability of F1 improvements. Document optimal ranges.
  3. **Scaling test:** Apply core-aware reduction to the HIGGS dataset (100K subset as in paper) with compression targets of 25%, 50%, 70%. Measure both accuracy retention and runtime/memory reduction compared to full-data training.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the core-boundary framework effectively identify high-quality data in high-dimensional text embeddings for LLM training?
- **Basis in paper:** [explicit] The conclusion explicitly proposes extending the method to "text, multimodal, and self-supervised learning scenarios" to reduce LLM training costs.
- **Why unresolved:** The current study restricts validation to tabular datasets with relatively low feature counts, whereas text embeddings are high-dimensional and sparse.
- **What evidence would resolve it:** Empirical results showing convergence speed or performance improvements when training LLMs on text data filtered by this adaptive resampling method.

### Open Question 2
- **Question:** How does the distance-based classification of core and border points perform in high-dimensional feature spaces where the "curse of dimensionality" affects distance metrics?
- **Basis in paper:** [inferred] The paper acknowledges difficulty distinguishing points in dense regions (Figure 3) and relies on p-norm distances, yet experiments are limited to datasets with fewer than 50 features.
- **Why unresolved:** Euclidean distances tend to converge in high dimensions, potentially making the percentile-based threshold ($d_t$) ineffective for distinguishing core vs. boundary points in complex domains like image analysis.
- **What evidence would resolve it:** Experiments on datasets with hundreds of features showing that the relative proximity metric remains discriminative for core and border separation.

### Open Question 3
- **Question:** What is the sensitivity of the distance threshold percentile ($P_\alpha$) and the number of neighbors ($k$) across different data distributions?
- **Basis in paper:** [inferred] The methodology defines the threshold $d_t$ based on a percentile of distances but does not elaborate on how $\alpha$ is optimized or if it requires manual tuning.
- **Why unresolved:** The paper does not include an ablation study on these hyperparameters, leaving it unclear if the 25% compression rate is robust or parameter-dependent.
- **What evidence would resolve it:** A systematic ablation study showing F1 score variance as $\alpha$ and $k$ change across diverse dataset geometries.

## Limitations
- No corpus validation exists for the core-reduction mechanism or percentile-based thresholding approach
- Hyperparameter sensitivity to k and α values is not characterized across diverse dataset geometries
- Performance in high-dimensional spaces (100+ features) remains untested
- Computational complexity of O(n²) distance calculations limits scalability to very large datasets

## Confidence
- **Mechanism 1 (Oversampling):** Medium - Strong empirical results (96% of datasets improved) but limited theoretical grounding for percentile-based threshold selection
- **Mechanism 2 (Downsampling):** Low - Relies on untested assumption that majority core points are uniformly redundant; no corpus validation of this specific approach
- **Mechanism 3 (Classification):** Low - Percentile-based thresholding lacks rigorous justification and has no direct corpus validation

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically sweep k ∈ {3, 5, 10, 20} and α ∈ {70, 80, 90, 95} on 5 diverse benchmark datasets to characterize F1 score stability and identify robust parameter ranges. Document failure modes when parameters are poorly chosen.

2. **High-Dimensional Performance Test:** Apply the method to synthetic datasets with 50-100 features where class clusters overlap. Compare core/border classification accuracy against ground truth cluster labels and measure distance metric degradation relative to 2D cases.

3. **Large-Scale Memory Efficiency Evaluation:** Test the full distance matrix computation approach on datasets of 1M points using both dense and sparse storage. Quantify memory usage, verify whether approximate nearest neighbor methods maintain F1 improvements, and establish practical size limits for the current implementation.