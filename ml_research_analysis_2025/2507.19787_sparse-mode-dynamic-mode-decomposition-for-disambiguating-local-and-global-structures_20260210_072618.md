---
ver: rpa2
title: Sparse-mode Dynamic Mode Decomposition for Disambiguating Local and Global
  Structures
arxiv_id: '2507.19787'
source_url: https://arxiv.org/abs/2507.19787
tags:
- modes
- data
- mode
- which
- sparse-mode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces sparse-mode dynamic mode decomposition (DMD),
  a novel variant of optimized DMD that leverages sparsity-promoting regularization
  to disambiguate between spatially local and global DMD modes. The algorithm maintains
  noise-robustness while automatically separating discrete (local) and continuous
  (global) spectral components in an unsupervised manner.
---

# Sparse-mode Dynamic Mode Decomposition for Disambiguating Local and Global Structures

## Quick Facts
- **arXiv ID:** 2507.19787
- **Source URL:** https://arxiv.org/abs/2507.19787
- **Reference count:** 40
- **Primary result:** Sparse-mode DMD uses sparsity-promoting regularization to automatically separate spatially local and global DMD modes while maintaining noise-robustness

## Executive Summary
This work introduces sparse-mode dynamic mode decomposition (DMD), a novel variant that leverages sparsity-promoting regularization to disambiguate between spatially local and global DMD modes. The algorithm maintains noise-robustness while automatically separating discrete (local) and continuous (global) spectral components in an unsupervised manner. By utilizing variable projection with proximal gradient methods, sparse-mode DMD handles noisy, unevenly-sampled data and supports various sparse regularizers. Applications demonstrate successful recovery of spatiotemporal modes in synthetic video data, optical waveguide systems with both discrete and continuous spectra, and real-world sea surface temperature data.

## Method Summary
Sparse-mode DMD solves an optimization problem that adds sparse regularizer ψ(Φb) to the standard DMD fitting objective. The algorithm uses variable projection to decouple eigenvalue optimization (via Levenberg-Marquardt) from sparse mode estimation (via proximal gradient methods or SR3). A threshold-based classifier automatically partitions modes into global and local categories, applying sparsity penalties only to local modes. SR3 relaxation provides faster convergence and reduced eigenvalue bias compared to direct proximal gradient approaches.

## Key Results
- Successfully recovered spatiotemporal modes in synthetic video data with known ground truth
- Isolated local El Niño warming band in sea surface temperature data while preserving global seasonal patterns
- Demonstrated improved accuracy over standard DMD in systems with mixed discrete and continuous spectral content
- SR3 variant showed faster convergence and reduced eigenvalue bias compared to FISTA

## Why This Works (Mechanism)

### Mechanism 1
Applying sparsity-promoting regularization to DMD modes enables automatic separation of spatially local structures from global ones. The algorithm solves a modified optimization objective that adds a sparse regularizer to the standard DMD fitting problem. During variable projection, modes are updated via proximal gradient methods, which iteratively shrink small coefficients toward zero. Global modes—those exceeding activity threshold τactive across >τglobal fraction of spatial points—are excluded from regularization, preserving their spatially extended structure while local modes become sparse. This works because local structures have compact spatial support while global structures have broad spatial support.

### Mechanism 2
Variable projection separates the nonlinear eigenvalue optimization from the sparse mode estimation, enabling efficient alternating updates. The optimization exploits separable structure: for fixed eigenvalues ω, the mode matrix Φb is computed via proximal methods; for fixed Φb, eigenvalues are updated via Levenberg-Marquardt. This decoupling allows each subproblem to use specialized solvers—proximal operators handle non-smooth sparsity penalties while Gauss-Newton-type iterations handle the smooth eigenvalue objective. This works because the DMD fitting objective is sufficiently well-conditioned near the optimum that alternating updates converge.

### Mechanism 3
The SR3 relaxation provides faster convergence and reduced eigenvalue bias compared to direct proximal gradient. SR3 introduces an auxiliary matrix W that absorbs the sparsity penalty, leaving the Φb update as a smooth least-squares problem. Alternating between W (via proximal operator) and Φb (via linear solve) avoids the eigenvalue shrinkage bias inherent in ℓ1 regularization. A final debiasing step recomputes Φb entries only at nonzero W locations using standard least squares. This works because the relaxation parameter η is sufficiently small that Φb ≈ W at convergence, allowing proper support identification.

## Foundational Learning

- **Concept: Variable projection for separable nonlinear least squares**
  - Why needed here: The entire algorithm builds on decoupling linear parameters (modes) from nonlinear parameters (eigenvalues). Understanding how the pseudoinverse solution for Φb emerges from fixing ω is essential.
  - Quick check question: Given fixed eigenvalues ω and data matrix X, write the closed-form solution for the optimal mode matrix Φb that minimizes ||X - Φb T(ω)||²_F.

- **Concept: Proximal operators and soft/hard thresholding**
  - Why needed here: Sparse-mode DMD relies on proximal gradient methods. You must understand how proximal operators implement sparsity via thresholding operations.
  - Quick check question: For ℓ1 regularization with parameter λ, compute the proximal operator prox_{λ||·||_1}(y) for a scalar y ∈ ℝ. What happens when |y| < λ?

- **Concept: Levenberg-Marquardt damping and trust-region interpretation**
  - Why needed here: Eigenvalue updates use damped Gauss-Newton steps. Understanding how damping parameter ν controls step size and convergence behavior is critical for debugging.
  - Quick check question: In Levenberg-Marquardt, does increasing the damping parameter ν make the update δ more conservative or more aggressive? How does the update rule adapt ν based on improvement ratio?

## Architecture Onboarding

- **Component map:** Input data matrix X → Eigenvalue optimizer (Levenberg-Marquardt) → Mode optimizer (FISTA/SR3) → Global-local classifier → Debiasing module (SR3 only) → Output sparse modes and eigenvalues

- **Critical path:**
  1. Initialize ω₀ (e.g., via standard DMD eigenvalues or random)
  2. Compute initial Φb via sparse mode update (Algorithm 3 or 4)
  3. Loop: (a) compute L-M step δ for eigenvalues using current Φb, (b) update ω, (c) recompute Φb via sparse update, (d) check convergence on objective F_s
  4. Output: sparse mode matrix Φb, eigenvalues ω, time dynamics T(ω)

- **Design tradeoffs:**
  - FISTA vs. SR3: FISTA handles any prox-friendly regularizer but introduces eigenvalue bias with ℓ₁; SR3 is faster and debiased but requires tuning relaxation parameter η
  - ℓ₀ vs. ℓ₁ regularizer: ℓ₀ produces exact sparsity with sharp edges but is non-convex (local minima risk); ℓ₁ is convex and stable but biases coefficient magnitudes
  - Compression vs. accuracy: POD compression speeds eigenvalue updates but may discard subtle local features critical for sparsity detection
  - Global-local threshold strictness: Low τglobal preserves more modes as global (less sparsity); high τglobal risks over-sparsifying truly global structures

- **Failure signatures:**
  - Eigenvalues diverge or cluster unrealistically → damping parameter ν not adapting properly; check improvement ratio calculation
  - All modes become identically zero → λ too large; reduce sparsity strength by 10×
  - Global modes incorrectly sparsified → τglobal threshold too high or τactive too low; examine mode spatial support histograms
  - SR3 fails to converge → η too small causing ill-conditioning; increase η by 10×

- **First 3 experiments:**
  1. Synthetic validation: Generate data with known local/global modes. Test recovery accuracy vs. noise level for both FISTA and SR3. Verify global-local splitting correctly identifies gradient mode as global.
  2. Hyperparameter sweep on SST data: Using HadISST data with r=8, sweep λ and plot reconstruction error vs. mode sparsity. Identify λ that isolates El Niño band without degrading seasonal mode.
  3. Ablation of global-local classification: Compare three variants on waveguide data: (a) no global-local split, (b) oracle global-local (manual selection), (c) automatic threshold-based. Measure discrete spectrum recovery accuracy and continuous spectrum representation quality.

## Open Questions the Paper Calls Out

- **Can the global-local splitting procedure be formulated as a strict optimization problem to automatically determine which modes to sparsify, rather than relying on threshold heuristics?**
- **Does group sparsity regularization provide superior feature extraction compared to the element-wise or row-wise sparsity currently implemented?**
- **How do the temporal frequencies of external forcing phenomena (e.g., solar radiation) interact with the local El Niño frequencies identified by sparse-mode DMD?**
- **What is the principled, automated method for selecting the regularization intensity (λ) and SR3 relaxation parameter (η) without relying on reconstruction error against a clean training set?**

## Limitations
- The automatic global-local classification threshold choice (τactive=0.1, τglobal=0.5) lacks systematic sensitivity analysis across diverse datasets
- The assumption that spatial sparsity correlates with discrete vs continuous spectral content is only partially validated
- The SR3 relaxation parameter η is not systematically tuned, potentially affecting debiasing effectiveness

## Confidence

- **High confidence:** Variable projection framework and alternating optimization structure (well-established in literature)
- **Medium confidence:** Global-local separation effectiveness (validated on synthetic and two real datasets only)
- **Low confidence:** Threshold parameter robustness across diverse applications

## Next Checks

1. Conduct systematic threshold sensitivity analysis: Sweep τglobal from 0.3 to 0.7 on SST data and quantify impact on El Niño mode isolation vs. seasonal mode preservation
2. Test algorithm performance on dataset with mixed discrete-continuous spectrum but weak spatial sparsity correlation (e.g., convection rolls with varying wavelengths)
3. Compare eigenvalue bias between FISTA and SR3 across noise levels σ=0.1 to 1.0 on synthetic data, measuring |Re(ω)| to verify SR3's debiasing advantage claims