---
ver: rpa2
title: Improving physics-informed neural network extrapolation via transfer learning
  and adaptive activation functions
arxiv_id: '2507.12659'
source_url: https://arxiv.org/abs/2507.12659
tags:
- equation
- points
- neural
- extrapolation
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of poor extrapolation performance
  in Physics-Informed Neural Networks (PINNs) when predicting beyond the training
  domain. The authors propose a transfer learning (TL) method that combines adaptive
  activation functions with targeted fine-tuning of the final layer.
---

# Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions

## Quick Facts
- arXiv ID: 2507.12659
- Source URL: https://arxiv.org/abs/2507.12659
- Reference count: 40
- Key outcome: Transfer learning with adaptive activation functions reduces PINN extrapolation error by 40% (L2) and 50% (MAE) on 1D PDEs

## Executive Summary
This paper addresses the challenge of poor extrapolation performance in Physics-Informed Neural Networks (PINNs) when predicting beyond the training domain. The authors propose a transfer learning (TL) method that combines adaptive activation functions with targeted fine-tuning of the final layer. The approach involves training the network on an extended domain, selecting high-loss collocation points from the validation region, and retraining only the final layer's parameters and activation function weights. The method achieves significant improvements, reducing relative L2 error by 40% and mean absolute error by 50% on average across three benchmark PDEs (Allen-Cahn, Korteweg-de Vries, and Burgers equations) without significant computational cost increase. The key innovation is the use of adaptive linear combinations of activation functions, particularly optimized combinations of the preferred activation function for each specific PDE, which improves both robustness and accuracy.

## Method Summary
The method employs a two-phase training approach. Phase 1 trains a standard PINN on the initial training domain using L-BFGS optimization with early stopping based on validation error. Phase 2 implements transfer learning by freezing the base network layers and retraining only the final layer and adaptive activation function parameters on 80 high-loss collocation points sampled from the validation domain. The adaptive activation function takes the form of a linear combination of standard activation functions, with the specific combination optimized for each PDE. Boundary and initial conditions are enforced as hard constraints through a mathematical transformation of the network output, reducing the loss function to purely the PDE residual. The method uses L2 regularization during the TL phase and selects the top 80 collocation points with the highest PDE loss from a pool of 4,000 sampled points.

## Key Results
- 40% reduction in relative L2 error on average across three benchmark PDEs
- 50% reduction in mean absolute error on average across three benchmark PDEs
- No significant computational cost increase compared to baseline PINN training
- Adaptive linear combinations of preferred activation functions outperform diverse activation function dictionaries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Restricting fine-tuning to the final layer with high-loss collocation points improves extrapolation efficiency while mitigating catastrophic forgetting.
- **Mechanism:** The authors utilize a Transfer Learning (TL) phase where base layers are frozen. By sampling a large number of points in the validation domain and selecting only the 80 points with the highest PDE loss (residuals), the optimization targets regions of maximum generalization error. The layer-wise gradient analysis indicates final layers contribute most to task-specific adaptation, allowing focused adjustments without disrupting the low-level feature extraction of the frozen base.
- **Core assumption:** The feature representations learned in the initial training phase are sufficiently generic and robust to support extrapolation if only the output mapping is adjusted.
- **Evidence anchors:**
  - [abstract] "...transfer learning (TL) within an extended training domain, using only a small number of carefully selected collocation points."
  - [section 3] "We retrain the network on those few collocation points... updating only the final layer's weights... This helps refine the model's predictive ability... without disrupting earlier learned dynamics."
  - [corpus] Weak direct evidence; neighbor papers like "Stiff Transfer Learning" and "CTL-PINN" focus on domain decomposition or curriculum learning rather than final-layer residual sampling.
- **Break condition:** If the base network has failed to converge or has learned trivial features during initial training, fine-tuning the final layer cannot correct the fundamental representation.

### Mechanism 2
- **Claim:** Adaptive linear combinations of activation functions (AFs) in the final layer provide better alignment with solution characteristics than generic or diverse AF mixtures.
- **Mechanism:** Rather than using a fixed activation or a generic mix (like ABU-PINN), the method employs a linear combination of the specific AF known to perform well for that PDE (e.g., `lctanh` for Allen-Cahn). This adaptability allows the output layer to better approximate the specific nonlinearities of the target solution. The authors note that "increasing the diversity of AFs... is suboptimal" compared to specializing the preferred AF.
- **Core assumption:** A PDE-specific "preferred" AF exists and forms a superior basis for the solution manifold than a generic dictionary of functions.
- **Evidence anchors:**
  - [abstract] "...adaptive AF that takes the form of a linear combination of standard AFs, which improves both the robustness and accuracy..."
  - [section 6] "...the key to improved extrapolation lies in the adaptability introduced by learnable AF weights and that increasing the diversity of AFs... is suboptimal."
  - [corpus] "REAct" supports the importance of specialized activations for PINNs, though via rational exponentials rather than linear combinations.
- **Break condition:** If the preferred AF is misspecified (e.g., using periodic components for a non-periodic solution), the adaptive combination may introduce spectral bias or instability.

### Mechanism 3
- **Claim:** Hard constraint formulation stabilizes training by strictly enforcing boundary and initial conditions.
- **Mechanism:** The network outputs a variable $v(t,x)$ which is then mathematically transformed to $u(t,x)$ (e.g., $u = u_0 + t(1-x^2)v$). This guarantees that $u$ satisfies initial and boundary conditions exactly, reducing the loss function to purely the PDE residual. This avoids the difficulty of balancing data loss ($w_u$) against physics loss ($w_f$).
- **Core assumption:** The physical constraints are known and differentiable, allowing for a closed-form transformation of the network output.
- **Evidence anchors:**
  - [section 3] "To enforce the initial and boundary conditions... as hard constraints, we perform a change of variables... ensuring that the constraints are satisfied exactly."
  - [section 5.1] "Then u automatically satisfies the initial and boundary conditions, and we only need to impose the PDE constraint on v."
  - [corpus] Not explicitly discussed in neighbors, though standard in advanced PINN implementations.
- **Break condition:** If the domain geometry is highly irregular or complex, constructing the hard constraint transformation becomes analytically difficult or impossible.

## Foundational Learning

- **Concept: Residual-based Sampling**
  - **Why needed here:** The method relies on identifying "high-loss" collocation points to guide the transfer learning phase. Without understanding that high loss $\approx$ high model uncertainty/poor fit, the logic of sampling only 80 specific points is lost.
  - **Quick check question:** If a collocation point has a loss of 0.001 and another has 0.5, which one provides more information for refining the model in the extrapolation domain?

- **Concept: Hard Constraints vs. Soft Constraints**
  - **Why needed here:** The paper enforces physics via a change of variables (hard constraint) rather than penalty terms (soft constraint). This distinction is critical for understanding why the loss function lacks boundary weighting terms ($w_u, w_f$).
  - **Quick check question:** Does the loss function penalize the network if the boundary condition is violated at $t=0$? (Answer: No, it is mathematically impossible by construction).

- **Concept: Transfer Learning (Freezing)**
  - **Why needed here:** The core innovation is freezing the base network. Learners must understand that "freezing" stops gradient updates to prevent the model from unlearning the base dynamics while it adapts to the validation domain.
  - **Quick check question:** During the TL phase, if the loss drops significantly but the base weights remain identical, has the model learned a new physics law or just a new mapping for the existing features?

## Architecture Onboarding

- **Component map:** Input $(t, x)$ -> Base Network (6 layers, 32 neurons/layer, tanh activation, frozen after Phase 1) -> Final Layer (neurons + Adaptive Activation) -> Hard Constraint Transform -> Output $u(t,x)$

- **Critical path:**
  1. **Phase 1 (Base Training):** Train full network on $[0, T_{train}]$ with L-BFGS. Stop using validation error on $[T_{train}, T_{val}]$.
  2. **Selection:** Run inference on 4k points in validation region; sort by PDE loss; pick top 80.
  3. **Phase 2 (TL/Fine-tuning):** Freeze layers 1-5. Train Layer 6 + Adaptive AF params on the 80 points using Adam + L2 regularization.

- **Design tradeoffs:**
  - **L2 Regularization vs. EWC:** The authors compared L2 regularization against Elastic Weight Consolidation (EWC). L2 was found to be simpler and more effective for PINNs (Table S1), likely because EWC's Fisher information calculation adds complexity without proportional gain in this specific context.
  - **Sampling Strategy:** The paper samples *only* from the validation interval $(0.5, 0.8]$, not the training interval. Sampling from training data increased "catastrophic forgetting" without aiding extrapolation.

- **Failure signatures:**
  - **Oscillation/Instability:** If the learning rate in Phase 2 is too high, the model may overfit the 80 points and lose physical consistency elsewhere.
  - **Stagnation:** If the base model (Phase 1) is poor, Phase 2 cannot recover performance (garbage in, garbage out).
  - **Wrong AF Choice:** Forcing a periodic AF on a non-periodic solution (or vice versa) results in higher error than the baseline.

- **First 3 experiments:**
  1. **Baseline Establishment:** Train a standard PINN on the Allen-Cahn equation using `tanh` everywhere. Record L2 error on extrapolation domain $(0.8, 1.0]$.
  2. **Ablation on Sampling:** Implement Phase 2, but sample the 80 points randomly instead of by highest loss. Compare L2 error to verify the importance of residual-based sampling.
  3. **AF Sensitivity:** Implement the full pipeline using `lctanh` (preferred) vs. `lcsin` (non-preferred) in the final layer to verify the "preferred AF" hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the transfer learning (TL) method with adaptive activation functions scale effectively to high-dimensional PDEs or systems with irregular geometric domains?
- Basis in paper: [explicit] The authors state in the Conclusion that the method was "evaluated only on 1D PDEs over regular domains" and that "scalability to more complex systems remains uncertain," particularly regarding hard constraints on irregular domains.
- Why unresolved: Enforcing boundary conditions as hard constraints is feasible for simple 1D geometries but becomes mathematically difficult or infeasible for complex, irregular 2D/3D boundaries, potentially limiting the proposed architecture's applicability.
- Evidence would resolve it: Successful application of the method to 2D or 3D benchmark problems (e.g., Navier-Stokes on complex airfoils) where boundary conditions are enforced as soft constraints or via complex geometry mappings.

### Open Question 2
- Question: How does the method perform when applied to real-world datasets characterized by noise and partial observability?
- Basis in paper: [explicit] The Conclusion notes that while the method can be applied where real-world data is available, it "may require substantial adaptations," as the current study relies on synthetic data with exact analytical solutions.
- Why unresolved: The proposed TL strategy relies on selecting collocation points with the "highest PDE loss." With noisy real-world data, the PDE loss might be dominated by measurement noise rather than model extrapolation error, making the point selection strategy potentially unstable.
- Evidence would resolve it: Benchmarking the adaptive TL method on experimental datasets (e.g., fluid flow imaging) with varying signal-to-noise ratios to verify robustness.

### Open Question 3
- Question: Why do linear combinations of a single "preferred" activation function (AF) yield better extrapolation performance than diverse combinations of different AFs?
- Basis in paper: [inferred] Section 5 notes that "increasing the diversity of AFs in the combination (as in ABU-PINN) is suboptimal" compared to specialized linear combinations (e.g., lctanh for AC/Burgers), but does not provide a theoretical justification for this preference.
- Why unresolved: It is counter-intuitive that a less diverse function space (linear combinations of one AF type) outperforms a space combining diverse properties (e.g., periodicity from sin and saturation from tanh) for general extrapolation tasks.
- Evidence would resolve it: A theoretical analysis or visualisation of the loss landscape showing that specialized combinations traverse flatter minima or align better with the spectral bias required for the specific PDEs.

### Open Question 4
- Question: Why does Elastic Weight Consolidation (EWC) perform significantly worse than simple L2 regularization for preventing catastrophic forgetting in this PINN context?
- Basis in paper: [inferred] Supplementary Material A states, "Surprisingly, our findings reveal that EWC performs the worst," despite EWC being specifically designed to protect important weights during transfer learning.
- Why unresolved: The authors verify the failure empirically but do not explain the mechanism. Since EWC relies on the Fisher Information Matrix to estimate weight importance, it may be misinterpreting the importance of weights in the physics-informed loss landscape.
- Evidence would resolve it: An ablation study analyzing the correlation between Fisher importance scores and the actual sensitivity of the PDE residual loss to specific network parameters.

## Limitations
- The method has only been validated on 1D PDEs with regular domains, raising questions about scalability to higher dimensions and complex geometries
- The selection of "preferred" activation functions appears empirical rather than systematically derived, potentially limiting generalizability
- The claim of no significant computational cost increase is qualified by the 150-epoch retraining phase, which may become prohibitive for larger networks

## Confidence
- **High Confidence:** The improvement in extrapolation performance (40% L2 error reduction, 50% MAE reduction) is well-supported by the presented experiments across three distinct PDEs
- **Medium Confidence:** The mechanism explanations for why transfer learning and adaptive activation functions work are plausible but not exhaustively validated
- **Low Confidence:** The claim that the method generalizes to arbitrary nonlinear PDEs without extensive hyperparameter tuning is not substantiated

## Next Checks
1. **Generalization Test:** Apply the method to a fourth, structurally different PDE (e.g., the Sine-Gordon equation or a reaction-diffusion system) to verify that the "preferred activation function" concept holds and that the 40% error reduction is reproducible

2. **Computational Scaling Test:** Measure the wall-clock time and memory usage for the TL phase on networks with 10× more parameters (e.g., 10 layers × 64 neurons) to empirically verify the "no significant computational cost increase" claim

3. **Sampling Strategy Sensitivity:** Systematically vary the number of high-loss points (e.g., 20, 40, 80, 160) and the selection criteria (e.g., top 1%, 5%, 10% by loss) to determine the sensitivity of the method to this critical hyperparameter