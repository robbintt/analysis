---
ver: rpa2
title: Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety
  Testing
arxiv_id: '2601.18061'
source_url: https://arxiv.org/abs/2601.18061
tags:
- expert
- disagreement
- evaluation
- mental
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tests whether expert human feedback provides reliable
  ground truth for AI alignment by having three board-certified psychiatrists independently
  evaluate 360 AI-generated mental health responses across eight safety and quality
  dimensions. Despite similar training and calibration, inter-rater reliability was
  poor across all factors (ICC 0.087-0.295), with systematic disagreement on high-stakes
  items like suicide and self-harm.
---

# Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing

## Quick Facts
- arXiv ID: 2601.18061
- Source URL: https://arxiv.org/abs/2601.18061
- Reference count: 40
- Three board-certified psychiatrists showed poor inter-rater reliability (ICC 0.087-0.295) when evaluating AI-generated mental health responses, with systematic disagreement on high-stakes items.

## Executive Summary
This study reveals a fundamental flaw in AI alignment methods that rely on aggregated human feedback: expert disagreement in safety-critical domains is not random noise but reflects principled differences in clinical frameworks. When three board-certified psychiatrists evaluated 360 AI-generated mental health responses across eight safety and quality dimensions, they showed poor inter-rater reliability with Krippendorff's alpha negative for several factors. The highest disagreement occurred on suicide and self-harm responses, creating a paradox where feedback is least reliable where it matters most. The findings suggest that current RLHF pipelines may inherit contested expert judgments as if they were ground truth, producing models that represent arithmetic compromises rather than any coherent clinical philosophy.

## Method Summary
Three board-certified psychiatrists independently evaluated 360 AI-generated mental health responses using an 8-factor rubric covering safety, quality, and clinical appropriateness. The evaluation included responses to prompts across different severity levels and risk categories (suicide, self-harm, depression, anxiety, ADHD, substance use). The study measured inter-rater reliability using ICC and Krippendorff's alpha, and conducted qualitative interviews to understand the sources of disagreement. The researchers also calculated mean absolute deviation (MAD) to quantify disagreement across different risk categories.

## Key Results
- Poor inter-rater reliability across all eight evaluation factors (ICC 0.087-0.295)
- Krippendorff's alpha was negative for several factors, indicating structured disagreement worse than chance
- Highest disagreement on suicide and self-harm responses (MAD 0.598 and 0.566), lowest on ADHD responses (MAD 0.461)
- Qualitative interviews revealed experts applied three distinct frameworks: safety-first, engagement-centered, and culturally-informed
- Aggregating expert ratings produces "arithmetic compromises" that don't reflect any expert's actual clinical philosophy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating expert judgments via averaging creates "arithmetic compromises" that do not reflect any expert's actual clinical philosophy.
- Mechanism: The standard RLHF pipeline collects annotations from multiple raters and averages them. When raters have fundamentally different frameworks (e.g., safety-first vs. engagement-centered), averaging produces a composite score that lies between these frameworks—a value no individual expert would endorse. This compromises the integrity of the ground truth signal used to train reward models.
- Core assumption: The current dominant paradigm assumes that annotation disagreement is primarily random noise that can be reduced through aggregation.
- Evidence anchors:
  - [abstract] "aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies."
  - [section 4.3] "When ratings diverge this systematically, averaging produces arithmetic midpoints reflecting no individual expert's clinical judgment..."
  - [corpus] Weak. Neighbor papers like CounselBench focus on benchmark creation, not on the mechanism of aggregation failure.
- Break condition: High inter-rater reliability (e.g., ICC > 0.75), which would indicate that experts are applying a shared standard and averaging is a valid noise-reduction strategy.

### Mechanism 2
- Claim: Disagreement in safety-critical domains is not random noise but reflects principled, coherent, and incompatible clinical frameworks.
- Mechanism: Experts bring "holistic risk heuristics" to evaluation tasks, shaped by their training, experience, and values. These frameworks are internally consistent for each expert but diverge systematically from one another. One expert prioritized harm avoidance, another focused on therapeutic engagement, and a third emphasized cultural context. This divergence is a fundamental difference in what is valued.
- Core assumption: The disagreement stems from the inherent normative complexity of the task, not from a lack of rubric clarity.
- Evidence anchors:
  - [abstract] "Qualitative interviews revealed that experts applied different clinical frameworks—safety-first, engagement-centered, and culturally-informed—producing principled rather than random divergence."
  - [section 5.1] "These three orientations... represent legitimate clinical philosophies that coexist in psychiatric practice."
  - [corpus] Weak. Related papers touch on safety challenges but don't analyze the sources of expert disagreement as this paper does.
- Break condition: When disagreement can be shown to be significantly reduced by clarifying task instructions or the rubric, suggesting it was measurement error.

### Mechanism 3
- Claim: The highest-stakes items produce the greatest disagreement, creating a paradox where feedback is least reliable where it is most consequential.
- Mechanism: Safety-critical items like those involving suicide or self-harm involve complex trade-offs between safety, autonomy, and support. These trade-offs activate the experts' differing value systems more strongly, leading to greater divergence. In contrast, lower-stakes items (e.g., ADHD-related responses) with more prescriptive guidelines showed more consensus.
- Core assumption: The inverse correlation between stakes and agreement holds across other high-stakes domains beyond mental health.
- Evidence anchors:
  - [abstract] "Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category..."
  - [section 4.2] "Disagreement was highest on the most safety-critical items: suicide and self-harm (MAD 0.598 and 0.566)... ADHD... showed the lowest disagreement (MAD 0.461)."
  - [corpus] Weak. The corpus papers don't systematically address this reliability-stakes correlation.
- Break condition: If more detailed rubrics for high-stakes items could demonstrably improve agreement.

## Foundational Learning

- Concept: **Inter-Rater Reliability (IRR)**
  - Why needed here: This entire paper is a study of IRR. Understanding what it measures (consistency among independent raters), its metrics (ICC, Krippendorff's alpha), and its thresholds is essential to interpreting the results and their implications for AI alignment.
  - Quick check question: What does an ICC of 0.087 mean for a single rater's reliability, and why is it considered "poor"?

- Concept: **Learning from Human Feedback (LHF) Paradigm**
  - Why needed here: The paper directly challenges a core assumption of the LHF paradigm—that aggregated human judgments provide valid ground truth. Understanding this paradigm (RLHF, DPO, safety classifiers) is necessary to grasp the significance of the findings.
  - Quick check question: At what point in the RLHF pipeline does this study's findings suggest the process could encode problematic compromises?

- Concept: **Ground Truth as a Construct**
  - Why needed here: In domains like mental health, there is no objective test for a "good" response. Ground truth is socially constructed by experts. When this construction is contested, aggregating labels is a policy choice, not a neutral statistical operation.
  - Quick check question: Why is averaging expert ratings in a contested domain described as a "policy choice with distributive consequences"?

## Architecture Onboarding

- Component map: Prompt Generation -> Evaluation Framework -> Expert Panel & Annotation Pipeline -> Aggregation Engine (Problematic) -> Reward Model (Downstream)

- Critical path:
  1. **Define Task & Rubric**: Normative choices are embedded here.
  2. **Collect Annotations**: Gather ratings. The study's key data point is the low ICC from this step.
  3. **Analyze Disagreement (Critical Fork)**:
      - **Path A (Status Quo)**: Aggregate labels (mean/majority). This path is shown to be problematic.
      - **Path B (Proposed)**: Preserve disagreement. Use multi-annotator models or report distributions.

- Design tradeoffs:
  - **Simplicity vs. Fidelity**: Aggregating to a single score is simple but loses fidelity to individual reasoning.
  - **Alignment vs. Coherence**: Aligning to an aggregated mean may produce a model that is therapeutically incoherent, reflecting no single clinical philosophy.

- Failure signatures:
  1. **Systematic Directional Bias**: One rater consistently scores higher/lower than others (e.g., Rater A vs. C). Averaging cannot correct this.
  2. **Negative Reliability Metrics**: A Krippendorff's alpha below zero, indicating structured disagreement worse than chance.
  3. **The High-Stakes Paradox**: Lower reliability (higher MAD) on the most critical items (suicide, self-harm).

- First 3 experiments:
  1. **Replicate IRR Analysis**: Before a new annotation effort, run a pilot with 2-3 experts on a small sample. Compute ICC. Do not proceed if metrics are poor.
  2. **Disagreement Preservation Test**: Instead of averaging, train separate reward models on each expert's labels. Evaluate if models reflect their respective framework.
  3. **Framework Disaggregation**: Annotate data with a "framework label" (e.g., safety-first, engagement-centered). Analyze whether reliability improves within each subgroup.

## Open Questions the Paper Calls Out
- How can AI alignment methods preserve the benefits of diverse expert perspectives while still producing coherent, safe models?
- What alternative approaches to human feedback can better handle systematic expert disagreement in safety-critical domains?
- How can we design evaluation frameworks that acknowledge and accommodate legitimate differences in professional judgment rather than trying to eliminate them?

## Limitations
- The study focuses on a single domain (mental health) and specific AI models (GPT-4, Gemini, Claude), limiting generalizability to other safety-critical areas.
- The expert panel consisted of only three psychiatrists, which may not capture the full spectrum of professional disagreement in the field.
- The findings highlight a problem but don't provide a comprehensive solution for how to handle expert disagreement in AI alignment pipelines.

## Confidence

- **High Confidence**: The empirical finding of poor inter-rater reliability (ICC 0.087-0.295) and the qualitative evidence of principled framework differences are well-supported by the data and analysis.
- **Medium Confidence**: The claim that current RLHF pipelines systematically encode contested judgments as ground truth is mechanistically sound but requires testing in actual deployment scenarios.
- **Low Confidence**: The broader claim about generalizability across all safety-critical AI domains requires additional empirical validation beyond the mental health context.

## Next Checks

1. **Cross-Domain Replication**: Replicate the expert evaluation methodology in another safety-critical domain (e.g., medical triage or autonomous driving) to test whether systematic expert disagreement appears in similar patterns.

2. **Framework Diversity Assessment**: Expand the expert panel to include a broader range of clinical philosophies and measure whether the current three frameworks capture the full spectrum of professional disagreement.

3. **Downstream Impact Analysis**: Train RLHF models using both aggregated labels and disagreement-preserving approaches (e.g., multi-annotator models) and empirically measure differences in model behavior, particularly on high-stakes scenarios.