---
ver: rpa2
title: 'SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection'
arxiv_id: '2509.20562'
source_url: https://arxiv.org/abs/2509.20562
tags:
- error
- learning
- reflection
- plan
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SAMULE introduces a framework for self-learning agents using multi-level
  reflection synthesis. It addresses the challenge of meaningful error analysis in
  complex tasks by synthesizing reflections across three levels: Single-Trajectory,
  Intra-Task, and Inter-Task learning.'
---

# SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection

## Quick Facts
- arXiv ID: 2509.20562
- Source URL: https://arxiv.org/abs/2509.20562
- Reference count: 23
- One-line primary result: SAMULE achieves up to 29.44% pass rate on TravelPlanner and 60.31% accuracy on NATURAL PLAN (Trip) through multi-level reflection synthesis.

## Executive Summary
SAMULE introduces a framework for self-learning agents using multi-level reflection synthesis. It addresses the challenge of meaningful error analysis in complex tasks by synthesizing reflections across three levels: Single-Trajectory, Intra-Task, and Inter-Task learning. The framework trains a retrospective language model via supervised fine-tuning to generate trajectory-specific reflections during inference. It also extends to interactive settings through foresight-based reflection, enabling real-time adaptation by comparing predicted and actual user responses. Experiments on TravelPlanner, NATURAL PLAN, and Tau-bench show significant performance improvements over baselines, with SAMULE achieving up to 29.44% pass rate on TravelPlanner and 60.31% accuracy on NATURAL PLAN (Trip), demonstrating the effectiveness of failure-centric learning and multi-level reflection synthesis.

## Method Summary
SAMULE operates through a three-stage reflection synthesis pipeline followed by supervised fine-tuning of a retrospective model. First, it generates reflections at the micro-level by comparing failed trajectories against reference plans. Second, it aggregates multiple trajectories per task to construct an error taxonomy at the meso-level. Third, it clusters same-typed errors across tasks to derive generalized insights at the macro-level. These reflections are merged and summarized into training pairs for the retrospective model (Qwen 2.5 3B), which is fine-tuned via SFT. During inference, the trained model generates reflections for failed trajectories, which are appended to the context for task retries. In interactive settings, the system triggers real-time reflections by comparing predicted and actual user responses.

## Key Results
- SAMULE achieves 29.44% pass rate on TravelPlanner, outperforming Expel (0%) and Simple ReAct (15.56%).
- On NATURAL PLAN (Trip), SAMULE reaches 60.31% accuracy, exceeding all baselines including Expel (53.79%).
- The multi-level synthesis approach demonstrates consistent improvements across diverse benchmarks, validating the effectiveness of hierarchical error analysis.

## Why This Works (Mechanism)

### Mechanism 1: Multi-Level Reflection Synthesis Enables Transferable Error Patterns
Hierarchical reflection across micro, meso, and macro levels produces more actionable feedback than single-trajectory reflection. Micro-level reflection compares failed trajectories against reference plans to identify specific errors. Meso-level aggregates multiple trajectories per task to construct an error taxonomy. Macro-level clusters same-typed errors across tasks to derive generalized insights. These are merged and summarized into a final reflection for supervised fine-tuning.

Core assumption: Errors cluster meaningfully across tasks and tasks share recurring failure modes amenable to abstraction.
Evidence anchors:
- [abstract]: "synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights"
- [section]: Table 4 shows Single (Reference) + Intra (No Reference) yields 20.00% pass rate versus 15.56% when reference is over-exposed, indicating micro-level reference use with meso-level abstraction is optimal.
- [corpus]: OmniReflect (arXiv:2506.17449) supports hierarchical, transferable constitutions via neuro-symbolic reflections, providing convergent signal that multi-level abstraction aids generalization.
Break condition: If tasks are highly dissimilar or error types are sparse and unique, macro-level clustering may yield overly generic reflections that fail to transfer.

### Mechanism 2: Failure-Centric Learning Outperforms Success-Based Reflection in Low-Success Domains
Prioritizing failure trajectories for reflection is more effective than relying on rare successful trajectories when task success rates are low. Error classification and taxonomy construction are performed primarily on failed trajectories, with reference plans enabling precise diagnosis. The retrospective model learns to generate targeted corrections without needing successful exemplars.

Core assumption: Failures carry denser learnable signal in complex tasks and references can ground error attribution without inducing overfitting to specific solutions.
Evidence anchors:
- [abstract]: "failure-centric learning" and "inadequate error analysis and a reliance on rare successful trajectories" are identified as key problems SAMULE addresses.
- [section]: Expel achieves 0% on TravelPlanner due to reliance on successful trajectories, while SAMULE achieves 20%; Expel performs better on NATURAL PLAN (53.79%) where success is more common.
- [corpus]: Weak direct corpus evidence comparing failure-centric vs. success-based reflection; neighbor papers focus on reflection mechanisms broadly rather than failure-centric approaches specifically.
Break condition: If reference outputs are unavailable or low-quality, micro-level comparison may degrade, reducing diagnostic precision.

### Mechanism 3: SFT-Trained Retrospective Model Outperforms RL-Based Reflection with Lower-Quality Reflections
A small LM fine-tuned via SFT on high-quality multi-level reflections can outperform RL-based methods that are sensitive to reflection quality. Multi-level synthesis produces structured training pairs (trajectory → reflection). The retrospective model learns to generate trajectory-specific reflections at inference without references. SFT avoids the instability of RL when synthesized reflections are noisy.

Core assumption: High-quality reflection synthesis is the bottleneck; simple SFT suffices if data quality is strong.
Evidence anchors:
- [abstract]: "even with simple supervised fine-tuning, our retrospective model—trained on multi-level synthesized reflections—achieves superior performance compared to advanced methods relying on RL"
- [section]: Retroformer variant (DPO) achieves 12.78% on TravelPlanner; SAMULE achieves 20%. On HotPotQA, Retroformer variant matches original Retroformer (44% vs. 43%), validating DPO as a fair comparison.
- [corpus]: MemOrb (arXiv:2509.18713) and OmniReflect use reflection-aware mechanisms to improve agents, aligning with the finding that structured reflection training enhances performance.
Break condition: If synthesized reflections are noisy or misaligned with task objectives, SFT may entrench incorrect feedback patterns, similar to RL failure modes.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SAMULE trains a retrospective model via SFT on trajectory-reflection pairs; understanding SFT fundamentals is essential to grasp why this approach is stable compared to RL.
  - Quick check question: Can you explain why SFT might be more robust than RL when training data quality is variable?

- Concept: Error Taxonomy Construction
  - Why needed here: Intra-task learning builds an error taxonomy to categorize failure modes; understanding how errors are classified and propagated is critical to the meso-level mechanism.
  - Quick check question: How would you design an error taxonomy for a multi-step planning task, and what risks arise if categories are too coarse or too granular?

- Concept: Reference-Grounded Reflection
  - Why needed here: Micro-level reflection compares failed trajectories to reference plans; this comparison is central to SAMULE's error diagnosis.
  - Quick check question: What are the tradeoffs of providing a reference plan during reflection, and when might it hinder generalization?

## Architecture Onboarding

- Component map:
  1. Multi-Level Reflection Synthesis: Single-Trajectory (Micro) → Intra-Task (Meso) → Inter-Task (Macro)
  2. Retrospective Model (Qwen 2.5 3B): SFT on synthesized trajectory-reflection pairs
  3. Actor Agent (Claude 3.5/3.7 Sonnet): Executes tasks using ReAct planning, consumes reflections from retrospective model
  4. Foresight-Based Reflection (Interactive Mode): Compares predicted vs. actual user responses to trigger real-time reflection

- Critical path:
  1. Collect failed trajectories during training with reference outputs available
  2. Run multi-level synthesis to produce trajectory-reflection pairs
  3. Train retrospective model via SFT on synthesized pairs
  4. At inference, feed trajectory to retrospective model, generate reflection, retry task with reflection in context
  5. (Interactive) After each turn, compare predicted vs. actual user response; if divergent, trigger mid-trajectory reflection

- Design tradeoffs:
  - Reference availability: Providing references at micro-level improves diagnosis, but over-exposure at meso-level reduces diversity and hurts generalization (Table 4: 15.56% vs. 20.00%)
  - Static vs. dynamic taxonomy: Error taxonomy is built offline; it may not adapt to novel errors encountered at inference
  - SFT vs. RL: SFT is computationally cheaper and more stable, but relies entirely on reflection quality; RL can adapt but is sensitive to reward noise

- Failure signatures:
  - Reflections too generic: Indicates macro-level clustering is over-generalizing; consider tightening error clusters or weighting micro-level reflections higher
  - No improvement across trials: May signal reference misalignment or taxonomy gaps; audit error classification accuracy
  - Interactive mode triggers excessive reflection: Foresight threshold may be too sensitive; calibrate divergence detection criteria

- First 3 experiments:
  1. Replicate micro-level vs. meso-level reference ablation on a held-out task domain to confirm generalization of Table 4 findings
  2. Compare SFT-only retrospective model against DPO-based variant on a benchmark with longer trajectories (e.g., extended TravelPlanner) to test scalability
  3. Evaluate interactive mode on Tau-bench with adjusted foresight thresholds to characterize tradeoff between reflection frequency and task completion speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the error taxonomy be updated dynamically during inference to support lifelong learning?
- Basis in paper: [explicit] The authors state in Section 6 that the current taxonomy "remains static throughout inference" and suggest future work should explore "incremental taxonomy construction and online adaptation methods."
- Why unresolved: The current framework relies on an offline synthesis process; it cannot adapt to previously unseen failure patterns encountered in production without retraining.
- What evidence would resolve it: An evaluation showing the agent successfully integrating novel error types into its taxonomy in real-time without performance degradation on a stream of evolving tasks.

### Open Question 2
- Question: Can the computational overhead of multi-level synthesis be reduced without compromising reflection quality?
- Basis in paper: [explicit] Section 6 notes that the "offline processes of trajectory analysis, error taxonomy construction, and cross-task clustering are resource-intensive" and calls for "scalable reflection synthesis techniques."
- Why unresolved: The current method requires multiple LLM passes per trajectory (micro, meso, macro), making it expensive for large-scale datasets.
- What evidence would resolve it: A modified synthesis approach that achieves comparable pass rates (e.g., ~60% on NATURAL PLAN) with a measurably lower computational cost (FLOPs or latency).

### Open Question 3
- Question: What mechanisms can mitigate the "narrowing of focus" caused by excessive reference exposure during meso-level learning?
- Basis in paper: [inferred] Section 4.6 shows that providing references during Intra-Task learning degrades performance (15.56% vs 20%), which the authors attribute to the model aligning "too closely with one specific plan" and losing diversity.
- Why unresolved: It is unclear how to selectively mask or augment reference data to guide error analysis without biasing the agent toward specific solution paths.
- What evidence would resolve it: An ablation study testing attention masking or contrastive learning objectives that maintain error detection diversity even when references are provided.

## Limitations
- Macro-level error clustering effectiveness depends heavily on error similarity across tasks; dissimilar domains may yield overly generic reflections.
- The retrospective model's ability to generalize beyond its training domain is untested; catastrophic forgetting or overfitting to synthesis-specific patterns is possible.
- Interactive mode's foresight-based reflection mechanism may introduce latency and over-triggering, especially in open-ended domains with noisy user responses.

## Confidence
- **High confidence**: Multi-level reflection synthesis improves performance over single-trajectory methods when references are available (validated by ablation in Table 4).
- **Medium confidence**: Failure-centric learning is superior to success-based reflection in low-success domains, though direct comparison with success-based baselines is sparse.
- **Medium confidence**: SFT on synthesized reflections outperforms RL variants, contingent on reflection quality.

## Next Checks
1. Apply SAMULE to a task domain with intentionally dissimilar subtasks (e.g., travel planning + code debugging) and measure macro-level clustering coherence and reflection utility.
2. Fine-tune the retrospective model on one domain (e.g., TravelPlanner) and evaluate on an unseen domain (e.g., Tau-bench) to quantify transfer decay.
3. Systematically vary foresight divergence thresholds on Tau-bench and measure the tradeoff between reflection frequency and task completion latency.