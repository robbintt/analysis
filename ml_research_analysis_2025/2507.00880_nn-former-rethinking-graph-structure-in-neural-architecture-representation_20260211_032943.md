---
ver: rpa2
title: 'NN-Former: Rethinking Graph Structure in Neural Architecture Representation'
arxiv_id: '2507.00880'
source_url: https://arxiv.org/abs/2507.00880
tags:
- neural
- graph
- architecture
- nodes
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NN-Former addresses the limitations of GNNs and transformers in
  neural architecture representation by rethinking DAG topology and introducing sibling
  nodes as pivotal information. The method combines GNNs and transformers through
  a novel Adjacency-Sibling Multihead Attention (ASMA) module that captures local
  topology features, and a Bidirectional Graph Isomorphism Feed-Forward Network (BGIFFN)
  that enhances topology learning without requiring position encoding.
---

# NN-Former: Rethinking Graph Structure in Neural Architecture Representation

## Quick Facts
- arXiv ID: 2507.00880
- Source URL: https://arxiv.org/abs/2507.00880
- Reference count: 40
- Primary result: Achieves Kendall's Tau up to 0.877 on NAS-Bench-101 and 0.890 on NAS-Bench-201 for accuracy prediction

## Executive Summary
NN-Former addresses the limitations of Graph Neural Networks (GNNs) and transformers in neural architecture representation by rethinking DAG topology and introducing sibling nodes as pivotal information. The method combines GNNs and transformers through a novel Adjacency-Sibling Multihead Attention (ASMA) module that captures local topology features, and a Bidirectional Graph Isomorphism Feed-Forward Network (BGIFFN) that enhances topology learning without requiring position encoding. NN-Former achieves superior performance in both accuracy prediction (Kendall's Tau up to 0.877 on NAS-Bench-101 and 0.890 on NAS-Bench-201) and latency prediction (MAPE reduction of 11.61% on NNLQ compared to NAR-Former V2).

## Method Summary
NN-Former is a hybrid GNN-transformer architecture designed for DAG-based neural architecture representation. It uses an Adjacency-Sibling Multihead Attention (ASMA) module that restricts attention to local topology through element-wise masking, allowing nodes to attend to themselves, direct neighbors, and sibling nodes. The Bidirectional Graph Isomorphism Feed-Forward Network (BGIFFN) replaces standard FFN by performing graph convolutions in both forward and backward directions, eliminating the need for positional encoding. The architecture is trained using AdamW optimizer with EMA and augmentation losses for accuracy tasks, while latency tasks use simpler configurations.

## Key Results
- Achieves Kendall's Tau of 0.877 on NAS-Bench-101 and 0.890 on NAS-Bench-201 for accuracy prediction
- Reduces MAPE by 11.61% on NNLQ latency prediction compared to NAR-Former V2
- Demonstrates strong generalization across architectures ranging from tens to hundreds of operations

## Why This Works (Mechanism)

### Mechanism 1: Local Attention Improves Generalization
Restricting transformer attention to local topology improves generalization for neural architecture prediction compared to global attention. The ASMA module uses element-wise masking to constrain attention to self, direct neighbors, and siblings, addressing the poor generalization of global attention on deep architectures.

### Mechanism 2: Sibling Nodes as Pivotal Features
Explicitly modeling "sibling nodes" (nodes with common parent or child) is a pivotal, overlooked feature for DAG-based neural architecture representation. The method mathematically derives sibling relationships from adjacency matrix products and incorporates them as attention masks, enabling learning from parallel branches in networks.

### Mechanism 3: BGIFFN Eliminates Positional Encoding
A Bidirectional Graph Isomorphism Feed-Forward Network can sufficiently capture topological position, making separate positional encodings unnecessary. The BGIFFN performs bidirectional graph convolutions that iteratively embed structural position into node features, as powerful as a directed WL test.

## Foundational Learning

### Concept: Message-Passing Graph Neural Networks (GNNs)
Why needed: NN-Former builds on GNN principles for aggregating neighbor information. Understanding GCN layer updates is essential to grasp how the model processes graph topology.
Quick check: Can you explain how a standard GCN layer updates a node's representation by aggregating information from its neighbors?

### Concept: The Transformer Attention Mechanism
Why needed: The core of NN-Former is a modified transformer. Understanding standard self-attention (Query, Key, Value) is prerequisite to understanding how ASMA modifies it with topological masks.
Quick check: In a standard transformer, how are the attention weights between a query and all keys computed, and how are they used to generate the output?

### Concept: Directed Acyclic Graphs (DAGs) for Neural Architectures
Why needed: The method is specialized for this specific graph structure. Concepts like "siblings," "parents," and "children" are used to define topology, and these only make sense in directed graph context.
Quick check: In a DAG representing a neural network, what do the nodes and directed edges typically represent, and why is the graph acyclic?

## Architecture Onboarding

### Component map:
Input Encoder -> Adjacency-Sibling Multihead Attention (ASMA) -> Bidirectional Graph Isomorphism Feed-Forward Network (BGIFFN) -> Prediction Head

### Critical path:
1. **Attention Masking:** Correctly computing four masks (A, A^T, AA^T, A^T A) and applying them to attention scores is the most critical step where topological prior is injected
2. **Bidirectional Propagation:** Implementing dual-path convolution in BGIFFN is essential for understanding structural context

### Design tradeoffs:
- **Expressiveness vs. Generalization:** Local attention improves generalization on deep networks but may sacrifice performance where global information is critical
- **Complexity vs. Inductive Bias:** Added complexity (ASMA, BGIFFN) bakes in strong structural inductive biases to achieve better performance with less data

### Failure signatures:
- **Attention Masking Errors:** Incorrect mask shapes or values leading to -inf in softmax or allowing attention to forbidden nodes
- **Directionality Bugs:** Mixing up A and A^T in BGIFFN or ASMA, reversing forward/backward information flow
- **Overfitting on Small Data:** Despite generalization design, can still overfit on extremely small training splits

### First 3 experiments:
1. **Ablate Sibling Attention:** Compare model with only adjacency masks (A, A^T) against full model with sibling masks (AA^T, A^T A) on NAS-Bench-101
2. **BGIFFN vs. Standard FFN:** Replace BGIFFN with standard FFN and measure performance drop to isolate topology-infused feed-forward contribution
3. **Depth Generalization Test:** Evaluate performance degradation as network depth increases compared to vanilla transformer baseline

## Open Questions the Paper Calls Out

### Open Question 1
Can the effectiveness of sibling-node modeling be theoretically characterized for general DAG tasks outside of neural architecture representation? The paper notes mixed empirical results where it aids citation prediction but not AST tasks, lacking theoretical explanation for why sibling importance varies.

### Open Question 2
Does the exclusion of global attention in NN-Former strictly limit performance on architectures defined by critical non-sibling long-range dependencies? While local approach improves generalization, the paper doesn't verify if this discards useful signals from long-range skip connections.

### Open Question 3
Would extending the ASMA module to include higher-order structural relations (e.g., 2-hop neighbors) yield significant performance improvements? The ablation studies test 1-hop masks but don't explore expanding to "cousins" or deeper structural analogues.

## Limitations
- Sibling node mechanism shows domain-specific effectiveness but lacks performance on AST tasks, suggesting limited utility for certain DAG structures
- Augmentation loss from NAR-Former is referenced but not fully specified, creating implementation uncertainty
- Conversion from OOE to OON format for NAS-Bench-201 requires clarification

## Confidence

- **High confidence:** Local attention improves generalization over global attention (supported by ablation studies showing 11.45% Acc(10%) improvement)
- **Medium confidence:** Sibling relationships are pivotal for architecture representation (supported within NAS-Bench domains but contradicted by AST performance)
- **Medium confidence:** BGIFFN eliminates need for positional encoding (empirical validation shows PEs don't improve performance)

## Next Checks

1. **Domain transferability test:** Evaluate NN-Former on hardware accelerator DAGs where parallel execution patterns differ from standard neural networks
2. **Mechanism isolation:** Create synthetic DAGs with varying sibling structures to measure specific contribution of sibling attention mechanism
3. **Long-range dependency test:** Design prediction task requiring global graph properties to determine if local attention restriction becomes bottleneck