---
ver: rpa2
title: Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD
arxiv_id: '2511.01830'
source_url: https://arxiv.org/abs/2511.01830
tags:
- high-fidelity
- dataset
- scaling
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores how dataset fidelity impacts the performance
  of neural surrogates in computational fluid dynamics under budget constraints. The
  authors reformulate classical scaling laws by decomposing the dataset axis into
  compute budget and dataset composition, enabling analysis of optimal fidelity mixes.
---

# Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD

## Quick Facts
- arXiv ID: 2511.01830
- Source URL: https://arxiv.org/abs/2511.01830
- Reference count: 40
- Primary result: Dataset fidelity mix impacts neural surrogate performance under compute budget constraints

## Executive Summary
This work investigates how dataset fidelity affects neural surrogate performance in computational fluid dynamics when constrained by compute budget. The authors reformulate classical scaling laws by decomposing dataset size into compute budget and composition, enabling analysis of optimal fidelity mixes. Experiments on RANS simulations for airfoil aerodynamics reveal compute-performance scaling behavior and budget-dependent optimal fidelity mixes. Key findings include: model error decreases with increasing dataset generation budget; for limited budgets, mixing low- and high-fidelity data yields better performance than using only high-fidelity samples due to broader data coverage; optimal dataset composition shifts toward higher fidelity as budget increases. This provides the first empirical investigation of multi-fidelity scaling laws in scientific machine learning, offering practical guidance for compute-efficient dataset generation.

## Method Summary
The study uses OpenFOAM RANS solver with k-ω SST turbulence model to generate matched low/high-fidelity simulation pairs (611 total, 491 train/val, 120 test) of NASA 4- and 5-digit airfoils. Low-fidelity simulations use y+ = 30–300 with wall functions, while high-fidelity resolve the viscous sublayer (y+ < 1). The Transolver model (~4M params, 8 transformer layers) predicts RANS solutions given initial conditions and mesh positions. Training uses AdamW optimizer with cosine LR decay, early stopping, and normalization of output fields. The key innovation is budget-constrained training splits where Db (core hours) determines sample counts at varying Dc (high-fidelity ratio) compositions.

## Key Results
- Model error decreases monotonically with increasing dataset generation budget
- Under tight compute constraints, mixing low- and high-fidelity data outperforms pure high-fidelity training for pressure and velocity fields
- Optimal high-fidelity ratio increases from ~0.5-0.7 at low budgets to 1.0 at high budgets
- Wall shear stress shows no positive transfer from low-fidelity data due to large fidelity gap (nMAE ~0.4-0.8)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Under constrained compute budgets, mixing low- and high-fidelity training data improves model performance for certain physical fields compared to using only high-fidelity data.
- Mechanism: Low-fidelity samples provide broader coverage of the input data manifold at lower cost per sample. When the budget severely limits the number of high-fidelity samples obtainable, the additional manifold coverage from low-fidelity data compensates for their reduced accuracy, allowing the model to learn better generalizable representations.
- Core assumption: The neural surrogate can extract transferable representations from low-fidelity simulations that remain useful for high-fidelity prediction tasks.
- Evidence anchors:
  - [abstract]: "for limited budgets, mixing low- and high-fidelity data yields better performance than using only high-fidelity samples due to broader data coverage"
  - [section 4]: "under tight compute constraints, the broader coverage of the data manifold offered by many low-fidelity samples outweighs the higher accuracy of a few high-fidelity ones"
  - [corpus]: "Progressive multi-fidelity learning for physical system predictions" (arxiv 2510.13762) supports progressive learning from lower to higher fidelity data
- Break condition: Transfer fails when the fidelity gap between simulation types exceeds a field-specific threshold (observed for wall shear stress in Figure 3).

### Mechanism 2
- Claim: The optimal ratio of high-fidelity to low-fidelity samples increases monotonically with available compute budget.
- Mechanism: At low budgets, data quantity and manifold coverage dominate performance; as budget increases, the marginal value of additional low-fidelity samples diminishes while the value of high-fidelity accuracy increases, shifting the optimum toward higher fidelity ratios.
- Core assumption: The relative cost ratio between fidelity levels remains approximately constant; the model capacity is sufficient to benefit from additional high-fidelity data.
- Evidence anchors:
  - [abstract]: "optimal dataset composition shifts toward higher fidelity as budget increases"
  - [section 4/Figure 2]: Lower budget curves (Db=1000-2000) show optimal performance at Dc≈0.5-0.7, while higher budgets show monotonic improvement toward Dc=1.0
  - [corpus]: "Data Mixture Optimization: A Multi-fidelity Multi-scale Bayesian Framework" (arxiv 2503.21023) addresses data mixture optimization but does not explicitly characterize budget-dependent shifts
- Break condition: Budget thresholds may vary significantly across physical domains and field types.

### Mechanism 3
- Claim: Different output fields exhibit distinct multi-fidelity transfer behaviors governed by their sensitivity to simulation fidelity differences.
- Mechanism: Fields that depend critically on boundary layer resolution (wall shear stress) cannot benefit from low-fidelity data because the low-fidelity simulation fundamentally models rather than resolves the relevant physics. Fields like pressure and velocity, which remain consistent across fidelity levels (low nMAE), enable positive transfer.
- Core assumption: The normalized Mean Absolute Error between low- and high-fidelity fields predicts transfer potential.
- Evidence anchors:
  - [section 4/Table 2]: Pressure nMAE = 0.040-0.043 shows positive transfer; wall shear stress nMAE = 0.405-0.796 shows no transfer
  - [section 4]: "we cannot observe any positive transfer from low- to high-fidelity samples for the wall shear stress"
  - [corpus]: Limited direct corpus evidence on field-specific fidelity sensitivity thresholds
- Break condition: Assumption: This relationship may not hold linearly for intermediate nMAE values or across different physics domains.

## Foundational Learning

- Concept: Reynolds-Averaged Navier-Stokes (RANS) equations and boundary layer treatment
  - Why needed: The fidelity difference in this work stems from how the viscous sublayer (y+ < 5) is handled—resolved with fine meshing (high-fidelity) vs. modeled with wall functions (low-fidelity).
  - Quick check question: Explain why y+ < 5 defines the viscous sublayer and how resolving versus modeling this region affects predictions of drag-related quantities.

- Concept: Classical neural scaling laws (Kaplan et al., Hoffmann et al.)
  - Why needed: This work reformulates the standard three-axis formulation (N, D, C) by decomposing dataset size D into generation budget Db and composition Dc.
  - Quick check question: What are the three axes in classical scaling laws, and what assumption about dataset cost does this work challenge?

- Concept: Wall shear stress vs. pressure field physics
  - Why needed: Understanding why different fields exhibit different transfer behaviors is essential for predicting which outputs will benefit from multi-fidelity training.
  - Quick check question: Why would wall shear stress depend more critically on boundary layer resolution than pressure fields in external aerodynamics?

## Architecture Onboarding

- Component map: OpenFOAM RANS solver -> Dataset composition layer (Db, Dc) -> Transolver model (~4M params) -> AdamW training with early stopping -> MSE evaluation on HF test set

- Critical path:
  1. Define Db and target Dc (high-fidelity ratio)
  2. Estimate sample counts from average simulation costs (3.4 hrs HF, 4.8 hrs LF)
  3. Sample simulations from available pool to meet budget constraint
  4. Train Transolver for up to 500 epochs with early stopping
  5. Evaluate on 120 held-out high-fidelity samples

- Design tradeoffs:
  - Higher Dc reduces sample count but increases per-sample accuracy
  - Budget allocation must consider field-specific transfer characteristics—optimal Dc for pressure may differ from wall shear stress
  - Model capacity fixed at ~4M params; larger models may shift optimal compositions

- Failure signatures:
  - No transfer detected: Wall shear stress MSE improves monotonically with Dc (Figure 3)—indicates fidelity gap too large
  - High variance at low budgets: Error bars in Figure 2 increase substantially at Db=1000
  - Suboptimal pure-HF performance: At low Db, Dc=1.0 underperforms mixed compositions for pressure/velocity fields

- First 3 experiments:
  1. Establish baseline: Train on full high-fidelity dataset (Db≈6600, Dc=1.0) to determine performance ceiling for each field
  2. Budget scaling sweep: Fix Dc=0.6, vary Db from 1000 to 5500 core hours in 5-6 steps to characterize compute-performance relationship
  3. Composition optimization at fixed budget: Set Db=2500 core hours, sweep Dc from 0.3 to 0.9 to identify field-specific optimal ratios and verify budget-dependent shift hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the optimal multi-fidelity data composition shift when simultaneously scaling model parameters and training compute?
- Basis in paper: [explicit] The authors state that "future work should also explore scaling the remaining axes, namely model size and training compute in order to eventually establish a more complete formulation of multi-fidelity scientific scaling laws."
- Why unresolved: The current study fixes the model architecture (Transolver with ~4M parameters) and training compute to isolate the effect of the data generation budget ($D_b$).
- What evidence would resolve it: Empirical scaling laws derived from experiments where model size $N$ and training compute $C$ are varied alongside the dataset budget $D_b$ and composition $D_c$.

### Open Question 2
- Question: Can a continuous formulation of simulation fidelity prevent the failure of positive transfer observed in discrete fidelity jumps?
- Basis in paper: [explicit] The conclusion suggests that "continuous fidelity formulations... [are] potentially more effective, as it can mitigate situations where fidelity levels are too far apart for meaningful knowledge transfer."
- Why unresolved: The current work relies on discrete fidelity levels (low vs. high RANS), where the wall shear stress failed to transfer due to the large physical gap between modeling assumptions.
- What evidence would resolve it: A study where fidelity is treated as a continuous variable (e.g., varying mesh resolution or solver tolerance gradients) showing successful transfer for wall shear stress or similar sensitive quantities.

### Open Question 3
- Question: Does the compute-optimal mix of low- and high-fidelity data generalize to other physical domains such as thermomechanics or electromagnetics?
- Basis in paper: [explicit] The authors propose that "Extending our multi-fidelity scaling analysis to other scientific domains... could reveal whether the identified scaling behaviors generalize across different physical systems."
- Why unresolved: The observed scaling laws and budget-dependent optimal mixes are derived solely from external aerodynamics (RANS simulations on airfoils).
- What evidence would resolve it: Similar curves relating dataset composition ($D_c$) and budget ($D_b$) to model error in distinct physics datasets, confirming the universality of the "limited budget $\rightarrow$ more low-fidelity" heuristic.

## Limitations
- Field-specific transfer boundaries remain poorly characterized, with unclear thresholds for intermediate fidelity ranges
- The assumption that normalized MAE between fidelity levels predicts transfer potential requires further validation across diverse scientific ML applications
- Optimal composition may depend on model architecture capacity, which was fixed at ~4M parameters in this study

## Confidence
- **High Confidence**: The compute-performance scaling relationship and monotonic shift of optimal Dc with budget are well-supported by Figure 2 data
- **Medium Confidence**: The mechanism explaining low-fidelity manifold coverage benefits relies on untested assumptions about representation transferability
- **Medium Confidence**: Field-specific transfer behaviors based on boundary layer sensitivity are supported by data but may not generalize beyond tested RANS configurations

## Next Checks
1. Systematically vary the fidelity gap to map the boundary where positive transfer breaks down for different physical fields
2. Repeat scaling law experiments with models of varying parameter counts to determine whether optimal Dc shifts with model capacity
3. Apply the multi-fidelity scaling framework to a different scientific ML domain to confirm generalization beyond aerodynamics