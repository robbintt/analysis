---
ver: rpa2
title: 'Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models
  and Large Language Models'
arxiv_id: '2510.22356'
source_url: https://arxiv.org/abs/2510.22356
tags:
- irony
- detection
- learning
- language
- urdu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses irony detection in Urdu, a low-resource language,
  by translating an English ironic corpus and evaluating its performance using both
  traditional machine learning models and large language models (LLMs). The authors
  compare ten machine learning algorithms with GloVe and Word2Vec embeddings against
  fine-tuned transformer models including BERT, RoBERTa, LLaMA 2, LLaMA 3, and Mistral.
---

# Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models

## Quick Facts
- arXiv ID: 2510.22356
- Source URL: https://arxiv.org/abs/2510.22356
- Reference count: 0
- LLaMA 3 (8B) achieves 94.61% F1-score for irony detection in Urdu

## Executive Summary
This study addresses irony detection in Urdu, a low-resource language, by translating an English ironic corpus and evaluating its performance using both traditional machine learning models and large language models (LLMs). The authors compare ten machine learning algorithms with GloVe and Word2Vec embeddings against fine-tuned transformer models including BERT, RoBERTa, LLaMA 2, LLaMA 3, and Mistral. Among traditional methods, Gradient Boosting achieved the highest F1-score of 89.18%, while LLaMA 3 (8B) outperformed all models with an F1-score of 94.61%. These results demonstrate that integrating transliteration and modern NLP models enables robust irony detection in Urdu, validating the feasibility of cross-lingual adaptation for resource-constrained languages.

## Method Summary
The study translates an English Ironic Corpus into Urdu through a semi-automatic process (machine translation + human post-editing) to create a 1,950-sample dataset with binary labels. Ten traditional ML models (including Gradient Boosting, SVM, Random Forest) are trained using GloVe and Word2Vec embeddings averaged per sentence. Five transformer models (BERT, RoBERTa, LLaMA 2, LLaMA 3, Mistral) are fine-tuned with Urdu-specific preprocessing including normalization, diacritics removal, and stopword removal. Training uses batch size 32, learning rate 2e-5, 5 epochs, and FP16 mixed-precision on NVIDIA A100 GPU, with experiments repeated 3Ã— using different seeds.

## Key Results
- LLaMA 3 (8B) achieved the highest F1-score of 94.61% among all models tested
- Gradient Boosting with GloVe embeddings achieved the best traditional ML performance at 89.18% F1
- GloVe embeddings showed marginal improvement (0.48 F1 points) over Word2Vec in Gradient Boosting
- LLaMA 3 significantly outperformed LLaMA 2 (82.75% F1) and BERT (78.75% F1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating irony corpora from high-resource to low-resource languages preserves sufficient signal for model training.
- Mechanism: Semi-automatic translation with human post-editing maintains semantic equivalence and irony markers, enabling cross-lingual resource adaptation.
- Core assumption: Irony patterns transfer across languages despite syntactic and cultural differences.
- Evidence anchors:
  - [abstract] "translating an English Ironic Corpus into the Urdu language"
  - [section 3.1] "multi-stage process of quality assurance to guarantee the semantic equivalent between the source and target languages"
  - [corpus] Limited direct evidence for cross-lingual irony transfer effectiveness; related work on Urdu LLMs (Qalb) addresses language-specific challenges but not irony specifically.
- Break condition: If cultural humor patterns differ fundamentally between source and target languages, translated irony may not generalize.

### Mechanism 2
- Claim: Dense semantic embeddings improve traditional ML performance for irony detection in morphologically rich languages.
- Mechanism: GloVe and Word2Vec map tokens to dense vector spaces; averaging produces fixed-length sentence representations capturing global semantic information.
- Core assumption: Static embeddings can capture sufficient irony-related semantics without contextual awareness.
- Evidence anchors:
  - [abstract] "Ten ML models using GloVe and Word2Vec embeddings"
  - [section 3.3] "For each sentence or document, the word embeddings were averaged resulting in a fixed-length dense vector"
  - [section 4.2] "GloVe was encoding deeper semantic relationships more appropriate for ironic detection"
- Break condition: When irony depends on word order, context, or contrastive sentiment within sentences, averaged embeddings lose discriminative signal.

### Mechanism 3
- Claim: Large-scale transformer models capture contextual irony patterns that static embeddings miss.
- Mechanism: Pre-trained LLMs use subword tokenization (BPE, WordPiece) and self-attention to model intra-sentence relationships; fine-tuning adapts representations to irony classification.
- Core assumption: Pre-training on massive heterogeneous corpora transfers to low-resource language irony detection despite limited Urdu pre-training data.
- Evidence anchors:
  - [abstract] "LLaMA 3 (8B) reaches 94.61% F1-score among LLMs"
  - [section 3.3] "models used the internal tokenizers and learned embeddings during the encoding to represent textual inputs as high-dimensional vectors"
  - [section 4.3] "LLaMA 3 (8B) exceeded all other models... showcasing the strength of immense transformer models to capture subtle linguistic phenomena"
- Break condition: If irony relies on cultural knowledge absent from pre-training data, fine-tuning on small translated corpora may overfit rather than generalize.

## Foundational Learning

- Concept: **Word embeddings (static vs. contextual)**
  - Why needed here: The paper compares GloVe/Word2Vec (static, context-independent) against transformer embeddings (contextual). Understanding this distinction explains the 5+ point F1 gap between Gradient Boosting and LLaMA 3.
  - Quick check question: Why would "great" in "great job!" vs. "what a great disaster" receive the same vector under Word2Vec but different representations under BERT?

- Concept: **Fine-tuning vs. feature extraction**
  - Why needed here: ML models use pre-computed embeddings as fixed features; LLMs update internal weights during fine-tuning. This affects what patterns each approach can learn.
  - Quick check question: If you freeze all LLM weights and only train a classifier head, which approach are you using?

- Concept: **Low-resource language challenges**
  - Why needed here: Urdu lacks dedicated irony corpora; the paper's translation strategy is a workaround. Understanding low-resource constraints clarifies why cross-lingual adaptation matters.
  - Quick check question: Why might a model trained on English sarcasm struggle with Urdu irony even after translation?

## Architecture Onboarding

- Component map:
  - Translated Urdu corpus (1,950 samples, binary labels) -> Preprocessing (normalization, diacritics removal, Urdu-specific tokenization, stopword removal) -> Feature layer (ML: GloVe/Word2Vec averaged vectors; LLM: native tokenizers with contextual embeddings) -> Model layer (ML: 10 classifiers; LLM: 5 transformers) -> Evaluation (Precision, recall, accuracy, F1-score with 3 random seeds)

- Critical path:
  1. Translation quality assurance -> directly impacts label validity
  2. Urdu-specific preprocessing -> affects token quality for both paths
  3. Hyperparameter tuning (grid search for ML; lr=2e-5, epochs=5, batch=32 for LLMs)
  4. Early stopping on validation set

- Design tradeoffs:
  - **ML + static embeddings**: Faster, lower compute, but caps at ~89% F1; loses word-order information
  - **LLM fine-tuning**: Higher performance (94.6% F1), but requires A100 GPU and 5 epochs; risk of overfitting on small dataset
  - **GloVe vs. Word2Vec**: GloVe shows marginal improvement (0.48 F1 points on Gradient Boosting); both insufficient for contextual irony

- Failure signatures:
  - Naive Bayes and Decision Tree underperform (F1 < 82%) -> indicates simple models cannot capture irony complexity
  - BERT underperforms other LLMs (78.75% F1) -> suggests encoder-only models may need more data or different fine-tuning strategy
  - Large gap between LLaMA 2 (82.75%) and LLaMA 3 (94.61%) -> newer model iterations may include better multilingual pre-training

- First 3 experiments:
  1. **Baseline replication**: Train Gradient Boosting with GloVe embeddings on the translated corpus; target ~89% F1. If significantly lower, check preprocessing pipeline or embedding quality.
  2. **LLM fine-tuning sanity check**: Fine-tune LLaMA 3 (8B) with specified hyperparameters; target >90% F1. If underperforming, verify tokenization matches pre-trained model expectations.
  3. **Ablation on translation quality**: Compare performance on machine-translated-only vs. human-post-edited subset to quantify translation artifact effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model generalization improve when training on diverse, native Urdu irony sources compared to the translated English corpus used in this study?
- Basis in paper: [explicit] The conclusion states that future work should involve "enlarging the corpus by using more diverse sources of Urdu irony such as social media, blogs, and forums" to improve generalization.
- Why unresolved: The current study relies exclusively on a dataset translated from English Reddit comments, which may not fully encapsulate the cultural and linguistic idiosyncrasies of native Urdu irony.
- What evidence would resolve it: A comparative study evaluating models trained on the current translated dataset against models trained on a new, multi-domain native Urdu dataset.

### Open Question 2
- Question: To what extent does the inclusion of multimodal data (emojis and images) enhance the performance of irony detection in Urdu beyond text-only analysis?
- Basis in paper: [explicit] The authors suggest that "combining multimodal data type (e.g. emojis and images) with text data may leverage such modality complementation" to better learn contextual signals.
- Why unresolved: The current experimental setup processed text in isolation, stripping away visual and emoji cues that frequently serve as primary indicators of ironic intent in social media.
- What evidence would resolve it: Experimental results from a multimodal architecture applied to the dataset that includes the associated images and emojis.

### Open Question 3
- Question: Can Explainable AI (XAI) methods effectively illuminate the decision-making processes of high-performing LLMs like LLaMA 3 in the context of Urdu irony?
- Basis in paper: [explicit] The conclusion identifies the need for future work in "combining XAI methods" to "offer more transparency into model predictions" and increase trust.
- Why unresolved: While LLaMA 3 achieved 94.61% F1-score, the study provides no analysis of *why* the model made specific predictions, leaving the linguistic features it relied on unverified.
- What evidence would resolve it: A qualitative and quantitative analysis using XAI tools (e.g., attention visualization or SHAP) mapping model predictions to specific linguistic features in the Urdu text.

## Limitations
- Translation quality and cross-lingual transfer assumptions are untested for irony preservation
- Corpus size (1,950 samples) may lead to overfitting during LLM fine-tuning
- Critical implementation details (exact splits, embedding files, model variants) are unspecified
- High computational requirements (A100 GPU) create accessibility barriers for replication

## Confidence

- **High Confidence**: The comparative methodology is sound, and the performance gap between traditional ML (89.18% F1) and LLM fine-tuning (94.61% F1) is statistically significant and reproducible given access to the same corpus and computational resources.
- **Medium Confidence**: The translation-based approach for creating low-resource irony datasets is feasible and effective for Urdu, but the extent of cross-lingual irony preservation requires further validation through back-translation studies and human evaluation of translated irony quality.
- **Low Confidence**: Claims about LLaMA 3's superiority over other LLMs (BERT, RoBERTa, LLaMA 2, Mistral) may reflect both model architecture improvements and differences in pre-training data composition rather than solely fine-tuning effectiveness on the Urdu corpus.

## Next Checks

1. **Cross-Lingual Transfer Validation**: Conduct a controlled experiment comparing irony detection performance using: (a) translated Urdu corpus, (b) original English corpus, and (c) parallel irony detection in both languages independently. Measure performance drops to quantify translation artifact effects and validate cross-lingual transfer assumptions.

2. **Translation Quality Assessment**: Perform human evaluation of 100 randomly sampled translated sentences to assess irony preservation. Calculate inter-annotator agreement on whether irony is maintained post-translation. Additionally, conduct back-translation to English and compare against original irony patterns.

3. **Out-of-Domain Generalization Test**: Evaluate the best-performing models (Gradient Boosting and LLaMA 3) on an independent Urdu irony detection dataset (if available) or on Urdu text with known ironic content from different domains (social media, literature, news comments) to assess generalization beyond the Reddit-based translated corpus.