---
ver: rpa2
title: 'NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement:
  Methods and Results'
arxiv_id: '2504.13131'
source_url: https://arxiv.org/abs/2504.13131
tags:
- they
- video
- quality
- image
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the NTIRE 2025 Challenge on Short-form UGC
  Video Quality Assessment and Enhancement, which includes two tracks: (i) Efficient
  Video Quality Assessment (KVQ) and (ii) Diffusion-based Image Super-Resolution (KwaiSR).
  Track 1 focuses on developing lightweight VQA models under a 120 GFLOPs constraint,
  with 266 participants and 18 valid submissions.'
---

# NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results

## Quick Facts
- arXiv ID: 2504.13131
- Source URL: https://arxiv.org/abs/2504.13131
- Reference count: 40
- Primary result: SharpMind achieved 0.922 final score in Track 1 with 47.39 GFLOPs and 33.01M parameters; TACO SR won Track 2 with 0.2775/0.3529 user study scores

## Executive Summary
This paper presents the NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement, featuring two tracks: (i) Efficient Video Quality Assessment (KVQ) and (ii) Diffusion-based Image Super-Resolution (KwaiSR). Track 1 focused on developing lightweight VQA models under a 120 GFLOPs constraint, with 266 participants and 18 valid submissions. Track 2 introduced the KwaiSR dataset for image super-resolution in short-form UGC scenarios. Key outcomes include SharpMind achieving a 0.922 final score in Track 1 with 47.39 GFLOPs and 33.01M parameters, and TACO SR winning Track 2 with a 0.2775/0.3529 user study score on synthetic/wild datasets. The challenge significantly advances short-form UGC VQA and image super-resolution research.

## Method Summary
The challenge introduced two tracks with distinct methodologies. Track 1 (KVQ) required efficient no-reference VQA models under 120 GFLOPs constraint. The winning SharpMind approach used knowledge distillation: a teacher network (Swin-B + SlowFast + FAST-VQA + LIQE + DeQA) trained on KVQ and ~30,000 closed-source UGC videos generated pseudo-labels, which a lightweight Swin-T student learned from using differentiable SRCC/PLCC losses. Track 2 (KwaiSR) addressed diffusion-based super-resolution for short-form UGC, with TACO SR employing a two-stage diffusion approach: generating high-fidelity (Ipsnr) and high-perceptual (Iper) candidates, then fusing them via a learned NAFusion module combining structural fidelity from Ipsnr with semantic details from Iper.

## Key Results
- SharpMind achieved 0.922 final score in Track 1 with 47.39 GFLOPs and 33.01M parameters
- TACO SR won Track 2 with user study scores of 0.2775 (synthetic) and 0.3529 (wild) datasets
- Top Track 1 teams (SharpMind, TenVQA) exceeded 0.90 final score with GFLOPs ranging from 47-118
- Objective metrics (PSNR, LPIPS) showed weak correlation with subjective user study rankings in Track 2

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation for Efficiency-Performance Tradeoff
Distilling knowledge from large teacher models to lightweight student networks enables competitive VQA performance under strict computational constraints. A two-stage pipeline where (1) a teacher model trained on multi-modal features (SlowFast, FAST-VQA, LIQE, DeQA, HVS-5M) generates pseudo-labels for 30,000+ UGC videos, then (2) a lightweight student (Swin-T) learns from these labels while operating at ~47 GFLOPs vs. the unconstrained teacher. Core assumption: Quality-aware features learned by ensemble/large models can be compressed into smaller architectures without catastrophic loss of discriminative power.

### Mechanism 2: Multi-Scale Spatiotemporal Feature Fusion with Quality-Aware Region Selection
Combining low-resolution global context with high-resolution local details, filtered through quality-aware region selection, improves VQA accuracy for short-form UGC where irrelevant regions are common. Dual-stream architectures (ZX-AIE-Vector, ZQE) extract (1) coarse global features via low-resolution branches, (2) fine local details via high-resolution branches, and (3) temporal features via lightweight SlowFast. A slicing preprocessing step discards low-texture-density regions using Laplacian operator thresholds empirically tuned on KVQ.

### Mechanism 3: Two-Stage Diffusion with Fidelity-Perception Tradeoff Control
Separating pixel-level fidelity optimization from semantic-level perceptual enhancement, then fusing outputs, outperforms single-objective approaches for in-the-wild S-UGC super-resolution. Generate two candidate images—Ipsnr (λpix=1.0, λsem=0.0, high fidelity) and Iper (λpix=1.0, λsem=1.0, high perceptual quality)—then fuse via a learned NAFusion module combining high-frequency details from Iper with structural fidelity from Ipsnr. Dual LoRA modules control pixel vs. semantic enhancement balance.

## Foundational Learning

- **Concept: No-reference VQA (NR-VQA)**
  - Why needed: Track 1 operates on UGC videos without pristine references, requiring models to predict quality from distorted content alone. Understanding how features like blur, compression artifacts, and temporal inconsistencies map to perceived quality is foundational.
  - Quick check: Can you explain why NR-VQA is harder than full-reference VQA, and what inductive biases (e.g., natural scene statistics, HVS modeling) help constrain the problem?

- **Concept: Diffusion Model Conditioning**
  - Why needed: Track 2 relies on diffusion models for super-resolution, where conditioning on low-resolution inputs via ControlNet, VAE latents, or semantic embeddings (SAM2) determines output fidelity and realism.
  - Quick check: What is the difference between conditioning a diffusion model on (a) the LR image as a latent prior vs. (b) semantic embeddings from a separate encoder? Which approach would you expect to preserve more structural detail?

- **Concept: Knowledge Distillation Losses**
  - Why needed: SharpMind's winning approach uses differentiable SRCC/PLCC losses to transfer ranking knowledge from teacher to student. Understanding these correlation losses is critical for reproducing their distillation pipeline.
  - Quick check: Why might SRCC loss be preferred over MSE for distilling quality scores? What happens if teacher and student predictions have high rank correlation but large absolute errors?

## Architecture Onboarding

- **Component map**: Video → Keyframe sampler → Multi-branch feature extractors (Swin-T, SlowFast, VideoMamba) → Cross-attention fusion → MLP regressor → Quality score
- **Critical path**: Track 1: Keyframe sampling strategy (RQ-VQA vs. uniform) directly determines temporal coverage; feature fusion quality drives correlation performance; GFLOPs constraint forces backbone choices (Swin-T over Swin-B). Track 2: Degradation simulation pipeline (RealESRGAN-style) must match wild S-UGC distributions; λpix/λsem balance determines user study vs. objective metric tradeoff.
- **Design tradeoffs**: GFLOPs vs. accuracy: Top teams (SharpMind: 47G, TenVQA: 118G) both exceeded 0.90 final score, suggesting diminishing returns beyond ~50 GFLOPs for this dataset. Objective vs. subjective quality: SYSU-FVL-Team ranked 1st in objective metrics but 4th in user study, confirming fidelity-perception divergence.
- **Failure signatures**: VQA: Models with >150M parameters (GoldenChef, ZQE) showed no accuracy advantage over ~30M models, indicating over-parameterization without corresponding data scaling. SR: High LPIPS scores (>0.78 for all teams) suggest persistent perceptual artifacts; user study ranking volatility indicates sensitivity to image content.
- **First 3 experiments**: 1) Reproduce SharpMind's distillation pipeline: Train teacher on KVQ+private data, generate pseudo-labels for 5K held-out videos, train student with SRCC+PLCC losses. Compare student performance with/without pseudo-label augmentation. 2) Ablate ZQE's slicing preprocessing: Test texture-density thresholds {0.1, 0.2, 0.3, 0.4} on KVQ validation set. Measure impact on PLCC and identify failure cases (e.g., low-contrast important regions). 3) Implement TACO SR's fusion module: Generate Ipsnr and Iper for KwaiSR synthetic validation pairs, train NAFusion with L1+SSIM+LPIPS losses. Compare user study proxy (CLIPIQA, MANIQA) against single-objective baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can no-reference (NR) objective metrics be refined to reliably predict perceptual quality in diffusion-based Short-form UGC (S-UGC) super-resolution?
- Basis in paper: [explicit] The paper notes a "noticeable inconsistency between subjective preferences and objective metrics" in Track 2 results, suggesting current metrics fail to reflect perceptual quality in generative restoration.
- Why unresolved: Current metrics (PSNR, MUSIQ, etc.) showed weak correlation with user study rankings; for example, the user study winner (TACO SR) ranked 6th in objective metrics.
- What evidence would resolve it: A quantitative evaluation showing a high correlation coefficient (>0.9) between a new NR metric and human ratings specifically on diffusion-enhanced S-UGC images.

### Open Question 2
- Question: Does the heavy reliance on test-set pseudo-labeling and external pre-training compromise the generalization capability of efficient VQA models to out-of-distribution S-UGC content?
- Basis in paper: [inferred] Top teams (e.g., SharpMind, ZX-AIE-Vector) used large teacher models to pseudo-label external data or the test set itself to train lightweight student models, raising concerns about overfitting to the specific KVQ distribution.
- Why unresolved: While effective for the challenge leaderboard, the paper does not evaluate these distilled models on distinct, unseen S-UGC platforms or degradation types.
- What evidence would resolve it: A cross-dataset evaluation where models trained via pseudo-labeling on KVQ are tested on a separate, unseen S-UGC dataset without adaptation.

### Open Question 3
- Question: Can efficient VQA model constraints be pushed below 20 GFLOPs while maintaining high performance (SROCC > 0.90) on dynamic short-form video content?
- Basis in paper: [inferred] The winning model used 47.39 GFLOPs, and while the challenge capped computation at 120 GFLOPs, there is a lack of exploration into extreme low-power regimes (e.g., <20 GFLOPs) in the submitted results.
- Why unresolved: The lowest complexity valid submission (DAIQAM) used 66.36 GFLOPs, leaving the performance cliff for significantly smaller models unexplored in this challenge.
- What evidence would resolve it: A proposed architecture achieving competitive accuracy (>0.90 SROCC) on the KVQ test set with a verified computational cost of under 20 GFLOPs.

## Limitations
- The 30,000 closed-source UGC videos used for knowledge distillation in Track 1 are inaccessible, preventing exact reproduction of SharpMind's 0.922 score.
- Track 2's user study results rely on subjective human judgments that may not generalize beyond the specific participant pool and image content used.
- Computational efficiency metrics (GFLOPs, parameters) are measured on idealized inputs without accounting for runtime variability across hardware or real-time processing constraints.

## Confidence
- **High confidence**: The distillation pipeline architecture and training objectives (SRCC/PLCC losses) are clearly specified and reproducible with public data.
- **Medium confidence**: Multi-scale spatiotemporal fusion and quality-aware region selection mechanisms are described but lack ablation studies on hyperparameter sensitivity.
- **Low confidence**: User study methodology and participant demographics are not disclosed, limiting interpretation of Track 2's subjective quality results.

## Next Checks
1. Replicate SharpMind's distillation pipeline using a public UGC dataset (e.g., LSVQ) as proxy for closed-source videos, measuring student performance degradation.
2. Conduct ablation on ZQE's texture-density thresholds across {0.1, 0.2, 0.3, 0.4} to quantify impact on low-contrast region preservation.
3. Validate TACO SR's fusion module on wild S-UGC pairs from KwaiSR, comparing CLIPIQA/MANIQA scores against single-objective baselines to confirm perceptual gains.