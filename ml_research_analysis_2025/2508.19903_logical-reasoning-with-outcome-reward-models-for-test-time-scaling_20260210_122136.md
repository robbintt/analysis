---
ver: rpa2
title: Logical Reasoning with Outcome Reward Models for Test-Time Scaling
arxiv_id: '2508.19903'
source_url: https://arxiv.org/abs/2508.19903
tags:
- reasoning
- data
- training
- echo
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Logical Reasoning with Outcome Reward Models for Test-Time Scaling

## Quick Facts
- **arXiv ID:** 2508.19903
- **Source URL:** https://arxiv.org/abs/2508.19903
- **Reference count:** 18
- **Primary result:** ORM-EcCoT consistently outperforms both majority voting and standard CoT-based ORMs on FOLIO, JustLogic, and ProverQA logical reasoning datasets

## Executive Summary
This paper proposes Outcome Reward Models (ORMs) trained on synthetically generated Chain-of-Thought (CoT) data to enable test-time scaling for logical reasoning tasks. The key innovation is an "echo generation" (EcCoT) technique that leverages LLMs' tendency to reflect incorrect assumptions, creating "hard negative" training examples. The approach shows consistent improvements over majority voting baselines across multiple logical reasoning datasets when using Best-of-N sampling during inference.

## Method Summary
The method generates synthetic training data by prompting a strong LLM (GPT-4o or Qwen2.5-7B-Instruct) to produce 8-10 Chain-of-Thought reasoning samples per question. Positive and negative rewards are assigned based on final answer correctness. The novel EcCoT technique prompts the LLM to justify incorrect answers, creating challenging training examples. A Qwen2.5-7B-Instruct model is fine-tuned as a binary classifier using LoRA-based PEFT on this data. During inference, the trained ORM scores multiple reasoning candidates from a reasoner LLM, selecting the highest-scoring response via Best-of-N sampling.

## Key Results
- ORM-EcCoT consistently outperforms both majority voting and standard CoT-based ORMs across FOLIO, JustLogic, and ProverQA datasets
- Generating 8 samples per question provides optimal training data diversity and performance
- The echo generation technique covers previously unexplored error types, improving ORM robustness
- Performance gains are most pronounced on smaller datasets (JustLogic) but degrade at high N values for larger datasets (FOLIO)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training ORMs on multiple reasoning candidates per question improves discrimination between correct and incorrect reasoning compared to single-sample training
- **Mechanism:** Multiple CoT samples create diverse reasoning paths, providing richer signals for distinguishing valid logic from spurious correlations
- **Core assumption:** Diversity in generated samples covers error modes missed by single samples
- **Evidence anchors:** Abstract mentions generating multiple samples; Section 4.1 shows performance improves from 1 to 8 samples; Corpus provides background on outcome vs process supervision

### Mechanism 2
- **Claim:** "Echo generation" (EcCoT) creates "hard negative" training examples that refine error detection capabilities
- **Mechanism:** Coercing LLMs to justify incorrect answers exploits their sycophantic tendencies, producing plausible yet flawed reasoning
- **Core assumption:** Echoed errors are structurally similar to natural reasoning failures
- **Evidence anchors:** Abstract mentions leveraging LLMs' tendency to reflect incorrect assumptions; Section 2 describes the technique; Corpus papers don't cover echo technique specifically

### Mechanism 3
- **Claim:** Test-time scaling via Best-of-N selection guided by well-trained ORMs outperforms simple majority voting
- **Mechanism:** ORM acts as verifier, scoring entire reasoning sequences to surface correct but less frequent paths
- **Core assumption:** ORM reward correlates strongly with logical validity
- **Evidence anchors:** Section 2 describes using scores to re-rank candidates; Section 3.2 details Best-of-N sampling; Section 4.2 shows ORM-EcCoT outperforms baselines; Corpus supports reward models in TTS settings

## Foundational Learning

- **Concept: Outcome Reward Models (ORMs) vs. Process Reward Models (PRMs)**
  - **Why needed here:** The paper uses ORMs (verifying final result) rather than PRMs (verifying every step)
  - **Quick check question:** Does your training data label the final answer token, or require step-by-step human annotations?

- **Concept: Test-Time Scaling (Best-of-N)**
  - **Why needed here:** This is the deployment strategy for using the trained ORM
  - **Quick check question:** If you generate 16 samples and the ORM scores them, do you pick the one with the highest score or the one that appears most often?

- **Concept: Hard Negative Mining**
  - **Why needed here:** The "Echo" technique is a sophisticated form of hard negative mining
  - **Quick check question:** Why would training on "obviously wrong" random answers be less effective than training on "plausible but wrong" answers generated by a strong LLM?

## Architecture Onboarding

- **Component map:** Generator LLM -> Filter/Labeler -> ORM Trainer -> Inference Reasoner -> Verifier ORM
- **Critical path:** The Echo generation loop - if generator doesn't successfully hallucinate reasoning for wrong answers, or filter is too aggressive, ORM lacks robustness
- **Design tradeoffs:** Data volume vs diversity (JustLogic required resampling 10k subset); Generator strength (GPT-4o better than Qwen)
- **Failure signatures:** Majority Vote Parity (ORM matches majority voting exactly); Performance Collapse at Scale (degrades as N increases on FOLIO)
- **First 3 experiments:**
  1. Establish Baseline: Run target Reasoner with Majority Voting (N=16) on test set
  2. Ablation on Data Source: Train two ORMs - one on CoT only, one on CoT + Echo data
  3. Calibration Check: Plot ORM confidence score vs accuracy for fixed N

## Open Questions the Paper Calls Out
- **Question 1:** Can Process Reward Models (PRMs) that verify intermediate steps effectively mitigate the "correct answer, wrong reasoning" issue inherent in Outcome Reward Models (ORMs)?
- **Question 2:** To what extent does outcome supervision reinforce "reward hacking," where the model is rewarded for reaching a correct conclusion through invalid deductive steps?
- **Question 3:** Does the "echo" generation technique introduce a distribution shift that limits its effectiveness when the data generator differs significantly from the reasoner?

## Limitations
- Relies heavily on strength of generator LLM - weaker generators produce worse results
- Performance gains appear most pronounced on smaller datasets, with potential degradation at high N values for larger datasets
- Focuses exclusively on logical reasoning tasks, leaving generalizability to other domains uncertain
- Effectiveness depends critically on LLM judge's ability to distinguish subtly wrong reasoning from correct reasoning

## Confidence
- **High Confidence:** General efficacy of test-time scaling via Best-of-N selection with ORMs
- **Medium Confidence:** Specific benefits of echo generation technique
- **Medium Confidence:** Relationship between sample count (N) and performance across different datasets

## Next Checks
1. **Echo Generation Quality Control:** Systematically evaluate quality of generated Echo examples by having multiple human annotators verify whether LLM judge correctly identifies echoed reasoning as flawed
2. **Cross-Dataset Generalization:** Apply trained ORMs from FOLIO and JustLogic to ProverQA to assess echo technique benefits across different logical reasoning domains
3. **Generator Strength Ablation:** Conduct controlled experiment varying generator LLM strength while keeping all other variables constant to quantify relationship between generator capability and ORM performance