---
ver: rpa2
title: 'SOCK: A Benchmark for Measuring Self-Replication in Large Language Models'
arxiv_id: '2509.25643'
source_url: https://arxiv.org/abs/2509.25643
tags:
- task
- tasks
- across
- replication
- self-replication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SOCK, a benchmark for measuring large language
  models' (LLMs) ability to self-replicate without human intervention. SOCK defines
  self-replication as not only creating a functioning copy of itself, but also the
  ability for that self-replication to persist and occur across different computational
  contexts.
---

# SOCK: A Benchmark for Measuring Self-Replication in Large Language Models

## Quick Facts
- arXiv ID: 2509.25643
- Source URL: https://arxiv.org/abs/2509.25643
- Authors: Justin Chavarria; Rohan Raizada; Justin White; Eyad Alhetairshi
- Reference count: 10
- The highest levels reached in this suite are RCL 2 and PCL 2, with a 65% task success rate across 40 model-task pairs.

## Executive Summary
SOCK is a novel benchmark designed to measure large language models' ability to self-replicate autonomously across different computational contexts. The benchmark defines self-replication as not just creating a functional copy, but ensuring that this replication persists and occurs in varied environments. Using a controlled setting with five practical CLI-based tasks, SOCK evaluates LLMs' agentic performance and categorizes them into Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL). Results indicate that even advanced models struggle with persistent self-replication, revealing challenges in context retention and multi-agent coordination. The benchmark offers a structured framework for assessing and comparing LLM self-replication capabilities.

## Method Summary
SOCK introduces a five-task suite based on practical CLI utilities to measure LLM self-replication in a controlled environment. The benchmark evaluates models using an agentic LLM, assessing task success to compute an R-score and classify models into RCL and PCL matrices. The controlled setup ensures reproducibility and isolates specific replication challenges, such as context retention and multi-agent decision-making. Experiments are run across diverse open-weight and proprietary models to reveal systematic obstacles to persistent self-replication.

## Key Results
- Highest achieved levels: RCL 2 and PCL 2
- 65% task success rate across 40 model-task pairs
- Significant obstacles identified: context retention and multi-agent decision-making

## Why This Works (Mechanism)
Assumption: SOCK's controlled environment enables systematic measurement of self-replication by isolating key components like context retention and multi-agent coordination. The CLI-based tasks provide concrete, reproducible scenarios for evaluating LLM performance. The R-score computation aggregates task success rates into interpretable capability levels, allowing for standardized comparison across models.

## Foundational Learning
Assumption: The benchmark builds on established concepts in autonomous agent evaluation and task completion metrics. By adapting these principles to self-replication scenarios, SOCK creates a novel framework for measuring LLM persistence across computational contexts. The five-task structure draws from practical CLI utilities to ground evaluation in real-world scenarios.

## Architecture Onboarding
- **Component Map:** SOCK benchmark -> Controlled environment -> Five CLI tasks -> R-score computation -> RCL/PCL classification
- **Critical Path:** Controlled environment setup -> Agentic LLM execution -> Task completion evaluation -> R-score calculation -> Level assignment
- **Design Tradeoffs:** Controlled environment ensures reproducibility but may not reflect real-world complexity
- **Failure Signatures:** Context retention failures and multi-agent decision-making errors
- **First Experiments:**
  1. Test benchmark generalizability with more complex CLI utilities
  2. Isolate failure modes through ablation studies
  3. Evaluate performance in less constrained environments

## Open Questions the Paper Calls Out
Unknown: The paper may not explicitly address broader implications of self-replication capabilities or potential safety considerations, though these could be important areas for future investigation.

## Limitations
- Controlled environment may not fully capture real-world self-replication complexity
- Five-task suite may not represent all autonomous self-replication challenges
- R-score metric may mask important individual replication stage failures
- Benchmark focuses on CLI utilities, potentially missing other important replication scenarios

## Confidence
- **Experimental Methodology:** Medium
- **Generalizability of Results:** Medium
- **R-score Metric Reliability:** Medium

## Next Checks
1. Test benchmark with progressively more complex CLI utilities and real-world file system operations
2. Conduct ablation studies isolating context retention vs. multi-agent coordination failures
3. Implement benchmark in less constrained environments to validate external validity