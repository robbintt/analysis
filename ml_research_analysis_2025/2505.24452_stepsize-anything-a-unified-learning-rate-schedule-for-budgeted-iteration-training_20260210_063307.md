---
ver: rpa2
title: 'Stepsize anything: A unified learning rate schedule for budgeted-iteration
  training'
arxiv_id: '2505.24452'
source_url: https://arxiv.org/abs/2505.24452
tags:
- learning
- training
- rate
- schedule
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UBA (Unified Budget-Aware), a theoretically
  grounded learning rate schedule for budgeted-iteration training. Unlike existing
  heuristic approaches, UBA is derived from a min-max optimization framework that
  explicitly accounts for landscape curvature variations, ensuring robustness across
  diverse architectures and tasks.
---

# Stepsize anything: A unified learning rate schedule for budgeted-iteration training

## Quick Facts
- arXiv ID: 2505.24452
- Source URL: https://arxiv.org/abs/2505.24452
- Reference count: 40
- One-line primary result: UBA (Unified Budget-Aware) is a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules across diverse architectures and training-iteration budgets with negligible computational overhead

## Executive Summary
This paper introduces UBA (Unified Budget-Aware), a theoretically grounded learning rate schedule for budgeted-iteration training. Unlike existing heuristic approaches, UBA is derived from a min-max optimization framework that explicitly accounts for landscape curvature variations, ensuring robustness across diverse architectures and tasks. The schedule is controlled by a single hyperparameter φ, which provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Extensive experiments on vision and language tasks demonstrate that UBA consistently outperforms commonly-used schedules across diverse network architectures and scales under different training-iteration budgets.

## Method Summary
UBA is a unified learning rate schedule for budgeted-iteration training that uses a single hyperparameter φ to control decay speed. The schedule is derived from a min-max optimization framework that minimizes worst-case loss under landscape curvature variations. The core formula η_t = (η_max - η_min) * [2(1 + cos(...))] / [2φ + (2-φ)(1 + cos(...))] + η_min applies to all training phases. The theoretical connection between φ and the condition number κ (φ = 2/κ) provides interpretation and justification for the approach. The method supports single-phase (K=2) and multi-phase scheduling, with single-phase recommended for simplicity.

## Key Results
- UBA consistently outperforms commonly-used schedules (Cosine, Step, CLR, OneCycle, BT, REX) across diverse architectures and training-iteration budgets
- Achieves state-of-the-art performance on approximately half of the language benchmarks and maintains superior average performance across all scales
- Performance improvements come with negligible computational overhead, making it a practical, must-try schedule for deep learning practitioners

## Why This Works (Mechanism)

### Mechanism 1: Min-Max Robust Optimization for Learning Rate Selection
UBA derives learning rates that minimize worst-case loss under landscape curvature variations by formulating learning rate selection as a min-max optimization problem. The outer maximization considers worst-case Hessian eigenvalue bounds (λ_l to λ_u), while the inner minimization finds learning rates η_t that bound the convergence term ∏(1 - η_t λ_i)² across all possible curvature configurations. This approach provides robustness guarantees that standard optimization doesn't by explicitly accounting for landscape uncertainty.

### Mechanism 2: φ Controls Decay Shape via Condition Number Connection
The hyper-parameter φ mathematically connects to optimization difficulty through the condition number κ = λ_u/λ_l, where φ = 2/κ in the theoretical derivation. Through Chebyshev polynomial theory, φ modulates the transition between gradual and accelerated decay modes—smaller φ yields slower decay (good for well-conditioned landscapes), larger φ yields faster decay (good for ill-conditioned landscapes). This relationship adds interpretation and justification to the approach, though empirical φ values sometimes deviate from theoretical predictions.

### Mechanism 3: AdamW Preconditioning Effect on Optimal φ Selection
AdamW's adaptive gradient scaling implicitly reduces the effective condition number, making smaller φ values optimal compared to SGD. AdamW scales gradients by 1/√v_t, which acts as preconditioning in steep landscape regions, effectively smoothing curvature variations. This reduces the "optimization difficulty" the schedule must handle, shifting optimal φ from larger values (SGD: φ=5-10) to smaller values (AdamW: φ=5-10). The second moment estimate v_t in AdamW correlates with local Hessian eigenvalue magnitudes, though this mechanism remains unverified in external work.

## Foundational Learning

- Concept: **Min-Max Optimization**
  - Why needed here: Understanding how UBA formulates worst-case robustness requires grasping min-max problems where you optimize for the worst adversary (max) while minimizing your objective (min)
  - Quick check question: Can you explain why min-max formulation provides robustness guarantees that standard optimization doesn't?

- Concept: **Condition Number and Hessian Eigenvalues**
  - Why needed here: The core theoretical contribution links φ to κ = λ_max/λ_min; understanding why high κ means "hard optimization" is essential
  - Quick check question: If λ_max = 100 and λ_min = 1, what is κ and does a larger or smaller φ help convergence?

- Concept: **Chebyshev Polynomials in Optimization**
  - Why needed here: The paper proves UBA is the closed-form solution to the min-max problem via Chebyshev polynomial theory
  - Quick check question: Why do Chebyshev polynomials minimize maximum error on bounded intervals, and how does this relate to bounding convergence rates?

## Architecture Onboarding

- Component map:
```
UBA Schedule = f(t, T, φ, η_max, η_min, k)
├── Phase index k: determines which local minimum's curvature parameters apply
├── Current iteration t and phase boundaries [T_k, T_{k+1}]
├── Core formula (Eq. 5): η_t = (η_max - η_min) * [2(1 + cos(...))] / [2φ + (2-φ)(1 + cos(...))] + η_min
└── Key parameter φ: controls decay speed, linked to condition number
```

- Critical path:
1. Define total iterations T and any phase boundaries (default: single phase k=1)
2. Choose φ based on optimizer (AdamW: start φ≈0.5-1, SGD: start φ≈5)
3. Set η_max, η_min (typically η_min=0, η_max from existing configs)
4. Compute η_t per iteration using Eq. 5
5. Integrate warmup if needed (10% of total iterations recommended)

- Design tradeoffs:
  - **Single-phase (K=2) vs multi-phase**: Single-phase simpler to tune; multi-phase may capture dynamic landscape changes but requires per-phase φ selection
  - **φ selection**: Smaller φ = more conservative decay (better for AdamW/preconditioned landscapes); larger φ = aggressive decay (better for SGD/unpreconditioned landscapes)
  - **Warmup integration**: Not inherent to UBA formula; must add explicitly if model requires it

- Failure signatures:
  - **Training instability**: φ too large for optimizer choice (try reducing φ)
  - **Slow convergence**: φ too small (try increasing φ)
  - **Late-training stagnation**: May need warmup phase; check if η_min > 0 helps
  - **Inconsistent results across runs**: Verify φ is fixed; check if landscape is highly non-convex

- First 3 experiments:
  1. **Baseline comparison on CIFAR100 with ResNet34**: Fix φ=5 for SGD, compare against cosine/step/REX schedules at 25%/50%/100% epoch budgets—expect 1-2% accuracy improvement
  2. **Cross-optimizer φ sweep**: Test φ∈{0.25, 0.5, 1.0, 2.5, 5, 10} with both SGD and AdamW on same task—expect AdamW prefers φ≈0.5-1, SGD prefers φ≈5-10
  3. **Scale transfer test**: Apply best φ from small model (e.g., OLMo-36M) to larger scale (OLMo-150M) without retuning—validate that fixed φ maintains performance advantage

## Open Questions the Paper Calls Out

### Open Question 1
How can optimization difficulty be explicitly quantified to enable automated, metric-driven selection of the hyper-parameter φ? Currently, φ is selected via ablation studies or heuristics (e.g., smaller φ for AdamW), lacking a formal, measurable metric to define "optimization difficulty." A defined metric or theoretical mapping that predicts the optimal φ based on local landscape properties (like condition number) prior to training would resolve this.

### Open Question 2
How can φ be dynamically adapted across multi-phase scheduling to better capture non-stationary loss landscapes? While multi-phase scheduling captures dynamic features, selecting optimal φ values per phase is "non-trivial" and motivates "automated landscape-aware φ tuning." An automated algorithm for setting φ per phase that successfully outperforms the single-phase UBA baseline on varying budgets would resolve this.

### Open Question 3
How robust is UBA's theoretical derivation when the loss landscape violates the assumption of being well-approximated by quadratic expansions near strict local optima? The methodology relies on the assumption that the loss surface can be approximated by quadratic expansion near strict local optima. If the optimization trajectory traverses regions far from local optima or highly non-convex areas, the min-max optimization framework may lose validity. Analysis of UBA performance on landscapes with extreme non-convexity or noise where quadratic approximations fail would resolve this.

## Limitations

- The theoretical φ-condition number relationship (φ = 2/κ) is empirically supported but not independently validated in external work
- The min-max formulation assumes quadratic landscape approximations that may not hold for highly non-convex regions or early training stages
- Cross-optimizer generalization relies on an intuitive preconditioning argument for AdamW rather than rigorous mathematical proof
- Multi-phase extensions (K>2) are mentioned but not thoroughly validated, with the paper recommending single-phase K=2 for simplicity

## Confidence

- **High confidence**: Empirical performance superiority across diverse tasks (CIFAR, ImageNet, OLMo benchmarks), the practical simplicity of φ as single hyperparameter, and the general observation that different optimizers require different φ ranges
- **Medium confidence**: The min-max optimization framework providing robustness guarantees, and the connection between φ and optimization difficulty through condition number
- **Low confidence**: The specific preconditioning mechanism for AdamW, and the theoretical derivation's quantitative predictions for optimal φ values

## Next Checks

1. **Landscape curvature validation**: Measure actual Hessian eigenvalue distributions during training to verify if the assumed quadratic approximation holds and if empirical φ values correlate with observed condition numbers
2. **Break-case testing**: Systematically evaluate UBA on architectures/tasks known for highly non-convex landscapes (e.g., GANs, reinforcement learning) to identify where the quadratic approximation assumption fails
3. **Independent theoretical verification**: Have the min-max formulation and Chebyshev polynomial solution independently reviewed by optimization theory experts to confirm the mathematical derivation and identify any hidden assumptions