---
ver: rpa2
title: Pessimistic Verification for Open Ended Math Questions
arxiv_id: '2511.21522'
source_url: https://arxiv.org/abs/2511.21522
tags:
- verification
- since
- proof
- pessimistic
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces pessimistic verification, a technique that
  improves mathematical proof verification by conducting multiple parallel checks
  and declaring a proof incorrect if any check finds an error. Three variants are
  proposed: simple, vertical, and progressive pessimistic verification.'
---

# Pessimistic Verification for Open Ended Math Questions

## Quick Facts
- arXiv ID: 2511.21522
- Source URL: https://arxiv.org/abs/2511.21522
- Authors: Yanxing Huang; Zihan Tang; Zejin Lin; Peng Li; Yang Liu
- Reference count: 40
- Key outcome: Three pessimistic verification variants (simple, vertical, progressive) consistently improve error detection and balanced F1 scores across multiple math verification benchmarks, with progressive verification offering highest efficiency.

## Executive Summary
This paper introduces pessimistic verification, a technique that improves mathematical proof verification by conducting multiple parallel checks and declaring a proof incorrect if any check finds an error. Three variants are proposed: simple, vertical, and progressive pessimistic verification. Experiments on three benchmarks show consistent improvements in error detection rates and balanced F1 scores across various model sizes. Case studies reveal that stronger models' apparent false negatives often stem from dataset annotation errors, indicating pessimistic verification's true performance is underestimated. The method demonstrates higher efficiency than scaling long chain-of-thought reasoning.

## Method Summary
Pessimistic verification is a test-time scaling technique for mathematical proof verification that runs multiple parallel checks and aggregates results using logical OR (fail if any check reports error). Three variants are introduced: simple pessimistic verification runs n parallel whole-proof checks; vertical pessimistic verification splits proofs into fixed-size chunks for focused verification; progressive pessimistic verification combines both approaches hierarchically with early pruning. The method exploits the asymmetry that detecting errors is harder than confirming correctness, using multiple diverse attempts to catch subtle mistakes missed by single-pass verification.

## Key Results
- Simple pessimistic verification achieves 0.03-0.10 balanced F1 improvements across all model sizes and benchmarks
- Progressive pessimistic verification provides highest efficiency-performance tradeoff, outperforming long-CoT scaling at lower equivalent output tokens
- Stronger models (GPT-5-mini) show fewer false negatives and higher error detection rates compared to weaker models (Qwen3-30B-A3B)
- Case studies reveal many "false negatives" from strong models actually uncover dataset annotation errors, suggesting true performance is underestimated

## Why This Works (Mechanism)

### Mechanism 1: Error-First Aggregation Over Majority Consensus
Running multiple parallel verifications and declaring a proof incorrect if ANY check reports an error improves error detection compared to majority voting. The approach exploits asymmetry in verification: detecting errors is substantially harder than confirming correct proofs. A single successful error detection outweighs multiple "no error found" verdicts because verifiers systematically miss subtle mistakes. The method uses logical OR across parallel checks rather than voting.

### Mechanism 2: Spatial Attention Partitioning via Chunked Verification
Splitting proofs into fixed-size segments for focused verification improves detection of localized errors that whole-proof review misses. Constrained attention window (controlled by hyperparameter `l` for line count) forces deeper scrutiny per segment while maintaining full context. The prompt instructs: "Check ONLY the given chunk for errors while considering the overall context."

### Mechanism 3: Multi-Scale Progressive Filtering with Early Pruning
Hierarchical verification combining whole-proof and chunked checks with pruning achieves higher token efficiency than simple pessimistic or long-CoT scaling. Starts with whole-proof verification, then iteratively subdivides by halving. Proofs flagged as incorrect at any stage are pruned—no further computation. Maximum checks per proof bounded at `2^n - 1` where `n` = iteration depth.

## Foundational Learning

- **Concept: Balanced F1 = 2 × (TPR × TNR) / (TPR + TNR)**
  - Why needed here: Primary evaluation metric. Unlike standard F1 (harmonic mean of precision/recall), balanced F1 equally weights True Positive Rate (correct acceptance rate) and True Negative Rate (error detection rate). Essential for understanding Figure 1, 5, 6 results.
  - Quick check question: If a verifier has TPR=0.95 and TNR=0.60, what's the balanced F1? (Answer: ~0.74)

- **Concept: Test-Time Scaling via Sampling vs. Chain-of-Thought Extension**
  - Why needed here: Paper positions pessimistic verification as an alternative to long-CoT scaling. Sampling approaches (parallel queries) distribute compute across independent checks; CoT extends sequential reasoning depth. Understanding this tradeoff is prerequisite for interpreting Figure 1 efficiency comparisons.
  - Quick check question: What are the latency characteristics of parallel sampling vs. sequential CoT extension? (Answer: Parallel sampling has wall-clock time ~max(single_query), CoT has wall-clock ~sum(all_steps))

- **Concept: Annotation Error vs. Model Error (False Negative Analysis)**
  - Why needed here: Section 3.4 case study reveals many "false negatives" from strong models actually uncovered dataset annotation errors. This means reported metrics underestimate true performance. Understanding this confound is prerequisite for interpreting Table 1 and Appendix A.3.
  - Quick check question: If 70% of GPT-5's "false negatives" on IMO-GradingBench identified real annotation errors, what does this imply about the dataset? (Answer: Ground truth labels contain systematic missed errors; model is more rigorous than annotators)

## Architecture Onboarding

- **Component map:**
  Input: (problem_text, proof_text)
     │
     ├─► [Simple Pessimistic] ──► n parallel whole-proof queries ──► OR-aggregation
     │
     ├─► [Vertical Pessimistic] ──► chunk(n_lines=l) ──► parallel chunk queries ──► OR-aggregation
     │
     └─► [Progressive Pessimistic] ──► iterate: whole-proof → half-split → quarter-split ...
                                      └─► prune flagged proofs at each level
  Output: (verdict: correct/incorrect, error_description_if_any)

- **Critical path:** Progressive pessimistic verification is recommended default (highest efficiency-performance tradeoff). Key hyperparameters: `n` (max iterations, paper uses 3), `l` (min chunk length, paper uses 6). Temperature=1.0 for diversity.

- **Design tradeoffs:**
  - Higher `n` (more iterations): ↑ error detection, ↓ efficiency (more checks), risk of over-fragmentation
  - Lower `l` (smaller chunks): ↑ granularity, ↑ context loss risk, ↑ false positive rate
  - Higher parallel query count: ↑ coverage, ↑ token cost (linear scaling)
  - Pruning enabled: ↑ efficiency on negative-heavy distributions, minimal effect on positive-heavy data

- **Failure signatures:**
  - TPR degradation: Overly aggressive chunking flags minor typos as critical errors (Section 3.4: "minor" vs. "critical" vs. "nonsense" FN categories)
  - Efficiency collapse on correct proofs: No pruning occurs when most proofs are valid; worst-case cost = full verification tree
  - Cross-chunk reasoning gaps: Errors requiring multi-chunk context get missed (not explicitly measured in paper)
  - Verifier hallucination: Weaker models generate "nonsense" justifications (Table 1: Qwen3-30B-A3B has 13/15 nonsense FNs vs. GPT-5's 2/20)

- **First 3 experiments:**
  1. Baseline comparison: Implement pes@3 (3 parallel whole-proof checks) on IMO-GradingBench subset. Measure balanced F1, TNR, TPR against single-pass baseline. Expected: +0.05-0.10 F1 gain per Figure 6.
  2. Hyperparameter sweep: Test progressive@n/l with n∈{2,3,4} and l∈{4,6,8,10} on Hard2Verify. Plot equivalent output tokens vs. balanced F1. Identify Pareto frontier.
  3. Error taxonomy audit: Sample 20 false negatives from strongest available model. Manually classify as critical/minor/nonsense per Table 1 methodology. Estimate annotation error rate in test dataset.

## Open Questions the Paper Calls Out

### Open Question 1
How can pessimistic verification be effectively integrated into the training pipeline of large reasoning models to raise their upper performance limits on verification and rigorous proof tasks? The paper demonstrates the method's efficacy at inference time but does not explore its use as a supervision or reward signal during training.

### Open Question 2
Can the principles of pessimistic verification (error-centric, multi-scale review) generalize effectively to other verification domains like code review or legal document analysis? The paper's experiments are confined to mathematical benchmarks, leaving its domain-generality an open question.

### Open Question 3
What are the optimal strategies for balancing the trade-off between True Positive Rate (TPR) and True Negative Rate (TNR) in pessimistic verification for different application contexts? The paper notes a trade-off where TPR may decrease as TNR improves, and that weaker models show a greater TPR loss.

## Limitations

- **Annotation error confound**: Many "false negatives" from strong models actually uncover dataset annotation errors, meaning reported metrics systematically underestimate true performance. This confound isn't quantified across all benchmarks.
- **Cross-chunk reasoning gaps**: The paper doesn't measure error detection when mistakes require reasoning across chunk boundaries. Progressive verification's pruning strategy may miss these distributed errors.
- **Generalizability constraints**: Results are benchmark-specific; performance gains on other mathematical domains or proof formats remain unknown. The method assumes proof steps are roughly line-aligned and errors are locally clustered.

## Confidence

- **High confidence**: Error-first aggregation (Mechanism 1) - directly supported by Figure 3's clear contrast between pes@n and maj@n results across all settings.
- **Medium confidence**: Chunked verification (Mechanism 2) - performance improvements shown, but without direct comparison to attention-based whole-proof methods or measurement of cross-chunk reasoning capabilities.
- **Medium confidence**: Progressive filtering efficiency claims - Figure 1 shows efficiency gains, but the "equivalent output tokens" calculation method isn't fully specified, and results depend on positive/negative example distribution.

## Next Checks

1. **Annotation error audit across all benchmarks**: Sample 50 false negatives from the strongest available model on each benchmark. Manually classify as critical/minor/nonsense and estimate ground truth error rates to quantify how much reported metrics underestimate true performance.

2. **Cross-chunk reasoning stress test**: Design adversarial proofs with errors requiring multi-chunk context. Compare progressive verification's error detection rate against whole-proof verification to measure reasoning gap impact.

3. **Positive-heavy distribution efficiency test**: Create a balanced dataset with 70-80% correct proofs (vs. current negative-heavy benchmarks). Measure progressive verification's efficiency advantage and identify when pruning becomes ineffective.