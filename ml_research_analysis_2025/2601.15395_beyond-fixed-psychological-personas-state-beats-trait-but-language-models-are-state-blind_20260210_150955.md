---
ver: rpa2
title: 'Beyond Fixed Psychological Personas: State Beats Trait, but Language Models
  are State-Blind'
arxiv_id: '2601.15395'
source_url: https://arxiv.org/abs/2601.15395
tags:
- psychological
- language
- user
- across
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chameleon, a dataset of 5,001 contextual psychological profiles
  from 1,667 Reddit users across 645 subreddits, enables the first principled decomposition
  of psychological variance in text into stable traits and contextual states. Using
  Latent State-Trait theory, the analysis reveals that 74% of variance is within-person
  (state) while only 26% is between-person (trait), demonstrating that context shapes
  expressed psychology 2-3x more than stable individual differences.
---

# Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind

## Quick Facts
- arXiv ID: 2601.15395
- Source URL: https://arxiv.org/abs/2601.15395
- Reference count: 39
- Primary result: Context shapes expressed psychology 2-3x more than stable individual differences (74% state vs 26% trait variance)

## Executive Summary
Chameleon reveals that psychological variance in text is predominantly contextual rather than stable, with 74% of variance within-person (state) versus 26% between-person (trait). This state-dominance challenges the common practice of fixed psychological personas in AI systems. When tested, large language models exhibit shallow persona detection—recognizing persona framing but failing to differentiate between distinct psychological profiles, producing nearly identical responses regardless of user state. Reward models violate state-invariance by scoring identical responses differently based on user psychology, with different models disagreeing on direction, potentially propagating arbitrary biases through RLHF training.

## Method Summary
The study constructs Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users across 645 subreddits, using dual extraction (SEANCE lexicon + LangExtract LLM) to assess 26 psychological dimensions. Latent State-Trait theory decomposes variance into stable traits and contextual states, revealing state-dominance (ICC=0.26-0.28). The dataset enables testing LLM adaptation through 127 questions across 6 archetypes plus baseline, and reward model state-invariance through evaluation of identical responses across psychological conditions.

## Key Results
- 74% of psychological variance is within-person (state) while only 26% is between-person (trait)
- LLMs exhibit shallow persona detection: semantic similarity shows low differentiation between archetypes (F=2.18, p=.054)
- Reward models violate state-invariance with large effect sizes (d > 1.0), explaining 7-30% of score variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Psychological variance in text is predominantly contextual (state) rather than stable (trait)
- Mechanism: Latent State-Trait theory decomposes observed profiles ψu,c = τu + σu,c + ϵu,c. Intraclass correlation (ICC) quantifies the proportion of variance from stable traits versus states; ICC < 0.30 indicates state-dominance
- Core assumption: Subreddit context serves as a valid proxy for situational demands that elicit different psychological expressions
- Evidence anchors: [abstract] "74% of variance is within-person (state) while only 26% is between-person (trait)"; [Section 2.4] SEANCE-derived ICC = 0.26; LangExtract-derived ICC = 0.28; convergence across both methods

### Mechanism 2
- Claim: LLMs exhibit "shallow persona detection"—recognizing framing but failing to differentiate distinct psychological profiles
- Mechanism: Models produce similar responses across conditions; semantic similarity (all-mpnet-base-v2) shows low differentiation between archetypes (F=2.18, p=.054)
- Core assumption: Adaptation to psychological state would manifest as measurably different semantic content
- Evidence anchors: [abstract] "they recognize persona framing but fail to differentiate between distinct psychological profiles"; [Section 3.2] Llama-3.1-8B (smallest) showed highest sensitivity (mean sim=.768); GPT-4o and Qwen more similar across conditions

### Mechanism 3
- Claim: Reward models react to user psychology inconsistently—same profile favored by one model and penalized by another
- Mechanism: State-invariance principle RM(r|q, ai) = RM(r|q, aj) is violated; ArmoRM rewards Distressed-Vulnerable (+0.76 d), Skywork penalizes it (-1.12 d)
- Core assumption: Identical responses should receive identical quality scores regardless of user psychology
- Evidence anchors: [abstract] "the same vulnerable user is maximally favored by one model and maximally penalized by another"; [Section 3.3] All reward models violate state-invariance with d > 1.0 effect sizes

## Foundational Learning

- Concept: Latent State-Trait (LST) theory
  - Why needed here: Provides the formal framework for decomposing observed behavior into stable trait (τu) and contextual state (σu,c) components; ICC operationalizes this decomposition
  - Quick check question: If ICC = 0.27, what proportion of variance is within-person versus between-person?

- Concept: Intraclass Correlation Coefficient (ICC)
  - Why needed here: Quantifies the ratio of between-person variance to total variance; ICC < 0.30 indicates state-dominant constructs
  - Quick check question: Would floor effects in small-k designs (k=3) inflate or deflate between-person variance estimates?

- Concept: RLHF reward model role
  - Why needed here: Reward models score response quality and drive LLM training; biases in reward models propagate into deployed model behavior
  - Quick check question: If reward models disagree on direction of psychological bias, what happens during RLHF training?

## Architecture Onboarding

- Component map: Webis-TLDR-17 corpus -> Dual extraction pipeline (SEANCE + LangExtract) -> Z-normalized fusion -> 26-dimensional profiles -> Archetype clustering -> LLM generation experiments + reward model evaluation

- Critical path: (1) Extract profiles from multi-context user posts; (2) Compute ICC to verify state-dominance; (3) Cluster into archetypes; (4) Prompt LLMs with archetype-conditioned questions; (5) Score responses with reward models across conditions

- Design tradeoffs: Lexicon-based (SEANCE) offers reproducibility but limited context sensitivity; LLM-based (LangExtract) captures nuance but introduces stochasticity; fusion balances both. k=3 posts per user enables within-person comparison but limits ICC precision; authors note 26% trait estimate is likely an upper bound

- Failure signatures: High similarity across archetype conditions (>0.80 semantic similarity) indicates state-blindness; reward model disagreement on sign (positive vs. negative d) indicates inconsistent context-reactivity

- First 3 experiments:
  1. Replicate ICC computation on a held-out subset to validate state-dominance (expected: ~72–74% within-person variance)
  2. Test whether smaller models consistently show higher psychological sensitivity, or if this is specific to current model families
  3. Measure correlation between reward model psychological bias and downstream RLHF model behavior to verify propagation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the 72–74% within-person variance finding generalize to other platforms (e.g., Twitter, professional forums, messaging apps) and communication styles beyond Reddit?
- Basis in paper: [explicit] "Our findings rely on a single dataset (Reddit); generalization to other platforms and communication styles remains to be tested."
- Why unresolved: Chameleon uses only Reddit posts (2006–2016), which may have specific community norms and anonymity patterns not representative of other contexts
- What evidence would resolve it: Replicating the variance decomposition methodology on datasets from Twitter/X, Discord, professional email, or longitudinal messaging data

### Open Question 2
- Question: How do precision estimates of state-trait variance change when users are sampled across more than three contexts (k≥10)?
- Basis in paper: [explicit] "Our ICC estimates may be affected by floor effects inherent in small-k designs (k=3 observations per user)... Future work could extend this approach by sampling users across a larger number of contexts (k=10+), enabling more precise variance decomposition."
- Why unresolved: The current design with exactly three posts per user may inflate between-person variance estimates, making the 26% trait figure an upper bound
- What evidence would resolve it: Collecting 10+ posts per user across diverse contexts and recomputing ICC estimates to determine if within-person variance exceeds the current 74% estimate

### Open Question 3
- Question: Can reward models be trained to respond to psychological context in principled, consistent ways rather than exhibiting arbitrary biases?
- Basis in paper: [inferred] "Resolving this requires reward models that respond to psychological context in principled, consistent ways." The paper demonstrates inconsistent reward model behavior but does not propose solutions
- Why unresolved: Different reward models favor or penalize the same user profiles in opposite directions, but the paper does not investigate training interventions to ensure consistency
- What evidence would resolve it: Developing and evaluating reward models trained with explicit state-invariance constraints or consistency regularization across psychological contexts

## Limitations
- Subreddit context serves as proxy for psychological state but selection bias remains unaddressed—users may choose subreddits matching stable traits
- k=3 posts per user design constrains ICC precision, making the 74% state estimate likely conservative
- Fusion of SEANCE lexicon and LangExtract outputs introduces uncertainty about whether scale-specific noise is properly controlled

## Confidence

- High confidence: State-dominance finding (74% variance within-person) supported by convergent ICC estimates from both extraction methods
- Medium confidence: LLM shallow persona detection results, though semantic similarity metrics may miss pragmatic adaptation
- Low confidence: Reward model inconsistency interpretations—without understanding training data provenance, bias versus genuine preference distinction is unclear

## Next Checks

1. Test whether ICC estimates converge at higher k values (5+ posts per user) to verify state-dominance robustness
2. Measure pragmatic features (hedging, formality, response length) across psychological conditions to detect non-semantic adaptation
3. Examine reward model training data for systematic correlations between user psychology and response preferences to distinguish arbitrary bias from genuine heterogeneity