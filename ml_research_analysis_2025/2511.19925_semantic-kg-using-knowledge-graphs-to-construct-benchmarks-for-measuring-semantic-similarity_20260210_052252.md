---
ver: rpa2
title: 'Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring
  Semantic Similarity'
arxiv_id: '2511.19925'
source_url: https://arxiv.org/abs/2511.19925
tags:
- name
- node
- semantic
- dataset
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for generating semantic
  similarity benchmarks using knowledge graphs (KGs), addressing the challenge of
  evaluating large language models' (LLMs) ability to capture semantic content. The
  method involves perturbing KGs to create semantic variations, generating natural-language
  statements from these graphs, and validating the outputs to ensure correctness.
---

# Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity

## Quick Facts
- **arXiv ID:** 2511.19925
- **Source URL:** https://arxiv.org/abs/2511.19925
- **Reference count:** 40
- **Primary result:** Novel framework generates semantic similarity benchmarks from knowledge graph perturbations, revealing that method performance varies by perturbation type and domain.

## Executive Summary
This paper introduces a novel framework for generating semantic similarity benchmarks using knowledge graphs (KGs), addressing the challenge of evaluating large language models' (LLMs) ability to capture semantic content. The method involves perturbing KGs to create semantic variations, generating natural-language statements from these graphs, and validating the outputs to ensure correctness. Benchmark datasets were created in four domains (general knowledge, biomedicine, finance, and biology) and used to evaluate various semantic similarity methods, including traditional NLP metrics and LLM-as-a-judge. The results revealed that method performance varies significantly depending on the type of semantic perturbation and the domain, with no single method consistently superior. This highlights the need for domain-specific and semantic-aware evaluation when assessing LLM outputs.

## Method Summary
The framework operates by first sampling subgraphs from knowledge graphs using BFS traversal, then applying four perturbation types (node removal, node replacement, edge removal, edge replacement) to create semantic variations. Generated text is produced from both original and perturbed subgraphs using LLM prompting, followed by validation through a two-stage LLM reconstruction process that requires exact triple matches. The method was applied to four domain-specific KGs to create benchmark datasets, with thresholds optimized on validation splits to maximize F1 score for binary classification (similar vs. dissimilar pairs).

## Key Results
- Different semantic similarity methods excel at different perturbation types, with no single method consistently superior across all conditions
- Traditional NLP metrics (ROUGE, BLEU, embedding similarity) perform well on node-based perturbations but struggle with edge-based perturbations
- LLM-as-a-judge performs better on edge perturbations but underperforms on node perturbations
- Domain-specific performance variations exist, with F1 scores ranging from 0.51 to 0.85 across methods and datasets

## Why This Works (Mechanism)

### Mechanism 1: Structure-to-Semantics Mapping via KG Perturbations
- Claim: Controlled graph perturbations produce predictable semantic variations in generated text.
- Mechanism: The framework modifies KGs through four perturbation types (node removal, node replacement, edge removal, edge replacement), then generates text from both original and perturbed subgraphs. Because KG nodes/edges directly encode entities and relationships, structural changes produce targeted semantic differences that can be labeled without human annotation.
- Core assumption: The semantic content of generated text is fully determined by the KG structure, and perturbations produce unambiguous semantic dissimilarity.
- Evidence anchors:
  - [abstract]: "Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types."
  - [section 2.2]: "Perturbations are applied both to entities contained within the subgraph, through perturbations to nodes, and the relationships between entities, by targeting edges."
  - [corpus]: Weak direct evidence; corpus focuses on causal graph comparison and RAG hallucination detection, not KG-based benchmark generation.
- Break condition: If generated text introduces information not grounded in the KG, or if perturbations produce ambiguous semantic changes (e.g., replacing "parent" with "guardian"), the controlled semantic mapping degrades.

### Mechanism 2: Validation-by-Reconstruction Ensures Grounding
- Claim: Requiring exact KG reconstruction from generated text ensures statements faithfully reflect source subgraphs.
- Mechanism: The validation pipeline uses a two-stage LLM process (entity extraction followed by KG extraction) to reconstruct triples from generated statements. Only statements where reconstructed triples exactly match original triples (after normalization) are retained.
- Core assumption: LLMs can reliably extract entities and relationships from natural language when provided with entity types and edge vocabularies.
- Evidence anchors:
  - [section 2.4]: "A generated response is only considered valid if the reconstructed triples exactly match the original triples."
  - [section 4, Table 2]: Manual validation shows 99% correctness but only 75% naturalness rating.
  - [corpus]: Not directly addressed in corpus papers.
- Break condition: If LLM extraction fails on complex statements with many entities (>12 triples noted as declining success rate in Figure 7), or if normalization removes meaningful distinctions, validation yields false positives or excessive rejection.

### Mechanism 3: Perturbation-Type Stratification Reveals Method-Specific Weaknesses
- Claim: Different semantic similarity methods excel at different perturbation types; no single method is universally superior.
- Mechanism: By categorizing dissimilar pairs by perturbation subtype, the benchmark reveals that traditional NLP methods (ROUGE, BLEU, embedding similarity) perform well on node-based perturbations (entity presence/absence) but struggle with edge-based perturbations (relationship changes). Conversely, LLMs perform better on edge perturbations but underperform on node perturbations.
- Core assumption: The four perturbation types represent distinct semantic failure modes that generalize to real-world semantic variations.
- Evidence anchors:
  - [abstract]: "We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior."
  - [section 6.2]: Statistical analysis shows significant interaction effects: GPT-4o x edge-replacement (β=0.184, p=0.031); Sentence-T5-Base x node-removal (β=0.260, p=0.002).
  - [corpus]: CSSG paper similarly finds surface-level metrics fail to capture semantic relationships in code.
- Break condition: If real-world semantic variations don't decompose cleanly into these four types (e.g., nuance, tone, implicit meaning), benchmark performance may not predict real-world performance.

## Foundational Learning

- Concept: Knowledge Graph Triples (Subject-Relation-Object)
  - Why needed here: The entire framework operates on subgraphs represented as triples. Understanding that nodes encode entities and edges encode relationships is prerequisite to understanding perturbation semantics.
  - Quick check question: Given the triple ("benidipine", "increases effect", "bradycardia"), what semantic change occurs if the edge is replaced with "decreases effect"?

- Concept: Semantic vs. Syntactic Similarity
  - Why needed here: The paper's central claim is that existing metrics (ROUGE, BLEU) capture syntax but miss semantics. Figure 1 demonstrates this with examples where high token overlap accompanies opposite meaning.
  - Quick check question: Two sentences share 90% of tokens but convey contradictory relationships. Would ROUGE-L score them as similar or dissimilar?

- Concept: Closed-World Assumption in KGs
  - Why needed here: The framework assumes missing information in KGs is false. Section 8 notes this as a limitation—it may not capture incomplete or evolving knowledge.
  - Quick check question: If a KG has no edge between entities A and B, does the framework assume no relationship exists, or that the relationship is unknown?

## Architecture Onboarding

- Component map: Subgraph Sampler -> Perturbation Engine -> Text Generator -> Validator
- Critical path: Sampling → Perturbation → Generation → Validation. Validation is the bottleneck—Figure 7 shows reconstruction success declines sharply above 12 triples per subgraph.
- Design tradeoffs:
  - Subgraph size vs. validation success: Larger subgraphs produce more complex statements but fail validation more often.
  - Perturbation coverage vs. semantic clarity: The paper uses "arbitrary" perturbation choices; more complex perturbations (noise, irrelevant info) are noted as future work.
  - Domain specificity vs. generalization: Custom edge-replacement mappings (Appendix H) require manual definition per dataset.
- Failure signatures:
  - Low reconstruction rate: Check subgraph size; if >12 triples, expect failures.
  - High token-overlap but low semantic accuracy: Indicates edge-perturbation failures; LLM judges preferred.
  - Domain-specific performance drops: ROUGE-1 shows marked decline on FinDKG edge-replacements (Table 11), suggesting relationship-detection weakness in financial domain.
- First 3 experiments:
  1. Run the framework on a new domain KG (e.g., legal case relationships). Manually validate a 100-statement sample to establish baseline correctness and naturalness for that domain.
  2. Isolate edge-replacement perturbations and compare LLM-as-judge vs. embedding similarity performance. Expect LLMs to outperform on relationship changes per Section 6.2 findings.
  3. Test generalization by training a threshold on one domain's validation split and evaluating on another domain's test split. If F1 drops >15%, domain-specific threshold tuning is required.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of semantic similarity methods change when evaluating against complex, realistic perturbations (e.g., intent, tone) rather than simple structural node/edge changes?
- Basis in paper: [explicit] Section 8 states that real-world semantic variations are complex and not captured by simple node or edge deletions, and suggests future work should "apply more complex, realistic perturbations" such as noise or irrelevant information.
- Why unresolved: The current study is strictly limited to four types of structural perturbations (node/edge removal/replacement), which may not fully represent the nuanced semantic variations found in natural language use.
- What evidence would resolve it: A comparative study benchmarking models on the current Semantic-KG dataset versus a new dataset generation incorporating perturbations for tone, intent, or implicature.

### Open Question 2
- Question: How would incorporating graded annotation schemes based on perturbation-count or graph similarity metrics impact the comparative evaluation of LLMs versus traditional NLP methods?
- Basis in paper: [explicit] Section 8 identifies the "simple binary setting" as a limitation and proposes that "graded annotation schemes inspired by human-labeled datasets... might be incorporated into future versions."
- Why unresolved: The current framework treats semantic similarity as a binary classification task (similar/dissimilar), failing to capture the degrees of similarity that human evaluators often distinguish.
- What evidence would resolve it: Experimental results comparing model performance on binary labels versus continuous similarity scores derived from graph edit distances or perturbation counts.

### Open Question 3
- Question: To what extent does the "closed-world assumption" inherent in Knowledge Graphs skew the evaluation of semantic similarity methods compared to an "open-world" context?
- Basis in paper: [explicit] Section 8 highlights that KGs operate under a closed-world assumption (missing info is false), which may not capture evolving knowledge, and suggests future work explore "how performance... varies under an open-world assumption."
- Why unresolved: Current benchmarks penalize models for inferring valid relationships that are absent from the source Knowledge Graph, potentially misrepresenting a model's true semantic understanding.
- What evidence would resolve it: An evaluation methodology that distinguishes between factual contradictions and missing information, allowing for "unknown" rather than "false" labels for non-existent edges.

### Open Question 4
- Question: Can semantic similarity methods maintain robust performance when applied to larger subgraphs containing more than 12 triples?
- Basis in paper: [explicit] Section 8 lists "Limitations of subgraph-size" noting that the success of the generation pipeline declines at higher subgraph sizes (>12 triples), limiting the complexity of generated statements.
- Why unresolved: The technical constraint of the current validation pipeline restricts the benchmark to relatively simple sentences, leaving the evaluation of complex, multi-faceted semantic reasoning unexplored.
- What evidence would resolve it: Improvements in the LLM-based generation and validation pipeline that successfully process larger subgraphs, followed by a benchmark analysis on these more complex data points.

## Limitations
- Closed-world assumption in KGs may not capture incomplete or evolving knowledge, penalizing models for inferring valid but absent relationships
- Validation pipeline reliability degrades significantly for subgraphs exceeding 12 triples, limiting the complexity of statements that can be generated
- Edge-replacement mappings require manual domain expertise and may not generalize across different knowledge domains

## Confidence
- **High Confidence:** The framework's core mechanism (KG perturbation → statement generation → validation) works as described, with 99% manual validation accuracy supporting the reconstruction validation approach. The observation that different methods excel at different perturbation types is well-supported by statistical analysis (p<0.05 for interaction effects).
- **Medium Confidence:** The domain-specific performance variations are convincing, though the relatively small number of test samples per perturbation type (17-67) limits generalizability. The claim that LLMs outperform traditional metrics on edge-based perturbations is supported but requires larger sample sizes for robust generalization.
- **Low Confidence:** The framework's ability to detect incomplete or evolving knowledge is limited by the closed-world assumption, but the extent of this limitation in real-world applications remains unclear without testing on dynamic KGs.

## Next Checks
1. Test framework performance on subgraphs with 12+ triples to quantify validation failure rates and determine if this is a fundamental limitation or can be mitigated through statement simplification.
2. Implement and evaluate more complex perturbation types (e.g., noise addition, irrelevant information insertion) to assess whether the four current types capture the full space of semantic variations.
3. Cross-domain threshold generalization test: Train a threshold on one domain's validation split and evaluate on another domain's test split to quantify domain transfer capabilities.