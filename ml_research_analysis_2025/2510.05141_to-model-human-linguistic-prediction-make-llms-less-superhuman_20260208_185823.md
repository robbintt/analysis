---
ver: rpa2
title: To model human linguistic prediction, make LLMs less superhuman
arxiv_id: '2510.05141'
source_url: https://arxiv.org/abs/2510.05141
tags:
- language
- llms
- human
- words
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models have become better at predicting upcoming\
  \ words, but paradoxically worse at modeling human reading behavior. This is because\
  \ they treat words as more predictable than humans do, due to their superior memory\u2014\
  both long-term (memorizing facts and training examples) and short-term (recalling\
  \ earlier words without decay)."
---

# To model human linguistic prediction, make LLMs less superhuman

## Quick Facts
- arXiv ID: 2510.05141
- Source URL: https://arxiv.org/abs/2510.05141
- Authors: Byung-Doh Oh; Tal Linzen
- Reference count: 13
- Key outcome: LLMs predict words better than humans but correlate worse with human reading times due to superhuman memory capabilities.

## Executive Summary
Large language models have become better at predicting upcoming words, but paradoxically worse at modeling human reading behavior. This is because they treat words as more predictable than humans do, due to their superior memory—both long-term (memorizing facts and training examples) and short-term (recalling earlier words without decay). This makes them "superhuman" and misaligned with human processing. The authors argue that for language models to better predict human reading, they need to be made less superhuman by limiting their training data and improving memory decay mechanisms. Current human data is insufficient to fully benchmark progress; more targeted experiments are needed.

## Method Summary
The paper synthesizes existing research on LLM-human reading time alignment, identifying mechanisms of misalignment (superhuman long-term and short-term memory) and proposing interventions (data reduction, attention decay, semantic diffusion). The analysis relies on evaluating pre-trained Transformer checkpoints against standard reading time corpora, extracting word-by-word surprisal estimates and correlating them with human reading durations using linear mixed-effects models. Key interventions include position-based or frequency-based attention decay, training on human-scale data (~100M words), and alternative training objectives that distribute probability mass across semantically similar words.

## Key Results
- Larger/better-performing LLMs show decreased correlation with human reading times despite improved perplexity
- Current LLMs have superhuman long-term memory (memorizing facts and training examples) and short-term memory (decay-free access to context)
- Interventions like attention decay and data reduction show promise for improving cognitive alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing training data volume may improve LLM fit to human reading behavior by limiting memorization of factual knowledge.
- Mechanism: Models trained on trillions of words encode world knowledge that typical human readers lack. When encountering proper nouns or rare multiword expressions, LLMs assign higher probabilities than humans would, lowering surprisal estimates below human-observable levels.
- Core assumption: Human-scale exposure (~100 million words by age 12) constrains long-term retention; reducing training data will proportionally reduce superhuman factual prediction.
- Evidence anchors:
  - [abstract] "LLMs are able to predict upcoming words much better than people can... driven by... much stronger long-term memory for facts and training examples"
  - [section 4] "language models trained on smaller amounts of data than typical mainstream LLMs yield probabilities that are better aligned to human reading times (Oh and Schuler, 2023a)"
  - [corpus] Weak direct evidence; neighbor paper "Towards Data-Efficient Language Models" aligns conceptually but doesn't validate cognitive alignment.
- Break condition: If reduced-data models still show superior factual prediction via better generalization rather than memorization, the mechanism is insufficient.

### Mechanism 2
- Claim: Adding memory decay to attention mechanisms improves alignment with human reading times.
- Mechanism: Standard Transformers have decay-free access to full context windows. Humans experience working memory decay and interference. Constraining attention (upweighting recent words, downweighting high-frequency function words) forces models to rely on compressed representations, producing more human-like surprisal.
- Core assumption: Temporal decay in attention approximates human working memory limitations causally.
- Evidence anchors:
  - [section 6] "downweighting the contribution of high-frequency words... reduces the mismatch between LLMs and humans in the prediction of the main verb (Hahn et al., 2022)"
  - [section 7] "alignment between LLM probabilities and human reading times improves when the LLM's ability to access earlier words in the sentence is reduced (de Varda and Marelli, 2024; Clark et al., 2025)"
  - [corpus] No direct corpus validation of decay mechanisms.
- Break condition: If decay implementations improve reading-time fit but degrade linguistic competence beyond human levels, the decay function is misspecified.

### Mechanism 3
- Claim: Alternative training objectives that distribute probability mass across semantically similar words produce more human-like prediction uncertainty.
- Mechanism: Standard next-token prediction rewards lexically sharp predictions (high probability on exact target). Humans without specific knowledge predict semantically plausible alternatives. Continuous loss functions based on embedding distance can produce diffuse predictions aligned with human semantic expectations.
- Core assumption: Human prediction errors cluster semantically rather than randomly; modeling this distribution improves cognitive fit.
- Evidence anchors:
  - [section 9] "humans make broader predictions based on meaning... a reader who does not know the birthplace of Elvis is likely to find the name of a plausible yet incorrect city to be somewhat predictable"
  - [section 9] "alternative training objectives that upweight semantically similar words together could be helpful (Kumar and Tsvetkov, 2019)"
  - [corpus] No corpus papers validate semantic diffusion in training objectives for cognitive modeling.
- Break condition: If diffuse predictions improve reading-time fit but reduce next-token accuracy below human-level baseline, the semantic weighting is over-regularized.

## Foundational Learning

- Concept: Surprisal theory (Hale, 2001; Levy, 2008)
  - Why needed here: The paper assumes reading time ≈ -log(P(word|context)). Understanding this link is prerequisite to interpreting why LLM surprisal misestimates human processing.
  - Quick check question: Given P("cassowary" | "I purchased a") = 0.001, what surprisal value would predict maximum reading slowdown?

- Concept: Working memory constraints in sentence processing (Lewis & Vasishth, 2005)
  - Why needed here: The short-term memory argument depends on understanding memory interference, decay, and retrieval bottlenecks in human parsing.
  - Quick check question: Why do center-embedded sentences ("The rat the cat the dog chased loved ate the malt") cause more difficulty than right-branching equivalents?

- Concept: Cloze probability vs. conditional probability from LMs
  - Why needed here: The paper contrasts human cloze estimates with LM probabilities; understanding the limitations of each is essential.
  - Quick check question: Why would cloze tasks underestimate human predictability for low-frequency but contextually appropriate words?

## Architecture Onboarding

- Component map:
  Transformer attention -> full context access (target for decay modification)
  Training corpus -> factual knowledge source (target for data reduction)
  Next-token loss -> sharp prediction objective (target for semantic diffusion)
  Model editing -> selective knowledge removal (alternative intervention)

- Critical path:
  1. Establish baseline reading-time correlation using standard LLM surprisal
  2. Implement attention decay (position-based or frequency-based)
  3. Validate on targeted memory probes (repeated text, center-embedding, agreement attraction)
  4. Compare against human-scale training data conditions

- Design tradeoffs:
  - Data reduction -> lower factual knowledge but potentially weaker generalization
  - Attention decay -> better cognitive fit but potential loss of long-distance dependencies
  - Semantic diffusion -> more human-like uncertainty but reduced perplexity

- Failure signatures:
  - Reading-time fit improves but grammatical competence drops below human baseline
  - Decay-only interventions improve some constructions but worsen others
  - No convergence between reduced-data and decay-modified models

- First 3 experiments:
  1. Replicate the position-based attention decay intervention (de Varda & Marelli, 2024) on a standard benchmark (e.g., Dundee, Natural Stories); report both reading-time fit and syntactic generalization (BLiMP).
  2. Train a small Transformer on 100M-word BabyLM corpus; compare surprisal-reading correlations against full-scale model on proper-noun-heavy passages.
  3. Construct targeted probe: repeated text passages with varying interleaved distances; compare human and model surprisal to quantify decay function shape.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will limiting LLM training data to human-scale amounts (e.g., 100 million words) sufficiently resolve the misalignment with human reading behavior, or is a specific mixture of data modalities required?
- Basis in paper: [explicit] The authors note that while human-scale training seems promising, "it remains to be seen whether it will address the superhuman long-term and short-term memory issues," and ask what the "correct mixture of training data" is.
- Why unresolved: It is unclear if simply reducing data volume prevents the memorization of facts and multiword expressions without also hindering the development of robust linguistic generalizations necessary for modeling.
- What evidence would resolve it: Evaluating models trained on the BabyLM Challenge datasets (limited to ~100M words) specifically on the targeted long-term and short-term memory benchmarks proposed in the paper.

### Open Question 2
- Question: Can implementing memory decay at the level of syntactic constituents or retrieval bottlenecks better align Transformer predictions with human working memory limitations?
- Basis in paper: [explicit] The authors suggest current decay methods make "simplifying assumptions" (decay by time or frequency) and explicitly recommend experiments that "evaluate the impact of more flexible methods... including implementing decay at the level of syntactic constituents."
- Why unresolved: The exact mechanisms of human memory interference (e.g., similarity-based interference) are complex, and it is unknown if architectural constraints forcing the model to forget specific syntactic units will improve fit more than simple recency bias.
- What evidence would resolve it: Comparing the fit to human reading times between standard Transformers and those modified with syntactically-aware memory decay or retrieval bottlenecks on complex sentences with multiple candidate subjects.

### Open Question 3
- Question: To what extent does a reader's specific factual knowledge and text familiarity quantitatively influence reading times compared to general linguistic prediction?
- Basis in paper: [explicit] The authors argue that "a more direct link between the readers' knowledge of text and their reading behavior needs to be established" and outline experiments to measure this.
- Why unresolved: Current naturalistic reading datasets do not separate processing difficulty caused by structural prediction from difficulty caused by a lack of specific background knowledge (e.g., the birthplace of Elvis Presley).
- What evidence would resolve it: Targeted human experiments that couple pre-experiment questionnaires gauging specific knowledge with reading time measurements on sentences containing that information.

## Limitations
- Most cited studies test individual interventions rather than the comprehensive framework
- Core claim about reduced training data relies on single study without independent replication
- Attention decay mechanisms lack systematic comparison across different decay functions and rates

## Confidence
- High confidence: The observation that larger/better-performing LLMs show decreased correlation with human reading times is well-established across multiple studies (Shain et al., 2024; Oh and Schuler, 2023a).
- Medium confidence: The mechanisms linking long-term memory and short-term memory to the misalignment are plausible and supported by partial evidence, but comprehensive validation is lacking.
- Low confidence: The specific interventions (data reduction, attention decay, semantic diffusion) are proposed based on theoretical grounds with minimal empirical validation for cognitive alignment.

## Next Checks
1. Replicate the decay mechanism: Implement the position-based attention decay from de Varda and Marelli (2024) on a standard benchmark (e.g., Natural Stories) and measure both reading-time fit improvement and syntactic generalization performance on BLiMP.
2. Compare data regimes systematically: Train multiple models across a spectrum of training data sizes (from BabyLM scale to full-scale) and measure the relationship between training corpus size, factual prediction accuracy, and reading-time correlation on proper-noun-heavy passages.
3. Test semantic diffusion empirically: Implement a continuous training objective that distributes probability mass across semantically similar words (e.g., using Kumar and Tsvetkov's 2019 approach) and compare its surprisal-reading-time correlation against standard next-token prediction on controlled semantic prediction tasks.