---
ver: rpa2
title: Fairness Dynamics During Training
arxiv_id: '2506.01709'
source_url: https://arxiv.org/abs/2506.01709
tags:
- fairness
- male
- training
- jsd-p
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces two new metrics\u2014Average Rank and Jensen-Shannon\
  \ Divergence by Parts\u2014to evaluate fairness dynamics during LLM training. These\
  \ metrics track how models' biases evolve over time, capturing both performance\
  \ and confidence in predictions."
---

# Fairness Dynamics During Training

## Quick Facts
- arXiv ID: 2506.01709
- Source URL: https://arxiv.org/abs/2506.01709
- Reference count: 29
- Key result: Introduces Average Rank and JSD-P metrics to track bias evolution; finds Pythia-6.9b can achieve 92.5% fairness gain via early stopping at 1.7% accuracy cost

## Executive Summary
This paper introduces two novel metrics—Average Rank and Jensen-Shannon Divergence by Parts—to evaluate how bias evolves during LLM training. Applied to Pythia models on WinoBias gender prediction, these metrics reveal that larger models like Pythia-6.9b exhibit more gender assumptions in neutral contexts and that bias does not always align with standard performance metrics. The authors demonstrate that early stopping at ~80k training steps can significantly improve fairness (92.5% gain) with minimal performance degradation (1.7% accuracy loss), suggesting training interventions as a viable bias mitigation strategy.

## Method Summary
The authors track bias evolution during training using WinoBias Type 2 samples converted into gender-disambiguated and gender-ambiguous prompts with three answer options (male, female, not specified). They evaluate Pythia model checkpoints across training steps using two metrics: Average Rank (mean rank of correct answer token) and Jensen-Shannon Divergence by Parts (per-option divergence from ideal one-hot distribution). Statistical significance is assessed via Mann-Whitney U Test (p < 0.01). The methodology identifies inflection points where fairness and performance dynamics diverge, enabling early stopping interventions.

## Key Results
- Pythia-6.9b shows increasing gender bias during training, making more unwarranted gender assumptions than Pythia-160m
- JSD-P captures both bias direction and confidence magnitude, overcoming limitations of all-or-nothing fairness metrics
- Early stopping at ~80k steps achieves 92.5% fairness improvement with only 1.7% accuracy loss on LAMBADA
- Larger models exhibit more gender bias in neutral contexts compared to smaller models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early stopping during pre-training can yield substantially fairer models with minimal performance degradation.
- Mechanism: Fairness and performance dynamics diverge after ~80k training steps in Pythia-6.9b. Halting training at this inflection point retains most capability (1.7% LAMBADA accuracy loss) while achieving 92.5% fairness improvement.
- Core assumption: The fairness-performance tradeoff curve observed in Pythia generalizes to other LLMs and training configurations.
- Evidence anchors: Abstract states 92.5% fairness increase via 1.7% accuracy trade-off; Section 2 confirms early stopping at ~80k steps. Weak corpus support for early stopping as fairness mechanism.
- Break condition: If bias emerges primarily during fine-tuning rather than pre-training, or if task-specific performance degrades sharply before fairness inflection point.

### Mechanism 2
- Claim: Per-token JSD-P captures both bias direction and confidence magnitude, overcoming all-or-nothing fairness metrics.
- Mechanism: Standard metrics treat {0.33, 0.34, 0.32} and {0.03, 0.95, 0.02} as equally biased. JSD-P computes divergence per answer option against ideal one-hot, revealing Pythia-6.9b places more probability mass on "male" than "female" with higher confidence over training.
- Core assumption: Next-token probabilities for constrained option sets meaningfully represent model bias in open-ended generation contexts.
- Evidence anchors: Abstract notes biases emerge suddenly and don't follow performance metrics; Section 2 describes JSD-P overcoming all-or-nothing limitations. Corpus papers address bias detection but don't validate JSD-P specifically.
- Break condition: If sampling strategies or generation parameters substantially alter probability distributions during deployment, JSD-P may not predict real-world bias.

### Mechanism 3
- Claim: Larger models can exhibit greater bias in gender-ambiguous contexts than smaller models.
- Mechanism: When correct answer is "not specified," Pythia-6.9b assigns more probability to gendered options (favoring "male") than Pythia-160m. Authors interpret this as larger models making more unwarranted gender assumptions.
- Core assumption: Behavior between 160m and 6.9b parameter scales generalizes across model families and training data compositions.
- Evidence anchors: Abstract states larger models exhibit more bias; Section 2 shows Pythia-6.9b more likely to incorrectly pick gendered answers in neutral contexts. Weak corpus support for scale effects on bias.
- Break condition: If training data curation (not scale) drives this effect, or if effect reverses at even larger scales.

## Foundational Learning

- Concept: Jensen-Shannon Divergence
  - Why needed here: JSD-P is the core fairness metric; understanding how JSD measures similarity between probability distributions is essential to interpret results.
  - Quick check question: Given two distributions P=[1,0,0] and Q=[0.5,0.25,0.25], which has higher divergence from a uniform reference?

- Concept: Next-token probability in autoregressive LLMs
  - Why needed here: Both AR and JSD-P operate on next-token probabilities; understanding how models assign probability across vocabulary is foundational.
  - Quick check question: How does softmax temperature affect the sharpness of next-token probability distributions?

- Concept: Early stopping as regularization
  - Why needed here: The paper proposes early stopping as a fairness intervention; understanding its conventional role helps evaluate this novel application.
  - Quick check question: What validation signal is typically used to determine early stopping points, and how does this paper's approach differ?

## Architecture Onboarding

- Component map:
  Prompt generator -> Inference layer -> AR calculator -> JSD-P calculator -> Checkpoint evaluator

- Critical path:
  1. Define constrained option vocabulary (e.g., {male, female, not})
  2. For each checkpoint, extract token probabilities for all options
  3. Compute AR per answer type to track performance disparities
  4. Compute JSD-P per answer type to track confidence/bias magnitude
  5. Statistical testing (Mann-Whitney U) to confirm significance of disparities

- Design tradeoffs:
  - Constrained vs. open-ended evaluation: Constrained enables precise metric computation but may not reflect generation-time behavior
  - Checkpoint frequency: More checkpoints improve inflection detection but increase compute cost
  - Single-axis vs. multi-axis fairness: Paper evaluates binary gender only; early stopping on one axis may affect others unpredictably

- Failure signatures:
  - Flat JSD-P across training suggests options not in vocabulary or probability extraction errors
  - AR improves for all options equally without divergence may indicate prompt design issues
  - High variance across random seeds (option ordering) suggests model sensitivity to prompt format

- First 3 experiments:
  1. Replicate AR/JSD-P tracking on Pythia-160m to validate metric behavior on smaller scale and compare against reported 6.9b results.
  2. Extend evaluation to a different bias axis (e.g., race, age) using adapted prompts to test whether early stopping points generalize.
  3. Ablate prompt format by removing "not specified" option to measure how constraint design affects detected bias magnitude.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed fairness dynamics and early stopping benefits generalize to model architectures and training datasets other than the Pythia suite?
- Basis in paper: [explicit] The authors note the evaluation is "limited to... the Pythia model family" and that these research models may not reflect "how models trained for deployment behave."
- Why unresolved: The study restricts its methodology to one specific model suite, leaving the applicability of Average Rank and JSD-P metrics to other LLMs unconfirmed.
- What evidence would resolve it: Applying the proposed metrics to diverse architectures (e.g., Llama, GPT) and proprietary production models during pre-training.

### Open Question 2
- Question: Does early stopping optimized for gender fairness inadvertently introduce or amplify bias on other social axes (e.g., race, religion, age)?
- Basis in paper: [explicit] The authors warn that "early stopping at the point identified may have some unintended consequences on other axes of bias."
- Why unresolved: The experimental design focused exclusively on binary gender bias via WinoBias, lacking data on fairness trade-offs across different demographic dimensions.
- What evidence would resolve it: A multi-dimensional fairness evaluation measuring bias across various protected attributes at the identified early stopping checkpoints.

### Open Question 3
- Question: Does the intervention of early stopping remove the underlying causal mechanisms of bias, or does it merely suppress the model's ability to express bias?
- Basis in paper: [inferred] The paper states "early stopping simply works around bias, instead of truly mitigating it," suggesting the bias might persist in latent representations.
- Why unresolved: The metrics rely on output probabilities (next-token predictions) rather than probing internal model states or representations.
- What evidence would resolve it: Probing classifier experiments on internal embeddings at early stopping checkpoints to test for the presence of biased associations.

## Limitations

- The study examines only binary gender bias, leaving unaddressed whether findings generalize to intersectional or non-binary gender dimensions or other bias axes.
- Metrics rely on constrained next-token prediction tasks rather than open-ended generation, potentially missing real-world bias patterns.
- Early stopping benefits are demonstrated only for Pythia-6.9b on WinoBias, without validation on alternative model architectures, training regimes, or datasets.

## Confidence

- High confidence: The validity of Average Rank and JSD-P metrics as measures of bias direction and magnitude, with clear mathematical formulations and appropriate statistical testing.
- Medium confidence: The claim that larger models exhibit more gender assumptions in neutral contexts, as generalization to other model families and scales is not established.
- Medium confidence: The early stopping mechanism as a generalizable fairness intervention, as the inflection point and tradeoff ratio may not transfer to other models or tasks.
- Low confidence: The broader claim that fairness dynamics don't align with standard performance metrics during training, as this pattern requires validation across diverse model families and tasks.

## Next Checks

1. Apply AR and JSD-P metrics to a different LLM family (e.g., LLaMA, GPT-Neo) trained on different data to determine whether early stopping inflection point and bias scale effects generalize beyond Pythia.

2. Adapt the WinoBias framework to test bias along a different protected attribute (e.g., racial or age bias in occupation prediction) to assess whether early stopping on gender bias affects or creates biases in other dimensions.

3. Implement the same metrics on open-ended generation tasks (e.g., occupation completion prompts without constrained options) to verify that constrained-prompt bias patterns detected by AR and JSD-P translate to real-world generation behavior.