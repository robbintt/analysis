---
ver: rpa2
title: Estimating the Effects of Sample Training Orders for Large Language Models
  without Retraining
arxiv_id: '2505.22042'
source_url: https://arxiv.org/abs/2505.22042
tags:
- training
- performance
- sample
- framework
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a retraining-free framework to estimate how
  training sample order affects large language model (LLM) performance. The core idea
  is to approximate Adam optimizer updates using first- and second-order Taylor expansions,
  and store these update terms efficiently using random projection.
---

# Estimating the Effects of Sample Training Orders for Large Language Models without Retraining

## Quick Facts
- arXiv ID: 2505.22042
- Source URL: https://arxiv.org/abs/2505.22042
- Reference count: 40
- Key outcome: A retraining-free framework estimates how training sample order affects LLM performance by approximating Adam updates using Taylor expansions and random projection storage

## Executive Summary
This paper introduces a retraining-free framework to estimate how training sample order affects large language model (LLM) performance. The core idea is to approximate Adam optimizer updates using first- and second-order Taylor expansions, and store these update terms efficiently using random projection. This allows estimation of model parameters for arbitrary training orders without retraining. The framework is applied to two problems: training curriculum design and analysis of memorization and generalization effects. Experiments on a 636M parameter LLM show that the framework accurately estimates true performance (with AbsDiff of 0.0085-0.0917 depending on batch size) and is up to 132.6× faster than retraining. For curriculum design, the proposed method outperforms baselines by 0.04-0.06 perplexity points. For memorization and generalization analysis, the framework captures key trends like better retention for later-training samples and improved generalization for similar test data. The method scales to larger models (up to 1.4B parameters) and provides insights into training dynamics without the prohibitive cost of repeated retraining.

## Method Summary
The framework estimates LLM parameters under arbitrary training orders without retraining by approximating Adam optimizer updates. It uses Taylor expansion to approximate the update function Γ(θ, B) around reference checkpoints, storing these terms with random projection to reduce memory. The process involves three stages: (1) train a reference order to obtain checkpoints {θt}, (2) compute and store Γ(θt, Blt) and its gradients for all checkpoint-batch pairs with random projection compression, (3) estimate new order via recursive Taylor expansion. The method achieves up to 132.6× speedup over retraining while maintaining AbsDiff values between 0.0085-0.0917 across different batch sizes.

## Key Results
- Framework accurately estimates true performance with AbsDiff of 0.0085-0.0917 depending on batch size
- Up to 132.6× faster than retraining for estimating different training orders
- Outperforms baselines by 0.04-0.06 perplexity points in curriculum design tasks
- Captures key trends in memorization (better retention for later-training samples) and generalization (improved generalization for similar test data)
- Scales to larger models up to 1.4B parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Model parameters under an arbitrary sample order can be estimated by approximating the Adam optimizer update function using Taylor expansion.
- **Mechanism:** The framework treats the Adam update term Γ(θ, B) as a function of model parameters θ. Instead of recalculating the exact update for a new trajectory, it uses a first- or second-order Taylor expansion around the reference checkpoints θt. This allows the system to predict the update direction for a batch Blt (the batch at position t in the new order) using the gradient and Hessian information precomputed at the reference checkpoint θt.
- **Core assumption:** The update dynamics of the Adam optimizer are locally smooth enough that a low-order polynomial expansion can sufficiently approximate the parameter shift between the reference trajectory and the permuted trajectory.
- **Evidence anchors:**
  - [Abstract]: "...approximating Adam optimizer updates using first- and second-order Taylor expansions..."
  - [Section 3]: "By using Taylor expansions on Γ(γt, Blt), we have: Γ(γt, Blt) ≈ Γ(θt, Blt) + (γt - θt)∇θΓ(θt, Blt)"
  - [Corpus]: Weak link; neighbors focus on "retraining-free" for pruning or quantization (e.g., PERP, Dropping Experts), not trajectory estimation via Taylor expansion.
- **Break condition:** The approximation degrades if the learning rate is too high or the loss landscape is highly non-convex, causing the Taylor expansion to fail to predict the optimizer's step accurately.

### Mechanism 2
- **Claim:** High-dimensional update terms can be stored efficiently without losing critical geometric information by using Random Projection.
- **Mechanism:** The terms required for Taylor expansion (Γ and ∇Γ) scale with model size (e.g., 636M parameters). The authors apply the Johnson-Lindenstrauss (JL) lemma, projecting these vectors into a much lower dimension k using a random Gaussian matrix. This preserves the pairwise distances and dot products necessary for the expansion calculation while reducing space complexity from O(d1d2) to O(d1k).
- **Core assumption:** The dimensionality reduction preserves the "direction" and relative magnitude of the update vectors well enough that the error introduced does not collapse the Taylor expansion approximation.
- **Evidence anchors:**
  - [Abstract]: "...employs random projection to efficiently store intermediate update terms."
  - [Section A.2]: "This projection reduces the space complexity from O(d1d2) to O(d1k) while approximately preserving the geometric structure..."
  - [Corpus]: No direct evidence in neighbors for this specific storage technique applied to sample ordering.
- **Break condition:** If the projection dimension k is set too low (over-compression), vector orthogonality is lost, leading to noisy gradient estimates and parameter divergence.

### Mechanism 3
- **Claim:** The full training trajectory for a permuted order can be reconstructed recursively from a single reference run.
- **Mechanism:** The process starts from the shared initial point θ0. For a new order l, at every step t, the framework retrieves the compressed update terms corresponding to batch Blt and checkpoint θt. It then updates the estimated parameter γt using the approximated gradient step. By repeating this for all T steps, it synthesizes the final checkpoint γT.
- **Core assumption:** The momentum statistics (mt, vt) computed during the reference run provide a sufficient basis for calculating updates in the permuted order (or that the Taylor expansion adequately corrects for the momentum discrepancy).
- **Evidence anchors:**
  - [Section 3]: "...if we can obtain Γ(θt, Blt) and ∇θΓ(θt, Blt) for all 0 ≤ t ≤ T-1, then γt+1 can be recursively computed..."
  - [Figure 1]: Visualizes "Stage 3: Estimating" where parameters are updated along a "Permuted Order k" using stored terms.
  - [Corpus]: Indirect support from "Dropping Experts" and "PERP" which validate "retraining-free" concepts generally, though not this recursive method.
- **Break condition:** Error accumulation; small approximation errors at early steps compound, causing the estimated trajectory γ to drift far from the true trajectory over long training runs (large T).

## Foundational Learning

- **Concept: Taylor Expansion (First/Second Order)**
  - **Why needed here:** This is the mathematical engine allowing the estimation of a function's value (the update step) at a new point based on its derivative information at a known point.
  - **Quick check question:** Can you explain why a second-order expansion (including the Hessian) might be more accurate but computationally harder than a first-order expansion?

- **Concept: Adam Optimizer Mechanics**
  - **Why needed here:** The method specifically approximates the *Adam* update rule (mt, vt). Understanding how momentum and variance scaling affect the update step is crucial to understanding what the Taylor expansion is approximating.
  - **Quick check question:** How does Adam differ from vanilla SGD, and does the paper approximate the gradients before or after Adam's scaling?

- **Concept: Random Projection (Johnson-Lindenstrauss Lemma)**
  - **Why needed here:** This justifies the memory efficiency of the method, allowing billion-parameter vectors to be compressed without losing the geometric properties needed for calculation.
  - **Quick check question:** If you project a vector into a random lower-dimensional space, what specific mathematical property does the JL lemma guarantee is preserved?

## Architecture Onboarding

- **Component map:** Reference Trainer -> Update Cacher -> Trajectory Estimator
- **Critical path:** The **Update Cacher** is the most expensive stage. It requires computing gradients for T² pairs (every batch against every checkpoint). While avoiding full retraining, this is still O(T²·C).
- **Design tradeoffs:**
  - **Projection Dimension (k):** Lower k saves memory but increases AbsDiff (estimation error).
  - **Expansion Order:** First-order is faster/computationally cheaper; Second-order (FUT++) is more accurate but requires Hessian approximations (or difference-of-gradients).
- **Failure signatures:**
  - **Divergence:** Estimated perplexity goes to infinity → usually implies the Taylor expansion step size is too aggressive or projection noise is too high.
  - **Stagnation:** Estimated performance does not change across different permutations → implies the update terms are over-compressed or the learning signal is lost.
- **First 3 experiments:**
  1. **Overfit Sanity Check:** Run FUT on a very small dataset/batch count (T=8) and verify that the "estimated" trajectory exactly matches the "retraining" trajectory when the order is identical to the reference order.
  2. **Projection Sweep:** Measure AbsDiff vs. Memory Usage by varying the projection dimension k to find the "knee" of the curve.
  3. **Curriculum Validation:** Pick a simple curriculum heuristic (e.g., "shortest to longest") and use FUT to estimate the final perplexity. Then actually retrain to validate if the estimate held.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the FUT framework's performance estimates, validated primarily on perplexity, accurately predict model performance on downstream natural language understanding and reasoning tasks?
- **Basis in paper:** [explicit] The authors state in the Limitations (Section F) that they evaluate effectiveness "solely based on perplexity performance" and that "further validation is needed to assess the generalizability of our framework in these more complex tasks."
- **Why unresolved:** Perplexity is a proxy for language modeling quality but does not perfectly correlate with reasoning or specific NLU capabilities, which may have different sensitivities to sample ordering.
- **What evidence would resolve it:** A correlation analysis between FUT-estimated perplexity for various sample orders and the resulting accuracy on standard benchmarks (e.g., MMLU, HellaSwag) without retraining.

### Open Question 2
- **Question:** How does estimation error accumulate when applying FUT to training regimes dominated by higher-order optimizer dynamics or multi-epoch training?
- **Basis in paper:** [explicit] Section F notes that "accuracy... relies on the validity of Taylor expansions, particularly when higher-order nonlinearities dominate... our first- and second-order approximations may fall short."
- **Why unresolved:** The current experiments are restricted to a single epoch; over multiple passes or complex loss landscapes, the linear/quadratic approximations of the Adam update term may drift significantly from the true optimization path.
- **What evidence would resolve it:** Experiments tracking the divergence (AbsDiff) between FUT estimates and true retraining parameters over multiple epochs or in high-learning-rate scenarios.

### Open Question 3
- **Question:** To what extent does random projection introduce noise that degrades estimation accuracy in models with significantly larger parameter spaces (>>1.4B parameters)?
- **Basis in paper:** [explicit] The authors acknowledge in Section F that while random projection reduces memory, it "may introduce approximation noise, especially for models with extremely large parameter spaces."
- **Why unresolved:** The Johnson-Lindenstrauss lemma provides theoretical bounds, but the empirical trade-off between aggressive compression (storage efficiency) and gradient fidelity in multi-billion parameter models remains untested.
- **What evidence would resolve it:** Ablation studies on 7B+ models measuring the signal-to-noise ratio of recovered update terms ∇Γ against varying projection dimensions k.

## Limitations

- Accuracy depends on the smoothness of the loss landscape and sufficiency of low-order Taylor approximations, with no clear per-setting baseline comparisons
- Assumes momentum statistics from reference trajectory provide sufficient basis for arbitrary orders, which may break down with substantially different batch orders
- Quadratic complexity of caching update terms (O(T²·C)) remains computationally expensive despite avoiding full retraining
- Random projection dimension k is chosen from ranges rather than optimized per configuration, potentially leaving accuracy-memory tradeoffs unexplored
- Claims about capturing "key trends" in memorization and generalization lack rigorous statistical validation across multiple runs

## Confidence

- **High Confidence:** The core mathematical framework (Taylor expansion + random projection) is sound and well-specified. The mechanism for efficient storage and estimation is clearly articulated.
- **Medium Confidence:** The experimental validation shows the method works on the tested 636M parameter model and specific curriculum design tasks, but generalizability to other model architectures, datasets, and more complex curriculum strategies remains unproven.
- **Low Confidence:** Claims about capturing "key trends" in memorization and generalization are supported by qualitative observations but lack rigorous statistical validation across multiple runs or ablation studies on the Taylor expansion order's impact.

## Next Checks

1. **Cross-Dataset Generalization:** Apply FUT to a completely different dataset (e.g., C4 or OpenWebText) and compare accuracy degradation against the WikiText-103 baseline to assess robustness.

2. **Curriculum Design Benchmark:** Implement a comprehensive curriculum design experiment comparing FUT against multiple established heuristics (e.g., difficulty-based, diversity-based) with statistical significance testing across 20+ random curriculum orders.

3. **Error Accumulation Analysis:** Systematically measure how AbsDiff grows with training length T by running experiments with T ∈ {8, 16, 32, 64, 128, 256} on the same model, plotting error vs. step count to quantify the drift rate and determine practical limits.