---
ver: rpa2
title: 'Large Language Models in Legislative Content Analysis: A Dataset from the
  Polish Parliament'
arxiv_id: '2503.12100'
source_url: https://arxiv.org/abs/2503.12100
tags:
- language
- polish
- dataset
- legal
- legislative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a novel dataset from the Polish Parliament
  to evaluate large language models (LLMs) in legislative content analysis. Three
  tasks were formulated: PPC (multi-label classification of legislative documents),
  PPO (binary classification predicting if amendments are needed), and STP (summarization
  of legislative drafts).'
---

# Large Language Models in Legislative Content Analysis: A Dataset from the Polish Parliament

## Quick Facts
- arXiv ID: 2503.12100
- Source URL: https://arxiv.org/abs/2503.12100
- Reference count: 35
- This study introduces a novel dataset from the Polish Parliament to evaluate large language models (LLMs) in legislative content analysis, finding that domain-specific Polish language models outperform multilingual ones in legislative analysis tasks.

## Executive Summary
This study introduces a novel dataset from the Polish Parliament to evaluate large language models (LLMs) in legislative content analysis. Three tasks were formulated: PPC (multi-label classification of legislative documents), PPO (binary classification predicting if amendments are needed), and STP (summarization of legislative drafts). The dataset was collected from official Polish legislative sources. Experiments tested various language models, including HerBERT, RoBERTa, and GPT variants. HerBERT performed best in classification tasks, while T5 showed slight advantages in summarization. Results indicate that domain-specific Polish language models outperform multilingual ones in legislative analysis tasks, highlighting both the potential and challenges of using LLMs for legal document processing in Polish.

## Method Summary
The study collected Polish legislative data from the Sejm RP API and official websites spanning 2011-2025, creating three tasks: PPC (3,738 documents, multi-label classification into top 20 of 312 categories), PPO (2,372 documents, binary classification predicting Senate amendments), and STP (1,327 document-summary pairs). Models were fine-tuned using 5-fold cross-validation on Kaggle with dual P100 GPUs, using AdamW optimization (lr=2e-5, weight_decay=0.01) for 10 epochs. Classification models (HerBERT, PL-RoBERTa, RoBERTa, PL-GPT2, GPT2) were evaluated with F1-micro and ROC-AUC, while summarization models (t5-small, PLBart, bart-large) were evaluated with ROUGE metrics.

## Key Results
- HerBERT achieved F1-micro scores of ~0.86 and ROC-AUC of ~0.94 in PPC multi-label classification, outperforming other models
- PPO binary classification performed poorly with accuracy around 0.6, close to random guessing
- T5 showed slight advantages over BART in STP summarization with ROUGE-1 scores around 0.036, though overall results were unsatisfactory

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific pre-training improves performance on Polish legislative tasks compared to multilingual training.
- Mechanism: Models pre-trained on Polish corpora (HerBERT, PL-RoBERTa, PL-GPT2) develop better representations of Polish legal vocabulary and syntactic patterns, enabling more accurate classification boundaries than models trained on mixed-language or English-dominant corpora.
- Core assumption: The performance gap stems from linguistic representation quality rather than model scale or architecture differences.
- Evidence anchors:
  - [abstract]: "Results indicate that domain-specific Polish language models outperform multilingual ones in legislative analysis tasks"
  - [section 4.3]: "This outcome strongly suggests that adapting the base model to specific language processing is crucial for better modeling the word space by the language model"
  - [corpus]: Related work on Polish linguistic competency evaluation (arXiv:2503.00995) confirms language-specific evaluation importance, though limited direct comparison data available
- Break condition: If multilingual models with significantly larger parameter counts were tested, scale advantages might offset language-specific pre-training benefits.

### Mechanism 2
- Claim: Task complexity, defined by required contextual understanding, predicts model difficulty more than task type (classification vs. summarization).
- Mechanism: PPC relies on vocabulary-pattern matching (e.g., "doctor" → healthcare category), which models capture via token-level associations. PPO requires understanding legislative intent, implications, and contextual nuance—cognitive demands that exceed pattern recognition alone.
- Core assumption: The performance difference reflects task complexity rather than dataset quality or label ambiguity.
- Evidence anchors:
  - [section 5]: "In the text categorization multi-classification task in the PPC dataset, models achieved very high scores... In contrast, the models performed poorly in the binary classification task in the PPO dataset"
  - [section 5]: "To classify whether a legislative draft requires amendments, a model must understand specific words and the overall context, the intent behind proposed changes"
  - [corpus]: Related legal AI work (arXiv:2502.12193) shows similar context-understanding challenges in Polish legal classification
- Break condition: If PPO's low performance stems from noisy labels (Senate amendments don't guarantee adoption), the mechanism would reflect data quality rather than task complexity.

### Mechanism 3
- Claim: T5's text-to-text pre-training confers slight advantages in Polish legislative summarization over autoencoder-based architectures like BART.
- Mechanism: T5's unified text-to-text training objective optimizes for coherent generation across diverse NLP tasks, potentially improving fluency and structure alignment. BART's autoencoder approach prioritizes reconstruction, which may transfer less effectively to abstractive summarization of complex legal documents.
- Core assumption: ROUGE score differences reflect architectural suitability rather than hyperparameter tuning quality.
- Evidence anchors:
  - [section 4.5]: "The best-performing model was T5, suggesting that it is the most effective in generating summaries for legislative documents compared to the other tested models"
  - [section 5]: "One key factor contributing to T5's superior performance may be its training approach. T5 was trained on large-scale datasets in a manner that enhances coherence and fluency"
  - [corpus]: Insufficient corpus evidence for architectural comparison in legal summarization; neighboring papers focus on classification and retrieval
- Break condition: If computational constraints (limited context windows) affected BART more than T5, the advantage may reflect experimental conditions rather than architecture.

## Foundational Learning

- Concept: **Multi-label vs. binary classification in legal contexts**
  - Why needed here: PPC uses 312 labels reduced to top 20; understanding label imbalance and synonym handling (e.g., "environmental protection" vs. "nature conservation") is critical for interpreting F1-micro scores.
  - Quick check question: Can you explain why F1-micro is preferred over accuracy for imbalanced multi-label datasets?

- Concept: **ROUGE metrics for summarization evaluation**
  - Why needed here: STP evaluation relies entirely on ROUGE-1, ROUGE-2, ROUGE-L scores; understanding what these capture (n-gram overlap vs. sequence matching) is essential for interpreting the "unsatisfactory" results.
  - Quick check question: What does a ROUGE-1 score of 0.036 indicate about the relationship between generated and reference summaries?

- Concept: **Transfer learning from general to domain-specific corpora**
  - Why needed here: The paper's central claim rests on Polish-specific pre-training advantages; understanding what pre-training captures helps evaluate whether this mechanism generalizes.
  - Quick check question: Why might a model pre-trained on Polish Wikipedia and BookCorpus still struggle with Polish legal terminology?

## Architecture Onboarding

- Component map:
  Sejm RP API + Senate website scraping → raw legislative documents (bills, amendments, BAS analyses) → preprocessing pipeline (text extraction, label normalization, train/test splitting) → model layer (task-specific selection: BERT-family for classification, seq2seq for summarization) → evaluation layer (classification metrics, summarization metrics)

- Critical path:
  1. Dataset construction (PPC → PPO → STP) determines task feasibility
  2. Model selection (Polish-specific vs. multilingual) drives performance ceiling
  3. Fine-tuning configuration (10 epochs, lr=2e-5, max_tokens=512) constrains what models can learn
  4. Metric interpretation (statistical significance testing via paired t-tests) validates claims

- Design tradeoffs:
  - Full BAS summaries vs. key-points extraction: Paper chose full summaries for consistency across years, sacrificing conciseness for coverage
  - Top-20 labels vs. full 312-label taxonomy: Reduced complexity but lost granularity for niche legal categories
  - 512-token limit vs. full document context: Computational feasibility traded against understanding long-range legislative dependencies

- Failure signatures:
  - PPO near-random performance (Acc ~0.6): Indicates models fail to capture legislative intent; may require explicit reasoning mechanisms or external knowledge
  - STP low ROUGE scores (0.036): Suggests generated summaries lack lexical overlap with references; could indicate abstractive hallucination or format mismatch
  - High variance in PPO (std up to 0.123): Signals unstable learning; dataset may be too small or labels too noisy for reliable generalization

- First 3 experiments:
  1. Baseline replication: Run HerBERT on PPC with 5-fold cross-validation; verify F1-micro ~0.86 and statistical significance over RoBERTa using paired t-tests
  2. Ablation on context length: Test HerBERT on PPO with 512 vs. 1024 token limits to determine if context window constrains amendment prediction
  3. Error analysis on STP: Manually inspect 20 T5-generated summaries vs. BAS references to characterize whether low ROUGE reflects hallucination, compression, or format divergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does standardizing the Bureau of Research (BAS) summary formats improve the ROUGE scores for the STP summarization task?
- Basis in paper: [explicit] The authors state in Section 6 that the STP task requires standardization of analyses because summary formatting "varies significantly across years."
- Why unresolved: The current experiments yielded "unsatisfactory" results, partly attributed to the inconsistent formatting of the ground-truth summaries.
- What evidence would resolve it: Re-training and evaluating models on a filtered, structurally consistent subset of the STP dataset.

### Open Question 2
- Question: Can incorporating Sejm amendments and Presidential enactment decisions enhance the predictive accuracy of the PPO task?
- Basis in paper: [explicit] Section 6 proposes extending the PPO dataset to include these additional legislative decision points to broaden the context.
- Why unresolved: The binary PPO task (predicting Senate amendments) resulted in low accuracy (~0.6), suggesting the current data scope may be insufficient for capturing the complexity of legislative revisions.
- What evidence would resolve it: Performance comparison of models trained on the original PPO dataset versus the extended dataset with new input features.

### Open Question 3
- Question: How does merging synonymous categories affect the performance of multi-label classification in the PPC task?
- Basis in paper: [explicit] Section 6 lists "merging synonymous categories" as a specific future plan to reduce the number of labels in the PPC dataset.
- Why unresolved: The paper notes the presence of synonymy in labels (e.g., "nature protection" vs. "environmental protection"), but the impact of resolving this semantic overlap on model precision remains untested.
- What evidence would resolve it: A comparative analysis of F1 scores on the current label set versus a consolidated taxonomy where synonyms are merged.

## Limitations
- PPO task's near-random performance may reflect fundamental task difficulty rather than model inadequacy, as predicting Senate amendments requires understanding complex legislative dynamics
- 512-token limit constrains context modeling for long legislative documents where critical information may reside beyond this boundary
- Top-20 label reduction for PPC sacrifices granularity, potentially missing important legal categories

## Confidence

**High Confidence**: The finding that Polish-specific models (HerBERT, PL-RoBERTa) outperform multilingual counterparts in PPC classification is well-supported by experimental results and statistical significance testing.

**Medium Confidence**: The claim that task complexity predicts performance differences between PPC and PPO has reasonable support, but alternative explanations exist regarding dataset quality and label ambiguity.

**Low Confidence**: The assertion that domain-specific pre-training is crucial for legal document processing in Polish requires more evidence, as the study doesn't test larger multilingual models or systematically vary pre-training corpora.

## Next Checks
1. **Context Length Ablation**: Re-run PPO classification with varying context windows (256, 512, 1024 tokens) to determine if near-random performance stems from insufficient context rather than fundamental task difficulty.

2. **Label Quality Audit**: Manually examine 100 randomly selected PPO instances where models failed to predict amendments correctly, comparing Senate decisions with bill content to assess whether errors reflect model limitations or noisy/ambiguous labels.

3. **Generation Format Analysis**: Conduct human evaluation of 50 T5-generated STP summaries versus BAS references, coding them for format adherence, factual accuracy, and legal terminology usage to complement ROUGE scores.