---
ver: rpa2
title: Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation
arxiv_id: '2508.13745'
source_url: https://arxiv.org/abs/2508.13745
tags:
- multi-modal
- item
- learning
- graph
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes REARM, a multi-modal recommendation framework
  that addresses data sparsity by refining contrastive learning and homography relations.
  The key innovation is a meta-network to denoise modal-shared features and orthogonal
  constraints to retain modal-unique information, coupled with exploration of user
  interest and item co-occurrence graphs.
---

# Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation

## Quick Facts
- **arXiv ID:** 2508.13745
- **Source URL:** https://arxiv.org/abs/2508.13745
- **Reference count:** 40
- **Key outcome:** REARM achieves R@20 improvements of 0.0479 (Baby), 0.0553 (Sports), and 0.0454 (Clothing) compared to the best baseline on Amazon datasets.

## Executive Summary
This paper proposes REARM, a multi-modal recommendation framework that addresses data sparsity by refining contrastive learning and homography relations. The key innovation is a meta-network to denoise modal-shared features and orthogonal constraints to retain modal-unique information, coupled with exploration of user interest and item co-occurrence graphs. Experiments on three Amazon datasets show significant performance gains over state-of-the-art methods, demonstrating the effectiveness of preserving both modal-shared and modal-unique recommendation-relevant information.

## Method Summary
REARM integrates homography and heterography learning to capture multi-modal relationships. Homography learning constructs four homogeneous graphs (user co-occurrence, user interest, item co-occurrence, item semantic) and applies self-attention and cross-attention on multi-modal features. Heterography learning uses LightGCN on the user-item bipartite graph. The framework then refines contrastive learning through a meta-network that denoises modal-shared features using item-specific low-rank transformations, and orthogonal constraints that preserve modal-unique information. The final recommendation uses both modal-shared and modal-unique features.

## Key Results
- REARM achieves R@20 improvements of 0.0479 (Baby), 0.0553 (Sports), and 0.0454 (Clothing) compared to the best baseline
- Ablation studies show removing homographs causes the largest performance drop, validating the importance of multi-graph integration
- The orthogonal constraint prevents modal-unique information loss, which simple contrastive learning tends to suppress

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Meta-networks denoise modal-shared features by learning item-specific transformation matrices that extract recommendation-relevant information.
- **Mechanism:** The meta-network takes concatenated multi-modal features as input and generates customized low-rank transformation matrices (W₁, W₂) for each item. These matrices project modal-shared features through the item ID embedding space, filtering task-irrelevant signals while preserving recommendation-relevant patterns.
- **Core assumption:** Modal-shared features contain both useful recommendation signals AND noise; item-specific transformations can discriminate between them.
- **Evidence anchors:** [abstract] "meta-network and orthogonal constraint strategies, which filter out noise in modal-shared features"; [Section 4.4.2] "we employ a meta-network to extract recommendation-relevant knowledge and parametrically preserve knowledge into customized transformation matrices"; [corpus] MMHCL paper similarly addresses noise in multi-modal features through hypergraph contrastive learning, supporting the noise-filtering premise.

### Mechanism 2
- **Claim:** Orthogonal constraints preserve modal-unique features that contrastive learning would otherwise suppress.
- **Mechanism:** By minimizing the Frobenius norm between visual and textual feature matrices (||ī_v ⊤ ī_t||²_F), the model penalizes overlap between modalities, forcing each to retain distinctive information. BPR loss then selects which unique features are recommendation-relevant.
- **Core assumption:** Task-relevant unique information exists in each modality; orthogonalization preserves it without losing discriminative power.
- **Evidence anchors:** [Section 3.1] "valuing unique information as well as ignoring redundant irrelevant information is also critical"; [Section 4.4.3] "we impose an orthogonal constraint between multi-modal features to distinguish modal-unique information"; [corpus] Rethinking Contrastive Learning paper identifies similar issues with standard contrastive learning losing item-specific signals.

### Mechanism 3
- **Claim:** Integrating user interest graphs with item co-occurrence graphs captures complementary structural and semantic relationships.
- **Mechanism:** Four homogeneous graphs (user co-occurrence, user interest, item co-occurrence, item semantic) are constructed pre-training. Self-attention refines intra-modal features; cross-attention captures inter-modal influences. These representations initialize heterography learning on the user-item bipartite graph.
- **Core assumption:** User interests (from multi-modal similarity) and item co-occurrence (behavioral patterns) provide orthogonal signals that jointly improve representations.
- **Evidence anchors:** [Section 4.2] "we jointly explore co-occurrence and similarity relations from both user and item perspectives"; [Table 3] Ablation shows removing homographs (w/o hom) causes largest performance drop; [corpus] Weak direct corpus support for this specific graph combination; related work focuses on either co-occurrence OR semantic graphs, not both simultaneously.

## Foundational Learning

- **Concept: InfoNCE Contrastive Learning**
  - Why needed here: Foundation for multi-modal alignment; maximizing mutual information between visual and textual views
  - Quick check question: Can you explain why InfoNCE loss pulls positive pairs (same item, different modalities) together while pushing negative pairs apart?

- **Concept: Graph Neural Network Message Passing**
  - Why needed here: Core operation for both homography (homogeneous graphs) and heterography (bipartite interaction graph) relation learning
  - Quick check question: How does LightGCN's message passing differ from vanilla GCN, and why does removing feature transformations help recommendation?

- **Concept: Low-rank Matrix Factorization**
  - Why needed here: Meta-network uses low-rank matrices (k < d) to control parameters and prevent overfitting when learning item-specific transformations
  - Quick check question: Why would a rank-5 transformation matrix be more robust than a rank-64 matrix for this denoising task?

## Architecture Onboarding

- **Component map:**
Input: Item multi-modal features (visual 4096-d, textual 384-d)
  ↓
Homography Learning:
  ├─ Construct 4 graphs: user co-occurrence, user interest, item co-occurrence, item semantic
  ├─ Self-attention → Cross-attention on multi-modal features
  └─ Message passing on homogeneous graphs
  ↓
Heterography Learning:
  └─ LightGCN on user-item bipartite graph (separate for ID and each modality)
  ↓
Refined Contrastive Learning:
  ├─ InfoNCE loss aligns modalities
  ├─ Meta-network denoises modal-shared features
  └─ Orthogonal constraint preserves modal-unique features
  ↓
Output: u* = [modal-shared || modal-unique] for prediction

- **Critical path:**
  1. Graph construction (pre-training, one-time) → 2. Attention-based feature refinement → 3. Meta-network transformation (must initialize properly) → 4. Loss balancing (λ_cl, λ_ort hyperparameters)

- **Design tradeoffs:**
  - Matrix rank k: Lower = more robust but less expressive; paper uses 1-10 range
  - Homograph weights (α_co): Controls co-occurrence vs. semantic balance; dataset-dependent
  - GNN layers: Paper finds 3+ optimal (unusual vs. typical 2); attention mechanism may preserve signal better

- **Failure signatures:**
  - R@20 drops significantly when removing homographs → check graph construction logic
  - Modal-shared features still noisy → increase meta-network capacity or check transformation initialization
  - Modal-unique features lost → increase λ_ort weight or verify orthogonal loss computation

- **First 3 experiments:**
  1. **Reproduce ablation:** Run w/o hom, w/o meta, w/o ort variants on Baby dataset; expect largest drop from w/o hom
  2. **Hyperparameter sweep:** Vary matrix rank (1-10) and item co-occurrence weight (0-0.9) on validation set; optimal should match Figure 3 patterns
  3. **Visualization check:** Generate interaction probability heatmap (Figure 5) to verify refined features improve prediction confidence on held-out test pairs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance and computational overhead of REARM scale when applied to scenarios involving more than two modalities, such as incorporating video or audio data alongside text and images?
- **Basis in paper:** [explicit] The authors state in Section 4.1 that "our model could be easily extended to scenarios with more than two modalities," yet all experiments and definitions (e.g., $\mathcal{M} = \{v, t\}$) are restricted to visual and textual modalities only.
- **Why unresolved:** The current implementation and evaluation strictly utilize two modalities. It is unclear if the orthogonal constraints and meta-networks would introduce optimization conflicts or excessive parameter costs when applied to $N > 2$ modalities.
- **What evidence would resolve it:** Experimental results on a multi-modal dataset with three or more active modalities (e.g., TikTok or MovieLens with video/audio), comparing performance and training time against the dual-modality setup.

### Open Question 2
- **Question:** To what extent does the construction and storage of multiple auxiliary graphs (user/item co-occurrence and semantic graphs) limit the deployability of REARM on industrial-scale datasets with millions of items?
- **Basis in paper:** [inferred] The methodology requires constructing four distinct homogeneous graphs ($\bar{G}_{ii}, \tilde{G}_{ii}, \bar{G}_{uu}, \tilde{G}_{uu}$) in addition to the interaction graph (Section 4.2). While the theoretical complexity of the meta-network is discussed ($k < d$), the memory overhead of pre-processing and storing these dense relationship graphs for massive corpora is not analyzed.
- **Why unresolved:** The experiments use small to medium Amazon subsets (max 23k items), whereas real-world systems often handle catalogs of millions. The "top-k" sparsification helps, but the construction cost remains a potential bottleneck.
- **What evidence would resolve it:** A complexity analysis and memory usage report on a dataset with significantly higher item counts (e.g., the full Amazon review dataset or a industrial benchmark) to verify scalability.

### Open Question 3
- **Question:** Does the meta-network successfully distinguish between "noise" and valid recommendation signals in modal-shared features when visual and textual modalities are semantically misaligned or adversarial?
- **Basis in paper:** [inferred] The paper asserts that simple contrasting introduces noise (e.g., the "girl" vs "boys" tag example in Figure 1) and claims the meta-network filters this. However, this relies on the assumption that the downstream BPR loss will successfully guide the meta-network to isolate the noise without explicit alignment supervision.
- **Why unresolved:** The datasets used (Baby, Sports, Clothing) typically exhibit high coherence between images and text. The mechanism is untested in "noisy" domains where modalities actively conflict or are irrelevant to the user's primary interest.
- **What evidence would resolve it:** An ablation study using synthetic datasets where image-text alignment is intentionally corrupted to measure the meta-network's robustness in filtering noise compared to baseline contrastive methods.

## Limitations
- Graph construction specifics: Exact k values for top-k edge selection in user/item co-occurrence and semantic graphs are not specified, potentially affecting performance
- Hyperparameter sensitivity: Optimal λ_cl, λ_ort, and matrix rank k appear dataset-dependent but exact values are unreported
- Feature quality dependency: Performance hinges on high-quality 4,096-dim visual and 384-dim textual features from prior work

## Confidence
- **High:** Graph-based multi-modal integration (homography + heterography) improves recommendation—supported by consistent R@20 gains when removing homographs
- **Medium:** Meta-network effectively denoises modal-shared features—plausible given low-rank transformation design, but mechanism depends on unknown graph sparsity and transformation initialization
- **Medium:** Orthogonal constraints preserve modal-unique information—theory supported by contrastive learning literature, but practical impact on recommendation metrics requires further validation

## Next Checks
1. **Graph sparsity analysis:** Systematically vary k in top-k edge selection and measure R@20; identify threshold where performance plateaus or degrades
2. **Loss component ablation:** Freeze homographs, then independently disable meta-network and orthogonal constraints; quantify each component's marginal contribution to final performance
3. **Feature attribution study:** Use gradient-based saliency to verify modal-unique features identified by orthogonalization correlate with recommendation-relevant attributes (e.g., visual style vs. textual description)