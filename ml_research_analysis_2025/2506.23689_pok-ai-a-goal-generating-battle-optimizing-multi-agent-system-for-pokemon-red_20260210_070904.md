---
ver: rpa2
title: "Pok\xE9AI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon\
  \ Red"
arxiv_id: '2506.23689'
source_url: https://arxiv.org/abs/2506.23689
tags:
- agent
- battle
- emon
- execution
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Pok\xE9AI is a text-based, multi-agent system for autonomously\
  \ playing Pok\xE9mon Red, comprising three specialized agents: Planning, Execution,\
  \ and Critique. The Planning Agent generates high-level tasks, the Execution Agent\
  \ performs actions within the game, and the Critique Agent verifies outcomes."
---

# PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red

## Quick Facts
- **arXiv ID:** 2506.23689
- **Source URL:** https://arxiv.org/abs/2506.23689
- **Reference count:** 15
- **Primary result:** 80.8% win rate in 50 wild Pokémon encounters, 6% below human baseline

## Executive Summary
PokéAI is a text-based, multi-agent system for autonomously playing Pokémon Red, comprising three specialized agents: Planning, Execution, and Critique. The Planning Agent generates high-level tasks, the Execution Agent performs actions within the game, and the Critique Agent verifies outcomes. As a preliminary evaluation, the battle module within the Execution Agent was tested in 50 wild encounters using Mt. Moon, achieving an average win rate of 80.8%—only 6% lower than an experienced human player. An ablation study revealed that removing the escape option had minimal impact, while disabling Pokémon switching or item usage significantly reduced performance. Performance varied across different LLM backends, with higher LLM Arena scores generally correlating with better battle outcomes. Analysis of action distributions showed that each model exhibited a unique playstyle. A pilot study with long-term memory (Letta framework) demonstrated improved decision-making, such as recalling past defeats to avoid unfavorable battles.

## Method Summary
The system uses memory-mapped I/O to detect battle start via RAM address 0xD057, then extracts structured state data (HP, moves) for LLM processing. The battle module reads state → sends to LLM → receives JSON response → executes button inputs. Evaluation used Level 15 Charmander and Pidgey with 5 potions in Mt. Moon wild encounters (Zubat 79%, Geodude 15%, Paras 5%, Clefairy 1%, avg level 8.18).

## Key Results
- Battle module achieved 80.8% win rate over 50 wild encounters
- Human baseline: 86% win rate (6% higher than AI)
- Ablation study: Disabling items dropped win rate to 32.6%; disabling switching had similar impact
- Higher LLM Arena scores generally correlated with better battle outcomes
- Different LLM models exhibited distinct playstyles in action distributions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Role-specialized multi-agent decomposition enables robust closed-loop decision-making by isolating planning, execution, and verification logic.
- **Mechanism:** The system separates high-level goal generation (Planning Agent) from low-level actuation (Execution Agent) and outcome validation (Critique Agent). If the Execution Agent fails a task, the Critique Agent detects the state mismatch and forces a retry, preventing the system from drifting into unrecoverable states.
- **Core assumption:** LLMs perform better when prompted with narrow, role-specific contexts rather than monolithic instructions, and that error correction is more reliable when verification is architecturally separate from execution.
- **Evidence anchors:**
  - [abstract] "Our system consists of three specialized agents... forming a closed-loop decision-making system."
  - [section II] "The Critique Agent serves as a task verifier... If not, it instructs the Execution Agent to retry the task."
  - [corpus] Related work "DynTaskMAS" supports the efficacy of dynamic task decomposition in LLM-based multi-agent systems.
- **Break condition:** If the Critique Agent hallucinates a success state (false positive) or if the Planning Agent generates impossible dependencies, the loop will reinforce errors rather than correct them.

### Mechanism 2
- **Claim:** Text-based state abstraction via memory monitoring provides a sufficiently dense signal for strategic reasoning while avoiding the computational overhead of vision.
- **Mechanism:** Instead of parsing pixels, the system hooks into emulator memory addresses (e.g., `0xD057` for battle detection) to extract structured state data (HP, moves). This text representation allows the LLM to leverage its pre-trained semantic understanding of "health," "damage," and "type" without requiring visual grounding.
- **Core assumption:** Critical game states can be fully captured by specific memory addresses, and the text representation of these states preserves the information necessary for the LLM to apply general strategic knowledge.
- **Evidence anchors:**
  - [section II] "detected by monitoring memory address 0xD057... indicates the onset of a battle."
  - [section I] "first fully text-based... avoiding computationally expensive vision."
  - [corpus] Corpus papers on RL in Pokemon (e.g., "Pokemon Red via Reinforcement Learning") contrast this by using pixel-based inputs, highlighting the efficiency trade-off.
- **Break condition:** The mechanism fails if the memory address mapping is incorrect for different game versions, or if "invisible" state variables (not exposed to the text prompt) are critical for the decision.

### Mechanism 3
- **Claim:** Strategic resource management (items/switching) is the primary driver of win rates, enabled by the LLM's ability to conduct forward-looking planning.
- **Mechanism:** The LLM does not just select the highest-damage move; it simulates future turns. The ablation study confirms that the "intelligence" of the agent lies in preserving resources (using potions) and optimizing matchups (switching Pokemon), capabilities derived from the model's general reasoning capacity rather than simple reflex.
- **Core assumption:** The LLM has internalized the rules of the game (or can infer them from context) well enough to value future state over immediate gratification.
- **Evidence anchors:**
  - [abstract] "Ablation studies show that item usage and Pokémon switching contribute most to performance."
  - [section III.B] "No Item variant performed the worst... 32.6%."
  - [corpus] "PokéChamp" (corpus) explicitly uses LLMs to enhance search, reinforcing that LLMs add value via strategic lookahead rather than just move selection.
- **Break condition:** If the context window is too small to track status effects or multi-turn strategies, or if the model lacks the specific game knowledge (e.g., type effectiveness), planning degrades into random selection.

## Foundational Learning

- **Concept: Memory-Mapped I/O (MMIO)**
  - **Why needed here:** The agent acts as a "brain" connected to a "body" (the emulator). Understanding that it reads specific RAM addresses rather than looking at a screen is crucial for debugging why the agent might miss a battle start.
  - **Quick check question:** If the memory address `0xD057` never flips to 1, will the Battle Module activate?

- **Concept: Ablation Studies**
  - **Why needed here:** To distinguish between the agent *feeling* smart and *being* effective. The paper uses this to prove that specific features (items/switching) are the engines of success, not just the presence of an LLM.
  - **Quick check question:** Why does the "No Item" win rate drop to ~32% while the "No Escape" rate stays nearly the same as the full model?

- **Concept: Context Window & State Compression**
  - **Why needed here:** The agent relies on the prompt containing the battle state. If the state description is verbose, it hits token limits; if too sparse, it lacks data. The "short-term memory" here is explicitly limited to the "last three rounds."
  - **Quick check question:** How does limiting operation history to 3 rounds potentially hurt the agent's ability to recognize a status effect (like Poison) applied 4 turns ago?

## Architecture Onboarding

- **Component map:**
  - Planning Agent (LLM + Vector Memory) -> Generates JSON Tasks
  - Execution Agent (LLM + Tool Memory + Battle Module) -> Executes navigation or combat
  - Critique Agent (LLM) -> Compares "Task Description" vs "Resulting Game State"
  - Emulator Interface -> Reads RAM -> Text State; Text Action -> Button Presses

- **Critical path:** The Battle Module execution loop is the most validated path.
  1. `Read RAM` (Check `0xD057`)
  2. `Serialize State` (HP, Moves, Enemy Type -> JSON)
  3. `LLM Inference` (Prompt -> "Use Item" or "Attack")
  4. `Act` (Execute button press)

- **Design tradeoffs:**
  - **Text vs. Vision:** Text is faster/cheaper but brittle (requires hardcoded memory addresses). Vision is robust but computationally heavy and prone to visual hallucinations.
  - **General LLM vs. Specialized Solver:** Using GPT-4o/DeepSeek allows emergent strategy but introduces "creative rule-bending" (e.g., GPT-4o trying to use a Pokeball to avoid damage). A specialized solver would follow rules but lack adaptability.

- **Failure signatures:**
  - **"Overthinking":** Strong reasoning models (e.g., DeepSeek-R1) may hallucinate complex threats in simple scenarios, leading to suboptimal play.
  - **Context Confusion:** Models like Qwen-3B attempting to use moves with 0 PP, indicating a failure to parse or retain resource constraints.
  - **Creative Rule-Bending:** Models attempting actions explicitly forbidden in the prompt (e.g., GPT-4o using Pokeballs) because they infer it as a valid survival strategy.

- **First 3 experiments:**
  1. **Replicate "No Item" Ablation:** Run 10 battles with the "No Item" flag to verify the ~32% win rate baseline against the reported 80.8%. This validates the environment is set up correctly.
  2. **Memory Injection Attack:** Using the Letta framework, inject a false memory: "Fire attacks heal Water Pokemon." Observe if the Battle Agent changes its behavior to *heal* the enemy with Fire moves, testing the influence of the long-term memory module.
  3. **Switch Logic Stress Test:** Force a battle against a "super effective" matchup (e.g., Fire vs. Water) and verify if the agent prioritizes switching over attacking, as the ablation study suggests switching is a high-value capability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does initializing agents with specific personality traits lead to quantifiable emergent behaviors and distinct playstyles?
- **Basis in paper:** [explicit] The authors state they aim to "initialize agents with different personality traits... to examine how these factors influence decision-making" in future iterations.
- **Why unresolved:** The current study focused on standard model capabilities and battle optimization without explicitly controlling for or injecting personality profiles.
- **What evidence would resolve it:** Comparative gameplay logs of agents initialized with distinct traits (e.g., "aggressive" vs. "defensive") showing statistically significant divergence in action distributions and strategic choices.

### Open Question 2
- **Question:** Can the complete three-agent loop (Planning, Execution, Critique) successfully navigate complex, multi-step game objectives outside of isolated battle scenarios?
- **Basis in paper:** [explicit] The paper notes that the current work focuses only on the battle module, and implementing the full Planning and Critique agents is listed as "immediate future work."
- **Why unresolved:** The system architecture is described, but the Planning and Critique agents have not yet been integrated or evaluated in a continuous gameplay loop.
- **What evidence would resolve it:** Successful autonomous completion of a multi-stage milestone (e.g., navigating Mt. Moon) using the full closed-loop system rather than just the battle module.

### Open Question 3
- **Question:** Do reasoning-oriented models (e.g., DeepSeek-R1) provide a strategic advantage in complex battles, or does the "overthinking" penalty observed in simple encounters persist?
- **Basis in paper:** [inferred] The authors note that reasoning models were not fully evaluated, and preliminary tests suggested they might make worse decisions in simple scenarios due to "overthinking."
- **Why unresolved:** Evaluation was limited to wild encounters; complex scenarios requiring deep calculation were not tested against the "overthinking" hypothesis.
- **What evidence would resolve it:** Performance metrics of reasoning models in high-stakes, multi-faceted battles (e.g., Gym Leaders) comparing decision quality and latency against standard models.

## Limitations

- Evaluation limited to wild encounters in single location (Mt. Moon), may not generalize to trainer battles or complex dungeons
- Win rate comparison to human players based on single experienced player's performance, introducing high variance
- Paper does not report statistical significance tests for win rate differences
- Memory address dependencies specific to Pokémon Red may not transfer to other game versions

## Confidence

- **High Confidence:** The ablation study results showing that disabling items or switching significantly reduces win rate (32.6% vs 80.8%) are well-supported by the data.
- **Medium Confidence:** The claim that higher LLM Arena scores correlate with better battle outcomes, as the paper only shows correlation without establishing causation or controlling for other variables.
- **Low Confidence:** The generalizability of the text-based approach to more complex game scenarios beyond wild encounters, as this remains largely untested.

## Next Checks

1. **Statistical Validation:** Run the battle module 50 times against each of the three Pokémon types in Mt. Moon (Zubat, Geodude, Paras) separately, calculating win rates per type. Compare these to the aggregated 80.8% to identify if performance varies significantly by opponent.

2. **Memory Injection Test:** Using the Letta framework, inject a false memory stating "Fire attacks heal Water Pokemon" and run 10 battles. Verify if the agent attempts to heal the opponent with Fire moves, confirming that long-term memory influences battle decisions.

3. **Context Window Stress Test:** Modify the battle module to only track the last 2 rounds instead of 3. Run 20 battles and measure if the win rate drops, testing the hypothesis that tracking status effects beyond 2 turns is critical for strategic decision-making.