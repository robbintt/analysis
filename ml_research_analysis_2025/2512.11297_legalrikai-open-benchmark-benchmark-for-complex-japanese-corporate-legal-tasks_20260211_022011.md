---
ver: rpa2
title: 'LegalRikai: Open Benchmark -- Benchmark for Complex Japanese Corporate Legal
  Tasks'
arxiv_id: '2512.11297'
source_url: https://arxiv.org/abs/2512.11297
tags:
- legal
- contract
- tasks
- evaluation
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LegalRikai: Open Benchmark introduces four complex Japanese corporate\
  \ legal tasks\u2014legal amendment explanation, statute-driven contract revision,\
  \ requirement-driven contract revision, and risk-driven contract revision\u2014\
  designed to emulate real-world legal workflows. Each task involves long-form, structured\
  \ outputs and was created by legal professionals under attorney supervision, with\
  \ 25 samples per task totaling 100."
---

# LegalRikai: Open Benchmark -- Benchmark for Complex Japanese Corporate Legal Tasks

## Quick Facts
- arXiv ID: 2512.11297
- Source URL: https://arxiv.org/abs/2512.11297
- Reference count: 0
- Key outcome: LegalRikai benchmark introduces four complex Japanese corporate legal tasks with long-form outputs, revealing that automated evaluation aligns with human judgment on semantic criteria but struggles with structural consistency assessment.

## Executive Summary
LegalRikai: Open Benchmark presents four specialized Japanese corporate legal tasks designed to evaluate LLM performance on real-world document editing workflows. Each task requires long-form, structured outputs and was created by legal professionals under attorney supervision. The benchmark demonstrates that while automated evaluation correlates well with human judgment on criteria with clear linguistic grounding (coverage, instruction following), it fails to assess structural consistency due to LLMs' inability to track global document references. Results show that abstract instructions cause models to make unnecessary modifications, revealing weaknesses in document-level editing that conventional short-text tasks miss.

## Method Summary
The benchmark comprises 100 samples (25 per task) created by legal professionals under attorney supervision, using real corporate contract templates and statutes sourced from Japan's Digital Agency e-Gov API. Four tasks target distinct legal workflows: legal amendment explanation, statute-driven contract revision, requirement-driven contract revision, and risk-driven contract revision. Inputs range from ~6,500 to 27,000 characters. Evaluation uses three major closed-source LLMs (GPT-5, Gemini 2.5 Pro, Claude Opus 4.1) with temperature=0.0, assessing outputs across multiple criteria including coverage, accuracy, structural consistency, and terminology. Automated evaluation produces structured JSON outputs for correlation analysis against human expert judgment.

## Key Results
- Automated evaluation correlates strongly with human judgment on coverage and instruction-following metrics (Spearman ~0.6-0.7) but poorly on structural consistency (Spearman ~0.07-0.16)
- Abstract instructions cause unnecessary modifications: GPT-5 achieved CP=0.22 on RiskRev vs CP=0.58 on ReqRev
- Model scale reduction causes severe performance degradation on long-context legal tasks: CI dropped from 0.52 (full) to 0.27 (nano) on StatRev/AmendExp
- GPT-5 performed best overall (average score 0.66) followed by Gemini 2.5 Pro (0.58) and Claude Opus 4.1 (0.55)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction specificity directly controls unnecessary modifications in legal document editing; abstract instructions cause models to exceed intended scope.
- Mechanism: When instructions lack explicit constraints, models interpret and supplement abstract guidance, producing excessive edits that deviate from practitioner intent. Specific instructions (e.g., exact wording changes) produce high instruction-following scores; abstract instructions (e.g., "revise to be more favorable") trigger broader interpretation and unnecessary alterations.
- Core assumption: Model tendency to "help" by making additional improvements aligns poorly with legal workflows where unintended changes introduce review costs and legal risk.
- Evidence anchors:
  - [abstract] "abstract instructions prompted unnecessary modifications, highlighting model weaknesses in document-level editing that were missed by conventional short-text tasks"
  - [section 3.1] "In ReqRev... all models achieved high scores in IF... In contrast, in StatRev... and RiskRev, model performance varied greatly, with a notable increase in unnecessary modifications"
  - [corpus] Related legal benchmarks (LegalBench, LawBench, JBE-QA) focus on knowledge recall and reasoning rather than document-level editing precision
- Break condition: If instructions consistently specify exact changes with scope constraints, unnecessary modification rates should converge near zero across models.

### Mechanism 2
- Claim: Automated LLM evaluation correlates with human judgment on criteria with clear linguistic grounding but fails on structural consistency and contextual terminology appropriateness.
- Mechanism: LLMs process text as linear token sequences, making them effective at local semantic assessment (coverage, instruction following) but unable to track global document references or perceive visual/logical structure that human evaluators assess holistically.
- Core assumption: Legal terminology appropriateness requires contextual judgment beyond dictionary correctness—for example, "immediately" vs. "promptly" carry different obligation levels that LLMs evaluate by word frequency rather than legal context.
- Evidence anchors:
  - [abstract] "automated evaluation aligns well with human judgment on criteria with clear linguistic grounding, and assessing structural consistency remains a challenge"
  - [section 3.2] "The difficulty in evaluating SC is likely due to LLMs processing text as a linear sequence of tokens. Human evaluators perceive the visual and logical structure of a document as a whole, such as clause number references and indentation breaks."
  - [corpus] No corpus evidence directly addresses structural consistency evaluation in legal documents; related work focuses on knowledge evaluation
- Break condition: If documents are restructured to make cross-references explicit in linear text flow, automated structural evaluation should improve.

### Mechanism 3
- Claim: Model scale reduction causes significantly worse performance degradation on complex legal editing tasks than on general reasoning benchmarks.
- Mechanism: Document-level editing requires retaining complex information simultaneously—overall structure, inter-clause references, legal definitions—across long contexts (27K+ characters). Smaller models lose this global awareness, causing coverage drops and structural errors.
- Core assumption: Legal document editing cannot be decomposed into independent sub-tasks without losing critical cross-reference integrity.
- Evidence anchors:
  - [section 3.3] "the CI score dropped by 0.25 from 0.52 for the full model to 0.27 for the nano model"
  - [section 3.3] "In the GPQA diamond benchmark... the performance drop was only 14.5%... This comparison reveals that the reduction in model scale leads to significantly more severe performance degradation in specialized, long-text reading and editing tasks"
  - [corpus] Related Japanese benchmarks (JMedBench, JBE-QA) do not examine scale effects on complex document-level tasks
- Break condition: If tasks can be meaningfully decomposed with explicit context handoffs between sub-components, smaller models should approach larger model performance.

## Foundational Learning

- Concept: Long-context document coherence
  - Why needed here: These tasks require processing 6,500–27,000 character inputs while maintaining awareness of cross-clause references and definitions throughout editing
  - Quick check question: Can you explain why modifying Article 8 might require awareness of terms defined in Article 2?

- Concept: Instruction specificity–performance tradeoff
  - Why needed here: The paper demonstrates that abstract legal instructions ("mitigate risk") produce fundamentally different model behavior than specific instructions ("add prior written consent requirement")
  - Quick check question: Given a contract revision task, how would you determine the minimum specificity needed to prevent unnecessary modifications?

- Concept: Multi-axis evaluation design
  - Why needed here: Legal output quality has distinct dimensions—coverage, accuracy, structural consistency, terminology—that require separate assessment rather than a single score
  - Quick check question: Why might a revised contract score perfectly on instruction following but fail on structural consistency?

## Architecture Onboarding

- Component map: Document ingestion -> Format normalization -> Context window management -> Task-specific prompt construction -> Structured output generation -> Multi-criteria automated evaluation -> Human review checkpoint
- Critical path: 1. Document ingestion and format normalization 2. Context window management (truncation strategy for statutes exceeding limits) 3. Task-specific prompt construction 4. Structured output generation 5. Multi-criteria automated evaluation 6. Human review checkpoint for structural consistency and terminology
- Design tradeoffs:
  - Model scale vs. latency/cost: Full-scale models required for StatRev/AmendExp (27K+ char contexts); smaller models viable for simpler ReqRev
  - Automated vs. human evaluation: Automate coverage/instruction-following screening; reserve human review for structural/terminology assessment
  - Instruction specificity vs. flexibility: Overly specific instructions reduce model initiative but prevent costly unnecessary modifications
- Failure signatures:
  - Unnecessary clause modifications beyond instruction scope (check CP metric)
  - Broken cross-references after revision (e.g., "Article 15 refers to Article 9" becomes invalid)
  - Terminology substitution that alters legal obligation levels
  - Coverage gaps in amendment impact analysis
- First 3 experiments:
  1. Run identical contract through ReqRev (specific) and RiskRev (abstract) to quantify instruction specificity effect on unnecessary modification rate
  2. Compare GPT-5 vs GPT-5-mini on StatRev to measure scale sensitivity for your specific use case
  3. Validate automated evaluation correlation with human expert on 10 held-out samples, focusing on structural consistency discrepancies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated evaluation methods be developed to accurately assess structural consistency (SC) in legal document editing tasks?
- Basis in paper: [explicit] The authors state that "assessing structural consistency remains a challenge" and that "LLMs processing text as a linear sequence of tokens" struggle to track global references like "Article 15 refers to Article 9," unlike human evaluators who perceive visual and logical document structure holistically.
- Why unresolved: Current LLM-based evaluators show weak correlations with human judgment on SC (correlation coefficients as low as 0.07-0.16 across tasks), indicating fundamental architectural limitations in evaluating document-level structure.
- What evidence would resolve it: Development of evaluation methods that achieve moderate-to-strong correlation (≥0.5) with human expert judgments on structural consistency across all four tasks.

### Open Question 2
- Question: How do open-source LLMs compare to closed-source models on complex legal document editing tasks?
- Basis in paper: [explicit] The Limitations section states: "the evaluation was limited to three major closed-source LLMs and did not include the open-source models that have seen recent performance improvements. Validation with a broader range of models is a task for the future."
- Why unresolved: Only GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1 were evaluated; open-source alternatives were excluded despite their improving capabilities.
- What evidence would resolve it: Benchmarking open-source models (e.g., Llama, Mistral variants) on LegalRikai tasks with comparable human and automated evaluation protocols.

### Open Question 3
- Question: Can the LegalRikai task framework generalize to other languages and legal systems (e.g., English common law jurisdictions)?
- Basis in paper: [explicit] The Future Work section states: "since the proposed task framework is language-independent, it is expected to apply to other languages and legal systems, such as those in English-speaking countries or civil law jurisdictions. This attempt would enable a comparative analysis of how differences in legal systems affect the editing behavior of LLMs."
- Why unresolved: The benchmark only covers Japanese corporate legal tasks; cross-linguistic and cross-jurisdictional applicability remains untested.
- What evidence would resolve it: Creation of equivalent benchmarks in other languages/jurisdictions demonstrating similar task structures can be applied and revealing whether model weaknesses (e.g., unnecessary modifications under abstract instructions) persist cross-linguistically.

### Open Question 4
- Question: Can prompt engineering or instruction design mitigate the tendency of LLMs to make unnecessary modifications when given abstract legal instructions?
- Basis in paper: [inferred] The paper finds that "abstract instructions prompted unnecessary modifications" with GPT-5 scoring CP (Change Precision) of only 0.22 on RiskRev vs. 0.58 on ReqRev, suggesting instruction specificity directly impacts model behavior, but no solution is proposed.
- Why unresolved: The paper identifies the problem but does not investigate interventions such as chain-of-thought prompting, explicit constraints, or intermediate reasoning steps to reduce over-editing.
- What evidence would resolve it: Systematic experiments comparing different prompting strategies on abstract-instruction tasks (StatRev, RiskRev) showing statistically significant improvements in Change Precision scores without sacrificing coverage or accuracy.

## Limitations

- The dataset requires application and agreement to terms, limiting independent verification and introducing potential selection bias
- Automated evaluation's inability to assess structural consistency represents a critical gap for legal document quality assessment
- The truncation strategy for long statutes is described but not fully specified, which could affect task difficulty and model performance
- The study does not address potential biases in the legal professionals' expertise or the representativeness of the contract templates used

## Confidence

- High Confidence: Automated evaluation correlates well with human judgment on coverage and instruction following (semantic assessment strengths of LLMs)
- Medium Confidence: Abstract instructions cause unnecessary modifications (supported but needs more rigorous ablation)
- Medium Confidence: Smaller models degrade more severely on long-context legal tasks (plausible but limited comparison)
- Low Confidence: Automated evaluation's inability to assess structural consistency (stated but not deeply explored)

## Next Checks

1. Replicate the instruction specificity experiment: Take a single contract sample and run it through both specific (ReqRev) and abstract (RiskRev) instruction sets. Quantify the rate of unnecessary modifications (CP metric) and verify that abstract instructions consistently trigger broader edits.

2. Test structural consistency evaluation: Manually annotate 10 samples for structural consistency (e.g., broken cross-references, invalid clause numbering) and compare automated vs. human scores. Calculate precision/recall of the automated SC metric to quantify its weakness.

3. Scale sensitivity benchmark: Run a subset of the StatRev and AmendExp tasks (27K+ char contexts) through GPT-5 and GPT-5-mini. Measure coverage and accuracy drops to confirm the reported 0.25 CI degradation and assess whether task decomposition can mitigate the effect.