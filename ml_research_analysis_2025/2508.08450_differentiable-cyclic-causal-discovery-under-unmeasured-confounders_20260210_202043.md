---
ver: rpa2
title: Differentiable Cyclic Causal Discovery Under Unmeasured Confounders
arxiv_id: '2508.08450'
source_url: https://arxiv.org/abs/2508.08450
tags:
- graph
- causal
- data
- dccd-conf
- directed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning nonlinear cyclic causal
  graphs in the presence of unmeasured confounders using interventional data. The
  authors propose DCCD-CONF, a differentiable framework that models causal mechanisms
  via implicit flows and learns graph structure by maximizing penalized log-likelihood.
---

# Differentiable Cyclic Causal Discovery Under Unmeasured Confounders

## Quick Facts
- arXiv ID: 2508.08450
- Source URL: https://arxiv.org/abs/2508.08450
- Reference count: 40
- One-line primary result: DCCD-CONF learns nonlinear cyclic causal graphs with unmeasured confounders, outperforming state-of-the-art methods on synthetic and real-world datasets.

## Executive Summary
This paper addresses the challenge of learning nonlinear cyclic causal graphs in the presence of unmeasured confounders using interventional data. The authors propose DCCD-CONF, a differentiable framework that models causal mechanisms via implicit flows and learns graph structure by maximizing a penalized log-likelihood. The method alternates between optimizing the graph structure and estimating the confounder distribution. Through extensive experiments, DCCD-CONF demonstrates superior performance in both causal graph recovery and confounder identification compared to existing methods.

## Method Summary
DCCD-CONF represents confounding as correlations in the exogenous Gaussian noise within an invertible generative model. The method models structural equations X = F(X, Z) via implicit flows, where Z follows a multivariate Gaussian with covariance ΣZ. Non-zero off-diagonal entries in ΣZ represent bidirectional edges (hidden confounders). The model jointly learns causal mechanisms (neural networks) and the noise covariance by alternating between updating graph/NN parameters via backpropagation and updating ΣZ via graphical lasso regression.

## Key Results
- DCCD-CONF outperforms state-of-the-art methods in both causal graph recovery and confounder identification
- On Perturb-CITE-seq gene perturbation dataset, DCCD-CONF achieves superior predictive accuracy compared to baselines
- Theoretical consistency guarantees establish that estimated graph belongs to same interventional equivalence class as ground truth under appropriate assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DCCD-CONF learns nonlinear cyclic causal structures with unmeasured confounders by representing confounding as correlations in the exogenous Gaussian noise within an invertible generative model.
- **Mechanism:** The method models structural equations X = F(X, Z) via implicit flows (Lu et al., 2021). The exogenous noise vector Z follows a multivariate Gaussian with covariance ΣZ; non-zero off-diagonal entries in ΣZ represent bidirectional edges i ↔ j (hidden confounders). The model jointly learns causal mechanisms (neural networks) and the noise covariance.
- **Core assumption:** Exogenous noise Z is Gaussian (non-Gaussian extensions not supported). The causal mechanism F yields a unique fixed point for any Z (contractive functions).
- **Evidence anchors:** [abstract] "Our approach alternates between optimizing the graph structure and estimating the confounder distribution by maximizing the log-likelihood of the data." [Section 2.1] Equation (1) and: "The collection of exogenous noise variables Z = (Z1, ..., Zd) account for the stochastic nature as well as the confounding observed in the system. We make the assumption that the exogenous noise vector follows a Gaussian distributions: Z ∼ N(0, ΣZ)."
- **Break condition:** If noise is non-Gaussian or the system does not reach a unique equilibrium (e.g., non-contractive dynamics), likelihood-based estimation and fixed-point guarantees degrade.

### Mechanism 2
- **Claim:** The causal graph (directed and bidirectional edges) is learned by maximizing a penalized log-likelihood score over a family of interventions, with theoretical consistency under σ-separation.
- **Mechanism:** The score S_I(G) sums log-likelihoods for each interventional setting I_k, minus sparsity regularizer λ|G|. Optimization alternates between updating graph/NN parameters (via Gumbel-Softmax gradients) and confounder covariance ΣZ (via column-wise lasso regression), driving the estimated graph into the interventional Markov equivalence class of the ground truth.
- **Core assumption:** Interventional stability (Assumption 1) ensures a unique fixed point under each intervention I_k. The data-generating distribution lies in the model class (Assumption A.13). I-σ-faithfulness (Assumption A.14), linking statistical independencies to σ-separation.
- **Evidence anchors:** [abstract] "Through experiments ... DCCD-CONF outperforms state-of-the-art methods in both causal graph recovery and confounder identification." [Section 3.2] Theorem 2 states that under these assumptions, the estimated graph is I-Markov equivalent to the ground truth.
- **Break condition:** If interventions are insufficient or violate interventional stability, the score may not distinguish graphs within the equivalence class, leading to non-identifiability.

### Mechanism 3
- **Claim:** The method remains computationally tractable by using unbiased stochastic estimators for the log-determinant of the Jacobian and by splitting optimization into graph/NN and covariance updates.
- **Mechanism:** The log-likelihood includes a log|det(Jacobian)| term (O(d²) cost). DCCD-CONF uses a power-series expansion combined with the Hutchinson trace estimator to obtain an unbiased estimate. Optimization alternates: (1) update θ and B via backpropagation with Straight-Through Gumbel estimator; (2) update ΣZ by solving a graphical lasso problem per column.
- **Core assumption:** The causal functions g_x and g_z are contractive, ensuring power-series convergence and valid log-det estimation. The covariance update remains stable (positive definiteness preserved via lasso).
- **Evidence anchors:** [Section 3.3.1] Equations (11) and (12) describe the unbiased log-det estimator. [Section 3.3.3] Equations (13)-(17) show the column-wise graphical lasso update for ΣZ.
- **Break condition:** If the power series is truncated without unbiased weighting or if the covariance update diverges, gradients become biased or optimization destabilizes.

## Foundational Learning

- **Concept: Directed Mixed Graphs (DMGs) and σ-separation**
  - Why needed here: The framework targets cyclic graphs with bidirectional edges (confounders). Standard d-separation fails in cyclic settings; σ-separation provides the correct Markov property for acyclified graphs.
  - Quick check question: For a simple 2-node cycle X1 ↔ X2 with a hidden confounder between them, can you state a σ-separation that does not hold under d-separation?

- **Concept: Implicit Flows and Fixed-Point Solvers**
  - Why needed here: The SEM X = F(X, Z) is invertibly parameterized via implicit flows, requiring root-finding (Broyden's method) to map between X and Z.
  - Quick check question: Explain how contractivity of g_x and g_z guarantees a unique fixed point for the implicit map.

- **Concept: Gumbel-Softmax and Straight-Through Estimators**
  - Why needed here: The adjacency matrix entries are modeled as Bernoulli variables with parameters B; gradients require continuous relaxations.
  - Quick check question: How does the Straight-Through Gumbel estimator enable backpropagation through discrete edge sampling?

## Architecture Onboarding

- **Component map:** Data input -> Implicit flow block -> Log-likelihood module -> Score maximization -> Output
- **Critical path:** 
  1. Initialize θ, B, and ΣZ = I
  2. For each epoch:
     - For each intervention I_k:
       - Compute forward map f_x^{I_k}(X) via Broyden root-finding
       - Evaluate unbiased log-det estimator using Hutchinson trace
       - Update θ and B via gradient descent with Gumbel-Softmax
       - Update ΣZ via column-wise lasso on sample covariance of recovered Z
  3. Threshold final B and ΣZ to obtain edge presence
- **Design tradeoffs:**
  - Gaussian noise assumption: Simplifies ΣZ estimation but limits applicability to non-Gaussian settings
  - Contractivity constraint: Ensures fixed-point uniqueness and log-det convergence, but may restrict expressiveness for highly cyclic systems
  - Alternating vs. joint optimization: Alternating decouples graph/NN and covariance updates for stability, but may be slower to converge
- **Failure signatures:**
  - Non-unique fixed points: Root-finding fails or oscillates; check Lipschitz constants of g_x, g_z
  - Diverging covariance: ΣZ loses positive definiteness; increase lasso regularization ρ
  - Biased log-det estimator: Power series truncation without unbiased weighting; ensure Poisson-sampled cut-off
- **First 3 experiments:**
  1. Synthetic ER graphs with known confounders: Vary confounder ratio (0.2-0.8) and number of cycles (0-8) on d=10 nodes; report normalized SHD and F1 for confounder recovery. Compare against baselines (NODAGS-Flow, LLC, DAGMA, ADMG).
  2. Scaling with nodes and interventions: Fix confounder ratio at 0.3-0.4, vary d from 10 to 80 and number of interventions from 0 to d; measure performance degradation and computational cost.
  3. Real-world Perturb-CITE-seq gene perturbation: Train on single-gene knockouts across three cell conditions; evaluate test-set negative log-likelihood and interventional MAE. Examine recovered adjacency for plausible feedback loops.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the necessary conditions on interventional targets required for perfect recovery in general directed mixed graphs (DMGs) containing both cycles and confounders?
- Basis in paper: [explicit] The authors state on page 6 that "determining the necessary conditions on interventional targets for perfect recovery in general DMGs with cycles and confounders remains an open problem," noting that current results rely on specific settings like single-node interventions.
- Why unresolved: While the paper provides consistency guarantees for the interventional equivalence class and empirical results show perfect recovery with single-node interventions, a general theoretical characterization of sufficient interventional sets for unique identifiability is lacking.
- What evidence would resolve it: A formal proof defining the minimal set of interventions (beyond the single-node case) required to uniquely identify the ground truth graph in nonlinear settings.

### Open Question 2
- Question: Can the DCCD-CONF framework be extended to handle non-Gaussian exogenous noise distributions while maintaining theoretical consistency?
- Basis in paper: [explicit] Section 5 (Discussion) states: "While the focus of this work is limited to Gaussian exogenous noise, we plan to investigate other noise distributions for future research."
- Why unresolved: The current mathematical formulation, specifically the likelihood computation and the modeling of confounders via a covariance matrix ΣZ, relies explicitly on the Gaussian assumption.
- What evidence would resolve it: Derivation of a modified score function and structural equation model that supports arbitrary noise distributions, accompanied by consistency proofs.

### Open Question 3
- Question: How can the framework be adapted to support causal discovery under soft interventions or when interventional targets are unknown?
- Basis in paper: [explicit] Section 5 lists "relaxing interventional assumptions by incorporating soft interventions and unknown interventional targets" as a future direction.
- Why unresolved: The current methodology relies on "surgical" (hard) interventions where incoming edges are removed, and it assumes the interventional targets are known a priori to construct the mutilated graph.
- What evidence would resolve it: An extension of the optimization framework that infers intervention parameters or targets directly from the data, validated on datasets with imperfect or partial intervention logs.

## Limitations
- Gaussian noise assumption limits applicability to non-Gaussian confounders
- Consistency guarantees rely on strong assumptions (interventional stability, I-σ-faithfulness) whose practical validity is unclear
- Contractivity constraint may restrict model expressiveness for highly cyclic systems

## Confidence
- **High Confidence:** The proposed differentiable framework is novel and the alternating optimization strategy is sound
- **Medium Confidence:** Empirical results show improvement over baselines, but the magnitude of gains and stability across varying conditions need further validation
- **Low Confidence:** Theoretical consistency under σ-separation is claimed but not fully verified in practice; the interplay between assumptions and real-world data is underexplored

## Next Checks
1. Test performance under non-Gaussian confounders to assess robustness of the noise-correlation mechanism
2. Evaluate sensitivity to the contractivity constraint by relaxing spectral normalization and measuring fixed-point stability
3. Apply the method to a real-world dataset with known confounders to verify consistency guarantees empirically