---
ver: rpa2
title: 'Listen to the Context: Towards Faithful Large Language Models for Retrieval
  Augmented Generation on Climate Questions'
arxiv_id: '2505.15633'
source_url: https://arxiv.org/abs/2505.15633
tags:
- claim
- faithfulness
- climategpt
- context
- faithful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of improving faithfulness in large
  language models (LLMs) used for retrieval-augmented generation (RAG) on climate-related
  questions. The authors propose an approach that evaluates and enhances the faithfulness
  of model outputs by focusing on the instruction fine-tuning (IFT) data used to train
  ClimateGPT, a specialized LLM for climate science.
---

# Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions

## Quick Facts
- **arXiv ID:** 2505.15633
- **Source URL:** https://arxiv.org/abs/2505.15633
- **Reference count:** 33
- **Primary result:** Improved claim support from 30% to 57% by filtering low-faithfulness training data for ClimateGPT

## Executive Summary
This paper addresses the critical challenge of improving faithfulness in retrieval-augmented generation (RAG) systems for climate science questions. The authors develop a novel approach to evaluate and enhance the faithfulness of large language models by analyzing the instruction fine-tuning data used to train ClimateGPT, a specialized LLM for climate-related tasks. By introducing a new faithfulness metric and systematically filtering out low-quality training data, they demonstrate significant improvements in the model's ability to generate responses that are properly supported by retrieved context.

## Method Summary
The authors create a faithfulness metric that evaluates whether model responses are supported by retrieved context for climate-related questions. They analyze the instruction fine-tuning data used to train ClimateGPT, identifying and excluding subsets with low faithfulness scores. This filtered dataset is used to create ClimateGPT Faithful+, which is then evaluated against the original model and additional datasets. The approach combines automated scoring with systematic data filtering to improve the alignment between generated claims and supporting evidence.

## Key Results
- Claim support increased from 30% to 57% using filtered training data
- Improvements validated on three additional datasets
- Results confirmed using alternative faithfulness metrics
- Approximately 20% of training data excluded based on low faithfulness scores

## Why This Works (Mechanism)
The approach works by directly addressing the root cause of unfaithful generations: training data that contains unsupported claims or poor context-response alignment. By measuring faithfulness at the data level rather than attempting to correct issues post-training, the model learns from higher-quality examples that demonstrate proper grounding in retrieved evidence.

## Foundational Learning

**Retrieval-Augmented Generation (RAG):** Why needed - Combines information retrieval with text generation for up-to-date responses; Quick check - Model retrieves relevant documents before generating answers.

**Instruction Fine-Tuning (IFT):** Why needed - Adapts pre-trained models to follow instructions and answer questions; Quick check - Model responds appropriately to diverse instruction formats.

**Faithfulness Metrics:** Why needed - Quantifies whether generated content is supported by retrieved context; Quick check - Automated scoring correlates with human judgments of factual consistency.

**ClimateGPT:** Why needed - Specialized LLM for climate science domain; Quick check - Model demonstrates domain-specific knowledge and terminology.

**Claim-Context Support:** Why needed - Ensures generated claims are grounded in retrieved evidence; Quick check - Generated responses can be verified against source documents.

## Architecture Onboarding

**Component Map:** Pre-trained LLM -> Instruction Fine-Tuning -> Faithfulness Evaluation -> Data Filtering -> Retrained Faithful+ Model

**Critical Path:** The faithfulness metric evaluation and data filtering process is critical, as it determines which training examples are retained and directly impacts the model's ability to generate well-supported responses.

**Design Tradeoffs:** The approach trades potential coverage of diverse training examples for improved quality and faithfulness. Excluding 20% of data may remove valuable information but improves overall alignment with supporting evidence.

**Failure Signatures:** Models may overfit to filtered data, become less diverse in responses, or miss nuanced information present in excluded examples. Faithfulness metrics may also miss context-dependent interpretations.

**3 First Experiments:**
1. Apply faithfulness metric to baseline model outputs to establish baseline claim support percentage
2. Filter training data and retrain model using only high-faithfulness examples
3. Evaluate retrained model on held-out test sets using both original and alternative faithfulness metrics

## Open Questions the Paper Calls Out
The study identifies several open questions regarding the generalizability of the approach to other domains and models, the potential biases introduced by data filtering, and the relationship between automated faithfulness scores and human judgments of response quality.

## Limitations
- Results limited to climate science domain and ClimateGPT model
- Automated faithfulness metrics may not capture all aspects of factual consistency
- Excluding 20% of training data may remove valuable complementary knowledge
- No human evaluation to verify automated metric accuracy or assess response utility

## Confidence

**Major Claim Clusters Confidence Assessment:**
- ClimateGPT Faithful+ improves faithfulness metrics: High confidence
- The faithfulness metric reliably measures claim-context support: Medium confidence
- Excluding low-faithfulness training data is an effective approach: Medium confidence
- Results generalize beyond climate science: Low confidence

## Next Checks

1. Conduct human evaluation studies to verify whether automated faithfulness scores align with human judgments of factual consistency
2. Test the filtering approach on general-purpose LLMs (e.g., LLaMA, Mistral) to assess domain generalizability
3. Investigate whether the excluded 20% of training data contains complementary knowledge that might be valuable despite lower faithfulness scores