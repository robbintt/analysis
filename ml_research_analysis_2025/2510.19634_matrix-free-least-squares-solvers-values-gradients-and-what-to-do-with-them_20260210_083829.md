---
ver: rpa2
title: 'Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With
  Them'
arxiv_id: '2510.19634'
source_url: https://arxiv.org/abs/2510.19634
tags:
- constraint
- least-squares
- null-space
- learning
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to differentiable least squares
  solvers, demonstrating their effectiveness in various machine learning applications.
  The authors develop custom gradients for least squares solvers, transforming them
  into differentiable operators like neural network layers.
---

# Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them

## Quick Facts
- arXiv ID: 2510.19634
- Source URL: https://arxiv.org/abs/2510.19634
- Authors: Hrittik Roy; Søren Hauberg; Nicholas Krämer
- Reference count: 29
- Key result: Differentiable least squares solvers achieve 5-10x speedup over unrolling loops while enabling constrained optimization for neural networks

## Executive Summary
This paper introduces differentiable least squares solvers as composable, matrix-free operators for machine learning. By developing custom gradients for the LSMR solver and implementing it in JAX, the authors create a framework that is significantly faster than unrolling solver iterations while maintaining exact gradients. The method enables constrained optimization of neural networks through the null-space method, achieving competitive results across diverse tasks including equivariance enforcement, weight sparsity, and conservativeness in generative models. Additionally, the backward pass of differentiable least squares is used to calibrate Gaussian processes, demonstrating over 10x speedup compared to marginal likelihood optimization while improving test performance.

## Method Summary
The authors develop custom gradients for least squares solvers, transforming them into differentiable operators that can be integrated into neural network architectures. They implement this using JAX with the LSMR solver, creating a matrix-free approach that computes exact gradients without unrolling the solver's iterative loop. The method leverages the implicit function theorem to compute gradients efficiently. For constrained optimization, they employ the null-space method, where constraints are encoded as linear equations and the null space is computed via least squares. The differentiable solver enables backpropagation through these constrained optimization steps. For Gaussian process calibration, they use the backward pass of the differentiable least squares to optimize kernel parameters by minimizing prediction errors on calibration data.

## Key Results
- Differentiable least squares solvers are 5-10x faster than unrolling solver iterations
- Achieved 90% weight sparsity on CIFAR-10 and 50% on ImageNet while maintaining accuracy
- Over 10x speedup for Gaussian process calibration with improved test performance compared to marginal likelihood optimization

## Why This Works (Mechanism)
The method works by making the least squares solver differentiable through custom gradient computation. Instead of treating the solver as a black box, the authors compute exact gradients using the implicit function theorem, which relates changes in the solution to changes in the input parameters. This allows backpropagation through the solver step. The matrix-free implementation means the solver only requires matrix-vector products rather than explicit matrix storage, making it scalable. For constrained optimization, the null-space method encodes constraints as linear equations, and the differentiable solver finds solutions in the null space that satisfy these constraints while optimizing the primary objective. The backward pass provides sensitivity information that can be used for calibration tasks like Gaussian process kernel optimization.

## Foundational Learning
- **Least squares problems**: Minimize ||Ax - b||²; fundamental optimization problem in linear algebra and machine learning. Needed because the entire framework builds on solving these efficiently. Quick check: Verify that the normal equations A^T A x = A^T b characterize the solution.
- **Implicit function theorem**: Relates derivatives of implicitly defined functions to derivatives of their defining equations. Needed for computing gradients through the solver without unrolling iterations. Quick check: Confirm that the theorem allows computing dx/dy from F(x,y)=0 without explicit x(y) formula.
- **Null-space method**: Solves constrained optimization by projecting onto the null space of constraint matrix. Needed for enforcing structural constraints on neural networks. Quick check: Verify that any solution can be written as particular solution plus linear combination of null space basis vectors.
- **Matrix-free methods**: Compute matrix-vector products without forming the full matrix. Needed for scalability to large problems. Quick check: Confirm that iterative solvers like LSMR only require matrix-vector products.
- **Automatic differentiation**: Computes exact gradients through computational graphs. Needed to make the solver composable with neural networks. Quick check: Verify that custom VJPs can replace default differentiation rules.
- **LSMR algorithm**: Iterative method for sparse least squares problems. Needed as the underlying solver implementation. Quick check: Confirm that LSMR solves min ||Ax - b||² + λ²||x||² for regularization parameter λ.

## Architecture Onboarding

**Component map**: Input matrix A and vector b -> LSMR solver -> Solution x -> Custom VJP computation -> Gradients w.r.t. A and b

**Critical path**: Forward pass computes least squares solution using LSMR; backward pass uses implicit function theorem to compute exact gradients without unrolling iterations.

**Design tradeoffs**: The matrix-free approach trades memory efficiency for potential numerical instability; custom gradients add implementation complexity but provide exact derivatives versus approximate unrolling; the method requires differentiable matrix-vector products which may not be available for all operators.

**Failure signatures**: Ill-conditioned matrices can cause numerical instability; non-differentiable matrix-vector products prevent gradient computation; approximate matrix-vector products may degrade solution quality; large constraint sets can make null-space computation expensive.

**First experiments**:
1. Verify gradient correctness by comparing custom VJP against finite differences for small least squares problems
2. Test speed comparison between matrix-free differentiable solver and unrolled iterations across varying problem sizes
3. Validate constrained optimization by checking that solutions satisfy linear constraints to within numerical tolerance

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on very large-scale problems or with different solver implementations remains unclear
- Method's behavior with ill-conditioned matrices or numerical instabilities could impact practical utility
- Claims about robustness to approximate matrix-vector products require broader validation across problem domains

## Confidence
- Speed advantages of matrix-free approach: High
- Effectiveness for constrained optimization tasks: Medium
- Gaussian process calibration results: Medium
- Breadth of applications and edge case behavior: Medium

## Next Checks
1. Evaluate the method on larger-scale problems (e.g., ImageNet-scale datasets or higher-resolution images) to assess scalability and computational efficiency
2. Test the robustness of the differentiable solver with increasingly ill-conditioned matrices and quantify the impact on training stability and solution quality
3. Compare the performance of the differentiable least squares approach with other differentiable linear algebra methods (e.g., differentiable SVD, Cholesky) on the same set of tasks to establish relative strengths and weaknesses