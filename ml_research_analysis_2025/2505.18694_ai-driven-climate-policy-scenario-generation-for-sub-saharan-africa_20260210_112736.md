---
ver: rpa2
title: AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa
arxiv_id: '2505.18694'
source_url: https://arxiv.org/abs/2505.18694
tags:
- climate
- energy
- policy
- scenarios
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes using generative AI, specifically large language\
  \ models (LLMs), to create climate policy scenarios for Sub-Saharan Africa. Traditional\
  \ methods are slow and struggle with the region\u2019s complex energy and climate\
  \ challenges."
---

# AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa

## Quick Facts
- arXiv ID: 2505.18694
- Source URL: https://arxiv.org/abs/2505.18694
- Reference count: 40
- LLM-based RAG pipeline generates climate policy scenarios with 88% expert validation rate

## Executive Summary
This paper proposes using generative AI, specifically large language models (LLMs), to create climate policy scenarios for Sub-Saharan Africa. Traditional methods are slow and struggle with the region's complex energy and climate challenges. The approach uses retrieval-augmented generation (RAG) with UN climate documents as a knowledge base to generate diverse, plausible scenarios. Of 34 generated responses, 30 (88%) were validated by experts. Evaluation using faithfulness, answer relevancy, and context utilization metrics showed high performance, with human experts and LLMs scoring the outputs above 70% across all metrics. Spearman correlation analysis confirmed strong alignment between human and model evaluations. The method offers a scalable, AI-driven framework for climate policy planning in data-constrained regions.

## Method Summary
The approach uses retrieval-augmented generation with UN COP documents as a knowledge base. Documents were processed with PyPDFLoader, chunked using RecursiveCharacterTextSplitter (1000/100), embedded with nomic-embed-text, and stored in a Chroma vector database. Llama3.2-3B generated scenarios via zero-shot prompts with temperature=0, followed by llama3-8B for implementation roadmaps. Evaluation used the RAGAS framework with human expert, gemma2-2B, and mistral-7B evaluators, measuring faithfulness, answer relevancy, and context utilization.

## Key Results
- 88% expert validation rate (30 of 34 generated scenarios passed validation)
- Human and LLM evaluators scored outputs above 70% across all RAGAS metrics
- Strong Spearman correlation for faithfulness (ρ > 0.8) but weak correlation for answer relevancy (ρ ≈ 0.31)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation grounds policy scenarios in authoritative climate documents, reducing hallucination while maintaining relevance.
- Mechanism: The RAG pipeline retrieves contextually relevant passages from 94 UN COP documents via embedding similarity before generation. This retrieved context constrains the LLM's output space, providing factual anchors that the model must synthesize rather than freely generating from potentially biased or incomplete parametric knowledge.
- Core assumption: UN COP documents contain sufficient regional specificity for Sub-Saharan Africa to generate locally relevant scenarios.
- Evidence anchors:
  - [abstract] "The approach uses retrieval-augmented generation (RAG) with UN climate documents as a knowledge base to generate diverse, plausible scenarios."
  - [section 2.1] "We collected 94 COP documents... Preprocessing involved chunking with RecursiveCharacterTextSplitter (chunk size: 1000, overlap: 100) and embedding stored in a chroma vector database using ollama embeddings"
  - [corpus] Related work on low-resource AI in Africa (Federated learning study, FMR=0.42) highlights data scarcity challenges that RAG directly addresses.
- Break condition: If query topics fall outside COP document scope (e.g., country-specific regulatory details), retrieved context becomes generic, reducing faithfulness scores.

### Mechanism 2
- Claim: Automated LLM-based evaluation can proxy human expert judgment for faithfulness assessment but diverges on subjective relevance interpretation.
- Mechanism: Multiple evaluators (human expert, gemma2-2B, mistral-7B) score generated scenarios on shared metrics. Spearman correlation analysis reveals that LLM evaluators align with humans on factual consistency (faithfulness ρ > 0.8) but poorly capture answer relevancy judgments (ρ ≈ 0.31), likely because LLMs optimize for surface-level prompt-addressing while humans assess deeper policy utility.
- Core assumption: High correlation on faithfulness indicates LLM evaluators can reliably detect factual grounding without human oversight.
- Evidence anchors:
  - [abstract] "Spearman correlation analysis confirmed strong alignment between human and model evaluations."
  - [section 3.2, Table 2] "Spearman correlation results show strong alignment between LLMs and the human evaluator for faithfulness. However, alignment is weak for answer relevancy"
  - [corpus] Limited corpus evidence on LLM-as-evaluator reliability in policy domains; most related work focuses on medical imaging segmentation.
- Break condition: When evaluating nuanced policy trade-offs requiring domain expertise beyond the LLM evaluator's training distribution, automated scores become unreliable proxies.

### Mechanism 3
- Claim: Zero-shot prompting with temperature=0 produces sufficiently diverse scenarios while maintaining consistency for validation.
- Mechanism: The llama3.2-3B model receives structured prompts specifying policy dimensions (incentives, challenges, mechanisms) without examples. Temperature zero ensures reproducibility across runs. Follow-up prompts using llama3-8B extract implementation roadmaps, creating a two-stage generation pipeline that separates scenario ideation from operationalization.
- Core assumption: Zero-shot generation captures sufficient domain knowledge from the model's pre-training on climate-policy-adjacent texts.
- Evidence anchors:
  - [section 2.2] "We used llama3.2-3B to generate scenarios with 34 zero-shot prompts, via a RAG pipeline... We set temperature to zero for consistency."
  - [section 4] "Limitations include reliance on zero-shot prompting, which may limit response depth"
  - [corpus] No direct corpus evidence on zero-shot climate policy generation; gap in literature.
- Break condition: Prompts requiring deep regional regulatory knowledge (e.g., specific national grid codes) produce generic responses that validators reject.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Core architecture separating knowledge retrieval from generation, enabling grounded scenario creation without fine-tuning.
  - Quick check question: Can you explain why RAG reduces hallucination compared to pure LLM generation?

- Concept: Embedding-based semantic search
  - Why needed here: Enables chunk retrieval from COP documents; chunk size and overlap parameters directly affect context quality.
  - Quick check question: What happens to retrieval quality if chunk size is too small versus too large?

- Concept: LLM evaluation metrics (faithfulness, answer relevancy, context utilization)
  - Why needed here: RAGAs framework provides quantitative assessment without ground-truth labels; understanding metric definitions is critical for interpreting results.
  - Quick check question: Why might a response score high on faithfulness but low on answer relevancy?

## Architecture Onboarding

- Component map:
  - 94 COP PDFs -> PyPDFLoader -> RecursiveCharacterTextSplitter (1000/100) -> Chroma vector DB with nomic-embed-text embeddings
  - llama3.2-3B with RAG retrieval -> zero-shot prompts -> follow-up prompts (llama3-8B)
  - RAGAs framework -> human expert + gemma2-2B + mistral-7B evaluators -> Spearman correlation analysis

- Critical path:
  1. Document preprocessing quality determines retrieval relevance
  2. Retrieval relevance determines generation faithfulness
  3. Prompt specificity determines validation pass rate (88% achieved)
  4. Evaluator alignment determines trust in automated scoring

- Design tradeoffs:
  - llama3.2-3B chosen for efficiency vs. larger models; may sacrifice depth
  - Temperature=0 for reproducibility vs. diversity
  - Zero-shot for scalability vs. few-shot for quality
  - Automated evaluation for throughput vs. human evaluation for nuanced judgment

- Failure signatures:
  - Low faithfulness + high context utilization: Retrieved context irrelevant to prompt
  - High faithfulness + low answer relevancy: Model too conservative, avoids direct claims
  - High variability (SD > 20% of mean): Evaluator unreliable for that metric
  - Validator rejection citing "not a policy scenario": Prompt framing issue, not model capability

- First 3 experiments:
  1. Ablate chunk size (500 vs. 1000 vs. 1500) and measure faithfulness/context utilization tradeoffs
  2. Compare zero-shot vs. 2-shot prompting on validation pass rate and response depth
  3. Test regional specificity by adding national climate reports to corpus and measuring faithfulness delta for country-specific prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning LLMs on region-specific corpora effectively mitigate the "western skew" and contextual biases present in base model weights?
- Basis in paper: [explicit] The Discussion notes LLMs risk introducing a "western skew" and suggests future work should fine-tune on regional corpora to better reflect local perspectives.
- Why unresolved: This study relied on general-purpose models (llama3.2-3B) rather than regionally specialized ones.
- What evidence would resolve it: A comparative study measuring cultural alignment and bias scores between base and regionally fine-tuned models.

### Open Question 2
- Question: To what extent do advanced prompt engineering techniques (e.g., few-shot, chain-of-thought) improve the depth of generated scenarios over zero-shot prompting?
- Basis in paper: [explicit] The Discussion lists reliance on zero-shot prompting as a limitation that may "limit response depth" and suggests exploring advanced prompt engineering.
- Why unresolved: The study used a fixed zero-shot approach; the trade-offs of complexity vs. depth were not tested.
- What evidence would resolve it: Ablation studies comparing zero-shot outputs against advanced prompting outputs using expert depth ratings.

### Open Question 3
- Question: Can alternative automated evaluation frameworks better align with human judgment on subjective metrics like "answer relevancy"?
- Basis in paper: [inferred] Table 2 shows a weak Spearman correlation (0.311) between human and LLM evaluators for answer relevancy, despite high faithfulness alignment.
- Why unresolved: The paper concludes that LLMs struggle with subjective assessments, but does not test methods to close this specific evaluation gap.
- What evidence would resolve it: Testing new evaluator models or ensemble methods to see if the correlation coefficient for relevancy improves significantly.

## Limitations

- Reliance on zero-shot prompting may limit response depth for complex policy scenarios
- Automated LLM evaluation shows weak alignment with human judgment on subjective metrics like answer relevancy
- Generalizability to other regions is uncertain due to heavy reliance on UN COP documents specific to Sub-Saharan Africa

## Confidence

- High confidence: Retrieval-augmented generation reduces hallucination when authoritative documents are available and relevant to the query domain.
- Medium confidence: Automated LLM evaluation can reliably proxy human judgment for factual consistency (faithfulness) but not for subjective policy relevance assessment.
- Low confidence: Zero-shot prompting without examples produces sufficiently diverse and actionable scenarios across all climate policy dimensions.

## Next Checks

1. Chunk size ablation study: Test retrieval quality and faithfulness scores across chunk sizes (500, 1000, 1500 tokens) to identify optimal balance between context richness and retrieval precision.
2. Human-AI evaluation gap analysis: For scenarios where Spearman correlation on answer relevancy falls below 0.5, conduct qualitative analysis to determine whether LLM evaluators systematically miss policy nuance or if human experts are applying inconsistent standards.
3. Regional transferability test: Apply the same RAG pipeline to climate policy documents from a different region (e.g., ASEAN) and measure changes in faithfulness scores to assess document corpus dependency.