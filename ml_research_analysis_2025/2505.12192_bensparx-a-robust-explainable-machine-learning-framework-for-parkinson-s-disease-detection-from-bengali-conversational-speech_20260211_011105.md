---
ver: rpa2
title: 'BenSParX: A Robust Explainable Machine Learning Framework for Parkinson''s
  Disease Detection from Bengali Conversational Speech'
arxiv_id: '2505.12192'
source_url: https://arxiv.org/abs/2505.12192
tags:
- features
- feature
- speech
- parkinson
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces BenSParX, the first Bengali conversational\
  \ speech dataset for Parkinson\u2019s disease detection, along with a robust and\
  \ explainable machine learning framework. It addresses the lack of culturally inclusive\
  \ speech datasets for PD detection in low-resource languages."
---

# BenSParX: A Robust Explainable Machine Learning Framework for Parkinson's Disease Detection from Bengali Conversational Speech

## Quick Facts
- arXiv ID: 2505.12192
- Source URL: https://arxiv.org/abs/2505.12192
- Authors: Riad Hossain; Muhammad Ashad Kabir; Arat Ibne Golam Mowla; Animesh Chandra Roy; Ranjit Kumar Ghosh
- Reference count: 40
- Key outcome: Introduces BenSParX, the first Bengali conversational speech dataset for PD detection, achieving 95.77% accuracy and 95.57% F1 score

## Executive Summary
BenSParX introduces the first Bengali conversational speech dataset for Parkinson's disease detection, addressing the critical gap in culturally inclusive speech-based diagnostics for low-resource languages. The framework combines diverse acoustic features with multi-stage feature selection and SHAP-based explainability to achieve state-of-the-art performance. Evaluated on 120 participants, the system demonstrates 95.77% accuracy and 0.982 AUC-ROC, with external validation confirming superior generalizability across multiple non-Bengali datasets. This work advances equitable AI-driven diagnostics for underrepresented linguistic communities.

## Method Summary
The framework extracts 41 acoustic features from speech segments, including fundamental frequency, formants, spectral centroid, and energy-based measures. A multi-stage feature selection process using Recursive Feature Elimination and Random Forest importance ranking identifies the most discriminative features. SHAP values provide explainability by quantifying feature contributions to model predictions. The system employs Random Forest and Support Vector Machine classifiers, with hyperparameter optimization through grid search and cross-validation. The BenSParX dataset consists of 120 participants providing 40-sentence conversational speech recordings.

## Key Results
- Achieves 95.77% accuracy, 95.57% F1 score, and 0.982 AUC-ROC on held-out test data
- Outperforms existing PD detection models on multiple non-Bengali datasets (Saarbruecken, Parkinson-MLII, mPower)
- Demonstrates superior generalization capability with external validation on diverse speech corpora

## Why This Works (Mechanism)
The framework's success stems from comprehensive feature engineering capturing multiple acoustic dimensions, rigorous feature selection eliminating noise, and ensemble modeling that leverages complementary classifier strengths. The SHAP-based explainability provides transparency in feature contributions, while the structured dataset collection protocol ensures consistency across recordings.

## Foundational Learning
- Acoustic feature extraction (why needed: captures subtle speech changes in PD; quick check: verify feature extraction produces expected values across known samples)
- Feature selection methods (why needed: reduces dimensionality and noise; quick check: compare model performance with and without feature selection)
- Explainable AI techniques (why needed: builds clinical trust; quick check: validate SHAP explanations align with domain expertise)

## Architecture Onboarding

Component map: Data collection -> Feature extraction -> Feature selection -> Model training -> SHAP explanation -> Validation

Critical path: Speech recording -> Acoustic feature extraction (41 features) -> RFE-based feature selection -> Random Forest classification -> SHAP interpretation

Design tradeoffs: Balanced dataset creation vs. real-world heterogeneity, computational efficiency vs. feature comprehensiveness, explainability vs. prediction accuracy

Failure signatures: Poor feature quality from low signal-to-noise ratio recordings, overfitting from insufficient feature selection, biased predictions from unbalanced training data

First experiments:
1. Test feature extraction pipeline on known PD and healthy speech samples
2. Validate feature selection process by comparing selected features against clinical literature
3. Perform cross-validation to assess model stability and generalization

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the BenSParX framework be adapted to support disease staging and progression monitoring in Parkinson's disease?
- Basis in paper: Section 5.3 states that "collecting longitudinal data with detailed clinical metadata... could enable the model to support PD staging and progression monitoring."
- Why unresolved: The current dataset is limited to patients within 1.5 years of diagnosis and lacks the detailed severity metadata (e.g., UPDRS scores) necessary for training models to distinguish between disease stages.
- What evidence would resolve it: A longitudinal evaluation of the model on a dataset annotated with disease severity scores, demonstrating a correlation between feature shifts and clinical progression.

### Open Question 2
- Question: How does the variability of vocal symptoms caused by medication cycles or fatigue affect the stability and accuracy of the detection model?
- Basis in paper: Section 5.3 notes that "incorporating multiple... recording sessions... under varying medication states, would allow modeling of intra-speaker variability."
- Why unresolved: The study relies on a single short recording session per participant, which fails to capture the temporal dynamics of symptoms that fluctuate throughout the day or in response to medication.
- What evidence would resolve it: Experimental results comparing model predictions across multiple recordings from the same subjects in different medication states (e.g., ON vs. OFF states).

### Open Question 3
- Question: Does the framework maintain robust performance across diverse Bengali dialects and socio-economic backgrounds?
- Basis in paper: Section 5.3 suggests that "expanding the dataset to include participants from diverse regional, socio-economic, and dialectal backgrounds would improve the model's generalizability."
- Why unresolved: The current dataset is drawn from a relatively homogeneous demographic, potentially introducing bias that affects performance on the broader, diverse Bengali-speaking population.
- What evidence would resolve it: A stratified cross-validation study showing consistent performance metrics across distinct regional dialects and demographic groups.

### Open Question 4
- Question: Can the integration of multimodal data (e.g., handwriting, gait) significantly improve detection accuracy compared to speech analysis alone?
- Basis in paper: Section 5.3 proposes that "integrating multimodal data sources... could lead to more comprehensive and accurate PD detection systems."
- Why unresolved: The current framework is strictly audio-based and does not account for non-vocal motor symptoms that might provide complementary diagnostic signals.
- What evidence would resolve it: A comparative analysis showing improved F1 score or AUC-ROC when the audio model is fused with handwriting or gait classifiers.

## Limitations
- Small sample size (120 participants) limits statistical power and generalizability
- Single recording session per participant fails to capture symptom variability across medication cycles
- Homogeneous demographic composition may introduce bias affecting performance on diverse populations

## Confidence
- Dataset creation and technical implementation: High
- Performance metrics on held-out data: Medium (limited by sample size)
- Cross-linguistic generalizability: Low (external validation uses different languages)
- Clinical utility and real-world deployment readiness: Medium (requires further validation)

## Next Checks
1. Conduct a longitudinal study with a larger, more diverse cohort to assess model stability and performance across disease progression stages
2. Perform demographic subgroup analyses to identify and mitigate potential biases in model predictions
3. Test the framework's performance on spontaneous conversational speech in real clinical settings, rather than structured elicitation protocols