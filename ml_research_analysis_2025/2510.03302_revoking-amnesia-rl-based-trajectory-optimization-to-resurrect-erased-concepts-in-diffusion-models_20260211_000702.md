---
ver: rpa2
title: 'Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts
  in Diffusion Models'
arxiv_id: '2510.03302'
source_url: https://arxiv.org/abs/2510.03302
tags:
- concept
- erasure
- arxiv
- preprint
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Concept erasure in diffusion models works by biasing sampling trajectories
  away from target concepts rather than truly removing them, making erasure fundamentally
  reversible. RevAm addresses this by formulating concept recovery as a reinforcement
  learning problem, using Group Relative Policy Optimization (GRPO) to dynamically
  steer the denoising velocity field during sampling without modifying model weights.
---

# Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models

## Quick Facts
- **arXiv ID:** 2510.03302
- **Source URL:** https://arxiv.org/abs/2510.03302
- **Reference count:** 31
- **Primary result:** RevAm achieves 10× speedup and superior concept resurrection compared to baselines by using RL to steer denoising trajectories.

## Executive Summary
Current concept erasure methods in diffusion models do not truly delete knowledge but instead bias sampling trajectories away from target concepts. This paper demonstrates that such "amnesia" is fundamentally reversible by formulating concept recovery as a reinforcement learning problem. The proposed RevAm method uses Group Relative Policy Optimization (GRPO) to dynamically steer the denoising velocity field during sampling, achieving superior concept resurrection fidelity while reducing computational time by 10× compared to baseline methods across multiple concept categories including NSFW content, artistic styles, and abstract relationships.

## Method Summary
RevAm addresses concept erasure by formulating recovery as an RL problem where a policy network outputs actions (ρ, φ) to scale and rotate the velocity field during sampling. The method intercepts the base model's velocity prediction v(x_t, c, t) and applies steering transformations before passing to the sampler. Using GRPO, the policy learns to maximize rewards from heterogeneous evaluators (classifiers and VLMs) without modifying model weights. The approach achieves efficient recovery through dynamic trajectory optimization rather than brute-force search.

## Key Results
- Achieves 10× speedup compared to baseline erasure reversal methods
- Superior Attack Success Rate (ASR) across multiple concept categories (NSFW, violence, artistic styles, abstract relationships)
- Reduces Time to Recovery (TTR) while maintaining image quality
- Demonstrates vulnerability in current LoRA-based erasure techniques that rely on trajectory manipulation

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Bias as the Erasure Primitive
Concept erasure methods do not delete knowledge but bias the velocity field to deflect sampling trajectories away from target concept manifolds. LoRA-based weight edits alter the predicted velocity vector, causing systematic angular deviation and magnitude scaling relative to the original model, redirecting the denoising process without touching underlying concept representations.

### Mechanism 2: Semantic Subspace Rotation via Classifier-Free Guidance
Rotating velocity vectors within a 2D subspace spanned by the current velocity and the classifier-free guidance signal suffices to re-enter erased concept regions. The policy constructs an orthonormal basis where the guidance signal provides the semantic axis, then rotates within this plane while scaling magnitude.

### Mechanism 3: Group Relative Policy Optimization for Sparse Reward Credit Assignment
GRPO's group-relative advantage computation converts heterogeneous, trajectory-level rewards into stable per-step policy updates without requiring intermediate supervision. For each prompt, generate multiple rollouts, evaluate with reward models, then compute advantages that update the policy to favor high-advantage actions.

## Foundational Learning

- **Concept: Flow Matching / Rectified Flow**
  - Why needed here: RevAm operates directly on the velocity field v(x_t, c, t) defined by flow matching, not on DDPM-style noise predictions.
  - Quick check question: Given a velocity field v(x_t, t) and a sampler D, what is the update rule for x_{t−1}? (Answer: x_{t−1} = D(x_t, v(x_t, c, t), t))

- **Concept: Classifier-Free Guidance (CFG)**
  - Why needed here: RevAm uses the CFG signal g_t = v(x_t, c, t) − v(x_t, ∅, t) to define the semantic rotation axis.
  - Quick check question: Why does CFG amplify semantic content relative to unconditional generation? (Answer: It moves the prediction in the direction from unconditional toward conditional, emphasizing prompt-relevant features)

- **Concept: Policy Gradient with Clipped Surrogate Objectives (PPO-family)**
  - Why needed here: GRPO is a variant of PPO-style RL. Understanding clipping, KL regularization, and advantage estimation is necessary to debug training instability.
  - Quick check question: In the clipped objective min(ρ·A, clip(ρ, 1−ε, 1+ε)·A), what happens when the policy ratio ρ >> 1+ε and A > 0? (Answer: The gradient is zeroed by clipping, preventing over-aggressive updates)

## Architecture Onboarding

- **Component map**: Policy Network → Velocity Field Wrapper → Rollout Engine → Reward Aggregator → GRPO Optimizer

- **Critical path**:
  1. Initialize noisy latents x_T ~ N(0, I) for G rollouts
  2. For each timestep t → 0: sample actions (ρ^i, φ^i) ~ π_old, apply steering, update latents via sampler
  3. Collect final images {x^i_0}, evaluate rewards, compute advantages A_i
  4. Update π_θ via GRPO objective; repeat for N ≤ 15 iterations

- **Design tradeoffs**:
  - Action space bounds: Tighter bounds ensure stable generation but may limit recovery for aggressively erased concepts
  - Number of rollouts G: G=3 balances exploration vs. compute; increasing G improves advantage estimation but linearly increases cost
  - Reward model choice: VLMs provide semantic understanding but introduce latency and API dependency; classifiers are fast but narrow

- **Failure signatures**:
  - Policy collapse to identity: If π_θ converges to (ρ=1, φ=0) for all steps, check reward sparsity or advantage scaling
  - Image quality degradation: Excessive magnitude scaling causes artifacts; tighten bounds or add perceptual quality reward
  - Slow convergence: If iterations exceed N=15 without success, inspect whether guidance signal g_t is near-zero

- **First 3 experiments**:
  1. Sanity check on unmodified Flux: Run RevAm with empty erasure. Expected: policy learns trivial (ρ≈1, φ≈0) actions; rewards saturate early
  2. Single-concept ablation on AC-erased "nudity": Compare ρ-only, φ-only, and full (ρ+φ) configurations. Expected: full method achieves highest ASR with fewest iterations
  3. Cross-architecture transfer test: Apply a policy trained on AC-erased Flux to ESD-erased Flux without retraining. Expected: policies may not transfer due to different velocity field perturbations

## Open Questions the Paper Calls Out

- Can defensive mechanisms be designed that specifically resist trajectory manipulation while preserving model utility? The paper explicitly states the need for "novel defensive mechanisms" that operate "beyond trajectory manipulation."

- What are the theoretical limits of erasure reversibility in diffusion models? The conclusion motivates future research into "the theoretical limits of erasure reversibility."

- Does the RevAm framework generalize effectively to non-Flux architectures, such as UNet-based models? Appendix F notes that generalizability to other diffusion models "needs further validation."

## Limitations

- Success fundamentally relies on attacker's knowledge of erasure method parameters and prompt targeting, making it an oracle-based attack rather than black-box threat
- Experimental evidence limited to specific erasure methods and controlled prompt settings, not proving universal reversibility
- Performance on non-Flux architectures (UNet-based models) remains unverified

## Confidence

- Core claim that trajectory steering can resurrect erased concepts: **Medium**
- Claim that erasure is "fundamentally reversible": **Low** (extrapolates from specific LoRA methods to all possible erasure techniques)
- Computational efficiency claim (10× speedup): **High** (directly measured against baseline methods)

## Next Checks

1. Test RevAm against unknown erasure configurations by training on one erasure method and evaluating on different methods to assess method transferability limits
2. Implement a black-box variant that iteratively discovers the concept and prompt through gradient-free search rather than assuming oracle knowledge
3. Evaluate RevAm's performance when the guidance signal g_t is adversarially perturbed during erasure, testing the assumption that CFG remains intact post-erasure