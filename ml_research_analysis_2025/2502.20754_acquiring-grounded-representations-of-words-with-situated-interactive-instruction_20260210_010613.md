---
ver: rpa2
title: Acquiring Grounded Representations of Words with Situated Interactive Instruction
arxiv_id: '2502.20754'
source_url: https://arxiv.org/abs/2502.20754
tags:
- agent
- learning
- knowledge
- object
- perceptual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an interactive instruction-based approach for
  acquiring grounded representations of language. The agent learns perceptual, spatial,
  semantic, and procedural knowledge through mixed-initiative dialog with a human
  instructor while performing tasks.
---

# Acquiring Grounded Representations of Words with Situated Interactive Instruction

## Quick Facts
- arXiv ID: 2502.20754
- Source URL: https://arxiv.org/abs/2502.20754
- Reference count: 3
- One-line primary result: Agent acquires 93-100% accuracy for nouns/adjectives, prepositions, and action verbs through interactive instruction with a table-top robot.

## Executive Summary
This paper presents an interactive instruction-based approach for acquiring grounded representations of language. The agent learns perceptual, spatial, semantic, and procedural knowledge through mixed-initiative dialog with a human instructor while performing tasks. The agent uses Soar's cognitive architecture to integrate perception, action, and language processing, storing learned knowledge in semantic and episodic memories. Evaluation shows fast, incremental learning: the agent acquired 93-100% accuracy for nouns/adjectives (1-13 examples), prepositions (3.17 examples), and action verbs (1.26 examples) through interactive instruction with a table-top robot. The approach demonstrates efficient learning by gathering more examples for harder concepts while minimizing examples for easily learned concepts.

## Method Summary
The agent uses Soar's cognitive architecture to learn grounded representations of language through mixed-initiative interaction with a human instructor. The system integrates perception, action, and language processing to learn nouns/adjectives (perceptual), prepositions (spatial), and action verbs (procedural). The agent employs a Link-Grammar parser for natural language processing, a Kinect-based perception system with KNN classifiers for object recognition, and Soar's learning mechanisms (chunking, semantic memory, episodic memory) to store acquired knowledge. Learning occurs through impasse-driven knowledge acquisition, where the agent identifies knowledge gaps and requests specific instructions from the instructor. For procedural verbs, the agent uses retrospective projection with episodic memory to generalize from single demonstrations. The approach is evaluated on a table-top robot manipulating foam blocks with varying colors, sizes, and shapes.

## Key Results
- Achieved 93-100% accuracy for nouns/adjectives, prepositions, and action verbs
- Required 1-13 examples to learn nouns/adjectives, averaging 3.17 examples for prepositions
- Required only 1.26 examples on average to learn action verbs
- Response time for learning tasks was under 2 seconds
- Successfully handled mixed-initiative interaction with human instructors

## Why This Works (Mechanism)

### Mechanism 1: Impasse-Driven Knowledge Acquisition
Interactive learning efficiency is significantly enhanced when the agent identifies and explicitly resolves knowledge gaps via structural "impasses" rather than relying solely on instructor-driven examples. The system monitors the translation of natural language into internal representations. If a word cannot be mapped to a known concept (Phase 3) or an action cannot be executed (Phase 4), a processing impasse occurs in the Soar architecture. This impasse automatically spawns a substate with a specific goal (e.g., "acquire mapping for orange"), prompting the agent to query the instructor for the missing specific knowledge. The system fails if the instructor provides a concept that relies on a perceptual feature the agent cannot extract (e.g., asking to learn "soft" when the vision system only extracts color/shape).

### Mechanism 2: Retrospective Projection for Procedural Verb Learning
Complex action sequences (verbs) can be generalized from single demonstrations by storing the experience in episodic memory and "replaying" it with internal action models to create causal rules. When taught a new verb, the agent executes primitive actions step-by-step. These events are stored in Episodic Memory. Once the goal is achieved, the agent retrieves the initial state and uses internal action models to simulate the executed actions. This simulation establishes a causal link between the context, the action, and the result. Chunking then compiles this trace into a general production rule for the new verb. If the primitive action models are incomplete (e.g., failing to predict that picking up an object changes the "hand-empty" status), the derived rule will be invalid.

### Mechanism 3: Layered Indirection for Perceptual Grounding
Decoupling linguistic terms from raw perceptual features via "perceptual symbols" allows for flexible concept association and synonym handling. The agent does not map the word "red" directly to RGB values. Instead, it maps "red" to a symbolic ID (e.g., `R43`) in Semantic Memory. This ID is then associated with a region in the feature space by the perceptual classifier (KNN). This abstraction layer allows the system to handle synonyms (mapping different words to `R43`) or polysemy without rewriting the low-level perceptual logic. If the KNN classifier is trained on inconsistent or overlapping feature distributions (e.g., lighting changes making orange look red), the symbolic mapping becomes unstable.

## Foundational Learning

- **Concept: Impasses and Substates**
  - **Why needed here:** This is the trigger for all learning in this architecture. Unlike error-backpropagation in neural nets, this system learns when it "gets stuck" (impasse) and creates a temporary problem space (substate) to solve the gap.
  - **Quick check question:** If an agent encounters a word it doesn't know, does it update weights or spawn a new goal context?

- **Concept: Chunking (Explanation-Based Learning)**
  - **Why needed here:** This is how the agent converts a one-time experience (episodic trace) into a permanent, general skill (production rule). It is the mechanism for "fast" learning.
  - **Quick check question:** Can the agent generalize a rule for "move" after seeing it done just once with a specific red triangle?

- **Concept: Semantic vs. Episodic Memory**
  - **Why needed here:** The agent distinguishes between static facts (red = color, definitions of prepositions) stored in Semantic Memory, and time-stamped experiences (the specific sequence of moving an object) stored in Episodic Memory.
  - **Quick check question:** Where does the agent store the definition of "left of" versus the memory of the last time it moved an object?

## Architecture Onboarding

- **Component map:** Link-Grammar Parser (Text) -> Syntactic Structure -> NL Comprehension -> Grounded Goal -> Execution/Impasse -> Interaction -> Learning -> Action/Response
- **Critical path:** Perception: Raw data -> Symbolic Object Representation -> Comprehension: NL Parse + Semantic Retrieval -> Grounded Goal -> Execution/Impasse: If knowledge missing -> Interaction -> Learning (Chunking/Storage)
- **Design tradeoffs:**
  - *Engineered Features vs. Raw Data:* The system uses pre-defined feature extractors (color, shape). This allows rapid, 1-shot learning but fails on undefined properties (texture, weight).
  - *Symbolic vs. Distributed Representation:* The agent uses discrete symbols (`R43`) for grounding. This ensures precision but may lack the robustness of vector embeddings for ambiguous inputs.
- **Failure signatures:**
  - *Runaway Interactions:* If an impasse is not resolved (e.g., instructor gives an answer the parser cannot process), the agent may get stuck in a loop asking the same question.
  - *Perceptual Aliasing:* If two distinct objects map to the same perceptual symbol due to poor lighting or camera angle, the agent will treat them as identical, confusing the semantic mapping.
- **First 3 experiments:**
  1. **Noun Acquisition Validation:** Place a colored block in view. Instruct "This is [color]." Verify the Semantic Memory creates a `Word -> Symbol` link and the KNN updates its boundary.
  2. **Impasse Triggering:** Issue a command "Lift the [unknown object]." Verify the agent halts execution and generates a specific query (e.g., "What is [unknown object]?") rather than failing silently.
  3. **Verb Generalization:** Teach "Store" using one object. Immediately test with a novel object of different color/shape. Verify the agent applies the procedural rule (chunk) learned from the first instance without re-training.

## Open Questions the Paper Calls Out

### Open Question 1
How can the agent detect and recover from instruction errors or suboptimal examples provided by a novice instructor? The authors state in Section 11, "We currently assume that the instructor has perfect information... However, this assumption does not hold... for novice instructors." They list "recovering from instruction error" as a specific focus of future work. Handling errors requires mechanisms for the agent to resolve references to past events and integrate reinforcement learning or statistical methods to identify and unlearn "bad examples."

### Open Question 2
How can the agent balance physical interaction with human interaction to resolve perceptual impasses caused by occlusion? Section 11 notes that a single perspective "occludes several features" and poses the specific research question: "what motivates further investigation [via physical manipulation] as opposed to further interaction with the human?" The current system relies on a static overhead view. Resolving this requires developing a cost-benefit analysis mechanism within the agent's decision procedure to choose between using the robot arm to gather more data (active perception) or simply asking the instructor for the information.

### Open Question 3
Can the system acquire spatial prepositions involving multiple reference objects (ternary relations) or contact-based relations? In Section 11, the authors identify a current limitation: "acquisition of prepositions is currently limited to simple binary spatial relationships... We plan to extend the system to acquire more complex relations involving multiple objects, such as between." The current spatial primitives are binary (comparing a primary object to a single reference object). Extending this to ternary relations requires new structural representations in semantic memory and new spatial queries in the Spatial Visual System (SVS).

### Open Question 4
How can the agent learn verbs defined by the *manner* of execution rather than solely by the end state? Section 11 outlines future work to include "verbs which are defined in part by the way in which the underlying actions are performed (such as push versus move)." The current verb acquisition method relies on "retrospective forward projection" to a goal state. "Manner" verbs require the agent to perceive, represent, and enforce constraints on the trajectory and intermediate states of the motion, not just the final result.

## Limitations
- Relies on engineered perceptual features that may not scale to complex visual properties
- Assumes accurate instructor input and parser performance, which may not hold in real-world scenarios
- Validated only in controlled table-top settings with limited object variety

## Confidence
- **High**: The core mechanisms of impasse-driven knowledge acquisition and retrospective projection for procedural learning are well-specified and theoretically sound.
- **Medium**: The reported learning efficiencies (93-100% accuracy with minimal examples) are impressive but rely on controlled conditions and may not generalize to more ambiguous or noisy language.
- **Medium**: The symbolic-perceptual coupling via perceptual symbols is a reasonable abstraction, but its effectiveness for handling polysemy and synonymy remains to be demonstrated in more complex linguistic contexts.

## Next Checks
1. Test the agent's ability to handle perceptual ambiguity by introducing lighting variations that could cause color misclassification, and measure how often this breaks semantic mappings.
2. Evaluate verb generalization beyond the specific object used in the demonstration - attempt to teach "move" with a red triangle, then test with a blue circle, measuring success rates.
3. Assess the discourse model's robustness by introducing ambiguous instructions that could belong to multiple ongoing segments, and verify the agent correctly resolves the ambiguity through interaction.