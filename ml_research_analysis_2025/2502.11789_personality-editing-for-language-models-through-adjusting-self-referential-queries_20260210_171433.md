---
ver: rpa2
title: Personality Editing for Language Models through Adjusting Self-Referential
  Queries
arxiv_id: '2502.11789'
source_url: https://arxiv.org/abs/2502.11789
tags:
- personality
- prompt
- target
- editing
- palette
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PALETTE, a method for editing personality
  in large language models by directly modifying their internal self-representations
  through self-referential adjustment queries. Instead of relying on prompts or costly
  fine-tuning, PALETTE uses a small set of structured queries grounded in psychological
  frameworks to make low-rank updates to model parameters, shifting personality traits
  like Thinking vs.
---

# Personality Editing for Language Models through Adjusting Self-Referential Queries

## Quick Facts
- arXiv ID: 2502.11789
- Source URL: https://arxiv.org/abs/2502.11789
- Reference count: 19
- Primary result: Achieves 5%-25% personality alignment improvement using only 12 adjustment queries

## Executive Summary
This paper introduces PALETTE, a method for editing personality in large language models by directly modifying their internal self-representations through self-referential adjustment queries. Instead of relying on prompts or costly fine-tuning, PALETTE uses a small set of structured queries grounded in psychological frameworks to make low-rank updates to model parameters, shifting personality traits like Thinking vs. Feeling. Experiments show that with only 12 adjustment queries, PALETTE achieves personality alignment improvements of 5%-25% across MBTI and Big Five dimensions, while maintaining response quality and exhibiting robustness against conflicting prompts.

## Method Summary
PALETTE edits personality traits by constructing self-referential adjustment queries where the subject is "I" and the target is a trait-associated word. It applies r-ROME (rank-one ROME) to compute low-rank updates to specific MLP layers, using key vectors derived from self-token hidden states and value vectors from target trait embeddings. The method requires only 12 queries per personality dimension, targets specific MLP layers (layer 15 for Qwen-2.5-1.5B), and evaluates on EmpatheticDialogues using GPT-4o for personality expression rate measurement.

## Key Results
- Achieves 5%-25% improvement in personality alignment across MBTI and Big Five dimensions
- Requires only 12 adjustment queries per dimension, outperforming prompt-based methods
- Maintains response quality (naturalness/coherence) while shifting personality expression
- Demonstrates robustness to conflicting prompts, keeping personality expression closer to target
- Shows data efficiency compared to fine-tuning baselines requiring thousands of examples

## Why This Works (Mechanism)

### Mechanism 1
Self-referential personality statements can be treated analogously to factual knowledge for direct editing. The method constructs adjustment queries with "I" as subject and trait words as targets, shifting internal self-representation rather than surface outputs. This assumes personality is encoded through self-referential token associations in MLP layers, similar to factual associations. Evidence includes the abstract's claim about self-referential statements enabling direct editing and Table 2 showing query structure. This fails if personality is primarily stored in attention patterns or if self-referential tokens don't reliably localize personality representations.

### Mechanism 2
Rank-one parameter updates to specific MLP layers can shift personality expression across contexts. Using r-ROME, the method computes a rank-one update matrix applied to target MLP weights, deriving key vectors from subject token hidden states and value vectors from trait words. This assumes personality-related self-concepts are locally stored in specific MLP layers (layer 15 for Qwen-2.5-1.5B) without catastrophic interference. Evidence includes Equation 2 showing the update formula and Table 25 specifying layer targeting. This fails if personality requires distributed updates across many layers.

### Mechanism 3
A small set of structured adjustment queries (12) suffices because psychological frameworks provide coverage of core trait expressions. Queries are derived from MBTI questionnaire items, systematically rephrased into declarative self-referential statements. This assumes personality dimensions can be adequately captured by ~12 distinct decision points that generalize to novel contexts. Evidence includes Table 6 showing 12 queries optimal through ablation, with 4 or 8 underperforming and 16 showing diminishing returns. This fails if personality is fundamentally more multi-faceted than 12 queries capture.

## Foundational Learning

- **Model Editing / Knowledge Editing**: Why needed - PALETTE builds on the premise that factual knowledge can be localized and edited; understanding ROME/MEMIT is prerequisite. Quick check - Can you explain why a rank-one update might be preferred over full fine-tuning for localized edits?

- **MBTI and Big Five Personality Frameworks**: Why needed - The adjustment queries are grounded in these psychological constructs; understanding trait dichotomies (Thinking/Feeling, etc.) is essential for query design and evaluation. Quick check - What are the four MBTI dichotomies, and how would you construct a query to shift the Feeling→Thinking dimension?

- **Self-Referential Processing in LLMs**: Why needed - The core innovation is targeting "I/me" tokens; understanding how models process self-reference informs why this works. Quick check - Where in a transformer might self-referential identity information be stored—attention heads, MLPs, or both?

## Architecture Onboarding

- **Component map**: Query Generator -> Editor Core (r-ROME) -> Target Layer Selector -> Evaluation Pipeline
- **Critical path**: 1) Select target personality dimension, 2) Generate 12 adjustment queries with "I" as subject and trait word as target_new, 3) Extract hidden states at layer 15 for "I" tokens, 4) Compute rank-one Δ using Eq. 2, 5) Apply ̂W = W₀ + Δ to mlp.down_proj, 6) Evaluate on held-out EmpatheticDialogues
- **Design tradeoffs**: r-ROME vs. MEMIT - r-ROME outperforms MEMIT for personality editing as MEMIT's multi-layer distribution may over-constrain context-dependent personality expression; 12 queries chosen empirically as optimal; Perceiving (P) dimension shows consistent degradation due to mismatch between spontaneous P-style language and structured LLM training
- **Failure signatures**: Low expression rate improvement (<5%) suggests wrong layer or insufficient query coverage; degraded naturalness/coherence scores indicate over-aggressive edit magnitude; P dimension instability is a known issue due to training data mismatch
- **First 3 experiments**: 1) Replicate Table 3 on Qwen-2.5-1.5B for F/T dimension to confirm ~0.62 Thinking expression rate with 12 queries, 2) Ablate query count (4/8/12/16) to verify Table 6 trend holds for your target dimension, 3) Test robustness under opposing prompts (Figure 4) to verify personality expression stays closer to 0.5 than base model when given contradictory prompts

## Open Questions the Paper Calls Out

### Open Question 1
Can the structural bias of LLMs toward organized text be mitigated to improve the editing of "open" personality traits like Perceiving? The paper observes that the Perceiving (P) variant consistently degrades performance (naturalness, coherence) and hypothesizes an "intrinsic mismatch" between the spontaneity of the P trait and the structured training of LLMs, but offers no solution to bridge this specific linguistic mismatch.

### Open Question 2
How can the self-referential adjustment framework be adapted for black-box models where internal parameter access is restricted? The method relies on direct manipulation of MLP layers using r-ROME, which is fundamentally impossible on closed-source APIs without architectural modifications or a paradigm shift.

### Open Question 3
Does the simultaneous application of edits to all personality dimensions result in negative interference or model collapse? While Appendix A.3 demonstrates success editing two dimensions (I+T), the main experiments isolate single dimensions. The interaction effects of editing the full self-concept (all weights simultaneously) remain unverified.

## Limitations
- P dimension edits consistently degrade naturalness and coherence due to mismatch between spontaneous P-style expression and structured LLM training
- The method requires internal parameter access, making it inapplicable to black-box models or API-only deployments
- The claim that 12 MBTI-derived queries provide complete coverage of personality expression space is speculative and lacks theoretical grounding

## Confidence
- **High Confidence**: The basic mechanism of using r-ROME for low-rank parameter updates is well-established in the literature. The mathematical formulation is correct and empirical results showing improvements over baselines are reproducible.
- **Medium Confidence**: The specific choice of 12 queries per dimension and targeting layer 15 is empirically justified but not theoretically grounded. The claim that personality is primarily stored in MLP layers has some support but lacks comprehensive validation across different model architectures.
- **Low Confidence**: The assumption that 12 MBTI-derived queries provide complete coverage of personality expression space is speculative. The explanation for P dimension instability is acknowledged but not well-understood.

## Next Checks
1. Systematically test whether editing different MLP layers (or combinations of layers) yields different personality alignment results to validate whether the layer 15 choice is optimal or arbitrary for the Qwen-2.5-1.5B architecture.

2. Test whether adding more diverse query types (beyond MBTI-derived self-referential statements) improves personality alignment, particularly for dimensions like Perceiving that show degradation, to validate whether 12 queries provide sufficient coverage.

3. Evaluate personality expression on datasets beyond EmpatheticDialogues, particularly ones that include non-self-referential personality manifestations, to test whether the edits truly generalize to the full spectrum of personality expression rather than just self-referential contexts.