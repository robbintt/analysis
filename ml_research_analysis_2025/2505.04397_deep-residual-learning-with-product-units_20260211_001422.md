---
ver: rpa2
title: Deep residual learning with product units
arxiv_id: '2505.04397'
source_url: https://arxiv.org/abs/2505.04397
tags:
- residual
- pure
- product
- resnet
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces PURe (Product-Unit Residual Network), a novel
  deep learning architecture that integrates product units into residual blocks to
  enhance the expressiveness and parameter efficiency of convolutional neural networks.
  Unlike standard summation neurons, product units enable multiplicative feature interactions,
  which PURe leverages by replacing the second convolutional layer in each residual
  block with a 2D product unit and removing nonlinear activation functions to preserve
  structural information.
---

# Deep residual learning with product units

## Quick Facts
- arXiv ID: 2505.04397
- Source URL: https://arxiv.org/abs/2505.04397
- Authors: Ziyuan Li; Uwe Jaekel; Babette Dellen
- Reference count: 36
- Primary result: PURe34 achieves 84.89% accuracy on Galaxy10 DECaLS, surpassing ResNet152 while converging 5x faster

## Executive Summary
This study introduces PURe (Product-Unit Residual Network), a novel deep learning architecture that integrates product units into residual blocks to enhance the expressiveness and parameter efficiency of convolutional neural networks. Unlike standard summation neurons, product units enable multiplicative feature interactions, which PURe leverages by replacing the second convolutional layer in each residual block with a 2D product unit and removing nonlinear activation functions to preserve structural information. This design improves the model's ability to capture complex nonlinear patterns and achieve faster convergence.

Experiments across three benchmark datasets—Galaxy10 DECaLS, ImageNet, and CIFAR-10—demonstrate PURe's effectiveness. On Galaxy10 DECaLS, PURe34 achieves the highest test accuracy of 84.89%, surpassing deeper ResNet152 while converging nearly five times faster and exhibiting strong robustness to Poisson noise. On ImageNet, PURe34 attains a top-1 accuracy of 80.27% and top-5 accuracy of 95.78%, outperforming ResNet50 and ResNet101 with fewer parameters. On CIFAR-10, PURe272 reaches 95.01% test accuracy, comparable to ResNet1001 but with less than half the model size. These results highlight PURe's favorable balance between accuracy, efficiency, and robustness, validating its potential as a scalable and reliable deep learning architecture for computer vision tasks.

## Method Summary
PURe modifies standard ResNet architectures by replacing the second convolutional layer in each residual block with a 2D Product Unit (ConvPU). The ConvPU computes y = exp(Conv(log(clamp(x, threshold)))), where the clamp prevents log(0) using a softplus-based threshold. All ReLU activations within residual blocks are removed. The architecture uses Kaiming Uniform initialization with scaling factor 5 for ConvPU weights, trainable threshold initialized to 0, and requires lower learning rates (0.01 vs 0.1) and different weight decay schedules. The design enables multiplicative feature interactions while residual connections stabilize training.

## Key Results
- Galaxy10 DECaLS: PURe34 achieves 84.89% test accuracy, surpassing ResNet152 while converging 5x faster
- ImageNet: PURe34 reaches 80.27% top-1 and 95.78% top-5 accuracy, outperforming ResNet50/101 with fewer parameters
- CIFAR-10: PURe272 attains 95.01% accuracy, comparable to ResNet1001 but with <50% of parameters

## Why This Works (Mechanism)

### Mechanism 1: Multiplicative Feature Interaction
Product units compute y = ∏x_i^{w_i} instead of standard summation, enabling direct modeling of polynomial terms and ratios. The 2D implementation uses log-transform + convolution + exponential to convert multiplicative interactions into summation in log-domain. This allows single layers to represent complex nonlinear relationships that would require multiple stacked linear layers with standard neurons.

### Mechanism 2: Residual Stabilization of Volatile Gradients
Product units involve exponentiation and logarithms that can produce extreme gradients. Residual connections provide gradient superhighways that bypass volatile product unit layers during backpropagation, ensuring stable convergence. The identity mapping serves as a stable baseline preventing multiplicative path from destabilizing the network.

### Mechanism 3: Invariance via Ratio-Based Filtering
Product units can create filters sensitive to ratios between pixel intensities rather than absolute differences. A filter comparing x₁/x₂ can be modeled as x₁¹ · x₂⁻¹, remaining constant under proportional scaling. This grants robustness to global intensity shifts like lighting changes and Poisson noise, as relative structure is preserved even when absolute magnitudes change.

## Foundational Learning

- **Log-Sum-Exp (LSE) Trick**: Critical for understanding how product units compute products stably using logarithms. Why needed: The 2D product unit relies on converting multiplication to addition via logarithms. Quick check: How does the network handle negative pixel values or zero when the operation requires a logarithm?

- **Residual vs. Identity Mapping**: Essential for understanding PURe's architecture modifications. Why needed: PURe replaces the second Conv+ReLU with a Product Unit while preserving the skip connection. Quick check: In a PURe block, does the gradient flow through the product unit, the skip connection, or both?

- **Inductive Bias of Convolutions**: Understanding the shift from additive to multiplicative interactions. Why needed: Standard convolutions assume translation invariance and locality; product units add bias for multiplicative interactions. Quick check: Would a product unit be better at detecting a "bright spot" or a "gradient ratio between two spots"?

## Architecture Onboarding

- **Component map**: Input -> 7×7 Conv -> [PURe Block]*N -> Global Avg Pool -> FC Layer
- **PURe Block**: Path A (Main): 3×3 Conv → BatchNorm → 2D Product Unit → BatchNorm; Path B (Skip): Identity (or 1×1 Conv for downsampling); Merge: Addition (Path A + Path B)
- **Critical path**: Initialization of trainable threshold parameter θ must be correct (paper uses zero). Forward Pass: Clamping logic (max(x, softplus(θ) + ε)) is most fragile part.
- **Design tradeoffs**: Efficiency vs. Stability - fewer parameters but requires careful hyperparameter tuning (lower LR 0.01, high weight decay). No Activations - removes implicit regularization of sparsity.
- **Failure signatures**: NaN Loss - usually implies log-clamp threshold too low or inputs are negative. Slow Convergence - if LR too high. Saturation - beyond PURe34/272, deeper models show diminishing returns.
- **First 3 experiments**: 1) Threshold Sanity Check - implement 2D Product Unit alone on dummy data to verify clamping prevents NaNs. 2) Ablation on Depth - compare PURe18 vs ResNet18 on CIFAR-10 to verify multiplicative bias offers accuracy boost. 3) Noise Robustness Stress Test - train on clean images and test on Poisson-noise augmented images.

## Open Questions the Paper Calls Out

- **Complex-valued product units**: The paper suggests complex-valued product units could eliminate the need for trainable threshold parameters by working without thresholds, potentially resolving numerical instability issues.

- **Stabilization for very deep architectures**: The authors encountered gradient explosion and non-convergence when training deeper variants like PURe486, suggesting a need for advanced normalization techniques or gradient clipping strategies.

- **Saturation mechanism identification**: The observed performance saturation beyond PURe34/272 layers may indicate either limited network representational capacity or insufficient dataset information density.

## Limitations

- **Parameter efficiency claims lack direct numerical support**: While PURe demonstrates fewer parameters than comparable ResNet architectures, the parameter count comparisons lack absolute numerical validation in the manuscript.

- **Noise robustness generalization is limited**: Robustness to Poisson noise is demonstrated only on Galaxy10 DECaLS dataset, with claims about general signal-dependent noise lacking validation across multiple noise types.

- **Mechanism validation is incomplete**: The paper presents theoretical arguments for product unit benefits but lacks ablation studies isolating specific contributions of multiplicative interactions versus activation removal.

## Confidence

- **High Confidence**: Basic architectural implementation, convergence speed improvements, and CIFAR-10 performance comparisons with ResNet
- **Medium Confidence**: ImageNet performance claims and Galaxy10 DECaLS accuracy results, dependent on correct initialization and hyperparameter tuning
- **Low Confidence**: Claims about ratio-based filtering providing robustness to arbitrary lighting conditions, and specific parameter efficiency improvements without direct numerical comparisons

## Next Checks

1. **Initialization Sensitivity Test**: Train PURe with varying initialization strategies for the product unit threshold parameter (θ) to determine stability range and verify θ=0 initialization optimality.

2. **Ablation on Activation Removal**: Create variant keeping ReLUs in residual blocks while maintaining product unit to isolate whether improvements come from multiplicative interactions or activation removal.

3. **Noise Type Generalization**: Test PURe's robustness across multiple noise distributions (Gaussian, Salt-and-Pepper, Speckle) beyond Poisson noise to validate general applicability of claimed robustness mechanism.