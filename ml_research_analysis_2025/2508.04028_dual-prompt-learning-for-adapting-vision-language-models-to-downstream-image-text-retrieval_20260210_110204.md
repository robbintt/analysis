---
ver: rpa2
title: Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text
  Retrieval
arxiv_id: '2508.04028'
source_url: https://arxiv.org/abs/2508.04028
tags:
- learning
- retrieval
- prompt
- image
- dcar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DCAR, a dual-prompt learning framework designed
  to adapt vision-language models (VLMs) to downstream image-text retrieval tasks.
  The key challenge addressed is the difficulty of aligning both fine-grained attributes
  and similar subcategories in retrieval data.
---

# Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval

## Quick Facts
- arXiv ID: 2508.04028
- Source URL: https://arxiv.org/abs/2508.04028
- Authors: Yifan Wang; Tao Wang; Chenwei Tang; Caiyang Yu; Zhengqing Zang; Mengmi Zhang; Shudong Huang; Jiancheng Lv
- Reference count: 40
- Primary result: Introduces DCAR, a dual-prompt learning framework for fine-grained image-text retrieval, achieving +10.61% I2T and +6.63% T2I Recall@1 gains over CLIP on 9 domains.

## Executive Summary
This paper introduces DCAR, a dual-prompt learning framework designed to adapt vision-language models (VLMs) to downstream image-text retrieval tasks. The key challenge addressed is the difficulty of aligning both fine-grained attributes and similar subcategories in retrieval data. DCAR dynamically adjusts prompt vectors from semantic and visual dimensions using two strategies: (1) attribute-aware token re-weighting based on mutual information to enhance attribute recognition, and (2) category-aware augmentation with negative samples to improve subcategory discrimination. To validate the method, the authors construct FDRD, a new benchmark dataset with over 1,500 fine categories and 230,000 image-caption pairs emphasizing detailed attributes and distinctions. Extensive experiments show DCAR achieves state-of-the-art performance, outperforming strong baselines like CLIP, CoOp, and FineCLIP across 9 diverse domains, with improvements up to +10.61% in image-to-text recall@1.

## Method Summary
DCAR adapts CLIP for downstream image-text retrieval through dual-prompt learning. It employs learnable text context vectors and visual vectors that are projected into CLIP's embedding space. The method introduces two key innovations: Dynamic Token Re-Weighting, which uses mutual information approximation to filter caption noise and emphasize attribute-relevant tokens, and Category Aware Augmentation, which generates negative samples with category-matching weighting to enforce subcategory discrimination. The framework freezes CLIP's encoders and trains only the prompt vectors and a small augmentation network, making it computationally efficient. Training uses 16-shot few-shot learning with a contrastive loss augmented by category-aware negative sampling, optimized with SGD and cosine learning rate decay.

## Key Results
- DCAR achieves state-of-the-art performance across 9 diverse domains including Caltech101, OxfordPets, StanfordCars, Flowers102, Food101, FGVCAircraft, SUN397, DTD, and UCF101
- Demonstrates +10.61% improvement in image-to-text recall@1 and +6.63% improvement in text-to-image recall@1 over the CLIP baseline
- Constructs FDRD benchmark with over 1,500 fine categories and 230,000 image-caption pairs emphasizing detailed attributes
- Outperforms strong baselines including CLIP, CoOp, and FineCLIP across all evaluated metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: **Attribute-awareness is enhanced by filtering caption noise via Mutual Information (MI) approximation.**
- Mechanism: The model estimates the relevance of each text token by calculating the similarity difference between the token-to-target-image and token-to-blank-image. This difference approximates MI; higher differences suggest the token carries image-specific information (attributes), while lower differences suggest abstract or structural language (noise). These tokens are dynamically weighted during embedding aggregation.
- Core assumption: The similarity score between a token and a "blank" (uninformative) image serves as a valid baseline for entropy, allowing the difference $\Delta S$ to act as a proxy for mutual information. It also assumes that generic sentence structure contributes less to retrieval than specific attribute descriptors.
- Evidence anchors:
  - [abstract]: "...dynamically updates the weights of attribute descriptions based on text-image mutual information correlation..."
  - [section 3.3]: "We employ mutual information... to evaluate the correlation between a given token and the image... MI(token; Image) $\propto \Delta S$."
  - [corpus]: Adding simple structure at inference improves Vision-Language Compositionality (arXiv:2506.09691) identifies that VLMs often behave like "bags-of-words," supporting the need for structural re-weighting to improve compositionality.
- Break condition: If the "blank image" used for baseline comparison is not truly uninformative (e.g., contains artifacts) or if the captions are extremely short, the MI approximation may become unstable or fail to distinguish signal from noise.

### Mechanism 2
- Claim: **Subcategory discrimination is enforced by prioritizing hard negatives that share category labels but differ in attributes.**
- Mechanism: The framework uses Category Aware Augmentation (CAA) to generate negative captions. It applies a specific loss weighting: positive pairs get a weight of $1+\alpha$, negative pairs with the correct category but wrong attributes get $1+\alpha$ (to force distinction), and negatives with wrong categories get weight 1. This prevents the model from relying solely on category names for matching.
- Core assumption: Standard contrastive loss treats all negatives equally, causing the model to satisfy itself with easy category-level distinctions rather than learning fine-grained attribute alignment.
- Evidence anchors:
  - [abstract]: "...introduces negative samples from multiple perspectives with category-matching weighting for subcategory discrimination."
  - [section 3.4]: "When $c_{T'} = c_{I'}$, indicating category matching... the weight $w = 1 + \alpha$, reinforcing the model to prioritize learning finer distinctions within matching categories."
  - [corpus]: FiCo-ITR (arXiv:2407.20114) highlights the distinct challenges of fine-grained vs. coarse-grained retrieval, validating the need for specialized fine-grained negative sampling strategies.
- Break condition: If the attribute annotations in the dataset are sparse or noisy, the "hard negatives" may be indistinguishable from positives, leading to gradient confusion.

### Mechanism 3
- Claim: **Visual and textual representation gaps are bridged jointly via dual-branch prompt tuning.**
- Mechanism: Instead of tuning only text prompts (like CoOp), the model introduces learnable visual prompts derived from the text context vectors. This projects semantic guidance directly into the visual encoder's input space, conditioning visual features on textual context.
- Core assumption: Distribution shift occurs in both image and text modalities; tuning only one side (text) is insufficient for precise image-text alignment in downstream tasks.
- Evidence anchors:
  - [abstract]: "...dynamically adjusts prompt vectors from both semantic and visual dimensions..."
  - [section 3.2]: "We construct a visual prompt by extracting data-discriminative features through semantic analysis... [using] a linear transformation layer."
  - [corpus]: Dual-View Alignment Learning (arXiv:2509.17747) demonstrates the efficacy of hierarchical/dual prompts in handling complex class imbalances, supporting the dual-prompt architectural choice.
- Break condition: If the visual prompt dimensionality is too small to capture the complexity of the text context, or if the linear projection $F(\cdot)$ fails to align the embedding spaces effectively.

## Foundational Learning

- Concept: **Mutual Information (MI) in Representation Learning**
  - Why needed here: The core of the "Dynamic Token Re-Weighting" mechanism relies on approximating MI to identify which tokens are statistically relevant to the visual input.
  - Quick check question: Can you explain why the similarity to a "blank image" is subtracted from the similarity to the "target image" to approximate information gain?

- Concept: **Contrastive Learning & Negative Sampling**
  - Why needed here: The model builds upon CLIP's contrastive loss. Understanding how to construct "hard negatives" (samples that are close in embedding space but semantically different) is essential for the Category Aware Augmentation module.
  - Quick check question: Why might a standard contrastive loss fail to distinguish between a "Yorkshire Terrier" and a "Silky Terrier" if only the category label is changed?

- Concept: **Prompt Tuning (Soft Prompts)**
  - Why needed here: The method freezes the heavy backbone and only trains small "prompt" vectors. This requires understanding how continuous vectors are prepended to inputs to steer the model's attention without modifying weights.
  - Quick check question: What is the risk of overfitting when using long prompt vectors in a few-shot learning scenario (e.g., 16-shot)?

## Architecture Onboarding

- Component map:
  - Frozen Encoders: CLIP ViT-B/32 (Image & Text)
  - Dual Prompts: Learnable Text Context Vectors ($v_1...v_k$) + Visual Vectors ($e_1...e_k$)
  - Projection: Linear layer $F$ mapping text context to visual prompt space
  - MI Calculator: Module to compute $\Delta S$ (similarity difference) for token weighting
  - CAA Network: Two linear layers processing text features before final similarity calculation
  - Inputs: Target Image, Text Caption, Blank Image (constant), Negative Captions (generated)

- Critical path:
  1. **Prompt Injection**: Text vectors are prepended to captions; Visual vectors (projected from text vectors) are prepended to image patches
  2. **Encoding**: Pass through frozen CLIP encoders to get embeddings ($I'$ and $T'$)
  3. **Token Weighting**: For each token in the caption, calculate similarity to $I'$ and a Blank Image embedding. Compute weights $w_{token} = |\Delta S|$
  4. **Re-embedding**: Aggregate text embeddings using calculated weights ($T'_w$)
  5. **Augmentation**: Pass $T'_w$ through CAA network; calculate contrastive loss against positive image and generated negative captions using the specific category-weighting formula

- Design tradeoffs:
  - **Complexity vs. Interpretability**: The MI weighting adds computational overhead (inference against a blank image per token) but provides interpretable attention maps
  - **Attribute vs. Category**: The loss function explicitly trades off broad category recognition for fine-grained attribute sensitivity. This may degrade performance on datasets like Caltech101 (mentioned in paper) where categories are distinct and attributes matter less

- Failure signatures:
  - **Semantic Drift**: Retrieving images that match the category name but fail on specific attributes (e.g., retrieving a "black cat" when the query specifies a "white cat")
  - **Token Collapse**: The MI weighting collapses to zero for all tokens if the blank image baseline is unstable, resulting in a standard (unweighted) retrieval behavior
  - **Over-fitting**: In 1-shot or 2-shot settings, the visual prompts might memorize the specific training images rather than learning generalizable features

- First 3 experiments:
  1. **Baseline Integrity**: Run the model on FDRD with the MI mechanism disabled (all weights = 1) to isolate the contribution of the dynamic re-weighting logic
  2. **Negative Sampling Ablation**: Compare "Category Aware" negatives against random negatives to verify that forcing the model to distinguish within-categories is the driver of performance gains
  3. **Blank Image Sensitivity**: Swap the "Blank Image" used in MI calculation for a noise tensor or a dataset average image to test the robustness of the MI approximation assumption

## Open Questions the Paper Calls Out
- Can the DCAR framework be effectively extended to generative multi-modal reasoning tasks, such as Visual Question Answering or image captioning?
- How can the token re-weighting mechanism be modified to improve performance on texture-centric or broad-category datasets where semantic tokens are less discriminative?

## Limitations
- The method performs worse on texture-centric datasets like DTD and broad-category datasets like Caltech101, suggesting limitations for non-object-centric domains
- The FDRD benchmark relies on VLLM-generated captions that required extensive manual refinement, raising questions about scalability to truly "in-the-wild" data
- The mutual information approximation using a static "blank image" baseline lacks rigorous theoretical validation across diverse caption styles

## Confidence

- **High Confidence**: The dual-prompt architecture and its integration with CLIP's frozen encoders is technically sound and well-supported by prior work on soft prompt tuning. The performance improvements on FDRD and other benchmarks are directly measurable and reproducible.
- **Medium Confidence**: The claim that the MI-based token re-weighting specifically improves attribute recognition is supported by ablation studies, but the exact mechanism by which $\Delta S$ disentangles attribute-relevant tokens from structural language remains heuristic. The choice of the blank image as a baseline is justified but not deeply explored.
- **Low Confidence**: The scalability claim to arbitrary domains is based on the FDRD benchmark, which itself required extensive manual refinement. The paper does not provide evidence that the method would perform equally well on a truly "in-the-wild" dataset without attribute-level annotations.

## Next Checks

1. **Blank Image Robustness**: Replace the static blank image used in MI calculation with a learned "neutral" embedding or a dataset average, and measure the impact on retrieval performance to test the stability of the $\Delta S$ approximation.

2. **Negative Sampling Ablation**: Conduct an ablation study comparing the Category Aware Augmentation module against a simpler random negative sampling strategy to isolate the contribution of the category-matching weighting scheme.

3. **Cross-Domain Generalization**: Evaluate DCAR on a dataset outside of FDRD (e.g., Flickr30k or Conceptual Captions) without any fine-tuning to assess its ability to generalize beyond curated, attribute-rich domains.