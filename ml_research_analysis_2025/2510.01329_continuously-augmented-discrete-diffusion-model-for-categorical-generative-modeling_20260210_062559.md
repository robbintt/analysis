---
ver: rpa2
title: Continuously Augmented Discrete Diffusion model for Categorical Generative
  Modeling
arxiv_id: '2510.01329'
source_url: https://arxiv.org/abs/2510.01329
tags:
- diffusion
- discrete
- cadd
- continuous
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the information loss problem in standard discrete
  diffusion models where masked tokens are represented as a single absorbing [MASK]
  token, eliminating semantic information between denoising steps. The authors propose
  Continuously Augmented Discrete Diffusion (CADD), which augments discrete token
  space with a paired continuous diffusion in a latent space.
---

# Continuously Augmented Discrete Diffusion model for Categorical Generative Modeling

## Quick Facts
- **arXiv ID:** 2510.01329
- **Source URL:** https://arxiv.org/abs/2510.01329
- **Authors:** Huangjie Zheng; Shansan Gong; Ruixiang Zhang; Tianrong Chen; Jiatao Gu; Mingyuan Zhou; Navdeep Jaitly; Yizhe Zhang
- **Reference count:** 40
- **Primary result:** CADD achieves improved generative quality over mask-based diffusion across text, image, and code generation tasks

## Executive Summary
The paper addresses the information loss problem in standard discrete diffusion models where masked tokens are represented as a single absorbing [MASK] token, eliminating semantic information between denoising steps. The authors propose Continuously Augmented Discrete Diffusion (CADD), which augments discrete token space with a paired continuous diffusion in a latent space. This allows masked tokens to retain semantic information through noisy but informative latent vectors rather than collapsing into information voids. The continuous latent serves as a soft semantic hint to guide discrete denoising, while discrete context constrains latent dynamics locally.

CADD maintains compatibility with existing discrete diffusion training and enables controlled trade-offs between mode-coverage and mode-seeking behaviors. Empirically, CADD improves generative quality over mask-based diffusion across text, image, and code generation tasks. On OpenWebText, CADD achieves 0.24 MAUVE and 35.3 generative perplexity with 4096 sampling steps, outperforming baselines. On CIFAR-10, CADD achieves FID of 2.88 with 512 function evaluations. On code generation benchmarks, CADD achieves 72.0 pass@1 on HumanEval and 55.7 average score across benchmarks, outperforming previous diffusion models.

## Method Summary
CADD introduces a novel approach to discrete diffusion by maintaining both discrete and continuous representations during the diffusion and denoising processes. The key innovation is augmenting the discrete token space with a paired continuous diffusion in a latent space, where masked tokens retain semantic information through noisy but informative latent vectors rather than collapsing into information voids. The continuous latent serves as a soft semantic hint to guide discrete denoising, while discrete context constrains latent dynamics locally. This architecture maintains compatibility with existing discrete diffusion training procedures while enabling controlled trade-offs between mode-coverage and mode-seeking behaviors.

## Key Results
- CADD achieves 0.24 MAUVE and 35.3 generative perplexity on OpenWebText with 4096 sampling steps
- CADD achieves FID of 2.88 on CIFAR-10 with 512 function evaluations
- CADD achieves 72.0 pass@1 on HumanEval and 55.7 average score across code generation benchmarks

## Why This Works (Mechanism)
The core mechanism of CADD addresses a fundamental limitation in discrete diffusion models: when tokens are masked during the diffusion process, they lose all semantic information and become represented by a single [MASK] token. This creates information voids that make the denoising process more challenging and can lead to poor generation quality. CADD solves this by maintaining a continuous latent representation alongside the discrete tokens, where masked tokens retain noisy but informative latent vectors. During denoising, these continuous latents provide soft semantic hints that guide the reconstruction of discrete tokens, while the discrete context helps constrain the latent dynamics locally. This dual representation allows for more effective information flow throughout the diffusion process.

## Foundational Learning
- **Discrete diffusion models**: Why needed - Standard approach for generative modeling on discrete data like text and categorical images; Quick check - Can be trained with simple masking and reconstruction objectives
- **Information loss in masking**: Why needed - Standard discrete diffusion collapses masked tokens to a single [MASK] token, eliminating semantic information; Quick check - This creates voids that hinder effective denoising
- **Continuous-discrete coupling**: Why needed - Allows information retention during masking by maintaining paired representations; Quick check - Continuous latents provide semantic hints while discrete context constrains dynamics
- **Latent space diffusion**: Why needed - Enables smooth semantic transitions and information preservation; Quick check - Can be trained with standard diffusion objectives on continuous data
- **Mode-coverage vs mode-seeking trade-offs**: Why needed - Important for balancing diversity and quality in generation; Quick check - Controlled by a parameter in the CADD framework

## Architecture Onboarding

**Component map:**
Discrete tokens -> Continuous latent space -> Diffusion process -> Dequantization -> Discrete reconstruction

**Critical path:**
1. Input tokens are encoded to discrete and continuous representations
2. Both representations undergo diffusion with different noise schedules
3. During denoising, continuous latents provide semantic hints for discrete reconstruction
4. Discrete context constrains latent dynamics locally
5. Final output is obtained through discrete token prediction

**Design tradeoffs:**
- The coupling between discrete and continuous spaces must balance information flow without creating dependencies that break the Markov property
- Latent dimensionality affects the quality of semantic hints versus computational cost
- Noise schedules for discrete and continuous processes need careful alignment
- The trade-off parameter between mode-coverage and mode-seeking behaviors requires empirical tuning

**Failure signatures:**
- If continuous latents collapse to uninformative noise, CADD reduces to standard discrete diffusion
- Poor alignment between discrete and continuous noise schedules can cause mode collapse
- Excessive coupling may break the theoretical guarantees of discrete diffusion
- Insufficient latent capacity leads to loss of semantic information

**First experiments:**
1. Compare CADD with standard discrete diffusion on a simple text completion task using the same model architecture
2. Perform ablation studies varying the latent dimensionality while keeping other components fixed
3. Test the controllability of the mode-coverage vs mode-seeking trade-off on a controlled synthetic dataset

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Empirical improvements, while promising, are demonstrated primarily on relatively standard benchmarks without extensive ablation studies
- The theoretical analysis relies on existing diffusion framework principles but does not fully establish why continuous augmentation would necessarily lead to improved information retention
- The extent to which CADD's benefits are additive versus dependent on specific implementation choices remains unclear
- Confidence in broader implications for generative modeling is Medium-Low due to variable improvement magnitude across domains

## Confidence
- **Core technical claims**: Medium - Architectural innovation is clear and empirical improvements over baseline discrete diffusion models are demonstrated
- **Empirical methodology**: Medium - Results show CADD outperforming standard discrete diffusion across tasks, but the magnitude of improvement varies significantly
- **Theoretical grounding**: Medium - Relies on existing diffusion framework principles but lacks full establishment of why continuous augmentation improves information retention
- **Broader implications**: Medium-Low - Unclear whether gains would persist on more challenging or diverse datasets

## Next Checks
1. Conduct systematic ablation studies comparing CADD with variants that use different continuous augmentation strategies (e.g., varying latent dimensionality, different noise schedules) to isolate which aspects of the design are most critical for performance gains.

2. Evaluate CADD on more diverse and challenging benchmarks beyond the standard datasets, including domains with more complex discrete structures or where discrete diffusion has historically underperformed, to test generalizability claims.

3. Perform controlled experiments varying the trade-off parameter between mode-coverage and mode-seeking behaviors across multiple scales to verify the claimed controllability and understand the practical limits of this capability.