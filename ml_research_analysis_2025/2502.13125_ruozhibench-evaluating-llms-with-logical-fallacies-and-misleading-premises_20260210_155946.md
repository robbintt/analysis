---
ver: rpa2
title: 'RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading Premises'
arxiv_id: '2502.13125'
source_url: https://arxiv.org/abs/2502.13125
tags:
- question
- llama-3
- qwen2
- logical
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RuozhiBench is a new benchmark that evaluates large language models'
  ability to detect and reason about logical fallacies and misleading premises. The
  dataset contains 677 carefully curated questions sourced from Chinese internet forums,
  covering six types of deceptive reasoning patterns.
---

# RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading Premises
## Quick Facts
- **arXiv ID:** 2502.13125
- **Source URL:** https://arxiv.org/abs/2502.13125
- **Reference count:** 40
- **Key outcome:** Even the best-performing model (Claude-3-haiku) achieves only 62% accuracy on detecting logical fallacies, significantly below human performance of over 90%

## Executive Summary
RuozhiBench is a benchmark designed to evaluate large language models' ability to detect and reason about logical fallacies and misleading premises. The dataset contains 677 carefully curated questions sourced from Chinese internet forums, covering six types of deceptive reasoning patterns. Experiments with 17 models from 5 model families reveal that even state-of-the-art models struggle significantly with this task, achieving only 62% accuracy at best. The benchmark includes both open-ended generation and multiple-choice evaluation formats, exposing consistent performance gaps across all models and highlighting the ongoing challenges in logical reasoning for current language models.

## Method Summary
RuozhiBench evaluates LLMs using two formats: RuozhiBench-Gen (open-ended generation with 0-4 LLM-as-Judge scoring) and RuozhiBench-MC (multiple-choice accuracy). The dataset contains 677 bilingual questions categorized into six fallacy types, with 342 having paired normal versions. Models are prompted with questions and responses are evaluated by three independent LLM judges (GPT-4o, Claude-3.5-Sonnet, Llama-3.3-70B-Instruct) for generation format, or asked to select between good/bad response pairs for MC format. The benchmark reveals consistent performance gaps across model families, with larger models generally performing better but still well below human capability.

## Key Results
- Best-performing model (Claude-3-haiku) achieves only 62% accuracy, significantly below human performance of >90%
- Larger models consistently outperform smaller ones across both generation and multiple-choice formats
- Strong correlation (r=0.909) between generation and MC format scores suggests both measure a common underlying capability
- Positional bias affects MC performance, with models showing better accuracy when correct answer appears first
- High inter-evaluator variance (0.433 correlation between top judges) indicates evaluation reliability challenges

## Why This Works (Mechanism)
### Mechanism 1
- **Claim:** Models process deceptive questions through surface-level pattern matching rather than premise verification, leading to acceptance of false premises as valid input constraints.
- **Mechanism:** When presented with questions containing logical fallacies, models generate responses that engage with the false premise rather than rejecting it. The cow tongue example shows GPT-4o producing elaborate explanations about "mutual tasting" rather than identifying that a consumed beef tongue cannot taste anything.
- **Core assumption:** Failure stems from training on affirmational reasoning patterns (modus ponens) without counterfactual verification.
- **Evidence anchors:** [abstract]: "their ability to identify and respond to text containing logical fallacies or deliberately misleading premises remains less studied"; [section 3.2]: "best-performing model only achieved a score of 62.00"; [corpus]: "Addressing Logical Fallacies In Scientific Reasoning" suggests training paradigms dominated by affirmation-based inference.

### Mechanism 2
- **Claim:** Performance on fallacy detection scales with model size, but architectural and training differences create non-uniform improvements across fallacy categories.
- **Mechanism:** Figure 9 shows positive correlation between parameter count and performance on both generation and multiple-choice formats. However, category-specific variations reveal that smaller models disproportionately struggle with "Absurd Imagination" and "Erroneous Assumption" categories compared to "Scientific Misconception."
- **Core assumption:** Larger models encode richer world knowledge representations that enable more effective premise verification.
- **Evidence anchors:** [section 3.2]: "larger models consistently outperform their smaller counterparts"; [section 5.1]: "general finding that larger models perform better still holds" in MC evaluation; [corpus]: "Human-Level Reasoning" paper examines scale effects.

### Mechanism 3
- **Claim:** Evaluation format (generation vs. multiple-choice) exposes different model capabilities due to variance in evaluator reliability and positional bias.
- **Mechanism:** Generation evaluation relies on LLM-as-Judge with high inter-evaluator variance (Table 3 shows Claude vs. Llama correlation of only 0.433). Multiple-choice reduces evaluation noise but introduces positional bias—Qwen2.5-0.5B shows 100% accuracy when correct answer is first, 0% when second.
- **Core assumption:** The strong correlation between generation and MC scores (r=0.909) indicates both formats measure a common underlying capability rather than different skills.
- **Evidence anchors:** [section 3.4]: "Claude-3.5 shows notably weaker correlation with others"; [section 5.2]: "Pearson correlation coefficient of 0.909" between generation and MC scores.

## Foundational Learning
- **Concept: Logical Fallacy Taxonomy**
  - Why needed here: RuozhiBench categorizes questions into 6 types (Table 1)—understanding these distinctions is prerequisite for interpreting category-specific performance gaps.
  - Quick check question: Can you distinguish "Erroneous Assumption" (false starting point) from "Logical Error" (invalid reasoning from potentially true premises)?

- **Concept: LLM-as-Judge Evaluation**
  - Why needed here: Generation evaluation uses three independent judge models with 0-4 scoring; understanding evaluator variance is critical for interpreting results.
  - Quick check question: Why might Claude-3.5-Sonnet and GPT-4o give systematically different ratings to the same response?

- **Concept: Positional Bias in Multiple-Choice**
  - Why needed here: Table 4 shows models perform better when the correct answer appears first; experiment design must counterbalance option ordering.
  - Quick check question: How would you detect if a model's MC performance is driven by position preference rather than reasoning ability?

## Architecture Onboarding
- **Component map:** Data crawling (86.3k) → filtering (820) → translation + human check → categorization → final 677 → RuozhiBench-Gen (question → response → 3 judges → 0-4 rating) OR RuozhiBench-MC (question + option pairs → model selection → accuracy)
- **Critical path:** Question quality → Evaluator selection → Position counterbalancing → Score aggregation
- **Design tradeoffs:** Generation format captures nuanced reasoning but introduces evaluator variance; MC format provides standardized evaluation but may oversimplify complex reasoning; Bilingual dataset enables cross-linguistic analysis but introduces translation fidelity concerns
- **Failure signatures:** Model directly answers deceptive question without questioning premise (rating 0-1); Positional bias >15% indicates unreliable MC evaluation; Format compliance <60% indicates instruction-following failure confounding reasoning ability
- **First 3 experiments:**
  1. Baseline evaluation: Run target model on both Gen and MC formats; verify strong correlation (r>0.85) between formats to confirm valid measurement
  2. Category breakdown: Analyze performance by fallacy type (Table 1 categories); identify which reasoning patterns the model handles worst
  3. Ablation with paired normal questions: Compare performance on tricky vs. paired normal questions to quantify the specific cost of deceptive reasoning

## Open Questions the Paper Calls Out
- **Open Question 1:** Can fine-tuning on RuozhiBench-style fallacy data improve model performance on deceptive reasoning tasks without degrading performance on normal questions? (The paper states "suggesting several promising directions for future research, including the enhancement of model training")
- **Open Question 2:** What mechanisms underlie the substantial positional bias observed in multiple-choice evaluation? (The paper documents positional bias but does not investigate its architectural or training-related causes)
- **Open Question 3:** How does model performance on fallacy detection differ between Chinese and English, given the bilingual dataset but English-focused experiments? (The Limitations section states "although the dataset is bilingual, our experiments focus primarily on English")

## Limitations
- Translation fidelity may affect benchmark validity since questions originate from Chinese forums and undergo translation before evaluation
- LLM-as-Judge evaluation introduces significant uncertainty due to high inter-evaluator variance (correlation as low as 0.433 between top judges)
- The benchmark's reliance on internet-sourced questions may introduce domain-specific patterns that don't generalize to other fallacy types or real-world reasoning scenarios

## Confidence
- **High Confidence:** The finding that current models perform significantly below human capability (>90% vs. 62% best model) is well-supported by consistent results across 17 models and two evaluation formats.
- **Medium Confidence:** The correlation between model size and performance is observed but the underlying mechanism (world knowledge vs. pattern matching) remains speculative without direct evidence.
- **Low Confidence:** The claim that training paradigms dominated by affirmation-based inference cause premise acceptance is largely inferred from related work rather than directly tested within this study.

## Next Checks
1. **Translation Validation:** Evaluate models on both original Chinese and translated English versions of the same questions to quantify translation impact on performance and identify culturally-specific fallacy patterns.
2. **Evaluator Reliability Study:** Conduct ablation experiments removing each judge model to measure individual contribution to score variance and test whether human evaluation confirms LLM-as-Judge ratings.
3. **Cross-Domain Generalization:** Test models on RuozhiBench questions alongside other fallacy benchmarks (e.g., Socratic Logic) to determine if performance correlates across different fallacy taxonomies and domains.