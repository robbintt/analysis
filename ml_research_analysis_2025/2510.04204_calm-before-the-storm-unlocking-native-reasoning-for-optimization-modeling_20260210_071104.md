---
ver: rpa2
title: 'CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling'
arxiv_id: '2510.04204'
source_url: https://arxiv.org/abs/2510.04204
tags:
- reasoning
- code
- trigger
- modeling
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large reasoning
  models (LRMs) for optimization modeling tasks. Existing approaches often fail to
  leverage LRMs' advanced reasoning capabilities by relying on static, non-reflective
  datasets.
---

# CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling

## Quick Facts
- arXiv ID: 2510.04204
- Source URL: https://arxiv.org/abs/2510.04204
- Reference count: 40
- STORM achieves 68.9% macro-average accuracy across five optimization modeling benchmarks, matching 671B-parameter model performance

## Executive Summary
This paper addresses the challenge of adapting large reasoning models (LRMs) for optimization modeling tasks. Existing approaches often fail to leverage LRMs' advanced reasoning capabilities by relying on static, non-reflective datasets. To overcome this, the authors propose CALM (Corrective Adaptation with Lightweight Modification), a framework that uses an expert intervener to identify reasoning flaws and provide targeted corrective hints, preserving the model's native reasoning patterns. These refined trajectories are used for supervised fine-tuning, followed by reinforcement learning. The resulting STORM model, a 4B-parameter LRM, achieves state-of-the-art performance, with a macro-average accuracy of 68.9% across five optimization modeling benchmarks, matching the performance of a 671B-parameter model. This demonstrates the effectiveness of dynamic, hint-based data synthesis in aligning LRMs with expert-level optimization modeling.

## Method Summary
CALM employs an expert intervener to identify reasoning flaws in LRM-generated optimization trajectories and inject targeted hints. These corrected trajectories are used for supervised fine-tuning, followed by reinforcement learning with verifiable rewards. The approach preserves native reasoning patterns while improving performance on complex optimization tasks. The resulting STORM model achieves state-of-the-art results with a 4B-parameter architecture.

## Key Results
- STORM achieves 68.9% macro-average accuracy across five optimization modeling benchmarks
- Matches performance of 671B-parameter model despite being 4B-parameter
- Overcomes degradation pattern seen in direct fine-tuning approaches (33.1% → 6.6% on OptMath)
- Demonstrates +10.2% additional gain from reinforcement learning over SFT alone

## Why This Works (Mechanism)

### Mechanism 1: Preservation of Native Reasoning Patterns via Lightweight Intervention
**Claim:** Direct fine-tuning on non-reflective datasets disrupts LRMs' inherent multi-step reasoning capabilities, particularly for complex optimization tasks, whereas lightweight hint-based intervention preserves and amplifies native reasoning.

**Mechanism:** When LRMs are fine-tuned on static problem-solution pairs, they are pushed to replace their native reflective reasoning with rigid, single-pass generation. This improves performance on easy tasks but causes catastrophic degradation on complex tasks (e.g., OptMath: 33.1% → 6.6%). CALM's interventions modify <2.6% of tokens, providing corrective signals without overwriting the model's inherent reasoning patterns.

**Core assumption:** LRMs possess latent multi-step reasoning capabilities that are more effective than rigid pattern-matching when properly guided.

**Evidence anchors:**
- [abstract]: "existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs"
- [section 2.2, Table 1]: SFT on non-reflective data causes +14.9% gain on MAMO-Easy but -26.5% degradation on OptMath
- [corpus]: Limited direct support; neighboring papers (Safety Tax, RELIEF) discuss alignment-reasoning tradeoffs but don't specifically validate the lightweight intervention hypothesis

**Break condition:** If interventions require modifying >15-20% of tokens or if hint quality degrades (incorrect guidance), the mechanism may corrupt rather than preserve reasoning patterns.

### Mechanism 2: Two-Stage Behavioral Evolution (SFT Calibrates → RL Accelerates)
**Claim:** CALM-curated SFT provides behavioral calibration that dramatically improves RL sample efficiency and final performance ceiling.

**Mechanism:** SFT on CALM trajectories acts as a "soft adapter" that gently corrects reasoning flaws without overwriting native patterns (+1.6% macro gain). This calibrated foundation then enables RL to act as an "accelerator," driving rapid improvement (+10.2% additional macro gain) through computation-driven reasoning (more code blocks, fewer tokens).

**Core assumption:** High-quality expert demonstrations provide better inductive bias for RL than unguided reasoning trajectories.

**Evidence anchors:**
- [abstract]: "The adapted model is then further improved through reinforcement learning"
- [section 4.3.1, Figure 5]: SFT alone yields 57.1%→58.7%; SFT+RL yields 57.1%→68.9%
- [section 4.3.3, Figure 7a]: "RL with CALM model exhibits a steeper and more stable learning curve... control model learns far more slowly"
- [corpus]: Weak direct evidence; neighboring papers discuss RL for reasoning (RELIEF, Mitigating Overthinking) but don't validate this specific two-stage interaction

**Break condition:** If SFT data contains >10-15% flawed trajectories, the inductive bias becomes harmful and RL may fail to converge or reach lower performance ceiling.

### Mechanism 3: Reasoning-to-Computation Shift
**Claim:** Effective optimization modeling requires shifting from verbose natural language reasoning to concise, tool-assisted computation.

**Mechanism:** The combined SFT+RL pipeline produces a behavioral shift: increased code block usage (+40-60% in Figure 7b) and reduced response length (-25% in Figure 7c). This reflects expert-like behavior—replacing unreliable manual calculations with systematic solver execution.

**Core assumption:** Tool-augmented computation is more reliable than natural language reasoning for optimization tasks.

**Evidence anchors:**
- [section 2.3, Figure 3]: "Code Utilization Distrust" (triggers 1-3) is a major flaw category, especially on easier benchmarks
- [section 4.3.3, Figures 7b/7c]: CALM-trained models progressively increase code usage while reducing verbosity
- [corpus]: "Thinking Isn't an Illusion" paper supports tool augmentation benefits for reasoning models; "Demystifying Reasoning Dynamics" provides complementary evidence on thinking token information peaks

**Break condition:** If solver feedback is noisy/unreliable or if code execution environment is unstable, the mechanism may reinforce incorrect computation patterns.

## Foundational Learning

**Concept: Non-reflective vs. Reflective Generation Paradigms**
- **Why needed here:** Understanding this distinction is critical for grasping why traditional fine-tuning fails for LRMs
- **Quick check question:** Can you explain why a dataset of static problem-solution pairs might harm an LRM's inherent iterative reasoning capability?

**Concept: Operations Research (OR) Modeling Pipeline**
- **Why needed here:** Optimization modeling requires translating natural language → mathematical formulation → solver code → solution validation
- **Quick check question:** What are the key failure modes when an LRM attempts to solve an optimization problem without solver access?

**Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
- **Why needed here:** The second training stage uses binary reward signals based on solution correctness
- **Quick check question:** Why might RL alone be insufficient without high-quality SFT initialization?

## Architecture Onboarding

**Component map:**
```
Reasoner (Qwen3-4B-Thinking) → generates initial trajectory
        ↓
Intervener (Gemini-2.5-Pro) → detects flaws, injects hints
        ↓
Code Interpreter Environment → executes solver, returns output
        ↓
Trajectory Filter → retains only correct + flawless traces
        ↓
Two-Stage Training Pipeline:
  Stage 1: SFT on 112 "golden" trajectories
  Stage 2: GRPO-based RL with solver access (T=4 executions/rollout)
```

**Critical path:** CALM data curation quality → SFT behavioral calibration → RL sample efficiency → final STORM performance

**Design tradeoffs:**
- **Intervener choice:** Gemini-2.5-Pro provides high-quality hints but introduces external dependency; could substitute with other expert-level models
- **Max interventions (N=5):** Prevents infinite loops but may discard fixable trajectories; tuning this affects data yield
- **Golden trajectory filtering:** 20.4% selection rate (112/549) ensures purity but limits dataset size; could relax constraints for larger SFT sets

**Failure signatures:**
- SFT degrades complex task performance → likely training on non-reflective data or low-quality trajectories
- RL fails to improve → check SFT calibration quality, reward signal integrity, code execution stability
- Excessive verbosity post-training → Code Utilization Distrust not properly addressed in SFT data

**First 3 experiments:**
1. **Baseline replication:** Fine-tune base LRM on non-reflective OR-Instruct-3K dataset; verify degradation pattern on IndustryOR/OptMath
2. **Ablation study:** Train with CALM-SFT only (no RL) to isolate calibration effect; expect modest gains (+1-3%)
3. **Hint injection analysis:** Manually examine 20-30 interventions to verify hint quality and trigger-to-hint mapping accuracy; ensure interventions are genuinely lightweight (<5% token modification)

## Open Questions the Paper Calls Out
None

## Limitations
- Intervention quality threshold unclear: The exact boundary where lightweight intervention becomes harmful (>15-20% token modification) needs empirical validation
- Solver dependency risk: The code-heavy behavioral shift may create blind spots for optimization problems requiring non-solver reasoning
- Limited generalization evidence: The approach's effectiveness on optimization domains beyond the five benchmarks remains untested

## Confidence

**Mechanism 1: Medium** - Lightweight intervention preserves reasoning patterns is supported by performance gains, but the exact token modification threshold and its relationship to reasoning preservation remains theoretically underspecified.

**Mechanism 2: High** - The two-stage training pipeline shows clear empirical validation through learning curves and performance gains with well-documented quantitative metrics.

**Mechanism 3: Medium** - While the behavioral shift toward code utilization is documented, the paper doesn't fully address potential overfitting to the specific solver environment or explore whether this computational approach generalizes to optimization problems requiring non-solver reasoning.

## Next Checks

1. **Intervention Quality Analysis**: Conduct a systematic human evaluation of 100+ interventions to verify hint accuracy and assess whether the <2.6% token modification claim holds across diverse problem types. This will validate whether interventions truly preserve reasoning patterns or introduce subtle biases.

2. **Solver Dependency Assessment**: Test STORM on optimization tasks requiring mixed reasoning-computation approaches (e.g., problems needing both solver execution and mathematical derivation) to determine if the code-heavy behavioral shift creates blind spots for non-solver-optimizable problems.

3. **Generalization Stress Test**: Evaluate STORM on optimization problems from domains not represented in the training data (e.g., quantum computing optimization, game theory problems) to assess whether the CALM adaptation generalizes beyond the five benchmarks or simply memorizes their patterns.