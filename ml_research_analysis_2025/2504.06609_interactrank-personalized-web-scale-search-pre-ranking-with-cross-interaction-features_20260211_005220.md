---
ver: rpa2
title: 'InteractRank: Personalized Web-Scale Search Pre-Ranking with Cross Interaction
  Features'
arxiv_id: '2504.06609'
source_url: https://arxiv.org/abs/2504.06609
tags:
- user
- features
- search
- pre-ranking
- tower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InteractRank is a pre-ranking model for web-scale search systems
  that integrates cross-interaction features into a two-tower architecture to improve
  performance. By combining user engagement-based query-item interactions with the
  two-tower dot product, InteractRank achieves 6.5% improvement in engagement metrics
  over BM25 and 3.7% over a vanilla two-tower baseline in online A/B tests at Pinterest.
---

# InteractRank: Personalized Web-Scale Search Pre-Ranking with Cross Interaction Features

## Quick Facts
- arXiv ID: 2504.06609
- Source URL: https://arxiv.org/abs/2504.06609
- Reference count: 40
- Primary result: 6.5% improvement in engagement metrics over BM25 and 3.7% over vanilla two-tower baseline in online A/B tests at Pinterest

## Executive Summary
InteractRank is a pre-ranking model for web-scale search systems that integrates cross-interaction features into a two-tower architecture to improve performance. By combining user engagement-based query-item interactions with the two-tower dot product, InteractRank achieves 6.5% improvement in engagement metrics over BM25 and 3.7% over a vanilla two-tower baseline in online A/B tests at Pinterest. The model incorporates real-time user-sequence modeling and uses ItemQueryPerf (IQP) features derived from two years of engagement data to capture item-query relevance. Offline ablation studies show user engagement sequences and cross-interaction features are the most impactful components. InteractRank maintains minimal latency and computational costs while significantly boosting ranking quality.

## Method Summary
InteractRank is a two-tower pre-ranking model that combines semantic embeddings with historical engagement features. The Query Tower processes user context, pre-trained query embeddings, and real-time user engagement sequences using Query Cross Attention. The Item Tower processes engagement rates, metadata, and pre-trained item embeddings. The model incorporates Parallel MaskNet layers for in-tower feature crossing and uses a projection layer to combine the two-tower dot product with ItemQueryPerf (IQP) features derived from two years of engagement logs. Training uses a weighted combination of BCE and Sampled Softmax loss with in-batch negatives and logQ correction.

## Key Results
- 6.5% improvement in engagement metrics over BM25 in online A/B tests
- 3.7% improvement over vanilla two-tower baseline in online A/B tests
- Offline HITS@3 improvement of 6.6% over vanilla two-tower model

## Why This Works (Mechanism)

### Mechanism 1
Pre-computed historical engagement features (IQP) bypass the computational bottleneck of real-time cross-attention layers while capturing query-item relevance. The architecture decouples "expensive" interactions from the real-time request path by retrieving pre-calculated historical engagement probabilities from a lookup table and combining them with the dense two-tower dot product via a lightweight projection layer during inference.

### Mechanism 2
Real-time user-sequence modeling captures dynamic intent that static user embeddings miss. The Query Tower consumes the user's 100 most recent actions and employs Query Cross Attention where the current query embedding attends to the embeddings of recently engaged items, re-weighting the user's history to surface interests specifically relevant to the current search.

### Mechanism 3
A hybrid loss function aligns model confidence with the distinct goals of recall and ranking. The model is trained on a weighted combination of Binary Cross Entropy and Sampled Softmax, where BCE optimizes for specific engagement labels while Sampled Softmax forces the model to distinguish true items from random in-batch negatives, correcting distribution mismatch from training only on impressions.

## Foundational Learning

- **Two-Tower Architecture vs. Late Interaction**: Understanding that standard two-tower models cannot "interact" query and item features until the very end (dot product) is crucial. InteractRank is a hack to inject interaction features without changing the fundamental decoupled inference structure. *Quick check: Why can't we feed the raw query text into the Item Tower in a standard two-tower model?*

- **Pre-ranking vs. Ranking Constraints**: Pre-ranking must process ~100,000 items in milliseconds. Complex features allowed in ranking (like cross-attention transformers) are forbidden here. *Quick check: If you add a feature that adds 0.1ms per item, how much total latency does it add to the pre-ranking stage for 100k items? (Trick question: it must be vectorized/lookup-based, not iterative).*

- **Distribution Mismatch / Sampling Bias**: The paper notes that training on impression logs misses "easy negatives." *Quick check: Why might a model trained only on clicked/shown items score an obviously irrelevant item (e.g., "shoes" for a "pizza" query) highly?*

## Architecture Onboarding

- **Component map:** Offline Pipeline (IQP features + Item Embeddings) -> Forward Index (Item Embeddings + IQP Features) -> Online Request (Query Tower + Retrieval + Lookup + Scoring)
- **Critical path:** The IQP Lookup and Join. This is the novelty. The system must efficiently lookup "Did this user's query historically engage with this item?" features from the forward index for 100k items faster than a neural network pass.
- **Design tradeoffs:** IQP features improve HEAD queries significantly but offer negative or zero gain on TAIL/SINGLE queries. Storing "top K engaging queries" per item increases forward index size compared to vanilla two-tower model.
- **Failure signatures:** Stale priors cause sudden drops in relevance for popular queries if IQP offline pipeline fails. Cold start items with no IQP features rely solely on dot product, potentially under-ranking. Low Ï†_s weight may cause model to rank popular items high regardless of query relevance.
- **First 3 experiments:** 1) Ablation on Sequence Length: Reduce user sequence length from 100 to 10 to measure latency/performance tradeoff. 2) Head vs. Tail Segmentation: Validate Table 5 results in staging environment. 3) Latency Budget Profiling: Isolate latency of "Projection Layer" to confirm it is < 2ms for 100k items.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the analysis, several areas warrant further investigation regarding integration with architectures like IntTower, cold-start performance, and multi-task learning objectives.

## Limitations
- Model Architecture Complexity: Parallel MaskNet and Query Cross Attention introduce complexity that may not generalize beyond Pinterest's engagement patterns.
- Offline Computation Bottleneck: IQP feature extraction requires substantial offline computation and storage not quantified in the paper.
- Distribution Shift Risk: Contrastive loss assumes in-batch negatives approximate "easy negatives," which may not hold during trending events or seasonal shifts.

## Confidence
- **High Confidence**: Core contribution of integrating pre-computed cross-interaction features is well-supported by offline ablation studies and online A/B test results.
- **Medium Confidence**: Real-time sequence modeling claims are supported by ablation studies but may be overfit to Pinterest's visual, engagement-heavy platform.
- **Low Confidence**: Hybrid loss function's contribution is less clear from ablation studies; exact impact of sampled softmax component versus BCE component is not isolated.

## Next Checks
1. **Cold Start Performance Analysis**: Measure InteractRank's performance on new items and queries with no IQP history to validate whether two-tower dot product baseline is sufficient.
2. **Latency Breakdown Under Load**: Profile actual serving latency of projection layer at scale under production traffic patterns to verify "minimal latency" claim.
3. **Cross-Domain Transferability**: Test IQP feature extraction methodology on a different domain (e.g., text-based search) to validate whether engagement-based interaction features generalize beyond Pinterest.