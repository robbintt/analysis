---
ver: rpa2
title: 'Better Supervised Fine-tuning for VQA: Integer-Only Loss'
arxiv_id: '2508.11170'
source_url: https://arxiv.org/abs/2508.11170
tags:
- video
- quality
- assessment
- evaluation
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving video quality assessment
  (VQA) using vision language models (VLMs). The proposed method, IOVQA (Integer-Only
  VQA), introduces a novel fine-tuning approach that converts decimal Mean Opinion
  Scores (MOS) into integer labels and employs an integer-only mask during loss computation.
---

# Better Supervised Fine-tuning for VQA: Integer-Only Loss

## Quick Facts
- arXiv ID: 2508.11170
- Source URL: https://arxiv.org/abs/2508.11170
- Reference count: 35
- 3rd place in VQualA 2025 GenAI-Bench AIGC Video Quality Assessment Challenge with 0.70 final score (average of SRCC and PLCC)

## Executive Summary
This paper addresses the challenge of improving video quality assessment (VQA) using vision language models (VLMs). The proposed method, IOVQA (Integer-Only VQA), introduces a novel fine-tuning approach that converts decimal Mean Opinion Scores (MOS) into integer labels and employs an integer-only mask during loss computation. This design leverages VLMs' inherent strengths in predicting integer values over decimal numbers, enhancing both accuracy and stability. By integrating this integer-focused strategy with prompt engineering and structural output constraints, the method aligns model evaluations more closely with human perception. Experimental results on the TaobaoVD-GC dataset demonstrate significant performance gains, achieving a 3rd place ranking in the VQualA 2025 GenAI-Bench AIGC Video Quality Assessment Challenge with a final score of 0.70 (average of SRCC and PLCC). Ablation studies confirm that integer labels and the integer-only mask synergistically improve results, outperforming both decimal-label baselines and softmax regression alternatives.

## Method Summary
The IOVQA method converts decimal MOS labels (1-5 range) into integers (10-50 range) by rounding to one decimal place and multiplying by ten. During training, an integer-only mask is applied to the loss function, masking all tokens except the 2-digit integer score. The approach uses LoRA fine-tuning on Qwen2.5-VL with r∈{32,128}, α=32, dropout=0.1, learning rate 3e-4, AdamW optimizer, cosine scheduler, and 6 epochs. Model ensemble combines 5 models with fixed weights [0.25, 0.15, 0.25, 0.1, 0.25]. For inference, temperature=0 is used. The method samples 1-2 frames per video and constrains outputs to single integers through prompt engineering.

## Key Results
- Achieved 3rd place in VQualA 2025 Challenge with final score of 0.70
- Integer labels improved performance from 0.60 to 0.63 compared to decimal labels
- Integer-only mask further improved performance from 0.63 to 0.64
- 7B model outperformed larger 72B model (0.642 vs 0.622), suggesting overfitting with larger models

## Why This Works (Mechanism)
VLMs are inherently better at predicting integers than decimals because their tokenization and probability distributions align more naturally with discrete values. The integer-only mask ensures the model focuses exclusively on learning the score prediction by masking out all other tokens during loss computation. This approach exploits the VLM's strength in generating single-token integers while avoiding the uncertainty introduced by decimal point tokenization.

## Foundational Learning
- **Integer vs Decimal Prediction**: VLMs show stronger performance on discrete tasks due to token distribution alignment
  - Why needed: Standard VLMs struggle with continuous decimal prediction in VQA
  - Quick check: Compare integer-only vs decimal prediction accuracy on validation set

- **Integer-only Masking**: Loss computation that masks all tokens except the integer score
  - Why needed: Prevents dilution of score learning by prompt/boilerplate tokens
  - Quick check: Measure correlation improvement with and without masking

- **Prompt Engineering for VQA**: Structured prompts embedding evaluation dimensions and output constraints
  - Why needed: Guides model to focus on specific quality aspects and output format
  - Quick check: Validate prompt adherence through output inspection

## Architecture Onboarding

**Component Map**: TaobaoVD-GC Dataset -> Frame Sampling -> Integer Label Conversion -> IOVQA Fine-tuning -> Model Ensemble -> Final Score

**Critical Path**: Input video → Frame sampling (1-2 frames) → Prompt engineering (4 quality dimensions) → Integer MOS conversion → LoRA fine-tuning with integer mask → Ensemble inference → SRCC/PLCC evaluation

**Design Tradeoffs**: Frame subsampling (1-2 frames) reduces computational cost but may miss temporal quality signals; Integer conversion simplifies prediction but may lose precision; Integer-only mask focuses learning but requires careful token identification

**Failure Signatures**: Model generates explanatory text instead of single integer; Performance degrades with larger models suggesting overfitting; Decimal predictions show higher uncertainty due to tokenization

**First Experiments**: 
1. Implement integer-only mask and compare against standard cross-entropy on synthetic VQA data
2. Test different frame sampling strategies (middle frame vs uniform sampling) on validation set
3. Compare integer conversion granularity (0.5 vs 0.1 increments) for optimal performance

## Open Questions the Paper Calls Out

**Open Question 1**: Does the performance degradation observed in larger parameter models (e.g., 72B) stem strictly from dataset insufficiency, or does the integer-label paradigm saturate differently across model scales? The paper notes scaling from 7B to 72B did not improve performance, hypothesizing it "may lie in limited size of dataset, leading to overfitting," but this is not empirically verified.

**Open Question 2**: To what extent does the extreme frame subsampling (1-2 frames) limit the model's ability to accurately score "temporal quality"? While the authors claim minimal semantic variation justifies low frame counts, it is unclear if static frames provide enough signal for a model to learn motion-related artifacts which are critical for VQA.

**Open Question 3**: Is the performance gain of IOVQA generalizable to non-AIGC video quality assessment tasks (e.g., user-generated content or compression artifacts)? The paper evaluates exclusively on TaobaoVD-GC dataset containing AI-generated videos, but it's unclear if the approach transfers to natural video artifacts.

## Limitations
- Implementation details for integer-only mask mechanism lack explicit algorithmic description
- Exact prompt template containing four evaluation dimensions is not fully provided
- Frame sampling methodology for multi-frame videos remains undefined
- Experimental validation relies on single dataset and one VLM architecture

## Confidence

**High Confidence**: The core methodological innovation—converting decimal MOS to integers and applying integer-only masking—is well-documented through ablation studies demonstrating consistent performance improvements (8/10)

**Medium Confidence**: Implementation specifics for critical components like the integer-only mask and prompt templates require substantial inference (6/10)

**Low Confidence**: Generalization claims beyond the TaobaoVD-GC dataset and Qwen2.5-VL architecture are unsupported (4/10)

## Next Checks

1. Implement and validate the integer-only mask mechanism by creating a controlled experiment comparing standard cross-entropy loss against the proposed masked variant on a synthetic VQA dataset with known integer labels

2. Conduct systematic ablation of integer conversion granularity by testing multiple rounding/precision schemes (e.g., rounding to 0.5 increments vs 0.1 increments vs no rounding) on the same dataset

3. Validate frame sampling impact by implementing and comparing multiple frame selection strategies (middle frame, uniform sampling, key frame extraction) on a subset of the dataset