---
ver: rpa2
title: 'Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning
  Capabilities'
arxiv_id: '2511.00340'
source_url: https://arxiv.org/abs/2511.00340
tags:
- legal
- text
- contradiction
- contract
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CLAUSE, a novel benchmark to systematically\
  \ evaluate LLMs\u2019 ability to detect fine-grained legal discrepancies in contracts.\
  \ CLAUSE generates over 7,500 perturbed contracts from real-world datasets (CUAD\
  \ and ContractNLI) across ten anomaly categories\u2014five for in-text contradictions\
  \ and five for legal contradictions\u2014using a persona-driven AI pipeline with\
  \ retrieval-augmented validation."
---

# Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities

## Quick Facts
- **arXiv ID**: 2511.00340
- **Source URL**: https://arxiv.org/abs/2511.00340
- **Reference count**: 40
- **Primary result**: Novel benchmark revealing LLMs' significant weaknesses in detecting fine-grained legal discrepancies, especially legal contradictions against statutes.

## Executive Summary
CLAUSE is a novel benchmark that systematically evaluates LLMs' ability to detect fine-grained legal discrepancies in contracts. It generates over 7,500 perturbed contracts from real-world datasets (CUAD and ContractNLI) across ten anomaly categories—five for in-text contradictions and five for legal contradictions—using a persona-driven AI pipeline with retrieval-augmented validation. Human experts validated 25% of perturbations, confirming 98.58% as meaningful contradictions. Experiments across four models (GPT-4o-mini, Gemini 2.0, Gemini 2.5, LLaMa 3.3) revealed significant weaknesses: models often miss subtle errors, struggle with legal justification, and show inconsistent performance across categories and datasets.

## Method Summary
CLAUSE uses Gemini 2.0 Flash with persona-based prompts to generate contract perturbations across 10 categories (5 in-text, 5 legal). Each perturbation undergoes RAG-based validation using scraped statutory text via Gemini 2.5 Flash. Human experts validate 25% of perturbations (98.58% agreement). Evaluation uses three tasks of increasing complexity: binary detection (Eval_1), three-way classification (Eval_2), and span localization plus explanation with legal citations (Eval_3). Models are evaluated using location alignment metrics (ROUGE, METEOR, BERTScore) and LLM-judge scores for explanation quality.

## Key Results
- Models consistently underperform on legal contradiction detection compared to in-text detection across all architectures and datasets
- Explanation quality shows high Clarity (>4.0) but low Completeness (<2.0), indicating fluent but shallow reasoning
- Law citation matching remains extremely limited (<14% best performance), suggesting retrieval or knowledge gaps
- One-shot prompting reduces miss rates but increases false positive rates, creating precision-recall tradeoffs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical perturbation taxonomy enables systematic failure-mode decomposition across internal coherence and statutory compliance dimensions.
- Mechanism: By separating in-text contradictions (internal document inconsistencies) from legal contradictions (external statutory violations), the benchmark isolates whether models fail on semantic coherence reasoning vs. external legal knowledge retrieval. Each dimension contains five subtypes (ambiguity, omission, misaligned terminology, structural flaws, inconsistencies), creating a 10-category diagnostic grid that reveals architecture-specific weaknesses.
- Core assumption: Models possess different capabilities for internal logical reasoning versus external legal knowledge application; these can be meaningfully decomposed through targeted perturbations.
- Evidence anchors:
  - [abstract] "generates 10 distinct anomaly categories, which are then validated against official statutes using a Retrieval-Augmented Generation (RAG) system"
  - [Section 2] "ten perturbation categories that span two fundamental dimensions: Legal Contradictions... InText Contradictions"
  - [corpus] ContractEval uses clause-level risk identification but does not systematically decompose contradiction types; CLAUSE fills this gap.
- Break condition: If models exhibit correlated failure patterns across in-text and legal categories (e.g., both ambiguity types fail together), the dimensional separation provides less diagnostic value than subtype-level analysis.

### Mechanism 2
- Claim: Persona-driven prompting with retrieval grounding increases perturbation realism and legal fidelity compared to unconstrained generation.
- Mechanism: The pipeline assigns domain-specific roles (compliance officers, legal editors) to guide LLMs toward plausible legal modifications, then augments generation with scraped statutory text via RAG before binary classification filtering. This multi-stage filtering (generation → RAG validation → automated quality checks → expert review) creates a quality gradient that retains only strong, unambiguous contradictions.
- Core assumption: Persona framing induces the generation model to produce more legally plausible perturbations; RAG grounding provides external knowledge anchoring that pure parametric generation lacks.
- Evidence anchors:
  - [Section 2] "Three complementary prompting strategies ensure legal plausibility: persona-based roles (compliance officers, legal editors), few-shot examples, and chain-of-thought reasoning"
  - [Section 2] "For legal contradictions, we scrape legal text from the provided URLs and augment the verification prompt with this statutory context"
  - [corpus] ACORD demonstrates retrieval for contract drafting but does not use retrieval for contradiction validation; CLAUSE extends this pattern to verification.
- Break condition: If the generation model (Gemini 2.0 Flash) has systematic biases toward certain perturbation types (e.g., over-generating ambiguities but under-generating structural flaws), category difficulty differences may reflect generation artifacts rather than target model capabilities.

### Mechanism 3
- Claim: Hierarchical evaluation tasks (detection → classification → explanation with citation) reveal depth-graded reasoning failures.
- Mechanism: Three evaluation levels progressively increase cognitive demand: Eval_1 requires binary detection (minimal reasoning), Eval_2 requires three-way classification (type discrimination), Eval_3 requires span localization plus natural language explanation plus legal citation (multi-component generation). Performance degradation across levels isolates where reasoning breaks down.
- Core assumption: Performance gaps between adjacent evaluation levels reflect genuine reasoning limitations rather than prompting artifacts.
- Evidence anchors:
  - [Section 4] "Each task is implemented using tailored prompting strategies, with increasing granularity and reasoning complexity from Eval_1 through Eval_3"
  - [Section 5.1] "Models universally generate text with high Clarity (often 4.0+) but low Completeness (<2.0), indicating fluent but shallow reasoning"
  - [corpus] Weak corpus connection—no directly comparable hierarchical evaluation frameworks found in neighbor papers.
- Break condition: If one-shot prompting (L2) degrades performance on certain categories (as observed for Gemini-2.5 on CUAD location alignment), prompting strategy confounds may mask genuine capability differences.

## Foundational Learning

- **Contract Perturbation Taxonomy**:
  - Why needed here: Understanding the 10-category structure (5 in-text: ambiguity, omission, misaligned terminology, structural flaws, inconsistencies; plus their legal counterparts) is essential for interpreting category-specific performance patterns and identifying failure signatures.
  - Quick check question: Can you explain why "omission legal" might be harder than "omission in-text" for models to detect?

- **Retrieval-Augmented Validation**:
  - Why needed here: The RAG stage grounds legal contradictions against actual statutes, distinguishing parametric knowledge failures from retrieval failures in model evaluation.
  - Quick check question: How would you determine whether poor legal citation matching reflects weak retrieval, weak knowledge, or both?

- **LLM-as-Judge Evaluation Paradigm**:
  - Why needed here: Eval_3 uses LLMs (GPT-4o, Gemini-2.5) to score explanation quality on 0-5 scales across Accuracy, Clarity, Completeness, and Legal Reasoning. Understanding judge variability (Gemini-2.5 judges consistently score lower on Completeness) is critical for interpreting results.
  - Quick check question: Why might cross-architecture judging (e.g., Gemini judging GPT outputs) introduce systematic bias?

## Architecture Onboarding

- **Component map**:
  Source datasets (CUAD, ContractNLI) → Document extraction + metadata parsing → Generation pipeline (Gemini 2.0 Flash, temp=0.7) → Persona-driven perturbation creation → RAG validation (Gemini 2.5 Flash, temp=0.2) → Binary YES/NO filtering with scraped statutory context → Human verification (3 NLP experts, 25% sample) → Quality validation (98.58% agreement) → Evaluation harness (4 models × 3 tasks × 2 prompting levels) → Multi-metric scoring

- **Critical path**:
  1. Perturbation quality hinges on the RAG validation stage—if scraped law snippets are missing or uninformative, binary classification degrades to parametric guessing.
  2. Location alignment metrics depend on the graph-based matching algorithm; sentence-level tokenization choices affect connected component detection.
  3. Explanation quality evaluation depends on judge model selection; the 0.5-1.0 point systematic differences between GPT-4o and Gemini-2.5 judges create scoring variance.

- **Design tradeoffs**:
  - **AI-driven generation vs. human-crafted perturbations**: Scalability (7,500+ contracts) gained, but perturbation character shaped by generation model's biases. Paper acknowledges: "The nature of these flaws is influenced by the capabilities of the generation model (Gemini 2.0 Flash)"
  - **Binary vs. graded validation**: Simplicity and throughput gained, but nuanced risk levels collapsed. Paper notes validation "simplifies risk into a 'YES/NO' decision"
  - **LLM-as-judge vs. human evaluation**: Cost efficiency and scale gained, but systematic judge variance observed (Table 8 shows 0.16-0.28 point differences across metrics)

- **Failure signatures**:
  - **High-Clarity/Low-Completeness explanations**: Models generate fluent but shallow justifications (Clarity 4.0+, Completeness <2.0) across all categories—indicates surface-level comprehension without legal depth
  - **Precision-recall tradeoff with L2 prompting**: One-shot examples reduce miss rates but increase false positive rates (Table 11 shows 10-20 point swings for GPT-4o-mini and Gemini-2.5)
  - **Category-specific brittleness**: Omission_Legal on NLI shows F1 scores as low as 9.3% (LLaMa-3.3), Structural_Flaws_Legal on CUAD shows 6.9% F1 (LLaMa-3.3)—these categories expose fundamental gaps in absence detection and structural reasoning
  - **Law citation matching failure**: Even best model (Gemini-2.5) achieves only 13.7% law_match on CUAD L1, dropping to 7.4% on NLI—indicates external legal knowledge retrieval is severely limited

- **First 3 experiments**:
  1. **Baseline category comparison**: Run all four models on a stratified sample (100 documents per category) across both datasets to establish category difficulty rankings and identify systematic cross-model patterns. Focus on the precision-recall tradeoff between L1 and L2 prompting.
  2. **Judge calibration study**: Compare GPT-4o vs. Gemini-2.5 judge scores on a held-out subset (15% of Eval_3 outputs) with human expert annotations to quantify systematic bias and determine which judge aligns more closely with human assessment. Reference Table 8 methodology.
  3. **Retrieval ablation**: For legal contradiction categories, evaluate model performance with vs. without access to scraped law snippets during generation. This isolates whether law citation failures stem from weak retrieval infrastructure or weak parametric knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:**
  Can LLMs perform holistic analysis when faced with compound discrepancies involving multiple interacting legal flaws (e.g., an ambiguous definition causing a structural flaw) within a single contract?
- **Basis in paper:**
  [Explicit] The authors state in the Limitations section that real-world contracts often contain "multiple, interacting issues," and they leave the creation of benchmarks for such "compound discrepancies" as an "exciting direction for future research."
- **Why unresolved:**
  The current CLAUSE methodology generates distinct perturbations for each document to test 10 types in isolation, preventing the evaluation of holistic reasoning capabilities.
- **What evidence would resolve it:**
  A new benchmark version featuring interacting flaw types and an evaluation of model performance on these complex, multi-faceted tasks compared to their isolated performance.

### Open Question 2
- **Question:**
  What architectural or training interventions are required to significantly improve LLMs' ability to perform semantic matching of legal citations, given the current best performance is under 14%?
- **Basis in paper:**
  [Explicit] The authors explicitly highlight in their findings that "The ability of LLMs to semantically match legal citations is extremely limited," with the best model (Gemini 2.5) scoring below 14%, primarily because models "struggle to extract the external knowledge needed."
- **Why unresolved:**
  The paper identifies this low score as a key weakness but only evaluates existing "out-of-the-box" models rather than testing solutions like specialized legal retrievers.
- **What evidence would resolve it:**
  An ablation study comparing RAG-heavy architectures against fine-tuned parametric models on the `law_match` metric within the CLAUSE framework.

### Open Question 3
- **Question:**
  Can models be optimized to improve the "completeness" of legal explanations without sacrificing "clarity," resolving the fluency-substance trade-off identified in the evaluation?
- **Basis in paper:**
  [Explicit] The analysis of Eval_3 shows a critical trade-off where models universally generate text with high Clarity (often 4.0+) but low Completeness (<2.0), indicating "fluent but shallow reasoning."
- **Why unresolved:**
  This pattern is consistent across all tested architectures, suggesting a fundamental limitation in how current LLMs prioritize linguistic fluency over deep legal substance.
- **What evidence would resolve it:**
  The development of a prompting strategy or model variant that achieves a Completeness score >3.5 while maintaining a Clarity score >4.0 on the CLAUSE explanation metrics.

## Limitations

- The benchmark's perturbation generation pipeline relies heavily on Gemini 2.0 Flash model's biases, potentially over-representing certain anomaly types while under-representing others.
- RAG validation effectiveness depends on the quality and completeness of scraped statutory snippets—if URLs fail or provide insufficient context, binary validation degrades to parametric guessing.
- LLM-as-judge evaluation introduces systematic variance between judge models (GPT-4o vs. Gemini-2.5 show 0.16-0.28 point differences on explanation quality) without human-judge calibration study.

## Confidence

- **High confidence**: Detection performance degradation across evaluation levels (Eval_1→Eval_2→Eval_3) is well-supported by consistent patterns across all four models and both datasets.
- **Medium confidence**: Category-specific difficulty rankings (e.g., legal contradictions consistently harder than in-text) are supported by data but may reflect generation model biases rather than inherent model capability gaps.
- **Low confidence**: Claims about retrieval augmentation improving legal contradiction detection lack empirical validation—no ablation study compares performance with vs. without access to scraped law snippets during evaluation.

## Next Checks

1. **Judge calibration study**: Compare GPT-4o and Gemini-2.5 judge scores on a held-out subset (15% of Eval_3 outputs) with human expert annotations to quantify systematic bias and determine which judge aligns more closely with human assessment.
2. **Retrieval ablation experiment**: For legal contradiction categories, evaluate model performance with vs. without access to scraped law snippets during generation to isolate whether law citation failures stem from weak retrieval infrastructure or weak parametric knowledge.
3. **Cross-dataset robustness test**: Run a stratified evaluation (100 documents per category) on an external legal corpus (e.g., EDGAR filings) to assess whether performance patterns replicate beyond CUAD and ContractNLI, testing generalizability of the 10-category taxonomy.