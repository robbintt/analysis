---
ver: rpa2
title: 'Understanding Public Perception of Crime in Bangladesh: A Transformer-Based
  Approach with Explainability'
arxiv_id: '2507.21234'
source_url: https://arxiv.org/abs/2507.21234
tags:
- sentiment
- data
- dataset
- accuracy
- bengali
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates public perception of crime-related news\
  \ in Bangladesh by classifying user-generated Bangla social media comments into\
  \ positive, negative, and neutral categories. A newly curated dataset of 28,528\
  \ comments was used to train and evaluate a transformer-based model employing XLM-RoBERTa\
  \ Base architecture, achieving 97% accuracy\u2014outperforming existing Bangla sentiment\
  \ analysis methods."
---

# Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability

## Quick Facts
- arXiv ID: 2507.21234
- Source URL: https://arxiv.org/abs/2507.21234
- Reference count: 23
- XLM-RoBERTa Base achieved 97% accuracy on 3-class Bangla sentiment classification

## Executive Summary
This study addresses public perception analysis of crime-related news in Bangladesh through sentiment classification of user-generated Bangla social media comments. The researchers developed a new dataset of 28,528 comments and employed a transformer-based approach using XLM-RoBERTa Base architecture. The model achieved 97% accuracy, significantly outperforming existing Bangla sentiment analysis methods. Explainable AI techniques were applied to identify key features influencing sentiment classification, demonstrating the effectiveness of transformer models for low-resource languages like Bengali.

## Method Summary
The researchers curated a dataset of 28,528 Bangla social media comments from Facebook and YouTube crime-related news. Text preprocessing removed special characters and symbols, followed by tokenization using XLM-RoBERTa's BPE-based tokenizer. Data augmentation was performed through back-translation (Bangla→English→Bangla) using GoogleTrans. The XLM-RoBERTa Base model was fine-tuned with AdamW optimizer (learning rate 2e-5, batch size 16, 15 epochs), achieving 97% accuracy. LIME explainability was applied to interpret model predictions and identify key features.

## Key Results
- XLM-RoBERTa Base achieved 97% accuracy on 3-class sentiment classification (positive, negative, neutral)
- Outperformed existing methods: LSTM (94.4%), LSTM+Attention (94.6%), and BanglaBERT (91.2%)
- Per-class F1-scores: Positive (0.97), Negative (0.95), Neutral (0.99)
- Training vs validation accuracy gap: 1.6%, indicating minimal overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual pretraining enables robust sentiment classification for low-resource languages.
- Mechanism: XLM-RoBERTa leverages pretrained multilingual representations from 100 languages, transferred to Bangla through fine-tuning on crime-comment dataset.
- Core assumption: Multilingual pretraining representations are transferable to Bangla crime-related social media comments.
- Evidence anchors: Achieved 97% accuracy outperforming existing Bangla methods; paper states "effectiveness of transformer-based models in processing low-resource languages such as Bengali"
- Break condition: Performance degrades if target language/domain has features absent from pretraining data or dataset lacks demographic diversity.

### Mechanism 2
- Claim: BPE tokenization preserves semantic integrity for informal Bangla text.
- Mechanism: XLM-RoBERTa's BPE tokenizer segments text into subword units, handling rare words, slang, and code-mixed patterns in social media comments.
- Core assumption: BPE subword units effectively capture semantic meaning of informal Bangla text.
- Evidence anchors: Paper describes tokenizer as preserving "semantic integrity, handles rare words effectively, and supports multilingual input"
- Break condition: Segmentation fails on highly non-standard slang or scripts not covered by tokenizer vocabulary.

### Mechanism 3
- Claim: Back-translation mitigates data sparsity through augmentation.
- Mechanism: Raw Bangla comments translated to English then back to Bangla create synthetic paraphrased training examples.
- Core assumption: Back-translation generates linguistically valid and semantically consistent variations.
- Evidence anchors: "Applied translation-based augmentation to enrich the dataset and reduce sparsity" with back-translation pipeline described
- Break condition: Poor translation quality introduces noise, grammatical errors, or semantic shifts degrading performance.

## Foundational Learning

- Concept: Transformer Architecture & Self-Attention
  - Why needed here: Core architecture of XLM-RoBERTa model; understanding self-attention is necessary to grasp how model processes sequential text data
  - Quick check question: How does self-attention mechanism differ from recurrent connections in LSTM?

- Concept: Pretraining vs. Fine-tuning
  - Why needed here: Study relies on pretrained multilingual model (XLM-RoBERTa) fine-tuned on specific Bangla dataset; distinguishing phases is key to understanding transfer learning approach
  - Quick check question: What data was XLM-RoBERTa trained on during pretraining, and what data is it trained on during fine-tuning phase?

- Concept: Explainability (XAI) with LIME
  - Why needed here: Paper uses LIME to interpret model predictions; understanding LIME's perturbation-based approach is necessary to evaluate interpretability claims
  - Quick check question: Does LIME explain model's internal weights directly, or does it explain model's behavior around specific prediction?

## Architecture Onboarding

- Component map:
  Input Layer (Raw Bangla comments) -> Preprocessing Module (Text cleaning) -> Tokenizer (XLM-RoBERTa BPE) -> Augmentation Module (Back-translation) -> Core Model (XLM-RoBERTa Base) -> Output Layer (3-class classification) -> Interpretability Layer (LIME)

- Critical path:
  1. Data Collection -> 2. Preprocessing & Tokenization -> 3. Data Augmentation -> 4. Model Fine-tuning -> 5. Inference & Evaluation -> 6. Explainability Analysis

- Design tradeoffs:
  - Accuracy vs. Interpretability: Complex transformer provides high accuracy (97%) but requires external explainability tool (LIME)
  - Data Volume vs. Quality (Augmentation): Back-translation increases size but may introduce noise if translation quality is poor
  - Generalizability vs. Domain Specificity: Fine-tuning on crime-related comments optimizes for that domain but may reduce performance on general Bangla sentiment tasks

- Failure signatures:
  - Overfitting: High training accuracy but significantly lower validation/test accuracy
  - Poor Generalization: Model fails on new, unseen comments especially with different slang or demographics
  - Misclassification of Negation/Sarcasm: Confusion matrix shows positive-negative confusion
  - Tokenization Failure: Model fails on text with scripts not in pretrained tokenizer vocabulary

- First 3 experiments:
  1. Baseline Comparison: Train and evaluate LSTM, LSTM+Attention, and BanglaBERT on same dataset to establish baseline against XLM-RoBERTa
  2. Ablation Study on Augmentation: Train XLM-RoBERTa on original non-augmented dataset and compare performance against augmented model
  3. Cross-Domain Validation: Evaluate fine-tuned model on different Bangla sentiment dataset (e.g., movie reviews) to test generalizability

## Open Questions the Paper Calls Out

- Question: How does integration of multimodal data (e.g., images, video) impact accuracy and robustness of sentiment classification for crime-related Bangla content?
  - Basis in paper: [explicit] Conclusion states "authors aim to analyse sentiment using multimodal dataset"
  - Why unresolved: Current study relies exclusively on text-based comments, ignoring visual/audio cues
  - What evidence would resolve it: Comparative benchmarks of proposed model against multimodal fusion model trained on same crime events

- Question: To what extent does current model's performance generalize to rural populations and digitally disconnected demographics in Bangladesh?
  - Basis in paper: [explicit] Authors acknowledge dataset "may over-represent urban users with internet access" and suggest expanding to include more representative sample
  - Why unresolved: Current data collection filters out opinions from rural areas or those without consistent internet access
  - What evidence would resolve it: Stratified sampling study including offline surveys or data from regions with lower internet penetration

- Question: Can domain-adaptive pre-training or fine-tuning strategies improve BanglaBERT performance to match or exceed XLM-RoBERTa on specific crime-related sentiment tasks?
  - Basis in paper: [inferred] Results show BanglaBERT achieved 91.2% vs XLM-RoBERTa's 97%, attributed to potential "domain mismatch or fine-tuning limitations"
  - Why unresolved: Paper does not investigate why BanglaBERT underperformed compared to multilingual model
  - What evidence would resolve it: Experiments applying domain-adaptive pre-training to BanglaBERT using crime-news corpus before fine-tuning

## Limitations

- Dataset Accessibility: Core dataset of 28,528 Bangla comments not publicly available, preventing independent verification
- Data Augmentation Quality: No evaluation of back-translation quality or semantic consistency of augmented data
- Demographic Bias: Acknowledged urban skew in dataset but biases not quantified or their impact measured

## Confidence

- High Confidence: Cross-lingual pretraining mechanism is well-supported by 97% accuracy result and comparison with existing Bangla methods
- Medium Confidence: BPE tokenization effectiveness and back-translation augmentation impact described but lack empirical validation
- Low Confidence: Claims about informing public policy and crime prevention strategies extend beyond technical scope requiring additional validation

## Next Checks

1. Replicate with Open Dataset: Replicate XLM-RoBERTa fine-tuning using publicly available Bangla sentiment dataset to test generalizability beyond crime-related content

2. Augmentation Quality Assessment: Implement manual/automated quality check of back-translated data to quantify semantic drift and grammatical errors; compare model performance with and without augmentation

3. Cross-Demographic Testing: Evaluate fine-tuned model on Bangla comments from different demographic sources (e.g., rural vs. urban platforms) to quantify impact of acknowledged dataset biases on real-world performance