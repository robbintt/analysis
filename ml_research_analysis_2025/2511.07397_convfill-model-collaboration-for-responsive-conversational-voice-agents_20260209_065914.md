---
ver: rpa2
title: 'ConvFill: Model Collaboration for Responsive Conversational Voice Agents'
arxiv_id: '2511.07397'
source_url: https://arxiv.org/abs/2511.07397
tags:
- convfill
- conversational
- knowledge
- backend
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying conversational voice
  agents that are both responsive and knowledgeable. The authors propose a hybrid
  architecture where a lightweight on-device model (ConvFill) generates contextually
  appropriate dialogue while seamlessly incorporating streaming knowledge from a powerful
  backend model, enabling low-latency responses without sacrificing reasoning capabilities.
---

# ConvFill: Model Collaboration for Responsive Conversational Voice Agents

## Quick Facts
- arXiv ID: 2511.07397
- Source URL: https://arxiv.org/abs/2511.07397
- Reference count: 10
- Primary result: 360M parameter model achieves sub-200ms TTFT and 46-52% QA accuracy on NaturalQuestions by streaming knowledge from backend models

## Executive Summary
The paper addresses the challenge of deploying conversational voice agents that are both responsive and knowledgeable. The authors propose a hybrid architecture where a lightweight on-device model (ConvFill) generates contextually appropriate dialogue while seamlessly incorporating streaming knowledge from a powerful backend model, enabling low-latency responses without sacrificing reasoning capabilities. ConvFill, a 360M parameter model trained on synthetic multi-domain conversations, achieves sub-200ms response latencies and accuracy improvements of 36-42% over standalone small models of the same size when paired with various backend models (GPT-5, Claude Sonnet 4.5, Gemini-2.5-Pro).

## Method Summary
ConvFill finetunes SmolLM2-360M-Instruct with special `<|sil|>` and `knowledge` tokens, trained on synthetic conversations validated via entailment. The system runs parallel threads: a backend model generates knowledge chunks while ConvFill interleaves filler phrases (on silence tokens) with knowledge-grounded responses. Training uses entailment-gated synthetic data across six domains, with inference employing a streaming queue and 1-second silence interval.

## Key Results
- ConvFill achieves sub-200ms TTFT across all backend configurations
- Accuracy improves from 10% (standalone SmolLM2-360M) to 46-52% when paired with backend models
- Accuracy gap of 23-34% remains between ConvFill and standalone backend models (69-80%)
- System generates appropriate fillers rather than generic placeholder words during knowledge gaps

## Why This Works (Mechanism)

### Mechanism 1: Asynchronous Knowledge Streaming with Interleaved Generation
Low-latency responsiveness and high-quality knowledge access are decoupled by having the on-device model generate responses incrementally while incorporating backend knowledge as it arrives. ConvFill operates on single-turn context, generating conversational phrases conditioned on a streaming queue of knowledge chunks. When no backend output is available within interval d (1 second), a silence token `<|sil|>` triggers filler generation. When knowledge arrives, the model incorporates it into subsequent phrases within the same turn.

### Mechanism 2: Hierarchical Context Partitioning
A small model can produce contextually appropriate responses without full conversation history if a larger model assumes responsibility for context management and knowledge retrieval. The backend model receives full conversation history and outputs concise knowledge chunks. The infill model sees only the current user utterance and the knowledge stream, reducing memory and compute requirements while preserving response quality.

### Mechanism 3: Entailment-Gated Synthetic Training
Training on synthetically generated conversations with entailment verification enables the model to learn knowledge incorporation without hallucinating beyond the provided information. GPT-4o generates synthetic multi-domain conversations with explicit `responder_thoughts` (knowledge chunks). A DeBERTaV3 model fine-tuned on MNLI validates that each knowledge chunk entails its corresponding response before inclusion in training.

## Foundational Learning

- **Textual Entailment (Natural Language Inference)**
  - Why needed here: Understanding training validation (DeBERTa-MNLI), evaluation metrics (Table 3's entailment/neutral/contradiction rates), and why "neutral" responses may still be conversant.
  - Quick check question: Given premise "Jack Nicklaus won Masters tournaments from 1963 to 1986" and hypothesis "He had amazing wins from 1963 to 1986," would an NLI model classify this as entailment, neutral, or contradiction—and why?

- **Streaming Token Generation**
  - Why needed here: ConvFill must generate tokens while new knowledge arrives mid-turn; understanding interleaved conditioning is essential.
  - Quick check question: How does a model's generation change if an external token sequence is injected into its context window after it has already begun producing output?

- **Time-to-First-Token (TTFT) vs. Total Latency**
  - Why needed here: The paper optimizes for TTFT (sub-200ms) to preserve conversational flow, accepting that full responses may extend longer.
  - Quick check question: For a voice agent, why might a 200ms TTFT with 3-second total response feel more natural than a 2-second TTFT with 2.5-second total response?

## Architecture Onboarding

- **Component map**: User utterance → Backend model (full history) → Knowledge chunks → Stream queue → ConvFill (single turn) → Interleaved response
- **Critical path**: User utterance → parallel dispatch to ConvFill thread and backend thread → Backend processes with full history, streams knowledge chunks to queue → ConvFill reads from queue: if `<|sil|>`, generates context-aware filler; if knowledge chunk, incorporates into response → Output: interleaved sequence of filler phrases and knowledge-grounded phrases
- **Design tradeoffs**: Silence interval (d): 1 second balances filler frequency vs. responsiveness; shorter intervals increase filler count; Backend prompt conciseness: Must instruct backend to output discrete knowledge chunks, not conversational responses; Infill model size: 360M enables <200ms TTFT on consumer hardware; larger models may improve accuracy but increase latency
- **Failure signatures**: Low entailment + high contradiction: Model hallucinates beyond knowledge chunks—check training data quality; Excessive filler phrases: Backend TTFT too high or silence interval too short; Knowledge ignored: Format mismatch between backend output and expected knowledge tag structure
- **First 3 experiments**:
  1. **Latency baseline**: Measure TTFT distribution for ConvFill alone, ConvFill + each backend, and backend-only on 100 NaturalQuestions samples. Confirm <200ms ConvFill TTFT across backends.
  2. **Knowledge incorporation ablation**: Run ConvFill with pre-canned correct vs. incorrect knowledge chunks on 50 QA pairs. Verify accuracy improves with correct knowledge, degrades with incorrect.
  3. **Entailment calibration**: Manually label 30 ConvFill responses for entailment/neutral/contradiction. Compare to DeBERTa classifications to identify systematic misclassification patterns (e.g., conversational embellishment marked as neutral).

## Open Questions the Paper Calls Out

### Open Question 1
Can improved training objectives or knowledge integration mechanisms close the accuracy gap between ConvFill (46-52%) and backend models (69-80%) on knowledge-intensive tasks? The authors state this gap represents a key area for future improvement, potentially through larger on-device models, improved training objectives, or better knowledge integration mechanisms.

### Open Question 2
What evaluation frameworks can better assess conversational appropriateness beyond strict logical entailment? The authors note that future work might directly leverage novel evaluation frameworks better aligned with conversational norms rather than strict logical inference after finding 59-64% neutral classifications that may reflect acceptable conversational variation.

### Open Question 3
What is the optimal silence token interval (d) for balancing filler naturalness against backend knowledge incorporation? The paper sets d=1 second without justification or ablation, yet this parameter controls the trade-off between conversational flow and knowledge integration opportunities.

### Open Question 4
How does user perception of ConvFill compare to direct backend model access with visible latency? The paper evaluates automated metrics but conducts no human user study to validate that sub-200ms responses with 46-52% accuracy are preferred over 0.7-10s latency with 69-80% accuracy in actual conversational use.

## Limitations
- Accuracy gap of 23-34% remains between ConvFill and backend models, suggesting the small infill model cannot fully leverage streaming knowledge
- Reliance on backend TTFT introduces variability - excessive filler generation may degrade user experience if backend latency exceeds 2-3 seconds
- Synthetic training data covers only six domains, potentially underrepresenting real-world query distributions

## Confidence

- **High confidence**: The latency claims (sub-200ms TTFT) and the core architectural innovation of asynchronous knowledge streaming are well-supported by the experimental design and reproducible methodology
- **Medium confidence**: The accuracy improvements over standalone small models (36-42%) are convincing but may not fully translate to production environments with diverse query distributions and variable backend latencies
- **Low confidence**: The long-term user experience implications of the filler-generation strategy remain uncertain, as the paper does not evaluate user perception of conversational naturalness over extended interactions

## Next Checks

1. **Domain Generalization Test**: Evaluate ConvFill on an out-of-domain QA dataset (e.g., HotpotQA or TriviaQA) to assess performance degradation beyond the six training domains, particularly focusing on whether the 46-52% accuracy range holds or collapses.

2. **User Experience Study**: Conduct a controlled experiment measuring user satisfaction and perceived naturalness when backend TTFT varies between 1-5 seconds, comparing ConvFill's filler strategy against traditional placeholder words and pure backend-only responses.

3. **Architecture Scaling Analysis**: Train and evaluate ConvFill variants at 1B and 2B parameters to determine whether the accuracy gap to backend models (69-80%) can be reduced without compromising the <200ms TTFT constraint, identifying the optimal parameter-count-to-latency tradeoff.