---
ver: rpa2
title: 'DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows'
arxiv_id: '2512.04416'
source_url: https://arxiv.org/abs/2512.04416
tags:
- data
- task
- code
- text
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DataGovBench, a benchmark for evaluating
  large language model (LLM) agents on real-world data governance tasks. Unlike existing
  data science benchmarks focused on code snippets or analytics, DataGovBench targets
  the unique challenge of ensuring data correctness and quality through 150 real-world
  tasks across six governance scenarios: filtering, refinement, imputation, deduplication,
  integration, and classification.'
---

# DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows

## Quick Facts
- arXiv ID: 2512.04416
- Source URL: https://arxiv.org/abs/2512.04416
- Reference count: 40
- Primary result: DataGovAgent achieves ATS 54.9 on complex DAG tasks vs 39.7 for baselines, reducing debug iterations by 77.9%

## Executive Summary
This paper introduces DataGovBench, a benchmark for evaluating LLM agents on real-world data governance workflows. Unlike existing data science benchmarks focused on code snippets or analytics, DataGovBench targets ensuring data correctness and quality through 150 real-world tasks across six governance scenarios. The benchmark uses a novel "reversed-objective" methodology to synthesize realistic noisy data and provides rigorous multi-metric evaluation scripts. To address the performance gap observed in current models on this benchmark, the authors propose DataGovAgent, a framework employing a Planner-Executor-Evaluator architecture with constraint-based planning, retrieval-augmented generation, and feedback-driven debugging.

## Method Summary
DataGovBench contains 150 real-world data governance tasks (100 operator-level, 50 DAG-level) across six scenarios: filtering, refinement, imputation, deduplication, integration, and classification. Tasks use 30 tables from Statista spanning tourism, e-commerce, and sports, with both structured and unstructured columns. The benchmark employs a "reversed-objective" methodology to synthesize realistic noisy data by inverting task goals. DataGovAgent implements a Planner-Executor-Evaluator pipeline: the Planner extracts governance contracts as (PRE, POST) tuples and builds DAGs with automatic repair step insertion; the Executor uses RAG over a curated operator library for grounded code generation; the Evaluator runs sandboxed execution with structured feedback-driven debugging. Evaluation metrics include Code Runnable Rate (CRR), Task Success Rate (TSR), Average Task Score (ATS), and Average Debug Iterations (ADI).

## Key Results
- DataGovAgent achieves ATS 54.9 on complex DAG tasks vs 39.7 for general-purpose baselines
- Reduces debugging iterations by over 77.9% compared to baseline approaches
- Outperforms baselines on CRR (93.6% vs 85.1%), TSR (61.3% vs 45.2%), and ATS (0.58 vs 0.40) on operator-level tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contract-guided planning improves task decomposition and structural coherence for complex governance workflows
- Mechanism: The Planner extracts verifiable governance contracts as (PRE, POST) tuples that formalize each operator's pre-conditions and post-conditions. When these constraints are violated, the system automatically inserts minimal repair steps (e.g., type casting, imputation), ensuring pipeline topological coherence and reducing the "runnable but incorrect" gap.
- Core assumption: Multi-step governance tasks benefit from explicit constraint propagation between operators.
- Evidence anchors:
  - [abstract] "integrates constraint-based planning"
  - [section 4.2] "governance contracts... formalize each operator as a (pre, post) tuple... when a constraint is not met, it inserts minimal repairs"
  - [corpus] Limited direct corpus support; MCP-Bench and OdysseyBench similarly emphasize multi-step coordination but without contract formalization.
- Break condition: If user intent is highly ambiguous or contracts cannot capture subtle semantic nuances, the Planner may generate compliant but semantically misaligned DAGs.

### Mechanism 2
- Claim: Retrieval-augmented generation (RAG) over curated operator libraries reduces hallucinations and improves code quality
- Mechanism: The Executor retrieves top-K relevant operators from a validated library and injects their descriptions/snippets as dynamic in-context exemplars. This grounds generation in pre-validated patterns, steering the model toward established best practices rather than free-form generation.
- Core assumption: Pre-validated code patterns transfer to novel governance tasks via semantic similarity matching.
- Evidence anchors:
  - [abstract] "retrieval-augmented generation"
  - [section 4.2] "retrieves the most relevant, validated operators... injects their descriptions and snippets as dynamic in-context exemplars"
  - [corpus] DAComp and Finch emphasize full-lifecycle task coverage but do not explicitly document RAG-based grounding.
- Break condition: If the operator library lacks coverage for novel task types or retrieval fails to surface relevant examples, the Executor falls back to free generation with increased error risk.

### Mechanism 3
- Claim: Feedback-driven debugging with structured error analysis drastically reduces debugging iterations
- Mechanism: The Evaluator executes code in a sandbox, captures error messages/stack traces, ties failures to violated contracts, and produces targeted revision advice. This enables precise fixes instead of trial-and-error guessing.
- Core assumption: Structured feedback with contract context enables more efficient error correction than raw error messages alone.
- Evidence anchors:
  - [abstract] "feedback-driven debugging"
  - [section 6.2] "reduces debugging iterations by over 77.9% compared to general-purpose baselines"
  - [corpus] Related benchmarks (MCP-Bench, OdysseyBench) focus on task complexity but do not document comparable debugging efficiency gains.
- Break condition: If errors stem from fundamental misunderstandings of task intent rather than contract violations, structured feedback may not converge efficiently.

## Foundational Learning

- Concept: Directed Acyclic Graph (DAG) workflows
  - Why needed here: Governance pipelines require explicit dependency ordering; the Planner outputs DAGs where each node's post-condition satisfies the next node's pre-condition.
  - Quick check question: Can you explain why circular dependencies would break the contract propagation mechanism?

- Concept: Design by contract (pre/post conditions)
  - Why needed here: Governance contracts formalize operator assumptions and guarantees, enabling automatic validation and repair insertion.
  - Quick check question: How would you define a PRE condition for a deduplication operator that requires sorted input?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The Executor uses RAG to ground code generation in validated operator patterns rather than free-form synthesis.
  - Quick check question: What are the trade-offs between retrieving top-1 vs. top-K operators for a novel task?

## Architecture Onboarding

- Component map: Planner (intent understanding → contract extraction → DAG synthesis with repair steps) -> Executor (RAG retrieval → augmented code generation) -> Evaluator (sandboxed execution → structured feedback generation → iterative debugging loop) -> Operator Library (curated validated governance operators)
- Critical path: Planner contract extraction → Executor RAG retrieval → Evaluator feedback loop. Performance degrades most sharply when Planner is bypassed (TSR drops 26 pp per ablation study).
- Design tradeoffs: Higher quality and debugging efficiency vs. increased token consumption (DataGovAgent uses ~2× tokens of baselines per successful task); Contract rigor vs. semantic nuance capture (contracts may miss subtle intent ambiguities); Sandbox isolation vs. external dependency flexibility.
- Failure signatures: High CRR but low TSR (code runs but doesn't meet business objectives, likely contract violations or intent misalignment); Excessive debug iterations (>10 on operator-level tasks, likely missing operator library coverage or weak RAG retrieval); Semantic misalignment (compliant DAG that doesn't match user's actual intent).
- First 3 experiments: 1) Run ablation: Disable Planner and observe TSR/ADI degradation to internalize contract-guided planning's contribution; 2) RAG sensitivity test: Vary top-K retrieval (1, 2, 4) and measure hallucination rate on novel task types; 3) Debug loop analysis: Trace one complex DAG task through full feedback cycle, mapping which error types get resolved in which iteration.

## Open Questions the Paper Calls Out

- Can semi-automated problem synthesis scale DataGovBench while maintaining the rigor of the current human-in-the-loop curation process? The paper notes that expanding the benchmark remains labor-intensive and aims to address this through semi-automated synthesis in future work.

- How can governance contracts be extended to capture semantic nuances in highly ambiguous natural language instructions? The paper acknowledges that current contracts may not fully capture subtle nuances, potentially leading to compliant but semantically misaligned outputs.

- What architectural innovations are needed to close the "runnable ≠ correct" gap, where high code runnable rates fail to translate into task success? The paper identifies this as a critical issue, with examples showing 85% CRR vs 46% TSR for some models.

- Can the token-quality trade-off be optimized to achieve high task success rates without the 2-4× token overhead of the Agentic Assembly Line? The paper documents that DataGovAgent achieves highest quality but at significantly higher token cost per successful task.

## Limitations

- The performance relies heavily on a curated operator library from DCAI (2025) that is not publicly released, creating a significant reproducibility gap.
- The reported results use GPT-5, which is not publicly available, making independent verification challenging.
- The sandbox environment for safe code execution is referenced but not detailed, limiting faithful reproduction of the Evaluator component.
- Full prompt templates for Planner, Executor, and Evaluator are not disclosed, only partial examples.

## Confidence

- **High Confidence**: The benchmark methodology (150 real-world tasks, reversed-objective data synthesis, multi-metric evaluation) is well-specified and reproducible. The performance gap between DataGovAgent and baselines is clearly demonstrated with available models.
- **Medium Confidence**: The DataGovAgent architecture's contribution (Planner-Executor-Evaluator with contract-guided planning, RAG, feedback-driven debugging) is logically sound and well-motivated, but full validation requires the complete operator library and prompts.
- **Low Confidence**: The claim that DataGovAgent achieves TSR 64% (operator) and 60% (DAG) with GPT-5 cannot be independently verified due to GPT-5's unavailability.

## Next Checks

1. Request or reconstruct the curated operator library to assess its coverage of the 150 benchmark tasks and determine if it's a performance bottleneck.

2. Run comprehensive ablations (disable Planner, disable RAG, disable feedback loop) on GPT-4o to quantify each component's contribution and validate the claimed performance gains.

3. Design experiments to measure the gap between CRR and TSR (alignment ratio A = TSR/CRR) to assess whether DataGovAgent truly solves the "runnable but incorrect" problem, not just code generation quality.