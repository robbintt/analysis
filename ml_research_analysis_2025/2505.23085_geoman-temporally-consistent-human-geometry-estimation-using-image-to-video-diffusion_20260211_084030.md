---
ver: rpa2
title: 'GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video
  Diffusion'
arxiv_id: '2505.23085'
source_url: https://arxiv.org/abs/2505.23085
tags:
- depth
- estimation
- video
- human
- geoman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GeoMan addresses the challenge of temporally consistent human geometry
  estimation from monocular videos, where existing methods suffer from flickering
  artifacts and lack fine-grained dynamic details. The core innovation is decomposing
  video geometry estimation into image-based geometry estimation for the first frame
  and image-to-video synthesis for subsequent frames, leveraging pre-trained diffusion
  models.
---

# GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion

## Quick Facts
- arXiv ID: 2505.23085
- Source URL: https://arxiv.org/abs/2505.23085
- Authors: Gwanghyun Kim; Xueting Li; Ye Yuan; Koki Nagano; Tianye Li; Jan Kautz; Se Young Chun; Umar Iqbal
- Reference count: 40
- One-line primary result: State-of-the-art temporally consistent human geometry estimation using image-to-video diffusion with root-relative depth representation

## Executive Summary
GeoMan addresses the challenge of temporally consistent human geometry estimation from monocular videos, where existing methods suffer from flickering artifacts and lack fine-grained dynamic details. The core innovation is decomposing video geometry estimation into image-based geometry estimation for the first frame and image-to-video synthesis for subsequent frames, leveraging pre-trained diffusion models. This approach reduces reliance on scarce 4D training data while maintaining temporal consistency and generalization. GeoMan introduces a root-relative depth representation that preserves human scale information while enabling metric depth estimation, overcoming limitations of traditional affine-invariant and metric depth representations.

## Method Summary
GeoMan estimates temporally consistent depth and normal maps from monocular human videos through a two-stage pipeline. First, an Image-to-Geometry (I2G) model predicts depth or normal maps for the first frame using a single-image diffusion prior. Second, a Video-to-Geometry (V2G) model generates subsequent frames conditioned on the full video sequence and the first-frame geometry, using a pre-trained image-to-video diffusion model with ControlNet. The method employs root-relative depth representation (depth relative to pelvis) to preserve human scale while avoiding per-frame normalization artifacts. Training uses synthetic human datasets (THuman-2.0 and XHumans) with camera augmentation, and the unified V2G model handles both depth and normal estimation through modality switching.

## Key Results
- Achieves state-of-the-art performance on ActorsHQ and Goliath datasets for both normal and depth estimation
- Outperforms baselines including Sapiens despite using only public data
- Demonstrates superior temporal stability with significantly reduced flickering artifacts
- Maintains geometric fidelity and generalizability to in-the-wild scenarios, long videos, and multi-person cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing video geometry estimation into a first-frame image-based prediction and an image-to-video synthesis step improves temporal consistency.
- Mechanism: The I2G model estimates depth or normal maps for the first frame using a single-image diffusion prior (Marigold-style). This output conditions the V2G model (an image-to-video diffusion model, specifically using I2VGen-XL and a ControlNet), which generates the subsequent frames. The V2G model uses the first-frame geometry as a "reference image" and the full video sequence as a "control signal," reframing geometry estimation as a consistent video generation task.
- Core assumption: Pre-trained image-to-video diffusion models encode strong priors about temporal coherence and motion, which can be transferred to geometry estimation with minimal fine-tuning.
- Evidence anchors:
  - [abstract] "GeoMan employs an image-based model to estimate depth and normals for the first frame... which then conditions a video diffusion model."
  - [section 4.1] Describes the I2G (Image-to-Geometry) and V2G (Video-to-Geometry) formulation.
  - [corpus] GeometryCrafter and DepthCrafter similarly leverage diffusion priors for video geometry.
- Break condition: If the first-frame I2G prediction is significantly erroneous, the V2G model will propagate these errors, leading to global inconsistencies. This is a primary dependency.

### Mechanism 2
- Claim: Using a unified video diffusion model for both depth and normal estimation improves generalization and efficiency.
- Mechanism: The V2G model is trained on both depth and normal data. The task (video generation from a reference) is identical; only the conditioning input (the first frame's geometry map) changes. This multi-modal training exposes the model to more diverse geometric patterns, acting as a form of data augmentation.
- Core assumption: The underlying process of generating a consistent video sequence from a reference image is fundamentally similar whether the output modality is depth or normal maps.
- Evidence anchors:
  - [section 4.1] "GeoMan unifies depth and normal estimation by leveraging a single video diffusion model... achieved by simply switching the conditioning image."
  - [table 5c] Shows multimodal training outperforms unimodal training on the V2G model.
  - [corpus] No direct corpus evidence for *unified* depth/normal video generation was found in neighbors.
- Break condition: If the input format for depth and normal maps is not normalized or standardized effectively, the shared model may struggle to differentiate or specialize, potentially leading to artifacts or lower fidelity in one modality.

### Mechanism 3
- Claim: A root-relative depth representation enables better human-scale preservation and temporal stability compared to affine-invariant or metric depth.
- Mechanism: Root-relative depth is calculated as `D_metric - d_root` (depth relative to the pelvis). This removes global translation while preserving the metric scale of the human subject. This constrained value range ([-h/2, h/2]) is easier for the network to model than large absolute metric values, avoiding the loss of detail often seen in metric depth methods. It also avoids the per-frame or per-sequence normalization of affine-invariant methods, which causes scale drift and temporal flickering.
- Core assumption: The relative scale of a human is a strong and consistent prior that helps the model converge, and the global translation can be recovered or is not necessary for many downstream tasks.
- Evidence anchors:
  - [abstract] "...introduces a root-relative depth representation that preserves human scale information..."
  - [section 4.2] Formulates root-relative depth and contrasts it with affine-invariant and metric representations. Table 1 compares properties.
  - [table 3 & 4] Quantitative results show superior performance on metric-aware and temporal consistency metrics.
  - [corpus] GeometryCrafter also identifies limitations of affine-invariant depth for reconstruction.
- Break condition: This mechanism depends entirely on accurate estimation of the root depth (`d_root`) to reconstruct absolute metric depth. If the external pose estimator providing root depth fails, the final output will be scale-inaccurate, though it may remain locally consistent.

## Foundational Learning

### Latent Diffusion Models (LDMs) & Latent Space
LDMs perform diffusion in a compressed latent space (using a VAE encoder/decoder) rather than pixel space. This makes training and inference computationally tractable.
- Why needed here: GeoMan's I2G and V2G models are LDMs. The entire mechanism operates on latent representations of depth/normal maps, which are decoded only at the final step. Understanding this is critical for debugging reconstruction artifacts.
- Quick check question: If the VAE decoder produces blurry outputs, is the issue likely with the diffusion model or the VAE itself? (Answer: Likely the VAE, as the diffusion model operates in its latent space).

### ControlNet and Conditioning
ControlNet adds an auxiliary, locked copy of a pre-trained model's weights, trained with an extra conditioning input (e.g., canny edges, depth, video frames). This allows structural control over generation without destroying the pre-trained knowledge.
- Why needed here: GeoMan uses a ControlNet (via Ctrl-Adapter) on the V2G model to inject the input video frames as a "control signal," forcing the generated geometry to follow the human's motion. This is how temporal consistency is achieved.
- Quick check question: What would happen to the output if you removed the ControlNet during inference? (Answer: The model would generate a geometrically plausible video but it would not be tied to the specific motion of the input video).

### Affine-Invariant vs. Metric vs. Root-Relative Depth
- **Affine-Invariant**: Normalizes depth to [0,1] per image/video. Loses absolute scale, good for relative structure.
- **Metric**: Predicts absolute depth in meters (e.g., 0-8m). Hard to learn, can lose fine detail.
- **Root-Relative**: Depth relative to a root keypoint (e.g., pelvis). Preserves human scale, discards only global translation.
- Why needed here: This is a core innovation of the paper. Understanding this trade-off is essential for interpreting the quantitative results and knowing when to use GeoMan (when human scale matters) versus other methods.
- Quick check question: Which representation is most suitable for reconstructing a 3D human mesh that needs to be correctly proportioned? (Answer: Root-relative or Metric. Affine-invariant would result in a distorted mesh).

## Architecture Onboarding
- Component map: Video -> Preprocessor (matting + cropping) -> I2G (first frame) -> V2G (all frames) -> Postprocessor (root depth integration) -> Output Geometry
- Critical path: Input Video -> Preprocessor -> I2G (First Frame) -> V2G (All Frames) -> Postprocessor -> Output Geometry. The accuracy of the entire system is gated by the I2G model's first-frame prediction, which anchors the V2G generation.
- Design tradeoffs:
  - **Accuracy vs. Speed**: GeoMan uses an ensemble of 8 and 100 denoising steps, making it slow (~10.5s on A100) compared to feed-forward methods but achieving higher fidelity.
  - **Data Efficiency vs. Generality**: The I2G+V2G decomposition is highly data-efficient (works with limited 4D data) but may not generalize as well to non-human or highly dynamic scenes as a model trained on massive generic video datasets.
  - **Root-Relative vs. Metric**: Root-relative preserves human scale and details better than direct metric prediction but adds a dependency on an external pose estimator for absolute metric depth.
- Failure signatures:
  - **Flickering or Inconsistency**: Often due to a weak first-frame I2G prediction or a failure in the V2G model to condition properly. Check ControlNet signal.
  - **Scale Drift**: If using root-relative depth and the external pose estimator provides an incorrect or jittery root depth.
  - **Blurry/Low-Fidelity Output**: Could be from the VAE decoder, too few denoising steps, or insufficient training of the diffusion model.
  - **Multi-Person Depth Conflicts**: The root-relative representation assumes a single root. The paper uses per-subject masking and aggregation for multi-person cases, but failures may occur if individuals occlude each other heavily.
- First 3 experiments:
  1. **Ablate the First-Frame Anchor**: Run GeoMan with a deliberately degraded first-frame I2G prediction (e.g., downscale 4x). Quantify the drop in temporal consistency and accuracy to establish the robustness and dependency on I2G quality. (See paper Table S8f).
  2. **Compare Depth Representations**: Train and evaluate the V2G model using affine-invariant, metric, and root-relative depth representations on a validation set. Compare using AbsRel, Î´<1.05, and temporal consistency metrics (OPW, TC-RMSE) to confirm the paper's claims about scale preservation and detail. (See paper Tables 1, 3, 4).
  3. **Test on Long Videos**: Run the full GeoMan pipeline on videos longer than the training sequence (e.g., 64+ frames). Use the mortise-and-tenon latent interpolation method and qualitatively assess for artifacts or drift at the segment boundaries. (See paper Section B.1).

## Open Questions the Paper Calls Out
- The authors envision GeoMan as a practical tool for acquiring large-scale, real-world supervision, enabling its distillation into the next generation of faster, lightweight human geometric estimation models.
- Due to VRAM limitations, they trained their model at a maximum resolution of 512 and plan to explore lightweight models or alternative training strategies to enable higher-resolution generation.
- For in-the-wild applications, their method relies on matting techniques to separate the subject from the background, making its performance inherently dependent on matting accuracy.

## Limitations
- Dependency on accurate root depth estimation for metric depth reconstruction introduces an external failure point
- Specialized for human subjects and may not generalize well to non-human or highly dynamic scenes
- Computational cost remains high due to iterative diffusion steps (10.57s on A100)

## Confidence
- **High Confidence**: The mechanism of decomposing video geometry estimation into first-frame image-based prediction and subsequent video synthesis is well-supported by quantitative results (Tables 3, 4) and ablation studies. The superiority of root-relative depth over affine-invariant and metric representations is demonstrated across multiple metrics.
- **Medium Confidence**: The claim that unified depth and normal estimation improves generalization is supported by Table 5c, but the corpus lacks direct evidence for unified video geometry generation. The effectiveness of ControlNet for temporal consistency is demonstrated but relies on the specific implementation details of Ctrl-Adapter.
- **Low Confidence**: Generalization claims to "in-the-wild" scenarios and multi-person cases are based on qualitative results in supplementary materials rather than comprehensive quantitative evaluation.

## Next Checks
1. **Root Depth Dependency Test**: Systematically vary the accuracy of the root depth estimator (simulate noise or errors) and measure the impact on final metric depth quality and temporal consistency to quantify this critical dependency.
2. **Multi-Person Boundary Case Analysis**: Evaluate GeoMan on challenging multi-person videos where subjects frequently occlude each other, measuring depth conflicts and temporal coherence at occlusion boundaries.
3. **Cross-Dataset Generalization**: Test GeoMan on a completely different human video dataset (e.g., Human3.6M or 3DPW) without fine-tuning to assess true generalization beyond the ActorsHQ and Goliath datasets.