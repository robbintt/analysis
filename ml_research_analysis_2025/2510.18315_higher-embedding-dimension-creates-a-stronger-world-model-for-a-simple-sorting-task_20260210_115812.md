---
ver: rpa2
title: Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting
  Task
arxiv_id: '2510.18315'
source_url: https://arxiv.org/abs/2510.18315
tags:
- embedding
- dimension
- attention
- agents
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how embedding dimension influences the emergence
  of interpretable internal representations in transformers trained via reinforcement
  learning to perform adjacent swaps for sorting. While models achieve high accuracy
  even with small embedding dimensions, larger dimensions yield more structured, consistent,
  and robust internal representations.
---

# Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task

## Quick Facts
- **arXiv ID**: 2510.18315
- **Source URL**: https://arxiv.org/abs/2510.18315
- **Reference count**: 30
- **Primary result**: Larger embedding dimensions improve internal representation quality (monotonic ordering encoding, consistent swap selection) beyond what's needed for task accuracy.

## Executive Summary
This study investigates how embedding dimension influences the emergence of interpretable internal representations in transformers trained via reinforcement learning to perform adjacent swaps for sorting. While models achieve high accuracy even with small embedding dimensions, larger dimensions yield more structured, consistent, and robust internal representations. Specifically, higher embedding dimensions strengthen two key mechanisms: (1) the last row of the attention weight matrix increasingly encodes the global ordering of tokens, and (2) the selected swap aligns with the largest adjacent difference in these encoded values. Across 475 agents trained on sequences of length 6 and 8, accuracy plateaus at low dimensions, but the fidelity of these interpretable circuits continues to improve until around dimension 30.

## Method Summary
The study trains transformer agents via proximal policy optimization (PPO) to sort permutations using only adjacent swaps. The architecture consists of learned token and positional embeddings feeding into a single-head self-attention block, followed by separate actor and critic linear layers. The embedding dimension controls the query/key dimensionality. Training uses standard PPO hyperparameters with reward +1 for successful sorting and -0.001 otherwise. The authors sweep embedding dimensions from 2 to 128, training multiple agents per dimension with various seeds, and evaluate both task performance and interpretability metrics like the monotonicity of attention weight patterns.

## Key Results
- Accuracy plateaus at relatively low embedding dimensions (2-16) for length-6 sequences
- Non-inversion proportion in the last attention row increases with embedding dimension, plateauing around 87% at dimension 30
- Swap selection aligns with largest adjacent differences in attention-encoded values in 76-90% of cases
- Higher embedding dimensions reduce the "local greedy trap" failure mode where agents swap locally incorrect pairs

## Why This Works (Mechanism)

### Mechanism 1: Global Order Encoding via Attention Weights
- Claim: The last row of the attention weight matrix monotonically encodes the global ordering of input tokens, with lower attention values corresponding to lower numerical tokens.
- Mechanism: The self-attention computation W = QK^T/√d_k produces attention weights where the final row creates a mapping from token positions to scalar values that reflect ordinal relationships. This emerges from learned key/query vectors that arrange tokens along a consistent direction in embedding space.
- Core assumption: The embedding matrix learns representations where tokens with higher numerical values occupy consistent directional positions in the embedding space.
- Evidence anchors: Abstract mentions monotonic encoding, section 5.2 shows non-inversion rates increase with dimension, section 6.1 explains why key vectors must be arranged orderly.

### Mechanism 2: Difference-Based Swap Decision Rule
- Claim: The selected adjacent swap corresponds to the largest difference between consecutive attention-weight encoded values.
- Mechanism: The value matrix and final linear layer compute differences between adjacent attention output values. The policy selects the transposition at the position with the maximum positive or negative difference.
- Core assumption: The network learns to route ordering information through attention weights to a difference-computing readout layer.
- Evidence anchors: Abstract mentions alignment with largest adjacent differences, section 5.3 shows 76-90% swap prediction alignment rates.

### Mechanism 3: Capacity-Driven Representation Quality
- Claim: Increasing embedding dimension beyond what is required for task accuracy improves representation faithfulness, consistency, and robustness without performance gains.
- Mechanism: Larger query/key dimensions enhance attention's expressive power, allowing smoother approximation of the monotonic ordering function. Excess capacity enables redundant encoding that reduces variance across random seeds.
- Core assumption: The bubble-sort circuit is the easiest solution for agents to converge to given the task structure.
- Evidence anchors: Abstract notes accuracy plateaus but representation quality improves until dimension 30, section 5.1 shows task success requires limited capacity, section 5.4 shows failure modes decrease with higher dimensions.

## Foundational Learning

- **Concept**: Self-Attention Mechanism in Transformers
  - Why needed here: The entire interpretability analysis depends on understanding how Q, K, V matrices produce attention weights W = QK^T/√d_k and why the last row carries ordinal information.
  - Quick check question: Given a 6-token sequence, can you explain why causal masking restricts each token to attend only to previous positions?

- **Concept**: Proximal Policy Optimization (PPO)
  - Why needed here: The RL training framework shapes which internal representations emerge; understanding PPO's clipped objective explains why agents converge to stable but not necessarily optimal sorting strategies.
  - Quick check question: Why does PPO's clipping prevent "destructive shifts in behavior" while still allowing policy improvement?

- **Concept**: Mechanistic Interpretability
  - Why needed here: This work's core contribution is identifying specific circuits (attention-based ordering + difference-based selection) rather than treating the model as a black box.
  - Quick check question: What distinguishes mechanistic interpretability from standard performance-based model evaluation?

## Architecture Onboarding

- **Component map**: Token embeddings + positional embeddings → single-head self-attention → linear layer → actor head (swap index) and critic head (value estimate)

- **Critical path**: Embedding dimension → query/key expressivity → attention weights (final row) encode global ordering → value matrix + linear layer compute adjacent differences → actor head selects swap position

- **Design tradeoffs**:
  - Lower embedding dimension (2-6): Sufficient for accuracy, but representation quality is poor and inconsistent across seeds
  - Medium embedding dimension (~30): Optimal balance—high accuracy AND interpretable representations
  - Higher embedding dimension (>30): No additional representation quality gains, wasted capacity
  - Sequence length 6 vs 8: Length 8 requires more capacity but shows lower convergence rates (37.4% vs 99.2% achieving 100% accuracy above dimension 16)

- **Failure signatures**:
  - "Local greedy trap": Model swaps locally incorrect pairs without global ordering representation
  - Non-convergence: Loss doesn't stabilize, particularly for length-8 sequences
  - Low non-inversion proportion (~50%): Indicates random-order attention weights, no global structure learned

- **First 3 experiments**:
  1. **Baseline sweep**: Train agents with embedding dimensions [2, 4, 6, 8, 12, 16, 24, 30, 48, 64, 128] on length-6 sequences (3 seeds each) for 10M timesteps. Measure: accuracy, non-inversion proportion, top-2 swap prediction rate.
  2. **Mechanism validation**: For high-performing agents (accuracy > 95%), visualize attention weight matrices for fully ordered permutations. Verify last-row monotonicity matches input ordering.
  3. **Failure mode analysis**: Identify agents stuck in local greedy traps. Examine attention weight violin plots to confirm wider value spreads correlate with ordering errors. Compare embedding dimensions of failing vs. succeeding agents.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do transformers discover known efficient algorithms (e.g., merge sort) when the adjacency constraint is relaxed to allow arbitrary in-place swaps?
- Basis in paper: Section 6.4 asks whether relaxing the adjacency constraint would reveal if transformers discover efficient algorithms or novel heuristics.
- Why unresolved: The current study strictly enforced adjacent swaps, limiting the solution space to bubble-sort-style mechanics.
- What evidence would resolve it: Mechanistic interpretability analysis of agents trained on unconstrained swap environments to identify circuit motifs matching $O(n \log n)$ algorithms.

### Open Question 2
- Question: Does multi-head attention naturally lead to functional specialization, such as separating local comparison from global structure processing?
- Basis in paper: Section 6.4 suggests exploring multi-head attention to examine if heads specialize in complementary subproblems.
- Why unresolved: The authors deliberately restricted the architecture to single-head attention to isolate the role of embedding dimension.
- What evidence would resolve it: Ablation studies on individual heads in multi-head models to determine if specific heads are necessary and sufficient for global ordering vs. local swap selection.

### Open Question 3
- Question: Does the global ordering circuit emerge reliably in environments with stochasticity, such as noisy token values or partial observability?
- Basis in paper: Section 6.4 proposes introducing stochasticity to test if the same ordering circuit emerges under uncertainty.
- Why unresolved: The experiments were conducted in a deterministic, fully observable setting with perfect token inputs.
- What evidence would resolve it: Training agents on perturbed/noisy inputs and measuring the "proportion of non-inversions" metric to see if the global ordering representation degrades or adapts.

## Limitations
- Interpretation of attention weight patterns as causal mechanisms is correlational rather than proven
- Focus on single-head, single-layer architecture limits generalizability to deeper or wider networks
- "Local greedy trap" failure mode identified qualitatively but not systematically quantified

## Confidence
- **High confidence**: Task setup, accuracy plateaus at low dimensions, embedding dimension directly controls representation capacity
- **Medium confidence**: Specific attention-weight-based circuit interpretation, optimal embedding dimension ~30
- **Low confidence**: Generalization to multi-head/multi-layer architectures, causal relationship between attention patterns and sorting decisions

## Next Checks
1. **Ablation study on attention mechanism**: Train agents with attention weights manually perturbed to break monotonicity in the last row. If performance degrades while accuracy is maintained in control groups, this would strengthen causal claims about the ordering circuit.
2. **Cross-task transfer**: Apply the same embedding dimension sweep to related permutation-based tasks (e.g., list ranking, reversal). If the 30-dimensional sweet spot and circuit patterns emerge consistently, this would support the generality of the findings.
3. **Mechanistic perturbation analysis**: For high-performing agents, systematically swap individual attention weights in the last row and measure downstream effects on swap selection. This would directly test whether the observed monotonic patterns causally influence the agent's decisions rather than being epiphenomenal.