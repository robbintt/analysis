---
ver: rpa2
title: 'EmoFeedback$^2$: Reinforcement of Continuous Emotional Image Generation via
  LVLM-based Reward and Textual Feedback'
arxiv_id: '2511.19982'
source_url: https://arxiv.org/abs/2511.19982
tags:
- emotional
- image
- emotion
- images
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EmoFeedback 2, a novel reinforcement learning
  framework for continuous emotional image generation. The method leverages a fine-tuned
  Large Vision-Language Model (LVLM) to provide both reward feedback and textual feedback
  during image generation, addressing the limitations of existing approaches in capturing
  emotional continuity and fidelity.
---

# EmoFeedback$^2$: Reinforcement of Continuous Emotional Image Generation via LVLM-based Reward and Textual Feedback

## Quick Facts
- arXiv ID: 2511.19982
- Source URL: https://arxiv.org/abs/2511.19982
- Reference count: 35
- Primary result: EmoFeedback$^2$ achieves V-Error: 0.521, A-Error: 0.710 on continuous emotional image generation

## Executive Summary
EmoFeedback$^2$ introduces a novel reinforcement learning framework for continuous emotional image generation using a Large Vision-Language Model (LVLM) for both reward feedback and textual feedback. The method addresses the limitations of existing approaches in capturing emotional continuity and fidelity by leveraging a fine-tuned LVLM to provide precise numerical rewards and iteratively refine prompts. Experiments demonstrate superior performance in emotional fidelity and image quality metrics compared to state-of-the-art methods.

## Method Summary
EmoFeedback$^2$ employs a two-pronged approach: (1) emotion-aware reward feedback using a fine-tuned LVLM that evaluates generated images through Valence-Arousal regression and classification, and (2) self-promotion textual feedback that iteratively refines prompts based on emotional content analysis. The LVLM is trained using Group Relative Policy Optimization (GRPO) with multi-task rewards, while the generator (Stable Diffusion 3.5-Medium) is fine-tuned using Flow-GRPO with combined emotion and PickScore rewards. At inference, a 3-iteration self-promotion loop refines prompts by comparing best/worst candidates.

## Key Results
- V-Error: 0.521 (vs. 0.579 for regression-only LVLM baseline)
- A-Error: 0.710 (vs. 0.674 for joint training baseline)
- CLIP-Score: 0.438 (outperforming state-of-the-art methods)
- Aesthetic Score: 0.338 (superior to baseline approaches)

## Why This Works (Mechanism)

### Mechanism 1: LVLM-Based Emotion-Aware Reward Feedback
- Claim: A fine-tuned LVLM provides precise numerical reward signals that guide reinforcement learning for continuous emotional control.
- Mechanism: The LVLM evaluates generated images by predicting Valence-Arousal (V-A) scores, comparing them to target values via a step-reward function, and computing group-normalized advantages. This reward directly updates the diffusion model's policy through GRPO.
- Core assumption: The LVLM can accurately assess emotional content in images, and V-A discrepancies are sufficient proxies for emotional fidelity.
- Evidence anchors:
  - [abstract] "emotion-aware reward feedback strategy... evaluates generated images and computes the reward against target emotions"
  - [section 3.2] "The emotion understanding model predicts Vâ€“A scores of each image and computes reward values as reward feedback"
  - [corpus] Weak direct evidence; related work on RL for T2I alignment exists (e.g., "Towards Better Alignment: Training Diffusion Models with Reinforcement Learning Against Sparse Rewards"), but no corpus papers specifically address LVLM-based emotion reward models.
- Break condition: If the LVLM's V-A predictions are systematically biased or fail to generalize to new image domains, reward signals will misguide training.

### Mechanism 2: Self-Promotion Textual Feedback via Chain-of-Thought Reasoning
- Claim: Iterative textual feedback, generated through LVLM chain-of-thought reasoning, progressively refines prompts to improve emotional fidelity without updating model parameters.
- Mechanism: The LVLM compares best/worst images from a candidate set, analyzes emotional discrepancies, and produces structured prompt suggestions. The optimized prompt is fed back to the generator in subsequent iterations.
- Core assumption: The LVLM can produce actionable, semantically coherent refinements that directly translate to improved emotional expression in generated images.
- Evidence anchors:
  - [abstract] "self-promotion textual feedback framework iteratively refines prompts based on emotional content analysis"
  - [section 3.3] "Leveraging its chain-of-thought reasoning capability, the LVLM can produce prompt refinement suggestions"
  - [corpus] No direct corpus evidence on textual feedback for emotional image generation; weakly related to LLM emotional expression work.
- Break condition: If LVLM refinements are ungrounded, over-specific, or conflict with the original prompt semantics, iterative refinement may diverge.

### Mechanism 3: Multi-Task LVLM Training for Robust Emotion Understanding
- Claim: Jointly training the LVLM on V-A regression and discrete emotion classification improves emotion assessment accuracy over single-task training.
- Mechanism: Three reward functions (format, V-A regression, emotion classification) are combined to fine-tune the LVLM using GRPO, encouraging both structured outputs and cross-task generalization.
- Core assumption: Classification and regression tasks share underlying emotional representations; improved classification aids regression.
- Evidence anchors:
  - [section 3.1] "jointly trained multi-task model significantly outperforms single-task regression-only and classification-only baselines"
  - [table 5] Ablation showing V-Error: 0.521 (joint) vs. 0.579 (regression-only) vs. 1.445 (classification-only)
  - [corpus] No corpus evidence on multi-task LVLM training for emotion; related speech emotion work exists but is not directly transferable.
- Break condition: If tasks are poorly balanced or one reward dominates, multi-task training may not converge or may degrade both tasks.

## Foundational Learning

- Concept: **Valence-Arousal (V-A) Emotion Model**
  - Why needed here: Continuous emotional control requires a dense, differentiable representation; V-A provides a 2D continuous space for reward computation.
  - Quick check question: Can you explain why V-A is preferred over discrete emotion categories for gradient-based RL?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO enables stable RL fine-tuning of diffusion models by normalizing rewards within groups and applying KL penalties.
  - Quick check question: What is the role of group-level advantage normalization in preventing unstable policy updates?

- Concept: **Reward Hacking Mitigation via Multi-Objective Rewards**
  - Why needed here: Pure emotion reward may cause content distortion; PickScore/CLIP-based rewards balance emotional fidelity with semantic consistency.
  - Quick check question: Why is PickScore necessary as an additional reward term, and what failure mode does it prevent?

## Architecture Onboarding

- Component map: Generator (SD3.5-M) -> LVLM Reward Model -> Textual Feedback Loop -> Generator
- Critical path:
  1. Train LVLM on custom dataset with multi-task GRPO rewards
  2. Use LVLM to compute emotion rewards for generated images
  3. Fine-tune generator via GRPO with combined emotion + PickScore rewards
  4. At inference, run self-promotion textual feedback loop (3 iterations, 8 images per iteration)

- Design tradeoffs:
  - **Model size**: 7B LVLM vs. 3B (ablation shows 7B better); 2.5B generator for efficiency
  - **Reward design**: Step reward vs. continuous (step better in ablation); multi-task vs. single-task (joint better)
  - **Iteration count**: Textual feedback set to 3 iterations; more iterations increase latency but may improve fidelity

- Failure signatures:
  - **Reward hacking**: Generated images become solid colors or distorted textures to maximize emotion reward
  - **Semantic drift**: Iterative textual feedback causes content to diverge from original prompt
  - **LVLM bias**: V-A predictions systematically misalign with human perception (see human annotation comparison)

- First 3 experiments:
  1. **LVLM ablation**: Compare 3B vs. 7B backbone and step vs. continuous reward on V-A error metrics.
  2. **Reward component ablation**: Train with emotion reward only vs. emotion + PickScore; check for content distortion.
  3. **Textual feedback iteration sweep**: Run 0, 1, 2, 3 iterations and measure V-A error vs. inference latency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the unified training paradigm be adapted to calibrate data and model outputs according to specific, subjective user emotional preferences?
- Basis in paper: [explicit] The conclusion states that "emotion understanding is highly subjective and each person's emotions are specific," and notes that "the current unified training paradigm will have deviation for each user." The authors list "calibrating data according to each user's emotional preference" as a direction for future work.
- Why unresolved: The current model relies on aggregated data and lexicon-based values, which assume a universal emotional mapping rather than a personalized one.
- What evidence would resolve it: The development of a personalization adapter or fine-tuning protocol that consistently lowers V-A error rates for individual users compared to the baseline.

### Open Question 2
- Question: How can Process Reward Models (PRM) be integrated to interpret the specific steps and visual elements contributing to the generated image's emotional score?
- Basis in paper: [explicit] The conclusion explicitly identifies a limitation: "our current method lacks exploration of process reward models (PRM). The model's steps and elements for evaluating image emotion are still unclear."
- Why unresolved: The current Emotion Understanding Model provides a final score (outcome reward) but lacks the granularity to identify which specific generative steps or visual features (e.g., lighting vs. object choice) were most critical to the emotional expression.
- What evidence would resolve it: A modified reward framework that assigns intermediate rewards to specific denoising steps or visual attributes, resulting in improved interpretability and faster convergence.

### Open Question 3
- Question: To what extent does training on lexicon-sampled V-A values limit the model's alignment with actual human emotional perception?
- Basis in paper: [inferred] Table 6 in Section 4.4.3 shows that while the model achieves a V-Error of 0.521 on lexicon-based annotations, the error rises to 0.781 when evaluated against human annotations. This gap suggests the synthetic data generation pipeline (Section 4.1) may not fully capture human ground truth.
- Why unresolved: The paper validates the consistency of trends but does not investigate if the performance ceiling is restricted by the noise or distributional assumptions of lexicon-based sampling.
- What evidence would resolve it: A comparative ablation study training the model on a fully human-annotated dataset to measure the reduction in error against human evaluators.

## Limitations
- Reward hacking: The generator may produce abstract color blocks to minimize V-A error while losing semantic content.
- Semantic drift: Iterative textual feedback can cause content to diverge from original prompt semantics.
- LVLM bias: V-A predictions may systematically misalign with human perception, as shown in human annotation comparison.

## Confidence
- LVLM-based reward mechanism: High
- Textual feedback loop effectiveness: Medium
- Multi-task LVLM training benefits: High
- Flow-GRPO implementation details: Low (implementation not fully specified)

## Next Checks
1. Reconstruct the EmoSet-118K dataset with lexicon-based V-A sampling and MLLM captions
2. Implement the multi-task GRPO training pipeline for the LVLM reward model
3. Validate the Flow-GRPO fine-tuning of SD3.5-M with combined emotion + PickScore rewards