---
ver: rpa2
title: 'S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought
  LLMs'
arxiv_id: '2602.01982'
source_url: https://arxiv.org/abs/2602.01982
tags:
- llms
- customers
- arxiv
- data
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of redundant reasoning in large
  language models (LLMs) with chain-of-thought (CoT) capabilities, which leads to
  increased latency and cost. The authors propose S3-CoT, a self-sampling framework
  that enables efficient CoT learning without requiring external teacher guidance.
---

# S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs

## Quick Facts
- arXiv ID: 2602.01982
- Source URL: https://arxiv.org/abs/2602.01982
- Authors: Yanrui Du; Sendong Zhao; Yibo Gao; Danyang Zhao; Qika Lin; Ming Ma; Jiayun Li; Yi Jiang; Kai He; Qianyi Xu; Bing Qin; Mengling Feng
- Reference count: 34
- Primary result: Self-sampled succinct CoT reasoning compresses reasoning traces by 80-83% while maintaining or improving accuracy on math and medical benchmarks

## Executive Summary
This paper addresses the problem of redundant reasoning in chain-of-thought (CoT) large language models (LLMs) by proposing S3-CoT, a self-sampling framework that enables efficient CoT learning without external teacher guidance. The method identifies a variable-length direction (VL-D) in the LLM's representation space through activation steering, allowing the model to generate variable-length CoT traces directly from itself. Extensive experiments on math benchmarks (GSM8K, MATH, AMC23, AIME24) and medical benchmarks (MedQA, MedMCQA, BULLET) demonstrate that S3-CoT consistently outperforms prompt-control and SFT-based baselines, achieving near-best accuracy while substantially compressing CoT length by approximately 150-1100 tokens (20-17% of initial length).

## Method Summary
S3-CoT extracts a variable-length direction (VL-D) via contrastive activation pairs between long and short CoT prompts, then intervenes on hidden states during generation to control reasoning verbosity. The method uses either answer verification or self-consistency verification to filter high-quality samples, and employs a dual-cognitive system with progressive compression during fine-tuning. The approach is trained using LoRA with an 8x16 configuration, starting with mild compression and gradually expanding to more aggressive length reduction across curriculum stages.

## Key Results
- Compresses CoT traces by 80-83% while maintaining or improving accuracy across math and medical benchmarks
- Outperforms prompt-control and SFT-based baselines on GSM8K, MATH, AMC23, AIME24, MedQA, MedMCQA, and BULLET
- Achieves near-perfect accuracy (99.71%-99.91%) on retained samples using self-consistency verification
- Strong adaptability across general and R1-style LLMs with consistent efficiency gains

## Why This Works (Mechanism)

### Mechanism 1: Variable-Length Direction (VL-D) Enables Direct Length Control via Linear Intervention
Intervening along a discovered linear direction in representation space causally controls CoT length without external guidance. The method computes difference-in-means from contrastive prompt pairs and adds scaled versions to hidden states during forward pass to modulate output verbosity. This works under the assumption that length-controlling directions are approximately linear and transfer across samples within a model.

### Mechanism 2: Self-Consistency Filtering Retains Correct Samples Without Gold Labels
Samples where variable-length CoT variants agree on the answer achieve near-perfect accuracy, enabling self-supervised data curation. Multiple α values produce different-length traces, and only inputs where all variants predict the same answer are retained for training. This relies on the assumption that prediction consistency across compression levels correlates with correctness.

### Mechanism 3: Progressive Compression Curriculum Prevents Accuracy Collapse from Over-Compression
Gradually expanding the length-ratio distribution during SFT preserves reasoning ability while learning to compress. Training begins with Len-R ∈ [0.9, 1.0] and progressively expands to [0.0, 1.0] in 0.1 increments using dual-cognitive prompts that distinguish System 1 (compressed) and System 2 (full) objectives. This assumes models can generalize from mild to aggressive compression if trained incrementally.

## Foundational Learning

- **Activation Steering / Representation Engineering**
  - Why needed here: Core technique for extracting VL-D and intervening on hidden states to control reasoning length
  - Quick check question: Can you explain how adding a direction vector to residual-stream activations modulates behavior?

- **Self-Consistency in CoT Reasoning**
  - Why needed here: Underlies the quality-filtering mechanism for self-evolution mode without gold labels
  - Quick check question: Why would agreement across multiple reasoning paths increase confidence in correctness?

- **Curriculum Learning**
  - Why needed here: Explains why progressive compression outperforms direct shortest-CoT supervision
  - Quick check question: What happens if you train directly on the most compressed samples without intermediate steps?

## Architecture Onboarding

- **Component map**: VL-D Extraction -> Probing Module -> Self-Sampling Engine -> Filtering Layer -> SFT Trainer

- **Critical path**: VL-D extraction → probe for stable (α, layers) → sample variable-length traces → filter → progressive SFT

- **Design tradeoffs**:
  - Stronger intervention (larger |α|, more layers) → more compression but risk of generation collapse
  - Self-consistency filtering → no gold labels needed but sample efficiency depends on base model capability
  - Dual-cognitive system → enables progressive curriculum but requires careful prompt design

- **Failure signatures**:
  - Repetitive/gibberish outputs: Intervention too strong; reduce |α| or layer count
  - Accuracy collapse on validation set: Curriculum progressing too fast; reduce step size or expand validation checks
  - Low retention rate in self-consistency mode: Base model too weak; use answer verification instead

- **First 3 experiments**:
  1. Reproduce VL-D visualization on a small pilot set (100 samples) to confirm layer-wise separation emergence
  2. Run probe experiments to identify stable (α, layer count) before large-scale sampling
  3. Train S3-CoT on GSM8K with answer verification; compare Len-R distribution and accuracy vs. Standardp baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can S3-CoT be effectively integrated with RL-based methods to yield additional performance gains beyond either approach alone?
- Basis in paper: Limitations section states: "Whether such an integration can yield additional performance gains is an important direction for future research."
- Why unresolved: The paper only compares S3-CoT against RL baselines separately; no combined experiments were conducted.
- What evidence would resolve it: Experiments showing that S3-CoT can serve as a warm-start pre-training stage before RL, with measurable gains in the length-accuracy Pareto frontier compared to either method alone.

### Open Question 2
- Question: How can the accuracy degradation observed in R1-style LLMs during compression be mitigated while preserving efficiency gains?
- Basis in paper: Limitations section states: "for R1-style LLMs, both our method and existing methods still face the challenge of slight accuracy degradation."
- Why unresolved: The progressive compression curriculum helps general LLMs maintain or improve accuracy, but R1-style models consistently show small accuracy trade-offs across all methods.
- What evidence would resolve it: Identification of R1-specific architectural or training characteristics that cause compression sensitivity, and modifications that achieve compression without accuracy loss.

### Open Question 3
- Question: How can self-consistency verification be made efficient for weaker LLMs where sampling retention rates are very low?
- Basis in paper: Section 3.3 notes LLaMA3 8B retained only 517 of 6,838 samples via self-consistency, while stronger models achieved substantially higher efficiency.
- Why unresolved: The method assumes reasonable baseline capability; the computational cost of discarding most samples for weaker models may outweigh benefits.
- What evidence would resolve it: Adaptive sampling strategies or relaxed consistency criteria that improve retention while maintaining quality guarantees for lower-capability models.

## Limitations
- VL-D linearity assumption is only weakly validated empirically across diverse samples
- Progressive compression curriculum schedule is underspecified, making reproducibility challenging
- Self-consistency filtering may fail on models with weaker reasoning capabilities (e.g., 7.6% retention for LLaMA3 8B)

## Confidence

- **High**: The general framework of activation steering for length control, the self-consistency filtering mechanism for data curation, and the progressive compression curriculum for preventing accuracy collapse are well-supported by ablation studies and quantitative results.
- **Medium**: The specific implementation details of VL-D extraction (long/short prompt templates, layer selection criteria) and the exact dual-cognitive prompt templates require reasonable reconstruction.
- **Low**: The claim that self-sampled data can enable "LLM self-evolution" is more speculative, as the paper focuses on a single-stage fine-tuning rather than iterative self-improvement.

## Next Checks

1. **VL-D Transferability Validation**: Test VL-D vectors extracted from one model on a held-out subset of the same model to confirm linear steering remains effective across diverse samples. Measure angle variance between VL-D vectors and compression effectiveness.

2. **Curriculum Step Sensitivity**: Systematically vary the Len-R expansion step size (0.05 vs. 0.1) and measure accuracy degradation points. Identify the minimum step size that avoids accuracy collapse.

3. **Self-Consistency vs. Gold Label Filtering**: Compare S3-CoT trained with self-consistency filtering versus answer verification on the same base model. Quantify accuracy differences and retention rate trade-offs to validate the filtering mechanism's reliability.