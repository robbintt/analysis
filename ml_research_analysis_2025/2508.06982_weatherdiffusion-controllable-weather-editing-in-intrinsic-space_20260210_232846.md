---
ver: rpa2
title: 'WeatherDiffusion: Controllable Weather Editing in Intrinsic Space'
arxiv_id: '2508.06982'
source_url: https://arxiv.org/abs/2508.06982
tags:
- weather
- intrinsic
- editing
- image
- renderer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WeatherDiffusion introduces a diffusion-based framework for controllable
  weather editing in intrinsic space. It features an inverse renderer that decomposes
  images into material, geometry, and illumination maps, and a forward renderer that
  synthesizes new weather conditions using text prompts.
---

# WeatherDiffusion: Controllable Weather Editing in Intrinsic Space

## Quick Facts
- **arXiv ID:** 2508.06982
- **Source URL:** https://arxiv.org/abs/2508.06982
- **Reference count:** 40
- **Primary result:** Introduces diffusion-based intrinsic-space framework for controllable weather editing, outperforming state-of-the-art methods in image quality, text-image consistency, and downstream task robustness.

## Executive Summary
WeatherDiffusion introduces a diffusion-based framework for controllable weather editing in intrinsic space. It features an inverse renderer that decomposes images into material, geometry, and illumination maps, and a forward renderer that synthesizes new weather conditions using text prompts. The approach leverages intrinsic map-aware attention for better spatial correspondence and CLIP-space interpolation for fine-grained weather control. Evaluated on synthetic and real-world autonomous driving datasets, it significantly outperforms state-of-the-art pixel-space and intrinsic-space methods in image quality, text-image consistency, and downstream task robustness.

## Method Summary
WeatherDiffusion uses two diffusion-based components: an inverse renderer that estimates intrinsic scene properties (material, geometry, illumination) from input images, and a forward renderer that generates weather-edited images from these maps and text prompts. The inverse renderer repurposes Stable Diffusion 3.5's DiT backbone with intrinsic map-aware attention (IMAA) to improve decomposition fidelity. The forward renderer uses CLIP-space interpolation for fine-grained weather control. The model is trained on synthetic WeatherSynthetic data (38k images from Unreal Engine 5) and real WeatherReal data (18k images from Waymo/KITTI with pseudo-labels), achieving state-of-the-art results in image quality, text-image consistency, and downstream task performance.

## Key Results
- Outperforms state-of-the-art methods in PickScore (image quality), CLIP-S (text-image consistency), and downstream task robustness
- Achieves high PSNR/SSIM in inverse rendering of intrinsic maps (albedo, normal, roughness, metallicity, irradiance)
- Demonstrates fine-grained weather control through CLIP-space interpolation beyond discrete weather types

## Why This Works (Mechanism)

### Mechanism 1: Intrinsic Decomposition via Diffusion Priors
Pre-trained diffusion models encode learnable joint distributions between images and their intrinsic scene properties. The inverse renderer repurposes SD3.5's DiT backbone to predict intrinsic maps (albedo, normal, roughness, metallicity, irradiance) conditioned on input images, leveraging strong generative priors that capture physically grounded relationships between appearance and scene factors.

### Mechanism 2: Intrinsic Map-Aware Attention (IMAA)
Different intrinsic maps require spatially distinct attention patterns. IMAA uses DINOv2-extracted patch tokens gated by learnable map embeddings via an MLP, creating attention biases applied to both text-image and image-image attention in the DiT. This explicit guidance improves decomposition fidelity in large outdoor scenes by focusing on semantically relevant regions.

### Mechanism 3: CLIP-Space Weather Interpolation
Weather variations lie along linear directions in CLIP text embedding space, enabling fine-grained control. The method computes direction vectors between weather prompts and shifts a weather-neutral embedding, using this as conditioning during forward rendering. This produces semantically meaningful transitions unlike naive pixel-space interpolation.

## Foundational Learning

- **Intrinsic Image Decomposition**
  - Why needed: Core assumption that images factor into weather-invariant (albedo, normal, roughness, metallicity) and weather-variant (irradiance) components
  - Quick check: Given an image of a red car in shadow, can you identify which pixel variations belong to albedo vs. irradiance?

- **Diffusion Models / Flow Matching**
  - Why needed: WeatherDiffusion uses rectified flow (velocity field prediction) rather than standard DDPM; understanding the training objective L_θ is essential for debugging convergence
  - Quick check: What is the difference between predicting noise ε and predicting velocity v = ε − z₀ in flow matching?

- **CLIP Embeddings**
  - Why needed: Weather control operates via CLIP text encoder outputs; interpolation quality depends on understanding embedding geometry
  - Quick check: Why might linear interpolation in CLIP space produce semantically meaningful transitions while pixel-space interpolation does not?

## Architecture Onboarding

- **Component map:**
  Input image → VAE encode → concatenate with noise → Inverse DiT → decode intrinsic maps → Forward DiT → decode → rendered image

- **Critical path:**
  1. Input image → VAE encode → concatenate with noise → Inverse DiT → decode intrinsic maps
  2. Intrinsic maps + text prompt → VAE encode maps → concatenate with noise → Forward DiT → decode → rendered image

- **Design tradeoffs:**
  - Single-image vs. video: Current architecture lacks temporal modeling; video extension would require video diffusion backbone
  - Synthetic vs. real training: WeatherReal pseudo-labels improve realism but introduce label noise; WeatherSynthetic provides ground truth but has domain gap
  - Resolution: Progressive training (512 → 1024) improves detail but increases compute

- **Failure signatures:**
  - Dense fog/heavy snow: Inverse renderer hallucinates background geometry
  - Temporal inconsistency: Video sequences show flickering
  - IMAA collapse: Near-zero attention masks if heuristic pretraining skipped

- **First 3 experiments:**
  1. Validate inverse renderer on held-out weather types: Run on WeatherSynthetic test split with unseen weather combinations; measure PSNR/SSIM per map against ground truth
  2. Ablate IMAA: Train inverse renderer without IMAA for same iterations; compare normal and metallicity PSNR (expect ~1-4 dB drop)
  3. Test CLIP interpolation smoothness: Generate outputs with α ∈ {0.0, 0.3, 0.5, 0.7, 1.0}; measure perceptual smoothness (LPIPS between adjacent α values) against WeatherWeaver baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the WeatherDiffusion framework be extended to ensure temporal consistency for video-based weather editing?
- Basis in paper: The authors explicitly state that the current framework "does not guarantee temporal consistency in video sequences" and identify extending the framework to video-based editing as a primary direction for future work.
- Why unresolved: Temporal modeling introduces orthogonal factors like object motion and occlusion changes that require video priors, which were beyond the scope of this single-image study.

### Open Question 2
- Question: How can the inverse renderer handle extreme weather occlusions (e.g., dense fog) without hallucinating background details?
- Basis in paper: The authors acknowledge a failure case where "dense fog" causes the inverse renderer to "generate hallucinations in the background where information is completely occluded."
- Why unresolved: The model attempts to reconstruct geometry/intrinsic maps from pixels that contain no scene information, forcing a "hallucination" based on priors rather than evidence.

### Open Question 3
- Question: Does the reliance on self-generated pseudo-labels for the WeatherReal dataset limit the model's ability to correct domain gaps?
- Basis in paper: The WeatherReal dataset is constructed by applying the inverse renderer to real images to create training pairs. If the inverse renderer struggles with real data initially, the generated labels may reinforce those errors.
- Why unresolved: Using the model's own output as ground truth (self-training) can entrench existing biases rather than bridging the sim-to-real gap through external supervision.

## Limitations
- Dense fog/heavy snow cause inverse renderer to hallucinate background geometry due to complete information occlusion
- Current framework lacks temporal consistency guarantees for video sequences
- Reliance on self-generated pseudo-labels for WeatherReal may entrench domain gap errors rather than correct them

## Confidence

- **High confidence** in core mechanism: intrinsic decomposition via diffusion priors is well-supported by related work and ablation showing IMAA importance
- **Medium confidence** in CLIP interpolation: method is clearly specified but smoothness depends on CLIP's weather embedding geometry
- **Low confidence** in WeatherReal pipeline: pseudo-label quality is asserted but not quantitatively evaluated; text generation pipeline is sketchily described

## Next Checks

1. Validate IMAA pretraining protocol: Replicate progressive training schedule on proxy dataset; measure mask entropy evolution to detect collapse early
2. Test CLIP interpolation generalization: Generate weather interpolation paths for unseen weather pairs; measure CLIP similarity preservation and LPIPS smoothness against WeatherWeaver baseline
3. Assess reversibility and consistency: Apply inverse → forward pipeline with same weather condition and measure intrinsic map PSNR; then apply different weather conditions and check material/geometry invariance