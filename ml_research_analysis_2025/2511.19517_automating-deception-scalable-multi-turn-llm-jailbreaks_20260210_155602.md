---
ver: rpa2
title: 'Automating Deception: Scalable Multi-Turn LLM Jailbreaks'
arxiv_id: '2511.19517'
source_url: https://arxiv.org/abs/2511.19517
tags:
- safety
- conversational
- history
- multi-turn
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an automated pipeline for generating large-scale,
  psychologically-grounded multi-turn jailbreak datasets using the Foot-in-the-Door
  principle. The authors create 1,500 attack scenarios across illegal activities and
  offensive content, then evaluate seven LLMs under both multi-turn and single-turn
  conditions.
---

# Automating Deception: Scalable Multi-Turn LLM Jailbreaks

## Quick Facts
- arXiv ID: 2511.19517
- Source URL: https://arxiv.org/abs/2511.19517
- Reference count: 40
- Key outcome: GPT models vulnerable to conversational history (up to +32pp ASR), Gemini 2.5 Flash nearly immune, Claude 3 Haiku highly resistant

## Executive Summary
This paper introduces an automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets using the Foot-in-the-Door principle. The authors create 1,500 attack scenarios across illegal activities and offensive content, then evaluate seven LLMs under both multi-turn and single-turn conditions. Results show that GPT family models are highly vulnerable to conversational history, with ASR increasing by up to 32 percentage points, while Gemini 2.5 Flash is nearly immune and Claude 3 Haiku shows strong resistance. The study highlights that conversational context is a critical vulnerability vector and proposes "pretext stripping" as a mitigation strategy to improve safety.

## Method Summary
The authors developed a three-phase automated pipeline: (1) GPT-5 generates 1,500 FITD scenarios using low-effort reasoning, (2) target models tested under multi-turn (with conversation history) and single-turn (stateless) conditions with temperature=0.5, and (3) Gemini 1.5 Flash judges responses using a rule-based rubric validated with 98% human agreement. The dataset includes 1,000 illegal activity scenarios and 500 offensive content scenarios using 5-turn escalation templates. Attack success is measured by comparing ASR with and without conversation history, with 95% Wilson confidence intervals.

## Key Results
- GPT-4o Mini shows 32.00 percentage point increase in ASR for illegal activities when conversational history is present
- Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to FITD attacks (0.10% average ASR with history)
- Some GPT models show category-specific divergence: history decreased offensive content attack success by 7.80 percentage points while increasing illegal activity success
- Pretext stripping proposed as mitigation, though not empirically validated in the paper

## Why This Works (Mechanism)

### Mechanism 1
Conversational history primes safety classifiers to accept harmful requests they would otherwise refuse. The FITD technique builds a benign conversational pretext through escalating turns, causing the safety system to evaluate the final harmful request within an established cooperative context rather than in isolation.

### Mechanism 2
Harm category affects contextual vulnerability differently across model families. Safety training appears inconsistently applied, with some GPT models showing decreased offensive content success rates but increased illegal activity success when conversational history is present.

### Mechanism 3
Preemptive safety filtering before text generation provides near-complete FITD resistance. Gemini 2.5 Flash's API returns a blocked status before generating any text when detecting harmful intent, decoupling safety evaluation from conversational context entirely.

## Foundational Learning

- **Foot-in-the-Door (FITD) compliance principle**: Understanding the psychological foundation explains why gradual escalation succeeds where direct requests fail. FITD exploits commitment consistency—once someone agrees to small requests, they feel psychological pressure to maintain that cooperation pattern.
  - Quick check: Can you explain why a 5-turn escalation achieves higher ASR than sending the same final prompt directly?

- **Attack Success Rate (ASR) with/without history comparison**: The paper's core methodology isolates context effects by comparing identical final prompts with and without conversation history. The "Diff" column quantifies contextual vulnerability directly.
  - Quick check: If a model shows ASR of 30% with history and 5% without, what does this 25pp gap indicate about its safety architecture?

- **Pretext stripping as a defense**: The proposed mitigation directly addresses the mechanism—re-evaluating final prompts in isolation neutralizes FITD's contextual priming effect.
  - Quick check: Why might pretext stripping reduce helpfulness for legitimate multi-turn queries?

## Architecture Onboarding

- **Component map**: Dataset Generator (GPT-5) → Target Model Tester → LLM Judge (Gemini 1.5 Flash) → Manual Review
- **Critical path**: Dataset generation → Template validation (96.2% escalation coherence) → Dual-track testing → Judge classification → Manual review of uncertain cases → ASR calculation with Wilson score CIs
- **Design tradeoffs**: Single judge vs. ensemble (simpler but risks bias), temperature 0.5 for realistic behavior vs. 0.0 for consistency, 5-turn fixed template vs. adaptive escalation (reproducible but may miss vulnerabilities)
- **Failure signatures**: Context-sensitive models show large positive ASR gap between conditions (GPT-4o Mini: +32pp), context-robust models show near-zero ASR regardless of history (Gemini 2.5 Flash: 0.10% with, 0.65% without), category divergence indicates fragmented safety training
- **First 3 experiments**: 1) Baseline replication on your target model, 2) Pretext stripping prototype implementation, 3) Category-specific probing to identify fragmented safety layers

## Open Questions the Paper Calls Out

### Open Question 1
How robust are current safety architectures against adaptive attacks that modify their strategy based on the model's refusal responses? The current study relies on static, pre-defined 5-turn templates and does not test dynamic attacks that react to specific model defenses in real-time.

### Open Question 2
Does the "pretext stripping" mitigation strategy effectively reduce Attack Success Rates (ASR) in vulnerable models like GPT-4o Mini without degrading performance on benign multi-turn queries? The authors propose this defense conceptually but lack empirical data confirming it works for specific models that failed the benchmarks.

### Open Question 3
Do the observed vulnerabilities in illegal and offensive content categories generalize to more nuanced harms, such as misinformation or psychological manipulation? The benchmark is limited to "Illegal Activities" and "Offensive Content"; it remains unknown if FITD is equally effective for less explicitly prohibited harms.

### Open Question 4
Does using an ensemble of LLM judges provide significantly higher fidelity in evaluating jailbreak success compared to the single-judge approach used in this study? While the authors validated their single judge with high human agreement, they acknowledge that ensemble methods could further refine the evaluation protocol.

## Limitations

- Dataset generation relies entirely on GPT-5's reasoning capabilities, introducing potential bias in attack template quality
- LLM judge methodology, despite 98% human agreement validation, may be subject to systematic bias from Gemini 1.5 Flash's own safety training
- Corpus lacks systematic exploration of intermediate conversation turns, making it difficult to pinpoint exactly where safety systems fail
- "Pretext stripping" defense is proposed conceptually but not empirically validated against the generated dataset

## Confidence

- **High Confidence**: The finding that conversational history affects attack success rates differently across model families is well-supported by ASR comparison data (GPT-4o showing +32pp difference)
- **Medium Confidence**: The FITD mechanism explanation and its interaction with safety classifiers is plausible but relies on inference from aggregate data rather than direct model introspection
- **Low Confidence**: The pretext stripping defense mechanism lacks empirical validation in the paper

## Next Checks

1. **Independent Dataset Generation**: Reproduce the 1,500-scenario dataset using a different LLM (e.g., Claude or GPT-4) to test whether GPT-5's specific reasoning patterns influence attack effectiveness, and whether observed ASR differences persist across generation sources.

2. **Intermediate Turn Analysis**: Systematically test whether the ASR gap emerges at specific conversation stages (e.g., after turn 3 vs. turn 5) to identify the precise vulnerability window and whether some models resist early escalation but fail at later stages.

3. **Cross-Judge Validation**: Evaluate the same dataset using multiple LLM judges (different model families) to assess whether Gemini 1.5 Flash's judgments are systematically biased by its own safety training, and to establish inter-judge reliability metrics.