---
ver: rpa2
title: How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn
  Conversations
arxiv_id: '2508.05625'
source_url: https://arxiv.org/abs/2508.05625
tags:
- persuasion
- persuadee
- persuader
- figure
- turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Linear probes were used to analyze persuasion dynamics in LLM conversations,
  focusing on persuasion outcomes, personality traits, and rhetorical strategies.
  The approach leverages insights from cognitive science and is computationally more
  efficient than prompting-based methods.
---

# How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations

## Quick Facts
- **arXiv ID**: 2508.05625
- **Source URL**: https://arxiv.org/abs/2508.05625
- **Reference count**: 40
- **Primary result**: Linear probes trained on synthetic LLM-generated conversations achieve high accuracy in detecting persuasion outcomes, personality traits, and rhetorical strategies, matching or outperforming prompting methods.

## Executive Summary
This paper introduces linear probes as an efficient method for analyzing persuasion dynamics in multi-turn LLM conversations. The approach trains logistic regression classifiers on frozen activation vectors from Llama-3.2-3B, achieving high accuracy in detecting persuasion outcomes, personality traits, and rhetorical strategies. Probes are trained on synthetic data and evaluated on both human-human and synthetic datasets, showing strong generalization. The work reveals that persuasion signals localize differently in human versus synthetic dialogues and identifies correlations between personality traits and strategy effectiveness.

## Method Summary
The method uses linear probes—logistic regression classifiers—trained on residual stream activations from Llama-3.2-3B to detect persuasion-related features. Probes are trained on synthetic dialogues generated by GPT-4o, with labels for persuasion outcomes, Big-5 personality traits, and rhetorical strategies. Activations are extracted from layer 26 for each token, and probes are evaluated on human-human (PersuasionforGood) and synthetic (DailyPersuasion) datasets using AUROC, MSE, and Jensen-Shannon divergence metrics.

## Key Results
- Linear probes match or outperform prompting methods for detecting persuasion outcomes and strategies.
- Persuasion signals concentrate in middle turns for human dialogues but in final turns for synthetic dialogues.
- Extraversion correlates with susceptibility to emotional appeals and resistance to credibility/logical appeals.
- Probes achieve high accuracy in personality trait detection despite noisy ground truth labels.

## Why This Works (Mechanism)

### Mechanism 1
Linear probes trained on residual stream activations can detect persuasion-related features with accuracy matching or exceeding prompting. A logistic regression classifier is trained on frozen activations from layer 26/30, learning to map activation directions to behavioral labels without modifying the base model. Core assumption: persuasion-relevant features are linearly separable in middle-to-late layer activation space. Evidence: probes achieve high AUROC on persuasion detection, with performance peaking at actual persuasion moments.

### Mechanism 2
Persuasion signals localize differently in human vs. synthetic dialogues—middle turns for human-human, final turns for LLM-generated. The probe's AUROC trajectory across turns reveals where discriminative signal concentrates. In human dialogues, signal peaks at turns 8-12; in synthetic dialogues, signal spikes at final 1-2 turns. Core assumption: the probe's performance trajectory reflects true signal distribution. Evidence: GPT-4.1 validation confirms this pattern, though cross-model applicability requires further testing.

### Mechanism 3
Extraversion correlates with susceptibility to emotional appeals and resistance to credibility/logical appeals. Probe-derived personality scores and strategy probabilities are correlated across datasets, showing consistent moderate correlations (r ≈ 0.3-0.4) with emotional appeal effectiveness. Core assumption: probe-derived personality estimates are sufficiently accurate to reveal trait-strategy interactions. Evidence: correlations are consistent across datasets, though personality prediction from short text remains challenging.

## Foundational Learning

- **Residual Stream Probing**: Understanding that LLM activations contain linearly decodable information about abstract behaviors, and that layer selection matters (middle-late layers for complex features). Quick check: Why would middle-to-late layers (26/30) yield better persuasion detection than early layers?

- **Cognitive Science Frameworks for Persuasion**: The paper grounds probe design in established frameworks (Big-5 personality, Aristotelian rhetorical triangle) rather than inventing new taxonomies. Quick check: Why might extraversion specifically correlate with emotional appeal susceptibility, based on dual-process theories?

- **AUROC and Jensen-Shannon Divergence for Behavioral Evaluation**: Ground truth for persuasion strategy is noisy (κ = 0.33), requiring distributional comparison rather than point estimates. Quick check: When would JSD distance to a reference model be preferred over direct accuracy metrics?

## Architecture Onboarding

- **Component map**: Synthetic data generator -> Activation extractor -> Linear probe -> Evaluation datasets
- **Critical path**: 1) Generate synthetic training data with explicit labels 2) Extract activations from proxy model 3) Train probes on ~100 samples/class, validate layer selection 4) Evaluate on held-out human and synthetic datasets
- **Design tradeoffs**: Single-layer probing chosen for simplicity and efficiency; may miss cross-layer features. Synthetic training enables controlled labels but may not capture human persuasion nuance. Binary classifiers for personality vs. one three-class classifier for strategy.
- **Failure signatures**: Probe performs well on synthetic but fails on OOD human data → synthetic distribution shift. AUROC trajectory flat across all turns → persuasion signal not captured or probe underfitting. High MSE on personality prediction → text insufficiently informative or probe architecture insufficient.
- **First 3 experiments**: 1) Layer sweep: Train probes on layers 0-30, plot AUROC vs. layer to validate layer 26/30 selection 2) Sample efficiency curve: Vary training samples from 10-500 per class to identify minimum viable training set size 3) Cross-dataset transfer: Train on PfG, evaluate on DP (and reverse) to quantify synthetic-to-human generalization gap

## Open Questions the Paper Calls Out

- Do linear probes trained on smaller models (e.g., 3B parameters) generalize to larger model architectures, or does probing effectiveness scale with model size?
- Can linear probes effectively detect other complex, multi-turn behaviors such as deception or manipulation in LLMs?
- How does probe performance change when analyzing conversations generated by models different from the one used to extract activations?

## Limitations

- Synthetic data dependency may limit probe generalizability to real human persuasion dynamics.
- Personality detection accuracy is limited by noisy ground truth and short conversational text.
- Strategy detection is challenged by low inter-annotator agreement and reliance on reference model evaluation.
- Single-layer probing may miss information distributed across multiple layers.

## Confidence

- **High Confidence**: Core methodology and probe performance on persuasion outcomes.
- **Medium Confidence**: Localization of persuasion signals and trait-strategy correlations, requiring further validation.

## Next Checks

1. Cross-dataset generalization test: Train on new synthetic dataset, evaluate on PfG, DP, and third held-out human dataset.
2. Multi-layer probe comparison: Compare single-layer probe to concatenated multi-layer probe performance.
3. Behavioral validation of personality correlations: Conduct human-subject experiment to validate probe-predicted trait-strategy relationships.