---
ver: rpa2
title: A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of
  Latent Subspaces
arxiv_id: '2508.00472'
source_url: https://arxiv.org/abs/2508.00472
tags:
- data
- samples
- class
- ctdgan
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses class imbalance in tabular data by proposing
  a conditional GAN model, ctdGAN, that synthesizes data for under-represented classes.
  Unlike prior models, ctdGAN uses clustering to assign cluster labels and incorporates
  both cluster and class labels into its sampling strategy, enabling data generation
  in subspaces that better match the original data distribution.
---

# A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces

## Quick Facts
- **arXiv ID:** 2508.00472
- **Source URL:** https://arxiv.org/abs/2508.00472
- **Reference count:** 40
- **Key outcome:** Proposes ctdGAN, a conditional GAN with clustering and probabilistic sampling, that outperforms ctGAN, CopulaGAN, and others on imbalanced tabular datasets.

## Executive Summary
This paper tackles the challenge of class imbalance in tabular data by introducing ctdGAN, a conditional GAN that leverages clustering to improve minority class generation. Unlike existing tabular GANs, ctdGAN assigns cluster labels to data points and uses both class and cluster labels in its sampling strategy. This approach enables more realistic synthetic data generation, particularly for under-represented classes. The model includes a novel generator loss that penalizes both misclassification and misclustering, and a probabilistic sampling method that selects appropriate clusters based on class membership. Experiments on 14 benchmark datasets show improved F1 scores and balanced accuracy compared to state-of-the-art tabular synthesis models.

## Method Summary
ctdGAN combines conditional GAN architecture with a clustering-based sampling strategy. The method clusters the original tabular data, assigns both class and cluster labels, and modifies the generator's loss function to penalize both misclassifications and misclusterings. A novel probabilistic sampling step selects clusters for the latent space according to the class membership probabilities, encouraging generation of minority class samples in relevant subspaces. The approach is designed to preserve the distribution of both features and minority classes in synthetic data.

## Key Results
- ctdGAN improves classification F1 score and balanced accuracy on 14 imbalanced tabular datasets.
- Outperforms ctGAN, CopulaGAN, CTAB-GAN+, SB-GAN, and TV AE in both fidelity and downstream task utility.
- Novel probabilistic sampling of latent subspaces better preserves minority class distributions.

## Why This Works (Mechanism)
ctdGAN's success stems from its integration of clustering and conditional sampling. By assigning both cluster and class labels, the model can generate synthetic data in subspaces that better reflect the underlying data distribution, especially for minority classes. The modified generator loss function enforces fidelity to both the original class and cluster structure, while the probabilistic sampling step ensures that minority class samples are generated in relevant regions of the latent space. This dual conditioning and targeted sampling mitigates the tendency of GANs to collapse toward majority classes.

## Foundational Learning
- **Conditional GANs:** Why needed: to generate samples conditioned on class labels, enabling class-specific synthesis. Quick check: Are both class and cluster labels incorporated into the generator?
- **Clustering-based sampling:** Why needed: to discover latent data structure and guide generation toward underrepresented regions. Quick check: Are clusters used to condition the latent space?
- **Probabilistic sampling:** Why needed: to ensure minority class samples are generated proportionally to their distribution. Quick check: Is sampling weighted by class-cluster membership probabilities?
- **Tabular data synthesis:** Why needed: to create realistic, privacy-preserving synthetic datasets for ML tasks. Quick check: Are both continuous and categorical features handled?
- **Generator loss modification:** Why needed: to enforce both classification and clustering accuracy during training. Quick check: Does the loss penalize both misclassifications and misclusterings?
- **Imbalanced learning:** Why needed: to address the over-representation of majority classes in synthetic data. Quick check: Are minority classes adequately generated?

## Architecture Onboarding
- **Component map:** Data → Clustering → Class/Cluster Labeling → Conditional GAN (Generator/Discriminator) → Probabilistic Latent Sampling → Synthetic Data
- **Critical path:** Clustering → Conditional Labeling → Modified Loss → Probabilistic Sampling → Generation
- **Design tradeoffs:** Clustering improves minority class generation but adds computational cost; probabilistic sampling increases fidelity but introduces hyperparameter sensitivity.
- **Failure signatures:** Overfitting to clusters, collapse to majority classes, poor scaling with high dimensionality.
- **First experiments:** 1) Generate synthetic data for a balanced dataset; 2) Evaluate minority class generation on a moderately imbalanced dataset; 3) Compare F1 score and balanced accuracy against ctGAN baseline.

## Open Questions the Paper Calls Out
- None explicitly listed in the paper.

## Limitations
- Limited evaluation on high-dimensional or non-standard tabular features (e.g., text, images).
- Clustering step may not capture complex latent structures, risking poor minority class generation.
- Probabilistic sampling introduces hyperparameter sensitivity and is not extensively validated.
- Computational overhead of clustering and modified loss not discussed.

## Confidence
- **High** for conditional GAN architecture and clustering integration.
- **Medium** for comparative performance gains and probabilistic sampling novelty.
- **Low** for robustness claims on highly skewed or complex feature distributions.

## Next Checks
1. Test ctdGAN on synthetic tabular datasets with extreme class imbalance and high dimensionality.
2. Conduct ablation studies to isolate the impact of clustering and modified loss.
3. Compare ctdGAN's fidelity and downstream utility against a broader set of tabular synthesis baselines, including transformer-based models.