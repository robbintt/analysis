---
ver: rpa2
title: Bayesian-based Online Label Shift Estimation with Dynamic Dirichlet Priors
arxiv_id: '2511.18615'
source_url: https://arxiv.org/abs/2511.18615
tags:
- test
- online
- shift
- prior
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Bayesian-based framework for online label
  shift estimation, which occurs when test data priors differ from training data while
  the conditional likelihood remains unchanged. The proposed FMAPLS algorithm dynamically
  optimizes Dirichlet hyperparameters and class priors using Expectation-Maximization,
  overcoming rigid assumptions in prior methods.
---

# Bayesian-based Online Label Shift Estimation with Dynamic Dirichlet Priors

## Quick Facts
- arXiv ID: 2511.18615
- Source URL: https://arxiv.org/abs/2511.18615
- Authors: Jiawei Hu; Javier A. Barria
- Reference count: 40
- Primary result: FMAPLS achieves up to 40% lower KL divergence and 12% lower online KL divergence vs. baselines on CIFAR100/ImageNet

## Executive Summary
This paper addresses the label shift problem where test priors differ from training priors while conditional likelihoods remain invariant. The authors propose FMAPLS, a Bayesian framework that dynamically optimizes Dirichlet hyperparameters and class priors using Expectation-Maximization. The method introduces a linear surrogate function for closed-form hyperparameter updates, significantly reducing computational complexity. An online variant, online-FMAPLS, extends this to streaming data via stochastic approximation, enabling real-time adaptation to changing distributions.

## Method Summary
The approach tackles label shift estimation through Bayesian inference with dynamic Dirichlet priors. FMAPLS operates in batch mode, initializing uniform Dirichlet parameters and class priors, then iteratively updating them via EM. The M-step uses a linear surrogate function (LSF) to provide closed-form updates for Dirichlet hyperparameters, while class priors are updated using weighted posteriors. Online-FMAPLS replaces the batch E-step with stochastic approximation for real-time updates on streaming data. Both variants reweight classifier posteriors to correct for estimated prior shifts, with extensive experiments demonstrating superior performance over state-of-the-art baselines.

## Key Results
- FMAPLS achieves up to 40% lower KL divergence compared to state-of-the-art baselines
- Online-FMAPLS attains up to 12% lower KL divergence with efficient online updates
- Substantial accuracy improvements observed under severe class imbalance and distributional uncertainty
- Superior performance validated on CIFAR100 and ImageNet datasets with various long-tail scenarios

## Why This Works (Mechanism)
The method works by leveraging Bayesian inference to dynamically adapt Dirichlet priors based on observed data patterns. The EM framework alternates between estimating posterior distributions (E-step) and optimizing hyperparameters (M-step). The key innovation is the linear surrogate function, which enables closed-form updates for Dirichlet parameters while maintaining theoretical convergence guarantees. This dynamic adjustment allows the algorithm to capture evolving prior distributions without rigid assumptions about their form, making it particularly effective in non-stationary environments.

## Foundational Learning

**Expectation-Maximization Algorithm**
- Why needed: Core optimization framework for estimating latent variables and model parameters
- Quick check: Verify alternating E-step (posterior estimation) and M-step (parameter updates) convergence

**Label Shift Assumption**
- Why needed: Foundation for valid reweighting strategy when P(X|Y) remains invariant
- Quick check: Confirm prior shift without likelihood shift in experimental setup

**Dirichlet Distribution Properties**
- Why needed: Conjugate prior for multinomial distributions enables tractable Bayesian updates
- Quick check: Validate α updates maintain valid Dirichlet parameter space

## Architecture Onboarding

**Component Map**
Input Data -> ResNet Classifier -> Soft Posteriors -> FMAPLS/Online-FMAPLS -> Estimated Priors -> Reweighted Predictions

**Critical Path**
Classifier output → E-step (posterior estimation) → M-step (hyperparameter updates) → Prior correction → Final predictions

**Design Tradeoffs**
Batch vs. online computation: FMAPLS provides accurate estimates but requires full dataset access; online-FMAPLS trades some accuracy for real-time adaptation and lower memory requirements

**Failure Signatures**
Non-convergence indicated by oscillating KL divergence; poor performance suggests violated label shift assumption or inadequate hyperparameter tuning

**First Experiments**
1. Validate convergence behavior on synthetic label shift data with known ground truth
2. Compare batch vs. online variants on CIFAR100 with controlled prior shifts
3. Test sensitivity to LSF constant c across different imbalance scenarios

## Open Questions the Paper Calls Out

**Open Question 1**
Can the Linear Surrogate Function (LSF) parameter $c$ be adaptively tuned to optimize the trade-off between convergence rate and estimation accuracy in non-stationary environments?
- Basis in paper: Section IV.C explicitly analyzes the trade-off governed by $c$, stating that large $c$ yields smaller update steps (slow convergence) while small $c$ introduces estimation errors
- Why unresolved: The paper characterizes the theoretical bound $O(1/\hat{c})$ but relies on a fixed $c$ in implementations, treating the trade-off as a static design choice rather than a dynamic optimization problem
- What evidence would resolve it: An adaptive scheduling mechanism for $c$ that maintains theoretical convergence guarantees while dynamically minimizing KL divergence on the fly

**Open Question 2**
How robust is the FMAPLS framework to violations of the strict label shift assumption, specifically when the conditional likelihood $P(X|Y)$ differs between training and test domains?
- Basis in paper: The theoretical derivation relies entirely on $P(X^s|Y^s) = P(X^t|Y^t)$. Real-world "concept shift" or "covariate shift" often violates this, but the paper only evaluates scenarios where this assumption holds perfectly
- Why unresolved: The method re-weights posteriors based on invariant likelihoods; if likelihoods drift, the estimation of $\pi$ may become biased, a sensitivity not explored in the ablation studies
- What evidence would resolve it: Experiments on datasets with simulated likelihood drift (e.g., corrupted test features) comparing FMAPLS robustness against baseline methods

**Open Question 3**
Does the fixed confidence parameter $\gamma$ in the stochastic approximation update limit the ability of online-FMAPLS to track sudden, drastic shifts in class priors?
- Basis in paper: Equation 24 uses a fixed $\gamma$ to balance new data against history. While discussed as controlling the emphasis on future vs. current data, the paper does not explore the impact of a static $\gamma$ in environments with abrupt distribution changes (concept drift)
- Why unresolved: A fixed step-size implies a fixed "memory" length, which may cause lag in sudden shifts or high variance in stable periods
- What evidence would resolve it: Ablation studies on streaming data with sudden "regime changes" in prior distribution to determine if an adaptive $\gamma$ improves tracking speed

## Limitations
- LSF constant c is only described as "sufficiently large" without specific values, hindering exact replication
- Confidence parameter γ and maximum iterations T_max are not specified in the paper
- Test sample sizes are incompletely specified, particularly for shuffled and ImageNet datasets

## Confidence
- **High confidence**: Core theoretical framework and mathematical formulations are clearly specified and reproducible
- **Medium confidence**: Empirical results are convincing but depend on unmentioned hyperparameter choices
- **Medium confidence**: Computational efficiency claims are supported by algorithmic design, though runtime comparisons are absent

## Next Checks
1. Conduct sensitivity analysis on LSF constant c values (e.g., 10-100) to identify optimal settings and stability thresholds
2. Verify convergence behavior across different γ and T_max combinations, documenting iteration counts and final KL divergence values
3. Replicate key results on CIFAR100 using publicly available pretrained ResNet models, comparing against at least two baseline methods under identical hyperparameter grids