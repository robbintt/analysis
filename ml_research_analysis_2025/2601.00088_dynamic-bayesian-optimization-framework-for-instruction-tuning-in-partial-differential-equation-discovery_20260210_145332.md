---
ver: rpa2
title: Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential
  Equation Discovery
arxiv_id: '2601.00088'
source_url: https://arxiv.org/abs/2601.00088
tags:
- equation
- discovery
- prompt
- optimization
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of instruction brittleness in
  large language models (LLMs) for partial differential equation (PDE) discovery.
  LLMs struggle with static prompts that cannot adapt to evolving search states, leading
  to suboptimal solutions.
---

# Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery

## Quick Facts
- arXiv ID: 2601.00088
- Source URL: https://arxiv.org/abs/2601.00088
- Reference count: 40
- Key outcome: NeuroSym-BO framework significantly outperforms fixed prompts for PDE discovery, achieving 5-11% improvements on challenging cases and solving problems where static prompting fails

## Executive Summary
NeuroSym-BO addresses instruction brittleness in LLMs for PDE discovery by treating prompt engineering as a sequential decision problem. The framework maintains a library of reasoning strategies and uses Bayesian Optimization to select optimal instructions at each step based on numerical feedback. Experiments on five benchmark PDEs show significant improvements over fixed prompts, with the adaptive system achieving higher recovery rates and more parsimonious solutions. The method demonstrates consistent performance across multiple trials and solves cases where static prompting fails.

## Method Summary
The method treats instruction selection as a sequential optimization problem where a Bayesian Optimization agent selects strategies from a pre-constructed bank of 100 reasoning directives. At each iteration, the selected strategy, static task context, and top-5 historical equations are assembled into a prompt for Llama-3.2-3B-Instruct, which generates five candidate equation skeletons. These candidates undergo sparse regression (STRidge) to fit coefficients, then receive a composite fitness score that balances accuracy against equation complexity. The BO surrogate (Gaussian Process) models the mapping from strategy indices to fitness outcomes, using Expected Improvement to balance exploration and exploitation. The framework iterates for 300 steps, maintaining a history buffer for in-context learning.

## Key Results
- NeuroSym-BO achieves 5-11% improvements over fixed prompts on challenging PDE discovery cases
- Allen-Cahn equation recovery rate improves from 0.7824 to 0.8914 (Test R²)
- Framework solves PDE discovery problems where static prompting fails entirely
- Consistent performance across five benchmark PDEs including Burgers, Fisher, and Chafee-Infante equations

## Why This Works (Mechanism)

### Mechanism 1: Bayesian Optimization Over Discrete Strategy Space
The framework treats instruction selection as sequential optimization, enabling adaptive strategy switching that fixed prompts cannot achieve. A Gaussian Process surrogate models the mapping from strategy index k to fitness outcomes, using Expected Improvement acquisition to balance exploitation against exploration. This works under the assumption that the fitness landscape over strategies is sufficiently smooth for GP interpolation to provide meaningful signal.

### Mechanism 2: Closed-Loop Numerical Feedback with Parsimony Penalty
Numerical evaluation provides grounded, task-specific feedback that guides both equation refinement and strategy selection. Each candidate receives a composite fitness score that combines NRMSE accuracy with a parsimony penalty, ensuring simpler equations with similar accuracy receive higher scores. This works under the assumption that the fitness function correctly encodes the true objective of finding parsimonious ground-truth equations.

### Mechanism 3: In-Context Learning via Dynamic Memory
Including top-N best equations in the prompt provides implicit gradient information that guides LLM generation toward promising structural patterns. The prompt concatenates static context, top-5 history, and selected strategy, allowing the LLM to observe which structural features correlate with high fitness without explicit gradient computation. This works under the assumption that the LLM can perform meaningful pattern recognition over equation-score pairs.

## Foundational Learning

- **Bayesian Optimization with Gaussian Processes**: Core mechanism for strategy selection; requires understanding surrogate modeling, acquisition functions, and exploration-exploitation tradeoffs. Quick check: Can you explain why Expected Improvement might select a strategy with lower predicted mean but higher uncertainty?

- **Sparse Regression for Symbolic Discovery**: Numerical evaluator uses STRidge to fit coefficients; understanding sparsity-promoting regularization is essential for interpreting fitness scores. Quick check: Why might L1-regularized regression fail to recover the true equation even when it's in the library?

- **Instruction Brittleness in LLMs**: Motivating problem; sensitivity to prompt phrasing explains why adaptive selection outperforms static prompts. Quick check: What types of prompt variations would you expect to cause largest output changes in equation discovery tasks?

## Architecture Onboarding

- **Component map**: Strategy Bank (K=100) -> Bayesian Optimizer (GP + EI) -> Prompt Constructor (Static + History + Strategy) -> Symbolic Proposer (LLM) -> Numerical Evaluator (STRidge + Fitness) -> History Buffer

- **Critical path**: 1. BO selects strategy k_t -> 2. Assemble prompt P_t -> 3. LLM generates candidates -> 4. Evaluate fitness -> 5. Update history + BO observations -> 6. Repeat

- **Design tradeoffs**: Strategy bank size (K=100) balances coverage against GP inference speed; history size (N=5) trades pattern recognition quality against prompt length; initial exploration phase (K_init=10) balances surrogate reliability against random sampling waste

- **Failure signatures**: Early plateau with no step-wise improvements indicates BO not successfully switching strategies; high variance across trials suggests Strategy Bank coverage issues or GP instability; equations growing in complexity without fitness gain indicates parsimony penalty λ too weak

- **First 3 experiments**: 1. Ablate BO → Random Selection to validate Bayesian optimization contribution; 2. Vary λ (parsimony penalty) on Allen-Cahn to test fitness formulation sensitivity; 3. Vary history size N on Burgers equation to probe in-context learning contribution

## Open Questions the Paper Calls Out
1. Can NeuroSym-BO maintain effectiveness when scaling to higher-dimensional systems or PDEs exhibiting chaotic behavior?
2. To what extent can dynamic prompt optimization compensate for a backbone LLM that lacks fundamental knowledge of specific mathematical operators?
3. How sensitive is the framework's performance to the specific composition and diversity of the pre-constructed Strategy Bank?

## Limitations
- Scaling to higher-dimensional systems with chaotic behavior remains future work
- If the LLM lacks fundamental knowledge of required mathematical operators, prompt optimization alone cannot solve the problem
- Performance sensitivity to Strategy Bank composition and diversity is unexplored

## Confidence
- **High confidence**: Bayesian optimization framework design, fitness function formulation, general experimental methodology
- **Medium confidence**: Specific fitness function hyperparameters (λ=0.01), strategy bank coverage (K=100), history buffer size (N=5)
- **Low confidence**: Claim that in-context learning provides "gradient-like" guidance without formal ablation studies

## Next Checks
1. **Strategy bank ablation**: Run with K=10 vs K=100 to quantify coverage vs computational cost trade-off
2. **Fitness function robustness**: Test λ sensitivity across [0.001, 0.01, 0.1] on all five datasets
3. **In-context learning isolation**: Compare performance with N=0 (no history) vs N=5 to isolate contribution of implicit gradient mechanism