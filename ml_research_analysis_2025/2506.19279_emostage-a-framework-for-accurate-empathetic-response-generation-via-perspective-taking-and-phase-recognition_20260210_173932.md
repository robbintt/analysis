---
ver: rpa2
title: 'EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking
  and Phase Recognition'
arxiv_id: '2506.19279'
source_url: https://arxiv.org/abs/2506.19279
tags:
- client
- counseling
- response
- psychological
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EmoStage, a framework that enhances empathetic
  response generation in psychological counseling by integrating perspective-taking
  and phase recognition. The framework leverages open-source LLMs without requiring
  additional training data, addressing limitations such as limited understanding of
  clients' psychological states, inappropriate timing of interventions, and data scarcity.
---

# EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition

## Quick Facts
- arXiv ID: 2506.19279
- Source URL: https://arxiv.org/abs/2506.19279
- Reference count: 22
- Primary result: EmoStage significantly improves empathetic response quality over base models without requiring training data

## Executive Summary
This paper introduces EmoStage, a framework that enhances empathetic response generation in psychological counseling by integrating perspective-taking and phase recognition. The framework leverages open-source LLMs without requiring additional training data, addressing limitations such as limited understanding of clients' psychological states, inappropriate timing of interventions, and data scarcity. EmoStage first infers the client's emotional state and needs through perspective-taking, then identifies the counseling phase to ensure contextually appropriate responses. Experiments in both Japanese and Chinese settings demonstrate that EmoStage significantly improves response quality over base models and performs competitively with data-driven approaches. Human and automatic evaluations confirm its effectiveness, with the combination of perspective-taking and phase recognition being critical to performance gains.

## Method Summary
EmoStage is a 3-step inference-only framework that generates empathetic counseling responses using base LLMs. The framework takes dialogue history as input and processes it through three sequential LLM calls: (1) Perspective-Taking analyzes the full history to infer the client's psychological state $z_t$, (2) Phase Recognition identifies the current counseling stage $p_t$ using the last 3 turns plus $z_t$, and (3) Response Generation produces the final output using the full history plus both intermediate results. The approach requires no fine-tuning and works with open-source models like Qwen-7B and Llama-3.1-Swallow-8B, demonstrating cross-lingual capability in Japanese and Chinese settings.

## Key Results
- EmoStage significantly outperforms base LLMs in automatic evaluation metrics (Comprehensiveness, Professionalism, Authenticity, Safety) with medium-to-large effect sizes
- Human pairwise preference evaluation shows EmoStage generates more empathetic and professional responses than baseline models
- The framework performs competitively with data-driven approaches while requiring no additional training data
- Ablation studies confirm both perspective-taking and phase recognition components are critical, with the full framework outperforming either component alone

## Why This Works (Mechanism)

### Mechanism 1
Inferring the client's latent psychological state via explicit perspective-taking improves empathetic resonance compared to direct response generation. The framework prompts the LLM to generate an intermediate text $z_t$ analyzing the client's "emotional state, psychological distress, and support needs" before generating a reply. This forces the model to attend to implicit cues rather than just surface-level semantics. Fails if the dialogue history is too short or ambiguous for the model to form a distinct psychological profile.

### Mechanism 2
Constraining response strategy via phase recognition mitigates the risk of premature advice-giving or contextually inappropriate interventions. The model classifies the dialogue into one of six predefined counseling stages (e.g., "Problem Clarification", "Emotion Exploration") and generates a response strategy aligned with that stage. This acts as a guardrail against skipped steps in the counseling process. Fails if the conversation is non-linear or if the client abruptly shifts topics.

### Mechanism 3
Sequential conditioning on psychological state ($z_t$) and counseling phase ($p_t$) is necessary for optimal performance; neither component is sufficient alone. The framework concatenates the dialogue history, the inferred state, and the phase strategy into a single prompt for the final response. This provides the LLM with explicit "reasoning traces" to ground the output. Fails if the intermediate outputs contain errors, as the final generation prompt includes these potentially noisy signals.

## Foundational Learning

- **Counseling Stage Theory**: Understanding the 6-stage counseling progression (Table 1) to interpret phase recognition outputs. Quick check: Can you explain why "Problem Solving" should not occur before "Emotion Exploration"?
- **Prompt Engineering (In-Context Learning)**: EmoStage is entirely inference-based; understanding how to structure the 3-step prompt chain is critical. Quick check: How does the "one-shot" example in the perspective-taking prompt guide the model's output format?
- **Theory of Mind (ToM) in LLMs**: The perspective-taking mechanism relies on the model's ability to simulate the user's mental state. Quick check: Does the model infer the client's state from explicit keywords or implied psychological dynamics?

## Architecture Onboarding

- **Component map**: Input Processor -> Perspective Module -> Phase Module -> Response Generator
- **Critical path**: The sequential dependency chain $D_t \rightarrow z_t \rightarrow p_t \rightarrow \text{Response}$. Latency is cumulative across three LLM calls.
- **Design tradeoffs**: 
  - Latency vs. Quality: The 3-step inference significantly increases time-to-first-token compared to direct generation but is required for the performance boost.
  - Windowing: Phase recognition uses only the "recent three turns" to focus on immediate context, potentially missing long-term context shifts.
- **Failure signatures**:
  - Hallucinated Empathy: Perspective-taking output ($z_t$) describes emotions not present in the text, leading to condescending responses.
  - Stage Locking: The model gets stuck in "Rapport Building" despite the client moving to "Problem Solving."
- **First 3 experiments**:
  1. Ablation Validation: Reproduce the "w/o Emo" and "w/o Stage" experiments on a small held-out set to verify the contribution of each module.
  2. Phase Classification Accuracy: Manually label 20 dialogue turns and compare against the model's predicted phase $p_t$ to establish ground-truth alignment.
  3. Cross-Model Portability: Apply the EmoStage prompt structure to a smaller/different base model to test if the mechanism is robust or dependent on specific LLM capabilities.

## Open Questions the Paper Calls Out

- **Open Question 1**: Would combining EmoStage's perspective-taking and phase recognition mechanisms with fine-tuning on counseling data yield additional performance gains over the inference-only approach? The authors emphasize avoiding data collection constraints but do not investigate hybrid approaches.

- **Open Question 2**: How well does EmoStage generalize to languages beyond Japanese and Chinese, particularly low-resource languages where counseling data is scarce? The authors demonstrate cross-lingual capability between two Asian languages but do not evaluate performance in English or truly low-resource languages.

- **Open Question 3**: Does limiting phase recognition to only the most recent three dialogue turns cause the system to miss important long-range context necessary for accurate stage identification? The authors justify this design choice for efficiency but provide no ablation comparing different context window sizes.

- **Open Question 4**: How does EmoStage perform when deployed with larger-scale open-source or commercial LLMs as base models? The paper uses 7B-8B parameter models and notes GPT-4.1 outperforms EmoStage, but does not test whether the framework would improve larger open-source models.

## Limitations

- The framework relies on the base LLM's Theory of Mind capabilities, which may vary significantly across models
- The sequential 3-step inference introduces cumulative latency and potential error propagation from earlier stages
- Effectiveness depends on the assumption that counseling follows a predictable multi-stage progression, which may not hold for all therapeutic approaches

## Confidence

- **High confidence**: The ablation study results showing performance degradation when removing perspective-taking or phase recognition components
- **Medium confidence**: The claim that EmoStage performs competitively with data-driven approaches, given that human evaluation is reported but detailed metrics are limited
- **Medium confidence**: The generalizability to different LLMs, as the framework was primarily tested on Qwen-7B and Llama-3.1-Swallow-8B without systematic cross-model validation

## Next Checks

1. **Cross-Model Robustness Test**: Apply EmoStage to a different base LLM (e.g., Llama-3.2-3B) to verify if the prompt engineering approach transfers across model capabilities and sizes.

2. **Phase Classification Validation**: Manually label 50 dialogue turns from the test set with their true counseling phases and calculate precision/recall against the model's predictions to establish baseline accuracy.

3. **Error Propagation Analysis**: Create controlled test cases where the perspective-taking output is intentionally corrupted, then measure how this affects the final response quality to quantify the vulnerability of the sequential pipeline.