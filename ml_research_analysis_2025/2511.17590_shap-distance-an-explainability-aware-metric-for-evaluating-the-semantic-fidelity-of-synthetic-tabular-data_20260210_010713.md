---
ver: rpa2
title: 'SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic
  Fidelity of Synthetic Tabular Data'
arxiv_id: '2511.17590'
source_url: https://arxiv.org/abs/2511.17590
tags:
- data
- synthetic
- shap
- real
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces the SHAP Distance, a novel explainability-aware
  metric for evaluating the semantic fidelity of synthetic tabular data. Traditional
  evaluation methods like KL divergence and TSTR accuracy focus on distributional
  similarity and predictive performance but fail to assess whether models trained
  on synthetic data follow the same reasoning patterns as those trained on real data.
---

# SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data

## Quick Facts
- **arXiv ID:** 2511.17590
- **Source URL:** https://arxiv.org/abs/2511.17590
- **Reference count:** 37
- **Primary result:** Introduces SHAP Distance to measure semantic fidelity of synthetic tabular data by comparing feature attribution vectors from classifiers trained on real vs. synthetic data.

## Executive Summary
The SHAP Distance metric addresses a critical gap in evaluating synthetic tabular data by measuring semantic fidelity beyond traditional statistical and predictive metrics. While KL divergence captures distributional similarity and TSTR accuracy measures predictive utility, neither reveals whether models trained on synthetic data follow the same reasoning patterns as those trained on real data. The SHAP Distance computes the cosine distance between global SHAP attribution vectors derived from classifiers trained separately on real and synthetic datasets, quantifying alignment in feature importance and decision logic. Experiments on UCI Heart Disease, Enterprise Invoice Usage, and Telco Churn datasets demonstrate that synthetic data can exhibit statistical deviations yet achieve high semantic fidelity, with iterative refinement guided by SHAP Distance yielding the highest downstream utility.

## Method Summary
The method trains identical classifiers on real and synthetic data separately, then extracts global SHAP attribution vectors (mean absolute SHAP values per feature) for each model. The SHAP Distance is calculated as 1 minus the cosine similarity between these attribution vectors, with lower values indicating better semantic alignment. The approach can be integrated into an iterative refinement loop where synthetic data generation is adjusted based on attribution misalignment. The metric captures semantic discrepancies invisible to standard distributional measures, particularly in tail effects and nonlinear dependencies that affect decision logic.

## Key Results
- Synthetic data refined using SHAP Distance achieved the highest downstream utility across all three datasets
- SHAP Distance effectively captured semantic misalignments even when KL divergence indicated good distributional similarity
- High TSTR accuracy did not guarantee semantic fidelity, with some synthetic datasets achieving correct predictions through spurious correlations
- Iterative refinement guided by SHAP Distance consistently improved attribution alignment and downstream performance

## Why This Works (Mechanism)

### Mechanism 1: Attribution-Based Semantic Alignment
If two datasets induce similar global feature attribution profiles in separate classifiers, they likely share similar underlying decision logic. The method trains classifiers on real and synthetic data, extracts global SHAP attribution vectors, and computes cosine distance to quantify angular dissimilarity in feature prioritization. The direction of the global SHAP vector serves as a proxy for the "reasoning process" or semantic structure.

### Mechanism 2: Detection of Distributional "Blind Spots"
Standard metrics like KL divergence may miss semantic shifts where synthetic data underrepresents critical tail effects or nonlinear dependencies required for correct reasoning. SHAP values capture nonlinear feature contributions, identifying cases where synthetic models over-rely on dominant features and under-weight complex interactions, revealing a "flattening" of decision logic.

### Mechanism 3: Attribution-Guided Feedback Loop
SHAP Distance can serve as a loss signal to guide synthetic generators toward correcting specific semantic misalignments. An iterative framework measures SHAP Distance and refines the generation process by emphasizing divergent features until attribution vectors align, improving downstream utility beyond purely statistical refinement.

## Foundational Learning

**Concept: SHapley Additive exPlanations (SHAP)**
- Why needed: This is the core signal quantifying "reasoning" by attributing prediction output portions to each feature, indicating both magnitude and direction of impact.
- Quick check: If a model predicts "High Risk" and Feature A has a SHAP value of +0.5, what does that imply about Feature A's contribution?

**Concept: Train-on-Synthetic-Test-on-Real (TSTR)**
- Why needed: This baseline utility metric measures if a model works but not how it works, failing to reveal spurious correlations.
- Quick check: Why might a model achieve high TSTR accuracy while relying on spurious correlations absent in the real data?

**Concept: Cosine Distance**
- Why needed: The paper uses angular distance (1 - cosine similarity) rather than Euclidean distance to compare attribution vectors, measuring the ratio of feature importances rather than absolute magnitude differences.
- Quick check: Will a uniform scaling of all SHAP values in a vector change the Cosine Distance? (Answer: No).

## Architecture Onboarding

**Component map:**
Preprocessing -> Dual-Model Training -> Explainer Module -> Metric Computation

**Critical path:**
The fidelity of the Base Classifier is paramount. If the classifier used to generate SHAP values is unstable or underfitted, the SHAP vectors will be noisy, rendering the distance metric unreliable. Ensure both real and synthetic classifiers are robust before comparison.

**Design tradeoffs:**
- *Model Agnosticism vs. Speed:* Tree-based models are standard for tabular SHAP but slower deep models are possible
- *Global vs. Local:* The paper uses Global SHAP (average absolute importance), which hides local inconsistencies where synthetic data might be wrong for specific subgroups but right globally

**Failure signatures:**
- High TSTR, High SHAP Distance: "The Right Answer for the Wrong Reason" - synthetic data is semantically misaligned but statistically predictive
- Low KL Divergence, High SHAP Distance: "Statistically Identical, Semantically Broken" - tail dependencies are missing, breaking decision logic despite matching marginal distributions

**First 3 experiments:**
1. **Sanity Check:** Train on a dataset and a copy of itself. Verify D_SHAP ≈ 0.
2. **Noise Injection:** Randomly shuffle labels in the synthetic set. Verify D_SHAP increases significantly (attribution alignment breaks).
3. **Feature Drop:** Remove a high-importance feature from the synthetic generator. Verify D_SHAP captures the specific shift in the attribution vector for that feature.

## Open Questions the Paper Calls Out

**Open Question 1:**
Can the SHAP Distance metric be effectively extended to incorporate causal feature attributions to verify if synthetic data preserves underlying causal structures rather than just statistical associations? The current metric relies on standard SHAP values which capture feature importance based on predictive contribution (correlation) but do not distinguish between correlation and causation.

**Open Question 2:**
Does combining the SHAP Distance with attention mechanisms from transformer-based models provide superior insights into feature interactions in high-dimensional datasets? The current methodology uses global attribution vectors which may flatten complex interaction effects.

**Open Question 3:**
Is the SHAP Distance metric valid and discriminative when applied to temporal or multimodal datasets such as electronic health records or financial transaction streams? The study validates the metric only on static tabular datasets.

**Open Question 4:**
How can semantic fidelity metrics be integrated into a fully automated generative loop to guide the synthesis process dynamically? While an iterative refinement framework is demonstrated, broader automated frameworks where attribution misalignment directly updates model parameters without manual intervention are needed.

## Limitations
- Lacks implementation details for the KGSynX synthetic data generator, making exact reproduction challenging
- Classifier architecture and hyperparameters are unspecified, potentially affecting SHAP attribution stability
- Threshold ε and iteration limits for the refinement loop are not defined, limiting protocol replication

## Confidence
- **High Confidence:** The core mechanism of comparing global SHAP vectors via cosine distance is technically sound and well-defined
- **Medium Confidence:** The empirical demonstration across three datasets shows promise, but results are presented comparatively rather than absolutely
- **Low Confidence:** The iterative refinement framework (KGSynX) lacks sufficient detail for independent validation

## Next Checks
1. Implement a basic synthetic data generator (e.g., CTGAN) and reproduce the SHAP Distance calculation on the UCI Heart Disease dataset
2. Conduct the "noise injection" sanity check by shuffling labels in synthetic data and measuring the SHAP Distance increase
3. Compare SHAP Distance results against KL divergence and TSTR accuracy on a held-out test set to validate the claim that it captures semantic misalignments invisible to standard metrics