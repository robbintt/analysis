---
ver: rpa2
title: What Does Explainable AI Mean in Practice? Evaluative Requirements from a Longitudinal
  Clinical Case Study
arxiv_id: '2501.09592'
source_url: https://arxiv.org/abs/2501.09592
tags:
- requirements
- system
- they
- clinicians
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how explainability requirements are elicited
  during AI system development, focusing on a clinical system predicting cerebral
  palsy risk in infants. The research team found that clinicians did not need technical
  explanations of the AI's inner workings; instead, they wanted to compare the AI's
  predictions with their own assessments.
---

# What Does Explainable AI Mean in Practice? Evaluative Requirements from a Longitudinal Clinical Case Study

## Quick Facts
- arXiv ID: 2501.09592
- Source URL: https://arxiv.org/abs/2501.09592
- Authors: Tor Sporsem; Stine Rasdal Finserås; Lars Adde; Inga Strümke
- Reference count: 40
- One-line primary result: Clinicians trusted AI predictions when they could evaluate them against their own assessments, not when given technical explanations.

## Executive Summary
This study investigated how explainability requirements are elicited during AI system development, focusing on a clinical system predicting cerebral palsy risk in infants. The research team found that clinicians did not need technical explanations of the AI's inner workings; instead, they wanted to compare the AI's predictions with their own assessments. This led to the introduction of "Evaluative Requirements" - system requirements that allow users to scrutinize AI outputs and compare them with their own expertise. The key method was iterative prototyping and observation, where developers tested prototypes with clinicians and observed their reactions. The primary result was a simple prediction graph showing 5-second windows of CP risk, which proved effective for building trust. Clinicians used this to validate predictions against their clinical judgment, even when disagreeing with the AI. This approach contrasts with traditional XAI methods and demonstrates that established software engineering practices like iterative development and observation can effectively elicit explainability requirements in AI system development.

## Method Summary
The study employed iterative prototyping and observation to elicit explainability requirements for an AI system predicting cerebral palsy risk from infant movement videos. The development team built progressively more advanced prototypes, starting with a basic prediction interface, and observed how clinicians interacted with them. Rather than asking directly what explanations they needed, the team deployed minimal viable products and watched for spontaneous questions and behaviors. Through multiple cycles of testing and refinement, they discovered that clinicians wanted to evaluate AI predictions against their own assessments rather than understand technical mechanisms. The final interface displayed CP risk scores in 5-second windows synchronized with video playback, allowing clinicians to compare AI outputs with their clinical judgment.

## Key Results
- Clinicians trusted AI predictions when they could evaluate them against their own assessments, not when given technical explanations
- Iterative prototyping and observation revealed that users cannot articulate explainability needs before hands-on interaction with an AI system
- Observation of expert decision-making revealed requirements that interviews missed, including the importance of time-windowed predictions aligned with video segments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Users cannot articulate explainability needs before hands-on interaction with an AI system.
- **Mechanism:** Direct inquiry (asking "what explanations do you need?") fails because users lack the conceptual vocabulary to describe needs for systems they haven't experienced. Providing an MVP creates concrete interaction scenarios that provoke spontaneous questions, revealing actual requirements through observed behavior rather than stated preferences.
- **Core assumption:** Tacit knowledge (expertise that is enacted rather than verbalized) underlies user needs in expert domains.
- **Evidence anchors:**
  - [abstract]: "requirements are best discovered through the well-known methods of iterative prototyping and observation"
  - [section 4.1]: Clinicians "struggled to conceptualize AI in an abstract setting" and "were not able to articulate that" when asked directly.
  - [corpus]: Related work on explainability requirements remains fragmented; corpus papers emphasize context-specific design but lack comparative elicitation methods.
- **Break Condition:** If users have extensive prior AI experience or the domain has highly standardized decision protocols, direct inquiry may succeed where this study showed failure.

### Mechanism 2
- **Claim:** Trust emerges from enabling users to *evaluate* AI outputs against their own expertise, not from explaining internal mechanisms.
- **Mechanism:** Users in high-stakes domains seek to maintain decision authority. When systems provide structured evidence (e.g., time-windowed predictions) that users can compare against their own assessments, they integrate AI outputs into existing judgment workflows. This supports abductive reasoning—testing and revising hypotheses—rather than passive acceptance of recommendations.
- **Core assumption:** Users prioritize accountability and control over understanding technical details in high-stakes decisions.
- **Evidence anchors:**
  - [abstract]: "clinicians trusted it when it enabled them to evaluate predictions against their own assessments"
  - [section 4.2]: Clinician quote: "Now I can compare what I see with what the model sees" — trust emerged through comparison, not technical explanation.
  - [corpus]: "Before the Clinic" paper similarly emphasizes transparent, operable design principles aligned with clinical workflows.
- **Break Condition:** In low-stakes or high-volume contexts where efficiency outweighs accountability, users may prefer direct recommendations with minimal evaluative overhead.

### Mechanism 3
- **Claim:** Observation of expert decision-making reveals requirements that interviews miss.
- **Mechanism:** Experts rely on non-verbal cues, repeated interactions with data, and tacit pattern recognition. Ethnographic observation captures these enacted practices (e.g., clinicians replaying video segments, using non-verbal confirmation), which are then translated into system features (e.g., 5-second prediction windows synchronized with video).
- **Core assumption:** Expert workflows contain implicit evaluative structures that can be mirrored in AI interfaces.
- **Evidence anchors:**
  - [section 4.2]: Observation of clinicians disagreeing about CP signs—watching 10-second snippets, replaying, pointing, nodding—revealed how they build consensus without verbalizing criteria.
  - [section 5.1]: "Only through direct observation... did the development team gain the necessary understanding"
  - [corpus]: Corpus papers lack direct studies of observational methods for requirements elicitation in AI contexts.
- **Break Condition:** If observation is impractical (privacy constraints, distributed users, time/cost limits), proxy methods such as retrospective walkthroughs or simulated tasks may partially substitute.

## Foundational Learning

- **Concept: Evaluative AI (vs. Explainable AI)**
  - **Why needed here:** The paper applies Miller's Evaluative AI framework—systems provide evidence for/against hypotheses rather than recommending and defending a single answer. This reframes explainability from "how does it work?" to "can I judge this output?"
  - **Quick check question:** Does your system design push users toward accepting/rejecting a recommendation, or does it support exploring evidence for multiple interpretations?

- **Concept: Iterative Requirements Elicitation**
  - **Why needed here:** The study demonstrates that requirements emerge incrementally through MVP testing and observation, not from upfront specification. This aligns with established SE practices (spiral model, agile) but is underused in AI development.
  - **Quick check question:** Have you planned for multiple prototype cycles with real users before finalizing explainability features?

- **Concept: Tacit Knowledge in Expert Domains**
  - **Why needed here:** Expert users often cannot verbalize decision criteria. Understanding Polanyi's tacit knowledge concept helps explain why direct questioning fails and why observation is essential.
  - **Quick check question:** Are your users experts whose knowledge is implicit? If so, have you budgeted for observational or ethnographic methods?

## Architecture Onboarding

- **Component map:** AI Model -> Prediction Interface -> Evidence Alignment Layer -> Observation Pipeline
- **Critical path:**
  1. Deploy minimal prediction interface (no explanations)
  2. Observe users' spontaneous questions and comparison behaviors
  3. Iterate on evidence presentation (time windows, highlighting, synchronized views)
  4. Validate through disagreement scenarios (AI vs. user assessment)

- **Design tradeoffs:**
  - Evaluation depth vs. workflow speed: More evaluative features slow decisions but build trust; may conflict with productivity goals.
  - Observation cost vs. requirement accuracy: Ethnographic methods are resource-intensive but reduce risk of building unused features.
  - Transparency illusion: Providing evaluative features may create false confidence that users understand the system; they may over-trust or misattribute reasoning.

- **Failure signatures:**
  - Users ignore or uniformly accept AI outputs without questioning → evaluative features not integrated into workflow
  - Users request technical explanations (feature importance, model internals) → domain may not fit Evaluative AI pattern (consider hybrid approach)
  - Observation reveals no spontaneous comparison behavior → users may lack expertise or system outputs not perceived as relevant

- **First 3 experiments:**
  1. **MVP Probe Test:** Deploy prediction-only interface with representative users; log spontaneous questions and interaction patterns.
  2. **Evidence Synchronization Test:** Add time-windowed outputs aligned with raw data; measure whether users engage in comparison behaviors.
  3. **Disagreement Scenario Test:** Present cases where AI predictions conflict with user assessments; observe whether evaluative features support reasoned rejection or re-evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does a Minimum Viable Product (MVP) for an AI system require a fully trained model to elicit valid requirements, or can mocked-up versions suffice?
- **Basis in paper:** [explicit] The authors ask, "Does an MVP require a fully trained AI system to generate realistic outputs, or can simplified or mocked-up versions provide sufficient value for eliciting requirements?"
- **Why unresolved:** The case study utilized a fully functional model, so the efficacy of low-fidelity prototypes for this specific type of requirement remains untested.
- **What evidence would resolve it:** Comparative studies measuring the quality of requirements elicited using fully trained models versus simulated prototypes.

### Open Question 2
- **Question:** How do Evaluative Requirements change when the end-users are domain novices rather than experts?
- **Basis in paper:** [explicit] The authors state, "This raises the question of how Evaluative Requirements differ when the users are novices rather than experts."
- **Why unresolved:** The study focused exclusively on expert clinicians who possessed the deep tacit knowledge necessary to evaluate AI outputs against their own judgment.
- **What evidence would resolve it:** Empirical studies testing "Evaluative AI" interfaces with user groups of varying expertise levels to see if evaluation is possible without domain knowledge.

### Open Question 3
- **Question:** Do Evaluative Requirements transfer effectively to low-stakes contexts or other expert domains like law and finance?
- **Basis in paper:** [explicit] The authors propose that "Future research should investigate whether the findings transfer to other expert domains such as law, finance, or critical infrastructure, as well as to low-stakes contexts."
- **Why unresolved:** The findings are situated in a high-stakes medical context where accountability is paramount; it is unclear if users in lower-stakes settings would tolerate the extra cognitive effort required.
- **What evidence would resolve it:** Replication of the longitudinal case study method in diverse industry sectors to observe if the need for "evaluation over explanation" persists.

## Limitations

- The study's findings are based on a single clinical domain (CP prediction) with a specific user group (clinicians), limiting generalizability to other medical specialties or AI applications.
- The iterative prototyping approach requires significant time and resources, which may not be feasible for all development teams.
- The paper does not address how to scale observational methods for larger or distributed user bases.

## Confidence

- **High Confidence:** The core finding that direct inquiry fails to elicit explainability requirements in expert domains is well-supported by multiple observations and consistent with established theories of tacit knowledge.
- **Medium Confidence:** The claim that Evaluative Requirements are more effective than traditional XAI methods is supported by this single case study but requires validation across diverse contexts.
- **Medium Confidence:** The mechanism linking observation to requirement discovery is theoretically sound but lacks comparative studies against alternative elicitation methods.

## Next Checks

1. **Cross-Domain Replication:** Apply the iterative prototyping and observation methodology to a different medical specialty (e.g., radiology) to test if Evaluative Requirements emerge similarly.
2. **Comparative Elicitation Study:** Compare the observation-based approach against structured interviews and surveys in the same domain to quantify differences in requirement quality and completeness.
3. **Resource Impact Analysis:** Conduct a cost-benefit analysis of observational requirements elicitation versus traditional methods, measuring development time, resource allocation, and requirement accuracy across multiple projects.