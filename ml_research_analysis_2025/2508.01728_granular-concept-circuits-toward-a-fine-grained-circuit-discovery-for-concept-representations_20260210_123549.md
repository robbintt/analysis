---
ver: rpa2
title: 'Granular Concept Circuits: Toward a Fine-Grained Circuit Discovery for Concept
  Representations'
arxiv_id: '2508.01728'
source_url: https://arxiv.org/abs/2508.01728
tags:
- concept
- circuit
- concepts
- circuits
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Granular Concept Circuits (GCC), a novel method
  to discover concept-specific circuits in deep vision models. The approach iteratively
  identifies neurons across layers that are both functionally connected (via Neuron
  Sensitivity Score) and semantically aligned (via Semantic Flow Score), constructing
  circuits that represent specific concepts within a query.
---

# Granular Concept Circuits: Toward a Fine-Grained Circuit Discovery for Concept Representations

## Quick Facts
- arXiv ID: 2508.01728
- Source URL: https://arxiv.org/abs/2508.01728
- Reference count: 40
- Key outcome: GCC discovers fine-grained concept-specific circuits across CNN and transformer models, achieving 8.67 percentage point logit drops when ablated, significantly outperforming random pruning.

## Executive Summary
This paper introduces Granular Concept Circuits (GCC), a novel method for discovering fine-grained concept-specific circuits in deep vision models. Unlike previous approaches that identify single unified circuits, GCC iteratively identifies neurons across layers that are both functionally connected (via Neuron Sensitivity Score) and semantically aligned (via Semantic Flow Score). The method constructs circuits that represent specific concepts within a query, enabling discovery of shared concepts across different classes and providing interpretable explanations for model behavior.

## Method Summary
GCC identifies concept-specific circuits by iteratively linking neurons across layers based on two scores: Neuron Sensitivity Score (S_NS) measuring functional connectivity through ablation, and Semantic Flow Score (S_SF) ensuring semantic alignment through sample overlap. The method uses Peak-over-Threshold (POT) for automated thresholding and builds circuits from root nodes selected as top 1% activated neurons. The approach applies to various architectures including CNNs (ResNet50, VGG19) and transformers (ViT, Swin), with circuits validated through ablation studies showing significant logit drops when removed.

## Key Results
- GCC-identified circuits cause an 8.67 percentage point average logit drop when ablated, significantly higher than random pruning (2.35pp) or non-circuit neurons (1.74pp)
- User studies confirm GCCs are relevant (3.65/5), diverse (4.0/5), and prototypical (4.45/5)
- Successfully identifies shared concepts like "radiant" across disparate classes and aids in auditing misclassifications
- Generalizes across CNNs and transformers, providing first-of-its-kind fine-grained circuit discovery approach

## Why This Works (Mechanism)

### Mechanism 1: Perturbation-Based Sensitivity Mapping
The method computes Neuron Sensitivity Score ($S_{NS}$) by zero-masking a source neuron and measuring activation drop in downstream layers. This intervention-based approach captures both direct and indirect dependencies, distinguishing critical functional paths from redundant ones. The assumption is that neural pathways critical for specific concepts will show significant activation degradation when interrupted.

### Mechanism 2: Semantic Consistency Filtering
High functional sensitivity alone is insufficient due to non-linearities creating spurious dependencies between semantically unrelated neurons. The Semantic Flow Score ($S_{SF}$) quantifies alignment by calculating overlap of highly activated samples between source and target neurons. Connections are retained only if both neurons fire consistently on the same input subset, assuming neurons encoding the same visual concept will activate maximally for the same images.

### Mechanism 3: Statistical Thresholding via Extreme Value Theory
The method applies Peak-over-Threshold (POT) from Extreme Value Theory to sensitivity score distributions, dynamically setting thresholds without manual tuning. POT models the tail distribution to isolate statistical outliers representing true circuit connections. This assumes the distribution of connection strengths has a heavy tail where genuine circuit connections reside.

## Foundational Learning

- **Intervention-based Causality (Ablation):** Critical because GCC defines importance by degradation upon removal rather than signal magnitude. Quick check: If a neuron has high activation but zero $S_{NS}$, is it included in the circuit? (Answer: No).

- **Distributed Representations:** GCC argues against single-neuron interpretability, defining concepts as graphs of neurons across multiple layers. Quick check: Does the method identify a single neuron as the concept "dog"? (Answer: No, it identifies a subgraph/circuit).

- **Polysemanticism:** Understanding that neurons might respond to multiple unrelated concepts helps interpret why semantic thresholds are necessary but potentially imperfect. Quick check: Why might high $S_{SF}$ be difficult for a neuron detecting both "red" and "cars"? (Answer: Top-activated samples might be a mix of red things and cars, reducing overlap with pure "car" or "red" neurons).

## Architecture Onboarding

- **Component map:** Root Extractor -> Sensitivity Engine -> Semantic Filter -> Threshold Optimizer -> Circuit Aggregator
- **Critical path:** The iterative backward pass from Root Nodes → Sensitivity Engine → Semantic Filter determines circuit quality. If Semantic Filter is too loose, circuit explodes; if too tight, it disconnects.
- **Design tradeoffs:** Granularity vs. Stability (strict thresholds may fragment concepts), Computational Cost (calculating $S_{NS}$ requires intervention/masking)
- **Failure signatures:** Disconnected Components (thresholds too strict), Generic Circuits (root nodes picked generic features), Logit Drop Mismatch (circuit complete but not faithful)
- **First 3 experiments:** 1) Faithfulness Test: Verify ~8.67pp logit drop on ResNet50 ablation, 2) Qualitative Inspection: Visualize Sankey diagram for misclassified example to confirm semantic alignment, 3) Threshold Sensitivity: Vary POT threshold and observe trade-off between circuit size and faithfulness

## Open Questions the Paper Calls Out
- How can circuit discovery methods prevent a single semantic concept from fragmenting across multiple distinct pathways under strict connectivity thresholds?
- Does utilizing higher-order neuron interactions improve faithfulness compared to first-order approximations?
- Can connectivity scoring be refined to guarantee model-centric strong connections are also human-interpretable?

## Limitations
- Performance critically depends on underspecified parameters (exact k value for semantic flow, POT implementation details)
- Method's statistical assumptions about tail distributions may not hold for all architectures
- High functional dependency doesn't always guarantee semantic alignment understandable to humans

## Confidence
- **High Confidence:** Intervention-based sensitivity scoring mechanism is well-established and implementation details are sufficient
- **Medium Confidence:** Semantic flow score approach is conceptually sound but relies on assumptions about neuron behavior
- **Low Confidence:** Statistical thresholding via POT lacks implementation specifics, making reproducibility uncertain

## Next Checks
1. **Parameter Sensitivity Analysis:** Systematically vary k (1, 5, 10, 20) in semantic flow calculation and measure impact on circuit size, faithfulness scores, and semantic coherence
2. **Cross-Architecture Generalization:** Apply method to transformer architecture (e.g., ViT) and compare whether same POT thresholding approach works or requires adjustment
3. **Polysemantic Neuron Stress Test:** Construct synthetic dataset with intentionally polysemantic neurons and evaluate whether S_SF successfully filters spurious connections or produces incoherent circuits