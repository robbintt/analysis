---
ver: rpa2
title: On a Connection Between Imitation Learning and RLHF
arxiv_id: '2503.05079'
source_url: https://arxiv.org/abs/2503.05079
tags:
- learning
- chosen
- arxiv
- imitation
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work establishes a theoretical connection between reinforcement
  learning from human feedback (RLHF) and imitation learning, showing that RLHF implicitly
  performs imitation learning on the preference data distribution. Based on this insight,
  the authors propose DIL, a direct imitation learning framework that bypasses the
  need for reward modeling by directly optimizing a density ratio estimation objective.
---

# On a Connection Between Imitation Learning and RLHF
## Quick Facts
- arXiv ID: 2503.05079
- Source URL: https://arxiv.org/abs/2503.05079
- Reference count: 25
- Key outcome: DIL outperforms DPO, SLiC, and SimPO on reasoning and alignment benchmarks by directly optimizing density ratio estimation

## Executive Summary
This work establishes a theoretical connection between reinforcement learning from human feedback (RLHF) and imitation learning, showing that RLHF implicitly performs imitation learning on the preference data distribution. Based on this insight, the authors propose DIL, a direct imitation learning framework that bypasses the need for reward modeling by directly optimizing a density ratio estimation objective. DIL unifies imitation learning on preference data and bridges the gap between density ratio estimation and preference alignment. Experiments demonstrate that DIL outperforms existing alignment methods like DPO, SLiC, and SimPO on various benchmarks including MMLU-PRO, BBH, MUSR, MATH, GSM8K, and ARC, with notable improvements in reasoning-heavy tasks. The method also shows superior alignment with human preferences in summarization and dialogue generation tasks, achieving win rates exceeding 60% against chosen responses.

## Method Summary
DIL (Direct Imitation Learning) is a preference optimization method that directly estimates the density ratio between chosen and rejected responses without learning an intermediate reward model. The approach uses Bregman divergence with h-functions (LSIF, BCE, UKL) to optimize the policy via a closed-form optimal solution. DIL employs reverse KL divergence, which promotes mode-seeking behavior suitable for alignment tasks. The method requires only a single training epoch, uses the SFT model as reference, and normalizes likelihoods per-token to avoid length bias. Experiments use UltraFeedback Binarized (64k prompts), Anthropic-HH (170k dialogues), and Reddit TL;DR datasets, with evaluation on MMLU-PRO, BBH, MUSR, MATH, GSM8K, ARC, and human preference studies.

## Key Results
- DIL achieves 74.0% win rate on MMLU-PRO vs DPO's 66.8%
- Outperforms baselines on reasoning tasks: BBH (+6.5%), MUSR (+1.7%), MATH (+1.4%), GSM8K (+0.9%)
- Maintains reasoning capability while improving alignment, unlike other methods that cause reasoning performance collapse
- Superior human preference alignment: 60.4% win rate on TL;DR summarization, 63.8% on dialogue generation

## Why This Works (Mechanism)

### Mechanism 1: RLHF Equivalence to Imitation Learning on Chosen Responses
RLHF's reward learning step is mathematically equivalent to behavior cloning on the preferred response distribution using energy-based models. When πref samples from the preference pair (yw, yl) with equal probability, the sample-based approximation of the forward KL objective over EBMs reduces exactly to the Bradley-Terry reward loss. This reveals that RLHF implicitly performs imitation learning on the preference data distribution, and the BT preference structure emerges naturally from this sampling choice rather than being a fundamental requirement.

### Mechanism 2: Mode-Seeking via Reverse KL Enables Better Alignment
DIL's use of reverse KL divergence produces mode-seeking behavior that concentrates probability mass on high-reward regions, which is more suitable for alignment than SFT's mass-covering behavior. Forward KL (used in SFT) penalizes the learner for assigning zero probability where the data has mass, encouraging coverage of all responses including long-tail. Reverse KL penalizes the learner for assigning mass where the data has none, encouraging focus on dominant modes. This makes reverse KL more appropriate for alignment goals that prioritize generating specific high-reward responses rather than covering the full distribution.

### Mechanism 3: Direct Density Ratio Estimation Bypasses Reward Modeling
The optimal policy can be extracted directly by parameterizing the density ratio as log(πθ/πref), eliminating the need for explicit reward function learning. Using Bregman divergence with h-functions (LSIF, BCE, UKL) provides a family of density ratio estimation objectives. The closed-form optimal policy π*(y|x) = (1/Z)πref(y|x)exp(log r*(x,y)) is self-normalized when r* = πchosen/πref. This approach is more robust than BT-based methods and doesn't require hyperparameter tuning.

## Foundational Learning

- Concept: **Forward vs Reverse KL Divergence**
  - Why needed here: Understanding why SFT (forward KL) produces different behavior than RLHF/DIL (reverse KL) is essential for selecting the right objective for alignment tasks.
  - Quick check question: Given a target distribution with multiple modes, which KL direction will produce a policy that covers all modes versus focusing on the largest mode?

- Concept: **Bregman Divergence for Density Ratio Estimation**
  - Why needed here: DIL generalizes density ratio estimation using Bregman divergence, enabling different h-functions (LSIF, BCE, UKL) that produce different training dynamics.
  - Quick check question: What property of Bregman divergence makes it suitable for directly estimating density ratios without computing individual densities?

- Concept: **Bradley-Terry Preference Model Limitations**
  - Why needed here: The paper argues DPO's reliance on BT assumptions causes overfitting; understanding these limitations motivates the density ratio approach.
  - Quick check question: Under what conditions would the BT preference assumption (preference = sigmoid of reward difference) fail to capture true human preferences?

## Architecture Onboarding

- Component map: SFT model -> DIL loss computation -> Policy optimization
- Critical path:
  1. Load SFT model as reference πref
  2. Compute log-likelihoods for chosen and rejected responses under both πref and πθ
  3. Apply DIL loss: L(θ;D) = E[-πθ(yw|x)/πref(yw|x) + 0.5(πθ(yl|x)/πref(yl|x))²] for LSIF variant
  4. Normalize response likelihoods (average log-prob per token) to avoid length bias
  5. Single-epoch training with cosine LR schedule

- Design tradeoffs:
  - **LSIF vs BCE vs UKL**: LSIF is default; UKL performs best on BBH reasoning; BCE improves MUSR. No universal winner—task-dependent.
  - **No hyperparameters**: DIL claims no β-tuning needed (unlike DPO), but this also removes a regularization knob.
  - **Rejected-only vs both responses**: Paper uses rejected responses to approximate πref expectations; could use both but empirically rejected-only works well.

- Failure signatures:
  - **Chosen likelihood collapse**: If chosen response log-prob drops during training (Figure 2 shows DPO/SimPO exhibit this), reasoning performance degrades
  - **Overfitting to preference noise**: Without BT regularization, noisy labels may cause unstable training
  - **Length bias**: Must normalize likelihoods per-token to prevent preference for shorter responses

- First 3 experiments:
  1. **Baseline comparison on single benchmark**: Implement DIL-LSIF on Mistral-7B with UltraFeedback, evaluate on GSM8K and MATH to verify reasoning preservation
  2. **Ablation of h-functions**: Compare LSIF, BCE, UKL variants on BBH and MUSR to identify task-specific best choices
  3. **Training dynamics monitoring**: Plot chosen/rejected likelihoods and margins during training to verify DIL prevents chosen likelihood collapse (replicate Figure 2 pattern)

## Open Questions the Paper Calls Out
None

## Limitations
- The density ratio estimation approach introduces numerical stability challenges with exponential terms that can lead to overflow/underflow issues
- The paper's experimental scope focuses primarily on single-epoch training without exploring multi-epoch convergence properties or effects of dataset size scaling
- Limited exploration of different h-Bregman variants across diverse task families, leaving uncertainty about when each variant is most appropriate

## Confidence
**High Confidence:** The mathematical derivation connecting RLHF's reward learning step to imitation learning objectives is sound and well-established in the literature. The empirical demonstration that DIL preserves reasoning performance while improving alignment metrics is also highly confident.

**Medium Confidence:** The claim that reverse KL promotes better alignment than forward KL for preference alignment tasks has strong theoretical backing but requires more extensive ablation studies across diverse domains to fully validate.

**Low Confidence:** The paper's assertion that DIL eliminates the need for hyperparameter tuning (specifically the β parameter in DPO) is supported by the experiments but hasn't been tested across diverse model architectures or training scales.

## Next Checks
1. **Numerical Stability Stress Test:** Implement gradient clipping and log-space computation for the density ratio terms, then systematically evaluate model performance degradation as sequence length increases from 128 to 2048 tokens. Measure both training stability (NaN/Inf occurrences) and downstream task performance to quantify the practical limits of the current formulation.

2. **Preference Noise Robustness Evaluation:** Introduce controlled levels of preference label noise (0%, 10%, 25%, 50%) into the UltraFeedback dataset and evaluate DIL's performance degradation compared to DPO and SimPO. This would validate the paper's implicit claim that density ratio estimation is more robust to noisy preference data than BT-based approaches.

3. **Multi-epoch Convergence Analysis:** Extend DIL training beyond the single epoch used in the paper to 3, 5, and 10 epochs, monitoring both alignment metrics (win rates) and reasoning preservation (MATH, GSM8K scores). This would validate whether the single-epoch training choice is optimal or merely sufficient, and identify any overfitting patterns that emerge with extended training.