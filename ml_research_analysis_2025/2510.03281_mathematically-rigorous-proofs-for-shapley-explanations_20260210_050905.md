---
ver: rpa2
title: Mathematically rigorous proofs for Shapley explanations
arxiv_id: '2510.03281'
source_url: https://arxiv.org/abs/2510.03281
tags:
- will
- have
- symmetry
- values
- shapley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis provides mathematically rigorous proofs for two key
  results from Lundberg and Lee's 2017 paper on Shapley explanations. The first result
  proves that Shapley values are the unique explanation satisfying local accuracy,
  missingness, restricted symmetry, and restricted consistency - correcting Lundberg
  and Lee's claim that consistency implies symmetry by providing a counterexample.
---

# Mathematically rigorous proofs for Shapley explanations
## Quick Facts
- arXiv ID: 2510.03281
- Source URL: https://arxiv.org/abs/2510.03281
- Reference count: 0
- Primary result: Provides mathematically rigorous proofs for two key results from Lundberg and Lee's 2017 paper on Shapley explanations

## Executive Summary
This thesis addresses critical gaps in the theoretical foundations of Shapley explanations by providing mathematically rigorous proofs for two key results from Lundberg and Lee's 2017 paper. The work corrects an inconsistency in the original consistency property, reformulates the fundamental axioms to eliminate contradictions, and establishes the uniqueness of Shapley values through both game-theoretic and regression-based formulations. The regression formulation enables practical approximation methods that address the exponential computational complexity of original Shapley value calculations.

## Method Summary
The thesis employs a systematic approach to validate and extend the theoretical foundations of Shapley explanations. It begins by identifying inconsistencies in Lundberg and Lee's original work, particularly in the consistency property which the authors claimed implied symmetry but was disproven through a counterexample. The work then reformulates the key properties - local accuracy, missingness, restricted symmetry, and restricted consistency - to create a consistent axiomatic framework. The main proofs establish the uniqueness of Shapley values through correspondence between machine learning models and cooperative games, and demonstrate that Shapley values solve a weighted linear regression problem. The regression formulation enables practical computation through optimization techniques.

## Key Results
- Proves Shapley values are the unique explanation satisfying local accuracy, missingness, restricted symmetry, and restricted consistency
- Shows Shapley values are the unique solution to a weighted linear regression problem
- Demonstrates correspondence between machine learning models and cooperative games

## Why This Works (Mechanism)
The theoretical framework works by establishing a rigorous mathematical foundation that connects cooperative game theory with machine learning interpretability. The uniqueness proofs rely on demonstrating that Shapley values are the only solution satisfying the carefully reformulated axiomatic properties. The regression formulation provides a computational pathway by transforming the explanation problem into an optimization task, where the optimal solution corresponds to Shapley values.

## Foundational Learning
- Cooperative game theory fundamentals: Understanding how characteristic functions and value distributions apply to feature attribution
- Why needed: Provides the theoretical basis for interpreting machine learning models as cooperative games
- Quick check: Verify understanding of how feature coalitions map to model predictions

- Axiomatic properties of feature attribution: Local accuracy, missingness, symmetry, and consistency
- Why needed: Establishes the mathematical requirements that any valid explanation method must satisfy
- Quick check: Confirm each property's meaning and implications for attribution methods

- Linear regression optimization: Weighted least squares formulation
- Why needed: Enables efficient computation of Shapley values through convex optimization
- Quick check: Validate understanding of how the regression formulation relates to feature attribution

## Architecture Onboarding
Component map: ML model -> Feature coalitions -> Characteristic function -> Shapley values -> Explanation
Critical path: Model predictions → Coalition value computation → Shapley value calculation → Feature attribution
Design tradeoffs: Theoretical rigor vs. computational efficiency, exact vs. approximate methods
Failure signatures: Inconsistent attributions, computational intractability, violated axioms
First experiments: 1) Verify restricted consistency counterexample, 2) Test regression formulation on simple models, 3) Compare exact vs. approximate Shapley calculations

## Open Questions the Paper Calls Out
None

## Limitations
- Practical performance of regression-based approximation methods requires empirical validation
- Connection between theoretical results and real-world implementation needs further exploration
- Scalability of the proposed methods to complex model architectures remains unproven

## Confidence
- Corrected restricted consistency property: High (supported by concrete counterexample)
- Uniqueness proofs: High (mathematically sound with minor technical details needing scrutiny)
- Regression formulation practical implications: Medium (theoretical contribution strong, empirical validation needed)

## Next Checks
1. Empirical validation of the regression-based approximation method on diverse machine learning models and datasets to verify computational efficiency claims
2. Extension of the theoretical framework to handle more complex model architectures and feature types not explicitly covered in the current proofs
3. Independent verification of the corrected restricted consistency property through additional counterexamples and edge cases to ensure robustness of the reformulation