---
ver: rpa2
title: 'LLMs that Understand Processes: Instruction-tuning for Semantics-Aware Process
  Mining'
arxiv_id: '2508.16270'
source_url: https://arxiv.org/abs/2508.16270
tags:
- process
- tasks
- task
- mining
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates instruction-tuning for semantics-aware process
  mining using LLMs. The core idea is to expose an LLM to prompt-answer pairs for
  multiple process mining tasks (anomaly detection, next-activity prediction, process
  discovery) to improve its generalization capabilities across unseen tasks.
---

# LLMs that Understand Processes: Instruction-tuning for Semantics-Aware Process Mining

## Quick Facts
- **arXiv ID:** 2508.16270
- **Source URL:** https://arxiv.org/abs/2508.16270
- **Reference count:** 29
- **Primary result:** Instruction-tuning LLMs improves generalization across process mining tasks, with notable gains in discovery and prediction tasks while showing mixed results on anomaly detection.

## Executive Summary
This paper investigates instruction-tuning for semantics-aware process mining using large language models (LLMs). The core idea is to expose an LLM to prompt-answer pairs for multiple process mining tasks (anomaly detection, next-activity prediction, process discovery) to improve its generalization capabilities across unseen tasks. The study finds that instruction-tuning leads to considerable improvements in process discovery and prediction tasks, with fitness scores improving from 0.63 to 0.71 for directly-follows graph discovery and F1 scores increasing from 0.53 to 0.65 for next-activity prediction. However, performance varies across models on anomaly detection tasks, with some models showing decreased accuracy. The results demonstrate that a single, instruction-tuned LLM can generalize across multiple semantics-aware process mining tasks, offering a more flexible alternative to task-specific fine-tuning.

## Method Summary
The authors instruction-tune Llama 3 70B and Mistral Large 2 models using a LoRA adapter (r=16, α=16) with 4-bit quantization. They create an instruction dataset from SAP-SAM collection (15,000+ process models) with 6 prompt variations per task and apply "negative inversions" to increase robustness. Using leave-one-group-out training, they train on 2 task groups (e.g., Anomaly + Prediction) and validate on the held-out group (e.g., Discovery) using task-specific metrics. The models are trained for 3 epochs with learning rate 1e-5, batch size 32 effective (8 × 4 gradient accumulation), and AdamW optimizer with weight decay 0.01.

## Key Results
- Instruction-tuning improves discovery task performance: fitness scores increase from 0.63 to 0.71 for directly-follows graph discovery (S-DFD)
- Next-activity prediction F1 scores improve from 0.53 to 0.65 (S-NAP)
- Models show improved format adherence and reduced hallucination in generative tasks
- Performance varies across models on anomaly detection tasks, with some showing decreased accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Instruction-tuning enables zero-shot generalization to unseen process mining tasks by aligning internal model representations with the semantic intent of natural language instructions.
- **Mechanism:** Training on a mixture of process mining tasks described via natural language instructions teaches the model to map "process intent" to behavioral logic, allowing it to infer solutions for held-out tasks by recognizing structural similarities in instructions.
- **Core assumption:** Semantic understanding required for one process task shares underlying behavioral representations with another.
- **Evidence anchors:** [abstract] "The idea of instruction-tuning here is to expose an LLM to prompt-answer pairs for different tasks... making it more familiar with process mining, thus allowing it to also perform better at unseen tasks." [section IV-A] "This enables LLMs to generalize across tasks, by aligning their internal representations with the meaning of the instructions, rather than solely relying on patterns seen in task-specific fine-tuning."
- **Break condition:** Generalization fails if the model overfits to training instruction phrasing or develops prediction bias (e.g., Llama's bias toward "True" labels in anomaly detection).

### Mechanism 2
- **Claim:** Multi-task instruction-tuning improves structural reasoning capabilities more reliably than classification tasks, as diverse prompt formats force holistic representation of process control-flow.
- **Mechanism:** Variability in instruction formulations (6 per task) and inclusion of "negative inversions" force the model to learn "process validity" rather than simple keyword association, particularly effective for generating complex structures like process trees.
- **Core assumption:** Exposure to diverse perspectives of a process creates more robust internal representation than seeing only one perspective.
- **Evidence anchors:** [section IV-B] "To increase the diversity and robustness of the resulting instruction dataset... We establish six formulations for each task... [and] generate reversed versions of a given task by flipping its objective." [section VI-B] "Instruction-tuned models demonstrate a more developed grasp of higher-level process representations... leading to more semantically sound process trees."
- **Break condition:** If training tasks lack diversity or structural similarity to test task, knowledge transfer fails (e.g., domain complexity in Healthcare/IT remains a challenge).

### Mechanism 3
- **Claim:** Instruction-tuning reduces syntactic hallucination and format errors in generative process mining by conditioning the model on strict output requirements.
- **Mechanism:** Explicit format constraints (e.g., "Answer should be one activity...") act as strong regularizers, forcing the model to prioritize required output schema over open-ended generation.
- **Core assumption:** Model has sufficient capacity to separate "formatting" rules from "reasoning" content.
- **Evidence anchors:** [section VI-B] "We find that instruction-tuned models consistently show improved adherence to specified instruction formats... largely avoiding malformed or incomplete outputs that are common with base models." [section VI-A] "Llama IT generated concise next-activity predictions, a considerable improvement over the base model's tendency to generate boilerplate text."
- **Break condition:** If instructions for format and logic conflict, or if model is too small, it may generate well-formatted but logically incorrect outputs.

## Foundational Learning

- **Concept: Semantics-Aware Process Mining**
  - **Why needed here:** The paper explicitly distinguishes this from traditional "frequency-based" mining. You must understand that the model is reasoning about the *meaning* of activity labels (text) to determine valid behavior (e.g., "approve" should follow "review"), not just counting transitions in a log.
  - **Quick check question:** Can you explain why a process might be "valid" semantically but "infrequent" in an event log?

- **Concept: Instruction-Tuning vs. Fine-Tuning (FT)**
  - **Why needed here:** This is the core methodological contribution. You need to grasp the trade-off: FT maximizes performance on a single task (specialist), while IT sacrifices some peak performance for the ability to handle multiple/unseen tasks (generalist).
  - **Quick check question:** If you need to solve a completely new process task with no training data, which approach (FT or IT) is viable, and why?

- **Concept: Directly-Follows Graphs (DFG) vs. Process Trees**
  - **Why needed here:** These are the primary output targets for the discovery tasks. You must understand that DFGs are simpler pairwise relations, while Process Trees capture complex hierarchy (loops, parallelism, choices).
  - **Quick check question:** Why is S-PTD (Process Tree Discovery) considered a "more complex discovery task" than S-DFD in the paper?

## Architecture Onboarding

- **Component map:** Base Model (Llama 3 70B or Mistral Large 2) -> LoRA Adapter (r=16, α=16) -> Instruction-formatted Input (Role + Task Formulation + Context) -> Sampler (examples-proportional mixing)

- **Critical path:**
  1. **Dataset Creation:** Convert raw event logs/model data into instruction-response pairs (6 prompt variations per task, apply inversions)
  2. **Leave-One-Group-Out Training:** Train LoRA adapters on 2 task groups using 4-bit quantization
  3. **Validation:** Checkpoint selection based on custom metrics (macro F1 or Fitness) on held-out group, not just cross-entropy loss

- **Design tradeoffs:**
  - **Generalization vs. Accuracy:** IT models lose to task-specific Fine-Tuned models (e.g., FT Llama 8B beats IT Llama 70B on A-SAD)
  - **Stability vs. Variability:** High variability in prompt formulations improves generalization but may introduce noise; Llama IT showed unstable results on classification despite good generative performance

- **Failure signatures:**
  - **Prediction Bias (Llama):** Model predicts one class predominantly (e.g., 89.1% "True" for T-SAD), failing to generalize for classification
  - **Domain Drift:** Performance drops significantly in "Healthcare" and "IT" domains due to high process variability
  - **Malformed Output (Base Models):** Generating code instead of natural language or failing to close graph structures

- **First 3 experiments:**
  1. **Zero-Shot Baseline:** Evaluate base Llama/Mistral on all 5 tasks (A-SAD, T-SAD, S-NAP, S-DFD, S-PTD) to establish "untuned" benchmark and check for format adherence issues
  2. **Hold-Out Discovery:** Train on Anomaly + Prediction tasks (S-NAP, A-SAD, T-SAD) and test *only* on Discovery (S-DFD, S-PTD) to verify semantic reasoning transfers from classification to generation
  3. **Ablation on Inversions:** Train two versions—one with "negative inversions" and one without—on Prediction task to measure impact on robustness against anomalous inputs

## Open Questions the Paper Calls Out
None

## Limitations
- Performance varies significantly across tasks and models, with instruction-tuning actually decreasing accuracy for some models on anomaly detection tasks
- Experiments only test on a limited set of process mining tasks (5 tasks across 3 groups), leaving unclear whether observed generalization extends to more diverse scenarios
- Paper doesn't explore the trade-off between instruction-tuning and task-specific fine-tuning in depth, not quantifying the accuracy cost of generalization

## Confidence

- **High Confidence:** Core finding that instruction-tuning improves performance on discovery and prediction tasks (fitness scores improving from 0.63 to 0.71 for S-DFD and F1 scores increasing from 0.53 to 0.65 for S-NAP) is well-supported by experimental results

- **Medium Confidence:** Claim that instruction-tuning enables zero-shot generalization to unseen tasks is supported but requires cautious interpretation - while improvement on held-out discovery tasks is shown, the mechanism (aligning internal representations with semantic intent) is theoretically sound but not fully validated through ablation studies

- **Low Confidence:** Assertion that instruction-tuning reduces syntactic hallucination and format errors in generative tasks, while demonstrated for some outputs, lacks comprehensive evaluation across all task types and doesn't address potential trade-offs between format adherence and logical correctness

## Next Checks
1. **Ablation on Inversion Mechanisms:** Conduct controlled experiments comparing instruction-tuned models trained with and without "negative inversion" formulations to quantify their specific contribution to robustness and generalization

2. **Cross-Domain Generalization:** Test instruction-tuned models on process mining tasks from domains not represented in training data (e.g., financial processes if training focused on healthcare/IT) to validate true generalization beyond seen task types

3. **Fine-Tuning vs. Instruction-Tuning Trade-off:** Systematically compare performance gap between instruction-tuned generalist models and task-specific fine-tuned specialists across all five tasks, quantifying the accuracy cost of generalization