---
ver: rpa2
title: Comparing Moral Values in Western English-speaking societies and LLMs with
  Word Associations
arxiv_id: '2505.19674'
source_url: https://arxiv.org/abs/2505.19674
tags:
- moral
- word
- concepts
- associations
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of assessing moral alignment
  between LLMs and humans. Direct moral prompting is unreliable due to training data
  leakage and sensitivity to prompt formulation.
---

# Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations

## Quick Facts
- arXiv ID: 2505.19674
- Source URL: https://arxiv.org/abs/2505.19674
- Reference count: 22
- Primary result: Word association graphs show LLMs and humans have comparable overall moral alignment but diverge systematically in dimensions like fairness and sanctity

## Executive Summary
This work addresses the challenge of assessing moral alignment between LLMs and humans by using word associations as low-level underlying representations rather than direct moral prompting. The authors generate LLM word associations using Llama-3.1-8B and construct word association graphs for both humans and LLMs. A novel moral value propagation algorithm based on random walks through global association networks is introduced. The method shows improved correlation with human moral judgments compared to prior work, revealing that humans exhibit more emotional and concrete associations while LLMs produce more abstract and sterile responses.

## Method Summary
The framework generates word associations for LLMs using Llama-3.1-8B-Instruct prompted with the same instructions as the human Small World of Words study. These associations are aggregated into a weighted graph (WA-L) and compared against human word association data (WA-H). Moral values are propagated through these graphs using a random walk algorithm initialized with seed words from the Moral Foundations Dictionary 2.0. The resulting Global Moral Networks (GMN-H and GMN-L) are evaluated against the extended Moral Foundations Dictionary (eMFD) to measure alignment between human and LLM moral reasoning.

## Key Results
- GMN-L shows higher correlation with human moral judgments than previous methods (0.54 vs 0.40 Spearman correlation)
- GMN-H and GMN-L exhibit comparable overall alignment but diverge systematically in fairness and sanctity dimensions
- Humans produce more emotional and concrete associations while LLMs generate more abstract and sterile responses
- Optimal temperature tuning (τ=2.1) successfully matches human variability in word association diversity

## Why This Works (Mechanism)

### Mechanism 1
Word associations serve as a more robust, lower-level proxy for moral reasoning than direct prompting. By prompting a model for spontaneous associations rather than moral judgments, the framework bypasses sensitivities to prompt framing and potential training data leakage of moral questionnaires. The resulting word association graph is treated as the model's "mental lexicon," which implicitly encodes conceptual relationships influenced by moral values without triggering surface-level, socially desirable responses.

### Mechanism 2
Global graph propagation via random walks successfully infers moral values for non-seed words. A seed set of words with known moral scores propagates its values across the entire word association network. The random walk algorithm spreads this moral information based on the connectivity and edge weights between concepts, allowing a word like "life" to inherit moral significance from "mother" through intermediate nodes like "birth."

### Mechanism 3
Systematic moral misalignments between humans and LLMs are revealed by structural differences in their association graphs. The framework constructs two parallel graphs (GMN-H and GMN-L). Divergence in moral scores is traced back to differences in the underlying associations. For example, humans associate "vomit" with concrete, sensory terms while the LLM uses abstract, causal terms.

## Foundational Learning

### Concept: Moral Foundation Theory (MFT)
MFT provides the core moral dimensions (Care, Fairness, Loyalty, Authority, Sanctity) and the initial seed dictionary required to "inject" morality into the otherwise value-neutral word association graphs. Without it, the propagation algorithm has no source signal.

### Concept: Random Walk with Restart
This is the core algorithm that makes moral inference possible across the entire graph. The restart probability (`1 - α`) anchors the propagation to the initial moral seed values, preventing the diffusion from becoming unmoored and ensuring that inferred values remain semantically related to the foundations.

### Concept: Graph Topology & Network Metrics
The paper uses graph statistics like diameter, density, and weighted average edge centrality to explain why moral values propagate differently in human vs. LLM graphs. Understanding these metrics is essential for diagnosing misalignment and interpreting the results.

## Architecture Onboarding

### Component map:
1. Data Generation Module: Takes cue words → prompts LLM → generates associations → constructs WA-L
2. Propagation Engine: Loads graphs → initializes moral seeds → applies random walk → produces GMN
3. Evaluation & Analysis Module: Compares propagated scores → computes correlations → analyzes divergence

### Critical path:
(1) Temperature Tuning → (2) Graph Construction → (3) Alpha Optimization → (4) Propagation

### Design tradeoffs:
- Prompting for Associations: Avoids direct moral questions to reduce leakage/bias but is an indirect measure
- Choice of Seed Dictionary: Framework relies on MFD 2.0; its accuracy directly constrains propagation
- Model Selection: Single model (Llama-3.1-8B) provides consistency but limits generalizability

### Failure signatures:
- Low Correlation with eMFD: Poor graph topology, suboptimal α value, or fundamentally misaligned LLM
- Nonsensical Propagated Values: Information leaking through irrelevant, high-weight edges
- Poor Reliability in WA-L: LLM's associations are unstable, making the generated graph unreliable

### First 3 experiments:
1. Reproduce Baseline Alignment: Generate WA-L, optimize temperature, confirm correlation with eMFD
2. Alpha Sensitivity Analysis: Systematically vary α for both GMN-H and GMN-L, plot correlation results
3. Qualitative Divergence Check: Identify top concepts, manually inspect divergent associations

## Open Questions the Paper Calls Out

### Open Question 1
Does conceptual alignment in word associations predict observable LLM behavior in ethical tasks? The paper suggests linking "associative alignments... to observable LLM behaviors" to understand practical alignment.

### Open Question 2
How do moral associations differ in non-Western, secular, or religiously diverse contexts? Section 6.1 calls for replicating experiments with "LLMs trained on corpora from secular societies (e.g., China)."

### Open Question 3
Can the framework be scaled to assess moral alignment in broader contexts, such as sentences or documents? Section 8 notes the current approach is "not directly applicable" to contexts beyond single concepts.

### Open Question 4
Are the findings robust across different model architectures and updated moral theories? Section 8 acknowledges the focus on Llama-3.1-8B, leaving "exploration of more large language models" to future work.

## Limitations
- Framework relies on a single LLM (Llama-3.1-8B), limiting generalizability across architectures
- Temperature tuning validated on 400-cue subset may not fully capture variability for full 12k-cue set
- Closed-form propagation assumes graph connectivity that may not hold uniformly across moral dimensions

## Confidence

### High Confidence
- Correlation improvement over MAG (0.54 vs 0.40 Spearman) is statistically robust
- Qualitative differences in association types (concrete/emotional vs abstract/systematic) are clearly observable

### Medium Confidence
- Word associations as proxy relies on indirect evidence rather than direct ablation studies
- Optimal temperature (2.1) and alpha values (0.75/0.9) are empirically determined but may be dataset-specific

### Low Confidence
- Assumption that LLM association graphs meaningfully parallel human mental lexicons not directly validated
- Propagation mechanism's ability to capture multi-hop moral relationships depends on untested assumptions

## Next Checks
1. Generate WA-L using GPT-4 and Claude-3-Opus on same 400-cue validation set to test architecture generalization
2. Systematically prune low-weight edges from WA-L and rerun propagation to assess topology sensitivity
3. Apply framework to WA-H data from non-Western languages and compare alignment patterns with eMFD