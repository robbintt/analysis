---
ver: rpa2
title: Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories
arxiv_id: '2602.02028'
source_url: https://arxiv.org/abs/2602.02028
tags:
- knowledge
- answer
- reasoning
- question
- fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of integrating new knowledge into
  large language models (LLMs) in a way that supports flexible multi-step reasoning.
  Existing knowledge editing methods focus on isolated facts and struggle with portability,
  meaning models fail to use updated information when solving complex, multi-hop reasoning
  tasks.
---

# Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories

## Quick Facts
- **arXiv ID**: 2602.02028
- **Source URL**: https://arxiv.org/abs/2602.02028
- **Reference count**: 40
- **Primary result**: Achieves 99.0 Port.-Hard accuracy on multi-hop reasoning tasks by integrating new knowledge as contextualized background stories

## Executive Summary
This paper addresses the challenge of integrating new knowledge into large language models (LLMs) in a way that supports flexible multi-step reasoning. Existing knowledge editing methods focus on isolated facts and struggle with portability, meaning models fail to use updated information when solving complex, multi-hop reasoning tasks. The proposed solution introduces new knowledge as contextualized background stories and generates self-generated multi-hop questions requiring the combination of new and existing knowledge. Training uses knowledge distillation with a teacher-student setup, where the student must internalize the teacher's reasoning behavior without direct access to the new information. Experiments with Qwen3-32B and Llama 3.1-70B on MQuAKE-Story and MQuAKE-CF datasets show that this approach achieves near-perfect accuracy on challenging multi-hop questions requiring multiple new facts, substantially outperforming baselines.

## Method Summary
The method represents new knowledge as coherent background stories (news articles and biographies) rather than isolated atomic facts. It generates self-supervised multi-hop questions that require combining new information with existing knowledge. Training employs a teacher-student distillation setup where the teacher has story context but the student does not (0.9 dropout rate), forcing internalization. The student learns to reproduce the teacher's output distribution via KL divergence minimization. LoRA adapters (rank 128) are used for efficient fine-tuning on base models (Qwen3-32B, Llama 3.1-70B-Instruct). The approach is evaluated on MQuAKE-Story and MQuAKE-CF datasets using metrics for factual accuracy, locality preservation, and portability in multi-hop reasoning.

## Key Results
- Achieves 99.0 Port.-Hard accuracy on multi-hop reasoning tasks, outperforming atomic fact baselines (84.4)
- Preserves factual accuracy and locality while significantly improving portability
- Shows robust performance on both plausible updates (MQuAKE-Story) and counterfactual edits (MQuAKE-CF-3k)
- Demonstrates effective knowledge internalization for reasoning without requiring direct access to updated context during inference

## Why This Works (Mechanism)

### Mechanism 1: Contextual Narrative Binding
Presenting new knowledge as coherent background stories improves integration with existing knowledge compared to isolated atomic facts. Stories provide relational context that explicitly links new facts to pre-existing entities and concepts. When the model reads a news article describing "Eleanor Sterling appointed Prime Minister" with biographical context (BP executive, Conservative Party, Oxford education), the new fact becomes densely connected to existing knowledge nodes rather than weakly attached to a single entity.

### Mechanism 2: Multi-hop Question Forcing
Training on self-generated multi-hop questions that require combining new knowledge with pre-existing facts forces genuine integration rather than surface memorization. When questions require the new fact as an intermediate reasoning step (not the terminal answer), models must integrate it into reasoning chains. Example: "What is the slogan of the company led by Robert Sterling?" requires: (1) Robert Sterling → McDonald's CEO (new), (2) McDonald's → slogan (existing).

### Mechanism 3: Context Distillation with Dropout
Teacher-student distillation where the teacher has story access but the student (with high dropout) does not forces knowledge internalization into parameters. Student learns to reproduce teacher's output distribution without the privileged context. The 0.9 dropout rate means the student rarely sees the story during training, forcing it to encode the knowledge internally. KL divergence on soft targets transfers implicit reasoning structure, not just surface patterns.

## Foundational Learning

- **Concept: Knowledge Editing Metrics (Factual Accuracy, Locality, Portability)**
  - Why needed here: The paper optimizes for all three simultaneously. Factual accuracy = can recall new fact directly. Locality = doesn't corrupt neighboring knowledge. Portability = can use new fact in multi-hop reasoning (the core innovation).
  - Quick check question: If a model learns "X is CEO of Company Y" but fails to answer "What country is Company Y headquartered in?", which metric fails?

- **Concept: Context Distillation (Soft Targets)**
  - Why needed here: Core training mechanism. Unlike SFT with hard labels, distillation uses KL divergence on full output distributions, transferring implicit reasoning patterns. Answer-only supervision works because soft targets encode latent reasoning structure.
  - Quick check question: Why might soft teacher distributions transfer reasoning ability better than hard answer labels?

- **Concept: Knowledge Graph Multi-hop Reasoning**
  - Why needed here: The evaluation assumes facts are structured (entity-relation-entity), and multi-hop questions traverse these paths. Understanding that "Who leads the country where Shakespeare was born?" requires traversing Shakespeare→birthplace→country→leader.
  - Quick check question: For a 3-hop question, if hop 2 fails, can the model still get the right answer by luck?

## Architecture Onboarding

- **Component map**: Story Generator -> Question Generator -> Teacher Model (with context) -> Student Model (without context) -> Evaluation Suite
- **Critical path**: Story quality → Question quality → Teacher response quality → Student internalization. Story generation uses Gemini 3; question generation uses the base model itself. Filtering removes questions not requiring the new fact.
- **Design tradeoffs**:
  - Answer-only vs. reasoning-trace supervision: Answer-only is cleaner for plausible updates (no noise from irrelevant intermediate steps), but reasoning traces help override strong priors on implausible counterfactuals
  - Atomic facts vs. stories: Stories add context cost but dramatically improve portability
  - Dropout rate (0.9): Higher dropout forces internalization but may underutilize context; ablation confirms 0.9 is optimal
- **Failure signatures**:
  1. Skepticism in reasoning traces: When trained on atomic facts + reasoning traces, models say "This is probably fictional" and revert to priors
  2. Self-correction during CoT: Answer-only models may recall new knowledge but then override it as "unrealistic"
  3. Locality degradation: Some neighboring knowledge loss is visible (e.g., 97%→85.5% locality); conservative sampling helps
- **First 3 experiments**:
  1. Replicate the answer-only + stories + multi-hop configuration on MQuAKE-Story subset with 5-10 edited facts to verify Port.-Hard improvement
  2. Ablate story vs. atomic facts while holding other components constant to isolate narrative contribution
  3. Test transfer: Train on MQuAKE-Story, evaluate on MQuAKE-CF to assess cross-dataset generalization of learned reasoning patterns

## Open Questions the Paper Calls Out

- **Open Question 1**: How can knowledge editing frameworks be modified to determine the appropriate scope and context for applying newly edited knowledge to prevent "over-application"?
  - Basis: The authors note they "observed occasional over-application of updated knowledge, suggesting the need for better mechanisms to learn when the new knowledge should be applied."

- **Open Question 2**: Can the proposed method effectively support continual learning and sequential updates where multiple edits may be temporally conflicting or contradictory?
  - Basis: The Discussion highlights the challenge that "Supporting continual updates under multiple, potentially conflicting edits requires models to maintain a coherent and temporally grounded world model."

- **Open Question 3**: Does the strategy of augmenting training with "conservative samples" (neighbor questions) scale to preserve locality without degrading the integration of new multi-hop reasoning?
  - Basis: Appendix G.3 introduces "conservative samples" to improve locality and states this "suggests a promising direction for future work."

- **Open Question 4**: How does reasoning-centric knowledge internalization generalize to complex agentic tasks requiring tool use or long-horizon planning beyond question answering?
  - Basis: The Discussion states: "Beyond multi-hop question answering, many real-world applications involve more complex agentic and multi-step reasoning, motivating extensions of knowledge update methods to these settings."

## Limitations

- The method relies on carefully constructed fictional biographies and news-style narratives that may not generalize to real-world knowledge editing scenarios where context is messier or incomplete
- Performance on MQuAKE-CF (counterfactual edits) shows significant degradation compared to MQuAKE-Story, suggesting the approach is more robust for plausible updates than highly implausible counterfactuals
- Training requires generation of detailed background stories for each knowledge update, adding significant computational and time overhead compared to atomic fact editing

## Confidence

- **High Confidence**: Atomic facts vs. stories for knowledge integration (14.6 point gap in Port.-Hard scores), context distillation mechanism, multi-hop question training effectiveness
- **Medium Confidence**: Story generation quality impact, generalizability to implausible updates
- **Low Confidence**: Cross-model generalization, real-world deployment readiness

## Next Checks

1. **Robustness to story quality variation**: Systematically vary the quality and completeness of background stories (e.g., remove biographical details, simplify narratives) to determine the minimum story quality required for effective knowledge internalization, and identify failure modes when stories are suboptimal.

2. **Cross-dataset generalization test**: Train the model on MQuAKE-Story using the proposed method, then evaluate on a completely different knowledge editing dataset (e.g., real-world news updates or Wikipedia edits) to assess whether the learned reasoning patterns transfer to novel domains and fact types.

3. **Resource efficiency benchmarking**: Compare the computational cost (training time, GPU memory, generation overhead) of the story-based approach against atomic fact editing methods across different model sizes to quantify the practical scalability limitations and identify potential optimizations.