---
ver: rpa2
title: 'Perturb Your Data: Paraphrase-Guided Training Data Watermarking'
arxiv_id: '2512.17075'
source_url: https://arxiv.org/abs/2512.17075
tags:
- data
- training
- text
- dataset
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPECTRA presents a watermarking approach that enables content creators
  to detect unauthorized use of their data in large language model training. The method
  works by paraphrasing text using an LLM and selecting a paraphrase whose Min-K%++
  score closely matches the original, thereby avoiding distribution shifts.
---

# Perturb Your Data: Paraphrase-Guided Training Data Watermarking

## Quick Facts
- arXiv ID: 2512.17075
- Source URL: https://arxiv.org/abs/2512.17075
- Reference count: 22
- Primary result: Achieves p-value gaps exceeding nine orders of magnitude between training and non-training data, reliably detecting unauthorized use of less than 0.001% of the training corpus

## Executive Summary
SPECTRA presents a watermarking approach that enables content creators to detect unauthorized use of their data in large language model training. The method works by paraphrasing text using an LLM and selecting a paraphrase whose Min-K%++ score closely matches the original, thereby avoiding distribution shifts. A statistical test compares the scores from a scoring model (pre-training state proxy) against those from a target model to detect membership. The approach achieves p-value gaps exceeding nine orders of magnitude between training and non-training data, outperforming all baselines tested, with reliable detection across multiple datasets containing less than 0.001% of the training corpus.

## Method Summary
SPECTRA employs paraphrase-guided watermarking by first paraphrasing input text using a high-quality LLM. For each paraphrase, the Min-K%++ score is computed, which measures the minimum similarity across multiple paraphrasing rounds. The paraphrase with a score closest to the original text is selected to minimize distribution shift. During detection, a statistical test compares the Min-K%++ scores from a scoring model (trained to represent the pre-training state) against those from the target model. Large differences in p-values between these models indicate membership in the training data. This approach maintains semantic fidelity while embedding a detectable signal that survives paraphrasing attacks.

## Key Results
- Achieves p-value gaps exceeding nine orders of magnitude between training and non-training data
- Detects unauthorized use of less than 0.001% of the training corpus with high reliability
- Outperforms all baseline watermarking methods tested across multiple datasets

## Why This Works (Mechanism)
The method works by exploiting the statistical differences between models trained on watermarked versus non-watermarked data. By carefully selecting paraphrases that maintain the original Min-K%++ score, the watermark avoids introducing detectable distribution shifts that would be evident during training. The Min-K%++ metric provides a stable measure of text similarity that remains consistent under controlled paraphrasing, creating a detectable signal in the model's learned representations. The statistical test leverages the scoring model as a proxy for the pre-training state, allowing detection even when the original model weights are unavailable.

## Foundational Learning

### Paraphrasing with LLMs
**Why needed:** Creates perturbed versions of training data while preserving semantic content, enabling watermark embedding without obvious distribution shifts.
**Quick check:** Verify that paraphrased outputs maintain semantic equivalence using automated metrics and human evaluation.

### Min-K%++ Scoring
**Why needed:** Provides a robust measure of text similarity that remains stable under paraphrasing, serving as the basis for watermark detection.
**Quick check:** Test score stability across multiple paraphrasing iterations and different text types.

### Statistical Hypothesis Testing
**Why needed:** Enables quantitative comparison between model behaviors to detect watermark presence with statistical significance.
**Quick check:** Validate p-value distributions and thresholds using controlled training experiments.

## Architecture Onboarding

### Component Map
Content Creator -> Paraphrasing LLM -> Min-K%++ Scoring -> Watermarked Data -> Model Training
Scoring Model (Pre-training state) -> Target Model -> Statistical Test -> Membership Detection

### Critical Path
1. Content creation and initial text generation
2. Paraphrasing and Min-K%++ score computation
3. Watermarked data injection into training
4. Scoring model training on original watermarked data
5. Statistical comparison between scoring and target models

### Design Tradeoffs
The method balances watermark detectability against distribution shift. More aggressive paraphrasing increases watermark strength but risks training instability. The Min-K%++ score optimization ensures minimal distribution shift but requires additional computation during data preparation. Using an auxiliary scoring model enables detection but adds complexity and potential points of failure.

### Failure Signatures
Detection failure occurs when paraphrased watermarked text introduces significant distribution shift, causing the scoring model to produce inconsistent results. This manifests as p-value gaps that no longer reliably separate training from non-training data. Attack scenarios include excessive paraphrasing or semantic-preserving transformations that alter the Min-K%++ score distribution.

### First Experiments
1. Baseline detection accuracy on synthetic watermarked/non-watermarked splits
2. Distribution shift measurement across different paraphrasing intensities
3. Robustness testing against controlled paraphrasing attacks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas warrant further investigation. The robustness against more sophisticated adversarial attacks beyond controlled paraphrasing scenarios needs exploration. The scalability of the approach for very large corpora and the computational overhead during both watermarking and detection phases require detailed analysis. The method's performance across different languages and domains, particularly when the paraphrasing LLM is not optimized for those domains, remains untested.

## Limitations
- Dependence on a high-quality paraphrasing LLM that may not be consistently available across domains
- Reliance on an auxiliary scoring model whose similarity scores could vary across different text types or languages
- Computational costs and scalability challenges for very large corpora are not addressed

## Confidence

**High confidence:**
- Core watermarking mechanism and its effectiveness on controlled test sets
- Statistical detection method achieves claimed p-value gaps in tested scenarios

**Medium confidence:**
- Robustness claims against paraphrasing attacks due to limited adversarial testing
- Practical applicability due to unaddressed deployment and scalability issues

## Next Checks

1. Test the watermarking and detection pipeline on multilingual and out-of-domain datasets to assess generalizability.
2. Conduct stress tests using advanced paraphrasing and adversarial text manipulation strategies to evaluate robustness.
3. Perform a scalability and computational cost analysis for large-scale data and model training scenarios.