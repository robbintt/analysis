---
ver: rpa2
title: Quantification and object perception in Multimodal Large Language Models deviate
  from human linguistic cognition
arxiv_id: '2511.08126'
source_url: https://arxiv.org/abs/2511.08126
tags:
- quantifiers
- language
- quantification
- humans
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how (Multimodal) Large Language Models
  (MLLMs) represent quantification compared to humans. The authors focus on three
  underexplored aspects of quantification: the ordering of quantifiers into scales,
  the ranges of use and prototypicality, and biases in the human approximate number
  system.'
---

# Quantification and object perception in Multimodal Large Language Models deviate from human linguistic cognition

## Quick Facts
- arXiv ID: 2511.08126
- Source URL: https://arxiv.org/abs/2511.08126
- Reference count: 17
- Primary result: MLLMs encode quantification differently from humans, lacking logical definitions and pragmatic typicality

## Executive Summary
This study investigates how Multimodal Large Language Models represent quantification compared to humans across six languages (English, Greek, Russian, Spanish, Italian, Catalan). The authors examine three aspects of quantification: scale ordering, ranges of use and prototypicality, and biases in the human approximate number system. Using visual stimuli and embedding analysis, they find that MLLMs, particularly GPT-4o, use quantifiers differently from humans, showing no clear scale ordering and struggling with typicality and ranges of use. While reasoning models (o4-mini) perform better on logical ordering, they still deviate from human behavior on pragmatic aspects.

## Method Summary
The study compares human participants (240 total, 40 per language via Prolific) with two MLLMs (GPT-4o and o4-mini) across visual quantification tasks. Visual stimuli consist of 512x512 pixel images with black squares (25px width) and white circles (25px diameter) created using p5.js. Participants and models select appropriate quantifiers from a set of four options across different proportions. The analysis includes embedding analysis using text-embedding-3-large to compare quantificational sentences with proportional sentences, and multinomial regression with post-hoc Tukey tests to analyze quantifier selection patterns.

## Key Results
- GPT-4o shows no clear scale ordering of quantifiers and uses "most" across 10-100% proportions, violating set-theoretic definitions
- Both models struggle with typicality and ranges of use, with o4-mini performing better on ordering but still deviating on pragmatic aspects
- Cross-linguistic differences exist, with Russian, Greek, and Catalan showing lower alignment with human responses
- Quantifier embeddings show high homogeneity (0.60-0.85 cosine similarity) without clear magnitude gradients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLMs fail to replicate human quantifier usage because distributional semantic approaches capture statistical co-occurrence patterns rather than rigid logical definitions.
- Mechanism: Models like GPT-4o learn textual associations (e.g., "most" with "large amounts") but lack grounding to enforce set-theoretic constraints like |P∩Q| > |P−Q|, leading to logically invalid quantifier use.
- Core assumption: Logical operators require symbolic grounding or explicit logical training data that current distributional methods don't provide.
- Evidence anchors:
  - [Page 9, Discussion]: "LLMs rely on statistical distributional patterns... it encounters difficulties when trying to capture the meaning of topic independent words, such as negation or numerals."
  - [Page 6, Results]: "GPT-4o uses the quantifier [most] across almost all the available proportions (ranges of use: 10-100%); even in contexts disallowed by the set theoretic definition."

### Mechanism 2
- Claim: Reinforcement Learning improves logical ordering but fails to capture human-like pragmatic flexibility.
- Mechanism: Reasoning models (o4-mini) are trained to maximize rewards for "correct" reasoning, enforcing logical hierarchy but optimizing for single ground truth rather than human intra-speaker variability in scalar implicatures.
- Core assumption: RL reward functions prioritize verifiable accuracy over probabilistic distribution of pragmatic language use.
- Evidence anchors:
  - [Page 9, Discussion]: "RL seems to be insufficient... due to the methodology relying on a ground truth answer, but lacking training on the intra-speaker variability that emerges when (scalar) implicatures are used."
  - [Page 6, Results]: "o4-mini performs better than the non-reasoning one [on ordering] but still deviates from human behavior [on typicality]."

### Mechanism 3
- Claim: Quantifier confusion is exacerbated by lack of distinct gradients in embedding space.
- Mechanism: The embedding model compresses semantically distinct quantifiers into similar vector locations, preventing the LLM from distinguishing fine-grained proportional meanings.
- Core assumption: Vector space structure directly constrains semantic precision of model outputs.
- Evidence anchors:
  - [Page 7, Results]: "The distribution is very homogeneous... [quantifiers] show a very similar score to all the proportional sentences."
  - [Page 7, Results]: "The values do not follow a continuous gradient... different quantifiers have several sentence pairs with the highest similarity and which are not adjacent."

## Foundational Learning

- Concept: **Generalized Quantifier Theory (GQT)**
  - Why needed here: Theoretical framework defining where models fail (e.g., conservativity, monotonicity)
  - Quick check question: Why does the sentence "Most squares are circles" violate the set-theoretic definition of *most*, independent of any visual context?

- Concept: **Scalar Implicature**
  - Why needed here: Explains failure to capture "ranges of use" and "prototypicality" as pragmatic phenomena
  - Quick check question: If a speaker says "Some students passed," why is it infelicitous in a context where *all* students passed, even if logically true?

- Concept: **Approximate Number System (ANS)**
  - Why needed here: Isolates ANS biases as separate failure point from semantic knowledge
  - Quick check question: How does the ANS explain why humans are better at estimating "5 vs 10" items than "95 vs 100" items, and how might this differ in a convolutional neural network?

## Architecture Onboarding

- Component map:
  Vision Encoder -> Projector -> LLM Backbone (GPT-4o/o4-mini) -> Embedding Model (text-embedding-3-large)

- Critical path:
  1. **Visual Stimuli**: Input image (e.g., 30 squares, 70 circles)
  2. **Perception & Counting (ANS)**: Model estimates counts/proportions
  3. **Semantic Mapping**: Mapping estimated count to quantifier token
  4. **Pragmatic Enrichment**: Adjusting choice based on typicality

- Design tradeoffs:
  - Reasoning vs. Standard Model: o4-mini improves logical adherence but doesn't fix pragmatic typicality
  - Resource Availability: Lower-resource languages show different embedding clustering and worse alignment

- Failure signatures:
  - "Most" Inflation: GPT-4o selecting "Most" for proportions as low as 10-20%
  - Embedding Homogeneity: High cosine similarity (>0.6) between "A few" and "Most"
  - Middle-Value Bias: Reporting middle values (40-60%) regardless of true count

- First 3 experiments:
  1. **Sanity Check (Visual Counting)**: Show images with 10, 50, 90 objects; verify if error is in vision or language
  2. **Logical Adherence Test**: Prompt "If 10% of squares are blue, is it true that 'Most squares are blue'?" Verify >50% threshold
  3. **Embedding Linearity Check**: Extract embeddings for "0%", "25%", "50%", "75%", "100%" and quantifiers; compute cosine similarity; check for gradient

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Reinforcement Learning with training on human intra-speaker variability in scalar implicature usage improve MLLMs' typicality values and ranges of use for quantifiers?
- Basis in paper: [explicit] The authors hypothesize that RL training "might be due to the methodology relying on a ground truth answer, but lacking training on the intra-speaker variability that emerges when (scalar) implicatures are used by humans."
- Why unresolved: The reasoning model (o4-mini) improved on ordering and logical definitions but still differed from humans on typicality and ranges of use, suggesting RL alone is insufficient.
- What evidence would resolve it: Train models with RL that includes human variability data for implicatures, then test whether typicality values and ranges of use align with human baselines.

### Open Question 2
- Question: Why is the quantifier "many" the hardest quantifier for MLLMs across all tested languages, despite not being more complex than "most" in set-theoretic terms?
- Basis in paper: [explicit] The authors state: "This result was unexpected given that this quantifier relies on the same linguistic mechanisms as the other quantifiers, and in set theoretic terms is not more complex than quantifiers such as most..."
- Why unresolved: No current linguistic theory predicts this pattern, and distributional semantic approach doesn't explain systematic difficulty.
- What evidence would resolve it: Systematic analysis of training data distributions for "many" vs. other quantifiers, or probing experiments targeting contextual threshold representations.

### Open Question 3
- Question: Do MLLMs possess a centralized conceptual representation for quantification that is externalized across languages, or do they have language-specific representations?
- Basis in paper: [explicit] The authors aimed to elucidate "whether their abilities are robust and stable across different languages" and whether models have "a centralized conceptual representation... or whether there are language-specific neurons/linguistic maps for each language."
- Why unresolved: Results show different internal representations across languages, but don't confirm either hypothesis definitively.
- What evidence would resolve it: Layer-wise analysis of multilingual models to identify whether quantifier concepts share neural substrates across languages, or cross-lingual transfer experiments probing quantifier understanding.

## Limitations
- Embedding analysis relies on a single embedding model without exploring alternative architectures
- Cross-linguistic variation is documented but causal mechanisms remain speculative
- Approximate number system analysis doesn't fully disentangle vision encoder versus language model errors

## Confidence

- **High Confidence**: GPT-4o shows no clear quantifier scale ordering and uses "most" inappropriately across proportions (10-100%)
- **Medium Confidence**: Reasoning models improve logical adherence but fail on pragmatic typicality; cross-linguistic differences observed but mechanisms speculative
- **Low Confidence**: Specific attribution of errors to distributional semantic approaches versus other architectural factors; proposed break conditions are theoretical

## Next Checks

1. **Ablation Study**: Test whether replacing the embedding model with one trained to encode magnitude information improves quantifier gradient representation and typicality judgments.

2. **Cross-Modal Error Isolation**: Run the same visual stimuli through a pure vision model to quantify counting errors separately from language model processing, then compare with MLLM outputs to isolate the source of quantifier mapping failures.

3. **Fine-Tuning Intervention**: Fine-tune GPT-4o on a synthetic dataset of logical quantifier entailment examples and measure whether this improves adherence to set-theoretic definitions while preserving pragmatic flexibility.