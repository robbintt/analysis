---
ver: rpa2
title: Adapting Tensor Kernel Machines to Enable Efficient Transfer Learning for Seizure
  Detection
arxiv_id: '2512.02626'
source_url: https://arxiv.org/abs/2512.02626
tags:
- data
- seizure
- tensor
- learning
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an efficient transfer learning method using
  tensor kernel machines for seizure detection on behind-the-ear EEG. The approach
  adapts a patient-independent model to individual patients using small amounts of
  patient-specific data through regularization, inspired by the adaptive SVM model.
---

# Adapting Tensor Kernel Machines to Enable Efficient Transfer Learning for Seizure Detection

## Quick Facts
- arXiv ID: 2512.02626
- Source URL: https://arxiv.org/abs/2512.02626
- Reference count: 40
- Primary result: Adapt-TKRR achieves better seizure detection performance than patient-independent models while using ~100x fewer parameters than adaptive SVM baselines

## Executive Summary
This paper presents Adapt-TKRR, an efficient transfer learning method for seizure detection using tensor kernel machines. The approach adapts a patient-independent model to individual patients through regularization, inspired by adaptive SVM techniques. By leveraging low-rank tensor networks, the method learns compact non-linear models in the primal domain, enabling adaptation without increasing model size. Evaluated on the SeizeIT1 dataset, Adapt-TKRR outperforms patient-independent and fully patient-specific models while requiring significantly fewer parameters than competing approaches, making it suitable for resource-constrained wearable devices.

## Method Summary
The method adapts a patient-independent tensor kernel ridge regression (TKRR) model to individual patients using small amounts of patient-specific data. It employs a Canonical Polyadic Decomposition (CPD) to represent the weight tensor in a low-rank form, reducing parameters from exponential to linear scaling. The key innovation is changing the regularization from standard Frobenius norm to a distance-based penalty between source and adapted weights, enabling knowledge transfer. A sinusoidal feature map approximates the RBF kernel while maintaining computational efficiency. The model is trained using block coordinate descent, with patient-specific hyperparameters inherited from the source model.

## Key Results
- Adapt-TKRR achieves sensitivity of 0.632 and precision of 0.601 on seizure detection
- Model requires approximately 100 times fewer parameters than adaptive SVM approaches
- Performance exceeds both patient-independent and fully patient-specific models
- Adaptation speed is significantly faster due to compact parameter representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptation transfers prior knowledge from source to target through regularization, preventing overfitting on small patient datasets
- Mechanism: Adapt-TKRR replaces standard Frobenius norm regularization with a distance-based penalty (∥W - Ws∥²F) that constrains adapted weights W to stay close to pre-trained source weights Ws, controlled by hyperparameter μ
- Core assumption: Source model contains useful, generalizable features relevant to patient-specific tasks
- Evidence anchors: Abstract mentions knowledge transfer via regularization; Section III details the squared Frobenius distance regularization; related work on Bayesian tensor networks supports need for complexity control
- Break condition: Mechanism fails when source model's decision boundary contradicts target data, potentially degrading performance

### Mechanism 2
- Claim: Low-rank tensor networks enable parameter-efficient learning in primal domain, making models compact and fast
- Mechanism: Instead of full weight tensor W (exponential scaling), model uses CPD representation with sum of rank-one tensors, reducing parameters to linear scaling and avoiding large support vector storage
- Core assumption: Weight tensor can be accurately approximated by low-rank tensor network without significant accuracy loss
- Evidence anchors: Abstract highlights compact model learning without added parameters; Section II.B discusses CPD/Tensor-Train approximations; related work on Volterra systems supports tensor network utility
- Break condition: Low-rank approximation insufficient for complex functions, causing underfitting

### Mechanism 3
- Claim: Sinusoidal feature map provides structured, expressive RBF kernel approximation while maintaining computational efficiency
- Mechanism: Uses outer product of local sinusoidal basis functions to construct Φ(x), approximating RBF kernel explicitly without infinite-dimensional kernel matrix computation
- Core assumption: Basis functions M and approximation interval U are sufficient for kernel behavior
- Evidence anchors: Section V.A describes sinusoidal basis approximation of RBF kernel; Section II.B mentions Fourier features for function approximation; corpus evidence limited for this specific application
- Break condition: Poor hyperparameter choices (M, U) lead to inadequate kernel approximation and suboptimal performance

## Foundational Learning

- **Kernel Ridge Regression (KRR) vs. SVMs**
  - Why needed here: Adapt-TKRR is based on KRR principles (squared loss) adapted for tensor networks; understanding differentiable loss landscape is crucial for block coordinate descent optimization
  - Quick check question: What loss function does KRR use, and how does it differ from hinge loss used by standard SVM?

- **The "Curse of Dimensionality" and Tensor Decompositions**
  - Why needed here: Core motivation is solving exponential parameter growth in primal kernel methods; understanding why full tensor storage is intractable explains why CPD decomposition is structurally necessary
  - Quick check question: For 42-dimensional input (D=42) and feature dimension 20 (M=20), how many elements would full weight tensor W have?

- **Transfer Learning via Regularization**
  - Why needed here: Entire Adapt-TKM method hinges on this specific formulation of transfer learning; not neural network fine-tuning but mathematical constraint in optimization objective
  - Quick check question: In Adapt-TKRR objective, what term controls trade-off between fitting new patient data and staying close to source model?

## Architecture Onboarding

- Component map: Feature Mapper -> Tensor Weights (CPD) -> Optimization Core (Adapt-TKRR) -> Source Weights (S)
- Critical path: Source weights S initialization -> (Target Data xt, Feature Map Φ(xt)) -> Optimization Loop (update W(1), W(2), ..., W(D) alternately using Eq. 14) -> Final Adapted Weights W
- Design tradeoffs:
  - R vs. Accuracy: Higher CPD rank R increases capacity but also parameter count and overfitting risk
  - Feature Map Complexity (M) vs. Speed: More basis functions M improve kernel approximation but increase factor matrix size linearly
  - Regularization (μ) vs. Plasticity: High μ makes model rigid (sticks to source), low μ allows more change (risking overfitting)
- Failure signatures:
  - Negative Transfer: Performance drops after adaptation (patients 33, 65, 78, 82, 99) when PI model is already good
  - Slow Convergence: Random initialization leads to slower convergence than source initialization
  - Underfitting: Insufficient rank R or feature dimensions M fail to capture seizure pattern complexity
- First 3 experiments:
  1. Hyperparameter Sensitivity Check: Replicate Table II approximation error experiment with small sample data
  2. Transfer Strength Sweep: Vary μ across orders of magnitude and plot F1-score vs. μ
  3. Initialization Ablation: Run training with random vs. source initialization and plot loss over iterations

## Open Questions the Paper Calls Out

- How can the regularization parameter μ be selected in a data-driven, patient-specific manner rather than using fixed value?
- How can the CPD rank R be determined or adapted optimally for individual patients without exhaustive grid search?
- Can Adapt-TKM approach be extended to prevent performance degradation for patients where PI model already performs well?

## Limitations
- Adaptation sensitivity to regularization strength μ, with fixed choice potentially suboptimal for individual patients
- Kernel approximation error must be kept below 10⁻³, requiring careful feature map parameter selection
- Fixed rank R inherited from PI model may cause over-parameterization or underfitting for specific patients

## Confidence

- **High Confidence**: Core mechanisms of parameter-efficient adaptation through tensor decomposition and regularization-based transfer are well-established and demonstrated
- **Medium Confidence**: Effectiveness of sinusoidal feature map approximation for seizure detection data is supported by results but corpus evidence is limited
- **Medium Confidence**: Practical benefit for resource-constrained devices is demonstrated through parameter reduction but real-world deployment validation is absent

## Next Checks

1. Patient-specific μ tuning: Perform granular sweep of regularization hyperparameter for subset of patients to identify optimal values and assess whether tuning can prevent negative transfer cases

2. Kernel approximation sensitivity: Systematically vary feature map parameters (M, U) and evaluate approximation error against classification performance to determine if 10⁻³ threshold is truly necessary

3. Cross-dataset generalization: Test Adapt-TKRR approach on seizure detection datasets from different institutions or with different EEG configurations to assess generalizability beyond SeizeIT1 dataset