---
ver: rpa2
title: Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting
  (3DGS)
arxiv_id: '2512.20148'
source_url: https://arxiv.org/abs/2512.20148
tags:
- images
- occlusion
- dataset
- estimation
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of apple pose estimation in orchards,
  where occlusions and variations in the environment make it difficult to accurately
  annotate fruit poses. The authors propose a novel pipeline that leverages 3D Gaussian
  Splatting (3DGS) to reconstruct orchard scenes, enabling simplified annotations
  and automated projection of annotations to images.
---

# Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)

## Quick Facts
- arXiv ID: 2512.20148
- Source URL: https://arxiv.org/abs/2512.20148
- Authors: Robert van de Ven; Trim Bresilla; Bram Nelissen; Ard Nieuwenhuizen; Eldert J. van Henten; Gert Kootstra
- Reference count: 33
- Primary result: 99.6% reduction in manual annotation effort while achieving 0.927 neutral F1 score for apple detection

## Executive Summary
This paper addresses the challenge of annotating fruit poses in orchard environments, where occlusions and variations make manual annotation time-consuming and inconsistent. The authors propose a pipeline that uses 3D Gaussian Splatting (3DGS) to reconstruct orchard scenes, enabling simplified 3D annotations that can be automatically projected to all images. This approach achieves a dramatic 99.6% reduction in annotation effort while maintaining high detection performance. However, while the method excels at position estimation, it fails to accurately learn apple orientation, with errors comparable to random guessing.

## Method Summary
The proposed pipeline uses Structure from Motion (SfM) to estimate camera poses, then 3D Gaussian Splatting to reconstruct dense orchard scenes. Each fruit is manually annotated once in 3D space (position + calyx direction), and these annotations are projected into every camera viewpoint using known intrinsics/extrinsics, automatically generating 2D bounding boxes and 3D oriented bounding box labels for all images containing that fruit. The method filters training labels by occlusion rate, finding that fruits with ≤95% occlusion yield optimal performance. Novel camera viewpoints can be rendered from the 3DGS representation to expand training data without additional annotation cost.

## Key Results
- 99.6% reduction in annotation effort: 105 manual annotations yield 28,191 training labels
- Best performance achieved with ≤95% occluded fruits: neutral F1 score of 0.927 on original images
- Position estimation accuracy: 7.13mm Euclidean error
- Orientation estimation failure: 48.1° vector angle error (near random)

## Why This Works (Mechanism)

### Mechanism 1: 3D-to-2D Annotation Projection
Annotating fruit poses once in a 3D reconstruction and projecting to all images reduces annotation effort by 99.6% while maintaining label consistency. The pipeline uses SfM to initialize camera poses, then 3DGS to build a dense scene representation. Each fruit is manually annotated once in 3D space (position + calyx direction). These annotations are then projected into every camera viewpoint using known intrinsics/extrinsics, automatically generating 2D bounding boxes and 3D oriented bounding box labels for all images containing that fruit.

### Mechanism 2: Occlusion-Aware Label Filtering
Including training labels for fruits with ≤95% occlusion maximizes detection performance while excluding heavily occluded samples that harm learning. The pipeline computes occlusion rate per-fruit per-image by comparing depth rendered from the fruit's isolated point cloud against depth rendered from the full 3DGS scene. Pixels with depth difference >15mm are counted as occluded. Training with occlusion thresholds filters which projected labels the model sees.

### Mechanism 3: Rendered View Augmentation
Novel camera viewpoints rendered from 3DGS can expand training data without additional annotation cost. After 3DGS training, arbitrary camera poses can be specified and both RGB and depth images rendered via splatting. The same 3D annotations project to these new views. This enables systematic coverage of the viewing sphere around each tree.

## Foundational Learning

- **Concept: 3D Gaussian Splatting representation**
  - Why needed here: The entire pipeline depends on understanding that 3DGS represents scenes as collections of 3D Gaussians (position, covariance, color, opacity) that can be differentiably rendered to any viewpoint.
  - Quick check question: Can you explain why 3DGS enables real-time rendering compared to NeRF-based volume rendering?

- **Concept: Camera intrinsics and extrinsics**
  - Why needed here: Projecting 3D annotations to 2D images requires understanding how 3D world coordinates transform through camera extrinsics (pose) and intrinsics (focal length, principal point) to image pixels.
  - Quick check question: Given a 3D point in world coordinates and camera matrices, can you compute its 2D image projection?

- **Concept: 5D fruit pose (3D position + 2D orientation)**
  - Why needed here: The paper estimates apple pose as position (x,y,z) plus axis orientation (pitch, yaw), exploiting the rotational symmetry of apples around their stem-calyx axis.
  - Quick check question: Why does representing apple orientation as a 5D pose rather than full 6-DoF reduce ambiguity?

## Architecture Onboarding

- **Component map:** Image capture → SfM pose estimation → 3DGS training → Manual 3D annotation → Projection to all views → Dataset creation → FRESHNet training → Evaluation
- **Critical path:** Image capture → SfM pose estimation → 3DGS training → Manual 3D annotation → Projection to all views → Dataset creation (original patched + rendered) → FRESHNet training → Evaluation on test split
- **Design tradeoffs:** Real vs rendered data (real images generalize better but rendered images expand coverage); occlusion threshold (lower thresholds exclude samples, higher thresholds include noisy labels); dataset size (diminishing returns beyond 100% of original labels); tree-based data splits (prevents leakage but limits size)
- **Failure signatures:** Orientation estimation failure (48.1° error, random prediction); position degradation with occlusion (error increases from ~5mm to ~10mm); rendered-only training underperforms (F1 ~0.75 vs ~0.93 for real images)
- **First 3 experiments:** 1) Validate 3DGS reconstruction quality by rendering from held-out poses and comparing to ground truth; 2) Occlusion threshold sweep on validation set to confirm ≤95% finding; 3) Baseline pose estimation check on synthetic data with known orientation distribution

## Open Questions the Paper Calls Out

### Open Question 1
Can network architecture modifications, loss function adjustments, or weight tuning enable FRESHNet to accurately estimate the orientation of apples? The authors state that focus should be on improving orientation estimation, suggesting "tuning the weights, improving the loss function on the orientation, or adjusting the network architecture." The current method yielded a vector angle error of 48.1°, which was statistically similar to random guessing, indicating the model failed to learn orientation cues. Evidence would be a modified training regime or network structure that achieves a vector angle error significantly lower than the 48.1° baseline.

### Open Question 2
How does the pose estimation performance generalize when applied to different fruit growing systems and varied orchard conditions? The Future Work section notes that the current dataset is homogeneous (single orchard) and recommends collecting datasets in different fruit growing systems to improve generalizability. The model was trained and tested on a single set of 13 trees in Randwijk, limiting the validity of the conclusions to similar environmental conditions. Evidence would be evaluation of the trained model on external datasets comprising different apple varieties, tree training systems, or lighting conditions without retraining.

### Open Question 3
Up to what occlusion rate can human annotators provide accurate and consistent labels compared to the 3DGS-based pipeline? The Future Work section explicitly recommends researching "up to what occlusion rate annotators provide annotations and how accurate these annotations are" compared to the proposed automated method. While the pipeline allows labeling highly occluded fruits (≤95%), it is unknown how these automated labels compare to human perception limits and consistency in 2D image annotation. Evidence would be a comparative study measuring inter-annotator agreement and positional accuracy between manual 2D annotations and 3DGS-projected labels across a spectrum of occlusion rates.

## Limitations
- FRESHNet fails to accurately estimate apple orientations, with 48.1° vector angle errors indicating random prediction performance
- Results are based on a single homogeneous dataset of 13 trees, limiting generalization claims
- Domain gap between rendered and real images suggests potential generalization challenges to other orchard environments

## Confidence
- **High confidence:** The 99.6% annotation reduction claim and the positive correlation between occlusion rate and pose estimation accuracy are well-supported by the presented data
- **Medium confidence:** The finding that training with ≤95% occluded fruits yields optimal performance, as this is based on a single dataset sweep without extensive hyperparameter tuning
- **Low confidence:** The orientation estimation failure is definitively demonstrated, but the underlying cause (data insufficiency vs. architectural limitation) remains speculative

## Next Checks
1. Test FRESHNet orientation estimation on synthetic data with known orientation distributions to determine if the failure stems from architectural limitations or data scarcity
2. Evaluate the pipeline on a different orchard dataset with varying lighting conditions and apple varieties to assess generalization and domain gap handling
3. Conduct ablation studies varying the number of manual annotations (e.g., 50, 200, 500) to determine the minimum annotation threshold needed for reliable orientation estimation