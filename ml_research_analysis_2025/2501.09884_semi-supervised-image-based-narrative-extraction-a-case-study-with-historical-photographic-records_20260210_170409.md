---
ver: rpa2
title: 'Semi-Supervised Image-Based Narrative Extraction: A Case Study with Historical
  Photographic Records'
arxiv_id: '2501.09884'
source_url: https://arxiv.org/abs/2501.09884
tags:
- narrative
- historical
- extraction
- visual
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a semi-supervised adaptation of the narrative
  maps algorithm for extracting visual narratives from historical photographic collections.
  The method combines deep learning-based feature extraction with expert-provided
  partial labels to identify coherent storylines in large image datasets.
---

# Semi-Supervised Image-Based Narrative Extraction: A Case Study with Historical Photographic Records

## Quick Facts
- arXiv ID: 2501.09884
- Source URL: https://arxiv.org/abs/2501.09884
- Reference count: 30
- Primary result: Semi-supervised adaptation of narrative maps algorithm extracts coherent storylines from historical photos, outperforming random sampling baselines especially for longer timelines

## Executive Summary
This paper presents a semi-supervised method for extracting visual narratives from historical photographic collections by combining deep learning feature extraction with expert-provided partial labels. The approach uses pre-trained DETR embeddings to compute visual similarities, propagates sparse thematic and temporal labels through label spreading, and solves a max-min coherence optimization to identify coherent storylines. Applied to the ROGER dataset of 1928 Sacambaya Expedition photographs, the method demonstrates improved performance over random sampling baselines, particularly for longer narrative timelines (10+ images). Expert evaluation confirms the historical accuracy and coherence of the extracted narratives.

## Method Summary
The method processes historical images by first extracting high-dimensional feature vectors using a pre-trained DETR model. These visual features are combined with location tags and passed through a label-spreading algorithm to propagate sparse expert-provided thematic and temporal labels across the dataset. A directed acyclic graph is constructed based on temporal ordering and coherence (visual and thematic similarity), and a Linear Program solves for the maximum coherence path subject to coverage constraints (minimum percentage of topics included). The resulting image sequence represents the extracted narrative.

## Key Results
- The semi-supervised approach outperforms random sampling baselines, particularly for longer narrative timelines (K > 10)
- Expert evaluation confirms historical accuracy and coherence of extracted narratives
- High-dimensional DETR embeddings yield better performance than low-dimensional UMAP representations for complex storylines
- Coverage constraints effectively prevent trivial or repetitive narratives

## Why This Works (Mechanism)

### Mechanism 1: Visual Semantic Coherence via DETR Embeddings
High-dimensional feature vectors from pre-trained vision transformers approximate semantic similarity between historical photographs, serving as a proxy for narrative connection. A DETR model extracts feature vectors, and cosine similarity between these vectors weights edges in a directed graph. The Narrative Maps algorithm solves for a path maximizing the coherence of the weakest edge (max-min coherence). This works because visual feature similarity in DETR latent space correlates positively with narrative relatedness in historical contexts.

### Mechanism 2: Semi-Supervised Manifold Approximation
Propagating sparse expert labels across the visual feature manifold structures the graph, allowing the algorithm to outperform random selection in data-sparse regimes. The system augments feature vectors with partial metadata and applies label-spreading to propagate thematic and temporal labels to unlabeled images based on manifold proximity. These probabilistic labels define topic clusters for coverage constraints. This works because the underlying data manifold is smooth, such that neighbors in feature space share thematic or temporal labels.

### Mechanism 3: Coverage-Constrained Path Optimization
Enforcing diversity via topic coverage constraints prevents trivial or repetitive storylines, particularly in longer timelines. The algorithm constructs a directed acyclic graph and solves an optimization problem to find a path maximizing coherence while satisfying a minimum coverage parameter. This works because a coherent narrative requires both sequential consistency (strong edges) and thematic diversity (broad coverage).

## Foundational Learning

- **Dynamic Time Warping (DTW)**
  - Why needed here: Primary evaluation metric measuring alignment between extracted non-linear narrative sequence and expert ground truth
  - Quick check question: If two sequences have same images but shifted positions, would Euclidean distance fail where DTW succeeds?

- **Label Spreading (Graph-based Semi-Supervised Learning)**
  - Why needed here: Core mechanism for handling partial metadata, generalizing expert dates/themes to full dataset
  - Quick check question: Does label spreading rely on hard classifier or proximity-based affinity matrix to propagate labels?

- **Narrative Maps Algorithm (Max-Min Coherence)**
  - Why needed here: Determines logic of Main Route Extraction, differentiating from standard pathfinding by optimizing weakest link
  - Quick check question: In max-min optimization, does adding single high-weight edge improve objective if weakest link remains unchanged?

## Architecture Onboarding

- **Component map:**
  Input -> Pre-trained DETR (Visual Features) -> Concatenation (Location Tags) -> Scikit-learn Label Spreading (Cluster Probabilities) -> DAG Construction -> Linear Programming (Max Coherence, Coverage Constraints) -> Ranked Image Sequence

- **Critical path:**
  Transition from High-Dimensional DETR embeddings to Semi-Supervised Clustering step. If label spreading fails to identify distinct themes (due to visual homogeneity), coverage constraints will force selection of irrelevant images, degrading narrative coherence.

- **Design tradeoffs:**
  - High-Dim vs. Low-Dim: High-dimensional embeddings yield better significance for longer timelines (p < 0.05) against random baselines
  - Coherence vs. Coverage: Increasing mincover parameter forces diversity but may force inclusion of images with lower edge weights

- **Failure signatures:**
  - Short-circuiting: Solver finds direct high-weight edge from Source to Target, bypassing complex narrative if K is too small
  - Topic Collapse: In visually similar datasets, algorithm may emulate class distribution rather than finding structured narrative

- **First 3 experiments:**
  1. Baseline Verification: Implement Random Sampling vs. Narrative Maps comparison on N=50 subset to verify NM achieves statistically significant lower DTW distances for K > 10
  2. Ablation on Dimensions: Run extraction pipeline using raw DETR embeddings vs. UMAP-reduced embeddings to reproduce finding that high-dim space preserves narrative nuance better
  3. Coverage Stress Test: Systematically vary mincover parameter (0% to 80%) on fixed timeline (K=20) to observe coherence degradation as coverage constraints tighten

## Open Questions the Paper Calls Out

- **Integrating multimodal data:** Could integrating textual metadata or captions improve narrative extraction accuracy compared to visual-only features? The paper suggests this could further enhance the process, but current methodology relies exclusively on visual features.

- **Domain-specific fine-tuning:** Can fine-tuning the visual feature extractor on historical datasets significantly improve capture of historical nuances compared to generic pre-trained models? The paper notes pre-trained embeddings may not fully capture historical photograph nuances.

- **Computational scalability:** How can computational complexity be reduced to scale efficiently for datasets larger than the 500-image case study? The paper identifies the linear program's computational complexity as a limitation for scaling to the full 47,000-image archive.

## Limitations
- Lacks explicit formulas for combining visual and thematic coherence in edge weights, requiring implementation assumptions
- No specification of DETR feature extraction layer or label spreading hyperparameters creates reproducibility variability
- Computational complexity of Linear Program solver for large graphs is acknowledged but not quantified

## Confidence
- **High confidence:** Core Narrative Maps algorithm using max-min coherence optimization is well-defined and theoretically sound
- **Medium confidence:** Semi-supervised label propagation approach is standard but effectiveness depends on manifold smoothness assumptions
- **Medium confidence:** Expert evaluation validates historical accuracy, though specific inter-rater reliability metrics are not reported

## Next Checks
1. Implement full pipeline (DETR + Label Spreading + Narrative Maps) and verify DTW distances improve significantly over random baselines for K > 10
2. Conduct ablation studies varying mincover parameter to observe trade-off between coverage and coherence degradation
3. Test algorithm on synthetic datasets with known ground truth to validate max-min coherence optimization produces semantically meaningful narratives rather than trivial paths