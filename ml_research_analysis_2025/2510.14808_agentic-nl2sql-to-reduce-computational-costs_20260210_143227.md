---
ver: rpa2
title: Agentic NL2SQL to Reduce Computational Costs
arxiv_id: '2510.14808'
source_url: https://arxiv.org/abs/2510.14808
tags:
- agent
- datalake
- information
- tables
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Datalake Agent, an agentic system that addresses
  the high computational costs of large-scale NL2SQL tasks by replacing direct solvers
  with an interactive reasoning loop. Instead of processing all database meta-information
  in a single prompt, the LLM incrementally requests only the relevant schema details
  through a structured command interface.
---

# Agentic NL2SQL to Reduce Computational Costs

## Quick Facts
- arXiv ID: 2510.14808
- Source URL: https://arxiv.org/abs/2510.14808
- Reference count: 40
- Primary result: 87% token reduction and up to 8× cost savings via iterative schema querying

## Executive Summary
This paper tackles the high computational costs of large-scale text-to-SQL (NL2SQL) tasks by replacing monolithic LLM prompts with an interactive reasoning loop. Instead of flooding the model with all database metadata upfront, the proposed Datalake Agent incrementally requests only the relevant schema details through a structured command interface. Evaluated on 23 databases (319 tables) with 100 manually curated table question answering tasks, the method reduces token usage by up to 87% while maintaining competitive accuracy—even improving performance on complex, multi-table queries. The approach significantly lowers operational costs (e.g., up to 8× cheaper at 319 tables) and scales efficiently with database size.

## Method Summary
Datalake Agent uses GPT-4 Mini to answer natural language questions over large collections of database tables via an iterative command-based reasoning loop. Instead of a single prompt with all schema, the LLM requests specific metadata (database description, tables, columns, or final SQL) using four structured commands. The agent processes each request and returns the relevant data, allowing the LLM to progressively build its understanding. Experiments across three scaling settings (42, 159, 319 tables) show up to 87% token reduction and up to 8× cost savings versus a direct prompt baseline, with accuracy improvements on complex multi-table queries.

## Key Results
- 87% reduction in token usage compared to direct prompt baseline
- Up to 8× cheaper in operational costs at 319 tables
- Maintained or improved accuracy, especially on complex, multi-table queries

## Why This Works (Mechanism)
The agentic approach works by decomposing the NL2SQL task into an interactive loop where the LLM requests only the schema information it needs at each reasoning step. This incremental querying avoids overwhelming the model with irrelevant metadata, reducing prompt size and focusing attention on relevant tables and columns. The structured command interface ensures consistent communication and efficient schema retrieval. This targeted interaction not only saves tokens but also allows the model to refine its understanding progressively, leading to better accuracy on complex queries.

## Foundational Learning
- **Natural Language to SQL (NL2SQL)**: Translating user questions into executable SQL queries. Needed because the core task is automating database querying via language.
- **Prompt Engineering / Few-shot Learning**: Crafting input prompts to elicit desired model behavior. Critical for designing the agent's command interface and baseline prompts.
- **Token Economy in LLMs**: Understanding that cost and latency scale with input/output token count. Central to motivating the iterative approach.
- **Schema-aware Querying**: Using database metadata (tables, columns, types) to inform query generation. Essential for the agent's command-driven schema retrieval.
- **Iterative Reasoning Loops**: Structuring problem-solving as a sequence of requests and responses. The main innovation enabling cost reduction and accuracy gains.

## Architecture Onboarding
- **Component Map**: User Question -> Datalake Agent (with commands) -> LLM (GPT-4 Mini) <-> Database Schema/SQL (via commands) -> Final SQL Answer
- **Critical Path**: Question → Agent Command → Schema Retrieval → LLM Response → Next Command (loop) → Final SQL
- **Design Tradeoffs**: Iterative querying saves tokens but risks infinite loops or missing relevant tables; solved with a 10-request cutoff and structured commands.
- **Failure Signatures**: Repeated schema requests (loop risk), failure to identify correct tables (accuracy drop), or overly verbose commands (token bloat).
- **First Experiments**: (1) Run agent and baseline on simple single-table queries; (2) Measure token usage and accuracy at each database scale (42/159/319); (3) Log command sequences to detect loop occurrences.

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Potential for infinite reasoning loops, mitigated by forced SQL output after 10 requests
- Evaluation limited to GPT-4 Mini and a single NL2SQL dataset
- Simulated database schemas not fully specified, limiting exact reproduction

## Confidence
- **High confidence** in token reduction and cost savings (well-supported by controlled experiments)
- **Medium confidence** in accuracy improvements and scalability claims (plausible but without ablation studies)
- **Low confidence** in robustness to prompt variations, other LLMs, and real-world schema complexity

## Next Checks
1. Reproduce token savings and accuracy on simple queries across all three database scales
2. Log command sequences to verify loop prevention and analyze causes of missing tables
3. Benchmark cost savings with actual API pricing at 319 tables