---
ver: rpa2
title: Can Large Language Models Match the Conclusions of Systematic Reviews?
arxiv_id: '2505.22787'
source_url: https://arxiv.org/abs/2505.22787
tags:
- llama
- higher
- lower
- effect
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are increasingly explored for automating
  systematic reviews (SRs) in medicine, yet their ability to match expert conclusions
  when given the same studies remains unclear. To address this, researchers created
  MedEvidence, a benchmark of 284 questions derived from 100 open-access SRs across
  10 medical specialties.
---

# Can Large Language Models Match the Conclusions of Systematic Reviews?

## Quick Facts
- arXiv ID: 2505.22787
- Source URL: https://arxiv.org/abs/2505.22787
- Reference count: 40
- Large language models achieve only 60-62% accuracy on systematic review conclusion matching tasks, far below expert level.

## Executive Summary
This paper evaluates whether large language models can replicate expert conclusions from systematic reviews when given the same source studies. The authors create MedEvidence, a benchmark of 284 questions derived from 100 Cochrane reviews across 10 medical specialties. Despite testing 24 models including reasoning and non-reasoning variants ranging from 7B to 700B parameters, even the best models achieve only 60-62% accuracy. All models struggle with uncertain evidence and fail to apply scientific skepticism toward low-quality studies, indicating current LLMs cannot reliably replace expert systematic reviewers.

## Method Summary
The study evaluates LLMs using MedEvidence, a benchmark of 284 questions from 100 Cochrane systematic reviews across 10 medical specialties. Each question asks whether a treatment outcome is higher, lower, no difference, uncertain effect, or insufficient data compared to a control. Models are evaluated zero-shot with two prompting strategies: basic and expert-guided (incorporating GRADE-style evidence assessment). The evaluation uses exact match accuracy on a 5-class taxonomy, with bootstrap confidence intervals. Source articles are included when available under CC-BY licenses; otherwise, abstracts are used.

## Key Results
- Top models (DeepSeek V3, GPT-4.1) achieve only 60-62% accuracy, far below expert performance
- Model performance degrades significantly with longer inputs and beyond 70B parameters
- Medical fine-tuned models often perform worse than base models
- All models systematically avoid the "uncertain effect" class, showing overconfidence in conclusions
- Models fail to apply scientific skepticism to low-quality studies with small sample sizes or wide confidence intervals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model accuracy improves monotonically as evidence certainty and source concordance increase.
- Mechanism: LLMs succeed when individual sources align with correct conclusions but fail when expert-level methodological critique is required to override apparent findings.
- Core assumption: Models rely on surface-level evidence synthesis rather than domain-specific quality heuristics.
- Evidence anchors:
  - All models struggle with uncertain evidence and fail to apply scientific skepticism toward low-quality studies
  - DeepSeek V3 achieves 92.45% accuracy at 100% source agreement vs. 41.21% at 0% source agreement
  - Related work finds models struggle with study-level inference when deeper methodological assessment is required
- Break condition: If models are explicitly prompted with expert-level quality assessment criteria, this performance gap should narrow.

### Mechanism 2
- Claim: Medical domain fine-tuning degrades performance on multi-source reasoning tasks.
- Mechanism: Fine-tuning on medical knowledge may overfit models to recall patterns rather than reasoning patterns, narrowing response distribution and reducing calibration.
- Core assumption: Degradation is due to overfitting to recall tasks rather than insufficient fine-tuning data quality.
- Evidence anchors:
  - Knowledge-based fine-tuning degrades accuracy on MedEvidence
  - Across all comparisons, medical fine-tuning fails to improve performance and actually degrades it
- Break condition: If fine-tuning explicitly targets multi-document reasoning with calibrated uncertainty, degradation should reverse.

### Mechanism 3
- Claim: Reasoning-augmented models do not consistently outperform non-reasoning models.
- Mechanism: Additional test-time compute for reasoning may amplify overconfidence rather than correcting evidence quality assessment gaps.
- Core assumption: Reasoning traces do not contain specific methodological critique steps that experts apply.
- Evidence anchors:
  - Reasoning does not necessarily improve performance
  - DeepSeek V3 outperforming its reasoning counterpart, while LLaMA 3.3 70B distilled from DeepSeek R1 outperforms the base model
- Break condition: If reasoning models are explicitly prompted to follow GRADE-style protocols with forced uncertainty calibration, reasoning advantage should emerge.

## Foundational Learning

- Concept: **GRADE evidence certainty framework**
  - Why needed here: The benchmark uses expert-assigned evidence certainty levels derived from GRADE. Understanding why experts downgrade evidence is essential to interpreting model failures.
  - Quick check question: Given a study with n=87 infants and only 2 total events, would a GRADE assessment rate this as high-certainty or low-certainty evidence?

- Concept: **Source concordance vs. meta-analytic synthesis**
  - Why needed here: The paper introduces "source concordance"—whether individual sources agree with final conclusion. Models perform well when concordance is high, but experts can override individual study findings through meta-analysis.
  - Quick check question: If Study A shows RR=0.80 (95% CI: 0.65-0.98) and Study B shows RR=0.76 (95% CI: 0.55-1.04), what is appropriate conclusion if both have high risk of bias?

- Concept: **LLM overconfidence and calibration**
  - Why needed here: Models are reluctant to express uncertainty (Section 6, Appendix H)—they avoid "uncertain effect" class. Understanding RLHF's role in amplifying overconfidence explains this systematic bias.
  - Quick check question: A model assigns 90% confidence to "no difference" when correct answer is "uncertain effect." Is this recall failure or calibration failure?

## Architecture Onboarding

- Component map: **MedEvidence dataset** (284 questions from 100 Cochrane SRs) -> **Evaluation pipeline** (zero-shot prompting -> rationale + answer extraction -> exact match accuracy) -> **Expert-guided prompt variant** (two-stage summarization -> synthesis) -> **Context handling** (LangChain RefineDocumentsChain for long inputs)

- Critical path: 1) Load question + source articles (full-text if CC-BY, else abstract) 2) Apply prompt template (basic or expert-guided) 3) Extract answer field, validate against 5-class taxonomy 4) Compute accuracy stratified by evidence certainty, source concordance, token length, treatment outcome class

- Design tradeoffs:
  - **Closed-form QA vs. open-ended synthesis**: Trades ecological validity for scalable evaluation; limits insight into why models fail
  - **Zero-shot evaluation**: Captures "natural" model behavior but may underestimate capability with domain-specific prompting
  - **Abstract-only fallback**: Increases dataset size but reduces signal (Appendix N shows frontier models benefit from full-text)

- Failure signatures:
  - **Invalid output rate**: Small/medically-tuned models (OpenBioLLM 8B, HuatuoGPT-o1 7B) produce invalid answers >40% of time
  - **Uncertainty avoidance**: Confusion matrices show near-zero recall for "uncertain effect" class across all models
  - **Context length degradation**: Performance drops monotonically with token count for models <100B parameters

- First 3 experiments:
  1. **Establish baseline on subset**: Evaluate 3 representative models on 50 questions stratified by evidence certainty to reproduce monotonic accuracy trend
  2. **Isolate uncertainty calibration**: Force models to assign confidence scores before selecting answer class; measure whether this shifts predictions toward "uncertain effect" when evidence is weak
  3. **Ablate source concordance**: For questions where source concordance <50%, test whether explicitly highlighting methodological limitations improves accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be explicitly trained or prompted to apply "scientific skepticism" to down-weight or discount findings from studies with high risk of bias or low sample sizes?
- Basis in paper: The authors state that "contrary to human experts, all models show a lack of scientific skepticism toward low-quality findings" and struggle to remain skeptical of results derived from flawed study designs.
- Why unresolved: Current models treat all provided input context as equally valid evidence, failing to replicate expert behavior of discounting statistically significant but methodologically weak results.
- What evidence would resolve it: Development of training methodology or prompting strategy that results in significantly higher accuracy on "very low" or "low" certainty evidence subset.

### Open Question 2
- Question: Why does medical domain fine-tuning degrade performance on evidence synthesis tasks compared to generalist base models?
- Basis in paper: The results show that "knowledge-based fine-tuning degrades accuracy on MedEvidence," with medically fine-tuned models performing worse than base counterparts.
- Why unresolved: Authors suggest fine-tuning without proper calibration harms generalization, but specific mechanism by which medical knowledge injection harms ability to reason over provided context remains unclear.
- What evidence would resolve it: Study analyzing internal attention mechanisms of fine-tuned vs. base models to determine if fine-tuning causes model to ignore provided context in favor of embedded prior knowledge.

### Open Question 3
- Question: Can LLMs be calibrated to reliably distinguish between "no difference" (definitive null result) and "uncertain effect" (inconclusive evidence) rather than defaulting to overconfident conclusions?
- Basis in paper: The authors note that "models are generally reluctant to express uncertainty, often committing to a more certain outcome that appears plausible," leading to poor performance on "uncertain effect" class.
- Why unresolved: Models appear biased toward definitive answers and struggle with nuance required to identify when evidence is insufficient to support a conclusion.
- What evidence would resolve it: Significant increase in recall for "uncertain effect" class in future model iterations, demonstrating ability to withhold judgment when confidence intervals are wide or data is conflicting.

### Open Question 4
- Question: Do "reasoning" capabilities (e.g., Chain of Thought) fail to improve systematic review synthesis because they rationalize incorrect priors rather than strictly weighing external evidence?
- Basis in paper: The authors find that "reasoning does not necessarily improve performance," noting that reasoning models like DeepSeek R1 did not outperform non-reasoning counterparts.
- Why unresolved: Unclear if "reasoning" traces serve to genuinely analyze provided studies or merely construct plausible-sounding justification for pre-determined (and often incorrect) answer derived from model's training data.
- What evidence would resolve it: Comparative analysis of reasoning traces in models that correctly vs. incorrectly synthesize conflicting evidence to see if logic is faithful to input text or hallucinated.

## Limitations
- Ecological validity gap: Closed QA format may underestimate true model capability since SRs involve iterative evidence gathering, not single-shot reasoning
- Prompt engineering variance: Study uses only two fixed prompts without exploring full prompt space or testing different strategies like chain-of-thought
- Context window constraints: LangChain RefineDocumentsChain is used for long inputs but chunk size, overlap, and intermediate prompt strategies are unspecified

## Confidence

**High confidence**: The core finding that LLMs struggle with uncertain evidence and fail to apply scientific skepticism toward low-quality studies is directly supported by accuracy breakdowns and confusion matrices showing systematic overconfidence and near-zero recall of the "uncertain effect" class.

**Medium confidence**: The claim that medical fine-tuning degrades performance is statistically supported but the mechanism (overfitting vs. insufficient fine-tuning data quality) remains unclear without deeper analysis.

**Medium confidence**: The finding that larger models show diminishing returns beyond 70B parameters is clear but the underlying reason (optimization dynamics vs. data scaling) is not established.

## Next Checks

1. **Prompt ablation study**: Test whether explicit GRADE-style reasoning prompts (e.g., "First, assess risk of bias; then, evaluate confidence in effect estimates") improve performance on low-certainty questions. This would validate whether the gap is missing reasoning capability vs. missing information.

2. **Context length sensitivity**: Systematically vary chunk sizes and overlap in the RefineDocumentsChain to measure how much information loss affects accuracy. This would clarify whether context window handling is a primary bottleneck.

3. **Calibration intervention**: Force models to output confidence scores before selecting answer classes, then retrain a simple logistic regression to map confidence to uncertainty. This would test whether calibration can partially correct the systematic overconfidence observed.