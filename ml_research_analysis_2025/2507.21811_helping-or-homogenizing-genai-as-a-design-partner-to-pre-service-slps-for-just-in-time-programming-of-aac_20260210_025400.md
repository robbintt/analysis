---
ver: rpa2
title: Helping or Homogenizing? GenAI as a Design Partner to Pre-Service SLPs for
  Just-in-Time Programming of AAC
arxiv_id: '2507.21811'
source_url: https://arxiv.org/abs/2507.21811
tags:
- hotspots
- vsds
- communication
- prototype
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether generative AI can assist non-experts,
  specifically pre-service speech-language pathologists, in creating effective visual
  scene displays (VSDs) for augmentative and alternative communication (AAC). The
  authors developed a prototype that automatically suggests hotspots for images using
  a multimodal large language model.
---

# Helping or Homogenizing? GenAI as a Design Partner to Pre-Service SLPs for Just-in-Time Programming of AAC

## Quick Facts
- arXiv ID: 2507.21811
- Source URL: https://arxiv.org/abs/2507.21811
- Reference count: 40
- Pre-service SLPs created VSDs faster and with greater confidence using AI assistance, but produced more homogeneous and less socially-focused AAC configurations

## Executive Summary
This study explores whether generative AI can assist non-experts in creating effective visual scene displays (VSDs) for augmentative and alternative communication (AAC). The authors developed a prototype that automatically suggests hotspots for images using a multimodal large language model. In a user study, participants created VSDs faster and with greater confidence when using the AI-assisted prototype compared to existing software. However, the quality of VSDs was mixed: while hotspots were more developmentally appropriate, participants used too many hotspots and included less socially-focused vocabulary. The study also found participants relied heavily on AI suggestions and created more homogeneous VSDs, raising concerns about personalization in AAC device design.

## Method Summary
The authors developed a prototype web application that uses OpenAI's GPT-4o to automatically generate hotspots for VSDs. The system takes an image and text prompt as input, then returns a ranked list of suggested hotspots with labels. Participants (pre-service SLPs) either used this prototype or existing software to create VSDs for two images. The prototype group received AI-generated suggestions they could edit, delete, or add to. Quality metrics included hotspot count, relevance, and social communication function analysis using Light's framework. Homogeneity was measured using semantic embeddings (cosine similarity with `all-MiniLM-L6-v2`).

## Key Results
- Participants created VSDs significantly faster with the prototype (1.75 min vs 3.55 min)
- User confidence was significantly higher for the prototype (4.14 vs 3.23 on 5-point scale)
- Prototype VSDs had more developmentally appropriate hotspots but fewer socially-focused vocabulary options
- VSDs created with the prototype were significantly more homogeneous than those created manually

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Vision-Language Grounding for Scaffolding
A multimodal LLM (GPT-4o) can generate an initial set of contextually relevant hotspots from an image, reducing time and cognitive load for non-experts programming VSDs. The model identifies objects and activities, providing an immediate starting point. This mechanism degrades if the model hallucinates objects or focuses on irrelevant background details instead of primary activities.

### Mechanism 2: Expert-in-the-Loop Moderation for Personalization
The system presents AI-generated hotspots in a fully editable interface, allowing users to delete irrelevant suggestions, edit text, and manually add personalized content. This hybrid approach aims to combine AI efficiency with human expertise. However, automation bias can break this mechanism when users trust the system too much and perform minimal edits.

### Mechanism 3: Prompt-Based Contextualization for Relevance
A carefully engineered prompt guides the multimodal model to generate vocabulary aligned with communication goals of emergent communicators. The prompt instructs the model to focus on objects/activities and consider building engagement. This can lead to homogenization when a generic prompt yields similar vocabulary across diverse images and users.

## Foundational Learning

- **Visual Scene Displays (VSDs) vs. Grid Displays**
  - Why needed here: VSDs are the core technology. Understanding their design principles is essential to see why quality and homogenization findings are problematic.
  - Quick check question: How does a VSD differ from a traditional grid-based AAC display, and for which type of communicator is it particularly suited?

- **Just-in-Time (JIT) Programming in AAC**
  - Why needed here: The prototype's primary value proposition is supporting JIT programming by reducing configuration time. This context is key to interpreting efficiency results.
  - Quick check question: What is the primary goal of Just-in-Time programming for AAC devices, and what is a major barrier to its widespread use?

- **Automation Bias & Over-reliance**
  - Why needed here: The study's key negative finding is user over-reliance on AI suggestions, which directly undermines the personalization mechanism.
  - Quick check question: What is automation bias, and how did it manifest in the study's participants when using the prototype?

## Architecture Onboarding

- **Component map:**
  1. Frontend (Tablet Web App): Built in ReactJS, manages image upload and hotspot editing
  2. Backend/AI Service: Sends image and prompt to OpenAI's GPT-4o API, receives and parses hotspot suggestions
  3. User State: Stateless design for single configuration sessions without long-term profiles

- **Critical path:**
  1. Image Input: User captures or uploads an image
  2. AI Inference: Image sent to GPT-4o model with predefined prompt to generate potential hotspots
  3. Suggestion Display: Returned hotspots shown in editing interface
  4. Curation (The Failing Step): User reviews, edits, and personalizes suggestions
  5. Configuration: User draws bounding regions for final hotspots
  6. Preview: User sees final interactive VSD

- **Design tradeoffs:**
  1. Static vs. Personalized Prompting: Generic prompt improves generalizability but contributes to homogenization
  2. Efficiency vs. Quality: AI suggestions reduce creation time but decrease quality measures
  3. Scaffolding vs. Replacement: AI intended to scaffold expert but often treated as finished product

- **Failure signatures:**
  1. Homogenization: Configured VSDs become semantically similar, detected via semantic embedding analysis
  2. Visual/Cognitive Overload: AI generates too many hotspots (averaging 5 vs recommended 2-4), overwhelming end-users
  3. Loss of Social Function: Generated vocabulary focuses almost exclusively on information transfer (98.7%), lacking social options

- **First 3 experiments:**
  1. Test if adding simple user profile to prompt reduces homogenization without privacy burden
  2. Modify UI to require explicit acknowledgment before finalizing to combat over-reliance
  3. Implement constraint to limit suggested hotspots to 3-4 and evaluate quality improvements

## Open Questions the Paper Calls Out

### Open Question 1
Can interface-level guardrails or cognitive forcing functions reduce over-reliance on AI suggestions when non-experts configure VSDs? The authors note future work is necessary to add interactive features supporting high-quality VSD configuration, as over-reliance (61.80% acceptance rate) undermines personalization.

### Open Question 2
How can AI-assisted AAC systems mitigate the homogenization of communication options while preserving efficiency gains? The authors state more research is needed to understand applying potential solutions to AI-enabled AAC devices regarding homogenization mitigation.

### Open Question 3
Do less expert communication partners (parents, caregivers) exhibit even stronger over-reliance and homogenization effects than pre-service SLPs? The authors hypothesize trends would be more apparent with these partners, as only pre-service SLPs were studied.

### Open Question 4
Can prompt engineering or multi-step generation increase social-focused vocabulary in AI-suggested hotspots? The study found only 0.7% of prototype hotspots addressed social closeness vs. 5.3% manually, with authors noting this might be addressed by refining prompts.

## Limitations

- Sample size (n=30) and single-session design may not capture long-term effects on AAC programming quality
- Tested only with pre-service SLPs rather than actual AAC users or caregivers, limiting ecological validity
- Automated hotspot generation occasionally produced irrelevant suggestions, though error rate wasn't precisely quantified

## Confidence

- **High Confidence**: AI prototype reduces configuration time and increases user confidence (measured directly in user study)
- **Medium Confidence**: AI suggestions lead to more developmentally appropriate hotspots (quality metric shows improvement)
- **Low Confidence**: Long-term impacts on AAC user outcomes and generalizability to actual AAC users (not tested)

## Next Checks

1. **Extended User Study**: Test the prototype with actual AAC users and caregivers over multiple sessions to assess whether homogenization effects persist and impact communication outcomes

2. **Prompt Personalization Experiment**: Evaluate whether adding minimal user profile information to prompts reduces homogenization while maintaining efficiency benefits

3. **Automated Quality Scoring**: Develop and validate an automated system to evaluate hotspot relevance and social communication function to enable larger-scale testing beyond manual coding limitations