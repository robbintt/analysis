---
ver: rpa2
title: Emoji-Based Jailbreaking of Large Language Models
arxiv_id: '2601.00936'
source_url: https://arxiv.org/abs/2601.00936
tags:
- prompts
- safety
- emoji
- emoji-based
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates emoji-based jailbreaking of large language
  models (LLMs), where emoji sequences are embedded in prompts to bypass safety mechanisms
  and elicit harmful outputs. Four open-source LLMs (Mistral 7B, Qwen 2 7B, Gemma
  2 9B, Llama 3 8B) were tested with 50 emoji-augmented prompts, evaluating jailbreak
  success rate, ethical compliance, and latency.
---

# Emoji-Based Jailbreaking of Large Language Models

## Quick Facts
- arXiv ID: 2601.00936
- Source URL: https://arxiv.org/abs/2601.00936
- Authors: M P V S Gopinadh; S Mahaboob Hussain
- Reference count: 24
- Primary result: Emoji sequences can jailbreak LLMs, with model-specific success rates from 0% to 10%

## Executive Summary
This study investigates emoji-based jailbreaking attacks on large language models, where emoji sequences embedded in prompts bypass safety mechanisms to elicit harmful outputs. Testing four open-source models (Mistral 7B, Qwen 2 7B, Gemma 2 9B, Llama 3 8B) with 50 emoji-augmented prompts revealed significant model-specific vulnerabilities. The research demonstrates that current safety filters fail to adequately handle non-textual adversarial inputs, with success rates ranging from complete resistance (Qwen 2 7B at 0%) to moderate vulnerability (Gemma 2 9B and Mistral 7B at 10%). A chi-square test confirmed significant inter-model differences (Ï‡Â² = 32.94, p < 0.001), highlighting inconsistent robustness across architectures.

## Method Summary
The study employed 50 emoji-based adversarial prompts tested across four open-source LLMs via Ollama API. Prompts used "emoji stuffing" (emojis between words) and "emoji chaining" (emoji sequences representing instructions). Responses were classified using keyword-based systems with 32 keywords per category (Successful, Partial, Failed), validated manually. Ethical compliance and latency were measured alongside jailbreak success rates. Models were tested locally on NVIDIA RTX 3050 hardware with Unicode NFC normalization applied to emoji sequences.

## Key Results
- Gemma 2 9B and Mistral 7B achieved 10% jailbreak success rates
- Llama 3 8B reached 6% success rate
- Qwen 2 7B showed complete resistance (0% success, 100% ethical compliance)
- Chi-square test confirmed significant inter-model differences (Ï‡Â² = 32.94, p < 0.001)

## Why This Works (Mechanism)

### Mechanism 1: Token Representation Overlap
Emojis map to internal representations overlapping with restricted concepts without triggering text-based filters. A knife emoji may have semantic overlap with "sword" or "cut" terms, bypassing explicit text detection when chained into sequences.

### Mechanism 2: Semantic Ambiguity Exploitation
Emoji stuffing and chaining create interpretive ambiguity that evades binary safe/unsafe classification. Models may partially fulfill restricted requests while misinterpreting intent as benign, resulting in "partial" compliance states.

### Mechanism 3: Architecture-Specific Tokenization Sensitivity
Different model architectures exhibit varying vulnerabilities based on how tokenizers and safety training handle non-linguistic tokens. Qwen 2 7B's perfect resistance suggests more robust emoji semantic mapping or stronger safety coverage in training data.

## Foundational Learning

- **Concept: Safety Alignment via RLHF/Safety Training**
  - Why needed: Understanding why models refuse requests and how adversarial inputs circumvent learned boundaries is essential for interpreting jailbreak mechanisms.
  - Quick check: Can you explain why a model trained on "helpful and harmless" data might still produce harmful outputs when prompted with emoji sequences?

- **Concept: Tokenization and Embedding Space**
  - Why needed: Emojis undergo tokenization before entering the model; their representation in embedding space determines semantic proximity to restricted concepts.
  - Quick check: How might a BPE tokenizer handle ðŸ—¡ï¸ differently from the word "knife," and what implications does this have for filter evasion?

- **Concept: Adversarial Prompt Engineering**
  - Why needed: Jailbreaking is a subset of adversarial attacks; understanding prompt-level manipulation strategies provides context for emoji-specific techniques.
  - Quick check: What distinguishes "prompt stuffing" from "emoji chaining" as attack strategies?

## Architecture Onboarding

- **Component map:**
  Input Layer: Unicode normalization (NFC) â†’ Tokenizer (model-specific BPE/SentencePiece) â†’ Token embeddings â†’ Safety Pipeline â†’ Model inference â†’ Post-generation classification â†’ Evaluation Loop

- **Critical path:**
  1. Emoji prompt design (stuffing/chaining strategies)
  2. Tokenization (emoji â†’ token IDs; fragmentation varies by model)
  3. Forward pass through transformer layers
  4. Safety filter evaluation (keyword matching in this study)
  5. Response classification (Successful/Partial/Failed)

- **Design tradeoffs:**
  - Keyword-based vs. semantic classification: This study used 32 keywords per category; more robust semantic classifiers would reduce false negatives but increase latency
  - Latency vs. safety depth: Gemma 2 9B showed highest latency (44.2s) and lowest compliance (66%), suggesting deeper processing may increase vulnerability without proper safety alignment
  - Local deployment (Ollama) vs. API: Local testing ensures reproducibility but may not reflect production system behaviors with additional safety layers

- **Failure signatures:**
  - Partial compliance responses: Ambiguous outputs that blend benign and restricted content indicate safety boundary confusion
  - High variance across models: Identical prompts producing 0% vs. 10% success rates indicates inconsistent emoji semantic handling
  - Latency outliers: Gemma 2 9B's high latency combined with low compliance suggests processing depth without corresponding safety robustness

- **First 3 experiments:**
  1. Tokenizer analysis: Submit identical emoji prompts to each model's tokenizer and compare token fragmentation patterns
  2. Embedding proximity test: Extract embeddings for emoji sequences and their textual equivalents; compute cosine similarity to identify models where emoji representations closely approach restricted term clusters
  3. Safety filter bypass validation: Submit the 50 prompts with emojis replaced by their textual equivalents to quantify the delta between emoji-based and text-based success rates per model

## Open Questions the Paper Calls Out
None

## Limitations
- Exact 50 prompts and their emoji substitutions are not provided, limiting reproducibility
- Specific classification keywords are not enumerated, creating uncertainty about classification consistency
- Testing conducted only on local deployment via Ollama, may not reflect production API behaviors with additional safety layers

## Confidence

**High Confidence (Evidence Anchors Present)**:
- Statistical significance of inter-model differences (Ï‡Â² = 32.94, p < 0.001)
- Qwen 2 7B's complete resistance (0% success rate, 100% ethical compliance)
- Existence of emoji-based jailbreaking vulnerabilities across multiple models

**Medium Confidence (Mechanisms Partially Supported)**:
- Token representation overlap as jailbreak mechanism
- Semantic ambiguity exploitation
- Architecture-specific tokenization sensitivity

**Low Confidence (Speculative or Unverified)**:
- Precise contribution of each jailbreak mechanism to success rates
- Claims about why Qwen 2 7B resists jailbreaking
- Relationship between latency and vulnerability

## Next Checks

1. **Tokenizer Fragmentation Analysis**: Submit the exact 50 emoji-based prompts to each model's tokenizer and document how emojis are fragmented into tokens. Compare fragmentation patterns with jailbreak success rates to identify correlations between tokenization strategy and vulnerability.

2. **Control Text Prompt Comparison**: Replace all emojis in the 50 prompts with their textual equivalents and rerun the jailbreak experiments. Compute the delta in success rates between emoji-based and text-based prompts for each model to isolate the specific contribution of emoji formatting to jailbreak effectiveness.

3. **Embedding Proximity Measurement**: Extract embeddings for successful emoji sequences and their textual equivalents from each model. Calculate cosine similarity to restricted concept clusters in embedding space to empirically test whether emoji representations indeed overlap with filtered concepts as hypothesized in the token representation mechanism.