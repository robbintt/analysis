---
ver: rpa2
title: Adaptive Insurance Reserving with CVaR-Constrained Reinforcement Learning under
  Macroeconomic Regimes
arxiv_id: '2504.09396'
source_url: https://arxiv.org/abs/2504.09396
tags:
- learning
- solvency
- reserving
- cvar
- reserve
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reinforcement learning (RL) framework for
  insurance reserving that integrates tail-risk sensitivity, macroeconomic regime
  modeling, and regulatory compliance. The reserving problem is formulated as a finite-horizon
  Markov Decision Process (MDP), in which reserve adjustments are optimized using
  Proximal Policy Optimization (PPO) subject to Conditional Value-at-Risk (CVaR) constraints.
---

# Adaptive Insurance Reserving with CVaR-Constrained Reinforcement Learning under Macroeconomic Regimes

## Quick Facts
- arXiv ID: 2504.09396
- Source URL: https://arxiv.org/abs/2504.09396
- Authors: Stella C. Dong; James R. Finlay
- Reference count: 40
- One-line primary result: RL-CVaR agent achieves superior performance relative to classical reserving methods across tail-risk control, capital efficiency, and regulatory violation rate

## Executive Summary
This paper introduces a reinforcement learning framework for insurance reserving that integrates tail-risk sensitivity, macroeconomic regime modeling, and regulatory compliance. The reserving problem is formulated as a finite-horizon Markov Decision Process (MDP), in which reserve adjustments are optimized using Proximal Policy Optimization (PPO) subject to Conditional Value-at-Risk (CVaR) constraints. To enhance policy robustness across varying economic conditions, the agent is trained using a regime-aware curriculum that progressively increases volatility exposure. The reward structure penalizes reserve shortfall, capital inefficiency, and solvency floor violations, with design elements informed by Solvency II and Own Risk and Solvency Assessment (ORSA) frameworks. Empirical evaluations on two industry datasets—Workers' Compensation, and Other Liability—demonstrate that the RL-CVaR agent achieves superior performance relative to classical reserving methods across multiple criteria, including tail-risk control (CVaR$_{0.95}$), capital efficiency, and regulatory violation rate. The framework also accommodates fixed-shock stress testing and regime-stratified analysis, providing a principled and extensible approach to reserving under uncertainty.

## Method Summary
The authors formulate insurance reserving as a finite-horizon MDP where the agent adjusts reserves over time to minimize risk and capital inefficiency while complying with solvency constraints. The policy is trained using Proximal Policy Optimization (PPO) with a reward function that incorporates penalties for reserve shortfall, CVaR of shortfalls, capital inefficiency, and solvency floor violations. Macroeconomic regime modeling is integrated through a curriculum learning approach, progressively exposing the agent to increasing volatility from calm to recessionary conditions. The solvency floor is dynamically adjusted based on volatility, and the framework supports fixed-shock stress testing and regime-stratified analysis.

## Key Results
- RL-CVaR agent achieves superior tail-risk control with CVaR$_{0.95}$ outperforming classical methods
- The agent demonstrates improved capital efficiency while maintaining lower regulatory violation rates
- Curriculum learning across macroeconomic regimes enhances policy robustness to economic shocks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CVaR-constrained reward shaping produces risk-averse reserving policies.
- Mechanism: The reward function penalizes Conditional Value-at-Risk (CVaR) of reserve shortfalls alongside capital inefficiency. By integrating CVaR into the loss function, the agent optimizes for worst-case tail events rather than just expected outcomes. This directly discourages under-reserving in adverse scenarios.
- Core assumption: The empirical estimation of CVaR during training accurately reflects the true tail-risk distribution, and the penalty weights are appropriately calibrated.
- Evidence anchors:
  - [abstract] "The reserving problem is formulated as a finite-horizon Markov Decision Process (MDP), in which reserve adjustments are optimized using Proximal Policy Optimization (PPO) subject to Conditional Value-at-Risk (CVaR) constraints."
  - [section] Section 3.2, Equation (2) defines the reward function with a CVaR penalty term: $\hat{CVaR}_t$.
  - [corpus] Weak. A related paper, "ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy Reinsurance Pricing," uses CVaR constraints for a related insurance problem, suggesting applicability but not confirming the mechanism's efficacy in reserving.
- Break condition: If CVaR estimates are highly unstable or biased during training (e.g., insufficient buffer size for tail sampling), the learned policy may not reflect true tail-risk aversion.

### Mechanism 2
- Claim: Curriculum learning across macroeconomic regimes improves policy robustness to economic shocks.
- Mechanism: Training progresses through a sequence of macroeconomic regimes (Calm → Moderate → Volatile → Recession) with increasing volatility. The agent first learns stable reserve management and then adapts these policies to handle progressively more extreme conditions. This graduated exposure prevents instability from early exposure to high-volatility noise.
- Core assumption: The defined macroeconomic regimes (modeled as Gaussian shocks with increasing variance) adequately represent the range of economic conditions the policy will encounter at deployment.
- Evidence anchors:
  - [abstract] "To enhance policy robustness across varying economic conditions, the agent is trained using a regime-aware curriculum that progressively increases volatility exposure."
  - [section] Section 3.3, Table 1 defines the four macroeconomic regimes with their associated Gaussian shock parameters.
  - [corpus] The paper "Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress" highlights that adversarial vulnerability is regime-dependent, supporting the intuition that stress-aware training is important, but does not validate the specific curriculum approach.
- Break condition: If real-world economic shocks follow a distribution significantly different from the Gaussian shocks used in training (e.g., fat-tailed, sudden "black swan" events), the policy's learned robustness may not transfer.

### Mechanism 3
- Claim: Explicit regulatory constraints in the reward function reduce solvency violations.
- Mechanism: A solvency floor ($R^{reg}_t$), which is volatility-adjusted, is incorporated directly into the reward function. A binary penalty is applied when reserves fall below this floor ($\lambda_4 \cdot I[R_t < R^{reg}_t]$). This creates a direct, negative feedback signal for any breach of regulatory capital requirements.
- Core assumption: The defined volatility-adjusted solvency floor accurately represents the true regulatory capital requirement, and the penalty weight ($\lambda_4$) is sufficient to force compliance without creating an overly conservative policy.
- Evidence anchors:
  - [abstract] "The reward structure penalizes reserve shortfall, capital inefficiency, and solvency floor violations, with design elements informed by Solvency II and Own Risk and Solvency Assessment (ORSA) frameworks."
  - [section] Section 3.2, Equation (2) defines the solvency floor $R^{reg}_t = 0.4 + 0.2 \cdot V_t$ and its associated penalty.
  - [corpus] The framework explicitly references Solvency II and ORSA, common regulatory standards, but there is no corpus evidence confirming the specific formulation of the solvency floor penalty.
- Break condition: If the regulatory landscape changes (e.g., new capital adequacy rules), the fixed formula for the solvency floor would become outdated and require retraining or reward function modification.

## Foundational Learning

- Concept: **Markov Decision Process (MDP)**
  - Why needed here: The paper frames insurance reserving as a sequential decision-making problem. An MDP provides the formal structure of state, action, transition probabilities, and reward, which is a prerequisite for applying any reinforcement learning algorithm.
  - Quick check question: Can you define the state and action space for the reserving agent described in the paper?

- Concept: **Conditional Value-at-Risk (CVaR)**
  - Why needed here: This is the core risk measure used to penalize tail risk in the reward function. Understanding that CVaR represents the expected loss in the worst-case quantile is crucial for interpreting the agent's objective.
  - Quick check question: What does a CVaR$_{0.95}$ value of 1.5 million indicate, and how does it differ from a Value-at-Risk (VaR) at the same confidence level?

- Concept: **Proximal Policy Optimization (PPO)**
  - Why needed here: PPO is the specific policy-gradient algorithm used for training. It's a standard, stable on-policy RL algorithm. Knowing this helps in understanding the training dynamics and hyperparameters.
  - Quick check question: What is the primary role of the "clipping" mechanism in PPO, and why does it contribute to training stability?

## Architecture Onboarding

- Component map:
  - Environment (Gymnasium-based) -> State Vector -> Policy Network -> Action -> Reward Module -> PPO Agent

- Critical path:
  1. **Environment Step:** The PPO agent samples an action, and the environment steps forward, updating claims, reserves, and macro conditions.
  2. **CVaR Estimation:** At each step, the current shortfall is added to a buffer. The 90-95th percentile (VaR) is estimated from this buffer, and the mean of values above it forms the CVaR penalty.
  3. **Reward Computation:** All penalty terms are combined into the final reward, which is then used by PPO to update the policy.

- Design tradeoffs:
  - **Gaussian Shocks:** Simple to implement but may not capture fat-tailed real-world economic shocks (Assumption: real shocks are Gaussian-like).
  - **Empirical CVaR:** Non-parametric and adaptive, but noisy compared to an analytical gradient. Requires a sufficiently large buffer (`shortfall_buffer_size=1024`) for stable estimates.
  - **Fixed Solvency Formula:** The volatility-adjusted floor ($0.4 + 0.2 \cdot V_t$) is interpretable but may not reflect complex, real regulatory rules.

- Failure signatures:
  - **Training Instability:** Divergent loss, NaN rewards. *Potential cause:* Incorrect reward normalization or extreme values from CVaR calculation.
  - **Conservative Policy:** Agent holds excessive reserves, maximizing safety but minimizing capital efficiency. *Potential cause:* `lambda_4` (solvency penalty) or `lambda_2` (CVaR penalty) is set too high.
  - **High Violation Rate:** Agent fails to meet the solvency floor, despite training. *Potential cause:* `lambda_4` is too low, or the CVaR quantile range (`alpha_t`) is not capturing the most relevant tail risk.

- First 3 experiments:
  1. **Overfitting/Generalization Check:** Train an agent on a single "Calm" regime and evaluate its performance on the "Recession" regime without retraining. This tests the necessity of the curriculum.
  2. **Ablation Study on Reward Components:** Train three agents: one with only shortfall penalty, one with shortfall + CVaR, and the full agent. Compare their CVaR$_{0.95}$ and Regulatory Violation Rate (RVR) to quantify the contribution of each term.
  3. **Hyperparameter Sensitivity:** Run a grid search over the penalty weights ($\lambda_1, \lambda_2, \lambda_3, \lambda_4$) to find the Pareto frontier between capital efficiency (CES) and solvency compliance (RVR).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can curriculum warm-starts or shared network layers enable effective transfer learning of the RL-CVaR agent across different insurance lines of business?
- Basis in paper: [explicit] Section 6.3 lists "Transferability Across Lines of Business" as a limitation, noting that agents are currently trained separately for each line.
- Why unresolved: The current implementation requires distinct training per line of business, leading to training redundancy and limited generalization across disparate claims structures.
- What evidence would resolve it: Successful fine-tuning of a pre-trained agent on a new line of business with fewer training steps and no catastrophic forgetting compared to training from scratch.

### Open Question 2
- Question: Does incorporating Bayesian actor-critic architectures or distributional RL improve policy robustness in low-data or high-volatility environments compared to the current point-estimate approach?
- Basis in paper: [explicit] Section 6.3 states the agent does not explicitly model uncertainty in policy or value estimates and suggests Bayesian or distributional methods as a future direction.
- Why unresolved: The current framework relies on deterministic policy estimates which may lack robustness when data is sparse or volatility is extreme.
- What evidence would resolve it: Empirical comparisons showing that a Bayesian variant maintains lower Regulatory Violation Rates (RVR) and variance in reserves during out-of-distribution stress tests.

### Open Question 3
- Question: How can multi-agent reinforcement learning extensions facilitate coordinated reserving across subsidiaries or regulatory groups operating under shared capital constraints?
- Basis in paper: [explicit] Section 6.3 identifies "Enterprise-Level Coordination" as a limitation, noting the current agent operates at the individual line level.
- Why unresolved: The single-agent focus prevents the optimization of aggregate capital buffers across multiple lines or entities, which is often required for enterprise-wide solvency management.
- What evidence would resolve it: A multi-agent simulation where agents collectively minimize group-level CVaR and capital inefficiency better than independent single-agent baselines.

## Limitations

- The Gaussian shock model for macroeconomic regimes may not capture real-world fat-tailed or regime-switching dynamics observed in financial markets.
- The CVaR estimation relies on empirical quantile estimation, which can be unstable with limited buffer sizes and may not reflect true tail risk during deployment.
- The solvency floor formula (0.4 + 0.2 · V_t) is a simplified, fixed formulation that may not reflect evolving regulatory requirements or complex jurisdictional rules.

## Confidence

- **High confidence**: The core RL formulation as an MDP with PPO optimization is standard and well-validated in the literature. The general architecture connecting state, action, reward, and policy update is sound.
- **Medium confidence**: The specific reward function design (penalty weights, CVaR buffer size, solvency floor parameters) is supported by domain knowledge and references to Solvency II/ORSA, but lacks empirical validation of these specific choices. The curriculum learning approach is intuitively reasonable but not proven optimal.
- **Low confidence**: The claim that this framework generalizes well to unseen macroeconomic regimes beyond the four defined training regimes. Real-world economic shocks may exhibit patterns (black swan events, structural breaks) not captured in the training distribution.

## Next Checks

1. **Robustness to Shock Distribution**: Evaluate the trained agent on macroeconomic shocks drawn from a fat-tailed distribution (e.g., Student's t-distribution) rather than the Gaussian assumption used in training to test generalization.
2. **Dynamic Regulatory Adaptation**: Modify the solvency floor formula mid-training to simulate a regulatory change, and measure the agent's ability to adapt without catastrophic performance degradation.
3. **Cross-Product Generalization**: Train the agent on one insurance line (e.g., Workers' Compensation) and evaluate its performance on a different line (e.g., Motor Insurance) to assess domain transferability.