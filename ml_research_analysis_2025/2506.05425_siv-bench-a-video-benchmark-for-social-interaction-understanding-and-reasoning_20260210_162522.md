---
ver: rpa2
title: 'SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning'
arxiv_id: '2506.05425'
source_url: https://arxiv.org/abs/2506.05425
tags:
- social
- video
- arxiv
- siv-bench
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SIV-Bench is a novel video benchmark designed to evaluate Multimodal
  Large Language Models'' capabilities in understanding and reasoning about complex
  social interactions across three dimensions: Social Scene Understanding (SSU), Social
  State Reasoning (SSR), and Social Dynamics Prediction (SDP). The benchmark comprises
  2,792 videos and 8,792 high-quality question-answer pairs derived from a human-LLM
  collaborative pipeline, covering diverse social relationships, video genres, and
  cultural backgrounds.'
---

# SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning

## Quick Facts
- arXiv ID: 2506.05425
- Source URL: https://arxiv.org/abs/2506.05425
- Reference count: 40
- Primary result: Benchmark shows MLLMs excel at Social Scene Understanding but struggle significantly with Social State Reasoning and Social Dynamics Prediction, particularly in Relation Inference tasks

## Executive Summary
SIV-Bench is a novel video benchmark designed to evaluate Multimodal Large Language Models' capabilities in understanding and reasoning about complex social interactions across three dimensions: Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP). The benchmark comprises 2,792 videos and 8,792 high-quality question-answer pairs derived from a human-LLM collaborative pipeline, covering diverse social relationships, video genres, and cultural backgrounds. Experiments on leading MLLMs show strong performance on SSU but significant challenges with SSR and SDP, where Relation Inference is particularly difficult. Transcribed dialogue consistently aids comprehension, while subtitle removal typically hinders performance. The benchmark provides critical insights into current model limitations and offers a foundation for advancing socially intelligent AI.

## Method Summary
SIV-Bench evaluates Multimodal Large Language Models across three social interaction understanding dimensions using 2,792 videos (average 32.49s) from TikTok and YouTube, each with three subtitle conditions: original, +sub (transcribed dialogue added via Whisper-large-v3), and -sub (on-screen text removed). The benchmark includes 8,792 multiple-choice QA pairs covering 10 fine-grained sub-tasks. Evaluation uses the VLMEvalKit framework with standardized prompts, implementing a two-stage answer parsing approach that first checks for option letters (A-E) then falls back to similarity-based text matching. Models are tested with default inference parameters, with video-capable models receiving raw video and image-only models receiving 16 uniformly sampled frames.

## Key Results
- MLLMs achieve strong performance on Social Scene Understanding but struggle significantly with Social State Reasoning and Social Dynamics Prediction
- Relation Inference sub-task is consistently the most challenging across all evaluated models
- Transcribed dialogue consistently improves model performance, while subtitle removal typically degrades it
- Qwen2.5-VL-72B achieves highest overall accuracy at 66.40%, followed by GPT-4o at 60.23%

## Why This Works (Mechanism)
SIV-Bench works by systematically evaluating MLLMs' ability to understand social interactions through structured video content with controlled subtitle conditions. The benchmark's effectiveness stems from its comprehensive coverage of social scenarios across multiple dimensions and its use of human-LLM collaborative annotation to ensure high-quality question-answer pairs. By varying subtitle presence and testing diverse social relationships and cultural contexts, the benchmark reveals specific strengths and weaknesses in current MLLM architectures.

## Foundational Learning
- Social Scene Understanding: Models must identify visual elements, relationships, and social roles in scenes. Needed because accurate scene comprehension forms the foundation for higher-level social reasoning.
- Social State Reasoning: Models need to infer mental states, emotions, and intentions from social cues. Critical for understanding why characters behave as they do.
- Social Dynamics Prediction: Models must forecast future social interactions and outcomes. Essential for demonstrating true social intelligence rather than just pattern recognition.

## Architecture Onboarding
- Component Map: Video input -> Preprocessing (subtitle manipulation) -> VLM processing -> Answer generation -> Two-stage parsing -> Accuracy calculation
- Critical Path: Video preprocessing → VLM inference → Answer parsing → Performance evaluation
- Design Tradeoffs: Uses multiple subtitle conditions to test dialogue importance, but this increases dataset complexity; employs human-LLM annotation pipeline for quality but adds cost
- Failure Signatures: Low Relation Inference scores indicate models confuse primary/secondary relationships, over-rely on scene cues, or miss commonsense reasoning
- First Experiments: 1) Test +sub vs -sub conditions on simple SSU tasks, 2) Evaluate SSR performance on emotion inference tasks, 3) Measure SDP accuracy on relationship prediction scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset accessibility is unclear—the paper doesn't specify access mechanism (direct download, request form, or licensing requirements)
- Model-specific configuration details are insufficient, including exact VLMEvalKit versions and inference parameters
- Answer parsing methodology lacks specific similarity metric thresholds and matching criteria
- The exact performance values for individual models cannot be independently verified without the complete dataset

## Confidence
- High Confidence: The benchmark's three-dimensional structure (SSU, SSR, SDP) and 10 sub-task taxonomy are clearly defined and reproducible
- Medium Confidence: The claim about transcribed dialogue consistently aiding comprehension while subtitle removal hindering performance is supported by the experimental design
- Low Confidence: The specific performance values reported for individual models across all sub-tasks cannot be fully validated without complete dataset and exact evaluation parameters

## Next Checks
1. Attempt to access the SIV-Bench dataset through the provided URL and document the exact access mechanism, file formats, and any usage restrictions
2. Implement the two-stage answer parsing method and validate it against sample questions, ensuring both letter-based and text-based matching work correctly
3. Document and test the exact VLMEvalKit configuration and inference parameters used for each evaluated model, verifying correct implementation of frame sampling strategies