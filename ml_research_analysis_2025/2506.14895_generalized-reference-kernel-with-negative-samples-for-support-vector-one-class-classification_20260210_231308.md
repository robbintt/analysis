---
ver: rpa2
title: Generalized Reference Kernel With Negative Samples For Support Vector One-class
  Classification
arxiv_id: '2506.14895'
source_url: https://arxiv.org/abs/2506.14895
tags:
- samples
- negative
- training
- kernel
- oc-svm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for small-scale one-class classification
  with limited negative samples. The approach, called GRKneg, uses One-class Support
  Vector Machine (OC-SVM) with a Generalized Reference Kernel that incorporates negative
  training samples as reference vectors.
---

# Generalized Reference Kernel With Negative Samples For Support Vector One-class Classification

## Quick Facts
- arXiv ID: 2506.14895
- Source URL: https://arxiv.org/abs/2506.14895
- Reference count: 25
- Primary result: GRKneg achieves 79.5-81.9% Gmean with 5-10 negative samples vs 73.7-75.0% for standard OC-SVM

## Executive Summary
This paper addresses the challenge of one-class classification when limited negative samples are available. The proposed GRKneg method improves the discriminative power of the standard OC-SVM by incorporating negative training samples as reference vectors in a Generalized Reference Kernel. The key innovation is generating synthetic negative samples from the distribution of available negative data and using both original and generated samples as reference points in kernel computation. This approach enhances separation between positive and negative classes without using labels in the optimization process. Experiments across 14 one-class classification tasks show GRKneg consistently outperforms standard OC-SVM and binary SVM when negative samples are scarce (5-10 samples), achieving average Gmean scores of 79.5-81.9% compared to 73.7-75.0% for OC-SVM and 70.0-76.4% for binary SVM.

## Method Summary
GRKneg modifies the OC-SVM by using a kernel that incorporates negative training samples as reference vectors. The method first estimates the mean and standard deviation of available negative samples, then generates synthetic samples from a normal distribution with these parameters. Both real and synthetic negative samples form the reference set. The Generalized Reference Kernel projects positive training samples onto this reference set using RBF kernel evaluations, centering, and eigendecomposition. The resulting kernel matrix captures discriminative information from the negative samples while preserving the one-class optimization framework. This allows standard OC-SVM implementations to leverage negative data implicitly through the kernel representation.

## Key Results
- GRKneg consistently outperforms standard OC-SVM and binary SVM when negative samples are scarce (5-10 samples)
- Average Gmean scores: 79.5-81.9% for GRKneg vs 73.7-75.0% for standard OC-SVM vs 70.0-76.4% for binary SVM
- Binary SVM performs better with larger numbers of negative samples (>30), showing expected crossover behavior
- Ion2 task shows dramatic improvement: OC-SVM baseline 25.3% vs GRKneg 65.7%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using negative samples as reference vectors in GRK creates a kernel that improves discriminative power for one-class classification.
- **Mechanism:** The GRK computes kernel values by projecting positive training samples onto a set of reference vectors. When reference vectors are drawn from the negative distribution (both real and generated), the resulting kernel representation implicitly encodes distance-from-negative information. This allows the standard OC-SVM to learn a boundary that better separates positives from negatives, even though labels never appear in the optimization objective.
- **Core assumption:** The available negative training samples are sufficiently representative of the true negative distribution to serve as useful reference points.
- **Evidence anchors:** [abstract] "uses both original and generated samples as reference vectors in the kernel computation. This improves the discriminative power of the kernel for positive samples without using labels in the model optimization" [Page 3, Fig. 1a discussion] "The variant 3 using only negative samples, even when there are only 5 of them, consistently outperforms the other variants. We can conclude that using positive training samples as reference vectors in GRK for OC-SVM is not recommended."
- **Break condition:** When negative training samples are highly unrepresentative of the true negative distribution, or when the negative class is extremely multi-modal with modes not captured in the available samples.

### Mechanism 2
- **Claim:** Generating synthetic negative samples from an approximated normal distribution amplifies the discriminative signal when few negatives are available.
- **Mechanism:** The method estimates mean (μ_neg) and standard deviation (σ_neg) from the N available negative samples, then generates P synthetic samples from N(μ_neg, σ²_neg). This augments the reference set, providing more complete coverage of the negative region in feature space without requiring additional labeled data.
- **Core assumption:** The negative class distribution can be reasonably approximated by a normal distribution with parameters estimated from very few samples.
- **Evidence anchors:** [Page 2, Algorithm 1] Explicitly shows sampling "M ∈ R^(D×P) ~ N(μ_neg, σ²_neg)" after computing mean and std from negative samples. [Page 4] "approximating the negative distribution with a normal distribution having the same mean and variance must be a really coarse approximation in most cases. We experimented also with some other ways to generate negative samples... However, we could not find anything better than the coarse normal distribution approximation."
- **Break condition:** When the true negative distribution is strongly non-Gaussian (heavy-tailed, highly skewed, or multi-modal), the coarse normal approximation may fail to capture structure, potentially degrading performance.

### Mechanism 3
- **Claim:** GRKneg achieves improved discrimination without modifying the OC-SVM optimization objective, preserving compatibility with standard implementations.
- **Mechanism:** The discriminative information from negative samples is encoded entirely within the kernel matrix K_PP. The OC-SVM solver receives this pre-computed kernel and performs standard one-class optimization. This means the method can use unmodified LIBSVM or similar implementations while gaining benefits from negative data.
- **Core assumption:** The standard OC-SVM objective can effectively leverage the negative-informed kernel representation without explicit negative constraints.
- **Evidence anchors:** [abstract] "It is worth noting that the proposed method does not use any labels in the model optimization but uses the original OC-SVM implementation. Only the kernel used in the process is improved using the negative data." [Page 2] "After forming the GRKneg kernel matrix, the original kernel OC-SVM implementation can be used for one-class classification."
- **Break condition:** When the positive class has complex internal structure that conflicts with the discriminative kernel's emphasis on negative separation, potentially fragmenting the positive region.

## Foundational Learning

- **Concept:** One-Class Support Vector Machine (OC-SVM)
  - **Why needed here:** GRKneg is a kernel modification specifically for OC-SVM; understanding the base algorithm (hyperplane through origin separating positives from origin in feature space, ν parameter controlling outlier fraction) is essential to grasp what the modified kernel changes.
  - **Quick check question:** In OC-SVM, what does the ν parameter control, and how does the algorithm create a decision boundary using only positive training samples?

- **Concept:** Radial Basis Function (RBF) Kernel
  - **Why needed here:** RBF is used as the base kernel κ̃ in Algorithm 1. Understanding how σ controls the locality of similarity (smaller σ = more local comparisons) is critical for hyperparameter selection and debugging.
  - **Quick check question:** How does the σ bandwidth parameter in RBF kernel affect the similarity measure, and what failure mode occurs if σ is set much too small or too large for your dataset?

- **Concept:** Kernel Matrix Centering and Eigendecomposition
  - **Why needed here:** Algorithm 1 centers the base kernel matrices and computes pseudoinverse via eigendecomposition of K̃_RR. Without understanding why centering removes bias and how pseudoinverse projects into the reference vector space, the matrix operations will seem arbitrary.
  - **Quick check question:** Why is centering necessary in kernel methods, and how does the pseudoinverse of the reference-reference kernel enable projection of positive samples into a representation defined by reference vectors?

## Architecture Onboarding

- **Component map:** P (D×P) -> μ_neg, σ_neg computation -> M (D×P) synthetic negatives -> R = [N; M] reference set -> K̃_RR, K̃_RP kernel matrices -> centering and eigendecomposition -> K_PP = K̃_PR × (K̃_RR)^+ × K̃_RP -> OC-SVM solver

- **Critical path:**
  1. Load P and N; standardize data with respect to positive training samples
  2. Estimate μ_neg, σ_neg from N
  3. Generate P synthetic samples M from estimated distribution
  4. Build reference set R = [N; M]
  5. Compute centered base kernels K̃_RR and K̃_RP using RBF with hyperparameter σ
  6. Eigendecompose K̃_RR; compute pseudoinverse (K̃_RR)^+ using non-zero eigenvalues only
  7. Form GRKneg: K_PP = K̃_PR × (K̃_RR)^+ × K̃_RP
  8. Invoke OC-SVM solver with K_PP, ν hyperparameter
  9. For test samples Y, compute K_PY using same projection and evaluate decision function

- **Design tradeoffs:**
  - **Generated sample count:** Paper tested P, 2P, P/2; P works well, 2P similar, P/2 slightly worse for scarce negatives. Default to P for efficiency.
  - **Reference vector composition:** Negative-only (variant 3) vs negative + generated (variant 7). Variant 7 is best for lowest negative counts; variant 3 competitive when negatives are plentiful.
  - **Distribution model:** Normal approximation is acknowledged as coarse but practical. Paper tried alternatives (e.g., adding noise to negatives) without improvement.

- **Failure signatures:**
  - **Gmean < 50%:** Check if positive samples are incorrectly used as reference vectors (variant 1 in Table I causes this).
  - **Performance degrades as negatives increase past 30:** Expected crossover — binary SVM should outperform one-class methods when negatives are plentiful.
  - **High variance across train-test splits:** May indicate unrepresentative negative samples or non-Gaussian distribution violating assumptions.
  - **Ion2 task pattern (Table III):** Very low OC-SVM baseline (25.3%) but GRKneg recovers to 65.7% — suggests some tasks benefit dramatically; investigate feature characteristics.

- **First 3 experiments:**
  1. **Baseline replication:** On 2–3 datasets from Table II (e.g., Iris1, Seed2, Ion1), compare GRKneg (variant 7) vs standard OC-SVM vs binary SVM at 5, 10, 20 negative samples. Target: replicate the Gmean ordering and approximate magnitude from Table III.
  2. **Reference vector ablation:** Implement variants 1, 2, 3, 5, 7 from Table I on a single dataset. Verify that variant 1 (positive references) fails, variant 3 (negatives only) is strong, and variant 7 (negatives + generated) is best at lowest N.
  3. **Synthetic distribution sensitivity:** Create a synthetic dataset where the negative class is bimodal or heavy-tailed. Run GRKneg with normal approximation vs a simple alternative (e.g., Gaussian mixture with 2 components). Measure Gmean to identify when the normal assumption breaks.

## Open Questions the Paper Calls Out
- **Question:** How does GRKneg performance compare to imbalanced and class-specific classification techniques?
  - **Basis in paper:** [explicit] The Conclusion explicitly states: "Wider comparisons of the proposed method with different comparative approaches, such as imbalanced and class-specific classification techniques, remains to be done as future work."
  - **Why unresolved:** The current study only benchmarks the proposed method against standard OC-SVM and binary SVM.
  - **What evidence would resolve it:** Experimental results benchmarking GRKneg against cost-sensitive learning or ensemble methods for imbalanced data on the same UCI tasks.

- **Question:** Can more sophisticated generative models for reference vectors outperform the simple normal approximation?
  - **Basis in paper:** [inferred] The authors acknowledge that approximating the negative distribution with a normal distribution based only on mean and variance is a "coarse approximation" and that attempts to add noise failed.
  - **Why unresolved:** The paper relies on a simple heuristic for generating samples; determining the optimal generation strategy remains an open investigation.
  - **What evidence would resolve it:** Ablation studies using techniques like Kernel Density Estimation or generative adversarial networks to create reference vectors.

- **Question:** Is the proposed reference kernel strategy effective when applied to Support Vector Data Description (SVDD)?
  - **Basis in paper:** [inferred] The paper cites previous work applying Generalized Reference Kernel (GRK) to SVDD and mentions SVDD variants using negative samples, but restricts the proposed negative-sample method (GRKneg) to OC-SVM.
  - **Why unresolved:** It is unclear if the benefits of using negative reference vectors transfer to the spherical decision boundary approach of SVDD.
  - **What evidence would resolve it:** Experiments applying the GRKneg kernel construction to the SVDD optimization problem.

## Limitations
- **Core assumption vulnerability:** Performance degrades when negative training samples are highly unrepresentative of the true negative distribution, as the reference-vector mechanism relies on representative negatives.
- **Distribution approximation weakness:** The normal approximation for synthetic sample generation is acknowledged as "really coarse" and no better alternatives were found during experimentation.
- **Domain-specific applicability:** The method shows expected crossover behavior where binary SVM outperforms GRKneg for large negative sample counts (>30), indicating it's specifically targeted at small-N regimes rather than being universally superior.

## Confidence
- **High:** Comparative results showing GRKneg outperforms standard OC-SVM with 5-10 negative samples, directly demonstrated across 14 tasks with clear Gmean improvements (79.5-81.9% vs 73.7-75.0%)
- **Medium:** Mechanism claims about negative-reference vectors improving discrimination, strong empirical evidence but limited theoretical grounding for why this works
- **Low:** Distribution-augmentation claims, given authors' own admission that the normal approximation is "really coarse" and alternative generation methods failed to improve results

## Next Checks
1. Test GRKneg on synthetic datasets with deliberately unrepresentative negative samples (e.g., negatives sampled from a different mode than the true negative distribution) to establish the break condition for the reference-vector mechanism.

2. Implement and compare the proposed normal-approximation synthetic sample generation against a simple alternative (e.g., adding Gaussian noise to real negative samples) on datasets where the negative distribution is visibly non-Gaussian.

3. Conduct hyperparameter sensitivity analysis for the RBF kernel σ parameter across the 14 datasets to determine if the average-distance heuristic for σ consistently produces optimal results or if dataset-specific tuning would yield larger improvements.