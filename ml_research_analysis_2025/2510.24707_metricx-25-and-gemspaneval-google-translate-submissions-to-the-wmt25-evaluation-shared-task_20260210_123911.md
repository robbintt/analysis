---
ver: rpa2
title: 'MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25 Evaluation
  Shared Task'
arxiv_id: '2510.24707'
source_url: https://arxiv.org/abs/2510.24707
tags:
- error
- translation
- task
- span
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce two new models for the WMT25 Translation Evaluation
  Shared Task. For quality score prediction, we develop MetricX-25, which adapts Gemma
  3 to an encoder-only architecture and demonstrates strong performance in predicting
  both MQM and ESA quality scores, significantly outperforming its predecessor.
---

# MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25 Evaluation Shared Task

## Quick Facts
- arXiv ID: 2510.24707
- Source URL: https://arxiv.org/abs/2510.24707
- Reference count: 18
- Key outcome: We introduce two new models for the WMT25 Translation Evaluation Shared Task. For quality score prediction, we develop MetricX-25, which adapts Gemma 3 to an encoder-only architecture and demonstrates strong performance in predicting both MQM and ESA quality scores, significantly outperforming its predecessor. For error span detection, we introduce GemSpanEval, a generative model that identifies error spans along with their severities and categories. We show that GemSpanEval is competitive with encoder-only baselines and uniquely identifies error spans by including context for non-unique spans. Both systems are fine-tuned on publicly available WMT data and demonstrate the effectiveness of using Gemma 3 for translation evaluation tasks.

## Executive Summary
We introduce two new models for the WMT25 Translation Evaluation Shared Task. For quality score prediction, we develop MetricX-25, which adapts Gemma 3 to an encoder-only architecture and demonstrates strong performance in predicting both MQM and ESA quality scores, significantly outperforming its predecessor. For error span detection, we introduce GemSpanEval, a generative model that identifies error spans along with their severities and categories. We show that GemSpanEval is competitive with encoder-only baselines and uniquely identifies error spans by including context for non-unique spans. Both systems are fine-tuned on publicly available WMT data and demonstrate the effectiveness of using Gemma 3 for translation evaluation tasks.

## Method Summary
MetricX-25 adapts Gemma 3 decoder weights to an encoder-only architecture with a regression head for quality score prediction. The model processes source, reference, and translation inputs through mean pooling, then predicts scalar quality scores. It supports both MQM and ESA scoring frameworks through a single regression head by mixing training examples with explicit score type indication. GemSpanEval uses Gemma 3 decoder to generate JSON-formatted error spans, including severity and category labels. For non-unique spans, it extends context until substrings become unique, enabling unambiguous identification. Both models are fine-tuned on WMT data, with MetricX-25 using a two-stage approach (DA then DA+MQM) and GemSpanEval using single-stage SFT.

## Key Results
- MetricX-25 significantly outperforms its mT5-based predecessor in predicting both MQM and ESA quality scores across multiple language pairs
- The two-stage training approach (DA → DA+MQM) achieves consistent improvements over single-stage training
- GemSpanEval achieves competitive F1 scores compared to encoder-only baselines while uniquely identifying error spans through context expansion
- Including score type tokens in training enables a single model to predict multiple score types effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapting Gemma 3 decoder weights to an encoder-only architecture with a regression head improves quality score prediction over mT5-based predecessors.
- Mechanism: The encoder processes source/reference/translation inputs through mean pooling, then a regression head predicts a scalar quality score. Weights initialize from the corresponding Gemma 3 decoder (Suganthan et al., 2025).
- Core assumption: Decoder pretraining captures multilingual representations transferable to encoder-only evaluation tasks.
- Evidence anchors:
  - [abstract] "MetricX-25, adapting Gemma 3 to an encoder-only architecture with a regression head on top, can be trained to effectively predict both MQM and ESA quality scores, and significantly outperforms its predecessor."
  - [section 3.4.2, Table 2] Shows segment-level pairwise accuracy improvements (e.g., ja-zh: 53.90 → 57.72).
  - [corpus] Weak external validation; no corpus papers directly test this encoder-only adaptation.
- Break condition: If decoder-to-encoder weight transfer degrades task-specific representations relative to natively trained encoder-decoder models.

### Mechanism 2
- Claim: A single regression head can predict multiple score types (MQM and ESA) by mixing training examples with explicit score type indication in the input.
- Mechanism: Training data includes DA (for ESA) and MQM examples with "Score type: MQM" or "ESA" tokens. Raw DA scores are rescaled to MQM scale during training, then outputs are rescaled back during postprocessing.
- Core assumption: The model learns score distributions rather than absolute scales, conditioned on score type tokens.
- Evidence anchors:
  - [section 3.4.1, Table 1] "DA + MQM" model performs on par with single-score-type models on respective validation sets.
  - [section 3.2] Describes input format with score type indication.
  - [corpus] No corpus evidence on multi-score training; remains paper-specific.
- Break condition: If score distributions diverge significantly across types and rescaling fails to align them.

### Mechanism 3
- Claim: Including context tokens around non-unique error spans enables unambiguous identification in generative error span detection.
- Mechanism: For spans appearing multiple times in text, extend context to previous/next word (or character for CJK) until the substring is unique. Output includes `span_with_context` field.
- Evidence anchors:
  - [section 4.2] "In WMT24 en-de MQM data, 21% of error spans are not unique."
  - [section 4.4] "For WMT24 en-de this leads to a marginal F1 improvement of 0.08."
  - [corpus] Related work (Mu-SHROOM) frames span-labeling similarly but doesn't test context expansion.
- Break condition: If context extension overshadows the error span or exceeds context window limits for dense errors.

## Foundational Learning

- Concept: MQM vs ESA scoring frameworks
  - Why needed here: MetricX-25 predicts both score types; understanding their scales (0–25 for MQM, 0–100 for ESA) and annotation protocols is essential.
  - Quick check question: Can you explain why DA scores are used as proxies for ESA training?

- Concept: Encoder-only vs decoder-only architectures for evaluation
  - Why needed here: MetricX-25 uses encoder-only for regression; GemSpanEval uses decoder-only for generation. Architecture choice affects pooling, output format, and inference cost.
  - Quick check question: What are the tradeoffs between mean pooling (encoder-only) and autoregressive generation (decoder-only) for error span detection?

- Concept: Meta-evaluation metrics (pairwise accuracy, soft pairwise accuracy)
  - Why needed here: Model selection and performance claims rely on segment-level tie-calibrated pairwise accuracy and system-level SPA.
  - Quick check question: Why might system-level correlation have higher variance than segment-level metrics?

## Architecture Onboarding

- Component map:
  - MetricX-25: Gemma 3 12B encoder → mean pooling → regression head (scalar output)
  - GemSpanEval: Gemma 3 27B decoder → JSON-formatted error span output (with optional context fields)

- Critical path:
  1. Prepare training data with score type tokens (MetricX) or JSON error annotations (GemSpanEval)
  2. Two-stage fine-tuning for MetricX (DA → DA+MQM); single-stage SFT for GemSpanEval
  3. Postprocessing: rescale scores (MetricX) or parse JSON and resolve spans (GemSpanEval)

- Design tradeoffs:
  - MetricX hybrid vs reference-only: hybrid offers input flexibility; reference-only slightly better when references available (Table 2).
  - GemSpanEval generative vs XCOMET encoder-only: competitive F1 but generative approach enables richer context handling.

- Failure signatures:
  - Repetition loops in GemSpanEval causing invalid JSON (paper notes fallback to earlier checkpoint for 22% of cases)
  - Score scale mismatch if postprocessing rescaling misapplied between MQM and ESA
  - Non-unique span resolution failures if context expansion insufficient

- First 3 experiments:
  1. Replicate MetricX-25 two-stage training on WMT15–23 DA + WMT20–23 MQM; validate on WMT24 MQM and ESA held-out sets.
  2. Ablate score type indication to confirm multi-score learning mechanism.
  3. Test GemSpanEval with and without context expansion on non-unique spans; measure F1 delta and parsing failure rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can generative error span detection models be stabilized to prevent invalid JSON outputs?
- Basis in paper: [explicit] The authors report "model repetition problems that resulted in invalid JSON output" in GemSpanEval, necessitating a fallback to an earlier model version.
- Why unresolved: Standard fine-tuning did not guarantee syntactically valid outputs, a known issue with generative LLMs.
- What evidence would resolve it: Implementation of constrained decoding or specialized training that yields 100% valid JSON without fallbacks.

### Open Question 2
- Question: Do the reported performance gains of MetricX-25 hold when accounting for training variance?
- Basis in paper: [explicit] The authors state, "Due to limited resource availability, we were only able to run each experiment with one random seed."
- Why unresolved: Neural network performance can vary significantly across initializations; a single seed limits the statistical robustness of the claimed improvements.
- What evidence would resolve it: Evaluation results reported with confidence intervals derived from multiple random seeds (e.g., n > 3).

### Open Question 3
- Question: Why does the non-fine-tuned Gemma 3 baseline outperform the fine-tuned GemSpanEval on Japanese-Chinese (ja-zh) error span detection?
- Basis in paper: [explicit] The authors note that the baseline "achieving the best result... for ja-zh" suggests the task is "highly dependent on rater behavior."
- Why unresolved: It is unclear if this anomaly stems from rater inconsistency in the training data or a failure of the fine-tuning objective for this specific language pair.
- What evidence would resolve it: An ablation study on ja-zh data quality or an analysis of error type distributions between the baseline and fine-tuned model.

## Limitations

- Transfer effectiveness from decoder to encoder is not directly validated against a scratch-trained encoder-only baseline
- Context window constraints may limit span resolution in dense error scenarios
- Score distribution alignment assumptions lack empirical validation

## Confidence

- MetricX-25 performance claims: High
- Multi-score training effectiveness: Medium
- Context expansion for span resolution: Medium
- Generative approach advantages: Low

## Next Checks

1. Direct encoder-only baseline comparison: Train a MetricX-25 variant from scratch without decoder weight initialization and compare performance to the transferred-weight version across all language pairs to isolate the transfer effect.

2. Context window stress test: Create synthetic MQM datasets with dense, overlapping error spans and evaluate GemSpanEval's context expansion mechanism at varying error densities to determine the practical limits of the approach.

3. Score distribution analysis: Conduct detailed analysis of predicted versus ground truth score distributions for both MQM and ESA, including correlation analysis before and after rescaling, to validate the multi-score training assumptions.