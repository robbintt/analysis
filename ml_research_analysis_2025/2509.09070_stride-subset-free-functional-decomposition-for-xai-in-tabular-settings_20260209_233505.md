---
ver: rpa2
title: 'STRIDE: Subset-Free Functional Decomposition for XAI in Tabular Settings'
arxiv_id: '2509.09070'
source_url: https://arxiv.org/abs/2509.09070
tags:
- stride
- functional
- decomposition
- kernel
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STRIDE addresses the limitation of scalar-based explanations by
  providing a subset-free, model-agnostic functional decomposition in RKHS, capturing
  interactions without explicit enumeration. It uses recursive kernel centering to
  compute orthogonal components analytically, improving interpretability by revealing
  synergy/redundancy and enabling quantitative interaction removal ("component surgery").
---

# STRIDE: Subset-Free Functional Decomposition for XAI in Tabular Settings

## Quick Facts
- arXiv ID: 2509.09070
- Source URL: https://arxiv.org/abs/2509.09070
- Authors: Chaeyun Ko
- Reference count: 31
- Primary result: 3.0× median speedup vs TreeSHAP with R²=0.93 fidelity and validated interaction necessity

## Executive Summary
STRIDE introduces a model-agnostic, subset-free functional decomposition method for XAI in tabular settings. It analytically computes orthogonal functional components in Reproducing Kernel Hilbert Space using recursive kernel centering, avoiding exponential subset enumeration. On 10 datasets, STRIDE achieves median 3.0× speedup over TreeSHAP while maintaining high reconstruction fidelity (mean R²=0.93) and strong global rank agreement. The method uniquely enables "component surgery" to validate interaction necessity by quantitatively proving that removing top interactions degrades model performance.

## Method Summary
STRIDE performs functional decomposition in RKHS by constructing centered kernels through recursive subtraction of lower-order terms, ensuring orthogonality without subset enumeration. It builds low-rank approximations of feature and interaction kernels, orthogonalizes interaction maps against main effects, and solves a single regularized least-squares problem to obtain all functional components simultaneously. The method supports scalar attribution through aggregation of functional components while preserving interpretability by revealing synergy and redundancy. Component surgery allows quantitative validation of interaction importance by measuring performance degradation when specific components are removed.

## Key Results
- Median 3.0× speedup versus TreeSHAP across 10 datasets
- Mean R²=0.93 for model reconstruction fidelity
- Removing top interaction from California Housing reduced test R² by 0.023±0.004
- Strong global rank agreement with TreeSHAP while providing richer functional insights

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Decomposition via Recursive Kernel Centering
The method constructs centered kernels K^(c)_S through recursive subtraction of lower-order terms using Möbius inversion. The partial zero-mean property (∫ K^(c)_S dμ_i = 0 for i∈S) ensures orthogonality: integrating over a dimension present in one kernel but not another causes the integral to vanish. This allows projecting f onto orthogonal subspaces H_S via ⟨f, K^(c)_S⟩. The core assumption is that the model function f resides in an RKHS spanned by centered kernels with integrals approximated empirically using sufficient samples.

### Mechanism 2: Subset-Free Computation via Low-Rank Projection
Rather than enumerating subsets, STRIDE builds a design matrix from feature maps (main effects and selected pairwise interactions), orthogonalizes interaction maps against lower-order terms, and solves jointly. Low-rank approximations reduce kernel matrix dimensionality from O(n²) to O(n·rank). The method assumes important interactions can be captured by limited feature pairs selected via proxy criteria, with higher-order interactions truncated. This avoids exponential 2^d complexity while maintaining fidelity.

### Mechanism 3: Component Surgery for Causal Validation
After decomposition f = Σ f_S, the method surgically removes f_ij by reconstructing f_{-ij} = f - f_ij and evaluates test R². The performance drop quantifies the interaction's contribution beyond statistical artifact. This assumes the decomposition is sufficiently complete that removing one component doesn't cascade into unmeasured distributional shifts. The approach provides quantitative proof of functional necessity rather than mere statistical association.

## Foundational Learning

- **Reproducing Kernel Hilbert Space (RKHS)**: Required because STRIDE's theoretical guarantees (orthogonality, convergence) rely on projecting functions onto kernel-defined subspaces. Quick check: Given a kernel K, can you explain why ⟨f, K(·, x)⟩_H = f(x) implies that evaluation is a linear functional?

- **Functional ANOVA / Hoeffding Decomposition**: Needed because STRIDE extends classical variance decomposition to functional components, separating main effects from interactions. Quick check: In a 3-feature ANOVA, what are the components f_S for S ⊆ {1,2,3} and why must they be orthogonal?

- **Shapley Value Axioms**: Required because STRIDE aggregates functional components into scalar attributions that satisfy efficiency, symmetry, dummy, and linearity. Quick check: If a feature is a "dummy" (never affects the output regardless of coalition), what must its Shapley value be?

## Architecture Onboarding

- **Component map**: Kernel Construction Module → Centering/Othogonalization Engine → Projection Solver → Component Surgery Interface

- **Critical path**: 1. Feature kernel selection (bandwidth, kernel type) → 2. Low-rank approximation (rank choice, landmark selection) → 3. Orthogonalization (tolerance settings) → 4. Projection solve (regularization λ) → 5. Reconstruction & validation

- **Design tradeoffs**:
  - Rank vs. fidelity: Higher rank improves R² but increases runtime and memory (O(n·rank) storage)
  - Interaction pair selection: Exhaustive enumeration is costly; proxy heuristics (e.g., dependence scores) may miss rare but critical interactions
  - Kernel choice: RBF captures smooth functions; Laplace handles sharper transitions; mismatched kernels degrade approximation

- **Failure signatures**:
  - Low R² (<0.8): Insufficient rank, inappropriate kernel, or missing high-order interactions
  - Slow runtime on small data: Over-engineered feature maps; consider simpler kernels or reduced rank
  - Negative synergy heatmap values everywhere: Over-regularization or whitening issues causing numerical instability

- **First 3 experiments**:
  1. **Sanity check**: On California Housing with d=8, verify reconstruction R² > 0.9 with default RBF kernel and rank≈50; compare runtime against TreeSHAP
  2. **Ablation**: Remove the top interaction component and confirm test R² drop of ~0.02; validate that removing a random low-importance interaction has negligible effect
  3. **Scalability test**: Run on YearPredictionMSD (d=90) with increasing rank (25, 50, 100); plot R² vs. runtime to find the efficiency frontier

## Open Questions the Paper Calls Out

### Open Question 1
Can the recursive kernel-centering procedure of STRIDE be generalized to efficiently handle unstructured data domains like computer vision and NLP? The authors state in Section 5.4 that "the principles of subset-free functional decomposition lay the groundwork for future adaptations to more complex domains like vision and NLP, a promising direction for the community." This remains unresolved because the current implementation and experiments are restricted to tabular data, and kernel methods often struggle with the high dimensionality of raw image or text data.

### Open Question 2
How can STRIDE efficiently scale to identify and quantify explicit higher-order interactions (order > 2) without combinatorial explosion? Section 5.4 lists "efficient solvers and selection strategies for higher-order interactions" as a specific next step; Algorithm 1 currently focuses on "selected pairs." The current implementation primarily targets main effects and pairwise interactions, likely to manage computational complexity, leaving the recovery of complex, higher-order synergies unexplored.

### Open Question 3
Does the functional view provided by STRIDE measurably improve human decision-making and model debugging compared to traditional scalar methods? Section 5.4 identifies "user studies to assess the practical utility of functional views for decision-making" as a necessary future step. While the paper argues qualitatively that functional components offer deeper insights, it provides no empirical evidence that these representations actually help users debug models more effectively or make better decisions.

## Limitations
- Rank truncation strategy not specified—critical for balancing speed vs. fidelity
- Feature pair selection heuristic unclear—may miss important interactions
- No ablation on higher-order interactions (>2-way) tested

## Confidence
- Subset-free computation claim: High confidence (direct experimental validation)
- Efficiency claim vs TreeSHAP: Medium confidence (rank details unspecified)
- Orthogonality proof: High mathematical confidence but Low practical robustness due to numerical errors

## Next Checks
1. Reproduce California Housing R² > 0.9 and 3× speedup with default RBF kernel
2. Confirm component surgery causes ~0.02 R² drop only for top interaction, not random ones
3. Test scalability on YearPredictionMSD with varying rank to identify efficiency frontier