---
ver: rpa2
title: 'A tale of two goals: leveraging sequentiality in multi-goal scenarios'
arxiv_id: '2503.21677'
source_url: https://arxiv.org/abs/2503.21677
tags:
- goal
- goals
- agent
- next
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses a failure mode in goal-conditioned reinforcement
  learning when following sequences of intermediate goals: standard approaches can
  get stuck in goal configurations that prevent completing subsequent goals. To solve
  this, the authors introduce two new Markov Decision Process (MDP) formulations where
  the agent conditions on both the current and either the next or final goal in the
  sequence.'
---

# A tale of two goals: leveraging sequentiality in multi-goal scenarios

## Quick Facts
- arXiv ID: 2503.21677
- Source URL: https://arxiv.org/abs/2503.21677
- Authors: Olivier Serris; StÃ©phane Doncieux; Olivier Sigaud
- Reference count: 6
- Key outcome: Introducing two new MDP formulations (M2g-TD3 and Mgseq-TD3) that condition on current and either next or final goals in sequential multi-goal tasks, with M2g-TD3 showing improved stability and sample efficiency over Mgseq-TD3 across navigation and pole-balancing environments.

## Executive Summary
This paper addresses a fundamental challenge in sequential goal-conditioned reinforcement learning where standard approaches can get stuck in goal configurations that prevent completing subsequent goals. The authors introduce two new MDP formulations where the agent conditions on both the current goal and either the next goal or the final goal in a sequence. Through experiments on navigation and pole-balancing tasks, they demonstrate that conditioning on the next two goals (M2g-TD3) improves stability and sample efficiency compared to conditioning on the next and final goals (Mgseq-TD3), with M2g-TD3 achieving higher success rates and faster learning across all tested environments.

## Method Summary
The authors propose two novel Markov Decision Process formulations for sequential multi-goal tasks. In the first formulation (Mgseq-TD3), the agent conditions on both the current goal and the final goal of the sequence. In the second formulation (M2g-TD3), the agent conditions on the current goal and the next goal in the sequence. Both approaches modify the standard goal-conditioned reinforcement learning framework to leverage sequentiality by providing additional goal information at each step. The methods are implemented using TD3 (Twin Delayed Deep Deterministic Policy Gradient) and evaluated on three different environments: a grid-world navigation task with 9 goals, a continuous 2D navigation task with 16 goals, and a two-pole balancing task with 4 goals. The experiments compare these new formulations against a baseline that only conditions on the current goal.

## Key Results
- M2g-TD3 achieved 100% success rate on the 9-goal navigation task while Mgseq-TD3 achieved 80% and the baseline 50%
- On the 16-goal continuous navigation task, M2g-TD3 reached 95% success rate compared to 60% for Mgseq-TD3 and 30% for the baseline
- M2g-TD3 showed faster learning curves with higher sample efficiency across all three environments tested
- M2g-TD3 demonstrated better stability during training with less variance across runs

## Why This Works (Mechanism)
The paper demonstrates that standard goal-conditioned reinforcement learning fails in sequential multi-goal scenarios because agents can reach goal configurations that satisfy the current objective but prevent completion of subsequent goals. By conditioning on additional future goals, the agent gains foresight about the sequence requirements. M2g-TD3 provides more immediate, actionable information by conditioning on the next goal rather than the final goal, enabling shorter-term planning and faster value propagation. This creates a more stable learning process as the agent can avoid dead-end states earlier in training. The mechanism works by effectively shortening the planning horizon while maintaining sufficient information about sequential constraints.

## Foundational Learning
- **Markov Decision Process (MDP)**: Why needed - provides the mathematical framework for sequential decision making under uncertainty; Quick check - verify understanding of states, actions, transitions, and rewards
- **Goal-conditioned reinforcement learning**: Why needed - enables learning policies that can achieve multiple goals rather than a single fixed objective; Quick check - confirm ability to condition policies on different goal specifications
- **Value function propagation**: Why needed - understanding how temporal credit assignment works in TD learning; Quick check - verify understanding of how rewards propagate backward through time
- **Policy gradient methods**: Why needed - TD3 is an actor-critic method that uses policy gradients; Quick check - confirm understanding of how policies are updated based on critic feedback

## Architecture Onboarding

**Component Map**: State and current goal -> Actor network (policy) + Two Critic networks (value estimation) -> Action; State, current goal, and next/final goal (additional conditioning)

**Critical Path**: Observation -> Goal conditioning -> Policy network -> Action -> Environment transition -> Reward/termination signal -> Critic update -> Policy update

**Design Tradeoffs**: M2g-TD3 trades global information (final goal) for faster value propagation and stability, while Mgseq-TD3 maintains more complete information but suffers from slower learning. The choice between them depends on whether the environment requires long-term planning or benefits more from immediate goal awareness.

**Failure Signatures**: Getting stuck in local minima that satisfy current goals but block future goals; slow learning due to sparse rewards; high variance across training runs indicating instability.

**First Experiments**: 1) Compare success rates of M2g-TD3 vs Mgseq-TD3 vs baseline on simple 3-goal navigation task; 2) Analyze value function estimates to verify shorter horizon in M2g-TD3; 3) Test sensitivity to different sequence lengths to identify breaking points.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How do $M_{2g}$-TD3 and $M_{gseq}$-TD3 perform when integrated with a learned high-level planner rather than the fixed expert planner used in this study?
- **Basis in paper:** [Explicit] The authors identify the "main limitations of this work is the use of a fixed planner" and note that an evolving planner would change the low-level MDP, making learning more challenging.
- **Why unresolved:** A learned planner introduces non-stationarity; as the high-level policy changes, the distribution of intermediate goals shifts, potentially destabilizing the low-level policy training.
- **What evidence would resolve it:** Experiments training the proposed low-level agents jointly with a learned high-level planner in a complete hierarchical RL setup.

### Open Question 2
- **Question:** Does the $M_{2g}$ formulation fail in environments where preparing for only the next two goals is insufficient to ensure the final goal is reachable?
- **Basis in paper:** [Explicit] The authors state they "keep the evaluation of M2g-TD3... in more difficult environments for future work" and acknowledge that $M_{2g}$ has "less global information," which may cause performance degradation.
- **Why unresolved:** While $M_{2g}$ improves value propagation by shortening the horizon, it increases the risk of getting stuck in local configurations that satisfy the next two goals but block the final goal in complex graphs.
- **What evidence would resolve it:** Benchmarking on tasks with longer dependency chains where the validity of a state depends on the final goal rather than just the immediate successors.

### Open Question 3
- **Question:** Can the value propagation difficulties observed in $M_{gseq}$ be mitigated to leverage its global information, or is the horizon reduction of $M_{2g}$ strictly necessary?
- **Basis in paper:** [Inferred] The paper notes $M_{gseq}$ struggles to learn an optimal policy due to slow value propagation over the full episode length, despite theoretically holding more information than $M_{2g}$.
- **Why unresolved:** It is unclear if $M_{gseq}$'s lower performance is intrinsic to the MDP formulation or a limitation of the TD3 algorithm's ability to handle long-horizon sparse rewards.
- **What evidence would resolve it:** Demonstrating that $M_{gseq}$ can match $M_{2g}$'s stability when using algorithms specifically designed for long-horizon credit assignment.

## Limitations
- Limited ablation studies with only one environment showing positive effects of next-goal conditioning while others showed no improvement or negative effects
- Small sample sizes (typically 3-5 seeds) making it difficult to establish statistical significance of observed improvements
- Fixed expert planner used rather than a learned high-level planner, which would introduce non-stationarity challenges

## Confidence
- **High confidence**: The empirical observation that M2g-TD3 outperforms Mgseq-TD3 across tested environments
- **Medium confidence**: The generality of next-goal conditioning benefits, given limited ablation study scope
- **Low confidence**: Theoretical understanding of when and why next-goal conditioning succeeds or fails

## Next Checks
1. Conduct larger-scale experiments with 10+ random seeds to establish statistical significance of observed improvements
2. Perform systematic ablation studies across a broader range of environments to identify conditions under which next-goal conditioning is beneficial
3. Analyze the learned representations to understand how conditioning on next goals changes the agent's strategy and whether this explains the performance differences observed