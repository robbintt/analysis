---
ver: rpa2
title: 'AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language
  Models in Autonomous Driving'
arxiv_id: '2601.14702'
source_url: https://arxiv.org/abs/2601.14702
tags:
- uni00000013
- uni00000011
- uni00000014
- uni00000015
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AutoDriDM is a decision-centric benchmark for vision-language
  models in autonomous driving, featuring 6,650 questions across three progressive
  levels: Object, Scene, and Decision. It evaluates model reasoning in complex driving
  scenarios and identifies key failure modes such as logical reasoning errors and
  semantic omissions.'
---

# AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving

## Quick Facts
- arXiv ID: 2601.14702
- Source URL: https://arxiv.org/abs/2601.14702
- Reference count: 40
- AutoDriDM is a decision-centric benchmark for vision-language models in autonomous driving, featuring 6,650 questions across three progressive levels: Object, Scene, and Decision.

## Executive Summary
AutoDriDM evaluates vision-language models on decision-making in autonomous driving through a progressive three-level protocol. The benchmark reveals that strong perception capability does not translate to reliable decision-making, with weak correlations between perception and decision tasks. Models show limited robustness to visually similar scenes and exhibit specific reasoning failure modes. A fine-tuned 7B analyzer model automates error-type annotation, enabling scalable explainability analysis. The work identifies critical gaps in VLM reasoning for safety-critical driving scenarios.

## Method Summary
The benchmark uses 1,295 front-facing images from nuScenes, KITTI, and BDD100K datasets, generating 6,650 QA pairs across six tasks: Object-1/2 (single-choice), Scene-1/2 (multiple-choice), and Decision-1/2 (single-choice). Evaluation uses zero-shot and few-shot (1/2/5-shot) settings with Chain-of-Thought prompting. Models are scored on exact match for full credit, partial credit for subsets in multi-choice, and zero for any incorrect selection. Pearson correlations measure cross-task alignment, and similar-scene pairs test robustness to compositional reasoning. A 7B analyzer model fine-tuned on 1,500 annotated reasoning traces automates error classification into nine categories.

## Key Results
- VLMs achieve high accuracy on object perception but significantly lower performance on decision-making tasks
- Weak correlations (mostly -0.2 to 0.2) between perception and decision tasks indicate decoupled capabilities
- Joint accuracy on visually similar scene pairs falls below the squared individual accuracy baseline, especially for smaller models
- The 7B analyzer model outperforms larger general-purpose VLMs (65.36% vs. 48.09-50.25% exact match) on error-type classification

## Why This Works (Mechanism)

### Mechanism 1
A progressive three-level evaluation protocol reveals that perception capability does not reliably transfer to decision-making capability in current VLMs. By structuring evaluation hierarchically, the benchmark isolates perceptual accuracy from downstream reasoning. Weak Pearson correlations (mostly -0.2 to 0.2) between perception tasks and decision tasks indicate that VLMs process these levels with limited integration—they may identify objects correctly without using that information causally for decisions.

### Mechanism 2
Joint accuracy on visually similar scene pairs exposes compositional rather than causal reasoning, especially in smaller models. Near-duplicate pairs share irrelevant visual cues. If a model relies on task-irrelevant features, joint accuracy drops below the squared individual accuracy baseline. Smaller models show significant drops (p < 0.05), indicating they cannot suppress irrelevant priors.

### Mechanism 3
A fine-tuned 7B analyzer model can automate error-type annotation with higher exact-match accuracy than larger general-purpose VLMs. The analyzer is trained on 1,500 manually annotated reasoning traces using supervised fine-tuning over nine error categories. Specialization enables it to outperform GPT-4.1 and Qwen-72B on this narrow classification task (65.36% vs. 48.09-50.25% exact match).

## Foundational Learning

- **Concept**: Perception–Decision Decoupling
  - Why needed here: The benchmark's central finding is that high perception scores do not guarantee safe decisions. Understanding this decoupling is critical for interpreting VLM evaluation results correctly.
  - Quick check question: If a VLM scores 90% on Object-1 but 45% on Decision-1, what does the weak correlation tell you about its internal reasoning chain?

- **Concept**: Near-Duplicate Robustness Testing
  - Why needed here: Evaluating decision consistency across visually similar scenes isolates causal reasoning from feature-matching shortcuts.
  - Quick check question: Why should joint accuracy approximate the product of individual accuracies if the model uses task-relevant semantics?

- **Concept**: Error Taxonomy for Explainability
  - Why needed here: Classifying reasoning traces into nine error types (e.g., Logical Reasoning Error, Semantic Feature Omission, Hallucination) enables systematic debugging and targeted improvement.
  - Quick check question: If a model correctly identifies a traffic light but still selects an unsafe action, which error category is most likely?

## Architecture Onboarding

- **Component map**: Data layer (1,295 images → 6,650 QA pairs) → Evaluation layer (six tasks, three levels) → Analysis layer (correlation matrices, similar-scene pairs, 7B analyzer)

- **Critical path**: Data filtering → Manual annotation → VLM inference with CoT prompting → Explainability analysis (manual trace tagging → analyzer fine-tuning → large-scale automation)

- **Design tradeoffs**: Single/multiple-choice format reduces ambiguity but may underestimate free-form reasoning; front-facing images only exclude multi-sensor and temporal data; 1,500-trace training balances scalability with annotation cost

- **Failure signatures**: High Object accuracy + low Decision accuracy + weak correlation = perception–decision decoupling; joint accuracy on similar scenes significantly below squared baseline = reliance on spurious visual cues; Internvl 38B anomaly = reasoning capacity–task demand mismatch

- **First 3 experiments**:
  1. Replicate correlation analysis on Llama variants to confirm perception–decoupling is not model-specific
  2. Curate 20 additional near-duplicate pairs from Waymo Open to test analyzer generalization
  3. Ablate analyzer training data size (500 vs. 1,000 vs. 1,500 traces) to measure annotation-efficiency tradeoffs

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the inclusion of temporal dynamics and multi-sensor data alter the weak correlation observed between perception and decision-making capabilities in VLMs?
- **Open Question 2**: What specific architectural mechanisms are required to strengthen the coupling between perception and decision-making, given that increasing parameter scale alone fails to improve cross-task integration?
- **Open Question 3**: What are the mechanistic causes of the "constrained reasoning" anomaly in intermediate-scale models (e.g., InternVL 38B) that leads to performance degradation compared to smaller models?

## Limitations

- Benchmark focuses on front-facing images, limiting coverage of occlusion scenarios and multi-sensor fusion contexts critical for real-world autonomous driving
- Error taxonomy trained on only 1,500 reasoning traces may not capture rare or novel failure modes
- Weak correlation between perception and decision tasks could reflect task design rather than true capability decoupling

## Confidence

- **High Confidence**: Perception–decision decoupling (supported by multiple correlation analyses across diverse VLMs)
- **Medium Confidence**: Near-duplicate robustness findings (statistically significant but limited to 60 pairs from three datasets)
- **Medium Confidence**: Analyzer model efficacy (strong within-dataset performance but unknown out-of-distribution generalization)

## Next Checks

1. Evaluate the 7B analyzer on reasoning traces from an independent autonomous driving dataset (e.g., Waymo Open) to measure error-category classification accuracy drop
2. Systematically remove perceptual features from high-performing models to test whether decision accuracy truly depends on object/scene recognition or exploits spurious correlations
3. Adapt 50 benchmark questions to include 2-3 frame sequences and measure performance degradation, testing the benchmark's relevance to video-based reasoning scenarios