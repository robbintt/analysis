---
ver: rpa2
title: 'MetaCluster: Enabling Deep Compression of Kolmogorov-Arnold Network'
arxiv_id: '2510.19105'
source_url: https://arxiv.org/abs/2510.19105
tags:
- arxiv
- accuracy
- metacluster
- compression
- clustering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MetaCluster compresses Kolmogorov-Arnold Networks (KANs) by addressing
  their high memory footprint caused by per-edge coefficient vectors. It uses a lightweight
  meta-learner to map low-dimensional embeddings to coefficient vectors, constraining
  them to a low-dimensional manifold that is amenable to clustering.
---

# MetaCluster: Enabling Deep Compression of Kolmogorov-Arnold Network

## Quick Facts
- arXiv ID: 2510.19105
- Source URL: https://arxiv.org/abs/2510.19105
- Authors: Matthew Raffel; Adwaith Renjith; Lizhong Chen
- Reference count: 19
- Key outcome: Achieves up to 80× parameter reduction on image classification and 124.1× on equation modeling with no accuracy loss

## Executive Summary
MetaCluster presents a novel compression framework specifically designed for Kolmogorov-Arnold Networks (KANs), which suffer from high memory footprints due to per-edge coefficient vectors. The method employs a lightweight meta-learner to map low-dimensional embeddings to coefficient vectors, constraining them to a low-dimensional manifold amenable to clustering. By applying K-means clustering and replacing per-edge parameters with shared centroids indexed by compact codes, MetaCluster achieves dramatic parameter reductions while maintaining accuracy. The approach is validated across multiple datasets including MNIST, CIFAR-10, CIFAR-100, and function approximation tasks, demonstrating superior performance compared to scalar quantization methods.

## Method Summary
MetaCluster addresses KANs' high memory footprint by learning a mapping from low-dimensional embeddings to coefficient vectors using a lightweight meta-learner. This meta-learner constrains the coefficient vectors to lie on a low-dimensional manifold that can be effectively clustered. K-means clustering is then applied to group similar coefficient vectors, and per-edge parameters are replaced with shared cluster centroids indexed by compact codes. After discarding the meta-learner, the centroids undergo fine-tuning to recover accuracy. This approach amortizes storage across multiple coefficients, achieving significant compression ratios while preserving the network's representational capacity.

## Key Results
- Achieves up to 80× parameter reduction on image classification tasks (MNIST, CIFAR-10, CIFAR-100)
- Achieves 124.1× compression on high-dimensional equation modeling tasks
- Maintains full accuracy after compression, outperforming scalar quantization baselines
- Demonstrates effectiveness across diverse tasks from image classification to function approximation

## Why This Works (Mechanism)
The core mechanism leverages the observation that KAN coefficient vectors, while individually large, exhibit redundancy that can be exploited through clustering. By training a meta-learner to map low-dimensional embeddings to coefficient vectors, the method implicitly learns a structured representation space. K-means clustering on this learned space groups similar coefficient vectors, enabling shared storage of centroids. The fine-tuning phase recovers any accuracy lost during clustering, ensuring the compressed model maintains full representational capacity.

## Foundational Learning
- Kolmogorov-Arnold Networks (KANs): Deep networks using learnable univariate functions on edges instead of fixed activations; needed for understanding the target architecture being compressed
- K-means clustering: Unsupervised algorithm that partitions data into k clusters by minimizing within-cluster variance; needed for the compression mechanism
- Meta-learning: Learning to learn, where a meta-learner produces parameters for another model; needed for constraining coefficient vectors to a learnable manifold
- Parameter quantization: Reducing precision or sharing parameters to reduce model size; needed for comparison and understanding compression goals
- Low-dimensional manifolds: Subspaces of lower dimension that data concentrates around; needed for understanding the clustering assumption

## Architecture Onboarding

**Component Map:** Input → Meta-learner → KAN coefficient vectors → K-means clustering → Centroid codebook → Compressed KAN

**Critical Path:** The critical path involves training the meta-learner to produce coefficient vectors, clustering these vectors, replacing them with centroid indices, and fine-tuning the centroids.

**Design Tradeoffs:** The method trades training complexity (meta-learner training) for deployment efficiency (smaller model size). The choice of cluster count represents a compression-accuracy tradeoff.

**Failure Signatures:** Poor clustering may occur if coefficient vectors don't lie on a suitable low-dimensional manifold, leading to accuracy degradation. Insufficient cluster count causes excessive compression loss.

**First Experiments:** 1) Apply MetaCluster to a small KAN on MNIST to verify basic functionality, 2) Test clustering effectiveness by varying k values, 3) Compare against scalar quantization on a simple regression task

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Scalability to larger vision tasks like ImageNet remains unproven
- Computational overhead of meta-learner during training is not quantified
- Clustering assumption may not hold universally across all KAN architectures
- Limited theoretical justification for why clustering preserves representational capacity

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Empirical compression results and accuracy preservation | High |
| Scalability to larger networks and complex tasks | Medium |
| Generalizability of clustering assumption across KAN applications | Medium |

## Next Checks
1. Evaluate MetaCluster on larger-scale vision benchmarks (ImageNet, COCO) to assess scalability limits
2. Measure training-time computational overhead introduced by the meta-learner to understand practical deployment costs
3. Test the method's effectiveness on KANs with varying depths and widths to determine architectural sensitivity