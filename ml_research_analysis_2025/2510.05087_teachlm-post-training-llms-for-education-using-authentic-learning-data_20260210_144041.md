---
ver: rpa2
title: 'TeachLM: Post-Training LLMs for Education Using Authentic Learning Data'
arxiv_id: '2510.05087'
source_url: https://arxiv.org/abs/2510.05087
tags:
- student
- data
- learning
- urlhttps
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the pedagogical limitations of large language
  models (LLMs) in educational settings, particularly their inability to effectively
  simulate authentic student-tutor interactions. The authors introduce TeachLM, an
  LLM optimized for teaching through parameter-efficient fine-tuning using a dataset
  of 100,000 hours of authentic one-on-one tutoring sessions.
---

# TeachLM: Post-Training LLMs for Education Using Authentic Learning Data

## Quick Facts
- arXiv ID: 2510.05087
- Source URL: https://arxiv.org/abs/2510.05087
- Authors: Janos Perczel; Jin Chow; Dorottya Demszky
- Reference count: 40
- Primary result: Parameter-efficient fine-tuning on 100,000 hours of authentic tutoring data doubles student talk time and reduces tutor verbosity compared to prompt engineering.

## Executive Summary
This paper addresses the pedagogical limitations of large language models in educational settings by introducing TeachLM, an LLM optimized for teaching through parameter-efficient fine-tuning using authentic student-tutor interactions. The authors develop a comprehensive pipeline to transcribe and clean audio data, train a high-fidelity student model, and propose a multi-turn evaluation protocol using synthetic dialogues. Their approach demonstrates that authentic learning data can substantially enhance LLMs' pedagogical capabilities compared to prompt engineering alone, achieving significant improvements in conversational and pedagogical performance metrics.

## Method Summary
The method involves three key components: (1) Data preparation using a proprietary pipeline that transcribes dual-track audio, performs speaker diarization, and cleans transcripts using LLM processing; (2) Student model fine-tuning using parameter-efficient fine-tuning on student turns with project-specific IDs; (3) Tutor model fine-tuning using parameter-efficient fine-tuning on tutor turns. The evaluation uses multi-turn synthetic dialogues between the fine-tuned student model and candidate tutor models, judged by LLMs across six pedagogical benchmarks.

## Key Results
- Fine-tuning on authentic learning data doubled student talk time (from 5-15% to ~30%) and reduced tutor verbosity by half
- Dialogue turns increased by 50% and model personalization improved significantly
- Multi-turn evaluation protocol using synthetic dialogues provides higher-fidelity assessment than single-turn benchmarks
- Model performance showed monotonic improvement across all six benchmarks during training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning on authentic tutoring data transfers pedagogical behavior patterns that prompt engineering cannot encode.
- **Mechanism**: Parameter-efficient fine-tuning exposes model parameters to statistical regularities of real tutor-student dialogues, updating weights to prioritize pedagogical friction over helpfulness.
- **Core assumption**: Transcript cleaning preserves pedagogically-relevant patterns while removing noise.
- **Evidence anchors**: Abstract shows conversational improvements; Figure 7 shows monotonic benchmark improvements; related work supports domain-specific fine-tuning benefits.
- **Break condition**: If cleaning over-smooths transcripts, removing hesitation patterns and authentic confusion signals.

### Mechanism 2
- **Claim**: A student model fine-tuned on authentic student data provides higher-fidelity evaluation than prompt-engineered simulators.
- **Mechanism**: PEFT on student turns with project-specific IDs embeds individual student behavior patterns into model parameters.
- **Core assumption**: Students are easier to simulate than tutors because they follow rather than lead.
- **Evidence anchors**: Abstract confirms high-fidelity synthetic dialogue generation; Figure 5 shows fine-tuned student model aligns with human-AI statistics (p < 0.001).
- **Break condition**: If student model overfits to specific projects, failing to generalize to new tutor behaviors.

### Mechanism 3
- **Claim**: Multi-turn synthetic evaluation using a fine-tuned student model reveals pedagogical capabilities that single-turn benchmarks miss.
- **Mechanism**: Generate N synthetic dialogues between candidate tutor model and fine-tuned student model, then aggregate single-shot LLM judgments across conversations.
- **Core assumption**: Six benchmarks correlate with effective pedagogy.
- **Evidence anchors**: Benchmarks informed by prior studies; human tutors consistently outperform off-the-shelf LLMs; related work supports multi-turn protocols.
- **Break condition**: If benchmarks are gameable, optimization may produce superficial scores without pedagogical substance.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**:
  - **Why needed here**: TeachLM uses PEFT to adapt frontier models without full retraining.
  - **Quick check question**: Can you explain why PEFT might overfit faster than full fine-tuning on small, homogeneous datasets?

- **ASR Pipeline Design (Transcription + Diarization)**:
  - **Why needed here**: The paper's data pipeline converts single-track audio to structured dialogues.
  - **Quick check question**: What types of pedagogical signals might be lost when merging consecutive same-speaker utterances during cleaning?

- **Synthetic Evaluation Validity**:
  - **Why needed here**: The entire evaluation framework depends on synthetic dialogues approximating real human-AI conversations.
  - **Quick check question**: If a student model trained on high-performing tutors is used to evaluate a new tutor model, what systematic bias might emerge?

## Architecture Onboarding

- **Component map**: Dual-track audio → ElevenLabs transcription → Speaker activity masks → Diarization → Gemini 2.5 Pro cleaning → Anonymized transcripts → PEFT on student turns (with project IDs) → Student model checkpoints → PEFT on tutor turns → TeachLM checkpoints → Synthetic dialogue generation → LLM judges → Aggregated metrics

- **Critical path**: Audio quality and diarization accuracy → Transcript cleaning coherence → Student model fidelity → Benchmark selection

- **Design tradeoffs**:
  - **Authenticity vs. privacy**: Aggressive anonymization removes tutor identity and personal context, potentially reducing pedagogical richness
  - **Evaluation complexity vs. reproducibility**: Multi-turn synthetic evaluation is scalable but may diverge from real student behavior
  - **Checkpoint selection**: Earlier checkpoints may underfit complex behaviors; later checkpoints risk overfitting conversational idiosyncrasies

- **Failure signatures**:
  - **Transcript hallucination**: Dropped segments or invented content during ASR propagates to training data
  - **Student model collapse**: Overfitting produces repetitive, unnatural responses
  - **Benchmark gaming**: Model learns to ask about coding background perfunctorily without adapting instruction
  - **Distribution shift**: Synthetic dialogues diverge from human-AI patterns

- **First 3 experiments**:
  1. **Validate transcription pipeline on 10 manually reviewed sessions**: Check diarization accuracy, cleaning quality, and information preservation before scaling
  2. **Train student model on small subset (n=50 projects), evaluate conversational statistics**: Confirm alignment with human-AI baselines before full training
  3. **Fine-tune tutor model on 1,000 cleaned sessions, run all six benchmarks**: Establish performance ceiling and identify which benchmarks improve fastest vs. slowest

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can reinforcement learning from human feedback (RLHF) applied to authentic learning data further reduce the "sycophantic" tendencies of LLMs compared to supervised fine-tuning alone?
- **Basis in paper**: Authors state they are focused on realizing the full benefit of RLHF as a natural next step.
- **Why unresolved**: Current study focused on supervised fine-tuning, which successfully adjusted metrics but may not fully instill complex value alignment.
- **What evidence would resolve it**: Comparison of SFT-only models against RLHF-enhanced models on benchmarks measuring withholding of direct answers.

### Open Question 2
- **Question**: Do improved scores on conversational metrics correlate with tangible improvements in student learning outcomes?
- **Basis in paper**: Section 4.1 notes benchmarks are "intentionally simple" proxies; Section 8 states aim to quantify student feedback on actual model performance.
- **Why unresolved**: Paper demonstrates behavior shifts toward human-like statistics but relies on automated evaluations rather than longitudinal learning measures.
- **What evidence would resolve it**: Randomized controlled trial measuring project completion quality and subject mastery compared to control group.

### Open Question 3
- **Question**: Can evaluation protocols using synthetic student models accurately assess quality of longitudinal, relationship-building interactions?
- **Basis in paper**: Authors acknowledge sophisticated evaluations are needed to capture full richness of human pedagogy, noting nuances of longitudinal interactions are critical for rapport.
- **Why unresolved**: Current evaluation relies on multi-turn dialogues and automated metrics which may not capture trust and dynamic adaptation developed over 4–6 month project timelines.
- **What evidence would resolve it**: Validation showing synthetic dialogue scores predict quality of rapport or instructional effectiveness in human-tutor interactions spanning multiple sessions.

## Limitations

- Proprietary dataset (100,000 hours of Polygence audio) creates fundamental reproducibility barriers
- Critical hyperparameters (LoRA ranks, learning rates, epoch counts) are not specified
- Complete prompt templates for transcript cleaning and evaluation are not provided
- Evaluation relies heavily on synthetic dialogues, which may not fully capture authentic pedagogical effectiveness

## Confidence

- **High confidence**: Conversational metric improvements (student talk time, tutor verbosity, dialogue turns) are well-documented with clear statistical evidence
- **Medium confidence**: Student model fidelity and multi-turn evaluation protocol are supported by internal validation but lack external benchmarking
- **Low confidence**: Claims about pedagogical effectiveness beyond conversational patterns (actual learning gains, student satisfaction) are not directly measured

## Next Checks

1. **Human evaluation validation**: Run a small-scale study comparing synthetic dialogues generated by TeachLM against actual human-tutor sessions on the same project tasks, measuring both conversational metrics and learning outcomes.

2. **Cross-dataset generalization test**: Fine-tune TeachLM on a publicly available educational dialogue dataset (e.g., MathDial) and evaluate whether conversational improvements transfer to different educational domains and student populations.

3. **Benchmark gaming analysis**: Design adversarial prompts that test whether the model can maximize student talk time without improving pedagogical depth, and whether it can pass context-uncovering checks through superficial questioning patterns.