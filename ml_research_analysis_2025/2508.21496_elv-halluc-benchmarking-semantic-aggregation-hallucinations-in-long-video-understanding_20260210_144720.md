---
ver: rpa2
title: 'ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video
  Understanding'
arxiv_id: '2508.21496'
source_url: https://arxiv.org/abs/2508.21496
tags:
- video
- caption
- qwen2
- captions
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ELV-Halluc, the first benchmark dedicated
  to evaluating semantic aggregation hallucinations (SAH) in long-video understanding.
  SAH occurs when a model correctly perceives frame-level semantics but misattributes
  them across events, a problem that becomes more severe in semantically complex,
  multi-event long videos.
---

# ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding

## Quick Facts
- **arXiv ID:** 2508.21496
- **Source URL:** https://arxiv.org/abs/2508.21496
- **Reference count:** 40
- **Primary result:** Introduces ELV-Halluc benchmark for semantic aggregation hallucinations (SAH) in long-video understanding, demonstrating SAH increases with semantic complexity and can be reduced by positional encoding and DPO training.

## Executive Summary
This paper introduces ELV-Halluc, the first benchmark dedicated to evaluating semantic aggregation hallucinations (SAH) in long-video understanding. SAH occurs when models correctly perceive frame-level semantics but misattribute them across events, a problem that becomes more severe in semantically complex, multi-event long videos. The benchmark uses an adversarial triplet QA design to quantify SAH and introduces a SAH ratio metric to measure the proportion of such errors. Experiments on 14 open-source and 2 closed-source models confirm the existence of SAH, showing it increases with semantic complexity and varies across semantic aspects.

## Method Summary
ELV-Halluc employs an adversarial triplet QA design where each question triplet consists of a target event, a similar event, and a distractor event. This design specifically isolates semantic aggregation hallucinations by testing whether models can correctly associate frame-level semantics with their proper events. The SAH ratio metric quantifies the proportion of semantic aggregation errors by measuring how often models incorrectly attribute information across events. The benchmark was evaluated across 14 open-source and 2 closed-source models to establish baseline SAH performance.

## Key Results
- Semantic aggregation hallucinations (SAH) are confirmed to exist across 16 evaluated models, with SAH ratio increasing as semantic complexity grows
- Positional encoding strategies like VideoRoPE reduce SAH by improving temporal localization of frame semantics
- DPO training on in-video adversarial data achieves up to 27.7% reduction in SAH ratio while improving overall video understanding performance

## Why This Works (Mechanism)
SAH occurs because long video models must aggregate frame-level information across extended temporal spans. When semantic complexity increases (multiple events, similar contexts), models struggle to maintain correct event boundaries and associations. The adversarial triplet design exposes this weakness by creating situations where correct frame understanding is insufficient without proper temporal aggregation. VideoRoPE and DPO training help by improving the model's ability to maintain spatial-temporal consistency and learn from difficult aggregation cases.

## Foundational Learning

**Semantic Aggregation Hallucinations:** Errors where models correctly identify frame-level semantics but misattribute them to wrong events. Needed to distinguish between perception errors and aggregation errors. Quick check: Does the model recognize objects correctly but place them in wrong temporal contexts?

**Temporal Event Boundary Detection:** The ability to segment videos into distinct events. Critical for proper semantic aggregation. Quick check: Can the model distinguish between similar-looking but temporally separate events?

**Adversarial Triplet Design:** QA format with target, similar, and distractor events to isolate aggregation errors. Needed to create controlled test conditions. Quick check: Does the model's error pattern change predictably when similar events are introduced?

## Architecture Onboarding

**Component Map:** Video frames -> Frame-level encoder -> Temporal aggregation module -> Event-level understanding -> SAH detection

**Critical Path:** The temporal aggregation module is critical as it bridges frame-level understanding and event-level comprehension, where SAH occurs.

**Design Tradeoffs:** The adversarial triplet design trades simplicity for specificity in detecting SAH, potentially missing other error types but providing clear isolation of aggregation hallucinations.

**Failure Signatures:** SAH manifests as correct recognition of objects/actions but incorrect temporal attribution across events, particularly when events share similar visual features.

**First Experiments:**
1. Test model performance on single-event vs. multi-event videos to establish baseline SAH growth
2. Evaluate VideoRoPE impact on positional encoding accuracy across different video lengths
3. Compare DPO training with and without adversarial data to measure SAH-specific improvements

## Open Questions the Paper Calls Out
None

## Limitations
- The adversarial triplet design may introduce annotation artifacts that don't reflect all real-world aggregation errors
- The SAH ratio metric treats all aggregation errors equally without considering severity or downstream impact
- Results may not generalize across different cultural contexts or video domains beyond those tested

## Confidence

**High confidence in:**
- Existence of semantic aggregation hallucinations in long-video models
- Effectiveness of positional encoding (VideoRoPE) in reducing SAH
- Observation that SAH increases with semantic complexity

**Medium confidence in:**
- Generalizability of SAH patterns across different video types
- Effectiveness of DPO training for SAH mitigation

**Low confidence in:**
- Completeness of adversarial design in capturing all SAH types
- Whether SAH ratio metric captures full spectrum of aggregation failures

## Next Checks
1. Evaluate SAH benchmark on videos from diverse cultural contexts and domains to assess cross-domain generalizability
2. Conduct ablation studies to determine whether VideoRoPE benefits are specifically from positional encoding improvements
3. Test robustness of adversarial triplet design by having multiple annotators create examples for the same videos