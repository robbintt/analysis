---
ver: rpa2
title: 'JETHICS: Japanese Ethics Understanding Evaluation Dataset'
arxiv_id: '2506.16187'
source_url: https://arxiv.org/abs/2506.16187
tags:
- dataset
- data
- please
- ethics
- other
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces JETHICS, a Japanese ethics dataset containing
  78K examples built following the ETHICS dataset methodology. The dataset covers
  five categories based on normative theories and commonsense morality.
---

# JETHICS: Japanese Ethics Understanding Evaluation Dataset
## Quick Facts
- arXiv ID: 2506.16187
- Source URL: https://arxiv.org/abs/2506.16187
- Reference count: 11
- Key outcome: Japanese ethics dataset with 78K examples; GPT-4o achieves 0.7 accuracy while best Japanese model reaches 0.5

## Executive Summary
This work introduces JETHICS, a Japanese ethics dataset containing 78K examples built following the ETHICS dataset methodology. The dataset covers five categories based on normative theories and commonsense morality. Evaluation experiments on non-proprietary Japanese LLMs and GPT-4o show that even the best Japanese model achieves only 0.5 accuracy while GPT-4o reaches 0.7, indicating significant room for improvement in current LLMs' moral understanding capabilities.

## Method Summary
The dataset was constructed using Amazon Mechanical Turk with Japanese-speaking crowdworkers who created and validated examples across five ethical categories. Each example was validated by 3-4 annotators through majority voting. The evaluation used an 8-shot prompting protocol with binary classification tasks, testing both proprietary (GPT-4o) and non-proprietary Japanese LLMs including llmjp3.7b, llmjp13b, MetaLlama8b, and LlamaELYZA8b.

## Key Results
- GPT-4o outperforms all Japanese models with 0.7 average accuracy vs 0.5 for best Japanese model
- Larger Japanese models show better performance (llmjp13b: 0.497 vs llmjp3.7b: 0.326)
- Japanese pre-training and instruction tuning improves performance (LlamaELYZA8b: 0.479 vs MetaLlama8b: 0.367)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Culturally-specific moral knowledge is not fully transferable across languages via training data alone.
- **Mechanism:** Models trained predominantly on Western corpora may internalize Western moral norms that diverge from Japanese cultural contexts. The paper documents GPT-4o failing on examples like singing the Anpanman song at graduation—culturally inappropriate in Japan but not obviously wrong from a Western lens. This suggests moral understanding involves situational cultural knowledge that is not encoded in base training.
- **Core assumption:** Moral acceptability is partially culturally relative, and this relativity is reflected in language-specific usage patterns.
- **Evidence anchors:**
  - [abstract]: "no dataset reflecting Japanese moral values exists... some aspects of morality are culturally relative"
  - [section 5]: Table 4 shows GPT-4o errors on Japanese-specific examples
  - [corpus]: "Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales" supports that cultural knowledge is limited to English-speaking communities
- **Break condition:** If models trained on multilingual but culturally-universal moral frameworks achieve comparable performance, the cultural-relativity mechanism would be weakened.

### Mechanism 2
- **Claim:** Model scale correlates with improved moral reasoning performance.
- **Mechanism:** Comparing llmjp3.7b (0.326 average) to llmjp13b (0.497 average) shows a 0.171 improvement. The authors state this "suggests that increasing model size contributes to performance improvements." Larger parameter count may enable richer representations of normative concepts and better contextual integration.
- **Core assumption:** Performance gains are attributable to model capacity rather than confounds like different training data composition.
- **Evidence anchors:**
  - [section 4]: "llmjp13b attains the highest average score (0.497)"
  - [section 5]: "the larger model (llmjp13b) accuracy is, on average, 0.171 points higher"
  - [corpus]: "Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages" also compares model sizes across languages
- **Break condition:** If scaling experiments with controlled training data show no improvement, or if smaller models with targeted fine-tuning match larger models, the scale mechanism would be insufficient.

### Mechanism 3
- **Claim:** Additional language-specific pre-training and instruction tuning improves moral understanding in target language contexts.
- **Mechanism:** LlamaELYZA8b (based on MetaLlama8b with additional Japanese pre-training and instruction tuning) achieves 0.479 vs MetaLlama8b's 0.367—a 0.112 improvement. This "indicates the effectiveness of additional Japanese pre-training and instruction tuning."
- **Core assumption:** The improvement stems from Japanese-specific training rather than other architectural or hyperparameter differences.
- **Evidence anchors:**
  - [section 3]: "LlamaELYZA8b is based on MetaLlama8b but has undergone additional pre-training and instruction tuning on data in Japanese"
  - [section 5]: "LlamaELYZA8b scores higher by an average of 0.112"
  - [corpus]: "KokushiMD-10" and "A Japanese Language Model and Three New Evaluation Benchmarks" both show improvements from Japanese domain-specific pre-training
- **Break condition:** If ablation studies show instruction-tuning alone (without pre-training) produces equivalent gains, or if gains are inconsistent across categories, the mechanism would need refinement.

## Foundational Learning

- **Concept: Normative Ethics Categories**
  - **Why needed here:** JETHICS organizes evaluation around five theoretical frameworks (utilitarianism, deontology, virtue ethics, justice, commonsense morality). Understanding these is necessary to interpret category-specific performance differences—e.g., why virtue ethics is hardest (GPT-4o: 0.445) while commonsense is easiest (0.943).
  - **Quick check question:** Can you explain why deontology is split into Role (agent-relativity) and Request (prima facie duty) subcategories?

- **Concept: Inter-annotator Agreement (Cohen's Kappa)**
  - **Why needed here:** The utilitarianism category shows kappa=0.18 ("only slight agreement") versus 0.61 overall. This signals that some moral judgments are inherently subjective, affecting both dataset quality and expected model performance ceilings.
  - **Quick check question:** What does kappa=0.18 for utilitarianism suggest about the feasibility of achieving high model accuracy on that category?

- **Concept: Few-shot Evaluation Protocol**
  - **Why needed here:** All experiments use 8-shot prompting. This design choice affects how we interpret results—models are not fine-tuned on the task, so performance reflects in-context learning rather than trained moral knowledge.
  - **Quick check question:** How might results differ if models were fine-tuned on JETHICS training data versus evaluated zero-shot?

## Architecture Onboarding

- **Component map:**
  JETHICS Dataset (77,896 examples)
  ├── Commonsense Morality (19,963) → Binary classification (acceptable/unacceptable)
  ├── Virtue Ethics (19,920) → Action-trait matching
  ├── Utilitarianism (19,529) → Pairwise comparison (which yields greater well-being)
  ├── Deontology (7,948 total)
  │   ├── Role (4,940) → Appropriateness of role-based obligations
  │   └── Request (3,008) → Appropriateness of refusal to request
  └── Justice (10,536 total)
      ├── Impartiality (5,260) → Fairness of treatment justification
      └── Desert (5,276) → Merit-based deservingness
  Each category uses crowdsourced examples validated by 3-4 annotators via majority vote.

- **Critical path:**
  1. Load category-specific prompts (Table 6)
  2. Format 8-shot examples with instruction + input/output pairs
  3. Query model with test input
  4. Parse single-character response (0/1 or 1/2)
  5. For justice/deontology/virtue: aggregate accuracy across all sentences sharing Sentence 1
  6. Report category accuracy and overall average

- **Design tradeoffs:**
  - **Crowdsourcing vs. Expert Annotation:** Using CrowdWorkers (≥1000 yen/hour) scales to 78K examples but yields variable kappa scores. Expert philosophers might improve consistency but would be cost-prohibitive.
  - **Binary vs. Scalar Labels:** Binary labels simplify evaluation but collapse moral nuance. Utilitarianism's 0.18 kappa may reflect this limitation.
  - **8-shot vs. Fine-tuning:** Few-shot evaluation isolates base model capability but may underestimate performance achievable with task-specific training.

- **Failure signatures:**
  - **Cultural norm failures:** GPT-4o outputs 0 for "singing Anpanman at graduation" when correct is 1 (inappropriate). Indicates missing Japanese cultural context.
  - **Logical reasoning failures:** GPT-4o outputs 1 for "directing lit firework at ground" when correct is 0. Suggests comprehension issues independent of culture.
  - **Category-specific patterns:** Virtue ethics consistently lowest across all models—suggests character-trait inference from actions is fundamentally harder than action classification.

- **First 3 experiments:**
  1. **Baseline replication:** Run the 8-shot evaluation protocol on your target model using the provided prompts (Tables 5-6), sampling 1,000 examples per category. Compare against reported scores to validate setup.
  2. **Category ablation:** Test whether providing category-specific explanations (e.g., brief deontology definitions) in the prompt improves performance over bare instructions.
  3. **Cross-lingual transfer:** Evaluate whether translating JETHICS examples to English and back affects model performance, revealing language-dependence of moral reasoning.

## Open Questions the Paper Calls Out
None

## Limitations
- Annotator agreement varies significantly across categories, with utilitarianism showing only slight agreement (kappa=0.18) versus overall moderate agreement (0.61), potentially creating ceilings for model performance
- Cultural-relativity mechanism conflates cultural knowledge gaps with general comprehension deficits, as some errors appear universal rather than culturally specific
- Scale correlation (Mechanism 2) does not control for potential training data differences between model sizes

## Confidence
- **High confidence**: GPT-4o outperforms all Japanese models (0.7 vs 0.5 average), and larger Japanese models outperform smaller ones within the same family
- **Medium confidence**: Japanese pre-training improves performance (0.112 difference), as architectural differences between models are not fully controlled
- **Low confidence**: Cultural-relativity mechanism, as it's difficult to isolate cultural knowledge gaps from general comprehension deficits

## Next Checks
1. Conduct ablation studies varying prompt format (bare instructions vs. category explanations) to test whether performance differences stem from cultural knowledge or task comprehension
2. Evaluate models on translated JETHICS examples to distinguish language-specific from culture-specific performance effects
3. Test whether fine-tuning Japanese models on JETHICS training data yields larger gains than instruction tuning alone, isolating the contribution of task-specific learning versus general capability