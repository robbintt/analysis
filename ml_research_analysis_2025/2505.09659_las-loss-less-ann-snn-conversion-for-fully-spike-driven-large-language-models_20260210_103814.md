---
ver: rpa2
title: 'LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models'
arxiv_id: '2505.09659'
source_url: https://arxiv.org/abs/2505.09659
tags:
- neuron
- spiking
- time
- llms
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of converting large language
  models (LLMs) to spiking neural networks (SNNs) while preserving performance and
  achieving energy efficiency. Existing methods struggle with activation outliers
  and incompatible nonlinear operations in transformer-based LLMs.
---

# LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models

## Quick Facts
- **arXiv ID**: 2505.09659
- **Source URL**: https://arxiv.org/abs/2505.09659
- **Authors**: Long Chen; Xiaotian Song; Yanan Sun
- **Reference count**: 40
- **Primary result**: Proposes loss-less ANN-SNN conversion framework for LLMs achieving near-lossless performance using 16 time steps

## Executive Summary
This paper addresses the challenge of converting large language models (LLMs) to spiking neural networks (SNNs) while preserving performance and achieving energy efficiency. Existing methods struggle with activation outliers and incompatible nonlinear operations in transformer-based LLMs. The authors propose LAS, a loss-less ANN-SNN conversion framework that introduces two novel neurons: an Outlier-Aware Threshold (OAT) neuron to handle extreme activations using dual sub-neurons, and a Hierarchically Gated (HG) neuron to approximate nonlinear functions through hierarchical decomposition. LAS also designs spike-equivalent Transformer components for fully spiking conversion without performance loss.

## Method Summary
LAS introduces a dual-neuron architecture for ANN-SNN conversion of LLMs. The Outlier-Aware Threshold (OAT) neuron handles extreme activations through a primary and secondary sub-neuron, activating the secondary only when inputs exceed normal ranges. The Hierarchically Gated (HG) neuron approximates nonlinear functions like GELU and LayerNorm through hierarchical decomposition into piecewise linear segments. The framework converts all Transformer components to spiking equivalents, including spike-based self-attention mechanisms that maintain the original attention computation while operating in the spiking domain. The conversion uses only 16 time steps while preserving accuracy across six language models and two vision-language models tested.

## Key Results
- Achieves near-lossless conversion across all tested models using only 16 time steps
- On OPT-66B, LAS improves accuracy by 2% on the WSC task compared to the original ANN model
- Energy analysis shows significant savings, with energy ratios dropping below 0.50 at moderate threshold levels
- Ablation study confirms critical role of both OAT neuron and spike-equivalent self-attention mechanism in maintaining high performance

## Why This Works (Mechanism)

## Foundational Learning
- **ANN-SNN Conversion**: Converting artificial neural networks to spiking neural networks while preserving accuracy requires handling temporal dynamics and spike-based computation. This is needed because standard ANN operations don't directly translate to spike-based representations.
- **Activation Outliers**: Extreme activation values in LLMs can cause information loss during conversion. These outliers occur frequently in transformer attention mechanisms and must be handled to maintain accuracy.
- **Nonlinear Function Approximation**: Transformer components like GELU and LayerNorm contain nonlinear operations incompatible with direct spiking conversion. Hierarchical decomposition into piecewise linear segments enables accurate approximation in the spiking domain.
- **Spiking Self-Attention**: Traditional self-attention mechanisms require matrix operations incompatible with spiking neurons. Spike-based attention mechanisms must preserve the original computation while operating in temporal domain.
- **Temporal Encoding**: Spiking networks operate over time steps, requiring careful balance between conversion accuracy and computational efficiency. Too few time steps cause information loss; too many increase latency and energy consumption.

## Architecture Onboarding

**Component Map**: Input -> OAT Neurons -> HG Neurons -> Spike-Equivalent Transformers -> Output

**Critical Path**: Input activations → OAT neuron processing → HG neuron approximation → Spike-equivalent self-attention → Output layer

**Design Tradeoffs**: 16 time steps provides near-lossless conversion but may not be optimal for all models; dual-sub-neuron approach adds complexity but handles outliers effectively; hierarchical decomposition enables nonlinear approximation but increases computational overhead

**Failure Signatures**: Accuracy degradation when activation outliers exceed secondary neuron capacity; performance loss when hierarchical decomposition insufficient for complex nonlinear functions; energy savings diminish when spiking patterns become dense rather than sparse

**First Experiments**:
1. Test OAT neuron performance on synthetic data with known outlier distributions to verify dual-sub-neuron activation thresholds
2. Validate HG neuron approximation accuracy on individual nonlinear functions (GELU, LayerNorm) before full model integration
3. Measure energy consumption on neuromorphic hardware for different threshold settings to validate theoretical efficiency claims

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on academic benchmarks without real-world deployment scenarios or longer sequence lengths
- 2% accuracy improvement on OPT-66B compared to original ANN model appears inconsistent with typical conversion behavior
- Energy efficiency claims rely on theoretical calculations rather than empirical hardware measurements
- Performance on multi-modal tasks beyond limited vision-language models remains unexplored

## Confidence

**High Confidence**: Architectural innovations (OAT and HG neurons) are well-defined and core conversion methodology is technically coherent; 16 time steps for near-lossless conversion is a concrete, testable claim.

**Medium Confidence**: Ablation study results showing importance of OAT neurons and spike-equivalent self-attention are convincing, but overall performance improvements need independent verification; energy efficiency analysis provides reasonable estimates but lacks hardware validation.

**Low Confidence**: 2% accuracy improvement claim on OPT-66B compared to original ANN model appears anomalous and requires additional validation; scalability to truly large models and method's behavior on diverse real-world datasets remain uncertain.

## Next Checks
1. Replicate the OPT-66B WSC accuracy results on an independent dataset to verify the claimed improvement over the original ANN model, investigating whether this represents genuine enhancement or measurement artifact.

2. Implement the converted SNN on actual neuromorphic hardware (e.g., Loihi, BrainScaleS) to measure real energy consumption and validate theoretical efficiency claims under realistic operating conditions.

3. Test the conversion framework on models with longer sequence lengths (4K-8K tokens) and more diverse multi-modal tasks to evaluate scalability limitations and performance degradation patterns that may emerge under stress conditions.