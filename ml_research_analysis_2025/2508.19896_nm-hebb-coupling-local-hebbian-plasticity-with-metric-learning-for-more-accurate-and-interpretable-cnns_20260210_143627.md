---
ver: rpa2
title: 'NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate
  and Interpretable CNNs'
arxiv_id: '2508.19896'
source_url: https://arxiv.org/abs/2508.19896
tags:
- hebbian
- nm-hebb
- phase
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NM-Hebb is a two-phase training framework that integrates Hebbian
  plasticity with metric learning to improve both accuracy and interpretability of
  CNNs. Phase 1 combines cross-entropy with a Hebbian regularizer and learnable neuromodulation
  to stabilize early features, while Phase 2 uses pairwise metric fine-tuning to compress
  intra-class distances and enlarge inter-class margins.
---

# NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs

## Quick Facts
- arXiv ID: 2508.19896
- Source URL: https://arxiv.org/abs/2508.19896
- Authors: Davorin Miličević; Ratko Grbić
- Reference count: 40
- Primary result: Achieves +2.0–10.0 pp accuracy gains and +0.07–0.15 NMI increases over baseline and comparative methods on CIFAR-10, CIFAR-100, and TinyImageNet across five backbones.

## Executive Summary
NM-Hebb is a two-phase training framework that integrates Hebbian plasticity with metric learning to improve both accuracy and interpretability of CNNs. Phase 1 combines cross-entropy with a Hebbian regularizer and learnable neuromodulation to stabilize early features, while Phase 2 uses pairwise metric fine-tuning to compress intra-class distances and enlarge inter-class margins. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet across five backbones, NM-Hebb achieves +2.0–10.0 pp accuracy gains and +0.07–0.15 NMI increases over baseline and comparative methods. The method also produces tighter, more interpretable class clusters and enhances filter selectivity and semantic alignment, with consistent improvements across architectures, especially for lightweight models.

## Method Summary
NM-Hebb is a two-phase training framework for CNNs that combines Hebbian plasticity with metric learning. Phase 1 trains with cross-entropy plus a Hebbian regularizer that aligns filter weights with activation statistics, modulated by a lightweight MLP that gates the regularization strength based on current loss. This phase stabilizes early features. Phase 2 fine-tunes using pairwise (same/different class) batches with a Euclidean margin loss to tighten class clusters and increase inter-class separation, while a consolidation term prevents deviation from Phase 1 features. The method is evaluated on CIFAR-10/100 and TinyImageNet with five backbone architectures.

## Key Results
- +2.0–10.0 percentage point accuracy gains over baseline and comparative methods on CIFAR-10, CIFAR-100, and TinyImageNet
- +0.07–0.15 increase in Normalized Mutual Information (NMI), indicating improved cluster quality
- Significant reduction in speckle patterns (noise-like filters) and improved filter selectivity (HAF metric)
- Consistent performance improvements across architectures, especially for lightweight models

## Why This Works (Mechanism)

### Mechanism 1: Activation-Weight Alignment
Constraining filter weights to match the spatial statistics of their activations induces higher feature selectivity and reduces redundancy. The $R_{Hebb}$ term minimizes the squared difference between the spatial mean of activations and the mean of kernel weights, forcing filters to evolve toward structured, reusable primitives rather than high-frequency noise. This assumes that aligning first-order statistics of weights and activations correlates with semantic meaningfulness.

### Mechanism 2: Loss-Conditioned Plasticity Gating
Dynamically adjusting the strength of consolidation and Hebbian regularization based on classification error prevents over-constraining features as performance saturates. A lightweight MLP takes the current scalar cross-entropy loss and outputs a gate that scales the consolidation penalty. When error is high, the gate opens for aggressive adaptation; as error drops, the gate closes to freeze learned features. This assumes the scalar loss value is a sufficient statistic for determining plasticity needs.

### Mechanism 3: Metric Space Expansion via Consolidated Fine-Tuning
Fine-tuning with a pairwise margin loss while consolidating Phase 1 features explicitly reshapes embedding geometry, compressing class clusters without destroying foundational features. Phase 2 uses a Euclidean-margin loss to pull same-class pairs and push different-class pairs, while the consolidation term prevents deviation from Phase 1 weights. This assumes Phase 1 features are robust enough that local geometric adjustments won't degrade decision boundaries.

## Foundational Learning
- **Concept**: Hebbian Learning ("Cells that fire together, wire together")
  - **Why needed here**: The $R_{Hebb}$ mechanism relies on the biological intuition that synaptic weights should correlate with pre-synaptic activity. Without this, the alignment logic is just a mathematical constraint without semantic grounding.
  - **Quick check question**: If a filter has high weights but consistently receives low-activation inputs, will the Hebbian term increase or decrease? (Answer: Increase, penalizing the misalignment).

- **Concept**: Elastic Weight Consolidation (EWC)
  - **Why needed here**: The Phase 2 consolidation term is an EWC variant. Understanding that this mathematically penalizes changes to "important" weights (here, all Phase 1 weights) is necessary to understand why Phase 2 doesn't unlearn Phase 1 features.
  - **Quick check question**: In Phase 2, if the consolidation weight $\lambda_{cons}$ were set to 0, what behavior would you expect? (Answer: Catastrophic forgetting or drift of the early features).

- **Concept**: Metric Learning (Contrastive/Margin Loss)
  - **Why needed here**: Phase 2 uses a specific distance-based loss. Understanding that this shapes the geometry of the latent space rather than just the decision boundary is key to interpreting the NMI and t-SNE results.
  - **Quick check question**: Why is a pairwise loss used in Phase 2 instead of just more Cross-Entropy? (Answer: Cross-Entropy only cares about class probabilities, not the absolute distance between class clusters).

## Architecture Onboarding

**Component map**: Backbone (CNN) -> Hebbian Hook (single layer) -> Neuromodulator (2-layer MLP) -> Memory Bank (stores $\theta^-$ for Phase 2)

**Critical path**:
1. Configure single-layer Hebbian hook based on Table 2
2. Run Phase 1 (Single-image batches) → Save $\theta^-$
3. Switch DataLoader to Pairs (Same/Diff class)
4. Run Phase 2 (Pairwise batches) with consolidation active

**Design tradeoffs**:
- **Layer Selection**: Manually selecting layers (e.g., "Last depthwise conv" for MobileNet) balances semantic abstraction vs. classifier disturbance
- **Neuromodulator Complexity**: Uses a tiny MLP (8 units); more complex modulators might overfit but simplicity is sufficient

**Failure signatures**:
- **NaN Loss**: If $\lambda_{hebb}$ is too high, regularization term dominates gradients, causing instability
- **No Accuracy Gain in Phase 2**: Indicates consolidation strength ($\lambda_{cons}$) is too high or margin $m$ is too small
- **Speckle Persistence**: If speckle rate doesn't drop in Phase 1, Hebbian hook may be on wrong layer or $\lambda_{hebb}$ is too low

**First 3 experiments**:
1. **Sanity Check (Phase 1 Only)**: Train ResNet-18 on CIFAR-10 with $L_{CE}$ only vs. $L_{CE} + R_{Hebb}$. Verify speckle reduction in target layer.
2. **Ablation (Neuromodulator)**: Replace learnable MLP $\nu_\phi$ with fixed scalar constant (e.g., 0.5) to confirm adaptive gating provides non-trivial benefit.
3. **Phase 2 Margin Sweep**: Run Phase 2 with margins $m \in \{0.5, 1.0, 2.0\}$ and observe NMI vs. Accuracy tradeoffs to find sweet spot for specific dataset.

## Open Questions the Paper Calls Out
- Does NM-Hebb maintain its performance advantages when scaled to large-scale datasets like ImageNet-1k? (Future work will extend to ImageNet-1k)
- Does the structured feature representation improve robustness against adversarial perturbations or distribution shifts? (Future work will assess robustness)
- Can the framework be effectively adapted for hybrid transformer-convolution or pure transformer architectures? (Future work will explore hybrid transformer-convolution architectures)

## Limitations
- Core Hebbian regularizer mechanism relies on assumptions about weight-activation alignment correlating with semantic filter selectivity, but direct causal evidence is limited to proxy metrics
- The claim that NM-Hebb is "scalable" is based only on five CNNs without testing on very deep or non-standard architectures
- Pairwise metric loss parameters (margin, weighting) lack ablation studies to confirm robustness across datasets

## Confidence
- **High confidence**: Claims about Phase 1 Hebbian stability effects and Phase 2 consolidation preventing forgetting are well-supported by ablation studies and quantitative metrics
- **Medium confidence**: Claims about improved interpretability (cluster visualization, filter selectivity) are supported by proxy metrics but lack qualitative user studies
- **Low confidence**: Claims about biological plausibility and general scalability lack direct evidence beyond immediate experimental scope

## Next Checks
1. Conduct an ablation study on the Euclidean margin $m$ and metric loss weight $\lambda_{metric}$ to confirm claimed NMI gains are not hyperparameter-dependent
2. Implement a Phase 2 ablation with no consolidation ($\lambda_{cons}=0$) to quantify degree of forgetting relative to baseline Phase 1 performance
3. Perform a semantic filter analysis (e.g., via class-conditional activation maximization) to verify that Hebbian regularization produces more interpretable features beyond proxy metrics