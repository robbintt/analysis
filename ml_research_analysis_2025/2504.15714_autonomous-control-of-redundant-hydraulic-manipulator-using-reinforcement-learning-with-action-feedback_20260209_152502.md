---
ver: rpa2
title: Autonomous Control of Redundant Hydraulic Manipulator Using Reinforcement Learning
  with Action Feedback
arxiv_id: '2504.15714'
source_url: https://arxiv.org/abs/2504.15714
tags:
- policy
- joint
- control
- learning
- manipulator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a data-driven approach for autonomous control
  of a 4-DOF hydraulic forestry crane using reinforcement learning. The method requires
  minimal system information, relying on an actuator network to model hydraulic dynamics
  and a forward network for kinematics.
---

# Autonomous Control of Redundant Hydraulic Manipulator Using Reinforcement Learning with Action Feedback

## Quick Facts
- **arXiv ID**: 2504.15714
- **Source URL**: https://arxiv.org/abs/2504.15714
- **Reference count**: 21
- **Primary result**: RL-trained policy for 4-DOF hydraulic forestry crane achieves <80mm tracking error in real-world deployment via sim-to-real transfer

## Executive Summary
This work presents a data-driven approach for autonomous control of a 4-DOF hydraulic forestry crane using reinforcement learning. The method requires minimal system information, relying on an actuator network to model hydraulic dynamics and a forward network for kinematics. The RL controller (DDPG) is trained in simulation with action feedback via the forward network, enabling efficient exploration. Policy 1 (with feedback) outperformed Policy 2 in trajectory tracking, achieving RMSE of [0.017, 0.008, 0.01] m versus [0.031, 0.017, 0.02] m in simulation. The trained policy was successfully deployed on the real system without adaptation, tracking helical trajectories with maximum errors of 75.2, 80.1, and 73.1 mm. This demonstrates the feasibility of Sim-2-Real transfer for heavy-duty manipulators using RL.

## Method Summary
The system uses a DDPG RL agent trained in simulation to control joint variables of a hydraulic manipulator. Two neural networks are trained from real-world data: an actuator network mapping cylinder displacements to joint angles (capturing non-linear hydraulics), and a forward network mapping joint angles to end-effector position. During training, the agent uses the forward network to evaluate multiple candidate actions and select the one minimizing distance to the target. The trained policy outputs joint targets, which are converted to cylinder displacements via the actuator network and sent to PWM valves. The policy is trained entirely in simulation and deployed directly to the real system without fine-tuning.

## Key Results
- Policy 1 (with forward network feedback) achieved RMSE of [0.017, 0.008, 0.01] m vs Policy 2's [0.031, 0.017, 0.02] m in simulation
- Real-world deployment achieved maximum tracking errors of 75.2, 80.1, and 73.1 mm on helical trajectories
- Sim-to-real transfer succeeded without policy adaptation or fine-tuning
- Action feedback reduced training episodes needed for convergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Data-driven actuator networks capture non-linear hydraulic dynamics more effectively than generic physics engine primitives.
- **Mechanism:** The system learns a bi-directional mapping between cylinder displacement and joint variables from real-world manual operation data, effectively "correcting" the simulation's understanding of actuation physics.
- **Core assumption:** Hydraulic non-linearities and hysteresis are repeatable and can be approximated by an MLP trained on gathered data.
- **Evidence anchors:** "non-linear hydraulic actuation dynamics are modeled using actuator networks... to effectively emulate the real system" [abstract]; "The actuator network performs a bi-directional mapping... [incorporating] all the non-linear dynamics" [section IV.B].
- **Break condition:** Physical changes to hydraulic system (temperature, wear) post-training invalidate the static actuator network.

### Mechanism 2
- **Claim:** Augmenting RL exploration with a supervised forward-kinematics network improves sample efficiency and policy quality.
- **Mechanism:** Instead of relying solely on random noise, the agent generates multiple candidate actions, uses the forward network to predict outcomes, and selects the action minimizing distance to target.
- **Core assumption:** The forward network is sufficiently accurate to predict end-effector position, allowing the agent to "look ahead" before committing to a simulation step.
- **Evidence anchors:** "RL agent also receives feedback based on supervised learning of the forward kinematics which facilitates selecting the best suitable action" [abstract]; "Policy 1 (with action feedback) constantly acquires better rewards" [section V.A].
- **Break condition:** High forward network prediction error causes the agent to select actions based on incorrect expectations.

### Mechanism 3
- **Claim:** Decoupling high-level policy (joint control) from low-level actuation (valve control) enables direct sim-to-real transfer.
- **Mechanism:** The RL policy outputs joint variables, which are translated into valve commands via learned actuator networks, isolating the RL agent from the raw voltage/current domain.
- **Core assumption:** The low-level control loop and actuator network translation execute fast enough on real hardware to approximate simulation's discrete time steps.
- **Evidence anchors:** "joint variables are then mapped to the hydraulic valve commands, which are then fed to the system without further modifications" [abstract].
- **Break condition:** Latency in real valve response not present in simulation causes phase lag or instability.

## Foundational Learning

- **Concept: Deep Deterministic Policy Gradient (DDPG)**
  - **Why needed here:** DDPG is an off-policy actor-critic algorithm suited for continuous action spaces (joint angles), which Q-learning cannot handle natively.
  - **Quick check question:** Can you explain why an actor-critic method is necessary for controlling continuous joint velocities/positions versus a discrete value function?

- **Concept: Supervised Learning for Kinematics (Forward/Inverse)**
  - **Why needed here:** The system avoids analytical geometry by training neural networks to approximate forward kinematics and actuator mapping.
  - **Quick check question:** How does the loss function for the forward network differ from the reward function for the RL agent?

- **Concept: Sim-to-Real Transfer**
  - **Why needed here:** Training RL directly on heavy machinery is dangerous and slow; the mechanism relies on bridging the "reality gap" using data-driven models.
  - **Quick check question:** What specific physical phenomenon (unmodeled in this paper's simulation) limits the real-world tracking accuracy to ~80mm max error?

## Architecture Onboarding

- **Component map:** Sensors (draw-wire + angular) -> Pre-processing (manual operation data collection) -> Actuator Network & Forward Network training (supervised) -> Simulation (CoppeliaSim) -> RL Agent (DDPG) training with forward network feedback -> Deployment (joint targets -> actuator network -> PWM valves)

- **Critical path:** The accuracy of the Actuator Network is the linchpin; if the mapping between cylinder displacement and joint angle is wrong, the simulation trains on false physics and the policy will fail immediately upon deployment.

- **Design tradeoffs:** The gripper was removed in simulation to ignore sway dynamics (simplified training but caused real-world tracking errors in Y-axis); using forward network for action selection adds computational overhead but reduces total episodes required.

- **Failure signatures:** Periodic error in Y-axis tracking indicates unmodeled gripper sway; real-world errors (75mm) significantly higher than simulation (27mm) indicates simulation model doesn't capture friction/backlash perfectly.

- **First 3 experiments:**
  1. Static Mapping Validation: Move joints manually; verify Actuator Network predicts cylinder positions with MAE < 2mm.
  2. Forward Network Check: Drive robot to random poses; verify Forward Network predicts EE position within 15-20mm error.
  3. Ablation Study (Sim): Train Policy 2 (no action feedback) vs Policy 1; confirm Policy 1 converges faster/higher reward to validate exploration heuristic.

## Open Questions the Paper Calls Out

- **Can the control framework be extended to actively compensate for the dynamic sway of the gripper during manipulator motion?**
  - **Basis in paper:** [explicit] "will extend our framework to integrate the sway motion during the learning process to model a compensating or aggressive control policy."
  - **Why unresolved:** Current approach detaches gripper during simulation training, leading to tracking errors due to unmodeled dynamic sway.
  - **What evidence would resolve it:** Modified policy accounting for gripper dynamics eliminating periodic Y-axis tracking errors.

- **Can an LSTM-based model effectively capture mechanical backlash to improve tracking accuracy during training?**
  - **Basis in paper:** [explicit] "incorporate a generalized Long Short Term Memory (LSTM) based backlash model, to also take the backlash motion into account during training."
  - **Why unresolved:** Backlash contributes to tracking errors but current methodology doesn't account for hysteresis-like effects.
  - **What evidence would resolve it:** Successful LSTM integration reducing maximum tracking errors by modeling direction-dependent delays.

- **To what extent does the accuracy of the forward kinematics feedback model limit the performance of the RL agent?**
  - **Basis in paper:** [explicit] "still contains minor inaccuracies which might be affecting the learning process"; "A better feedback model will undoubtedly improve the controller performance."
  - **Why unresolved:** Current model facilitates efficient exploration but approximations may prevent optimal precision.
  - **What evidence would resolve it:** Comparison study showing increased forward network training data density directly correlates with reduced RMSE.

## Limitations
- Significant gap between simulation and real-world performance (3Ã— error increase) indicates unmodeled dynamics
- Method relies on manually collected data that may not generalize if hydraulic system properties change over time
- Gripper dynamics were simplified in simulation, leading to real-world tracking errors

## Confidence

- **High**: Direct sim-to-real transfer works without adaptation; actuator networks effectively model hydraulic dynamics
- **Medium**: Forward-network-guided action selection improves exploration efficiency; actuator networks are sufficient for accurate valve control

## Next Checks

1. **Real-world long-term robustness test**: Run deployed policy continuously for 1000+ cycles and monitor tracking error drift to detect degradation from seal wear or temperature shifts.

2. **Forward network ablation on real hardware**: Compare trajectory tracking with and without action feedback (Policy 1 vs Policy 2) on the physical crane to validate claimed exploration benefit.

3. **Explicit unmodeled dynamics test**: Instrument real system to measure gripper sway amplitude; quantify how much of Y-axis tracking error is attributable to this single unmodeled degree of freedom.