---
ver: rpa2
title: Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation
arxiv_id: '2505.12744'
source_url: https://arxiv.org/abs/2505.12744
tags:
- gripper
- axis
- reasoning
- block
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReasonManip, a method that leverages large
  multimodal models' (LMMs) reasoning capabilities for direct robot manipulation by
  predicting next-goal gripper poses through multi-round dialogue. The key innovation
  is reformulating manipulation as sequential goal-wise reasoning using an axis-based
  rotation representation that aligns better with LMM spatial reasoning than traditional
  Euler angles.
---

# Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation

## Quick Facts
- **arXiv ID:** 2505.12744
- **Source URL:** https://arxiv.org/abs/2505.12744
- **Reference count:** 40
- **Primary result:** ReasonManip achieves strong manipulation performance with minimal data (65 samples) through axis-based spatial reasoning and multi-round dialogue.

## Executive Summary
ReasonManip introduces a novel approach to robot manipulation that leverages large multimodal models' reasoning capabilities through sequential next-goal gripper pose prediction in a dialogue format. The key innovation is reformulating manipulation as sequential goal-wise reasoning using an axis-based rotation representation that aligns better with LMM spatial reasoning than traditional Euler angles. The method combines supervised fine-tuning on a small, high-quality reasoning dataset with reinforcement learning to enhance system-2 level reasoning behaviors, achieving strong performance on both simulated and real-world manipulation tasks while maintaining interpretability.

## Method Summary
ReasonManip uses a Qwen2.5-VL-7B-Instruct backbone to predict gripper end poses through multi-round dialogue. The method extracts orthogonal axis vectors (longitudinal, normal, binormal) from point cloud PCA instead of using Euler angles, providing geometrically meaningful representations for LMM spatial reasoning. Training involves two stages: supervised fine-tuning on 65 human-guided reasoning samples across 7 tasks, followed by Group Relative Policy Optimization (GRPO) in simulation to incentivize emergent reasoning behaviors. The system generates complete gripper poses after explicit reasoning steps including collision checking, failure mode discovery, and task replanning, accumulating context across dialogue rounds.

## Key Results
- **Data efficiency:** Achieves strong performance with only 65 training samples (5-10 per task)
- **Generalization:** Zero-shot transfer to MetaWorld tasks with success rates of 30.0% (Move) and 40.0% (Close drawer)
- **Robustness:** Maintains performance across viewpoint changes and demonstrates sim-to-real transfer to UR5 robot with RealSense 435i
- **Representation advantage:** Axis-based rotation representation achieves 96.7% vs 42.0% performance compared to Euler angles on Lift coke can task

## Why This Works (Mechanism)

### Mechanism 1: Axis-Based Spatial Representation for LMM Reasoning
Representing gripper and object orientations via orthogonal axis vectors (longitudinal, normal, binormal) enables more accurate spatial reasoning in LMMs compared to traditional Euler angles. The paper extracts three orthogonal unit vectors from point cloud PCA, providing LMMs with geometrically meaningful axes that can be reasoned about through vector operations rather than complex trigonometric relationships. This representation achieves 96.7% vs 42.0% success rates on the Lift coke can task.

### Mechanism 2: Sequential Goal Prediction via Multi-Round Dialogue
Reformulating manipulation as sequential next-goal predictions in a dialogue format enables LMMs to leverage their reasoning capabilities more effectively than direct action token prediction. Each dialogue round includes current state analysis, collision checking, failure mode discovery, task replanning, and concrete pose prediction. The accumulated dialogue history provides sufficient context for sequential decision-making, improving MetaWorld generalization from 23.3% to 30.0% on Move tasks.

### Mechanism 3: GRPO-Enhanced Reasoning Emergence
Group Relative Policy Optimization applied after supervised fine-tuning incentivizes emergent system-2 reasoning behaviors (self-reflection, verification) that improve generalization to out-of-distribution scenarios. GRPO samples K trajectories with identical initial conditions, computes binary rewards based on task completion, and calculates group-relative advantages, encouraging the model to discover reasoning patterns that generalize across variations rather than memorizing specific trajectories.

## Foundational Learning

- **Rotation representations in robotics (Euler angles vs. axis-angle vs. quaternions vs. rotation matrices):** Understanding gimbal lock, singularities, and geometric interpretability is essential for evaluating the axis-based representation claim. Quick check: Given 90° X rotation then 90° Y rotation, can you predict the final orientation using Euler angles?

- **System-1 vs. System-2 reasoning (Kahneman's framework):** The paper explicitly targets "system-2 level reasoning"—deliberate, conscious thought with self-reflection and verification. Quick check: When catching a ball, which system dominates? When planning multi-step manipulation while checking for obstacles, which system is engaged?

- **Group Relative Policy Optimization (GRPO) and advantage estimation:** Understanding how group-relative advantages normalize rewards across sampled trajectories is critical for reproducing the method. Quick check: In GRPO with K=4 trajectories and rewards [1, 0, 0, 0], what are the normalized advantage values for each trajectory?

## Architecture Onboarding

- **Component map:**
  RGB-D Observation -> Support-Query Segmentation -> Point Cloud Extraction -> Quantitative Scene Information (12D per object part) -> LMM Backbone (Qwen2.5-VL-7B-Instruct) -> Structured Reasoning Output -> Simulator/Robot Execution

- **Critical path:**
  1. Perception fidelity: RGB-D → segmentation accuracy → point cloud completeness → axis extraction quality
  2. Training data quality: Human-guided reasoning chains → SFT initialization → GRPO exploration breadth
  3. Reasoning depth: Dialogue history length → context accumulation → failure detection capability

- **Design tradeoffs:**
  - Data efficiency vs. generalization: 65 samples enable training but create task conflict risks during GRPO
  - Interpretability vs. speed: Multi-round dialogue with explicit reasoning (40s average task completion) vs. direct action prediction
  - Axis representation dimensionality: 9D orientation (3 axis vectors) vs. 3D Euler angles—more LMM-friendly but higher token count

- **Failure signatures:**
  - Incomplete point clouds causing axis estimation errors
  - Complex curved trajectories incompatible with discrete goal formulation
  - Task confusion with limited diverse training data
  - Real-world perception gap from support-query segmentation failures

- **First 3 experiments:**
  1. Ablation on rotation representation: Train identical SFT models using axis-based, Euler angles, and quaternions on same 65 samples
  2. Generalization boundary test: Train on SIMPLER, evaluate zero-shot on MetaWorld with controlled variations
  3. GRPO reward shaping analysis: Run GRPO with alternative reward formulations and measure emergent reasoning behaviors

## Open Questions the Paper Calls Out

### Open Question 1
Can a unified policy be developed to automatically switch between slow System-2 reasoning and high-frequency System-1 action generation? The authors plan to improve control frequency by automatically switching between system-2 reasoning and high-frequency direct action generation, addressing the current limitation of 40s average task completion.

### Open Question 2
How does the integration of upstream point cloud completion or pose estimation modules quantitatively improve robustness against partial occlusions? The current pipeline requires sufficiently dense point clouds to derive the axis-based spatial representation, causing instability in cluttered or sparse visual environments.

### Open Question 3
Is the mathematical formulation for next-goal prediction extensible to high-curvature trajectories that cannot be solved by straightforward mathematical calculation? The discrete, goal-oriented nature of the current formulation struggles with the continuous, fine-grained adjustments required for complex tool manipulation.

## Limitations
- Limited specification of support-query segmentation network and object part database construction
- Task conflicts during GRPO training with only 65 samples acknowledged but not fully resolved
- Real-world deployment details and extensive robustness testing across varied conditions remain under-specified

## Confidence
- **High Confidence:** Axis-based rotation representation improving LMM spatial reasoning (96.7% vs 42.0% performance)
- **Medium Confidence:** GRPO-enhanced reasoning emergence and generalization to MetaWorld (limited task diversity tested)
- **Low Confidence:** System-2 reasoning emergence through GRPO (primarily qualitative claims), viewpoint robustness (not systematically tested)

## Next Checks
1. **Rotation Representation Ablation Study:** Train three identical SFT models using axis-based, Euler angles, and quaternion representations on the same 65 samples to definitively quantify the representation effect.
2. **Controlled Generalization Boundary Test:** Systematically vary generalization conditions by testing zero-shot MetaWorld performance across controlled scenarios to isolate whether generalization stems from axis representation, reasoning format, or GRPO.
3. **GRPO Reward Shaping Analysis:** Implement GRPO with three alternative reward formulations and measure emergent reasoning behaviors including self-correction frequency, reflection depth, and reasoning quality scores.