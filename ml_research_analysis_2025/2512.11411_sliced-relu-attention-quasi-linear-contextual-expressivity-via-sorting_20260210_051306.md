---
ver: rpa2
title: 'Sliced ReLU attention: Quasi-linear contextual expressivity via sorting'
arxiv_id: '2512.11411'
source_url: https://arxiv.org/abs/2512.11411
tags:
- attention
- relu
- sliced
- https
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sliced ReLU attention introduces a new attention mechanism that
  uses one-dimensional projections and sorting to achieve quasi-linear complexity.
  Instead of applying a nonlinearity to pairwise dot products as in softmax attention,
  it operates on projections of key-query differences and leverages sorting to obtain
  O(n log n) complexity.
---

# Sliced ReLU attention: Quasi-linear contextual expressivity via sorting

## Quick Facts
- arXiv ID: 2512.11411
- Source URL: https://arxiv.org/abs/2512.11411
- Reference count: 39
- Primary result: Sliced ReLU attention achieves O(n log n) complexity while preserving universal approximation properties of softmax attention

## Executive Summary
Sliced ReLU attention introduces a novel attention mechanism that replaces the quadratic complexity of traditional softmax attention with quasi-linear O(n log n) complexity by using one-dimensional projections and sorting operations. Instead of computing pairwise dot products between keys and queries, the mechanism projects key-query differences onto random directions and applies a ReLU-based kernel, leveraging sorting to efficiently approximate attention scores. The approach maintains the universal approximation and contextual expressivity properties of softmax attention while offering significant computational advantages for long sequences.

## Method Summary
The method operates by projecting high-dimensional key-query differences onto one-dimensional random directions, then applying a ReLU kernel to these projected scores. Rather than computing all pairwise interactions, it uses sorting to efficiently approximate the attention distribution, reducing complexity from O(n²d) to O(n log n). Value vectors are centered to maintain stability during training. The mechanism uses a learnable projection matrix to capture relevant interaction patterns, and the ReLU activation provides sparse, piecewise-linear attention patterns that can approximate complex dependencies.

## Key Results
- Achieves O(n log n) computational complexity compared to O(n²) for softmax attention
- Preserves universal approximation properties for sequence-to-sequence tasks
- Demonstrates competitive performance on Long Range Arena, Vision Transformer classification, 3D point cloud classification, and masked language model pretraining
- Maintains measure-theoretic universality while reducing computational overhead

## Why This Works (Mechanism)
The mechanism works by exploiting the structure of attention computations through dimensionality reduction and efficient sorting. By projecting high-dimensional interactions onto one dimension, the method reduces the problem to sorting operations, which are computationally efficient. The ReLU kernel introduces sparsity and piecewise-linearity that allows the model to capture relevant interaction patterns without computing all pairwise combinations. The centering of value vectors prevents instability during training by ensuring that the attention distribution remains well-behaved.

## Foundational Learning

**Attention Mechanisms**: Understanding how self-attention computes relationships between sequence elements; needed to grasp why quadratic complexity is problematic; quick check: can you explain softmax attention computation?

**Kernel Methods**: Knowledge of positive definite kernels and their approximation properties; needed to understand the theoretical foundations; quick check: what is the connection between kernels and universal approximation?

**Sorting Algorithms**: Familiarity with O(n log n) sorting complexity; needed to understand the computational advantage; quick check: what is the theoretical lower bound for comparison-based sorting?

**Projection Theorems**: Understanding Johnson-Lindenstrauss and related dimensionality reduction results; needed to grasp why projections preserve relevant information; quick check: what guarantees does random projection provide?

**Universal Approximation**: Knowledge of approximation theory in neural networks; needed to evaluate theoretical claims; quick check: what does it mean for a function class to be universal?

## Architecture Onboarding

**Component Map**: Input sequences → Projection layer → ReLU kernel → Sorting operation → Weighted sum of centered values → Output

**Critical Path**: The sequence of projection → ReLU activation → sorting → weighted aggregation forms the core computational pipeline that determines both efficiency and expressiveness.

**Design Tradeoffs**: The method trades exact pairwise computation for approximate but efficient computation via sorting. This introduces controlled approximation error but gains substantial computational efficiency. The ReLU kernel provides sparsity but may miss certain smooth attention patterns that softmax naturally captures.

**Failure Signatures**: Poor performance on tasks requiring fine-grained, smooth attention distributions; instability when value centering is insufficient; degraded accuracy when projection dimensionality is too low to capture relevant interactions.

**First Experiments**: 1) Compare attention score distributions between softmax and sliced ReLU on synthetic data with known patterns; 2) Benchmark computational time scaling with sequence length for both methods; 3) Evaluate sensitivity to projection dimensionality on downstream task performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims may not fully translate to practical scenarios with varying sequence lengths and data distributions
- Limited empirical validation on large-scale models and datasets raises questions about scalability
- Sorting operations may encounter performance bottlenecks in distributed or memory-constrained environments
- The ReLU kernel design may have limitations in capturing certain attention patterns that softmax naturally handles

## Confidence
**Theoretical Claims (High)**: Mathematical proofs for universal approximation and contextual expressivity appear rigorous and well-founded
**Empirical Validation (Medium)**: Competitive benchmark performance but limited scale and ablation studies reduce generalizability confidence
**Complexity Analysis (Medium)**: O(n log n) claim is theoretically sound but practical implementation details need further investigation

## Next Checks
1. **Scalability Testing**: Evaluate sliced ReLU attention on larger-scale models (>1B parameters) and datasets (full ImageNet-1k, large-scale language modeling) to assess performance degradation and memory efficiency at scale

2. **Ablation Studies**: Systematically test the impact of centering mechanism, ReLU kernel design, and projection dimensionality on performance across diverse task types to identify critical components and potential optimizations

3. **Alternative Normalization Strategies**: Compare sliced ReLU attention against variants using different normalization techniques (layer normalization, batch normalization) and alternative activation functions to determine robustness and optimal configuration choices