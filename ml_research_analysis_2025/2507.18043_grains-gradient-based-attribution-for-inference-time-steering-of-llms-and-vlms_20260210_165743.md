---
ver: rpa2
title: 'GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and
  VLMs'
arxiv_id: '2507.18043'
source_url: https://arxiv.org/abs/2507.18043
tags:
- steering
- tokens
- attribution
- grai
- both
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of steering large language and
  vision-language models at inference time to reduce undesirable behaviors like hallucinations
  and toxicity without retraining. It introduces GRAINS, which uses contrastive gradient-based
  attribution via Integrated Gradients to identify the top-k most influential input
  tokens (visual or textual) contributing to preferred versus dispreferred outputs.
---

# GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs

## Quick Facts
- **arXiv ID:** 2507.18043
- **Source URL:** https://arxiv.org/abs/2507.18043
- **Reference count:** 29
- **Primary result:** Uses gradient-based attribution to steer LLMs/VLMs at inference, outperforming fine-tuning and existing baselines on truthfulness, hallucination reduction, and alignment tasks while preserving capabilities.

## Executive Summary
GrAInS addresses the challenge of steering large language and vision-language models at inference time to reduce undesirable behaviors like hallucinations and toxicity without retraining. The method uses contrastive gradient-based attribution via Integrated Gradients to identify the top-k most influential input tokens contributing to preferred versus dispreferred outputs. These attributions are used to construct directional steering vectors that are applied to hidden activations at each transformer layer during inference. GRAINS consistently outperforms fine-tuning and existing steering baselines, achieving significant accuracy gains on TruthfulQA and reducing hallucination rates on MMHal-Bench while preserving model fluency and general capabilities.

## Method Summary
GrAInS computes Integrated Gradients on a contrastive preference loss to identify causally influential input tokens (visual or textual) that drive behavioral shifts. These tokens are used to construct directional steering vectors via PCA aggregation of activation differences, which are then applied to hidden states at each transformer layer during inference with magnitude normalization. The method requires 50 paired preference examples per steering objective and operates entirely at inference time without modifying model weights.

## Key Results
- Achieves 13.22% accuracy gain on TruthfulQA with Llama-3.1-8B compared to base model
- Reduces hallucination rates on MMHal-Bench from 0.624 to 0.514 with LLaVA-1.6-7B
- Improves alignment win rates on SPA-VL by 8.11% while preserving MMLU/MMMU performance within 0.5% of base

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Gradient Attribution for Token Selection
Identifying causally influential input tokens via Integrated Gradients over a preference-based loss enables more targeted steering than global intervention vectors. IG computes token-level attributions by integrating gradients along a path from a baseline to the actual input, using the contrastive objective f(x) = log P(ypos|x) - log P(yneg|x). Positive attributions identify tokens supporting preferred outputs; negative attributions identify tokens driving dispreferred behavior.

### Mechanism 2: PCA-Derived Directional Steering Vectors
Aggregating contrastive activation shifts via PCA produces stable, generalizable steering directions that capture the semantic shift from undesirable to desirable behavior. After identifying top-k positive and negative tokens, the method masks each set to create contrastive inputs, extracts hidden states at the final token position across layers, computes delta vectors, and applies PCA to extract the top principal component as the steering vector.

### Mechanism 3: Normalized Additive Intervention with Magnitude Preservation
Adding steering vectors to hidden activations followed by magnitude normalization preserves representational scale, reducing overcorrection and maintaining fluency. At inference, the steering vector is added to hidden states, then normalized to the original magnitude using the ratio of norms. This ensures the intervention shifts direction without exploding magnitude.

## Foundational Learning

- **Integrated Gradients (IG):** Why needed: Core attribution method for identifying causally influential tokens. IG addresses gradient saturation and satisfies sensitivity/implementation invariance properties that vanilla gradients lack. Quick check: Can you explain why IG integrates gradients along a path from baseline to input rather than computing gradients directly at the input?
- **Inference-Time Activation Steering:** Why needed: The overall paradigm—modifying hidden states without weight updates. Requires understanding how additive interventions affect transformer computations. Quick check: What is the difference between steering via activation addition vs. decoding-time interventions (e.g., contrastive decoding)?
- **Contrastive Preference Objectives:** Why needed: The attribution objective uses log P(ypos|x) - log P(yneg|x) rather than single-output logits. This aligns with preference optimization frameworks (RLHF, DPO). Quick check: Why might a contrastive preference loss provide better steering signals than a single-reference likelihood objective?

## Architecture Onboarding

- **Component map:** Input examples → Attribution Module (IG computation) → Vector Construction Module (masking, delta extraction, PCA) → Inference Steering Module (additive intervention with normalization)
- **Critical path:** 1) Collect 50 paired preference examples per dataset 2) Run IG attribution (5-10 steps) → ~96-302s on A6000 3) Extract hidden states, run PCA → steering vectors 4) Tune λ on dev set 5) Apply at inference with normalization
- **Design tradeoffs:** k (token count): Lower k (3-5) more effective; larger k dilutes attribution. IG steps: More steps = better approximation but higher compute. Steering dataset size: Paper uses 50 samples; scaling effects not analyzed. Layer-wise vs. single-layer: Paper applies at all layers; layer selection not ablated.
- **Failure signatures:** Overcorrection: Fluency degradation, factual errors → reduce λ. Undercorrection: No behavior change → increase λ or check preference data quality. Capability loss on general tasks → method may be applying too globally.
- **First 3 experiments:** 1) Attribution validation: Ablate top-k positive vs. negative tokens and measure preference shift. 2) Steering strength sweep: Run λ ∈ {2,4,6,8,10} on TruthfulQA dev set. 3) Modality ablation on VLM: Compare vision-only, text-only, and joint attribution.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can steering vectors trained for one undesirable behavior (e.g., hallucination reduction) transfer to mitigate other behaviors (e.g., toxicity or bias), or must vectors be task-specific? The paper constructs separate steering vectors for each dataset/task but does not test cross-task transfer, leaving unclear whether latent directions encode task-specific semantics or general alignment properties.
- **Open Question 2:** How does steering performance scale with the number of samples used to construct vectors, and what is the minimum viable sample size? The paper uses 50 samples per dataset but provides no analysis of how performance varies with sample size, only hyperparameter analysis for α and k.
- **Open Question 3:** Is applying steering vectors at all transformer layers necessary, or could selective layer intervention achieve comparable results with reduced computational overhead? Section 3.3 states vectors are applied "across layers" and "at each transformer layer," but no ablation studies isolate which layers contribute most to steering effectiveness.

## Limitations
- Attribution quality bounds performance: Random token selection still achieves 52.81% accuracy, suggesting IG attributions may not be truly causal.
- PCA aggregation sensitivity: The paper assumes top principal component captures dominant behavioral shift but doesn't ablate alternatives like mean aggregation or using multiple PCs.
- Task-specific steering vectors: Separate vectors must be computed for each undesirable behavior, limiting composability and requiring new preference data collection.

## Confidence

- **High:** Claims about outperforming baselines on specific datasets (TruthfulQA, MMHal-Bench, SPA-VL). These are directly measurable with clear comparisons provided.
- **Medium:** Claims about IG attributions being causally meaningful, PCA aggregation being optimal, and normalization preserving fluency. These rely on assumptions validated indirectly but not ablated.
- **Low:** Claims about scalability to new tasks/datasets, long-term behavior, and robustness to adversarial inputs. These are not tested in the paper.

## Next Checks

1. **Ablation on Steering Vector Construction:** Compare PCA-based aggregation vs. mean vs. weighted averaging of delta vectors. Test using multiple principal components vs. top-1. Measure impact on TruthfulQA accuracy and SPA-VL win rates.

2. **Attribution Quality Validation:** Compare IG attributions vs. SmoothGrad, SHAP, or random selection on the same preference datasets. Use ablation studies to confirm that top-k IG tokens causally shift preference more than random tokens. Test sensitivity to IG step count (m=5 vs. m=10 vs. m=20).

3. **Capability Preservation Analysis:** Run steering on diverse tasks: MMLU (knowledge), MMMU (multimodal reasoning), HumanEval (coding), and creative writing prompts. Measure task-specific gains/losses to identify steering's domain dependence. Compare against baselines (DSO, SteerVLM, CorrSteer) on the same task mix.