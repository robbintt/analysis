---
ver: rpa2
title: 'Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables
  Expert-Level Humor Ranking in LLMs'
arxiv_id: '2502.20356'
source_url: https://arxiv.org/abs/2502.20356
tags:
- caption
- humor
- human
- cartoon
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of humor understanding in Large
  Language Models (LLMs), specifically in the context of the New Yorker Cartoon Caption
  Contest. The authors decompose the problem into three components: visual understanding,
  humor reasoning, and alignment with human preferences.'
---

# Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs

## Quick Facts
- arXiv ID: 2502.20356
- Source URL: https://arxiv.org/abs/2502.20356
- Reference count: 16
- Primary result: 82.4% accuracy in New Yorker cartoon caption ranking, matching human expert performance

## Executive Summary
This paper addresses the challenge of humor understanding in Large Language Models (LLMs), specifically in the context of the New Yorker Cartoon Caption Contest. The authors decompose the problem into three components: visual understanding, humor reasoning, and alignment with human preferences. They systematically improve each component by enhancing visual annotations, utilizing LLM-generated humor explanations, and implementing targeted alignment with human preference data. Their refined approach achieves 82.4% accuracy in caption ranking, significantly improving upon the previous 67% benchmark and matching the performance of world-renowned human experts. Notably, while persona-based prompting showed minimal impact, model finetuning with crowd preferences proved remarkably effective.

## Method Summary
The approach involves three key stages: First, generating and human-correcting visual descriptions of cartoons using GPT-4o, producing both literal ("canny") and unexpected ("uncanny") elements. Second, generating humor explanations using o1-preview for each caption. Third, fine-tuning GPT-4o on 5,580 pairwise comparisons from the contest crowd, learning to predict which caption the crowd preferred. The model is evaluated on a 100-cartoon test set, comparing easy pairs (top-ranked vs. bottom-ranked captions) and hard pairs (close rankings).

## Key Results
- Fine-tuned GPT-4o achieves 82.4% accuracy in ranking captions, matching world-renowned human experts
- Human-corrected visual descriptions improve accuracy from 70% to 73% over uncorrected machine-generated descriptions
- Supervised fine-tuning on crowd preference data (5,580 pairs) outperforms persona-based prompting by 6 percentage points (82.4% vs 76.5%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Correcting visual annotation errors improves downstream humor ranking.
- Mechanism: Human-in-the-loop review identifies and fixes factual/semantic errors in LLM-generated cartoon descriptions; models receive accurate scene representations for reasoning.
- Core assumption: Accurate visual grounding is a prerequisite for correct humor inference.
- Evidence anchors:
  - [section 4.1] "23.5% (89/379) of the machine-generated descriptions contain inaccuracies"
  - [section 4.1] "Comparative experiments between using the original and refined descriptions show an accuracy improvement from 70% to 73% with GPT-4o prompting"
  - [corpus] Weak direct corpus support; related work on multimodal humor (Baluja 2025) emphasizes visual cues but not annotation correction per se.
- Break condition: If descriptions are already high-quality, further correction yields diminishing returns; if humor depends primarily on linguistic rather than visual elements, visual grounding matters less.

### Mechanism 2
- Claim: High-quality humor explanations from reasoning models improve ranking accuracy.
- Mechanism: o1-preview generates detailed explanations of why captions are humorous; these serve as intermediate reasoning traces that help the ranking model distinguish objective humor cues from subjective elements.
- Core assumption: Explicit reasoning about humor mechanisms helps models generalize preference patterns.
- Evidence anchors:
  - [section 4.2] "o1-preview explanations boost ranking accuracy to 76%, compared to 73% for the baseline and 71% for GPT-4o-generated explanations"
  - [section 4.2] "Our humor expert found that over 85% of o1-preview explanations effectively captured a cartoon's humor"
  - [corpus] HUMORCHAIN (arxiv 2511.21732) supports theory-guided multi-stage reasoning for humor tasks.
- Break condition: If explanations are low-quality or misaligned with crowd preferences, they add noise; if the target audience's preferences diverge from "objective" humor analysis, explanations may not help.

### Mechanism 3
- Claim: Direct supervised fine-tuning on crowd preference data is substantially more effective than persona-based prompting for aligning to subgroup preferences.
- Mechanism: Fine-tuning on 5,580 pairwise comparisons from the contest crowd directly adjusts model weights to internalize the specific preference distribution of this audience subgroup.
- Core assumption: Subgroup preferences are learnable from pairwise comparison data but not reliably simulable via prompting.
- Evidence anchors:
  - [abstract] "while persona-based prompting showed minimal impact, model finetuning with crowd preferences proved remarkably effective"
  - [section 4.3.1] "persona-based prompts yield only modest improvements... with the highest accuracy of 76.5%... a mere 3% gain over the baseline"
  - [section 4.3.2] "fine-tuned GPT-4o models can significantly improve upon all prompting-based baseline... achieve slightly higher accuracy than the average over human experts"
  - [corpus] Creative Preference Optimization (arxiv 2505.14442) aligns with preference-based creative alignment, though not humor-specific.
- Break condition: If preference data is noisy, sparse, or unrepresentative of the target subgroup, fine-tuning may overfit or misalign; if the target subgroup has heterogeneous internal preferences, a single fine-tuned model may not capture diversity.

## Foundational Learning

- Concept: **Pairwise preference ranking**
  - Why needed here: The core evaluation task is comparing two captions and predicting which the crowd ranked higher; this is simpler than absolute scoring and naturally captures relative preferences.
  - Quick check question: Can you explain why pairwise comparison is more robust than asking a model to score humor on a 1-10 scale?

- Concept: **Supervised Fine-tuning (SFT) for alignment**
  - Why needed here: The paper's breakthrough came from SFT on preference data, not from prompting; understanding how SFT adapts pretrained models to specific objectives is essential.
  - Quick check question: What is the difference between in-context learning (few-shot prompting) and supervised fine-tuning in terms of how model weights are affected?

- Concept: **Subjectivity and subgroup preference heterogeneity**
  - Why needed here: The paper argues that creative tasks lack verifiable rewards and require understanding group-specific preferences; this frames the broader AGI challenge.
  - Quick check question: Why might a model trained on general internet data fail to match the specific humor preferences of New Yorker readers?

## Architecture Onboarding

- Component map: Visual descriptions (GPT-4o + human correction) -> Humor explanations (o1-preview) -> Alignment model (GPT-4o fine-tuned on crowd preferences)

- Critical path: Correct visual descriptions -> High-quality explanations -> Fine-tuned ranking model. Errors propagate; bad descriptions lead to bad explanations.

- Design tradeoffs:
  - Prompting vs. fine-tuning: Prompting is cheaper and faster but failed to capture subgroup preferences in this study; fine-tuning requires labeled data but achieved expert-level performance.
  - Data scale: Only 5,580 pairs needed; suggests small-scale alignment can be highly effective for narrow domains.
  - Explanation inclusion: Fine-tuning with explanations (82.4%) slightly outperformed without (79.4%), but marginal benefit.

- Failure signatures:
  - Persona prompting yields <5% improvement over baseline -> indicates LLMs cannot reliably simulate specific subgroup preferences via prompts alone.
  - GPT-4o explanations hurt performance (71% vs. 73% baseline) -> lower-quality reasoning adds noise.
  - Narrow margin tasks (30 vs. 300 ranking) harder -> 63.2% vs. 82.4% on easy pairs.

- First 3 experiments:
  1. Replicate the baseline: Run GPT-4o with 5-shot in-context learning on the pairwise ranking task; confirm ~67-73% accuracy.
  2. Ablate explanation quality: Compare o1-preview vs. GPT-4o vs. no explanations; verify that explanation quality correlates with ranking accuracy.
  3. Compare alignment methods: Test persona-based prompting vs. SFT on a small held-out set; quantify the gap (should be ~10-15 percentage points based on paper).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the fine-tuned ranking model effectively serve as a verifier to improve LLM humor generation through inference-time scaling?
- Basis in paper: [explicit] The Conclusion states the model "can serve as a reliable verifier for humor generation, enabling inference-time scaling techniques... to improve creative output."
- Why unresolved: The paper empirically validates discrimination (ranking) accuracy, but does not demonstrate success in production (generation).
- What evidence would resolve it: Using the fine-tuned model as a reward model or verifier for an LLM generating new captions, followed by human evaluation of the generated humor.

### Open Question 2
- Question: Does the decomposition strategy (visual, reasoning, alignment) generalize to other creative domains that lack objective metrics, such as musical composition or architectural design?
- Basis in paper: [explicit] Section 5 asks "how might we gather equivalent preference data for domains like musical composition, architectural design...?" and Section 7 notes the "narrow focus may limit the generalizability."
- Why unresolved: The study is restricted to the New Yorker Cartoon Caption Contest, a specific visual/linguistic domain with a structured contest format.
- What evidence would resolve it: Applying the same three-component framework to a different creative domain (e.g., evaluating music) and measuring alignment with expert human preferences.

### Open Question 3
- Question: How can creative alignment frameworks be adapted to detect or mitigate offensive content while maintaining high humor accuracy?
- Basis in paper: [explicit] Section 7 acknowledges the framework "does not explicitly address the detection or mitigation of offensive content, highlighting the need for future research."
- Why unresolved: Humor is subjective and often tests social boundaries; current accuracy metrics do not penalize or filter for offensive outputs.
- What evidence would resolve it: Evaluating the model on a dataset containing sensitive topics to measure the trade-off between ranking accuracy and the generation of unsafe or offensive content.

## Limitations

- Narrow domain specificity: The model is fine-tuned on a specific dataset (New Yorker Cartoon Caption Contest) and may not generalize to other humor contexts or creative tasks.
- Human annotation scalability: The human correction process for visual descriptions, while effective, may not scale efficiently to larger datasets.
- Premium model dependency: Reliance on GPT-4o and o1-preview raises questions about practical deployment costs and accessibility.

## Confidence

- **High confidence**: The core finding that supervised fine-tuning on crowd preference data significantly outperforms persona-based prompting (82.4% vs 76.5% accuracy) is well-supported by systematic ablations and aligns with established preference learning literature.
- **Medium confidence**: The claim that human-corrected visual annotations improve accuracy from 70% to 73% is supported, but the exact contribution of "uncanny" vs "canny" descriptions and the 23.5% error rate warrant replication across different annotation teams.
- **Medium confidence**: The mechanism that o1-preview explanations improve accuracy to 76% is supported, but the marginal gain from including explanations in fine-tuning (82.4% vs 79.4%) suggests this benefit may be overstated or context-dependent.
- **Low confidence**: The broader claim about addressing "the core challenge of AGI" through this approach extrapolates beyond the empirical scope and lacks validation outside the specific humor ranking task.

## Next Checks

1. **Cross-domain transfer test**: Evaluate the fine-tuned model on a different humor dataset (e.g., memes, stand-up comedy transcripts, or international humor) to assess generalizability beyond New Yorker cartoons. Measure performance drop and identify which components (visual understanding, humor reasoning, or alignment) are most domain-specific.

2. **Scaling analysis of human annotation**: Systematically vary the proportion of corrected descriptions (0%, 10%, 50%, 100%) to quantify the relationship between annotation effort and accuracy gains. This would reveal whether the 23.5% correction rate is optimal or if there's a threshold beyond which additional corrections yield diminishing returns.

3. **Ablation of explanation quality vs. quantity**: Generate explanations using different models (GPT-4o, Claude, Llama) and with varying levels of detail (one sentence vs. detailed reasoning). Compare not just final accuracy but also the correlation between explanation quality scores and ranking performance to determine if high-quality explanations are necessary or if quantity alone suffices.