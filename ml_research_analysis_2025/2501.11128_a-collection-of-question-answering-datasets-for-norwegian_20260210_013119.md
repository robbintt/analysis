---
ver: rpa2
title: A Collection of Question Answering Datasets for Norwegian
arxiv_id: '2501.11128'
source_url: https://arxiv.org/abs/2501.11128
tags:
- datasets
- question
- norwegian
- answer
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new suite of question answering datasets\
  \ for Norwegian, covering both Bokm\xE5l and Nynorsk variants. The datasets include\
  \ NorOpenBookQA (3.5k examples), NorCommonSenseQA (1.1k examples), NorTruthfulQA\
  \ (545 multiple-choice and 471 generation examples), and NRK-Quiz-QA (4.9k examples)."
---

# A Collection of Question Answering Datasets for Norwegian

## Quick Facts
- arXiv ID: 2501.11128
- Source URL: https://arxiv.org/abs/2501.11128
- Authors: Vladislav Mikhailov; Petter Mæhlum; Victoria Ovedie Chruickshank Langø; Erik Velldal; Lilja Øvrelid
- Reference count: 10
- Primary result: Introduces four Norwegian QA datasets covering both Bokmål and Nynorsk, showing models perform better in Bokmål than Nynorsk and struggle with commonsense reasoning.

## Executive Summary
This paper introduces a comprehensive suite of question answering datasets for Norwegian, covering both written standards (Bokmål and Nynorsk). The datasets include NorOpenBookQA (3.5k examples), NorCommonSenseQA (1.1k examples), NorTruthfulQA (545 multiple-choice and 471 generation examples), and NRK-Quiz-QA (4.9k examples). Created through manual translation and localization by native speakers, these datasets evaluate various skills including world knowledge, commonsense reasoning, and truthfulness. The authors evaluate 11 language models, finding that most perform better in Bokmål than Nynorsk, struggle with commonsense reasoning, and tend to generate untruthful answers.

## Method Summary
The authors created four Norwegian QA datasets through a two-stage annotation process: manual translation/localization of English datasets by native speakers followed by quality curation. NorOpenBookQA covers world knowledge (3.5k examples), NorCommonSenseQA tests commonsense reasoning (1.1k examples), NorTruthfulQA evaluates truthfulness in both multiple-choice (545 examples) and generation formats (471 examples), and NRK-Quiz-QA contains Norwegian-specific knowledge questions (4.9k examples). Datasets are hosted on HuggingFace with curation status metadata. The authors evaluate 11 language models using zero-shot and few-shot settings through the NorEval framework, comparing performance across language variants and reasoning types.

## Key Results
- Most language models perform 5-18% better on Bokmål than Nynorsk across all datasets
- Continuous pretraining on Norwegian corpora improves Norwegian-specific knowledge but shows asymmetric benefits across language variants
- Models struggle with commonsense reasoning (NorCommonSenseQA) and tend to generate untruthful answers on NorTruthfulQA
- NRK-Quiz-QA proves most challenging, testing Norwegian-specific knowledge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Manual translation with localization by native speakers preserves linguistic and cultural validity better than machine translation.
- **Mechanism:** Native annotators translate English examples while adapting content to Norwegian contexts (e.g., replacing "president" with "statsministeren," "White House" with "Stortinget"), then create novel examples from scratch.
- **Core assumption:** Human-mediated localization captures nuances that machine translation misses, particularly for culture-specific knowledge.
- **Evidence anchors:** [abstract] "datasets... created by native speakers"; [section 3.1.1] "The annotation task here involves adapting the English examples... using two strategies: 1. Manual translation and localization... 2. Creative adaptation"
- **Break condition:** If quality validation shows no significant difference between machine-translated and human-localized examples, the resource-intensive manual process may not justify costs.

### Mechanism 2
- **Claim:** Two-stage annotation (creation + curation) filters low-quality examples more effectively than single-pass annotation.
- **Mechanism:** Stage 1 collects translations/adaptations; Stage 2 applies quality judgment (filter low-quality) and quality control (fix spelling/grammar). Each example validated by one annotator.
- **Core assumption:** Sequential quality gates catch errors that single-pass annotation misses.
- **Evidence anchors:** [section 3.1] "We conduct a two-stage in-house annotation... followed by a separate stage for curating NRK-Quiz-QA"; [section 7] "Due to limited resources, we curate only 80% of all 10.5k collected examples, with each example validated by one annotator. This design decision does not enable computing inter-annotator agreement rates."
- **Break condition:** If inter-annotator agreement were very high, single-stage might suffice; if very low, more stages or annotators per example needed.

### Mechanism 3
- **Claim:** Continuous pretraining on Norwegian corpora improves Norwegian-specific knowledge but may not transfer equally to both written standards (Bokmål vs. Nynorsk).
- **Mechanism:** Models adapted with Norwegian data (NorwAI-Mistral-7B, NorMistral-7B-warm) outperform base models on NRK-Quiz-QA (Norwegian-specific knowledge) and NorCommonSenseQA, but the improvement is asymmetric across NB and NN.
- **Core assumption:** Pretraining data contains more Bokmål than Nynorsk, creating unequal representation.
- **Evidence anchors:** [section 5] "Most LMs perform better in NB than NN on all datasets... The accuracy δ-scores range from 5% to 8% on NorCommonSenseQA"; [section 5] "Continuous pretraining of Mistral-7B on the Norwegian corpora... generally improves the LMs' Norwegian-specific knowledge (NRK-Quiz-QA) and common sense reasoning abilities"
- **Break condition:** If pretraining corpora were balanced across NB/NN, performance gaps should narrow; persistent gaps suggest architectural or data distribution issues.

## Foundational Learning

- **Concept: Zero-shot vs. Few-shot Evaluation**
  - **Why needed here:** The paper evaluates models in both regimes; understanding this distinction is essential for interpreting results correctly.
  - **Quick check question:** Can you explain why k=16 does not consistently outperform k=4 on NorOpenBookQA?

- **Concept: Extractive vs. Multiple-Choice vs. Generative QA**
  - **Why needed here:** Each dataset targets different reasoning types requiring different evaluation metrics (accuracy vs. ROUGE-L).
  - **Quick check question:** Which metric would you use for NorTruthfulQA Generation and why?

- **Concept: Language Varieties and Resource Asymmetry**
  - **Why needed here:** Norwegian has two official written standards with unequal digital resources; this directly affects model performance patterns.
  - **Quick check question:** Why might a model perform 5-18% better on Bokmål than Nynorsk on the same task?

## Architecture Onboarding

- **Component map:** NorEval framework (built on lm-evaluation-harness) -> handles prompt pooling (50 prompts per language variety) and evaluation -> HuggingFace Transformers -> hosts all 11 evaluated models -> Dataset cards on HuggingFace -> store all four datasets with curation status metadata

- **Critical path:** 1. Load dataset from HuggingFace (e.g., `ltg/noropenbookqa`) 2. Format with appropriate prompt template (see Table 3) 3. For MC: compute token probabilities for each choice, select max 4. For generation: greedy decode, compute ROUGE-L against reference answers

- **Design tradeoffs:** Single-annotator validation (faster, cheaper) vs. multi-annotator (enables IAA, higher quality); Zero-shot test sets (uncontaminated) vs. train splits (enables few-shot/fine-tuning) — only NorOpenBookQA has train split; Localization fidelity vs. cross-lingual comparability — localized examples harder to compare with English originals

- **Failure signatures:** Models scoring near random baseline (25% on 4-choice, 20% on 5-choice) -> likely insufficient Norwegian capability; Large NB/NN gap (>10%) -> Nynorsk underrepresentation in training; High TruthfulQA scores -> possible data contamination or memorization

- **First 3 experiments:** 1. Establish baseline: Run Mistral-7B on all four datasets in zero-shot, compare NB vs. NN performance delta 2. Ablate prompt sensitivity: Test same model with 5 random prompts from the 50-prompt pool, measure variance 3. Validate localization effect: Compare performance on human-translated vs. human-written (from scratch) examples in NorCommonSenseQA

## Open Questions the Paper Calls Out

- **Open Question 1:** What are human baselines on the four Norwegian QA datasets, and how large is the gap between human and model performance? The authors state in the Limitations section: "While we recognize the importance of human baselines, limited resources prevent us from establishing them for our datasets. We leave this for future work."

- **Open Question 2:** How does instruction fine-tuning affect model performance on these Norwegian QA benchmarks compared to the pretrained-only models evaluated? Future work includes "conducting experiments in a cross-lingual scenario using related QA resources in other languages and instruction-finetuned LMs."

- **Open Question 3:** To what extent does data contamination affect performance on NRK-Quiz-QA, given its source from publicly available NRK quizzes? The authors acknowledge: "We acknowledge that the performance on NRK-Quiz-QA can be influenced by potential data leakage" and note that methods for detecting test data contamination are of special interest.

- **Open Question 4:** How do human judges rate the truthfulness and quality of LM-generated answers on NorTruthfulQA Generation compared to automated ROUGE-L scores? The authors state: "We leave a human-based evaluation of the generated outputs for a more detailed analysis of the LMs' performance for future work."

## Limitations
- Single-annotator validation prevents computing inter-annotator agreement rates
- Only 80% of collected examples were curated due to resource constraints
- Prompt pool is only partially specified, limiting reproducibility
- No human baselines established for comparison with model performance

## Confidence

- **High Confidence:** General performance patterns showing NB outperforming NN across models (5-18% gaps) and the relative difficulty rankings of datasets
- **Medium Confidence:** Specific performance numbers and the claim that continuous pretraining improves Norwegian-specific knowledge
- **Low Confidence:** Precise impact of localization quality on model performance due to lack of systematic comparison with machine-translated baselines

## Next Checks

1. **Inter-annotator agreement validation:** Re-evaluate a subset of NorCommonSenseQA examples with multiple annotators to establish IAA rates and validate the single-annotator curation approach.

2. **Localization fidelity test:** Systematically compare model performance on human-localized examples versus their machine-translated counterparts in NorOpenBookQA to quantify the localization effect.

3. **Prompt sensitivity analysis:** Evaluate model variance across the full 50-prompt pool (not just the 5 shown) for each language variant to assess prompt sensitivity and establish confidence intervals around performance metrics.