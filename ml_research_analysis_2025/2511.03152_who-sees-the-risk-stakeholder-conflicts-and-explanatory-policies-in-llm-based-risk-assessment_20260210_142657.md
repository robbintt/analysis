---
ver: rpa2
title: Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based
  Risk Assessment
arxiv_id: '2511.03152'
source_url: https://arxiv.org/abs/2511.03152
tags:
- risk
- stakeholder
- risks
- stakeholders
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a stakeholder-grounded framework for AI risk
  assessment using LLMs as judges to predict and explain risks. The method employs
  the Risk Atlas Nexus and GloVE explanation pipeline to generate stakeholder-specific,
  interpretable policies that reveal where and why stakeholders agree or disagree
  on the same risks.
---

# Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment

## Quick Facts
- arXiv ID: 2511.03152
- Source URL: https://arxiv.org/abs/2511.03152
- Reference count: 7
- One-line primary result: Stakeholder-grounded LLM framework predicts and explains divergent AI risk perceptions across personas using paraphrase-robust intersection and IF/DESPITE clause conflict scoring.

## Executive Summary
This paper introduces a stakeholder-grounded framework for AI risk assessment using LLMs as judges to predict and explain risks. The method employs the Risk Atlas Nexus and GloVE explanation pipeline to generate stakeholder-specific, interpretable policies that reveal where and why stakeholders agree or disagree on the same risks. Experiments with three real-world AI use cases (medical AI, autonomous vehicles, and fraud detection) demonstrate that stakeholder perspectives significantly influence risk perception and conflict patterns. An interactive visualization is proposed to help users explore these conflicts and their underlying reasoning. The approach enhances transparency and interpretability of LLM-based evaluations, supporting more inclusive and context-sensitive AI governance. Limitations include reliance on synthetic stakeholders and binary risk outcomes.

## Method Summary
The framework generates synthetic stakeholders for each use case, then creates paraphrased versions of stakeholder-specific use cases using six linguistic transformations. IBM Risk Atlas Nexus predicts risks for each paraphrase, with only risks predicted across all paraphrases retained for each stakeholder. GloVE generates IF/DESPITE rule-based explanations for each stakeholder-risk pair. Conflicts are detected when stakeholders disagree on the same risk, and conflict scores are computed via semantic similarity between opposing IF and DESPITE clauses. The approach produces binary risk classifications and an interactive visualization for exploring stakeholder conflicts.

## Key Results
- Stakeholder perspectives significantly influence risk perception and conflict patterns
- Paraphrase-invariant risk prediction increases robustness to prompt sensitivity
- Semantic similarity of IF/DESPITE clauses reveals conceptual conflicts between stakeholder reasoning
- Interactive visualization enables exploration of stakeholder conflicts and underlying explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Assigning stakeholder personas to LLM judges induces divergent risk assessments for identical use cases
- Mechanism: Persona-conditioned prompts bias the LLM's attention toward role-specific concerns, causing the same base use case to produce different predicted risk sets
- Core assumption: LLM persona simulation faithfully approximates real stakeholder reasoning patterns
- Evidence anchors:
  - [abstract]: "Our results show that stakeholder perspectives significantly influence risk perception and conflict patterns."
  - [section: Experiments and Results]: "These variation support our core hypothesis that stakeholder roles and contexts significantly influence risk perception"
  - [corpus: Multi-Stakeholder Alignment in LLM-Powered Collaborative AI Systems]: Multi-stakeholder alignment frameworks similarly assume diverse stakeholder values produce conflicting assessments requiring negotiation mechanisms
- Break condition: If persona-conditioned LLM outputs do not correlate with actual human stakeholder judgments, the mechanism may amplify synthetic artifacts rather than real perspective differences

### Mechanism 2
- Claim: Intersecting predictions across paraphrased prompts yields more stable, conservative risk sets than single-prompt evaluation
- Mechanism: Six linguistic transformations generate semantically equivalent variants. Only risks predicted across all valid paraphrases are retained, filtering out prompt-specific brittleness
- Core assumption: Paraphrase-invariant predictions indicate genuine risk relevance rather than prompt artifacts
- Evidence anchors:
  - [section: Risk Assessment]: "This approach ensures that the final risk set for each stakeholder reflects only stable, paraphrase-invariant predictions."
  - [section: Experiments and Results]: "For most stakeholders, the majority of predictions were classified as not-a-risk, reflecting the conservative nature of our strict consensus rule"
  - [corpus: weak/missing] - No direct corpus neighbor validates this specific intersection mechanism, though prompt robustness concerns are noted broadly
- Break condition: If the strict 100% intersection threshold is too aggressive, it may systematically under-detect legitimate risks

### Mechanism 3
- Claim: IF/DESPITE clause pairs in rule-based explanations reveal conceptual conflicts where stakeholders reason about the same factors from opposite directions
- Mechanism: GloVE generates explanations with supporting conditions (IF) and contrasting factors (DESPITE). When stakeholder A's IF clause semantically matches stakeholder B's DESPITE clause for the same risk, it indicates opposing interpretations of the same underlying concept
- Core assumption: Semantic similarity between textual clauses captures meaningful conceptual alignment in reasoning
- Evidence anchors:
  - [section: Policy Conflicts and Proposed Visualization]: "A high Cu(s1, s2, ri) indicates that two stakeholders are reasoning about the same underlying concept but from opposing directions"
  - [section: Methodology]: "we use GloVE to generate

## Foundational Learning
The approach builds on IBM's Risk Atlas Nexus as a risk prediction model and GloVE for generating interpretable IF/DESPITE explanations. The key innovation is combining these tools with stakeholder persona conditioning and paraphrase invariance to reveal divergent risk perceptions and their underlying reasoning conflicts.

## Architecture Onboarding
The system consists of three main components: 1) Stakeholder persona generation and paraphrase creation, 2) Risk Atlas Nexus for paraphrase-invariant risk prediction per stakeholder, and 3) GloVE for generating IF/DESPITE explanation pairs. Conflicts are detected by comparing semantic similarity between IF and DESPITE clauses across stakeholders for the same risks.

## Open Questions the Paper Calls Out
The paper acknowledges several limitations but does not explicitly list open questions in the traditional sense. Key considerations include the generalizability of synthetic stakeholders to real-world scenarios, the effectiveness of the strict paraphrase intersection threshold, and the potential for alternative conflict visualization approaches.

## Limitations
- Relies on synthetic stakeholders rather than real human participants
- Uses binary risk classifications rather than probabilistic assessments
- Employs a strict 100% intersection threshold for paraphrase invariance that may be overly conservative
- Limited to three real-world use cases for validation
- Does not address potential biases in the underlying Risk Atlas Nexus model

## Confidence
Confidence is moderate. The methodology is well-structured and addresses important concerns about LLM evaluation transparency and stakeholder diversity. The three mechanisms provide plausible explanations for how the approach works, though some evidence anchors are weak or missing. The reliance on synthetic stakeholders and binary classifications represents notable limitations.

## Next Checks
- Examine the actual paraphrase transformations used and their effectiveness at maintaining semantic equivalence
- Investigate the semantic similarity metrics used for comparing IF/DESPITE clauses
- Review the specific examples of stakeholder conflicts and their underlying explanations
- Assess the interactive visualization prototype and its usability for exploring conflicts
- Consider alternative approaches to handling paraphrase variability beyond strict intersection