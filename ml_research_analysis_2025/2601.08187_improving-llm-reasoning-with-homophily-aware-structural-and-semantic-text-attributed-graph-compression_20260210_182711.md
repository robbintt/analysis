---
ver: rpa2
title: Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed
  Graph Compression
arxiv_id: '2601.08187'
source_url: https://arxiv.org/abs/2601.08187
tags:
- graph
- nodes
- node
- target
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a homophily-aware framework for text-attributed
  graph compression to improve LLM reasoning. The method performs global hierarchical
  partitioning based on structural entropy minimization to detect naturally cohesive
  communities, then applies community-variant semantic aggregation using LLMs to condense
  redundant context into concise summaries.
---

# Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression

## Quick Facts
- arXiv ID: 2601.08187
- Source URL: https://arxiv.org/abs/2601.08187
- Authors: Zijun Di; Bin Lu; Huquan Kang; Luoyi Fu; Jiaxin Ding; Xiaoying Gan; Lei Zhou; Xinbing Wang; Chenghu Zhou
- Reference count: 40
- Primary result: Achieves up to 94.98% graph scale compression while improving accuracy by 3.06%-4.92% on node and graph classification tasks

## Executive Summary
This paper proposes HS2C, a homophily-aware framework for compressing text-attributed graphs (TAGs) to improve Large Language Model (LLM) reasoning efficiency. The method performs global hierarchical partitioning based on structural entropy minimization to detect naturally cohesive communities, then applies community-variant semantic aggregation using LLMs to condense redundant context into concise summaries. Extensive experiments on 10 node-level and 7 graph-level benchmarks demonstrate superior compression rates and accuracy compared to baselines, with strong scalability and task generalizability.

## Method Summary
HS2C operates in three main stages: (1) Graph Structure Enhancement (GSE) uses BERT embeddings to compute node similarities and adds KNN edges based on entropy-guided selection; (2) Hierarchical Community Partition (HCP) performs structural entropy minimization to create a coding tree and detect homophilic communities; (3) Community-variant Semantics Aggregation (CSA) uses type-specific LLM prompts to summarize background nodes within each community, preserving semantic consistency with target nodes. The compressed graph retains target nodes with aggregated backgrounds and remapped edges, enabling efficient LLM inference.

## Key Results
- Achieves up to 94.98% graph scale compression on Instagram dataset
- Improves accuracy by 3.06%-4.92% compared to state-of-the-art baselines
- Demonstrates strong scalability across diverse graph types and sizes
- Maintains or improves performance across 17 different node and graph classification benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Structural Entropy-Guided Community Detection
Minimizing structural entropy reveals natural homophilic communities while discarding stochastic noise in graph topology. The algorithm constructs a coding tree that minimizes H_T(G) = -Σ (g_α/vol(G)) × log(vol(α)/vol(α⁻)), representing uncertainty during biased random walks. Greedy MERGE operations combine communities maximizing entropy reduction, followed by DROP operations to enforce fixed tree height, yielding communities where intra-community connectivity is high and inter-community connectivity is low.

### Mechanism 2: Context-Aware Semantic Aggregation
LLM-generated summaries conditioned on target node context preserve homophilic semantics while compressing redundant background information. For each community, background node texts are summarized via an LLM using type-specific prompts that instruct extraction of content semantically aligned with target nodes. Four community types receive distinct templates to ensure relevant information preservation.

### Mechanism 3: KNN-Augmented Graph Enhancement
Adding similarity-based edges recovers latent relationships missing from observed topology, improving community detection. Node embeddings from BERT are used to calculate Pearson correlation similarity matrix, and k-nearest neighbor edges are added based on entropy plateau detection. This recovers and reinforces latent relationships while mitigating information loss from incomplete original graph structures.

## Foundational Learning

- **Concept: Structural Entropy (SE)**
  - Why needed here: Core mathematical foundation for hierarchical partitioning algorithm; quantifies uncertainty in graph topology via random walk codeword lengths
  - Quick check question: Explain why minimizing H_T(G) naturally produces communities with high internal connectivity and low external connectivity

- **Concept: Homophily in Graphs**
  - Why needed here: The entire compression framework assumes nodes with similar attributes/labels form connected communities; preserving homophilic structure retains task-relevant information
  - Quick check question: Given a citation network, why would homophily-based compression outperform random sampling for paper classification?

- **Concept: Text-Attributed Graphs (TAGs)**
  - Why needed here: Target data structure has both topological (edges) and semantic (text) signals that must be jointly compressed
  - Quick check question: What two complementary signals must HS2C preserve during compression, and why does random sampling fail at both?

## Architecture Onboarding

- **Component map:** GSE (BERT embeddings → similarity matrix → KNN augmentation) → HCP (MERGE iterations → DROP iterations → coding tree) → CSA (community typing → type-specific LLM prompts → aggregated summaries) → Reconstruction (retain targets + aggregated backgrounds → remap edges)

- **Critical path:** GSE (k selection) → HCP (coding tree quality) → CSA (summary relevance) → Reconstruction → LLM inference. Errors in coding tree construction propagate downstream; poor community partitions cannot be fixed by better summarization.

- **Design tradeoffs:**
  - Tree height h: Lower height = higher compression but risks merging semantically distinct communities
  - KNN parameter k: Larger k = denser enhancement but introduces more noise edges
  - LLM backbone for CSA: Larger models (8B vs 3B) yield better summaries but increase latency/cost
  - Community type granularity: More types = more targeted prompts but more template maintenance

- **Failure signatures:**
  - Homophily score H_S remains near baseline → GSE/HCP failing to detect structure; check k selection or SE computation
  - Accuracy drops despite high compression → CSA losing task-relevant information; inspect summary quality
  - High variance across random seeds → MERGE greediness sensitive to initialization; consider deterministic ordering
  - GCR stagnates around 80%+ → communities too granular; increase tree height or adjust DROP threshold

- **First 3 experiments:**
  1. **GSE ablation:** Run HCP on original graph G vs. KNN-enhanced Ĝ on OGBN-ArXiv; report homophily score H_S and final accuracy to quantify enhancement benefit
  2. **Tree height sweep:** Vary h ∈ {2, 3, 4, 5, 6} on TAPE dataset; plot GCR vs. ACC curve and identify knee point where accuracy degrades significantly
  3. **CSA prompt validation:** Manually inspect 20 community summaries across all 4 types; score each for semantic alignment with target nodes (1-5 scale) to catch systematic prompt failures before full pipeline runs

## Open Questions the Paper Calls Out

- **Question:** How can the HS2C framework be adapted to handle dynamic temporal graphs where node attributes and structural connectivity evolve over time?
  - Basis in paper: [explicit] The conclusion explicitly identifies the extension to temporal graphs as a key area for future work
  - Why unresolved: Current methodology relies on static structural entropy minimization and one-time semantic aggregation, which cannot capture time-varying dependencies or concept drift
  - What evidence would resolve it: A dynamic variant incorporating incremental coding tree updates or time-aware attention mechanisms, validated on temporal graph benchmarks

- **Question:** Can the compression strategy be further optimized to handle industrial-scale Text-Attributed Graphs (TAGs) while maintaining computational efficiency?
  - Basis in paper: [explicit] Authors list "investigating the scalability of HS2C to industrial-scale TAGs" as a primary goal for future research
  - Why unresolved: While method shows strong performance on datasets like DBLP (2M nodes), industrial graphs may contain billions of nodes, potentially straining hierarchical partitioning and LLM inference steps
  - What evidence would resolve it: Complexity analysis and runtime benchmarks on graphs with >10^8 nodes, potentially utilizing distributed computing or sampling approximations

- **Question:** To what extent does the strict homophily assumption limit the framework's effectiveness on heterophilic graphs where dissimilar nodes connect?
  - Basis in paper: [inferred] Method is explicitly designed to detect "naturally cohesive, homophilic communities" and discard "stochastic connectivity noise"
  - Why unresolved: In heterophilic networks, edges connecting dissimilar nodes are often informative signals rather than noise; removing them to enforce homophily could degrade reasoning accuracy
  - What evidence would resolve it: Ablation studies on specific heterophilic benchmarks (e.g., Chameleon, Squirrel) analyzing retention of inter-class edges and resulting classification performance

## Limitations

- **Implementation dependencies:** The method's effectiveness hinges on LLM performance for semantic aggregation, with different architectures potentially yielding significantly different compression quality
- **Structural entropy optimization:** Practical implementation details (MERGE/DROP stopping criteria, tree height selection) are underspecified, potentially affecting reproducibility and optimization quality
- **Evaluation scope:** While extensive across 17 benchmarks, evaluation focuses primarily on classification tasks; effectiveness for other downstream tasks (link prediction, community detection) remains untested

## Confidence

**High Confidence:** Graph compression rates (GCR) and overall accuracy improvements are well-supported by experimental results across multiple benchmarks. The core claim that HS2C achieves superior compression while maintaining/improving accuracy is robustly demonstrated.

**Medium Confidence:** The homophily-aware community detection mechanism and its relationship to downstream performance is well-argued but relies heavily on SE optimization details that aren't fully specified. The mechanism is plausible but implementation-dependent.

**Low Confidence:** The scalability claims are based on runtime observations without systematic scaling analysis. The KNN enhancement's contribution to accuracy gains is demonstrated but the entropy-based k-selection method lacks precise thresholds.

## Next Checks

1. **Ablation of KNN Enhancement:** Run HCP directly on original graphs without KNN augmentation across all 10 node-level benchmarks. Compare homophily scores H_S and accuracy to quantify the enhancement's contribution versus potential overfitting.

2. **Cross-Domain LLM Transfer:** Test HS2C's community summarization with a different LLM backbone (e.g., Mistral-7B or GPT-4) on 3 diverse datasets (one from each domain: academic, social, e-commerce). Measure summary quality and downstream accuracy to assess LLM dependency.

3. **Structural Entropy Parameter Sensitivity:** Systematically vary tree height h ∈ {2, 3, 4, 5, 6} on OGBN-ArXiv and TAPE datasets while keeping all else constant. Plot GCR vs. ACC curves to identify optimal height ranges and test the claimed "knee point" behavior.