---
ver: rpa2
title: Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model
arxiv_id: '2505.19406'
source_url: https://arxiv.org/abs/2505.19406
tags:
- reasoning
- compositional
- task
- arxiv
- shape
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of compositional generalization
  in vision-language models (VLMs), focusing on whether VLMs can integrate independently
  learned skills across modalities and tasks. The authors introduce ComPABench, a
  diagnostic benchmark that evaluates cross-modal, cross-task, and out-of-distribution
  (OOD) compositional reasoning.
---

# Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model
## Quick Facts
- arXiv ID: 2505.19406
- Source URL: https://arxiv.org/abs/2505.19406
- Authors: Tianle Li; Jihai Zhang; Yongming Rao; Yu Cheng
- Reference count: 6
- Primary result: RL-Ground improves compositional accuracy from 31.2% to 52.8% through explicit visual grounding and progress-based rewards

## Executive Summary
This paper addresses the critical challenge of compositional generalization in vision-language models (VLMs), focusing on whether these models can effectively integrate independently learned skills across different modalities and tasks. The authors identify a significant gap in current VLMs' ability to reason compositionally when faced with cross-modal, cross-task, and out-of-distribution scenarios. Through systematic experimentation comparing supervised fine-tuning (SFT) and reinforcement learning (RL) approaches, they demonstrate that while RL-trained models consistently outperform SFT in compositional settings, both struggle substantially with multimodal input integration. To address this fundamental limitation, the authors propose RL-Ground, a novel training method that combines explicit visual-to-text captioning with progress-based rewards, achieving significant improvements in compositional reasoning performance.

## Method Summary
The research introduces ComPABench, a diagnostic benchmark specifically designed to evaluate compositional reasoning capabilities in VLMs across three dimensions: cross-modal reasoning, cross-task generalization, and out-of-distribution scenarios. The authors conduct controlled experiments comparing two primary training strategies - supervised fine-tuning and reinforcement learning - to understand their respective strengths and limitations in compositional tasks. They then develop and validate RL-Ground, which incorporates explicit visual grounding through captioning objectives alongside progress-based reward mechanisms during training. This approach is systematically evaluated against baseline methods, demonstrating substantial improvements in the model's ability to compose learned skills across modalities and tasks.

## Key Results
- RL-trained models outperform SFT models in compositional reasoning tasks by significant margins
- Standard RL achieves 31.2% accuracy on compositional tasks, highlighting baseline limitations
- RL-Ground achieves 52.8% accuracy on the same compositional tasks, representing a 68.6% relative improvement
- Both SFT and standard RL struggle significantly with multimodal input integration despite their relative performance differences

## Why This Works (Mechanism)
The improved compositional performance stems from RL-Ground's dual approach of explicit visual grounding combined with progress-based rewards. The visual grounding component ensures that the model develops robust representations of visual content through captioning tasks, creating stronger associations between visual and textual modalities. The progress-based rewards provide intermediate supervision signals that guide the model toward incremental reasoning steps rather than attempting to solve compositional problems in a single leap. This combination addresses the fundamental challenge that VLMs face in maintaining coherent multimodal representations while composing learned skills across different domains and tasks.

## Foundational Learning
- Compositional Generalization: The ability to combine independently learned skills to solve novel tasks; needed because VLMs must reason beyond their training distribution; quick check: can the model solve tasks requiring skill combination not seen during training?
- Cross-Modal Reasoning: Integration of information across visual and language modalities; critical for VLMs to function effectively; quick check: does performance degrade when input modalities are mixed?
- Out-of-Distribution (OOD) Generalization: Performance on data distributions different from training data; essential for real-world applicability; quick check: how does performance change when task distributions shift?
- Progress-Based Rewards: Intermediate supervision signals during training; helps models learn compositional reasoning step-by-step; quick check: are intermediate reasoning steps improving before final task completion?
- Visual Grounding: Establishing strong associations between visual content and corresponding textual descriptions; foundational for multimodal understanding; quick check: can the model accurately caption visual scenes?

## Architecture Onboarding
Component Map: Visual Encoder -> Language Encoder -> Reasoning Module -> Output Generator
Critical Path: Visual input → Visual Encoder → Multimodal Fusion → Compositional Reasoning → Task-Specific Output
Design Tradeoffs: Explicit visual grounding vs. end-to-end training efficiency; progress-based rewards vs. simpler reward structures; computational overhead vs. compositional reasoning capability
Failure Signatures: Degraded performance on cross-modal tasks, inability to combine learned skills, poor OOD generalization, collapse of multimodal integration
First Experiments:
1. Compare RL-Ground performance against SFT and standard RL on ComPABench's compositional reasoning tasks
2. Ablation study isolating visual grounding component's contribution to overall performance
3. Evaluate model's ability to generalize compositional skills to entirely new task domains

## Open Questions the Paper Calls Out
None

## Limitations
- ComPABench benchmark may have limited scope to specific task domains covered
- Improvements measured primarily within controlled settings rather than real-world scenarios
- Absolute performance levels remain modest despite significant relative improvements
- Mechanism by which components specifically address compositional generalization needs further investigation

## Confidence
- High confidence in the empirical observation that VLMs struggle with compositional generalization across modalities
- Medium confidence in the comparative performance advantage of RL over SFT for compositional tasks
- Medium confidence in the specific effectiveness of RL-Ground's proposed mechanisms
- Low confidence in the absolute performance levels as indicators of practical capability

## Next Checks
1. Test RL-Ground on diverse real-world datasets beyond the controlled ComPABench environment to assess generalizability
2. Conduct ablation studies to isolate the individual contributions of visual grounding versus progress-based rewards
3. Evaluate model performance on compositional tasks requiring longer temporal dependencies and more complex reasoning chains