---
ver: rpa2
title: Adjusting the Output of Decision Transformer with Action Gradient
arxiv_id: '2510.05285'
source_url: https://arxiv.org/abs/2510.05285
tags:
- algorithms
- gradient
- action
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in Decision Transformer (DT) for
  offline reinforcement learning, specifically its poor state-level extrapolation
  and trajectory stitching capabilities. The authors propose Action Gradient (AG),
  a method that refines the action output by DT using gradients of a Q-value with
  respect to the action.
---

# Adjusting the Output of Decision Transformer with Action Gradient

## Quick Facts
- **arXiv ID:** 2510.05285
- **Source URL:** https://arxiv.org/abs/2510.05285
- **Authors:** Rui Lin; Yiwen Zhang; Zhicheng Peng; Minghao Lyu
- **Reference count:** 9
- **Primary result:** Action Gradient (AG) improves Decision Transformer's state-level extrapolation by iteratively refining actions using Q-value gradients, achieving 757.8 normalized score on Gym and 193.9 on Maze2d tasks.

## Executive Summary
This paper addresses limitations in Decision Transformer (DT) for offline reinforcement learning, specifically its poor state-level extrapolation and trajectory stitching capabilities. The authors propose Action Gradient (AG), a method that refines the action output by DT using gradients of a Q-value with respect to the action. AG searches locally around the initial DT action to find a better action via gradient ascent, using a trained critic. This approach is orthogonal to token prediction techniques and can be combined with them. Experiments on D4RL benchmarks show that RF+AG outperforms prior DT-based methods, with total normalized scores reaching 757.8 on Gym tasks and 193.9 on Maze2d. Ablation studies confirm that AG improves performance even without token prediction, and hyperparameter tuning (e.g., learning rate η and iteration count) further enhances results. The method provides a stable and effective way to enhance DT's extrapolation ability.

## Method Summary
The authors propose Action Gradient (AG), which refines the action output of a Decision Transformer by using gradients of a Q-value with respect to the action. During inference, AG computes the gradient of a trained critic Q(s,a) with respect to the action and performs gradient ascent to iteratively improve the action: a^(i+1) = a^i + η∇_a Q(s, a^i). This is done for n iterations, and the action with the highest Q-value is selected. The method is trained separately from the transformer, avoiding the "deadly triad" instability. AG can be combined with Token Prediction (TP) for trajectory stitching, addressing both state-level and trajectory-level limitations of DT.

## Key Results
- RF+AG achieves 757.8 normalized score on Gym tasks and 193.9 on Maze2d, outperforming prior DT-based methods.
- AG improves performance even without TP, demonstrating its effectiveness for state-level extrapolation.
- Ablation studies show that both AG and TP are necessary for optimal performance, with TP enabling trajectory stitching and AG enabling action refinement.
- Hyperparameter tuning (learning rate η and iteration count) further enhances results, with diminishing returns beyond certain thresholds.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Applying gradient ascent to the action vector during inference (rather than weight updates during training) improves state-level extrapolation without destabilizing the training dynamics of the Decision Transformer.
- **Mechanism:** The method uses a pre-trained critic Q(s, a) to compute the gradient with respect to the action ∇_a Q(s, a). It initializes an action a^0 using the DT and iteratively refines it: a^(i+1) = a^i + η ∇_a^i Q(s, a^i). This performs a local search in the action space to maximize the expected return without altering the transformer's weights.
- **Core assumption:** The critic Q(s, a) provides a smooth and sufficiently accurate landscape such that the gradient points toward higher-value regions even for out-of-distribution actions.
- **Evidence anchors:**
  - [abstract] "AG utilizes the gradient of the Q-value with respect to the action to optimize the action."
  - [section 3.2] "Instead, the gradient of the Q-value with respect to the action is computed, and this gradient is backward propagated to adjust the action during the evaluation phase."
  - [corpus] Weak direct corpus support for inference-time action refinement; neighbors focus on multi-agent action generation orders.
- **Break condition:** If the critic suffers from severe overestimation errors or produces a flat/non-convex landscape for OOD actions, the gradient may lead to erratic or low-quality actions.

### Mechanism 2
- **Claim:** Decoupling the policy gradient influence from the transformer's training loss avoids the instability typically caused by the "deadly triad" in offline RL.
- **Mechanism:** Traditional methods integrate policy gradient (PG) terms directly into the loss function, which combines off-policy learning, function approximation, and bootstrapping—triggering the deadly triad. Action Gradient (AG) isolates the gradient application to the inference phase, leaving the transformer's likelihood-based training objective untouched.
- **Core assumption:** The primary limitation of DT is not the representational capacity of the architecture but the sub-optimality of the actions retrieved via pure sequence modeling.
- **Evidence anchors:**
  - [abstract] "Existing methods... fail to improve performance stably when combined due to inherent instability. To address this, we propose Action Gradient (AG)... without altering the training process."
  - [section 1] "...integration of these two approaches cannot stably yield satisfactory results due to the deadly triad... AG... can be conventionally integrated with TP."
- **Break condition:** If the base DT policy is fundamentally incapable of generating an initial action a^0 within the basin of attraction for a high-value Q-region, the inference-time refinement will fail to converge.

### Mechanism 3
- **Claim:** Combining Action Gradient (state-level extrapolation) with Token Prediction (trajectory-level stitching) is necessary to unlock performance gains in sparse reward or multi-goal environments.
- **Mechanism:** AG optimizes the action for the *current* state but cannot inherently fix a trajectory heading toward a low-reward region. Token Prediction (TP) replaces the fixed Return-To-Go (RTG) with a predicted value, allowing the agent to "stitch" different trajectories from the dataset. AG then optimizes the specific actions within that stitched trajectory.
- **Core assumption:** The offline dataset contains optimal sub-trajectories that can be stitched together, and the value predictor accurately identifies these high-value paths.
- **Evidence anchors:**
  - [section 3.1] "This enhancement [TP] allows the agent to generate previously unobserved trajectories but does not enable the agent to select unseen actions... state-level extrapolation involves identifying the optimal action... To further illustrate state-level extrapolation..."
  - [section 4.4] "The limited effectiveness of AG [without TP] can be attributed to the scarcity of stitching ability..."
- **Break condition:** In environments where the dataset lacks diverse trajectories or accurate value estimates, TP may fail to condition the agent correctly, rendering the action optimization moot.

## Foundational Learning

- **Concept: Offline RL Distributional Shift**
  - **Why needed here:** The paper addresses the specific constraints of offline RL (fixed dataset, no exploration). Understanding that standard RL fails here because the agent might query states/actions not in the dataset (OOD) explains why DT struggles with extrapolation and why AG relies on a conservative critic (IQL).
  - **Quick check question:** Why does maximizing the likelihood of actions in a dataset (standard DT) fail to produce an optimal policy if the dataset contains sub-optimal data?

- **Concept: The Deadly Triad**
  - **Why needed here:** The authors explicitly cite this to explain why combining Policy Gradient with DT is unstable. You must understand that function approximation + bootstrapping + off-policy data creates divergence risks to appreciate why AG moves the gradient step to inference.
  - **Quick check question:** What three components, when combined, cause learning instability in RL?

- **Concept: Implicit Q-Learning (IQL)**
  - **Why needed here:** The paper uses IQL to train the critic required for AG. IQL trains the value function using expectile regression on the dataset only, avoiding querying OOD actions during critic training, which is crucial for AG's stability.
  - **Quick check question:** How does IQL estimate the Q-value without querying the target network for actions outside the offline dataset distribution?

## Architecture Onboarding

- **Component map:** Policy Network (DT/Reinforcer) -> Value Network (Critic) -> RTG Predictor (Token Prediction)
- **Critical path:**
  1. **Phase 1 (Training):** Train the Decision Transformer (Reinforcer) on the offline dataset D using standard sequence modeling (NLL loss).
  2. **Phase 2 (Training):** Train the Critic (IQL) on the same dataset D. Ensure this is completely separate from the transformer training.
  3. **Phase 3 (Inference/Deployment):**
      - Feed state sequence to DT to get initial action a^0.
      - **AG LOOP:** Iteratively update a ← a + η ∇_a Q(s, a).
      - Select the action from the trajectory that yields the highest Q.

- **Design tradeoffs:**
  - **AG vs. Policy Gradient (PG):** PG updates weights (risking irreversible training instability), while AG updates actions (risking inference latency but preserving model stability).
  - **Optimizer for AG:** The paper notes that complex optimizers (Adam/RMSProp) for the action gradient can sometimes fail if the critic error is high. Plain SGD with momentum is often preferred.

- **Failure signatures:**
  - **Stitching Failure:** If TP is disabled, AG may improve immediate rewards but the agent will still follow sub-optimal long-term trajectories (low total return).
  - **Gradient Noise:** If the critic is inaccurate, the action gradient may oscillate or degrade performance (seen in `maze2d-large` where AG underperformed).
  - **Overestimation:** If the critic is not trained conservatively (e.g., standard TD3), AG may exploit Q-errors, generating physically impossible actions.

- **First 3 experiments:**
  1. **Sanity Check (Simple Env):** Replicate the r(a) = 1 - a^2 experiment from Figure 2. Train a simple DT and Critic on a restricted dataset (e.g., |a| > 0.5). Verify that AG pushes actions toward a=0 while vanilla DT does not.
  2. **Ablation on AG Steps:** On a D4RL environment (e.g., `halfcheetah-medium`), plot performance vs. number of gradient iterations (n=0 to 15) and step size η. Identify the point of diminishing returns or collapse.
  3. **TP vs. TP+AG:** Compare `Reinforcer` (baseline) against `Reinforcer+TP` and `Reinforcer+TP+AG`. Isolate whether the performance gain comes from better stitching (TP) or better action selection (AG).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced gradient optimizers (e.g., Adam, Momentum) robustly improve Action Gradient performance across diverse environments?
- Basis: [explicit] Section 5 states that "exploration of advanced gradient methods" has potential. Section 4.4 notes that ablation results (Table 4) were inconclusive, calling for a "more comprehensive experimental approach."
- Why unresolved: Ablation studies showed momentum-based methods improved some environments but degraded performance in others (e.g., walker2d-medium).
- What evidence would resolve it: Theoretical analysis of the action landscape or empirical results demonstrating consistent, stable improvements using adaptive learning rates across all D4RL tasks.

### Open Question 2
- Question: How does the architecture or accuracy of the critic network constrain the effectiveness of Action Gradient?
- Basis: [explicit] Section 5 identifies "optimized critic training techniques" as a key area for future research to enhance AG's performance.
- Why unresolved: The method relies on a pre-trained critic (IQL) to guide actions, but the sensitivity of AG to critic estimation errors or overestimation bias is not fully characterized.
- What evidence would resolve it: A comparative analysis of AG performance using various offline critic training methods (e.g., CQL, TD3+BC) versus the standard IQL approach.

### Open Question 3
- Question: Is the Action Gradient method compatible with other Token Prediction (TP) techniques for improving trajectory-level stitching?
- Basis: [explicit] The Conclusion states future research should concentrate on "exploration of the integration with other advanced TP techniques," as the study primarily validated AG using Reinforcer.
- Why unresolved: It is unclear if AG's state-level adjustments conflict with the trajectory planning of other TP methods.
- What evidence would resolve it: Applying AG to alternative TP-based algorithms (e.g., ADT) and demonstrating performance gains without destabilizing the trajectory stitching.

## Limitations
- The method's effectiveness is highly dependent on the quality and accuracy of the pre-trained critic; poor critic estimates can lead to suboptimal or dangerous actions.
- AG introduces computational overhead during inference, as it requires multiple iterations of gradient ascent for each action, which may not scale efficiently to high-dimensional action spaces.
- The performance gains are contingent on the diversity of the offline dataset; in datasets with limited trajectory variety, the benefits of trajectory stitching (Token Prediction) and state-level extrapolation (Action Gradient) may be muted.

## Confidence

- **High Confidence:** The core mechanism of using action gradients during inference to improve state-level extrapolation is sound and well-supported by the experimental results, particularly the ablation showing AG's contribution even without Token Prediction.
- **Medium Confidence:** The claim that Action Gradient can be "conventionally integrated" with Token Prediction to achieve stable performance gains is supported by the aggregate scores, but the specific interaction and relative contributions of each component in all environments are not fully disentangled.
- **Medium Confidence:** The assertion that the method is "orthogonal" to token prediction techniques is plausible given the modular design, but a more rigorous ablation study isolating the exact conditions under which each method is necessary would strengthen this claim.

## Next Checks

1. **Critic Robustness Test:** Conduct an experiment where the critic is deliberately trained with varying levels of conservatism (e.g., different expectile values in IQL). Measure how the performance of AG degrades as the critic becomes less accurate or more prone to overestimation. This would directly test the break condition related to critic quality.
2. **High-Dimensional Action Space:** Apply AG to a D4RL task with a higher-dimensional action space (e.g., `hopper` or `walker2d`) and measure the computational overhead. Quantify the latency introduced per action and assess whether the performance gain justifies the cost.
3. **Dataset Diversity Ablation:** Create a series of datasets for a single environment with varying levels of trajectory diversity (e.g., by subsampling the medium-replay dataset). Train RF+AG on each and plot performance against dataset diversity to isolate the method's dependence on the quality of the offline data for both stitching and extrapolation.