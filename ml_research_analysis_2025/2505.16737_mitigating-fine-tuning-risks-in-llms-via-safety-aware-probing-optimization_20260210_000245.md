---
ver: rpa2
title: Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization
arxiv_id: '2505.16737'
source_url: https://arxiv.org/abs/2505.16737
tags:
- fine-tuning
- safety
- harmful
- arxiv
- lsafety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the critical safety challenge where fine-tuning
  large language models (LLMs) on benign data can still degrade their safety alignment.
  The authors propose Safety-Aware Probing (SAP), a novel optimization framework that
  integrates a safety-aware probe into the gradient propagation process to identify
  and avoid harmful optimization directions during fine-tuning.
---

# Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization

## Quick Facts
- arXiv ID: 2505.16737
- Source URL: https://arxiv.org/abs/2505.16737
- Authors: Chengcan Wu; Zhixin Zhang; Zeming Wei; Yihao Zhang; Meng Sun
- Reference count: 40
- Primary result: Safety-Aware Probing (SAP) significantly reduces harmful outputs (32.5% → 23.1% harmful score) during fine-tuning while maintaining task performance.

## Executive Summary
This paper addresses the critical safety challenge where fine-tuning large language models on benign data can still degrade their safety alignment. The authors propose Safety-Aware Probing (SAP), a novel optimization framework that integrates a safety-aware probe into the gradient propagation process to identify and avoid harmful optimization directions during fine-tuning. SAP employs a bi-level optimization strategy to maximize a safety-usefulness loss function, encouraging the model to prefer safe updates while maintaining task-specific performance. Extensive experiments across three popular LLMs (Llama-2, Vicuna, Qwen2.5) and multiple datasets demonstrate that SAP significantly reduces harmful outputs while achieving comparable test loss to standard fine-tuning methods. The method also shows robustness against adversarial attacks and compatibility with existing safety techniques, offering a versatile solution for secure LLM deployment.

## Method Summary
Safety-Aware Probing (SAP) is a bi-level optimization framework that identifies and avoids harmful gradient directions during fine-tuning. The method computes a harmful direction using a small contrastive safety dataset, then uses a safety-aware probe to modify hidden states in middle layers of the model. The probe is optimized to maximize a safety-usefulness loss function, which approximates minimizing the contrastive safety loss. This process guides the main optimizer to update model parameters in safer directions while maintaining task performance. SAP shows significant reductions in harmful outputs across multiple LLM architectures while maintaining comparable test loss to standard fine-tuning.

## Key Results
- SAP reduces harmful outputs from 32.5% to 23.1% on average across multiple models and datasets
- Maintains comparable test loss to standard fine-tuning (BLEURT and CL scores)
- Shows robustness against adversarial attacks and compatibility with existing safety techniques
- Probing middle layers (11-20) is most effective for safety steering
- Demonstrates effectiveness across Llama-2, Vicuna, and Qwen2.5 models

## Why This Works (Mechanism)

### Mechanism 1: Gradient Direction Entanglement Hypothesis
Standard fine-tuning on benign data degrades safety because useful-critical gradients and harmful-critical gradients are positively correlated during optimization. During fine-tuning, the gradient descent algorithm follows task-specific useful directions. If these directions overlap with harmful directions (positive cosine similarity), the model inadvertently minimizes loss on harmful tasks, moving towards less safe parameter configurations. Evidence shows cosine similarity > 0.3 between useful and harmful gradients across layers and epochs. This mechanism fails if the cosine similarity is consistently near zero or negative.

### Mechanism 2: Safety-Aware Probing Loss (Lsu) Approximation
Maximizing the proposed Safe-Useful Loss (Lsu) approximately minimizes the contrastive safety loss (Lsafety) without requiring explicit safety data in the main training loop. Lsu = Lusefulness(W + ΔWharmful) - Lusefulness(W). Maximizing Lsu penalizes parameter regions where moving in a harmful direction would also lower the task loss. The paper proves ∇V Lsu ≈ -C * ∇V Lsafety. This approximation may fail due to high non-linearity in the loss landscape or inappropriate step sizes.

### Mechanism 3: Bi-Level Optimization with Hidden State Probing
Intervening on hidden states (activations) with a safety-aware probe is a computationally efficient and effective alternative to full parameter perturbation. Instead of perturbing all model parameters W, SAP perturbs hidden states V at specific layers. This probe modifies the forward pass to calculate the corrected gradient for W using a bi-level loop: (1) find Vsafe to maximize Lsu, (2) update W using the gradient calculated at W, Vsafe. This mechanism fails if the computational overhead becomes prohibitive or if the probe fails to capture safety dynamics in critical layers.

## Foundational Learning

- **Concept: Bi-Level Optimization**
  - Why needed here: SAP's core algorithm is bi-level: it solves an inner maximization (find the probe V) and an outer minimization (update model W). Understanding this nested structure is crucial to distinguish it from simple regularization.
  - Quick check question: How does SAP's bi-level loop differ from standard gradient descent with a fixed regularization term?

- **Concept: Representation Engineering / Activation Steering**
  - Why needed here: The probe V modifies hidden states. This is related to the broader idea that model behavior (like safety) can be controlled by steering internal representations rather than just changing weights.
  - Quick check question: What is the "probe" V in SAP actually modifying during the forward pass: model weights, input embeddings, or intermediate layer activations?

- **Concept: Sharpness-Aware Minimization (SAM)**
  - Why needed here: The paper explicitly draws inspiration from SAM, which uses weight perturbation to find flatter minima for better generalization. SAP adapts this concept to find "safer" minima.
  - Quick check question: What is the primary objective of standard SAM, and how does SAP's objective differ?

## Architecture Onboarding

- **Component map:** Harmful Direction Calculator -> Probe Optimizer (Inner Loop) -> Main Optimizer (Outer Loop)
- **Critical path:** The inner loop (computing ΔWharmful and optimizing V) is the critical new path. If this fails to identify a meaningful harmful direction or if the probe V cannot effectively shape the gradient, the outer optimization for W remains unsafe.
- **Design tradeoffs:**
  - Safety vs. Speed: Table 6 shows a ~2x slowdown per training step due to the extra forward/backward passes for the probe
  - Probe Scope: Probing all layers is expensive; probing middle layers (11-20) offers a balance. Probing fewer layers reduces overhead but might miss critical safety signals (Table 7)
- **Failure signatures:**
  - Task Performance Drop: If V is updated too aggressively (large β), it might distort the useful gradient too much, harming task performance
  - Safety Collapse: If the contrastive dataset (Dsafe, Dharmful) is too small (50 samples used in paper) or unrepresentative, the computed "harmful direction" might be noise, rendering the probe ineffective
- **First 3 experiments:**
  1. Baseline Comparison: Replicate Table 1/2 on a single model (e.g., Llama-2) to verify the Harmful Score (HS) reduction vs. SFT and BLEURT/CL maintenance
  2. Layer Sensitivity: Ablate the probe layers (e.g., use only v[1:10], then v[20:30]) to verify the paper's claim that middle layers are most critical for safety steering
  3. Poisoning Robustness: Run the experiment from Table 4 with a small poisoning rate (e.g., 5%) to see if SAP effectively neutralizes the harmful data's influence compared to standard fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- The contrastive safety dataset used to compute harmful directions is minimal (50 safe/harmful pairs), raising concerns about whether the harmful direction captures true safety-critical patterns versus noise
- The computational overhead of approximately 2x slower training per batch could limit practical deployment, particularly for very large models or resource-constrained environments
- While the method shows robustness against adversarial attacks, the paper does not extensively explore scenarios with severe poisoning rates or sophisticated attack patterns
- The analysis focuses primarily on three model architectures, which may not generalize to all LLM families

## Confidence

- **High Confidence:** The core empirical results showing SAP's effectiveness in reducing harmful outputs while maintaining task performance are well-supported by the experimental data across multiple models and datasets. The bi-level optimization framework and gradient direction entanglement hypothesis are clearly articulated and experimentally validated.

- **Medium Confidence:** The mechanism explaining why standard fine-tuning degrades safety through gradient direction entanglement is theoretically sound but relies on observed correlations that may vary across different model architectures or training scenarios. The effectiveness of middle-layer probing (layers 11-20) is demonstrated but could be task-dependent.

- **Low Confidence:** The scalability claims to very large models (>70B parameters) remain untested, as experiments were conducted on models up to 70B parameters. The long-term stability of safety improvements and behavior under extreme adversarial conditions are not thoroughly explored.

## Next Checks

1. **Dataset Size Sensitivity:** Systematically vary the size of the contrastive safety dataset (e.g., 10, 50, 100, 500 pairs) to quantify how the quality and quantity of harmful direction data affects SAP's performance and identify the minimum viable dataset size.

2. **Transferability Across Domains:** Apply SAP to fine-tuning scenarios outside the current scope (e.g., medical, legal, or creative writing domains) to test whether the safety improvements transfer to different task types and safety requirements.

3. **Adversarial Robustness Stress Test:** Design and execute experiments with varying poisoning rates (1%, 5%, 10%, 20%) and sophisticated adversarial patterns to rigorously evaluate SAP's robustness limits and identify failure modes under extreme conditions.