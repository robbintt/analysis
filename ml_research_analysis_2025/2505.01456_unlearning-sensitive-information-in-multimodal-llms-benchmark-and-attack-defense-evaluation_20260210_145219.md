---
ver: rpa2
title: 'Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense
  Evaluation'
arxiv_id: '2505.01456'
source_url: https://arxiv.org/abs/2505.01456
tags:
- information
- rephrase
- multimodal
- attack
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UnLOK-VQA, a benchmark and attack-defense
  framework for evaluating targeted unlearning in multimodal large language models
  (MLLMs). The authors generate a high-quality dataset by extending VQA samples with
  rephrased and neighborhood data to test generalization and specificity.
---

# Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation

## Quick Facts
- **arXiv ID**: 2505.01456
- **Source URL**: https://arxiv.org/abs/2505.01456
- **Reference count**: 22
- **Primary result**: UnLOK-VQA benchmark shows multimodal attacks achieve 45.5% success rate versus 32-39% for unimodal, with best defense reducing success to 15.7%

## Executive Summary
This paper introduces UnLOK-VQA, a comprehensive benchmark and attack-defense framework for evaluating targeted unlearning in multimodal large language models (MLLMs). The authors develop a high-quality dataset by extending VQA samples with rephrased and neighborhood data to test generalization and specificity of unlearning. Through systematic evaluation of six defense objectives against seven attacks—including a novel whitebox attack exploiting hidden state interpretability—the framework reveals that multimodal attacks significantly outperform unimodal ones. The work demonstrates that removing answer information from internal states provides the most effective defense, and that larger models show greater resilience post-editing, suggesting scaling as a viable robustness strategy.

## Method Summary
The authors created UnLOK-VQA by extending VQA samples with rephrased and neighborhood data to evaluate unlearning generalization. They evaluated six defense objectives (including InfoLOK, direct answer removal, and feature space manipulation) against seven attack types, including a novel whitebox attack exploiting hidden state interpretability. The evaluation measured both attack success rates and defensive effectiveness across different model scales, with particular attention to multimodal versus unimodal attack performance and the impact of model size on unlearning resilience.

## Key Results
- Multimodal attacks achieve 45.5% success rate versus 32-39% for unimodal attacks
- Best defense reduces attack success to 15.7% by removing answer information from internal states
- Larger models exhibit greater resilience post-editing, suggesting scaling as a viable robustness strategy

## Why This Works (Mechanism)
The effectiveness of the framework stems from its comprehensive approach to testing unlearning generalization through multiple data variants and attack types. By evaluating both multimodal and unimodal attacks, the benchmark captures the full attack surface that sensitive information might face. The novel whitebox attack exploiting hidden state interpretability reveals vulnerabilities in how models store and retrieve sensitive information at the representation level. The state-based defense mechanism works by directly targeting and removing answer-related information from internal model states before it can be reconstructed during inference.

## Foundational Learning
- **Multimodal attack vectors**: Understanding how visual and textual information combine to reconstruct sensitive data - needed to evaluate complete attack surface
- **Hidden state interpretability**: Techniques for analyzing and manipulating internal model representations - critical for developing effective whitebox defenses
- **Unlearning generalization**: Methods to test whether removed information stays removed across variations - ensures robustness of unlearning
- **Defense objective formulation**: Systematic approaches to defining what information should be removed - enables targeted and measurable unlearning

## Architecture Onboarding
**Component map**: VQA samples -> Extended dataset (rephrased + neighborhood) -> Attack generation -> Defense application -> Success evaluation
**Critical path**: Dataset generation → Attack formulation → Defense implementation → Evaluation metrics calculation
**Design tradeoffs**: Comprehensive benchmark coverage vs. computational cost; state-based defense effectiveness vs. potential performance degradation; attack surface completeness vs. practical attack feasibility
**Failure signatures**: High attack success on rephrased data indicates poor generalization; multimodal superiority suggests visual-textual information coupling; model size correlation reveals scaling benefits
**First experiments**: 1) Baseline attack success on original VQA samples, 2) Multimodal vs unimodal attack comparison, 3) Defense effectiveness measurement across model scales

## Open Questions the Paper Calls Out
None

## Limitations
- Attack surface remains incomplete, not exploring prompt engineering or chain-of-thought reasoning attacks
- Defense evaluation focuses on specific removal techniques without examining long-term stability
- Benchmark generalizability uncertain as dataset construction may not capture all real-world scenarios

## Confidence
- **High confidence** in multimodal attack superiority findings and state-based defense effectiveness
- **Medium confidence** in scalability observations across tested models
- **Low confidence** in benchmark generalizability to all real-world unlearning scenarios

## Next Checks
1. Conduct longitudinal studies to assess whether defended models maintain their unlearned state across multiple retraining cycles and diverse prompting strategies
2. Expand attack surface analysis to include prompt engineering-based attacks and evaluate whether current defenses remain effective
3. Test the benchmark framework across a broader range of model architectures to validate generalizability of scaling observations