---
ver: rpa2
title: 'GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application'
arxiv_id: '2502.05113'
source_url: https://arxiv.org/abs/2502.05113
tags:
- corpus
- https
- language
- gieskane
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This article presents the GiesKaNe corpus, a reference corpus for
  New High German that combines historical depth with syntactic annotation. The project
  balances innovation with established standards by using a complementary workflow
  of human and machine-assisted processing.
---

# GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application

## Quick Facts
- **arXiv ID:** 2502.05113
- **Source URL:** https://arxiv.org/abs/2502.05113
- **Reference count:** 0
- **Primary result:** GiesKaNe corpus achieves precision 0.996, recall 0.760, F-score 0.857 for normalization, and IAA 0.95-0.96 for syntactic annotation

## Executive Summary
This article presents the GiesKaNe corpus, a reference corpus for New High German (17th–19th century) that combines historical depth with syntactic annotation. The project balances innovation with established standards by using a complementary workflow of human and machine-assisted processing. The workflow includes automatic normalization, rule-based tokenization, and sentence segmentation based on a grammatical sentence concept rather than orthographic sentences. The article addresses challenges in corpus compilation, including text selection, annotation schemes, and interoperability with other corpora.

## Method Summary
The GiesKaNe corpus compilation workflow combines machine processing with human expertise through an alternating approach. Historical texts from the Deutsches Textarchiv are first normalized using the CAB tool (finite-state canonicalization with HMM-based statistical models), achieving high precision but variable recall depending on text type. Rule-based tokenization handles contractions and particle verbs, while manual segmentation defines grammatical sentences rather than relying on punctuation. A SpaCy dependency parser trained on 80% of the data provides pre-annotations, which are then converted to constituent structure using rule-based methods and corrected manually in a spreadsheet environment. The final data is converted to ANNIS format for querying.

## Key Results
- Achieved precision 0.996, recall 0.760, and F-score 0.857 for automatic normalization
- Parser evaluation shows UAS 0.88, LAS 0.77, and PoS accuracy 0.94
- Inter-Annotator Agreement of 0.95 for syntactic functions and 0.96 for parts of speech
- Normalization performance varies significantly by text type (F=0.984 for formal texts vs F=0.601 for oral texts)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The alternating, complementary workflow between human expertise and automated processing may allow for efficient creation of a high-quality historical treebank where fully manual or fully automatic approaches would fail.
- **Mechanism:** The workflow sequences tasks to leverage machine speed for pattern-based operations (normalization, tokenization, initial parsing) and human judgment for interpretive, disambiguating tasks (sentence segmentation, syntactic analysis, error correction). This reduces bottlenecks while preserving quality.
- **Core assumption:** Human annotators can effectively correct machine-generated pre-annotations without introducing systematic bias or excessive cognitive load.
- **Evidence anchors:**
  - [abstract] "...managed through a complementary interplay of human expertise and machine-assisted processes."
  - [section 2.2.2] The workflow diagram (Fig. 7) explicitly alternates manual (white) and machine (gray) steps.
  - [section 2.2.3] Cites multiple studies (Marcus et al., 1993; Eckhoff & Berdičevskis, 2016) showing pre-annotation increases speed by 25-50% without loss of quality.
  - [corpus] Neighbor papers on historical corpus methods (e.g., "Decoding the Past...") similarly rely on hybrid human-machine workflows for processing historical data, suggesting this is a viable pattern in the domain.
- **Break condition:** The efficiency gain collapses if the machine pre-annotation quality is too low (e.g., <70% accuracy), overwhelming annotators with corrections, or if the annotation task is so novel that humans cannot reliably correct machine errors.

### Mechanism 2
- **Claim:** Rule-based and statistical automatic normalization (via tools like CAB) can achieve high precision, significantly reducing manual effort for preparing historical texts, but its recall is contingent on the text's adherence to orthographic norms.
- **Mechanism:** The normalization tool (CAB) uses a finite-state canonicalization architecture and Hidden Markov Models trained on contemporary German to map historical spelling variants to modern equivalents. This standardizes texts for downstream parsing without manual correction for most tokens.
- **Core assumption:** The majority of historical spelling variants can be mapped to modern forms via character-level and lexical transformations without requiring deep syntactic-semantic disambiguation.
- **Evidence anchors:**
  - [abstract] Reports "precision of 0.996, recall of 0.760, and F-score of 0.857 for automatic normalization."
  - [section 2.2.1] Details the use of CAB (Jurish, 2012) and shows its performance varies drastically between conceptually literal texts (F=0.984) and oral texts (F=0.601).
  - [section 2.2.1] Correlates normalization difficulty with the "conceptual orality and literacy" (COL) value of a text (r=0.71).
  - [corpus] Direct corpus evidence is strong from the paper's own evaluation. Related work (neighbor papers on historical text processing) commonly employs similar normalization techniques.
- **Break condition:** The mechanism underperforms on texts with high conceptual orality, non-standard dialect, or idiosyncratic spelling, where recall may drop below usable levels, requiring extensive manual intervention.

### Mechanism 3
- **Claim:** A spreadsheet-based annotation environment can effectively replace specialized, complex annotation tools for constituent treebanking, lowering barriers to entry and ensuring data persistence.
- **Mechanism:** The constituent tree is represented in a 2D grid where columns correspond to syntactic depth and rows to tokens. Merged cells represent phrases. This leverages the ubiquitous, stable, and scriptable (via Python APIs) nature of spreadsheet software to manage hierarchical data.
- **Core assumption:** The target annotation scheme (constituent structure with edges) can be losslessly mapped to a grid-cell representation, and annotators can work efficiently with this visual metaphor.
- **Evidence anchors:**
  - [abstract] "...demonstrates that... workflow can be based on the strategic use of a simple spreadsheet..."
  - [section 2.2.4] Describes the conversion from dependency parser output to the spreadsheet (Fig. 9.1 -> 9.2) and from spreadsheet to the ANNIS graph format (via PAULA XML and Pepper).
  - [section 2.2.4] Notes other corpora (RIDGES, GUM, Praaline) also use spreadsheet-based annotations for complex tasks.
  - [corpus] No direct corpus signal challenges this, but the pattern of using general-purpose tools (like Python, XML) for NLP pipelines is common in related papers.
- **Break condition:** The approach becomes unwieldy for extremely long sentences or annotation schemes requiring complex, non-grid-like relationships (e.g., dense coreference chains, arbitrary graph edges) that are difficult to visualize in a merged-cell format.

## Foundational Learning

- **Concept: Conceptual Orality vs. Literacy (Nähe und Distanz)**
  - **Why needed here:** It is a core theoretical framework for text selection and a predictor of annotation difficulty. It explains why some historical texts (e.g., personal letters) are harder to normalize and parse than others (e.g., formal treatises).
  - **Quick check question:** How would you classify a 17th-century merchant's account book with frequent abbreviations and informal phrasing? How might this affect the normalization F-score?

- **Concept: Grammatical Sentence vs. Orthographic Sentence**
  - **Why needed here:** It underpins the project's segmentation strategy. Historical texts often use punctuation inconsistently, so relying on orthographic sentence boundaries would lead to incorrect syntactic analysis units.
  - **Quick check question:** In a text where a full stop functions like a paragraph marker, would an automatic sentence boundary detector trained on modern text likely over-segment or under-segment?

- **Concept: Additive vs. Consecutive Annotation**
  - **Why needed here:** It defines the relationship between the PoS tags and the syntactic treebank. Additive annotation uses PoS to provide *supplementary* information (e.g., a "donor PoS" for a focus particle) rather than to confirm or duplicate the syntactic function.
  - **Quick check question:** If a token is annotated as a "Subject" in the treebank, what does an additive PoS tag add? What would a consecutive annotation strategy do instead?

## Architecture Onboarding

- **Component map:** Text Source & Pre-processing: DTA (TEI XML) -> CAB (normalization/tokenization) -> Spreadsheet (GiesKaNe Basic Version) -> Annotation: Manual segmentation in spreadsheet -> SpaCy dependency parser (trained on converted data) -> Rule-based conversion to constituent spreadsheet -> Manual correction -> Conversion & Storage: Spreadsheet -> openpyxl API -> PAULA XML -> Pepper converter -> ANNIS XML for the query/graph interface

- **Critical path:**
  1. Text Selection (manual, aided by automatic COL-score calculation)
  2. Machine Normalization & Tokenization (CAB)
  3. Manual Segmentation (defining grammatical sentences)
  4. Automatic Parsing & Pre-annotation
  5. Manual Syntactic Correction & PoS Tagging
  6. Data Conversion to final corpus format (ANNIS)

- **Design tradeoffs:**
  - **Precision vs. Recall in Normalization:** Optimizing for high precision (0.996) at the cost of recall (0.760) to minimize false positives that create manual work (splitting cells)
  - **Spreadsheet vs. Specialized Tool:** Trading off advanced visualization and real-time parser interaction for tool stability, low maintenance, and universal access
  - **Granular vs. Standard Tagset:** Choosing a project-specific, granular tagset (based on GTA) for internal research needs, with a plan to *derive* a standard (HiTS) later for interoperability

- **Failure signatures:**
  - **Normalizer bottlenecks:** High false negative rate on conceptually oral texts, signaled by many un-normalized tokens and low CAB F-scores
  - **Parser drift:** Parsing accuracy (LAS) drops significantly on texts from new centuries or genres not in the training data
  - **Annotator inconsistency:** Inter-Annotator Agreement (alpha) drops below 0.8, particularly on semantic roles and complex coordination structures
  - **Conversion errors:** Information lost or structure corrupted during spreadsheet-to-PAULA-to-ANNIS conversions, detectable via automated validation scripts

- **First 3 experiments:**
  1. **Run the normalizer on a sample:** Take a short text from the DTA, process it with CAB, and manually compute precision/recall against the GiesKaNe guidelines to understand the error profile
  2. **Evaluate the parser on a known segment:** Use a small, manually corrected segment, convert it to dependency format, train the SpaCy parser, and measure UAS/LAS to