---
ver: rpa2
title: 'LLMs for Translation: Historical, Low-Resourced Languages and Contemporary
  AI Models'
arxiv_id: '2503.11898'
source_url: https://arxiv.org/abs/2503.11898
tags:
- translation
- these
- safety
- gemini
- ottoman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that Google\u2019s Gemini 1.5 Pro translation\
  \ model frequently fails to translate sensitive historical content due to overly\
  \ aggressive AI safety filters, with 14\u201323% of an 18th-century Ottoman Turkish\
  \ manuscript left untranslated. The safety mechanisms\u2014designed to block harmful\
  \ material\u2014misclassify historical violence, war accounts, and mentions of sexual\
  \ assault as dangerous or explicit, even when translating rather than generating\
  \ such content."
---

# LLMs for Translation: Historical, Low-Resourced Languages and Contemporary AI Models

## Quick Facts
- **arXiv ID**: 2503.11898
- **Source URL**: https://arxiv.org/abs/2503.11898
- **Reference count**: 8
- **Primary result**: Gemini 1.5 Pro blocks 14–23% of an 18th-century Ottoman Turkish manuscript translation due to safety filters misclassifying historical content

## Executive Summary
This study demonstrates that Google's Gemini 1.5 Pro translation model frequently fails to translate sensitive historical content due to overly aggressive AI safety filters, with 14–23% of an 18th-century Ottoman Turkish manuscript left untranslated. The safety mechanisms—designed to block harmful material—misclassify historical violence, war accounts, and mentions of sexual assault as dangerous or explicit, even when translating rather than generating such content. Analysis shows these flags correlate with manuscript themes rather than language sparsity, indicating systemic issues in how LLMs handle context-rich historical narratives.

The research reveals critical limitations in current LLM safety protocols that risk silencing historical testimonies and modern victim accounts by refusing translation of sensitive but necessary content. Through systematic testing, the author identifies three key mechanisms behind these failures: use-mention confusion where translation of violence is treated as generating harm, context-lexical disambiguation failures with diachronic language drift, and hardcoded safety overrides that supersede API-configurable thresholds. The work calls for rethinking AI safety in translation to balance harm prevention with the need for accurate, complete rendering of complex human experiences.

## Method Summary
The study translates an 18th-century Ottoman Turkish manuscript ("Prisoner of the Infidels") using Gemini 1.5 Pro API, employing SentAlign for OT-EN alignment and VecAlign for DE-EN alignment. The author conducts sentence-by-sentence translation with two-pass retry for blocked sentences, logging outputs and safety ratings to CSV. Safety settings are configured with default thresholds for hate speech, dangerous content, harassment, and sexually explicit material. The methodology compares blocking rates between Ottoman Turkish and German translations to distinguish content-driven from language-driven triggers, and tests API safety threshold configurability through systematic modifications of age references and content framing.

## Key Results
- Gemini 1.5 Pro fails to translate 14–23% of an 18th-century Ottoman Turkish manuscript due to safety filters
- Blocking correlates with manuscript themes (violence, war, sexual assault) rather than language sparsity
- Safety settings at lowest thresholds still fail to translate content involving minors in sexual contexts, indicating hardcoded guardrails
- Diachronic semantic drift causes false positives when historical terms (e.g., "kerhâne") are misinterpreted through modern definitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety filters exhibit a "use-mention" confusion, misclassifying the translation of violent testimony as the generation of harmful content.
- Mechanism: The safety classification layer evaluates the semantic content of the source text rather than the functional task (translation). Because the model is trained to refuse generating harmful outputs, it applies this refusal policy to the mention of historical violence found in the source manuscript, blocking the translation request as if it were a prompt to generate new violent stories.
- Core assumption: Safety classifiers operate on input embedding or prompt content independently of "translation mode" instruction, failing to distinguish between an agent producing harm and an agent reporting historical fact.
- Evidence anchors:
  - [abstract] "The safety mechanisms... misclassify historical violence, war accounts... even when translating rather than generating such content."
  - [section 2.3] "Gligoric et al. (2024) argues that the use of words... is traditionally distinguished from the mention... This distinction is pivotal for our research, as translation further complicates this issue."
  - [corpus] Related work on "NLP systems that can't tell use from mention" confirms this is a known systemic issue in content moderation.

### Mechanism 2
- Claim: Context-lexical disambiguation fails when historical semantics conflict with modern safety triggers.
- Mechanism: Safety layer relies on pattern matching specific tokens or phrases. In low-resource or historical languages, words may have drifted in meaning (e.g., kerhâne shifting from "workshop" to "brothel"). The model may correctly translate the word based on context but the safety layer flags the historical usage based on modern definitions or associations with sensitive topics, resulting in false positive.
- Core assumption: Safety filters weigh specific token triggers heavily, potentially overriding context window's semantic resolution of translation model.
- Evidence anchors:
  - [section 7 - Example 1] "We believe that this mistake arose from the word kerhâne... In Modern Turkish, kerhâne, refers exclusively to a place of sex work... in OT it refers to a place of work."
  - [abstract] "Analysis shows these flags correlate with manuscript themes rather than language sparsity."
  - [corpus] Weak/Direct link: Corpus signals discuss "cultural nuances" and "spelling normalization" in LLMs, supporting difficulty of historical semantic drift.

### Mechanism 3
- Claim: Hard-coded "immutable" safety overrides exist that supersede API-configurable thresholds.
- Mechanism: API allows users to adjust safety settings, but system appears to have secondary, non-configurable "guardrail" layer. When specific high-risk patterns are detected (e.g., minors in sexual contexts), this layer blocks request regardless of user-set probability thresholds or "off" switches.
- Core assumption: Architecture employs tiered safety system where enterprise/user-configurable filters are subordinate to rigid, hardcoded "harm-prevention" model focused on specific severe categories (like CSAM or minor safety).
- Evidence anchors:
  - [section 7 - Example 4] "This sentence remained untranslated even after turning the model safety settings off... coupled with the sexual contents in the passage triggered deeper, unchangeable settings."
  - [section 7 - Example 4] "We tested this further by changing the age... to twenty. The model then translated..."
  - [corpus] No direct corpus evidence for Gemini's internal hierarchy, but standard industry practice suggests tiered safety systems.

## Foundational Learning

- Concept: **Use-Mention Distinction in NLP**
  - Why needed here: To understand why model refuses to process valid historical data. Engineers must grasp that current LLMs often conflate processing a harmful string (mention) with producing it (use), which is core failure mode identified in paper.
  - Quick check question: Does the model refuse to translate the phrase "The bomb exploded" because it thinks you are asking it to make a bomb, or because it thinks the output itself is harmful content?

- Concept: **Diachronic Linguistic Drift**
  - Why needed here: Ottoman Turkish words have different meanings than Modern Turkish. Robust system must recognize that a "harmful" word in modern lexicon may be benign in historical context (e.g., kerhâne).
  - Quick check question: If a user translates a text from 1720 containing the word "gay" (meaning joyful), does the safety filter flag it based on modern sexual connotations?

- Concept: **API Safety Thresholds vs. Hard Guardrails**
  - Why needed here: Developers need to know limits of their control. Paper proves that setting `safety_settings={"type": "block_none"}` is insufficient for certain "high-risk" triggers, implying hidden hierarchy of controls.
  - Quick check question: If you set the "harmful content" probability threshold to 0, can you still elicit a refusal by using specific trigger words related to child safety?

## Architecture Onboarding

- Component map: Input Interface -> Safety Layer 1 (Configurable) -> Safety Layer 2 (Hardcoded/Rigid) -> Translation Engine (LLM) -> Output Filter
- Critical path: Request flow intercepted by Safety Layer 1. If it passes, Safety Layer 2 evaluates it. Failure occurs primarily in Layer 1 (due to semantic drift) or Layer 2 (due to rigid immutable rules), preventing Translation Engine from receiving context or returning result.
- Design tradeoffs:
  - Safety vs. Utility: System prioritizes minimizing false negatives (generating actual harm) at cost of massive false positives (blocking historical data).
  - Configurability vs. Control: Google retains ultimate control via hardcoded layers to prevent liability, limiting utility of tool for sensitive domains (history, law).
- Failure signatures:
  - Silent Refusals: API returns empty response or specific error code without translation, often citing "safety" without detailing which layer triggered.
  - Inconsistent Blocking: Sentence blocked in Ottoman Turkish but German translation (same meaning) might pass or vice versa, indicating filter reacts to token patterns rather than semantic intent.
  - Threshold Immunity: Setting all safety scores to `BLOCK_NONE` fails to produce translation for specific sensitive triggers.
- First 3 experiments:
  1. Threshold Probe: Send the "kerhâne" example to API. Set safety settings to lowest threshold. Vary prompt to explicitly frame as "historical text." Observe if refusal persists or explanation changes.
  2. Semantic Substitution: Take blocked sentence involving minor. Incrementally change age variable (14, 16, 18, 20) and chart exactly where translation is allowed to pinpoint rigid boundary of hardcoded guardrail.
  3. Pivot Language Test: Translate blocked Ottoman text into third high-resource language (e.g., French) first, then French to English. Determine if "breaking" direct link to sensitive source tokens allows content to pass safety filters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AI safety protocols be redesigned to distinguish between generating harmful content and translating existing content, particularly for sensitive historical or legal testimony?
- Basis in paper: [explicit] Authors call for "rethinking AI safety in translation to balance harm prevention with need for accurate, complete rendering of complex human experiences."
- Why unresolved: Current safety filters conflate translation with content generation, blocking 14–23% of manuscript even when safety settings are nominally disabled.
- What evidence would resolve it: Development and testing of safety architectures that use document-level context, source-language intent classification, or explicit "translation mode" flags that relax content filters while maintaining output monitoring.

### Open Question 2
- Question: Do safety classification thresholds vary systematically between high-resourced and low-resourced languages for same content?
- Basis in paper: [inferred] Comparison between Ottoman Turkish and German translations showed similar flagging patterns, suggesting content-driven rather than language-driven triggers, but severity scores differed (higher for OT in three categories).
- Why unresolved: Severity difference was not statistically significant due to sample size; interaction between language resource level and safety classification remains unclear.
- What evidence would resolve it: Systematic comparison of safety flagging across multiple language pairs with controlled content, varying resource levels from high to extremely low.

### Open Question 3
- Question: How do LLMs handle age-related content in translation contexts, and what are implications for translating historical documents or legal testimony involving minors?
- Basis in paper: [explicit] Example 4 shows model refused to translate passage about 15-year-old even with safety settings off; authors note this "merits further investigation."
- Why unresolved: Mechanisms blocking age-related content appear to operate at deeper, inaccessible layer than user-configurable safety settings.
- What evidence would resolve it: Controlled experiments translating same passage with varied age references across multiple models, combined with transparency from model developers about hard-coded safety layers.

### Open Question 4
- Question: Whose experiences are systematically excluded from AI-mediated translation due to safety filter thresholds?
- Basis in paper: [explicit] Authors ask: "Whose experiences do not meet the threshold of safety requirements or information policies of companies and governments?"
- Why unresolved: No empirical framework exists to measure representation bias introduced by safety filters across different content types, languages, or speaker populations.
- What evidence would resolve it: Large-scale analysis of translation refusal rates across diverse corpora (war testimonies, legal documents, medical records) correlated with demographic or content characteristics.

## Limitations

- Single manuscript source limits generalizability of safety filter behavior across different types of historical content or languages
- Exact API implementation details, including precise translation prompt structure and safety threshold configurations, remain unspecified
- Analysis focuses on Gemini 1.5 Pro specifically without comparative testing across multiple translation models or safety frameworks
- No granular error analysis distinguishing between false positives and legitimate safety interventions

## Confidence

**High Confidence Claims:**
- Empirical observation that Gemini 1.5 Pro fails to translate 14-23% of manuscript due to safety filters is well-supported by presented data and methodology
- Correlation between blocking rates and manuscript themes rather than language sparsity is clearly demonstrated through comparative analysis
- Existence of hardcoded safety guardrails that override user-configurable settings is convincingly shown through minor age manipulation experiments

**Medium Confidence Claims:**
- Assertion that safety filters exhibit "use-mention" confusion is well-reasoned but relies on inference about internal model behavior
- Diachronic semantic drift explanation for blocking historical terms is plausible and supported by examples, but extent remains uncertain
- Characterization of safety filter failures as systemic rather than isolated incidents is reasonable but would benefit from additional testing

**Low Confidence Claims:**
- Broader claims about impact on historical scholarship and modern victim testimony extend beyond empirical scope and remain speculative

## Next Checks

1. **Cross-Model Safety Filter Comparison:** Replicate blocking rate analysis using alternative translation models (GPT-4, Claude, DeepL) with identical safety configurations to determine whether observed blocking behavior is specific to Gemini 1.5 Pro or represents broader industry pattern in LLM safety implementation.

2. **Historical Content Type Sensitivity Analysis:** Test safety filters using diverse corpus of historical texts spanning different time periods, cultures, and subject matters (medieval legal documents, ancient religious texts, colonial-era correspondence) to map which content categories and historical contexts most frequently trigger safety mechanisms.

3. **Safety Filter Granularity Testing:** Systematically vary individual safety category thresholds (hate speech, sexual content, dangerous content, harassment) to identify which specific categories drive blocking behavior and whether granular control over safety parameters can reduce false positives while maintaining genuine harm prevention.