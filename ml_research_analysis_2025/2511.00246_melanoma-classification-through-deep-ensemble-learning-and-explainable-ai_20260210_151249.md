---
ver: rpa2
title: Melanoma Classification Through Deep Ensemble Learning and Explainable AI
arxiv_id: '2511.00246'
source_url: https://arxiv.org/abs/2511.00246
tags:
- melanoma
- ensemble
- learning
- images
- skin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of early melanoma detection using
  dermoscopy images, focusing on improving accuracy and interpretability of deep learning
  models. A deep ensemble learning framework combining ResNet-101, DenseNet-121, and
  Inception v3 was developed to classify melanoma images, leveraging transfer learning
  and weighted averaging with hyperbolic tangent-based weights.
---

# Melanoma Classification Through Deep Ensemble Learning and Explainable AI

## Quick Facts
- arXiv ID: 2511.00246
- Source URL: https://arxiv.org/abs/2511.00246
- Reference count: 11
- Primary result: Deep ensemble framework achieves 85.80% accuracy and 0.93 ROC-AUC on ISIC 2019/2020 melanoma classification

## Executive Summary
This study addresses the challenge of early melanoma detection using dermoscopy images, focusing on improving accuracy and interpretability of deep learning models. A deep ensemble learning framework combining ResNet-101, DenseNet-121, and Inception v3 was developed to classify melanoma images, leveraging transfer learning and weighted averaging with hyperbolic tangent-based weights. SHAP analysis was used to interpret model predictions, enhancing trust and transparency. The framework achieved an accuracy of 85.80% and ROC-AUC score of 0.93 on ISIC 2019 and 2020 datasets, outperforming individual base models and prior methods. SHAP explanations highlighted relevant lesion features and identified issues like hair and artifacts affecting predictions, suggesting improvements through image preprocessing and clinical validation for future work.

## Method Summary
The framework employs a deep ensemble of three pre-trained convolutional neural networks (ResNet-101, DenseNet-121, and Inception v3) for melanoma classification from dermoscopy images. Transfer learning is utilized by initializing models with ImageNet weights and adding custom fully connected layers. The dataset is balanced by downsampling the majority (benign) class to 5,106 samples per class from ISIC 2019 and 2020. Image preprocessing includes resizing to 224x224, enhancement (Color 1.2, Sharpness 25.0, Brightness -20, Contrast 1.5, Center Crop 0.75), and normalization. Models are trained with batch size 64, Adam optimizer (lr=0.0001), categorical cross-entropy loss, and early stopping. The ensemble uses weighted averaging where weights are computed using hyperbolic tangent functions of Precision, Recall, F1, and ROC-AUC scores (excluding Accuracy). SHAP analysis provides interpretability by highlighting influential pixels in predictions.

## Key Results
- Ensemble achieves 85.80% accuracy and 0.93 ROC-AUC on ISIC 2019/2020 datasets
- Ensemble outperforms individual base models (DenseNet-121: 84.1% accuracy, ResNet-101: 81.6%, Inception v3: 82.3%)
- SHAP analysis reveals model attention on lesion tissue but also identifies spurious artifacts (hair, rulers) affecting predictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The ensemble framework improves classification robustness by fusing feature representations from architecturally distinct base learners, conditioned on their individual performance metrics.
- **Mechanism:** ResNet-101, DenseNet-121, and Inception v3 possess different inductive biases (e.g., residual connections vs. dense connectivity vs. inception modules). By weighting their probability outputs using a hyperbolic tangent function derived from Precision, Recall, F1, and ROC-AUC scores (rather than simple accuracy), the system rewards models that better handle the specific trade-offs in medical screening.
- **Core assumption:** The base learners make uncorrelated errors; if all models misclassify the same artifacts (like hair), the ensemble will amplify rather than correct the error.
- **Evidence anchors:**
  - [abstract]: "...leveraging transfer learning and weighted averaging with hyperbolic tangent-based weights."
  - [section 4.4.4]: "The weight of the ith base learner (wi) was computed using the Hyperbolic Tangent function... if the value of a metric m is high, the function acknowledges it by rewarding the weight."
  - [corpus]: Related work supports the general efficacy of deep ensembles for melanoma, though specific hyperbolic tangent weighting schemes are not explicitly validated in the provided neighbor abstracts.

### Mechanism 2
- **Claim:** Transfer learning allows the system to achieve competitive accuracy on relatively small, balanced datasets by leveraging pre-learned feature hierarchies.
- **Mechanism:** The models are initialized with ImageNet weights. During fine-tuning, the early layers (detecting edges/textures) are largely preserved, while custom fully connected layers are trained to map these generic features to specific lesion characteristics (malignant vs. benign).
- **Core assumption:** Dermoscopy images share low-level visual primitives (edges, color gradients) with general photography datasets like ImageNet.
- **Evidence anchors:**
  - [abstract]: "...deep ensemble learning framework combining ResNet-101, DenseNet-121, and Inception v3... leveraging transfer learning..."
  - [section 4.3]: "Transfer learning was used to train all the networks that were pre-trained using ImageNet dataset... all the pre-trained networks were loaded without the top output layers and custom fully connected output layers were added."
  - [corpus]: General consensus in "Explainable Melanoma Diagnosis" and related papers supports transfer learning for data-scarce medical imaging.

### Mechanism 3
- **Claim:** SHAP (SHapley Additive exPlanations) analysis validates model reliability by ensuring predictions are based on lesion tissue rather than spurious artifacts.
- **Mechanism:** Gradient-based SHAP values approximate the contribution of each pixel to the output. By visualizing these values, one can verify if high-contribution regions (red pixels) align with the lesion center (trustworthy) or artifacts like hair/rulers (spurious).
- **Core assumption:** Features with high SHAP values are interpretable by humans as clinically relevant structures.
- **Evidence anchors:**
  - [abstract]: "SHAP explanations highlighted relevant lesion features and identified issues like hair and artifacts affecting predictions..."
  - [section 5.3]: "In the results for sample B... the hair atop the surface of the skin lesion is highlighted in red... thus hair has increased the probability of the class predicted."
  - [corpus]: "Bluish Veil Detection..." and "Brain Stroke Detection..." papers confirm SHAP's utility in verifying model attention in medical imaging.

## Foundational Learning

- **Concept: Class Imbalance & Downsampling**
  - **Why needed here:** The ISIC 2020 dataset has extreme imbalance (1.8% malignant). The paper explicitly balances this by downsampling the majority class to prevent the model from simply predicting "benign" all the time.
  - **Quick check question:** If you have 100 images (99 benign, 1 malignant) and achieve 99% accuracy by always guessing "benign," why is your model useless for early detection?

- **Concept: Weighted Averaging vs. Majority Voting**
  - **Why needed here:** The paper compares hard voting (simple democracy) to weighted averaging. Understanding how to assign weights based on performance metrics (Precision/Recall) is crucial for the final 0.93 ROC-AUC score.
  - **Quick check question:** Why might "Hard Majority Voting" fail if three models are all 70% accurate but all make the exact same mistakes?

- **Concept: SHAP (SHapley Additive exPlanations)**
  - **Why needed here:** This is the interpretability layer. It moves the system from a "black box" to a support tool by identifying *which* pixels drove the decision.
  - **Quick check question:** If a SHAP heatmap highlights a ruler mark instead of the tumor, what does this tell you about the training data the model learned from?

## Architecture Onboarding

- **Component map:**
  - Input (ISIC 2019/2020 Dermoscopy Images + Metadata) -> Preprocessing (PIL/OpenCV enhancement, resize to 224x224) -> Base Learners (ResNet-101, DenseNet-121, Inception v3 with ImageNet weights + Custom FC layers) -> Aggregator (Weighted Probability Averaging with tanh weights) -> Explainer (Gradient SHAP overlay)

- **Critical path:** The **Weight Calculation Logic**. The system does not treat models equally. It computes a specific weight for each model based on its validation performance metrics *excluding* accuracy (using only PRE, REC, F1, AUC as per the best result). An engineer must ensure these weights are recalibrated if the data distribution shifts.

- **Design tradeoffs:**
  - **Downsampling vs. Augmentation:** The paper chose to downsample the majority class (throwing away data) to ensure balance, rather than purely relying on synthetic generation. This reduces training time but may lose benign variance.
  - **Tanh Weights:** The use of `tanh` for weighting bounds the weights but is a somewhat arbitrary transformation compared to linear scaling or softmax.

- **Failure signatures:**
  - **Artifact Sensitivity:** The paper explicitly notes the model focuses on hair and "microscopic effects" (circular vignettes).
  - **High FN (False Negative) Rate:** Despite 85.8% accuracy, the clinical risk is high if the model misses a malignant case. The paper notes future work is needed to boost sensitivity.

- **First 3 experiments:**
  1.  **Artifact Ablation:** Retrain the ensemble on images with aggressive hair removal preprocessing (e.g., morphological filtering) to see if SHAP attention shifts strictly to lesion tissue.
  2.  **Weight Sensitivity Analysis:** Compare the "Tanh Weighted" averaging against a simple Softmax-weighted approach to verify if the hyperbolic tangent adds specific value or if any weighted averaging suffices.
  3.  **Metric Isolation:** Run the ensemble using *only* Recall (Sensitivity) for weights to specifically minimize False Negatives, comparing the ROC curve against the current multi-metric approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the SHAP-based explanations be validated against specific clinical dermoscopic criteria used by dermatologists?
- Basis in paper: [explicit] The authors state future research will focus on "validating the explanations of the model's output against the clinical features."
- Why unresolved: The current study confirms the model looks at the lesion area, but it does not verify if the "regions of interest" align with standard medical diagnostic rules (e.g., ABCD rule).
- What evidence would resolve it: A quantitative study correlating high SHAP value regions with ground-truth annotations of clinical features provided by medical experts.

### Open Question 2
- Question: How can the ensemble framework be optimized to maximize sensitivity (recall) specifically, rather than general accuracy?
- Basis in paper: [explicit] The conclusion lists "sensitivity (recall) enrichment" as a primary goal for future work due to the critical nature of false negatives in cancer diagnosis.
- Why unresolved: The current model optimizes for overall accuracy (85.80%), potentially sacrificing the detection of difficult malignant cases to minimize false positives.
- What evidence would resolve it: Implementation of cost-sensitive learning or decision threshold tuning demonstrating improved recall scores with manageable precision trade-offs.

### Open Question 3
- Question: Does the implementation of automated occlusion removal (e.g., hair and artifact removal) significantly improve the predictive reliability of the ensemble?
- Basis in paper: [explicit] Authors identify via SHAP that artifacts like hair mislead the model and suggest "occlusion removal" as a necessary preprocessing step for future improvements.
- Why unresolved: The paper identifies the problem (artifacts influencing predictions) but does not implement the proposed solution to measure its impact on performance.
- What evidence would resolve it: Ablation studies comparing model performance and SHAP focus on raw images versus images processed with digital hair removal techniques.

## Limitations
- **Dataset Scope**: Results are validated on ISIC 2019/2020 only; generalization to other clinical settings or population demographics is unknown.
- **Data Balancing Method**: Downsampling the majority class may have discarded useful benign lesion variation that could improve real-world robustness.
- **Artifact Sensitivity**: The system's attention to non-pathological features (hair, rulers) suggests it may not yet be clinically deployable without significant preprocessing.

## Confidence
- **High**: Ensemble architecture specification, SHAP-based interpretability findings, overall accuracy (85.80%) and ROC-AUC (0.93) on reported test sets.
- **Medium**: Claims about superiority over individual models (requires assumption about test split fairness), effectiveness of hyperbolic tangent weighting scheme (not compared to alternatives in paper).
- **Low**: Generalizability to other datasets, clinical impact on diagnostic decision-making, long-term performance in dynamic clinical environments.

## Next Checks
1. **Artifact Ablation Study**: Retrain the ensemble on hair-removed images to verify if SHAP attention shifts strictly to lesion tissue and whether this improves clinical sensitivity.
2. **Alternative Weighting Comparison**: Implement and compare softmax-weighted averaging against the hyperbolic tangent approach to determine if the specific weighting function adds value beyond basic performance-based weighting.
3. **Cross-Dataset Validation**: Test the trained ensemble on a held-out ISIC archive year or an external dermatology dataset to assess true generalization beyond the training cohort.