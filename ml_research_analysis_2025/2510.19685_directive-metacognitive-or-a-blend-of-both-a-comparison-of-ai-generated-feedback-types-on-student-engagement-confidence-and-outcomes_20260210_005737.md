---
ver: rpa2
title: Directive, Metacognitive or a Blend of Both? A Comparison of AI-Generated Feedback
  Types on Student Engagement, Confidence, and Outcomes
arxiv_id: '2510.19685'
source_url: https://arxiv.org/abs/2510.19685
tags:
- feedback
- metacognitive
- directive
- hybrid
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared AI-generated directive, metacognitive, and
  hybrid feedback in a university course with 329 students. The hybrid approach led
  to the highest revision rates, though confidence and final work quality were similar
  across all conditions.
---

# Directive, Metacognitive or a Blend of Both? A Comparison of AI-Generated Feedback Types on Student Engagement, Confidence, and Outcomes

## Quick Facts
- arXiv ID: 2510.19685
- Source URL: https://arxiv.org/abs/2510.19685
- Reference count: 40
- Primary result: Hybrid AI feedback yielded highest revision rates, though confidence and quality were similar across conditions

## Executive Summary
This study compared directive, metacognitive, and hybrid AI-generated feedback in a university course with 329 students. The hybrid approach led to the highest revision rates, though confidence and final work quality were similar across all conditions. Students receiving metacognitive feedback engaged deeply but revised less often, suggesting cognitive processing without action. The findings highlight hybrid feedback's potential to balance actionable guidance with reflective opportunities, supporting both immediate improvement and self-regulation. The study demonstrates that AI feedback can be tailored to pedagogical goals, with hybrid models offering a promising blend of clarity and reflection for scalable, personalized learning support.

## Method Summary
The study used a randomized controlled trial in an introductory educational design/programming course. Students (n=329) created learning resources (MCQs, notes, or flip cards) and received AI-generated feedback in one of three conditions: directive (imperatives), metacognitive (reflective prompts), or hybrid (both). Feedback followed a fixed structure (Summary, Strengths, Suggestions) but varied in the linguistic style of the Suggestions section. Engagement time, revision behavior, confidence ratings, and resource quality were measured. Data were analyzed using mixed-effects models (Gamma GLMM for time, binomial GLMM for revision, CLMM for confidence, Beta GLMM for quality).

## Key Results
- Hybrid feedback produced the highest revision rates (27.5%) compared to directive (21.1%) and metacognitive (12.1%)
- Metacognitive feedback led to deep engagement but low revision, suggesting cognitive processing without action
- Confidence ratings were uniformly high across all feedback types, with no significant differences
- Final resource quality was similar across conditions, possibly due to ceiling effects from high baseline performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid AI feedback combining directive guidance with reflective prompts yields higher revision rates than either approach alone.
- Mechanism: Directive elements (imperatives, explicit corrections) reduce cognitive load for identifying what to fix, while metacognitive prompts (reflective questions) encourage deeper processing. The combination scaffolds action for novices who might otherwise freeze on open-ended reflection, while still promoting metacognitive engagement.
- Core assumption: Learners possess sufficient feedback literacy to translate at least some reflective prompts into action when paired with concrete guidance.
- Evidence anchors:
  - [abstract] "Hybrid prompting the most revisions compared to Directive and Metacognitive" with 27.5% vs 21.1% vs 12.1%
  - [section 3.2.2] Logistic model showed Hybrid had ~4.3x higher odds of revision than Metacognitive (p < .001)
  - [corpus] Related work on AI-assisted feedback (Ba et al., 2025) supports that LLM-generated feedback can positively influence learner actions, though direct mechanism evidence is limited
- Break condition: If learners lack baseline feedback literacy or task knowledge is too low, even hybrid feedback may fail to prompt revision; reflective prompts could increase cognitive load without productive action.

### Mechanism 2
- Claim: Metacognitive-only feedback produces lower revision rates because learners engage in "strategic non-action" or cognitive overload without clear guidance on what to do.
- Mechanism: Reflective prompts require learners to diagnose issues and plan revisions themselves. For novices or time-pressured learners, this increases extraneous cognitive load and may lead to a cognitive "saturation point" where they process the feedback but do not act.
- Core assumption: Learners in this population had variable metacognitive readiness; without scaffolding, many chose not to revise despite spending equivalent time engaging with feedback.
- Evidence anchors:
  - [abstract] Metacognitive condition had lowest revision rate (12.1%)
  - [section 4.1] "Metacognitive feedback...often encourages learners to engage in strategic non-action...cognitive overload may have hindered some students"
  - [corpus] Limited direct corpus evidence; assumption-based inference from SRL literature
- Break condition: If learners have high metacognitive skill and strong feedback literacy, metacognitive-only feedback may produce deeper learning and more meaningful revisions; effect is learner-dependent.

### Mechanism 3
- Claim: Confidence is preserved across feedback types when all feedback includes positive elements (strengths affirmation) alongside improvement suggestions.
- Mechanism: Balanced feedback that affirms competence while offering constructive guidance supports self-efficacy and prevents negative emotional responses that could inhibit motivation.
- Core assumption: The consistent high confidence across conditions is partially attributable to the fixed "Summary" and "Strengths" sections in all feedback types.
- Evidence anchors:
  - [abstract] "Confidence ratings were uniformly high"
  - [section 3.3] No significant differences in confidence; medians identical (Mdn = 4.0) across groups
  - [corpus] Limited corpus evidence directly testing this mechanism
- Break condition: If feedback is perceived as AI-generated and thus less credible, confidence effects may differ; or if negative feedback dominates without strengths, confidence could decline.

## Foundational Learning

- Concept: **Self-Regulated Learning (SRL)**
  - Why needed here: The study frames metacognitive feedback as supporting SRL cycles (forethought, performance, reflection). Understanding this helps interpret why reflective prompts alone may not lead to action without learner readiness.
  - Quick check question: Can you explain the difference between metacognitive knowledge (awareness of strategies) and metacognitive regulation (planning, monitoring, evaluating)?

- Concept: **Cognitive Load Theory**
  - Why needed here: Directive feedback is theorized to reduce extraneous cognitive load by providing explicit corrections. Understanding this explains why novices may struggle with metacognitive-only feedback.
  - Quick check question: What is the difference between intrinsic, extraneous, and germane cognitive load in a feedback context?

- Concept: **Feedback Literacy**
  - Why needed here: The effectiveness of metacognitive and hybrid feedback depends on learners' ability to interpret, manage affect, and translate feedback into action. Without this, reflective prompts may fail.
  - Quick check question: What are the three core components of student feedback literacy per Carless & Boud (2018)?

## Architecture Onboarding

- Component map:
  - Prompt design layer -> Content generation layer -> Delivery layer -> Analytics layer

- Critical path:
  1. Define feedback type and linguistic constraints (imperative rate, reflective prompt rate)
  2. Generate feedback with consistent structure (Summary → Strengths → Suggestions)
  3. Deliver immediately after student submission, before self-assessment
  4. Log engagement time (interval: feedback receipt → next action)
  5. Capture revision events (binary: revised/not revised)
  6. Collect confidence rating (post-revision or pre-submission)
  7. Measure outcome quality via peer/instructor evaluation

- Design tradeoffs:
  - **Verbosity vs. actionability**: Hybrid feedback was longest (M=134 words) vs. directive (83 words); longer feedback may increase processing time without guaranteeing better outcomes
  - **Reflection vs. revision**: Metacognitive prompts may increase cognitive engagement but reduce behavioral response; directive prompts increase revision but may limit depth
  - **Personalization vs. consistency**: AI enables consistent delivery at scale but may reduce perceived credibility compared to human feedback

- Failure signatures:
  - Low revision rates with high engagement time (learners reading but not acting) → likely metacognitive overload or low feedback literacy
  - High revision rates with low outcome improvement → surface-level compliance without conceptual integration
  - Uniformly high confidence with low performance → possible overconfidence or ceiling effects

- First 3 experiments:
  1. **A/B test hybrid vs. directive feedback** in a new domain (e.g., writing, math) with lower baseline performance to test if revision effects replicate and whether outcome quality improves
  2. **Stratified analysis by prior achievement or feedback literacy** to identify learner subgroups who benefit most from metacognitive vs. hybrid vs. directive feedback
  3. **Add qualitative follow-up** (think-aloud or short survey) after feedback receipt to capture why learners chose to revise or not, testing the "strategic non-action" hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does hybrid AI feedback significantly improve work quality in student populations with lower baseline performance or prerequisite knowledge?
- Basis in paper: [inferred] The authors note the study was conducted in a course with "relatively high baseline performance," which may have created ceiling effects that prevented significant differences in resource quality outcomes from emerging.
- Why unresolved: The lack of variance in the dependent variable (quality) makes it difficult to determine if the feedback type is ineffective or if the assessment task was insufficiently difficult for the cohort.
- What evidence would resolve it: A replication of this RCT with a cohort identified as having lower prior proficiency or using a more complex assessment task.

### Open Question 2
- Question: To what extent do specific AI feedback types influence the depth of revisions versus superficial compliance?
- Basis in paper: [inferred] The limitations section states that the binary "action on feedback" measure did not "distinguish between surface edits and deeper integration of feedback," leaving the quality of the revision process ambiguous.
- Why unresolved: High revision rates in the Hybrid group could indicate deep engagement or merely successful compliance with explicit directives without conceptual change.
- What evidence would resolve it: Qualitative content analysis of the specific changes made in student revisions to code them as surface-level corrections or deep conceptual restructuring.

### Open Question 3
- Question: Do the self-regulatory behaviors prompted by AI-generated feedback transfer to subsequent learning contexts without AI support?
- Basis in paper: [inferred] The paper explicitly acknowledges in the limitations that the 12-week design "did not assess whether gains transferred beyond the course."
- Why unresolved: It remains unclear if the Hybrid group's higher engagement reflects a temporary reaction to the feedback interface or a lasting development of self-regulated learning (SRL) skills.
- What evidence would resolve it: A longitudinal follow-up study tracking participants' SRL behaviors and performance in future courses where AI feedback is unavailable.

## Limitations
- The study used GPT-4 Turbo but exact prompts and temperature settings are not disclosed, limiting replication
- High baseline performance created ceiling effects that may have suppressed observable differences in quality outcomes
- Results are specific to educational resource creation in a programming/design course and may not generalize to other domains

## Confidence

- **High confidence**: Hybrid feedback produces significantly higher revision rates than metacognitive or directive alone
- **Medium confidence**: Metacognitive feedback leads to deep engagement but low revision due to strategic non-action/cognitive overload
- **Medium confidence**: Confidence preservation across conditions is due to balanced feedback structure

## Next Checks
1. Test in lower-performing cohorts to determine if hybrid feedback's revision advantage holds when there is more room for improvement
2. Measure feedback literacy to test whether effects vary by learner readiness, validating the "strategic non-action" hypothesis
3. Add qualitative probes (think-aloud or surveys) after feedback receipt to capture why learners chose to revise or not, directly testing the cognitive overload vs. strategic engagement mechanism