---
ver: rpa2
title: 'PMMD: A pose-guided multi-view multi-modal diffusion for person generation'
arxiv_id: '2512.15069'
source_url: https://arxiv.org/abs/2512.15069
tags:
- image
- pose
- diffusion
- text
- pmmd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PMMD, a pose-guided multi-view multimodal diffusion
  framework for person image generation, addressing the challenge of occlusion, garment
  style drift, and pose misalignment in existing methods. PMMD integrates a multimodal
  encoder for joint modeling of multi-view images, pose maps, and text prompts, a
  ResCV A module for local detail enhancement, and a cross-modal fusion module for
  semantic-text alignment.
---

# PMMD: A pose-guided multi-view multi-modal diffusion for person generation

## Quick Facts
- arXiv ID: 2512.15069
- Source URL: https://arxiv.org/abs/2512.15069
- Reference count: 0
- Primary result: State-of-the-art multi-view person generation with FID 8.56, SSIM 0.74, LPIPS 0.19 on DeepFashion-MultiModal

## Executive Summary
This paper introduces PMMD, a diffusion-based framework for pose-guided multi-view person image generation that addresses occlusion, garment style drift, and pose misalignment challenges. The method integrates a multimodal encoder for joint modeling of multi-view images, pose maps, and text prompts, a ResCVA module for local detail enhancement, and a cross-modal fusion module for semantic-text alignment. PMMD achieves state-of-the-art results on the DeepFashion-MultiModal dataset, outperforming baselines like IP-Adapter and ControlNet in both quantitative metrics and user studies.

## Method Summary
PMMD builds on Stable Diffusion 1.5 with a multimodal encoder that jointly processes multi-view images (combined into 2×2 format), DensePose maps, and compressed text descriptions. The framework employs a ResCVA module with residual connections for local detail enhancement, and a cross-modal fusion module that injects compressed text-image features into cross-attention layers throughout the denoising pipeline. Training uses random masking of 1-3 views to simulate occlusion, with a 0.05 image condition dropout probability for classifier-free guidance.

## Key Results
- Achieves FID of 8.56, SSIM of 0.74, and LPIPS of 0.19 on DeepFashion-MultiModal dataset
- Outperforms baselines with 55.4% G2R and 45.6% Jab in user studies
- Ablation studies confirm effectiveness of each module in preserving texture, pose, and semantic consistency

## Why This Works (Mechanism)

### Mechanism 1: Multi-view Joint Latent Encoding with Random Masking
Jointly encoding multiple views with random masking during training enables handling occlusion and synthesizing plausible unseen regions. Multi-view images are combined into a 2×2 joint image, encoded to latent space, then perturbed with Gaussian noise and binary mask. During training, 1-3 views are randomly masked to simulate varying reference availability, forcing the model to learn to interpolate missing information. Core assumption: Information from visible views can be transferred to fill occluded or missing regions in target generation.

### Mechanism 2: Residual Cross-View Attention (ResCVA) for Detail Preservation
A residual connection around cross-view attention incrementally enhances local texture detail without disrupting global structure. After standard cross-attention in the U-Net, the CVA module splits global features into local patches, applies self-attention among them, and reassembles. The output is added back via: y = x + CVA(x). This allows local enhancement while preserving the original feature baseline. Core assumption: The residual pathway acts as an additive refinement; base features already capture correct global structure.

### Mechanism 3: Compressed Text-Visual Fusion via Cross-Attention Injection
Compressing long clothing descriptions into semantic keywords and fusing them with visual features in cross-attention improves semantic controllability. Long text → Sentence-BERT compression → CLIP text encoder → fused with CLIP image features via IP-Adapter → injected into cross-attention layers of the denoising U-Net throughout the denoising pipeline. Core assumption: Compressed keyword summaries preserve essential garment attributes without exceeding encoder input limits.

## Foundational Learning

- **Latent Diffusion Models (LDM)**: Why needed here - PMMD builds on Stable Diffusion 1.5, which operates in compressed latent space rather than pixel space. Understanding this is essential for debugging reconstruction quality. Quick check question: Why does diffusion in latent space reduce computational cost compared to pixel-space diffusion, and what tradeoff does it introduce?

- **Cross-Attention vs. Self-Attention in U-Net**: Why needed here - The cross-modal fusion module injects text and image features into cross-attention layers. Distinguishing this from self-attention is critical for understanding conditioning. Quick check question: In a diffusion U-Net, what does cross-attention condition on, and how does it differ from the self-attention operation?

- **DensePose Representation**: Why needed here - PMMD uses DensePose (pixel-level body part mappings) rather than sparse keypoints. This provides stronger structural constraints. Quick check question: What additional semantic information does DensePose provide compared to a 17-keypoint skeleton?

## Architecture Onboarding

- **Component map**: Multi-view input → 2×2 joint image → VAE encoder → add noise + mask → 8-channel U-Net input → ControlNet pose injection → cross-attention with fused text-image features → ResCVA → denoised latent → VAE decoder → output image

- **Critical path**: Multi-view input → 2×2 joint image → VAE encoder → add noise + mask → 8-channel U-Net input → ControlNet pose injection → cross-attention with fused text-image features → ResCVA → denoised latent → VAE decoder → output image

- **Design tradeoffs**:
  - **Downsampling factor f**: f=4 preserves resolution but increases latent size; larger f reduces cost but risks blurring
  - **Guidance weight ω=0.7**: Balances pose conditioning vs. text-image conditioning
  - **Random drop probability 0.05**: Enables classifier-free guidance but reduces conditioning signal frequency

- **Failure signatures**:
  - Blurry facial regions: Check downsampling factor or ResCVA integration
  - Garment style drift: Verify text summarization preserves key attributes
  - Pose misalignment: Inspect DensePose map quality and ControlNet injection
  - Cross-view inconsistency: May indicate insufficient masking diversity during training

- **First 3 experiments**:
  1. Reproduce baseline comparison on DeepFashion-MultiModal at 256×176; measure FID, SSIM, LPIPS against IP-Adapter + ControlNet
  2. Ablate ResCVA (remove residual, use plain CVA); expect LPIPS increase and texture detail degradation
  3. Vary input view count (1, 2, 3) at inference; measure identity preservation to validate masking strategy effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
Does the Sentence-BERT compression of text prompts result in the loss of fine-grained control over specific garment attributes? The paper compresses text descriptions to "summary-like keywords" to fit CLIP input limits, but this discards linguistic nuance potentially removing specific details essential for high-fidelity generation. Evidence needed: An ablation study comparing generation accuracy using compressed prompts versus raw, segmented prompts on a dataset with fine-grained attribute labels.

### Open Question 2
Can the framework scale to high-resolution outputs without prohibitive computational cost associated with smaller downsampling factors? The paper trains at 256×176; it is unclear if the current architecture remains stable and efficient at standard HD resolutions (e.g., 1024×1024). Evidence needed: Benchmarking FID and inference latency at resolutions >512px while comparing memory usage against the standard f=8 latent space.

### Open Question 3
Does the 2×2 joint image construction limit the model's ability to ingest more than three source views or non-uniform aspect ratios? The rigid grid structure imposes a hard constraint on the number of input views and forces resizing, potentially limiting flexibility for diverse real-world inputs. Evidence needed: Testing generation quality and consistency when varying the number of input views (e.g., 5 or 6 views) to see if the fixed grid forces detrimental down-sampling.

## Limitations
- Unknown training duration and exact VAE downsampling factor create reproducibility challenges
- Sentence-BERT compression may lose fine-grained garment attribute details essential for high-fidelity generation
- 2×2 joint image construction imposes hard constraints on input view count and aspect ratios

## Confidence
- Multi-view joint encoding with random masking: Medium
- ResCVA detail preservation: Medium-High
- Cross-modal text-visual fusion: Medium-High

## Next Checks
1. **Ablation of ResCVA with quantitative metrics**: Remove the residual connection and measure LPIPS and FID degradation to confirm local detail preservation is specifically due to the residual design.
2. **Cross-view masking robustness test**: Evaluate identity preservation and generation quality when varying input view count (1, 2, 3 views) at inference to validate the masking strategy effectiveness.
3. **Compressed text attribute preservation**: Compare garment attribute consistency between full-length descriptions and compressed keyword versions using attribute-based evaluation metrics.