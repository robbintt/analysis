---
ver: rpa2
title: Can we repurpose multiple-choice question-answering models to rerank retrieved
  documents?
arxiv_id: '2504.06276'
source_url: https://arxiv.org/abs/2504.06276
tags:
- mcqa
- retrieval
- reranking
- document
- cross-encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that multiple-choice question-answering
  (MCQA) models can be effectively repurposed for document reranking in retrieval-augmented
  generation systems. The author developed R, a lightweight prototype model that harmonizes
  MCQA decision-making with cross-encoder semantic relevance assessments.
---

# Can we repurpose multiple-choice question-answering models to rerank retrieved documents?

## Quick Facts
- arXiv ID: 2504.06276
- Source URL: https://arxiv.org/abs/2504.06276
- Reference count: 4
- Multiple-choice QA models can be repurposed for document reranking, achieving competitive performance with cross-encoders

## Executive Summary
This study demonstrates that multiple-choice question-answering (MCQA) models can be effectively repurposed for document reranking in retrieval-augmented generation systems. The author developed R*, a lightweight prototype model that harmonizes MCQA decision-making with cross-encoder semantic relevance assessments. R* was trained on the MS MARCO dataset and achieved the highest Recall@1 (0.2315) and MRR@10 (0.3019) scores among compared models, outperforming baselines and showing that MCQA approaches can approximate cross-encoder effectiveness.

## Method Summary
The method involves fine-tuning a RoBERTa-based model using sentence-transformers CrossEncoder with binary cross-entropy loss on 5M query-passage pairs (2.5M positive, 2.5M negative) from MS MARCO. The model employs hard-negative sampling and is trained for 7 epochs with batch size 2048 on V100 GPU. R* uses sigmoid-normalized relevance scores [0,1] and can operate in both MCQA and cross-encoder interfaces. BM25 retrieves top-10 candidates for reranking, with evaluation using Recall@k, MRR@10, and ROUGE-L metrics.

## Key Results
- R* achieves Recall@1=0.2315 and MRR@10=0.3019 on MS MARCO validation
- Outperforms baseline models including BGE M3 (2.3GB) despite being much smaller (112MB)
- MCQA reranking performance closely approximates cross-encoder effectiveness
- Model size alone does not guarantee superior performance for this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MCQA probabilistic frameworks can approximate cross-encoder relevance scoring for document reranking.
- Mechanism: The softmax-based answer selection in MCQA (P(a|q) = exp(score(q,a)) / Σexp(score(q,a'))) is structurally similar to cross-encoder relevance scoring R(q,d) = σ(w^T·Enc(q,d) + b). When trained with binary cross-entropy loss with built-in sigmoid, the model learns to output probabilities directly comparable between both paradigms.
- Core assumption: The sigmoid in BCE loss enables equivalence between document probability P(d|q) and relevance score R(q,d).
- Evidence anchors: Equations (1), (2), and approximation (4) formalize the mathematical bridge; "The performance between the MCQA reranker versions of our models and their cross-encoder counterparts is remarkably close."

### Mechanism 2
- Claim: Balanced training with hard-negative sampling improves discrimination between relevant and marginally irrelevant passages.
- Mechanism: Training on 2.5M positive-negative pairs with hard-negative sampling exposes the model to semantically similar but irrelevant candidates, forcing it to learn finer-grained distinctions rather than surface-level heuristics.
- Core assumption: Hard negatives provide sufficient signal for the model to learn nuanced relevance boundaries.
- Evidence anchors: "The author employs a hard-negative sampling strategy" with loss function (5) optimizing discrimination; training on balanced 5M query-passage pairs.

### Mechanism 3
- Claim: Architecture and training specificity matter more than raw model size for reranking effectiveness.
- Mechanism: Smaller models (R* at 112MB, MiniLM at 90.9MB) with appropriate pretraining objectives (RoBERTa's masked language modeling) and task-specific fine-tuning outperform larger models (BGE M3 at 2.3GB, ELECTRA base at 438MB) when the larger models' objectives misalign with reranking.
- Core assumption: Pretraining objectives and fine-tuning procedures transfer more effectively than parameter count alone.
- Evidence anchors: R* and MiniLM outperform BGE M3 and ELECTRA base across Recall@1 and MRR@10; "Model size alone does not guarantee superior performance for this task."

## Foundational Learning

- Concept: Cross-encoder joint encoding
  - Why needed here: R* relies on jointly encoding query-document pairs to capture semantic interactions, unlike bi-encoders that embed separately. Understanding this distinction explains why the model requires pairwise inference at rerank time.
  - Quick check question: Given a query and 10 candidate documents, how many forward passes does a cross-encoder require compared to a bi-encoder?

- Concept: Binary cross-entropy with logits
  - Why needed here: The paper uses BCEWithLogitsLoss, which internally applies sigmoid—critical for understanding why MCQA models can output relevance scores without manual probability transformation.
  - Quick check question: Why does BCEWithLogitsLoss negate the need for a manual sigmoid application before loss computation?

- Concept: Hard-negative sampling
  - Why needed here: The model's discrimination capability depends on being exposed to challenging negatives during training, not random irrelevant documents.
  - Quick check question: What makes a negative "hard" versus "easy," and why would easy negatives provide weaker training signal?

## Architecture Onboarding

- Component map: MS MARCO dataset -> Balanced positive/negative pairs -> Hard-negative sampling -> BCEWithLogitsLoss -> RoBERTa-based transformer -> MCQA/cross-encoder inference -> BM25 retrieval + reranking -> Recall@k, MRR@10, ROUGE-L evaluation

- Critical path: Load pretrained RoBERTa weights -> Prepare MS MARCO-format data with hard negatives (50/50 balanced) -> Fine-tune with BCEWithLogitsLoss for 7 epochs (batch size 2048, V100 16GB) -> Validate on held-out queries using BM25 first-stage retrieval -> Deploy via CrossEncoder or AutoModelForMultipleChoice interface

- Design tradeoffs:
  - MCQA interface vs. cross-encoder interface: Near-identical performance, but framework differences may cause minor score variations due to tokenization/handling nuances
  - Model size vs. inference speed: TinyBERT L2 v2 (17.5MB) sacrifices Recall@1 (0.1995) for 6x smaller footprint vs. R* (112MB)
  - Precision vs. breadth: R* excels at Recall@1; MiniLM L6 v2 better at Recall@5—choose based on whether top-1 accuracy or candidate pool coverage matters more

- Failure signatures:
  - Length bias: R* favors longer passages—preprocess to normalize lengths
  - Domain drift: Performance degrades outside MS MARCO distribution—recommend domain-specific fine-tuning
  - Token-level tasks: Not suited for word/phrase similarity; passage-level only
  - ELECTRA/All-MPNet underperformance: Likely objective mismatch—avoid simply swapping base models without reranking-specific fine-tuning

- First 3 experiments:
  1. Reproduce R* baseline on MS MARCO validation set to verify Recall@1 ≈ 0.23 and MRR@10 ≈ 0.30
  2. Ablate hard-negative sampling by training with random negatives—expect degraded discrimination (hypothesis: lower Recall@1)
  3. Test MCQA vs. cross-encoder interfaces on the same model checkpoint to quantify framework-induced variance

## Open Questions the Paper Calls Out

- Can preprocessing text to normalize passage lengths eliminate the length bias in R* reranking scores?
- How does MCQA-based reranking generalize to domain-specific datasets beyond MS MARCO?
- How do MCQA rerankers compare against commercially-available reranking models?
- Can the MCQA reranking approach be extended to word- or phrase-level similarity tasks?

## Limitations

- Model initialization and hyperparameters are unspecified, making exact reproduction difficult
- Hard-negative sampling implementation details are not provided
- Domain generalization is not systematically evaluated beyond MS MARCO and three additional datasets
- Commercial reranking models were unavailable for comparison during experimentation

## Confidence

- High Confidence: R* achieves Recall@1=0.2315 and MRR@10=0.3019 on MS MARCO validation, outperforming baseline models
- Medium Confidence: The theoretical mechanism linking MCQA probabilistic frameworks to cross-encoder relevance scoring is mathematically sound but not rigorously proven
- Low Confidence: The assertion that hard-negative sampling is the primary driver of discrimination capability lacks ablation studies

## Next Checks

1. Ablation of Hard-Negative Sampling: Train R* with random negative sampling instead of hard negatives while keeping all other hyperparameters constant. Compare Recall@1 and MRR@10 to the original R* to quantify the contribution of hard-negative mining to performance gains.

2. Framework Interface Comparison: Evaluate the exact same R* model checkpoint using both Huggingface's AutoModelForMultipleChoice and sentence-transformers' CrossEncoder interfaces. Measure the variance in scores and metrics to determine if framework differences introduce systematic bias or normalization differences.

3. Cross-Domain Robustness Test: Fine-tune R* on a domain-specific dataset (e.g., biomedical or legal documents) and evaluate on both in-domain and MS MARCO test sets. Measure performance degradation to quantify domain adaptation requirements and assess whether the model's effectiveness transfers beyond its training distribution.