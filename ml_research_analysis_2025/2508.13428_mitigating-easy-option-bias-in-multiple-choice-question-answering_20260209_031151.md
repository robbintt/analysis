---
ver: rpa2
title: Mitigating Easy Option Bias in Multiple-Choice Question Answering
arxiv_id: '2508.13428'
source_url: https://arxiv.org/abs/2508.13428
tags:
- options
- question
- negative
- bias
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies an Easy-Options Bias (EOB) in multiple-choice
  VQA benchmarks, where VLMs can often answer correctly without seeing the question
  by exploiting visual-text similarities between the correct answer and visual content.
  The authors attribute this to a visual relevance imbalance, where correct answers
  align more closely with image/video content than distractors in feature space.
---

# Mitigating Easy Option Bias in Multiple-Choice Question Answering

## Quick Facts
- arXiv ID: 2508.13428
- Source URL: https://arxiv.org/abs/2508.13428
- Reference count: 40
- Primary result: GroundAttack reduces VLM accuracy under (V,O) settings to near random guessing, mitigating Easy-Options Bias in multiple-choice VQA

## Executive Summary
This paper identifies Easy-Options Bias (EOB) in multiple-choice visual question answering benchmarks, where vision-language models can answer correctly without seeing the question by exploiting visual-text similarities between correct answers and visual content. The authors attribute this to visual relevance imbalance, where correct answers align more closely with image/video content than distractors in feature space. To address this, they introduce GroundAttack, a toolkit that automatically generates visually grounded, diverse, and challenging negative options using a captioner, distractor, and selector pipeline. Experiments on NExT-QA and MMStar show that GroundAttack significantly reduces VLM accuracy under (V,O) settings, bringing performance close to random guessing, and lowers accuracy under (V,Q,O) settings, demonstrating more robust evaluation.

## Method Summary
GroundAttack is a three-agent pipeline that generates adversarial negative options for multiple-choice VQA benchmarks. The captioner (Qwen2.5VL-7B) converts visual input into textual descriptions. The distractor (DeepSeek-V3) generates 128 candidate negatives from the question, correct answer, and visual description. The selector (SigLIP-so400m) uses K-means clustering on text features followed by CLIP similarity selection to choose the most visually similar option from each cluster. This produces four adversarial negatives that are both visually grounded and semantically diverse. The pipeline costs approximately $8.75 to process 1.3×10⁷ tokens via DeepSeek API and runs on a single NVIDIA A100 81GB GPU.

## Key Results
- GroundAttack reduces Qwen2.5VL-7B accuracy on original NExT-QA from 79.56% to 50.36% under (V,O) setting, approaching random baseline
- EOB proportion decreases from 89.0% to 67.7% on NExT-QA, closer to theoretical random expectation of 67%
- VLM(V,Q,O) accuracy drops from 93.24% to 44.52% on NExT-QA, showing more robust evaluation
- GroundAttack outperforms simpler methods like Random sampling and CLIP-only selection in mitigating EOB

## Why This Works (Mechanism)

### Mechanism 1: Visual Relevance Imbalance Detection
VLMs can select correct answers without the question because correct options have systematically higher visual-text alignment than distractors. The model performs vision-option similarity matching, where correct answers align more closely with visual content in feature space, creating an exploitable shortcut. CLIP similarity scores reveal this imbalance—correct answers consistently rank higher than negatives even without the question. If distractors were generated to have equal or higher visual grounding than correct answers, VLM(V,O) performance should approach random chance (~20% for 5 options).

### Mechanism 2: GroundAttack's Clustering+CLIP Selection
K-means clusters 128 candidate negatives by text features; CLIP then picks the top-1 visually similar option per cluster. This ensures selected negatives are: (1) confusing (high visual similarity), (2) diverse (different semantic clusters), and (3) representative of the candidate pool. If negatives are too similar to correct answer, they may become indistinguishable even with the question (V,Q,O) → excessive difficulty.

### Mechanism 3: Three-Agent Pipeline Factorization
Separating captioning, distractor generation, and selection into specialized agents reduces human effort while maintaining quality. Each agent handles a distinct subtask: visual grounding, semantic plausibility, and adversarial selection. Off-the-shelf foundation models substitute for specialized fine-tuned components. If any agent fails (e.g., captioner misses key visual details), downstream agents propagate errors.

## Foundational Learning

- Concept: **Shortcut Learning in VQA**
  - Why needed here: EOB is a form of shortcut learning—models exploit dataset artifacts rather than reasoning. Understanding prior bias types (language bias, texture bias) contextualizes EOB as a new modality-specific shortcut.
  - Quick check question: Can you explain why "white" being a frequent answer to color questions is a language bias, versus EOB being a visual-relevance bias?

- Concept: **CLIP Vision-Language Alignment**
  - Why needed here: CLIP similarity serves as the proxy for detecting visual relevance imbalance and selecting adversarial negatives. Understanding contrastive pre-training explains why CLIP captures vision-text alignment.
  - Quick check question: Why does CLIP enable zero-shot selection of visually-grounded distractors without fine-tuning?

- Concept: **Out-of-Distribution (OOD) vs. Identical Distribution (IID) Evaluation**
  - Why needed here: Prior debiasing efforts (VQA-CP, VQA-CE) focused on OOD splits. EOB persists because VLMs are trained on web-scale data, making IID/OOD boundaries unclear for proprietary models.
  - Quick check question: Why can't traditional OOD testing detect EOB in VLMs trained on unknown web data?

## Architecture Onboarding

- Component map: Visual input → Captioner (Qwen2.5VL-7B) → Distractor (DeepSeek-V3) → Selector (SigLIP-so400m) → Adversarial negatives
- Critical path: Visual input → Captioner → Distractor → Selector → Adversarial negatives. Failure at any stage propagates.
- Design tradeoffs: Random sampling: Fast but ineffective (Table 2: 65.57% vs. 50.36% for GroundAttack on NExT-QA). CLIP-only selector: Effective but less diverse than Clustering+CLIP. N=128 candidates: Higher coverage but more API cost. m=4 negatives: Standard 5-option MCQ format.
- Failure signatures: VLM(V,O) >> 20%: EOB not mitigated; negatives insufficiently grounded. VLM(V,Q,O) ~ 20%: Negatives too similar to correct answer; task becomes impossible. High per-category variance: Some question types (e.g., descriptive) may resist adversarial generation.
- First 3 experiments: 1) Baseline EOB detection: Run VLM(V,O) on original NExT-QA/MMStar. Compute CLIP similarity for correct vs. distractor options. Confirm imbalance. 2) Ablate selector strategies: Compare Random vs. CLIP-only vs. Clustering+CLIP on VLM(V,O) and VLM(V,Q,O) accuracy. Target: VLM(V,O) → ~20%, VLM(V,Q,O) → non-saturated (40-60%). 3) Cross-benchmark validation: Apply GroundAttack to held-out benchmarks (e.g., SEED-Bench, Video-MME). Measure whether EOB reduction generalizes or is dataset-specific.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can fine-tuning VLMs on GroundAttack-augmented datasets improve their robustness and generalization to out-of-distribution (OOD) tasks?
  - Basis in paper: [explicit] The authors state they "did not test whether models trained on GroundAttack-augmented data generalize better or improve robustness across tasks."
  - Why unresolved: The paper focuses exclusively on using GroundAttack for evaluation rather than as a data augmentation strategy for training.
  - Evidence to resolve it: Fine-tune VLMs on datasets augmented with GroundAttack negatives and evaluate their performance on standard OOD benchmarks compared to baselines trained on original data.

- **Open Question 2**: How do human experts assess the quality, plausibility, and difficulty of GroundAttack-generated distractors compared to human-authored options?
  - Basis in paper: [explicit] The authors admit they "have not conducted human studies to assess the quality, plausibility, or diversity of the generated distractors, relying instead on manual inspection."
  - Why unresolved: While automated metrics show reduced bias, it remains unverified if the generated negatives are semantically valid or if they introduce unwanted artifacts that confuse humans.
  - Evidence to resolve it: Conduct a human evaluation where annotators rate the "visual plausibility" and "question relevance" of GroundAttack negatives versus original negatives without knowing the source.

- **Open Question 3**: How does GroundAttack's effectiveness compare to alternative distractor generation methods, such as perturbation-based or contrastive approaches?
  - Basis in paper: [explicit] The authors note they "did not compare our approach with alternative distractor generation methods (e.g., adversarial, contrastive, or perturbation-based techniques)."
  - Why unresolved: It is unclear if the proposed agent-based pipeline is superior to or more cost-efficient than existing algorithms for creating hard negatives.
  - Evidence to resolve it: A comparative analysis measuring the reduction in VLM accuracy (EOB mitigation) and computational cost between GroundAttack and baselines like DisGeM or counterfactual contrastive decoding.

- **Open Question 4**: To what extent are VLMs' susceptibility to EOB rooted in pre-training data distribution versus architectural vision-language fusion mechanisms?
  - Basis in paper: [inferred] The authors list "understanding the root causes of EOB, such as dataset artifacts, training dynamics, or modality interplay" as a limitation requiring further study.
  - Why unresolved: The paper empirically demonstrates the existence of the bias but does not isolate whether the "visual relevance imbalance" is learned from web-scale data or is an inherent failure mode of current attention mechanisms.
  - Evidence to resolve it: Perform ablation studies on models trained from scratch with controlled negative option distributions, or analyze attention maps to see if models focus on question-relevant regions when EOB is present.

## Limitations
- The core claim relies on assumptions about VLMs exploiting visual-text alignment shortcuts that cannot be definitively proven without model internals
- GroundAttack depends heavily on the quality of frozen foundation models—failures propagate through the pipeline
- The paper lacks error analysis by question type, leaving uncertainty about whether EOB mitigation generalizes across all question categories
- Human evaluation of distractor quality was not conducted, relying instead on manual inspection

## Confidence

- **High confidence**: CLIP similarity scores consistently show correct answers have higher visual alignment than distractors in original benchmarks; GroundAttack significantly reduces VLM(V,O) accuracy toward random baseline.
- **Medium confidence**: The three-agent pipeline factorization improves efficiency without sacrificing quality; K-means clustering with CLIP selection produces more diverse and challenging negatives than simpler methods.
- **Low confidence**: EOB is truly a new bias category versus an extension of existing positional or language biases; GroundAttack's effectiveness generalizes beyond NExT-QA and MMStar to other VQA benchmarks.

## Next Checks

1. **Error Type Analysis**: Manually categorize failed samples from GroundAttack to determine if failures stem from captioner omissions, distractor generation issues, or selector limitations. This would reveal whether EOB mitigation is bottlenecked by upstream components.

2. **Cross-Benchmark Generalization**: Apply GroundAttack to held-out benchmarks (e.g., SEED-Bench, Video-MME) and measure whether EOB reduction patterns replicate. This tests whether the bias is dataset-specific or represents a fundamental VLM vulnerability.

3. **Human Evaluation of Distractor Quality**: Have human annotators rate GroundAttack-generated negatives on visual grounding, plausibility, and difficulty. Compare these ratings to original distractors to verify that the adversarial selection process maintains semantic quality while increasing challenge.