---
ver: rpa2
title: 'SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation'
arxiv_id: '2507.13381'
source_url: https://arxiv.org/abs/2507.13381
tags:
- graph
- saft
- bleu
- latexit
- pointer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SAFT introduces a lightweight method to fine-tune LLMs for structured\
  \ data by injecting graph-based positional encodings derived from the magnetic Laplacian\
  \ of AMR graphs. It enables structure-aware generation without architectural changes,\
  \ simply projecting magnetic-Laplacian eigenvectors into the LLM\u2019s embedding\
  \ space during fine-tuning."
---

# SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation

## Quick Facts
- arXiv ID: 2507.13381
- Source URL: https://arxiv.org/abs/2507.13381
- Authors: Rafiq Kamel; Filippo Guerranti; Simon Geisler; Stephan GÃ¼nnemann
- Reference count: 40
- Primary result: Achieves state-of-the-art BLEU performance on AMR 3.0, improving by 3.5 over baselines

## Executive Summary
SAFT introduces a lightweight method to fine-tune LLMs for structured data by injecting graph-based positional encodings derived from the magnetic Laplacian of AMR graphs. It enables structure-aware generation without architectural changes, simply projecting magnetic-Laplacian eigenvectors into the LLM's embedding space during fine-tuning. On AMR 3.0, SAFT achieves state-of-the-art performance, improving BLEU by 3.5 over baselines and showing larger gains on deeper, more complex graphs and document-level AMRs.

## Method Summary
SAFT computes the Magnetic Laplacian eigenvectors of AMR graphs to capture edge directionality through complex phase shifts, then projects these spectral coordinates into the LLM embedding space via a lightweight MLP. The method transforms AMRs into Semantically-Preserving Graphs (SPGs), extracts the lowest $k=30$ eigenvectors, concatenates them with token-level sinusoidal encodings, and adds the projected result to input embeddings before the first transformer layer. Fine-tuning uses LoRA adapters on LLaMA 3.2 or Qwen 2.5 models with separate learning rates for the projection MLP.

## Key Results
- Achieves SOTA BLEU performance on AMR 3.0, improving by 3.5 over baselines
- Gains scale with graph complexity, showing +1.1 to +4.4 BLEU improvement on deep graphs
- Outperforms standard fine-tuning across model scales (1B-3B parameters)
- Demonstrates consistent performance on document-level AMRs

## Why This Works (Mechanism)

### Mechanism 1
Injecting spectral topology restores directional information typically lost during graph linearization. Standard linearization flattens AMRs into sequences. SAFT computes the Magnetic Laplacian eigenvectors of the graph, which encode edge directionality via complex phase shifts. These are projected into the embedding space, providing the decoder with explicit structural coordinates. The pre-trained embedding space contains unused or "safe" dimensions for structural signal injection.

### Mechanism 2
The projection strategy augments semantic representations rather than overwriting them. A lightweight MLP projects graph encodings (AMRPE) into the LLM space. Geometric analysis shows these encodings are added in directions orthogonal to the text embeddings ($\cos(X, \text{AMRPE}) \approx 0$), potentially increasing intrinsic dimensionality without suppressing semantic content. This additive representation $H = X + \text{AMRPE}$ enriches the embedding space.

### Mechanism 3
Structural inductive bias scales with graph complexity. As AMR depth increases, standard attention mechanisms struggle with long-range dependencies. The fixed spectral coordinates provided by AMRPEs offer a stable reference frame for these deep structures, yielding higher relative gains on complex inputs. The "curse of depth" in AMRs is better solved by global topology awareness than by deeper transformer layers alone.

## Foundational Learning

### Graph Laplacians & Spectral Theory
Why needed here: To understand how the "Magnetic Laplacian" converts directed graph connectivity into differentiable positional vectors.
Quick check question: How does the phase parameter $q$ in the Magnetic Laplacian differ from standard Laplacian frequency terms?

### Abstract Meaning Representation (AMR)
Why needed here: To understand the "Semantic-Preserving Graph" transformation (edge-to-node conversion) required before applying spectral methods.
Quick check question: Why must edge labels (e.g., `:ARG0`) be converted into nodes (reification) for the Laplacian calculation?

### Positional Encodings (PE) in Transformers
Why needed here: To distinguish between the model's internal RoPE (relative) and SAFT's external AMRPE (absolute/structural).
Quick check question: Why does SAFT add AMRPEs to the input embeddings rather than replacing the model's native positional encodings?

## Architecture Onboarding
- **Component map:** AMR Graph -> Preprocessor (BFS Linearize + SPG Transform) -> Solver (Magnetic Laplacian EVD) -> Projector (MLP $f_\theta$) -> LLM Backbone (Frozen LLM + LoRA Adapters)
- **Critical path:** The alignment between the SPG node indices and the tokenized sequence is the most fragile component; a mismatch here invalidates the PE injection
- **Design tradeoffs:** Dense EVD is faster for small graphs ($n < 2000$); sparse solvers are required for larger documents (DocAMR). A 2-layer MLP provides sufficient non-linearity to align spectral and semantic spaces without overfitting
- **Failure signatures:** Misalignment causes token-level BLEU variance spikes; Dimensionality Collapse occurs if MLP learning rate is too high, causing dot-product similarity between distinct graph nodes to exceed 0.9
- **First 3 experiments:** Sanity Check (Random PE) - replace Magnetic Laplacian eigenvectors with Gaussian noise; Ablation (Directionality) - set magnetic parameter $q=0$ (standard Laplacian); Complexity Stress Test - evaluate on DocAMR subset

## Open Questions the Paper Calls Out
- Whether SAFT's effectiveness generalizes to other graph-to-text generation tasks beyond AMR, such as knowledge graph-to-text or semantic role graph-to-text (Appendix C.6)
- Whether SAFT preserves the LLM's general capabilities on tasks unrelated to graph processing (Section 4.6)
- Whether the benefits of SAFT diminish at model scales beyond 3B parameters (Appendix C.2)

## Limitations
- Graph construction fragility: SPG transformation assumptions lack ablation verification
- Inductive bias scalability: Performance beyond 3B parameters untested
- Reproducibility constraints: MLP initialization unspecified, alignment precision required

## Confidence
- Structure-aware PE improves BLEU by 3.5 over baselines: High confidence
- Gains scale with graph complexity: Medium confidence
- AMRPE vectors are geometrically orthogonal to semantic embeddings: Medium confidence
- Magnetic Laplacian encoding provides directional information: Low confidence

## Next Checks
1. Ablation of magnetic parameter: Set $q=0$ (standard Laplacian) and retrain to isolate the contribution of edge directionality versus topology alone
2. Gradient flow analysis: Monitor MLP gradients versus LoRA gradients during training to confirm the projection MLP learns stable alignment without disrupting semantic representations
3. Scaling experiment: Fine-tune SAFT on LLaMA 7B or 13B to test whether the additive projection strategy maintains performance benefits at larger scales