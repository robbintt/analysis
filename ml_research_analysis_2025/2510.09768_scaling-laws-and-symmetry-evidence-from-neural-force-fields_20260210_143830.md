---
ver: rpa2
title: Scaling Laws and Symmetry, Evidence from Neural Force Fields
arxiv_id: '2510.09768'
source_url: https://arxiv.org/abs/2510.09768
tags:
- scaling
- neural
- learning
- equivariant
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the scaling laws of neural interatomic
  potentials (NNIPs) by examining how equivariance and model complexity affect performance
  as model size, data, and compute scale. We systematically compare unconstrained
  MPNNs, lower-order equivariant architectures (GemNet-OC, EGNN), and higher-order
  equivariant models (eSEN) across diverse scales.
---

# Scaling Laws and Symmetry, Evidence from Neural Force Fields

## Quick Facts
- **arXiv ID:** 2510.09768
- **Source URL:** https://arxiv.org/abs/2510.09768
- **Reference count:** 40
- **Primary result:** Equivariant neural network architectures consistently outperform unconstrained models at larger scales, with higher-order equivariance showing steeper scaling exponents

## Executive Summary
This study investigates scaling laws of neural interatomic potentials by comparing unconstrained message passing neural networks (MPNNs) with equivariant architectures (GemNet-OC, EGNN, eSEN) across systematically varied model sizes, datasets, and compute budgets. The authors find that equivariant models maintain performance advantages as scale increases, challenging the assumption that symmetry inductive biases become unnecessary at large scale. Compute-optimal scaling requires balanced growth of model and dataset size, with symmetry loss providing minimal benefits beyond equivariant architecture.

## Method Summary
The authors conduct a systematic scaling study using the OpenMol (OMol25) neutral-molecule dataset with 34M training samples and 27K validation samples. Four architectures are compared: unconstrained MPNN, lower-order equivariant GemNet-OC and MC-EGNN, and higher-order equivariant eSEN. All models are trained in a scheduler-free single-epoch regime using AdamW optimizer with transferred learning rates. Width scaling is varied systematically while maintaining depth at saturation levels (12-16 layers for equivariant models). Graph construction uses 6Å cutoffs for most models and 10Å for GemNet-OC. Validation loss is tracked against PFLOPs and GPU-hours to establish scaling relationships.

## Key Results
- Equivariant architectures consistently outperform unconstrained MPNNs at larger scales
- Higher-order equivariant models (eSEN) exhibit steeper scaling exponents (0.76) compared to lower-order equivariant (0.64) and unconstrained models (0.49)
- Compute-optimal scaling requires balanced growth of model and dataset size, not just increasing compute
- Symmetry loss regularization provides minimal benefits beyond equivariant architecture alone

## Why This Works (Mechanism)
The study demonstrates that geometric symmetry constraints fundamentally alter the scaling landscape of neural force field learning. By enforcing rotational and translational invariance through equivariant architectures, models can better capture the inherent symmetries of physical systems, leading to more efficient learning at scale. The higher-order equivariant models benefit more from scale because they can represent more complex geometric relationships while maintaining symmetry constraints.

## Foundational Learning
- **Scaling Laws:** Understanding how performance varies with model size, data, and compute; needed to optimize resource allocation in ML development
- **Symmetry Equivariance:** Mathematical frameworks ensuring physical predictions respect rotational/translational symmetries; critical for physical consistency in force field predictions
- **Message Passing Neural Networks:** Graph neural network architecture for processing point cloud data; fundamental for representing atomistic systems
- **Scheduler-Free Optimization:** Training regime without learning rate scheduling; simplifies scaling analysis by removing schedule-dependent variables
- **Compute-Optimal Scaling:** Balancing model and dataset growth for efficient resource utilization; essential for cost-effective model development
- **Geometric Deep Learning:** Applying deep learning to non-Euclidean data with geometric structure; necessary for modeling molecular systems

## Architecture Onboarding

**Component Map:** Graph Construction → Message Passing Layers → Output Projection → Loss Computation

**Critical Path:** Input point cloud → Neighbor graph construction (6-10Å cutoff) → Equivariant message passing (varies by architecture) → Energy/force prediction → Combined loss

**Design Tradeoffs:** Unconstrained MPNNs offer flexibility but poor scaling; equivariant models enforce physical constraints with better scaling; higher-order equivariance provides steeper scaling but increased complexity

**Failure Signatures:** GemNet-OC shows high variance without explicit normalization; EGNN suffers gradient instability without proper variance scaling; unconstrained models plateau as scale increases

**Three First Experiments:**
1. Implement baseline MPNN and verify scaling trends on small subset
2. Add equivariant constraints to MPNN and compare scaling behavior
3. Test variance scaling (1/E) for EGNN vector channels to stabilize training

## Open Questions the Paper Calls Out
None

## Limitations
- Single-epoch training regime may not capture full model potential
- Different neighbor cutoffs (6Å vs 10Å) introduce systematic input feature differences
- Focus on neutral molecules limits generalizability to charged or periodic systems
- Computational constraints limited exploration of depth vs width trade-offs

## Confidence
- **High Confidence:** Equivariant models outperform unconstrained at scale; balanced scaling is compute-optimal
- **Medium Confidence:** Specific scaling exponents depend on training regime and architectural choices
- **Low Confidence:** Minimal benefit of symmetry loss beyond equivariant architecture, limited testing

## Next Checks
1. Extend training duration beyond one epoch to verify scaling trends persist
2. Retrain GemNet-OC with 6Å cutoff to isolate architectural vs input feature effects
3. Test symmetry loss regularization on EGNN and eSEN across multiple width scales