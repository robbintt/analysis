---
ver: rpa2
title: 'Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers'
arxiv_id: '2510.13444'
source_url: https://arxiv.org/abs/2510.13444
tags:
- basis
- monomials
- polynomial
- matrices
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces the first learning-augmented algorithm for\
  \ Sum of Squares (SOS) programming that predicts compact monomial bases using a\
  \ Transformer model, achieving over 100\xD7 speedups compared to state-of-the-art\
  \ solvers. The method generates polynomials with known SOS decompositions for training,\
  \ uses a repair mechanism to handle incorrect predictions while maintaining correctness\
  \ guarantees, and achieves significant computational improvements across sparse,\
  \ block-diagonal, low-rank, and dense polynomial structures."
---

# Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers

## Quick Facts
- **arXiv ID:** 2510.13444
- **Source URL:** https://arxiv.org/abs/2510.13444
- **Reference count:** 40
- **Primary result:** First learning-augmented algorithm for Sum of Squares (SOS) programming using Transformers, achieving over 100× speedups by predicting compact monomial bases.

## Executive Summary
This paper introduces the first learning-augmented algorithm for Sum of Squares (SOS) programming that uses a Transformer model to predict compact monomial bases for polynomial nonnegativity certification. By learning structural patterns in polynomials, the method reduces SDP matrix dimensions from O(m²) to near-minimal O(m*²), achieving over 100× speedups on sparse and block-diagonal polynomials. The approach includes a repair mechanism that guarantees correctness even when predictions fail, and theoretical analysis shows worst-case computational cost exceeds the standard baseline by at most a constant factor.

## Method Summary
The method trains a Transformer encoder-decoder to predict monomial bases for SOS decompositions. Training data is generated via reverse sampling: construct p(x) = z_B(x)^T Q z_B(x) with known compact bases B and PSD matrix Q. At inference, the Transformer predicts an initial basis B₀, which undergoes COVERAGE_REPAIR to ensure necessary conditions. If the resulting SDP is infeasible, an ordered expansion algorithm iteratively grows the basis using permutation-based scoring until solving succeeds or candidates are exhausted. The approach is analyzed within the Algorithms with Predictions framework, guaranteeing worst-case bounds.

## Key Results
- Achieves over 100× speedups on sparse and block-diagonal polynomials by reducing basis sizes from 73 to 56 monomials
- Repair mechanism successfully handles incorrect predictions while maintaining correctness guarantees
- Theoretical worst-case analysis shows computational cost bounded by constant factor over standard approach
- Performance degrades gracefully on dense and low-rank polynomials, matching baseline when no speedup is possible

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformer-based basis prediction reduces SDP dimensionality by identifying near-minimal monomial sets that traditional methods miss.
- **Mechanism:** The model takes tokenized polynomial input and autoregressively generates monomial sequences. It learns structural patterns (sparsity, block-diagonality, low-rank) that correlate with compact SOS representations. By predicting smaller bases, SDP matrix dimension drops from O(m²) toward O(m*²) where m* is the minimal basis size.
- **Core assumption:** Polynomial structure exhibits learnable patterns that correlate with SOS decomposability.
- **Evidence anchors:**
  - [abstract] "train a Transformer model that predicts an almost-minimal monomial basis for a given polynomial, thereby drastically reducing the size of the corresponding SDP"
  - [section 3.1] Reverse sampling generates training pairs by constructing p(x) = z_B(x)^T Q z_B(x) with known compact bases
  - [corpus] Related work on transformers for permutation invariant polynomials suggests architectural compatibility, but direct corpus support for SOS-specific prediction is limited.
- **Break condition:** Dense polynomials with no exploitable sparsity patterns; cross-terms obscure structural signals.

### Mechanism 2
- **Claim:** The repair mechanism guarantees correctness by enforcing coverage constraints and iteratively expanding toward feasibility.
- **Mechanism:** Two-stage fallback: (1) COVERAGE_REPAIR enforces Lemma 1's necessary condition S(p) ⊆ B·B via greedy monomial addition; (2) Ordered expansion (Algorithm 2) scores candidates via permutation-based frequency and grows basis geometrically until SDP solves or candidates exhausted.
- **Core assumption:** Failed SDP solves are detected reliably; infeasibility implies missing basis elements (not numerical issues).
- **Evidence anchors:**
  - [section 3.2] Lemma 1 proves any valid SOS basis must satisfy coverage property
  - [section 3.3] Permutation-based scoring SCORE(u,p) exploits Transformer non-equivariance to rank candidates
  - [corpus] No direct corpus validation of repair mechanisms for SOS; this appears novel.
- **Break condition:** Strong numerical instability in SDP solver; false infeasibility signals from solver tolerance issues.

### Mechanism 3
- **Claim:** Theoretical guarantees ensure worst-case bounded degradation even when predictions are adversarially bad.
- **Mechanism:** Within the "Algorithms with Predictions" framework, geometric expansion (ρ > 1) ensures the competitive ratio is O(k) where k is the number of expansion steps. Since k = O(log(η/m₁)), total cost is Θ(ρ^ω/(1-ρ^{-ω}) η^ω), bounded by constant factor over standard approach.
- **Core assumption:** SDP cost scales as Θ(m^ω) primarily through basis size, not constraint count.
- **Evidence anchors:**
  - [section 5, Lemma 3] "Algorithm 2 performs at most 1 + ⌈log_ρ(η/m₁)⌉ SDP solves and has total cost at most Θ(ρ^ω/(1-ρ^{-ω}) η^ω)"
  - [section 5, Remark 2] Best case: single SDP with minimal basis; worst case: O(k) factor
  - [corpus] Algorithms-with-predictions framework cited (Roughgarden, 2021) but no direct corpus validation for SOS domain.
- **Break condition:** When η ≈ |½N(p)| (coverage rank near full), performance matches baseline—no speedup possible.

## Foundational Learning

- **Concept: Sum of Squares (SOS) decomposition**
  - **Why needed here:** The entire paper targets certifying polynomial nonnegativity through SOS; without this, SDP formulation makes no sense.
  - **Quick check question:** Given p(x) = 4x₁⁴ + 12x₁²x₂² + 9x₂⁴ + 1, can you express it as z_B(x)^T Q z_B(x) for some basis B and PSD matrix Q?

- **Concept: Semidefinite Programming (SDP)**
  - **Why needed here:** SOS certification reduces to solving an SDP whose dimension grows quadratically with basis size—the computational bottleneck this paper addresses.
  - **Quick check question:** Why does reducing basis size from m to m* yield more than linear speedup in SDP solve time?

- **Concept: Newton polytope and half Newton polytope**
  - **Why needed here:** These geometric structures bound the candidate monomial set (½N(p)) and provide necessary conditions for valid bases.
  - **Quick check question:** If monomial x³y is in the polynomial's support, what degree constraint does the half Newton polytope impose on potential basis elements?

## Architecture Onboarding

- **Component map:**
  Polynomial input → Tokenizer (Kera et al. scheme) → Transformer encoder (6 layers, 512 dim) → Autoregressive decoder → Predicted basis B₀ → COVERAGE_REPAIR → B_cov → SDP solver (MOSEK/SCS) → If infeasible: SCORE + Ordered Expansion → Loop

- **Critical path:** Transformer inference → Coverage repair → First SDP attempt. This path determines whether you get 100× speedup or require iterative fallback.

- **Design tradeoffs:**
  - More permutations (L) in scoring improves success rate but linearly increases inference overhead; L=4–8 is optimal (Section D.2)
  - Larger expansion factor ρ reduces iterations but risks overshooting minimal basis; ρ≤1.4 slightly preferred (Section D.3)
  - Greedy repair is conservative (fewer false positives); permutation-based repair adds more monomials but recovers from more failures

- **Failure signatures:**
  - Dense/low-rank polynomials: High false positive rates, near-100% failure for complex instances (Figure 4i)
  - Non-SOS polynomials: Overhead of 1.05–2.24× compared to standard approach (Table 8)
  - Distribution shift: Models trained on degree 12 struggle with degree 20+ without compositional structure

- **First 3 experiments:**
  1. **Baseline comparison on sparse polynomials:** Train on Grid 1 (sparse matrices), test on n=6, d=20, m=60. Expect 100–200× speedup over Newton polytope. Validate basis size reduction (73→56 monomials per Table 1).
  2. **Repair mechanism ablation:** Disable permutation scoring (L=1), measure success rate drop on block-diagonal instances. Expect ~15% degradation per Figure 7.
  3. **Out-of-distribution stress test:** Train on dense matrices, test on sparse (or vice versa). Compare against Table 23 vs. Table 26 to quantify structure transfer gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the computational complexity of finding the true minimal basis for a polynomial's Sum of Squares (SOS) decomposition?
- Basis in paper: [explicit] The authors state, "To the best of our knowledge, the computational complexity of finding the true minimal basis remains open."
- Why unresolved: Finding the minimal basis involves $\ell_0$ minimization over combinatorial structures, which typically resists polynomial-time solutions.
- What evidence would resolve it: A formal NP-hardness proof or the derivation of a polynomial-time algorithm for specific polynomial classes.

### Open Question 2
- Question: How well does the Transformer model generalize to real-world SOS problems compared to the synthetic data used for training?
- Basis in paper: [inferred] The Conclusion notes the reliance on synthetic data due to the "absence of widely available real-world SOS benchmarks," limiting empirical validation.
- Why unresolved: The "reverse sampling" training method may not capture the structural distributions of polynomials in robotics or control theory.
- What evidence would resolve it: Successful benchmarking on a curated dataset of real-world problems from control or optimization domains.

### Open Question 3
- Question: Can the verification of non-SOS polynomials (proving infeasibility) be accelerated using learning-augmented methods?
- Basis in paper: [inferred] The method currently "cannot accelerate the computation of negative SOS certificates" and incurs overhead compared to baselines for non-SOS instances.
- Why unresolved: The current approach relies on expansion until the full Newton polytope is searched; predicting infeasibility requires learning properties of the dual problem or non-negativity bounds.
- What evidence would resolve it: An algorithm that predicts infeasibility certificates or tight lower bounds to prune the search space without solving the full SDP.

## Limitations
- Performance highly dependent on polynomial structure; dense and low-rank polynomials show minimal or no speedup
- Reliance on reverse sampling for training data may not capture real-world polynomial distributions
- Repair mechanism effectiveness depends on reliable SDP infeasibility detection from solvers
- Cannot accelerate verification of non-SOS polynomials, incurring overhead compared to standard approach

## Confidence

- **High confidence:** The mechanism of basis size reduction → SDP dimensionality reduction → computational speedup. This follows directly from established SOS theory and is empirically validated across multiple polynomial structures.
- **Medium confidence:** The repair mechanism's correctness guarantees. While Lemma 1 provides necessary coverage conditions and the algorithm is well-defined, empirical validation of failure rate reduction is limited to specific test distributions.
- **Medium confidence:** The worst-case theoretical bounds. The Algorithms with Predictions framework provides solid theoretical grounding, but the assumption that SDP cost scales purely with basis size may not hold in practice.

## Next Checks

1. **Coverage repair convergence validation:** Generate 1000 polynomials with known minimal bases. Measure the average number of SDP iterations and total basis expansion required by the repair mechanism. Compare against the theoretical bound of O(log(η/m₁)) iterations.

2. **Solver reliability assessment:** For polynomials where the predicted basis fails, systematically vary MOSEK/SCS solver tolerances and record how often infeasibility flags are false positives due to numerical instability versus true missing basis elements.

3. **Real-world applicability test:** Apply the trained model to a benchmark set of polynomials from control theory, optimization, or computer vision (e.g., Lyapunov function certificates, collision avoidance constraints). Compare performance against domain-specific SOS solvers like SOSTools or GloptiPoly to assess practical value beyond synthetic instances.