---
ver: rpa2
title: 'Closing the Loop: Universal Repository Representation with RPG-Encoder'
arxiv_id: '2602.02084'
source_url: https://arxiv.org/abs/2602.02084
tags:
- semantic
- code
- repository
- feature
- functional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RPG-Encoder generalizes the Repository Planning Graph from a static
  generative blueprint into a unified, high-fidelity representation for existing codebases.
  It bridges the reasoning disconnect in repository agents by combining semantic lifting
  with topological constraints through three mechanisms: encoding raw code into an
  RPG that merges semantic features with dependencies, evolving topology incrementally
  to reduce maintenance overhead by 95.7%, and operating as a unified interface for
  structure-aware navigation.'
---

# Closing the Loop: Universal Repository Representation with RPG-Encoder

## Quick Facts
- **arXiv ID:** 2602.02084
- **Source URL:** https://arxiv.org/abs/2602.02084
- **Reference count:** 40
- **Primary result:** Establishes SOTA localization performance on SWE-bench Verified (93.7% Acc@5) and 98.5% reconstruction coverage on RepoCraft

## Executive Summary
RPG-Encoder transforms raw codebases into Repository Planning Graphs (RPGs) that combine semantic features with dependency information, enabling both repository understanding and reconstruction. By unifying semantic lifting with topological constraints through incremental updates and structured agentic tooling, the framework bridges the reasoning disconnect in repository agents. In evaluations, it achieves state-of-the-art localization performance on SWE-bench Verified and exceeds baselines by over 10% accuracy on SWE-bench Live Lite, while demonstrating 98.5% reconstruction coverage on RepoCraft.

## Method Summary
RPG-Encoder extracts Repository Planning Graphs through three phases: semantic lifting via LLM prompting to extract verb-object features from functions/classes, hierarchical construction through functional centroid discovery and aggregation, and artifact grounding via AST-based dependency injection. The framework supports incremental updates through commit diff parsing, handling deletions, modifications, and insertions with three atomic operations. A structured tool interface (SearchNode, FetchNode, ExploreRPG) provides navigation over the graph with 40-step limits for agentic use.

## Key Results
- Achieves 93.7% Acc@5 localization accuracy on SWE-bench Verified
- Exceeds best baseline by over 10% accuracy on SWE-bench Live Lite
- Demonstrates 98.5% reconstruction coverage on RepoCraft benchmark
- Reduces maintenance overhead by 95.7% through incremental updates

## Why This Works (Mechanism)

### Mechanism 1: Dual-View Semantic-Structural Encoding
Coupling semantic features ($f$) with structural metadata ($m$) in a unified graph enables more precise intent-to-implementation mapping than isolated API documentation or dependency graphs. The RPG-Encoder constructs a graph $G=(V,E)$ where nodes $V$ contain functional descriptions (lifted via LLM) and code metadata. Edges $E$ are split into Functional edges ($E_{feature}$) for hierarchy and Dependency edges ($E_{dep}$) for execution logic.

### Mechanism 2: Incremental Topological Evolution
Maintaining the RPG via differential updates (parsing commits) decouples maintenance cost from repository scale, preserving semantic fidelity with low overhead. Instead of rebuilding the graph, the system parses commit diffs to execute atomic operations: *Deletion* (pruning empty ancestors), *Modification* (updating features if drift $> \tau$), or *Insertion* (semantic routing to the best parent).

### Mechanism 3: Constrained Agentic Tooling
Providing a structured tool interface (Search, Fetch, Explore) over the graph enforces a "Search-then-Zoom" reasoning pattern, reducing navigational failures common in unconstrained agents. The framework exposes three tools: `SearchNode` (intent-to-code mapping), `FetchNode` (retrieving exact context), and `ExploreRPG` (traversing dependencies).

## Foundational Learning

### Concept: Intermediate Representations (IR)
- **Why needed:** The paper posits that fragmentation between semantic (docs) and structural (graphs) views causes reasoning disconnects. Understanding IRs (like RPG) is necessary to see how the system unifies "intent" and "execution."
- **Quick check:** Can you distinguish between a *dependency edge* (calls/imports) and a *functional edge* (hierarchical intent)?

### Concept: Static Analysis (AST Parsing)
- **Why needed:** The "Artifact Grounding" and $E_{dep}$ injection rely on parsing code structure (imports, classes) to anchor abstract semantic nodes to physical paths.
- **Quick check:** How would an AST parser identify a dynamic function call versus a static import, and which one is captured by this mechanism?

### Concept: LLM-based Feature Extraction
- **Why needed:** The "Semantic Lifting" phase depends entirely on an LLM following strict constraints (verb-object format, no implementation details) to generate node features.
- **Quick check:** If an LLM summarizes a function as "processes input" (generic) vs "validates JWT token" (specific), which one preserves the required "semantic density"?

## Architecture Onboarding

### Component map:
Input: Raw Code Files / Commit Diffs -> Extraction Pipeline: Semantic Parser (LLM) -> Feature Tree Builder -> Artifact Grounding (AST) -> State: Repository Planning Graph (Nodes: Files/Functions + Semantic Features; Edges: Hierarchy + Dependencies) -> Interface: Tool Layer (`SearchNode`, `ExploreRPG`, `FetchNode`)

### Critical path:
The *Semantic Lifting* prompt design (Appendix A.1.1) is the bottleneck. If feature extraction is noisy, the subsequent *Hierarchical Aggregation* (sorting features into functional areas) will misalign, breaking the entire navigation logic.

### Design tradeoffs:
- *Granularity vs. Cost:* Storing function-level nodes provides high precision but increases graph traversal steps compared to file-level nodes.
- *Incremental vs. Full Rebuild:* Incremental updates save 95.7% cost (Section 7.2) but risk accumulating "semantic drift" over time compared to periodic full rebuilds.

### Failure signatures:
- **Insufficient Coverage:** Agent fails to locate a bug because `SearchNode` found no matching semantic features (likely due to vague feature naming).
- **Context Fail:** Agent retrieves the correct file but misses the root cause because dependency edges ($E_{dep}$) did not capture a cross-module call.

### First 3 experiments:
1. **Localization Sanity Check:** Run `SearchNode` on 10 diverse bug reports from the paper's dataset (SWE-bench) to verify if the top-5 results contain the ground truth file.
2. **Drift Detection Test:** Modify a core function's logic significantly without changing its signature. Run the *Incremental Update* to see if the system detects the semantic drift and triggers a re-routing.
3. **Ablation on Tooling:** Disable `ExploreRPG` (dependency traversal) and measure the drop in function-level localization accuracy (Acc@1) to quantify the value of structural connectivity vs. semantic search alone.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the "negligible degradation" observed in incremental updates accumulate over long-term evolution, eventually necessitating a global rebuild regardless of the cost reduction benefits?
- **Basis:** Section 7.2 validates the incremental strategy over a short commit sequence but does not analyze how small semantic shifts compound over hundreds of commits.
- **Why unresolved:** The paper establishes short-term statistical parity but does not model long-term error propagation or "semantic rot" in the graph.
- **What evidence would resolve it:** A longitudinal analysis measuring localization accuracy over the entire lifetime of a high-activity repository (e.g., >1,000 commits).

### Open Question 2
- **Question:** What specific heuristics or thresholds can reliably automate the decision between incremental patching and full reconstruction during major refactoring events?
- **Basis:** Section 3.2 mentions reserving global reconstruction for "major refactoring" but does not define the detection criteria for such events.
- **Why unresolved:** The paper relies on LLM judgment for drift detection but lacks a deterministic policy for triggering the expensive global rebuild.
- **What evidence would resolve it:** An ablation study testing various structural metrics (e.g., file moves, dependency volume) as triggers for switching between update modes.

### Open Question 3
- **Question:** Does the semantic lifting protocol and dependency encoding generalize effectively to non-Python ecosystems with different module systems (e.g., JavaScript, C++)?
- **Basis:** Section 4 restricts evaluation to Python repositories (SWE-bench, RepoCraft), and the semantic parsing prompts (Appendix A.1) appear tailored to Python idioms.
- **Why unresolved:** It is unclear if the "verb-object" feature extraction and AST-based dependency tracking handle the complexity of other language ecosystems robustly.
- **What evidence would resolve it:** Cross-language evaluation on multi-lingual benchmarks or repositories with complex build systems (e.g., Maven, Cargo).

## Limitations

- The 95.7% maintenance efficiency gain lacks ablation on drift accumulation or fallback triggers for major refactors
- Semantic lifting phase's reliance on LLM consistency remains untested without inter-annotator agreement analysis
- Coverage claim conflates topological mirroring with semantic completeness without verifying feature preservation

## Confidence

- **High Confidence:** Localization accuracy results (93.7% Acc@5 on SWE-bench Verified, 10%+ improvement on Live Lite) are supported by clear metric definitions and comparative baselines
- **Medium Confidence:** The 95.7% efficiency gain and structured exploration patterns are plausible but lack ablation studies or long-term drift analysis
- **Low Confidence:** Semantic consistency under incremental updates and resilience to large-scale refactors are asserted but not empirically validated

## Next Checks

1. **Semantic Drift Monitoring:** Run 50+ incremental updates on a medium-sized repo (e.g., scikit-learn) and measure localization accuracy decay over time. Compare with periodic full rebuilds.
2. **Refactor Resilience Test:** Apply a package-renaming or architectural restructuring commit (e.g., Django's 4.2 to 5.0 transition) and evaluate whether the incremental update mechanism degrades or requires fallback.
3. **Tooling Ablation Study:** Disable `ExploreRPG` in the agent pipeline and measure the drop in function-level localization accuracy (Acc@1) to quantify the structural navigation contribution beyond semantic search alone.