---
ver: rpa2
title: 'Adversarial learning for nonparametric regression: Minimax rate and adaptive
  estimation'
arxiv_id: '2506.01267'
source_url: https://arxiv.org/abs/2506.01267
tags:
- adversarial
- where
- bukh
- inequality
- minimax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the minimax rates of convergence for nonparametric\
  \ regression under adversarial attacks on future input data. The authors consider\
  \ a general adversarial framework where the adversary can perturb each input within\
  \ a perturbation set, and the performance is measured by adversarial Lq-risk for\
  \ 1 \u2264 q \u2264 \u221E."
---

# Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation

## Quick Facts
- **arXiv ID:** 2506.01267
- **Source URL:** https://arxiv.org/abs/2506.01267
- **Reference count:** 16
- **Primary result:** Establishes minimax rates for nonparametric regression under adversarial input perturbations, showing rates consist of standard estimation error plus an adversarial term, and provides adaptive estimators achieving these rates.

## Executive Summary
This paper investigates nonparametric regression under adversarial attacks on future input data. The authors establish that the minimax rate under adversarial risk consists of two terms: the standard minimax rate depending on smoothness and dimensionality, plus an adversarial term depending on perturbation magnitude and smoothness. A piecewise local polynomial estimator with regularization achieves optimal minimax rates, requiring bandwidth adjustment as attack magnitude increases. The paper also constructs a data-driven adaptive estimator using Lepski's method that achieves optimal rates (up to a logarithmic factor) over broad nonparametric and adversarial classes.

## Method Summary
The paper proposes a regularized piecewise local polynomial estimator for nonparametric regression under adversarial attacks. The method discretizes the domain into a grid, computes local polynomial fits with regularization to prevent matrix singularities, and adjusts bandwidth based on the attack magnitude. For adaptive estimation without knowing smoothness parameters, Lepski's method is employed to select the optimal bandwidth from a candidate set. The adversarial risk is measured by sup-norm deviation within perturbation balls, requiring localized uniform convergence stronger than standard L2 risk.

## Key Results
- The minimax rate under adversarial risk is $r^{q(1\wedge\beta)} + n^{-q\beta/(2\beta+d)}$, combining standard estimation error with an adversarial term
- Bandwidth must be increased to $h \asymp r \vee n^{-1/(2\beta+d)}$ for robustness against attacks
- A phase transition occurs when attack magnitude exceeds a critical threshold, causing the adversarial term to dominate convergence
- Adaptive Lepski's method achieves near-optimal rates (up to logarithmic factor) without knowing smoothness parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The adversarial minimax rate is determined by the sum of standard estimation error and a term capturing function deviation within the perturbation radius.
- **Mechanism:** The paper derives a minimax rate that adds the standard convergence rate ($n^{-q\beta/(2\beta+d)}$) to an adversarial term ($r^{q(1\wedge\beta)}$). The latter arises because an adversary can perturb inputs within radius $r$, causing the estimated function to deviate based on its local smoothness (Lipschitz properties). If the attack magnitude $r$ exceeds a critical threshold (phase transition), the adversarial term dominates, preventing further convergence.
- **Core assumption:** The regression function $f$ belongs to a Hölder class $\mathcal{F}(\beta, C_\beta)$, ensuring predictable local behavior (smoothness).
- **Evidence anchors:**
  - [abstract] Establishes the rate as a sum of two terms depending on smoothness and perturbation.
  - [section 3 (Theorem 1)] Explicitly defines the minimax rate equation (3.1) including the phase transition points.
  - [corpus] "Adversarial Robustness of Nonparametric Regression" investigates similar corruption settings, supporting the decomposition of risk into estimation and adversarial components.
- **Break condition:** If the function $f$ is not smooth (e.g., non-differentiable chaotic signals outside Hölder classes), the adversarial term $r^{q(1\wedge\beta)}$ may fail to bound the deviation, breaking the rate guarantee.

### Mechanism 2
- **Claim:** Adversarial robustness is achieved by enlarging the kernel bandwidth when attack magnitude exceeds a specific threshold.
- **Mechanism:** Theoretical analysis shows that the bandwidth $h$ must be adjusted to $h \asymp r \vee n^{-1/(2\beta+d)}$. If the attack radius $r$ is large, using the standard optimal bandwidth results in high variance within the perturbation ball. Increasing $h$ effectively smooths the estimator over a larger region, trading off bias to dampen the impact of local input perturbations.
- **Core assumption:** The kernel function satisfies Assumption 1 (bounded support) to ensure localized smoothing properties hold.
- **Evidence anchors:**
  - [section 3.1 (Theorem 2)] Equation (3.12) dictates the optimal bandwidth choice.
  - [section 3.1.1] Describes the regularized local polynomial estimator used to implement this smoothing.
  - [corpus] "On damage of interpolation to adversarial robustness in regression" suggests interpolation (very small bandwidth/overfitting) hurts robustness, implicitly supporting the need for larger bandwidths (smoothing) in adversarial settings.
- **Break condition:** If the bandwidth is increased too much ($h \gg r$), the estimator over-smooths, failing to capture the true signal and violating the bias constraints.

### Mechanism 3
- **Claim:** Localized uniform convergence (sup-norm) is the necessary condition for adversarial robustness, stronger than standard $L_2$ convergence.
- **Mechanism:** Standard $L_2$ risk allows failure on sets of measure zero, which an adversary can target. By bounding the sup-norm risk over the perturbation set $A(x)$ (inequality 3.14), the paper ensures the estimator behaves well uniformly, closing the loopholes an adversary might exploit.
- **Core assumption:** The adversarial attack is constrained by the "Shift-in-at-least-one-direction" (SODA) geometric condition (Definition 4), ensuring perturbations are bounded.
- **Evidence anchors:**
  - [section 3.1.2] Inequality (3.14) explicitly links adversarial risk to the localized sup-norm risk.
  - [section 4.1] Construction of base functions relies on the geometry of perturbations to derive lower bounds.
- **Break condition:** If the perturbation set is unbounded or allows for "teleportation" (moving between disjoint high-density regions not connected by smooth paths), local uniform bounds may fail to capture global adversarial risks.

## Foundational Learning

- **Concept:** Hölder Smoothness ($\beta$)
  - **Why needed here:** The minimax rate explicitly depends on $\beta$. Understanding this smoothness parameter is required to determine the critical attack threshold and the optimal bandwidth.
  - **Quick check question:** If the true regression function becomes rougher (decreasing $\beta$), does the critical attack radius decrease or increase?

- **Concept:** Minimax Rates
  - **Why needed here:** The paper defines optimality based on the minimax paradigm. This concept frames the "no free lunch" trade-off between estimation difficulty and adversarial vulnerability.
  - **Quick check question:** Can you explain why the adversarial term does not vanish as $n \to \infty$ if $r$ is constant?

- **Concept:** Lepski's Method (Adaptation)
  - **Why needed here:** The adaptive estimator uses Lepski's method (3.21) to select bandwidth without knowing $\beta$. This is crucial for practical implementation where smoothness is unknown.
  - **Quick check question:** How does Lepski's method balance the risk of over-smoothing against the risk of insufficient robustness?

## Architecture Onboarding

- **Component map:** Generate Grid -> Compute Local Design Matrices -> Run Adaptive Bandwidth Selector -> Fit Local Polynomial -> Predict
- **Critical path:** Generate Grid → Compute Local Design Matrices → Run Adaptive Bandwidth Selector → Fit Local Polynomial → Predict
- **Design tradeoffs:**
  - **Grid Resolution ($M$) vs. Computational Cost:** $M \asymp 1/h$. Finer grids improve approximation accuracy but increase quadratic complexity.
  - **Regularization ($\tau$) vs. Bias:** $\tau$ prevents matrix singularity (Eq 3.8) in sparse data regions but introduces bias if set too high.
  - **Bandwidth ($h$) vs. Robustness:** Large $h$ increases robustness (Theorem 2) but degrades standard accuracy (bias increases).
- **Failure signatures:**
  - **Flat Estimation:** Output is nearly constant; likely caused by over-regularization ($\tau$ too high) or excessive bandwidth ($h$).
  - **Volatile Risk:** Adversarial risk fluctuates wildly; likely caused by bandwidth selector picking values too small for the attack magnitude $r$.
  - **Matrix Errors:** Singular matrix warnings; regularization term $\tau$ may be too small or $n$ too low for the chosen degree $\ell$.
- **First 3 experiments:**
  1. **Phase Transition Verification:** Generate data with known $\beta$ and sweep attack radius $r$ to observe the shift from standard minimax rate to adversarial term dominance.
  2. **Ablation on Regularization:** Implement the regularized matrix (3.8) vs. standard inversion to verify robustness against singularities in low-density regions.
  3. **Adaptive vs. Oracle Comparison:** Compare the data-driven Lepski bandwidth selector against the "Oracle" bandwidth (known $r, \beta$) to measure the "cost of adaptation" (the log factor).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the logarithmic factor in the data-driven adaptive estimator unavoidable for $1 \le q < \infty$?
- **Basis:** [explicit] Remark 5 (p. 18) states, "Whether the logarithmic increase is also an unavoidable cost for adaptive adversarial learning remains an open question," noting the contrast with standard settings where L2-rates can sometimes avoid such factors.
- **Why unresolved:** While Lepski's method is known to introduce logarithmic factors in pointwise or sup-norm adaptation, it is theoretically undetermined if the adversarial setting forces this cost specifically for the $L_q$ risk.
- **What evidence would resolve it:** A minimax lower bound for adaptation over the scales $\mathcal{P}$ and $\mathcal{T}$ proving a logarithmic penalty is necessary, or the construction of a new adaptive estimator achieving the exact minimax rate without the log factor.

### Open Question 2
- **Question:** Can popular defense strategies, such as adversarial training algorithms, achieve the minimax rates established for local polynomial methods?
- **Basis:** [explicit] Section 5 (p. 27) lists "evaluating whether other popular defense strategies, like adversarial training... achieve the minimax optimality or not" as an "important open question."
- **Why unresolved:** The paper analyzes a specific piecewise local polynomial estimator, whereas adversarial training typically involves minimizing an empirical adversarial loss whose convergence properties are harder to bound tightly relative to the theoretical optimum.
- **What evidence would resolve it:** A theoretical analysis showing that empirical risk minimizers under adversarial loss converge at the rate $r^{q(1 \wedge \beta)} + n^{-q\beta/(2\beta+d)}$.

### Open Question 3
- **Question:** How do the minimax rates change when the regression function possesses anisotropic smoothness (different smoothness in different directions)?
- **Basis:** [inferred] Remark 1 (p. 11) explicitly contrasts the paper's isotropic results with anisotropic classes, noting, "the attack direction $v$ may affect the adversarial risk" in those cases.
- **Why unresolved:** The current proofs and optimal bandwidth selection rely on a single smoothness parameter $\beta$ and principal direction $v$. Anisotropic functions require bandwidth adaptation to multiple directional smoothness levels, complicating the interaction with the perturbation set.
- **What evidence would resolve it:** A derivation of minimax rates dependent on a vector of smoothness parameters $(\beta_1, \dots, \beta_d)$ and the geometric alignment between the attack and the function's principal axes.

## Limitations
- The phase transition behavior requires precise knowledge of the smoothness parameter β, which is typically unknown in practice
- The adaptive estimator incurs a logarithmic penalty, suggesting a fundamental cost of adaptation
- Computational complexity scales poorly with dimensionality, limiting applicability to moderate-dimensional problems

## Confidence

- **High Confidence:** The minimax rate decomposition (standard term + adversarial term) and the phase transition phenomenon are rigorously proven and well-supported by theoretical arguments. The regularized local polynomial estimator's optimality under known parameters is mathematically sound.
- **Medium Confidence:** The adaptive estimator's near-optimal performance relies on Lepski's method working as intended, but the sensitivity to the choice of constants (particularly C in equation 3.21) introduces uncertainty. The practical implementation details are somewhat sparse.
- **Low Confidence:** The assumption that adversaries are constrained to bounded perturbations may be overly restrictive. Real-world attackers might employ unbounded or adaptive strategies that could invalidate the theoretical guarantees.

## Next Checks

1. **Phase Transition Verification:** Implement a simulation study that systematically varies the smoothness parameter β and perturbation magnitude r to empirically verify the predicted phase transition where the adversarial term dominates the convergence rate.

2. **Adaptive Method Sensitivity:** Conduct an ablation study on the adaptive estimator by varying the initialization constant C in Lepski's method across multiple orders of magnitude to quantify its impact on the achieved rate and understand the tradeoff between adaptation cost and performance.

3. **Robustness to Assumption Violations:** Design experiments where the Hölder smoothness assumption is violated (e.g., using piecewise smooth functions with discontinuities) to test whether the adversarial term still provides meaningful bounds or if the theoretical framework breaks down.