---
ver: rpa2
title: 'T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal
  Distillation'
arxiv_id: '2602.01937'
source_url: https://arxiv.org/abs/2602.01937
tags:
- forecasting
- temporal
- time
- series
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling large language models
  (LLMs) to perform time series forecasting without relying on extensive time-series
  pretraining. The proposed T-LLM framework uses temporal distillation to teach an
  LLM forecasting behavior by imitating a lightweight temporal teacher during training.
---

# T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation

## Quick Facts
- **arXiv ID**: 2602.01937
- **Source URL**: https://arxiv.org/abs/2602.01937
- **Reference count**: 40
- **Primary result**: Enables LLMs to forecast time series via temporal distillation, outperforming existing LLM-based methods across full-shot, few-shot, and zero-shot settings

## Executive Summary
This paper addresses the challenge of enabling large language models (LLMs) to perform time series forecasting without extensive time-series pretraining. The proposed T-LLM framework uses temporal distillation to teach an LLM forecasting behavior by imitating a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis, while the student LLM learns through prediction-level supervision and selective guidance. Experiments on benchmark datasets and infectious disease forecasting tasks show T-LLM consistently outperforms existing LLM-based methods across full-shot, few-shot, and zero-shot settings, achieving up to 2.2% lower error rates in zero-shot transfer while maintaining computational efficiency.

## Method Summary
T-LLM implements a temporal distillation framework where a lightweight teacher model provides structured temporal supervision to an LLM student. The teacher performs trend decomposition (using moving-average filtering) and frequency-domain analysis (via FFT and Adaptive Spectral Block with Dominant Spectral Projection). The LLM student, augmented with LoRA adapters, learns to imitate the teacher's predictions through a multi-component loss combining direct supervision, imitation loss, and selective layer-wise guidance. During inference, only the student LLM remains, enabling efficient forecasting. The approach transfers forecasting behavior from the teacher to the LLM without requiring extensive time-series pretraining.

## Key Results
- Achieves up to 2.2% lower error rates in zero-shot transfer compared to baseline LLM methods
- Outperforms existing LLM-based forecasting approaches across full-shot, few-shot, and zero-shot settings
- Demonstrates computational efficiency by removing the teacher during inference while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1: Prediction-Level Imitation via Reverse Distillation
Training the LLM to reproduce a temporal teacher's forecasts transfers forecasting behavior more effectively than representation alignment alone. The imitation loss directly penalizes divergence between student and teacher predictions, combined with direct supervision to stabilize learning. This bypasses the need for the LLM to discover temporal patterns from scratch. Early stopping based on teacher convergence is critical, as systematic teacher bias would be inherited by the student.

### Mechanism 2: Structured Temporal-Spectral Supervision
Combining trend decomposition with frequency-domain filtering provides compact, horizon-aware supervision that exposes the LLM to diverse temporal patterns. The teacher uses DLinear-based decomposition (trend + seasonal) plus FFT-based Adaptive Spectral Block with Dominant Spectral Projection. Horizon-conditioned capacity adjusts spectral information based on prediction length—longer horizons receive richer periodic context. This structured supervision enables the LLM to learn both trend and periodic components effectively.

### Mechanism 3: Selective Head-Tail Guidance
Guiding only early (layer 2) and late (layer 3) layers provides sufficient temporal structure injection without full layer-wise alignment overhead. The guidance loss applies projection heads only at selected depths, based on attention analysis showing middle layers exhibit low CKA similarity and unstable attention patterns. This approach achieves comparable accuracy to full guidance with approximately N/2× fewer guidance modules.

## Foundational Learning

- **Knowledge Distillation (Teacher-Student)**: The entire framework relies on transferring behavior from teacher to student. Without understanding distillation objectives, the multi-loss formulation will seem ad-hoc. Quick check: Can you explain why the student needs both direct supervision and teacher matching?

- **Time Series Decomposition (Trend-Seasonal-Residual)**: The temporal teacher uses moving-average decomposition. Understanding this helps diagnose why the teacher captures certain patterns and misses others. Quick check: Given a time series with a strong weekly cycle and upward drift, which component would each capture?

- **FFT-Based Spectral Analysis**: The frequency-domain pathway requires understanding how FFT represents periodicity and what DSP dimension reduction preserves. Quick check: Why might higher-dimensional spectral representations require dimension reduction before pooling?

## Architecture Onboarding

- **Component map**: Input X → Embedding → Self-Attention (E1) → Cross-Attention (Z1) → [Temporal Teacher] → [LLM Student] → Ŷ_S. The critical path is input embedding → temporal teacher's trend/frequency branches → aggregation → imitation loss computation between teacher and student predictions.

- **Critical path**: Input embedding → temporal teacher's trend/frequency branches → aggregation → imitation loss computation between Ŷ_T and Ŷ_S. This is where behavior transfer occurs. The guidance loss at layers 2-3 is secondary but stabilizes training.

- **Design tradeoffs**:
  - Teacher complexity vs. distillation quality: Simpler teachers train faster but may provide weaker supervision
  - Guidance granularity vs. compute: Head-tail guidance achieves similar accuracy with fewer parameters
  - DSP capacity vs. horizon: Fixed capacity underperforms at extreme horizons; horizon-conditioned selection adds complexity but improves robustness

- **Failure signatures**:
  - Student diverges from teacher: Check if imitation weight is too low, or if teacher hasn't converged before distillation begins
  - Poor zero-shot transfer: May indicate overfitting to source domain temporal patterns
  - High FLOPs at inference: Ensure teacher and guidance components are fully removed

- **First 3 experiments**:
  1. Ablate imitation loss alone to establish baseline for how much distillation contributes vs. direct supervision
  2. Compare head-tail vs. single-layer guidance to test if layer 2 alone or layer 3 alone suffices
  3. Zero-shot cross-domain with different DSP capacities to verify horizon-conditioned selection generalizes

## Open Questions the Paper Calls Out

### Open Question 1
How does temporal distillation scale to larger language models beyond GPT-2, and does the efficiency-accuracy tradeoff persist with modern LLM architectures? The implementation uses only GPT-2 with 6 transformer layers, leaving unexplored whether the benefits of reverse distillation generalize to larger pretrained models or different LLM families.

### Open Question 2
What is the theoretical limit of knowledge transfer from a lightweight temporal teacher to an LLM student, and can the student ever surpass the teacher's forecasting capability? The framework explicitly trains the LLM to imitate teacher predictions, but does not investigate whether the LLM's pretraining priors could enable extrapolation beyond teacher performance.

### Open Question 3
Can horizon-conditioned spectral capacity selection be learned dynamically rather than through predefined pairs, and would adaptive capacity improve generalization to novel forecasting horizons? The DSP capacity schedule uses manually defined horizon-capacity pairs, acknowledged as a discrete design choice that avoids per-task tuning.

## Limitations
- The framework hinges on the lightweight teacher providing high-quality supervision, with no ablation studies examining teacher architecture sensitivity
- Results are demonstrated on GPT-2 backbone, with layer role assumptions that may not transfer to other LLM architectures
- Training requires full teacher-student joint optimization, with computational overhead not fully characterized

## Confidence

**High Confidence**: Claims about superior zero-shot performance relative to baseline LLM methods (baseline performance gaps are substantial and consistent across datasets).

**Medium Confidence**: Claims about selective guidance being sufficient vs. full alignment (supported by ablation but with narrow margin; architectural specificity not fully explored).

**Low Confidence**: Claims about teacher convergence timing and early stopping criteria (stated as "critical" but not empirically validated or quantified).

## Next Checks

1. **Teacher failure mode analysis**: Train T-LLM with a deliberately weakened teacher to quantify how teacher quality impacts student performance and establish baseline degradation curves.

2. **Cross-LLM architecture validation**: Implement T-LLM on a different backbone (e.g., OPT-125m) to test if head-tail guidance assumptions hold or if guidance layer selection needs adaptation.

3. **Training efficiency profiling**: Measure wall-clock training time and memory usage across different DSP capacities and sequence lengths to characterize computational scaling and identify bottlenecks in the temporal distillation process.