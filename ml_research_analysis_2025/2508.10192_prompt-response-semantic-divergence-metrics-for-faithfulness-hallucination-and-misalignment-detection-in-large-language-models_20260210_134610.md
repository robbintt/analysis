---
ver: rpa2
title: Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination
  and Misalignment Detection in Large Language Models
arxiv_id: '2508.10192'
source_url: https://arxiv.org/abs/2508.10192
tags:
- prompt
- semantic
- answer
- response
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Semantic Divergence Metrics (SDM), a lightweight
  framework for detecting faithfulness hallucinations in LLMs, where responses deviate
  severely from input contexts. The method improves on Semantic Entropy by using multiple
  semantically equivalent paraphrases of prompts and joint clustering of sentence
  embeddings to create a shared topic space for prompts and answers.
---

# Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models

## Quick Facts
- arXiv ID: 2508.10192
- Source URL: https://arxiv.org/abs/2508.10192
- Authors: Igor Halperin
- Reference count: 16
- Key outcome: Introduces Semantic Divergence Metrics (SDM) to detect faithfulness hallucinations in LLMs by measuring semantic alignment between prompts and responses using joint clustering and information-theoretic metrics

## Executive Summary
This paper presents Semantic Divergence Metrics (SDM), a lightweight framework for detecting faithfulness hallucinations in large language models where responses deviate from input contexts. The method improves on Semantic Entropy by using multiple semantically equivalent paraphrases of prompts and joint clustering of sentence embeddings to create a shared topic space. SDM computes information-theoretic metrics like Jensen-Shannon divergence, Wasserstein distance, and KL divergence to measure semantic alignment, with the SH score combining these metrics to quantify semantic drift. Experiments show SDM successfully tracks response stability across factual, interpretive, and creative tasks, distinguishing faithful recall from confident confabulation.

## Method Summary
SDM generates M=10 paraphrases of an original prompt and produces N=4 responses per paraphrase from a target LLM. All sentences are embedded using Qwen3-Embedding-0.6B, then jointly clustered using hierarchical agglomerative clustering to create shared topic spaces. The framework calculates topic distributions for prompts and responses, then computes ensemble Jensen-Shannon divergence, KL divergence, and Wasserstein distances. These metrics are aggregated into the primary hallucination score SH, with secondary metrics including KL divergence for semantic exploration and normalized conditional entropy. The method is designed for real-time black-box LLM analysis.

## Key Results
- SDM successfully tracks response stability across factual, interpretive, and creative tasks
- KL divergence emerges as a key indicator of semantic exploration, distinguishing creative generation from faithful recall
- The framework effectively identifies "confident hallucination" where models provide stable but factually incorrect responses
- Semantic Instability (variance across paraphrases) reveals the cognitive demands of different tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint clustering of prompt and response sentences creates a shared semantic basis, allowing for a direct comparison of topic distributions.
- **Mechanism:** By pooling embeddings from both prompts and answers before clustering, the framework ensures that "Topic 1" refers to the same semantic concept in both the input and output spaces. This enables the calculation of divergence metrics between the prompt's topic distribution P(X) and the answer's distribution P(Y).
- **Core assumption:** The sentence embedding model captures semantic meaning sufficiently well that proximity in embedding space correlates with semantic relevance to the prompt's context.
- **Evidence anchors:** [abstract] "...uses joint clustering on sentence embeddings to create a shared topic space for prompts and answers." [section 4.5] "This joint clustering approach is crucial as it ensures that semantically similar sentences, regardless of whether they originate from a prompt or an answer, are grouped into the same topic cluster..."

### Mechanism 2
- **Claim:** Testing stability across semantically equivalent prompt paraphrases reveals "deep arbitrariness" that single-prompt sampling misses.
- **Mechanism:** Standard consistency checks sample multiple answers for one prompt. SDM adds a layer of robustness by paraphrasing the prompt M times. If an LLM changes its response strategy drastically for a trivial paraphrase, it indicates the response is "brittle" or highly sensitive to noise, signaling a potential hallucination.
- **Core assumption:** The paraphrasing mechanism preserves the original intent perfectly; if the paraphrase drifts, the metric may falsely penalize a faithful model.
- **Evidence anchors:** [abstract] "...measuring response consistency not only across multiple answers but also across multiple, semantically-equivalent paraphrases of the original prompt." [section 4.1] "...Semantic Instability (The 'Variance' Dimension): The second dimension measures the semantic distances between prompts and responses across a range of semantically equivalent prompts."

### Mechanism 3
- **Claim:** The asymmetric KL divergence KL(Answer || Prompt) acts as a proxy for "Semantic Exploration," distinguishing creative generation from faithful recall.
- **Mechanism:** This metric measures the information gain when moving from the prompt distribution to the answer distribution. A high score indicates the answer contains high-probability topics that were low-probability or absent in the prompt (hallucination or creativity). A low score indicates the answer stayed within the prompt's conceptual boundaries (recall).
- **Core assumption:** The directionality of KL divergence (A || P) effectively captures the "drift" away from the source material rather than just the distance between them.
- **Evidence anchors:** [section 4.1] "We measure this using the Kullback-Leibler (KL) divergence... A high value... indicates that the model has performed a significant creative or interpretive leap." [section 6.4] "Generative Scaffolding Prompts... yield explosive KL(Answer || Prompt) values..."

## Foundational Learning

- **Concept: Jensen-Shannon Divergence (JSD)**
  - **Why needed here:** JSD is the core symmetric metric used to calculate the primary stability score (SH). Unlike KL, it is bounded (0 to 1) and symmetric, making it suitable for measuring the simply the "distance" between two distributions without directionality.
  - **Quick check question:** If P and Q are identical distributions, what is the JSD value? (Answer: 0)

- **Concept: Wasserstein Distance (Earth Mover's Distance)**
  - **Why needed here:** Used alongside JSD in the SH score. It measures the geometric "cost" to transform the cloud of prompt embeddings into the cloud of answer embeddings. It captures semantic drift in the continuous embedding space, complementing the discrete clustering of JSD.
  - **Quick check question:** Does Wasserstein distance rely on cluster labels, or raw embeddings? (Answer: Raw embeddings)

- **Concept: Hierarchical Agglomerative Clustering**
  - **Why needed here:** This is the algorithm used to build the shared topic space. It bottoms-up merges sentences into clusters. Understanding this helps in diagnosing why certain unrelated sentences might be grouped (e.g., due to "ward" linkage minimizing variance).
  - **Quick check question:** Why might hierarchical clustering be preferred over K-Means here? (Hint: Section 4.5 mentions it handles varying complexity without pre-specifying k, though the authors use an elbow-method pre-step)

## Architecture Onboarding

- **Component map:** Original Prompt -> Paraphraser (M variations) -> Target LLM (N responses) -> Embedder (Qwen3-Embedding-0.6B) -> Joint Clusterer (Agglomerative clustering) -> Metric Engine (JSD, KL, Wasserstein) -> Classifier (Semantic Box)
- **Critical path:** The Joint Clustering step is the bottleneck (Complexity O(S^2)). If this is slow, the real-time analysis fails.
- **Design tradeoffs:**
  - Stability vs. Detection: A low threshold for SH catches more hallucinations but flags creative writing as errors
  - Embedding Quality vs. Speed: Larger embedding models improve cluster cohesion but increase latency
  - Ensemble Size: Increasing M and N improves statistical robustness but costs linearly more LLM tokens and compute time
- **Failure signatures:**
  - "Confident Hallucination" (Red Box): Low SH and Low KL. The model confidently repeats a stable lie or evasion
  - "Creative" False Positive: High SH and High KL on a valid open-ended question
- **First 3 experiments:**
  1. Calibration Run: Run 20 prompts with known ground truth (10 factual, 10 nonsense) to find optimal weights and thresholds for your specific domain
  2. Ablation on Clustering: Test the framework using fixed K clusters vs. the "elbow method" to see if dynamic cluster counts are necessary for your prompt lengths
  3. Paraphrase Sensitivity: Verify the paraphraser isn't degrading quality. Run SDM using M=1 (no paraphrase) vs M=10 on a set of ambiguous prompts to visualize the variance reduction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Semantic Box boundaries be empirically calibrated using large-scale labeled datasets (e.g., TruthfulQA) to reliably distinguish between faithful, creative, and hallucinated responses?
- **Basis in paper:** [explicit] Section 9.1 states that "rigorous, large-scale evaluation is crucial" to "establish data-driven thresholds for the boundaries of the Semantic Box," moving it from a conceptual model to a calibrated tool.
- **Why unresolved:** The current study uses controlled, small-scale experiments. The authors note that absolute metric values are context-dependent, and the decision boundaries for the Semantic Box quadrants (S* and KL*) have not been validated on diverse, real-world benchmarks.
- **What evidence would resolve it:** Results from cross-validated experiments on standard hallucination benchmarks showing performance metrics (e.g., ROC-AUC) for the SH and KL scores when classifying specific error types.

### Open Question 2
- **Question:** How can a self-calibrating system automatically generate a local stability spectrum for a given prompt to interpret SDM metrics in relative rather than absolute terms?
- **Basis in paper:** [explicit] Section 9.2 proposes developing a "self-calibrating framework" to address the context-dependence of metrics without needing pre-labeled datasets, suggesting the automatic modification of prompts to create a baseline.
- **Why unresolved:** The paper establishes that a single universal threshold for hallucination is not feasible. The proposed solution—dynamically generating a stability spectrum to establish a relative baseline for every unique prompt—remains an unimplemented concept.
- **What evidence would resolve it:** An algorithm that programmatically varies prompt constraints (e.g., adding specificity vs. abstractness) and successfully normalizes the SH score for different prompt types.

### Open Question 3
- **Question:** Does excluding instructional prompts and jointly embedding only reference documents with generated answers improve the detection of ungrounded hallucinations in Retrieval-Augmented Generation (RAG)?
- **Basis in paper:** [explicit] Section 9.3 notes that the framework can be tuned for RAG with a "minimal tweak" to measure "semantic groundedness" by excluding instructions from the embedding space.
- **Why unresolved:** While theoretically proposed as a method to measure semantic extrapolation from source documents, this specific configuration has not been tested against standard groundedness metrics or RAG-specific benchmarks.
- **What evidence would resolve it:** Comparative experiments showing the correlation between the modified SH score and human annotations of groundedness in RAG tasks.

### Open Question 4
- **Question:** How can the framework be extended to detect dynamic topic changes in multi-turn conversations using incremental updates to the joint probability distribution?
- **Basis in paper:** [explicit] Section 9.4 proposes extending SDM to "dynamic topic change detection" by monitoring the evolution of the topic co-occurrence heatmap and potentially using dynamic clustering.
- **Why unresolved:** The current methodology analyzes static prompt-response pairs. Detecting conversational drift or "topic change" requires a temporal component and a mechanism for updating clusters dynamically, which the current experiments do not address.
- **What evidence would resolve it:** A study demonstrating that sudden shifts in the dynamic Pavg(X, Y) distribution or the emergence of new clusters correlate with annotated topic shifts in dialogue datasets.

## Limitations
- The framework detects semantic stability rather than factual correctness—a high score indicates coherent, on-topic responses but doesn't guarantee accuracy against external knowledge
- The clustering approach may fail on polysemous terms or domain-specific jargon where embeddings collapse distinct concepts
- The paraphrasing mechanism assumes perfect meaning preservation, but subtle semantic shifts could inflate variance scores artificially

## Confidence
- **High:** The core mechanism of joint clustering creating shared topic spaces is well-founded and empirically supported. The use of multiple information-theoretic metrics (JSD, KL, Wasserstein) for semantic alignment is methodologically sound.
- **Medium:** The claim that KL divergence effectively distinguishes creative generation from faithful recall is supported by experiments but may require domain-specific calibration. The choice of ensemble averaging over paraphrases is justified but could be sensitive to paraphrase quality.
- **Low:** The threshold values (w_jsd=0.7, w_wass=0.3) and the "Semantic Box" classification logic appear heuristic and may not generalize across domains without tuning.

## Next Checks
1. **Ground Truth Validation:** Test SDM on a dataset with human-annotated faithfulness labels (e.g., Factuality-Aware Summarization Dataset) to measure precision/recall against established baselines
2. **Paraphrase Robustness:** Systematically vary paraphrase quality (using models with different capabilities) to determine sensitivity to paraphrase-induced variance versus true semantic drift
3. **Domain Transfer:** Apply SDM to non-text domains (e.g., code generation, structured data queries) to assess whether embedding-based clustering generalizes beyond natural language