---
ver: rpa2
title: 'Assessing Small Language Models for Code Generation: An Empirical Study with
  Benchmarks'
arxiv_id: '2507.03160'
source_url: https://arxiv.org/abs/2507.03160
tags:
- slms
- code
- performance
- pass
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study empirically evaluates 20 small language models (SLMs)
  across five benchmarks to assess their code generation performance, efficiency,
  and multilingual capabilities. Results show that larger SLMs generally achieve higher
  accuracy, but several mid-sized and smaller models perform competitively while requiring
  substantially fewer resources.
---

# Assessing Small Language Models for Code Generation: An Empirical Study with Benchmarks

## Quick Facts
- **arXiv ID:** 2507.03160
- **Source URL:** https://arxiv.org/abs/2507.03160
- **Reference count:** 40
- **Primary result:** Larger SLMs generally achieve higher accuracy, but several mid-sized and smaller models perform competitively while requiring substantially fewer resources.

## Executive Summary
This study evaluates 20 small language models (0.4B-10B parameters) across five benchmarks to assess their code generation performance, efficiency, and multilingual capabilities. The research reveals that while larger models typically achieve higher accuracy, several mid-sized and smaller models deliver competitive performance with significantly reduced computational requirements. The analysis identifies trade-offs between accuracy and efficiency, demonstrating that well-optimized smaller models can serve as practical alternatives in resource-constrained environments.

## Method Summary
The study evaluates 20 open-source SLMs (0.4Bâ€“10B parameters) using zero-shot prompting across five benchmarks: HumanEval, MBPP, Mercury, HumanEvalPack, and CodeXGLUE. Models are categorized into three groups by parameter count and tested using the BigCode Evaluation Harness with consistent decoding parameters (temperature=0.2, top-p=0.95). Performance is measured using pass@k for functional correctness, BLEU for code summarization, and VRAM usage plus inference time for efficiency. Statistical analysis (ANOVA, Tukey's HSD) validates observed differences across model groups and programming languages.

## Key Results
- Larger SLMs achieve higher accuracy, but several mid-sized models perform competitively while requiring 4x less VRAM for 10% performance gains
- VRAM consumption scales non-linearly with model size while inference latency remains statistically similar across the 0.4B-10B parameter range
- Multilingual performance varies by language but differences are not statistically significant across the tested set

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Larger parameter counts correlate with higher functional correctness, but well-optimized smaller models can achieve competitive performance through architectural and training quality rather than scale alone.
- **Mechanism:** Parameter scaling increases model capacity to capture code patterns and syntactic structures. However, training data curation, architectural refinements, and optimization techniques can partially compensate for reduced capacity in smaller models.
- **Core assumption:** The pass@k metric accurately reflects real-world code generation utility.
- **Evidence anchors:** [abstract] "Results show that larger SLMs generally achieve higher accuracy, but several mid-sized and smaller models perform competitively while requiring substantially fewer resources."

### Mechanism 2
- **Claim:** VRAM consumption scales non-linearly with model size while inference latency remains statistically similar across the 0.4B-10B parameter range under fixed decoding configurations.
- **Mechanism:** Memory requirements grow with parameter count (loading weights, activations, KV cache). However, inference time depends on sequence length, batch size, and hardware parallelism, which can mask latency differences within the SLM range.
- **Core assumption:** Single-GPU deployment without advanced optimizations (vLLM, quantization) represents typical deployment constraints.
- **Evidence anchors:** [abstract] "For 10% performance improvements, models can require nearly a 4x increase in VRAM consumption."

### Mechanism 3
- **Claim:** Multilingual performance differences exist at the task level (Python/Java/PHP outperform Go/C++/Ruby) but fall within statistical noise thresholds for the tested language set.
- **Mechanism:** Performance variation likely stems from training data distribution bias (more Python/Java code in pretraining corpora) and syntactic complexity differences (C++ templates, Ruby idioms). Statistical non-significance suggests model architectures generalize reasonably across paradigms.
- **Core assumption:** The benchmark language set (Python, Java, JavaScript, C++, Go, PHP, Ruby) is representative of common programming tasks.
- **Evidence anchors:** [abstract] "SLMs tend to perform better in languages such as Python, Java, and PHP, while exhibiting relatively weaker performance in Go, C++, and Ruby. However, statistical analysis suggests these differences are not significant."

## Foundational Learning

- **Concept: pass@k functional correctness metric**
  - **Why needed here:** Core evaluation metric quantifying probability that at least one of k generated samples passes all test cases. Understanding this is essential for interpreting Table 4 rankings and stability scores.
  - **Quick check question:** If a model generates 10 samples (n=10) with 3 correct solutions, what is pass@1 approximately? (Answer: ~0.30 using the formula in Equation 1)

- **Concept: Zero-shot prompting in code generation**
  - **Why needed here:** All experiments use zero-shot prompting without in-context examples. This design choice affects comparability with few-shot evaluations and may underestimate instruction-tuned model capabilities.
  - **Quick check question:** Why did the authors exclude few-shot prompting despite its common use in practice? (Answer: Section 2.4.1 states base models do not reliably benefit from demonstrations; few-shot primarily helps instruction-tuned models)

- **Concept: Decoder-only transformer architecture for autoregressive code generation**
  - **Why needed here:** All 20 evaluated SLMs use decoder-only architecture. Understanding this constraint helps interpret why encoder-decoder or encoder-only models were excluded from scope.
  - **Quick check question:** What is the practical implication of restricting evaluation to decoder-only models? (Answer: Findings may not generalize to encoder-decoder code models like CodeT5)

## Architecture Onboarding

- **Component map:** BigCode Evaluation Harness -> Model loading -> Prompt formatting -> Inference execution -> Automated test execution -> Metric aggregation and statistical testing
- **Critical path:** 1. Model selection -> 2. Benchmark configuration -> 3. Unified decoding setup (temp=0.2, top-p=0.95) -> 4. Generation (n=10 samples per task) -> 5. Automated test execution -> 6. Metric aggregation and statistical testing
- **Design tradeoffs:** Zero-shot vs. few-shot: Chose zero-shot for comparability across base models, sacrificing real-world workflow fidelity; Single-GPU vs. multi-GPU: Single-GPU deployment ensures consistent VRAM measurements but limits throughput
- **Failure signatures:** Out-of-memory errors on larger models with extended context (mitigated by reducing batch size from 10 to 5); Inconsistent rankings across pass@k values (indicates model sensitivity to sampling diversity); High SD/CV gap (Table 6) signals unstable cross-benchmark performance
- **First 3 experiments:** 1. Baseline calibration: Run Qwen2.5-Coder 1.5B and 7.0B on HumanEval with n=10 to verify reproducibility of reported pass@1 scores (0.44 vs. 0.62); 2. VRAM profiling: Measure peak memory for Group 1 vs. Group 3 models on Mercury benchmark (expects ~5GB vs. ~23GB from Table 10); 3. Multilingual spot-check: Test OpenCodeInterpreter 6.7B on HumanEvalPack C++ vs. Java tasks to confirm observed performance gap (expects lower C++ scores per Figure 4)

## Open Questions the Paper Calls Out

- **Question:** How do SLMs perform in real-world industrial software engineering contexts compared to static benchmarks?
  - **Basis in paper:** [explicit] The Conclusion states, "As a future direction, we plan to extend this study through industrial case studies to assess how these models perform in practical software development settings."
  - **Why unresolved:** The current study relies on static, synthetic benchmarks (e.g., HumanEval) which lack the complexity of production environments like multi-file dependencies and legacy system integration.
  - **What evidence would resolve it:** Field studies or deployment metrics tracking SLM success rates in live development workflows (e.g., PR acceptance rates, debugging efficiency).

- **Question:** How does few-shot or instruction-tuned prompting affect the performance trade-offs of SLMs?
  - **Basis in paper:** [inferred] The authors acknowledge that relying exclusively on zero-shot prompting "limits the ability to assess model performance in example-driven or interactive development workflows."
  - **Why unresolved:** Base models were evaluated without examples to ensure comparability, but practitioners often use few-shot prompting, which may alter the observed balance between size and accuracy.
  - **What evidence would resolve it:** Ablation studies measuring pass@k scores across the same models using varied prompting strategies (zero-shot vs. few-shot).

- **Question:** What are the energy consumption and power efficiency trade-offs for SLMs during code generation?
  - **Basis in paper:** [inferred] The limitations section notes the study focused on VRAM and time but excluded "energy consumption, CPU utilization, thermal characteristics" which are relevant for edge deployment.
  - **Why unresolved:** While VRAM usage increases with size, the power efficiency (e.g., energy per token) of smaller models versus larger ones remains unknown.
  - **What evidence would resolve it:** Hardware-level profiling measuring joules consumed per inference task across the model groups.

## Limitations

- Training data contamination remains a critical unknown, potentially inflating performance scores for models with benchmark overlap
- The 20-model selection excludes proprietary and specialized domain models that could shift performance rankings
- Zero-shot prompting may underestimate capabilities compared to few-shot or instruction-tuned approaches used in production

## Confidence

- **Model Size vs. Performance:** High - Strong empirical support from systematic comparisons across 5 benchmarks
- **Efficiency Trade-offs:** High - VRAM measurements are deterministic and reproducible
- **Multilingual Performance:** Medium - Statistical analysis shows non-significant differences, but sample size limits broader claims
- **Zero-shot Generalizability:** Low - Limited by the assumption that base models benefit equally from zero-shot prompting

## Next Checks

1. **Reproducibility Test:** Run Qwen2.5-Coder 1.5B and 7.0B on HumanEval with fixed seeds (seed=42) to verify pass@1 scores of 0.54 and 0.65 respectively. Monitor for SD/CV stability across 5 independent runs.

2. **Training Data Contamination Check:** Use the Pile Scraper or similar tool to estimate overlap between benchmark task descriptions and model pretraining corpora. Flag any models with >5% token overlap as potentially compromised.

3. **Few-shot Validation:** Re-run Mercury benchmark with 5-shot prompting for top-3 Group 3 models (Qwen2.5-Coder 7.0B, DeepSeek-Coder 6.7B, OpenCodeInterpreter 6.7B). Compare pass@1 gains to zero-shot baseline to quantify practical workflow improvements.