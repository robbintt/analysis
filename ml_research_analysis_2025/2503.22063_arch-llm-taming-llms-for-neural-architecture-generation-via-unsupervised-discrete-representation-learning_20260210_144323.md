---
ver: rpa2
title: 'Arch-LLM: Taming LLMs for Neural Architecture Generation via Unsupervised
  Discrete Representation Learning'
arxiv_id: '2503.22063'
source_url: https://arxiv.org/abs/2503.22063
tags:
- neural
- architectures
- architecture
- search
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating valid and unique
  neural architectures in Neural Architecture Search (NAS) by proposing a novel approach
  called Arch-LLM. The core idea is to use a Vector-Quantized Variational Autoencoder
  (VQ-VAE) to learn a discrete latent space for neural architectures, and then leverage
  a Large Language Model (LLM) to generate sequences of these discrete codes.
---

# Arch-LLM: Taming LLMs for Neural Architecture Generation via Unsupervised Discrete Representation Learning

## Quick Facts
- arXiv ID: 2503.22063
- Source URL: https://arxiv.org/abs/2503.22063
- Authors: Deshani Geethika Poddenige; Sachith Seneviratne; Damith Senanayake; Mahesan Niranjan; PN Suganthan; Saman Halgamuge
- Reference count: 40
- Key outcome: Arch-LLM achieves over 80% improvement in absolute uniqueness on NAS-Bench-101 and over 8% improvement on NAS-Bench-201 by using VQ-VAE to learn discrete latent spaces for neural architectures and fine-tuning an LLM to generate architecture code sequences.

## Executive Summary
This paper addresses the challenge of generating valid and unique neural architectures in Neural Architecture Search (NAS) by proposing a novel approach called Arch-LLM. The core innovation is using a Vector-Quantized Variational Autoencoder (VQ-VAE) to learn a discrete latent space for neural architectures, then leveraging a Large Language Model (LLM) to generate sequences of these discrete codes. This approach avoids the issue of invalid and duplicate architectures that arise from sampling from continuous latent spaces. The method is evaluated on NAS-Bench-101 and NAS-Bench-201 datasets, demonstrating significant improvements in generating valid and unique architectures compared to existing VAE-based methods.

## Method Summary
Arch-LLM combines VQ-VAE with LLM fine-tuning to generate valid neural architectures. First, a VQ-VAE with GIN encoder learns a discrete latent representation of neural architectures from NAS-Bench-101 or NAS-Bench-201 datasets. The VQ-VAE maps architectures to discrete codebook indices rather than continuous vectors. Second, the discrete sequences are fine-tuned using a pre-trained T5 LLM through "generate" and "fill" prompts, treating architecture codes as a language. During generation, temperature scaling controls the trade-off between validity/uniqueness (lower temperature) and novelty (higher temperature). The method is unsupervised, avoiding reliance on architecture accuracy labels during training.

## Key Results
- Achieves over 80% improvement in absolute uniqueness on NAS-Bench-101 compared to existing VAE-based methods
- Demonstrates over 8% improvement in absolute uniqueness on NAS-Bench-201
- Shows competitive results in NAS when using sequence modeling algorithm for architecture mutation
- Maintains high reconstruction accuracy (>98%) while improving generation quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping neural architectures to a discrete latent space (via VQ-VAE) rather than a continuous space (via VAE) improves the validity and uniqueness of generated samples.
- **Mechanism:** Standard VAEs force discrete graph structures into a continuous Gaussian distribution; sampling from this continuous space often lands in "voids" that decode to invalid or duplicate graphs. VQ-VAE constrains the latent space to a learned codebook, ensuring every latent point corresponds to a discrete structural prototype.
- **Core assumption:** The underlying distribution of neural architectures is inherently discrete and multi-modal, which aligns better with a categorical prior than a continuous Gaussian one.
- **Evidence anchors:**
  - [abstract] "sampling from these [continuous] spaces often leads to a high percentage of invalid or duplicate neural architectures."
  - [section 3.1.2] "The discrete latent variable Z... is calculated by nearest neighbourhood lookup using the codebook vectors."
  - [corpus] Weak direct evidence; neighboring papers (e.g., RevoNAD) discuss discrete search challenges but do not validate the VQ-VAE specific mechanism.
- **Break condition:** If the codebook size is too small or collapsed (mode collapse), the discrete space will fail to represent the diversity of the search space, reducing uniqueness to mere memorization.

### Mechanism 2
- **Claim:** Fine-tuning a Large Language Model (LLM) on discrete architecture codes transfers general sequence modeling capabilities to the NAS domain, overcoming data scarcity.
- **Mechanism:** Neural architecture datasets (e.g., NAS-Bench-101 with 420k samples) are insufficient to train a high-variance generative model (like a Transformer) from scratch. The authors "transplant" a pre-trained T5 model, treating architecture codes as a language, leveraging the LLM's pre-existing ability to model sequential dependencies.
- **Core assumption:** The sequential dependencies in codebook indices share structural regularities (syntax) with natural language that the LLM can adapt to via fine-tuning.
- **Evidence anchors:**
  - [section 1] "...we adopt a domain transplant approach... we leverage Large Language Models (LLMs), pretrained on data-abundant domains..."
  - [section 3.2] "We represent each architecture as a numerical sequence s and then convert s into a sentence-like format."
  - [corpus] Weak evidence; corpus papers discuss LLMs for code/text (UCoder) but not specifically numerical sequence transfer for graphs.
- **Break condition:** If the sequence of codebook indices lacks autoregressive structure (i.e., the order is arbitrary), the LLM will fail to learn a coherent prior, generating invalid sequences.

### Mechanism 3
- **Claim:** Temperature scaling during LLM generation allows explicit control over the trade-off between generating valid, unique architectures (Absolute Uniqueness) and novel architectures (Absolute Novelty).
- **Mechanism:** Lower temperatures (e.g., 0.7) restrict sampling to high-probability latent codes, resulting in valid but conservative (seen) architectures. Higher temperatures (e.g., 1.8) encourage sampling from lower-probability tails of the learned distribution, increasing novelty at the risk of validity.
- **Core assumption:** The LLM has learned a smooth probability distribution over the discrete latent space where validity correlates with probability density.
- **Evidence anchors:**
  - [section 4.3] "Arch-LLM t=0.7... is optimized for better Absolute Uniqueness... Arch-LLM t=1.8... is optimized for better Absolute Novelty."
  - [figure 2] Illustrates the inverse relationship between Validity/Uniqueness and Novelty as temperature increases.
  - [corpus] No specific corpus evidence for this control mechanism.
- **Break condition:** If the LLM overfits to the training sequences, even high temperatures may fail to induce novelty, producing only memorized sequences with noise.

## Foundational Learning

- **Concept: Vector Quantization (VQ)**
  - **Why needed here:** This is the core bottleneck replacing the standard VAE. Without understanding how continuous vectors map to discrete codebooks (and how gradients pass via the straight-through estimator), the encoder/decoder training logic is opaque.
  - **Quick check question:** How does the model backpropagate loss through the non-differentiable quantization step (argmin)?

- **Concept: Graph Isomorphism Networks (GIN)**
  - **Why needed here:** The encoder uses GINs to process DAGs. You need to understand how GINs aggregate node features to realize why the model captures topological properties of the neural architectures.
  - **Quick check question:** Why is the GIN capable of distinguishing different graph structures better than standard GCNs?

- **Concept: Autoregressive Generation**
  - **Why needed here:** The LLM (T5) is treated as an autoregressive model of architecture "sentences." Understanding the sequential dependency is key to grasping why "fill-in-the-blank" prompts work for architecture mutation.
  - **Quick check question:** Does the generation order of the latent codes matter for the final decoded architecture?

## Architecture Onboarding

- **Component map:** Adjacency Matrix $\tilde{A}$ & Operation Matrix $X$ -> GIN Encoder -> Continuous vector $Z_e$ -> Vector Quantizer (Codebook lookup) -> Discrete index sequence $Z$ and vector $Z_q$ -> Decoder -> Reconstructed graph

- **Critical path:**
  1. Verify **Reconstruction Accuracy** of the VQ-VAE first (must be >98% as per Table 1). If this fails, the codebook is insufficient.
  2. Convert latent indices to text strings.
  3. Fine-tune T5 using "generate" and "fill" prompts.

- **Design tradeoffs:**
  - **Codebook Size ($K$):** Larger $K$ captures more detail but increases LLM sequence vocabulary and sparsity. Paper uses $K=512$.
  - **Temperature ($t$):** Low $t$ for NAS search refinement (exploitation); High $t$ for exploring diverse architectures (exploration).
  - **Dataset Size:** The method relies on the VQ-VAE learning a good reconstruction; if the search space is massive and under-sampled, the codebook will be sparse.

- **Failure signatures:**
  - **Random Strings:** LLM generates text that doesn't map to codebook indices (handled by post-processing).
  - **Invalid Graphs:** Decoder receives a valid sequence but produces a graph that violates DAG constraints (validity check drops to ~46% at high temps).
  - **Mode Collapse:** VQ-VAE codebook usage is uneven; only a few codes are active.

- **First 3 experiments:**
  1. **Reconstruction Test:** Train VQ-VAE and measure reconstruction accuracy on a hold-out set. Target >98%.
  2. **Validity Test:** Fine-tune LLM, generate 1,000 samples at temperature 1.0, and measure the percentage that decodes into valid graphs.
  3. **Search Dry Run:** Run Algorithm 1 for 5 iterations on NAS-Bench-101 to verify the "fill" prompt successfully mutates architectures without crashing the pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can reinforcement learning (RL) fine-tuning be integrated into Arch-LLM to optimize for task-specific performance (e.g., accuracy) without compromising the structural validity of the generated architectures?
- **Basis in paper:** [explicit] "However, reinforcement learning-based fine-tuning could be applied to further optimize Arch-LLM for NAS." (Page 8)
- **Why unresolved:** The authors train the model in an unsupervised manner, explicitly avoiding parameter updates based on architecture accuracies to focus on representation learning, leaving the potential for RL-based optimization unexplored.
- **What evidence would resolve it:** A comparative study showing that an RL-finetuned Arch-LLM achieves higher test accuracy on NAS benchmarks than the unsupervised baseline while maintaining comparable validity rates.

### Open Question 2
- **Question:** How can the generation of invalid samples (random strings or incorrect sequence lengths) be mitigated inherently within the model, rather than relying on post-processing filters?
- **Basis in paper:** [explicit] "...fine-tuning LLMs for numerical sequences can increase randomness, sometimes leading to invalid samples... Addressing this requires additional post-processing steps to filter out such data." (Page 8)
- **Why unresolved:** The paper identifies the generation of invalid sequences as a limitation of the current fine-tuning approach but only suggests external filtering as the solution.
- **What evidence would resolve it:** A modification to the decoding strategy or training objective that results in 100% structural validity without discarding generated samples.

### Open Question 3
- **Question:** Does the discrete representation learning approach generalize effectively to search spaces beyond cell-based DAGs, such as macro-architectures or transformers with variable depth?
- **Basis in paper:** [inferred] The method is evaluated exclusively on NAS-Bench-101 and NAS-Bench-201, which define architectures as small, fixed-size directed acyclic graphs (Page 5)
- **Why unresolved:** The VQ-VAE encoder relies on graph isomorphism networks tailored for the specific constraints of these cell-based benchmarks; it is unclear if the discrete codebook can capture the complexity of larger, variable-length topologies.
- **What evidence would resolve it:** Successful application of Arch-LLM to a search space involving variable network depth or macro-structure, demonstrating comparable uniqueness and validity metrics.

## Limitations

- Missing key hyperparameter details for both VQ-VAE training (learning rate, batch size, optimizer, GIN architecture depth) and LLM fine-tuning (epochs, learning rate, batch size, T5 variant)
- Limited evaluation scope to only two fixed benchmark datasets (NAS-Bench-101 and NAS-Bench-201) with small, fixed-topology search spaces
- Reliance on post-processing filters to handle invalid samples generated by the LLM fine-tuning process

## Confidence

**High Confidence:** The core mechanism of using VQ-VAE to create discrete latent spaces for neural architectures is well-established in the literature and the paper provides clear implementation details for this component. The observed improvements in validity and uniqueness over continuous VAE approaches are consistent with theoretical expectations about discrete vs continuous sampling.

**Medium Confidence:** The LLM-based generation approach shows promising results on benchmark datasets, but the lack of detailed training hyperparameters and the limited scope of evaluation (only two fixed benchmark datasets) prevent stronger confidence. The temperature-based control mechanism is intuitive but requires more systematic exploration across different search spaces.

**Low Confidence:** Claims about the method's general applicability to arbitrary neural architecture search problems, particularly those with variable topologies or larger search spaces, lack empirical support. The paper doesn't address computational costs or scalability challenges that would arise in practical applications.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary the VQ-VAE codebook size (K), training epochs, and LLM fine-tuning parameters to determine the robustness of the reported improvements. Document how performance changes with these parameters.

2. **Generalization Test:** Apply Arch-LLM to a more complex NAS benchmark with variable topologies (e.g., DARTS search space) and evaluate whether the method maintains its effectiveness beyond fixed-topology search spaces.

3. **Computational Efficiency Benchmark:** Measure and compare the wall-clock time and memory requirements of Arch-LLM against competing methods across different dataset sizes, particularly focusing on the overhead introduced by the LLM fine-tuning stage.