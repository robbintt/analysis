---
ver: rpa2
title: What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for
  Multi-modal Large Language Models from the Perspective of Graph
arxiv_id: '2501.02268'
source_url: https://arxiv.org/abs/2501.02268
tags:
- tokens
- visual
- g-prune
- token
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of visual redundancy in multimodal
  large language models (MLLMs), where excessive visual tokens lead to high computational
  costs and redundant information. The authors propose a graph-based method called
  G-Prune, which treats visual tokens as nodes in a graph and constructs connections
  based on semantic similarities.
---

# What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph

## Quick Facts
- arXiv ID: 2501.02268
- Source URL: https://arxiv.org/abs/2501.02268
- Authors: Yutao Jiang; Qiong Wu; Wenhao Lin; Wei Yu; Yiyi Zhou
- Reference count: 11
- Key outcome: G-Prune reduces 63.57% FLOPs on VQA2.0 and TextVQA with only 0.95% and 2.34% accuracy drops, respectively

## Executive Summary
This paper addresses visual redundancy in multimodal large language models (MLLMs) by proposing a training-free visual token pruning method called G-Prune. The method constructs a graph where visual tokens are nodes connected based on semantic similarities, then uses iterative information propagation to identify the most representative tokens for each object. The approach can reduce computational costs while maintaining accuracy by selectively preserving tokens that carry the most semantic information.

## Method Summary
G-Prune treats visual tokens as nodes in a graph and constructs connections based on semantic similarities between tokens. An iterative information propagation algorithm is then applied to identify the most representative tokens for each object, which can be from either foreground or background. The method is training-free and operates directly on the visual tokens produced by the vision encoder, making it applicable without requiring model retraining or fine-tuning.

## Key Results
- G-Prune achieves 63.57% FLOPs reduction on VQA2.0 and TextVQA benchmarks
- Accuracy drops are limited to 0.95% and 2.34% on these benchmarks, respectively
- The method outperforms existing pruning approaches in the trade-off between computational efficiency and accuracy retention

## Why This Works (Mechanism)
G-Prune leverages graph theory to model relationships between visual tokens, allowing it to identify redundant information through semantic similarity analysis. The iterative information propagation algorithm effectively distributes importance scores across the graph, ensuring that tokens representing key semantic concepts are preserved while redundant tokens are pruned. This graph-based approach captures global semantic relationships that traditional token pruning methods might miss.

## Foundational Learning
- **Graph-based token representation**: Treating visual tokens as graph nodes allows modeling of semantic relationships between tokens; needed to identify redundancy beyond local token neighborhoods
- **Semantic similarity metrics**: Used to construct edges between tokens; needed to quantify relationships between visual concepts
- **Information propagation algorithms**: Iteratively update token importance scores; needed to capture global semantic importance across the token set
- **Training-free pruning**: Operates without model retraining; needed to make the method applicable to existing MLLM deployments
- **Visual token semantics**: Understanding what each token represents; needed to make intelligent pruning decisions

## Architecture Onboarding

**Component Map**: Vision Encoder -> Visual Tokens -> Graph Construction -> Information Propagation -> Token Selection -> MLLM Input

**Critical Path**: The critical path is Vision Encoder → Graph Construction → Information Propagation → Token Selection, as this sequence determines which tokens reach the MLLM. The vision encoder produces raw tokens, graph construction establishes semantic relationships, information propagation computes importance scores, and token selection determines the final pruned set.

**Design Tradeoffs**: The method trades off between computational savings and information retention. A more aggressive pruning rate yields higher FLOPs reduction but risks accuracy degradation. The graph-based approach trades the overhead of graph construction and propagation for better semantic understanding compared to simple heuristics.

**Failure Signatures**: Potential failures include over-pruning critical tokens leading to accuracy drops, graph construction errors causing incorrect semantic relationships, or information propagation getting stuck in local minima preventing optimal token selection.

**First Experiments**: 1) Ablation study varying the number of graph propagation iterations to find the optimal balance between computation and accuracy. 2) Sensitivity analysis of different semantic similarity thresholds to understand their impact on pruning effectiveness. 3) Qualitative visualization of preserved vs. pruned tokens to validate semantic meaningfulness of the pruning decisions.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to a single MLLM architecture (LLaVA-NeXT) and two specific benchmarks
- Generalizability to other MLLM architectures, visual encoders, and downstream tasks remains uncertain
- Performance on more complex visual scenes or specialized domains beyond standard VQA benchmarks is unclear

## Confidence

**High Confidence**: The core methodology of using graph-based representation and iterative information propagation for token selection is technically sound and well-explained. The reported FLOPs reduction metrics are specific and verifiable.

**Medium Confidence**: The claimed accuracy retention (0.95-2.34% drops) is impressive but requires validation across broader task distributions and model variants. The comparison with existing pruning methods is limited to a small set of baselines.

**Low Confidence**: Claims about identifying "most representative tokens for each object" from both foreground and background need empirical validation through qualitative visualization studies showing which tokens are preserved and why.

## Next Checks
1. Cross-architecture validation: Test G-Prune on at least two additional MLLM architectures (e.g., BLIP-2, Flamingo) to assess generalizability of the 63.57% FLOPs reduction claim.

2. Sensitivity analysis: Systematically evaluate the impact of different semantic similarity metrics (cosine, Euclidean, learned metrics) and threshold values on both computational savings and accuracy retention.

3. Qualitative ablation study: Generate visual explanations showing preserved vs. pruned tokens across diverse image types, validating whether the method indeed identifies semantically meaningful tokens rather than arbitrary reductions.