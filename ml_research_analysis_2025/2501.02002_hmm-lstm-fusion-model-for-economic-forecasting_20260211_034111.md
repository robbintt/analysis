---
ver: rpa2
title: HMM-LSTM Fusion Model for Economic Forecasting
arxiv_id: '2501.02002'
source_url: https://arxiv.org/abs/2501.02002
tags:
- lstm
- data
- economic
- none
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a fusion model combining Hidden Markov Models
  (HMM) and Long Short-Term Memory (LSTM) neural networks for economic forecasting,
  specifically predicting CPI inflation rates. The approach uses HMM to identify hidden
  economic states and their means, then incorporates these as additional features
  into LSTM models to enhance predictive accuracy and interpretability.
---

# HMM-LSTM Fusion Model for Economic Forecasting

## Quick Facts
- **arXiv ID:** 2501.02002
- **Source URL:** https://arxiv.org/abs/2501.02002
- **Authors:** Guhan Sivakumar
- **Reference count:** 12
- **Primary result:** HMM-LSTM fusion model improves CPI inflation prediction R² from 0.519 to 0.803 by incorporating regime-conditional means

## Executive Summary
This paper presents a fusion model combining Hidden Markov Models (HMM) and Long Short-Term Memory (LSTM) neural networks for economic forecasting, specifically predicting CPI inflation rates. The approach uses HMM to identify hidden economic states and their means, then incorporates these as additional features into LSTM models to enhance predictive accuracy and interpretability. The model is trained on 647 monthly observations from 1970-2023, including features like Federal Funds Rate, Unemployment Rate, GDP growth, and others. Results show significant improvement in predictive performance when adding HMM-derived means to the LSTM models, with R² scores increasing from 0.519 to 0.803.

## Method Summary
The method involves three key stages: first, a Gaussian HMM with 4 states is trained on economic indicators to identify hidden regimes using the EM algorithm and Viterbi decoding. Second, the model extracts state-conditional means for each feature, which are then concatenated to the original feature set. Third, these augmented features are fed into a 2-layer LSTM architecture with 50 units per layer, trained on lagged sequences to predict inflation rates. The approach uses temporal cross-validation with 70:30 train-test splits and MinMax scaling, testing both 1-year and 2.5-year forecast horizons.

## Key Results
- R² improves from 0.519 (original features) to 0.803 (features + HMM means)
- 1-year forecasts achieve R² of 0.897 vs 0.807 for original data
- Model successfully predicts 2022 inflation rise and outperforms official March 2024 forecasts
- Integrated Gradients reveals GDP growth and inflation lags as most important features

## Why This Works (Mechanism)

### Mechanism 1: HMM Latent State Identification
- **Claim:** HMM identifies hidden economic regimes that provide structural context for downstream prediction.
- **Mechanism:** A Gaussian HMM with 4 states applies the EM algorithm to estimate transition and emission probabilities, then Viterbi decoding assigns the most likely state sequence. The model outputs both discrete state labels and state-conditional means for each feature.
- **Core assumption:** Economic conditions cluster into discrete latent states that persist over time and exhibit Markovian transitions.
- **Evidence anchors:** 62% accuracy classifying against NBER recessions; state 0 represents economic stability while states 1, 2, and 3 represent economic crisis.

### Mechanism 2: Regime-Conditional Feature Enhancement
- **Claim:** HMM-derived means provide regime-specific expected values that improve LSTM pattern recognition beyond raw observations alone.
- **Mechanism:** For each timestep, the HMM outputs the mean of each feature conditioned on the current state. These means are concatenated to the original feature vector, giving the LSTM explicit information about the prevailing regime's expected behavior.
- **Core assumption:** State-conditional means capture structural information that is not immediately visible in single-timestep raw observations.
- **Evidence anchors:** R² increases from 0.519 (original data) to 0.803 (means data); hidden states alone drop R² to 0.144.

### Mechanism 3: LSTM Long-Range Temporal Integration
- **Claim:** LSTM with sufficient lag depth captures multi-period dependencies and non-linear temporal patterns in inflation dynamics.
- **Mechanism:** LSTM gates (input, forget, output) selectively retain or discard information across 24-120 timesteps; tanh activation transforms cell updates while sigmoid gates control information flow, mitigating vanishing gradients.
- **Core assumption:** Inflation dynamics depend on patterns spanning 2-10+ years of history, requiring longer memory than standard RNNs provide.
- **Evidence anchors:** 24 lags for in-sample, 48 for 1-year forecast, 120 for 2.5-year; outperforms baseline LSTM on same temporal structure.

## Foundational Learning

- **Concept: Hidden Markov Models and State Inference**
  - **Why needed here:** Core to identifying latent economic regimes; requires understanding EM algorithm, Viterbi decoding, and Gaussian emission assumptions.
  - **Quick check question:** Given a 4-state HMM with transition matrix T, how would you detect an absorbed state, and why does the paper use diagonal covariance to prevent this?

- **Concept: LSTM Architecture and Gradient Flow**
  - **Why needed here:** Understanding how gates control information retention explains why HMM features help and why lag length affects performance.
  - **Quick check question:** Why does the LSTM cell use tanh for candidate cell updates but sigmoid for gate activations? What would happen if you reversed these?

- **Concept: Time Series Cross-Validation and Temporal Leakage**
  - **Why needed here:** Economic data requires forward-chaining CV; random shuffling would leak future information and inflate metrics.
  - **Quick check question:** With 647 monthly observations and 5-fold forward-chaining CV, what is the approximate training set size for fold 3 vs fold 5?

## Architecture Onboarding

- **Component map:** Data Pipeline -> HMM Module -> Feature Augmentation -> LSTM Module
- **Critical path:** Train HMM on full dataset → Extract state sequence and per-feature means → Augment original dataset → Create lagged 3D tensors → Apply MinMax scaling → Train LSTM with temporal 70:30 split → Evaluate on test set
- **Design tradeoffs:** 4 states vs 2-3 states (more states capture nuance but reduce interpretability); diagonal vs full covariance (diagonal prevents absorbed states but ignores correlations); Oil price inclusion (excluded due to extreme volatility)
- **Failure signatures:** Absorbed HMM states (transition probability → 1.0 to self); R² drops with hidden states alone (from 0.519 → 0.144); training loss oscillates sharply; 2.5-year forecast highly volatile
- **First 3 experiments:** 1) Replicate baseline comparison: Train LSTM on original 9 features vs LSTM on features + HMM means; 2) Ablate HMM components: Test states only, means only, states + means; 3) Vary state count: Train HMM with 2, 3, 4, 5 states; compare NBER recession alignment and downstream LSTM R²

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Temporal Fusion Transformers (TFT) outperform the HMM-LSTM fusion model regarding accuracy and interpretability for long-term economic forecasting?
- **Basis in paper:** The "Future Work" section suggests investigating the adaptability and scalability of TFTs, which offer "interpretable multivariate forecast[s]," unlike standard LSTMs.
- **Why unresolved:** The paper focuses solely on LSTM architectures; TFTs utilize attention mechanisms that may capture complex temporal patterns differently.
- **What evidence would resolve it:** Comparative benchmarking of TFT against the HMM-LSTM model on the same CPI inflation dataset, analyzing both R² scores and feature importance outputs.

### Open Question 2
- **Question:** How does a Markov Switching Neural Network (integrating a Markov layer directly into the architecture) compare to the feature-augmentation approach used in this study?
- **Basis in paper:** The author suggests that instead of adding HMM features to the dataset, future research could involve "integrating a Markov layer within the neural network architecture to autonomously capture regime switches."
- **Why unresolved:** The current fusion model uses a "loose" coupling where HMM outputs are mere inputs; architectural integration implies a fundamental change in how the network processes regime changes.
- **What evidence would resolve it:** Developing a Markov Switching Neural Network and comparing its ability to capture economic shifts against the HMM-feature LSTM.

### Open Question 3
- **Question:** Does replacing interpolated monthly GDP data with exact quarterly values significantly improve forecasting performance?
- **Basis in paper:** The paper notes that GDP and GDP Investment were interpolated and that "utilizing the exact values would be more appropriate and could potentially lead to better results."
- **Why unresolved:** Interpolation introduces artificial smoothness into high-volatility features, potentially misleading the LSTM's training on economic shocks.
- **What evidence would resolve it:** Re-training the model using non-interpolated quarterly data (or a mixed-frequency approach) and comparing the error rates against the interpolated baseline.

## Limitations

- **Novelty without direct comparison:** The HMM-LSTM fusion approach appears novel with no direct corpus evidence or comparative benchmarks against other fusion methods.
- **State count justification:** 4 states are used without rigorous justification of why this number is optimal versus 2-3 or 5 states.
- **Temporal split sensitivity:** Results depend heavily on the 70:30 temporal split ending at 2008 recession trough, which may not generalize to different training periods.

## Confidence

- **High confidence:** The empirical improvement from 0.519 to 0.803 R² when adding HMM-derived means to LSTM (tested via ablation and temporal validation)
- **Medium confidence:** The mechanism that HMM states provide interpretable economic regimes (62% recession classification accuracy, but limited qualitative validation)
- **Low confidence:** Claims about feature importance from Integrated Gradients analysis (method correctly applied but interpretation requires additional economic validation)

## Next Checks

1. **Ablation study replication:** Systematically test (a) original features only, (b) states only, (c) means only, (d) states + means to confirm means are the critical component and states alone degrade performance
2. **State count sensitivity:** Train HMM with 2, 3, 4, and 5 states; evaluate both NBER recession alignment and downstream LSTM R² to determine if 4 states is optimal or overfit
3. **Temporal robustness:** Apply the trained HMM-LSTM model to out-of-sample periods (e.g., 2024 data) and compare against official forecasts to validate real-world predictive capability beyond in-sample metrics