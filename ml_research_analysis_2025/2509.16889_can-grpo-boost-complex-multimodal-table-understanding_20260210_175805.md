---
ver: rpa2
title: Can GRPO Boost Complex Multimodal Table Understanding?
arxiv_id: '2509.16889'
source_url: https://arxiv.org/abs/2509.16889
tags:
- table
- reasoning
- policy
- grpo
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Table-R1 introduces a three-stage reinforcement learning framework
  to enhance multimodal table understanding. It addresses the challenges of complex
  table structures and intricate logical reasoning by incorporating a warm-up stage
  to boost initial accuracy, PA-GRPO with continuous TEDS rewards for table structure
  recognition, and HC-GRPO with fine-grained residual-step rewards for reasoning.
---

# Can GRPO Boost Complex Multimodal Table Understanding?

## Quick Facts
- **arXiv ID:** 2509.16889
- **Source URL:** https://arxiv.org/abs/2509.16889
- **Reference count:** 13
- **Primary result:** Table-R1 achieves 3.93% improvement over SFT and 16.38% over GRPO on held-in datasets

## Executive Summary
Table-R1 introduces a three-stage reinforcement learning framework to enhance multimodal table understanding, addressing the challenges of complex table structures and intricate logical reasoning. The approach incorporates a warm-up stage to boost initial accuracy, PA-GRPO with continuous TEDS rewards for table structure recognition, and HC-GRPO with fine-grained residual-step rewards for reasoning. Experiments demonstrate significant performance gains across multiple table understanding tasks, with Qwen2-VL-7B achieving notable improvements over both SFT and GRPO baselines.

## Method Summary
Table-R1 employs a three-stage reinforcement learning framework: (1) Warm-up SFT on perception and reasoning datasets to boost initial accuracy, (2) PA-GRPO optimizing continuous TEDS rewards for table structure recognition, and (3) HC-GRPO with residual-step accuracy and format rewards for reasoning. The framework uses unified GRPO loss with clipped surrogate objective and KL penalty, trained on sampled held-in datasets with filtering for image resolution and output length. Key hyperparameters include G=4 rollouts per question, temperature=1.0, batch size=1, 2 epochs, and learning rate 1e-6.

## Key Results
- Qwen2-VL-7B achieves 3.93% improvement over SFT and 16.38% over GRPO on held-in datasets
- 7.72% and 8.79% improvements on held-out datasets compared to SFT and GRPO respectively
- Matches GPT-4o performance on held-in tasks while surpassing larger specific table models like Table-LLaVA 13B

## Why This Works (Mechanism)

### Mechanism 1
GRPO requires moderate initial policy accuracy to enable effective learning dynamics. The variance of binary rewards follows Bernoulli(p), where p is initial accuracy. When accuracy is too low or too high, variance approaches zero, producing weak gradients. Moderate accuracy (~55%) maximizes variance to ~0.25, enabling stronger policy updates.

### Mechanism 2
Continuous TEDS rewards provide denser learning signals than binary correctness rewards for table structure recognition. TEDS computes tree-edit-distance similarity between predicted and gold table structures, producing normalized scores in [0,1]. This allows gradient signals even for partially correct structures, unlike binary rewards.

### Mechanism 3
Residual-step rewards enable finer-grained credit assignment than solution-level rewards. By splitting solutions into hints and completions, and rewarding accuracy on remaining steps, the model receives feedback proportional to its partial progress rather than all-or-nothing outcomes.

## Foundational Learning

- **Concept:** GRPO (Group Relative Policy Optimization)
  - **Why needed here:** Core algorithm underlying PA-GRPO and HC-GRPO; requires understanding how group-based advantage estimation works
  - **Quick check question:** Can you explain why GRPO replaces the critic model with multiple samples from the policy?

- **Concept:** Tree-Edit-Distance Similarity (TEDS)
  - **Why needed here:** Reward function for PA-GRPO; measures structural similarity between table representations
  - **Quick check question:** How would TEDS score a table with correct content but incorrect row/column structure?

- **Concept:** Policy gradient variance and advantage estimation
  - **Why needed here:** Understanding why initial accuracy matters requires grasping how variance affects gradient magnitude
  - **Quick check question:** Why does low reward variance lead to weak policy updates?

## Architecture Onboarding

- **Component map:** Warm-up SFT -> PA-GRPO with TEDS rewards -> HC-GRPO with residual-step rewards
- **Critical path:** Warm-up accuracy → determines GRPO variance → TEDS provides dense perception rewards → HC-GRPO refines reasoning. Failure at any stage cascades; warm-up is most critical (ablation shows -40.4% drop on TabFact without it).
- **Design tradeoffs:** More generations (G) per question improves baseline stability but increases compute (default G=4). More HC splits per solution creates more training data but depends on GPT-4o chain quality. Continuous TEDS rewards may over-optimizing structure at expense of reasoning (mitigated by sequential stages).
- **Failure signatures:** Low initial accuracy (<30%) → minimal GRPO improvement despite training. High TEDS but low accuracy → perception overfitted, reasoning undertrained. Format reward gaming → model outputs correct tags but wrong reasoning.
- **First 3 experiments:** 1) Ablate warm-up: Train GRPO directly on Qwen2-VL-7B without SFT warm-up; expect <10% improvement vs 16.38% with full framework. 2) Vary G (generations per question): Test G=2,4,6 on TabMWP; expect plateau after G=4 with diminishing returns. 3) Swap TEDS for binary reward: Replace PA-GRPO reward with binary correctness; expect lower TEDS scores and downstream reasoning degradation.

## Open Questions the Paper Calls Out

- **Can Table-R1's framework be extended to open-ended table generation tasks such as summarization and description, where rewards are not verifiable via binary correctness?**
  - Basis: Limitations section states current framework focuses on definitive answers, leaving table text generation unexplored
  - Why unresolved: TEDS and binary accuracy rewards rely on structured outputs with ground-truth references; free-form text lacks verifiable, fine-grained reward signals
  - What evidence would resolve it: Demonstrating a modified reward (e.g., semantic similarity or learned reward models) that improves summarization/description quality without ground-truth references

- **Does the observed sensitivity of GRPO to initial policy accuracy generalize to other LVLM families beyond Qwen2-VL?**
  - Basis: Experiments only use Qwen2-VL-2B/7B; whether warm-up necessity holds for other architectures is unstated
  - Why unresolved: Different tokenizers, vision encoders, and pretraining corpora may yield different initial accuracy-variance dynamics
  - What evidence would resolve it: Systematic evaluation across diverse LVLM families (e.g., DeepSeek-VL2, LLaVA) correlating initial accuracy with GRPO gains

- **Would replacing binary accuracy/format rewards in HC-GRPO with continuous, step-level validity scores improve reasoning performance?**
  - Basis: Limitations note HC-GRPO uses coarse binary rewards and suggest richer signals could yield more nuanced training
  - Why unresolved: Binary rewards provide sparse feedback; continuous step-level scores may offer more stable gradients but require annotation or learned process reward models
  - What evidence would resolve it: Ablation comparing HC-GRPO with binary vs. continuous step-level rewards on held-out reasoning tasks

## Limitations

- The framework relies heavily on the assumption that initial policy accuracy critically determines GRPO effectiveness
- TEDS reward depends on quality of table structure parsing which wasn't extensively validated across diverse table formats
- Residual-step reward design assumes GPT-4o reliably decomposes solutions, but decomposition quality wasn't thoroughly evaluated

## Confidence

- **High Confidence:** The empirical results showing Table-R1 outperforming SFT and GRPO baselines across multiple datasets
- **Medium Confidence:** The theoretical explanation of warm-up importance based on reward variance mechanics
- **Medium Confidence:** The continuous TEDS reward providing better learning signals than binary rewards
- **Low Confidence:** The specific mechanisms of residual-step reward design, as no direct corpus evidence validates this novel approach

## Next Checks

1. **Cross-domain generalization test:** Evaluate Table-R1 on completely unseen table domains (e.g., medical tables, financial reports) to verify robustness beyond the tested datasets
2. **Ablation of TEDS dependency:** Replace TEDS reward with an alternative continuous reward (e.g., cell-level similarity) to confirm the benefit comes from continuity rather than TEDS specifically
3. **Scaling behavior analysis:** Test Table-R1 with different model sizes (1B, 13B) to determine if the three-stage framework provides proportional benefits across scales