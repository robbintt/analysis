---
ver: rpa2
title: Advanced Black-Box Tuning of Large Language Models with Limited API Calls
arxiv_id: '2511.10210'
source_url: https://arxiv.org/abs/2511.10210
tags:
- proxy
- training
- black-box
- tuning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large language models
  in black-box settings where parameter access is unavailable. The proposed method
  employs a Gaussian Process surrogate trained on a minimal yet informative subset
  of data to approximate the foundation model's output logits.
---

# Advanced Black-Box Tuning of Large Language Models with Limited API Calls

## Quick Facts
- arXiv ID: 2511.10210
- Source URL: https://arxiv.org/abs/2511.10210
- Reference count: 37
- Key outcome: Improves pre-trained model accuracy from 55.92% to 86.85% while using only 1.38% of API calls required by previous methods

## Executive Summary
This paper presents a novel approach for adapting large language models in black-box settings where direct parameter access is unavailable. The method employs a Gaussian Process surrogate trained on a minimal yet informative subset of data to approximate the foundation model's output logits, which then guides the training of a smaller proxy model. By strategically selecting diverse training samples and using uncertainty-based gating to minimize expensive API calls, the approach achieves state-of-the-art performance while dramatically reducing computational costs. Extensive experiments demonstrate the method's effectiveness across 11 NLP benchmarks with various model pairs.

## Method Summary
The method uses a two-phase approach: first, it filters training data to identify a diverse subset using both input embeddings and proxy model logits, then queries the black-box model only for these selected pairs to create LogitMap Pairs. A Gaussian Process surrogate is trained on these pairs to approximate the foundation model's logit behavior. The proxy model is then fine-tuned using GP predictions, with an uncertainty-based gate that falls back to the black-box model when the GP's predictive variance exceeds a threshold. During inference, the tuned proxy model's logits are combined with the difference between black-box and untuned proxy logits to produce final predictions.

## Key Results
- Achieves 86.85% accuracy on downstream tasks, improving from 55.92% baseline
- Uses only 1.38% of API calls compared to previous methods
- Outperforms state-of-the-art black-box tuning approaches across 11 NLP benchmarks
- Demonstrates robust performance with minimal API calls even in extreme data scarcity scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A Gaussian Process can approximate the logit behavior of a large black-box foundation model using only a sparse subset of queries.
- **Mechanism:** The GP models the functional relationship between input embeddings and output logits, allowing the small proxy model to train against the foundation model's knowledge without paying API costs for every training step.
- **Core assumption:** The mapping from input embedding to logit space is sufficiently smooth to be modeled by a kernel method.
- **Evidence anchors:** Abstract states GP approximates foundation model outputs; Equation 4 defines GP predictive mean; neighbor paper supports GP efficacy as LLM surrogates.
- **Break condition:** If the foundation model's decision boundary is highly irregular relative to the proxy's embedding distance, the GP will fail to generalize.

### Mechanism 2
- **Claim:** Filtering training candidates via Euclidean distance in both input and output spaces maximizes information density.
- **Mechanism:** Algorithm 1 rejects data points that are too close to existing candidates in both embedding space and proxy logit space, prioritizing diverse behaviors.
- **Core assumption:** Diversity in the small proxy's output logits serves as a proxy for diversity in the large model's behavior.
- **Evidence anchors:** Abstract mentions querying foundation model on minimal but highly informative subset; Section describes Algorithm 1's diversity filtering.
- **Break condition:** If the small proxy model is too weak to distinguish between distinct inputs, filtering will fail to select informative samples.

### Mechanism 3
- **Claim:** GP predictive variance acts as a reliable uncertainty score, gating expensive API calls.
- **Mechanism:** During proxy training, the system checks GP variance and queries the actual foundation model only when variance exceeds a threshold.
- **Core assumption:** High variance in GP posterior correlates with high error in logit approximation.
- **Evidence anchors:** Abstract mentions reducing need for direct queries; Equation 6 defines gated supervision based on variance.
- **Break condition:** If GP is overconfident (low variance but high error) on specific failure modes, the gate will fail to trigger.

## Foundational Learning

- **Concept: Gaussian Processes (GPs)**
  - **Why needed here:** This is the core engine of the method, providing both predictions and uncertainty estimates.
  - **Quick check question:** If a GP has high variance for a specific input, does that mean the model is confident or uncertain in its prediction?

- **Concept: Logit Arithmetic (Proxy Tuning)**
  - **Why needed here:** The inference phase relies on manipulating pre-softmax logits using the difference between tuned and untuned proxy models.
  - **Quick check question:** In Equation 7, what happens to the final output if the tuned proxy and the black-box model agree perfectly?

- **Concept: Active Learning / Core-set Selection**
  - **Why needed here:** Understanding that not all data points are equally valuable for training is key to grasping why the filtering algorithm works.
  - **Quick check question:** Why might training on a random 1% subset yield worse results than training on a filtered 1% subset selected via diversity metrics?

## Architecture Onboarding

- **Component map:** Large Black-box (M_l) <- API -> GP Surrogate (M_gp) <- embedding/logit mapping <- Small White-box (M_s)
- **Critical path:**
  1. Phase 1: Use Algorithm 1 with frozen proxy M⁻_s to identify diverse candidates → Query M_l to create LogitMap Pairs
  2. Phase 2: Fit GP model on LogitMap Pairs
  3. Phase 3: Train M⁺_s, checking GP variance and falling back to M_l when high
  4. Phase 4: Combine M⁺_s, M⁻_s, and M_l logits using Equation 7

- **Design tradeoffs:**
  - API Budget vs. Accuracy: Lowering variance threshold increases API calls but improves supervision quality
  - GP Complexity vs. Scale: O(N³) complexity limits scaling beyond ~2k-5k samples
  - Alpha (α) Weight: High α increases reliance on foundation model but may propagate its noise

- **Failure signatures:**
  - Numerical Instability: GP training produces NaN predictions on large datasets (>3K samples)
  - Unimproved Performance: Loose filtering thresholds cause GP to overfit to redundant data
  - Overfitting: Low proxy model capacity prevents matching complex GP logit targets

- **First 3 experiments:**
  1. Baseline vs. Filtered: Compare GP-random vs. GP-filter performance on single dataset
  2. Uncertainty Threshold Sweep: Vary gate threshold θ to plot API Calls vs. Accuracy curve
  3. Alpha (α) Sensitivity: Test different α values (0.5, 0.8, 1.2) to verify logit weighting robustness

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the text provided. However, several implicit questions arise from the methodology and results, particularly around the theoretical understanding of why the GP surrogate remains effective under extreme sparsity and how the approach might generalize to open-ended generation tasks.

## Limitations

- GP training complexity (O(N³)) severely limits scalability, causing numerical instability and NaN predictions when exceeding ~2k-3k samples
- Method's dependence on proxy model's embedding space alignment with black-box model is a critical unverified assumption
- Variance-based gating mechanism may fail if GP produces high-confidence but inaccurate predictions on certain failure modes

## Confidence

- **High Confidence:** Overall framework design (GP surrogate + proxy tuning + uncertainty gating) is coherent and mathematically well-formulated
- **Medium Confidence:** Data filtering algorithm appears theoretically sound but depends on proxy model's ability to produce meaningful output diversity
- **Low Confidence:** Scalability claims are questionable given O(N³) complexity limitation; stability under minimal API calls may not generalize across diverse architectures

## Next Checks

1. **Ablation on GP Kernel Choice:** Systematically test multiple kernel functions (RBF, Matern, Rational Quadratic) and hyperparameter optimization methods to determine generalizability.

2. **Scaling Experiment with Controlled N:** Deliberately scale LogitMap Pair count from 100 to 5000 samples while measuring computational runtime and prediction accuracy.

3. **Proxy Model Capacity Sensitivity:** Vary proxy model size (0.5B, 1B, 3B parameters) while keeping black-box constant to determine efficiency claims' dependence on proxy capability.