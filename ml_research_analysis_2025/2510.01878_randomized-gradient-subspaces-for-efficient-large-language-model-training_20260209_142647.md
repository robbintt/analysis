---
ver: rpa2
title: Randomized Gradient Subspaces for Efficient Large Language Model Training
arxiv_id: '2510.01878'
source_url: https://arxiv.org/abs/2510.01878
tags:
- subspace
- gradient
- training
- low-rank
- subspaces
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GrassWalk and GrassJump, two randomized algorithms
  that improve memory-efficient training of large language models by projecting gradients
  into low-dimensional subspaces. The key insight is that while gradients evolve in
  a low-rank subspace, this subspace becomes less dominant over time, especially in
  deeper layers, and resides in near-flat curvature.
---

# Randomized Gradient Subspaces for Efficient Large Language Model Training

## Quick Facts
- arXiv ID: 2510.01878
- Source URL: https://arxiv.org/abs/2510.01878
- Reference count: 13
- LLaMA-1B achieves 3.86-3.87 eval loss, LLaMA-7B achieves 4.27-4.37 eval loss

## Executive Summary
This paper introduces GrassWalk and GrassJump, two randomized algorithms for memory-efficient training of large language models. The key insight is that while gradients evolve in a low-rank subspace, this subspace becomes less dominant over time, especially in deeper layers, and resides in near-flat curvature. To address this, GrassWalk performs random walks on the Grassmannian manifold, while GrassJump employs random projections to update the subspace. Both methods adapt optimizer states to subspace changes and recover information lost during projection. Experiments on LLaMA-1B and LLaMA-7B pretraining show that these methods achieve superior performance compared to strong baselines while maintaining GaLore-level memory efficiency and faster convergence.

## Method Summary
The method projects gradients into low-dimensional subspaces using either GrassWalk (random walk on Grassmannian manifold) or GrassJump (random projections). When the subspace basis updates, the optimizer states (Adam moments) are rotated to align with the new basis. The approach also recovers the residual gradient component orthogonal to the low-rank subspace using a scaling mechanism. This allows for memory-efficient training while maintaining gradient signal and optimizer state consistency. The methods achieve memory complexity of O(mr + 2nr) where r is the rank and n is the model size.

## Key Results
- GrassWalk and GrassJump achieve evaluation losses of 3.86-3.87 on LLaMA-1B and 4.27-4.37 on LLaMA-7B
- Methods maintain GaLore-level memory efficiency (O(mr + 2nr) complexity)
- Faster convergence compared to strong baselines
- Superior performance on both LLaMA-1B and LLaMA-7B architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Randomized subspace exploration prevents the optimizer from overfitting to a potentially stale or noisy low-rank basis, particularly as gradient dynamics shift in later training stages.
- **Mechanism:** GrassWalk performs a random walk on the Grassmannian manifold and GrassJump uses fully random projections, injecting stochasticity into the optimization path.
- **Core assumption:** The optimization landscape allows for productive exploration via random steps without destabilizing convergence.
- **Evidence anchors:** [abstract] "We find that while a small subspace captures most gradient energy... the influence of the core subspace diminishes over time... gradient space exhibits near-flat curvature."
- **Break condition:** If the curvature were sharp or the gradient signal strictly confined to a static low-rank basis, random jumps would degrade performance.

### Mechanism 2
- **Claim:** Adapting optimizer states to align with changing subspace bases is strictly required to maintain the validity of momentum history when the projection matrix changes.
- **Mechanism:** When the subspace basis $S_t$ updates to $S_{t+1}$, Adam moments are rotated into the new basis using the transformation $(S_{t+1}^T S_t)$.
- **Core assumption:** Adam's first and second moments can be approximated as statistical estimators that survive linear projection transformations.
- **Evidence anchors:** [abstract] "...adapt the optimizer states to subspace changes..."
- **Break condition:** If the optimizer states are reset or improperly projected during a subspace update, the optimizer loses historical gradient information.

### Mechanism 3
- **Claim:** Recovering the "residual" gradient using scaling factors from the dominant subspace restores lost training signal without incurring full-rank memory costs.
- **Mechanism:** The Recovery Scaling technique reapplies the residual gradient to the weight update, scaled by the ratio of the optimizer's output to the raw low-rank gradient.
- **Core assumption:** The magnitude of the update required in the bulk (residual) subspace correlates with the magnitude computed for the dominant subspace.
- **Evidence anchors:** [abstract] "...recover information lost during projection."
- **Break condition:** If the scaling ratio diverges significantly, the recovery mechanism could inject noise or excessive magnitude into the update.

## Foundational Learning

- **Concept:** **Grassmannian Manifold ($Gr(r, n)$)**
  - **Why needed here:** The paper formulates subspace updates not as matrix operations but as movements (geodesics/walks) on a geometric manifold of all possible $r$-dimensional subspaces.
  - **Quick check question:** Can you explain why a "random walk" on this manifold is different from simply adding a random matrix to the current projection matrix?

- **Concept:** **Adam Optimizer States ($M_t, V_t$)**
  - **Why needed here:** The core contribution involves modifying how Adam updates these states when the coordinate system (subspace) changes.
  - **Quick check question:** What happens to the first moment ($M_t$) if you change the basis of the gradients but fail to rotate $M_t$ accordingly?

- **Concept:** **SVD & Low-Rank Approximation**
  - **Why needed here:** The paper contrasts its method against SVD-based approaches and uses SVD for initialization and theoretical analysis of gradient energy.
  - **Quick check question:** Why might SVD be unstable or "noisy" in the later stages of LLM training compared to a randomized projection?

## Architecture Onboarding

- **Component map:** Input: Gradient Matrix $G_t$ -> Subspace Projector (GrassWalk/GrassJump) -> Adaptive Optimizer (AO) -> Recovery Scaler (RS) -> Weight Updater
- **Critical path:** The subspace update interval ($T$) and the rotation of moments (Eq. 7 & 8)
- **Design tradeoffs:** GrassWalk offers smoother transitions but requires exponential map calculation; GrassJump is simpler but more aggressive in discarding previous subspace information
- **Failure signatures:** Loss instability (AO misalignment), stagnation (too small rank or disabled RS), exploding gradients (growth-rate limiter $\zeta$ issues)
- **First 3 experiments:**
  1. Ablation on AO/RS: Run LLaMA-1B pretraining with (a) No adaptation, (b) Adaptation only, (c) Adaptation + Recovery Scaling
  2. Update Frequency Sweep: Test subspace update intervals ($T$) to find the sweet spot
  3. Scale Test (1B vs 7B): Verify GrassJump outperforms GrassWalk at 7B scale

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Results are primarily demonstrated on LLaMA-1B and LLaMA-7B models; generalizability to other architectures remains untested
- Paper does not explore the impact of different update frequencies beyond the reported value of 100 steps
- Computational overhead from randomized SVD and QR decompositions is not thoroughly benchmarked against other low-rank methods

## Confidence
- **Core Mechanisms (AO/RS):** High - supported by ablation studies and mathematical formulation
- **Generalizability to Larger Models:** Medium - limited experimental validation beyond 7B parameters
- **Computational Efficiency Claims:** Medium - memory efficiency demonstrated but computational overhead not fully characterized

## Next Checks
1. Test the methods on a diverse set of LLM architectures (e.g., GPT-NeoX, OPT) to assess generalizability
2. Conduct a systematic ablation study on the subspace update interval ($T$) to determine optimal scheduling
3. Benchmark the computational overhead of randomized SVD/QR steps against other low-rank methods (e.g., GaLore, LoRA) to quantify the trade-off between memory and compute