---
ver: rpa2
title: 'Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive
  Conversations'
arxiv_id: '2509.10184'
source_url: https://arxiv.org/abs/2509.10184
tags:
- emotional
- responses
- positivity
- severe
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines incongruent positivity\u2014well-intentioned\
  \ but emotionally misaligned support\u2014in both human and LLM-generated conversations.\
  \ Researchers categorized responses into dismissive, minimizing, and unrealistic\
  \ positivity types, and analyzed how reframing strategies like growth, impermanence,\
  \ and relatability vary by emotional intensity (mild vs."
---

# Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations

## Quick Facts
- arXiv ID: 2509.10184
- Source URL: https://arxiv.org/abs/2509.10184
- Reference count: 13
- Key outcome: LLMs generate more dismissive responses in high-stakes contexts and rely on generic optimism, while human responses are preferred for severe concerns.

## Executive Summary
This study examines incongruent positivity—well-intentioned but emotionally misaligned support—in both human and LLM-generated conversations. Researchers categorized responses into dismissive, minimizing, and unrealistic positivity types, and analyzed how reframing strategies like growth, impermanence, and relatability vary by emotional intensity (mild vs. severe concerns). Using a new annotated dataset of Reddit dialogues, they found LLMs produce more dismissive responses in high-stakes contexts and rely on generic optimism. A weakly supervised multi-label ensemble (DeBERTa + MentalBERT) was developed to detect these miscalibrations, achieving improved classification across all positivity types. Fine-tuning LLMs on emotion reactions reduced some misaligned responses but increased minimizing language. Human responses were preferred in severe concerns, while LLMs were favored in mild contexts. The work highlights the need for context-aware, emotionally aligned support in conversational AI.

## Method Summary
The study collected 1,490 Reddit conversation turns from r/stress, r/grief, r/situationshipsadvice, and r/advice, stratified by concern level (379 severe + 366 mild dialogues). Responses were annotated with four misguided positivity types (Dismissive, Minimizing, Unrealistic, None) and five reframing strategies (Growth, Impermanence, Optimism, Thankfulness, Relatability). A weakly supervised ensemble of DeBERTa-v3-base and MentalBERT-base classifiers with focal loss was trained, then used to label fine-tuned responses. LLMs (Mistral-7B-v0.1, LLaMA-3.2-3B-Instruct) were fine-tuned on emotion reactions data (1,047 weak + 1,047 strong seeker-response pairs) using QLoRA. Performance was evaluated with macro-F1, McNemar tests for model comparison, and Chi-squared tests for distributional analysis.

## Key Results
- LLMs produce more dismissive responses in severe contexts (15.4%) compared to human responses (7.0%)
- Fine-tuning on strong emotion reactions increases minimizing language (Mistral-Mild: 62.1%, LLaMA-Severe: 53.4%)
- Human responses preferred in severe concerns (50.1% vs. 27.7% for LLMs), while LLMs favored in mild contexts (65.8% vs. 26.1%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs generate higher rates of dismissive responses in severe emotional contexts (e.g., grief, anxiety) compared to human responders.
- **Mechanism:** LLMs lack contextual emotional calibration—when distress intensity exceeds training distribution patterns, models default to generic positive reframing strategies (impermanence, optimism) that invalidate user emotional needs.
- **Core assumption:** Assumes dismissiveness arises from misalignment between response positivity and situational severity rather than intent.
- **Evidence anchors:**
  - [abstract] "LLMs produce more dismissive responses in high-stakes contexts and rely on generic optimism."
  - [section: Results] "LLM-generated responses within the Severe Concerns context show higher rate of dismissive positivity (15.4%) compared to human responses (7.0%)."
  - [corpus] Related work (HEART benchmark, Emotional Supporters paper) confirms LLMs struggle with multi-strategy emotional support but doesn't directly address severity calibration.
- **Break condition:** If fine-tuning on emotion-labeled data eliminated dismissiveness in severe contexts, or if annotation agreement for severe concerns was high (it was only fair: κ=0.28-0.35).

### Mechanism 2
- **Claim:** Fine-tuning on strong emotion reactions reduces unrealistic optimism but increases minimizing language.
- **Mechanism:** Exposure to emotionally intense training examples shifts model behavior toward surface-level affirmations and generalized reassurance—appearing supportive while avoiding deep engagement with distress content.
- **Core assumption:** Assumes the shift represents learned behavior from training data patterns, not inherent model limitation.
- **Evidence anchors:**
  - [section: Results] "Fine-tuning on strong emotion reactions resulted in higher rates of Minimizing language... Mistral-Mild concerns cases (62.1%) and LLaMA-Severe concerns cases (53.4%)."
  - [section: Results] "Under weak emotion training, models produced the highest proportion of neutral or emotionally congruent responses."
  - [corpus] No direct corpus evidence; related work focuses on empathy detection rather than fine-tuning side effects.
- **Break condition:** If weak-emotion fine-tuning produced worse alignment than strong-emotion, or if minimizing language decreased with larger fine-tuning datasets.

### Mechanism 3
- **Claim:** Human preference for emotional support varies by concern severity—humans preferred in severe contexts (50.1%), LLMs in mild contexts (65.8%).
- **Mechanism:** Human responders more effectively employ relatability strategies (sharing similar experiences) in high-stakes situations, creating emotional connection that LLMs rarely replicate.
- **Core assumption:** Assumes preference reflects perceived emotional alignment rather than response length, style, or other confounds.
- **Evidence anchors:**
  - [abstract] "Human responses were preferred in severe concerns, while LLMs were favored in mild contexts."
  - [section: Results] "Relatability was more common in human responses to severe concerns... humans' tendency to establish an emotional connection in more distressing situations."
  - [corpus] HEART benchmark similarly finds humans outperform LLMs on interpersonal support skills but doesn't segment by severity.
- **Break condition:** If LLMs matched human performance in severe contexts after relatability-specific fine-tuning.

## Foundational Learning

- **Concept: Multi-label classification with class imbalance**
  - **Why needed here:** Misguided positivity categories (Dismissive, Minimizing, Unrealistic, None) are non-exclusive and unevenly distributed—focal loss and per-label threshold tuning are required.
  - **Quick check question:** Can you explain why macro-F1 is preferred over accuracy when detecting rare labels like "Unrealistic" (occurring <5%)?

- **Concept: Weak supervision for annotation**
  - **Why needed here:** The study uses ensemble predictions (DeBERTa + MentalBERT) to label fine-tuned responses when gold annotations are unavailable.
  - **Quick check question:** How would you validate weak labels before using them for downstream evaluation?

- **Concept: Emotional intensity calibration**
  - **Why needed here:** The core problem is mismatch between user distress level and response tone—understanding calibration beyond sentiment polarity is essential.
  - **Quick check question:** What's the difference between positive sentiment and congruent positivity in a grief context?

## Architecture Onboarding

- **Component map:** Reddit API → Data collection (r/stress, r/grief, r/advice) → Annotation (Labelbox) → Gold dataset (1,490 turns, multi-label) → Classification models: Baselines: SVM, LR, BiLSTM → Transformers: BERT, RoBERTa, DeBERTa, MentalBERT → LLM classifiers: Mistral-7B (QLoRA) → Ensemble (DeBERTa + MentalBERT) → Weak supervision labels → Fine-tuning targets: Mistral-7B-v0.1, LLaMA-3.2-3B-Instruct → Training data: Sharma et al. (2020) emotion reactions

- **Critical path:**
  1. Data stratification by concern level (Mild vs. Severe) before annotation
  2. Multi-label annotation with inter-annotator agreement tracking
  3. Ensemble training with focal loss + per-label threshold tuning
  4. Weak supervision pipeline for scaling labels to fine-tuning data

- **Design tradeoffs:**
  - **Transformer choice:** MentalBERT captures mental health domain language but may overfit to clinical patterns; DeBERTa provides general robustness. Ensemble balances both.
  - **Annotation depth vs. cost:** Gold labels limited to 1,490 turns; weak supervision scales but introduces label noise (confirmed by confidence score variance in Figure 1).
  - **Fine-tuning data:** Strong emotion data improves some alignment but increases minimizing; weak emotion data produces more neutral responses.

- **Failure signatures:**
  - Low confidence scores on grief-related responses (IQR wider in Figure 1) → ensemble uncertainty in severe contexts
  - Fair inter-annotator agreement on severe concerns (κ≈0.28-0.35) → annotation ambiguity in high-stakes language
  - Persistent minimizing language after fine-tuning → surface-level optimism isn't resolved by emotion exposure alone

- **First 3 experiments:**
  1. **Reproduce ensemble baseline:** Train DeBERTa-v3-base and MentalBERT-base classifiers independently with focal loss, sigmoid activation for multi-label output; apply per-label threshold tuning on validation set; evaluate with macro-F1 and run McNemar tests comparing individual models vs. ensemble
  2. **Ablate training data:** Fine-tune LLaMA-3.2-3B separately on weak vs. strong emotion subsets, quantify minimizing language shift using ensemble weak labels
  3. **Severity-stratified evaluation:** Split test set by Mild/Severe, measure performance gap—expect higher false positive rates on Severe "Dismissive" detection given annotation ambiguity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** When do positive reframing strategies (growth, impermanence, optimism, relatability) transition from being supportive to becoming misguided positivity?
- **Basis in paper:** [explicit] The authors ask "When do positive reframing strategies become misguided positivity rather than supportive?" in their RQ1 analysis, noting strategies like impermanence are prevalent in severe LLM responses but may not always be appropriate.
- **Why unresolved:** The study shows reframing strategies are used differently by humans and LLMs across concern levels, but does not establish boundary conditions where the same strategy helps versus harms emotional alignment.
- **What evidence would resolve it:** Controlled experiments varying the same reframing strategy across different emotional contexts and measuring perceived support, or longitudinal analysis of actual user outcomes after receiving specific strategy-based responses.

### Open Question 2
- **Question:** Can fine-tuning approaches simultaneously reduce unrealistic optimism and minimizing language, or is there an inherent trade-off?
- **Basis in paper:** [explicit] The authors report that "fine-tuning LLMs on emotion reactions reduced some misaligned responses but increased minimizing language," creating a partial improvement with persistent problems.
- **Why unresolved:** The paper demonstrates that strong-emotion fine-tuning increases minimizing responses while weak-emotion training produces more neutral responses, but does not identify whether a unified approach can address both issues.
- **What evidence would resolve it:** Comparing multi-objective fine-tuning approaches that explicitly penalize both unrealistic and minimizing outputs, or developing intensity-matched training curricula that adjust to the emotional level of input contexts.

### Open Question 3
- **Question:** How generalizable are incongruent positivity patterns across conversational platforms and cultural contexts beyond Reddit?
- **Basis in paper:** [inferred] The authors acknowledge "our dataset comprises conversations derived from a single platform (Reddit), which may not capture the full spectrum of interactions on other platforms."
- **Why unresolved:** Reddit has distinct discourse norms, user demographics, and community expectations that may shape how positivity miscalibration manifests compared to other support-seeking contexts.
- **What evidence would resolve it:** Replicating the annotation framework and analysis on datasets from diverse platforms (e.g., crisis hotlines, mental health apps, counseling chat transcripts) and cross-cultural contexts.

### Open Question 4
- **Question:** What detection improvements are needed to reliably identify incongruent positivity in emotionally ambiguous or complex discourse?
- **Basis in paper:** [inferred] The authors note "the variation in confidence scores between grief and advice contexts highlights the challenges models face in emotionally ambiguous or complex discourse," with higher uncertainty and lower annotation agreement in severe concern contexts.
- **Why unresolved:** The ensemble classifier shows fair performance on rare categories, but model confidence and inter-annotator agreement both decline substantially for severe/grief contexts where detection matters most.
- **What evidence would resolve it:** Developing context-adaptive classifiers that explicitly model uncertainty in high-stakes scenarios, or incorporating multi-annotator disagreement signals as features rather than treating disagreement as noise.

## Limitations
- Annotation reliability is compromised by fair-to-moderate inter-annotator agreement (κ=0.28-0.35) for severe concerns, creating fundamental uncertainty in the ground truth
- Weak supervision pipeline introduces label noise that may inflate or deflate model performance metrics, especially for rare categories
- Response-level analysis misses conversational context, limiting understanding of how miscalibrated positivity evolves across dialogue turns

## Confidence
- **High confidence:** The finding that human responses are preferred in severe contexts (50.1% vs. 27.7% for LLMs) is well-supported by controlled preference testing and aligns with known human advantages in emotional connection.
- **Medium confidence:** Claims about LLM-specific dismissiveness patterns in severe contexts are constrained by annotation reliability and could reflect annotator bias toward human responses rather than objective model behavior.
- **Low confidence:** Fine-tuning effectiveness conclusions are weakened by the weak supervision pipeline—performance differences may stem from label noise rather than true behavioral changes.

## Next Checks
1. **Annotation validation study:** Have independent annotators rate a stratified sample of severe concern responses (especially those classified as dismissive) to measure whether original agreement levels persist and whether label patterns reflect actual miscalibration.
2. **Turn-level context analysis:** Re-run the preference study using complete dialogue threads rather than isolated utterances to determine if response-level findings hold when conversational flow is preserved.
3. **Controlled fine-tuning experiment:** Compare fine-tuning outcomes using gold-annotated subsets versus weak labels on identical data splits to quantify the impact of supervision quality on model behavior and reported performance.