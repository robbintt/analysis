---
ver: rpa2
title: 'CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable
  Elo Ratings'
arxiv_id: '2501.01257'
source_url: https://arxiv.org/abs/2501.01257
tags:
- problems
- code
- rating
- problem
- ratings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CodeElo, a competition-level code generation
  benchmark designed to evaluate the reasoning capabilities of large language models
  (LLMs). Existing benchmarks fall short due to the unavailability of private test
  cases, lack of support for special judges, and misaligned execution environments.
---

# CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings

## Quick Facts
- arXiv ID: 2501.01257
- Source URL: https://arxiv.org/abs/2501.01257
- Reference count: 35
- Primary result: Introduces CodeElo benchmark with human-comparable Elo ratings for 30+ LLMs on competition-level code generation

## Executive Summary
CodeElo addresses critical limitations in existing LLM code generation benchmarks by introducing competition-level evaluation on the official CodeForces platform. The benchmark leverages platform submissions to eliminate false positives and support special judges, providing human-comparable Elo ratings across 387 problems from 54 contests. Results show significant performance gaps between models, with o1-mini achieving the highest Elo rating of 1578, while most models struggle even on the easiest problems. The benchmark reveals that C++ prompting consistently outperforms Python and that models excel at math/implementation problems but struggle with dynamic programming and tree structures.

## Method Summary
CodeElo evaluates LLMs by submitting generated code directly to CodeForces via an automated bot, leveraging the platform's hidden test cases and special judges. The benchmark uses standardized chain-of-thought prompting (temperature 0.7, top_p 0.8, top_k 20) with C++ specified for optimal performance. Each problem allows up to 8 submissions, and Elo ratings are calculated by treating each contest independently and averaging results across 54 contests. The dataset includes 387 problems with difficulty ratings (800-3500), algorithm tags, and contest division information, available at https://hf.co/datasets/Qwen/CodeElo.

## Key Results
- o1-mini achieves the highest Elo rating of 1578, significantly outperforming other models
- C++ prompting consistently yields higher Elo ratings than Python across all tested models
- Models perform well on math and implementation problems but struggle with dynamic programming and trees
- Most models achieve Elo ratings placing them in the lowest 25% among human participants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct platform submission reduces evaluation false positives compared to offline test cases
- Mechanism: By submitting model-generated solutions to CodeForces, the benchmark leverages hidden test cases and special judges rather than relying on self-generated or publicly available test sets
- Core assumption: CodeForces test cases are more comprehensive and adversarially designed than researcher-generated alternatives
- Evidence anchors: Abstract states "problems are submitted directly to the platform"; section 3.3.1 notes "zero false positives without needing access to the full test set"

### Mechanism 2
- Claim: Independent contest Elo calculation reduces variance compared to incremental updates
- Mechanism: The system treats each contest independently and calculates expected rating via binary search, then averages across contests
- Core assumption: Model performance across contests is independent and identically distributed
- Evidence anchors: Section 3.3.2 states "much lower variance"; appendix C shows variance converges to Var(r)/k versus Var(r) for incremental updates

### Mechanism 3
- Claim: C++ prompting elicits stronger competition-level performance than Python
- Mechanism: Competitive programming problems have strict time/memory constraints where C++'s efficiency advantages allow correct algorithms to pass that would time out in Python
- Core assumption: Models can generate syntactically correct C++ despite being primarily trained on Python code
- Evidence anchors: Section 5.2 shows "all models achieved higher ratings when using C++"; Figure 2 compares C++ outperforming Python across multiple models

## Foundational Learning

- Concept: Elo rating system fundamentals (expected score formula, rating updates, variance properties)
  - Why needed here: Understanding how CodeElo computes human-comparable ratings and why the independent-contest approach reduces variance
  - Quick check question: Given player ratings r₁=1500 and r₂=1200, what is the expected score for player 1 using the standard Elo formula?

- Concept: Competitive programming problem structure (time limits, memory limits, special judges, contest divisions)
  - Why needed here: Recognizing why offline test cases fail and why special judges (~30% of problems) require platform submission
  - Quick check question: Why can't a problem with multiple valid outputs be evaluated by simple string comparison against a reference solution?

- Concept: Chain-of-thought (CoT) reasoning in code generation
  - Why needed here: Understanding why o1-mini and QwQ-32B-Preview (long-CoT reasoning models) significantly outperform standard instruct models
  - Quick check question: How does the pass@n metric differ conceptually from an Elo rating in capturing model capability?

## Architecture Onboarding

- Component map: HTML scraper -> parsed problem objects -> Bot -> CodeForces API -> status parsing -> Elo equation solver -> averaged rating
- Critical path: 1) Parse problem HTML into structured format 2) Generate model response with CoT prompting 3) Extract code block from response 4) Submit to CodeForces via bot within rate limits 5) Parse judgment status and compute per-contest rating 6) Aggregate ratings across 54+ contests
- Design tradeoffs: 8 submissions/problem cap reduces platform contamination risk but may underestimate true capability; platform dependency achieves zero false positives but requires CodeForces availability; C++ default maximizes pass rates but may not reflect typical LLM usage
- Failure signatures: High rating variance (σ > 500) suggests insufficient contest coverage; systematic timeouts in Python but not C++ indicate algorithm correctness but efficiency issues; zero solves on specific tags indicates structural reasoning gaps
- First 3 experiments: 1) Reproduce Elo rating for Qwen2.5-7B-Instruct using C++ prompting; expect ~315 rating 2) Compare Elo ratings for Qwen2.5-72B-Instruct under C++, Python, and free-choice conditions 3) Test o1-mini on DP vs. math-tagged problems; expect ~10.65% pass@1 on DP vs. ~31.11% on math

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the imposed limit of eight submissions per problem impact the calculated Elo rating compared to the theoretical maximum rating achievable with unlimited attempts?
- Basis in paper: The authors state, "One limitation is that we only allow eight submissions per problem... This constraint might result in the tested Elo ratings being slightly lower than the actual ratings, as models may successfully solve problems with more attempts."
- Why unresolved: The benchmark intentionally restricts submissions to avoid spamming the platform, but the precise penalty this inflicts on model capability estimation remains unquantified
- What evidence would resolve it: A comparison of Elo ratings calculated with a submission cap of 8 versus a much higher cap (e.g., 32 or 64) in a controlled environment

### Open Question 2
- Question: Can model performance be improved by specifically training or prompting LLMs to select C++ over Python for efficiency-critical competition problems?
- Basis in paper: The paper notes, "This highlights a limitation of previous competition code benchmarks that evaluate solely in Python... [and] suggests that model developers should consider training their models to output C++ code when efficiency is a critical factor."
- Why unresolved: While the paper identifies that C++ yields better results, it does not test methods to correct the models' default preference for Python
- What evidence would resolve it: An experiment where models are fine-tuned on language selection heuristics or explicitly prompted to choose the optimal language based on problem constraints

### Open Question 3
- Question: What specific architectural or reasoning deficits cause models to struggle significantly with dynamic programming and tree-based problems compared to math and implementation tasks?
- Basis in paper: The results section observes that "models perform well on math and implementation problems but struggle with dynamic programming and trees," yet the paper provides no causal explanation for this performance gap
- Why unresolved: The paper quantifies the performance drop but does not analyze whether these failures are due to context length, logical recursion handling, or lack of training data
- What evidence would resolve it: A fine-grained error analysis of failed submissions on these specific tags to determine if failures are logical (wrong algorithm) or structural (time limit exceeded)

## Limitations

- Platform dependency creates critical vulnerability as the evaluation pipeline requires automated bot submissions that must comply with CodeForces' terms of service and avoid rate limiting
- Rating calculation assumptions may not hold if model performance improves non-uniformly across problem types or contest difficulty distributions shift over time
- Language bias toward C++ may not represent typical LLM usage patterns where Python dominates 95%+ of practical applications

## Confidence

**High Confidence**: The mechanism of using platform submissions to eliminate false positives and support special judges is well-validated by the zero false positive claim. The Elo rating calculation methodology is mathematically sound given stated assumptions. The performance gap between C++ and Python is consistently observed across multiple models.

**Medium Confidence**: The human-comparable rating interpretation assumes CodeForces' rating distribution remains stable and that the averaged Elo calculation accurately reflects true capability ordering. Conclusions about model reasoning gaps (DP, trees) are based on limited sample sizes per tag.

**Low Confidence**: The generalizability of C++-optimized performance to real-world code generation tasks where Python dominates. The assumption that CodeForces' hidden test cases are definitively more comprehensive than researcher-generated alternatives lacks direct validation.

## Next Checks

1. **Ablation Study on Language Effects**: Replicate the Elo rating for a mid-tier model (e.g., Qwen2.5-7B-Instruct) under three conditions: forced C++, forced Python, and free-choice prompting. Compare not just Elo ratings but also code quality metrics (correctness rate, runtime efficiency) to quantify the true impact of language specification versus inherent model capabilities.

2. **Special Judge Problem Isolation**: Identify the ~30% of problems with special judges and analyze performance differences between platform-submitted solutions versus offline evaluation attempts using public test cases. This would validate the claimed advantage of platform submission and quantify the false positive rate reduction achieved.

3. **Cross-Platform Rating Validation**: Run the same models on an alternative competitive programming platform (e.g., LeetCode, AtCoder) or using offline evaluation with comprehensive test suites. Compare Elo ratings and pass rates to assess whether CodeElo ratings generalize beyond CodeForces' specific environment and rating system.