---
ver: rpa2
title: Wait, Wait, Wait... Why Do Reasoning Models Loop?
arxiv_id: '2512.12895'
source_url: https://arxiv.org/abs/2512.12895
tags:
- looping
- temperature
- root
- action
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Reasoning models often loop at low temperatures or under greedy
  decoding, generating repetitive chains of thought. The study identifies two key
  mechanisms: risk aversion caused by hardness of learning, where models prefer easy
  cyclic actions over hard progress-making ones, and an inductive bias toward temporally
  correlated errors, where small, repeated mistakes cause the model to keep choosing
  the same few actions.'
---

# Wait, Wait, Wait... Why Do Reasoning Models Loop?
## Quick Facts
- arXiv ID: 2512.12895
- Source URL: https://arxiv.org/abs/2512.12895
- Authors: Charilaos Pipis; Shivam Garg; Vasilis Kontonis; Vaishnavi Shrivastava; Akshay Krishnamurthy; Dimitris Papailiopoulos
- Reference count: 40
- Primary result: Reasoning models loop due to risk aversion and temporally correlated errors, especially under greedy decoding or low temperatures

## Executive Summary
Reasoning models often generate repetitive chains of thought, particularly under greedy decoding or low temperatures. This study identifies two core mechanisms behind looping: risk aversion caused by the hardness of learning, where models prefer easy cyclic actions over challenging progress-making ones, and an inductive bias toward temporally correlated errors, where small, repeated mistakes cause the model to keep choosing the same few actions. Larger models loop less, and distilled students loop more than their teachers, pointing to imperfect learning as a root cause. While higher temperature reduces looping by promoting exploration, it doesn't fix underlying errors, so generations remain longer than necessary. Thus, temperature is a stopgap rather than a holistic solution.

## Method Summary
The authors analyze looping behavior in reasoning models by systematically varying decoding strategies, model sizes, and training regimes. They employ behavioral analysis and ablation studies to isolate the mechanisms driving loops, focusing on the effects of temperature, model scale, and distillation. Error propagation and action selection patterns are tracked to identify the role of temporally correlated mistakes and risk-averse decision-making.

## Key Results
- Looping is driven by risk aversion (hardness of learning) and temporally correlated errors.
- Larger models exhibit fewer loops than smaller models.
- Distilled models loop more than their teacher models.
- Higher temperature reduces loops but doesn't eliminate underlying errors.

## Why This Works (Mechanism)
Looping emerges from two interacting factors: risk aversion due to the hardness of learning and temporally correlated errors. Risk aversion leads models to prefer repeating familiar, safe actions rather than taking uncertain steps toward progress. Temporally correlated errors cause small, repeated mistakes to compound, pushing the model into cycles. These mechanisms are exacerbated under greedy decoding and low-temperature settings, where exploration is minimal.

## Foundational Learning
- **Risk aversion in decision-making**: Models prefer low-risk, cyclic actions over high-risk, progress-making ones.
  - *Why needed*: Explains why models avoid uncertain steps that could break loops.
  - *Quick check*: Compare action selection patterns under varying risk conditions.
- **Temporally correlated errors**: Small, repeated mistakes accumulate, reinforcing looping behavior.
  - *Why needed*: Identifies how minor errors can lead to persistent cycles.
  - *Quick check*: Measure error propagation across consecutive steps.
- **Temperature's role in exploration**: Higher temperature encourages exploration, reducing loop frequency.
  - *Why needed*: Clarifies why temperature adjustments only mask, not fix, looping.
  - *Quick check*: Evaluate loop frequency across a range of temperature settings.

## Architecture Onboarding
- **Component map**: Input -> Encoder -> Decoder -> Action Selection -> Output Chain of Thought
- **Critical path**: Encoder processes input, Decoder generates actions, Action Selection determines next step, Output forms chain of thought
- **Design tradeoffs**: Greedy decoding favors efficiency but increases looping; higher temperature reduces loops but may introduce noise
- **Failure signatures**: Repetitive action sequences, failure to progress, increased generation length
- **First experiments**:
  1. Compare loop frequency under greedy vs. sampled decoding.
  2. Measure error accumulation over time in generated chains.
  3. Test anti-looping regularization techniques on distilled models.

## Open Questions the Paper Calls Out
None

## Limitations
- Causal mechanisms are not fully isolated from other potential causes (e.g., reward function design).
- Findings are based on specific model families and may not generalize to all architectures.
- Temperature's role as a stopgap is qualitatively argued but lacks comprehensive quantitative validation.

## Confidence
- Risk aversion as a mechanism: **High**
- Temporally correlated errors as a mechanism: **High**
- Temperature as a stopgap rather than a solution: **Medium**

## Next Checks
1. **Ablation of correlated error patterns**: Design controlled experiments to measure the impact of perturbing small, repeated mistakes on loop frequency and duration, directly testing the hypothesis of temporally correlated errors.
2. **Comparative analysis across model sizes and families**: Systematically evaluate looping behavior across a broader range of model scales and architectures (e.g., including non-LLM-based reasoners) to test the claim that larger models loop less due to better learning.
3. **Fine-tuning with anti-looping objectives**: Train models with explicit regularization against cyclic behavior (e.g., novelty rewards or state-tracking penalties) and measure whether looping is reduced without relying on temperature, addressing whether the problem is fundamentally about imperfect learning or architectural bias.