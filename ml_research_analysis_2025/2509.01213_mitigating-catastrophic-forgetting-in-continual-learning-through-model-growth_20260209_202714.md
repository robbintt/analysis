---
ver: rpa2
title: Mitigating Catastrophic Forgetting in Continual Learning through Model Growth
arxiv_id: '2509.01213'
source_url: https://arxiv.org/abs/2509.01213
tags:
- tasks
- forgetting
- bias
- stackllm
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates catastrophic forgetting in continual learning
  by comparing a model trained with a growth-based stacking approach (StackLLM) against
  a traditionally trained model (LLM). Both models were fine-tuned sequentially on
  tasks including text simplification, empathetic dialogue generation, and inquisitive
  question generation.
---

# Mitigating Catastrophic Forgetting in Continual Learning through Model Growth

## Quick Facts
- arXiv ID: 2509.01213
- Source URL: https://arxiv.org/abs/2509.01213
- Reference count: 12
- Primary result: Growth-based stacking reduces catastrophic forgetting in continual fine-tuning, particularly for reading comprehension tasks

## Executive Summary
This study investigates catastrophic forgetting in continual learning by comparing a model trained with a growth-based stacking approach (StackLLM) against a traditionally trained model (LLM). Both models were fine-tuned sequentially on tasks including text simplification, empathetic dialogue generation, and inquisitive question generation. Evaluation across domain knowledge, reasoning, reading comprehension, and bias showed that both models improved in domain knowledge but degraded in reasoning and reading comprehension over time. StackLLM exhibited less forgetting, especially in reading comprehension. Regarding bias, the baseline LLM became progressively more neutral, while StackLLM maintained a steady bias ratio around 60-61%. These results suggest that growth-based pretraining may provide modest improvements in resisting catastrophic forgetting, though trade-offs remain in handling social biases.

## Method Summary
The study compares StackLLM, a model initialized via layer duplication from a smaller pretrained model (growth factor g=4), against a traditionally trained LLM. Both models were fine-tuned sequentially on three tasks: text simplification, empathetic dialogue generation, and inquisitive question generation. StackLLM achieved the same performance level with 35% fewer training tokens (194B vs 300B). The evaluation used the forgetting metric (FG) across four capability categories: domain knowledge, reasoning, reading comprehension, and bias. Training used ALPACA prompt templates with different regularization strategies across tasks.

## Key Results
- Both models showed domain knowledge improvement but reasoning and reading comprehension degradation during sequential fine-tuning
- StackLLM exhibited lower forgetting in reading comprehension (FG=6.8 vs 11.9 for baseline LLM)
- StackLLM maintained steady bias ratios around 60-61% while baseline LLM became progressively more neutral (FG=-0.9 vs 11.0)
- Growth-based pretraining reached target performance with 35% fewer training tokens

## Why This Works (Mechanism)

### Mechanism 1
Stacking-based model growth may reduce catastrophic forgetting by preserving structural knowledge from smaller pretrained models during depth-wise expansion. A smaller model is first trained on ~10B tokens, then its layers are duplicated (growth factor g=4) to initialize a larger 7B model, which continues training on ~300B tokens. This transfers compressed representations from the smaller model, potentially anchoring foundational knowledge more robustly than random initialization.

### Mechanism 2
StackLLM's lower evaluation loss and faster convergence during fine-tuning suggest better optimization landscapes, which correlate with reduced forgetting. The paper reports StackLLM achieved 7-10% lower evaluation loss on InqQG and 2.7-3.1% lower loss on Simp compared to the baseline LLM. Smoother optimization trajectories may leave more gradient budget for preserving prior task representations.

### Mechanism 3
Growth-based models exhibit reduced plasticity in bias-related representations, maintaining stable bias ratios rather than progressively neutralizing. Baseline LLM's bias ratio dropped from 63.4% to 55.8% across continual tasks, while StackLLM fluctuated around 60-61%. This suggests stacking "locks" certain social biases inherited from the smaller model's pretraining.

## Foundational Learning

- **Catastrophic Forgetting in Sequential Fine-Tuning**: Understanding that continual fine-tuning on new tasks degrades performance on prior capabilities is essential for interpreting FG metrics and why mitigation matters.
- **Forgetting Metric (FG)**: FG quantifies average performance degradation across evaluation tasks; negative FG indicates improvement, positive indicates forgetting. Critical for comparing StackLLM vs LLM objectively.
- **Transformer Stacking (Depth-wise Growth)**: The core intervention relies on duplicating layers from a pretrained smaller model to initialize a larger one. Engineers must distinguish this from width-wise growth or knowledge distillation.

## Architecture Onboarding

- **Component map**: StackLLM-7B (32 layers, 5.9B params, layer-duplicated from 8-layer precursor) -> LLM-7B (32 layers, 5.9B params, from scratch) -> Evaluation Harness (lm-evaluation-harness)
- **Critical path**: Train small model (g=4 depth factor) on 10B tokens → Duplicate layers to reach target depth (32 layers) → Continue training stacked model on 300B tokens → Sequential fine-tune on Simp → Emdg → InqQG with ALPACA prompt template → Evaluate after each fine-tuning stage using FG metric across four capability categories
- **Design tradeoffs**: StackLLM retains reading comprehension better (FG=6.8 vs 11.9) but shows reduced bias adaptability (FG=-0.9 vs 11.0); training efficiency vs forgetting (35% fewer tokens but may lock undesirable properties); regularization sensitivity (Tasks 1-2 used constant LR with no warmup → overfitting; Task 3 added cosine scheduler + warmup + weight decay → healthier training)
- **Failure signatures**: Training loss drops sharply at epoch boundaries while validation loss rises → overfitting; bias ratio stagnates at ~60% across all tasks → locked bias representations; reading comprehension accuracy drops monotonically (37.1% → 31.4% for LLM) → catastrophic forgetting
- **First 3 experiments**:
  1. Reproduce single-task fine-tuning: Fine-tune StackLLM-7B and LLM-7B on Simp only, compare SARI/BLEU/ROUGE to verify baseline performance gap
  2. Ablate growth factor: Train StackLLM variants with g=2 and g=8 to test whether retention scales with stacking depth; evaluate FG on RACE-high after sequential fine-tuning
  3. Regularization sweep: Repeat Task 3 (InqQG) with varying weight decay (0.0, 0.01, 0.1) and warmup ratios (0%, 8.5%, 15%) to isolate effect on evaluation loss and BERTScore

## Open Questions the Paper Calls Out

- **Bias Locking Mechanism**: Why does StackLLM maintain steady bias ratios (~60-61%) during continual fine-tuning while the baseline LLM becomes progressively more neutral? The authors state this leads to further research about how and why bias is handled by stacked models.

- **Generalizability Across Scales**: Can the mitigating effect of growth-based pretraining on catastrophic forgetting in reading comprehension be replicated across different model sizes, architectures, and task sequences? The paper suggests checking other open source and similar models to extend performance to real-world scenarios.

- **Task-Specific Retention**: What mechanisms enable StackLLM to resist forgetting better in reading comprehension than in reasoning tasks, where both models degrade similarly? The paper demonstrates differential retention but does not explain why the stacking advantage is task-specific.

## Limitations
- Results based on a single architecture variant with one growth factor (g=4), limiting generalizability
- FG metric aggregates across diverse task types without task-specific analysis
- Bias results suggest stacking "locks" social representations, but this could reflect pretraining data rather than architectural effects
- Ablation over hyperparameters was limited to the third task, confounding growth effects from regularization effects

## Confidence
- **High confidence**: Sequential fine-tuning degrades reasoning and reading comprehension across both models; StackLLM shows lower FG in reading comprehension (6.8 vs 11.9)
- **Medium confidence**: Growth-based pretraining improves optimization efficiency (7-10% lower loss) and reduces forgetting in reading comprehension
- **Low confidence**: Stacking's mechanism for mitigating forgetting (layer duplication preserving representations) is assumed but not experimentally isolated; bias locking is observed but unexplained

## Next Checks
1. **Ablate growth factor**: Train StackLLM variants with g=2 and g=8, evaluate FG on RACE-high after sequential fine-tuning to test whether retention scales with stacking depth
2. **Freeze ablation**: Freeze first N layers of StackLLM and baseline LLM during fine-tuning; compare FG on reading comprehension to isolate whether lower layers drive retention
3. **Task-specific FG**: Compute FG separately for each task (Simp, Emdg, InqQG) on all benchmarks to identify which tasks benefit most from stacking