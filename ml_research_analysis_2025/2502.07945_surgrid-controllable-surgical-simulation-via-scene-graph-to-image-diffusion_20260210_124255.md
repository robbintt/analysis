---
ver: rpa2
title: 'SurGrID: Controllable Surgical Simulation via Scene Graph to Image Diffusion'
arxiv_id: '2502.07945'
source_url: https://arxiv.org/abs/2502.07945
tags:
- image
- surgical
- graph
- scene
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SurGrID introduces a novel approach for controllable surgical simulation
  using Scene Graph to Image Diffusion Models. By leveraging Scene Graphs to encode
  spatial and semantic information of surgical scenes, SurGrID enables precise control
  over anatomy and tool positioning during image synthesis.
---

# SurGrID: Controllable Surgical Simulation via Scene Graph to Image Diffusion

## Quick Facts
- **arXiv ID:** 2502.07945
- **Source URL:** https://arxiv.org/abs/2502.07945
- **Reference count:** 36
- **Primary result:** Achieves state-of-the-art surgical image synthesis with FID 26.6 and KID 0.019 on CaDIS dataset

## Executive Summary
SurGrID introduces a novel approach for controllable surgical simulation using Scene Graphs to Image Diffusion Models. By leveraging Scene Graphs to encode spatial and semantic information of surgical scenes, SurGrID enables precise control over anatomy and tool positioning during image synthesis. The method employs a two-step pre-training process that captures both local and global scene information, followed by conditional diffusion modeling for high-fidelity image generation. Evaluated on cataract surgery data, SurGrID outperforms existing methods in image quality metrics and demonstrates superior adherence to conditioning inputs. Clinical experts confirmed the realism and controllability of generated images through a user study, validating SurGrID's potential for interactive surgical simulation training.

## Method Summary
SurGrID converts surgical images and their segmentation masks into scene graphs where nodes represent connected components with class, spatial, and centroid information. Two GNN encoders are pre-trained: one for local reconstruction via masked latent prediction, and one for global contrastive alignment using mask embeddings. These encoders produce concatenated embeddings that condition a latent diffusion model. Classifier-Free Guidance with scale 2.0 enables high-fidelity image generation while maintaining adherence to the scene graph specifications. The system is trained on the CaDIS cataract surgery dataset at 128×128 resolution.

## Key Results
- Achieves state-of-the-art FID score of 26.6 and KID of 0.019 on CaDIS validation set
- Clinical expert study confirms generated images are realistic and adhere to scene graph specifications
- Ablation studies show both pre-training stages and combined embeddings are critical for performance
- Outperforms existing methods including SGDiff and LDM-CLIP in both image quality and conditioning adherence

## Why This Works (Mechanism)

### Mechanism 1
Scene graphs provide precise spatial and semantic conditioning for surgical image synthesis, overcoming limitations of text and mask-based conditioning. Connected components in segmentation masks become graph nodes encoding: class vector (d-dim), spatial spreading (2-dim), and centroid coordinates (2-dim). Edges represent spatial adjacency. This explicit encoding enables direct manipulation of node features to control generated content position, size, and type. The core assumption is that the graph structure fully captures the semantically relevant degrees of freedom for surgical scene generation.

### Mechanism 2
Separate pre-training objectives for local and global information prevent embedding collapse caused by video-specific clustering in surgical datasets. Local encoder (Eloc_G) is trained via masked reconstruction—randomly masking object classes and predicting VQ-GAN latent codes. Global encoder (Eglob_G) uses contrastive alignment with segmentation mask embeddings (not image embeddings), clustering frames by tool/position rather than video identity. The core assumption is that image embeddings cluster by video due to shared anatomical texture, while mask embeddings better reflect scene structure for contrastive learning.

### Mechanism 3
Classifier-Free Guidance (CFG) with concatenated local-global embeddings improves adherence to scene graph conditioning without sacrificing image quality. Concatenate zloc_G and zglob_G as conditioning c. Train denoiser with 20% random conditioning dropout. During sampling, substitute predicted noise: ε'_θ = (1+ω)ε_θ(x_t,t,c) - ωε_θ(x_t,t). Optimal ω≈2.0 balances fidelity and coherence. The core assumption is that CFG scaling interpolates between unconditional and conditional distributions in a way that preserves spatial coherence from graph embeddings.

## Foundational Learning

- **Graph Neural Networks (GNNs) for scene representation**: Node features are updated via neighbor aggregation; understanding message passing is required to modify encoder architecture. Quick check: Can you explain how mean/max pooling over neighbors affects the invariance properties of node embeddings?

- **Denoising Diffusion Probabilistic Models (DDPMs)**: SurGrID builds on latent diffusion; you must understand forward/noise process and reverse denoising to modify training or sampling. Quick check: What is the role of the noise schedule ᾱ_t in determining signal-to-noise ratio during training?

- **Contrastive Learning (CLIP-style)**: Global encoder uses contrastive loss to align graph and mask embeddings; modifying this requires understanding positive/negative sampling. Quick check: How does the choice of negative samples affect what the encoder learns to cluster or separate?

## Architecture Onboarding

- **Component map**: Mask → SG extraction → Pre-trained encoders → Concatenated conditioning → Diffusion sampling → Generated image
- **Critical path**: Scene Graph Generator → Dual Graph Encoders → Latent Diffusion Model → Generated image
- **Design tradeoffs**: Two-encoder vs. single-encoder (local-only FID 93.2, global-only FID 120.2, combined 26.6); CFG scale ω: 1.0-2.0 stable, >3.0 degrades; image resolution at 128×128
- **Failure signatures**: Tools missing or incorrect in batch; unrealistic tool-tissue interaction; spatial positions far from training distribution
- **First 3 experiments**: 1) Reproduce FID/KID on CaDIS validation with provided SG representations; 2) Ablate pre-training with t-SNE visualization of embeddings; 3) CFG sweep testing ω∈{1.0, 1.5, 2.0, 2.5, 3.0} with BB IoU measurements

## Open Questions the Paper Calls Out

### Open Question 1
Can SurGrID be extended to generate temporally consistent video sequences rather than isolated frames? The current implementation generates independent frames, lacking mechanisms to maintain temporal consistency in textures or motion across a sequence. Evidence would be demonstration of a video generation mode where textures (e.g., blood vessels) and lighting remain consistent across frames generated from a sequence of scene graphs.

### Open Question 2
Can the Scene Graph conditioning be refined to control fine-grained attributes such as tool angle or anatomical size (e.g., pupil dilation)? The current node features encode centroid coordinates and class but do not explicitly parameterize rotational orientation or specific anatomical scaling. Evidence would be a user study confirming that modifying specific angle or size parameters in the graph results in accurate visual changes in the synthesized image.

### Open Question 3
Is the proposed architecture effective when applied to other surgical domains beyond cataract surgery? The method is currently validated exclusively on the CaDIS dataset (cataract surgery). Evidence would be quantitative results (FID/KID) and qualitative clinical evaluations on datasets from distinct surgeries, such as laparoscopy or neurosurgery.

## Limitations

- Surgical domain specificity: Results validated only on cataract surgery dataset; generalization to other surgical procedures remains untested
- Temporal coherence: Method generates single frames; no evaluation of temporal consistency across video sequences
- Real-time performance: No timing analysis provided; potential bottleneck in graph encoding or diffusion sampling for interactive training applications

## Confidence

- **High confidence**: Scene graph encoding mechanism and its integration with diffusion models (supported by ablation studies and quantitative metrics)
- **Medium confidence**: Pre-training strategy effectiveness (based on t-SNE visualizations and baseline comparisons, but limited ablation of architectural choices)
- **Medium confidence**: Clinical validation (expert feedback positive but based on subjective assessment of limited sample size)

## Next Checks

1. Evaluate on multi-modal surgical datasets (e.g., Cholec80 for laparoscopic surgery) to test domain generalization
2. Implement temporal extension: Condition diffusion on both current and previous scene graphs to generate coherent video sequences
3. Benchmark inference latency with varying guidance scales to determine practical limits for real-time surgical simulation