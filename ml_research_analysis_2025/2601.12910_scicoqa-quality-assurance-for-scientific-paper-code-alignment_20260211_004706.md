---
ver: rpa2
title: 'SciCoQA: Quality Assurance for Scientific Paper--Code Alignment'
arxiv_id: '2601.12910'
source_url: https://arxiv.org/abs/2601.12910
tags:
- code
- discrepancy
- discrepancies
- data
- gpt-5
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SciCoQA, a dataset for detecting discrepancies
  between scientific papers and their code implementations. The dataset is constructed
  from GitHub issues and reproducibility papers, supplemented with synthetically generated
  discrepancies.
---

# SciCoQA: Quality Assurance for Scientific Paper--Code Alignment

## Quick Facts
- arXiv ID: 2601.12910
- Source URL: https://arxiv.org/abs/2601.12910
- Authors: Tim Baumgärtner; Iryna Gurevych
- Reference count: 40
- Primary result: GPT-5 achieves 45.7% recall on real-world paper-code discrepancies, highlighting the difficulty of cross-modal semantic alignment.

## Executive Summary
SciCoQA introduces a dataset for detecting discrepancies between scientific papers and their code implementations, addressing a critical need for automated quality assurance in computational research. The dataset, constructed from GitHub issues and reproducibility papers plus synthetic generation, includes 611 discrepancies across diverse scientific domains. A detailed taxonomy categorizes discrepancies into Algorithm, Model, Loss, Evaluation, Data, and Training aspects. Evaluation of 21 large language models reveals that even the best-performing model, GPT-5, detects only 45.7% of real-world discrepancies, underscoring the challenge of cross-modal reasoning over long contexts and the need for improved automated tools.

## Method Summary
The study defines paper-code discrepancies as semantic conflicts between scientific methods described in publications and their implementations in codebases. Real discrepancies are sourced from GitHub issues and reproducibility papers, then validated through human filtering and GPT-5 verification. To scale the dataset, synthetic discrepancies are generated by prompting GPT-5 to create minimal, conceptually meaningful code diffs that create controlled conflicts with paper descriptions. The dataset includes 81 real and 530 synthetic discrepancies across diverse computational science domains. Evaluation uses an LLM-as-a-Judge (GPT-OSS 20B) to match predicted discrepancies against ground truth, with zero-shot inference across 21 models using standardized prompt templates that fill context to 90% of the model window.

## Key Results
- GPT-5 achieves 45.7% recall on real-world discrepancies and 71.3% on synthetic data
- Performance degrades significantly with longer input contexts, particularly for instances exceeding 100k tokens
- Models struggle most with "Paper Omissions" where code implements logic not described in the paper
- Strong correlation (r=0.94) between model performance on synthetic versus real data supports synthetic generation validity

## Why This Works (Mechanism)

### Mechanism 1: Cross-modal Semantic Alignment
The task requires mapping natural language descriptions of algorithms, losses, and models from a paper to their formal implementations in a multi-file codebase, then identifying where the semantics diverge (e.g., a paper describes L2 norm while code implements L1 norm). The defined taxonomy of discrepancy types (Difference, Paper Omission, Code Omission) and categories (Algorithm, Model, Loss, Evaluation, Data, Training) sufficiently captures the space of meaningful scientific misalignments, excluding trivial engineering artifacts.

### Mechanism 2: Synthetic Discrepancy Generation
Synthetic discrepancies are generated via LLM prompting to create minimal, conceptually meaningful code diffs (e.g., changing a normalization step) that create controlled conflicts with the paper. This enables controlled scaling beyond sparse real-world examples. The validity is supported by a strong correlation (r=0.94) between model performance on synthetic vs. real data, indicating synthetic discrepancies are representative of real-world paper-code mismatches in their conceptual nature and difficulty.

### Mechanism 3: Long-Context Reasoning Bottleneck
The task requires ingesting tens of thousands of tokens from both paper and code, locating relevant sections across modalities, and performing fine-grained semantic comparison. Performance degrades predictably as input token count increases, with the "lost in the middle" phenomenon causing models to lose track of relevant information when processing extremely long sequences.

## Foundational Learning

**Concept: Paper-Code Alignment**
- Why needed here: The core task is not bug detection but verifying that the code is a faithful implementation of the scientific method described in the paper. Understanding what constitutes a "meaningful mismatch" versus an engineering artifact is critical.
- Quick check question: Does a difference in a default hyperparameter value (e.g., learning rate=0.01 vs 0.001) count as a discrepancy if the code supports both via CLI? (Answer: No, per the paper's definition).

**Concept: Long-Context Processing**
- Why needed here: Inputs routinely exceed 50,000 tokens (paper median 14,350, code median 38,978). Models must handle this scale without losing critical information.
- Quick check question: If a model's context window is 32k tokens and the input is 60k tokens, what is the most likely consequence for a discrepancy located in a file truncated from the end of the code prompt? (Answer: The model cannot detect it).

**Concept: Discrepancy Taxonomy**
- Why needed here: Understanding the three types (Difference, Paper Omission, Code Omission) and six categories (Algorithm, Model, etc.) is necessary for analyzing model failures and for the synthetic generation pipeline.
- Quick check question: In a Paper Omission, which modality contains the extra information? (Answer: The code implements a concept not described in the paper).

## Architecture Onboarding

**Component map:**
Data Sources -> Processing Pipeline -> Model Interface -> Evaluation System
(GitHub issues & reproducibility papers + synthetic generation) -> (OCR processing, file filtering, prompt construction) -> (Unified prompt templates, context management) -> (LLM-as-Judge matching, recall computation)

**Critical path:**
1. Source raw discrepancies from GitHub issues (Qwen3 filter → human filter → GPT-5 validation) and reproducibility papers (GPT-5 extraction → human filter → GPT-5 validation)
2. Generate synthetic discrepancies by prompting GPT-5 to create minimal code diffs in sampled repositories
3. Process each (paper, code) pair into a standardized prompt
4. Query LLMs to generate discrepancy lists
5. Use embedding similarity + LLM judge to match predictions to ground truth and compute recall

**Design tradeoffs:**
- Precision vs. Recall: Ground truth is constructed for high precision (verified by humans & GPT-5), but this may miss unlabeled, valid discrepancies found by models, affecting recall calculation
- Real vs. Synthetic Data: Real data is high-quality but sparse and CS-focused; synthetic data enables scale and domain diversity but may not fully capture real-world discrepancy distributions
- Context Truncation: Simple file-truncation from the end risks losing relevant code; more sophisticated retrieval-based context management is not used

**Failure signatures:**
- Low Recall on Paper Omissions: Models struggle when the paper lacks a description for code logic, as there's no textual specification to ground the search
- Performance Drop on Recent Papers: Models perform worse on papers likely not in their pre-training corpus (contamination), indicating reliance on memorization
- Long-Context Degradation: Recall consistently drops as prompt token count increases, signaling fundamental limits in current LLMs' context utilization

**First 3 experiments:**
1. **Baseline Model Replication**: Re-run the discrepancy prediction task with GPT-5 and the top open-source model (e.g., GPT-OSS 20B) on a subset of 10 real and 10 synthetic examples to familiarize with the prompt, generation parsing, and evaluation pipeline
2. **Context Ablation**: Take 5 papers from the dataset with >100k tokens. Run the model under three conditions: (a) full context, (b) paper only, (c) code only. Measure recall drop to quantify the contribution of each modality and the impact of context length
3. **Synthetic Validity Check**: Manually inspect 10 randomly sampled synthetic discrepancies (code diffs) and their corresponding paper descriptions to verify they adhere to the "small, conceptually meaningful" constraint and represent plausible real-world mismatches

## Open Questions the Paper Calls Out

**Open Question 1:** Can specialized fine-tuning on the SciCoQA dataset significantly improve the low recall (45.7%) of current LLMs, particularly for "Paper Omissions" where code logic is absent from the text?
- Basis: The authors conclude that current models "suffer from insufficient recall" and struggle most with "Paper Omissions," which require distinguishing deviations from permissible engineering artifacts without a textual reference
- Why unresolved: The study primarily benchmarks off-the-shelf or instruction-tuned models; it does not evaluate whether training on the proposed dataset closes the performance gap
- Evidence needed: A comparative study measuring recall improvements after fine-tuning baseline models on the SciCoQA training split, specifically analyzing error rates on the "Paper Omission" subset

**Open Question 2:** Do the synthetic discrepancies generated for non-CS domains (e.g., Physics, Quantitative Biology) accurately reflect the distribution and nature of real-world implementation errors in those fields?
- Basis: The authors state in the Limitations section that while they include synthetic discrepancies from diverse fields, "the distribution of errors in these fields may differ from our synthetic approximations"
- Why unresolved: The "real-world" data is skewed heavily toward Computer Science/AI; the validity of the synthetic pipeline for other scientific domains remains an assumption based on the CS/AI correlation
- Evidence needed: A follow-up analysis involving manual verification of synthetic discrepancies by domain experts in Physics and Biology, or the collection of a "Real" test set for these domains to compare against synthetic performance

**Open Question 3:** How does the performance of discrepancy detection degrade relative to the placement of the discrepancy within the long-context window (i.e., "lost in the middle" phenomenon) in repositories exceeding 100k tokens?
- Basis: The paper notes that performance drops as prompt length increases and identifies 70 papers with >100k tokens as a challenge, but does not isolate whether specific context positions (start/middle/end) affect detection rates
- Why unresolved: The analysis aggregates performance by total token count but does not control for the location of the discrepancy within the long input context
- Evidence needed: A controlled experiment where identical discrepancies are injected at the beginning, middle, and end of long-context prompts to measure the positional bias of the models

## Limitations
- Real-world data coverage is limited to 81 samples, primarily from Computer Science/AI domains, potentially constraining generalizability
- Synthetic discrepancies, while validated by correlation metrics, may not fully capture the complexity and distribution of real-world implementation errors across diverse scientific fields
- The taxonomy may not capture all meaningful discrepancies, particularly scientific logic errors unrelated to paper-code description-implementation alignment

## Confidence

**High**: The discrepancy taxonomy (Difference, Paper Omission, Code Omission) and its application to define the task are robust and clearly specified

**Medium**: Synthetic data generation validity is supported by correlation metrics but requires manual validation to confirm representativeness

**Medium**: Long-context degradation mechanism is observed but needs further controlled experiments to isolate reasoning limits from truncation effects

## Next Checks

1. **Manual Synthetic Validation**: Inspect 20 randomly sampled synthetic discrepancies to verify they represent small, conceptually meaningful mismatches that would plausibly occur in real papers

2. **Context Ablation Experiment**: Run 10 papers through the model under three conditions (full context, paper-only, code-only) to quantify the relative contribution of each modality and isolate truncation effects

3. **Taxonomy Coverage Audit**: Identify 5 real-world paper-code discrepancies not captured by the current taxonomy and propose extensions to handle cases like scientific logic errors unrelated to description-implementation alignment