---
ver: rpa2
title: Causal Imitation Learning Under Measurement Error and Distribution Shift
arxiv_id: '2601.22206'
source_url: https://arxiv.org/abs/2601.22206
tags:
- expert
- learning
- latent
- imitation
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles imitation learning when the decision-relevant
  state is partially observed through noisy measurements and the data distribution
  may shift between training and deployment. Such settings can cause standard behavioral
  cloning (BC) to latch onto spurious correlations, leading to systematically biased
  policies under shift.
---

# Causal Imitation Learning Under Measurement Error and Distribution Shift

## Quick Facts
- arXiv ID: 2601.22206
- Source URL: https://arxiv.org/abs/2601.22206
- Reference count: 40
- Primary result: CausIL achieves lower imitation error and greater robustness to both measurement and population shifts compared to behavioral cloning baselines

## Executive Summary
This paper addresses imitation learning when the decision-relevant state is partially observed through noisy measurements and the data distribution may shift between training and deployment. Standard behavioral cloning fails in such settings because it learns spurious correlations between noisy observations and actions. The authors propose CausIL, a causal imitation learning framework that treats noisy state observations as proxy variables and leverages proximal causal inference to define an intervention-based target policy. The method provides identification conditions for both discrete and continuous settings and demonstrates superior robustness to measurement and population shifts through experiments on simulated and semi-simulated ICU data.

## Method Summary
The method recovers the causal-optimal policy $\pi_{opt}$ from expert demonstrations where the decision-relevant state is latent and only noisy proxies are observed. In discrete settings, it uses a plug-in estimator via matrix inversion to solve a confounding bridge equation. In continuous settings, it employs an RKHS-adversarial estimation approach that solves a regularized saddle-point problem. The framework leverages two distinct proxies: the lagged observed state $S_{t-1}$ (treatment-inducing proxy) and the noisy measurement $W_{t-1}$ (outcome-inducing proxy). This allows identification of the interventional distribution $P(A_t^{(s)})$ rather than the observational conditional $P(A_t|S_t)$, creating robustness to shifts in the measurement mechanism.

## Key Results
- CausIL outperforms behavioral cloning (BC) in imitation error when measurement mechanisms shift between training and deployment
- The causal target policy remains stable under measurement shifts while BC degrades sharply
- Experiments show effectiveness on both synthetic data and semi-simulated ICU data from PhysioNet Challenge 2019
- The method achieves identification and estimation of the optimal policy despite latent confounding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The optimal imitation policy can be identified from observational data despite unobserved latent states by treating time-lagged variables as proxy variables in a proximal causal inference framework.
- **Mechanism:** The framework uses two distinct proxies for the latent state $U_{t-1}$: the lagged observed state $Z_t = S_{t-1}$ (treatment-inducing proxy) and the noisy measurement $W_{t-1}$ (outcome-inducing proxy). By solving a confounding bridge equation, the system decouples the true action mechanism from spurious correlations induced by the latent $U_{t-1}$.
- **Core assumption:** The system satisfies completeness and existence of bridge functions (Assumptions 4.1-4.4).
- **Evidence anchors:** Section 4.1 describes the proxy setup; Section 6.1 shows CausIL stability under measurement shift.
- **Break condition:** Fails if $S_{t-1}$ and $W_{t-1}$ are not sufficiently informative regarding $U_{t-1}$, or if completeness is violated.

### Mechanism 2
- **Claim:** Targeting the interventional distribution $P(A_t^{(s)})$ creates a policy robust to shifts in the measurement mechanism.
- **Mechanism:** Standard BC conditions on the noisy proxy $W_{t-1}$. If sensor noise changes between domains, BC fails because learned correlations break. The causal target aggregates over the population distribution of $U_{t-1}$, ignoring the measurement channel $P(W|U)$ entirely, making it invariant to sensor calibration changes.
- **Core assumption:** The expert policy mechanism is invariant across domains, and $P(U_{t-1})$ remains stable.
- **Evidence anchors:** Section 3 shows theoretical invariance; Figure 2 demonstrates empirical stability.
- **Break condition:** Fails if the marginal distribution of the latent state $P(U_{t-1})$ changes drastically between training and deployment.

### Mechanism 3
- **Claim:** In continuous state spaces, bridge functions can be estimated via minimax optimization in RKHS.
- **Mechanism:** The estimation solves a conditional moment restriction framed as a saddle-point problem where a bridge function $h$ is minimized against an adversarial test function $q$. This adversarial procedure solves the integral equation without requiring explicit matrix inversion.
- **Core assumption:** The RKHS hypothesis spaces are sufficiently expressive and regularization parameters are properly tuned.
- **Evidence anchors:** Section 5.2 describes the adversarial approach; Appendix C.7 provides the closed-form solution.
- **Break condition:** Fails if RKHS kernels are mismatched to data geometry or if regularization prevents convergence.

## Foundational Learning

- **Concept: Proximal Causal Inference**
  - **Why needed here:** This is the theoretical engine. Understanding that you need *two* distinct proxies to correct for an unobserved confounder is essential to grasp why the method uses both $S_{t-1}$ and $W_{t-1}$.
  - **Quick check question:** If you only had the noisy measurement $W$ but no history $S_{t-1}$, could you identify the policy? (Answer: No, you lack the second proxy required for proximal inference).

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - **Why needed here:** The paper solves the continuous imitation problem by defining a minimax game in an RKHS, not with standard backprop on a neural net.
  - **Quick check question:** Why is a closed-form solution available for the seemingly complex minimax game? (Answer: The Representer Theorem allows the solution to be expressed as a linear combination of kernel evaluations).

- **Concept: Interventional vs. Observational Distributions**
  - **Why needed here:** The paper argues $\pi_{opt} \neq \pi_{BC}$. This distinction only makes sense if you understand the difference between seeing $S=s$ naturally vs. setting $S=s$ forcibly.
  - **Quick check question:** Does $P(A|S) = P(A|do(S))$ hold in this setting? (Answer: No, because of the latent confounder $U_{t-1}$ affecting both).

## Architecture Onboarding

- **Component map:** Data Ingest -> Kernel Layer -> Bridge Solver -> Policy Extractor
- **Critical path:** The estimation of the bridge function coefficients $\alpha$ (Eq. 5.4). If the matrix $\Gamma$ is ill-conditioned, the inverse becomes unstable, breaking the policy extraction.
- **Design tradeoffs:**
  - **Discrete vs. Continuous Estimator:** Use discrete matrix inversion for small state spacesâ€”it is exact and fast. Use RKHS Adversarial approach for continuous/high-dim spaces, but accept higher computational cost and hyperparameter sensitivity.
  - **Assumption 4.4 (Rank):** Discrete mode requires $|Z|, |W| \ge |U|$. If this fails, you must use the continuous/RKHS approach or reduce dimensionality.
- **Failure signatures:**
  - **Exploding Gradients/Coefficients:** In continuous mode, if $\lambda_Q$ is too small, the matrix inversion becomes unstable.
  - **No Generalization:** If the proxy $Z=S_{t-1}$ is uninformative about $U_{t-1}$, the method will fit training data but output garbage policies under shift.
  - **BC outperforms CausIL:** Check if the measurement shift $P(W|U)$ is actually smaller than the population shift $P(U)$; in this specific regime, BC might get lucky.
- **First 3 experiments:**
  1. **Toy Validation (Discrete):** Implement the matrix inversion estimator on a small $4\times4$ grid world. Verify that the recovered $P(A^{(s)})$ matches the ground truth interventional distribution derived manually.
  2. **Shift Ablation:** Train on one environment, test on another with flipped sensor noise. Plot MSE of BC vs. CausIL to verify that BC diverges while CausIL remains flat.
  3. **Hyperparameter Sensitivity:** In the continuous setting, sweep $\lambda_Q$ and $\lambda_H$. Plot the condition number of the kernel matrix vs. policy error to find the stability region.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed proximal causal inference framework for imitation learning be extended to handle stochastic (non-deterministic) expert policies?
- **Basis in paper:** The paper explicitly focuses on deterministic policies and defines the optimal policy using an arg max over action probabilities, leaving stochastic policies unaddressed.
- **Why unresolved:** The theoretical identification and estimation results are derived assuming a deterministic expert mechanism; the confounding bridge function formulation may not directly transfer.
- **What evidence would resolve it:** A theoretical extension proving identification for stochastic policies, followed by an estimator and empirical validation on demonstrations with stochastic actions.

### Open Question 2
- **Question:** How sensitive is the method's performance to violations of the completeness assumption in the continuous setting?
- **Basis in paper:** Completeness is a key assumption for identification via the confounding bridge; its practical implications and robustness to near-violations are not analyzed.
- **Why unresolved:** Completeness is a functional condition difficult to verify empirically; the paper does not study the estimator's behavior when proxies are weakly informative.
- **What evidence would resolve it:** Simulation experiments systematically weakening the proxy-latent relationship, reporting performance degradation and potential diagnostics for detecting near-incompleteness.

### Open Question 3
- **Question:** Can the framework incorporate multiple distinct latent state components with different delay structures affecting the expert's action?
- **Basis in paper:** The model assumes a single latent state $U_{t-1}$ with a one-step delay; real-world settings may involve multiple latent factors with varying delays.
- **Why unresolved:** The identification strategy relies on a specific temporal structure with one treatment-inducing and one outcome-inducing proxy; generalizing this to multiple latent variables is non-trivial.
- **What evidence would resolve it:** Theoretical analysis of identifiability conditions with multiple latent components, possibly requiring additional proxies, and a corresponding estimator demonstrated in simulations.

### Open Question 4
- **Question:** Does the adversarial RKHS estimator provide finite-sample statistical guarantees for recovering the causal-optimal policy?
- **Basis in paper:** The continuous estimator is motivated by minimax learning but the paper provides no formal analysis of its estimation error or sample complexity.
- **Why unresolved:** Proximal causal inference estimators can be ill-posed; establishing finite-sample guarantees would require analyzing the interplay of the inverse problem and RKHS regularization.
- **What evidence would resolve it:** A theoretical proof bounding the imitation error between the estimated and true causal-optimal policy in terms of sample size and RKHS norms, complemented by empirical convergence curves.

## Limitations
- Proximal identification assumptions are unverifiable from observational data alone and untestable without ground truth latent states.
- Discrete estimator requires invertible matrices, which may fail for high-cardinality latent states depending on coarsening choices.
- Continuous RKHS estimator depends on kernel selection and regularization hyperparameters with uncharacterized sensitivity.
- Shift robustness claims assume $P(U_{t-1})$ stability and degrade if the latent state distribution changes drastically.

## Confidence
- **High Confidence:** The causal framework for handling noisy measurements and distribution shift is theoretically sound. The experimental results showing CausIL outperforming BC under measurement shift are compelling.
- **Medium Confidence:** The RKHS-based continuous estimator is theoretically justified, but practical implementation details and robustness are less established. The PhysioNet semi-simulation provides real-world relevance but introduces domain-specific uncertainties.
- **Low Confidence:** The completeness assumption's practical validity and the sensitivity of the discrete estimator to coarsening choices are not fully explored.

## Next Checks
1. **Bridge Function Existence Validation:** Generate synthetic data where the completeness assumption is deliberately violated (low-rank proxy structure) and verify that CausIL produces unstable or degenerate policies.
2. **Hyperparameter Sensitivity Analysis:** For the continuous estimator, systematically sweep $\lambda_H$ and $\lambda_Q$ across orders of magnitude and plot the condition number of the kernel matrices against policy performance to identify the stability region.
3. **Latent Distribution Shift Test:** Modify the synthetic DGP to include a shift in the latent state distribution $P(U_{t-1})$ between training and test, and measure the degradation of CausIL compared to BC.