---
ver: rpa2
title: A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams
  for Maximum Weighted Independent Set Problem
arxiv_id: '2512.15198'
source_url: https://arxiv.org/abs/2512.15198
tags:
- variable
- ordering
- mwisp
- variables
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a clustering-based framework to improve variable
  ordering in relaxed Decision Diagrams (DDs) for the Maximum Weighted Independent
  Set Problem (MWISP). Instead of applying dynamic variable ordering heuristics like
  MIN globally across all variables, the framework partitions variables into clusters
  using k-means clustering, then applies ordering strategies within this structure.
---

# A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem

## Quick Facts
- arXiv ID: 2512.15198
- Source URL: https://arxiv.org/abs/2512.15198
- Reference count: 16
- Primary result: Clustering framework reduces MWISP solution times by up to 2.5× compared to standard dynamic variable ordering

## Executive Summary
This paper introduces a clustering-based framework to improve variable ordering in relaxed Decision Diagrams (DDs) for the Maximum Weighted Independent Set Problem (MWISP). Instead of applying dynamic variable ordering heuristics like MIN globally across all variables, the framework partitions variables into clusters using k-means clustering, then applies ordering strategies within this structure. Two strategies are proposed: Cluster-by-Cluster (CbC), which processes clusters sequentially using aggregate criteria, and Pick-and-Sort (PaS), which iteratively selects and sorts representative variables from each cluster. Theoretical analysis on DD size growth for MWISP informs two policies for setting the number of clusters: adaptive and fixed. The framework is integrated into a DD-based branch-and-bound algorithm and evaluated on MWISP instances with varying graph densities. Results show consistent computational improvements over standard dynamic variable ordering baselines, particularly for denser graphs, demonstrating the framework's effectiveness in reducing solution times.

## Method Summary
The framework implements DD-based branch-and-bound for MWISP using DP formulation where states represent available vertices. The key innovation is clustering variables using k-means on features [degree, weight] before applying dynamic ordering heuristics. Two strategies process these clusters: CbC sorts clusters by aggregate weight and exhausts each sequentially, while PaS iteratively picks one representative from each cluster then sorts them. Cluster count is determined by fixed (n_c=2) or adaptive (n_c = Max(⌊n'/(2·⌈log₂ W⌉)⌋, 1)) policies, with the adaptive policy leveraging theoretical DD width bounds. The framework is tested on 180 benchmark instances across densities 0.1-0.9, comparing against baseline MIN variable ordering.

## Key Results
- Framework reduces solution times by 1.5-2.5× compared to standard MIN heuristic
- CbC with adaptive policy achieves best results on densities 0.4-0.9
- PaS with fixed policy excels on densities 0.2-0.3
- All approaches underperform baseline on density 0.1
- Theoretical adaptive policy based on DD width bounds effectively scales cluster count

## Why This Works (Mechanism)

### Mechanism 1: Search Space Reduction via Clustering
- **Claim**: Partitioning variables into clusters before applying dynamic ordering heuristics reduces per-layer computational complexity while maintaining bound quality within acceptable margins.
- **Mechanism**: The MIN heuristic evaluates each unfixed variable's frequency across all states in the current layer, incurring O(W·|V|) complexity per layer. By first clustering variables using k-means and applying the heuristic only within each cluster (of approximate size |V|/n_c), complexity reduces to O(W·|V|/n_c). This decomposition leverages the assumption that variables within a cluster share relevant structural properties, so intra-cluster ordering preserves enough solution-space information for quality bounds.
- **Core assumption**: The clustering features (e.g., vertex degree and weight for MWISP) capture problem structure sufficiently such that heuristic guidance within clusters does not severely degrade bound quality compared to global application.
- **Evidence anchors**:
  - [abstract]: "Instead of applying dynamic ordering heuristics to the full set of unfixed variables, we first partition variables into clusters. We then leverage this structural decomposition to guide the ordering process, significantly reducing the heuristic's search space."
  - [Section 3]: "Assume that the variables are clustered into n_c clusters and let |V|/n_c be the size of the clusters (approximately); then the complexity changes to O(W·|V|/n_c) per layer."
  - [corpus]: No direct corpus support found for clustering-based variable ordering in DDs; related work on ordering problems (e.g., "Unsupervised Ordering for Maximum Clique") addresses ordering but not within a DD context.
- **Break condition**: If the number of clusters exceeds the natural structural decomposition of the problem, intra-cluster heuristic evaluation loses discrimination power, potentially yielding worse bounds than baseline.

### Mechanism 2: Theoretical DD Size Bounds Inform Adaptive Cluster Policy
- **Claim**: The exponential growth pattern of DD layer widths for MWISP provides a principled basis for dynamically setting the number of clusters during branch-and-bound.
- **Mechanism**: Lemma 3 establishes that DD width for an n-variable MWISP instance grows to at most 2^⌊n/2⌋ at the middle layers, then decreases. This implies subproblems with n ≤ 2·⌈log₂ W⌉ variables never exceed maximum width W—such DDs are automatically exact. The adaptive policy n_c = Max(⌊n'/(2·⌈log₂ W⌉)⌋, 1) scales cluster count inversely with subproblem size, avoiding unnecessary clustering overhead on small subproblems while applying more aggressive decomposition to larger ones.
- **Core assumption**: The worst-case exponential bounds provide sufficiently tight approximations of actual DD growth for MWISP instances encountered in practice.
- **Evidence anchors**:
  - [Section 3.3]: "Lemma 3 implies that the width of no layer in a DD D for a MWISP (MISP) with the number of variables n ≤ 2·⌈log₂ W⌉, where W is the given maximum width, will exceed the given maximum width."
  - [Section 3.3]: "Therefore, for such subproblems, no clustering of variables is used. Another use of this property is that the number of clusters n_c cannot be fixed to a value greater than the size of the largest of such subproblems."
  - [corpus]: No direct corpus support found for DD width bounds informing adaptive variable ordering strategies.
- **Break condition**: If actual DD growth significantly deviates from the theoretical worst-case due to instance-specific structure (e.g., very sparse graphs with narrower layer growth), the adaptive policy may under-cluster or over-cluster.

### Mechanism 3: Strategy-Specific Trade-offs Between Locality and Diversity
- **Claim**: The Cluster-by-Cluster (CbC) and Pick-and-Sort (PaS) strategies capture distinct structural trade-offs, with effectiveness varying by graph density.
- **Mechanism**: CbC sorts clusters by aggregate criteria (e.g., cumulative vertex weights), then exhausts each cluster sequentially. This preserves intra-cluster locality but may delay consideration of structurally important variables in later clusters. PaS iteratively selects one representative variable from each cluster, then sorts the selected batch—either by problem-specific criteria (PaS) or by the heuristic values assigned during selection (PaS-VO). This "spreads" decisions across the problem structure, potentially capturing inter-cluster dependencies earlier in compilation.
- **Core assumption**: Denser graphs benefit more from the spreading effect of PaS because their solution structures involve more inter-cluster edges; sparser graphs may benefit from CbC's locality preservation.
- **Evidence anchors**:
  - [abstract]: "Cluster-to-Cluster, which processes clusters sequentially using problem-specific aggregate criteria... and Pick-and-Sort, which iteratively selects representative variables from each cluster to balance local diversity with heuristic guidance."
  - [Section 4, Table 1]: CbC with adaptive policy achieves best results on densities 0.4-0.9; PaS with fixed policy excels on densities 0.2-0.3; all approaches underperform baseline on density 0.1.
  - [corpus]: No direct corpus support comparing CbC vs. PaS ordering strategies within DD frameworks.
- **Break condition**: If graph structure lacks meaningful cluster separation (e.g., near-uniform degree/weight distributions), both strategies may introduce artificial constraints that degrade performance relative to baseline.

## Foundational Learning

- **Concept: Decision Diagrams as Layered DAGs**
  - **Why needed here**: The framework operates entirely on top-down DD compilation where each layer corresponds to one decision variable. Understanding that states represent partial solution information and arcs represent feasible decisions is prerequisite to grasping how variable ordering affects bound quality.
  - **Quick check question**: In a DD for MWISP with 4 variables, if a node at layer 3 has state {v₂, v₄}, what does this state represent?

- **Concept: Relaxed DDs and Valid Merge Operators**
  - **Why needed here**: The framework targets relaxed DDs for dual bounds. The merge operator ⊕ determines how states combine when width exceeds W. For MWISP, the valid merge operator is set union—merged states over-approximate available vertices.
  - **Quick check question**: If two nodes with states {v₁, v₃} and {v₂, v₃} are merged, what is the resulting state? Does this over-approximate, under-approximate, or exactly represent the union of their solution spaces?

- **Concept: Dynamic Variable Ordering Heuristics (MIN)**
  - **Why needed here**: The MIN heuristic is the baseline being improved. It selects the variable appearing in the fewest states of the current layer, aiming to minimize branching early. The clustering framework restricts MIN's evaluation scope.
  - **Quick check question**: If vertices v₁, v₂, v₃ appear in current-layer states with frequencies 3, 1, 4 respectively, which does MIN select and why?

## Architecture Onboarding

- **Component map**: Instance → Feature extraction → K-means clustering → Policy selects n_c → Strategy (CbC/PaS) invokes NextVariable on cluster subset → BuildLayer creates nodes → Width check → Merge if |N_k| > W → Repeat until terminal

- **Critical path**: Instance → Feature extraction → K-means clustering → Policy selects n_c → Strategy (CbC/PaS) invokes NextVariable on cluster subset → BuildLayer creates nodes → Width check → Merge if |N_k| > W → Repeat until terminal

- **Design tradeoffs**:
  - Higher n_c → faster compilation but potentially weaker bounds; lower n_c → slower but tighter bounds
  - CbC → locality preservation, better for structured problems; PaS → diversity, better for dense interconnected problems
  - Fixed policy → simple, predictable; Adaptive → scales with subproblem size but adds overhead
  - Problem-specific sorting → strong for MWISP, weak generalization; PaS-VO sorting → more portable across problems

- **Failure signatures**:
  - Solution time exceeds baseline: Check if n_c is too high or clustering features are irrelevant to problem structure
  - Root bound significantly weaker than baseline: Clustering may be destroying critical variable relationships; reduce n_c or redesign features
  - Performance collapses on specific density ranges: Per Table 1, density 0.1 is problematic—consider density-aware strategy switching
  - Adaptive policy produces n_c=1 for large subproblems: Check if W is set too large relative to n

- **First 3 experiments**:
  1. **Baseline comparison across densities**: Run all configurations (CbC/PaS/PaS-VO × fixed/adaptive) against MIN baseline on MWISP instances with densities 0.1-0.9. Report total solve time and root-node bound gap.
  2. **Feature ablation**: Test clustering with single features (degree only, weight only) vs. combined [degree, weight] on medium-density instances. Measure impact on both solution time and bound quality.
  3. **Cluster count sensitivity**: Fix strategy to CbC and vary n_c ∈ {2, 4, 8, 16} on a single density (0.5). Plot solution time and bound quality vs. n_c to identify optimal range.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the clustering-based variable ordering framework be effectively generalized to other combinatorial optimization problems beyond the Maximum Weighted Independent Set Problem (MWISP)?
- **Basis in paper**: [explicit] The conclusion states, "While we only considered MWISP, we believe that the proposed framework, policies, and theory can be generalized and therefore suggests promising potential for broader application to other combinatorial optimization problems."
- **Why unresolved**: The current implementation relies on MWISP-specific features (vertex degree and weight) for clustering and aggregate criteria (cumulative weights) for sorting. It is unclear if generic feature extraction methods can support this framework for problems with less intuitive structural properties.
- **What evidence would resolve it**: Successful integration of the framework into DD-based solvers for distinct problem classes (e.g., sequencing or knapsack problems) demonstrating similar computational speedups without extensive manual feature engineering.

### Open Question 2
- **Question**: Does defining variable similarity via an undirected graph and applying graph clustering algorithms improve performance over the current k-means feature-vector approach?
- **Basis in paper**: [explicit] The authors propose: "In a future work, it would be interesting to define a concept of similarity on the set of variables and then represent it via a simple undirected graph... And then use graph clustering algorithms for clustering of the variables."
- **Why unresolved**: The current study utilizes k-means clustering on a 2D feature vector. While effective, this requires pre-defined features. A graph-based similarity approach could offer a more robust, domain-agnostic way to group variables based on their direct interactions within the DD states.
- **What evidence would resolve it**: A comparative study measuring solution times and bound quality between the k-means approach and a graph-clustering approach (e.g., spectral clustering) across various problem densities.

### Open Question 3
- **Question**: Why does the Cluster-by-Cluster strategy degrade performance on very sparse graphs (density 0.1), and can this be mitigated?
- **Basis in paper**: [inferred] Table 1 shows that for graphs with density 0.1, the CbC strategies (fixed and adaptive) result in solution times (573s and 535s) significantly worse than the baseline (196s), whereas they outperform the baseline in all other densities.
- **Why unresolved**: The paper concludes general success but does not analyze the anomaly at density 0.1. The theoretical policies for cluster numbers (Lemma 3) or the loss of heuristic accuracy in sparse graphs may introduce overhead that outweighs the benefits for this specific structure.
- **What evidence would resolve it**: An analysis of the trade-off between heuristic computation time and bound quality for sparse graphs, or a modified adaptive policy that detects low density and defaults to the baseline dynamic ordering.

## Limitations

- Framework introduces computational overhead that may offset benefits for smaller instances or problems where global variable interactions are critical
- Performance is sensitive to feature selection and cluster count, with suboptimal choices potentially degrading solution quality
- Adaptive policy's reliance on theoretical worst-case bounds assumes these approximations hold for practical MWISP instances

## Confidence

- **High Confidence**: The theoretical analysis of DD size growth and its application to adaptive cluster policy is well-founded. The claim that subproblems with n ≤ 2·⌈log₂ W⌉ never exceed width W is mathematically proven and directly applicable.
- **Medium Confidence**: The mechanism by which clustering reduces heuristic complexity (O(W·|V|) → O(W·|V|/n_c)) is sound, but its practical impact depends on implementation details and problem structure not fully specified in the paper.
- **Medium Confidence**: The density-dependent performance differences between CbC and PaS strategies are observed but the underlying reasons (locality vs. diversity trade-offs) are hypothesized rather than empirically validated across diverse problem types.

## Next Checks

1. **Cluster Sensitivity Analysis**: Systematically vary n_c across a wider range (2-32) on medium-density instances to quantify the trade-off between compilation speed and bound quality, validating the adaptive policy's claimed optimality range.

2. **Cross-Problem Generalization**: Apply the framework to other combinatorial optimization problems (e.g., Maximum Clique, Graph Coloring) with appropriate feature engineering to assess whether the clustering mechanism generalizes beyond MWISP.

3. **Feature Ablation Study**: Compare clustering performance using different feature combinations (degree-only, weight-only, degree×weight) across all densities to quantify the contribution of each feature and identify scenarios where clustering might be detrimental.