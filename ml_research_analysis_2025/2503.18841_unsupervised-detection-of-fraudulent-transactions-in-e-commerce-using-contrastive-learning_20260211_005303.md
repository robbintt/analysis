---
ver: rpa2
title: Unsupervised Detection of Fraudulent Transactions in E-commerce Using Contrastive
  Learning
arxiv_id: '2503.18841'
source_url: https://arxiv.org/abs/2503.18841
tags:
- fraud
- learning
- data
- detection
- e-commerce
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes an unsupervised fraud detection method for e-commerce
  using SimCLR, a contrastive learning framework. Traditional supervised approaches
  require labeled data, which is difficult to obtain and struggles with evolving fraud
  patterns.
---

# Unsupervised Detection of Fraudulent Transactions in E-commerce Using Contrastive Learning

## Quick Facts
- arXiv ID: 2503.18841
- Source URL: https://arxiv.org/abs/2503.18841
- Authors: Xuan Li; Yuting Peng; Xiaoxuan Sun; Yifei Duan; Zhou Fang; Tengda Tang
- Reference count: 24
- One-line primary result: SimCLR achieves 0.9849 AUC, outperforming K-means (0.8521), Isolation Forest (0.8883), Autoencoders (0.9035), and VAEs (0.9426)

## Executive Summary
This study proposes an unsupervised fraud detection method for e-commerce using SimCLR, a contrastive learning framework. Traditional supervised approaches require labeled data, which is difficult to obtain and struggles with evolving fraud patterns. SimCLR learns representations by constructing positive and negative sample pairs without labels, enhancing fraud differentiation while reducing reliance on labeled datasets. Experiments on eBay transaction data show the model achieves an AUC of 0.9849, outperforming several baseline methods.

## Method Summary
The approach uses SimCLR to learn transaction representations through contrastive learning without labels. Data augmentation creates positive pairs by applying perturbations like noise and amount changes to transactions. The encoder network maps these views to latent representations, which are optimized using NT-Xent contrastive loss. After training, transactions are scored based on their average cosine similarity to others, with those falling below a statistical threshold flagged as fraudulent.

## Key Results
- SimCLR achieves AUC of 0.9849 on eBay transaction dataset
- Outperforms K-means (0.8521), Isolation Forest (0.8883), Autoencoders (0.9035), and VAEs (0.9426)
- Precision of 0.9321, recall of 0.8737, and F1-score of 0.9177

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning creates separable representations by pulling augmented views of the same transaction together while pushing different transactions apart.
- Mechanism: SimCLR constructs positive pairs (two augmented views of the same transaction) and negative pairs (views from different transactions), then optimizes a contrastive loss that maximizes cosine similarity within positive pairs and minimizes it across negative pairs. This forces the encoder to learn features that distinguish transaction patterns.
- Core assumption: Fraudulent transactions will exhibit representations that are systematically dissimilar from the majority (normal) transaction distribution after contrastive pretraining.
- Evidence anchors:
  - [abstract] "SimCLR learns representations by constructing positive and negative sample pairs without labels, enhancing fraud differentiation"
  - [section III] "The core idea of SimCLR is to learn the representation of data through contrastive loss functions so that similar samples are close in the latent space and different samples are far away"
- Break condition: If fraudulent transactions are not outliers in the learned representation space (e.g., they cluster with normal transactions due to feature overlap or adversarial adaptation), the similarity-based detection will fail.

### Mechanism 2
- Claim: Data augmentation on tabular transaction data creates semantically equivalent views that enable self-supervised representation learning.
- Mechanism: Random perturbations—adding noise, modifying transaction amounts—are applied to each transaction to generate two views (A and B). The encoder must learn representations invariant to these perturbations while remaining sensitive to structural differences between transactions.
- Core assumption: The augmentations preserve the semantic identity of transactions (a perturbed fraudulent transaction remains fraudulent) and realistic noise levels do not destroy discriminative patterns.
- Evidence anchors:
  - [section III] "We perform random data augmentation operations on the e-commerce transaction data, including perturbing the original transaction data (such as adding noise, randomly changing the transaction amount, etc.)"
- Break condition: If augmentations destroy critical fraud signals or create unrealistic transaction representations, the encoder may learn trivial invariances that do not generalize to fraud detection.

### Mechanism 3
- Claim: Post-hoc anomaly scoring using distributional similarity statistics identifies outliers as fraudulent transactions.
- Mechanism: After training, each transaction's latent representation is compared to all others via cosine similarity. Mean similarity (μ) and standard deviation (σ) are computed; transactions falling below threshold (μ - σ) are flagged as anomalous.
- Core assumption: Fraudulent transactions are rare and their representations will have significantly lower average similarity to the population than normal transactions.
- Evidence anchors:
  - [section III] "If the similarity of sample h_i is lower than that of most other samples, that is, its potential representation is significantly different from other normal transactions, then the transaction is considered abnormal"
  - [section IV, Table 1] Precision 0.9321, Recall 0.8737, F1 0.9177 on eBay dataset
- Break condition: If normal transactions exhibit high variance (causing false positives) or fraud patterns drift to resemble normal representations (causing false negatives), the fixed threshold scheme will degrade.

## Foundational Learning

- Concept: **Contrastive Learning (SimCLR)**
  - Why needed here: Core framework enabling unsupervised representation learning by treating augmented views of the same sample as positive pairs and all other samples as negatives.
  - Quick check question: Can you explain why increasing temperature τ makes the softmax over similarities softer, and how this affects gradient flow?

- Concept: **Representation Learning for Tabular Data**
  - Why needed here: Transactions are high-dimensional, mixed-type records; mapping them to dense latent vectors enables similarity-based reasoning without hand-crafted rules.
  - Quick check question: What properties should a good transaction representation have for downstream anomaly detection?

- Concept: **Statistical Thresholding for Anomaly Detection**
  - Why needed here: Converts continuous similarity scores into binary fraud/normal decisions using distributional statistics (μ, σ).
  - Quick check question: Why might a fixed threshold (μ - σ) fail under concept drift, and what adaptive alternatives exist?

## Architecture Onboarding

- Component map:
  Input features -> Data augmentation -> Encoder network -> Contrastive loss -> Latent representations -> Cosine similarity scoring -> Statistical thresholding -> Fraud detection

- Critical path:
  1. Data standardization (mean=0, variance=1 per feature)
  2. Augmentation pipeline generating views A and B
  3. Encoder training via contrastive loss minimization
  4. Extraction of latent representations for all transactions
  5. Pairwise similarity computation and threshold-based flagging

- Design tradeoffs:
  - Augmentation strength: Stronger perturbations increase robustness but risk destroying fraud signals
  - Temperature τ: Lower values sharpen the contrastive objective but may cause training instability
  - Threshold selection: μ - σ is heuristic; tighter thresholds increase precision at recall cost
  - Batch size: SimCLR benefits from large batches (more negatives); constrained by memory

- Failure signatures:
  - High false positive rate: Normal transactions exhibit representation drift or high intra-class variance
  - High false negative rate: Fraud representations converge toward normal cluster (adversarial adaptation or concept drift)
  - Training collapse: All representations converge to identical vectors (temperature too low, insufficient negatives)
  - Poor generalization: Model overfits to augmentation artifacts rather than semantic transaction patterns

- First 3 experiments:
  1. **Baseline sanity check**: Run SimCLR with identity augmentation (no perturbation) to verify that the contrastive objective is actually driving learning, not trivial shortcuts.
  2. **Augmentation ablation**: Test individual augmentation types (noise only, amount scaling only, combined) to identify which preserves fraud signals while creating meaningful positive pairs.
  3. **Threshold sensitivity analysis**: Sweep threshold values (μ - kσ for k ∈ [0.5, 1.0, 1.5, 2.0]) and plot precision-recall tradeoffs to understand operational limits before deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's detection capability scale and generalize when applied to significantly larger and more diverse e-commerce datasets?
- Basis in paper: [explicit] The conclusion states, "Future research could explore the use of larger and more representative datasets to improve the model's generalization ability and robustness."
- Why unresolved: The current study validates the approach on a specific eBay dataset, but the authors acknowledge that dataset scale and diversity directly impact performance.
- What evidence would resolve it: Benchmarking results from training the model on multi-platform datasets with varying transaction volumes and fraud definitions.

### Open Question 2
- Question: Can the proposed SimCLR framework be effectively integrated into real-time transaction monitoring systems without inducing prohibitive latency?
- Basis in paper: [explicit] The abstract and conclusion suggest the model "could be integrated with real-time monitoring systems," implying this architectural adaptation has not yet been implemented.
- Why unresolved: The paper focuses on offline accuracy metrics (AUC, F1) but does not provide throughput or latency analysis required for real-time application.
- What evidence would resolve it: Latency benchmarks and system performance metrics demonstrating the model's ability to process streaming transaction data within operational time limits.

### Open Question 3
- Question: To what extent do specific data augmentation strategies (e.g., random amount perturbation) risk violating the semantic invariance required for effective contrastive learning?
- Basis in paper: [inferred] The method section describes creating positive pairs by "randomly changing the transaction amount," which assumes the fraud label remains invariant.
- Why unresolved: In financial contexts, altering transaction amounts could theoretically change a normal transaction into a fraudulent-looking one (or vice versa), potentially introducing noise that undermines the learned representations.
- What evidence would resolve it: An ablation study analyzing the correlation between augmentation magnitude and the preservation of ground-truth fraud labels in the latent space.

## Limitations

- Encoder architecture and training hyperparameters are unspecified, requiring assumptions that may affect reproducibility
- No validation of whether the contrastive objective actually separates fraud from normal transactions in the latent space
- Thresholding approach assumes fraud representations are outliers, but may fail under concept drift or adversarial adaptation
- Data augmentation strategy for tabular transactions is heuristic with limited validation
- eBay dataset access is restricted, preventing direct replication

## Confidence

- **High confidence**: SimCLR framework can learn separable representations for fraud detection (validated by strong AUC result)
- **Medium confidence**: Augmentation strategy preserves fraud signals while enabling contrastive learning (weak empirical support)
- **Medium confidence**: Statistical thresholding effectively identifies fraud in the learned representation space (reasonable but untested under drift)

## Next Checks

1. **Latent space validation**: Visualize learned representations (t-SNE/UMAP) to confirm fraud/normal separation after contrastive training
2. **Augmentation ablation study**: Test individual augmentation types and strengths to identify which preserve fraud signals most effectively
3. **Threshold sensitivity analysis**: Sweep threshold values and plot precision-recall curves to understand operational limits and robustness to distribution shifts