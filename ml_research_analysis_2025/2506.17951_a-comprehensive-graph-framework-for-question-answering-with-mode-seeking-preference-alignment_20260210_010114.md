---
ver: rpa2
title: A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference
  Alignment
arxiv_id: '2506.17951'
source_url: https://arxiv.org/abs/2506.17951
tags:
- graph
- documents
- layer
- layers
- graphmpa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GraphMPA, a comprehensive graph-based framework
  for question answering that combines hierarchical document graphs with mode-seeking
  preference alignment. The method constructs a multi-layer graph using document similarity
  and community summarization to capture both low- and high-level knowledge, then
  applies mode-seeking preference optimization using small-scale LLMs to better align
  with human preferences.
---

# A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment

## Quick Facts
- arXiv ID: 2506.17951
- Source URL: https://arxiv.org/abs/2506.17951
- Reference count: 40
- Key outcome: GraphMPA achieves state-of-the-art performance with ROUGE F1 scores up to 0.3873, accuracy up to 51.76%, and MIRAGE scores up to 68.66%, significantly outperforming baselines including RAPTOR, LightGraphRAG, and Reward-RAG.

## Executive Summary
GraphMPA introduces a comprehensive graph-based framework for question answering that combines hierarchical document graphs with mode-seeking preference alignment. The method constructs a multi-layer graph using document similarity and community summarization to capture both low- and high-level knowledge, then applies mode-seeking preference optimization using small-scale LLMs to better align with human preferences. Experiments on six datasets show GraphMPA achieves state-of-the-art performance with ROUGE F1 scores up to 0.3873, accuracy up to 51.76%, and MIRAGE scores up to 68.66%, significantly outperforming baselines including RAPTOR, LightGraphRAG, and Reward-RAG.

## Method Summary
GraphMPA builds a hierarchical document graph by splitting large documents into chunks, embedding them with BGE-M3, and connecting them via cosine similarity to form layer-0. The Leiden algorithm detects communities, which are summarized to form layer-1 nodes, repeating for L layers. For preference alignment, GraphMPA generates synthetic training data using small-scale LLMs (7-8B parameters) with Chain-of-Thought reasoning to create "chosen" (reason + answer) and "rejected" (answer only) pairs. Training uses mode-seeking loss, minimizing reverse KL divergence to concentrate on the primary mode of human preferences rather than averaging across multiple modes. The retrieval engine queries across all graph layers, returning merged context for generation.

## Key Results
- ROUGE F1 scores up to 0.3873 on QASPER, accuracy up to 51.76% on QuALITY and RiddleSense, MIRAGE scores up to 68.66% on medical datasets
- Significant performance gains over baselines including RAPTOR, LightGraphRAG, and Reward-RAG
- RAG ablation experiments show up to 14.95% degradation when removing the retrieval component
- Performance peaks at 2-3 graph layers, with diminishing returns beyond that depth

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Document Graph with Community Summarization
Organizing documents into a multi-layer graph with community-based summarization captures both fine-grained details and high-level themes, improving retrieval quality for complex queries. Large documents are split into chunks, embedded, and connected via cosine similarity (threshold τ). The Leiden algorithm detects communities, which are summarized to form the next graph layer. This iterates for L layers, creating a hierarchy from raw segments to abstract summaries. Semantic similarity in embedding space correlates with meaningful topical relationships that benefit downstream retrieval. When τ is set too high (sparse graph) or too low (noisy edges), or when excessive summarization at deep L makes content overly abstract, performance degrades.

### Mechanism 2: Mode-Seeking Preference Optimization
Minimizing reverse KL divergence (mode-seeking) instead of forward KL divergence (mean-seeking, used in DPO) produces outputs that concentrate on the primary patterns of human preference rather than averaging across multiple modes. Standard DPO minimizes D_KL(π*‖π), encouraging the policy to cover all modes. MS loss minimizes D_KL(π‖π*), concentrating probability mass on the dominant mode of the optimal distribution. This is formalized in Equation 8 with probability-matching constraints between the parametrized policy and optimal policy. Human preference distributions have a primary mode worth prioritizing; mean-seeking produces "compromised" outputs that lack clear focus. If the true human preference distribution is genuinely multimodal with no dominant mode, mode-seeking may fail to capture legitimate preference variance.

### Mechanism 3: Small-Scale LLM Synthetic Preference Data
High-quality preference training data can be synthesized using 7-8B parameter LLMs with Chain-of-Thought reasoning, eliminating dependence on expensive large-scale models like GPT-4. For each query, retrieve top-k documents and generate paired responses: "chosen" includes reasoning chain + answer; "rejected" contains only the answer. Multiple small LLMs (Qwen2.5-7B, LLaMA3-8B, Mistral-8B) with CoT generate diverse training samples. Varying k values create contexts from strongly to weakly correlated. Responses with explicit reasoning better represent human-preferred outputs than answers alone; small-scale LLMs can approximate this sufficiently. If small-scale LLMs have systematic reasoning biases, these propagate into the preference dataset, potentially misaligning rather than aligning the model.

## Foundational Learning

- **Concept: KL Divergence (Forward vs. Reverse)**
  - Why needed here: The entire mode-seeking vs. mean-seeking distinction hinges on understanding that D_KL(P‖Q) ≠ D_KL(Q‖P) and that reverse KL concentrates on modes while forward KL covers support.
  - Quick check question: Given a bimodal target distribution, which KL direction would produce a unimodal approximation centered on one peak vs. a broad distribution covering both?

- **Concept: Graph Community Detection**
  - Why needed here: The Leiden algorithm organizes document chunks into coherent groups for summarization; understanding community detection explains why the hierarchy captures meaningful structure.
  - Quick check question: What property makes a set of nodes a "community" in a graph, and why would grouping semantically similar documents help retrieval?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: The paper positions its MS loss as an improvement over DPO; understanding DPO's reward model elimination clarifies what's being modified.
  - Quick check question: How does DPO avoid training a separate reward model, and what approximation does it make to the RLHF objective?

## Architecture Onboarding

- **Component map:**
  Document Processing Pipeline: TextSplit → Embed (BGE-M3) → Build similarity matrix → Community detection (Leiden) → Summarize (LLM) → Repeat for L layers
  Hierarchical Graph Store: Multi-layer graph where layer-0 contains raw chunks, layer-1+ contains community summaries
  Retrieval Engine: Query embedding → Top-k similarity search across ALL layers → Return merged context
  Preference Alignment Training: Synthetic data generation (chosen/rejected pairs) → MS loss optimization via trl library

- **Critical path:**
  1. Offline: Build hierarchical graph from corpus (hours depending on size; requires LLM for summarization)
  2. Offline: Generate preference dataset from training queries + retrieved contexts
  3. Offline: Train model with MS loss (2 epochs, lr=1e-5, batch_size=4 per paper)
  4. Online: Query → Retrieve top-k from graph → Generate with (optionally fine-tuned) model

- **Design tradeoffs:**
  - Graph depth (L): Paper shows 2-3 layers optimal; deeper risks over-abstraction
  - Similarity threshold (τ): Paper uses ~0.5; Figure 9 shows ~1 point variance across 0.0-1.0 range
  - Top-k retrieval: Higher k adds context but increases noise; trained models degrade slower than untrained (Figure 5)
  - Small vs. large LLMs for synthesis: Trade resource cost against potential quality gap with GPT-4-scale generation

- **Failure signatures:**
  - Retrieval component ablation causes largest performance drop (Table 3: up to 14.95% degradation)
  - Untrained models show sharp accuracy decline as k increases (Figure 5)
  - Layer-2-only retrieval underperforms layer-1-only, but both underperform combined (Table 7)

- **First 3 experiments:**
  1. Baseline reproduction: Implement basic RAG with flat retrieval on QASPER or PubMedQA; measure ROUGE/Accuracy
  2. Ablation on graph depth: Compare n_layers={1, 2, 3} holding τ=0.5 and top-k=10 constant; expect peak at 2-3 layers
  3. Loss comparison: Train identical models with DPO vs. MS loss on the same synthetic preference data; compare log probability distributions as in Figure 6

## Open Questions the Paper Calls Out
None

## Limitations
- Claims about generalizability beyond QA tasks lack supporting evidence, particularly assertions about extending to summarization, translation, and sentiment analysis
- Mode-seeking assumption may fail for genuinely multimodal human preferences with no dominant mode
- Synthetic data generation depends heavily on small-scale LLM reasoning quality, introducing potential systematic biases
- Graph construction threshold τ=0.5 appears arbitrary with no exploration of sensitivity across different domains or embedding spaces

## Confidence
- **High Confidence:** The hierarchical graph construction mechanism and its implementation details are well-specified and reproducible. The RAG ablation experiments showing significant performance drops are methodologically sound.
- **Medium Confidence:** The mode-seeking preference optimization's mathematical formulation is correct, but its practical advantage over DPO needs more empirical validation across diverse preference distributions. The synthetic data generation approach is plausible but untested against human-annotated preferences.
- **Low Confidence:** Claims about the framework's generalizability beyond QA tasks, particularly the assertion that it can be extended to summarization, translation, and sentiment analysis, lack supporting evidence.

## Next Checks
1. **Cross-Domain Embedding Sensitivity:** Test GraphMPA with multiple embedding models (e.g., BGE-M3, SBERT, OpenAI embeddings) on the same datasets to measure performance variance and determine if the 0.5 threshold is universally optimal.
2. **Preference Distribution Analysis:** Conduct experiments where human annotators provide preference data for the same query sets, then compare the learned distributions from MS loss vs. DPO to empirically verify the mode-seeking vs. mean-seeking behavior claims.
3. **Real-World Resource Evaluation:** Implement GraphMPA on a dataset requiring document preprocessing exceeding 10K documents to measure actual computational costs, graph construction time, and storage requirements, validating the claimed efficiency advantages.