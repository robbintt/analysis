---
ver: rpa2
title: Improving Retrospective Language Agents via Joint Policy Gradient Optimization
arxiv_id: '2503.01490'
source_url: https://arxiv.org/abs/2503.01490
tags:
- agent
- planner
- learning
- reflector
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving the performance
  of open-source language models in autonomous agent tasks, which often struggle to
  match the capabilities of closed-source models like ChatGPT. The core method, RetroAct,
  introduces a framework that jointly optimizes task-planning and self-reflective
  capabilities in language agents through a two-stage process combining imitation
  learning and reinforcement learning.
---

# Improving Retrospective Language Agents via Joint Policy Gradient Optimization

## Quick Facts
- arXiv ID: 2503.01490
- Source URL: https://arxiv.org/abs/2503.01490
- Reference count: 40
- Primary result: RetroAct achieves 22.3% to 348.3% performance gains over baselines across three environments

## Executive Summary
This paper addresses the challenge of improving open-source language models in autonomous agent tasks, which typically underperform compared to closed-source models like ChatGPT. The authors propose RetroAct, a framework that jointly optimizes task-planning and self-reflective capabilities in language agents through a two-stage process combining imitation learning and reinforcement learning. The approach uses an off-policy joint policy gradient optimization algorithm with imitation learning regularization to enhance data efficiency and training stability. Experimental results across three representative environments demonstrate substantial improvements, with performance gains ranging from 22.3% to 348.3% over baseline methods, achieving results comparable to or exceeding those of ChatGPT-based agents.

## Method Summary
RetroAct introduces a two-stage framework that first pretrains planner and reflector components using imitation learning on high-quality trajectories from ChatGPT, then jointly optimizes them through off-policy policy gradient with imitation learning regularization. The joint optimization algorithm alternates between optimizing the planner policy using advantage-weighted gradients and updating the reflector policy using the planner's improved trajectories. This design enables mutual facilitation between planning and reflection capabilities while maintaining training stability through regularization. The framework addresses the limitations of separately trained components and the data inefficiency of pure reinforcement learning approaches.

## Key Results
- Achieves 348.3% improvement on HotpotQA compared to traditional offline RL methods
- Matches or exceeds ChatGPT-based agent performance on ALFWorld and InterCode tasks
- Demonstrates 22.3% improvement over joint optimization without imitation learning regularization

## Why This Works (Mechanism)
The joint optimization framework enables planner and reflector components to evolve together, allowing the reflector to adapt to the planner's improved decision-making while the planner benefits from more accurate feedback through enhanced reflection. The imitation learning regularization provides stable gradients early in training by anchoring policies to high-quality demonstrations, preventing catastrophic degradation during exploration. The off-policy formulation enables efficient use of historical trajectories, reducing sample complexity compared to on-policy methods. This synergistic interaction between components creates a positive feedback loop where improvements in one component amplify gains in the other.

## Foundational Learning
- **Imitation Learning**: Learning from expert demonstrations; needed to bootstrap policies with high-quality initial behaviors and provide stable gradients during early training stages
- **Reinforcement Learning**: Learning through environmental interaction and reward feedback; required for agents to discover optimal strategies beyond demonstrated behaviors
- **Joint Policy Optimization**: Simultaneous optimization of multiple interdependent policies; essential for enabling mutual facilitation between planner and reflector components
- **Off-Policy Learning**: Learning from historical trajectories regardless of current policy; critical for data efficiency and stable training when using mixed demonstration and interaction data
- **Policy Gradient Methods**: Direct optimization of expected return through gradient ascent; necessary for fine-tuning policies based on environmental feedback
- **Advantage Weighting**: Normalizing rewards by baseline values; helps reduce variance in policy updates and stabilize learning

## Architecture Onboarding

**Component Map**: Planner -> Environment -> Reflector -> Planner (cyclic)

**Critical Path**: Input state → Planner selects action → Environment transitions → Reward/termination → Reflector generates reflection → Planner updates

**Design Tradeoffs**: Joint vs. sequential optimization (joint provides mutual facilitation but higher complexity), imitation learning regularization (improves stability but may limit exploration), off-policy vs. on-policy (data efficiency vs. potential bias)

**Failure Signatures**: 
- Divergence during training indicates insufficient regularization or learning rate issues
- Stagnant performance suggests exploration problems or poor reward shaping
- Performance collapse on validation tasks indicates overfitting to training trajectories

**First Experiments**:
1. Run ablation study removing imitation learning regularization to measure stability impact
2. Test planner-only and reflector-only variants to quantify individual component contributions
3. Evaluate on out-of-distribution tasks to assess generalization beyond training environments

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on high-quality ChatGPT trajectories creates dependency on closed-source models
- Computational overhead of joint optimization may limit deployment in resource-constrained settings
- Performance comparisons depend on specific task configurations that may not generalize universally

## Confidence
- High confidence: Joint optimization of planner and reflector leads to performance improvements
- Medium confidence: Achieving performance comparable to or exceeding ChatGPT-based agents
- Medium confidence: Mutual facilitation between planner and reflector components

## Next Checks
1. Test RetroAct's performance on tasks where ChatGPT trajectories are unavailable or of poor quality
2. Conduct comprehensive computational cost analysis comparing joint optimization against sequential training approaches
3. Perform cross-domain transfer experiments to evaluate generalization beyond specific tested environments