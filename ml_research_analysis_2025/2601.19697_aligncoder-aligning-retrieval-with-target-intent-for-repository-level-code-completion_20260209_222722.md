---
ver: rpa2
title: 'AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code
  Completion'
arxiv_id: '2601.19697'
source_url: https://arxiv.org/abs/2601.19697
tags:
- code
- arxiv
- completion
- wang
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AlignCoder addresses repository-level code completion challenges
  by introducing a query enhancement mechanism and reinforcement learning-based retriever
  training. The framework generates multiple candidate completions to construct an
  enhanced query that bridges the semantic gap between initial queries and target
  code, then trains an AlignRetriever to leverage inference information for more accurate
  retrieval.
---

# AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion

## Quick Facts
- arXiv ID: 2601.19697
- Source URL: https://arxiv.org/abs/2601.19697
- Reference count: 40
- Improves EM score by 18.1% over baselines on CrossCodeEval Python benchmark

## Executive Summary
AlignCoder addresses repository-level code completion challenges by introducing a query enhancement mechanism and reinforcement learning-based retriever training. The framework generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between initial queries and target code, then trains an AlignRetriever to leverage inference information for more accurate retrieval. Evaluated on CrossCodeEval and RepoEval benchmarks across five backbone models, AlignCoder achieves an 18.1% improvement in EM score compared to baselines on CrossCodeEval Python, demonstrating high generalizability across various code LLMs and programming languages.

## Method Summary
AlignCoder constructs a dual-context codebase using Split-Aggregate for base snippets and tree-sitter for dependency snippets. It employs BM25 for coarse retrieval, then generates k candidate completions using a lightweight LLM. These candidates are concatenated with unfinished code to form an enhanced query. An AlignRetriever, trained via reinforcement learning with perplexity-based rewards, performs fine-grained retrieval on this enhanced query. The final completion is generated using the retrieved context. The method uses 10K Python+Java repositories for training and is evaluated on CrossCodeEval and RepoEval benchmarks.

## Key Results
- Achieves 18.1% improvement in EM score on CrossCodeEval Python compared to baselines
- Generalizes across five different backbone models and two programming languages (Python, Java)
- Outperforms RLCoder by 17.4% in EM score when RL training is applied

## Why This Works (Mechanism)

### Mechanism 1: Query Enhancement via Multiple Candidate Sampling
Generating multiple candidate completions and concatenating them with unfinished code creates an enhanced query that has higher probability of containing key tokens relevant to the target code. A sampler LLM generates k candidates based on coarsely-retrieved context. These candidates are appended to unfinished code to form an enhanced query. Per quasi-independence assumptions, this increases the probability that at least one candidate contains target-relevant tokens, improving subsequent fine-grained retrieval. The optimal threshold is approximately 4 samples.

### Mechanism 2: RL-Based Retriever Alignment with Perplexity Rewards
Training the dense retriever via reinforcement learning with perplexity-based rewards teaches it to retrieve snippets that maximize target-code generation quality, not just semantic similarity. The retriever produces top-k snippets given an enhanced query. A reward model computes perplexity of generating the target code conditioned on each snippet. The reward is maximized when low-perplexity snippets are ranked higher. RL updates retriever parameters accordingly. The reward signal assumes lower perplexity correlates with genuinely helpful retrieved context for correct completion.

### Mechanism 3: Dual-Context Codebase Construction
Separating retrieval corpus into base snippets (structural code blocks) and dependency snippets (imported signatures) provides complementary retrieval targets capturing both syntactic patterns and semantic hierarchies. Base snippets use Split-Aggregate to preserve continuity within bounded length; dependency snippets use tree-sitter to resolve intra-repository imports into class/method signatures. Both are indexed for retrieval. Cross-file imports frequently contain the critical context needed for completing API calls and class instantiations.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG) for Code**
  - Why needed: AlignCoder builds on vanilla RAG by adding query enhancement and RL-based retriever training; understanding the baseline paradigm is prerequisite
  - Quick check: Can you trace how a standard RAG pipeline retrieves code snippets and injects them into an LLM prompt for completion?

- **Concept: Perplexity as a Quality Signal**
  - Why needed: The reward model uses perplexity to evaluate retrieval quality; understanding what PPL measures—and its limitations—is essential for debugging reward design
  - Quick check: If a completion has perplexity 5 vs. 50, which is more likely correct, and under what conditions might this heuristic mislead?

- **Concept: Reinforcement Learning with Learned Reward Models**
  - Why needed: AlignRetriever is trained via RL with a PPL-derived reward; understanding policy gradients and reward noise is necessary to diagnose training instability
  - Quick check: In RL training, what happens if the reward signal is weakly correlated with the true objective?

## Architecture Onboarding

- **Component map:** Codebase Constructor -> Coarse Retriever -> Sampler -> Query Enhancer -> AlignRetriever -> Reward Model -> Generator
- **Critical path:** 1) Unfinished code → BM25 → coarse context; 2) Coarse context + unfinished code → Sampler → k candidates; 3) Unfinished code + k candidates → enhanced query; 4) Enhanced query → AlignRetriever → fine-grained snippets; 5) Fine-grained snippets + unfinished code → Generator → final completion
- **Design tradeoffs:** k (sampling number): higher k increases token coverage but adds noise/latency; temperature/top-p controls candidate diversity; snippet length L provides richer context but may dilute precision
- **Failure signatures:** Low EM with high ES indicates syntactically similar but wrong completions; performance collapse when k>4 shows noise from incorrect candidates degrades retrieval; minimal drop in w/o RL ablation suggests enhanced query alone may be sufficient
- **First 3 experiments:** 1) Reproduce Table I (DeepSeekCoder-1B on CrossCodeEval Python) to verify ~18% EM improvement; 2) Sampling ablation (k=1,2,3,4,5,6) to confirm k=4 optimal; 3) PPL–EM correlation analysis to validate reward signal reliability

## Open Questions the Paper Calls Out

### Open Question 1
Can the number of candidate samples (k) be dynamically adjusted based on the complexity or ambiguity of the unfinished code rather than relying on a fixed optimal threshold (e.g., k=4)? The paper theoretically formulates an optimal sampling threshold n* dependent on success probability and correlation, but empirically resorts to a fixed value of 4 for all tasks.

### Open Question 2
How does the performance of AlignCoder degrade when the lightweight sampler LLM is significantly weaker than the generator LLM, particularly in low-resource programming languages? The method relies on a "lightweight LLM" as a sampler to generate candidate completions. If the sampler lacks the capability to produce relevant key tokens, the AlignRetriever may receive low-quality or noisy input.

### Open Question 3
What is the acceptable latency trade-off for the query enhancement mechanism in real-time IDE coding assistants? The framework requires generating k candidates and computing PPL rewards, a process the authors accelerate using vLLM. However, code completion is a latency-sensitive task where users expect suggestions within milliseconds.

## Limitations
- Reward signal may optimize for surface fluency rather than functional correctness without rigorous PPL-EM correlation validation
- Sampling enhancement mechanism lacks theoretical bounds on optimal diversity-temperature tradeoff
- Dependency extraction via tree-sitter may fail on non-standard or incomplete code, limiting robustness across diverse codebases

## Confidence
- **High Confidence:** Query enhancement mechanism effectiveness (k=4 optimal), dual-context extraction design, and overall EM/ES improvements on benchmarks
- **Medium Confidence:** RL-based retriever training with PPL rewards, cross-model generalizability claims, and repository-level generalization
- **Low Confidence:** Theoretical justification for sampling diversity assumptions, reward signal reliability correlation with correctness, and tree-sitter dependency extraction robustness

## Next Checks
1. Compute Pearson/Spearman correlation between snippet-level perplexity scores and final EM scores on held-out validation set to empirically verify reward signal quality
2. Run tree-sitter import resolution on codebases with varying syntax styles to quantify extraction failure rates and impact on EM
3. Compare AlignCoder performance using alternative reward signals against PPL-based rewards to isolate whether perplexity uniquely captures retrieval quality