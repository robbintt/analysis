---
ver: rpa2
title: 'ConPress: Learning Efficient Reasoning from Multi-Question Contextual Pressure'
arxiv_id: '2602.01472'
source_url: https://arxiv.org/abs/2602.01472
tags:
- reasoning
- uni00000013
- conpress
- arxiv
- multi-question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large reasoning models often produce unnecessarily long chain-of-thought
  traces, increasing inference costs. This work identifies a phenomenon where models
  naturally generate shorter reasoning traces when solving multiple questions in a
  single prompt, due to contextual pressure.
---

# ConPress: Learning Efficient Reasoning from Multi-Question Contextual Pressure

## Quick Facts
- **arXiv ID**: 2602.01472
- **Source URL**: https://arxiv.org/abs/2602.01472
- **Reference count**: 40
- **Primary result**: ConPress reduces reasoning token usage by 33–59% with minimal accuracy loss on mathematical reasoning benchmarks.

## Executive Summary
Large reasoning models often generate unnecessarily long chain-of-thought traces, increasing inference costs. This work identifies that presenting multiple independent questions in a single prompt induces the model to produce shorter per-question reasoning traces—a phenomenon termed "contextual pressure." ConPress leverages this by sampling multi-question outputs, filtering for correct answers, extracting the compressed reasoning traces, and fine-tuning on these as self-supervised targets. The method achieves significant token savings (33–59%) while maintaining competitive accuracy across multiple models and benchmarks, without requiring external teachers, manual pruning, or reinforcement learning.

## Method Summary
ConPress is a self-supervised fine-tuning approach that exploits contextual pressure to learn compressed reasoning. The method samples N questions (typically N=3) per prompt, generates multi-question outputs, parses per-question reasoning traces, filters for correctness, and trains on the resulting (question, compressed_trace) pairs using standard supervised fine-tuning. The approach is simpler than RL-based methods and achieves strong token reduction with minimal accuracy loss.

## Key Results
- 33–59% token reduction on mathematical reasoning benchmarks while maintaining competitive accuracy
- Effective across multiple models including Qwen2.5, DeepSeek-R1, and others
- Requires only ~8k examples for effective fine-tuning
- Outperforms or matches RL-based compression methods without external teachers or manual pruning

## Why This Works (Mechanism)

### Mechanism 1
Multi-question prompts induce shorter per-question reasoning traces through contextual pressure. When N independent questions share a single context window, the model's generation dynamics shift—local continuations favor concise reasoning paths over extended elaboration. The paper hypothesizes this is a context-induced preference shift rather than explicit resource allocation. Evidence shows leftward shift in reasoning-length distributions from N=1 to N=2, and multi-question settings produce much stronger compression than prompt modifications without additional questions.

### Mechanism 2
Correctness-filtered self-generated traces provide viable supervision for compressed reasoning. Multi-question sampling elicits compressed traces; rejection sampling discards incorrect outputs. The remaining traces preserve core reasoning steps while removing redundancy. These serve as SFT targets without external teachers or manual pruning. The paper notes this rejection step is necessary to counteract accuracy degradation under multi-question contexts.

### Mechanism 3
Supervised fine-tuning on compressed traces transfers concise behavior to single-question inference. Standard next-token prediction on (question, compressed_trace) pairs internalizes the shorter reasoning patterns. The paper reports 33–59% token reduction with minimal accuracy loss. Related methods (DPO_shortest, LC-R1, AdaptThink) achieve compression via RL or preference optimization; ConPress matches compression with simpler SFT.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) reasoning and "overthinking"**
  - Why needed here: The paper targets unnecessary verbosity in CoT traces. Understanding that LRMs often generate redundant intermediate steps motivates compression methods.
  - Quick check question: Can you explain why longer CoT traces increase inference cost beyond just token count? (Hint: KV cache and attention complexity.)

- **Concept: Rejection sampling for self-supervised data curation**
  - Why needed here: ConPress filters generated traces by correctness. Understanding this quality-control step is essential for the extraction pipeline.
  - Quick check question: Given a multi-question prompt with N=3, if 60% of generations contain at least one incorrect answer, what fraction survives filtering if you require all N answers correct?

- **Concept: Accuracy-efficiency trade-offs in reasoning models**
  - Why needed here: The paper explicitly positions ConPress as trading tokens for accuracy. Understanding this trade-off helps interpret results and set expectations.
  - Quick check question: If a model achieves 50% token reduction but loses 5 accuracy points on AIME25, is this a favorable trade-off? What factors would inform your answer?

## Architecture Onboarding

- **Component map**: Multi-question sampler -> Trace parser -> Correctness filter -> SFT trainer
- **Critical path**: Sampler → Parser → Filter → SFT Trainer. If any stage fails (low sample quality, parsing errors, high rejection rate), downstream stages degrade.
- **Design tradeoffs**:
  - N selection: Higher N yields stronger compression but diminishing returns (N=2→3 gives ~10% gain; N=6→8 gives ~4%). N=3–4 balances compression and stability.
  - Sampling position: Earlier positions show stronger compression (Position 1: −49.3% vs. Position 3: −37.8%), but paper uses all positions for data efficiency.
  - Training data size: Only 8k examples needed; larger datasets may not improve results proportionally.
- **Failure signatures**:
  - Accuracy drops >2 points suggest over-aggressive compression or insufficient filtering
  - Token reduction <20% suggests N too low or model resistant to contextual pressure
  - High rejection rate (>80%) suggests sampling parameters or question pairing issues
- **First 3 experiments**:
  1. Reproduce self-compression phenomenon: Compare N=1 vs. N=2 reasoning length on MATH500 subset with your target model; verify leftward distribution shift.
  2. End-to-end ConPress run: Build full pipeline with N=3, 1k samples; measure token reduction and accuracy on held-out set.
  3. Ablate N: Train separate models with N=2, 3, 4; plot compression vs. accuracy to find optimal setting for your model and domain.

## Open Questions the Paper Calls Out

### Open Question 1
What is the mechanistic explanation for why multi-question contexts induce self-compression in reasoning models? The paper identifies and empirically characterizes self-compression but does not investigate the underlying model mechanisms (e.g., attention patterns, learned heuristics). Evidence would include attention head analysis, probing classifiers for token-budget states, or ablation studies on model components during multi-question generation.

### Open Question 2
How does ConPress perform on non-mathematical reasoning domains (e.g., code generation, logical deduction, natural language inference)? Experiments focus primarily on mathematical benchmarks, with only brief out-of-distribution evaluation on MMLU-STEM. Evidence would include evaluation on code benchmarks (HumanEval, MBPP), logical reasoning datasets (LogiQA), and open-domain QA.

### Open Question 3
Can ConPress be effectively combined with reinforcement learning approaches for further efficiency gains? The paper positions ConPress as an alternative to RL-based methods but does not explore whether the approaches are complementary. Evidence would include experiments applying ConPress as pre-training before RL fine-tuning, or using RL on top of ConPress-trained models with length-aware rewards.

## Limitations
- Limited interpretability of the compression mechanism—does not explain why multi-question contexts induce self-compression
- Dataset and model specificity—validated primarily on mathematical reasoning benchmarks and specific model families
- Quality of self-supervised supervision—reliance on correctness-filtered traces without detailed yield rate analysis
- Trade-off calibration—favors token reduction over accuracy in some cases

## Confidence
- Multi-question contextual pressure induces self-compression: High
- Correctness-filtered traces serve as effective SFT supervision: Medium
- SFT on compressed traces transfers to single-question efficiency: High
- No external teachers or RL needed: High

## Next Checks
1. Test whether compression persists under domain heterogeneity (mix math, code, and commonsense questions in one prompt) and whether it depends on question difficulty or similarity.
2. Inspect a sample of filtered compressed traces to assess whether essential reasoning steps are preserved; compare against manually pruned traces or oracle summaries.
3. Apply ConPress to models outside the paper's scope (smaller open models, non-math domains) and measure whether token reduction and accuracy preservation hold.