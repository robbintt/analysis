---
ver: rpa2
title: 'BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment
  of Alcohol and Substance Use Disorder with Electronic Health Records'
arxiv_id: '2511.04998'
source_url: https://arxiv.org/abs/2511.04998
tags:
- icd10
- bipete
- risk
- visit
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BiPETE introduces a Bi-Positional Embedding Transformer Encoder
  for predicting alcohol and substance use disorder (ASUD) risk using electronic health
  records (EHRs) from depression and PTSD cohorts. The model combines rotary positional
  embeddings (RoPE) to encode relative visit timing and sinusoidal positional embeddings
  (SPE) to preserve visit order, addressing the challenge of irregular visit intervals
  in EHR data.
---

# BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records

## Quick Facts
- arXiv ID: 2511.04998
- Source URL: https://arxiv.org/abs/2511.04998
- Reference count: 40
- Primary result: AUROC of 96.46% and AUPRC of 93.18% for ASUD risk prediction in depression cohort without pretraining

## Executive Summary
BiPETE introduces a Bi-Positional Embedding Transformer Encoder for predicting alcohol and substance use disorder (ASUD) risk using electronic health records (EHRs) from depression and PTSD cohorts. The model combines rotary positional embeddings (RoPE) to encode relative visit timing and sinusoidal positional embeddings (SPE) to preserve visit order, addressing the challenge of irregular visit intervals in EHR data. Trained without large-scale pretraining, BiPETE achieves strong performance, with AUROC of 96.46% and AUPRC of 93.18% in the depression cohort, and AUROC of 96.50% and AUPRC of 94.04% in the PTSD cohort. These results represent improvements of 34% and 50% in AUPRC over baseline models, respectively. Ablation studies confirm the effectiveness of the dual positional encoding strategy. Integrated gradients analysis identifies clinical features associated with ASUD risk, such as abnormal inflammatory, hematologic, and metabolic markers, as well as specific medications and comorbidities. BiPETE offers a practical and interpretable framework for disease risk prediction using EHR data.

## Method Summary
BiPETE is a transformer-based encoder trained specifically for ASUD risk prediction in depression and PTSD cohorts. The model processes EHR sequences containing ICD10 diagnosis codes (truncated to 3-character roots), DrugBank IDs, LOINC codes for abnormal lab tests, and emergency room visit counts. It uses a 15-month observation window with visits merged within 7-day windows and requires a minimum of 3 visits per patient. The architecture combines sinusoidal positional embeddings (SPE) added to token embeddings with rotary positional embeddings (RoPE) applied to query/key attention vectors. The model is trained from scratch without pretraining, using 6 encoder layers for the depression cohort (65K patients) and 3 layers for the PTSD cohort (9K patients). A BiGRU classifier head produces binary ASUD risk predictions. The model is evaluated using 5-fold cross-validation with AUROC and AUPRC as primary metrics.

## Key Results
- BiPETE achieves AUROC of 96.46% and AUPRC of 93.18% in the depression cohort, and AUROC of 96.50% and AUPRC of 94.04% in the PTSD cohort
- Ablation studies show RoPE+SPE combination outperforms SPE-only (AUROC 88.33%) and RoPE-only (AUROC 94.46%) with improvements of +9.2% AUROC and +20.4% AUPRC
- ICD10 vocabulary compression reduces codes from 32,662 to 1,625 (95% reduction) while maintaining predictive performance
- Integrated gradients analysis identifies abnormal inflammatory, hematologic, and metabolic markers as key risk factors for ASUD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining rotary (RoPE) and sinusoidal (SPE) positional encodings captures both relative timing and absolute sequence order in irregular EHR visit patterns, improving disease prediction over single-encoding approaches.
- Mechanism: RoPE applies position-dependent rotations to query/key embeddings, making attention scores decay with relative temporal distance—capturing clinical relevance of recency. SPE adds fixed sinusoidal vectors to token embeddings, encoding absolute visit order. Together, RoPE handles variable inter-visit gaps while SPE stabilizes token grouping within visits.
- Core assumption: EHR sequences contain predictive signal in both relative time gaps (clinical urgency patterns) AND sequential ordering (disease progression patterns).
- Evidence anchors:
  - [abstract]: "combines rotary positional embeddings (RoPE) to encode relative visit timing and sinusoidal positional embeddings (SPE) to preserve visit order... +9.2% AUROC, +20.4% AUPRC compared to SPE Only"
  - [section]: Table 2 shows BiPETE (96.46% AUROC) vs RoPE-only (94.46%) vs SPE-only (88.33%); Figure 2 shows RoPE-only has unstable validation loss while SPE-only underperforms
  - [corpus]: Related transformer work on Alzheimer's progression prediction validates transformer efficacy for longitudinal visits but does not compare positional encoding strategies
- Break condition: If EHR data has uniformly spaced visits or if target disease depends primarily on sequence rather than timing, RoPE component provides diminishing returns.

### Mechanism 2
- Claim: Truncating ICD10 codes to 3-character roots reduces vocabulary by ~95% while preserving predictive signal, preventing diagnosis dominance over other clinical vocabularies.
- Mechanism: Aggregating related diagnoses into broader categories increases token frequency, mitigates redundancy, and balances attention across multi-vocabulary inputs (diagnoses, medications, lab tests). This reduces attention skew and overfitting to rare specific codes.
- Core assumption: Category-level diagnostic information captures most predictive value for ASUD risk; fine-grained distinctions are noise rather than signal for this task.
- Evidence anchors:
  - [section]: "reduced the ICD10 vocabulary size by 95%, from 32,662 unique codes to 1,625 in the depression cohort... Without this step, ICD10 codes would dominate the vocabulary, leading to attention skew toward diagnosis"
  - [corpus]: No direct corpus evidence on vocabulary compression strategies in EHR transformers
- Break condition: If target disease requires fine-grained diagnostic specificity (e.g., distinguishing cancer stages), truncation may lose essential discriminative information.

### Mechanism 3
- Claim: Task-specific training from random initialization achieves strong performance without large-scale pretraining when vocabulary is compressed and observation windows are focused.
- Mechanism: With reduced vocabulary (4,904 tokens) and constrained 15-month observation windows, the model learns discriminative token patterns directly from the binary classification task. Model scales are matched to dataset sizes (6 layers for depression, 3 for PTSD) per compute-optimal principles.
- Core assumption: Single-disease prediction tasks contain sufficient signal in token co-occurrence patterns; semantic understanding of clinical codes is not required.
- Evidence anchors:
  - [abstract]: "Trained without large-scale pretraining, BiPETE achieves strong performance, with AUROC of 96.46% and AUPRC of 93.18%"
  - [section]: "This suggests that the model identified distinct token patterns for the classes without learning the semantic meaning of the tokens... BiPETE is currently limited to single-disease prediction and requires retraining for each new disease prediction task"
  - [corpus]: Related work (BEHRT, Med-BERT in citations) uses large-scale pretraining; no corpus examples of comparable no-pretraining performance for direct comparison
- Break condition: If target task requires transfer learning across diseases or understanding of clinical semantics, pretraining becomes necessary.

## Foundational Learning

- Concept: Rotary Positional Embeddings (RoPE)
  - Why needed here: Core innovation for encoding relative temporal distances; standard positional encodings cannot represent that a 30-day gap has different meaning at different sequence positions.
  - Quick check question: Why does RoPE apply rotations to Q and K rather than adding vectors to token embeddings?

- Concept: Integrated Gradients (IG) attribution
  - Why needed here: Interpretability method identifying which EHR tokens drive predictions; paper uses IG to identify clinical risk/protective factors.
  - Quick check question: Why does IG require defining a baseline input, and what baseline was used for EHR tokens in this paper?

- Concept: Compute-optimal model scaling
  - Why needed here: Paper explicitly reduces model depth (6→3 layers) for smaller PTSD cohort; understanding this tradeoff is essential for deployment.
  - Quick check question: What is the approximate tokens-per-parameter ratio suggested by compute-optimal training literature, and how does BiPETE's configuration compare?

## Architecture Onboarding

- Component map:
Input Processing -> Standardized vocabularies (ICD10 truncated, DBID, LOINC, ER count) -> Visit indices (0, 1, 2...) + days-ago indices (0, 5, 20...)
Token embeddings + SPE (sinusoidal, added to tokens)
Encoder (×3-6 blocks): Pre-LayerNorm -> Multi-Head Attention (RoPE on Q,K) -> Skip-Connection -> LayerNorm -> FC Layer -> Skip-Connection
BiGRU -> Linear -> Sigmoid -> Binary output

- Critical path:
  1. Data: Extract codes within 15-month window, merge visits within 7 days, exclude <3 visits
  2. Encoding: SPE provides absolute position; RoPE enables attention to decay with relative distance
  3. Training: 5-fold CV, early stopping on validation loss (~epoch 15), BCE loss
  4. Interpretation: IG attribution -> Relative Contribution (RC) ratio: TP attribution / TN attribution

- Design tradeoffs:
  - Model depth vs dataset size: 6 layers for 65K depression cohort; 3 layers for 9K PTSD cohort
  - Vocabulary detail vs efficiency: 95% ICD10 reduction trades specificity for trainability
  - RoPE-only vs combined: Combined adds complexity but critical for stability; RoPE alone shows high variance

- Failure signatures:
  - Validation loss spikes during training: Indicates RoPE-only instability from variable days-ago indices
  - Near-perfect train accuracy with diverging validation: Overfitting signal—reduce depth or add regularization
  - Inconsistent token attributions: Same code with different IG signs across patients indicates context-dependency (not a bug, but limits interpretability)

- First 3 experiments:
  1. Ablation replication: Train SPE-only, RoPE-only, and combined on same fold; plot training curves to verify stability differences match Figure 2
  2. Vocabulary threshold: Test 2-char vs 3-char ICD10 truncation; measure AUROC drop vs vocabulary size reduction
  3. Observation window sensitivity: Compare 6-month, 15-month, and 24-month windows to validate the 15-month design choice for ASUD prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does BiPETE generalize effectively to disease prediction tasks other than Alcohol and Substance Use Disorder (ASUD)?
- Basis in paper: [explicit] The authors state in the Discussion that "we tested BiPETE only on the ASUD risk prediction task, so its performance on other disease prediction tasks remains uncertain."
- Why unresolved: The model's architecture, specifically the dual positional encoding strategy, has only been validated on ASUD risk within Depression and PTSD cohorts.
- What evidence would resolve it: Successful application and evaluation of BiPETE on distinct clinical outcomes (e.g., cardiovascular disease or diabetes) using similar EHR structures.

### Open Question 2
- Question: Can BiPETE maintain high performance when applied to external Electronic Health Record (EHR) datasets outside the All of Us (AoU) program?
- Basis in paper: [explicit] The authors list as a limitation that "BiPETE has not yet been evaluated on datasets beyond the AoU data, and its generalizability remains to be fully validated."
- Why unresolved: Model performance may be specific to the data quality, coding practices, or population demographics of the AoU cohort.
- What evidence would resolve it: External validation studies testing the trained model on independent datasets such as MIMIC-IV or institutional partners' EHR data.

### Open Question 3
- Question: Can pretraining BiPETE on large-scale EHR corpora improve its ability to learn token semantics compared to training from scratch?
- Basis in paper: [explicit] The authors note that "BiPETE's disease prediction performance could be further improved through pretraining on large-scale EHR corpora... we leave this for the future work."
- Why unresolved: The current model learns token relationships solely from the single-disease classification task without prior contextual knowledge, potentially limiting semantic understanding.
- What evidence would resolve it: A comparative study measuring performance gaps between the current BiPETE and a version pretrained using self-supervised objectives like masked language modeling.

### Open Question 4
- Question: Does the exclusion of demographic information limit the predictive capability or fairness of the model?
- Basis in paper: [explicit] The authors acknowledge they "did not include any demographic information in our input" but suggest "incorporating such information could improve performance."
- Why unresolved: Social determinants of health are often embedded in demographic data; excluding them may ignore critical risk factors, especially given the diverse racial composition of the cohorts.
- What evidence would resolve it: Ablation experiments that include demographic tokens (e.g., age, race, sex) to quantify their impact on AUROC, AUPRC, and performance parity across subgroups.

## Limitations

- BiPETE has only been validated on ASUD risk prediction in depression and PTSD cohorts, with uncertain generalization to other disease prediction tasks
- The model has not been evaluated on external EHR datasets beyond the All of Us program, limiting generalizability claims
- Task-specific training without pretraining may limit semantic understanding of clinical tokens, though the authors suggest pretraining could improve performance

## Confidence

- **High Confidence**: The ablation study demonstrating combined RoPE+SPE superiority over either alone (AUROC +9.2%, AUPRC +20.4%); the vocabulary compression strategy reducing ICD10 codes by ~95% while maintaining predictive performance; the observation that task-specific training without pretraining achieved strong results
- **Medium Confidence**: The claim that BiPETE is "practical and interpretable" given the limited description of the integrated gradients analysis methodology and the dependence on specific hyperparameter configurations
- **Low Confidence**: The assertion of "34% and 50% improvements in AUPRC" without pretraining baselines on identical datasets, as comparisons are made to unspecified baseline models

## Next Checks

1. Replicate the ablation study (SPE-only, RoPE-only, combined) on the same dataset folds and verify the reported stability differences and performance gains
2. Test the sensitivity of results to vocabulary truncation depth (2-character vs 3-character ICD10 codes) to quantify the precision-efficiency tradeoff
3. Compare BiPETE performance against a BERT-style model with large-scale pretraining on the same depression/PTSD datasets to establish the true value proposition of the no-pretraining approach