---
ver: rpa2
title: Enhancing Breast Cancer Prediction with LLM-Inferred Confounders
arxiv_id: '2511.17662'
source_url: https://arxiv.org/abs/2511.17662
tags:
- cancer
- features
- breast
- clinical
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study enhances breast cancer prediction by inferring confounder
  diseases using large language models. By augmenting routine clinical features with
  LLM-generated likelihoods of diabetes, obesity, and cardiovascular disease, the
  model performance improves significantly.
---

# Enhancing Breast Cancer Prediction with LLM-Inferred Confounders

## Quick Facts
- arXiv ID: 2511.17662
- Source URL: https://arxiv.org/abs/2511.17662
- Reference count: 0
- Primary result: LLM-inferred confounder probabilities improve breast cancer prediction accuracy by 3.9-6.4% over baseline

## Executive Summary
This study demonstrates that augmenting clinical biomarker data with LLM-inferred probabilities of confounding diseases significantly improves breast cancer prediction accuracy. Using the Breast Cancer Coimbra dataset (116 patients), the approach employs Llama-3.3-70B and Gemma-2-27B models to infer likelihoods of diabetes, obesity, and cardiovascular disease from routine clinical features. The Random Forest classifier trained on these augmented features achieved performance improvements of 3.9% (Gemma) and 6.4% (Llama) over the baseline model, with the "All-confounders" configuration showing the strongest gains.

## Method Summary
The method involves using pre-trained LLMs (Llama-3.3-70B and Gemma-2-27B) via the Together AI API to infer disease probabilities from nine clinical features (Age, BMI, Glucose, Insulin, HOMA, Leptin, Adiponectin, Resistin, MCP-1). The LLMs generate probability scores (0-1) for diabetes, obesity, cardiovascular disease, and breast cancer itself. These synthetic features are then concatenated with the original clinical features and used to train Random Forest classifiers across 20 random 80/20 train-test splits. Four feature configurations are evaluated: baseline (9 original features), LLM-only (BC likelihood), single-confounder (9 features + one disease), and all-confounders (9 features + all three disease likelihoods).

## Key Results
- All-confounders configuration improved baseline accuracy by 3.9% (Gemma) and 6.4% (Llama)
- Llama-3.3-70B generally outperformed Gemma-2-27B, likely due to larger model size and stronger meta-learning
- The approach shows promise for noninvasive prescreening and clinical integration in breast cancer diagnosis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Augmenting biomarker data with LLM-inferred confounder probabilities improves classification accuracy by explicitly modeling latent health states.
- **Core assumption:** The LLM accurately infers disease likelihood from limited biomarker data better than the RF classifier could implicitly learn from the raw data alone.
- **Evidence anchors:** [abstract]: "...using large language models to infer the likelihood of confounding diseases... from routine clinical data." [methods]: "...many other diseases can alter these markers, reducing specificity... LLMs [generate] proxy measures for confounders not included in the original data."
- **Break condition:** If the LLM's pre-training data does not reflect the specific patient demographic or biomarker ranges in the target dataset, the inferred probabilities will be miscalibrated, adding noise rather than signal.

### Mechanism 2
- **Claim:** Larger LLMs provide superior synthetic features due to stronger meta-learning capabilities and medical knowledge encoding.
- **Core assumption:** Model parameter count correlates positively with the accuracy of zero-shot medical inference on this specific task.
- **Evidence anchors:** [results]: "Llama generally outperformed Gemma... likely due to its larger model size and stronger ability for meta-learning." [results]: Table 1 shows the Llama-augmented model achieving higher AUC (0.753) compared to Gemma (0.740) in the All-confounders configuration.
- **Break condition:** If the larger model suffers from over-confidence or hallucination on out-of-distribution biomarker values, it may generate falsely high confounder probabilities, degrading classifier specificity.

### Mechanism 3
- **Claim:** The "All-Confounders" feature configuration captures synergistic risk profiles that single-feature additions miss.
- **Core assumption:** The confounders interact in ways that are distinct from the sum of their parts, and the LLM captures these distinct probabilities independently.
- **Evidence anchors:** [results]: "The wAllConfounders configuration produced the highest gains, improving over the baseline by an average of 3.9% (Gemma) and 6.4% (Llama)." [results]: Table 1 shows a consistent trend where adding more confounder features (Single -> All) generally increases AUC and Accuracy.
- **Break condition:** If the confounders are perfectly collinear in the LLM's inference logic, the additional features provide redundant information that could lead to overfitting in small datasets.

## Foundational Learning

- **Concept:** Confounders in Medical AI
  - **Why needed here:** The core premise of the paper is that blood biomarkers are not exclusive to breast cancer; they are influenced by other diseases. If you don't account for diabetes, a spike in glucose might be misinterpreted by the model as a cancer signal.
  - **Quick check question:** If a patient has high glucose, how does the model distinguish if it's caused by diabetes or breast cancer risk?

- **Concept:** Zero-Shot Inference
  - **Why needed here:** The LLMs used were not retrained on the patient dataset. They used their pre-existing knowledge to "guess" the confounder likelihoods directly from the prompt.
  - **Quick check question:** How does the system generate a "Diabetes Probability" if the training data labels only say "Cancer" or "Healthy"?

- **Concept:** Synthetic Feature Augmentation
  - **Why needed here:** The original dataset had only 9 features. The study adds 3-4 "synthetic" features (generated by the LLM).
  - **Quick check question:** What is the difference between the 9 "Original" features and the 3 "Synthetic" features in the final dataset?

## Architecture Onboarding

- **Component map:** Breast Cancer Coimbra Dataset (9 features) -> LLM Inference Engine (Together AI API) -> Feature Generator (parses probability scores) -> Augmented Vector (original + synthetic) -> Random Forest Classifier -> Evaluation (20-fold split)

- **Critical path:** The Prompt Engineering and Response Parsing. The system relies entirely on the LLM accepting the biomarker string and returning a clean, parsable probability float.

- **Design tradeoffs:**
  - Accuracy vs. Latency/Cost: Using a 70B parameter model (Llama) via API yields better results (+6.4%) but is slower and more expensive than the 27B model (Gemma, +3.9%)
  - Stability vs. Complexity: Averaging over 20 random splits stabilizes results on a tiny dataset (116 rows) but increases total inference/computation time by 20x during the evaluation phase

- **Failure signatures:**
  - API Failure: "Refusal to answer medical questions" based on LLM safety guardrails
  - Format Error: LLM returning "High likelihood (80%)" instead of "0.8", breaking the float parser
  - Data Leakage: If the LLM prompt includes the target label (Cancer/Healthy) when asking for confounders, the resulting synthetic features would be contaminated

- **First 3 experiments:**
  1. Baseline Reproduction: Train the Random Forest on the raw 9 features using an 80/20 split to establish the benchmark accuracy (approx. 71.9%)
  2. Single-Pass LLM Inference: Create a prompt template: "Given a patient with [BMI], [Glucose], [Insulin]... what is the likelihood of Diabetes? Return a float between 0 and 1." Run this on the training set and check output format validity
  3. Ablation on Confounders: Train 3 separate RF models. Model A: Original + Diabetes prob. Model B: Original + Obesity prob. Model C: Original + All probs. Compare AUC to verify the paper's claim that "All" performs best

## Open Questions the Paper Calls Out

- Does the improvement in prediction accuracy persist when applied to larger, more diverse patient cohorts?
- Can model performance be further improved by inferring additional confounders such as Polycystic Ovary Syndrome (PCOS) or autoimmune diseases?
- Does generating natural language explanations for confounder likelihoods enhance clinical trust and utility?

## Limitations
- Small sample size (116 patients) with only ~92 training samples per fold, vulnerable to overfitting
- LLM inference quality not validated against ground truth (actual patient diagnoses for diabetes/obesity/CVD)
- Data leakage risk if target labels were included in LLM prompts for confounder inference

## Confidence
- **High Confidence**: The methodology of augmenting clinical features with LLM-inferred probabilities is sound and reproducible
- **Medium Confidence**: The attribution of performance gains specifically to confounder modeling versus general LLM knowledge infusion remains uncertain without ground truth validation
- **Low Confidence**: The claim that larger models provide superior medical inference on this task lacks direct evidence from the paper

## Next Checks
1. Obtain or simulate ground truth labels for diabetes, obesity, and CVD in a subset of the Coimbra dataset to validate LLM inference accuracy
2. Test the full pipeline on a different breast cancer dataset with similar clinical features to assess generalization
3. Systematically remove each synthetic confounder feature from the All-confounders configuration to determine which specific disease probabilities drive the performance improvement