---
ver: rpa2
title: 'MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning'
arxiv_id: '2505.16964'
source_url: https://arxiv.org/abs/2505.16964
tags:
- medical
- reasoning
- image
- images
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MedFrameQA, the first benchmark for multi-image
  medical visual question answering (VQA), addressing the gap between single-image
  VQA datasets and the real-world clinical need for cross-image reasoning. A scalable
  pipeline extracts temporally coherent frames from medical education videos, aligns
  them with transcribed captions, merges related clips, and generates multi-image
  VQA pairs with explicit reasoning chains.
---

# MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning

## Quick Facts
- **arXiv ID:** 2505.16964
- **Source URL:** https://arxiv.org/abs/2505.16964
- **Reference count:** 40
- **Primary result:** Introduces first multi-image medical VQA benchmark for clinical reasoning

## Executive Summary
MedFrameQA addresses the critical gap between single-image medical VQA datasets and real-world clinical scenarios that require reasoning across multiple images. The benchmark introduces a scalable pipeline that extracts temporally coherent frames from medical education videos, aligns them with transcribed captions, merges related clips, and generates multi-image VQA pairs with explicit reasoning chains. The resulting dataset contains 2,851 high-quality questions spanning nine body systems and 43 organs, each requiring synthesis across two to five images. Evaluation of 11 state-of-the-art MLLMs reveals poor performance, with most models achieving under 50% accuracy, demonstrating the significant challenge of multi-image clinical reasoning tasks.

## Method Summary
The dataset construction pipeline operates through several key stages: frame extraction from medical education videos using temporal coherence analysis, caption alignment through optical character recognition and natural language processing, clip merging to identify related image sequences, and VQA pair generation with explicit reasoning chains. The process leverages medical education content to ensure clinical relevance while maintaining diversity across body systems. Quality control measures include multi-stage validation to ensure question-answer pairs meet clinical reasoning requirements and image relevance criteria. The final dataset encompasses nine major body systems and 43 organs, with questions designed to require synthesis across multiple images for accurate diagnosis or assessment.

## Key Results
- MedFrameQA contains 2,851 multi-image VQA pairs requiring reasoning across 2-5 images
- State-of-the-art MLLMs achieve less than 50% accuracy on the benchmark
- Model performance decreases as the number of images in reasoning chains increases
- Error analysis reveals models frequently ignore salient findings and mis-aggregate evidence

## Why This Works (Mechanism)
Multi-image VQA requires models to perform temporal reasoning, cross-reference findings across different views or time points, and synthesize complex clinical narratives from disparate visual evidence. The explicit reasoning chains in MedFrameQA force models to demonstrate their understanding of how individual findings contribute to overall diagnosis or assessment. The temporal coherence extraction ensures that related findings are properly sequenced, mimicking the clinical workflow of tracking disease progression or treatment response across multiple imaging studies.

## Foundational Learning
**Temporal coherence analysis**: Required to identify related frames across video sequences; quick check involves verifying frame similarity metrics and temporal spacing between extracted frames.

**Medical image understanding**: Essential for recognizing anatomical structures and pathological findings; quick check includes testing object detection and segmentation accuracy on medical images.

**Multi-modal reasoning**: Needed to integrate visual and textual information effectively; quick check involves evaluating model performance on single-image VQA before extending to multi-image scenarios.

**Clinical reasoning patterns**: Understanding how clinicians synthesize information across multiple studies; quick check includes expert review of generated questions for clinical validity.

## Architecture Onboarding

**Component Map:**
Frame extraction -> Caption alignment -> Clip merging -> VQA generation -> Quality validation

**Critical Path:**
The pipeline follows a sequential flow where each stage depends on the successful completion of the previous step. Frame extraction quality directly impacts caption alignment accuracy, which in turn affects clip merging reliability and ultimately the quality of generated VQA pairs.

**Design Tradeoffs:**
The use of medical education videos provides controlled, high-quality content but may limit real-world clinical complexity. The explicit reasoning chain requirement ensures clinical validity but increases annotation burden. The multi-stage quality validation ensures dataset reliability but reduces overall throughput.

**Failure Signatures:**
Common failure modes include poor temporal coherence leading to unrelated frames being merged, caption misalignment causing context loss, and reasoning chain generation that doesn't accurately reflect clinical diagnostic pathways. Models typically fail by ignoring key findings, making incorrect inferences from limited evidence, or propagating errors across multiple images.

**First 3 Experiments:**
1. Evaluate single-image VQA performance on subset of MedFrameQA to establish baseline capability
2. Test model performance with varying numbers of images (2, 3, 4, 5) to assess scaling effects
3. Conduct ablation study removing reasoning chains to measure their impact on model accuracy

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Dataset construction relies on medical education videos, potentially limiting real-world clinical complexity
- Sample size constraints for some body systems and organs may affect generalizability
- Focus on GPT-4V evaluation may not fully represent broader MLLM capabilities
- Potential bias toward commonly taught medical conditions and presentations

## Confidence

**Confidence Labels for Major Claims:**
- Dataset quality and diversity: **High**
- MLLM performance assessment: **Medium**
- Clinical reasoning difficulty: **High**

## Next Checks
1. Validate dataset generalization by testing on clinical cases from actual patient records and imaging studies
2. Expand evaluation to include a broader range of MLLMs and different prompt engineering approaches
3. Conduct expert clinician review of question-answer pairs to assess clinical relevance and difficulty calibration across different medical specialties