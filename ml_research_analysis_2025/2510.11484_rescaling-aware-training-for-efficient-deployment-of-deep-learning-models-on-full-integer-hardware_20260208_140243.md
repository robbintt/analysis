---
ver: rpa2
title: Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on
  Full-Integer Hardware
arxiv_id: '2510.11484'
source_url: https://arxiv.org/abs/2510.11484
tags:
- quantization
- rescaling
- training
- accuracy
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the computational and hardware cost of integer
  rescaling in full-integer AI inference, a critical step in edge AI deployment. Current
  methods like quantization-aware training (QAT) neglect the impact of rescaling on
  model accuracy and hardware efficiency.
---

# Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware

## Quick Facts
- **arXiv ID:** 2510.11484
- **Source URL:** https://arxiv.org/abs/2510.11484
- **Reference count:** 13
- **Primary result:** Achieved up to 8x reduction in rescaler bit-width (32→8-bit) with zero accuracy loss on ImageNet models

## Executive Summary
This work addresses the computational and hardware cost of integer rescaling in full-integer AI inference, a critical step in edge AI deployment. Current methods like quantization-aware training (QAT) neglect the impact of rescaling on model accuracy and hardware efficiency. The authors propose rescaling-aware training, a fine-tuning technique that reduces the bit-width of rescaling multiplicands without degrading accuracy. By applying quantization constraints during training, they achieve up to 8x reduction in rescaler width (from 32-bit to 8-bit) with zero accuracy loss, and 4-bit rescaling with full recovery via minimal retraining. Experimental results on ImageNet models show significant hardware area-delay savings (up to 4x) and preserved accuracy. This enables more energy- and cost-efficient AI inference on resource-constrained embedded systems.

## Method Summary
The authors developed rescaling-aware training by modifying TensorFlow Lite's integer-only inference pipeline to support reduced-bit-width rescaling multiplicands. They first demonstrated post-training reduction of rescaler width from 32-bit to 8-bit with zero accuracy loss by applying stronger quantization to rescale factors. For aggressive 4-bit rescaling, they implemented a fine-tuning approach that emulates integer quantization errors during training using float64 precision and Straight-Through Estimation for gradient flow. The method extracts pre-trained model rescale factors, applies dyadic quantization with reduced bit-width, and fine-tunes for 2 epochs with SGD to recover accuracy.

## Key Results
- 8-bit rescaling achieved 0% accuracy loss on EfficientNet-Lite0 and MobileNet-V1 post-training
- 4-bit rescaling recovered 71.62% accuracy (from 65.39%) after 2 epochs of fine-tuning
- Hardware area-delay product improved by >2× (8-bit) to >4× (4-bit) due to narrower rescaler multipliers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rescaler multiplicand bit-width can be reduced from 32-bit to 8-bit post-training with zero accuracy loss on standard ImageNet models.
- **Mechanism:** The dyadic quantization of rescale factors (M = m·2^(-s)) retains sufficient precision at 8-bit because the original 32-bit allocation substantially exceeds the information content required for rescaling. Quantization error from reduced multiplicand width remains dominated by the inherent 8-bit output quantization error rather than rescaler precision loss.
- **Core assumption:** Pre-trained quantized models have rescale factor distributions that are not information-theoretically dense at 32-bit precision.
- **Evidence anchors:** [abstract] "This work shows that rescaling cost can be dramatically reduced post-training, by applying a stronger quantization to the rescale multiplicands at no model-quality loss." [Table I] EfficientNet-Lite0: 71.28% → 70.93% (8-bit); MobileNet-V1: 70.51% → 70.51% (8-bit)
- **Break condition:** Aggressive reduction below 5-bit causes exponential error growth in rescale quantization component, exceeding rounding error dominance threshold.

### Mechanism 2
- **Claim:** Fine-tuning with emulated low-bit-width rescaling during training recovers accuracy for aggressive (4-bit) rescaler quantization.
- **Mechanism:** Forward-pass emulation injects the exact quantization error that will occur during deployment. Neural network noise resilience allows weight redistribution to compensate for systematic rescaling bias. Updates concentrate in later layers and depth-wise convolutions, suggesting compensatory adjustments for mean activation shifts.
- **Core assumption:** DNNs possess sufficient redundancy and gradient flow to redistribute quantization error through minor weight adjustments (±1–2 LSB changes affecting <1% of parameters).
- **Evidence anchors:** [Section II.C] "This method emulates the quantization errors of the rescale hardware during the forward pass. Proper gradient propagation during the backward pass is ensured by Straight-Through Estimation." [Table II] 4-bit rescaler: 65.39% → 71.62% after 2 epochs; only 0.66% of weights changed with mean absolute deviation <0.71%
- **Break condition:** At 3-bit rescaler width, quantization error magnitude exceeds network's compensatory capacity even with retraining (Figure 3 shows degraded ceiling).

### Mechanism 3
- **Claim:** Hardware area-delay product improves by >2× (8-bit) to >4× (4-bit) due to rescaler multiplier width reduction.
- **Mechanism:** Rescaler multiplication sits on the critical path in systolic dot-product arrays. Reducing multiplicand width from 32-bit narrows both the multiplier hardware and subsequent shift-and-cast operations. Savings compound across parallel MAC units.
- **Core assumption:** Commercial NPU architectures implement per-MAC rescalers as shown in Figure 1; rescaler multiplication determines critical path timing.
- **Evidence anchors:** [Section III.D] "Scaling down from 32 to 4 bits yields area reductions of 58.5%, 47.5%, and 34.8% for dot-product-engines deploying 4, 8, and 16 parallel MACs." [Section III.D] "Timing analysis shows nearly 50% improvement in the critical path across all MAC trees... since the rescaler multiplication consistently determines the critical path."
- **Break condition:** Assumption: Benefits diminish if target architecture shares rescalers across MACs or uses different accumulator-to-output casting strategies.

## Foundational Learning

- **Dyadic Quantization:**
  - Why needed here: Rescale factors M ∈ (0,1] are represented as integer multiplier m × power-of-two shift 2^(-s). Understanding this decomposition is prerequisite to manipulating bit-widths.
  - Quick check question: Given M = 0.1875, can you express it in dyadic form m·2^(-s) with k-bit m?

- **Straight-Through Estimator (STE):**
  - Why needed here: Rounding operations in quantized rescaling are non-differentiable. STE approximates gradients by passing them unchanged through discretization, enabling backpropagation.
  - Quick check question: In Algorithm 1, why does `Floor(mul × 2^(-shift))` require STE for gradient flow?

- **Quantization Error Decomposition:**
  - Why needed here: Section II.A.4 decomposes total error into (a) rescale factor quantization error and (b) right-shift rounding error. Recognizing which component dominates at each bit-width is essential for knowing when retraining is necessary.
  - Quick check question: At what condition does |M_q - M| · max|S_y · a_q| exceed the rounding error bound S_y/2?

## Architecture Onboarding

- **Component map:**
  ```
  Input (int8) → MAC Accumulator (int32) → Rescaler (m × 2^(-s)) → Shift+Round → Saturate → Output (int8)
                                                ↑
                                         [TARGET: reduce m bit-width]
  ```

- **Critical path:**
  1. Modify `MultiplyByQuantizedMultiplier()` to accept k-bit multiplier
  2. Recompile LiteRT with modified quantization routine (Section II.B)
  3. Emulate int64 intermediate precision during training forward pass
  4. Apply STE for backward pass through rounding

- **Design tradeoffs:**
  - 8-bit rescaler: Zero retraining cost, ~2× area-delay improvement
  - 4-bit rescaler: 2 epochs retraining (~6% weights changed), ~4× area-delay improvement
  - 3-bit rescaler: Retraining fails to recover baseline (hard floor)

- **Failure signatures:**
  - Accuracy collapse at 5-bit without retraining (MobileNet-V2: 71.09% → 54.55% at 4-bit post-training)
  - Gradient instability if float32 used instead of float64 for accumulator emulation (24-bit mantissa insufficient)
  - Training/inference mismatch if rounding mode differs (Section II.C.1 specifies round-half-up)

- **First 3 experiments:**
  1. **Baseline characterization:** Take pre-trained W8A8 model, reduce rescaler to 8-bit, measure accuracy degradation on ImageNet validation split. Expect <0.5% drop.
  2. **Breakpoint detection:** Sweep rescaler bit-width (8→6→5→4→3) post-training to find degradation threshold (>0.5% accuracy loss) for your specific model.
  3. **Fine-tuning recovery:** For bit-width at breakpoint (e.g., 4-bit), implement Rescale-Aware Training with Algorithm 1 emulation, train 2 epochs with SGD, measure weight change percentage and accuracy recovery.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Rescale-Aware Training generalize to Generative AI and Agentic-AI models deployed on resource-constrained edge devices?
- Basis in paper: [explicit] The conclusion states, "In future work, we will extend this approach to emerging Gen-AI and Agentic-AI applications at the edge."
- Why unresolved: The current study exclusively validates the technique on image classification models (e.g., EfficientNet, MobileNet) using ImageNet.
- What evidence would resolve it: Empirical results demonstrating accuracy recovery and hardware efficiency gains when applying the method to large language models or diffusion models.

### Open Question 2
- Question: Can a single model be optimized to support variable rescaler bit-widths simultaneously, or must models remain fixed to a specific hardware configuration?
- Basis in paper: [inferred] Section III.C notes that the model "will only reach its maximum accuracy for the specific rescaler bit-width... employed during fine tuning."
- Why unresolved: The current method requires retraining specifically for the target bit-width, complicating deployment across heterogeneous hardware.
- What evidence would resolve it: A training technique that produces a model robust to multiple rescaler widths without requiring separate fine-tuning runs.

### Open Question 3
- Question: What modifications are required to recover model quality when using aggressive sub-4-bit rescaling multiplicands?
- Basis in paper: [inferred] Figure 3 and Section III.C show that the proposed retraining fails to recover accuracy at 3 bits, even though 4-bit recovery is successful.
- Why unresolved: The current Rescale-Aware Training approach hits a performance floor at 3 bits, suggesting the method or network capacity is insufficient for lower bit-widths.
- What evidence would resolve it: A modified training scheme or architectural adjustment that closes the accuracy gap for 2-bit or 3-bit rescaler configurations.

## Limitations

- Core claims rely on LiteRT-specific implementations not fully disclosed (rescale factor extraction, kernel modifications)
- Hardware savings are estimated from analytical models rather than fabricated silicon measurements
- The 0.5% accuracy degradation threshold is arbitrary without statistical validation across multiple runs

## Confidence

- Mechanism 1 (8-bit post-training rescaling): High confidence - empirical validation shows <0.5% accuracy loss on two models with clear error analysis
- Mechanism 2 (4-bit retraining recovery): Medium confidence - demonstrated on one model with limited epoch count, no ablation on learning rate/batch size
- Mechanism 3 (hardware area-delay savings): Low confidence - analytical estimates only, no measured silicon data or alternative architecture considerations

## Next Checks

1. Implement statistical significance testing across multiple training runs to validate the 0.5% degradation threshold is meaningful
2. Measure actual hardware area-delay product on FPGA/ASIC for 8-bit vs 32-bit rescaling implementations
3. Test retraining recovery across all three models (EfficientNet-Lite0, MobileNet-V1, MobileNet-V2) with 3-4 bit-widths and different learning rates