---
ver: rpa2
title: Actionable Interpretability Must Be Defined in Terms of Symmetries
arxiv_id: '2601.12913'
source_url: https://arxiv.org/abs/2601.12913
tags:
- interpretability
- interpretable
- inference
- symmetries
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper argues that existing interpretability definitions are
  insufficient because they lack formal frameworks for verification and design. The
  authors propose that actionable interpretability should be defined through four
  symmetries: inference equivariance, information invariance, concept-closure invariance,
  and structural invariance.'
---

# Actionable Interpretability Must Be Defined in Terms of Symmetries

## Quick Facts
- arXiv ID: 2601.12913
- Source URL: https://arxiv.org/abs/2601.12913
- Authors: Pietro Barbiero; Mateo Espinosa Zarlenga; Francesco Giannini; Alberto Termine; Filippo Bonchi; Mateja Jamnik; Giuseppe Marra
- Reference count: 21
- Key outcome: The paper argues that existing interpretability definitions are insufficient because they lack formal frameworks for verification and design. The authors propose that actionable interpretability should be defined through four symmetries: inference equivariance, information invariance, concept-closure invariance, and structural invariance.

## Executive Summary
The paper addresses the fundamental problem that interpretability research lacks formal frameworks for verification and design. The authors propose that actionable interpretability must be defined through four symmetries that enable both formal verification of interpretability claims and inform model design. These symmetries subsume existing interpretability properties and unify alignment, intervention, and counterfactual inference as forms of Bayesian inversion. The work provides a mathematical foundation that makes interpretability both testable and actionable, while accounting for user-centrism and task-dependence through structural invariance.

## Method Summary
The paper formalizes interpretable models as a subclass of probabilistic models using Markov categories. The core approach involves defining interpretable models through four symmetries: inference equivariance (translating between model and human representations preserves inference), information invariance (compressed representations preserve predictive information), concept-closure invariance (translations preserve semantic meaning), and structural invariance (models match user's hypothesis class). The authors demonstrate that concept-based models satisfying these symmetries can be verified through commutative diagrams, and that alignment, intervention, and counterfactual inference unify as Bayesian inversion operations within this framework.

## Key Results
- The four symmetries subsume existing interpretability properties and enable formal verification
- Information invariance makes interpretability verification tractable by compressing input space while preserving predictive information
- Concept-closure invariance guarantees that model-to-human translations preserve semantic meaning through fixed-point structure
- Alignment, intervention, and counterfactual inference unify as Bayesian inversion in the interpretable model category

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information invariance makes interpretability verification tractable by compressing input space while preserving predictive information.
- Mechanism: A surjective map P(Z|X) marginalizes the input such that H(Z) ≪ H(X) while maintaining I(Y;X) = I(Y;Z). This renders Y conditionally independent of X given Z (I(Y;X|Z) = 0), meaning verification only requires examining the compressed representation.
- Core assumption: The task-relevant information is compressible into a lower-dimensional sufficient statistic.
- Evidence anchors:
  - [abstract] "interpretable models as a subclass of probabilistic models"
  - [Section 2.2] "Verifying inference equivariance for P(Y|Z) is equivalent but more tractable than for P(Y|X)"
  - [corpus] No direct corpus support for this specific compression-tractability claim.
- Break condition: When input features are irreducibly entangled with the target (no low-dimensional sufficient statistic exists).

### Mechanism 2
- Claim: Concept-closure invariance guarantees that model-to-human translations preserve semantic meaning through fixed-point structure.
- Mechanism: Concepts are formalized as tuples (T, M) where T ⊆ S (sentences) and M ⊆ U (objects) form fixed points under functions β and γ. Sound translations τ_C preserve this closure—translated sentences map to the same object sets.
- Core assumption: Human conceptual knowledge can be captured via object-sentence relations with Galois connection structure.
- Evidence anchors:
  - [Section 2.3] Definition 1 defines concepts as fixed points of β and γ
  - [Section 2.3] "concept-based translations τ_C are sound... if they satisfy... concept closure invariance"
  - [corpus] Related work on "Foundations of Interpretable Models" appears but lacks specific fixed-point formalism.
- Break condition: When concepts are polysemous or context-dependent across the model and user vocabularies.

### Mechanism 3
- Claim: Alignment, intervention, and counterfactual inference unify as Bayesian inversion in the interpretable model category.
- Mechanism: String diagrams represent probabilistic computations; bending wires backward (observing evidence) and normalizing yields posterior inference. All three interpretable inference types reduce to this pattern with different priors and likelihoods.
- Core assumption: Interpretable inference admits compositional representation in partial Markov categories with Bayesian inversion.
- Evidence anchors:
  - [Section 4.1] Definition 4 formalizes concept alignment as Bayesian inversion
  - [Section 4.2] "Alignment, interventional, and counterfactual inference are all forms of Bayesian inversion"
  - [corpus] "Causal Abstraction Inference under Lossy Representations" connects to abstraction but not the Bayesian unification specifically.
- Break condition: When required inversions are computationally intractable or priors are misspecified.

## Foundational Learning

- Concept: **Markov Categories**
  - Why needed here: The paper's formalization of interpretable models uses Markov categories as the underlying probabilistic framework; objects are spaces, morphisms are conditional distributions.
  - Quick check question: Can you explain why copy maps and discard maps are primitive operations in Markov categories?

- Concept: **Commuting Diagrams**
  - Why needed here: Each symmetry is expressed diagrammatically—commutativity means alternative computational paths yield identical results, which is the formal test for interpretability.
  - Quick check question: If X → Y → Y[h] equals X → X[h] → Y[h], what does this imply about the user's ability to predict model outputs?

- Concept: **Bayesian Inversion**
  - Why needed here: Unifies the three inference types; understanding how observing evidence (bending wires) induces posterior distributions is essential.
  - Quick check question: How does the interventional inference diagram differ from the alignment diagram in terms of what serves as the "prior"?

## Architecture Onboarding

- Component map:
  - Input space X → [P(Z|X) compressor] → Latent Z → [P(C|Z) concept encoder] → Concept space C → [P(Y|C) predictor] → Output Y
  - Translation maps τ_X, τ_Y connect model representations to user spaces X[h], Y[h]
  - User hypothesis category H_m constrains allowable structural properties

- Critical path:
  1. Verify information invariance: Confirm I(Y;X) = I(Y;Z) with H(Z) ≪ H(X)
  2. Verify concept closure: Test that translations τ_C preserve fixed-point structure
  3. Verify structural invariance: Ensure model functor F: Im → H_m preserves structural properties
  4. Verify inference equivariance: Test commutativity of translated inference diagram

- Design tradeoffs:
  - Stronger compression (lower H(Z)) improves tractability but risks information loss
  - Richer concept vocabularies improve semantic alignment but increase verification complexity
  - Restrictive structural properties (e.g., linearity) enhance user-simulability but may reduce expressiveness

- Failure signatures:
  - Intractable verification tables suggest information invariance violation
  - Inconsistent translations across similar inputs suggest concept closure violation
  - User prediction errors on simple inputs suggest structural invariance violation
  - Non-commuting diagrams indicate inference equivariance failure

- First 3 experiments:
  1. Implement P(Z|X) as a variational encoder on a tabular dataset; measure I(Y;Z) vs. I(Y;X) to test information invariance empirically.
  2. Define a small concept vocabulary (5-10 concepts) for image classification; verify τ_C preserves object membership via held-out annotation consistency.
  3. Restrict P(Y|C) to linear functions; test whether users can predict outputs on novel inputs compared to an unconstrained neural predictor.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are the four proposed symmetries sufficient to subsume all desirable interpretability properties, or do future properties require additional symmetries?
- **Basis in paper:** [explicit] Section 5 discusses the alternative position that "Symmetries are not enough," noting that "future research may uncover additional properties that require either further symmetries or an altogether different formal framework."
- **Why unresolved:** The authors hypothesize sufficiency but acknowledge the field is young; no comprehensive proof exists showing these four symmetries derive every possible interpretability desideratum (e.g., distinct forms of simplicity).
- **What evidence would resolve it:** A formal derivation showing that a comprehensive list of existing interpretability properties (e.g., stability, robustness) emerge as consequences of these four symmetries.

### Open Question 2
- **Question:** How can we develop concrete "symmetry-complete" models that satisfy all four symmetries without performance trade-offs?
- **Basis in paper:** [explicit] Section 6 (Call to Action) urges the community to "develop new methods that are explicitly symmetry-complete w.r.t. their intended tasks and users."
- **Why unresolved:** The authors note that current concept-based models often violate structural invariance (using DNNs) while mechanistic interpretability often ignores concept-closure; a unified architecture is missing.
- **What evidence would resolve it:** The design and empirical validation of a model architecture that strictly enforces structural and concept-closure invariance while maintaining competitive predictive accuracy.

### Open Question 3
- **Question:** How can existing interpretability methods be systematically mapped to the proposed symmetries to identify implicit assumptions and violations?
- **Basis in paper:** [explicit] Section 6 (Call to Action) urges the community to "map existing interpretability methods to the proposed symmetries" and "identify which symmetries are implicitly assumed or violated."
- **Why unresolved:** The paper provides a theoretical framework but does not supply a taxonomy applying these definitions to current methods like Sparse Autoencoders or LIME.
- **What evidence would resolve it:** A comprehensive survey categorizing popular interpretable AI methods by which of the four symmetries they satisfy and which they violate.

## Limitations
- Heavy reliance on abstract category theory makes practical verification challenging without concrete algorithms
- Concept-closure invariance assumes human conceptual knowledge can be captured via fixed-point structures, which may not hold for polysemous concepts
- Structural invariance depends on specifying user's hypothesis category H_m, but provides no guidance on determining appropriate structural constraints

## Confidence
- **High Confidence**: Information invariance mechanism and its tractability benefits; the basic Markov category framework for probabilistic models
- **Medium Confidence**: Concept-closure invariance formalisms; the unification of inference types through Bayesian inversion; structural invariance as user-centrism
- **Low Confidence**: Practical verification algorithms for commutative diagrams; computational feasibility of concept alignment in high-dimensional spaces

## Next Checks
1. **Empirical Tractability Test**: Implement a concept bottleneck model on a real dataset and empirically measure whether information invariance (I(Y;X) = I(Y;Z)) translates to verifiable tractability in practice by comparing verification times with and without compression.

2. **Concept Closure Stress Test**: Design experiments with intentionally polysemous concepts where the same concept label applies to visually distinct objects; test whether concept-closure invariance predictions break down and measure the degree of semantic drift.

3. **Structural Invariance User Study**: Recruit users with different levels of ML expertise (novices, practitioners, experts) and determine whether different hypothesis categories H_m are needed; test whether models with different structural constraints (linear, monotonic, neural) are more or less interpretable to each group.