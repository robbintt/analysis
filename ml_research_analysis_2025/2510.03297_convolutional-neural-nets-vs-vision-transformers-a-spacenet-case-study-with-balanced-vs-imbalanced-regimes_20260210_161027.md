---
ver: rpa2
title: 'Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with
  Balanced vs Imbalanced Regimes'
arxiv_id: '2510.03297'
source_url: https://arxiv.org/abs/2510.03297
tags:
- balanced
- imbalanced
- vit-base
- spacenet
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We compare CNNs (EfficientNetB0) and Vision Transformers (ViT-Base)\
  \ on SpaceNet astronomy data under two label-distribution regimes: a naturally imbalanced\
  \ five-class split and a balanced-resampled split (700 images/class). Both models\
  \ use 224\xD7224 inputs, ImageNet normalization, and lightweight augmentations,\
  \ trained for 40 epochs on a single NVIDIA P100."
---

# Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes

## Quick Facts
- arXiv ID: 2510.03297
- Source URL: https://arxiv.org/abs/2510.03297
- Authors: Akshar Gothi
- Reference count: 22
- Primary result: On SpaceNet astronomy data, EfficientNetB0 achieves 93% test accuracy with strong macro-F1 and lower latency under imbalanced labels; ViT-Base is competitive at 93% but with higher parameter count and runtime.

## Executive Summary
This study compares EfficientNetB0 (CNN) and ViT-Base (Vision Transformer) on SpaceNet astronomy classification under naturally imbalanced and balanced label distributions. Both architectures achieve 93% accuracy on the imbalanced split, but EfficientNetB0 is faster and smaller. On the balanced split, both exceed 93% accuracy with EfficientNetB0 reaching 99%, demonstrating that class balance narrows architecture gaps while CNNs retain efficiency advantages. The work releases manifests, logs, and predictions for reproducibility.

## Method Summary
The study trains EfficientNetB0 and ViT-Base on 224×224 SpaceNet astronomy images under two regimes: naturally imbalanced (2869 train/820 val/410 test) and balanced-resampled (2450 train/700 val/350 test). Both use ImageNet normalization and lightweight augmentations. EfficientNetB0 trains with Adam (lr=1e-4) for 40 epochs; ViT-Base uses AdamW (lr=1e-4, weight_decay=1e-2) for 40 epochs on a single NVIDIA P100. Evaluation includes accuracy, macro-F1, balanced accuracy, per-class metrics, model size, and inference latency.

## Key Results
- On imbalanced data: EfficientNetB0 achieves 93% accuracy with strong macro-F1 and lower latency; ViT-Base matches accuracy but has higher parameter count and runtime
- On balanced data: Both models exceed 93% accuracy, with EfficientNetB0 reaching 99% while ViT-Base remains competitive
- CNNs maintain efficiency advantage (smaller model size, faster inference) across both regimes
- Class balancing narrows performance gaps between architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Balancing the label distribution narrows performance gaps between CNNs and ViTs, while CNNs retain an efficiency advantage
- **Mechanism:** Class imbalance disproportionately affects macro-F1 and minority-class recall. When classes are balanced via resampling, both architectures access more uniform gradient signal across classes, reducing the advantage of any architecture-specific robustness properties
- **Core assumption:** The balancing strategy (oversampling minority classes via augmentation) does not introduce distribution shift that favors one architecture over another
- **Evidence anchors:** [abstract] "On the balanced split, both models exceed 93% accuracy, with EfficientNetB0 reaching 99% while ViT-Base remains competitive, indicating that class balance narrows architecture gaps while CNNs retain an efficiency edge." [Section VI.B] "All models are strong; CNN is most accurate and fastest overall."

### Mechanism 2
- **Claim:** EfficientNetB0 achieves lower latency and smaller model size than ViT-Base while maintaining competitive accuracy, due to convolutional inductive biases
- **Mechanism:** CNNs encode translation invariance and local connectivity directly into the architecture, requiring fewer parameters to achieve similar representational capacity on image classification tasks. ViTs must learn spatial relationships via self-attention from data, increasing parameter count and compute
- **Core assumption:** The 40-epoch budget is sufficient for both architectures to reach near-convergence on this dataset scale (~4K images)
- **Evidence anchors:** [Section VI.A] "Dataset inference time (s): EffB0 (60.3), ViT-Base (76.3)... Model sizes (MB): 46.97 / 327.31." [Section VII] "On balanced data, ViT variants approach CNN accuracy, but CNNs remain faster and smaller."

### Mechanism 3
- **Claim:** Class-weighted cross-entropy improves minority-class recall on imbalanced data, with focal loss providing marginal additional gains
- **Mechanism:** Weighting the loss inversely to class frequency upweights minority class gradients, counteracting the dominance of majority class samples. Focal loss further reduces the relative contribution of easy (typically majority) examples
- **Core assumption:** Uniform sampling (no class-balanced sampler) is used; the improvement comes from loss re-weighting alone
- **Evidence anchors:** [Section IV.A] "Class-weighted cross-entropy with w_c = N/(K*n_c)... focal loss with focusing parameter γ∈{1,2}." [Section VI.G] "Class-weighted CE consistently improved minority recall; focal loss (γ=2) provided marginal additional gains."

## Foundational Learning

- **Concept: Class imbalance effects on macro vs. micro metrics**
  - **Why needed here:** The paper explicitly evaluates both imbalanced and balanced regimes; understanding why accuracy can be misleading under skew is essential
  - **Quick check question:** If a dataset is 90% class A and 10% class B, what accuracy would a naive classifier achieve by always predicting A?

- **Concept: Inductive bias in CNNs (translation invariance, locality) vs. ViTs (global attention, learned spatial relations)**
  - **Why needed here:** The efficiency vs. accuracy tradeoff between architectures stems from these differing structural assumptions
  - **Quick check question:** Why might a ViT require more training data than a CNN to reach similar performance?

- **Concept: Transfer learning with ImageNet pretrained weights**
  - **Why needed here:** Both architectures use ImageNet normalization and likely leverage pretrained backbones; this affects convergence speed and final performance
  - **Quick check question:** What is the difference between fine-tuning all layers vs. freezing the backbone and only training a classification head?

## Architecture Onboarding

- **Component map:** Images (224×224) -> ImageNet normalization -> Data augmentation -> EfficientNetB0/ViT-Base -> Classification head -> Loss (CE/focal) -> Evaluation metrics

- **Critical path:**
  1. Load SpaceNet data and apply 70:20:10 split (use provided manifests for reproducibility)
  2. Apply preprocessing and augmentation pipeline
  3. Initialize pretrained EfficientNetB0 and ViT-Base with ImageNet weights
  4. Train for 40 epochs with specified optimizers and loss functions
  5. Evaluate on held-out test set; report macro-F1 and per-class metrics

- **Design tradeoffs:**
  - **EfficientNetB0:** Lower latency, smaller model, strong accuracy on both regimes; preferred when deployment constraints exist
  - **ViT-Base:** Competitive accuracy, higher compute; may offer robustness advantages (cited in discussion), but unproven in this study
  - **Loss choice:** Class-weighted CE recommended for imbalanced regimes; focal loss adds complexity with marginal gains

- **Failure signatures:**
  - Validation accuracy fluctuating or diverging (may indicate learning rate too high or insufficient regularization)
  - Minority-class recall near zero under imbalance (may need stronger re-weighting or oversampling)
  - ViT training loss dropping but validation accuracy plateauing early (may indicate overfitting; consider dropout or early stopping)

- **First 3 experiments:**
  1. Reproduce the baseline: EfficientNetB0 on imbalanced SpaceNet-5 with uniform CE, 40 epochs. Log accuracy, macro-F1, and per-class recall
  2. Ablate loss: Repeat experiment 1 with class-weighted CE. Compare minority-class recall and macro-F1
  3. Compare architectures: Train ViT-Base on balanced SpaceNet-5 for 40 epochs. Compare accuracy and inference latency to EfficientNetB0

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what training data volume or pretraining scale does ViT consistently surpass EfficientNet on SpaceNet-like astronomy data?
- Basis in paper: [explicit] "we expect ViTs to increasingly outperform small CNNs as data volume and/or pretraining scale grows"
- Why unresolved: Experiments used fixed dataset sizes (2869 imbalanced, 3500 balanced) with ImageNet pretrained weights; no scaling curve was measured
- What evidence would resolve it: Systematic experiments varying training set size (e.g., 1k, 5k, 10k, 50k images) and/or pretraining scale, reporting accuracy and efficiency metrics at each point

### Open Question 2
- Question: How do CNNs and ViTs compare under actual distribution shift, noise, or out-of-distribution conditions on astronomy imagery?
- Basis in paper: [inferred] Paper cites literature suggesting "ViTs may be comparatively robust (e.g., to weather noise, OOD)" but only evaluates clean test splits without any perturbations
- Why unresolved: No experiments with injected noise, synthetic corruptions, or domain-shifted astronomical images were conducted
- What evidence would resolve it: Benchmark both architectures under controlled noise levels (Gaussian, shot noise), cosmic ray artifacts, or telescope-specific sensor variations, measuring accuracy degradation curves

### Open Question 3
- Question: Does the CNN efficiency advantage persist at higher input resolutions (384×384, 512×512) common in astronomy?
- Basis in paper: [explicit] "Single dataset at 224×224; additional astronomy corpora and higher resolutions left for future work."
- Why unresolved: All experiments used 224×224 inputs; ViT's patch-based attention may scale differently than CNN convolutions at higher resolutions
- What evidence would resolve it: Repeat experiments at multiple resolutions, reporting accuracy, latency, and memory consumption trade-offs

### Open Question 4
- Question: Would extended training (100+ epochs) narrow or widen the performance gap, particularly for ViT on balanced data?
- Basis in paper: [inferred] ViT-Base reached 97% F1 on balanced data vs EfficientNet's 99% at 40 epochs; ViT training curves suggest possible underfitting with training loss still decreasing
- Why unresolved: Fixed 40-epoch budget was applied uniformly; longer training was not explored
- What evidence would resolve it: Train both models to convergence with early stopping, comparing final accuracy, training time to convergence, and overfitting behavior

## Limitations
- Single dataset (SpaceNet) and specific architectural choices limit generalizability
- Implementation details like augmentation library versions and exact pretrained checkpoint sources are unspecified
- 40-epoch training horizon may not capture full convergence differences, particularly for transformers

## Confidence
- **High Confidence:** Architecture efficiency comparison (model size, inference latency) and balanced vs imbalanced regime effects on accuracy
- **Medium Confidence:** Claims about ViT robustness and superiority at scale, as these are inferred from literature citations rather than empirically validated in this study
- **Low Confidence:** Generalization of findings to other domains (medical imaging, natural images) and ViT performance under different pretraining regimes

## Next Checks
1. Reproduce the imbalanced regime experiment with class-weighted cross-entropy, comparing minority-class recall and macro-F1 across architectures using the provided manifests
2. Implement the balanced regime with 700 images/class, measuring accuracy gaps and training time per epoch to validate the efficiency claims
3. Vary the augmentation library and ViT pretrained checkpoint source to assess sensitivity of results to these implementation details