---
ver: rpa2
title: Insights into a radiology-specialised multimodal large language model with
  sparse autoencoders
arxiv_id: '2507.12950'
source_url: https://arxiv.org/abs/2507.12950
tags:
- features
- changes
- findings
- steering
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work applied Matryoshka-Sparse Autoencoders (SAEs) to interpret
  the internal representations of MAIRA-2, a multimodal large language model specialized
  for radiology report generation. By training SAEs on model activations and using
  LLM-based automated interpretability, the study identified a subset of interpretable
  features, including medical devices, pathologies, and textual patterns, with 288
  out of 16,384 features achieving detection F1 above 0.75.
---

# Insights into a radiology-specialised multimodal large language model with sparse autoencoders

## Quick Facts
- arXiv ID: 2507.12950
- Source URL: https://arxiv.org/abs/2507.12950
- Authors: Kenza Bouzid; Shruthi Bannur; Felix Meissen; Daniel Coelho de Castro; Anton Schwaighofer; Javier Alvarez-Valle; Stephanie L. Hyland
- Reference count: 40
- This work applied Matryoshka-Sparse Autoencoders (SAEs) to interpret the internal representations of MAIRA-2, a multimodal large language model specialized for radiology report generation. By training SAEs on model activations and using LLM-based automated interpretability, the study identified a subset of interpretable features, including medical devices, pathologies, and textual patterns, with 288 out of 16,384 features achieving detection F1 above 0.75. Steering experiments showed that while some features could guide model outputs toward or away from specific concepts, success was highly variable, with off-target changes often exceeding on-target effects. The findings reveal both the potential and limitations of mechanistic interpretability for specialized multimodal models in healthcare. The trained SAEs and interpretations are publicly released to support further research.

## Executive Summary
This paper explores mechanistic interpretability of MAIRA-2, a radiology-specialized multimodal large language model, using Matryoshka-Sparse Autoencoders (SAEs). The authors train SAEs on the model's residual stream activations to decompose them into a sparse feature space, then use LLM-based automated interpretation to identify clinically relevant concepts. While the approach successfully identifies some interpretable features related to medical devices, pathologies, and textual patterns, the majority remain uninterpretable. Steering experiments demonstrate that even interpretable features produce variable and often unpredictable effects on model outputs, highlighting the challenges of achieving precise control over specialized multimodal models.

## Method Summary
The authors extract 4096-dimensional residual stream activations from MAIRA-2 at layer 15 for tokens in radiology reports, filtering out boilerplate text and intermediate image tokens. They train a Matryoshka-SAE with expansion factor 4 (16,384 features), k=256 sparsity, and batch normalization using approximately 34.7M training tokens from MIMIC-CXR. Automated interpretability is performed using GPT-4o, which generates feature descriptions and scores them via detection F1 on held-out samples. Steering experiments involve adding scaled decoder vectors (±10) to the residual stream during generation to assess directional control over model outputs.

## Key Results
- 288 out of 16,384 SAE features achieved detection F1 above 0.75 for interpretability
- 46% of features scored below F1 0.5, indicating limited interpretability overall
- Steering experiments showed on-target changes were rare (max 11.3%) with off-target effects often dominating
- Feature activation frequency correlated with interpretability (ρ=0.40 for F1>0.85 features)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse autoencoders can decompose MAIRA-2's dense 4096-dimensional residual stream activations into a larger sparse latent space where a subset of features corresponds to human-interpretable clinical concepts.
- Mechanism: The Matryoshka-SAE maps input activations x ∈ R^n to sparse latent codes f(x) = σ(W_enc x + b_enc) using BatchTopK activation, simultaneously optimizing reconstruction at multiple nested dictionary sizes. This forces the model to learn hierarchical, potentially monosemantic features.
- Core assumption: Interpretable concepts in MAIRA-2 are approximately linearly represented in the residual stream, and sparsity constraints disentangle polysemantic neurons into cleaner features.
- Evidence anchors:
  - [abstract] "By training SAEs on model activations and using LLM-based automated interpretability, the study identified a subset of interpretable features, including medical devices, pathologies, and textual patterns, with 288 out of 16,384 features achieving detection F1 above 0.75."
  - [section 3.4] Formal definition of Matryoshka-SAE objective encouraging reconstruction at multiple levels.
  - [corpus] Related work (Cunningham et al. 2024, Templeton et al. 2024) shows SAEs finding interpretable features in general LLMs, but domain-specific multimodal models remain underexplored.
- Break condition: If concepts are fundamentally nonlinear, or if feature splitting/absorption dominates, interpretability scores will remain low regardless of SAE size or sparsity.

### Mechanism 2
- Claim: LLM-based automated interpretability can label SAE features by detecting patterns in activation differences between high-activation and non-activating samples.
- Mechanism: For each feature, sample top-decile activating and non-activating token contexts; prompt GPT-4o to generate an explanation; score by asking the LLM to predict activation on held-out examples; report detection F1.
- Core assumption: The interpreting LLM can accurately infer activation patterns from text context alone, and detection F1 correlates with human interpretability.
- Evidence anchors:
  - [section 3.6] "We score the quality of these interpretations following the detection approach from Paulo et al. (2024), which scores a (textual) feature interpretation by how well it can be used to predict whether that feature will be active on a new sample."
  - [section 4.1] Higher recall than precision observed, "indicating that feature descriptions tend towards being non-specific."
  - [corpus] Paulo et al. (2024) established detection scoring; Minder et al. (2025) suggest "hard" negatives may improve results.
- Break condition: If radiology-specific visual concepts cannot be captured through text-only interpretation, or if the interpreting LLM lacks domain knowledge, F1 scores will ceiling low.

### Mechanism 3
- Claim: Steering model outputs by adding SAE decoder vectors to the residual stream can directionally influence generations, but success is feature-dependent and frequently produces off-target effects.
- Mechanism: Extract decoder column W_dec,i for feature f_i, multiply by coefficient α (±10), add to all token positions during each decoding step. Positive α should emphasize; negative α should suppress.
- Core assumption: The feature's decoder vector approximately represents a linear direction in activation space that causally controls the concept.
- Evidence anchors:
  - [abstract] "Steering experiments showed that while some features could guide model outputs toward or away from specific concepts, success was highly variable, with off-target changes often exceeding on-target effects."
  - [section 4.2] "Purely on-target changes are relatively rare, with the highest observed proportion being 11.3%"; strong positive-negative steering correlation (ρ = 0.90).
  - [corpus] Wu et al. (2025) found steering methods underperform prompting and fine-tuning; O'Brien et al. (2024), Durmus et al. (2024) report similar failure cases.
- Break condition: If features are not well-disentangled (splitting/absorption/composition), or representations are nonlinear, steering will cause unintended activations across related features.

## Foundational Learning

- Concept: **Residual stream structure in transformers**
  - Why needed here: SAEs are applied to the residual stream output at layer 15; understanding that this accumulates information from all previous layers helps interpret why middle layers capture "abstract and semantically rich features" (per Templeton et al. 2024).
  - Quick check question: Can you explain why intervening at the middle of the network (layer 15 of 32) might capture different information than early or late layers?

- Concept: **Sparsity in dictionary learning**
  - Why needed here: The entire SAE approach relies on L0 or L1-like constraints (implemented via BatchTopK with k=256) to force features to be selective. Without understanding sparsity, the interpretability hypothesis makes no sense.
  - Quick check question: Why would a sparse representation be more interpretable than a dense one? What's the trade-off with reconstruction quality?

- Concept: **Monosemanticity vs. polysemanticity**
  - Why needed here: The paper's goal is finding "monosemantic" features (one concept per feature). The low interpretability rate (46% below F1=0.5) suggests many features remain polysemantic or uninterpretable.
  - Quick check question: What evidence in the paper suggests feature splitting or composition might be occurring? (Hint: check the discussion of repeated feature descriptions.)

## Architecture Onboarding

- Component map:
  - MAIRA-2 base model: 32-layer, 7B-parameter decoder (Vicuna v1.5 init) + radiology image encoder + MLP adapter producing 1369 visual tokens per image
  - Hookpoint: Residual stream output at layer 15 (middle layer), dimension 4096
  - Matryoshka-SAE: Encoder (4096 → 16384), BatchTopK (k=256), decoder (16384 → 4096), nested dictionary sizes per group fractions [1/2, 1/4, 1/8, 1/16, 1/16]
  - Automated interpretability pipeline: GPT-4o for explanation generation and detection scoring
  - Steering module: Decoder vector extraction, coefficient scaling (α = ±10), injection at all token positions during decoding

- Critical path:
  1. Extract 34.7M token representations from MIMIC-CXR after filtering boilerplate/intermediate image tokens
  2. Train Matryoshka-SAE (ef=4, k=256) with normalization factor 22.34
  3. For each feature, collect 50 exemplars (25 top-decile activating, 25 non-activating); generate interpretation via LLM
  4. Score on 200 held-out samples using detection F1
  5. Select high-F1 features; extract decoder vectors; apply steering with α = ±10 on validation set

- Design tradeoffs:
  - **Expansion factor**: ef=16 gave best reconstruction but ef=4 chosen for computational tractability in downstream interpretation (16K features vs. 64K)
  - **Sparsity (k)**: k=256 balanced reconstruction fidelity and low dead features; lower k increases sparsity but may lose information
  - **Layer selection**: Layer 15 chosen based on prior work suggesting middle layers contain abstract features; no ablation reported
  - **Coefficient magnitude**: α = ±10 chosen empirically; higher values cause more obvious changes but risk out-of-distribution outputs

- Failure signatures:
  - **Low interpretability (F1 < 0.5)**: Feature may be polysemantic, correspond to task-internal patterns (e.g., position in sequence), or reflect noise
  - **Off-target steering dominant**: Feature vector not cleanly disentangled; adding it activates multiple correlated concepts
  - **No steering effect**: Feature may be an "input" feature (activated by context) rather than "output" feature (causally affecting generation)
  - **Repeated feature descriptions**: Possible feature splitting—same concept spread across multiple SAE latents

- First 3 experiments:
  1. **Reproduce interpretability distribution**: Train SAE on a subset of MIMIC-CXR; verify ~1.8% of features achieve F1 > 0.75; confirm 46% below 0.5
  2. **Steering on a single interpretable feature**: Pick f1599 ("Describing findings without comparison to prior images", F1=0.79); apply α=10; manually inspect 10 outputs for on-target vs. off-target changes
  3. **Layer ablation**: Train SAEs on layers 10, 15, 20; compare interpretability score distributions to test the "middle layers are most abstract" assumption

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on LLM-based automated scoring rather than human validation, particularly for domain-specific radiology concepts
- Reversed Matryoshka group size implementation may have reduced feature interpretability compared to intended architecture
- Steering experiments show highly variable success with off-target effects frequently exceeding on-target changes
- Multimodal nature means many features encode visual-textual combinations difficult to capture through text-only LLM interpretation

## Confidence
- **High confidence**: The SAE training methodology and basic reconstruction results are well-established from prior work; the detection scoring framework follows Paulo et al. (2024) conventions.
- **Medium confidence**: The distribution of interpretability scores (288/16,384 above F1 0.75) is reproducible, but the clinical meaningfulness of individual features requires expert validation.
- **Low confidence**: Claims about feature splitting and composition are speculative, based primarily on repeated feature descriptions rather than systematic analysis.

## Next Checks
1. **Human expert validation**: Have board-certified radiologists independently evaluate the top 50 interpretable features to assess clinical accuracy and relevance of LLM-generated interpretations.

2. **Ablation on reversed Matryoshka implementation**: Train a corrected SAE with proper group fractions and compare interpretability score distributions to quantify the architectural error's impact.

3. **Multimodal steering verification**: Design experiments that directly test steering on features with strong visual components by comparing outputs with and without corresponding medical images present in the input.