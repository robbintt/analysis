---
ver: rpa2
title: 'GenieBlue: Integrating both Linguistic and Multimodal Capabilities for Large
  Language Models on Mobile Devices'
arxiv_id: '2503.06019'
source_url: https://arxiv.org/abs/2503.06019
tags:
- arxiv
- language
- training
- data
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents GenieBlue, an efficient multimodal large language
  model (MLLM) structure for on-device deployment. The authors identify two key challenges:
  (1) existing MLLMs suffer from performance degradation on pure language tasks, and
  (2) current smartphone NPUs do not support Mixture-of-Experts (MoE) architectures
  commonly used to preserve language capabilities during multimodal training.'
---

# GenieBlue: Integrating both Linguistic and Multimodal Capabilities for Large Language Models on Mobile Devices

## Quick Facts
- arXiv ID: 2503.06019
- Source URL: https://arxiv.org/abs/2503.06019
- Reference count: 40
- One-line primary result: Achieves 97% MLLM performance retention with 100% language capability preservation on mobile devices

## Executive Summary
GenieBlue addresses the challenge of deploying efficient multimodal large language models (MLLMs) on mobile devices while preserving both multimodal and pure language capabilities. The approach tackles two key problems: performance degradation on language tasks during multimodal training, and the lack of Mixture-of-Experts (MoE) support in current smartphone NPUs. By freezing original language model parameters and selectively duplicating transformer blocks with LoRA modules, GenieBlue achieves competitive multimodal performance without sacrificing language capabilities.

The model is trained on 2.5M pre-training and 645M fine-tuning data, demonstrating practical deployment on Qualcomm Snapdragon 8 Elite NPU with 30 tokens/second output speed. This makes GenieBlue suitable for real-world mobile applications while maintaining high performance across both linguistic and multimodal tasks.

## Method Summary
GenieBlue employs a non-shared base approach where original language model parameters are frozen during multimodal training. The model acquires multimodal capabilities by duplicating specific transformer blocks for full fine-tuning while adding lightweight LoRA modules to remaining blocks. This architecture preserves pure language performance while achieving competitive multimodal results. The approach is trained on 2.5M pre-training data and 645M fine-tuning data, achieving over 97% retention of MLLM performance compared to full fine-tuning while maintaining 100% of the original language model's capabilities.

## Key Results
- Achieves over 97% retention of MLLM performance compared to full fine-tuning
- Maintains 100% of original language model capabilities on pure language tasks
- Deployed on Snapdragon 8 Elite NPU with 30 tokens/second output speed

## Why This Works (Mechanism)
The approach works by preserving the integrity of the original language model through parameter freezing, while strategically adding multimodal capabilities through selective block duplication and LoRA modules. This non-shared base architecture prevents the catastrophic forgetting typically observed when fine-tuning language models for multimodal tasks. The lightweight LoRA modules provide efficient parameter updates without the computational overhead of full fine-tuning, making the approach suitable for mobile deployment.

## Foundational Learning

**LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that updates models through low-rank matrix decomposition, reducing computational overhead while maintaining performance. Why needed: Enables efficient multimodal capability addition without full fine-tuning. Quick check: Verify parameter count reduction compared to full fine-tuning.

**Transformer Block Duplication**: Strategic replication of specific transformer layers for specialized fine-tuning while keeping others frozen. Why needed: Allows targeted enhancement of multimodal capabilities without disrupting language performance. Quick check: Confirm which blocks are duplicated vs. kept frozen.

**NPU Architecture Constraints**: Understanding mobile processor limitations regarding MoE support and computational efficiency requirements. Why needed: Guides design decisions for mobile deployment. Quick check: Verify NPU compatibility and performance benchmarks.

**Catastrophic Forgetting**: The phenomenon where models lose previously learned capabilities when trained on new tasks. Why needed: Explains the need for parameter freezing in the approach. Quick check: Test language performance before and after multimodal training.

## Architecture Onboarding

**Component Map**: Frozen Language Model Base -> Duplicated Transformer Blocks -> LoRA Modules -> Multimodal Integration Layer

**Critical Path**: Input Processing -> Frozen Language Blocks -> Selective Duplication Layer -> LoRA-Enhanced Blocks -> Output Generation

**Design Tradeoffs**: The approach sacrifices some parameter sharing efficiency for guaranteed language capability preservation. While this increases model size compared to shared approaches, it ensures 100% retention of original language performance. The use of LoRA modules provides a middle ground between full fine-tuning and pure parameter sharing.

**Failure Signatures**: 
- Performance degradation on language tasks indicates insufficient parameter freezing
- Computational inefficiency suggests suboptimal LoRA configuration
- Multimodal capability gaps point to inadequate block duplication strategy

**First 3 Experiments**:
1. Language-only performance test to verify 100% retention claim
2. Multimodal capability assessment on standard benchmarks
3. Mobile deployment efficiency testing on Snapdragon 8 Elite NPU

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation focused primarily on benchmark performance rather than comprehensive real-world user experience testing
- Reliance on LoRA modules and selective block duplication introduces architectural complexity that may affect long-term maintenance
- Claims about "100% retention" of language capabilities appear overly optimistic without broader testing across diverse linguistic tasks

## Confidence

**High confidence**: The core architectural approach and its rationale are well-established in the literature
**Medium confidence**: Performance metrics on benchmarks appear robust but may not fully capture real-world utility
**Low confidence**: Claims about complete language capability retention and cross-platform generalizability

## Next Checks

1. Conduct comprehensive user experience testing across diverse real-world scenarios beyond benchmark datasets
2. Evaluate performance consistency across multiple mobile hardware platforms and NPU architectures
3. Test language capability retention across broader linguistic tasks including rare languages, complex reasoning, and edge-case prompts