---
ver: rpa2
title: Action-Free Offline-to-Online RL via Discretised State Policies
arxiv_id: '2602.00629'
source_url: https://arxiv.org/abs/2602.00629
tags:
- state
- learning
- online
- offline
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of offline reinforcement learning
  when action data is unavailable in the dataset, a scenario common in privacy-sensitive
  domains like healthcare, finance, and robotics. It proposes learning state policies
  that predict desirable next-state transitions rather than actions, enabling effective
  pre-training from action-free data.
---

# Action-Free Offline-to-Online RL via Discretised State Policies

## Quick Facts
- arXiv ID: 2602.00629
- Source URL: https://arxiv.org/abs/2602.00629
- Reference count: 40
- One-line primary result: OSO-DecQN achieves state-of-the-art action-free offline-to-online RL by discretizing state differences and applying conservative regularization, consistently outperforming existing methods across diverse benchmarks.

## Executive Summary
This paper addresses the challenge of offline reinforcement learning when action data is unavailable in the dataset, a scenario common in privacy-sensitive domains like healthcare, finance, and robotics. It proposes learning state policies that predict desirable next-state transitions rather than actions, enabling effective pre-training from action-free data. The key innovation is OSO-DecQN, a value-based algorithm that discretizes state differences and applies conservative regularization to avoid instability and ensure predictions are reachable. This method scales efficiently to high-dimensional problems and is integrated with a guided online learning mechanism using a policy-switching strategy and inverse dynamics model. Empirical results show consistent improvements in convergence speed and asymptotic performance across diverse benchmarks, including environments with up to 78 state dimensions, outperforming existing action-free methods.

## Method Summary
The method learns from action-free datasets containing only $(s, r, s')$ tuples by first normalizing states with z-score normalization, then discretizing state differences $\Delta s = s' - s$ into $\{-1, 0, 1\}$ bins. An ensemble of 5 critics estimates $Q(s, \Delta s)$ using MSE/Huber loss with conservative regularization that penalizes Q-values for state differences not observed in the dataset. During online learning, a policy-switching mechanism with annealed probability $\beta$ executes actions from either the inverse dynamics model (IDM) translating predicted $\Delta s$ to actions, or from the online agent's policy. The IDM and online agent are trained concurrently, with offline data relabeled using the online policy for IDM training.

## Key Results
- OSO-DecQN consistently outperforms action-free baselines across multiple D4RL and DeepMind Suite environments
- Discretized state prediction (BC_Δs) achieves returns comparable to action-based behavioral cloning while continuous prediction methods fail
- The guided online mechanism with annealed β accelerates convergence compared to unguided TD3/SAC baselines
- Regularization is critical: DecQN_N (without regularization) fails across all environments while OSO-DecQN succeeds
- The method scales efficiently to high-dimensional problems (up to 78 state dimensions)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Discretizing state differences into {-1, 0, 1} bins converts an unstable continuous regression problem into a stable discrete classification problem.
- **Mechanism:** The transformation δ_ε(s, s') maps continuous state differences to three bins per dimension. Z-score normalization before binning ensures scale-invariance across heterogeneous state dimensions. This allows value-based Q-learning (designed for discrete spaces) to be applied directly.
- **Core assumption:** The discretization granularity (3 bins) preserves sufficient information about optimal state transitions; finer-grained transitions are not critical for learning high-value trajectories.
- **Evidence anchors:**
  - [abstract] "integrates the transformation to scale efficiently to high-dimensional problems while avoiding instability and overfitting associated with continuous state prediction"
  - [section 4.1] "This transformation enables us to replace the instability of continuous regression with a discrete prediction problem"
  - [section 6, Table 1] BC_Δs (discrete targets) achieves returns comparable to action-based BC, while BC_s' and BC_s'-s fail dramatically
  - [corpus] Weak direct corpus support; related offline-to-online work (BAQ, uncertainty-guided frameworks) focuses on action-based settings

- **Break condition:** Environments where optimal behavior requires distinguishing fine-grained state changes (smaller than ε-threshold) may experience value degradation. Theorem 1 bounds this error as O(H√M/k).

### Mechanism 2
- **Claim:** Conservative regularization adapted to discrete state differences constrains predictions to reachable states and mitigates overestimation bias.
- **Mechanism:** The regularization term R_θ = Σ log‖exp(Q_θ(s,·))‖_1 - Q_θ(s,Δs) penalizes Q-values for state differences not observed in the dataset. This is equivalent to negative log-likelihood behavioral cloning in the discrete setting.
- **Core assumption:** The offline dataset contains sufficient coverage of reachable high-value state transitions; OOD state differences should be penalized.
- **Evidence anchors:**
  - [abstract] "applies conservative regularization to avoid instability and ensure predictions are reachable"
  - [section 4.3] "adapt this regularisation term...to mitigate the issues of both overestimation bias and state reachability constraints"
  - [section 6, Table 1] DecQN_N (without regularization) fails across all environments; OSO-DecQN consistently outperforms
  - [section 6, Table 2] DecQN_N shows prediction errors comparable to random policy; OSO-DecQN achieves low error

- **Break condition:** Highly diverse datasets (medium-replay, medium-expert) may cause the regularized policy to average across conflicting transitions, slightly increasing prediction error.

### Mechanism 3
- **Claim:** Policy switching between offline-guided actions (via IDM) and online policy accelerates convergence while avoiding instability from inaccurate early IDM predictions.
- **Mechanism:** With probability β, execute action a = I_φ(s, argmax Q(s,Δs)) from the IDM; otherwise use online policy π_on(s). Annealing β from high to low transitions from offline-guided exploration to online exploitation as the online agent matures.
- **Core assumption:** Local dynamics continuity exists—small action changes produce coherent state changes; the IDM can learn this mapping from online interaction alone.
- **Evidence anchors:**
  - [abstract] "integrated with a guided online learning mechanism using a policy-switching strategy and inverse dynamics model"
  - [section 4.4] "we consider how to leverage the offline Q-values that define the state policy to guide training online"
  - [Appendix D.3, Figure 3] Annealed β outperforms fixed β=0.8; degradation occurs when β is too high during early training (noisy IDM)
  - [corpus] Related offline-to-online methods (Cal-QL, AWAC) require action labels; this mechanism addresses the action-free gap

- **Break condition:** Strongly discontinuous dynamics or multi-modal inverse mappings where (s,Δs) corresponds to many distinct actions may require more expressive IDM architectures.

## Foundational Learning

- **Concept: Offline RL Distributional Shift**
  - **Why needed here:** Standard Q-learning overestimates values for out-of-distribution actions/states. OSO-DecQN extends conservative methods from actions to discretized state differences.
  - **Quick check question:** Can you explain why Q-values tend to be overestimated for state transitions not seen in the offline dataset?

- **Concept: Value Decomposition (DecQN)**
  - **Why needed here:** OSO-DecQN decomposes Q-values across state-difference dimensions rather than action dimensions, reducing complexity from exponential to linear in dimension count.
  - **Quick check question:** How does Q(s,a) = (1/N) Σ U_j(s, a_j) change when applied to discretized state differences instead of actions?

- **Concept: Inverse Dynamics Models**
  - **Why needed here:** The IDM translates predicted Δs into executable actions. Without it, the state policy cannot interact with the environment.
  - **Quick check question:** What assumptions about environment dynamics must hold for a lightweight IDM to successfully map (s, Δs) → a?

## Architecture Onboarding

- **Component map:** Dataset (s, r, s') → Z-score normalization → Discretization δ_ε → OSO-DecQN critic with regularization → Pre-trained Q(s, Δs) → Online phase: State s → Q(s, Δs) → argmax Δs → IDM → Guided action a_guided OR Online policy π_on → Environment → Buffer (trains both IDM and online agent)

- **Critical path:** The regularization weight α must be tuned per dataset (Table 5: values range 2-10). Incorrect α causes either overestimation (too low) or excessive conservatism (too high). The discretization threshold ε = 1e-4 is robust across environments (Appendix D.1).

- **Design tradeoffs:**
  - 3 bins vs 2 bins: 3 bins capture "no change" explicitly; 2 bins work but show higher variance (Appendix D.2)
  - IDM architecture: Lightweight (2-3 layers) sufficient; performance weakly correlated with IDM loss (r=0.19, -0.09)
  - β scheduling: Annealing from 0.5→0 outperforms fixed values; balances early exploration with late exploitation

- **Failure signatures:**
  - State policy predicts unreachable states → Check regularization α; increase if Q-values explode
  - IDM fails to translate Δs → Check for discontinuous dynamics; may need more expressive architecture
  - Online agent underperforms baseline → β may be too high early; use annealing schedule

- **First 3 experiments:**
  1. **Sanity check:** Train BC_Δs (imitation on discretized targets) on a medium-quality dataset. If returns match action-based BC, discretization preserves information.
  2. **Ablation:** Compare OSO-DecQN vs DecQN_N (no regularization) on Walker2D-medium. Expect dramatic gap confirming regularization necessity.
  3. **Online guidance:** Run guided TD3 with annealed β on HalfCheetah-medium. Compare convergence speed and asymptotic return vs unguided TD3 baseline.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can adaptive discretization mechanisms improve upon the fixed 3-bin transformation used in OSO-DecQN?
  - **Basis in paper:** [explicit] The authors suggest in the Discussion that adopting adaptive mechanisms (e.g., Seyde et al., 2024) could investigate whether "more fine-tuned control can enhance pre-training performance."
  - **Why unresolved:** The current fixed bin size (ε) may lose precision in some dimensions while being unnecessarily granular in others, requiring manual tuning.
  - **Evidence:** Benchmarks comparing the fixed {-1, 0, 1} state difference bins against adaptive binning strategies on high-dimensional control tasks.

- **Open Question 2:** Can the method be adapted to learn from video demonstrations where state vectors and explicit rewards are unavailable?
  - **Basis in paper:** [explicit] The Discussion identifies "adapt[ing] our method to learn from video comprised of suboptimal demonstrations" as a promising avenue, noting the challenge of extracting reward information.
  - **Why unresolved:** The current framework relies on vectorized states and explicit (s, r, s') tuples, which are absent in raw video data.
  - **Evidence:** Application to offline visual RL datasets (e.g., Atari or DeepMind Control from pixels) using unsupervised reward extraction or encoder-based state mapping.

- **Open Question 3:** Does the performance of the guided online mechanism degrade in environments with discontinuous dynamics or highly multi-modal inverse mappings?
  - **Basis in paper:** [inferred] Section 4.4 states the minimal Inverse Dynamics Model (IDM) assumes local continuity and might fail under "strongly discontinuous dynamics," but the paper only validates the method on standard smooth control benchmarks.
  - **Why unresolved:** It is unclear if a lightweight IDM is sufficient for contact-rich or complex robotic tasks without more expressive translation mechanisms.
  - **Evidence:** Evaluation of the policy-switching guidance on environments specifically designed with hard contact dynamics or discontinuous state transitions.

## Limitations

- The discretization granularity assumption (3 bins) may not hold for tasks requiring fine-grained state distinctions, potentially limiting applicability to domains with coarse dynamics.
- The regularization weight α requires careful per-dataset tuning (ranging 2-10), suggesting sensitivity to dataset characteristics.
- The IDM's reliance on local dynamics continuity may struggle with discontinuous or multi-modal transition functions.
- The method's effectiveness with very large state spaces (hundreds of dimensions) remains untested beyond the reported 78 dimensions.

## Confidence

- **High confidence:** The core mechanism of discretizing state differences and applying conservative regularization is well-supported by ablation studies (Table 1, 2) and empirical results across multiple benchmarks. The improvement over continuous prediction baselines is dramatic and consistent.
- **Medium confidence:** The policy switching mechanism with annealed β shows improvement over fixed schedules, but the optimal annealing schedule and β ranges may be environment-dependent. The weak correlation (r=0.19, -0.09) between IDM loss and performance suggests the IDM's role is more nuanced than direct prediction accuracy.
- **Low confidence:** The method's scalability to extremely high-dimensional state spaces (>100 dimensions) and its behavior with highly diverse datasets (multiple behavior policies) require further validation.

## Next Checks

1. **Fine-grained dynamics test:** Evaluate OSO-DecQN on environments requiring precise state control (e.g., robotic manipulation with continuous control demands) to test the 3-bin discretization assumption's limits.

2. **Regularization sensitivity analysis:** Systematically vary α across orders of magnitude for each dataset to map the performance landscape and identify potential overfitting or under-conservatism regimes.

3. **Multi-modal dynamics stress test:** Create or identify environments with non-smooth, multi-modal transition functions where small action changes produce disparate state outcomes to evaluate IDM robustness.