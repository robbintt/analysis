---
ver: rpa2
title: Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT
arxiv_id: '2510.08404'
source_url: https://arxiv.org/abs/2510.08404
tags:
- language
- arxiv
- babylm
- phillips
- adeel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Co4, a novel single-layer neural architecture
  inspired by cellular neurobiology, designed to overcome limitations of deep learning
  models such as Transformers. Unlike traditional deep architectures that rely on
  sequential processing and backpropagation, Co4 leverages triadic modulation loops
  among three two-point neuron agents (representing queries, keys, and values) to
  enable parallel, context-sensitive reasoning within a single layer.
---

# Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT

## Quick Facts
- arXiv ID: 2510.08404
- Source URL: https://arxiv.org/abs/2510.08404
- Authors: Noor Ul Zain; Mohsin Raza; Ahsan Adeel
- Reference count: 35
- 8M parameter single-layer architecture outperforms GPT-2 and GPT-BERT on BabyLM challenge benchmarks

## Executive Summary
This paper introduces Co4, a novel single-layer neural architecture inspired by cellular neurobiology, designed to overcome limitations of deep learning models such as Transformers. Unlike traditional deep architectures that rely on sequential processing and backpropagation, Co4 leverages triadic modulation loops among three two-point neuron agents (representing queries, keys, and values) to enable parallel, context-sensitive reasoning within a single layer. This mechanism allows Co4 to achieve an approximate computational cost of O(N), where N is the number of input tokens, compared to O(N²) for standard Transformers.

## Method Summary
Co4 is a single-layer transformer architecture with 8M parameters, 2 attention heads, and 256 embedding size. It uses triadic modulation loops among query, key, and value pyramidal two-point neurons to enable parallel co-evolution of representations without requiring deep layers. The model employs latent queries to reduce self-attention complexity from O(N²) to approximately O(N). Trained on BabyLM 10M token corpus for 2 epochs with 3 different hyperparameter configurations, then evaluated on zero-shot metrics and fine-tuned on SuperGLUE tasks.

## Key Results
- Outperforms GPT-2 and GPT-BERT on 5 out of 7 zero-shot metrics
- Achieves 68.00% accuracy on WUGs morphological generalization task
- Demonstrates 26.71% accuracy on Entity Tracking task
- Shows strong performance on BLiMP benchmark with 53.57% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Triadic modulation among Q, K, and V enables parallel co-evolution of representations within a single layer.
- Mechanism: Three populations of two-point neurons (TPNs) hold Q, K, and V representations. During feedforward, each updates based on the other two: Q updates from K and V, K from Q and V, V from Q and K. This creates iterative refinement without depth.
- Core assumption: That iterative intra-layer modulation can substitute for hierarchical layer-wise abstraction in transformers.
- Evidence anchors:
  - [abstract] "triadic modulation loops among query, key, and value pyramidal two-point neurons, enabling parallel co-evolution of representations without requiring deep layers"
  - [section 2] "each agent independently develops its own Q, K, and V, leading to 24 attention maps and 24 possibly different conclusions...before applying latent self-attention at an approximate cost of O(N)"
  - [corpus] No direct corpus validation of triadic loops; neighbor papers focus on layer-wise GPT-2 mechanisms, not intra-layer co-evolution.
- Break condition: If modulation iterations fail to converge or require many steps, O(N) claim weakens; if parallelism doesn't emerge, no advantage over depth.

### Mechanism 2
- Claim: Two-point neurons integrate feedforward and contextual signals, amplifying coherent inputs via burst firing.
- Mechanism: Pyramidal TPNs integrate external feedforward input at basal sites and contextual input at apical dendrites. When both depolarize simultaneously, burst firing amplifies contextually-relevant signals while attenuating mismatched signals.
- Core assumption: That this biological selectivity mechanism transfers to artificial neurons and improves token relevance detection during forward pass.
- Evidence anchors:
  - [section 1] "TPNs trigger high-frequency firing (bursting) when the FF and C inputs are matched in time...resulting in the amplification of coherent signals"
  - [section 2] "amplifies FF transmission if it is relevant in that context...Otherwise, it attenuates the signal"
  - [corpus] Weak/missing. No corpus papers validate TPN-based processing in LMs; related work on layer pruning suggests redundancy in deep nets but doesn't address intra-layer contextual gating.
- Break condition: If burst-amplification logic degrades gradient flow or fails to generalize beyond 10M token regimes.

### Mechanism 3
- Claim: Latent queries reduce self-attention complexity from O(N²) to approximately O(N).
- Mechanism: Instead of full token-to-token attention (P² × E), Co4 uses a small set of latent queries (L) that attend to all tokens (L × P × E), where L is much smaller than sequence length P.
- Core assumption: That a small fixed number of latent queries can capture sufficient representational diversity for language tasks.
- Evidence anchors:
  - [abstract] "operating at an approximate cost of O(N)...versus O(N²) for baselines"
  - [section 4] "In Co4, this is replaced by a more efficient linear term Lq · P · E, enabled by a small set of latent queries"
  - [corpus] No direct corpus validation. Neighbor papers discuss layer pruning and efficiency but through different mechanisms (e.g., layer skipping, token pruning).
- Break condition: If L must scale with N to maintain quality, linear complexity breaks; if latent queries bottleneck expressivity, downstream task performance degrades.

## Foundational Learning

- Concept: Transformer self-attention (Q·K^T·V)
  - Why needed here: Co4 modifies standard attention by replacing direct Q·K computation with modulated latent queries; understanding baseline is essential.
  - Quick check question: Can you compute attention output for a 4-token sequence given Q, K, V matrices?

- Concept: Computational complexity notation (O(N), O(N²))
  - Why needed here: Paper's efficiency claims hinge on understanding how attention scales; critical for evaluating whether claims hold at scale.
  - Quick check question: For a 1024-token sequence, what's the ratio of O(N²) to O(N) operations?

- Concept: Feedforward vs. feedback processing in neural networks
  - Why needed here: Co4's triadic loops introduce intra-layer feedback during "feedforward" phase; distinguishing these is conceptually important.
  - Quick check question: In standard backpropagation, when do weight updates occur relative to forward pass?

## Architecture Onboarding

- Component map:
  Embedding layer (learned) -> Positional encoding -> Q, K, V initialization (learned) -> Triadic modulation loops (non-parametric, L iterations) -> Latent self-attention (2 heads) -> Output projection -> Softmax

- Critical path:
  1. Token embedding → Q, K, V vectors initialized
  2. TPN modulation: For each iteration, Q updates from K,V; K from Q,V; V from Q,K using MOD transfer functions
  3. Context integration via P, D, U fields at apical sites
  4. Latent self-attention over refined representations
  5. Autoregressive next-token prediction

- Design tradeoffs:
  - Fewer parameters vs. reliance on iterative modulation convergence
  - Single layer vs. potential limits on hierarchical abstraction
  - O(N) efficiency vs. fixed latent query capacity (may bottleneck on very long sequences)
  - 2 training epochs vs. potential underfitting on larger corpora

- Failure signatures:
  - Non-convergence: If modulation loops oscillate without settling, outputs become unstable
  - Context bottleneck: If P, D, U fields don't capture sufficient context, coherence degrades
  - Latent query starvation: With too few queries (L small), model fails on tasks requiring diverse attention patterns (observed: BLiMP underperformance vs. BLiMP Supplement)

- First 3 experiments:
  1. Ablation: Remove triadic modulation (use static Q, K, V) and measure zero-shot performance drop to isolate loop contribution.
  2. Scaling test: Train Co4 on 100M tokens (10× increase) and compare convergence epochs vs. GPT-2/GPT-BERT to test sample-efficiency persistence.
  3. Latent query sweep: Vary L (8, 16, 24, 48) on Entity Tracking and WUGs tasks to identify capacity ceiling and validate O(N) assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Co4 architecture's efficiency advantage persist when scaled beyond 8M parameters, and how does performance scale relative to parameter count compared to standard Transformers?
- Basis in paper: [explicit] "In addition, scaling beyond 8M parameters is part of ongoing work."
- Why unresolved: The paper only tests a single, tiny configuration (8M parameters, 1 layer, 2 heads). Whether the O(N) complexity and triadic modulation benefits translate to larger, more practical model sizes remains untested.
- What evidence would resolve it: Systematic evaluation of Co4 models at multiple parameter scales (e.g., 30M, 124M, 350M+) with direct comparisons to equivalently-sized Transformer baselines on the same benchmarks.

### Open Question 2
- Question: How does training on larger corpora (beyond 10M tokens) affect Co4's sample efficiency and generalization relative to baselines?
- Basis in paper: [explicit] "Future directions include scaling to larger datasets."
- Why unresolved: The study is confined to the BabyLM challenge's 10M token constraint. Whether Co4's advantages are specific to low-data regimes or generalize to standard pretraining scales is unknown.
- What evidence would resolve it: Pretraining Co4 on datasets ranging from 100M to billions of tokens, comparing convergence speed, final performance, and compute requirements against GPT-2/Transformer baselines.

### Open Question 3
- Question: What is the relative contribution of the triadic modulation loops versus other architectural choices (e.g., latent queries, single-layer design) to Co4's performance gains?
- Basis in paper: [inferred] The paper attributes success to "triadic modulation loops among query, key, and value pyramidal two-point neurons" but provides no ablation studies isolating this mechanism's contribution.
- Why unresolved: Without controlled ablations, it remains unclear whether the biological inspiration or simply the architectural restructuring drives the observed improvements.
- What evidence would resolve it: Ablation experiments comparing: (1) Co4 with disabled triadic modulation, (2) single-layer Transformers with latent queries but no TPN mechanisms, and (3) full Co4, all trained under identical conditions.

### Open Question 4
- Question: How does Co4 compare against other linear-complexity architectures (e.g., Linformer, Performer, State Space Models) rather than only quadratic-attention baselines?
- Basis in paper: [inferred] The paper compares only to GPT-2 and GPT-BERT (both O(N²)), but claims O(N) complexity. Other efficient architectures are not discussed or evaluated.
- Why unresolved: Demonstrating superiority over inefficient baselines does not establish whether Co4 is competitive within the class of linear-attention models.
- What evidence would resolve it: Direct comparison of Co4 against state-of-the-art linear-attention and efficient Transformer variants on the same BabyLM benchmarks, controlling for parameter count and training compute.

## Limitations
- Architectural claims rely on external papers for mathematical formulation of triadic modulation loops
- Sample efficiency generalization untested beyond 10M token corpus with 2 training epochs
- Biological plausibility transfer lacks empirical validation against simpler alternatives
- Direct baseline comparison requires replicating GPT-2/GPT-BERT training on identical data

## Confidence
- High confidence: Zero-shot and fine-tuning results on BabyLM Challenge benchmarks are verifiable given access to evaluation code
- Medium confidence: Performance advantages over GPT-2 and GPT-BERT baselines are plausible but require direct replication
- Low confidence: Generalizability to larger datasets, longer training schedules, or different domains remains unproven

## Next Checks
1. **Ablation study on triadic modulation**: Implement a baseline Co4 variant without the iterative Q/K/V co-evolution (using static queries, keys, and values initialized once). Compare zero-shot performance on the 7 BabyLM metrics to isolate the contribution of triadic modulation loops.

2. **Scaling experiment to 100M tokens**: Train Co4 on 10× the BabyLM data (100M tokens) and measure convergence speed and final performance relative to GPT-2/GPT-BERT trained on the same corpus. This tests whether the claimed sample efficiency advantage persists at larger scales.

3. **Latent query capacity sweep**: Systematically vary the number of latent queries (L = 8, 16, 24, 48) while holding other hyperparameters constant. Measure performance on Entity Tracking and WUGs tasks to identify the capacity threshold where additional queries no longer improve results.