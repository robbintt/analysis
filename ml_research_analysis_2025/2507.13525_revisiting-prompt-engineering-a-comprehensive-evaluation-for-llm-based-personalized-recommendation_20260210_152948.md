---
ver: rpa2
title: 'Revisiting Prompt Engineering: A Comprehensive Evaluation for LLM-based Personalized
  Recommendation'
arxiv_id: '2507.13525'
source_url: https://arxiv.org/abs/2507.13525
tags:
- user
- table
- prompts
- prompt
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated 23 prompt types across 8 datasets and 12 LLMs
  to determine their effectiveness for LLM-based personalized recommendation. Results
  showed that for cost-efficient LLMs like gpt-4.1-mini, prompts that rephrased instructions,
  considered background knowledge, or clarified the task process improved accuracy,
  while NLP-style prompts like step-by-step reasoning often reduced performance.
---

# Revisiting Prompt Engineering: A Comprehensive Evaluation for LLM-based Personalized Recommendation

## Quick Facts
- arXiv ID: 2507.13525
- Source URL: https://arxiv.org/abs/2507.13525
- Authors: Genki Kusano; Kosuke Akimoto; Kunihiro Takeoka
- Reference count: 40
- One-line primary result: For cost-efficient LLMs like gpt-4.1-mini, prompts that rephrased instructions, considered background knowledge, or clarified the task process improved accuracy, while for high-performance LLMs like claude-3.7-sonnet, simple prompts matched or exceeded complex ones in accuracy while lowering cost.

## Executive Summary
This study systematically evaluated 23 prompt types across 8 datasets and 12 LLMs to determine their effectiveness for LLM-based personalized recommendation. The research found that prompt engineering strategies effective for cost-efficient models (like gpt-4.1-mini) often fail or even reduce accuracy in high-performance models (like claude-3.7-sonnet). Specifically, prompts that rephrase instructions, consider background knowledge, or clarify the task process improved accuracy for cost-efficient LLMs, while NLP-style prompts like step-by-step reasoning often reduced performance. For high-performance LLMs, simple baseline prompts achieved comparable accuracy to complex ones at significantly lower cost, with overthinking phenomena sometimes decreasing accuracy. The findings suggest using claude-3.7-sonnet with simple prompts for best accuracy, or gpt-4.1-mini with effective prompts for a cost-efficient balance.

## Method Summary
The study evaluated LLM-based personalized recommendation in a zero-shot, single-user setting across 8 public datasets (Yelp, MIND, Food, and Amazon Music/Movies/Groceries/Clothes/Books) with 100 users per dataset per user type (light: 6-11 interactions, heavy: 31+ interactions). For each user, the most recent interaction was held out as the positive item, and 9 negatives were sampled from uninteracted items. The evaluation tested 23 prompt types across 12 LLMs using temperature=0.1, with retries up to 5 times on failure and random ranking after failures. Performance was measured using nDCG@3 and Hit@3 for ranking quality, along with inference cost in USD. Statistical analysis included Wilcoxon signed-rank tests and linear mixed-effects models.

## Key Results
- For cost-efficient LLMs like gpt-4.1-mini, Rephrase, Step-Back, and ReAct prompts improved accuracy by 4.5-8.9% relative improvement over baseline
- For high-performance LLMs like claude-3.7-sonnet, simple baseline prompts achieved comparable accuracy to complex prompts at significantly lower cost
- Overthinking phenomena were observed where activating reasoning modes (claude thinking mode, o3-mini with complex prompts) sometimes decreased accuracy
- gpt-4.1-mini + Rephrase achieved ~90% of claude-3.7-sonnet accuracy at 1/5th the cost
- ReAct showed 21.5% partial output rate in gpt-4o-mini, requiring random completion of remaining items

## Why This Works (Mechanism)

### Mechanism 1: Task Rephrasing Improves Contextual Understanding
- Rephrasing prompts can improve recommendation accuracy for cost-efficient LLMs by ~4.5% relative improvement over baseline
- By asking the LLM to restate and expand the input before answering, the model engages in self-clarification—surfacing implicit user preferences and task constraints that may be buried in raw item descriptions
- Core assumption: The model's ability to infer user preferences depends on explicit task framing; cost-efficient models benefit more from this scaffolding than high-performance models that already internalize task structure
- Evidence: "prompts that rephrased instructions, considered background knowledge, or clarified the task process improved accuracy" [abstract]; "Rephrase, Step-Back, Explain, Recency-Focused, and Summarize-User showed higher accuracy" [section 4.1.2]
- Break condition: Rephrasing adds token cost (2-3x baseline in Table 33) and may confuse models with limited instruction-following capacity

### Mechanism 2: Background Knowledge Activation Anchors Recommendations
- Step-Back prompting improves accuracy by 4.4-8.9% relative improvement in cost-efficient LLMs
- By explicitly invoking abstract reasoning about recommendation principles, the model accesses latent knowledge about what drives user-item matching, rather than relying solely on surface-level pattern matching
- Core assumption: Recommendation tasks require principled reasoning about user preferences, not just pattern recognition; cost-efficient models lack this by default and need explicit prompts
- Evidence: "prompts that...considered background knowledge...improved accuracy" [abstract]; "Step-Back performed significantly better across all datasets and LLMs" [section 4.2.2]
- Break condition: For models with weak reasoning capacity (amazon-nova-lite), Step-Back showed -11.1% relative degradation (Table 8)

### Mechanism 3: High-Performance Models Resist Complex Prompting ("Overthinking")
- For high-performance LLMs like claude-3.7-sonnet, simple baseline prompts achieve comparable accuracy to complex prompts at significantly lower cost
- Advanced models have sufficient internal reasoning capacity; additional prompt scaffolding adds noise or conflicting signals
- Core assumption: Model capability and prompt complexity have a non-linear relationship; beyond a capability threshold, complexity hurts
- Evidence: "simple prompts matched or exceeded complex ones in accuracy while lowering cost. Overthinking...sometimes decreased accuracy" [abstract]; "claude-3.7-sonnet with thinking mode did not improve accuracy and even resulted in lower performance" [section 5.2]
- Break condition: For accuracy-critical applications, claude-3.7-sonnet + baseline is the recommended configuration

## Foundational Learning

- **Zero-shot LLM-based Recommendation**
  - Why needed here: The entire paper operates in a zero-shot, single-user setting where no training data or collaborative signals from other users are available
  - Quick check question: Can you explain why collaborative filtering fails in cold-start scenarios and how LLMs address this differently?

- **Ranking Metrics (nDCG@k, Hit@k)**
  - Why needed here: The paper evaluates all prompts using nDCG@3 and Hit@3
  - Quick check question: Why does nDCG penalize errors at top positions more than lower positions?

- **Instruction Following vs. Reasoning Capacity**
  - Why needed here: The error analysis reveals that many failures stem from models not following output format instructions
  - Quick check question: If a model returns correct reasoning but wrong output format, is that a prompt design issue or model capability issue?

## Architecture Onboarding

- **Component map**: User History → Prompt Template → LLM API → Output Parser → Ranking Evaluator

- **Critical path**:
  1. Standardize prompt phrasing using Table 2's `user_info`, `candidates_info`, `task_inst` structure
  2. Select prompt type based on LLM tier (Rephrase/Step-Back/ReAct for cost-efficient; Baseline for high-performance)
  3. Parse output format strictly—handle partial rankings with random completion fallback
  4. Track failure rates (Table 10) and token costs (Tables 33-35) for each prompt-LLM combination

- **Design tradeoffs**:
  - Accuracy vs. Cost: gpt-4.1-mini + Rephrase achieves ~90% of claude-3.7-sonnet accuracy at 1/5th the cost (section 5.2)
  - Complexity vs. Reliability: ReAct showed 21.5% partial output rate in gpt-4o-mini (Table 10)—high accuracy potential but execution risk
  - Model selection: phi-4 and amazon-nova-lite showed high failure rates (16-50% across prompts)—avoid for production without fallback handling

- **Failure signatures**:
  - Partial ranking returns (<10 items): Common in ReAct with gpt-4o-mini; requires random completion
  - Token length exceeded: Prevalent in phi-4; mitigated by Summarize-Item preprocessing
  - Policy violations: o3-mini blocked ReAct prompt due to content policy—test prompts before deployment
  - "None match preferences" responses: Figure 13 shows ReAct generating empty rankings when no candidates seem relevant

- **First 3 experiments**:
  1. **Baseline calibration**: Run Baseline prompt across your target LLMs on a sample dataset (100 users) to establish accuracy/cost floor. Record failure rates.
  2. **Prompt sweep for cost-efficient tier**: Test Rephrase, Step-Back, and ReAct on gpt-4.1-mini or llama3.3-70b. Compare nDCG@3, Hit@3, and token cost. Select winner based on accuracy-cost tradeoff.
  3. **High-performance validation**: If budget permits, test claude-3.7-sonnet with Baseline only. Verify that complex prompts don't improve accuracy beyond the simple baseline before committing to prompt complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does "overthinking" (activating internal reasoning modes) degrade accuracy in recommendation tasks?
- Basis in paper: The authors observe that enabling thinking modes (e.g., in claude-3.7-sonnet) or using complex reasoning prompts (e.g., ReAct on o3-mini) reduced accuracy, noting that "adding more reasoning does not always improve performance."
- Why unresolved: The paper identifies the negative correlation but does not isolate the specific features of recommendation data that cause reasoning models to hallucinate or diverge from user intent
- What evidence would resolve it: A comparative analysis of reasoning traces in models with thinking modes enabled, correlating specific reasoning steps with ranking errors

### Open Question 2
- Question: Do the identified effective prompts (Rephrase, Step-Back, ReAct) maintain their performance advantage when incorporating multi-user collaborative data?
- Basis in paper: The study explicitly restricts its scope to a "single-user setting" to isolate prompt effects, acknowledging that incorporating other users would add complexity
- Why unresolved: It is unknown if prompt techniques that clarify a single user's history are redundant or conflicting when collaborative filtering signals are introduced
- What evidence would resolve it: A comparative study measuring the performance of these prompts in a zero-shot LLM setup augmented with aggregated user behavior or graph-based context

### Open Question 3
- Question: Does the efficacy of context-expanding prompts like Rephrase scale to large-scale retrieval tasks beyond the 10-item reranking setup?
- Basis in paper: The methodology section notes the evaluation fixed the problem to re-ranking "a set of 10 items," a constrained scenario compared to full retrieval
- Why unresolved: Prompts like Rephrase significantly increase token usage; their benefits might vanish or costs might become prohibitive when applied to full retrieval tasks
- What evidence would resolve it: Experiments evaluating Rephrase and Step-Back prompts on candidate sets exceeding 100 items or on direct retrieval tasks rather than reranking

## Limitations

- The study's reproducibility is significantly constrained by missing methodological details, with only 7 of 23 prompt types fully specified
- Exact random sampling procedures for user selection and negative item sampling are unspecified, preventing exact replication of user-item splits
- Preprocessing details for item metadata across different datasets are not standardized, introducing potential variation in input quality
- The study does not report statistical power calculations for sample size decisions, making it unclear whether the 100 users per dataset per group provides sufficient power

## Confidence

- **High Confidence**: The core finding that claude-3.7-sonnet with simple baseline prompts achieves comparable accuracy to complex prompts while reducing cost. This is directly supported by Tables 8 and 9 showing consistent performance across datasets.
- **Medium Confidence**: The effectiveness of Step-Back and Rephrase prompts for cost-efficient LLMs. While Table 8 shows statistically significant improvements, the magnitude varies considerably across datasets and LLMs.
- **Low Confidence**: The generalizability of overthinking effects across all high-performance models. The study primarily tests claude-3.7-sonnet's thinking mode, with limited validation of o3-mini's reasoning capabilities due to policy restrictions.

## Next Checks

1. **Cross-dataset replication**: Validate the consistency of prompt effectiveness by testing the top 5 prompts on at least 3 new recommendation datasets not included in the original study (e.g., MovieLens, LastFM, and BookCrossing).

2. **Failure mode characterization**: Conduct a detailed failure analysis by logging the exact reasons for LLM rejections and partial rankings across all prompt-LLM combinations. Track whether failures are due to format non-compliance, token limits, or content policy violations.

3. **Cost-accuracy Pareto frontier validation**: For each LLM tier (cost-efficient vs. high-performance), construct the complete cost-accuracy Pareto frontier by testing not just the top-performing prompts but all 23 variants.