---
ver: rpa2
title: Evolution of Concepts in Language Model Pre-Training
arxiv_id: '2509.17196'
source_url: https://arxiv.org/abs/2509.17196
tags:
- feature
- features
- training
- learning
- crosscoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel method for tracking feature evolution\
  \ during language model pre-training using crosscoders\u2014a variant of sparse\
  \ autoencoders adapted to analyze activations from multiple training snapshots.\
  \ By applying cross-snapshot crosscoders, the authors reveal that features exhibit\
  \ two distinct developmental patterns: initialization-dependent features existing\
  \ from random initialization, and emergent features forming primarily around step\
  \ 1000, with more complex patterns emerging later in training."
---

# Evolution of Concepts in Language Model Pre-Training

## Quick Facts
- arXiv ID: 2509.17196
- Source URL: https://arxiv.org/abs/2509.17196
- Reference count: 40
- Authors: Xuyang Ge; Wentao Shu; Jiaxing Wu; Yunhua Zhou; Zhengfu He; Xipeng Qiu
- One-line primary result: Cross-snapshot crosscoders reveal two distinct phases of feature evolution in language model pre-training: statistical learning (early) and feature learning (late).

## Executive Summary
This paper introduces crosscoders—a variant of sparse autoencoders—to track feature evolution across language model training snapshots. By analyzing activations from multiple training checkpoints, the authors identify two distinct developmental phases: an initial statistical learning phase where models fit n-gram distributions, followed by a feature learning phase where sparse, interpretable features emerge. The method reveals that features can be initialization-dependent (present from random initialization) or emergent (forming primarily around step 1000), with more complex patterns emerging later in training. The authors demonstrate that decoder norms serve as effective proxies for feature evolution status and establish causal connections between feature evolution and downstream task performance through attribution-based circuit tracing.

## Method Summary
The method adapts crosscoders to analyze activations across 32 stratified training snapshots from Pythia models. The crosscoder uses shared sparse feature activations with snapshot-specific decoders, where decoder norms directly reflect feature presence and strength at each snapshot. JumpReLU activation with learned thresholds and a combined sparsity loss (tanh + quadratic frequency penalty) prevents feature death. The authors train on 800M tokens from SlimPajama, using dictionary sizes of 98,304 features for Pythia-160M. Evolution patterns are validated through linear probing correlations and causal attribution analysis using integrated gradients to identify feature subsets that drive downstream task performance.

## Key Results
- Cross-snapshot crosscoders successfully track feature evolution, revealing initialization-dependent features and emergent features forming around step 1000
- Decoder norms from crosscoder decoders serve as quantitative proxies for feature evolution status across training
- Pre-training exhibits a two-phase transition: statistical learning (fitting n-gram distributions) followed by feature learning (sparse interpretable features emerge)
- Attribution-based intervention on tens of crosscoder features can causally disrupt or recover downstream task performance across training snapshots

## Why This Works (Mechanism)

### Mechanism 1
Decoder norms from cross-snapshot crosscoders serve as quantitative proxies for feature evolution status across training. The crosscoder learns shared sparse feature activations across all snapshots, but snapshot-specific decoders reconstruct each snapshot's activations. When a feature does not "exist" at a particular snapshot, the sparsity penalty drives that snapshot's decoder norm toward zero to minimize loss. This creates a natural alignment where decoder norm magnitude directly tracks feature presence and strength over training time.

### Mechanism 2
Pre-training exhibits a two-phase transition from statistical learning (fitting n-gram distributions) to feature learning (sparse interpretable features emerge). In early training, the model rapidly reduces unigram and bigram KL divergence toward entropy baselines—learning Zipf's law statistical regularities with dense representations. Only after this statistical optimization nears completion do sparse features emerge in superposition, indicated by total feature dimensionality first decreasing then increasing around a turning point (~step 1000 in Pythia-160M).

### Mechanism 3
Attribution-based intervention on tens of crosscoder features can causally disrupt or recover downstream task performance across training snapshots. Model activations are decomposed into per-feature representations via crosscoder reconstruction. Integrated gradient attribution scores quantify each feature's causal effect on task metrics. By ablating top-ranked features (or keeping only them), the method identifies necessary and sufficient feature subsets for specific tasks at each training stage.

## Foundational Learning

- **Sparse Autoencoders / Dictionary Learning**: Crosscoders are a variant of SAEs; understanding the sparsity–reconstruction tradeoff is essential for interpreting crosscoder outputs. Quick check: Can you explain why L1 regularization approximates L0 sparsity and what failure modes arise from this approximation?

- **Superposition Hypothesis**: The paper's core premise is that features exist in superposition and can be disentangled via sparse dictionary learning; decoder norm dynamics rely on this framework. Quick check: In a d-dimensional activation space, how can more than d features be represented linearly?

- **Integrated Gradients / Attribution Patching**: Section 5's causal intervention relies on IG-based attribution to rank feature importance; understanding baseline selection and interpolation is critical. Quick check: What is the purpose of interpolating between clean and corrupted inputs in attribution patching, and how does it differ from single-point gradient methods?

- **Two-Phase Training Dynamics (Information Bottleneck / NTK)**: The statistical-to-feature learning transition echoes information bottleneck theory; contextualizes the paper within broader learning dynamics literature. Quick check: How does the "fitting then compression" framework from IB theory relate to the KL divergence convergence observed in early training?

## Architecture Onboarding

- **Component map**: Pythia activations -> Crosscoder encoder (W^θ_enc) -> Shared sparse features f(x) -> Snapshot-specific decoders (W^θ_dec) -> Reconstructed activations with decoder norms as evolution indicators

- **Critical path**: Collect activations from 32 stratified snapshots across training (high early resolution, sparser later sampling) -> Train crosscoder on 800M tokens to produce unified feature space with snapshot-specific decoder norms -> Validate decoder norms as evolution proxies via linear probing (expected correlation: ~-0.87 between probe error and decoder norm) -> Compute IG attribution scores, rank features, perform complementary ablations -> Cross-reference with KL divergence analysis and feature dimensionality to identify phase transitions

- **Design tradeoffs**: Dictionary size vs. sparsity (larger n_features improves explained variance but increases L0; paper uses 8× to 128× expansion ratios), snapshot count vs. granularity (more snapshots improve temporal resolution but scale memory linearly; paper uses 32 of 154 available snapshots), layer selection (middle layers balance feature richness with interpretability; early layers dominated by initialization features), JumpReLU threshold learning rate (reduced multiplier prevents threshold collapse; sensitive to initialization)

- **Failure signatures**: Feature death (features with permanently near-zero decoder norms; mitigated by including decoder norm in JumpReLU activation decision), temporal feature splitting (same concept encoded as multiple features active at different training stages; increases with dictionary size), misalignment (unrelated features forced into same dimension causes polysemanticity; detected via inconsistent top activations across snapshots), reconstruction error propagation (high ε_θ(x) distorts attribution scores; monitored via explained variance)

- **First 3 experiments**: Baseline crosscoder validation (train 16× crosscoder on Pythia-160M middle layer with 8 snapshots; confirm explained variance ≥85% and L0 <120), decoder norm probing (for 50 randomly sampled features, train linear probes on each snapshot to classify feature activation; confirm strong negative correlation r < -0.8 between probe error and decoder norm), single-task attribution (run full attribution pipeline on Simple SVA task; verify ablating top-15 features reduces metric by >50% and keeping only top-500 features recovers >90% performance)

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the observed two-phase learning dynamics (statistical learning vs. feature learning) generalize across different model architectures and training datasets? The authors note the analysis is restricted to the Pythia suite and the extent to which feature evolution patterns are consistent across diverse settings remains to be established.

- **Open Question 2**: Can crosscoder-based circuit tracing scale to explain complex, multi-step reasoning tasks rather than simple syntactic agreement? The paper identifies scaling to more complex downstream tasks as a natural direction for future work since current tasks (SVA, IOI) are relatively simple.

- **Open Question 3**: How can feature evolution be tracked continuously during training rather than at discrete snapshots? The authors identify the discrete snapshot requirement as a limitation, noting that memory costs limit observational granularity and online multi-snapshot processing techniques represent an open direction.

## Limitations
- The restricted feature set (≤1000 features per snapshot) and 800M token training data may miss subtler evolutionary patterns or longer-term consolidation effects
- Claims about feature splitting and consolidation patterns in larger models (6.9B) rely on qualitative patterns from limited snapshots without comprehensive quantitative validation
- The attribution-based causal analysis assumes linear approximations remain valid, which may break down for complex feature interactions

## Confidence

- **High confidence**: Decoder norm correlation with feature strength (validated via linear probes with r ≈ -0.87), two-phase transition evidenced by KL divergence convergence followed by feature dimensionality increase, and tens-of-features attribution effects on downstream tasks

- **Medium confidence**: Crosscoder alignment quality and the interpretation that decoder norm dynamics represent genuine feature evolution rather than reconstruction artifacts, given the methodology's novelty and lack of direct external validation

- **Low confidence**: Claims about feature splitting and consolidation patterns in larger models (6.9B), as these rely on qualitative patterns from limited snapshots and dictionary sizes without comprehensive quantitative validation

## Next Checks

1. **Crosscoder Alignment Validation**: For 50 randomly sampled features, compute pairwise cosine similarity of top-10 activations across adjacent snapshots. If average similarity remains >0.7 across the training trajectory, this supports genuine feature alignment; if it drops below 0.4, crosscoder reconstructions may be forcing false alignments.

2. **Attribution Linearity Test**: For 10 features with strong task attribution scores, perform pairwise feature ablation experiments. If combined ablation effects are strictly additive (ablating A then B reduces performance by X+Y%, not X*Y%), this supports linear approximation validity; if synergistic effects dominate, attribution rankings may be misleading.

3. **Phase Transition Robustness**: Repeat KL divergence and feature dimensionality analysis using 16× and 64× crosscoders on the same Pythia-160M snapshots. If the statistical-to-feature learning transition occurs at similar step ranges (~1000) regardless of dictionary size, this strengthens claims about genuine representational reorganization; if transition timing shifts dramatically with expansion ratio, results may be detection artifacts.