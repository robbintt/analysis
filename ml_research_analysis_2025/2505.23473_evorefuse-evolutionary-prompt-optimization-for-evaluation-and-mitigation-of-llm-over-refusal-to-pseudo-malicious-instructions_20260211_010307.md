---
ver: rpa2
title: 'EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation
  of LLM Over-Refusal to Pseudo-Malicious Instructions'
arxiv_id: '2505.23473'
source_url: https://arxiv.org/abs/2505.23473
tags:
- instruction
- language
- instructions
- large
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EVOREFUSE, an evolutionary prompt optimization
  algorithm that automatically generates diverse pseudo-malicious instructions to
  evaluate and mitigate LLM over-refusals. EVOREFUSE uses an evolutionary algorithm
  with mutation, recombination, and fitness evaluation based on variational approximation
  to maximize the Evidence Lower Bound (ELBO) on LLM refusal probability.
---

# EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions

## Quick Facts
- arXiv ID: 2505.23473
- Source URL: https://arxiv.org/abs/2505.23473
- Reference count: 40
- Introduces evolutionary prompt optimization algorithm that generates pseudo-malicious instructions to evaluate and mitigate LLM over-refusals

## Executive Summary
EVOREFUSE addresses the critical challenge of LLM over-refusal to benign but semantically sensitive instructions through an evolutionary prompt optimization framework. The method automatically generates diverse pseudo-malicious instructions that effectively trigger over-refusals across multiple LLM architectures, then uses these instructions to fine-tune models and reduce unnecessary refusals while maintaining safety. The approach employs a variational ELBO-based fitness function to overcome numerical instability in direct probability estimation, combined with strategy-guided mutation and recombination operations that discover broadly effective refusal triggers.

## Method Summary
EVOREFUSE uses an evolutionary algorithm to iteratively evolve seed instructions by maximizing evidence lower bound on LLM refusal probability. The method applies mutation operations based on three strategy categories (deceptive contexts, sensitive keywords, emotional tones), filters unsafe instructions via GPT-4o judge, evaluates fitness using ELBO decomposition, selects top candidates for recombination, and employs simulated annealing to prevent premature convergence. The algorithm generates two datasets: EVOREFUSE-TEST for evaluation (582 examples) and EVOREFUSE-ALIGN for mitigation (3,000 examples), achieving significant over-refusal reduction through supervised fine-tuning and preference-based optimization.

## Key Results
- EVOREFUSE-TEST achieves 85.34% higher average refusal rates across 9 LLMs without safety prompts
- EVOREFUSE-TEST shows 34.86% greater lexical diversity and 40.03% higher response confidence than next-best benchmark
- Fine-tuning LLAMA3.1-8B-INSTRUCT on EVOREFUSE-ALIGN reduces over-refusals by 29.85% (supervised) and 45.96% (preference-based) while maintaining safety

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maximizing ELBO on refusal probability generates more effective pseudo-malicious instructions than direct probability optimization.
- Mechanism: Direct computation of refusal probability via Monte Carlo sampling becomes numerically unstable due to extremely low likelihoods assigned to specific response sequences (observed expected log-probability of -466.97, corresponding to probabilities ~1.57×10^-203). The variational ELBO decomposition (Eq. 4) provides a tractable lower bound by combining response confidence (log pθ(y|x,s)) and refusal log-probability (log pθ(r|x,y,s)), estimated via a pre-trained refusal classifier and token logits respectively.
- Core assumption: The entropy term H(qθ(y|x)) exhibits sufficiently low variance across instructions that it can be treated as approximately constant under fixed decoding schemes (empirically, variance of entropy is ~250x smaller than variance of response confidence).
- Evidence anchors:
  - [abstract]: "EVOREFUSE employs an evolutionary algorithm... iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability"
  - [section 3.2]: "Directly computing the refusal probability is challenging... Monte Carlo sampling... becomes numerically unstable when estimating the extremely low likelihoods"
  - [corpus]: Weak direct validation; neighboring papers address over-refusal mitigation but not ELBO-based optimization specifically.
- Break condition: If entropy variance increases significantly across different instruction types or decoding schemes, the constant approximation fails, requiring explicit entropy estimation in the fitness function.

### Mechanism 2
- Claim: Evolutionary search with strategy-guided mutation and recombination produces more diverse and effective refusal-triggering instructions than gradient-based or rewriting methods.
- Mechanism: The algorithm iteratively applies three mutation categories derived from empirical analysis of 500 low-similarity instructions: (I) introducing deceptive contexts (controversial topics, imaginary scenarios, potential harm implications), (II) adding sensitive keywords (violent, prejudiced, or other sensitive terms), and (III) amplifying emotional tones (anger, disgust, despair). Recombination synthesizes semantically salient segments from high-fitness pairs. Simulated annealing with Metropolis criterion prevents premature convergence.
- Core assumption: The identified trigger strategies generalize across diverse LLM architectures, not just the target model (LLAMA3.1-8B-INSTRUCT used for fitness evaluation).
- Evidence anchors:
  - [abstract]: "outperforms the next-best benchmark with 85.34% higher average refusal triggering rate across 9 LLMs... 34.86% greater lexical diversity"
  - [section 5.1]: "EVOREFUSE-TEST generalizes well beyond the target model, indicating that EVOREFUSE discovers broadly effective over-refusal triggers rather than model-specific exploits"
  - [corpus]: Related work on evolutionary prompt search exists (e.g., "A Toolbox for Improving Evolutionary Prompt Search") but does not specifically validate the mutation/recombination strategies for over-refusal.
- Break condition: If refusal patterns are highly model-specific, transferability degrades; the 364.29% gain on LLAMA3.1-8B-INSTRUCT vs. smaller gains on other models suggests partial specialization.

### Mechanism 3
- Claim: Over-refusals stem from shortcut learning where models disproportionately attend to sensitive keywords in early transformer layers while ignoring broader harmless context.
- Mechanism: Gradient-based attribution analysis reveals that refusal predictions concentrate on tokens like "dangerous," "explode," "manipulate," "exploit," and "fraud." When these keywords are replaced with neutral alternatives while preserving semantics (e.g., "dangerous cake" → "bold cake"), attention shifts to benign terms and refusals cease. Information flow analysis shows elevated flow for sensitive tokens concentrated in layers 1-15.
- Core assumption: The gradient-based attribution and information flow metrics accurately reflect the model's internal decision process for safety judgments.
- Evidence anchors:
  - [abstract]: "over-refusals primarily stem from shortcut learning where models focus on sensitive keywords while ignoring broader context, with early transformer layers playing a critical role"
  - [section 5.2]: "gradient-based attribution reveals that LLAMA3.1-8B-INSTRUCT disproportionately attends to sensitive keywords... largely ignoring the broader semantic context"
  - [corpus]: The paper "Understanding and Mitigating Over-refusal for Large Language Models via Safety Representation" addresses similar phenomena but through safety representation analysis rather than gradient attribution.
- Break condition: If gradient-based attribution captures spurious correlations rather than causal mechanisms, interventions based on keyword replacement may not generalize to structurally different over-refusal patterns.

## Foundational Learning

- Concept: **Variational Inference and ELBO**
  - Why needed here: Understanding why the paper uses ELBO instead of direct probability estimation requires grasping how Jensen's inequality provides tractable lower bounds for intractable marginal likelihoods.
  - Quick check question: Can you explain why adding a constant to an objective function doesn't change the location of its maximum, and how this justifies omitting the entropy term?

- Concept: **Evolutionary Algorithms (Mutation, Recombination, Selection)**
  - Why needed here: EVOREFUSE's core search mechanism relies on evolutionary operations; understanding simulated annealing's role in escaping local optima is critical for debugging convergence issues.
  - Quick check question: If fitness evaluation is removed, what failure mode would you expect in the optimization trajectory (hint: see ablation results in Figure 2)?

- Concept: **Gradient-based Attribution and Information Flow**
  - Why needed here: The paper's mechanistic explanation for over-refusal relies on interpreting gradient norms and attention-based information flow as indicators of which tokens influence safety decisions.
  - Quick check question: What is the difference between gradient-based weight (Eq. 13) and information flow (Eq. 14), and why might they identify different high-attribution tokens?

## Architecture Onboarding

- Component map:
  Seed Instructions (x₀) -> Mutation (9 strategies via GPT-4o) -> Safety Classifier (GPT-4o judge) -> Fitness Evaluation (ELBO estimate) -> Selection (top-L=4 for recombination) -> Recombination (N=2 pairs via GPT-4o) -> Simulated Annealing (τ₀=0.1, β=0.005) -> Best-of-run selection (return max across all iterations)

- Critical path: The fitness evaluation (Eq. 5) is the computational bottleneck—it requires K forward passes through the target LLM per candidate instruction. The λ/Tk normalization balances refusal probability against response confidence; misconfiguring this can cause the optimizer to prefer either low-confidence refusals or high-confidence compliances.

- Design tradeoffs:
  - **Target model choice**: Using LLAMA3.1-8B-INSTRUCT as fitness evaluator improves performance on that model (364.29% gain) but may reduce transferability to other architectures.
  - **Safety verification**: GPT-4o judge adds computational overhead but is necessary to prevent generating truly malicious instructions; the paper reports 93% safe classification with 5% debatable.
  - **Mutation strategy mix**: The ablation shows "imaginary scenario" has highest success rate (20%) while "prejudiced words" has lowest (5%); the default uses all strategies uniformly.

- Failure signatures:
  - **Premature convergence**: Fitness plateaus early with low lexical diversity; check simulated annealing temperature schedule and recombination rate.
  - **Safety filter over-rejection**: Valid pseudo-malicious instructions classified as unsafe; verify judge prompt calibration against human annotation guidelines (Table 5).
  - **Low refusal transfer**: High fitness on target model but low refusal rates on other LLMs; indicates overfitting to target model's shortcut patterns.

- First 3 experiments:
  1. **Validate fitness-refusal correlation**: Run EVOREFUSE on 20 seed instructions, compute both fitness scores and actual refusal rates (PRR/CRR) on target model. Confirm positive correlation; if not present, investigate λ scaling or classifier calibration.
  2. **Ablate mutation strategies**: Run single-round mutation on XSTEST with each of the 9 strategies independently. Replicate Table 7 success rates; significant deviations suggest implementation errors in mutation prompts.
  3. **Test transferability**: Generate 50 EVOREFUSE instructions using GPT-4o as mutator, evaluate refusal rates on at least 3 LLMs not used during optimization. Expect some degradation but >50% of target model performance indicates reasonable generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- Transferability varies significantly across LLM architectures (364.29% gain on LLAMA3.1-8B-INSTRUCT vs. 32.46% on LLAMA3.1-70B-INSIGHT)
- Use of GPT-4o for mutation and safety filtering may introduce distributional shifts affecting generalization
- ELBO-based fitness function assumes stable entropy across instructions, which may not hold for all instruction types

## Confidence
- **High confidence**: ELBO formulation for stable fitness evaluation; evolutionary algorithm implementation; over-refusal mitigation results
- **Medium confidence**: Transferability across diverse LLM architectures; gradient-based attribution mechanism for explaining over-refusals
- **Low confidence**: Universal applicability of identified mutation strategies; safety filter's reliability

## Next Checks
1. **Cross-architecture transferability test**: Generate 100 EVOREFUSE instructions using GPT-4o as mutator, then evaluate refusal rates across 5 diverse LLM families (decoder-only, encoder-decoder, MoE, etc.) with varying parameter counts. Compute both average improvement and variance to quantify generalization beyond the current 9-model sample.
2. **Mutation strategy ablation under distribution shift**: Test each of the 9 mutation strategies on seed instructions from completely different domains (e.g., scientific queries, creative writing prompts) and compare success rates to those reported for XSTEST. This validates whether the strategies generalize beyond the narrow domain used in the paper.
3. **ELBO approximation stress test**: Systematically vary the entropy approximation assumption by explicitly computing entropy for different instruction types and decoding schemes. Measure how fitness rankings change when the constant entropy assumption is relaxed, and quantify the impact on the final instruction set's refusal effectiveness.