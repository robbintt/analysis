---
ver: rpa2
title: Adaptation of Agentic AI
arxiv_id: '2512.16301'
source_url: https://arxiv.org/abs/2512.16301
tags:
- agent
- adaptation
- tool
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive survey of adaptation
  strategies for agentic AI systems, introducing a unified framework that organizes
  adaptation into four paradigms: agent adaptation with tool execution signals (A1),
  agent adaptation with agent output signals (A2), agent-agnostic tool adaptation
  (T1), and agent-supervised tool adaptation (T2). The survey systematically reviews
  representative methods in each category, analyzes their trade-offs across dimensions
  like cost, flexibility, and generalization, and provides practical guidance for
  system design.'
---

# Adaptation of Agentic AI

## Quick Facts
- arXiv ID: 2512.16301
- Source URL: https://arxiv.org/abs/2512.16301
- Reference count: 40
- Primary result: Introduces four-paradigm framework for agentic AI adaptation, showing T2 methods achieve A2-level performance with 70× less training data

## Executive Summary
This paper presents the first comprehensive survey of adaptation strategies for agentic AI systems, organizing methods into four paradigms: agent adaptation with tool execution signals (A1), agent adaptation with agent output signals (A2), agent-agnostic tool adaptation (T1), and agent-supervised tool adaptation (T2). The framework clarifies the design space by analyzing trade-offs across cost, flexibility, generalization, and modularity. Key findings include T2 methods achieving comparable performance to A2 with 70× less data through symbiotic adaptation, where lightweight tools are trained under supervision from frozen agents. The survey identifies emerging research directions including co-adaptation, continual adaptation, safe adaptation, and efficient adaptation, while demonstrating applications across deep research, software development, computer use, and drug discovery.

## Method Summary
The paper systematically reviews 40+ representative methods across agent adaptation and tool adaptation paradigms. The four adaptation paradigms are defined mathematically: A1 optimizes agents via tool-execution rewards (e.g., retrieval metrics, code pass rates), A2 optimizes agents via output quality rewards (e.g., answer correctness), T1 trains tools independently of agents (plug-and-play), and T2 trains tools using frozen agent outputs as supervision. The survey analyzes methods along dimensions including cost (data and compute requirements), flexibility (ability to adapt to new tasks), generalization (performance on out-of-distribution data), and modularity (interchangeability of components). Key quantitative findings emerge from comparing data efficiency across paradigms, with T2 methods like s3 achieving A2-level performance with 70× fewer training samples.

## Key Results
- T2-style agent-supervised tool adaptation achieves comparable performance to A2 methods with 70× less training data through symbiotic adaptation
- RLVR methods enable agents to learn tool-use mechanics through direct environment feedback rather than imitation
- Trained agents can be "graduated" into reusable T1 tools, creating a pipeline from A1/A2 training to T1 deployment
- Monolithic adaptation (A1/A2) offers parametric flexibility but risks catastrophic forgetting, while modular adaptation (T1/T2) enables hot-swapping but is bounded by frozen agent capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: T2-style agent-supervised tool adaptation may achieve comparable performance to A2-style agent adaptation while requiring significantly less training data.
- Mechanism: A frozen foundation model provides stable supervision signals to train lightweight peripheral tools. The tool only learns narrow procedural skills (e.g., search policy), while the frozen agent retains domain knowledge and reasoning—decoupling skill acquisition from general reasoning.
- Core assumption: The frozen agent already possesses sufficient domain knowledge and reasoning capability; only procedural tool-use skill needs learning.
- Evidence anchors:
  - [abstract] "T2 methods achieving comparable performance to A2 methods with 70× less data through symbiotic adaptation"
  - [section 6.4] "s3 achieves 58.9% average accuracy with only 2.4k training samples—70 × less data than Search-R1 (an A2-style agent requiring 170k examples)"
  - [corpus] Limited corpus evidence on T2-specific data efficiency; this appears to be an emerging finding from this survey
- Break condition: If the frozen agent lacks sufficient domain knowledge or reasoning capability for the target task, T2 adaptation will plateau at the agent's ceiling.

### Mechanism 2
- Claim: RLVR (Reinforcement Learning with Verifiable Rewards) enables agents to learn tool-use mechanics through direct environment feedback rather than imitation.
- Mechanism: The agent takes actions in an environment, tools execute and return verifiable outcomes (e.g., code pass/fail, retrieval metrics), and these outcomes serve as reward signals for policy optimization via PPO or GRPO. The feedback loop is causal and grounded in actual tool behavior.
- Core assumption: The environment provides reliable, sufficiently dense verifiable signals; reward design captures the intended behavior without gaming.
- Evidence anchors:
  - [section 4.1.2] "DeepRetrieval formalizes query reformulation as an MDP where reward is directly derived from retrieval metrics like Recall@K or NDCG"
  - [section 4.1.2] "RLEF frames code synthesis with rewards from test-case execution"
  - [corpus] "Scaling Agents via Continual Pre-training" notes post-training approaches building on general-purpose foundation models underperform in agentic tasks—suggesting environment-specific adaptation matters
- Break condition: If rewards are sparse, noisy, or gameable, RLVR may converge to suboptimal policies or exhibit reward hacking.

### Mechanism 3
- Claim: Adapted agents can be "graduated" into reusable T1 tools, creating a pipeline from A1/A2 training to T1 deployment.
- Mechanism: An agent trained under A1 or A2 paradigms reaches expert performance on a specific task (e.g., query rewriting, code generation), is frozen, and redeployed as a callable subagent-tool that other agents can invoke without retraining.
- Core assumption: The trained agent's skill generalizes sufficiently across different host agents and task contexts.
- Evidence anchors:
  - [section 6.3.1] "DeepRetrieval is trained via on-policy A1 RL as a query reformulation agent, but once frozen it can be used as an interchangeable T1 retrieval-augmentation tool"
  - [section 6.3.1] "SWE-Grep is trained as a specialized RL subagent for fast, multi-turn, highly parallel code context retrieval, and then exposed as a tool"
  - [corpus] "Chain-of-Agents" proposes end-to-end agent foundation models via multi-agent distillation—related but distinct from the graduation concept
- Break condition: If the graduated agent overfits to its training distribution, it may fail when invoked by different host agents or in novel contexts.

## Foundational Learning

- Concept: **Foundation Models as Agents**
  - Why needed here: The paper assumes readers understand that LLMs/multimodal models serve as reasoning cores that can be augmented with tools, memory, and planning modules.
  - Quick check question: Can you explain the difference between a foundation model used for single-turn generation versus one embedded in an agentic loop with tool access?

- Concept: **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: Multiple A1 methods (DeepRetrieval, RLEF, Code-R1) use RLVR; understanding policy optimization from environment feedback is essential.
  - Quick check question: What distinguishes RLVR from supervised fine-tuning in terms of data requirements and feedback granularity?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: RAG serves as a primary example domain for A1, A2, and T2 paradigms; understanding retriever-generator interaction is foundational.
  - Quick check question: How does the supervision signal differ between training a retriever via perplexity reduction (T2) versus training an agent via final-answer correctness (A2)?

## Architecture Onboarding

- Component map:
  - **Agent (A)**: Foundation model (LLM/multimodal) serving as reasoning core; can be adapted (A1/A2) or frozen (T1/T2)
  - **Tool (T)**: External callable components—retrievers, code executors, APIs, subagents, memory modules
  - **Environment (E)**: Provides verifiable feedback signals (execution results, test outcomes, retrieval metrics)
  - **Offline Data (D)**: Curated trajectories, demonstrations, or preference pairs for supervised learning

- Critical path:
  1. Identify adaptation target: agent parameters (A1/A2) or tool parameters (T1/T2)
  2. Select supervision source: tool-execution signals (A1), agent-output signals (A2), agent-agnostic data (T1), or frozen-agent supervision (T2)
  3. Choose training paradigm: SFT for imitation, DPO for preferences, RLVR for environment-grounded optimization
  4. If using RL: define reward function, select optimizer (PPO/GRPO), implement KL regularization

- Design tradeoffs:
  - A1 vs A2: A1 optimizes tool mechanics with causal feedback; A2 optimizes holistic strategy but may suffer sparse rewards
  - T1 vs T2: T1 offers plug-and-play modularity but may under-optimize for specific agents; T2 achieves data efficiency but ties tools to specific frozen agents
  - Monolithic (A1/A2) vs Modular (T1/T2): Monolithic offers parametric flexibility but risks catastrophic forgetting; Modular enables hot-swapping but bounded by frozen agent capability

- Failure signatures:
  - Reward hacking: Agent discovers shortcuts to maximize reward without solving task (e.g., falsifying logs)
  - Distribution shift: Tool trained on one data distribution fails on deployment distribution
  - Cascading errors: Multi-tool pipelines accumulate errors across stages
  - Forgetting: Monolithic agent adaptation degrades previously learned capabilities

- First 3 experiments:
  1. **Baseline comparison**: Implement a simple RAG pipeline with frozen retriever (T1) vs. retriever adapted via frozen-agent perplexity feedback (T2). Measure retrieval quality and downstream QA accuracy.
  2. **Data efficiency test**: Train a search subagent using T2 (s3-style, ~2k samples) versus A2 (Search-R1-style, ~170k samples) on identical task. Compare accuracy and training time.
  3. **Graduation validation**: Train an A1 agent for code debugging, freeze it, deploy as T1 tool for a different host agent. Measure transfer performance and identify failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can unified agent-tool co-adaptation frameworks achieve stable convergence, and what mechanisms (e.g., centralized critics, pacemaker mechanisms) are required to prevent oscillating or degenerate agent-tool equilibria?
- Basis in paper: [explicit] §8.1 identifies co-adaptation as a key opportunity, noting the "intractable credit assignment problem" and "Stability–Plasticity Dilemma" where simultaneous adaptation risks chaotic dynamics or "Red Queen" regimes without convergent improvement.
- Why unresolved: Current paradigms freeze one component (agent in T2, tools in A1/A2); nascent work (e.g., MATPO) handles only prompt-level roles within a single LLM, not distributed heterogeneous systems.
- What evidence would resolve it: Empirical demonstrations of bi-level optimization converging to stable symbiotic equilibria in heterogeneous agent-tool systems, with ablations isolating which stabilization mechanisms are necessary.

### Open Question 2
- Question: Why does T2-style agent-supervised tool adaptation achieve comparable performance to A2 with ~70× less training data, and what determines the boundary conditions where this efficiency advantage holds?
- Basis in paper: [explicit] The paper reports that s3 (T2) achieves 58.9% accuracy with 2.4k samples versus Search-R1 (A2) requiring ~170k samples, attributing this to T2 only learning procedural skill while A2 must learn knowledge, reasoning, and tool use simultaneously.
- Why unresolved: The "symbiotic inversion" explanation is conceptual; it remains unclear whether this advantage transfers across domains, tool types, and agent scales, or whether it breaks down for tasks requiring more complex reasoning.
- What evidence would resolve it: Controlled experiments varying task complexity, frozen agent capability, and tool expressivity to map where T2 efficiency gains emerge versus degrade.

### Open Question 3
- Question: To what extent does aggressive RL-based adaptation erode safety guardrails established during prior SFT or alignment, and what countermeasures preserve safety while optimizing task performance?
- Basis in paper: [explicit] §8.3 reports empirical analysis of DeepSeek-R1 revealing that "aggressive RL optimization for reasoning can erode safety guardrails," with models becoming more susceptible to jailbreaks through chain-of-thought reasoning around refusal mechanisms.
- Why unresolved: The trade-off between reasoning capability gains and safety degradation is not well-characterized; it is unclear whether this is inherent to RLVR or an artifact of current training procedures.
- What evidence would resolve it: Systematic safety benchmarking across adaptation paradigms, with metrics quantifying both capability improvement and guardrail retention.

### Open Question 4
- Question: What are the computational and performance limits of on-device, quantized, and parameter-efficient adaptation for agentic systems operating under resource constraints?
- Basis in paper: [explicit] §8.4 identifies efficient adaptation as a key opportunity, noting that current systems rely on large-scale GPU clusters and that shifting adaptation to edge devices could enable privacy-preserving continual learning.
- Why unresolved: Preliminary evidence (FlashRL, LoRA without Regrets) suggests feasibility, but the performance gap relative to full fine-tuning and the scalability to multi-tool agentic systems remain unquantified.
- What evidence would resolve it: Benchmarks evaluating agentic task performance under constrained memory, compute, and precision settings, with comparisons to unconstrained baselines.

## Limitations

- Evidence for T2's 70× data efficiency is anchored primarily in s3 results with limited cross-method validation
- Framework assumes frozen agents retain sufficient capability, but real-world agents may degrade or have incomplete knowledge
- Many described methods lack public implementations, making empirical validation difficult

## Confidence

- **High**: The four-paradigm taxonomy (A1, A2, T1, T2) is logically coherent and well-grounded in surveyed literature
- **Medium**: Quantitative claims about T2 data efficiency (70×) are supported by s3 results but need broader validation
- **Low**: Practical guidance for choosing between paradigms assumes idealized conditions and may not capture real-world constraints

## Next Checks

1. Implement a controlled experiment comparing T2-style search adaptation (s3 approach) against A2 baseline using identical frozen agents and datasets to verify the 70× data efficiency claim
2. Test the graduation hypothesis by training an A1 agent, freezing it, and deploying as a T1 tool across multiple host agents to measure transfer performance and identify generalization limits
3. Conduct ablation studies on reward functions in RLVR methods (DeepRetrieval, RLEF) to quantify sensitivity to reward design and identify conditions leading to reward hacking