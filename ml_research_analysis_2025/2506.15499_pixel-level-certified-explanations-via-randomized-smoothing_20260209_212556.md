---
ver: rpa2
title: Pixel-level Certified Explanations via Randomized Smoothing
arxiv_id: '2506.15499'
source_url: https://arxiv.org/abs/2506.15499
tags:
- certified
- methods
- attribution
- input
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first framework for pixel-level certification\
  \ of post-hoc attribution methods. It transforms continuous attribution maps into\
  \ binary pixel-importance classes and applies Randomized Smoothing for segmentation\
  \ to certify each pixel\u2019s robustness under $\\ell2$-bounded perturbations."
---

# Pixel-level Certified Explanations via Randomized Smoothing

## Quick Facts
- arXiv ID: 2506.15499
- Source URL: https://arxiv.org/abs/2506.15499
- Reference count: 40
- This paper introduces the first framework for pixel-level certification of post-hoc attribution methods.

## Executive Summary
This paper introduces the first framework for pixel-level certification of post-hoc attribution methods. It transforms continuous attribution maps into binary pixel-importance classes and applies Randomized Smoothing for segmentation to certify each pixel's robustness under ℓ₂-bounded perturbations. The authors evaluate 12 attribution methods across 5 ImageNet models and propose three new metrics: %certified (robustness), Certified GridPG (localization), and a deletion-based faithfulness score. Results show that RISE and LRP achieve the best balance of robustness, localization, and faithfulness. The work enables fine-grained, interpretable, and certified attributions suitable for high-stakes applications.

## Method Summary
The framework reformulates pixel-level attribution certification as a segmentation problem. It sparsifies continuous attribution maps into binary pixel-importance classes (top K% vs. bottom-100-K%), then applies Randomized Smoothing to certify each pixel's importance against ℓ₂-bounded perturbations. For each pixel, Monte Carlo sampling with Gaussian noise estimates class probabilities, enabling certification as "important" (1), "unimportant" (0), or "abstain" (⊘). The method evaluates 12 attribution methods across 5 ImageNet models using three proposed metrics: %certified (ratio of certified pixels), Certified GridPG (localization score), and deletion-based faithfulness (confidence drop when removing certified pixels).

## Key Results
- RISE and LRP achieve the best balance of robustness, localization, and faithfulness across all evaluated models
- Gradient-based methods (Grad, GB, IxG, IntGrad) show near-zero certification rates at input layer due to noise sensitivity
- Higher certification thresholds (τ) and noise levels (σ) increase certified radius but significantly reduce %certified
- Sparsification level K controls spatial granularity, with K=5% enabling fine feature certification (e.g., eyes) while K=50% produces coarse regions

## Why This Works (Mechanism)

### Mechanism 1: Randomized Smoothing for Robustness Certification
Adding calibrated Gaussian noise to inputs enables probabilistic certification of prediction stability under ℓ₂-bounded adversarial perturbations. By transforming any black-box function f into a smoothed version g through sampling f(x + ε) where ε ~ N(0, σ²I), the method certifies that predictions hold within radius R = σ·Φ⁻¹(τ). Higher σ yields larger certified radii but degrades base performance.

### Mechanism 2: Sparsification Enables Binary Segmentation Reformulation
Binarizing continuous attribution maps into top-K% vs. bottom-(100-K)% pixels transforms the attribution problem into a segmentation problem. This binary output space allows direct application of segmentation certification, enabling per-pixel certification with abstention capabilities.

### Mechanism 3: Per-Pixel Certification via Monte Carlo Sampling
Each pixel can be independently certified as "important" (1), "unimportant" (0), or "abstain" (⊘) by estimating class probabilities through repeated noisy sampling. The framework computes h_K(x + εᵢ) for each noise realization, counts frequency each pixel appears in top-K%, and applies threshold τ to determine certification status.

## Foundational Learning

- Concept: Randomized Smoothing Basics
  - Why needed here: The entire certification framework builds on this; without understanding how noise provides guarantees, you cannot interpret certified radii or tune σ/τ.
  - Quick check question: If you increase noise level σ from 0.15 to 0.25, what happens to the certified radius R, and what trade-off must you accept?

- Concept: Post-hoc Attribution Method Taxonomy
  - Why needed here: The paper certifies 12 methods across three families (backpropagation, activation-based, perturbation-based); knowing which family a method belongs to predicts its certification behavior.
  - Quick check question: Categorize Grad, Grad-CAM, and RISE. Which family tends to have higher %certified at the input layer according to Figure 5?

- Concept: Segmentation Certification (Fischer et al., 2021)
  - Why needed here: This prior work provides Theorem 3.1, the mathematical foundation enabling per-pixel certification with abstention.
  - Quick check question: In segmentation certification, what does it mean when a pixel is assigned the abstain label ⊘, and what threshold τ controls this?

## Architecture Onboarding

- Component map: Input Image x -> Attribution Method h (12 options) -> Sparsification h_K (param: K%) -> Monte Carlo Sampling (n noisy samples, noise σ) -> Probability Estimation (per-pixel) -> Certification Decision (threshold τ → output: 1/0/⊘ per pixel) -> Evaluation Metrics (%certified, Certified GridPG, Faithfulness)

- Critical path:
  1. **Attribution method selection**: Determines base robustness (RISE/LRP robust; gradient methods fragile at input layer)
  2. **Sparsification K**: Controls granularity (K=5% → fine features like eyes; K=50% → coarse regions)
  3. **Noise level σ**: Controls certified radius R = σ·Φ⁻¹(τ) (higher σ → larger R but potential accuracy drop)
  4. **Threshold τ**: Controls certification strictness (higher τ → fewer but more confident certifications)
  5. **Sample count n**: Controls estimation confidence (default n=100 with α=0.001)

- Design tradeoffs:
  - **Robustness vs. coverage**: Higher τ/σ → larger certified radius but more abstentions
  - **Granularity vs. stability**: Lower K → finer details but higher ranking instability under noise
  - **Input layer vs. final layer**: Input layer → finer spatial resolution but more fragile; final layer → coarser but more robust
  - **Method choice**: RISE best overall; LRP strong on CNNs but not extended to ViT

- Failure signatures:
  - Near-zero %certified at input layer: Gradient-based methods (Grad, GB, IxG, IntGrad) → indicates attribution is highly noise-sensitive
  - High %certified but random localization: Grad/GB at final layer → grid-like constant patterns
  - All pixels abstain (⊘): σ too high for the attribution method's stability, or τ too strict for the sample count n
  - Certified but not faithful: High %certified but shallow deletion curve → certified pixels not actually important

- First 3 experiments:
  1. **Single-image certification sweep**: Run certification on one ImageNet image with RISE at K ∈ {5, 10, 25, 50} and σ ∈ {0.15, 0.25, 0.33}. Visualize overlayed certified maps to understand how K controls spatial granularity and how σ affects abstention rate.
  2. **Cross-method robustness comparison**: Certify all 12 methods on the same 10 images at K=30%, R=0.10. Compute %certified and Certified GridPG. Hypothesis: RISE ≈ LRP > Occlusion > activation methods > gradient methods at input layer.
  3. **Radius vs. certification rate curve**: For RISE and Grad-CAM, vary σ to achieve R ∈ {0.10, 0.17, 0.22} at K=50%. Plot %certified vs. R. Hypothesis: RISE maintains >50% certification at R=0.22; Grad-CAM drops sharply.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can pixel-level certification be achieved for attribution methods under ℓ∞-bounded perturbations?
- Basis in paper: The authors state their framework certifies robustness "against ℓ₂-bounded perturbations" because it relies on Randomized Smoothing with Gaussian noise.
- Why unresolved: Randomized Smoothing provides strict guarantees primarily for ℓ₂ radii; extending this to ℓ∞ requires different noise distributions or smoothing techniques not yet adapted for this specific segmentation-based certification pipeline.
- What evidence would resolve it: A theoretical extension of the certification theorem to ℓ∞ norms using appropriate noise distributions, followed by empirical verification of certification rates.

### Open Question 2
- Question: How can token-based attribution methods like LRP be effectively certified within Vision Transformer (ViT) architectures?
- Basis in paper: The authors note they excluded LRP and CAM from ViT experiments "due to the lack of direct extensions for these methods" to the token-based architecture used in transformers.
- Why unresolved: The current certification pipeline requires spatially aligned attribution maps. ViTs treat images as sequences of patches (tokens), making direct application of pixel-level certification logic to relevance propagation methods structurally difficult.
- What evidence would resolve it: A methodology that maps token-level relevance back to the pixel space in a way that preserves properties required for stable binarization and smoothing.

### Open Question 3
- Question: Does pixel-level certification improve performance in concrete downstream tasks compared to standard attribution maps?
- Basis in paper: The abstract claims the work creates "certified attribution maps that can be used in downstream tasks," but does not demonstrate specific downstream performance.
- Why unresolved: The paper evaluates robustness, localization, and faithfulness in isolation without showing that the "abstain" mechanism or certified maps lead to better outcomes in safety-critical workflows.
- What evidence would resolve it: Experiments showing that models trained on or guided by certified attributions outperform those guided by non-certified or merely robust attributions.

## Limitations
- Cannot provide meaningful certifications for gradient-based attribution methods (Grad, GB, IxG, IntGrad) at the input layer due to inherent noise sensitivity
- Higher certification thresholds and noise levels significantly reduce the percentage of certified pixels, potentially making explanations too sparse for practical use
- Computational cost requires 100+ Monte Carlo samples per pixel for certification, challenging for real-time deployment

## Confidence
- High confidence: The randomized smoothing mechanism for pixel-level certification is mathematically sound and follows established theoretical foundations
- Medium confidence: The comparison across 12 attribution methods is comprehensive, but some method implementations may have subtle differences affecting results
- Low confidence: The deletion-based faithfulness metric may not fully capture semantic importance

## Next Checks
1. **Single-image certification sweep**: Run certification on one ImageNet image with RISE at K ∈ {5, 10, 25, 50} and σ ∈ {0.15, 0.25, 0.33}. Visualize overlayed certified maps to understand how K controls spatial granularity and how σ affects abstention rate.
2. **Cross-method robustness comparison**: Certify all 12 methods on the same 10 images at K=30%, R=0.10. Compute %certified and Certified GridPG. Hypothesis: RISE ≈ LRP > Occlusion > activation methods > gradient methods at input layer.
3. **Radius vs. certification rate curve**: For RISE and Grad-CAM, vary σ to achieve R ∈ {0.10, 0.17, 0.22} at K=50%. Plot %certified vs. R to operationalize the robustness-coverage trade-off for method selection.