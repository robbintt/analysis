---
ver: rpa2
title: Automated Circuit Interpretation via Probe Prompting
arxiv_id: '2511.07002'
source_url: https://arxiv.org/abs/2511.07002
tags:
- features
- functional
- activation
- semantic
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces probe prompting, an automated method for
  interpreting neural network attribution graphs by grouping features into interpretable
  concept-aligned supernodes. The approach uses cross-prompt activation signatures
  to classify features as Semantic (content detectors), Relationship (structural binding),
  or Say-X (output promotion), with transparent rule-based thresholds.
---

# Automated Circuit Interpretation via Probe Prompting

## Quick Facts
- **arXiv ID**: 2511.07002
- **Source URL**: https://arxiv.org/abs/2511.07002
- **Reference count**: 9
- **Primary result**: Automated grouping of neural network features into interpretable supernodes (Semantic, Relationship, Say-X) achieves 83% completeness while outperforming geometric clustering baselines in functional coherence.

## Executive Summary
This paper introduces probe prompting, a method that automates the interpretation of neural network attribution graphs by grouping features into concept-aligned supernodes. The approach uses cross-prompt activation signatures to classify features based on their behavioral responses across varied probe prompts, with transparent rule-based thresholds. The method demonstrates that interpretability can be partially automated from hours to minutes while revealing systematic computational structure, particularly a layerwise hierarchy where early features encode transferable relational structure and late features specialize for output promotion.

## Method Summary
The method operates on attribution graphs generated from replacement models, using CLT features as the dictionary. It generates 5-10 probe prompts per concept that vary semantic content while preserving syntactic structure. For each candidate feature, activation patterns (peak token position, sparsity ratio, cosine similarity to seed) are measured across these probes. Features are then classified into Semantic (content detectors), Relationship (structural binding), or Say-X (output promotion) categories using transparent decision rules based on aggregated cross-prompt signatures. The process prioritizes interpretability and debuggability through explicit thresholds rather than learned classifiers.

## Key Results
- Achieves 83% completeness while compressing complexity from 600+ features to 30-50 interpretable supernodes
- Outperforms geometric clustering baselines with 2.3× higher peak-token consistency (0.425 vs 0.183) and 5.8× higher activation-pattern similarity (0.762 vs 0.130)
- Entity-swap experiments reveal early-vs-late layer hierarchy: early features transfer robustly (64% rate, mean layer 6.3), late Say-X features specialize for output promotion (mean layer 16.4)

## Why This Works (Mechanism)

### Mechanism 1: Cross-Prompt Activation Signatures
Grouping features by their behavioral responses across varied probe prompts yields functionally coherent supernodes, outperforming geometric clustering on behavioral metrics. For each candidate feature, the method measures activation patterns across 5-10 probe prompts that vary semantic content while preserving syntactic structure. Features are classified into Semantic, Relationship, or Say-X categories based on transparent rules applied to these aggregated cross-prompt signatures, not on raw activation vector proximity. Features serving similar functional roles exhibit similar behavioral profiles across context variations, even when their activation vectors are geometrically distant in high-dimensional space.

### Mechanism 2: Layerwise Computational Hierarchy (Backbone-and-Specialization)
Early-layer features encode transferable relational structure, while late-layer features specialize for entity-specific output promotion. In factual recall circuits, features detecting entities or relations concentrate in early layers and activate appropriately across entity substitutions. Features promoting specific outputs concentrate in late layers and fail to transfer. This creates staged computation: relational binding first, output selection second. The model's factual recall computation decomposes causally into a task-general "backbone" (entity detection, structural binding) and a task-specific "specialization" stage (token promotion).

### Mechanism 3: Rule-Based Classification for Interpretability
Transparent, hand-tuned decision rules for feature classification enable interpretability of the grouping process itself. Instead of learned classifiers (k-means, DBSCAN), the method uses explicit thresholds (e.g., peak consistency ≥0.80 for Semantic Dictionary; functional dominance ≥50% + layer ≥7 for Say-X). Each assignment traces to specific threshold crossings, enabling debugging and user adjustment. Interpretability tools must themselves be interpretable; users should understand why a feature received a given label and be able to adjust rules for domain-specific needs.

## Foundational Learning

- **Concept: Attribution graphs and replacement models**
  - **Why needed here:** The entire pipeline operates on attribution graphs, which formalize how features contribute to target logits via replacement models (frozen attention patterns, layer norms). Without this, the input format and "influence" metrics are opaque.
  - **Quick check question:** Can you explain why replacement models linearize residual stream pathways and what components they freeze?

- **Concept: Sparse Autoencoders (SAE) / Cross-Layer Transcoders (CLT)**
  - **Why needed here:** The paper uses CLT features as the dictionary for attribution graphs. These sparse, often monosemantic features make replacement-model analyses tractable. Understanding what a "feature" represents is prerequisite.
  - **Quick check question:** What does it mean for a CLT feature to be "monosemantic," and why does sparsity aid interpretability?

- **Concept: Functional vs. semantic tokens**
  - **Why needed here:** Say-X detection hinges on distinguishing functional tokens (copulas, articles, prepositions) from semantic tokens (content-bearing words). Misunderstanding this leads to incorrect target-token mapping.
  - **Quick check question:** In "The capital of Texas is Austin," which tokens are functional vs. semantic? For a feature peaking on "is," which direction finds the semantic target?

## Architecture Onboarding

- **Component map:** Attribution Graph Input → Node Selection → Probe Prompt Generation → Activation Measurement → Cross-Prompt Signature Computation → Rule-Based Classification → Subgraph Export

- **Critical path:** Graph generation → Node selection → Probe generation → Activation measurement → Classification → Subgraph export. Activation measurement is the bottleneck (~10-15 min); other steps are sub-minute.

- **Design tradeoffs:**
  - Completeness vs. Replacement: Subgraphs prioritize legibility (30-50 interpretable supernodes) over exhaustive coverage (600+ features). Completeness ~0.83; Replacement ~0.54.
  - Behavioral coherence vs. geometric compactness: Concept-aligned groups score lower on Silhouette (0.124) but higher on peak-token consistency (0.425 vs 0.183) and activation similarity (0.762 vs 0.130).
  - Rule-based vs. learned classification: Explicit thresholds enable debuggability but may miss edge cases that learned classifiers capture.

- **Failure signatures:**
  - Language mismatch: English functional token vocabulary applied to non-English prompts (e.g., French) produces incoherent activations and no valid supernodes.
  - Low cross-prompt stability: Features failing ≥60% stability threshold are marked "ungrouped" and excluded.
  - Mixed-behavior features: Features peaking on both semantic and functional tokens with comparable confidence may misclassify; conflict resolution uses weighted scoring.
  - Late-layer Say-X without target: Features peaking on functional tokens with no semantic token within window (default: 7 tokens) are excluded unless ≥90% functional confidence.

- **First 3 experiments:**
  1. **Reproduce Dallas→Austin:** Run the full pipeline on provided example data to verify setup and understand output formats. Compare your supernode labels to the paper's.
  2. **Test entity-swap transfer:** Run on Oakland→Sacramento and compare feature transfer to Dallas→Austin. Verify transferred features have lower mean layer (target: ~6.3) than failed features (target: ~16.4).
  3. **Threshold sensitivity:** Vary Say-X layer threshold (default: ≥7) by ±3 layers and observe classification count changes. Report how many features shift categories.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the observed early-vs-late layer hierarchy (backbone vs. specialization) generalize to complex reasoning tasks, such as chain-of-thought or moral reasoning, or is it specific to factual recall?
- **Basis in paper:** The authors state that testing the hierarchy across diverse task families (arithmetic, code generation, moral reasoning) is a "key next step" to determine if this is a "domain-general principle."
- **Why unresolved:** The current validation relies entirely on short factual recall prompts (e.g., capitals, sports entities), which may have distinct computational structures compared to multi-step reasoning.
- **What evidence would resolve it:** Application of probe prompting to non-recall circuits (e.g., mathematical reasoning or code generation) showing consistent early-layer transfer rates and late-layer specialization.

### Open Question 2
- **Question:** Do the concept-aligned supernodes reflect genuine causal mechanisms, or are they potentially spurious correlations introduced by the geometric clustering baselines?
- **Basis in paper:** The authors list "reliance on correlational validation" as a limitation and explicitly outline "pre-specified causal interventions" (ablations, steering, cross-circuit transfer) as future work.
- **Why unresolved:** The current results rely on correlational graph metrics (Replacement/Completeness) and activation similarity, which do not prove that the grouped features are functionally necessary for the behavior.
- **What evidence would resolve it:** Targeted ablation of specific supernodes (e.g., "Say-X" features) resulting in predictable changes to model output, or successful steering of model behavior by activating specific supernode groups.

### Open Question 3
- **Question:** Can the probe prompting pipeline generalize effectively to multilingual contexts without requiring extensive, manual language-specific adaptation?
- **Basis in paper:** The paper notes a "notable failure" when applying the English pipeline to a French prompt, resulting in "incoherent activations," and admits the prototype "remains unreliable in multilingual contexts."
- **Why unresolved:** The method relies heavily on a predefined English "functional token" vocabulary (e.g., copulas like 'is', 'was') which lacks direct equivalents or requires different directionality rules in other languages.
- **What evidence would resolve it:** Successful automated classification of features in non-English prompts (e.g., French, Spanish) using automatically adapted or cross-lingual functional token vocabularies, achieving comparable peak-token consistency scores.

## Limitations
- Method's generalizability beyond factual recall circuits remains untested, with unknown applicability to complex reasoning tasks
- Rule-based thresholds are hand-tuned and may not transfer across models or domains, based on single-circuit validation
- Cross-lingual robustness is limited, with explicit failure on non-English prompts due to hardcoded functional token vocabularies

## Confidence
- **High Confidence**: Mechanism 1 (cross-prompt activation signatures outperform geometric clustering on behavioral metrics) - supported by direct Table 2 comparisons showing 2.3× higher peak-token consistency and 5.8× higher activation similarity
- **Medium Confidence**: Mechanism 2 (layerwise hierarchy: early transferable vs. late specialized features) - supported by Dallas→Oakland entity-swap experiments, but based on a single circuit pair with n=1 for each category
- **Medium Confidence**: Mechanism 3 (rule-based classification enables interpretability) - transparent rules are explicitly designed for debuggability, but threshold brittleness and edge-case handling remain open questions

## Next Checks
1. **Cross-Task Transfer**: Apply the full pipeline to a non-factual circuit (e.g., arithmetic reasoning or causal inference) and verify whether the early-layer transferable vs. late-layer specialized feature pattern persists. This tests if Mechanism 2 is task-general or circuit-specific.
2. **Threshold Sensitivity Analysis**: Systematically vary each classification threshold (±10-20%) across multiple circuits and measure impact on group assignments, completeness, and behavioral coherence metrics. This validates the robustness of Mechanism 3's rule-based approach.
3. **Multilingual Extension**: Adapt the functional token vocabulary for a non-English language (e.g., Spanish or Chinese) and run the pipeline on a translated version of the Dallas→Austin circuit. This tests whether cross-lingual failures are fixable through vocabulary adaptation.