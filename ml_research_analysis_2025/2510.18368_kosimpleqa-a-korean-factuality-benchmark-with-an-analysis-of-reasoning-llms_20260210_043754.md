---
ver: rpa2
title: 'KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs'
arxiv_id: '2510.18368'
source_url: https://arxiv.org/abs/2510.18368
tags:
- korean
- kosimpleqa
- llms
- simpleqa
- cultural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KoSimpleQA introduces the first Korean-focused factuality benchmark
  targeting cultural knowledge. It contains 1,000 short, unambiguous questions designed
  to challenge models while remaining easy to grade.
---

# KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs

## Quick Facts
- arXiv ID: 2510.18368
- Source URL: https://arxiv.org/abs/2510.18368
- Reference count: 6
- Korean factuality benchmark showing cultural knowledge creates model ranking reversals versus English benchmarks

## Executive Summary
KoSimpleQA introduces the first Korean-focused factuality benchmark targeting cultural knowledge. It contains 1,000 short, unambiguous questions designed to challenge models while remaining easy to grade. The benchmark was validated through multi-round human review and includes verifiable source links. Evaluation of diverse open-source LLMs showed performance capped at 33.7% correct, with Korean community models outperforming multilingual ones on KoSimpleQA—a reversal from English SimpleQA rankings. This divergence highlights KoSimpleQA's unique cultural dimension. Analysis of reasoning models revealed that reasoning capabilities improve latent knowledge elicitation and increase abstention when uncertain, though some models experienced performance drops due to catastrophic forgetting. KoSimpleQA provides new baselines for Korean factuality assessment and demonstrates that reasoning can enhance both precision and cautiousness in factual QA tasks.

## Method Summary
KoSimpleQA was developed through dual crowdsourcing platforms with cross-validation, expert review, and iterative guideline refinement. The benchmark contains 1,000 short-answer questions across 6+ categories with unambiguous answers and source links. Models were evaluated using SimpleQA-style grading with temperature=1.0, top_p=1.0, max_tokens=2048. Reasoning models ran in thinking mode, with incomplete responses classified as "Not Attempted." Performance metrics included Correct (CO), Not Attempted (NA), Incorrect (IN), Correct Given Attempted (CGA), and F-score.

## Key Results
- Korean community models (EXAONE, HCX SEED, kanana) outperform multilingual models on KoSimpleQA despite smaller parameter counts
- Reasoning models show increased abstention rates while preserving or improving correct responses, suggesting better uncertainty calibration
- EXAONE Deep experiences substantial performance drops due to catastrophic forgetting from reasoning fine-tuning
- Maximum correct rate across all models is 33.7%, indicating benchmark difficulty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data cultural composition creates specialized knowledge representations that outperform scale alone on culturally-grounded factuality tasks.
- Mechanism: Models trained predominantly on Korean corpora encode cultural knowledge more robustly than multilingual models with larger parameter counts but distributed training objectives.
- Core assumption: Cultural knowledge requires sufficient representation in pre-training data; it cannot emerge from linguistic competence alone.
- Evidence anchors: Korean community models outperform multilingual ones on KoSimpleQA; multilingual group includes largest model but none surpass Korean community models.
- Break condition: If multilingual models with retrieval augmentation match Korean community models, the mechanism shifts from parametric knowledge to retrieval quality.

### Mechanism 2
- Claim: Engaging reasoning capabilities increases both latent knowledge elicitation and uncertainty-aware abstention in factual QA.
- Mechanism: Extended "thinking" sequences provide additional computational steps for knowledge retrieval and self-verification. When models cannot confidently retrieve an answer, they enter extended thinking loops that consume token budgets, effectively abstaining rather than generating low-confidence outputs.
- Core assumption: Models possess latent knowledge that requires additional computation to access; reasoning traces serve as retrieval augmentation within the model.
- Evidence anchors: Rate of not attempted responses increased sharply while correct responses were largely preserved or slightly improved.
- Break condition: If reasoning models show increased correct answers without increased abstention, the mechanism may reflect improved calibration rather than computational retrieval.

### Mechanism 3
- Claim: Reasoning-specific fine-tuning can cause catastrophic forgetting of factual knowledge acquired during pre-training.
- Mechanism: When reasoning capabilities are introduced through additional fine-tuning, the optimization objective shifts toward reasoning patterns at the expense of factual recall.
- Core assumption: Factual knowledge and reasoning behavior compete for model capacity; fine-tuning on reasoning tasks overwrites factual associations.
- Evidence anchors: EXAONE Deep exhibited substantial drop in performance compared to base instruct version across most metrics.
- Break condition: If performance drops correlate with training data mixing ratios rather than fine-tuning itself, the mechanism reflects data imbalance rather than capacity constraints.

## Foundational Learning

- Concept: **SimpleQA-style benchmark design (short, unambiguous, verifiable)**
  - Why needed here: KoSimpleQA inherits design principles requiring single correct answers for reliable grading. Understanding this constraint explains why the benchmark cannot assess long-form factual generation.
  - Quick check question: Can you explain why unambiguous answers are necessary for reliable cross-model comparison?

- Concept: **Cultural vs. linguistic competence in LLMs**
  - Why needed here: The paper's central claim depends on distinguishing fluent Korean generation from culturally-grounded Korean knowledge. Without this distinction, the benchmark's value proposition is unclear.
  - Quick check question: A model generates grammatically correct Korean but incorrectly answers "Who is on the 50,000 won bill?"—is this a linguistic or cultural failure?

- Concept: **Abstention as uncertainty calibration**
  - Why needed here: The "Not Attempted" metric is central to evaluating reasoning models. Understanding abstention as a positive behavior (avoiding hallucination) rather than failure is essential for interpreting results.
  - Quick check question: If a model's "Not Attempted" rate doubles while "Correct" rate stays constant, is this an improvement?

## Architecture Onboarding

- Component map:
  Dataset -> Annotation pipeline -> Evaluation harness -> Metric computation
  Questions -> Crowdsourcing platforms -> Cross-validation -> Expert review -> Model inference -> Answer extraction -> Grading -> CO/NA/IN/CGA/F-score

- Critical path:
  1. Question quality filtering (difficulty requirement: at least one of four reference LLMs must fail)
  2. Source verification (up to 4 links per question)
  3. Cross-platform validation (Platform A data validated by Platform B annotators)
  4. Expert manual inspection and guideline iteration

- Design tradeoffs:
  - Constrained evaluation scope: Short answers ensure reliable grading but do not assess long-form factuality
  - Difficulty threshold: Requiring model failures ensures challenge but may bias toward niche knowledge
  - Temporal cutoff: December 2023 knowledge boundary ensures stability but limits currency

- Failure signatures:
  - High Incorrect (IN) rate with low Not Attempted (NA): Model hallucinates rather than abstaining
  - Reasoning models showing substantial CO drops: Catastrophic forgetting from reasoning fine-tuning
  - Large gap between CGA and F-score: High false-positive rate when attempting answers

- First 3 experiments:
  1. Baseline evaluation: Run Korean community LLMs and multilingual LLMs on KoSimpleQA to reproduce the ranking divergence pattern
  2. Reasoning ablation: Compare base instruct models vs. thinking-enabled versions, measuring CO, NA, and IN changes
  3. Category analysis: Break down performance by category to identify cultural knowledge domains where multilingual models systematically underperform

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does strong performance on short-form factuality benchmarks like KoSimpleQA predict a model's ability to generate longer, fact-rich responses?
- Basis in paper: The Limitations section states it does not capture whether the ability to generate factual short answers translates to producing longer, fact-rich responses.
- Why unresolved: KoSimpleQA constrains evaluation to single, verifiable answers; no analysis was conducted on extended generation tasks.
- What evidence would resolve it: A follow-up study correlating KoSimpleQA scores with performance on long-form factuality metrics or human evaluation of paragraph-length factual outputs.

### Open Question 2
- Question: Why do some reasoning-enhanced models (e.g., EXAONE Deep) suffer catastrophic forgetting in factual QA while others (e.g., Qwen3 think, HCX SEED think) maintain or improve performance?
- Basis in paper: The paper observes that EXAONE Deep exhibited a substantial drop in performance with catastrophic forgetting likely compromising factual reliability, but other reasoning models improved—without explaining the divergent mechanisms.
- Why unresolved: The analysis documents the phenomenon but does not isolate training procedures, architecture choices, or data mixtures responsible for the difference.
- What evidence would resolve it: Controlled experiments varying reasoning fine-tuning strategies across model families, with diagnostic probing of retained factual knowledge before and after fine-tuning.

### Open Question 3
- Question: Can training strategies be developed that improve performance on both culturally-specific benchmarks (KoSimpleQA) and general multilingual benchmarks (SimpleQA) simultaneously?
- Basis in paper: Figure 3 and main results show a ranking reversal: Korean community models excel on KoSimpleQA but underperform on SimpleQA, while multilingual models show the opposite pattern, suggesting a tradeoff.
- Why unresolved: The paper demonstrates the divergence exists but does not propose or test methods to close the gap in either direction.
- What evidence would resolve it: Training experiments with curriculum learning, balanced multilingual-cultural data mixtures, or domain adaptation techniques, evaluated on both benchmarks.

## Limitations
- Short-answer format limits assessment of long-form factual generation capabilities
- Temporal cutoff at December 2023 restricts evaluation of current knowledge
- Limited sample size (1,000 questions) may not fully capture cultural knowledge diversity

## Confidence
- High confidence: Benchmark design validity, data collection methodology, and basic evaluation pipeline
- Medium confidence: Performance rankings and comparative analysis between model families
- Low confidence: Mechanistic explanations for reasoning effects and catastrophic forgetting

## Next Checks
1. **Catastrophic forgetting validation**: Run controlled experiments comparing reasoning-fine-tuned models with matched-parameter models trained on identical data distributions but without reasoning objectives. Measure factual recall retention across both groups to isolate the forgetting effect.

2. **Cultural knowledge encoding probe**: Design retrieval-augmented experiments where multilingual models with access to Korean cultural knowledge bases are compared against parametric Korean community models. If retrieval-augmented models match parametric models, cultural knowledge may be distributional rather than encoded.

3. **Reasoning mechanism dissection**: Implement token-level analysis of thinking traces to identify patterns distinguishing knowledge retrieval from uncertainty-based abstention. Compare models that show high abstention with those showing high correctness improvements to isolate the two mechanisms.