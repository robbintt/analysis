---
ver: rpa2
title: Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial
  Observability
arxiv_id: '2504.08417'
source_url: https://arxiv.org/abs/2504.08417
tags:
- learning
- agents
- belief
- state
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Belief-I2Q, a method for cooperative multi-agent
  reinforcement learning under partial observability. The core idea is to learn probabilistic
  belief states over the underlying system state using a conditional variational autoencoder
  (CVAE) trained on offline state-labeled data.
---

# Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability

## Quick Facts
- arXiv ID: 2504.08417
- Source URL: https://arxiv.org/abs/2504.08417
- Authors: Paul J. Pritz; Kin K. Leung
- Reference count: 40
- Primary result: Belief-I2Q outperforms recurrent baselines in most grid-world environments by separating belief learning from RL using a pre-trained CVAE.

## Executive Summary
This paper introduces Belief-I2Q, a method for cooperative multi-agent reinforcement learning under partial observability that learns probabilistic belief states using a conditional variational autoencoder (CVAE) trained on offline state-labeled data. The approach separates belief learning from policy learning, enabling fully decentralized training and execution without requiring communication. Experiments on four grid-world environments show that Belief-I2Q outperforms recurrent baselines (Rec.-I2Q and Rec.-Hyst.-IQL) in most settings, with faster convergence and better final performance, especially in scenarios where agents can infer the state from observation histories.

## Method Summary
Belief-I2Q uses a two-stage approach: first pre-training a CVAE belief model on offline state-labeled data to learn probabilistic representations of the underlying system state from observation histories, then training decentralized I2Q-based Q-learning using these belief states as input. The CVAE outputs a distribution over states rather than point estimates, capturing both inferred state information and uncertainty. The I2Q framework uses state-state value functions to enable stable decentralized training by removing dependence on joint actions during value updates. This separation allows for stable representation learning before RL training begins.

## Key Results
- Belief-I2Q outperforms Rec.-I2Q and Rec.-Hyst.-IQL baselines in Oracle, Gathering, and Escape Room environments
- Faster convergence than recurrent baselines, particularly in Oracle environment where state can be inferred from histories
- Struggles in HoneyComb environment where information asymmetry prevents accurate belief inference from local histories
- Belief states capture both state information and uncertainty, though this advantage is not directly validated

## Why This Works (Mechanism)

### Mechanism 1: Offline Representation Grounding
A CVAE pre-trained on offline trajectories containing ground-truth states learns to map local observation histories to probabilistic belief states, creating stable state representations before RL training begins. This separation simplifies the policy learning task by providing well-structured inputs.

### Mechanism 2: State-State Value Functions (I2Q)
Using state-state value functions $Q_{ss}(s, s')$ enables stable decentralized training by removing dependence on joint actions during value updates. Since $s'$ is a physical outcome of the environment, it provides a stationary training target assuming cooperative dynamics.

### Mechanism 3: Uncertainty-Aware Policy Input
The CVAE outputs probabilistic distributions rather than point estimates, providing the policy with uncertainty information that may enable better decision-making under ambiguity. High variance beliefs could signal higher decision risk to the Q-function.

## Foundational Learning

- **Concept: Dec-POMDPs (Decentralized Partially Observable Markov Decision Processes)**
  - **Why needed here:** The paper explicitly defines the environment as a Dec-POMDP. Understanding that agents receive individual observations $o_i$ but act in a shared state $s$ is crucial.
  - **Quick check question:** Can you explain why standard Q-learning fails in a Dec-POMDP if agents treat each other as static environment dynamics?

- **Concept: Variational Autoencoders (VAEs) & ELBO**
  - **Why needed here:** The core "Belief" module is a CVAE trained using the Evidence Lower Bound. Understanding the balance between reconstruction accuracy and KL divergence is necessary to debug the pre-training phase.
  - **Quick check question:** In the loss function $L_{CVAE}$, what does the KL divergence term actually penalize?

- **Concept: Non-stationarity in MARL**
  - **Why needed here:** The paper motivates the use of I2Q specifically to handle non-stationarity. You must understand that as other agents learn, the environment transition function appears to change from any single agent's perspective.
  - **Quick check question:** Why does a centralized critic (CTDE) solve non-stationarity, and how does Belief-I2Q solve it without one?

## Architecture Onboarding

- **Component map:** GRU History Encoder -> CVAE (Encoder/Decoder) -> Belief Distribution (Mean/Log-var) -> RL Core (Qss, Transition f, Q)

- **Critical path:**
  1. Pre-training: Collect random rollouts with full state labels. Train CVAE + History Encoder to reconstruct $s$ from $h_i$. Discard CVAE Encoder, keep Decoder/History Encoder.
  2. Initialization: Freeze Belief Model (History Encoder + Decoder).
  3. RL Training: Agent interacts with env using local $o_i$. Belief module generates $b(h_i)$. Train $Q_{ss}$, $f_i$, and $Q_i$ using specified losses.

- **Design tradeoffs:**
  - Offline Data Requirement: Needs labeled dataset before learning a policy, which is safer for representation learning but harder to scale than end-to-end learning.
  - Determinism Assumption: The I2Q backbone assumes deterministic transitions, which may limit performance in stochastic environments.

- **Failure signatures:**
  - High Variance Beliefs: If belief visualization shows high variance even after informative events, the History Encoder is failing to integrate temporal data.
  - Lagging $Q_{ss}$: If $Q_{ss}$ fails to converge, the transition function $f_i$ will have no gradient signal to follow.
  - HoneyComb-style Failure: If performance degrades with more agents, check for information asymmetry which this architecture cannot solve via local history alone.

- **First 3 experiments:**
  1. Belief Reconstruction Accuracy: Before RL, validate the CVAE by plotting reconstructed state vs. ground truth to ensure it learns to "fill in" unobserved grid squares.
  2. Baseline Comparison (Rec-I2Q): Replicate the "Oracle" environment comparison to verify that Belief-I2Q converges faster than Rec-I2Q.
  3. Ablation on Samples ($m$): Run a sweep on the number of belief samples to see if averaging samples (reducing noise) is critical for $Q_{ss}$ stability.

## Open Questions the Paper Calls Out
None

## Limitations
- Depends on expensive offline state-labeled data for pre-training the belief model, which may not scale to complex environments
- Assumes deterministic transitions through the I2Q backbone, potentially limiting performance in stochastic environments
- Fails in information-asymmetric settings (HoneyComb) where agents cannot infer state from local histories alone

## Confidence

- **High confidence:** The technical framework (CVAE-based belief learning + I2Q policy) is well-specified and reproducible
- **Medium confidence:** The claim that separating belief learning from RL improves sample efficiency is supported by Oracle results but lacks full mechanistic explanation
- **Low confidence:** The performance degradation in HoneyComb is attributed to information asymmetry, but alternative belief architectures are not explored

## Next Checks

1. **Belief model validation:** Before RL training, validate the CVAE's ability to reconstruct ground-truth states from observation histories across all environments, plotting reconstruction error vs. history length.

2. **Ablation on sample averaging:** Systematically vary the number of belief samples (m=1, 5, 10, 20) in the belief construction step to quantify the impact of uncertainty averaging on Q-learning stability.

3. **Stochastic environment test:** Modify the Oracle environment to include stochastic transitions and evaluate whether Belief-I2Q performance degrades relative to baselines, testing the I2Q determinism assumption.