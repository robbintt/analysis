---
ver: rpa2
title: 'From Binary to Continuous: Stochastic Re-Weighting for Robust Graph Explanation'
arxiv_id: '2508.01925'
source_url: https://arxiv.org/abs/2508.01925
tags:
- explanation
- graph
- graphs
- edges
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the distributional shift problem in graph
  neural network explanation methods, where GNNs trained on unweighted graphs struggle
  to produce reliable predictions on weighted graphs used during explanation extraction.
  The authors propose STORE (Graph Explanation with Stochastic Re-weighting), an iterative
  framework that alternates between explanation subgraph identification and model
  adaptation.
---

# From Binary to Continuous: Stochastic Re-Weighting for Robust Graph Explanation

## Quick Facts
- arXiv ID: 2508.01925
- Source URL: https://arxiv.org/abs/2508.01925
- Reference count: 28
- Primary result: STORE improves GNN explanation AUC-ROC by up to 12.8% on BA-2motifs

## Executive Summary
This paper addresses the distributional shift problem in graph neural network explanation methods, where GNNs trained on unweighted graphs struggle to produce reliable predictions on weighted graphs used during explanation extraction. The authors propose STORE (Graph Explanation with Stochastic Re-weighting), an iterative framework that alternates between explanation subgraph identification and model adaptation. The method assigns importance-aware edge weights to explanatory and non-explanatory edges using truncated Gaussian distributions, then retrains the GNN on these weighted graphs to improve consistency. Experiments across five benchmark datasets with GCN and GIN backbones show that STORE consistently improves explanation quality and stability compared to vanilla baselines.

## Method Summary
STORE is an iterative framework that alternates between explanation extraction and model adaptation to address distributional shift in GNN explanations. The method samples edge weights from truncated Gaussian distributions (TN(μ₁,σ²;0,1) for explanatory edges and TN(μ₂,σ²;0,1) for non-explanatory edges) with separation constraint Δμ=0.5. These weights are used to retrain the GNN on a union of original and weighted graphs, improving consistency between training and explanation phases. The framework uses a progressive top-k shrinking strategy (0.9→0.1) across L iterations to refine explanations, and employs the GIB objective with fidelity-compactness trade-off for explainer training.

## Key Results
- STORE achieves up to 12.8% AUC-ROC improvement for PGExplainer on BA-2motifs dataset
- STORE achieves up to 11.4% AUC-ROC improvement for ProxyExplainer on BA-2motifs dataset
- STORE enhances stability and produces more faithful explanations compared to vanilla baselines across all five benchmark datasets

## Why This Works (Mechanism)
The distributional shift between unweighted training graphs and weighted explanation graphs causes GNN explanation methods to produce unreliable results. By iteratively retraining the GNN on weighted graphs that reflect the importance structure of explanatory subgraphs, STORE bridges this gap. The truncated Gaussian sampling with separation constraint ensures clear distinction between explanatory and non-explanatory edges while maintaining valid probability distributions. This adaptation process allows the GNN to better handle the weighted graphs used during explanation extraction, resulting in more consistent and faithful explanations.

## Foundational Learning
- **Truncated Gaussian distributions**: Needed for sampling edge weights while maintaining valid probability ranges [0,1]; quick check: verify sampling implementation correctly handles truncation boundaries
- **GIB (Graph Information Bottleneck)**: Objective function balancing fidelity and compactness in explanations; quick check: confirm λ parameter appropriately trades off these competing objectives
- **Top-k shrinking strategy**: Progressive refinement of explanation subgraphs across iterations; quick check: monitor AUC-ROC per iteration to detect over-pruning
- **Distributional shift**: Core problem addressed where model trained on unweighted graphs fails on weighted explanation graphs; quick check: compare baseline vs. STORE performance on weighted graphs
- **GNN retraining with weighted graphs**: Key adaptation mechanism that aligns training and explanation phases; quick check: verify both original and weighted graphs are included in training

## Architecture Onboarding
**Component Map**: Dataset → GNN Backbone → Explainer → STORE Iterative Loop → Weighted Graphs → Retrained GNN → Updated Explainer

**Critical Path**: Training baseline GNN → Initial explainer training → STORE iterations (weight sampling → GNN retraining → explainer update) → Final explanation extraction

**Design Tradeoffs**: Iterative refinement vs. computational cost; separation constraint Δμ vs. flexibility in weight assignment; top-k shrinking aggressiveness vs. explanation completeness

**Failure Signatures**: Performance degradation after iteration 3-4 (over-pruning), high variance across random seeds (sampling issues), model misclassification on weighted graphs (incomplete retraining)

**First Experiments**:
1. Verify truncated Gaussian sampling correctly implements separation constraint with Δμ=0.5
2. Test baseline performance gap between unweighted and weighted graphs before STORE
3. Monitor AUC-ROC improvement per iteration to determine optimal stopping point

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Several critical hyperparameters are underspecified, including hidden dimensions, GIB objective weight λ, and exact top-k shrinking schedule
- Default separation probability α for Gaussian sampling is mentioned but not explicitly set
- Generalization to other GNN architectures beyond GCN and GIN remains untested
- Computational overhead of iterative retraining may limit scalability to larger graphs

## Confidence
**High confidence**: The core STORE framework and its theoretical motivation regarding distributional shift are well-defined and internally consistent
**Medium confidence**: Experimental results showing 12.8% AUC-ROC improvement on BA-2motifs, given that methodology is sound but depends on unspecified hyperparameters
**Low confidence**: Generalization to other GNN architectures or datasets beyond the five tested

## Next Checks
1. Verify that truncated Gaussian sampling correctly implements the separation constraint with Δμ=0.5 and default α=0.05
2. Test sensitivity to iteration count L by monitoring AUC-ROC degradation beyond L=3-5 iterations
3. Validate the fidelity-compactness trade-off by sweeping λ values in the GIB objective